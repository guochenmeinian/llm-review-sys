# From Tempered to Benign Overfitting

in ReLU Neural Networks

 Guy Kornowski

Weizmann Institute of Science

guy.kornowski@weizmann.ac.il

&Gilad Yehudai

Weizmann Institute of Science

gilad.yehudai@weizmann.ac.il

&Ohad Shamir

Weizmann Institute of Science

ohad.shamir@weizmann.ac.il

###### Abstract

Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as "tempered overfitting", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the overfitting profile in this setting, which we also validate empirically for intermediate dimensions. Overall, our results shed light on the intricate connections between the dimension, sample size, architecture and training algorithm on the one hand, and the type of resulting overfitting on the other hand.

## 1 Introduction

Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. Although quite standard in deep learning, the so-called interpolation learning regime challenges classical statistical wisdom regarding overfitting, and has attracted much attention in recent years.

In particular, the phenomenon commonly referred to as "benign overfitting" (Bartlett et al., 2020) describes a situation in which a learning method overfits, in the sense that it achieves zero training error over inherently noisy samples, yet it is able to achieve near-optimal accuracy with respect to the underlying distribution. So far, much of the theoretical work studying this phenomenon focused on linear (or kernel) regression problems using the squared loss, with some works extending this to classification problems. However, there is naturally much interest in gradually pushing this research towards neural networks. For example, Frei et al. (2023) recently showed that the implicit bias of gradient-based training algorithms towards margin maximization, as established in a series of works throughout the past few years, leads to benign overfitting of leaky ReLU networks in high dimensions (see discussion of related work below).

Recently, Mallinar et al. (2022) suggested a more nuanced view, coining the notion of "tempered" overfitting. An interpolating learning method is said to overfit in a tempered manner if its error (withrespect to the underlying distribution, and as the training sample size increases) is bounded away from the optimal possible error yet is also not "catastrophic", e.g., better than a random guess. To be more concrete, consider a binary classification setting (with labels in \(\{ 1\}\)), and a data distribution \(\), where the output labels correspond to some ground truth function \(f^{*}\) belonging to the predictor class of interest, corrupted with independent label noise at level \(p(0,)\) (i.e., given a sampled point \(\), its associated label \(y\) equals \((f^{*}())\) with probability \(1-p\), and \(-(f^{*}())\) with probability \(p\)). Suppose furthermore that the training method is such that the learned predictor achieves zero error on the (inherently noisy) training data. Finally, let \(L_{}\) denote the "clean" test error, namely \(L_{}(N)=_{}(N() f^{*}( ) 0)\) for some predictor \(N\) (omitting the contribution of the label noise). In this setting, benign, catastrophic or tempered overfitting corresponds to a situation where the clean test error \(L_{}\) of the learned predictor approaches \(0\), \(\), or some value in \((0,)\) respectively. In the latter case, the clean test error typically scales with the amount of noise \(p\). For example, for the one-nearest-neighbor algorithm, it is well-known that the test error asymptotically converges to \(2p(1-p)\) in our setting (Cover and Hart, 1967), which translates to a clean test error of \(p\). Mallinar et al. (2022) show how a similar behavior (with the error scaling linearly with the noise level) occurs for kernel regression, and provided empirical evidence that the same holds for neural networks.

The starting point of our paper is an intriguing experiment from that paper (Figure (b)b), where they considered the following extremely simple data distributioin: The inputs are drawn uniformly at random from the unit sphere, and \(f^{*}\) is the constant \(+1\) function. This is perhaps the simplest possible setting where benign or tempered overfitting in binary classification may be studied. In this setting, the authors show empirically that a three-layer vanilla neural network exhibits tempered overfitting, with the clean test error almost linear in the label noise level \(p\). Notably, the experiment used an input dimension of \(10\), which is rather small. This naturally leads to the question of whether we can rigorously understand the overfitting behavior of neural networks in this simple setup, and how problem parameters such as the input dimension, number of samples and the architecture affect the type of overfitting.

Our contributions:In this work we take a step towards rigorously understating the regimes under which simple, 2-layer ReLU neural networks exhibit different types of overfitting (i.e. benign, catastrophic or tempered), depending on the problem parameters, for the simple and natural data distribution described in the previous paragraph. We focus on interpolating neural networks trained with exponentially-tailed losses, which are well-known to converge to KKT points of a max-margin problem (see Section 2 for more details). At a high level, our main conclusion is that for such networks, the input dimension plays a crucial role, with the type of overfitting gradually transforming from tempered to benign as the dimension increases. In contrast, most of our results are not sensitive to the network's width, as long as it is sufficient to interpolate the training data. In a bit more detail, our contributions can be summarized as follows:

* **Tempered overfitting in one dimension (Theorem 3.1 and Theorem 3.2).** We prove that in one dimension (when the inputs are uniform over the unit interval), the resulting overfitting is provably tempered, with a clean test error scaling as \(((p))\). Moreover, under the stronger assumption that the algorithm converges to a local minimum of the max-margin problem, we show that the clean test error provably scales linearly with \(p\). As far as we know, this is the first result provably establishing tempered overfitting for non-linear neural networks.
* **Benign overfitting in high dimensions (Theorem 4.1 and Theorem 4.3).** We prove that when the inputs are sampled uniformly from the unit sphere (and with the dimension scaling polynomially with the sample size), the resulting overfitting will generally be benign. In particular, we show that convergence to a max-margin predictor (up to some constant factor) implies that the clean test error decays exponentially fast to 0 with respect to the dimension. Furthermore, under an assumption on the scaling of the noise level and the network width, we obtain the same result for any KKT point of the max-margin problem.
* **The role of bias terms and catastrophic overfitting (Section 4.2).** The proof of the results mentioned earlier crucially relied on the existence of bias parameters in the network architecture. Thus, we further study the case of a bias-less network, and prove that without such biases, neural networks may exhibit catastrophic overfitting, and that in general they do not overfit benignly without further assumptions. We consider this an interesting illustration of how catastrophic overfitting is possible in neural networks, even in our simple setup.

* **Empirical study for intermediate dimensions (Section 5).** Following our theoretical results, we attempt at empirically bridging the gap between the one-dimensional and high dimensional settings. In particular, it appears that the tempered and overfitting behavior extends to a wider regime than what our theoretical results formally cover, and that the overfitting profile gradually shifts from tempered to benign as the dimension increases. This substantially extends the prior empirical observation due to Mallinar et al. (2022, Figure 6b), which exhibited tempered overfitting in input dimension \(10\).

We note that in an independent and concurrent work, Joshi et al. (2023) studied ReLU neural networks with one-dimensional inputs, similar to the setting we study in Section 3, and proved that tempered and catastrophic overfitting can occur for possibly non-constant target functions, depending on the loss function. However, they focus on a _regression_ setting, and on min-norm predictors, whereas our results cover classification and any KKT point of the max-magnin problem. Moreover, the architecture considered differs somewhat from our work, with Joshi et al. including an untrained skip connection and an untrained bias term.

### Related work

Following the empirical observation that modern deep learning methods can perfectly fit the training data while still performing well on test data (Zhang et al., 2017), many works have tried to provide theoretical explanations of this phenomenon. By now the literature on this topic is quite large, and we will only address here the works most relevant to this paper (for a broader overview, see for example the surveys Belkin 2021, Bartlett et al. 2021, Vardi 2022).

There are many works studying regression settings in which interpolating methods, especially linear and kernel methods, succeed at obtaining optimal or near-optimal performance (Belkin et al., 2018, 2019, 2020, Mei and Montanari, 2022, Hastie et al., 2022). In particular, it is interesting to compare our results to those of Rakhlin and Zhai (2019), Beaglehole et al. (2023), who proved that minimal norm interpolating (i.e. "ridgeless") kernel methods are not consistent in any fixed dimension, while they can be whenever the dimension scales with the sample size (under suitable assumptions) (Liang and Rakhlin, 2020). Our results highlight that the same phenomenon occurs for ReLU networks.

There is by now also a large body of work on settings under which benign overfitting (and analogous definitions thereof) occurs (Nagarajan and Kolter, 2019, Bartlett et al., 2020, Negrea et al., 2020, Nakkiran and Bansal, 2020, Yang et al., 2021, Koehler et al., 2021, Bartlett and Long, 2021, Muthukumar et al., 2021, Bachmann et al., 2021, Zhou et al., 2023, Shamir, 2023). In classification settings, as argued by Muthukumar et al. (2021), many existing works on benign overfitting analyze settings in which max-margin predictors can be computed (or approximated), and suffice to ensure benign behavior (Poggio and Liao, 2019, Montanari et al., 2019, Thrampoulidis et al., 2020, Wang and Thrampoulidis, 2021, Wang et al., 2021, Cao et al., 2021, Hu et al., 2022, McRae et al., 2022, Liang and Recht, 2023). It is insightful to compare this to our main proof strategy, in which we analyze the max-margin problem in the case of NNs (which no longer admits a closed form). Frei et al. (2022) showed that NNs with smoothed leaky ReLU activations overfit benignly when the data comes from a well-seperated mixture distribution, a result which was recently generalized to ReLU activations (Xu and Gu, 2023). Similarly, Cao et al. (2022) proved that convolutional NNs with smoothed activations overfit benignly when the data is distributed according to a high dimensional Guassian with noisy labels, which was subsequently generalized to ReLU CNNs (Kou et al., 2023). We note that these distributions are reminiscent of the setting we study in Section 4 for (non convolutional) ReLU NNs. Chatterji and Long (2023) studied benign overfitting for deep linear networks.

As mentioned in the introduction, Mallinar et al. (2022) formally introduced the notion of tempered overfitting and suggested the study of it in the context of NNs. Subsequently, Manoj and Srebro (2023) proved that the minimum description length learning rule exhibits tempered overfitting, though noticeably this learning rule does not explicitly relate to NNs. As previously mentioned, we are not aware of existing works that prove tempered overfitting in the context of NNs.

In a parallel line of work, the implicit bias of gradient-based training algorithms has received much attention (Soudry et al., 2018, Lyu and Li, 2020, Ji and Telgarsky, 2020). Roughly speaking, these results drew the connection between NN training to margin maximization - see Section 2 for a formal reminder. For our first result (Theorem 3.1) we build upon the analysis of Safran et al. (2022) who studied this implicit bias for univariate inputs, though for the sake of bounding the number of linear regions. In our context, Frei et al. (2023) utilized this bias to prove benign overfitting for leaky ReLU NNs in a high dimensional regime.

## 2 Preliminaries

Notation.We use bold-faced font to denote vectors, e.g. \(^{d}\), and denote by \(\|\|\) the Euclidean norm. We use \(c,c^{},,C>0\) etc. to denote absolute constants whose exact value can change throughout the proofs. We denote by \([n]:=\{1,,n\}\), by \(\{\}\) the indicator function, and by \(^{d-1}^{d}\) the unit sphere. Given sets \(A B\), we denote by \((A)\) the uniform measure over a set \(A\), by \(A^{c}\) the complementary set, and given a function \(f:B\) we denote its restriction \(f|_{A}:A\). We let \(I\) be the identity matrix whenever the dimension is clear from context, and for a matrix \(A\) we denote by \(\|A\|\) its spectral norm. We use standard big-O notation, with \(O(),()\) and \(()\) hiding absolute constants, write \(f g\) if \(f=O(g)\), and denote by \(()\) polynomial factors.

Setting.We consider a classification task based on _noisy_ training data \(S=(_{i},y_{i})_{i=1}^{m}^{d}\{ 1\}\) drawn i.i.d. from an underlying distribution \(:=_{}_{y}\). Throughout, we assume that the output values \(y\) are constant \(+1\), corrupted with independent label noise at level \(p\) (namely, each \(y_{i}\) is independent of \(_{i}\) and satisfies \([y_{i}=1]=1-p,\ [y_{i}=-1]=p\)). Note that the Bayes-optimal predictor in this setting is \(f^{*} 1\). Given a dataset \(S\), we denote by \(I_{+}:=\{i[m]:y_{i}=1\}\) and \(I_{-}=\{i[m]:y_{i}=-1\}\). We study 2-layer ReLU networks:

\[N_{}()=_{j=1}^{n}v_{j}(_{j} +b_{j})\;,\]

where \(n\) is the number of neurons or network width, \((z):=\{0,z\}\) is the ReLU function, \(v_{j},b_{j},\ _{j}^{d}\) and \(=(v_{j},_{j},b_{j})_{j=1}^{n}^{(d+2 )n}\) is a vectorized form of all the parameters. Throughout this work we assume that the trained network classifies the entire dataset correctly, namely \(y_{i}N_{}(_{i})>0\) for all \(i[m]\). Following Mallinar et al. (2022, Section 2.3) we consider the _clean_ test error

\[L_{}(N_{}):=_{_{ }}[N_{}() 0]\;,\]

corresponding to the misclassification error with respect to the clean underlying distribution. It is said that \(N_{}\) exhibits benign overfitting if \(L_{} 0\) with respect to large enough problem parameters (e.g. \(m,d\)), while the overfitting is said to be _catastrophic_ if \(L_{}\). Formally, Mallinar et al. (2022) have coined tempered overfitting to describe any case in which \(L_{}\) converges to a value in \((0,)\), though a special attention has been given in the literature to cases where \(L_{}\) scales monotonically or even linearly with the noise level \(p\)(Belkin et al., 2018; Chatterji and Long, 2021; Manoj and Srebro, 2023).

Implicit bias.We will now briefly describe the results of Lyu and Li (2020); Ji and Telgarsky (2020) which shows that under our setting, training the network with respect to the logistic or exponential loss converges towards a KKT point of the _margin-maximization problem_. To that end, suppose \((z)=(1+e^{-z})\) or \((z)=e^{-z}\), and consider the empirical loss \(}()=_{i=1}^{n}(y_{i}N_{}(_{i}))\). Suppose that \((t)\) evolves according to the gradient flow of the empirical loss, namely \((t)}{dt}=-}((t))\).1 Note that this corresponds to performing gradient descent over the data with an infinitesimally small step size.

**Theorem 2.1** (Rephrased from Lyu and Li, 2020; Ji and Telgarsky, 2020).: _Under the setting above, if there exists some \(t_{0}\) such that \((t_{0})\) satisfies \(_{i[m]}y_{i}N_{(t_{0})}(_{i})>0\), then \((t)}{\|(t)\|}}{{}}}{\| \|}\) for \(\) which is a KKT point of the margin maximization problem_

\[\|\|^{2} y_{i}N_{ }(_{i}) 1 i[m]\;\;.\] (1)The result above shows that although there are many possible parameters \(\) that result in a network correctly classifying the training data, the training method has an _implicit bias_ in the sense that it yields a network whose parameters are a KKT point of Problem (1). Recall that \(\) is called a KKT point of Problem (1) if there exist \(_{1},,_{m}\) such that the following conditions hold:

\[=_{i=1}^{m}_{i}y_{i}_{}N_{ }(_{i})\] (stationarity) (2) \[ i[m]:\;\;y_{i}N_{}(_{i}) 1 \] (3) \[_{1},,_{m} 0 \] (4) \[ i[m]:\;\;_{i}(y_{i}N_{}(_ {i})-1)=0 \] (5)

It is well-known that global or local optima of Problem (1) are KKT points, but in general, the reverse direction may not be true, even in our context of 2-layer ReLU networks (see Vardi et al., 2022). In this work, our results rely either on convergence to a KKT point (Theorem 3.1 and Theorem 4.3), or on stronger assumptions such as convergence to a local optimum (Theorem 3.2) or near-global optimum (Theorem 4.1) of Problem (1) in order to obtain stronger results.

## 3 Tempered overfitting in one dimension

Throughout this section we study the one dimensional case \(d=1\) under the uniform distribution \(_{x}=()\), so that \(L_{}(N_{})=_{()}[N_{}(x) 0]\). We note that although we focus here on the uniform distribution for simplicity, our proof techniques are applicable in principle to other distributions with bounded density. Further note that both results to follow are independent of the number of neurons, so that in our setting, tempered overfitting occurs as soon as the network is wide enough to interpolate the data. We first show that any KKT point of the margin maximization problem (and thus any network we may converge to) gives rise to tempered overfitting.

**Theorem 3.1**.: _Let \(d=1\), \(p[0,)\). Then with probability at least \(1-\) over the sample \(S^{m}\), for any KKT point \(=(v_{j},w_{j},b_{j})_{j=1}^{n}\) of Problem (1), it holds that_

\[c(p^{5}-}) L_{}( N_{}) C(+} )\;,\]

_where \(c,C>0\) are absolute constants._

The theorem above shows that for a large enough sample size \(L_{}(N_{})=((p))\), proving that the clean test error must scale roughly monotonically with the noise level \(p\), leading to tempered overfitting.2

We will now provide intuition for the proof of Theorem 3.1, which appears in Appendix A.1. The proofs of both the lower and the upper bounds rely on the analysis of Safran et al. (2022) for univariate networks that satisfy the KKT conditions. For the lower bound, we note that with high probability over the sample \((x_{i})_{i=1}^{m}_{x}^{m}\), approximately \(p^{5}m\) of the sampled points will be part of a sequence of 5 consecutive points \(x_{i}<<x_{i+4}\) all labeled \(-1\). Based on a lemma due to Safran et al. (2022), we show any such sequence must contain a segment \([x_{j},x_{j+1}],\;i j i+3\) for which \(N_{}|_{[x_{j},x_{j+1}]}<0\), contributing to the clean test error proportionally to the length of \([x_{j},x_{j+1}]\). Under the probable event that most samples are not too close to one another, namely of distance \((1/m)\), we get that the total length of all such segments (and hence the error) is at least of order \((}{m})=(p^{5})\).

As to the upper bound, we observe that the number of neighboring samples with opposing labels is likely to be on order of \(pm\), which implies that the data can be interpolated using a network of width of order \(n^{*}=O(pm)\). We then invoke a result of Safran et al. (2022) that states that the class of interpolating networks that satisfy the KKT conditions has VC dimension \(d_{}=O(n^{*})=O(pm)\). Thus, a standard uniform convergence result for VC classes asserts that the test error would be of order \(_{i[m]}\{(N_{})(x_{ i}) y_{i}\}+O(}/m})=0+O()=O( )\).

While the result above establishes the overfitting being tempered with an error scaling as \((p)\), we note that the range \([cp^{5},C]\) is rather large. Moreover, previous works suggest that in many cases we should expect the clean test error to scale _linearly_ with \(p\)(Belkin et al., 2018b; Chatterji and Long, 2021; Manoj and Srebro, 2023). Therefore it is natural to conjecture that this is also true here, at least for trained NNs that we are likely to converge to. We now turn to show such a result under a stronger assumption, where instead of any KKT point of the margin maximization problem, we consider specifically local minima. We note that several prior works have studied networks corresponding to global minima of the max-margin problem, which is of course an even stronger assumption than local minimality (e.g. Savarese et al., 2019; Boursier and Flammarion, 2023). We say that \(\) is a local minimum of Problem (1) whenever there exists \(r>0\) such that if \(\|^{}-\|<r\) and \( i[m]:y_{i}N_{^{}}(x_{i}) 1\) then \(\|\|^{2}\|^{}\|^{2}\).3

**Theorem 3.2**.: _Let \(d=1\), \(p[0,)\). Then with probability at least \(1-\) over the sample \(S^{m}\), for any local minimum \(=(v_{j},w_{j},b_{j})_{j=1}^{n}\) of Problem (1), it holds that_

\[c(p-}) L_{}(N_{ }) C(p+})\,\]

_where \(c,C>0\) are absolute constants._

The theorem above indeed shows that whenever the sample size is large enough, the clean test error indeed scales linearly with \(p\).4 The proof of Theorem 3.2, which appears in Appendix A.2, is based on a different analysis than that of Theorem 3.1. Broadly speaking, the proof relies on analyzing the structure of the learned prediction function, assuming it is a local minimum of the max-margin problem. More specifically, in order to obtain the lower bound we consider segments in between samples \(x_{i-1}<x_{i}<x_{i+1}\) that are labeled \(y_{i-1}=1,y_{i}=-1,y_{i+1}=1\). Note that with high probability over the sample, approximately \(p(1-p)^{2}=(p)\) of the probability mass lies in such segments. Our key proposition is that for a KKT point which is a local minimum, \(N_{}()\) must be linear on the segment \([x_{i},x_{i+1}]\), and furthermore that \(N_{}(x_{i})=-1,N_{}(x_{i+1})=1\). Thus, assuming the points are not too unevenly spaced, it follows that a constant fraction of any such segment contributes to the clean test error, resulting in an overall error of \((p)\). In order to prove this proposition, we provide an exhaustive list of cases in which the network does _not_ satisfy the assertion, and show that in each case the norm can be locally reduced - see Figure 3 in the appendix for an illustration.

For the upper bound, a similar analysis is provided for segments \([x_{i},x_{i+1}]\) for which \(y_{i}=y_{i+1}=1\). Noting that with high probability over the sample an order of \((1-p)^{2}=1-O(p)\) of the probability mass lies in such segments, it suffices to show that along any such segment the network is positive - implying an upper bound of \(O(p)\) on the possible clean test error. Indeed, we show that along such segments, by locally minimizing parameter norm the network is incentivized to stay positive.

## 4 Benign overfitting in high dimensions

In this section we focus on the high dimensional setting. Throughout this section we assume that the dataset is sampled according to \(_{}=(^{d-1})\) and \(d m\), where \(m\) is the number of samples, so that \(L_{}(N_{})=_{( ^{d-1})}[N_{}() 0]\). We note that our results hold for other commonly studied distributions (see Remark 4.2). We begin by showing that if the network converges to the maximum margin solution up to some multiplicative factor, then benign overfitting occurs:

**Theorem 4.1**.: _Let \(,>0\). Assume that \(p c_{1}\), \(m c_{2}\) and \(d c_{3}m^{2}()()\) for some universal constants \(c_{1},c_{2},c_{3}>0\). Given a sample \((_{i},y_{i})_{i=1}^{m}^{m}\), suppose \(=(_{j},v_{j},b_{j})_{j=1}^{n}\) is a KKT point of Problem (1) such that \(\|\|^{2}}{}\|^{*}\|^{2}\), where \(^{*}\) is a max-margin solution of Eq. (1) and \(c_{4}>0\) is some universal constant. Then, with probability at least \(1-\) over \(S^{m}\) we have that \(L_{}(N_{})\)._

The theorem above shows that under the specified assumptions, benign overfitting occurs for any noise level \(p\) which is smaller than some universal constant. Note that this result is independent of the number of neurons \(n\), and requires \(d=(m^{2})\). The mild lower bound on \(m\) is merely to ensure that the negative samples concentrate around a \((p)\) fraction of the entire dataset. Also note that the dependence on \(\) is only logarithmic, which means that in the setting we study, the clean test error decays exponentially fast with \(d\).

We now turn to provide a short proof intuition, while the full proof can be found in Appendix B.1. The main crux of the proof is showing that \(_{j=1}^{n}v_{j}(b_{j})=(1)\), i.e. the bias terms are the dominant factor in the output of the network, and they tend towards being positive. In order to see why this suffices to show benign overfitting, first note that \(N_{}()=_{j=1}^{n}v_{j}(_{j}^{} +b_{j})\) equals

\[_{j=1}^{n}v_{j}(_{j}^{}+b_{j})+_{j=1}^{ n}v_{j}(b_{j})-_{j=1}^{n}v_{j}(b_{j})_{j=1}^{n}v_{j} (b_{j})-_{j=1}^{n}|v_{j}(_{j}^{} )|\.\]

All the samples (including the test sample \(\)) are drawn independently from \((^{d-1})\), thus with high probability \(|^{}_{i}|=o_{d}(1)\) for all \(i[m]\). Using our assumption regarding convergence to the max-margin solution (up to some multiplicative constant), we show that the norms \(\|_{j}\|\) are bounded by some term independent of \(d\). Thus, we can bound \(|^{}_{j}|=o_{d}(1)\) for every \(j[n]\). Additionally, by the same assumption we show that both \(_{j=1}^{n}\|_{j}\|\) and \(_{j=1}^{n}|v_{j}|\) are bounded by the value of the max-margin solution (up to a multiplicative factor), which we bound by \(O()\). Plugging this into the displayed equation above, and using the assumption that \(d\) is sufficiently larger than \(m\), we get that the \(_{j=1}^{n}|v_{j}(_{j}^{})|\) term is negligible, and hence the bias terms \(_{j=1}^{n}v_{j}(b_{j})\) are the predominant factor in determining the output of \(N_{}\).

Showing that \(_{j=1}^{n}v_{j}(b_{j})=(1)\) is done in two phases. Recall the definitions of \(I_{+}:=\{i[m]:y_{i}=1\}\), \(I_{-}=\{i[m]:y_{i}=-1\}\), and that by our label distribution and assumption on \(p\) we have \(|I_{+}|(1-p)m pm|I_{-}|\). We first show that if the bias terms are too small, and the predictor correctly classifies the training data, then its parameter vector \(\) satisfies \(\|\|^{2}=(|I_{+}|)\). Intuitively, this is because the data is nearly mutually orthogonal, so if the biases are small, the vectors \(_{j}\) must have a large enough correlation with each different point in \(I_{+}\) to classify it correctly. On the other hand, we explicitly construct a solution that has large bias terms, which satisfies \(\|\|^{2}=O(|I_{-}|)\). By setting \(p\) small enough, we conclude that the biases cannot be too small.

**Remark 4.2** (Data distribution).: _Theorem 4.1 is phrased for data sampled uniformly from a unit sphere, but can be easily extended to other isotropic distributions such as standard Gaussian and uniform over the unit ball. In fact, the only properties of the distribution we use are that \(|_{i}^{}_{j}|=o_{d}(1)\) for every \(i j[m]\), and that \(\|XX^{}\|=O(1)\) (independent of \(m\)) where \(X\) is an \(m d\) matrix whose rows are \(_{i}\)._

### Benign overfitting under KKT assumptions

In Theorem 4.1 we showed that benign overfitting occurs in high dimensional data, but under the strong assumption of convergence to the max-margin solution up to a multiplicative factor (whose value is allowed to be larger for smaller values of \(p\)). On the other hand, Theorem 2.1 only guarantees convergence to a KKT point of the max-margin problem (1). We note that Vardi et al. (2022) provided several examples of ReLU neural networks where there are in fact KKT points which are not a global or even local optimum of the max-margin problem.

Consequently, in this section we aim at showing a benign overfitting result by only assuming convergence to a KKT point of the max-margin problem. For such a result we use two additional assumptions, namely: (1) The output weights \(v_{j}\) are all fixed to be \( 1\), while only \((_{j},b_{j})_{j[n]}\) are trained;5 and (2) Both the noise level \(p\) and the input dimension \(d\) depend on \(n\), the number of neurons. Our main result is the following:

**Theorem 4.3**.: _Let \(,>0\). Assume that \(p}{n^{2}}\), \(m c_{2}n()\) and \(d c_{3}m^{4}n^{4}()( }{})\) for some universal constants \(c_{1},c_{2},c_{3}>0\). Assume that the output weights are fixed so that \(|\{j:v_{j}=1\}|=|\{j:v_{j}=-1\}|\) while \((_{j},b_{j})_{j[n]}\) are trained, and that \(N_{}\) converges to a KKT point of Problem (1). Then, with probability at least \(1-\) over \(S^{m}\) we have \(L_{}(N_{})\)._Note that contrary to Theorem 4.1 and to the results from Section 3, here there is a dependence on \(n\), the number of neurons. For \(n=O(1)\) we get benign overfitting for any \(p\) smaller than a universal constant. This dependence is due to a technical limitation of the proof (which we will soon explain), and it would be interesting to remove it in future work. The full proof can be found in Appendix B.2, and here we provide a short proof intuition.

The proof strategy is similar to that of Theorem 4.1, by showing that \(_{j=1}^{n}v_{j}(b_{j})=(1)\), i.e. the bias terms are dominant and tend towards being positive. The reason that this suffices is proven using similar arguments to that of Theorem 4.1. The main difficulty of the proof is showing that \(_{j=1}^{n}v_{j}(b_{j})=(1)\). We can write each bias term as \(b_{j}=v_{j}_{i[m]}_{i}y_{i}^{}_{i,j}\), where \(^{}_{i,j}=1(_{j}^{}_{i}+b_{j}>1)\) is the derivative of the \(j\)-th ReLU neuron at the point \(_{i}\). We first show that all the bias terms \(b_{j}\) are positive (Lemma B.5) and that for each \(i I_{+}\), there is \(j\{1,,n/2\}\) with \(^{}_{i,j}=1\). This means that it suffices to show that \(_{i}\) is not too small for all \(i I_{+}\), while for every \(i I_{-}\), \(_{i}\) is bounded from above.

We assume towards contradiction that the sum of the biases is small, and show it implies that \(_{i}=()\) for \(i I_{+}\) and \(_{i}=O(1)\) for \(i I_{-}\). Using the stationarity condition we can write for \(j\{1,,n/2\}\) that \(_{j}=_{i=1}^{m}_{i}y_{i}^{}_{i,j} _{i}\). Taking some \(r I_{+}\), we have that \(_{j}^{}_{r}_{r}^{}_{r,j}\) since \(_{r}\) is almost orthogonal to all other samples in the dataset. By the primal feasibility condition (Eq. (3)) \(N_{}(_{r}) 1\), hence there must be some \(j\{1,,n/2\}\) with \(^{}_{r,j}=1\) and with \(_{r}\) that is larger than some constant. The other option is that the bias terms are large, which we assumed does not happen. Note that we don't know how many neurons \(_{j}\) there are with \(^{}_{j,r}=1\), which means that we can only lower bound \(_{r}\) by a term that depends on \(n\).

To show that \(_{s}\) are not too big for \(s I_{-}\), we use the complementary slackness condition (Eq. (5)) which states that if \(_{s} 0\) then \(N_{}(_{s})=-1\). Since the sum of the biases is not large, then if there exists some \(s I_{-}\) with \(_{s}\) that is too large we would get that \(N_{}(_{s})<-1\) which contradicts the complementary slackness. Combining those bounds and picking \(p\) to be small enough shows that \(_{j=1}^{n}v_{j}(b_{j})\) cannot be too small.

### The role of the bias and catastrophic overfitting in neural networks

In both our benign overfitting results (Theorem 4.1 and Theorem 4.3), our main proof strategy was showing that if \(p\) is small enough, then the bias terms tend to be positive and dominate over the other components of the network. In this section we further study the importance of the bias terms for obtaining benign overfitting, by examining bias-less ReLU networks of the form \(N_{}()=_{j=1}^{n}v_{j}(_{j}^ {})\) where \(=(v_{j},_{j})_{j[n]}\). We note that if the input dimension is sufficiently large and \(n 2\), such bias-less networks can still fit any training data. The proofs for this section can be found in Appendix C.

We begin by showing that without a bias term and with no further assumption on the network width, any solution can exhibit catastrophic behaviour:

**Proposition 4.4** (Catastrophic overfitting without bias).: _Consider a bias-less network with \(n=2\) which classifies a dataset correctly. If the dataset contains at least one sample with a negative label, then \(L_{}(N_{})\)._

While the result above is restricted to any network of width two (in particular, it holds under any assumption such as convergence to a KKT point), this already precludes the possibility of achieving either benign or tempered overfitting without further assumptions on the width of the network - in contrast to our previous results. Furthermore, we next show that in the bias-less case, the clean test error is lower bounded by some term which depends only on \(n\) (which again, holds for any network so in particular under further assumptions such as convergence to a KKT point).

**Proposition 4.5** (Not benign without bias).: _For any bias-less network of width \(n\), \(L_{}(N_{})}\)._

Note that the result above does not depend on \(m\) nor on \(d\), thus we cannot hope to prove benign overfitting for the bias-less case unless \(n\) is large, or depends somehow on both \(m\) and \(d\). We emphasize that Proposition 4.5 does no subsume Proposition 4.4 although they appear to be similar. Indeed, for the case of \(n=2\), Proposition 4.5 provides a bound of \(\) which does not fall under the definition of catastrophic overfitting, as opposed to Proposition 4.4. Next, we show that even for arbitrarily large \(n\) (possibly scaling with other problem parameters such as \(m,d\)), there are KKT points which do not exhibit benign overfitting:

**Proposition 4.6** (Benign overfitting does not follow from KKT without bias).: _Consider a bias-less network, and suppose that \(_{j}^{}_{i}=0\) for every \(i j\). Denote \(I_{-}:=|\{i[m]:y_{i}=-1\}|\). Then there exists a KKT point \(\) of Problem (1) with \(L_{}(N_{})-}}\)._

The result above shows that even if we're willing to consider arbitrarily large networks, there still exist KKT points which do not exhibit benign overfitting (at least without further assumptions on the data). This implies that a positive benign overfitting result in the bias-less case, if at all possible, cannot possibly apply to all KKT points - so one must make further assumptions on which KKT points to consider or resort to an alternative analysis. The assumption that \(_{j}^{}_{i}=0\) for every \(i j\) is made to simplify the construction of the KKT point, while we hypothesize that the proposition can be extended to the case where \(_{i}(^{d-1})\) (or other isotropic distributions) for large enough \(d\).

## 5 Between tempered and benign overfitting for intermediate dimensions

In Section 3, we showed that tempered overfitting occurs for one-dimensional data, while in Section 4 we showed that benign overfitting occurs for high-dimensional data. In this section we complement these results by empirically studying intermediate dimensions and their relation to the number of samples. Our experiments extend an experiment from (Mallinar et al., 2022, Figure 6b) which considered the same distribution as we do, for \(d=10\) and \(m\) varying from \(300\) to \(3.6 10^{5}\). Here we study larger values of \(d\), and how the ratio between \(d\) and \(m\) affects the overfitting profile. We also focus on \(2\)-layer networks (although we do show our results may extend to depth \(3\)).

In our experiments, we trained a fully connected neural network with \(2\) or \(3\) layers, width \(n\) (which will be specified later) and ReLU activations. We sampled a dataset \((_{i},y_{i})_{i=1}^{m}(^{d-1}) _{y}\) for noise level \(p\{0.05,0.1,,0.5\}\). We trained the network using SGD with a constant learning rate of \(0.1\), and with the logistic (cross-entropy) loss. Each experiment ran for a total of \(20k\) epochs, and was repeated \(10\) times with different random seeds, the plots are averaged over the runs. In all of our experiments, the trained network overfitted in the sense that it correctly classified all of the (inherently noisy) training data. Furthermore, we remark that longer training times (\(100k\) epochs) were also examined, and the results remained unaffected, thus we omit such plots.

In Figure 1 we trained a \(2\)-layer network with \(1000\) neurons over different input dimensions, with \(m=500\) samples (left) and \(m=2000\) samples (right). There appears to be a gradual transition from tempered overfitting (with clean test error linear in \(p\)) to benign overfitting (clean test error close to \(0\) for any \(p\)), as \(d\) increases compared to \(m\). Our results indicate that the overfitting behavior is close to tempered/benign in wider parameter regimes than what our theoretical results formally cover (\(d=1\) vs. \(d>m^{2}\)). Additionally, benign overfitting seems to occur for lower values of \(p\), whereas for \(p\) closer to \(0.5\) the clean test error is significantly above \(0\) even for the highest dimensions we examined.

Figure 1: Training a \(2\)-layer network with \(1000\) neurons on \(m\) samples drawn uniformly from \(^{d-1}\) for varying input dimensions \(d\). Each label is equal to \(-1\) with probability \(p\) and \(+1\) with probability \((1-p)\). Left: \(m=500\), Right: \(m=2000\). The line corresponding to the identity function \(y=x\) was added for reference. Best viewed in color.

We further experimented on the effects of the width and depth on the overfitting profile. In Figure 2 (left) we trained a \(2\)-layer network on \(m=500\) samples with input dimension \(d\{250,2000,10000\}\) and \(n\) neurons for \(n\{50,3000\}\). It can be seen that even when the number of neurons vary significantly, the plots remain almost the same. It may indicate that the network width \(n\) has little effect on the overfitting profile (similar to most of our theoretical results), and that the dependence on \(n\) in Theorem 4.3 is an artifact of the proof technique (in accordance with our other results). In Figure 2 (right) we trained a \(3\)-layer neural networks on \(m=500\) samples, as opposed to a \(2\)-layer networks in all our other experiments. Noticeably, this plot looks very similar to Figure 1 (left) which may indicate that our observations generalize to deeper networks.

## 6 Discussion

In this paper, we focused on a classification setting with 2-layer ReLU neural networks that interpolate noisy training data. We showed that the implicit bias of gradient based training towards margin maximization yields tempered overfitting in a low dimensional regime, while it causes benign overfitting in high dimensions. Furthermore, we provide empirical evidence that there is a gradual transition from tempered to benign overfitting as the input dimension increases.

Our work leaves open several questions. A natural follow-up is to study data distributions for which our results do not apply. While our analysis in one dimension is readily applicable for other distributions, it is currently not clear how to extend the high dimensional analysis to other data distributions (in particular, to settings in which the Bayes-optimal predictor is not constant). Moreover, establishing formal results in any constant dimension larger than \(1\), or whenever the dimension scales sub-quadratically (e.g., linearly) with the number of training samples, is an interesting open problem which would likely require different proof techniques.

At a more technical level, there is the question of whether our assumptions can be relaxed. In particular, Theorem 3.2 provides a tight bound under the assumption of convergence to a local minimum of the margin maximization problem, whereas Theorem 3.1 only requires convergence to a KKT point, yet leaves a polynomial gap. Similarly, Theorem 4.1 and Theorem 4.3 require assumptions on the predictor norm or on the noise level and width, respectively.

Our work also leaves open a more thorough study of different architectures and their possible effect on the type of overfitting. In particular, we note that our results suggest that as long as the network is large enough to interpolate, the width plays a minor role in our studied setting -- which we also empirically observe. On the other hand, whether this also holds for other architectures, as well as the role of depth altogether, remains unclear. Finally, it would be interesting to further study the bias-less case and assess its corresponding overfitting behavior under suitable assumptions.