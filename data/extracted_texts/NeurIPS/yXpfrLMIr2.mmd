# Binarized Diffusion Model for Image Super-Resolution

Zheng Chen\({}^{1}\),  Haotong Qin\({}^{2}\),  Yong Guo\({}^{3}\),  Xiongfei Su\({}^{4}\),

**Xin Yuan\({}^{4}\),  Linghe Kong\({}^{1}\),  Yulun Zhang\({}^{1}\)**

\({}^{1}\)Shanghai Jiao Tong University, \({}^{2}\)ETH Zurich,

\({}^{3}\)Max Planck Institute for Informatics, \({}^{4}\)Westlake University

Corresponding authors: Haotong Qin, qinhaotong@gmail.com; Yulun Zhang, yulun100@gmail.com

###### Abstract

Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment. Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs. Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation. In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization. We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection. Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA). The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation ability of the binarized module. Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.

## 1 Introduction

Image super-resolution (SR) is a fundamental task in low-level vision and image processing. It aims to reconstruct high-resolution (HR) images from low-resolution (LR) counterparts. Currently, the mainstream methods for this task are deep neural networks, which employ learning-based techniques to map LR images to HR images . Among these methods, generative models  have garnered widespread attention for their ability to restore more realism results.

Especially, the diffusion model (DM) , a newly proposed generative model, exhibits impressive performance. With its superior generation quality and more stable training, diffusion model is widely used in various image tasks, including image SR . Specifically, the diffusion model transforms a standard Gaussian distribution into a high-quality image through a stochastic iterative denoising process. In image SR, it further constrains the generation scope by conditioning on the LR image to produce the targeted HR image.

However, to produce high-quality results, diffusion models require thousands of iterative steps, leading to slow inference processes and high computational costs. Some methods  implement faster sampling strategies via learning sample trajectories, effectively reducing the number of iterations to tens. Yet, a single inference step still demands substantial memory usage and floating-point computations, especially for SR tasks involving high-resolution images. Meanwhile, most edge devices (_e.g._, mobile and IoT devices), have limited storage and computational resources. This hampers the deployment of diffusion models on these platforms and limits their application. Therefore, it is essential to compress diffusion models to accelerate inference speed and reduce computational costs while maintaining model performance.

Common compression approaches include pruning , distillation , and quantization [45; 66; 26]. Among these, 1-bit quantization (_i.e._, binarization) stands out for its effectiveness. As the most aggressive form of bit-width reduction, binarization significantly reduces memory and computational costs by quantizing the weights and activations of full-precision (32-bit) models to 1-bit.

Nonetheless, existing binarization research primarily deals with higher-level tasks (_e.g._, classification) and end-to-end models [49; 19; 39]. Applying existing binarization methods directly to current diffusion model architectures results in a significant performance drop. This is primarily due to two aspects: **(1) Model Structure.** Diffusion models typically apply the UNet architecture  for noise estimation, which is not easy to binarize directly. **I. Dimension Mismatch:** The identity shortcut is crucial for the binarized SR model, since it facilitates the transfer of full-precision (FP) information, compensating for the binarized model . However, in UNet, the feature dimensions change since downsampling/upsampling. The dimension mismatch prevents the usage of shortcuts, cutting off the full-precision propagation. **II. Fusion Difficulty:** The UNet structure uses skip connections to transfer information from encoder to decoder. However, the typical fusion method, concatenation, leads to the dimension mismatch. Alternatively, other methods (_e.g._, addition) also struggle to achieve effective fusion due to significant differences in value ranges between encoder and decoder features. **(2) Activation Distribution.** Due to the multi-step iterative nature of diffusion models, the activation distribution dramatically changes with timesteps. Furthermore, the activation binarization will even amplify activation differences . The difference increases the learning challenges for binarized modules (_e.g._, binarized convolution), thereby hindering the effective representation of features. Consequently, the SR performance of the binarized diffusion model is limited.

Based on the above analysis, we propose a novel binarized diffusion model, BI-DiffSR, to achieve effective image SR. Our design comprises two main aspects: structure and activation. **(1) Structure.** We develop a simple yet effective convolutional UNet architecture, which is suitable for binarization. **I. Dimension Consistency:** We propose consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to ensure dimensional consistency in binarized computation. CP-Down and CP-Up maintain the full-precision information transfer during feature scaling. **II. Feature Fusion:** We develop the channel-shuffle-fusion (CS-Fusion) to facilitate the fusion of different features within skip connections and suit binarized modules. Through channel shuffle, we combine two input features into two shuffled features to balance their activation value ranges. **(2) Activation.** Considering the activation differences at different timesteps, we design the timestep-aware redistribution (TaR) and timestep-aware activation function (TaA). The TaR and TaA adjust the binarized module input and output activations according to different timesteps. This timestep-aware adjustment improves the flexibility and representational ability of the binarized module to various activation distributions.

Extensive experiments demonstrate that our proposed BI-DiffSR significantly outperforms existing binarization methods. As shown in Fig. 1, our BI-DiffSR restores more perceptually pleasing results than other methods. Overall, our contributions are as follows:

* We design the novel binarized model, BI-DiffSR, for image SR. To the best of our knowledge, this is the first binarized diffusion model applied to SR.
* We develop a UNet architecture optimized for binarization, with consistent-pixel-downsample (CP-Down) and upsample (CP-Up), and channel-shuffle-fusion (CS-Fusion).
* We introduce the timestep-aware redistribution (TaR) and activation function (TaA) to adapt activation distributions by timestep, enhancing the capabilities of the binarized module.
* Our BI-DiffSR surpasses current state-of-the-art binarization methods, and offers comparable perceptual performance to full-precision diffusion models.

Figure 1: Visual comparison (\(\)4) of binarization methods. Some methods (_e.g._, BNN ) cannot work on diffusion models. Several methods (_e.g._, BBCU ) suffer from blurring and artifacts. In contrast, our proposed BI-DiffSR outperforms other methods with accurate results.

## 2 Related Work

### Image Super-Resolution

Since the advent of SRCNN , deep neural networks have gradually become the mainstream for image SR. Numerous architectures [33; 70; 46; 31; 5] are designed to advance reconstruction accuracy. Concurrently, generative methods are widely applied to improve the quality of restored image details. This includes autoregressive model [23; 9], normalizing flow [51; 41; 32], and generative adversarial network (GAN) [13; 24]. For instance, SRFlow  utilizes normalizing flows to transform a Gaussian distribution into the HR image space. Meanwhile, SRGAN  employs GAN as supervision loss and combines it with perceptual loss to produce visually pleasing results. Subsequent methods [62; 4] further refine the network and loss to yield more natural results. Recently, the diffusion model (DM) [16; 8] has been introduced into SR, displaying impressive performance, especially regarding perception. Thereby, DM has been attracting widespread attention [54; 25; 65].

### Diffusion Model

Through the Markov chain, the diffusion model (DM) generates images from the Gaussian distribution . It has demonstrated exceptional performance in various tasks [3; 17; 52; 7; 14; 30; 29; 36; 35; 28; 15]. Naturally, DM has also been extensively researched in the field of image SR [54; 21; 63; 34; 65]. For instance, SR3  achieves conditional diffusion by concatenating the resized LR image with the noise image as the input of the noise estimation network. Meanwhile, some methods, _e.g._, DDNM , utilize an unconditional pre-trained diffusion model as a prior for zero-shot SR. Additionally, some approaches [34; 65] employ text-to-image diffusion models to achieve realistic and controllable SR. Despite promising results, these methods require hundreds or thousands of sampling steps to generate HR images. Although some acceleration algorithms [58; 37; 28] reduce the inference steps to tens, each denoising step still demands substantial resources. The high memory and computational costs hinder the practical application of DMs on resource-limited platforms (_e.g._, mobile devices). To address this issue, we design a practical binarized SR diffusion model.

### Binarization

Binarization is a popular model compression approach . As an extreme case of quantization, it reduces the weights and activations of a full-precision neural network to 1-bit. This significantly decreases the model size and computational complexity, making it widely used in both high-level [19; 39; 48; 38; 67] and low-level [20; 66; 66; 69] vision tasks. For example, BNN  directly binarizes weights and activations during forward and backward processes. IRNet  retains information accurately through the proposed information retention network. ReActNet  proposes the RSign and RPReLU to enable explicit distribution reshape and shift of activations. Meanwhile, in the image SR field, BBCU  introduces a meticulously designed basic binary conv unit, which removes batch normalization (BN) in the binarized model. However, for DM, though some methods realize low-bit (_e.g._, 4 or 8) quantization [55; 26; 27], implementing binarization remains challenging. Due to the structure of the noise estimation network and the multi-step iterative attribute, existing binarization methods often result in significant SR performance degradation.

## 3 Method

In this section, we introduce our proposed BI-DiffSR. First, we describe the structural designs suitable for binarization: _consistent-pixel-downsample_ (CP-Down), _consistent-pixel-upsample_ (CP-Up), and _channel-shuffle-fusion module_ (CS-Fusion). The CP-Down and CP-Up achieve dimension adjustment and ensure the transfer of full-precision information. The CS-Fusion effectively integrates different features within the skip connection. Secondly, we present the dynamic designs tailored for varying activations: _timestep-aware redistribution_ (TaR) and _activation function_ (TaA). The TaR and TaA enhance the representational learning of the binarized modules across multiple timesteps.

### Model Structure

**Overall.** We employ a convolutional UNet  as the noise estimation network. Details of the diffusion model for SR are provided in the supplementary materials. As the common choice within DMs, using UNet as the backbone for binarization offers generalizability. Moreover, for binarized models, the design should be compact and well-defined. Compared to the non-local self-attention operations, convolution is simpler and easier to implement. Our architecture is shown in Fig. 1(a), featuring an encoder-bottleneck-decoder (\(\)-\(\)-\(\)) design.

Given the noise image \(_{t}{}^{H W 3}\) at \(t\)-th timestep, and the LR image \(^{H W 3}\) (bicubic to HR resolution), two images are concatenated along the channel dimension as the UNet input, where \(H{}W\) is the resolution. For timestep \(t\), the sinusoidal position encoding  is applied to obtain the timestep embedding \(_{em}{}^{C}\). The input images first pass through a convolutional layer to produce the shallow feature \(_{s}{}^{H W C}\), where \(C\) is the channel number. Then, the shallow feature \(_{s}\) are further refined by the \(\)-\(\)-\(\) into the deep feature \(_{d}{}^{H W C}\). Each level of the \(\)-\(\)-\(\) is composed of multiple (\(N_{e}\) in \(\) and \(N_{d}\) in \(\)) residual blocks (ResBlocks), with details illustrated in Fig. 1(b). Within the ResBlocks, the timestep embedding \(_{em}\) is incorporated to provide temporal information. In the encoder \(\), downsample module (_i.e._, CP-Down) progressively reduces feature resolution and increases channel number. Conversely, in the decoder \(\), upsample module (_i.e._, CP-Up) gradually restores the high-resolution representation. Moreover, to compensate for information loss during downsampling, the skip connection is used to link features between the encoder and decoder. Finally, through one convolution, the predicted noise \(_{t}{}^{H W 3}\) is obtained.

**Structure Analysis.** Although the UNet architecture is suitable for diffusion models, its unique structure poses challenges for direct binarization, which results in a substantial accuracy decrease compared to full-precision models. We identify two main issues/challenges that contribute to the problem: _dimension mismatch_ and _fusion difficulty_.

_Challenge I: Dimension Mismatch._ In the binarized model, 1-bit quantization leads to significant information loss, limiting the capability for feature representation and the ultimate SR performance. Compared to binary activations, full-precision activations contain more information. Therefore, we can apply the identity shortcut to preserve the full-precision information. This operation effectively compensates for the information loss caused by binarization. However, in UNet, the frequent changes in feature resolution and channel size lead to dimension mismatches. This prevents the effective use of the identity shortcut and cuts off the propagation of full-precision information.

_Challenge II: Fusion Difficulty._ Another crucial structure of UNet is the skip connection, which links encoder and decoder features. The typical approach is to concatenate these features along the channel dimension and pass them to subsequent layers. However, concatenate causes dimension mismatch. As analyzed in _Challenge I_, it is unsuitable for binarization. Furthermore, we find that there is a significant difference in the activation ranges between the two inputs (from encoder and decoder) of the skip connection (Fig. 2(d)). This imbalance makes other fusion methods, _e.g._, addition, also unsuitable, since the smaller range activation is masked by the larger one, as illustrated in Fig. 2(d).

To better adapt binarization for the UNet architecture, we propose two structures: _Consistent-Downsample/Upsample_ and _Channel-Shuffle Fusion_, as illustrated in Fig. 3.

**Consistent-Pixel-Downsample/Upsample.** To address the dimension mismatch in the UNet structure, we first confine all feature reshaping operations to the Upsample and Downsample modules. That is to ensure that the dimension of the main module, _i.e._, ResBlock, remains matched. Meanwhile, we propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up).

Figure 2: The overall structure of the noise estimation network. (a) UNet: The model consists of ResBlock, CP-Down, CP-Up, and CS-Fusion. It predicts noise \(_{t}\) with the upscaled LR image \(\), noise image \(_{t}\), and timestep \(t\). (b) ResBlock: Residual block, utilizes the binarized convolution (BI-Conv) block. The input and output dimensions of the block remain consistent, making it suitable for binarization. (c) TE: Time encoding, encoders timestep \(t\) to produce the timestep embedding \(_{em}\).

_(1) CP-Down:_ We evenly split the input features \(_{in}^{do}\!\!\!\!^{H W C}\) along the channel dimension and process them through two convolutions with identical input and output dimensions. The stable (matching) dimension allows the usage of identity shortcuts. Finally, by applying Pixel-UnShuffle , we reduce the resolution of the features while increasing the channel number. The formula is:

\[_{in}^{do}=[_{s}^{1},_{s}^{2}],_{ s}^{i}^{H W},_{out}^{do}= ^{-1}(_{1}(_{s}^{1})+_{2}( _{s}^{2})),\] (1)

where \(_{out}^{do}\!\!\!\!^{ 2C}\) is the output of CP-Down; \(_{1}()\) and \(_{2}()\) represent two (binarized) convolutions; \(^{-1}\) denotes the Pixel-UnShuffle operation.

_(2) CP-Up:_ Similarly, feature upsampling is achieved through two convolutions combined with Pixel-Shuffle. The operation can be mathematically expressed as follows:

\[_{out}^{up}=((_{1} (_{in}^{up}),_{2}(_{in}^{up} ))),\] (2)

where, \(_{in}^{up}\!\!\!\!^{H W C}\) and \(_{out}^{up}\!\!\!\!^{2H 2W}\) denotes the input and output of CP-Up; \(()\) represents the channel concatenation operation; \(\) is the Pixel-Shuffle operation.

With the above design, we ensure the flow of full-precision information throughout the UNet, effectively improving feature representation and enhancing SR performance.

**Channel-Shuffle Fusion.** To effectively fuse the features in the skip connection while meeting the requirements for dimension matching in binarization, we propose the channel-shuffle fusion (CS-Fusion), as shown in Fig. 3c. Given two features \(_{1}\), \(_{2}\!\!\!^{H W C}\), we first employ the channel-shuffle operation to mitigate the differences in their value ranges, as illustrated in Fig. 3e. Specifically, we split the two features according to the odd and even channel indexes. Then, we pair and concatenate features along the channel dimension, based on odd and even indexes, to produce two new shuffle features \(_{1}^{sh}\), \(_{2}^{sh}\!\!\!\!^{H W C}\). This process can be formulated as follows:

\[_{n}=[_{n}^{1},_{n}^{2},,_{n}^{ C-1},_{n}^{C}], n\{1,2\},\] (3)

\[_{m}^{sh}=(\{_{j}^{2i+(m-1)} i =1,,}{{2}},\,j=1,2\}), m\{1,2\},\]

Through visualization in Fig. 3e, we can observe that the value range of features after channel shuffle becomes balanced. Subsequently, we process the shuffled features through two convolutions and addition to produce the final fused feature \(_{out}^{sh}\!\!\!\!^{H W C}\), in a manner similar to Eq. (1), as:

\[_{out}^{sh}=_{1}^{sh}(_{1}^{sh})+_ {2}^{sh}(_{2}^{sh}),\] (4)

where \(_{1}^{sh}()\) and \(_{2}^{sh}()\) are two (binarized) convolutions. This process realizes the fusion of two features, ensuring that dimensions are matched within the fusion process and in subsequent modules (_e.g._, ResBlock). Meanwhile, the matched dimension allows the usage of the identity shortcut, thus effectively transferring full-precision information. Overall, our proposed CS-Fusion achieves effective feature integration in the skip connection. Therefore, the binarized model can better represent features and improve SR performance. Furthermore, our CS-Fusion does not introduce additional memory or computational overhead since the channel shuffle only involves feature transformation operations. Experiments in Sec. 4.2 further reveal the impacts of CS-Fusion.

Figure 3: (a) CP-Down: Consistent-pixel-downsample. (b) CP-Up: Consistent-pixel-upsample. (c) CS-Fusion: Channel-shuffle fusion. (d) In the skip connection, the value ranges of two features (\(_{1}\), \(_{2}\)) may be significant differences, which impedes effective fusion. (e) The illustration of channel shuffle. the shuffled features (\(_{1}^{sh}\), \(_{2}^{sh}\)) have closely matched value ranges.

### Activation Distribution

**Basic Binarized Convolutional Block.** We first introduce the basic binarized module, as illustrated in Fig. 5a. For the full-precision activation \(^{f}{}^{H W C}\), we initially shift its distribution and binarize the shifted activation to 1-bit activations with sign function \(()\). The process is:

\[^{r}=^{f}+, x^{b}=(x^{r} )=+1,&x^{r} 0\\ -1,&x^{r}<0( x^{r}^{r},\; x^{b} ^{b}),\] (5)

where \(^{C}\) is a learnable parameter; \(^{b}{}^{H W C}\) is the 1-bit activation. Meanwhile, for the binarized convolution, the full-precision weight \(^{f}{}^{C_{out} C_{in} K_{h} K_{w}}\) is also binarized to 1-bit weight \(^{b}{}^{C_{out} C_{in} K_{h} K_{w}}\). To compensate for the differences between binary and full-precision weights, we scale \(^{b}\) using the mean absolute value of \(^{f}\). The total operation is:

\[w^{b}=^{f}\|_{1}}{n}(w^{f}),  w^{f}^{f},\; w^{b}^{b},\] (6)

where \(n\) is the number of \(^{f}\) values. Subsequently, the floating-point matrix multiplication in full-precision convolution can be replaced by logical XNOR and bit-counting operations as:

\[^{b}_{out}=^{b}*^{b}=( (^{b},^{b}))\] (7)

where \(*\) means the convolutional operation; \(^{b}_{out}{}^{H W C}\) is the output of 1-bit convolution. Then, we adjust \(^{b}_{out}\) with the activation function RPReLU , resulting in \(^{b}_{act}{}^{H W C}\).

Finally, we combine \(^{b}_{out}\) with full-precision activation \(^{f}\) via an identity shortcut to get the final output \(_{out}{}^{H W C}\). Moreover, since the sign function \(()\) is non-differentiable, we use the straight-through estimator (STE)  for backpropagation to train binarized models.

**Distribution Analysis.** In diffusion models, the multi-step iterative design leads to changes in the activation distribution as the timestep changes. By visualizing the activation distributions at different timesteps in Fig. 4, we can observe that activation distributions of adjacent timesteps are similar, whereas those separated by larger intervals show significant differences.

For full-precision models, the impact of these variations may be small due to the real-valued weight and activation. In contrast, for binarized modules, the activation distribution has a substantial impact on feature representation, and consequently, affects the SR performance. This is because 1-bit modules, due to the binary weights, struggle to effectively learn representations from different distributions, thereby limiting their modeling capabilities. Meanwhile, during the activation binarization, the sign function further amplifies activation differences, particularly for values around zero .

The basic binarized module utilizes the learnable bias and the activation function RPReLU to adjust the input and output activations. This approach mitigates the representational challenges posed by activation distribution differences across timestep to some extent. However, these static designs are insufficient to cope with the extreme activation changes across multiple timesteps in diffusion models. Consequently, the SR performance of the binarized diffusion model is limited. Experiments in Sec. 4.2, further demonstrate the above analyses.

Figure 4: Visualization of the changes in activation distribution across 50 timesteps.

Figure 5: (a) The basic binarized convolutional (BI-Conv) block. The learnable bias \(\) and the activation function RPReLU adjust the activations. (b) In timestep-aware redistribution (TaR) and activation function (TaA), multiple pairs of \(\) and RPReLU are applied to adapt to the multi-step in DM. At each step \(t\), only one pair of \(\) and RPReLU is used (the darker modules with solid lines).

**Timestep-aware Redistribution/Activation Function.** To cope with the variability of activation distribution with timestep, we propose the timestep-aware redistribution (TaR) and timestep-aware activation function (TaA). The module details are illustrated in Fig. 5. The design of TaR and TaA is inspired by the mixture of experts (MoE) , applying a set of learnable biases and RPReLU activation functions to accommodate different timesteps.

Specifically, we apply \(K\) pairs of bias and RPReLU for TaR (\(^{(i)}\)\(\)\(^{C}\)) and TaA (\(^{(i)}\)), where \(i\)\(\)\(\{1,2,,K\}\). Given the total timesteps (_e.g._, \(\{1,2,,T\}\)), we evenly divide them into \(K\) groups in sequence. For the input activation \(^{f,t}\)\(\)\(^{H W C}\) at \(t\)-th timestep (\(t\)\(\)\(\{1,2,,T\}\)), we select the corresponding pair of bias and RPReLU based on the group associated with \(t\), to adjust its input and output activation. The process can be formulated as:

\[^{r,t}=(^{t}_{in})=^{t}_{in}+_{ i=1}^{K}_{i= t/T}^{(i)},\] (8)

\[^{b,t}_{act}=(^{b,t}_{out})=_{i=1}^{K} _{i= t/T}^{(i)}( ^{b,t}_{out}),\]

where \(_{()}\) is the indicator function; \(^{r,t}\), \(^{b,t}_{out}\), \(^{b,t}_{act}\)\(\)\(^{H W C}\), represent, at \(t\)-th timestep, the shifted input activation, the output of the 1-bit convolution, the output of the RPReLU activation function, respectively. Since the activations at adjacent timesteps exhibit a certain degree of similarity (as shown in Fig. 4), we employ the fixed grouping sampling strategy (defined in Eq. (8)).

Essentially, the TaR and TaA segment the multi-step process into smaller groups, limiting the range of activation changes. This reduces the difficulty of adjusting activations, allowing the binarized module to better adapt to changing activations. Therefore, the proposed TaR and TaA can effectively enhance the representation ability of the binarized module and ultimately improve SR performance. Meanwhile, compared to the basic module, there are no additional computational costs in our TaR and TaA. This is because, for each timestep, only one pair of bias and RPReLU are selected for use.

## 4 Experiments

### Experimental Settings

**Data and Evaluation.** We take DIV2K  and Flickr2K  as the training dataset. Meanwhile, we evaluate the models with four benchmark datasets: Set5 , B100 , Urban100 , and Manga109 . Experiments are conducted under two upscale factors: \( 2\) and \( 4\). The LR images are generated from HR images through bicubic downsampling degradation. We apply two distortion-based metrics, PSNR and SSIM , which are calculated on the Y channel (_i.e._, luminance) of the YCbCr space. We also use the perceptual metrics: LPIPS . Following previous work [66; 49], the total parameters (**Params**) of the model are calculated as \(^{b}{+}^{f}\), and the overall operations (**OPs**) as \(^{b}{+}^{f}\), where \(^{b}{=}^{f}/32\) and \(^{b}{=}^{f}/64\); the superscripts \(f\) and \(b\) denote full-precision and binarized modules, respectively.

**Implementation Details.** For the noise estimation network, we set the encoder and decoder level to 4. In each level of the encoder, we use 2 Residual Blocks (ResBlocks), while in the decoder, we apply 3 ResBlocks. The number of channels \(C\) is set to 64. We set the number of bias and RPReLU in TaR and TaA as \(K{=}5\). For the diffusion model, we set the total number of timesteps to \(T{=}2\),000. During the inference phase, we employ the DDIM sampler with 50 timesteps.

**Training Settings.** We train models with the \(_{1}\) loss. We employ the Adam optimizer  with \(_{1}{=}0.9\) and \(_{2}{=}0.99\), and a learning rate of 1\(\)10\({}^{-4}\). The batch size is set to 16, with a total of 1,000K iterations. Input LR images are randomly cropped to size 64\(\)64. Random rotations of \(90^{}\), \(180^{}\), and \(270^{}\) and horizontal flips are used for data augmentation. Our model is implemented based on PyTorch  with two Nvidia A100-80G GPUs.

### Ablation Study

In this section, we conduct all experiments on the \(\)2 scale factor. We apply DIV2K  and Flickr2K  as the training dataset, and Manga109  as the testing dataset. The training iterations are set to 500K. Other settings are the same as defined in Sec. 4.1. We test the computational complexity (_i.e._, OPs) of one single sampling step on the output size 3\(\)256\(\)256.

**Break Down.** We first execute a break-down ablation on different components of our method. The results are listed in Tab. 0(a). The baseline is established by using binarized convolution (BI-Conv) and Pixel-(Un)Shuffle for dimension scaling in the downsample, upsample, and fusion (skip connection) modules of the UNet. Meanwhile, the basic BI-Conv block (Fig. 5) is employed without the identity shortcut. The baseline performance is poor, with the PSNR of 27.66 dB. Then, we add identity shortcut, consistent-pixel-downsample (CP-Down) and upsample (CP-Up), channel-shuffle-fusion module (CS-Fusion), and timestep-aware redistribution (TaR) and activation function (TaA) in sequence. We can find that the performance gradually increases. Ultimately, the final model achieves gains of 5 dB in PSNR and 0.0580 in LPIPS, compared to the baseline.

**Channel-Shuffle Fusion.** We experiment on the fusion module for the skip connection. We attempt four methods: directly add two features (Add); concatenation and adjust dimension by binarized convolution (Concat); process each feature via binarized convolution and add them; and our proposed CS-Fusion. The results are shown in Tab. 0(b). Due to the differences between features, direct addition (Add) can hardly work, even with convolution (Split). Moreover, since the concatenation changes the dimensions, the Method (Concat) also degrades the performance. In contrast, our proposed CS-Fusion, eliminates the distribution imbalances by channel fusion, thereby achieving effective fusion. The visualization in Fig. 6, further indicates that addition cannot fuse data with narrow value distributions, whereas channel shuffle can effectively integrate.

**Timestep-aware Module.** We conduct experiments on the time-aware redistribution (TaR) and activation function (TaA). Firstly, we experiment with the combinations of TaR and TaA in Tab. 0(c). We find that effective improvements are only achieved when both TaR and TaA are employed. This may be because both input and output activation impact the learning of the binarized module. Then, in Tab. 0(d), we experiment with the pair number (\(\#\)Pair) of bias and RPReLU. The experiments show that 5 pairs already lead to effective improvements. Considering the additional parameters, we adopt 5 as the pair number in BI-DiffSR. Moreover, we present the weights of five learnable biases in the TaR (module position shown at the image top) in Fig. 7. The difference in weights indicates that TaR can effectively adapt to the varying activation distributions at different timesteps.

### Comparison with State-of-the-Art Methods

We compare our proposed BI-DiffSR with recent binarization methods, including BNN , DoReFa , XNOR , IRNet , ReActNet , and BBCU . To ensure a fair comparison, we set the parameters (Params) and complexity (OPs) of all binarization methods to be similar. We also compare our BI-DiffSR with the full-precision (FP) model, SR3 .

[MISSING_PAGE_FAIL:9]