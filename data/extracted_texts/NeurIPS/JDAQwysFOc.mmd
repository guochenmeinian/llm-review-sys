# Non-convolutional Graph Neural Networks

Yuanqing Wang

Center for Data Science

and Simons Center

for Computational Physical Chemistry

New York University

New York, N.Y. 10004

wangyq@wangyq.net

&Kyunghyun Cho

Center for Data Science, New York University

and Prescient Design, Genetech

New York, N.Y. 10004

kc119@nyu.edu

###### Abstract

Rethink convolution-based graph neural networks (GNN)--they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined _random walk with unifying memory_ (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.

## 1 Introduction: Convolutions in GNNs

Graph neural networks (GNNs) [1; 2; 3; 4; 5]--neural models operating on representations of nodes (\(\)) and edges (\(\)) in a _graph_ (denoted by \(=\{,\}, \), with structure represented by the adjacency matrix \(A_{ij}=[(v_{i},v_{j})]\))--have shown promises in a wide array of social and physical modeling applications. Most GNNs follow a _convolutional_ scheme, where the \(D\)-dimensional node representations \(^{|V| D}\) are aggregated based on the structure of local neighborhoods:

\[^{}=.\] (1)

Here, \(\) displays a unique duality--the input features doubles as a compute graph. The difference among _convolutional_ GNN architectures, apart from the subsequent treatment of the resulting intermediate representation \(^{}\), typically amounts to the choices of transformations (\(\)) of the original adjacency matrix (\(A\))--the normalized Laplacian for graph convolutional networks (GCN) , a learned, sparse stochastic matrix for graph attention networks (GAT) , powers of the graph Laplacian for simplifying graph networks (SGN) , and the matrix exponential thereof for graph neural diffusion (GRAND) , to name a few. For all such transformations, it is easy to verify that permutation equivariance (Equation 9) is trivially satisfied, laying the foundations of data-efficient graph learning. At the same time, this class of methods share common pathologies as well:

**Limited expressiveness. (Figure 1)** Xu et al.  groundbreakingly elucidates that GNNs cannot exceed the expressiveness of Weisfeiler-Lehman (WL) isomorphism test . Worse still, when the support of neighborhood multiset is uncountable, no GNN with a single aggregation function can be

[MISSING_PAGE_FAIL:2]

iterative 1-dimensional convolutional network. AWE (, Table 3) and Wang et al. , like ours, use anonymous experiments for graph-level unsupervised learning and temporal graph learning, respectively. More elaborately,  and AgentNet (, Table 3) use agent-based learning on random walks and paths on graphs.

Random walk kernel GNNs.In a closely related line of research, RWK (, Table 3) employs the reproducing kernel Hilbert space representations in a neural network for graph modeling and GSN (, Table 3) counts the explicitly enumerated subgraphs to represent graphs. The subgraph counting techniques intrinsically require prior knowledge about the input graph of a predefined set of node and edge sets. For these works, superior expressiveness has been routinely argued, though usually limited to a few special cases where the WL test fails whereas they do not, and often within the _unlabelled graphs_ only.

More importantly, focusing mostly on expressiveness, no aforementioned **walk-based** or **random walk kernel**-based GNNs address the issue of over-smoothing and over-squashing in GNNs. Some of these works are also _de facto_ convolutional, as the random walks are only incorporated as features in the message-passing scheme. Interestingly, most of these works are either not applicable to, or have not been tested on, node-level tasks. In the experimental SS 5, we show that RUM not only outperforms these baselines on most graph-level tasks (Table 3) but also competes with a wide array of state-of-the-art convolutional GNNs on node-level tests (Table 2). Moreover, random walk kernel-based architectures, which explicitly enumerate random walk-generated subgraphs are typically much slower than WL-equivalent convolutional GNNs, whereas RUM is faster than even the simplest variation of convolutional GNN (Figure 4).

Graph transformers.Graph transformers [26; 27]--neural models that perform attention among all pairs of nodes and encode graph structure via positional encoding--are well-known solutions that are not locally convolutional. Their inductive biases determine that over-smoothing and over-squashing among local neighborhoods are, like RUM, also not prominent.

Because of its all-to-all nature, the runtime complexity of graph transformers, just like that of almost all transformers, contains a quadratic term, w.r.t. the size of the system (number of nodes, edges, or subgraphs). This makes it prohibitively expensive and memory intensive on large social graphs (such as that used in Table 8 with millions of nodes). On smaller social graphs, we show in Tables 9, 4 that graph transformers are experimentally outperformed by RUM.

State-of-the-art methods to alleviate oversmoothing._Stochastic regularization._ DropEdge (, Figure 2) regularizes the smoothing of the node representations by randomly disconnecting edges. Its associated Dirichlet energy indeed decreases slower, though eventually still diminishes as the number of layers increases. _Graph rewiring._[29; 30] and GPR-GNN (, Appendix Table 9) rewire the graph using personalized page rank algorithm  and generalized page rank on graphs, respectively. Similar to JKNet , they mitigate over-smoothing by allowing direct message passing between faraway nodes. _Constant-energy methods._ Zhao and Akoglu , Rusch et al.  constrain the pair-wise distance or Dirichlet energy among graphs to be constant. Nevertheless, the non-decreasing energy does not necessarily translate to better performance, as they sometimes come with the sacrifice of expressiveness, as argued in Rusch et al. . _Residual GNNs._ Residual connection  can naturally be added to the GNN architecture, such as GCNII (, Table 2), to restraint activation to be similar to the input to allow deep networks. They however can make the model less robust to perturbations in the input. In sum, these works have similar, if not compromised expressiveness compared to a barebone GCN.

## 3 Architecture: combining topologic and semantic trajectories of walks

Random walks on graphs.An unbiased random walk \(w\) on a graph \(\) is a sequence of nodes \(w=(v_{0},v_{1},)\) with landing probability:

\[P(v_{j}|(v_{0},,v_{i-1}))=[(v_{i},v_{j})_{ }]/D(v_{i}),\] (4)

where \(D(v_{i})= A_{ij}\) is the degree of the node \(v_{i}\). Walks _originating_ from or _terminating_ at any given node \(v\) can thus be easily generated using this Markov chain. We record the trajectory of embeddings associated with the walk as \(_{x}(w)=(_{i})=(_{0},_{1},,_{l})\). In this paper, we only consider finite-long \(l\)-step random walk \(|w|=l^{+}\). In our implementation, the random walks are sampled _ad hoc_ during each training and inference step directly on GPU using Deep Graph Library  (see Appendix SSA). Moreover, the _walk_ considered here is not necessarily a _path_, as repeated traversal of the same node \(v_{i}=v_{j}\), \(i j\) is not only permitted, but also crucial to effective topological representation, as discussed below.

Anonymous experiment.We use a function describing the topological environment of a walk, termed _anonymous experiment_, \(_{u}(w):^{l}^{l}\) that records **the first unique occurrence of a node in a walk** (Appendix Algorithm C). To put it plainly, we label a node as the number of _unique_ nodes insofar traversed in a walk if the node has not been traversed, and reuse the label otherwise. Practically, this can be implemented using any tensor-accelerating framework _in one line_ (w is the node sequence of a walk) and trivially parallelized 2: (1*(w[..., :,None]==w[..., None,:])).argmax(-1)

Unifying memory: combining semantic and topological representations.Given any walk \(w\), we now have two sequences \(_{x}(w)\) and \(_{u}(w)\) describing the _semantic_ and _topological_ (as we shall show in the following sections) features of the walk. We project such sequential representations onto a latent dimension to combine them (illustrated in Table 1):

\[h(w)=f(_{x}(_{x}(w)),_{u}(_{u}(w))),\] (5)

where \(_{x}:^{l D}^{D_{x}}\) maps the sequence of semantic embeddings generated by a \(l\)-step walk to a fixed \(D_{x}\)-dimensional latent space, \(_{u}:^{l}^{D_{u}}\) maps the indicies sequence to another latent space \(D_{u}\), and \(f:^{D_{x}}^{D_{u}}^{D}\) combines them. We call Equation 5 the _unifying memory_ of a random walk. Subsequently, the node representations can also be formed as the average representations of \(l\)-step (\(l\) being a hyperparameter) walks _terminating_ (for the sake of gradient aggregation) at that node:

\[(v)=_{\{w\},|w|=l,w_{l}=v}p(w)h(w),\] (6)

which can be stochastically sampled with unbiased Monte Carlo gradient and used in downstream tasks as such node classification and regression. We note that this is the only time we perform \(\) or \(\) operations. Unlike other GNNs incorporating random walk-generated features (which are sometimes still convolutional and iterative), we do not iteratively pool representations within local neighborhoods. The likelihood of the data written as:

\[P(y|,)=_{\{w\},|w|=l,w_{l}=v}p(w)p(y|, ,w)\] (7)

The node representation can be summed

\[()=_{v}(v)\] (8)

to form global representations for graph classification and regression. We call \(\) in Equation 6 and \(\) in Equation 8 the node and graph output representations of RUM.

Layer choices.Obvious choices to model \(f\) include a feed-forward neural network after concatenation, and \(_{x},_{u}\) recurrent neural networks (RNNs). This implies that, different from most convolutional GNNs, parameter sharing is natural and the number of parameters is going to stay constant as the model incorporates a larger neighborhood. Compared to dot product-based, transformer-like modules , RNNs not only have linear complexity (see detailed discussions below) w.r.t. the sequence length but also naturally encodes the inductive bias that nodes closer to the origin have stronger impact on the representation. The gated recurrent unit (GRU) variant is used everywhere in this paper. Additional regularizations are described in Appendix SSA.1.

Runtime complexity.To generate random walks for one node has the runtime complexity of \((1)\), and for a graph \((||)\), where \(||\) is the number of nodes in a graph \(=\{,\}\). To calculate the _anonymous experiment_, as shown in Appendix Algorithm C, has \((1)\) complexity (also see Footnote 2). If we use linear-complexity models, such as RNNs, to model \(_{x},_{u}\), the overall complexity is \((||lkD)\) where \(l\) is the length of the random walk, \(k\) the samples used to estimate Equation 6, and \(D\) the latent size of the model (assumed uniform). Note that different from convolutional GNNs, RUM does not depend on the number of edges \(||\) (which is usually much larger than \(||\)) for runtime complexity, and is therefore agnostic to the _sparsity_ of the graph. See Figure 4 for an empirical verification of the time complexity. In Appendix Table 8, we show, on a large graph, the overhead introduced by generating random walks and computing anonymous experiments accounts for roughly \(1/1500\) of the memory footprint and \(1/8\) of the wall time.

Mini-batches.RUM is naturally compatible with mini-batching. For convolutional GNNs, large graphs that do not fit into the GPU memory have traditionally been a challenge, as all neighbors are required to be present and boundary conditions are hard to define . RUM, on the other hand, can be inherently applied on subsets of nodes of a large graph without any alteration in the algorithm--the random walks can be generated on a per-node basis, and the \(\) loop in Algorithm C can be executed sequentially, in parallel, or on subsets. Empirically, in Appendix Table 8, RUM can be naturally scaled to huge graphs.

## 4 Theory: RUM as a joint remedy.

We have insofar designed a new graph learning framework--convolution-free graph neural networks (GNNs) that cleanly represent the semantic (\(_{x}\)) and topological (\(_{u}\)) features of graph-structured data before unifying them. First, we state that RUM is permutation equivariant,

_Remark 1_ (Permutation equivariance).: For any permutation matrix \(P\), we have

\[P_{v}()=_{v}(P()),\] (9)

which sets the foundation for the data-efficient modeling of graphs. Next, we theoretically demonstrate that this formulation jointly remedies the common pathologies of the convolution-based GNNs by showing that: (a) the _topological_ representation \(_{u}\) is more expressive than convolutional-GNNs in distinguishing non-isomorphic graphs; (b) the _semantic_ representation \(_{x}\) no longer suffers from over-smoothing and over-squashing.

### RUM is more expressive than convolutional GNNs.

For the sake of theoretical arguments in this section, we assume that in Equation 5:

**Assumption 2**.: \(_{x},_{u},f\) are universal and injective.

**Assumption 3**.: Graph \(\) discussed in this section is always connected, unweighted, and undirected.

Assumption 2 is easy to satisfy for feed-forward neural networks  and RNNs . Note that RUM can be easily extended to weighted graphs by sampling a biased random walk with edge weights \(w_{ij}\) and keeping rest of the algorithm the same: \(P(v_{j}|(v_{0},...,v_{i-1})) I[(v_{i},v_{j}) E_{G}]*w_{ij}/D(v_{i})\). Composing injective functions, we remark that \(h(w)\) is also injective w.r.t. \(_{x}(w)\) and \(_{u}(w)\); despite of Assumption 3, our analysis can be extended to disjointed graphs by restricting the analysis to the connected regions in a graph. Under such assumptions, we show, in **Remark 8** (deferred to the Appendix), that \(\) is **injective**, meaning that nodes with different random walks will have different distributions of representations \((v_{1})(v_{2})\). We also refer the readers to the **Theorem 1** in Micali and Zhu  for a discussion on the representation power of _anonymous experiments_ on _unlabelled_ graphs. Combining with the semantic representations and promoting the argument from a node level to a graph level, we arrive at:

**Theorem 4** (RUM can distinguish non-isomorphic graphs).: _Up to the Reconstruction Conjecture , RUM with sufficiently long \(l\)-step random walks can distinguish non-isomorphic graphs satisfying Assumption 3._

The main idea of the proof of Theorem 4 (in Appendix SS D.2) involves explicitly enumerating all possible non-isomorphic structures for graphs with 3 nodes and showing, by induction, that if the theorem stands for graph of \(N-1\) size it also holds for \(N\)-sized graphs. We also show in Appendix SS D.1 that a number of key graph properties such as cycle size (Example 8.1) and radius (Example 8.2) that convolutional GNNs struggle [45; 46] to learn can be analytically expressed using \(_{u}\). As these are solely functions of \(_{x}\), they can be approximated arbitrarily well by universal approximators. These examples are special cases of the finding that RUM is stricly more expressive than Weisfeiler-Lehman isomorphism test :

**Corollary 4.1** (RUM is more expressive than WL-test).: _Up to the Reconstruction Conjecture, two graphs with \(G_{1},G_{2}\) labeled as non-isomorphic by the \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) isomorphism test, is the necessary, but not sufficient condition that the representations resulting from RUM with walk length \(k\) are also different._

\[(_{1})(_{2})\] (10)

Thus, due to Xu et al. , RUM is also more expressive than convolutional GNNs in distinguishing non-isomorphic graphs. This also confirms the intuition that RUM with longer walks are more expressive. (See Figure 3 on an empirical evaluation.) The proof of this corollary is straightforward to sketch--even if we only employ the embedding trajectory \(_{x}\), it would have the effect of performing the function \(_{x}\) in Equation 5 on each traversal of the WL expanded trees.

### RUM alleviates over-smoothing and over-squashing

Over-smoothing refers to the phenomenon where the node dissimilarity (e.g., measured by Dirichlet energy in Equation 2) decreases exponentially and approaches zero with the repeated rounds of message passing. Cai and Wang  relates Dirichlet energy directly with the convolutional operator:

**Lemma 3.1 from Cai and Wang .**

\[((1-))(1-)^{2}( )\] (11)

_where \(\) is the smallest non-zero eigenvalue of \(\), the normalized Laplacian of a graph._

Free of convolution operators, it seems only natural that RUM does not suffer from this symptom (Figure 2). We now formalize this intuition by first restricting ourselves to a class of _non-contractive_ mappings for \(f\) in Definition 5.

**Definition 5**.: A map \(f\) is non-contractive on region \(\) if \([1,+)\) such that \(|f(x)-f(y)|||x-y||, x,y\).

A line of fruitful research has been focusing on designing non-contractive RNNs [48; 49], and to be non-contractive is intimately linked with desirable properties such as preserving the long-range information content and non-vanishing gradients. From this definition, it is easy to show that, for each sampled random walk in Equation 5, the Dirichlet energy is greater than its input. One only needs to verify that the integration in Equation 6 does not change this to arrive at:

**Lemma 6** (RUM alleviates over-smoothing.).: _If \(_{x},f\) are non-contractive w.r.t. all elements in the sequence, the expected Dirichlet energy of the corresponding RUM node representation in Equation 6 is greater than its initial value_

\[((()))().\] (12)

This implies that the expectation of Dirichlet energy does not diminish even when \(l+\), as it is bounded by the Dirichlet energy of the initial node representation, which is consistent with the trend shown in Figure 2, although the GRU is used out-of-box without constraining it to be explicitly non-contractive.

RUM alleviates over-squashingis deferred to Appenx SS B.2, where we verify that the inter-node Jacobian \(|_{v}^{(l+1)}/_{u}^{(0)}|\) decays slower as the distance between \(u,v\) grows vis-a-vis the convolutional counterparts. Briefly, although RUM does not address the information bottleneck with exponentially growing receptive field (the \(1/(^{l+1})_{uv}\) term in Equation 16), it nevertheless can have a non-vanishing (nor exploding) gradient from the aggregation function (\(|_{x}|\)).

## 5 Experiments

On a wide array of real-world node- and graph-level tasks, we benchmark the performance of RUM to show its utility in social and physical modeling. Next, to thoroughly examine the performance of RUM, we challenge it with carefully designed illustrative experiments. Specifically, we ask the following questions in this section, with **Q1**, **Q2**, and **Q3** already theoretically answered in SS 4: **Q1:** Is RUM more expressive than convolutional GNNs? **Q2:** Does RUM alleviate over-smoothing? **Q3:** Does RUM alleviate over-squashing? **Q4:** Is RUM slower with convolutional GNNs? **Q5:** Is RUM robust? **Q6:** How does RUM scale up to huge graphs? **Q7:** What components of RUM are contributing most to the performance of RUM?

Real-world benchmark performance.For node classification, we benchmark our model on the popular Planetoid citation datasets , as well as the coauthor  and co-purchase  datasets common in social modeling. Additionally, we hypothesize that RUM, without the smoothing operator, will perform competitively on heterophilic datasets --we test this hypothesis. For graph classification, we benchmark on the popular TU dataset . We also test the graph regression performance on molecular datasets in MoleculeNet  and Open Graph Benchmark . In sum, RUM almost always outperforms, is within the standard deviation of, the state-of-the-art architectures, as shown in Tables 2, 3, 4, 5, as well as in Tables 6, 7, 8, 9 moved to the Appendix due to space constraint.

On sparsity: the subpar performance on the Computer dataset.The most noticeable exception to the good performance of RUM is that on the Computer co-purchase  dataset, where RUM is outperformed even by GCN and GAT. This dataset is very dense with an average node degree (\(||/||\)) of \(18.36\), the highest among all datasets used in this paper. As the variance of the node embedding (Equation 6) scales with the average degree, we hypothesize that dense graphs with very high average node degrees would have high-variance representations from RUM.

On the other hand, RUM outperforms _all_ models surveyed in two large-scale benchmark studies on molecular learning, GAUCHE  and MoleculeNet . The atoms in the molecules always have a degree of \(2 4\) with intricate subgraph structures like small rings. This suggests the utility of _unifying memory_ in chemical graphs and furthermore chemical and physical modeling.

Graph isomorphism testing **(Q1).** Having illustrated in SS 4 that RUM can distinguish non-isomorphic graphs, we experimentally test this insight on the popular Circular Skip Link dataset . Containing 4-regular graph with edges connected to form a cycle and containing skip-links between nodes, this dataset is artificially synthesized in Murphy et al.  to create an especially challenging task for GNNs. As shown in Appendix Table 7, all convolutional GNNs fail to perform better than a constant baseline (there are 10 classes uniformly distributed). 3WLGNN , a higher-order GNN of at least \((2)\) complexity that operates on explicitly enumerated triplets of graphs, can distinguish these highly similar 4-regular graphs by comparing subgraphs. RUM, with \((||)\) linear complexity, achieves similarly high accuracy. One can think of RUM as a stochastic approximationof the higher-order GNN, with all of its explicitly enumerated subgraphs being identified by RUM with a probability that decreases with the complexity of the subgraph.

Effects of walk lengths and number of samples on performance (Q2, Q3).Having studied the relationship between inference speed and the walk lengths and number of samples, we furthermore study its impact on performance. Using Cora  citation graph and vary the walk lengths and number of samples from \(1\) to \(9\), where the performance of RUM improves as more samples are taken and longer walks are employed, though more than 4 samples and walks longer than \(L>4\) yield qualitatively satisfactory results; this empirical finding has guided our hyperparameter design. In Figure 2, we also compare the Dirichlet energy of RUM-generated layer representations with not only baselines GCN  and GAT , but also strategies to alleviate over-smoothing discussed in SS 2, namely residual connection and stochastic regularization , and show that when \(L\) gets large, only RUM can maintain Dirichlet energy. Traditionally, since Kipf and Welling  (see its Figure 5 compared to Figure 3), the best performance on Cora graph was found with \(2\) or \(3\) message-passing rounds, since mostly local interactions are dominating the classification, and more rounds of message-passing almost always lead to worse performance. As theoretically demonstrated in SS 4.2, RUM is not as affected by these symptoms. Thus, RUM is especially appropriate for modeling long-range interactions in graphs without sacrificing local representation power.

   &  Texas \\  &  Wisc. \\  &  Cornell \\ \(55.1_{ 4.2}\) \\  &  \(51.8_{ 3.3}\) \\  & 
 \(60.5_{ 4.8}\) \\  \\ GCN & \(52.2_{ 6.6}\) & \(51.8_{ 3.1}\) & \(60.5_{ 5.3}\) \\ GCN & \(77.6_{ 3.8}\) & \(80.4_{ 3.4}\) & \(77.9_{ 3.8}\) \\ Geom-GCN & \(66.8_{ 2.7}\) & \(64.5_{ 3.7}\) & \(60.5_{ 3.7}\) \\ PairNorm & \(60.3_{ 4.3}\) & \(48.4_{ 6.1}\) & \(58.9_{ 3.2}\) \\  GPS & \(75.4_{ 1.5}\) & \(78.0_{ 2.9}\) & \(65.4_{ 5.7}\) \\ Graphomer & \(76.8_{ 1.8}\) & \(77.7_{ 2.0}\) & \(68.4_{ 1.7}\) \\  RUM & \(80.0_{ 7.0}\) & \(85.8_{ 4.1}\) & \(71.1_{ 5.6}\) \\  

Table 4: **Node classification** test set accuracy \(\) and standard deviation on heterophilic  datasets.

   &  Cora \\  &  CiteSeer \\  &  PubMed \\  &  Coathor CS \\  &  Computer \\  & 
 Photo \\  \\  GCN & \(81.5\) & \(70.3\) & \(79.0\) & \(91.1_{ 0.5}\) & \(82.6_{ 2.4}\) & \(91.2_{ 1.2}\) \\ GAT & \(83.0_{ 0.7}\) & \(72.5_{ 0.7}\) & \(79.0_{ 0.3}\) & \(90.5_{ 0.6}\) & \(78.0_{ 19.0}\) & \(85.7_{ 20.3}\) \\ GraphSAGE & \(77.4_{ 1.0}\) & \(67.0_{ 1.0}\) & \(76.6_{ 0.8}\) & \(85.0_{ 1.1}\) & & \(90.4_{ 1.3}\) \\ MoNet & \(81.7_{ 0.5}\) & \(70.0_{ 0.6}\) & \(78.8_{ 0.4}\) & \(90.8_{ 0.6}\) & \(83.5_{ 2.2}\) & \(91.2_{ 2.3}\) \\ GCN & \(85.0_{ 0.5}\) & \(73.4_{ 0.6}\) & \(80.3_{ 0.4}\) & & & \\ PairNorm & \(81.1\) & \(70.6\) & \(78.2\) & & & \\ GraphDCN & \(84.2_{ 1.3}\) & \(74.2_{ 1.7}\) & \(79.4_{ 1.3}\) & & & \\  RUM & \(84.1_{ 0.9}\) & \(75.5_{ 0.5}\) & \(82.2_{ 0.2}\) & \(93.2_{ 0.0}\) & \(77.8_{ 2.3}\) & \(92.7_{ 0.1}\) \\  

Table 2: **Node classification** test set accuracy \(\) and standard deviation.

   & IMDB-B & MUTAG & PROTEINS & PTC & NCII \\  RWK & & \(79.2_{ 2.1}\) & \(59.6_{ 0.1}\) & \(55.9_{ 0.3}\) & \\ GK & & \(81.4_{ 1.7}\) & \(71.4_{ 0.3}\) & \(55.7_{ 0.5}\) & \(62.5_{ 0.3}\) \\ WLK & \(73.8_{ 3.9}\) & \(90.4_{ 5.7}\) & \(75.0_{ 3.1}\) & \(59.9_{ 4.3}\) & \(86.0_{ 1.8}\) \\ AWE & \(74.5_{ 5.9}\) & \(87.8_{ 9.8}\) & & \\ GIN & \(75.1_{ 5.1}\) & \(90.0_{ 8.8}\) & \(76.2_{ 2.6}\) & \(66.6_{ 6.9}\) & \(82.7_{ 1.6}\) \\ GSN & \(77.8_{ 3.3}\) & \(92.2_{ 7.5}\) & \(76.6_{ 5.0}\) & \(68.2_{ 7.2}\) & \(83.5_{ 2.0}\) \\ CRaW & \(73.4_{ 2.1}\) & \(90.4_{ 7.1}\) & \(76.2_{ 3.7}\) & \(68.0_{ 6.5}\) & \\ AgentNet & \(75.2_{ 4.6}\) & \(93.6_{ 8.6}\) & \(76.7_{ 3.2}\) & \(67.4_{ 5.9}\) & \\  RUM & \(81.1_{ 4.5}\) & \(91.0_{ 7.1}\) & \(77.3_{ 3.8}\) & \(69.8_{ 6.3}\) & \(81.7_{ 1.4}\) \\  

Table 3: **Binary graph classification** test set accuracy \(\).

   &  ESOL \\  & 
 FreeSolv \\  & Lipophilicity \\  GAUCHE & \(0.67_{ 0.01}\) & \(0.96_{ 0.01}\) & \(0.73_{ 0.02}\) \\ MoleculeNet & \(0.58\) & \(1.15\) & \(0.80\) \\  RUM & \(0.62_{ 0.06}\) & \(0.96_{ 0.24}\) & \(0.66_{ 0.01}\) \\ 

Table 5: **Graph regression** RMSE \(\) compared with the _best_ model studied in two large-scale benchmark studies on OGB  and MoleculeNet  datasets.

Long-range neighborhood matching (Q3).To verify that RUM indeed alleviates over-squashing (SS B.2), in Figure 5, we adopt the tree neighborhood matching synthetic task in Alon and Yahav  where binary tree graphs are proposed with the label _and the attributes_ of the root matching a faraway leave. The full discussion is moved to the Appendix SS B.3.

Speed (Q4).Though both have linear runtime complexity (See SS 3), intuitively, it might seem that RUM would be slower than convolution-based GCN due to the requirement of multiple random walk samples. This is indeed true for CPU implementations shown in Figure 4 left. When equipped with GPUs (specs in Appendix SS A), however, RUM is significantly faster than even the simplest convolutional GNN--GCN . It is worth mentioning that the GCN and GAT  results were harvested using the heavily optimized Deep Graph Library  sparse implementation whereas RUM is implemented naively in PyTorch , though the popular GRU component  have already undergone CUDA-specific optimization.

Robustness to attacks (Q5).With the stochasticity afforded by the random walks, it is natural to suspect RUM to be robust. We adopt the robustness test from Feng et al.  and attack by randomly adding fake edges to the Cora  graph and record the test set performance in Figure 6. Indeed, RUM is much more robust than traditional convolutional GNNs including GCN  and GAT  and is even slightly more robust than the convolutional GNN specially designed for robustness , with the performance only decreased less than \(10\%\) with \(10\%\) fake edges added.

Scaling to huge graphs (Q6) and Ablation study (Q7)are deferred to Appendix SS B.5.

Conclusions

We design an innovative GNN that uses an RNN to unify the semantic and topological representations along stochastically sampled random walks, termed _random walk with unifying memory_ (RUM) neural networks. Free of the convolutional operators, our methodology does not suffer from symptoms characteristic of Laplacian smoothing, including limited expressiveness, over-smoothing, and over-squashing. Most notably, our method is more expressive than the Weisfeiler-Lehman isomorphism test and can distinguish all non-isomorphic graphs up to the _reconstruction conjecture_. Thus, it is more expressive than all of convolutional GNNs equivalent to the WL test, as we demonstrate theoretically in SS 4. RUM is significantly faster on GPUs than even the simplest convolutional GNNs (SS 5) and shows superior performance across a plethora of node- and graph-level benchmarks.

Limitations._Very dense graphs._ As evidenced by the underwhelming performance of the Computer  dataset and discussed in SS 5, RUM might suffer from high variance with very dense graphs (average degree over 15). _Tottering._ In our implementation, we have not ruled out the 2-cycles from the random walks, as that would require specialized implementation for walk generation. This, however, would reduce the average information content in a fixed-length random walk (known as tottering ). We plan to characterize the effect of excluding these walks. _Biased walk._ Here, we have only considered unbiased random walk, whereas biased random walk might display more intriguing properties as they tend to explore frawaway neighborhoods more effectively . _Directed graphs._ Since we have only developed RUM for undirected graph (random walk up to a random length is not guaranteed to exist for directed graphs), we would have to symmetrize the graph to perform on directed graphs (such as the heterophilic datasets ); this create additional information loss and complexity.

Future directions._Theoretical._ We plan to expand our theoretical framework to account for the change in layer width and depth to derive analytical estimates for realizing key graph properties. _Applications._ Random walks are intrinsically applicable to uncertainty-aware learning. We plan to incorporate the uncertainty naturally afforded by the model to design active learning models. On the other hand, the physics-based graph modeling field is also heavily dominated by convolutional GNNs. Inspired by the superior performance of RUM on chemical datasets, we plan to apply our method in drug discovery settings  and furthermore on the equivariant modeling of \(n\)-body physical systems .

Impact statement._ We here present a powerful, robust, and efficient learning algorithm on graphs. Used appropriately, this algorithm might advance the modeling of social  and physical  systems, which can oftentimes modeled as graphs. As with all graph machine learning methods, negative implications may be possible if used in the design of explosives, toxins, chemical weapons, and overly addictive recreational narcotics.