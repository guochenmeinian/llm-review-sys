# How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective

Qiaozhe Zhang, Ruijie Zhang, Jun Sun, Yingzhuang Liu

School of EIC, Huazhong University of Science and Technology

qiaozhezhang@hust.edu.cn, ruijiezhang@ucsb.edu

juns@hust.edu.cn, liuyz@hust.edu.cn

Jun Sun (juns@hust.edu.cn) is the corresponding author.

###### Abstract

Network pruning is a commonly used measure to alleviate the storage and computational burden of deep neural networks. However, the fundamental limit of network pruning is still lacking. To close the gap, in this work we'll take a first-principles approach, i.e. we'll directly impose the sparsity constraint on the loss function and leverage the framework of _statistical dimension_ in convex geometry, thus enabling us to characterize the sharp phase transition point, which can be regarded as the fundamental limit of the pruning ratio. Through this limit, we're able to identify two key factors that determine the pruning ratio limit, namely, _weight magnitude_ and _network sharpness_. Generally speaking, the flatter the loss landscape or the smaller the weight magnitude, the smaller pruning ratio. Moreover, we provide efficient countermeasures to address the challenges in the computation of the pruning limit, which mainly involves the accurate spectrum estimation of a large-scale and non-positive Hessian matrix. Moreover, through the lens of the pruning ratio threshold, we can also provide rigorous interpretations on several heuristics in existing pruning algorithms. Extensive experiments are performed which demonstrate that our theoretical pruning ratio threshold coincides very well with the experiments. All codes are available at: https://github.com/QiaozheZhang/Global-One-shot-Pruning

## 1 Introduction

Deep neural networks (DNNs) have achieved huge success in the past decade, which relies heavily on the overparametrization, i.e. the number of parameters is normally several order of magnitudes more than the number of data samples. Though being a key enabler for the striking performance of DNN, overparametrization poses huge burden for computation and storage in practice. It is therefore tempting to ask: 1) Can we prune the DNN by a large ratio without performance sacrifice? 2) What's the fundamental limit of network pruning?

For the first question, a key approach is to perform network pruning, which was first introduced by . Network pruning can substantially decrease the number of parameters, thus alleviating the computational and storage burden. The basic idea of network pruning is simple, i.e., to devise metrics to evaluate the significance of parameters and remove the insignificant ones. Various pruning algorithms have been proposed so far: [21; 12; 13; 23; 42; 35; 36; 24; 22; 15] and .

In contrast, our understanding on the second question, i.e., the fundamental limit of network pruning, is far less. Some relevant works are:  proposed to characterize the degrees of freedom of a DNN by exploiting the framework of Gaussian width.  directly applied the above degrees of freedom result to the pruning problem, in the main purpose of unveiling the mechanisms behind theLottery Thicket Hypothesis (LTH) . The lower bound of pruning ratio is briefly mentioned in , unfortunately, their predicted lower bound does not match the actual value well, in some cases even with big gap. And there is no discussion on the upper bound in that paper.

Despite the above progress, a systematic of the study on the **fundamental limit** of network pruning is still lacking. To close this gap, we'll take a first principles approach to address this problem and exploit the powerful framework of the high-dimensional convex geometry. Essentially, we impose the sparsity constraint directly on the loss function, thus we can reduce the _pruning limit_ problem to a _set intersection_ problem, then we leverage the powerful approximate kinematics formula  in convex geometry, which provides a very sharp phase transition point to easily address the set intersection problem, thus enabling a very tight characterization of the limit of network pruning. Intuitively speaking, the limit of pruning is determined by the dimension of the loss sublevel set (whose definition is in Sec. 2) of the network, the higher the latter, the smaller the former.

The key **contributions** of this paper can be summarized as follows:

* We fully characterize the limit of network pruning, which coincides perfectly with the experiments. Moreover, this limit conveys two valuable messages: 1) The smaller the _network sharpness_ (defined as the trace of the Hessian matrix), the more we can prune the network; 2) The smaller the _weight magnitude_, the more we can prune the network.
* We provide an efficient _spectrum estimation_ algorithm for _large-scale_ Hessian matrices when computing the Gaussian width of a high-dimensional _non-convex_ set.
* We present intuitive explanations on many heuristics utilized in existing pruning algorithms through the lens of our pruning ratio limit, which include: (a). Why gradually changing the pruning ratio during iterative pruning is preferred. (b). Why employing \(l_{2}\) regularization makes significant performance difference in Rare Gems algorithm . (c).Why magnitude pruning might be the optimal pruning strategy.

### Related Work

**Pruning Methods:** Unstructured pruning involves removing unimportant weights without adhering to some structural constraints. Typical methods in this class include:  presented the train-prune-retrain method, which reduces the storage and computation of neural networks by learning only the significant connections. [37; 38] employed the energy consumption of each layer as the metric to determine the pruning order and developed latency tables to identify the layers that should be pruned.  proposed dynamic network surgery, which reduced network complexity significantly by pruning connections in real time.  proposed pruning by iteratively removing part of the small weights, and based on Frankle's iterative pruning[6; 28] introduced \(l_{2}\)-norm to constrain the magnitude of unimportant parameters during iterative training. To the best of our knowledge, there is still no _systematic_ study on the fundamental limit of pruning from the theoretical perspective.

**Understanding Neural Networks through Convex Geometry:** Convex Geometry is a powerful tool for characterizing the performance limit of high-dimensional statistical inference  and learning problems. For statistical inference,  pioneered to employ the convex geometry to study the recovery threshold of the classical linear inverse problem. For statistical learning,  studied the training dimension threshold of the network from a geometric point of view by utilizing the Gordon's Escape theorem, which shows that the network can be trained with less degrees of freedom (DoF) than the network size in the affine subspace. The most relevant work to ours is , which studied the Lottery Tickets Hypothesis (LTH) by applying the above DoF results in  to demonstrate that iteration is needed in LTH and that pruning is impacted by the eigenvalues of the loss landscape. The main difference between  and ours are as follows: 1) The lower bound of the pruning ratio is only briefly mentioned in , and their predicted lower bound does not match the actual value well, in some cases even with quite big gap (the main reason lies in the spectrum estimation error in their algorithm). 2) The core results in  are based on Gordon's Escape theorem , which can only provide the lower bound (necessary condition). 3) Rigorous analysis as well as the computational issues regarding the pruning limit are lacking in . In contrast, all the above issues are addressed in this paper.

Problem Setup & Key Notions

To explore the fundamental limit of network pruning, we'll take the first principles approach. In specific, we directly impose the sparsity constraint on the original loss function, thus the feasibility of pruning can be reduced to determining whether two sets, i.e. _the sublevel set_ (determined by the Hessian matrix of the loss function) and the _\(k\)-sparse set_ intersects. Through this framework, we're able to leverage tools in high-dimensional convex geometry, such as statistical dimension , Gaussian width  and Approximate Kinematics Formula .

**Model Setup.** Let \(}=f(,)\) be a deep neural network with weights \(^{D}\) and inputs \(^{K}\). For a given training data set \(\{_{n},_{n}\}_{n=1}^{N}\) and a loss function \(\), the empirical loss landscape is defined as \(()=_{n=1}^{N}(f(, _{n}),_{n})\).

**Pruning Objective.** In essence, network pruning can be formulated as the following optimization problem:

\[\|\|_{0}() (^{*})+\] (1)

where \(\) is the pruned weight and \(^{*}\) is the original one.

**Sparse Network.** Given a dense network with weights \(^{*}\), we denote its sparse counterpart as a \(k\)-sparse network, whose weight is given by: \(^{k}=^{*}\), where \(\) is element-wise multiplication and \(\|\|_{0}=k\).

**Loss Sublevel Sets.** A loss sublevel set of a network is the set of all weights \(\) that achieve the loss up to \((^{*})+\):

\[S():=\{^{D}:() (^{*})+\}.\] (2)

**Feasible \(k\)-Sparse Pruning.** We define the **pruning ratio** as \(=k/D\) and call a sparse weight \(^{k}\) as a feasible \(k\)-sparse pruning if it satisfies:

\[S()\{^{k}\},\] (3)

Below are some key notions and results from high dimensional convex geometry, which are of critical importance to our work.

**Definition 2.1** (Convex Cone & Conic Hull): _A convex cone \(^{D}\) is a convex set that satisfy: \(_{i}_{i}x_{i}\) for all \(_{i}>0\) and \(x_{i}\). The convex conic hull of a set \(S\) is defined as:_

\[(S):=\{_{i}_{i}_{i} ^{D}:_{i}>0,\;_{i} S\}\] (4)

**Definition 2.2** (Gaussian Width ): _The gaussian width of a subset \(S^{D}\) is given by:_

\[w(S)=_{, S} ,-,( ,_{D D}).\] (5)

Gaussian width is useful to characterize the complexity of a convex body. On the other hand, statistical dimension is an important metric to characterize the complexity of convex cones. Intuitively speaking, the bigger the cone, the larger the statistical dimension, as illustrated in Fig. 1(b).

**Definition 2.3** (Statistical Dimension ): _The statistical dimension \(()\) of a convex cone \(\) is:_

\[():=[\|_{}()\|_{2}^{2}]\] (6)

_where \(_{}\) is the Euclidean metric projector onto \(\) and \((,_{D D})\) is a standard normal vector._

To characterize the sufficient and necessary condition of the set intersection, we'll resort to the powerful Approximate Kinematics Formula , which basically says that for two convex cones (or generally, sets), if the sum of their statistical dimension exceeds the ambient dimension, these two cones would intersect with probability 1, otherwise they would intersect with probability 0.

**Theorem 2.4** (Approximate Kinematics Formula, Theorem 7.1 of ): _Let \(\) be a convex conic hull of a sublevel set \(S()\) in \(^{D}\), and draw a random orthogonal basis \(^{D D}\). For a \(k\)-dimensional subspace \(S_{k}\), it holds that:_

\[()+k D\{ S_{k}=\} 1\] \[()+k D\{ S_{k}=\} 0\]

## 3 Bounds of Pruning Ratio

### Lower Bound of Pruning Ratio

In this section, we aim to characterize the lower bound of pruning ratio, i.e. when the pruning ratio falls below some threshold, it's _impossible_ to retain the generalization performance. To establish this impossibility result, we'll leverage the Approximate Kinematics Formula as detailed in Theorem 2.4.

#### 3.1.1 Network Pruning As Set Intersection

To demonstrate that when \(k\) is smaller than a given threshold, it is impossible to find a performance-preserving \(k\)-sparse network induced by the dense network, we need to prove that the loss sublevel set has no intersection with any \(k\)-sparse set resulting from the dense weight, i.e. \(S()\{^{k}\}=\).

To that end, it suffices to prove its translated version, namely \(S_{^{k}}()\{\}=\), where \(S_{^{k}}():=\{-^{k}: S( )\}\). To prove the latter, we'll further prove its strengthened version, i.e. the convex conic hull of \(S_{^{k}}()\) and a random orthogonal rotation of the subspace \(S(^{k})\), which is comprised of all vectors that share the same zero-pattern as \(^{k}\) (including the point \(\)), has no intersection. Namely, we'll prove that

\[(S_{^{k}}())S(^{k})=,\] (7)

where \(\) denotes the Haar-measured orthogonal rotation.

To prove Eq.7, we can easily invoke the necessary condition of the Approximate Kinematics Formula (Theorem 2.4). In order to calculate the involved statistical dimension therein, we choose to calculate the corresponding Gaussian width as the proxy by taking advantage of the following theorem.

**Theorem 3.1** (Gaussian Width vs. Statistical Dimension, Proposition 10.2 of ): _Given a unit sphere \(^{D-1}:=\{^{D}:\|\|=1\}\), let \(\) be a convex cone in \(^{D}\), then:_

\[w(^{D-1})^{2}() w(^{D-1})^{2}+1\] (8)

To calculate the Gaussian width of \((S_{^{k}}())\), we need to project the sublevel set \(S_{^{k}}()\) onto the surface of the unit sphere centered at origin. which is defined as

\[(S_{^{k}}())=\{(-^{k})/\| -^{k}\|_{2}: S_{^{k}}()\},\] (9)

and illustrated in Fig. 1(c). It is easy to see that as the distance \(\|-^{k}\|_{2}\) increases, the projected Gaussian width will decrease, as a result the statistical dimension of the set will also decrease, thus increasing the difficulty of its intersecting with a given subspace.

**Theorem 3.2** (Lower Bound of Pruning Ratio): _Let \(\) be a convex conic hull of a sublevel set \(S_{^{k}}()\) in \(^{D}\). \(^{k}\) doesn't constitute a feasible \(k\)-sparse pruning with probability 1, if the following holds:_

\[w(p(S_{^{k}}()))^{2}+k D\] (10)

This theorem tells us that when the dimension \(k\) of the sub-network is lower than \(k_{L}=D-w((S_{^{k}}()))^{2}\), the subspace will not intersect with \(S_{^{k}}()\), i.e., no feasible \(k\)-sparse pruning can be

Figure 1: **Panel (a, b):** Illustration of a convex conic hull and the statistical dimension. **Panel (c):** Effect of projection distance on projection size and intersection probability.

found. Therefore, the lower bound of the pruning ratio of the network can be expressed as:

\[_{L}=(S_{^{k}}()))^{2}}{D}=1-(S_{^{k}}()))^{2}}{D}.\] (11)

It's worth mentioning that this lower bound has been provided in  by utilizing the Gordon's Escape Theorem. The main difference between our work and  lies in that Gordon's Escape Theorem is not strong enough to provide the upper bound (sufficient condition) of the pruning ratio, while the Approximate Kinematic Formula we employ does.

**Reformulation of the Sublevel Set.** Consider a well-trained deep neural network model with weights \(^{*}\) and a loss function \(()\), where \(\) lies in a small neighborhood of \(^{*}\). By performing a Taylor expansion of \(()\) at \(^{*}\), using the fact that the first derivative is equal to \(\) and ignoring the higher order terms, the loss sublevel set \(S()\) can be reformulated as:

\[S()=\{}^{D}:}^{ T}}\}\] (12)

where \(}=-^{*}\) and \(\) denote the Hessian matrix of \(()\) w.r.t. \(\). Due to the positive-definiteness of \(\), \(S()\) corresponds to an ellipsoid. The related proofs can be found in Appendix D.1.

#### 3.1.2 Gaussain Width of the Ellipsoid

We leverage tools in high-dimensional probability, especially the concentration of measure, which enables us to present a rather precise expression for the Gaussian width of a high-dimensional ellipsoid.

**Lemma 3.3**: _For an ellipsoid \(S()\) defined by : \(S():=\{^{D}:^{T}\}\), where \(^{D D}\) is a positive definite matrix, its Gaussian width is given by:_

\[w(S())(2(^{-1}))^{1/2}=( _{i=1}^{D}r_{i}^{2})^{1/2}\] (13)

_where \(r_{i}=}\) is the radius of ellipsoidal body and \(_{i}\) is the \(i\)-th eigenvalue of \(\)._

The proof of Lemma 3.3 is in Appendix D.1. The Gaussian width of an ellipsoid has been provided in  as in the interval \([(}(_{i=1}^{D}r_{i}^{2})^{1/2},(_{i=1}^{D}r_{i}^{2 })^{1/2}]\), in contrast we sharpen the estimation of Gaussian width to a point \((_{i=1}^{D}r_{i}^{2})^{1/2}\). For the settings which involve projection, the squared radius \(r_{i}^{2}\) should be modified to \(^{2}}{\|^{*}-^{k}\|_{2}^{2}+r_{i}^{2}}\). Therefore, the Gaussian width of projected \(S()\) defined in Eq.(12) equals:

\[w((S_{^{k}}()))(_{i=1}^{D}^{2 }}{\|^{*}-^{k}\|_{2}^{2}+r_{i}^{2}})^{1/2}\] (14)

#### 3.1.3 Computable Lower Bound of Pruning Ratio

Combining Eq.(11) and Eq.(14), we obtain the following computable lower bound of the pruning ratio:

**Corollary 3.4**: _Given a well-trained deep neural network with trained weight \(^{*}^{D}\) and a loss function \(()\), for a \(k\)-sparse pruned weight \(^{k}\), the lower bound of pruning ratio of model is:_

\[_{L}=1-_{i=1}^{D}^{2}}{\|^{*}- ^{k}\|_{2}^{2}+r_{i}^{2}}.\] (15)

_where \(r_{i}=}\) and \(_{i}\) is the eigenvalue of the Hessian matrix of \(()\) w.r.t. \(\)._

#### 3.1.4 Pruning Ratio vs Magnitude & Sharpness

It is evident from Eq.(15) that for a given trained network (whose spectrum of the Hessian matrix is fixed), to minimize the lower bound of the pruning ratio, we just need to minimize \(\|^{*}-^{k}\|_{2}\), i.e. the sum of magnitudes of the pruned parameters. Therefore, the commonly-used magnitude-based pruning algorithms get justified. Moreover, it also inspires us to employ the one-shot magnitude pruning algorithm as detailed in Section 4, whose performance proves to be better than other existing algorithms, to the best of our knowledge.

Besides the above-discussed magnitude of the pruned sub-vector, we also identify another important factor that determines the pruning ratio, i.e. the _network sharpness_, which describes the sharpness of the loss landscape around the minima, as defined by the trace of the Hessian matrix, namely \(()\) ( and , network flatness is the opposite of network sharpness; as sharpness increases, flatness decreases, and vice versa.).

**Lemma 3.5** (Pruning Ratio vs. Sharpness): _Given a well-trained neural network \(f()\), where \(\) is the parameters. The lower bound of the pruning ratio and the sharpness obeys:_

\[_{L} 1-^{*}-^{k}\|_{2}^{2} ()+2 D}\] (16)

_where \(\) is the hessian matrix of the loss function w.r.t. \(\)._

Lemma 3.5 is obtained by utilizing the Cauchy-Schwarz Inequality, whose proof can be found in Appendix F. It can be seen from Lemma 3.5 that the lower bound of the network pruning ratio is heavily dependent on the sharpness of the network, i.e. flatter networks imply more sparsity. This can be a valuable guideline for both training and pruning the networks. Intuitively, a flatter loss landscape is less sensitive to weight perturbations, indicating greater tolerance to weight removal.

### Upper Bound of Pruning Ratio

In order to establish the upper bound of the pruning ratio, we need to prove that there _exists_ an \(k\)-sparse weight vector intersects with the loss sub-level set.

For a given trained weight \(^{*}\), we split it into two parts, i.e. the unpruned subvector, \(^{1}=[^{*}_{1},^{*}_{2},,^{*}_{ k}]\) and the pruned one \(^{2}=[^{*}_{k+1},^{*}_{k+2},,^{*}_{ D}]\). By fixing \(^{1}\), the loss sublevel set can be reformulated as:

\[S(^{{}^{}},)=\{^{{}^{}}^ {D-k}:([^{1},^{{}^{}}])( ^{*})+\}\] (17)

In order to prove the existence of a \(k\)-sparse weight vector \(^{k}\), we just need to show that the all-zero vector is in \(S(^{{}^{}},)\). To that end, we'll take advantage of the sufficient condition of the approximate kinematics formula (Theorem 2.4) to show that it suffices to render the statistical dimension of the projected cone of \(S(^{{}^{}},)\) being full, i.e. \(D-k\). Thus we can obtain the upper bound of the number of unpruned parameters, i.e. \(k\).

Specifically, by invoking the sufficient part of Theorem 2.4, the upper bound of the pruning ratio by a given pruning strategy is as follows:

**Theorem 3.6** (Upper Bound of Pruning Ratio): _Given a sublevel set \(S(^{{}^{}},)\) in \(^{D-k}\). To ensure that the all-zero vector \(^{D-k}\) contained in \(S(^{{}^{}},)\), it suffices that:_

\[w(p(S(^{{}^{}},)))^{2} D-k.\]

The Gaussian width of projected \(S(^{{}^{}},)\) can be easily obtained by employing Lemma 3.3, i.e. \(w((S(^{{}^{}},)))^{2}=_{i}^{D-k}_{i}^{2}}{\|^{*}-^{k}\|_{2}^{2}+_{i}^{2}}\), where \(_{i}=_{i}}\), \(_{i}\) is the eigenvalue of the hessian matrix of \(([^{1},^{{}^{}}])\) w.r.t. to \(^{{}^{}}\) and the fact that \(\|^{*}-^{k}\|_{2}=\|^{2}\|_{2}\) is used. Correspondingly, the upper bound of the pruning ratio can be expressed as

\[_{U} 1-(S(^{{}^{}},)))^{2}}{ D}=1-_{i=1}^{D-k}_{i}^{2}}{\|^{*}- ^{k}\|_{2}^{2}+_{i}^{2}}.\] (18)

### Fundamental Limit of Pruning Ratios

As demonstrated above, the pruning ratio can be bounded as follows:

\[1-_{i=1}^{D}^{2}}{\|^{*}-^{k}\|_{2 }^{2}+r_{i}^{2}} 1-_{i}^{D-k}_{i}^{2} }{\|^{*}-^{k}\|_{2}^{2}+_{i}^{2}}.\] (19)

It is easy to notice that the upper bound and lower bound are of nearly identical form. In fact, as we'll elaborate in the following, they are also of quite close value, which implies that we are able to obtain a sharp characterization of the fundamental limit of pruning ratio. Meanwhile, it is worthwhile noting that the pruning limit depends on the magnitude of the final weights, which might be significantly impacted by the weight initialization. Therefore, we still need to explore whether the magnitude of final weights is dependent on the initialization values. In the appendix, we'll demonstrate that once the data, network architecture and training method are fixed, the distribution of trained network weights remains nearly insensitive to the initializations, thus yielding an affirmative answer about the above question.

## 4 Achievable Scheme & Computational Issues

Thus far we have established the lower bound and upper bound of the pruning ratio by leveraging the Approximate Kinematic Formula in convex geometry . To proceed, we will demonstrate that our obtained bounds are tight in the sense that we can devise an achievable pruning algorithm whose corresponding upper bound is quite close to the lowest possible value of the lower bound. As argued in Corollary 3.4, the magnitude pruning, which removes all the smallest \(D-k\) weights, will result in the lowest pruning ratio lower bound. Inspired by this result, we'll focus on the magnitude pruning methods in order to approach the lower bound in the sequel.

For the lower bound part, we need to address several challenges regarding the computation of the Gaussian width of a high-dimensional deformed ellipsoid, which involves tackling the _non-positiveness_ of the Hessian matrix as well as the _spectrum estimation_ of a large-scale Hessian matrix.

For the upper bound part, we'll focus on a _relaxed_ version of Eq. 1 by introducing the \(l_{1}\) regularization term, for the sake of computational complexity. Then we'll employ the _one-shot_ magnitude pruning to compress the network.

### Computational Challenges & Countermeasures

To compute the lower bound of the pruning ratio, we need to address the following two challenges:

**Gaussian Width of the Deformed Ellipsoid.** In practice, it is usually hard for the network to converge to the exact minima, thus leading to a non-positive definite Hessian matrix. In other words, the ideal ellipsoid gets deformed due to the existence of negative eigenvalues. Determining the Gaussian width of the deformed ellipsoid is a challenging task. To address this problem, we resort to convexifying (i.e. taking the convex hull of) the deformed ellipsoid and then calculating the Gaussian width of the latter instead, by proving that the convexifying procedure has no impact on the Gaussian width. (The proof is presented in Appendix D.2).

**Improved Spectrum Estimation.** Neural networks often exhibit a quite significant number of zero-valued or vanishingly small eigenvalues in their Hessian matrices. It's hard for the spectrum estimation algorithm SLQ (Stochastic Lanczos Quadrature) proposed by  to obtain accurate estimation of these small eigenvalues. To address this issue, we propose to enhance the existing large-scale spectrum estimation algorithms by a key modification, i.e, to estimate the number of these exceptionally small eigenvalues by employing the Hessian matrix sampling. See Algorithm 1 for the details of the improved spectrum Estimation algorithm. A comprehensive description of the algorithm and its experimental results are presented in Appendix C.

### Achievable Scheme: \(l_{1}\) Regularization & One-shot Magnitude Pruning

Inspired by the lower bound as well as upper bound of the pruning ratio, in which the magnitude of pruning parameters plays a key role, it's sensible to focus on the magnitude-based pruning methods.

On the other hand, to find exact solutions of our original problem for the best pruning in Eq. 1, it is obviously very hard due to the existence of \(l_{0}\) norm. To make it feasible, it's natural to perform a convex relaxation of \(l_{0}\) norm, namely, by employing \(l_{1}\) regularization instead. Aside from the computational advantage of this relaxation, it is worthy noting that \(l_{1}\) regularization provides two extra benefits: 1) A large portion of eigenvalues of the trained Hessian matrix are zero-valued or of quite small value, which renders the calculation of the pruning limit more accurately and fast. 2) A large portion of trained weights are of quite small value, thus making the lower bound and upper bound very close. Detailed statistics about the eigenvalues and magnitudes can be found in Figure 6.

Specifically, by utilizing the Lagrange formulation and convex relaxation of \(l_{0}\) norm, the pruning objective in Eq.1 can be reformulated as:

\[\ ()+\|\|_{1}\] (20)

After training with this relaxed objective, the network weights will be pruned based on magnitudes _one time_, rather than in an iterative way as in [13; 6; 28]. The performance of the above described pruning scheme (termed as "\(l_{1}\)_regularized one-shot magnitude pruning_" and abbreviated as "LOMP") can be found in Table 10 in Appendix. The above stated "zero-dominating" property due to \(l_{1}\) regularization gets supported in Figure 2(b), where it can be seen that the majority of weights are indeed extremely small.

The above "zero-domination" property turns out to be of critical value for our proposed pruning scheme to nearly achieve the limit (lower bound) of the pruning ratio. Fig. 2(c) illustrates the curve \(\|^{2}\|_{2}^{2}\), i.e. the \(l_{2}\) norm of the \(D-k\) smallest weights, w.r.t. \(k/D\). The vertical line therein represents \(_{L}\), the lower bound of the pruning ratio predicted in Section 3.1. When \(k=D_{L}\), the curve and the line will intersect as shown in Figure 2(c). Mathematically, the upper bound for the pruning ratio can be approximated as follows:

\[_{U}=1-_{i=1}^{D-k}_{i}^{2}}{\|^{2}\|_{2}^{2}+_{i}^{2}} 1-_{i=1}^{D-k} _{i}^{2}}{_{i}^{2}}==_{L}\] (21)

Figure 2: Effect of extremely small projection distance on projection size and intersection probability and statistics of ResNet50 on TinyImagenet. Statistics regarding all experiments can be found in Appendix G.

It can be seen from the above demonstration that the upper bound corresponding to our proposed pruning scheme almost _coincides_ with the minimal lower bound! In other words, we have established the fundamental limit of the pruning ratio. To provide further validation of the above claim, we performed the experiments five times across eight tasks and reported the differences between the upper bound and lower bound, denoted as \(\), in Table 9.

## 5 Experiments

In this section, we validate our pruning method as well as the theoretical limit of the pruning ratio by experiments.

Tasks.We evaluate the pruning ratio threshold on: Full-Connect-5(FC5), Full-Connect-12(FC12), AlexNet  and VGG16  on CIFAR10 , ResNet18 and ResNet50  on CIFAR100 and TinyImageNet . We employ the \(l_{1}\) regularization during training, and execute a one-shot magnitude-based pruning and assess its performance with various sparsity ratios, in terms of the metrics of accuracy and loss. Detailed descriptions of datasets, networks, hyper-parameters, and eigenspectrum adjustment can be found in Section B of the Appendix. Moreover, the performance comparison between \(l_{1}\)-regularization-based magnitude pruning and other pruning methods can be found in Table 10 in Appendix.

### Validation of Pruning Lower Bound

After training with the \(l_{1}\) regularization, we compute the eigenvalues and present the theoretical limit of pruning ratio. By pruning the trained network to various sparsity levels, we depict in Figure 3 both the line of theoretical lower bound and the sparsity-accuracy curve for the above-listed tasks. From the figures we can see that our theoretical result matches the numerical pruning ratio quite well.

### Prediction Performance

We present a more detailed comparison between our theoretical limit of pruning ratio, and the actual values by experiments in Table 2, which shows nearly perfect agreement between them. The difference between the theoretical value and the actual value is donated as \(\).

## 6 Interpretation of Pruning Heuristics

Equipped with the fundamental limit of network pruning, we're now able to provide rigorous interpretations of several heuristics employed by existing pruning algorithms.

  
**CIFAR10** & **FC5** & **FC12** & **Alexnet** & **VGG16** \\ \(\)(\%) & 0.17\(\)0.05 & 0.05\(\)0.03 & 0.02\(\)0.01 & 0.01\(\)0.00 \\ 
**ResNet** & **18 on CIFAR100** & **50 on CIFAR100** & **18 on TinyImagenet** & **50 on TinyImagenet** \\ \(\)(\%) & 0.12\(\)0.05 & 0.11\(\)0.09 & 0.09\(\)0.01 & 0.27\(\)0.22 \\   

Table 1: The Difference Between Lower Bound and Upper Bound of Pruning Ratio.

Figure 3: The impact of sparsity on loss and test accuracy are obtained on the test dataset, and we mark the theoretical pruning ratio limit with vertical lines. The loss values have been normalized and translated.

**Pruning ratio adjustment is needed in IMP.** For the IMP (Iterative Magnitude Pruning) algorithm , we determine the pruning ratio thresholds for various stages through calculations, as depicted in the top row of Figure 4. It is noteworthy that as the pruning depth gradually increases, the theoretical pruning ratio threshold also increases. Therefore, it is appropriate to prune smaller proportions of weights gradually during iterative pruning, Both  and  have employed pruning rate adjustment, which gradually prunes smaller proportions of the weights with the iteration of the algorithm.

\(l_{2}\)**-regularization enhances the performance of Rare Gems.** For the Rare Gems algorithm , it is shown that \(l_{2}\) regularization makes a significant difference in terms of the final performance, as shown in the bottom row of Figure 4. The main reason behind this phenomenon is: when \(l_{2}\)-regularization is applied, the pruning ratio tends to be larger than the theoretical limit, however, the absence of \(l_{2}\)-regularization would result in excessive pruning, which can be regarded as wrong pruning.

## 7 Conclusion

In this paper we explore the fundamental limit of pruning ratio of deep networks by taking the first principles approach and leveraging the framework of convex geometry. Specifically, we reduce the pruning limit problem to the sets intersection problem, and by taking advantage of the powerful Approximate Kinematic Formula, we are able to sharply characterize the fundamental limit of the network pruning ratio. This fundamental limit conveys a key message as follows: the network pruning limit is mainly determined by the _weight magnitude_ and the _network sharpness_. These two guidelines can provide intuitive explanations of several heuristics in existing pruning algorithms. Moreover, to address the challenges in computing the involved Gaussian width, we develop an improved spectrum estimation for large-scale and non-positive Hessian matrices. Experiments demonstrate the almost perfect agreement between our theoretical results and the experimental ones.

Limitations.In this paper, the (almost) coincidence of the upper bound and lower bound of the pruning ratio depends on the condition that the removed weights are of quite small value, which is enabled by the \(l_{1}\) regularization we employed. Therefore, it is important to demonstrate whether the \(l_{1}\) regularized training is optimal or nearly optimal in the sense of obtaining the smallest sub-network without performance degradation for the original learning problem.

  
**Dataset** & **Model** & **Theo. Value(\%)** & **Actual Value(\%)** & \(\)(\%) \\   & FC5 & 2.1\(\)0.25 & 1.7\(\)0.12 & -0.40\(\)0.35 \\  & FC12 & 1.0\(\)0.30 & 0.8\(\)0.06 & -0.24\(\)0.33 \\  & AlexNet & 0.9\(\)0.00 & 0.8\(\)0.08 & -0.14\(\)0.08 \\  & VGG16 & 0.8\(\)0.06 & 0.8\(\)0.08 & 0.04\(\)0.08 \\   & ResNet18 & 1.5\(\)0.05 & 2.0\(\)0.13 & 0.54\(\)0.15 \\  & ResNet50 & 1.9\(\)0.05 & 2.1\(\)0.16 & 0.28\(\)0.19 \\   & ResNet18 & 3.9\(\)0.82 & 4.3\(\)0.38 & 0.46\(\)0.71 \\  & ResNet50 & 2.6\(\)0.24 & 2.9\(\)0.33 & 0.36\(\)0.10 \\   

Table 2: Comparison between Theoretical and Actual Values of Pruning Ratio

Figure 4: **Top Row:** From left to right, as the number of iterations increases, it leads to an increase in the theoretical pruning ratio threshold. The horizontal line represents the last pruning ratio. **Bottom Row:** The comparison of the pruning ratio threshold between using and not using \(l_{2}\)-regularization. Sparse networks are obtained by magnitude-based pruning with fixed pruning ratios. The two plots on the left and the two plots on the right correspond to different fixed pruning ratios. Here, \(R=\|^{*}-^{k}\|_{2}\), which is the projection distance.