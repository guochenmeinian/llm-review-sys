# Is Knowledge Power?

On the (Im)possibility of Learning from Strategic Interactions

Nivasini Ananthakrishnan

UC Berkeley, {nivasini,nika,kunheyang}@berkeley.edu

Nika Haghtalab

UC Berkeley, {nivasini,nika,kunheyang}@berkeley.edu

Chara Podimata

MIT & Archimedes AI, podimata@mit.edu

Kunhe Yang

UC Berkeley, {nivasini,nika,kunheyang}@berkeley.edu

###### Abstract

When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players' actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value.

## 1 Introduction

Learning to act in strategic environments is fundamental to the study of decision making under uncertainty in a wide range of applications, such as security, economic policy, and market design (e.g., ). In these environments, acting and learning are intimately connected: agents' actions and the reactions they elicit generate payoffs, and help clarify the latent preferences of other agents. A central question is whether, through repeated interactions alone, agents (aka _players_) can overcome uncertainty about each other's preferences in order to achieve outcomes they could have achieved in the absence of uncertainty. An extensive line of work on _learning in Stackelberg Games_ has focused on answering this question for achieving the _Stackelberg value_, which is the optimal payoff a player guarantees herself when assuming other players will best respond to her actions. While a player who hopes to attain her Stackelberg value in a one-shot game must know the game (i.e., know the utilities of all players), this line of work asks whether a player who is a-priori uninformed can overcome her lack of knowledge and attain her Stackelberg value through repeated interactions with other players.

By and large, existing works have studied this question by constructing learning algorithms for uninformed players that attain their Stackelberg value, through repeated interactions with other players who myopically best respond. While these results are encouraging for learning about thepreferences of well-behaved best responding agents1, they do not provide clear evidence of the ability of uninformed players to learn from strategic interactions alone. Indeed, the two players' different attitudes towards their outcomes -- namely, one player planning a long-term strategy to maximize her long-term payoff while the other player responding without considering the impact of her actions on her long-term payoff -- confounds the overall impact uncertainty may have on how well players learn from strategic interactions; leaving one to wonder whether it was the lack of rational long-term planning on the part of one agent or some genus of the learning algorithm employed by the other agent that enabled her to learn from strategic interactions.

In this paper, we revisit the problem of learning in strategic environments with a renewed focus on the impact of information asymmetry between two equally rational players who aim to maximize their total payoff. We ask again: _can an uninformed player learn to attain the value of her Stackelberg outcome, through repeated interactions alone?_ In contrast to the aforementioned results, our findings largely imply that _strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player the value of her optimal strategy._

**Our Model and Contributions.** To study the impact of informativeness on the ability of players to gain the payoff of their Stackelberg outcome, we study repeated interactions between two rational agents playing repeatedly a one-shot game \(G\). While the players know \(\), they may not know the realized game \(G\) except perhaps through signals of differing precision about \(G\). For example, one player may know \(G\) and another may have access to a signal that reveals \(G\) with probability \(0.5\) and is uninformative (i.e., independently drawn from \(\)) otherwise. Each player deploys an algorithm, which specifies her actions at every round, given all that the player has observed so far (e.g., history of actions and/or utilities experienced) and the information she possesses about \(G\). We consider a meta-game where players' actions are algorithms that specify agent's strategies in \(T\) rounds of interactions and study pairs of algorithms that form pure Nash Equilibria in the meta-game. We use the overall utility attained by pairs of algorithms that form pure Nash Equilibria to draw a clear separation between the informed and uninformed players' ability to attain the value of their Stackelberg optimal strategies.

In the following, we use StackVal\({}_{i}(G)\) to refer to player \(i\)'s value of her optimal Stackelberg strategy in game \(G\). We summarize our results as follows.

* In Section 3, we study the _full information asymmetry_ when player 1 (\(_{1}\)) knows the realized game \(G\) and player 2 (\(_{2}\)) only has partial information based on an imperfect signal. We show a full separation in the achievable utilities. In particular, we show in Theorem 3.1 that for every distribution \(\) and realized game \(G\), there is a pure Nash Equilibrium (PNE) in the meta game induced between the algorithms' of the two players for which \(_{1}\) achieves her Stackelberg value, i.e., \(_{1}(G)\). On the other hand, Theorem 3.2 gives a distribution \(\) such that no PNE of the meta game allows \(_{2}\) to achieve \(_{G}[_{2}(G)]\). In other words, for some realized game \(G\) and all PNE of the meta-game, \(_{2}\) cannot achieve the value of her optimal Stackelberg strategy for \(G\). Taken together, Theorem 3.1 and Theorem 3.2 establish that learning through interactions alone is not sufficient to allow an uninformed player (in this case, \(_{2}\)) to attain the value of her optimal Stackelberg strategy. Does this mean that \(_{2}\) in unable to _learn_ the game matrix through interactions that form a PNE in the meta-game? This is not necessarily the case (see Observation A.4) as indeed \(_{2}\) may be able to learn the underlying game \(G\) eventually. What our results imply is that in every PNE, either \(_{2}\) never learns the game \(G\) sufficiently well, or she has enough information to identify \(G\) but the stability condition for her algorithm to be in a PNE does not allow her to extract the value of her optimal Stackelberg strategy. This points to the limitations on what agents can achieve if their only source of learning is through repeated interactions.
* In Section 4, we study a setting where neither player fully knows the realized game \(G\). Interestingly, a separation need not hold in this case. In particular, there are distributions \(\) where the player with a less informative signal about \(G\) is able to extract her benchmark \(_{G}[(G)]\) while the player with the more informative signal cannot achieve her corresponding benchmark. This occurs when the less-informed player is able to learn the identity of \(G\) more efficiently,possibly due to the structure of \(\). This is perhaps not surprising, given that a less-informed player can become more informed or even perfectly aware of the realized game faster, while the player who started the meta-game with a more informative signal continues to remain only partially informed.

On the possibility and impossibility of learning from strategic interactions.We view our work as providing a different lens on studying learnability in the presence of strategic interactions that also elucidates the context and subtleties of a vast line of prior work in this space. By and large, prior work in this space  has attempted to establish the following message: "An uninformed player can always learn to achieve (even surpass) its Stackelberg value through repeated strategic interactions alone". At a high level, our work demonstrates the opposite, that "In some cases, an uninformed player cannot learn, through repeated interactions alone, to achieve its Stackelberg value". Of course, these messages, while both technically correct, are contrary to each other. So, what accounts for this difference?

One of our takeaways is that prior work's findings (that an uninformed can always overcome her informational disadvantage through repeated strategic interactions) heavily hinges on the lack of rationality of at least one of the agents in those strategic interactions. That is, the dynamics studied in prior work involve pairs of agent algorithms that are not best-responses to each other. On the other hand, our work shows that the inherent uncertainty about the game -- or more precisely, the information asymmetry between two equally rational agents -- can persists throughout repeated interactions and makes it impossible for an uninformed agent to overcome her informational disadvantage.

The processes of learning and acting based on the learned knowledge are naturally intertwined when dealing with uncertainty in strategic environments. Our work implies that it is precisely because of their intertwined nature that an uninformed agent cannot overcome her informational disadvantage from strategic interactions alone. That is, information disadvantage between a pair of rational agents persists for one of two reasons: Either actions taken by the agents' algorithms do not reveal enough information to identify the game at play, or if they do, the less-informed agents use of the elicited information would have lead the informed agent to deviate to an algorithm that barred her from learning in the first place.

### Related Works

**Algorithms and benchmarks for repeated principal-agent interactions.** There is a vast literature investigating online algorithms and benchmarks in repeated games with agents under various behavioral models such as: 1) myopically best-responding ; 2) optimizing time-discounted utilities ; 3) employing no-regret , no-swap-regret , no counterfactual-internal-regret algorithms , or online calibrated forecasting algorithms .

Given a particular model of the agent, what is the optimal algorithm to employ? This has been studied in both the complete and incomplete information setting. In the complete information setting, the static algorithm of playing the optimal Stackelberg strategy is shown to be optimal against no-swap-regret agents . But it is not necessarily optimal against general no-regret algorithms, including common algorithms such as EXP3 . Additionally, it is not optimal against no-swap-regret agents in Bayesian games where agents have hidden information but is optimal if agents satisfy a stronger notion called no-polytope-swap-regret .

**Long-term rationality of agents in the meta-game.** Instead of modeling agents as no-regret learners, another line of research treats the repeated game as a _meta-game_ in which players' actions are their choice of _algorithms_. Towards understanding the PNE of this meta-game, Brown et al.  show that no pair of no-swap-regret algorithms can form a PNE unless the stage game has a PNE. Previous work discussed above on optimally responding to no-regret agents also has implications on the meta-game's PNE such as (1) no-swap-regret algorithms are supported in a meta-game PNE for all games \(G\), and (2) there are games where no meta-game PNE contains certain common regret-minimizing algorithms such as EXP3 . We discuss these implications in Appendix C.

Kolumbus and Nisan  study a meta-game where players are restricted to choose no-regret algorithms but have the option to manipulate their private information. They show that non-truthful PNE exists in multiple classes of games. Besides PNE, some previous works also study Stackelberg strategies of meta-games . Recently, Arunachaswaran et al.  study the Pareto optimality relative to all possible games instead of exact optimality in a particular game.

**Information asymmetry in repeated games.** The final line of related work is the substantial literature on information asymmetry and repeated interactions including classical work by Aumann et al. . They also study repeated games between a player knowing the game and one who does not. For zero-sum games, they show that all PNE of the meta-game yields the same utility to the informed player and this utility can be higher than the informed player's one-shot utility. The higher utility is due to the informed player's ability to shape the learned beliefs of the uninformed player and this power of information is also shown in a recent line of work on follower deception in Stackelberg games [24; 33; 23; 6; 13; 12].

## 2 Model and Preliminaries

We study games between two players, referred to as \(_{1}\) and \(_{2}\). Wlog, we assume that \(_{1}\) is generally _more informed_ than \(_{2}\) (to be defined formally below). Although we focus on _repeated_ games, we first provide the setting for one-shot games and then build upon it for repeated games.

Bayesian Games.A game \(G\) is a tuple \((_{1},_{2},U_{1},U_{2})\), where \(_{i}\) is \(_{i}\)'s discrete action space, and \(U_{i}:_{1}_{2}\) is \(_{i}\)'s utility function (\(i\{1,2\}\)). A Bayesian game is described by a family of games \(\) and a distribution \(()\) over games in this family, where \(\) consists of games sharing the same action space. When \(_{1}\) plays action \(x_{1}\) and \(_{2}\) plays action \(y_{2}\), then they receive utilities \(U_{1}(x,y)\) and \(U_{2}(x,y)\) respectively. We sometimes overload notation and write \(U_{1}(x,y;G),U_{2}(x,y;G)\) to denote that the utilities of the two players come from a particular game instance \(G\). Instead of pure strategies (i.e., playing discrete actions), the players can also choose to play _mixed_ strategies \((_{1})\) and \((_{2})\) for players 1 and 2 respectively. To simplify notation, we sometimes write \(U_{i}(,)\) in place of \(_{x,y}[U_{i}(x,y)]\) for \(_{i}\)'s utility. Unless specified otherwise, we assume that the players are moving _simultaneously_ and they both know the prior distribution \(\). We also assume that every game \(G\) in the support of \(\) has no _weakly dominated action_ for either player. An action \(x_{0}_{1}\) is weakly dominated for \(_{1}\) in \(G\) if there exists \((_{1}\{x_{0}\})\) s.t., \(U_{1}(,y;G) U_{1}(x_{0},y;G)\) for every \(y_{2}\). The weakly dominance property of actions \(y_{0}_{2}\) is defined symmetrically for \(_{2}\).

**Optimistic Stackelberg Value.** The optimistic Stackelberg value of a game \(G\) for \(_{1}\), denoted with StackVal\({}_{1}(G)\), is the optimal value of the following optimization problem:

\[_{1}(G)_{^{}(_ {1})}_{y_{2}(^{};G)}U_{1}(^{},y),\]

where \(_{2}(^{};G)*{argmax}_{y _{2}}U_{2}(^{},y)\) indicates \(_{2}\)'s set of best responses to \(^{}\). When there are multiple actions in \(_{2}(^{};G)\), ties are broken _optimistically_ in favor of \(_{1}\). We use \((^{}(G),y(^{};G))\) to denote the pair of strategies that achieves the value StackVal\({}_{1}(G)\). For \(_{2}\), the optimistic Stackelberg value StackVal\({}_{2}(G)\) and \((x(^{};G),^{}(G))\) are defined symmetrically. Finally, we define StackVal\({}_{i}()_{G}[_{i}(G)]\) to be the _expected_ optimistic Stackelberg value for \(_{i}\) under the prior distribution \(\) (\(i\{1,2\}\)).

Game Information.We assume that both players know the prior \(\). After the game \(G\) is realized, each player \(_{i}\) also receives additional information about the realization of \(G\), which is characterized by a signal \(s_{i}\). We assume that nature generates both signals \(s_{1},s_{2}\) independently and with potentially different precision levels \(p_{1},p_{2}\), and each player can only observe their own signal. Fixing a precision \(p_{i}\), \(s_{i}\) perfectly reveals the true game \(G\) with probability \(p_{i}\), and with probability \(1-p_{i}\), it provides an independent draw from the prior distribution \(\). Formally, the conditional distribution of \(s_{i}\) given \(G\) is defined as follows:

\[ G,s_{i},_{p_{i}}(s_{i} G)=p_{i} \{s_{i}=G\}+(1-p_{i})(s_{i}).\]

While each player can only observe their own signal \(s_{i}\), we assume that the distributions generating both signals are common knowledge, i.e., both players know \(p_{1}\) and \(p_{2}\). Note that when \(p_{i}=1\), the signal \(s_{i}\) perfectly correlates with the realization \(G\), in which case we say that \(_{i}\) is _fully-informed_ or have _perfect knowledge_ about which game is being played. On the other extreme, if \(p_{i}=0\), then the signal \(s_{i}\) reveals no additional information compared to the prior distribution \(\). In this case, we call \(_{i}\)_uninformed_. Throughout this paper, we focus on settings with _information asymmetry_ where we always assume \(_{1}\) is more informed than \(_{2}\), i.e., \(p_{1}>p_{2}\).

Repeated Games.In this paper, we focus on repeated (Bayesian) games. Initially, nature draws a game \(G\) from prior \(\). The game is then fixed and repeated for \(T\) rounds. At each round \(t[T]\), \(_{1}\) and \(_{2}\) play strategies \(^{t},^{t}\) and obtain utilities \(U_{1}(^{t},^{t};G),U_{2}(^{t},^{t};G)\) respectively. We call \(G\) the _stage game_ of the repeated interaction.

Without loss of generality, we use _algorithms_ to describe both players' adaptive strategies in the repeated game. For \(i\{1,2\}\), we use \(_{i}\) to denote the algorithm used by \(_{i}\), which is a sequence of mappings \((_{i}^{t})_{t[T]}\) that at each round maps from player \(i\)'s information about the game and historical observations to the distribution of mixed strategies from which the next strategy is drawn. Specifically, for each round \(t\), the mapping is defined as \(_{i}^{t}:(s_{i};H_{i}^{1:t-1})(_{t})\), where \(s_{i}\) is the signal received by \(_{i}\) about the realization of \(G\), and \(H_{i}^{r}\) is the feedback that \(_{i}\) observed at round \(r\) (\(r[t-1]\)). When both players observe each other's realized strategies as well as their own (but not the other's) realized utilities, we have \(H_{i}^{r}=(^{r},^{r},U_{i}(^{r},^{r};G))\). We call this the _full-information feedback_ setting. We also consider the _bandit feedback_ setting, where the players do not observe the strategies of their opponent, i.e., \(H_{1}^{r}=(^{r},U_{1}(^{r},^{r};G))\) and \(H_{2}^{r}=(^{r},U_{2}(^{r},^{r};G))\).

Trajectories and expected utilities.Consider a fixed pair of algorithms \((_{1},_{2})\). Under every realization of \((G,s_{1},s_{2})\), algorithms \((_{1},_{2})\) induce a distribution over trajectories of mixed strategy pairs of length \(T\), which we denote with \((^{t},^{t})_{t[T]}^{T}(_{1},_{2};G,s_{1},s_{2})((_{1})^{T}(_{2 })^{T})\). In particular, the signals \(s_{i}\) (\(i\{1,2\}\)) are inputs of \(_{i}\) that specify \(_{i}\)'s behavior upon receiving certain feedbacks, whereas \(G\) influences \(_{i}\)'s observed utilities, which is part of the feedback and indirectly influences \(_{i}\)'s strategies of the next round.2 We also use \(^{T}(_{1},_{2};G)\) to denote the mixture of \(^{T}(_{1},_{2};G,s_{1},s_{2})\) as \(s_{i}_{p_{i}}( G)\) for \(i\{1,2\}\).

When the realized game is \(G\) and players use algorithms \((_{1},_{2})\) with time horizon \(T\), the _expected average utility_ of \(_{i}\) under \(G\), denoted as \(_{i}(_{1},_{2};G)\), can be expressed as

\[_{i}^{T}(_{1},_{2};G)^{T} (_{1},_{2};G)}{}[_{t[T]}U_{i}(^{t},^{t};G)].\]

We further define \(_{i}^{T}(_{1},_{2};)_{G }\,_{i}^{T}(_{1},_{2};G)\) as the expected average utility under \(\).

Equilibrium in the Meta-Game.We model the rationality of long-term players by treating the repeated Bayesian game \(\) as a meta-game, where each player \(_{i}\)'s action is an algorithm \(_{i}\), and the utilities of each pair of action \((_{1},_{2})\) are given by \(_{i}(_{1},_{2};)\). Our analysis focuses on the _pure Nash equilibria_ (PNE) of this meta game applied to the asymptotic regime \(T\).

**Definition 2.1** (PNE of the Meta-Game).: _We say that a pair of algorithms \((_{1},_{2})\) form a pure Nash equilibrium (PNE) in the meta-game if for all \(i\{1,2\}\) and all other algorithms \(_{i}^{}\),_

\[_{T}(_{i}^{T}(_{i}^{},_{-i};)-_{i}^{T}(_{i},_{-i};)) 0,\]

_where \(_{-i}\) denotes the algorithm of \(_{i}\)'s opponent._

Finally, we define no-regret and no-swap regret algorithms below.

**Definition 2.2** (No-(Swap) Regret Algorithms).: _An algorithm \(_{1}\) of \(_{1}\) is called no-regret if for all adversarial sequences \(^{1:T}(_{2})^{T}\), the strategies \(^{1:T}\) output by \(_{1}\) satisfies_

\[[_{1}^{T}][_{x^{t} _{1}}_{t[T]}U_{1}(x^{},^{t})-U_{1}( ^{t},^{t})] o(T).\]

_Furthermore, \(_{1}\) is called no swap-regret if_

\[[_{1}^{T}][_{f:_{1}_{1}}_{t[T]}U_{1}(f(^{t}),^{t})-U_{1}( ^{t},^{t})] o(T),\]

_where \(f()(_{1})\) denotes the mixed strategy induced by \(f(x)\) as \(x\). We define no-(swap) regret algorithms for \(_{2}\) symmetrically._

We remark that there exist no-regret and no-swap regret algorithms under both the full-information feedback and bandit feedback setting.

## 3 Interactions between fully-informed \(_{1}\) and partially-informed \(_{2}\)

In this section, we analyze the setting with a fully-informed \(_{1}\) -- i.e., \(_{1}\) knows the game \(G\) being played -- and a partially informed \(_{2}\). In other words, algorithms \(_{1}\) and \(_{2}\) can each take an observable signal as input, where the signals received by \(_{1}\) and \(_{2}\) are independently drawn from signal distributions \(_{p_{1}}(|G)\) and \(_{p_{2}}(|G)\) with precision \(p_{1}=1\) and \(p_{2}<1\), respectively. Recall that each \(_{i}\) sees her realized signal \(s_{i}\) and knows the precision levels \(p_{1},p_{2}\) of both players' signals.

Our main takeaway is that there is a separation in the benchmarks for achievable cumulative utilities between \(_{1}\) and \(_{2}\) in this setting, when \(_{1}\) and \(_{2}\) employ algorithms that form a PNE of the meta-game. This is not surprising in the one-shot setting. But in the repeated setting, even with infinite rounds for \(_{2}\) to learn the game from feedback gained throughout the interaction, we show that there is still a separation in achievable benchmarks.

This separation could be due to two factors: 1) \(_{2}\)'s inability to learn the game based on repeated interactions, and 2) \(_{2}\)'s failure to achieve the benchmark utility despite successful learning. We discuss this in more detail in Section 3.2 and Appendix A. We show that if \(_{2}\) was able to learn the game based on external signals, then \(_{2}\) would be able to achieve the benchmark. This highlights a fundamental difference between learning based on interactions with the other player and learning independently without relying on the other player. In the latter scenario, a utility benchmark is always achievable, whereas in the former, it is sometimes unattainable.

The benchmark that we will show separates \(_{1}\) from \(_{2}\) is the average Stackelberg value with the player of interest as leader. Recall that StackVal\({}_{i}()=_{G}[_{i}(G)]\) for each \(_{i}\). We will demonstrate the separation by showing that \(_{1}\) is always able to achieve this benchmark through a PNE of the meta-game, for all \(\), but there exists some distribution \(\) in which no PNE of the meta-game yields \(_{2}\) her counterpart benchmark.

We will first state the theorems and provide proof sketches later. Our first theorem (Theorem 3.1) asserts that \(_{1}\)_can_ achieve the benchmark StackVal\({}_{1}()\) by explicitly constructing a PNE pair of algorithms \((_{1},_{2})\) that grants \(_{1}\) this utility in the asymptotic regime. In the proof of this theorem, we provide the rate of convergence to this utility (Remark B.1).

**Theorem 3.1** (Benchmark achievable by \(_{1}\) for all \(\)).: _For every game family \(\) and every distribution \(()\) supported on it, there exists an algorithm pair \((_{1},_{2})\) such that \((_{1},_{2})\) is a PNE of the meta-game, and \( G,_{1}^{T}(_{1},_{2};G) _{1}(G)-_{T}(1)\)._

_That is, for every realized game \(G\), the expected average utility of \(_{1}\) over \(T\) rounds tends to StackVal\({}_{1}(G)\) as \(T\). The expectation is over the trajectories -- sequence of player strategies, and resulting utilities induced by the algorithms \(_{1},_{2}\) and \(G\)._

The next theorem completes the separation argument by constructing a specific game distribution where no PNE of the meta-game allows \(_{2}\) to asymptotically achieve the benchmark StackVal\({}_{2}()\).

**Theorem 3.2** (Benchmark unachievable by \(_{2}\) for some \(\)).: _For all thresholds \(p^{}[0,1)\), there exists a game family \(\) and a distribution \(()\), s.t., \( p_{2} p^{}\), all PNE \((_{1},_{2})\) of the meta-game where \(_{2}\)'s signal is of precision \(p_{2}\) must suffer \(_{G}_{2}^{T}(_{1},_{2};G)_{2}()-_{T}(1)\)._

_This implies that there is a game \(G\) such that when \(G\) is realized, \(_{2}\)'s expected average utility over \(T\) rounds remains strictly bounded below StackVal\({}_{2}(G)\) even as \(T\)._

Theorems 3.1 and 3.2 show that there is a separation in achievable benchmark whenever the less-informed player is at any informational disadvantage, however small, compared to the fully-informed player. \(_{2}\)'s signal could be arbitrarily close to being fully informative (i.e., \(p_{2}\) is arbitrarily close to 1), but there is still a barrier between what \(_{2}\) can achieve compared to \(_{1}\), when \(_{1}\) has full knowledge.

### Proof sketches of main theorems

Now we present proof sketches for the two theorems, defering the full proofs to the appendices.

Proof sketch of Theorem 3.1.: Our proof puts together results from previous work [17; 28]. We present the proofs of these results for completion. In this proof sketch, we will prove the theorem when every game \(G\) in the support of \(\) is such that \(_{2}\) has a unique best-response \(y(^{};G)\) to \(_{1}\)'s optimal Stackelberg strategy \(^{}(G)\). The full proof is in Appendix B.1.

Let \(_{1}\) be the algorithm that plays \(_{1}\)'s optimal Stackelberg strategy of the realized game \(G\) (i.e., \(^{}(G)\)) at every round. Since \(_{1}\) has access to a signal that fully reveals the realized game \(G\), \(_{1}\) can compute \(^{}(G)\) and employ this strategy. Let \(_{2}\) be a no-swap-regret algorithm in the bandit-feedback setting (the algorithm is only based on the utilities received in each round). Such algorithms exist  and are deployable by \(_{2}\) without any knowledge of the game played or \(_{1}\)'s strategies. Note that \(_{2}\) does not use \(_{2}\)'s signal \(s_{2}\). Therefore our analysis holds for all levels of precision of \(s_{2}\).

First, let us analyze the expected utility of \(_{2}\) due to \((_{1},_{2})\). The generated trajectories when the game \(G\) is realized are of the form \((^{}(G),^{})_{t=1}^{}\). Since \(_{2}\) is a no-swap-regret algorithm, the regret of this trajectory up to round \(T\) is sub-linear in \(T(o(T))\).

Since we assumed that \(_{2}(^{};G)\) is unique, any round where \(_{2}\) is not employing this unique best-response \((y(^{};G))\) causes \(_{2}\) to incur regret. The no-swap-regret property for \(_{2}\) essentially means that \(_{2}\)'s strategies in the trajectory \((^{t})_{t=1}^{}\) become close to \(y(^{};G)\). And as a result, \(_{1}\)'s utility per round gets close to \(U_{1}(^{}(G),y(^{};G);G)\) which is StackVal\({}_{1}(G)\).

More formally, \(_{1}\)'s cumulative utility over \(T\) rounds satisfies \(_{t[T]}U_{1}(^{}(G),^{t})_{1 }(G) T-c_{1}_{t[T]}\|^{t}-(^{}; G)\|_{1}\), where \((^{};G)\) is the one-hot vector encoding of \(y(^{};G)\) and \(c_{1}=_{y_{2}\{y(^{};G)\}}U_{1}( ^{}(G),y;G)\). We bound term \(_{t[T]}\|^{t}-(^{};G)\|_{1}\) using the no-swap-regret property. \(_{2}\)'s swap regret is at least \(_{t[T]}c_{2}\|^{t}-(^{};G)\|_{1}\), where \(c_{2}=U_{2}(^{}(G),y(^{};G))-_{y_{2}\{y(^{};G)\}}U_{2}(^{}(G),y)\) is the minimum difference of \(_{2}\)'s utility between playing the best response action \((^{};G)\) and any other action in \(_{2}\). Sub-linear swap regret therefore implies that \([_{t=1}^{T}\|^{t}-(^{}; G)\|_{1}] o(T)\) and thus \([_{t=1}^{T}U_{1}(^{}(G),^{t}) ]_{1}(G) T-o(T)\), i.e., \(_{1}\)'s expected average utility in \(T\) rounds is at least StackVal\({}_{1}(G)-o_{T}(1)\).

We have shown that the pair \((_{1},_{2})\) achieves \(_{1}\)'s benchmark utility. We now show that it is a PNE of the meta-game. Fixing \(_{1}\), the maximum utility \(_{2}\) can get is the utility achieved by playing \(y(^{};G)\), \( t\). \(_{2}\) does not necessarily know \(G\) to play \(y(^{};G)\) for all \(t[T]\), but we have shown that due to \(_{2}\) being a no-swap-regret algorithm, \(_{2}\) ends up playing strategies close to \(y(^{};G)\) asymptotically. The difference between \(_{2}\)'s cumulative utility between playing \(_{2}\) against \(_{1}\), versus playing per-round best response against \(_{1}\) is at most \(O(_{t[T]}\|^{t}-(^{} ;G)\|_{1})\) which is \(o(T)\) by the no-swap-regret property. So \(_{2}\) has vanishing incentive to deviate from \(_{2}\) in the meta-game.

Next fixing \(_{2}\) to be a no-swap-regret algorithm, previous work  caps \(_{1}\)'s achievable utility through any algorithm \(_{1}^{}\) (Deng et al. [17, Theorem 6]). These results show that for every \(_{1}^{}\), \(_{1}\)'s expected average utility induced by \((_{1}^{},_{2})\) in \(T\) rounds is at most StackVal\({}_{1}(G)+o_{T}(1)\). Since we have shown that \((_{1},_{2})\) yields at least StackVal\({}_{1}(G)-o_{T}(1)\) for \(_{1}\), there is vanishing incentive for \(_{1}\) to deviate.

In Appendix B.1, we extend this proof to the scenario with potential ties in \(_{2}\)'s best response, but under the assumption that \(_{2}\) has no weakly dominated action. Using regret rates of standard swap-regret algorithms, we also provide the rate of convergence to the Stackelberg benchmark. 

Proof sketch of Theorem 3.2.: To prove this theorem, we construct a family of two games \(G_{1}\) and \(G_{2}\) (shown in Figure 1) and let the prior distribution \(\) to be uniform over \(G_{1}\) and \(G_{2}\). Note that the maximum value of game parameters depends inversely on \(}{1+p^{}}\), where \(p^{}\) is the maximum precision of the signal received by \(_{2}\). In this construction, the utility functions in both games are identical for \(_{2}\) but different for \(_{1}\). This implies that \(_{2}\) cannot gain any additional knowledge about which game is realized from looking at her own utility function.

We first illustrate the high-level idea by considering a hypothetical situation where the trajectory always converges to the Stackelberg equilibrium led by \(_{2}\) for all \(G\). In other words, the trajectory converges to \((x(^{};G_{1}),^{}(G_{1}))\) when \(G_{1}\) is realized and \((x(^{};G_{2}),^{}(G_{2}))\) when \(G_{2}\) is realized. It is not hard to check that the Stackelberg equilibria turns out to be supported on different pure-strategy pairs: \((A,C)\) in \(G_{1}\) and \((B,D)\) in \(G_{2}\) (shaded cells in Figure 1). Because the Stackelberg strategies differ for \(G_{1}\) and \(G_{2}\), to converge to the correct equilibrium, \(_{2}\) must have gained full information about which game \(G\) is being played through repeated interactions with \(_{1}\). However, from \(_{1}\)'s perspective, the strategy pair \((A,C)\)--the Stackelberg equilibrium led by \(_{2}\) in \(G_{1}\)--ismore favorable than the other equilibrium \((B,D)\) in both \(G_{1}\) and \(G_{2}\). Therefore, instead of disclosing information about which \(G\) is realized, it would be more beneficial for \(_{1}\) to conceal this information and always behave as if \(G\) were \(G_{1}\). Therefore, any pair of algorithms that give rise to this hypothetical situation cannot be an equilibrium in the space of algorithms.

Our actual proof applies similar ideas to establish a stronger claim: not only is it impossible for \(_{2}\) to have the trajectory always converge to their Stackelberg equilibrium, but they cannot recover an average utility of StackVal\({}_{2}()\) through _any_ repeated interactions with \(_{1}\) that are specified by PNE algorithm pairs. To argue this, we will use the notion of _correlated strategy profiles (CSP)_ as a succinct way of analyzing the expected utility of each player. For a distribution \(^{T}\) over trajectories of length \(T\), the CSP induced by \(^{T}\), denoted as \(_{^{T}}\), is a correlated distribution in \((_{1}_{2})\) which is taken as the empirical average of the mixed-strategy profiles in each time step, i.e., \(_{^{T}}_{(_{i},_{i})_{t[]^{T}}}[(1/T)_{t[T]}_{t} _{t}]\). Since CSPs serve as a sufficient statistics of both players' expected utility (which is a direct consequence of the linearity of utilities), working with them significantly reduces the dimension of the problem.

**Special case: full information asymmetry.** We start with the full information asymmetry setting, i.e., \(p_{1}=1\) and \(p_{2}=0\). For the sake of contradiction, assume that a pair of equilibrium algorithms \((_{1},_{2})\) can let \(_{2}\) achieve the benchmark StackVal\({}_{2}()=3/2\). With the CSPs introduced above, we can rewrite \(_{2}\)'s average expected utility as \(}_{(x,y)_{1}}U_{2}(x,y;G_{ 1})+}_{(x,y)_{2}}U_{2}(x, y;G_{2})\), where we have used \(_{1}\) and \(_{2}\) to denote the CSPs induced by the distribution over trajectories generated by \(^{T}(_{1},_{2};G_{1})\) and \(^{T}(_{1},_{2};G_{2})\), respectively.

Similar to the hypothetical situation sketched above, we want to argue that there is incentive for \(_{1}\) to deviate to an algorithm \(_{1}^{}\) that always behaves according to \(_{1}(G_{1})\) even when the actual game is \(G_{2}\). In other words, we aim to show that \(_{1}\)'s expected utility in \(G_{2}\) strictly increases after replacing the induced CSP from \(_{2}\) to \(_{1}\), i.e., \(}_{_{1}}U_{1}(;G_{2})> }_{_{2}}U_{1}(;G_{2})\). Note that for \(_{1}\)'s utility in \(G_{2}\), cells involving action \(C\) all have utility close to \(1\), whereas those involving action \(D\) all have utility close to \(0\). Therefore, it suffices to show that cells involving action \(D\) take up a significant probability mass in \(_{2}\) but very little in \(_{1}\). We break these into the following three claims and use the equilibrium condition to establish them in Appendix B.3.

* **Claim 1.**\(_{1}(B,D)\) is very small, otherwise \(_{1}\) would deviate to always playing action \(A\).
* **Claim 2.**\(_{1}(A,D)\) is very small, otherwise \(_{2}\) would deviate to always playing action \(C\).
* **Claim 3.**\(_{2}(B,D)\) is very large, otherwise \(_{2}\) cannot achieve benchmark StackVal\({}_{2}()\).

Towards partial information asymmetry.In the remainder of this sketch, we discuss the extension of the above approach to the partial asymmetry setting where \(p_{1}=1\) and \(0 p_{2} p^{}<1\). The fact that \(_{2}\)'s signal is partially informative introduces extra challenge to our analysis, since \(_{2}\)'s belief about the true game depends not only on \(_{1}\)'s behavior during the interaction, but also on the information carried by the external signal \(s_{2}\). As a result, if \(_{1}\) deviates to acting according to \(G_{1}\) when the actual game is \(G_{2}\), it does not trigger the expected CSP when the realized game is \(G_{1}\), but instead causes a "distorted" posterior since the distribution of \(s_{2}_{p_{2}}(|G_{2})\) does not change.

To illustrate this, consider the four different CSPs introduced by all combinations of the realized signals received by both players. For \((i,j)\{1,2\}^{2}\), let \(_{ij}\) to denote the CSP induced by \(^{T}(_{1},_{2};s_{1},s_{2},G=s_{1})\) when \(s_{1}=G_{i}\) and \(s_{2}=G_{j}\) (we have set \(G=s_{1}\) because \(s_{1}\) perfectly reveals \(G\)). When the realized game is \(G_{2}\), \(_{1}\)'s expected utility before deviation is given by

\[_{1}(_{1},_{2};G_{2})=}{2}} _{_{21}}U_{2}(;G_{2})+}{2}}_{_{22}}U_{2}(;G_{2}),\]

Figure 1: game matrices \(G_{1}\) and \(G_{2}\). \(_{1}\) is the row player and \(_{2}\) is the column player. The values in each cell are (\(_{1}\)’s utility, \(_{2}\)’s utility). Shaded cells represent the action profiles supported in the Stackelberg equilibria led by the column player. The parameter \(\) is defined as \(}{1+p^{}}(0,1]\), where \(p^{}\) is the precision threshold of \(p_{2}\).

because the probability of \(_{2}\) seeing signals \(s_{2}=G_{1}\) and \(s_{2}=G_{2}\) are \(}{2}\) and \(}{2}\), respectively. So, as for the expected utility after deviation, the coefficients \(}{2}\) and \(}{2}\) remain the same, but the first distribution \(_{21}\) becomes \(_{22}\) and the second distribution changes from \(_{22}\) to \(_{12}\):

\[_{1}(_{1}^{},_{2};G_{2})=}{2}}_{_{11}}U_{2}(;G_{2})+}{2}}_{_{12}}U_{2}(;G_{2}),\]

However, if the true game were \(G_{1}\), then \(_{11}\) and \(_{12}\) would be realized with swapped probability \(}{2}\) and \(}{2}\), not the ones appeared in \((_{1}^{},_{2};G_{2})\)! Hence, even if we can guarantee that action pairs \((A,D)\) and \((B,D)\) occur very infrequently when the true game is \(G=G_{1}\), they may only occur under \(_{12}\), whose frequency gets amplified by \(}{1-p_{2}}\) times when factoring into the utility after deviation \((_{1}^{},_{2};G_{2})\). Therefore, establishing the benefit of deviation requires a much smaller probability of \((A,D)\) and \((B,D)\) under the CSPs induced by \(G=G_{1}\). This is why we need the game parameters to inversely depend on \(=}{1+p^{*}}\), where \(p^{*}\) is an upper bound on \(p_{2}\). 

### Difference in learning through repeated interactions and learning independently

In this section, we provide an informal discussion on the reason behind \(_{2}\)'s failure to recover their Stackelberg value benchmark through repeated interactions, with a more formal treatment deferred to Appendix A. We argue that the failure is not due to the PNE of meta-game always preventing \(_{2}\) from "learning" the game, but rather because \(_{2}\) cannot apply her learned knowledge to recover her Stackelberg value in any equilibrium.

At each round, \(_{2}\) can form a posterior belief about the realized game \(G\) based on her observed feedback from the historical interactions and the initial signal \(s_{2}\) received. We say that \(_{2}\)_successfully learns_\(G\) if her posterior belief converges to the point distribution on \(G\) (formally in Definition A.2). Interestingly, using the same pair of PNE algorithms designed to show that \(_{1}\) can achieve their Stackelberg value, we can show that \(_{2}\)_is indeed_ able to successfully learn \(G\) through strategic interactions. This is because \(_{2}\)'s strategy converges to the best response \(y(^{*};G)\), which perfectly reveals \(G\) for some game families \(\). Thus, successful learning of \(G\) through repeated interactions _can_ happen in a PNE of a meta-game where \(_{2}()\) cannot be achieved (Observation A.4).

The problem preventing \(_{2}\) from achieving \(_{2}()\) is not an insufficient rate or accuracy of learning, but rather the fact that learning and acting on this learned knowledge are intertwined. In fact, if \(_{2}\)'s learning was independent of the repeated interaction, i.e., when \(_{2}\) has access to external signals that become more accurate over time, she _can_ achieve \(_{2}()\) (Proposition A.5).

## 4 Interactions between two partially-informed players

In this section, we consider the setting where neither player is fully informed. That is, the precision of both player's signals (\(p_{1},p_{2}\)) are less than one. Even though there may be information asymmetry in the form of different precision levels of player signals, we show that there is no longer a clear separation between players through the average Stackelberg value benchmark.

At a high level, what distinguishes this setting from the previous setting (hence resulting in the lack of separation), is that the identity of the more-informed player can shift throughout the course of the repeated interaction. Due to the structure of \(\), more information about the realized game may be released to one player compared to the other. In contrast, when the more informed player starts with perfectly knowing the realized game, there is no possibility of her becoming less informed since there is no information beyond what she already knows.

**Example 4.1**.: _Consider \(\) to be the uniform distribution over the two game matrices defined in the figure below._

Figure 2: Example game matrices \(=\{G_{1},G_{2}\}\) revealing more information to \(_{2}\) compared to \(_{1}\). Here, \(_{1}\) is the row player and \(_{2}\) is the column player.

_Note that if \(_{2}\) chooses the pure strategy \(C\), then for any strategy of \(_{1}\), \(_{2}\)'s utility lies in the range \(\) if \(G_{1}\) is realized and in the range \(\) if \(G_{2}\) is realized. Since both ranges are non-intersecting, \(_{2}\) can deduce \(G\) exactly after a single round by choosing the pure strategy \(B\) in the first round._

_However, since the utilities of \(_{1}\) for all action profiles are the same in both \(G_{1},G_{2}\), \(_{1}\) gains no additional information about the realized game. So even if \(_{1}\) started off with a more informative signal, after a single round, \(_{2}\) becomes more informed and in fact perfectly informed._

To show that the average Stackelberg value benchmark does not separate the more- from the less-informed player, we will show that neither player can achieve the average Stackelberg value in all instances. Put another way, for every possible pair of player signals' precision, there is an instance such that this player cannot achieve the benchmark value at equilibrium.

**Proposition 4.2**.: _For every player signal precision values \(p_{1},p_{2}[0,1)\), for each \(i\{1,2\}\), there exists \(\) such that for every PNE \((_{1},_{2})\) of the meta-game, \(_{i}^{T}(_{1},_{2};)_{i}( )-_{T}(1)\)._

Proof.: The proof of this proposition reduces to the proof of Theorem 3.2. This is due to the following game-revealing property (similar to Example 4.1) of the construction \(\) used in the proof of Theorem 3.2 described by Figure 1. Since the range of the set of attainable utilities in \(G_{1},G_{2}\) when \(_{1}\) chooses action \(A\), has no intersection for \(_{1}\), regardless of \(_{2}\)'s strategy, \(_{1}\) can deduce the game exactly after a single round while \(_{2}\) gains no additional information after a single round.

After the first round, we are in the regime of a fully informed \(_{1}\) and a partially informed \(_{2}\) since \(p_{2}<1\). Theorem 3.2 already shows that in this regime, no equilibrium provides \(_{2}\) her average Stackelberg value benchmark. Using a distribution \(^{}\) that is the same as \(\) but with player utilities flipped proves the proposition for \(_{2}\). 

## 5 Discussion

In this paper, we study the effects of information asymmetry (codified in terms of signals about the game played) on the achievable benchmarks of two non-myopic players interacting repeatedly over \(T\) rounds. First, we showed that when \(_{1}\) is fully informed (i.e., knows \(G\)) while \(_{2}\) is not, then there is a separation between the more and the less informed player by way of each player's achievable benchmarks. Next, we showed that when neither player is fully informed (i.e., both \(p_{1},p_{2}<1\)) then, there is no longer a clear separation between players in terms of benchmarks.

There are several avenues for future research stemming from our work.

**Characterizations of algorithms that can be supported in an equilibrium.** We should gain a better understanding of what algorithms from natural classes can be in equilibrium. A useful step would be to characterize the necessary and/or sufficient conditions for an algorithm to be supported in the PNE of the meta-game. Our work and previous work provide sufficient conditions such as no-swap-regret algorithms and best-responding per round: an algorithm satisfying either condition can be supported in a PNE of every meta-game if at least one player is fully informed. Previous work also implies that no-regret is not sufficient for an algorithm to be part of a meta-game PNE (see Appendix C for more details). Finally, in the case where neither player is fully informed, it would be very useful to characterize the structure of the meta-game that causes a shift wrt the information advantage.

**Other models of how signals are generated.** Alleviating some of our modeling assumptions, one could ask how the results would change if nature was not assumed to be truthful with respect to the signal reporting but it may strategically modify the signals to achieve its own goals, such as maximizing social welfare. Taking this aspect into account, we can consider an information design setting where the nature designs a signaling scheme that shapes both agents' beliefs about the state and therefore their algorithms of choice. In addition, we have assumed that nature provides signals cost-free. This is a required and natural first step, but an interesting direction would be to understand what happens when the signals are costly and their accuracy is positively correlated with their cost.

**Computational aspects of meta-game equilibrium.** Moreover, it would be very interesting to see how the results about the effects of information asymmetry generalize in the case where the players are computationally bounded; note that our current setup provides information-theoretic results, but it could be computationally hard for players to communicate their algorithms to each other, or even verify that two algorithms are at equilibrium.

#### Acknowledgements

This work was supported in part by the National Science Foundation under grant CCF-2145898, by the Office of Naval Research under grant N00014-24-1-2159, a C3.AI Digital Transformation Institute grant, an Alfred P. Sloan fellowship, a Schmidt Science AI2050 fellowship, and an Amazon Research Award.