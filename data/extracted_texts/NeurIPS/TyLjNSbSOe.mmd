# An Efficient Doubly-Robust Test for the

Kernel Treatment Effect

 Diego Martinez-Taboada

Department of Statistics and Data Science

Carnegie Mellon University

Pittsburgh, PA 15213

diegomar@andrew.cmu.edu

&Aaditya Ramdas

Department of Statistics and Data Science

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

aramdas@stat.cmu.edu

&Edward H. Kennedy

Department of Statistics and Data Science

Carnegie Mellon University

Pittsburgh, PA 15213

edward@stat.cmu.edu

###### Abstract

The average treatment effect, which is the difference in expectation of the counterfactuals, is probably the most popular target effect in causal inference with binary treatments. However, treatments may have effects beyond the mean, for instance decreasing or increasing the variance. We propose a new kernel-based test for distributional effects of the treatment. It is, to the best of our knowledge, the first kernel-based, doubly-robust test with provably valid type-I error. Furthermore, our proposed algorithm is computationally efficient, avoiding the use of permutations.

## 1 Introduction

In the context of causal inference, potential outcomes (Rubin, 2005) are widely used to address counterfactual questions (e.g. what would have happened had some intervention been performed?). This framework considers

\[(X,A,Y),\]

where \(X\) and \(Y\) represent the covariates and outcome respectively, and \(A\{0,1\}\) is a binary treatment. Furthermore, the following three conditions are assumed:

1. (Consistency) \(Y=AY_{1}^{*}+(1-A)Y_{0}^{*}\) (where \(Y_{1}^{*},Y_{0}^{*}\) are the potential outcomes).
2. (No unmeasured confounding) \(Y_{0}^{*},Y_{1}^{*}\!\!\! A X\).
3. (Overlap) For some \(>0\), we have \(<(X):=_{A X}(A=1|X)<1-\) almost surely.

Such assumptions allow for identification of causal target parameters. For instance, one may be interested in the average variance-weighted treatment effects (Robins et al., 2008; Li et al., 2011), stochastic intervention effects (Munoz and Van Der Laan, 2012; Kennedy, 2019), or treatment effect bounds (Richardson et al., 2014; Luedtke et al., 2015). However, most of the literature focuses on estimation and inference of the average treatment effect (Imbens, 2004; Hernan and Robins, 2020), defined as the difference in expectation of the potential outcomes

\[=[Y_{1}^{*}-Y_{0}^{*}].\]Given \((X_{i},A_{i},Y_{i})_{i=1}^{N}(X,A,Y)\), there are three widespread estimators of \(\). First, the plug-in (PI) estimator:

\[_{}=_{i=1}^{N}\{_{1}(X_{i})- _{0}(X_{i})\},\]

where \(_{1}(X),_{0}(X)\) estimate \([Y_{1}^{*}|X],[Y_{0}^{*}|X]\). Second, the inverse propensity weighting (IPW) estimator:

\[_{}=_{i=1}^{N}\{A_{i}}{ {}(X_{i})}-(1-A_{i})}{1-(X_{i})}\},\]

where \((X)\) estimates the propensity scores \([A=1|X]\). Third, the so called Augmented Inverse Propensity Weighted (AIPW) estimator:

\[_{}=_{i=1}^{N}_{1}(X_ {i})-_{0}(X_{i})+(}{(X_{i})}-}{1-(X_{i})})(Y_{i}-_{A_{i}}(X_{i}) )}.\]

Under certain conditions (e.g., consistent nuisance estimation at \(n^{-1/4}\) rates), the asymptotic mean squared error of the AIPW estimator is smaller than that of the IPW and PI estimator, and minimax optimal in a local asymptotic sense (Kennedy, 2022), hence it has become increasingly popular in the last decade. The AIPW estimator is often referred to as the doubly-robust estimator. We highlight that double-robustness is an intriguing property of an estimator that makes use of two models, in which the estimator is consistent even if only one of the two models is well-specified and the other may be misspecified; we refer the reader to Kang and Schafer (2007) for a discussion on doubly-robust procedures.

However, the treatment might have effects beyond the mean, for instance in the variance or skewness of the potential outcome. The average treatment effect will prove insufficient in this case. Consequently, one may be interested in testing whether the treatment has _any_ effect in the distribution of the outcome. This question naturally arises in a variety of applications. For instance, one may want to check whether there is any difference between a brand-name drug and its generic counterpart, or understand whether a treatment simply shifts the distribution of the outcome (or, in turn, it also affects higher order moments).

In this work, we revisit the problem of testing the null hypothesis \(H_{0}:P_{Y_{i}^{*}}=P_{Y_{0}^{*}}\) against \(H_{1}:P_{Y_{1}^{*}} P_{Y_{0}^{*}}\). We propose a distributional treatment effect test based on kernel mean embeddings and the asymptotic behaviour of the AIPW estimator. Our contributions are three-fold:

* Up to our knowledge, we propose the first kernel-based distributional test to allow for doubly-robust estimators with provably valid type-I error.
* The proposed distributional treatment effect test is permutation-free, which makes it computationally efficient.
* We empirically test the power and size of the proposed test, showing the substantial benefits of the doubly robust approach.

## 2 Related work

Distributional treatment effects have been addressed from a variety of points of view. Abadie (2002) was one of the first works to propose to test distributional hypothesis attempting to estimate the counterfactual cumulative distribution function (cdf) of the outcome of the treated and untreated. Chernozhukov et al. (2013) proposed to regress the cdf after splitting the outcome in a grid. Further contributions followed with alike cdf-based approaches (Landmesser, 2016; Diaz, 2017).

Other approaches to the problem include focusing on the probability density function (pdf) instead of the cdf. Robins and Rotnitzky (2001) introduced a doubly robust kernel estimator for the counterfactual density, while Westling and Carone (2020) proposed to conduct density estimation under a monotone density assumption. Kim et al. (2018) and Kennedy et al. (2021) suggested to compute \(L^{p}\) distances between the pdf of the outcome distribution for the different counterfactuals. Conditional distributional treatment effects have also been addressed, with Shen (2019) proposing to estimate the cdf of the counterfactuals for each value to be conditioned on.

On the other hand, kernel methods have recently gained more and more attention in the context of causal inference. Kernel-based two-stage instrumental variable regression was proposed in Singh et al. (2019), while Singh et al. (2020) presented estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. Furthermore, Singh et al. (2021) conducted mediation analysis and dynamic treatment effect using kernel-based regressors as nuisance functions. Causal inference with treatment measurement error i.e. when the cause is corrupted by error was addressed in Zhu et al. (2022) using kernel mean embeddings to learn the latent characteristic function.

However, kernel mean embeddings for distributional representation were not suggested in the causal inference literature until Muandet et al. (2021) proposed to use an IPW estimator to estimate the average treatment effect on the embedding, which leads to a kernel-based distributional treatment effect test based on this embedding and the MMD. With a very similar motivation, Park et al. (2021) proposed a test for conditional distributional treatment effects based on kernel conditional mean embeddings. In a preprint, Fawkes et al. (2022) extended the work to AIPW estimators, however no theoretical guarantees regarding type-1 error control of the proposed tests were provided.

Finally, the kernel-based tests used in the aforementioned cases involve test statistics that are degenerate U-statistics under the null, hence obtaining theoretical p-values of the statistic is not possible. In turn, Kim and Ramdas (2023) proposed the idea of cross U-statistics, which is based on splitting the data for achieving a normal asymptotic distribution after studentization. Similarly, Shekhar et al. (2022) exploited sample splitting for proposing a permutation-free kernel two sample test.

## 3 Preliminaries

In this section, we introduce the concepts that the proposed test for distributional treatment effects is mainly based on: Maximum Mean Discrepancy (Gretton et al., 2012), Conditional Mean Embeddings (Song et al., 2009), Kernel Treatment Effects (Muandet et al., 2021), dimension-agnostic inference using cross U-statistic (Kim and Ramdas, 2023; Shekhar et al., 2022), and the normal asymptotic behaviour of the doubly robust estimator (Funk et al., 2011).

### Maximum Mean Discrepancy (MMD) and Conditional Mean Embeddings

Let \(\) be a non-empty set and let \(\) be a Hilbert space of functions \(f:\) with inner product \(,_{}\). A function \(k:\) is called a reproducing kernel of \(\) if (i) \(k(,x),x\), (ii) \( f,k(,x)_{}=f(x)\) for all \(x,f\). If \(\) has a reproducing kernel, then it is called a Reproducing Kernel Hilbert Space (RKHS).

Building on a reproducing kernel \(k\) and a set of probability measures \(\), the Kernel Mean Embedding (KME) maps distributions to elements in the corresponding Hilbert space as follows:

\[:,_{}:= k(,x)d(x).\]

If the kernel \(k\) is "_characteristic_" (which is the case for frequently used kernels such as the RBF or Matern kernels), then \(\) is injective. Conditional mean embeddings (Song et al., 2009) extend the concept of kernel mean embeddings to conditional distributions. Given two RKHS \(_{k_{x}},_{k_{y}}\), the conditional mean embedding operator is a Hilbert-Schimdt operator \(_{Y|X}:_{k_{x}}_{k_{y}}\) satisfying \(_{Y|X=x}=_{Y|X}k_{x}(,x)\), where \(_{Y|X}:=_{YX}_{XX}^{-1}\), \(_{YX}:=_{Y,X}[k_{y}(,Y) k_{x}(,X)]\) and \(_{XX}:=_{X,X}[k_{x}(,X) k_{x}(,X)]\). Given a dataset \(\{,\}\), a sample estimator may be defined as

\[}_{Y|X}=_{}^{T}(K_{}+  I)^{-1}_{},\] (1)

where \(_{}:=[k_{y}(,y_{1}),,k_{y}(,y_{n})]^{T}\), \(_{}:=[k_{x}(,x_{1}),,k_{x}(,x_{n})]^{T}\), \(K_{}:=_{}_{}^{T}\) denotes the Gram matrix, and \(\) is a regularization parameter.

Building on the KME, Gretton et al. (2012) introduced the kernel maximum mean discrepancy (MMD). Given two distributions \(\) and \(\) and a kernel \(k\), the MMD is defined as the largest difference in expectations over functions in the unit ball of the respective RKHS:

\[(,)=_{f}_{X }[f(X)]-_{X^{}}[f(X^{})]=||_{ }-_{}||_{}.\]It can be shown (Gretton et al., 2012) that \(^{2}(,)=||_{}-_{}||_{ }^{2}\). If \(k\) is characteristic, then \((,)=0\) if and only if \(=\). Given two samples drawn from \(\) and \(\), the MMD between the empirical distributions may be used to test the null hypothesis \(H_{0}:=\) against \(H_{1}:\). However, this statistic is a degenerate two-sample U-statistic under the null, thus one cannot analytically calculate the critical values. Consequently, a permutation-based resampling approach is widely used in practice (Gretton et al., 2012).

### Kernel Treatment Effect: A distributional kernel-based treatment effect test

Based on the MMD and the potential outcomes framework, Kernel Treatment Effects (KTE) were introduced in Muandet et al. (2021) for testing distributional treatment effects in experimental settings (i.e. with known propensity scores).

Let \((X_{i},A_{i},Y_{i})_{i=1}^{n}(X,A,Y)\) such that (i) (consistency) \(Y=AY_{1}^{*}-(1-A)Y_{0}^{*}\), (ii) no unmeasured confounding and overlap assumptions hold. The KTE considers the MMD between \(Y_{0}^{*}\) and \(Y_{1}^{*}\) to test \(H_{0}:_{Y_{0}^{*}}=_{Y_{1}^{*}}\) against \(H_{0}:_{Y_{0}^{*}}_{Y_{1}^{*}}\). They define

\[}^{2}=||_{Y_{1}^{*}}-_{Y_{0}^{*}}||^{2},\]

where

\[_{Y_{1}^{*}}:=_{i=1}^{n}k(,Y_{i})}{ (X_{i})},_{Y_{0}^{*}}:=_{i=1}^{n}) k(,Y_{i})}{1-(X_{i})}.\]

Alternatively, we may also define an unbiased version of it. Again, under the null, these are degenerate two-sample U-statistics. Hence, Muandet et al. (2021) proposed a permutation-based approach for thresholding.

The KTE considers the MMD between the mean embeddings of the counterfactual distributions. This is the cornerstone of the proposed distributional test, as will be exhibited in Section 4, which we extend in a doubly-robust manner to observational settings.

### Permutation-free inference using cross U-statistics

The previously mentioned permutation-based approach for obtaining the threshold for the MMD statistic (and hence the KTE statistic) comes with finite-sample validity (Gretton et al., 2012). The number of permutations \(B\) used to find the empirical p-values generally varies from 100 to 1000. Consequently, the computational cost of finding a suitable threshold for the statistic is at least \(O(Bn^{2})\). Such computational cost reduces the applicability of the approach, especially when time or computational resources are limited.

Driven by developing dimension-agnostic inference tools, Kim and Ramdas (2023) presented a permutation-free approach to test null hypotheses of the form \(H_{0}:=0\) against \(H_{1}: 0\), where \(\) is the mean embedding of a distribution \(\), based on the idea of sample splitting. If \(X_{1},...,X_{2n}\), the usual degenerate V-statistic considers

\[}=||_{2n}||^{2},\]

where \(_{2n}=_{i=1}^{2n}k(,X_{i})\) (or the similar unbiased version). Kim and Ramdas (2023) proposed to split the data in two and study

\[}=_{n}^{A},_{n}^{B},\]

where \(_{n}^{A}=_{i=1}^{n}k(,X_{i}),_{n}^{B}= _{j=n+1}^{2n}k(,X_{j})\). Denoting \(U_{i}= k(,X_{i}),_{n}^{B}\), we have that

\[}=_{i=1}^{n}U_{i}.\]

Under the null and some mild assumptions on the embeddings, Kim and Ramdas (2023, Theorem 4.2) proved that

\[}:=}{_{u}}N(0, 1),\]where \(=_{i=1}^{n}U_{i}\), \(_{u}^{2}=_{i=1}^{n}(U_{i}-)^{2}\). Consequently, \(}\) is the statistic considered and the null is rejected when \(}>z_{1-}\), where \(z_{1-}\) is the \((1-)\)-quantile of \(N(0,1)\). Such test avoids the need for computing the threshold so it reduces the computational cost by a factor of \(\), and it is minimax rate optimal in the \(L^{2}\) distance and hence its power cannot be improved beyond a constant factor (Kim and Ramdas, 2023).

The permutation-free nature of cross U-statistic is key in the proposed distributional test. It will allow us to circumvent the need for training regressors and propensity scores repeatedly, while preserving theoretical guarantees.

### Empirical mean asymptotic behavior of AIPW

The main property from AIPW estimators that will be exploited in our proposed distributional treatment effect is the asymptotic empirical mean behaviour of the estimator. We present sufficient conditions in the next theorem.

**Theorem 3.1**.: _Let \(f(x,a,y)=\{-\}\{y-_{a}(x)\}+_ {1}(x)-_{0}(x)\), so that \(=\{f(X,A,Y)\}\) is the average treatment effect. Suppose that_

* \(\) _is constructed from an independent sample or_ \(f\) _and_ \(\) _are contained in a Donsker class._
* \(||-f||=o_{}(1)\)_._

_Suppose also that (by clipping) \(([,1-])=1\). If \(||-||_{a}||_{a}-_{a}||=o_{}( {1}{})\), then it follows that_

\[_{}-=(_{n}-)f(X,A,Y)+o_{ }(}),\]

_so it is root-n consistent and and asymptotically normal._

Note that the IPW estimator can be seen as an AIPW with \(_{0}(X)=_{1}(X)=0\) almost surely. The IPW estimator is also asymptotically normal if \(\|-\|=o_{}(})\). In experimental settings \(\|-\|=0\), hence the root-n rate is achieved. Under certain conditions (e.g., consistent nuisance estimation at \(n^{-1/4}\) rates), the asymptotic variance of the AIPW estimator is minimized for \(_{1}=_{1}\), \(_{0}=_{0}\), thus the IPW estimator is generally dominated by the AIPW if \(_{1},_{0}\) are consistent.

The idea exhibited in Theorem 3.1 will allow for using cross U-statistics in estimated mean embeddings, rather than the actual embeddings. Nonetheless, Theorem 3.1 applies to finite-dimensional outcomes \(Y\). We state and prove the extension of Theorem 3.1 to Hilbert spaces in Appendix C, which will be needed to prove the main result of this work.

## 4 Main results

We are now ready to introduce the main result of the paper. Let \(Z(X,A,Y)\) be such that \(Y=AY_{1}^{*}+(1-A)Y_{0}^{*}\) and that both no unmeasured confounding and overlap assumptions hold. We denote the space of observations by \(=\). We are given \(Z_{i}(X_{i},A_{i},Y_{i})_{i=1}^{2n}(X,A,Y)\) and we wish to test \(H_{0}:P_{Y_{1}^{*}}=P_{Y_{0}^{*}}\) against \(H_{1}:P_{Y_{1}^{*}} P_{Y_{0}^{*}}\). Given characteristic kernel \(k\) i.e. \(k(y,)= k(,y),k(,)\) with induced RKHS \(_{k}\), we equivalently test \(H_{0}:[k(,Y_{1}^{*})-k(,Y_{0}^{*})]=0\).

Under consistency, no unmeasured confounding, and overlap, we have

\[[k(,Y_{1}^{*})-k(,Y_{0}^{*})]=[(Z)],\]

where

\[(z) =\{-\}\{k(,y)-_{a}( x)\}+_{1}(x)-_{0}(x),\] \[(x) =[A X=x],_{a}(x)=[k(,Y) A =a,X=x].\]Note the change in notation, from \(_{a}\) to \(_{a}\), to emphasize that such regression functions are now \(_{k}\)-valued. Thus, we can equivalently test for \(H_{0}:[(Z)]=0\). With this goal in mind, we denote \(_{1}=(X_{i},A_{i},Y_{i})_{i=1}^{n},_{2}=(X_{j},A_{j},Y_{j })_{j=n+1}^{2n}\) and define

\[T_{h}^{}:=f_{h}^{}}{S_{h}^{}}\] (2)

where

\[f_{h}^{}(Z_{i})=_{j=n+1}^{2n}^{(1)}(Z_{i}),^{(2)}(Z_{j}),\ i[n].\]

Above, \(^{(r)}(z)\) is the plug-in estimate of \((z)\) for \(r\{1,2\}\) using \(^{(r)}\) and \(_{a}^{(r)}\), which approximate \(\) and \(_{a}\) respectively. Further, \(_{h}^{}\) and \(S_{h}^{}\) denote the empirical mean and standard error of \(f_{h}^{}\):

\[_{h}^{}=_{i=1}^{n}f_{h}^{}(Z_ {i}), S_{h}^{}=_{i=1}^{n}(f_{h}^{}(Z_{ i})-_{h}^{})^{2}}.\]

The next theorem, which is the main result of the paper, establishes sufficient conditions for \(T_{h}^{}\) to present Gaussian asymptotic behavior. While the main idea relies on combining cross U-statistics and the asymptotic empirical mean-like behaviour of AIPW estimators, we highlight a number of technical challenges underpinning this result. The proof combines the central idea presented in Kim and Ramdas (2023) with a variety of techniques including causal inference results, functional data analysis, and kernel method concepts. Furthermore, additional work is needed to extend Theorem 3.1 to \(_{k}\)-valued outcomes. Donsker classes are only defined for finite dimensional outcomes; in the \(_{k}\)-valued scenario, we ought to refer to asymptotically ejectontinuous empirical processes (Park and Muandet, 2023) and Glivenko-Cantelli classes. We refer the reader to Appendix C for a presentation of such concepts, clarification of the norms used, and the proof of the theorem.

**Theorem 4.1**.: _Let \(k\) be a kernel that induces a separable RKHS and \(_{V_{}^{}},_{V_{1}^{}}\) be two distributions. Suppose that (i) \([(Z)]=0\), (ii) \([(Z_{1}),(Z_{2})^{2}]>0\), (iii) \([\|(Z)\|_{}^{4}]\) is finite. For \(r\{1,2\}\), suppose that (iv) \(^{(r)}\) is constructed independently from \(_{r}\) or (v) the empirical process of \(^{(r)}\) is asymptotically ejectontinuous at \(\) and \(\|^{(r)}\|_{}^{2}\) belongs to a Glivenko-Cantelli class. If it also holds that (vi) \(\|^{(r)}-\|=o_{}(1)\), (vii) \((^{(r)}[,1-])=1\), and_

\[\|^{(r)}-\|_{a}\|_{a}^{(r)}-_{a} \|=o_{}(})\] (3)

_for \(r\{1,2\}\), then it follows that_

\[T_{h}^{}}{{}}N(0,1).\]

We would like to highlight the mildness of the assumptions of Theorem 4.1. The separability of the RKHS is achieved for any continuous kernel on separable \(\)(Hein and Bousquet, 2004). Assumption (i) is always attained under the null hypothesis (it is precisely the null hypothesis). Assumption (ii) prevents \((Z)\) from being constant. In such degenerate case, \(S_{h}^{}=0\) thus \(T_{h}^{}\) is not even well-defined. Assumption (iii) is the more restrictive out of the first three assumptions, inherited from the use of Lyapunov's CLT in the proof. However, we note that this condition is immediately satisfied under frequently used kernels. For instance, under bounded kernels (for example the common Gaussian and Laplace kernels) such that \(\|k(,Y)\|_{} M\), we have that

\[\|_{a}(x)\|_{}=[k(,Y)|A=a,X=x]\|_{ }[\|k(,Y)\|_{}|A=a,X=x] M,\]

hence

\[\|(z)\|_{}^{-1}\|k(,Y)\|_{}+ ^{-1}(\|_{1}(x)\|_{},\|_{0}(x)\|_{ })+\|_{1}(x)\|_{}+\|_{0}(x)\|_{},\]

which is upper bounded by \(2(^{-1}+1)M\). Consequently, \([\|(Z)\|_{}^{4}]^{2}[2(^{- 1}+1)M]^{8}\).

Furthermore, conditions (iv), (vi), (vii) and (3) deal with the proper behaviour of the AIPW estimator; they are standard in the causal inference scenario. Condition (iv) is equivalent to two-fold cross-fitting i.e., training \(^{(r)}\) on only half of the data and evaluating such an estimator on the remaining half. Condition (v) replaces the Donsker class condition from the finite dimensional setting.

We emphasize the importance of double-robustness in the test; normality of the statistic is achieved due to the \(o_{}(1/)\) rate, which is possible in view of the doubly robust nature of the estimators. We also highlight the fact that IPW estimators of the form \(^{(r)}(z)=\{(x)}-(x)}\}k( ,y)\) can be embedded in the framework considering \(_{0},_{1}=0\). In fact, the doubly robust kernel mean embedding estimator may be viewed as an augmented version of the KTE (which is a kernelized IPW) using regression approaches to kernel mean embeddings (Singh et al., 2020), just as AIPW augments IPW with regression approaches. Furthermore, (3) is always attained when the propensity scores \(\) are known (i.e. experimental setting), given that \(\|^{(r)}-\|=0\).

Based on the normal asymptotic behaviour of \(T_{h}^{}\), we propose to test the null hypothesis \(H_{0}:P_{Y_{1}^{*}}=P_{Y_{0}^{*}}\) given the p-value \(p=1-(T_{h}^{})\), where \(\) is the cdf of a standard normal. For an \(\)-level test, the test rejects the null if \(p\). We consider a one-sided test, rather than studying the two-sided p-value \(1-(|T_{h}^{}|)\), given that positive values of \(T_{h}^{}\) are expected for \([(Z)] 0\). The next algorithm illustrates the full procedure of the test, which we call AIPW-xKTE (Augmented Inverse Propensity Weighted cross Kernel Treatment Effect).

```
1:input Data \(=(X_{i},A_{i},Y_{i})_{i=1}^{2n}\).
2:output The p-value of the test.
3:Choose kernel \(k\) and estimators \(_{a}^{(r)}\), \(^{(r)}\) for \(r\{1,2\}\).
4:Split data in two sets \(_{1}=(X_{i},A_{i},Y_{i})_{i=1}^{n},_{2}=(X_{i},A_{i},Y_{ i})_{i=n+1}^{2n}\).
5:If \(_{a}^{(r)}\), \(^{(r)}\) are such that condition (v) from Theorem 4.1 is attained, train them on \(\). Otherwise, train them on \(_{1-r}\).
6:Define \(^{(r)}(z)=\{^{(r)}(x)}-^{(r) }(x)}\}\{k(,y)-_{a}^{(r)}(x)\}+_{1}^{(r)}(x)-_{0}^{(r)}(x)\).
7:Define \(f_{h}^{}(Z_{i})=_{j=n+1}^{2n}^{(1)}(Z_ {i}),^{(2)}(Z_{j})\) for \(i=1,,n\).
8:Calculate \(T_{h}^{}:=f_{h}^{}}{S_{h}^{}}\), where \(_{h}^{}=_{i=1}^{n}f_{h}^{}(Z_{i})\) and \(S_{h}^{}=_{i=1}^{n}(f_{h}^{}(Z_{i})- _{h}^{})^{2}}\).
9:return p-value \(p=1-(T_{h}^{})\). ```

**Algorithm 1** AIPW-xKTE

Note that the proposed statistic is, at heart, a two sample test (with a nontrivial causal twist); in contrast to Shekhar et al. (2022), the two samples are not independent and are potentially confounded. Extensive literature focuses on designing estimator \(^{(r)}\), logistic regression being the most common choice. At this time, not so many choices exist for estimators \(_{a}^{(r)}\), given that it involves a regression task in a Hilbert space. Conditional mean embeddings are the most popular regressor, although other choices exist (Cevid et al., 2022).

Note that we have motivated the proposed procedure for testing distributional treatment effects with characteristic kernels. However, the actual null hypothesis being tested is \(H_{0}:[k(,Y_{1}^{*})]=[k(,Y_{0}^{*})]\). If the kernel chosen is not characteristic, the test would continue to be valid for \(H_{0}\), although it would not be valid to test equality between \(_{Y_{0}^{*}}\) and \(_{Y_{1}^{*}}\). For instance, AIPW-xKTE with a linear kernel could be used to test equality in means of counterfactuals.

Furthermore, the proposed test is permutation-free, as the statistic \(T_{h}^{}\) ought to be computed only once. This permutation-free nature is crucial, as it avoids the repeated estimation of \(^{(r)},_{a}^{(r)}\). For instance, conditional mean embeddings involve the inversion of a matrix, which scales at least at \(O(n^{})\), with practical values being \(=2.87\) by Strassen's algorithm (Strassen et al., 1969). Calculating the conditional mean embedding for every permutation would imply \(O(Bn^{})\), where \(B\) is the number of permutations. Furthermore, regressors for mean embeddings of different nature might involve a higher complexity, hence avoiding permutations becomes even more important in the approach.

If the actual embedding \(\) was known, the power of AIPW-xKTE could not be improved beyond a constant factor (by minimax optimality of cross U-statistics in \(L^{2}\) distance). Further, every procedure will suffer from the error in estimation of \(\). This means that we are potentially incurring in a loss of power by avoiding a permutation-based approach, however such a loss is controlled by a small factor. Nonetheless, this potential loss is inherited from splitting the data in our estimator (only half of the data is used on each side of the inner product). We highlight that sample splitting is needed when using flexible doubly-robust estimators, hence we expect no loss in power compared to other potential doubly-robust approaches in that case.

## 5 Experiments

In this section, we explore the empirical calibration and power of the proposed test AIPW-xKTE. For this, we assume that we observe \((x_{i},a_{i},y_{i})_{i=1}^{n}(X,A,Y)\) and that (causal inference assumptions) consistency, no unmeasured confounding, and overlap hold. Both synthetic data and real data are evaluated. All the tests are considered at a 0.05 level. For an exhaustive description of the simulations and outcomes, including additional experiments, we direct the reader to Appendix B.

**Synthetic data.** All data (covariates, treatments and responses) are artificially generated. We define four scenarios:

* Scenario I: There is no treatment effect; thus, \(_{Y_{0}^{*}}=_{Y_{1}^{*}}\).
* Scenario II: There exists a treatment effect that only affects the means of \(_{Y_{0}^{*}},_{Y_{1}^{*}}\).
* Scenario III and Scenario IV: There exists a treatment effect that does not affect the means but only affects the higher moments of \(_{Y_{0}^{*}}\) and \(_{Y_{1}^{*}}\), differently for each scenario.

For all four scenarios, we consider the usual observational study setting, where the propensity scores \((X)\) are treated as unknown and hence they must be estimated. We define the proposed AIPW-xKTE test with the mean embedding regressions fitted as conditional mean embeddings and the propensity scores estimated by logistic regression.

We first study the empirical calibration of AIPW-xKTE and the Gaussian behaviour of \(T_{h}^{}\) under the null. Figure 1 exhibits the performance of AIPW-xKTE in Scenario I. Both a standard normal behaviour and proper calibration are empirically attained in the simulations.

Due to the fact that the KTE (Muandet et al., 2021) may not be used in the observational setting, where the propensity scores are not known, there is no natural benchmark for the proposed test. In particular, we were unable to control the type-1 error of the test presented in Fawkes et al. (2022), and hence omitted from our simulations. Consequently, we compare the power of the proposed AIPW-xKTE and IPW-xKTE with respect to three methods that are widely used while conducting inference on the average treatment effect: Causal Forests (Wager and Athey, 2018), Bayesian Additive Regression Trees (BART) (Hahn et al., 2020), and a linear regression based AIPW estimator (Baseline-AIPW).

Figure 1: Illustration of 500 simulations of the AIPW-xKTE under the null: (A) Histogram of AIPW-xKTE alongside the pdf of a standard normal for \(n=500\), (B) Normal Q-Q plot of AIPW-xKTE for \(n=500\), (C) Empirical size of AIPW-xKTE against different sample sizes. The figures show the Gaussian behaviour of the statistic under the null, which leads to a well calibrated test.

Figure 2 exhibits the performance of such tests in Scenario II, Scenario III, and Scenario IV. The three methods dominate AIPW-xKTE in Scenario II, where there exists a mean shift in counterfactuals. However, and as expected, such methods show no power if the distributions differ but have equal means. In contrast, AIPW-xKTE detects distributional changes beyond the mean, exhibiting power in all scenarios.

_Remark:_ While this work focuses on the observational setting, where double robustness is crucial, we highlight that the proposed AIPW-xKTE test may also be used in experiments (where propensity scores are known) for computational gains. The proposed test avoids permutations, which makes it more computationally efficient than the KTE (Muandet et al., 2021). We refer the reader to Appendix A for a comparison between the proposed AIPW-xKTE and the KTE.

**Real data.** We use data obtained from the Infant Health and Development Program (IHDP) and compiled by Hill (2011), in which the covariates come from a randomized experiment studying the effects of specialist home visits on cognitive test scores. This data has seen extensive use in causal inference (Johansson et al., 2016; Louizos et al., 2017; Shalit et al., 2017). We work with 18 variables of the covariate set and unknown propensity scores. We highlight that the propensity score model is likely misspecified in this real life scenario.

We consider six scenarios with the IHDP data. For Scenarios I, II, III and IV, we generate the response variables similarly to the previous experiments. In Scenario V, we take the IQ test (Stanford Binet) score measured at the end of the intervention (age 3) as our response variable. For Scenario VI, we calculate the average treatment effect in Scenario V using Causal Forests (obtaining 0.003 i.e. a positive shift) and subtract it from the IQ test score of those who are treated, thus obtaining a distribution of the IQ test scores with zero average treatment effect.

For each of the scenarios, we consider the tests AIPW-xKTE, Causal Forests, BART and Baseline-AIPW on 500 bootstrapped subsets to estimate the rejection rates. All of them showed expected levels of rejection under the null i.e. Scenario I. The results for the remaining scenarios can be found in Table 1. We note that the performance of the tests on Scenarios I, II, III, and IV is similar to the analogous scenarios on synthetic data. While AIPW-xKTE exhibits a loss in power with respect to

   Test &  \\   & II & III & IV & V & VI \\  AIPW-xKTE & 0.44 \(\) 0.05 & 0.34 \(\) 0.05 & 0.53 \(\) 0.05 & 0.99 \(\) 0.01 & 0.03 \(\) 0.02 \\ Baseline-AIPW & 0.95 \(\) 0.02 & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 & 1.00 \(\) 0.00 & 0.00 \(\) 0.00 \\ BART & 0.77 \(\) 0.04 & 0.15 \(\) 0.04 & 0.24 \(\) 0.04 & 0.10 \(\) 0.03 & 0.04 \(\) 0.02 \\ CausalForest & 1.00 \(\) 0.00 & 0.04 \(\) 0.02 & 0.05 \(\) 0.02 & 1.00 \(\) 0.00 & 0.00 \(\) 0.00 \\   

Table 1: True positive rates (\(\) std) for the different scenarios and tests using the IHDP data. While AIPW-xKTE shows less power in Scenario II, it outperforms its competitors in Scenarios III and IV.

Figure 2: True positive rates of 500 simulations of the tests in Scenarios II, III, and IV. AIPW-xKTE shows notable true positive rates in every scenario, unlike competitors.

the other tests when the average treatment effect is non zero, it detects distributional changes beyond the mean.

Besides BART, all of the tests reject the null in almost every simulation of Scenario V. As expected, Causal Forest, BART and Baseline-AIPW barely reject the null in Scenario VI, which considers the data of Scenario V with zero average treatment effect. Interestingly, the proposed distributional test has a rejection rate below 0.05, which supports the fact that specialist home visits have no effect on the distribution of cognitive test scores beyond an increase in the mean.

## 6 Conclusion and future work

We have developed a computationally efficient kernel-based test for distributional treatment effects. It is, to our knowledge, the first kernel-based test for distributional treatment effects that allows for a doubly-robust approach with provably valid type-I error. Furthermore, it does not suffer from the computational costs inherent to permutation-based tests. The proposed test empirically proves valid in the observational setting, where its predecessor KTE may not be used: the test is well calibrated and shows power in a variety of scenarios. Procedures designed to test for average treatment effects only outperform the proposed test if there is a mean shift between counterfactuals.

There are several possible avenues for future work. We highlighted that our procedure holds if consistency, no unmeasured confounding, and overlap hold. However, this may often not be the case in observational studies. Generalizing the work to other causal inference frameworks, for example by considering instrumental variables, would be of interest. Exploring the extension of this work to test for conditional treatment effects could also be a natural direction to follow. Lastly, we expect state-of-the-art regressors for kernel mean embeddings, such as distributional random forests (Cevid et al., 2022), to find outstanding use in our estimator. We envisage that this may motivate the development of flexible mean embedding estimators designed for complex data, such as image or text.