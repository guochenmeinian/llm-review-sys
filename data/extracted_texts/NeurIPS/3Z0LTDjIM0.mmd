# Faster Local Solvers for Graph Diffusion Equations

Jiahe Bai \({}^{1}\)  Baojian Zhou \({}^{1,2}\)1  Deqing Yang \({}^{1,2}\)  Yanghua Xiao \({}^{2}\)

\({}^{1}\) the School of Data Science, Fudan University,

\({}^{2}\) Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

jhbai20,bjzhou,yangdeqing,shawyh@fudan.edu.cn

###### Abstract

Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing _local solvers_ approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the _participation ratio_, this paper introduces a novel framework for approximately solving GDEs using a _local diffusion process_. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient _local message-passing_ mechanisms for GNNs.

## 1 Introduction

Graph diffusion equations (GDEs), such as Personalized PageRank (PPR) , Katz centrality (Katz) , Heat kernel (HK) , and Inverse PageRank (IPR) , are fundamental tools for modeling graph data. These score vectors for nodes in a graph capture various aspects of their importance or influence. They have been successfully applied to many graph learning tasks including local clustering , detecting communities , semi-supervised learning , node embeddings , training graph neural networks (GNNs) , and many other applications . Specifically, given a propagation matrix \(\) associated with an undirected graph \((,)\), a general graph diffusion equation is defined as

\[_{k=0}^{}c_{k}^{k},\] (1)

where \(\) is the diffusion vector computed from a source vector \(\), and the sequence of coefficients \(c_{k}\) satisfies \(c_{k} 0\). Equation (1) represents a system of linear, constant-coefficient ordinary differential equation, \(}(t)=(t)\), with an initial condition \((0)\) and \(t 0\). Standard solvers  for computing \(\) require access to matrix-vector product operations \(\), typically involving \((m)\) operations, where \(m\) is the total number of edges in \(\). This could be time-consuming when dealing with large-scale graphs .

A key property of \(\) is the high localization of its entry magnitudes, which reside in a small portion of \(\). Figure 1 demonstrates this localization property of \(\) on PPR, Katz, and HK. Leveraging this locality property allows for more efficient approximation using _local iterative solvers_, which heuristically [2; 7; 49] or their variants [4; 12] are known to be closely related to Gauss-Seidel [49; 18]. However, current local push-based methods are fundamentally sequential iterative solvers and focused on specific types such as PPR or HK. These disadvantages limit their applicability to modern GPU architectures and their generalizability.

By leveraging the locality of \(\), we propose, for the first time, a general local iterative framework for solving GDEs using a _local diffusion process_. A novel component of our framework is to model _local diffusion_ as a _locally evolving set process_ inspired by its stochastic counterpart . We use this framework to localize commonly used standard solvers, demonstrating faster local methods for approximating \(\). For example, the local gradient descent is simple, provably sublinear, and highly parallelizable. It can be accelerated further by using local momentum. **Our contributions are**

* By demonstrating that popular diffusion vectors, such as PPR, Katz, and HK, have strong localization properties using the participation ratio, we propose a novel graph diffusion framework via a _local diffusion process_ for efficiently approximating GDEs. This framework effectively tracks most energy during diffusion while maintaining local computation.
* To demonstrate the power of our proposed framework, we prove that APPR , a widely used local push-based algorithm, can be treated as a special case. We provide better diffusion-based bounds \((}(_{t})/( {}_{t}))\) where \(}(_{t})/_{t}\) is a lower bound of \(1/\). This bound is effective for both PPR and Katz and is empirically smaller than \((1/())\), previously well-known for APPR.
* We design simple and fast local methods based on standard gradient descent for APPR and Katz, which admit runtime bounds of \((}(_{t})/(_{ t}),1/())\) for both cases. These methods are GPU-friendly, and we demonstrate that this iterative solution is significantly faster on GPU architecture compared to APPR. When the propagation matrix \(\) is symmetric, we show that this local method can be accelerated further using the local Chebyshev method for PPR and Katz.
* Experimental results on GDE approximation of PPR, HK, and Katz demonstrate that these local solvers significantly accelerate their standard counterparts. We further show that they can be naturally adopted to approximate dynamic diffusion vectors. Our experiments indicate that these local methods for training are twice as fast as standard PPR-based GNNs. These results may suggest a novel _local message-passing_ approach to train commonly used GNNs. _All proofs, detailed experimental settings, and related works are postponed to the appendix. Our code is publicly available at https://github.com/JiaheBai/Faster-Local-Solver-for-GDEs._

## 2 GDEs, Localization, and Existing Local Solvers

**Notations.** We consider an undirected graph \((,)\) where \(=\{1,2,,n\}\) is the set of nodes, and \(\) is the edge set with \(||=m\). The degree matrix is \(=(d_{1},d_{2},,d_{n})\), and \(\) is the adjacency matrix of \(\). The _volume_ of subset nodes \(\) is \(()=_{v}d_{v}\). The set of _neighbors_ of node \(v\) is denoted by \((v)\). The standard basis for node \(s\) is \(_{s}\). The support of \(\) is defined as \(()=\{u:x_{u} 0\}\).

  Equ. & \(\) & \(c_{k}\) & \(\) \\  PPR  & \(^{-1}\) & \((1-)^{k}\) & \(_{s}\) \\  Katz  & \(\) & \(^{k}\) & \(_{s}\) \\  HK  & \(^{-1}\) & \(e^{-}^{k}/k!\) & \(_{s}\) \\  IPR  & \(^{-1}\) & \(}{(^{k}+^{10})^{2}}\) & \(_{s}\) \\  APPNP  & \(^{-}^{-}\) & \((1-)^{k}\) & \(\) \\  

Table 1: Example GDEs with their corresponding propagation matrix \(\), coefficients \(c_{k}\), and source \(\).

Figure 1: The maximal participation ratio \(p()\)=\((_{i=1}^{n}|f_{i}|^{2})^{2}/(n_{i=1}^{n}|f_{i}|^{4})\) of example diffusion vectors \(\) over 18 graphs, ordered from small (_cora_) to large (_ogbn-papers100M_). The ratio \(p()\) is normalized by the number of nodes \(n\).

Many graph learning tools can be represented as diffusion vectors. Table 1 presents examples widely used as graph learning tools. The coefficient \(c_{k}\) usually exhibits exponential decay, so most of the energy of \(\) is related to only the first few \(c_{k}\). We revisit these GDEs and existing local solvers.

**Personalized PageRank.** Given the weight decaying strategy \(c_{k}=(1-)^{k}\) and a predefined damping factor \((0,1)\) with \(=^{-1}\), the analytic solution for PPR is

\[_{}=(-(1-)^{-1})^{-1}_{s}.\] (2)

These vectors can be used to find a local graph cut or train GNNs . The well-known approximate PPR (APPR) method  locally updates the estimate-residual pair \((,)\) as follows

\[:+_{u} {e}_{u},-_{u}_{u}+(1-) _{u}^{-1}_{u}.\]

Here, each _active_ node \(u\) has a large residual \(r_{u} d_{u}\) with initial values \( 0\) and \(_{s}\). The runtime bound for reaching \(r_{u}< d_{u}\) for all nodes is graph-independent and is \((1/())\), leveraging the monotonicity property. Previous analyses  suggest that APPR is a localized version of the Gauss-Seidel (GS) iteration and thus has fundamental sequential limitations.

**Katz centrality.** Given the weight \(c_{k}=^{k+1}\) with \((0,1/\|\|_{2})\) and a different propagation matrix \(=\). The solution for Katz centrality can then be rewritten as:

\[_{}=(-)^{-1}_{s}.\] (3)

The goal is to approximate the Katz centrality \(((-)^{-1}-)_{s}\). Similar to APPR, a local solver  for Katz centrality, denoted as AKatz, can be designed as follows

\[:+r_{u}_{u}, -r_{u}_{u}+_{u}_{u}.\]

AKatz is a coordinate descent method . Under the nonnegativity assumption of \(\) (i.e., \( 1/d_{}\)), a convergence bound for the residual \(\|^{t+1}\|_{1}\) is provided.

**Heat Kernel.** The heat kernel (HK)  is useful for finding smaller but more precise local graph cuts. It uses a different weight decaying \(c_{k}=^{k}e^{-}/k!\) and the vector is then defined as

\[_{}=\{-(-)\}_{s},\]

where \(\) is the temperature parameter of the HK equation and \(=^{-1}\). Unlike the previous two, this equation does not admit an analytic solution. Following the technique developed in , given an initial vector \(\) and temperature \(\), the HK vector can be approximated using the first \(N\) terms of the Taylor polynomial approximation: define \(_{N}=_{k=0}^{N}_{k}\) with \(_{0}=_{s}\) and \(_{k+1}=_{k}/k\) for \(k=0,,N\). It is known that \(\|_{}-_{N}\|_{1} 1/(N!N)\). This is equivalent to solving the linear system \((_{N+1}_{n}-_{N+1})= _{1}_{s}\), where \(\) denotes the Kronecker product. Then, the push-based method, namely AHK, has the following updates

\[:+r_{u}(_{k}_{u}),-r_{u}(_{N\!+\!1}_{n}- {S}_{N\!+\!1})(_{k}_{u}).\]

There are many other types of GDEs, such as Inverse PageRank , which generalize PPR. Many GNN propagation layers, such as SGC  and APPNP , can be formulated as GDEs. For all these \(\), we use the _participation ratio_ to measure its localization ability, defined as

\[p()=_{i=1}^{n}|f_{i}|^{2}^{2}n_{ i=1}^{n}|f_{i}|^{4}.\]

When the entries in \(\) are uniformly distributed, such that \(f_{i}(1/n)\), then \(p()=(1)\). However, in the extremely sparse case where \(\) is a standard basis vector, \(p()=1/n\) indicates the sparsity effect. Figure 1 illustrates the participation ratios, showing that almost all vectors have ratios below \(0.01\). Additionally, larger graphs tend to have smaller participation ratios.

APPR, AKatz, and AHK all have fundamental limitations due to their reliance on sequential Gauss-Seidel-style updates, which limit their parallelization ability. In the following sections, we will develop faster local methods using the local diffusion framework for solving PPR and Katz.

Faster Solvers via Local Diffusion Process

We introduce a local diffusion process framework, which allows standard iterative solvers to be effectively localized. By incorporating this framework, we show that the computation of PPR and Katz defined in (2) and (3) can be locally approximated sequentially or in parallel on GPUs.

### Local diffusion process

To compute \(_{}\) and \(_{}\), it is equivalent to solving the linear systems, which can be written as

\[=,\] (4)

where \(\) is a (symmetric) positive definite matrix with eigenvalues bounded by \(\) and \(L\), i.e., \(() L\). For PPR, we solve \((-(1-)^{-1})=_{s}\), which is equivalent to solving \((-(1-)^{-1/2}^{-1/2})^{-1/2}= ^{-1/2}_{s}\). For Katz, we compute \((-)=_{s}\). The vector \(\) is sparse, with \(() n\). We define _local diffusion process_, a locally evolving set procedure inspired by its stochastic counterpart  as follows

**Definition 3.1** (Local diffusion process).: Given an input graph \(\), a source distribution \(\), precision tolerance \(\), and a local iterative method \(\) with parameter \(\) for solving (4), the local diffusion process is defined as a process of updates \(\{(^{(t)},^{(t)},_{t})\}_{0 t T}\). Specifically, it follows the dynamic system

\[(^{(t+1)},^{(t+1)},_{t+1})=(^ {(t)},^{(t)},_{t};,,,_{ }), 0 t T.\] (5)

where \(\) is a mapping via some iterative solver \(_{}\). For all three diffusion processes (PPR, Katz, and HK), we set \(_{0}=\{s\}\). We say this process _converges_ when \(_{T}=\) if there exists such \(T\); the generated sequence of active nodes are \(_{t}\). The total number of operations of the local solver \(_{}\) is

\[_{_{}}=_{t=0}^{T-1}( _{t})=T}(_{T}),\]

where we denote the average of active volume as \(}(_{T})=T^{-1}_{t=0}^{T-1} (_{t})\).

Intuitively, we can treat this local diffusion process as a random walk on a Markov Chain defined on the space of all subsets of \(\), where the transition probability represents the operation cost. More efficient local solvers try to find a _shorter_ random walk from \(_{0}\) to \(_{T}\). The following two subsections demonstrate the power of this process in designing local methods for PPR and Katz.

### Sequential local updates via Successive Overrelaxation (SOR)

Given the estimate \(^{(t)}\) at \(t\)-th iteration, we define residual \(^{(t)}=-^{(t)}\) and \(\) be the matrix induced by \(\) and \(\). The typical local Gauss-Seidel with Successive Overrelaxation has the following online estimate-residual updates (See Section 11.2 of ): at each time \(t=0,1,,T-1\), for each active node \(u_{i}_{t}=\{u_{1},u_{2},,u_{|_{t}|}\}\) with \(i=1,2,,|_{t}|\), we update each \(u_{i}\)-th entry of \(\) and corresponding residual \(\) as the following

\[:}^{(+_{t+1})}=^{(+ _{i})}+}}_{u_{i}}^{(+_{i})}, ^{(+_{i+1})}=^{(+_{i})}- }}_{u_{i}}^{(+_{i})},\] (6)

where \(_{i}(i-1)/|_{t}|\) is the current time, and update unit vector is \(}_{u_{i}}^{(t+t_{i})} r_{u_{i}}^{(t+t_{i})}_{u_{ i}}/q_{u_{i}u_{i}}\). Note that each update of (6) is \((d_{u_{i}})\). If \(_{t}=\), it reduces to the standard GS-SOR . Therefore, APPR is a special case of (6), a local variant of GS-SOR with \(=1\). Figure 2 illustrates this procedure. APPR updates \(\) at some entries and keeps track of large magnitudes of \(\) per iteration, while LocalSOR allows for a better choice of \(\); hence, the total number of operations is reduced. We establish the following fundamental property of LocalSOR.

**Theorem 3.2** (Properties of local diffusion process via LocalSOR).: _Let \(-\) where \(_{n n}\) and \(P_{} 0\,\,(u,v)\); 0 otherwise. Define maximal value \(P_{}=_{u}\|_{u}\|_{1}\). Assume that \(^{(0)}\) is nonnegative and \(P_{}\), \(\) are such that \( P_{}<1\), given the updates of (6), then the local diffusion process of \((^{(t)},^{(t)},_{t};,,,_{}=(,))\) with \((0,1)\) has the following properties_

1. _Nonnegativity._ \(^{(t+t_{i})}\) _for all_ \(t 0\) _and_ \(t_{i}=(i-1)/|_{t}|\) _with_ \(i=1,2,,|_{t}|\)
**Monotonicity property**.: \(\|^{(0)}\|_{1}\|^{(t+t_{i})}\|_{1}\|^{(t+t_{i+1})}\| _{1}\).

_If the local diffusion process converges (i.e., \(_{T}=\)), then \(T\) is bounded by_

\[T_{T}(1- P_{})}^ {(0)}\|_{1}}{\|^{(T)}\|_{1}},_{T}_{t=0}^{T-1} \{_{t}^{|_{t}|}r_{u_{i}}^{(t+t _{i})}}{\|^{(t)}\|_{1}}\}.\]

Based on Theorem 3.2, we establish the following sublinear time bounds of LocalSOR for PPR.

**Theorem 3.3** (Sublinear runtime bound of LocalSOR for PPR).: _Let \(_{T}=(^{(T)})\). Given an undirected graph \(\) and a target source node \(s\) with \((0,1),=1\), and provided \(0< 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((-(1-)^{-1})_{}=_{s}\) with the stop condition \(\|^{-1}^{(T)}\|_{}\) and initials \(^{(0)}=\) and \(^{(0)}=_{s}\) is bounded as the following_

\[_{}\{,}(_{T})}{_{T}} }}{}\},}(_{T})}.\] (7)

_where \(C_{}=1/((1-)|_{T}|)\). The estimate \(^{(T)}\) satisfies \(\|^{-1}(^{(T)}-_{})\|_{}\)._

The above theorem demonstrates the usefulness of our framework. It shows a new evolving bound where \(}(_{T})/_{T} 1/\) as long as \(\) satisfying certain assumption (It is true for PPR and Katz). Similarly, applying Theorem 3.2, we have the following result for approximating \(_{}\).

**Corollary 3.4** (Runtime bound of LocalSOR for Katz).: _Let \(_{T}=(^{(T)})\) and \(C_{}=1/((1-)|_{T}|)\). Given an undirected graph \(\) and a target source node \(s\) with \((0,1/d_{}),=1\), and provided \(0< 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((-)=_{s}\) with the stop condition \(\|^{-1}^{(T)}\|_{}\) and initials \(^{(0)}=\) and \(^{(0)}=_{s}\) is bounded as the following_

\[_{}\{)},}(_{T})}{(1- d_{ })_{T}}}}{}\},}(_{T})}.\] (8)

_The estimate \(}_{}=^{(T)}-_{s}\) satisfies \(\|}_{}-_{}\|_{2}\|(- )^{-1}\|_{1}\)._

**Accerlation when \(\) is Stieltjes matrix (\(^{*}=2/(1+})\)).** It is well-known that when \((1,2]\), GS-SOR has acceleration ability when \(\) is Stieltjes. However, it is fundamentally

Figure 2: The first row illustrates the local diffusion process of APPR  over a toy network topology adopted from . It uses \(T_{}=6\) local iterations with \(_{}=42\) operations and additive error \( 0.29241\). The second row shows the process of LocalSOR (\(=1.19^{*}\)). It uses \(T_{}=5\) local iterations with \(_{}=28\) operations and additive error \( 0.21479\). LocalSOR uses fewer local iterations, costs less total active volume, and obtains better approximate solutions. We choose the source node \(s=0\) with \(=0.02\) and \(=0.25\).

difficult to prove the runtime bound as the monotonicity property no longer holds. It has been conjectured that a runtime of \(}(1/())\) can be achieved . Incorporating our bound, a new conjecture could be \(}(}(_{T})/(_{T}))\): If \(\) is Stieltjes, then with the proper choice of \(\), one can achieve speedup convergence to \(}(}(_{T})/(_{T}))\)? We leave it as an open problem.

### Parallelizable local updates via GD and Chebyshev

The fundamental limitation of LocalSOR is its reliance on essentially sequential online updates, which may be challenging to utilize with GPU-type computing resources. _Interestingly, we developed a local iterative method that is embarrassingly simpler, highly parallelizable, and provably sublinear for approximating PPR and Katz_. First, one can reformulate the equation of PPR and Katz as 2

\[_{t}^{*}=*{arg\,min}_{^{n}}f() ^{}-^{},\] (9)

A natural idea for solving the above is to use standard GD. Hence, following a similar idea of local updates, we simultaneously update solutions for \(_{t}\). We propose the following local GD.

\[:}^{(t+1)}=^{(t)}+\ _{_{t}}^{(t)},^{(t+1)}=^{(t)}- _{_{t}}^{(t)}\] (10)

**Theorem 3.5** (Properties of local diffusion process via LocalGD).: _Let \(-\) where \(_{n n}\) and \(P_{wv} 0\) if \((u,v)\); 0 otherwise. Define maximal value \(P_{}=_{_{t}}\|_{ _{t}}\|_{1}/\|_{_{t}}\|_{1}\). Assume that \(^{(0)}=\) and \(P_{}\), \(\) are such that \( P_{}<1\), given the updates of (10), then the local diffusion process of \((_{t},^{(t)},^{(t)};,_{ }=(,,L))\) has the following properties_

1. _Nonnegativity._ \(^{(t)}\) _for all_ \(t 0\)_._
2. _Monotonicity property._ \(\|^{(0)}\|_{1}\|^{(t)}\|_{1}\|^{(t+1)}\|_{1}.\)__

_If the local diffusion process converges (i.e., \(_{T}=\)), then \(T\) is bounded by_

\[T_{T}(1- P_{})}^{(0)} \|_{1}}{\|^{(T)}\|_{1}},_{T}_{t=0}^{T-1} \{_{t}_{_{t}}^{(t)}\|_{1}}{\| ^{(t)}\|_{1}}\}.\]

Indeed, the above local updates have properties that are quite similar to those of SOR. Based on Theorem 3.5, we establish the sublinear runtime bounds of LocalGD for solving PPR and Katz.

**Corollary 3.6** (Convergence of LocalGD for PPR and Katz).: _Let \(_{T}=*{supp}(^{(T)})\) and \(C=_{T}|}\). Use LocalGD to approximate PPR or Katz by using iterative procedure (10). Denote \(_{}\) and \(_{}\) as the total number of operations needed by using LocalGD, they can then be bounded by_

\[_{}\{}},}(_{T})}{_{} _{T}}\},}(_{T})}{_{T}}\] (11)

_for a stop condition \(\|^{-1}^{(t)}\|_{}_{}\). For solving Katz, then the toal runtime is bounded by_

\[_{}\{}  d_{})},}(_{T})} {(1-_{} d_{})_{T}}\},}(_{T})}{ _{T}}\] (12)

_for a stop condition \(\|^{-1}^{(t)}\|_{} d_{u}\). The estimate of equality is the same as that of LocalSOR._

_Remark 3.7_.: Note that LocalGD is quite different from iterative hard-thresholding methods  where the time complexity of the thresholding operator is \((n)\) at best, hence not a sublinear algorithm.

**Accerrelated local Chebyshev.** One can extend the Chebyshev method , an optimal iterative solver for (9). Following , Chebyshev polynomials \(T_{n}\) are defined via following recursions

\[_{t}(x)=2x_{t-1}(x)-_{t-2}(x),k 2,_{0}(t)=1,_{1}(t)=x.\] (13)Given \(_{1}=,_{1}=_{0}-f (_{0})\), the standard Chevyshev method is defined as

\[_{k}=_{k-1}-}{L-} f(_{k-1} )+(1-2_{k})(_{k-2}- _{k-1}),_{k}=-_{k-1}},\]

where \(() L\). For example, for approximating PPR, we propose the following local updates

\[:^{(t+1)}=^{(t)}+}{1-}^{1/2}_{_{t}}^{(t)}+_{t: t+1}(^{(t)}-^{(t-1)})_{_{t}},_{t+1}=( -_{t})^{-1}}\] \[^{1/2}^{(t+1)}=^{1/2}^{(t)}-(^{(t +1)}-^{(t)})+(1-)^{-1}(^{(t+1)}-^{(t)}).\]

The sublinear runtime analysis for LocalCH is complicated since it does not follow the monotonicity property during the updates. Whether it admits, an accelerated rate remains an open problem.

## 4 Applications to Dynamic GDEs and GNN Propagation

Our accelerated local solvers are ready for many applications. We demonstrate here that they can be incorporated to approximate dynamic GDEs and GNN propagation. We consider the _discrete-time dynamic graph_ which contains a sequence of snapshots of the underlying graphs, i.e., \(_{0},_{1},,_{T}\). The transition from \(_{t-1}\) to \(_{t}\) consists of a list of events \(_{t}\) involving edge deletions or insertions. The graph \(_{t}\) is then represented as: \(_{0}_{1}}_{1}_{2}}_{2}_{3}} _{T-1}_{T}}_{T}\). The goal is to calculate an approximate \(_{t}\) for the PPR linear system \(_{t}_{t}=_{s}\) at time \(t\). That is,

\[(-(1-)_{t}_{t}^{-1})_{t}=_{s}.\] (14)

The key advantage of updating the above equation (presented in Algorithm 3) is that the APPR algorithm is used as a primary component for updating, making it much cheaper than computing from scratch. Consequently, we can apply our local solver LocalSOR to the above equation, and we found that it significantly sped up dynamic GDE calculations and dynamic PPR-based GNN training.

## 5 Experiments

**Datasets.** We conduct experiments on 18 graphs ranging from small-scale (_cora_) to large-scale (_papers100M_), mainly collected from Stanford SNAP  and OGB  (see details in Table 3). We focus on the following tasks: 1) approximating diffusion vectors \(\) with fixed stop conditions using both CPU and GPU implementations; 2) approximating dynamic PPR using local methods and training dynamic GNN models based on InstantGNN models . 3

**Baselines and Experimental Setups.** We consider four methods with their localized counterparts: Gauss-Seidel (GS)/LocalGS, Successive Overrelaxation (SOR)/LocalSOR, Gradient Descent (GD)/LocalGD, and Chebyshev (CH)/LocalCH.4 Specifically, we use the stop condition \(\|^{-1}^{(t)}\|_{}\) for PPR and \(\|^{-1}^{(t)}\|_{}\) for Katz, while we follow the parameter settings used in  for HK. For LocalSOR, we use the optimal parameter \(^{*}\) suggested in Section 3.2. We randomly sample 50 nodes uniformly from lower to higher-degree nodes for all experiments. All results are averaged over these 50 nodes. We conduct experiments using Python 3.10 with CuPy and Numba on a server with 80 cores, 256GB of memory, and two 28GB NVIDIA-4090 GPUs.

### Results on efficiency of local GDE solvers

**Local solvers are faster and use fewer operations.** We first investigate the efficiency of the proposed local solvers for computing PPR and Katz centrality. We set \((_{}=0.1,=1/n)\) for PPR and \((_{}=1/(\|\|_{2}+1),=1/m)\) for Katz. We use a temperature of \(=10\) and \(=1/\) for HK. All algorithms for HK estimate the number of iterations by considering the truncated error of Taylor approximation. Table 2 presents the _speedup ratio_ of four local methods over their standard counterparts. For five graph datasets, the speedup is more than 100 times in terms of the number of operations in most cases. This strongly demonstrates the efficiency of these local solvers. Figure4 presents all such results. Key observations are: 1) _All local methods significantly speed up their global counterparts on all datasets_. This strongly indicates that when \(\) is within a certain range, local solvers for GDEs are much cheaper than their global counterparts. 2) _Among all local methods, LocalGS and LocalGD have the best overall performance_. This may seem counterintuitive since LocalSOR and LocalCH are more efficient in convergence rate. However, the number of iterations needed for \(=1/n\) or \(=1/m\) is much smaller. Hence, their improvements are less significant.

**Which local GDE solvers are the best under what settings?** In the lower precision setting, previous results suggest that LocalSOR and LocalGD perform best overall. To test the proposed LocalSOR efficiency, we use the _ogbn-arxiv_ dataset to evaluate the local algorithm efficiency under a high precision setting. Figure 5 illustrates the performance of LocalSOR and LocalGS for PPR and Katz. As \(\) becomes smaller, the speedup of LocalSOR over LocalGS becomes more significant.

**When do local GDE solvers (not) work?** The next critical question is when standard solvers can be effectively localized, meaning under which \(\) conditions we see speedup over standard solvers. We conduct experiments to determine when local GDE solvers show speedup over their global counterparts for approximating Katz, HK, and PPR on the _wiki-talk_ graph dataset for different \(\) values, ranging from lower precision \((2^{-17})\) to high precision \((2^{-32})\). As illustrated in Figure 6,

   Graph & SOR/LocalSOR & GS/LocalGS & GD/LocalGD & CH/LocalCH \\  Citeseer & 86.21 & 114.89 & 157.41 & 5.86 \\  ogbn-arxiv & 183.43 & 392.26 & 528.03 & 101.88 \\  ogbn-products & 667.39 & 765.97 & 904.21 & 417.41 \\  wiki-talk & 169.67 & 172.95 & 336.53 & 30.45 \\  ogbn-papers100M & 243.68 & 809.47 & 1137.41 & 131.30 \\   

Table 2: Speedup ratio of computing PPR vectors. Let \(}\) and \(A}}\) be the number of operations of the standard algorithm \(\) and the local solver Local\(\), respectively. The speedup ratio \(=}/A}}\).

Figure 4: Number of operations required for four representative methods and their localized counterparts over 18 graphs. The graph index is sorted according to the performance of LocalGS.

Figure 5: The number of operations as a function of \(\) for comparing LocalSOR and LocalGS.

Figure 6: Running time (seconds) as a function of \(\) for HK, Katz, and PPR GDEs on the _wiki-talk_ dataset. We use \(_{}=0.1\), \(_{}=1/(\|\|_{2}+1)\), and \(_{}=10\) with 50 sampled source nodes.

when \(\) is in the lower range, local methods are much more efficient than the standard counterparts. As expected, the speedup decreases and becomes worse than the standard counterparts when high precision is needed. This indicates the broad applicability of our framework; the required precisions in many real-world graph applications are within a range where our framework is effective.

### Local GDE solvers on GPU-architecture

We conducted experiments on the efficiency of the GPU implementation of LocalGD and GD, specifically using LocalGD's GPU implementation to compare with other methods. Figure 7 presents the running time of global and local solvers as a function of \(\) on the _wiki-talk_ dataset. LocalGD is the fastest among a wide range of \(\). This indicates that, when a GPU is available and \(\) is within the effective range, LocalGD (GPU) can be much faster than standard GD (GPU) and other methods based on CPUs.

We observed similar patterns for computing Katz in Figure 11.

### Dynamic PPR approximating and training GNN models

To test the applicability of the proposed local solvers, we apply LocalSOR to GNN propagation and test its efficiency on _ogbn-arxiv_ and _ogbn-products_. We use the InstantGNN  model (essentially APPNP ), following the experimental settings in . Specifically, we set \(=0.1\), \(=10^{-2}/n\), and \(=^{*}\) to the optimal value for our cases. For _ogbn-arxiv_, we randomly partition the graph into 16 snapshots (each snapshot with 59,375 edges), where the initial graph \(_{0}\) contains 17.9% of the edges. With a similar performance on testing accuracy, as illustrated in Figure 8, the InstantGNN model with LocalSOR (Alg. 7) has a significantly shorter training time than its local propagation counterpart (Algo. 6). Figure 9 presents the accumulated operations over these 16 snapshots. The faster local solver is LocalSOR (Dynamic), which dynamically updates \((,)\) for (14) according to Algo. 5 and Algo. 3, whereas LocalSOR (Static) updates all approximate PPR vectors from scratch at the start of each snapshot for (14). We observed a similar pattern for LocalCH, where the number of operations is significantly reduced compared to static ones.

Figure 8: InstantGNN model using local iterative solver (LocalSOR) to do propagation.

Figure 7: Comparison of running time (seconds) for CPU and GPU implementations.

Figure 9: Acculmulated total number of operations of local solvers on the _ogbn-arxiv_ dataset.

## 6 Limitations and Conclusion

When \(\) is sufficiently small, the speedup is insignificant, and it is unknown whether more efficient local solvers can be designed under this setting. Although we observed acceleration in practice, the accelerated bounds for LocalSOR and LocalCH have not been proven. Another limitation of local solvers is that they inherit the limitations of their standard counterparts. This paper mainly develops local solvers for PPR and Katz, and it remains interesting to consider other types of GDEs.

We propose the local diffusion process, a local iterative algorithm framework based on the locally evolving set process. Our framework is powerful in capturing existing local iterative solvers such as APPR for GDEs. We then demonstrate that standard iterative solvers can be effectively localized, achieving sublinear runtime complexity when monotonicity properties hold in these local solvers. Extensive experiments show that local solvers consistently speed up standard solvers by a hundredfold. We also show that these local solvers could help build faster GNN models [31; 13; 14; 20]. We expect many GNN models to benefit from these local solvers using a _local message passing_ strategy, which we are actively investigating. Several open problems are worth exploring, such as whether these empirically accelerated local solvers admit accelerated sublinear runtime bounds without the monotonicity assumption.