# 4D Gaussian Splatting in the Wild

with Uncertainty-Aware Regularization

Mijeong Kim\({}^{1}\)

mijeong.kim@snu.ac.kr

Jongwoo Lim\({}^{2,\,3}\)

jongwoo.lim@snu.ac.kr

Bohyung Han\({}^{1,3}\)

bhhan@snu.ac.kr

###### Abstract

Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.

## 1 Introduction

Dynamic novel View Synthesis (DVS) aims to reconstruct dynamic scenes from captured videos and generate photorealistic frames for an arbitrary new combination of a viewpoint and a time step. This task has emerged as a vital research area in the 3D vision community with rapid advancements in augmented reality and virtual reality. Early DVS research primarily relied on neural radiance fields . In contrast, more recent methods  extend 3D Gaussian Splatting  to account for the additional time dimension in dynamic scenes, and these techniques are referred to as 4D Gaussian Splatting.

Despite the recent success of 4D Gaussian Splatting models , their applicability remains largely limited to controlled and purpose-built environments. Most existing models are developed and tested with multi-view video setups . While there are several methods tackling monocular video settings, these setups are still controlled and fall short of in-the-wild scenarios. For instance,  maintain multi-view characteristics, where the camera captures a broad arc around a slow-moving object. Also, HyperNeRF  relies on unrealistic train-test splits, with both sets sampled from the same video trajectory, which renders the task closer to video interpolation than genuine novel view synthesis. In this paper, we focus for the first time on more natural, real-world monocular videos , where a single handheld camera moves around fast-moving objects.

In casually recorded monocular videos, which often lack sufficient multi-view information, 4D Gaussian Splatting algorithms tend to overfit the training frames in real-world scenarios. To address overfitting, recent regularization techniques  can be applied to provideadditional priors for unseen views. However, these regularization techniques often involve a balancing issue: while they effectively improve novel view synthesis performance during testing, they inherently sacrifice the reconstruction accuracy of training images. Since both the reconstruction accuracy and the novel view synthesis quality are equally important in our target task, the trade-off caused by the naive application of the regularization techniques is not desirable.

In this paper, we address this balancing issue with a simple yet effective solution: uncertainty-aware regularization. First, we quantify the uncertainty of each Gaussian primitive based on its contribution to rendering for training images. Then, a 2D uncertainty map is constructed for unseen views using an \(\)-blending method. Regularization is selectively applied to uncertain regions, guided by the diffusion and depth smoothness priors, while low-uncertainty regions, where training data already provide sufficient reconstruction detail, are left unregularized, as illustrated in Figure 1. This approach results in a better balance between training and test performance, achieving good performance.

In real-world scenarios involving fast motions, especially in casually recorded videos, 4D Gaussian Splatting additionally faces considerable challenges with initialization. The algorithms based on Gaussian Splatting initialize Gaussian primitives using point clouds obtained by Structure from Motion (SfM) . However, SfM struggles to reconstruct dynamic regions, particularly those with fast motion, often treating them as noise and leaving these areas without initialized primitives. Such an incomplete initialization disrupts training, causing primitives in static regions to be repeatedly cloned and split in an attempt to fill the dynamic areas. This can lead to an excessive number of primitives and, in some cases, out-of-memory issues. To address this limitation, we propose a dynamic region densification technique that initializes additional Gaussian primitives in dynamic regions.

We address the challenging problem of 4D reconstruction from an in-the-wild monocular video recorded casually with a handheld camera--a scenario that has been rarely explored. The main contributions of this paper are summarized as follows:

* We propose an uncertainty quantification technique based on contribution to training image rendering and introduce adaptive regularization techniques based on the uncertainty map, which balances between novel view synthesis performance and training image reconstruction quality.
* We address the issue of incomplete initialization in dynamic regions, emphasizing the importance of proper initialization in the training process of 4D Gaussian Splatting.
* We demonstrate the effectiveness of our algorithm on casually recorded monocular videos, showing improvements over baselines. Additionally, we validate the applicability of our method in few-shot static scene reconstruction.

The rest of this paper is organized as follows. Section 2 reviews related work and Section 3 discusses the basic concepts of 4D Gaussian splatting, which builds upon 3D Gaussian Splatting by integrating

Figure 1: Concept of uncertainty-aware regularization. Existing models often use regularization techniques to introduce additional priors for unseen views, aiming to enhance novel view synthesis performance. However, these methods tend to over-regularize accurately reconstructed pixels, which degrades the reconstruction quality of training images. To address this issue, our uncertainty-aware regularization selectively focuses on uncertain regions in unseen views, preserving the quality of well-reconstructed pixels with low uncertainty.

deformation strategies. The details of our approach are described in Section 4, followed by the presentation of experimental results in Section 5. Finally, we conclude this paper in Section 6.

## 2 Related Work

### Dynamic Novel View Synthesis

In recent years, significant advances have been made in novel view synthesis [34; 6; 15; 60; 35; 23]. Initially focused on static scenes, novel view synthesis has shifted towards dynamic scenes through the integration of motion modeling, now referred to as Dynamic novel View Synthesis (DVS). Early approaches [13; 10; 38; 5; 12; 50; 11] are largely driven by Neural Radiance Fields (NeRF). Some studies [13; 10] capture dynamics implicitly through temporal inputs or latent codes, and other approaches [38; 5; 12; 50; 11] focus on training the canonical NeRF and its deformation fields.

The introduction of 3D Gaussian Splitting (3DGS)  has marked a paradigm shift in novel view synthesis, leading to the development of 4D Gaussian Splatting (4DGS) [68; 18; 61; 31] for DVS. These 4DGS methods deform canonical 3D Gaussian primitives over time using additional deformation networks, which can be based on MLPs, learnable control points, Hexplane [5; 61], or polynomial functions. While these models excel at reconstructing dynamic scenes in controlled environments [38; 69; 29; 41; 39], they face significant challenges when applied to casually recorded monocular videos, posing substantial hurdles for real-world applications.

### Regularization Techniques in Sparse Reconstruction

Casually recorded monocular videos typically provide limited multi-view information, as they are typically captured with a single handheld camera that only exhibits gentle motion. Consequently, reconstructing dynamic scenes from these videos is often regarded as a form of sparse reconstruction due to the lack of multi-view data.

In the context of sparse reconstruction, various regularization techniques have been proposed to mitigate overfitting on limited training images [36; 25; 20; 48; 22; 4; 49; 53; 24; 63; 43; 8; 59; 55; 16; 65; 62; 2; 17; 16]. These approaches generally involve rendering images or depth maps for unseen views to provide additional priors. For instance, some methods leverage depth priors based on estimated depths for novel views [43; 8; 59; 55; 16; 72], while others incorporate color smoothness constraints to enhance these views [25; 36]. Building on the success of diffusion models , recent approach  starts to incorporate diffusion priors to produce more realistic images of novel views. While these methods effectively enhance novel view synthesis performance at test time, they inherently compromise the quality of training image reconstructions. Since both the reconstruction accuracy and the novel view synthesis quality are equality important in our target task, the trade-off caused by the naive application of the regularization techniques is not desirable.

### Uncertainty Quantifications in Novel View Synthesis

Uncertainty estimation in novel view synthesis has primarily been explored with Neural Radiance Fields (NeRF) . Pioneering approaches [37; 51; 52] re-parameterize MLP networks in NeRF using Bayesian models to compute the uncertainty of network predictions. Inspired by InfoNeRF , which considers entropy along rays for few-shot NeRFs, some studies [70; 66; 27] quantify uncertainty using the entropy of density along rays from a novel view. Additionally, Density-aware NeRF Ensembles  measures uncertainty by examining the variance in RGB images produced by an ensemble of models.

In contrast, uncertainty quantification in Gaussian Splatting  remains largely underexplored, with only a few works addressing this issue. Savant _et al._ incorporate variational inference into the rendering pipeline, but this approach increases learnable parameters. Similarly, FisherRF  quantifies the uncertainty of Gaussian primitives by aggregating the diagonal of the Hessian matrix of the log-likelihood function; however, it is not straightforward to obtain a scalar value of uncertainty from the Hessian matrix for each Gaussian primitive. Our approach, on the other hand, directly quantifies the observed information of each Gaussian primitive by aggregating their contributions to the reconstruction of training images.

Most existing works that utilize estimated uncertainty primarily focus on quantifying model predictions after training  or on active learning for next-view selection . In contrast, our approach leverages estimated uncertainty for adaptive regularization during training, specifically targeting Gaussian Splatting in sparse reconstruction.

## 3 Preliminary: 4D Gaussian Splatting

This section briefly overviews 3D Gaussian splatting (3DGS)  and explains the deformation modeling in 4D Gaussian Splatting (4DGS) [68; 18; 61; 31] for dynamic scenes.

### 3D Gaussian Splatting

Gaussian primitive3D Gaussian splatting has demonstrated real-time, state-of-the-art rendering quality on static scenes. It uses an explicit 3D scene representation consisting of a set of 3D Gaussian ellipsoids, denoted by \(=\{_{1},...,_{K}\}\). Each Gaussian primitive, \(_{k}\), is based on an unnormalized 3D Gaussian kernel, \(_{k}()\), parameterized by \(_{k}\) and \(_{k}\) as follows:

\[_{k}(;_{k},_{k})(- {1}{2}(-_{k})^{}_{k}^{-1}(-_{k}) ),\] (1)

where \(_{k}^{3}\) is the center position, \(_{k}^{3 3}\) is an anisotropic covariance matrix, and \(^{3}\) is an arbitrary location in 3D space. The covariance matrix \(_{k}\) is valid only when positive semi-definite, which is challenging to enforce during optimization. To ensure this condition, we learn \(_{k}\) by decomposing it into two learnable components, a rotation matrix \(}\) and a scaling matrix \(}\) as follows:

\[_{k}_{k}_{k}_{k}^{}_{k}^{}.\] (2)

In addition to the standard Gaussian parameters such as \(_{k},_{k}\), and \(_{k}\), the Gaussian primitive requires additional learnable parameters for its opacity, \(_{k}\), and feature, \(_{k}^{d}\), which is typically represented by RGB colors or spherical harmonic (SH) coefficients. Thus, each Gaussian primitive is represented as \(_{k}(_{k},_{k},_{k},_{k},_{k})\).

Differentiable rasterizationBefore rendering with the Gaussian primitives \(\) on an image space, each 3D Gaussian kernel, \(_{k}(;_{k},_{k})\), is projected onto a 2D image space and forms a 2D Gaussian kernel, \(_{k}^{}(;_{k}^{},_{k}^{})\), where \(:^{3}^{2}\) denotes a projection from the world coordinate to an image space. In the projected Gaussian representation, \(^{2}\) indicates a pixel location in an image, and the 2D mean \(_{k}^{}^{2}\) and covariance \(_{k}^{}^{2}\) are given by

\[_{k}^{}:=(_{k})_{k}^{ }_{k}^{}^{},\] (3)

where \(\) denotes the Jacobian of the affine approximation of the projective transformation, and \(\) is the world-to-camera transform matrix. When rendering the primitives in \(\) to a target camera, they are sorted by their depths with respect to the camera center. The color of a pixel \(\) is then obtained by \(\)-blending, which is given by

\[}()_{k=1}^{K}_{k}^{}()c(_{ k},),\] (4)

where \(_{k}^{}()\) represents a relative contribution of each Gaussian primitive to pixel \(\) and \(c(_{k},)\) is the color of a pixel \(\) measured along the view direction. If a feature vector \(_{k}\) is based on spherical harmonics coefficients, the color is decoded from \(_{k}\) using the view direction associated with pixel \(\); otherwise, the feature vector \(_{k}\) can be identical to the RGB color of the primitive. For more details, please refer to the original Gaussian Splatting paper . Note that, following the \(\)-blending procedure in 3DGS , \(_{k}^{}()\) is given by

\[_{k}^{}()_{k}_{k}^{}(;_{k}^{},_{k}^{})_{j=1}^{k-1}(1-_{j}_{j}^{}(;_{k}^{},_{k}^{})),\] (5)

where \(_{k}_{k}^{}(;_{k}^{},_{k}^{})\) is the opacity of the \(k^{}\) projected primitive at the junction with a ray corresponding to pixel \(\) and \(_{j=1}^{k-1}()\) is the transmittance at the primitive \(_{k}\) on pixel \(\), which measure how much light penetrates the preceding primitives along the ray.

### Deformation Modeling in 4D Gaussian Splatting

To represent 4D scenes using Gaussian splatting, recent algorithms [68; 18; 61; 31] deform the 3D Gaussian primitives from their canonical states to a target state over time. The transformed position \(^{t}\), rotation \(^{t}\), and scale \(^{t}\) at time \(t\) are given by

\[(^{t}_{k},^{t}_{k},^{t}_{k})=(_{k}+_{}(_{k},t),_{k}+_{r}(_{k},t),_{k}+_{s}(_{k},t )),\] (6)

where the deformation functions \(_{}()\), \(_{r}()\), and \(_{s}()\) can be various forms, including MLPs, learnable control points , Hexplane [5; 61], or polynomial functions. A deformed 3D Gaussian primitive at time \(t\) is represented as \(_{k}(t)(^{t}_{k},^{t}_{k},^{t}_{k},_ {k},_{k})\). The projection onto a 2D space follows the same procedure as the static 3D Gaussian splatting, presented in Equation (4). Our approach adopts the Hexplane structure for deformation, similar to .

## 4 Uncertainty-Aware 4D Gaussian Splatting

### Uncertainty-Aware Regularization

We now discuss the proposed uncertainty-aware regularization technique designed for the balance between reconstruction quality on training images and generalization to unseen views.

Uncertainty quantificationWe first estimate how informative each Gaussian primitive is for reconstruction, based on its visibility from all pixels in training images and its opacity, which is given by

\[C_{k}=_{}_{}_{k}^{}(),\] (7)

where \(\) is a pixel in a training image \(\), and \(_{k}^{}()\) is the contribution of each Gaussian primitive \(_{k}\) to pixel \(\) during \(\)-blending, as described in Equation (5). For the computation of this value, we customize the CUDA kernel to modify the backward process of the 3DGS .

The parameters of an informative Gaussian primitive are typically estimated accurately with high confidence. Conversely, a Gaussian primitive that is not properly supported by training images struggles with low accuracy and high uncertainty of its parameter estimation. Based on these observations, the uncertainty of each Gaussian primitive, \(U_{k}\), is defined as

\[U_{k}=1-(C_{k};c_{0},c_{1}),\] (8)

where the sigmoid function is used to bound and normalize \(C_{k}\) and \(\{c_{0},c_{1}\}\) control the inflection point shift and the slope of the sigmoid function, respectively. Given an arbitrary unseen viewpoint, a 2D uncertainty map \(\) is constructed using \(\)-blending as follows:

\[()=_{k=1}^{K}_{k}^{}()U_{k},\] (9)

where \(\) is a pixel in the uncertainty map \(\). We employ the estimated uncertainty map for the adaptive regularization to unseen views. Specifically, we adopt a diffusion prior as well as a depth smoothing prior and the details of these two priors are discussed next.

Uncertainty-aware diffusion-based regularizationTo render natural-looking images for novel views and times, we incorporate Stable Diffusion  into our pipeline. We begin by generating text prompts from the training frames using the vision-language model BLIP . These prompts guide fine-tuning of the diffusion model via DreamBooth  with the training images, which aligns the model's understanding to the specific content in the training images as discussed in the image-to-3D reconstruction algorithm . Using this fine-tuned model, we produce a refined image, \(_{}\) from the rendered image \(}\) for novel views or times. Specifically, we first encode \(}\) into the latent space using the latent diffusion encoder Enc, then perturb it into a noisy latent representation as follows:

\[_{t}=_{t}}(})+_{t}},(,) t[0,T],\] (10)where \(_{t}\) is a scalar value that controls the noise level and \(t\) is a diffusion time step. Similar to SDEdit , we generate \(_{}\) by performing the DDIM sampling  over \(k= 50\) steps and running the diffusion decoder Dec as follows:

\[_{}=((_{t},)),\] (11)

where \(\) is the text embedding of the prompt from BLIP. Applying a reconstruction loss between \(}\) and \(_{}\) is helpful for generating natural-looking images for unseen views; however, it may compromise reconstruction quality because \(_{}\) sometimes contains misaligned context with the actual 3D scene inherent in the training images. To achieve accurate and realistic reconstruction by balancing the two properties, we propose uncertainty-aware diffusion loss, \(_{}\). This loss applies the estimated uncertainty to the computation of the reconstruction error between the synthesized image, \(}\) and the corresponding DDIM-sampled image, \(_{}\), where uncertain regions are more regularized than low-uncertainty regions--where the training data already provide sufficient reconstruction detail--as follows:

\[_{}=(}-_{})|_{2}}{||_{2}}+(}-_{})|_{1}}{||_{1}},\] (12)

where \(\) is the uncertainty map of the unseen view and \(\) denotes element-wise product. Since the DDIM sampling for generating \(_{}\) is time-consuming, performing it every iteration could slow down the training process, which is not desirable. To avoid the computational burden for training, we randomly sample 200 images at the beginning of every 2,000 iterations, refine them by DDIM, and store them in memory. During the next 2,000 iterations, we utilize the stored images to compute the diffusion-based regularization term, \(_{}\).

Uncertainty-aware depth smoothing regularizationWe introduce an additional regularization term to encourage smooth depth predictions in uncertain regions. To this end, we first generate a depth map \(\) for unseen views using an \(\)-blending method as follows:

\[}()=_{k=1}^{K}_{k}^{}()d_{k},\] (13)

where \(^{2}\) is a pixel coordinate in the depth map \(}\), and \(d_{k}\) denotes the depth at the center of the \(k^{}\) Gaussian with respect to the camera center. We employ the total variation to regularize the estimated depth map \(}\), which promotes smooth depth transitions between neighboring pixels. However, the uniform total variation loss over all pixels produces blur artifacts on accurately predicted regions, resulting in large reconstruction error. To address this drawback, we propose an uncertainty-aware total variation loss, \(_{}\), which applies stronger smoothing to high-uncertainty regions for noise reduction in the depth map, while leaving low-uncertainty areas unregularized to preserve details. Our uncertainty-aware total variation loss is given by

\[_{}=}_{i,j}_{i,j}+ _{i+1,j}}{2}|}_{i,j}-}_{i+1,j}|+ }_{i,j}_{i,j}+_{i,j+1}}{2}| }_{i,j}-}_{i,j+1}|,\] (14)

where \(}_{i,j}\) is the estimated depth at pixel \((i,j)\) and \(u_{r}\) and \(u_{c}\) respectively denote the sums of the average uncertainties of vertically and horizontally adjacent pixels, in other words, \(u_{r}=_{i,j}_{i,j}+_{i+1,j}}{2}\) and \(u_{c}=_{i,j}_{i,j}+_{i,j+1}}{2}\).

### Dynamic Region Densification

Existing 4D Gaussian Splattings initialize Gaussian primitives using point clouds obtained from Structure from Motion (SfM) . However, since SfM assumes static scenes, it is fundamentally unable to reconstruct dynamic regions, particularly those involving rapid motion, as shown in Figure 2c. This failure occurs because the algorithm treats dynamic regions as noise, leaving these areas without initialized primitives. Such incomplete initialization disrupts the training process, causing primitives in static regions to be repeatedly cloned and split in an attempt to fill the dynamic areas. This leads to an excessive number of primitives and, in some case, out-of-memory issues.

To address this limitation, we propose a dynamic region densification that initializes additional Gaussian primitives \(^{}=\{_{1}^{},...,_{K}^{}\}\) in dynamic regions. To this end, we first identify dynamic pixels in the training images using scene flows  and randomly select a subset of the pixels. For each selected pixel \(\), the corresponding Gaussian primitive \(_{k}^{}\) is initialized with position \(_{}\) and feature vector \(_{}\) as:

\[_{}=^{-1}(,())[_{k}^{(0)},_{k}^{(1)},_{k}^{(2)}]= (),\] (15)

where \(()\) is the depth value of pixel \(\) estimated from , and \(^{-1}\) is the inverse projection function that reprojects \(\) into 3D space. The feature vector \(_{}\) encodes the information of the \(k^{}\) Gaussian primitive, where its first three components are set to the RGB colors of pixel \(\) in an image \(\) and the rest of dimensions, which are optional for the spherical harmonics representation, are padded to zeros. This method provides reasonable placements of Gaussian primitives in dynamic regions as shown in Figure 1(d).

### Data-Driven Losses

Since the dynamic scene reconstruction from casually recorded monocular video is a highly ill-posed problem, we apply the additional data-driven losses based on depth and flow maps. The depth-driven loss is defined by the difference between the depth maps \(}\) and \(\), which are respectively obtained via the \(\)-blending shown in Equation (13) and the algorithm proposed in , as shown in the following equation:

\[_{}=|}-|_{1}.\] (16)

The flow-driven loss is analogously defined. Given two frames \(\) and \(^{}\), the flow of a pixel \(\) is estimated using \(\)-blending, which is expressed by

\[}^{^{}}()=_{k=1}^{K}_{k }^{}()F_{k}^{^{}}, F_{ k}^{^{}}=^{}(_{k}^{t^{}})-(_{k}^{t}),\] (17)

where \(F_{k}^{^{}}\) represents the deformation of the \(k^{}\) Gaussian primitive from frame \(\) to frame \(^{}\) in the projected image space, where \((,t)\) and \((^{},t^{})\) denote the projection function and timestamp associated with frames \(\) and \(^{}\), respectively. Similar to \(_{}\), the data-driven loss for flow is defined by

\[_{}=|}^{^{}}- ^{^{}}|_{1},\] (18)

where \(^{^{}}\) is an optical flow map obtained from RAFT .

The final data-driven loss, \(_{}\), is defined as the sum of the two data-driven loss terms, as shown in the following equation:

\[_{}=_{}+_{}.\] (19)

Figure 2: Visualization of the dynamic region densification on the _Backpack_ scene. Since SfM  is designed for static scenes, it fails to properly initialize Gaussian primitives in dynamic regions. Our dynamic region densification module initializes additional Gaussian primitives in the identified dynamic regions using scene flow and depth map.

### Total Loss

To train our uncertainty-award 4D Gaussian splatting models, the total loss function is given by

\[=_{}+_{}_{}+_{}_{}+_{} _{}++_{}_{},\] (20)

where \(_{}\) is the standard reconstruction loss based on training images, \(_{}\) is a loss term associated with the Heylane-based deformation adopted in our baseline , and \(\{_{},_{},_{},_{ }\}\) are balancing hyperparameters for individual loss terms.

## 5 Experiments

This section compares the proposed method, referred to as UA-4DGS, with existing 4D Gaussian splatting algorithms including D-3DGS , Zhan _et al._, and 4DGS . Our method is implemented based on the official code of 4DGS  and tested on a single RTX A5000 GPU.

### Settings

DatasetOur primary goal is to reconstruct dynamic scenes from casually recorded monocular videos, for which we use DyCheck  as our main dataset. This dataset consists of monocular videos captured with a single handheld camera, featuring scenes with fast motion to provide a challenging and realistic scenario for dynamic scene reconstruction. The DyCheck dataset includes 14 videos; however, only the half of scenes--_apple_, _block_, _paper-windmill_, _teddy_, _space-out_, _spin_, and _wheel_--are suitable for evaluation due to the availability of held-out views.

Evaluation protocolTo evaluate novel view rendering quality, we use three metrics: peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and a perceptual metric called learned perceptual image patch similarity (LPIPS). Additionally, since our target dataset provides a co-visibility mask, we also compute masked versions of these metrics, mPSNR, mSSIM, and mLPIPS, focusing on co-visible regions. For masked evaluations, we employ the JAX implementation provided by DyCheck .

### Experimental Results

Table 1 presents the quantitative comparison of our algorithm against existing methods based on 4D Gaussian Splatting [68; 61; 31] and MLPs  on the DyCheck dataset . Our approach, UA-4DGS, surpasses the performance of all other 4D Gaussian Splatting algorithms across all metrics. Figure 3 shows qualitative results on the _space-out_, _paper-windmill_, _teddy_, and _spin_ scenes, where UA-4DGS synthesizes more realistic images, clearly outperforming existing 4D Gaussian Splatting algorithms.

Although Gaussian Splatting generally outperforms MLP-based approaches on multi-view or less challenging datasets, our experiments show that they fall behind MLP-based methods in our target setting based on casually recorded videos with a monocular handheld camera as presented in Table 1. This is probably because the methods based on Gaussian Splatting focus more on local optimization with respect to individual Gaussian primitives and, consequently, are prone to overfitting to training images in in-the-wild monocular scenarios.

### Analysis

Generalization to static scenesTo demonstrate the generality of our method in static scene reconstruction, we incorporate the proposed uncertainty-aware regularization to FSGS , a few-shot Gaussian Splatting algorithm for static scenes, and refer to this version of our model as UA-FSGS. We test both FSGS and UA-3DGS on the LLFF dataset  using three training images with five different runs. Table 2 presents quantitative comparisons where UA-3DGS outperforms existing methods, including both the original results and our reproduced ones of FSGS.

Ablation studyTable 3 shows the results of the ablation study on each proposed component. Dynamic region densification improves performance compared to the data-driven loss alone, implying that better alignment of primitives with scene geometry enhances the effectiveness of the loss term. Moreover, uncertainty-aware regularization yields further improvements, where \(_{}\) provides substantial benefits, and adding \(_{}\) results in additional gains.

**Impact of uncertainty consideration** To evaluate the impact of incorporating uncertainty, we test regularization methods without uncertainty consideration, where we refer to this version as \(_{}\) and \(_{}\), respectively. Table 4 shows both train and test performance; while \(_{}\) and \(_{}\) are still effective for test performance, they exhibit underfitting on training images with lower reconstruction performance. In contrast, by integrating uncertainty through \(_{}\) and \(_{}\), we can enhance the balance between training reconstruction quality and test performance in novel view synthesis.

## 6 Conclusions

We proposed a novel training framework for 4D Gaussian Splatting, targeting dynamic scenes captured from casually recorded monocular cameras. Our uncertainty-aware regularizations, which incorporate diffusion and depth-smoothness priors, effectively improve novel view synthesis performance while preserving reconstruction quality on training images. Additionally, we addressed the initialization challenges of Gaussian primitives in fast-moving scenes by introducing dynamic region densification. Our method demonstrated performance gains over baseline approaches, both in dynamic scene reconstructions and few-shot static scene reconstructions. We conducted a detailed analysis through extensive experiments, and we believe this work initiates research on an important, emerging problem in 4D Gaussian Splatting, offering valuable insights to the field.

Limitations and future workNovel view synthesis performance on casually recorded monocular videos still lags behind that on multi-view or simpler datasets, highlighting potential areas for improvement in future research. Currently, our regularization techniques rely on image-level regularization using 2D uncertainty maps; future work could enhance this by incorporating regularization in the Gaussian primitive level  to directly leverage each Gaussian primitive's uncertainty. Additionally, our dynamic region densification does not consider temporal consistency for primitive initialization, but this issue may be addressed by integrating long-term tracking algorithms .

   Representation & Method & FPS & mPSNR \(\) & PSNR \(\) & mSSIM \(\) & SSIM \(\) & mLPIPS\(\) & LPIPS \(\) \\   & T-NeRF  & 1 & 16.96 & 16.23 & 0.577 & 0.420 & 0.379 & 0.453 \\  & NSFF  & 1 & 16.45 & 15.79 & 0.570 & 0.415 & 0.339 & 0.409 \\  & Nerfies  & 1 & 16.81 & 16.43 & 0.569 & 0.417 & 0.332 & 0.399 \\  & HyperNeRF  & 1 & 15.46 & 15.20 & 0.551 & 0.399 & 0.396 & 0.464 \\  & D-3DGS  & 65 & 12.98 & 12.72 & 0.444 & 0.280 & 0.470 & 0.583 \\  & Zhan _et al._ & **111** & 13.47 & 13.15 & 0.456 & 0.289 & 0.448 & 0.522 \\  & 4DGS  & 73 & 14.14 & 13.90 & 0.465 & 0.297 & 0.430 & 0.508 \\  & UA-4DGS (ours) & 75 & **15.25** & **14.89** & **0.488** & **0.325** & **0.390** & **0.476** \\   

Table 1: Quantitative comparisons on a challenging dataset, DyCheck. Our approach shows the best performance among 4D Gaussian Splatting-based methods. However, Gaussian Splatting is generally worse than MLP-based methods in more challenging settings with casually recorded videos using a monocular handheld camera.

   Representation & Method & FPS & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\   & MipNeRF  & 0.21 & 16.11 & 0.401 & 0.46 \\  & DietNeRF  & 0.14 & 14.94 & 0.370 & 0.496 \\  & RegNeRF  & 0.21 & 19.08 & 0.587 & 0.336 \\  & FreeNeRF  & 0.21 & 19.63 & 0.612 & 0.308 \\  & SparseNeRF  & 0.21 & 19.86 & 0.624 & 0.328 \\  & 3DGS  & 385 & 17.83 & 0.582 & 0.321 \\  & Chung _et al._ & – & 17.17 & 0.497 & 0.337 \\ Gaussian Splatting & FSGS  & 458 & 20.43 & 0.682 & 0.248 \\  & FSGS  & **461** & 19.82 & 0.672 & 0.238 \\  & UA-3DGS (Ours) & **461** & **20.89** & **0.716** & **0.217** \\   

Table 2: Few-shot novel view synthesis results with three views for static scenes, tested on the LLFF  dataset. Our method significantly outperforms existing methods across all metrics. FSGS\({}^{}\) and UA-3DGS were tested over five runs, with (\(\)) indicating reproduced results. Results for other methods are taken from  and .

    &  &  &  \\  & & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  FSGS  & – & 42.38 & 0.981 & 0.029 & 20.60 & 0.822 & 0.184 \\  FSGS w/ \(_{}\) & & 38.40 & 0.986 & 0.035 & 20.40 & 0.811 & 0.198 \\ FSGS w/ \(_{}\) (Ours) & & **41.28** & **0.989** & **0.029** & **20.98** & **0.835** & **0.174** \\ FSGS w/ \(_{}\) & & 38.25 & **0.985** & 0.042 & 20.77 & 0.827 & 0.186 \\ FSGS w/ \(_{}\) (Ours) & ✓ & **38.26** & **0.985** & **0.038** & **21.08** & **0.831** & **0.184** \\   

Table 4: Quantitative comparison of regularization methods with and without uncertainty estimation on the static _room_ scene in the LLFF dataset, where FSGS  are used as the baseline. Incorporating uncertainty into the regularization improves novel view synthesis by enhancing the balance between reconstruction quality on training images and performance on novel views.

   Method & \(_{}\) & _Dynamic Dens._ & \(_{}\) & \(_{}\) & mPSNR & mSSIM & mLPIPS \\ 
4DGS  & – & – & – & – & 15.25 & 0.424 & 0.419 \\  & – & – & – & – & 15.74 & 0.444 & 0.373 \\  & ✓ & ✓ & & & 17.04 & 0.463 & 0.375 \\ Ours & ✓ & ✓ & ✓ & & 17.32 & 0.474 & 0.355 \\  & ✓ & ✓ & ✓ & ✓ & **17.37** & **0.481** & **0.342** \\   

Table 3: Ablation test results of our training schemes on the _spin_ scene in the DyCheck  dataset. _Dynamic Dens._ refers to dynamic region densification.

Figure 3: Qualitative results on the _space-out_, _paper-windmill_, _teddy_, and _spin_ scenes in the DyCheck dataset. UA-4DGS (Ours) shows outstanding quality of rendered images compared to existing methods, including D-3DGS , Zhan _et al._, and 4DGS