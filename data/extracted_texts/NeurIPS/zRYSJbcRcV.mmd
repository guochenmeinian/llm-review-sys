# Stanford-ORB: A Real-World 3D Object

Inverse Rendering Benchmark

 Zhengfei Kuang1   Yunzhi Zhang1   Hong-Xing Yu

Samir Agarwala   Shangzhe Wu   Jiajun Wu

Stanford University

Equal contribution.

###### Abstract

We introduce _Stanford-ORB_, a new real-world 3D Object inverse Rendering Benchmark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commercial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. Existing real-world datasets typically consist only of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes and compare the performance of various existing methods. All data, code, and models can be accessed at https://stanfordrb.github.io/.

## 1 Introduction

When we take a photograph of an object, light travels through space, bounces off surfaces, and eventually reaches the camera sensors to create an image. Inverting this image formation process is a fundamental problem in computer vision--from just the resulting single 2D image, we aim to reconstruct the physical scene in 3D, inferring 3D object shape, reasoning about its reflectance, and disentangling its intrinsic appearance from illumination effects. This process is often referred to as _inverse rendering_. Enabling machines to do so is not only crucial for general image understanding and object analysis, but also facilitates numerous downstream applications, such as controllable 3D content generation  and robotic manipulation .

Recent years have witnessed significant progress in inverse rendering, with the emergence of various differentiable and neural rendering techniques. Neural Radiance Fields (NeRFs) , for instance, have emerged as a powerful volumetric representation that effectively encodes the geometry and view-dependent appearance of a scene into a neural network, allowing for high-fidelity novel view synthesis of complex real-world scenes given raw multi-view images as input. This framework has been successfully extended to further decompose the appearance into explicit material representations (_e.g._, BRDFs) and environment lighting . As the visual granularity of these inverse rendering results continues to improve, it becomes an increasingly pressing challenge to quantify and compare the performance of different methods.

Quantitative evaluation is particularly challenging for inverse rendering tasks because obtaining ground-truth data for material and lighting is prohibitively difficult in practice. Many existing works, therefore, resort to synthetic data for quantitative evaluation [30; 63; 7; 85; 88; 44; 74]. However, evaluation using synthetic data has several critical drawbacks. First, despite the constant evolution of 3D modeling and graphics rendering engines, it remains challenging to date to simulate intricate lighting effects that simply appear in the natural world, _e.g_., light bouncing inside a pitcher made of shiny metal. Second, synthetic rendering engines typically assume a perfect image formation model, omitting all the noise in a real-world imaging process, which abolishes the generalization guarantee to real-world scenarios. More fundamentally, such synthetic evaluation protocols inevitably give an advantage to methods that exploit similar assumptions that were used to generate the evaluation data.

Existing real-world evaluation benchmarks, on the other hand, are limited on two fronts. Many of them are designed primarily to evaluate geometry and novel view synthesis and only include multi-view images and 3D captures of the objects, without access to the underlying material or lighting properties [26; 28; 7]. Datasets that provide ground-truth lighting and relit images for material and relighting evaluation [29; 67], however, are typically captured in constrained environments and do not represent the rich lighting variations in the natural world.

In this work, we aim to bridge this gap in real-world inverse rendering evaluation by introducing **Stanford-ORB**, a new Real-World 3D **O**bject Inverse **R**endering **B**enchmark. To do this, we collect a new real-world dataset of 14 objects captured under 7 different in-the-wild scenes, with ground-truth 3D scans, dense multi-view images, and environment lighting. This allows us to evaluate different aspects of inverse rendering methods through a number of metrics, including shape estimation, in-the-wild relighting with ground-truth lighting, and novel view synthesis. We compare a wide range of inverse rendering methods using this benchmark, including classical optimization-based methods and learning-based methods with neural representations. All data, trained models, and the evaluation protocol will be released to facilitate future work on this topic.

## 2 Related Work

State of the Art on Inverse Rendering.Early works study the interaction of object shape, material, and lighting to estimate individual components, such as shape from shading [6; 87; 2], material acquisition [31; 32; 48; 47; 78], and lighting estimation [73; 80], or to recover reflectance and illumination assuming known shapes [36; 37]. Full-fledged inverse rendering methods aim at estimating all components simultaneously , which can be achieved with differentiable renderers [39; 13; 11]. Neural representations have recently emerged as a powerful approach in many state-of-the-art inverse rendering methods, due to their continuous nature and capacity to model complex geometry and appearance. Volumetric representations such as Neural Radiance Fields (NeRFs)  and the like [4; 7; 9; 8; 88; 54; 81] encode geometry and appearance as volumetric densities and radiance with a Multi-Layer Perceptron (MLP) network, and render images using the volume rendering equation . While permitting flexible geometry, volumetric representations often suffer from over-parameterization, resulting in artifacts like floaters.

Other surface-based representations [69; 85; 84; 44; 89; 24; 75; 64] extract surfaces as the zero level set, for instance, of a signed distance function (SDF) or an occupancy field , allowing them to efficiently model the appearance on the surface with an explicit material model, such as using bidirectional reflectance distribution functions (BRDFs). This also enables modeling more complex global illumination effects, such as self-shadows. Most of these methods focus on per-scene optimization and require dense multiple views as input. Recently, researchers have incorporated learning-based models, distilling priors from large training datasets for fast inference on limited test views [59; 25; 32; 5; 10; 76; 74; 90].

Inverse rendering tasks have also been explored at a scene level beyond single objects. For indoor scenes, existing work has created large synthetic datasets, such as OpenRooms , and trained supervised models to predict geometry, material, and spatially-varying lighting with ground-truth data [30; 71; 92]. For outdoor scenes, others have used time-lapse images or Internet photos and trained models with weak supervision from optimization [83; 82; 34]. In this work, we focus on evaluating object-centric inverse rendering tasks.

Existing Inverse Rendering Benchmarks.Evaluating inverse rendering results is challenging because collecting ground-truth data is difficult. Table 1 summarizes existing datasets for object inverse rendering evaluation. The early MIT Intrinsics dataset  provides a small dataset of intrinsic images of real objects, including albedo and shading maps obtained through polarization techniques . Bell _et al._ instead proposes to leverage human judgments on relative reflectance for evaluation. However, these datasets do not provide shape or physically-based material ground truth. Synthetic datasets [59; 32; 7; 76; 74; 16] are widely used for evaluation as ground truth, as all components can be directly exported. However, these scenes are relatively simple and do not warrant generalization to complex real-world environments. Existing real object datasets [26; 19; 29; 67] are typically captured in studio setups with constrained lighting. In-the-wild object datasets, on the other hand, typically do not capture shape and ground-truth lighting, which are crucial for disentangled material evaluation through relighting. To the best of our knowledge, Lombard _et al._[36; 47] is the only in-the-wild dataset providing ground-truth lighting, but is of a small scale compared to ours. In this work, we propose the first comprehensive real-world object inverse rendering benchmark with ground-truth scans and in-the-wild environment lighting.

## 3 Stanford-ORB: A Real-World Object Inverse Rendering Benchmark

Our goal is to create a quantitative evaluation benchmark for real-world 3D object inverse rendering tasks. The primary objective of object inverse rendering is to recover the 3D shape and surface material of the underlying object from images, where the input to the systems can be either a dense coverage of views of the object or as few as a single image, and optionally with additional assumptions, such as actively lighting. This task can be framed either as a per-scene optimization problem or as a learning-based one. In general, the fewer views available as input, the more challenging it is to recover accurate geometry and material, and hence the more the method relies on learned priors. In this work, we focus on evaluating the quality of the shape and material recovered by various inverse rendering methods with a comprehensive benchmark. To do this, we collected a new dataset of 14 common real-world objects with different materials, captured from multiple viewpoints in a diverse set of real-world environments, paired with their ground-truth 3D scans and environment lighting.

### Evaluation Benchmarks

We design a number of metrics to evaluate the recovered shape and material from three perspectives. We give an outline of these benchmarks in this section and lay out the metric definitions in Section 4.

Geometry Estimation Benchmark.To assess the quality of the 3D shape reconstructions, we provide ground-truth 3D scans of each object. These 3D scans are obtained using a high-definition 3D scanner and carefully processed into watertight meshes. We measure the quality of the geometry estimated from different methods by comparing the predicted depth maps and normal maps to those generated from the ground-truth scans, as well as directly computing the bidirectional Chamfer Distance between the predicted meshes and the scans.

Novel Scene Relighting Benchmark.Evaluating surface materials is challenging as obtaining ground-truth data is generally difficult in practice, and different methods might adopt different

   Dataset & \# Scenes & \# Objects & Real & Scene Type & Multi-view & Shape & Relit Image & Lighting \\  ShapeNet-Intrinsics  & 98 & 31K & ✗ & synthetic & ✓ & ✓ & ✓ & ✓ \\ NeRD Synthetic  & 30 & 3 & ✗ & synthetic & ✓ & ✓ & ✓ & ✓ \\ ABO  & 40K & 8K & ✗ & synthetic & ✓ & ✓ & ✓ & ✓ \\  MIT Intrinsics  & 1 & 20 & ✓ & studio & ✓ & ✗ & ✗ & ✗ \\ DTU-MVS  & 1 & 80 & ✓ & studio & ✓ & ✓ & ✗ & ✗ \\ Obijaveres  & 818K & 818K & (✓)* & studio & ✓ & ✓ & ✗ & ✗ \\ DiLiGenT-MV  & 1 & 5 & ✓ & studio & ✓ & ✓ & ✓ & ✓ \\ ReNe  & 1 & 20 & ✓ & studio & ✓ & ✗ & ✓ & ✓ \\ OpenILumination  & 155 & 64 & ✓ & studio & ✓ & ✗ & ✓ & ✓ \\ Lombard _et al._ & 5 & 6 & ✓ & in-the-wild & ✗ & ✓ & ✓ & ✓ \\ NeRD Real  & 4 & 4 & ✓ & in-the-wild & ✓ & ✗ & ✓ & ✗ \\ NeROIC  & 10 & 3 & ✓ & in-the-wild & ✓ & ✗ & ✓ & ✗ \\ Oxholm _et al._ & 3 & 4 & ✓ & in-the-wild & ✓ & ✓ & ✓ & ✓ \\ Stanford-ORB (ours) & 7 & 14 & ✓ & in-the-wild & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with Existing Object-centric Inverse Rendering Datasets. *Objaveres  consists of both synthetic objects and real scans.

material representations. Here, we propose to evaluate the quality of material recovery through the task of relighting in a _real-world_ environment unseen during training, given the _ground-truth environment lighting_, regardless of the underlying material representations. To do this, for each object, we capture multi-view image sets under multiple in-the-wild scenes, together with the ground-truth environment map without the object for each scene. To evaluate the performance of a model, we render the object using the predicted geometry and materials and the ground-truth lighting of a new scene, and compare the rendered image with the captured ground-truth image from a given viewpoint.

Novel View Synthesis Benchmark.In addition to the novel-scene relighting evaluation, we also perform an evaluation on the typical novel view synthesis task, where novel views of the object are rendered in the _same_ scene as the input but from _novel_ viewpoints, and compared to the ground-truth images. Note that this metric measures the accuracy of appearance modeling within one scene, but not necessarily the accuracy of material modeling that ensures accurate appearance in arbitrary scenes.

### Data Capture

To support these evaluations, we capture a new real-world dataset comprised of ground-truth 3D scans, multi-view images taken in the wild, and the corresponding environment lighting. The dataset contains 14 common objects captured in 7 natural scenes. For each object, we take 60 training views and 10 testing views in high dynamic range (HDR) under 3 different scenes, resulting in a total of 42 scenes. For each testing view, we also capture an HDR panorama environment map. The ground-truth shape for each object is obtained using a professional 3D scanner. Fig. 1 gives an overview of our data capture pipeline.

#### 3.2.1 Object Selection

Existing work often curates a small selection of objects for evaluation specific to the focus of their proposed methods. For instance, PhySG  evaluates on glossy objects, and NeRD  tests on objects with rougher surfaces. To establish a comprehensive benchmark, we carefully select 14 objects that cover a wide variety of both geometry and material properties. Fig. 2 gives a glimpse into the list of objects. The geometry ranges from simple cylindrical shapes to complex shapes,

Figure 1: **Data Capture Pipeline Overview. For each object, _Left_: we obtain its 3D shape using a 3D scanner and Physics-Based Rendering (PBR) materials using high-quality light box images. _Middle_: we also capture multi-view masked images in 3 different in-the-wild scenes, together with the ground-truth environment maps. _Right_: we carefully register the camera poses for all images using the scanned mesh and recovered materials, and prepare the data for the evaluation benchmarks. Credit to Maurice Sway  for the low-poly camera mesh model.**

Figure 2: **Selection of Objects. From top to bottom: _Block_, _Gnome_, _Ball_, _Car_; _Curry_, _Pepsi_, _Salt_, _Baking_, _Chips_; _Cactus_, _Pitcher_, _Grogu_, _Cup_, _Teapot_.**

and the materials cover diffuse, glossy as well as mirror-like metal surfaces. We focus primarily on non-translucent objects in this work.

#### 3.2.2 3D Shape and Appearance Capture in Studio

For each object, we obtain its ground-truth 3D shape using an EinScan Pro HD 3D Scanner. Fig. 3 illustrates the capture setup. Despite the high scanning quality in general, the scanner can struggle to reconstruct shiny and dark surfaces for some objects, and we paint these areas with a special spray designed to improve the robustness. To help with camera registration, we print geometric patterns from the HPatches dataset  and place them underneath the objects. Each object is scanned 2-3 times in different poses to cover the entire shape. All scans are aligned and processed in the ExScan Pro software  to obtain a watertight textured mesh. For computation efficiency, we reduce the number of faces in each mesh to 200,000 using the method proposed by Garland  and further smooth the surfaces using the HC Laplacian smoothing algorithm from Vollmer .

In addition to the 3D shapes, we also obtain pseudo material decomposition for each object by capturing them in a light box. These pseudo materials are used to refine relative camera poses for in-the-wild scenes as detailed in Section 3.3, and to build a set of high-performing baseline results on the relighting benchmark as shown in Table 2. As illustrated in Fig. 3, we place each object on a remote-controlled turntable inside a simple light box. The turntable rotates \(6^{}\) at a time; a bracket of three images are taken from each view at different exposure, which are fused into a high dynamic range (HDR) image for high-quality recovery using a simplified version of the algorithm in . To solve for materials, we also capture the environment map inside the light box using a 3-inch steel chrome ball, following . The chrome ball is placed in the same place as the object and reflects the environment light, which allows us to recover the environment map as detailed in Section 3.3.

#### 3.2.3 Image and Lighting Capture in the Wild

To build the relighting and novel view synthesis benchmarks, we capture dense multi-view images of each object in different real-world in-the-wild environments, together with the corresponding ground-truth environment lighting. Each object is captured in 3 different scenes (7 unique scenes in total), including both indoor and outdoor environments, resulting in a total of 36 captures.

Fig. 4 illustrates the capture setup. In each scene, we fixate the object to a small platform and move the camera around the object in a circle from various heights. We take images from approximately 70 viewpoints roughly uniformly covering 360\({}^{}\) views of the objects, including 10 test views and 60 training views, which are suitable for training most existing inverse rendering methods.

For each scene, we also capture the ground-truth environment lighting for relighting evaluation. Similar to Section 3.2.2, we reuse the chrome ball and solve for the environment maps following . Unlike in the studio setup, where lighting is highly constrained and static, real-world (, outdoor) environments are constantly changing while we capture the data. To minimize such errors, we capture one environment map per test view, forming an 'image-envmap' pair as illustrated in Fig. 4 (b). To

Figure 3: **Studio Capture Setup. (a) 3D shape scanning. \(A\): object, \(B\): hand-held EinScan Pro HD 3D Scanner, \(C\): printed patterns for camera registration, \(D\): spray for high-quality scanning, \(E\): desktop for processing. The scanned mesh is visualized in ExScan Pro  and MeshLab  on the right. (b) Light box capture setup viewed from outside and inside. \(F\): DSLR camera, \(G\): cloth cover to block light from outside, \(H\): object, \(I\): printed patterns for camera registration, \(J\): (optional) dark background for object segmentation, \(K\): remote-controlled turntable. The captured image and the chrome ball are visualized on the right.**

avoid swapping the object for the chrome ball every time (which leads to inconsistent poses), we affix the object and only move the chrome ball in front of the object in each view. The photographer will hide themselves inside a white cloth underneath the camera to avoid being captured in the images.

For better inverse rendering quality, all images are taken in \(2048 2048\) resolution and in HDR by fusing multiple exposure shots. We carefully tune the exposure values for each scene, taking into account the lighting condition and the reflections on the object surface. All cameras are calibrated with a chessboard and undistorted using standard OpenCV libraries.

### Data Processing

Next, we describe the detailed processing steps for preparing the evaluation data. In essence, we need to obtain: (1) object masks for all images, (2) environment maps solved from the chrome ball images, (3) pseudo materials for each object using the studio captures, and (4) relative camera poses across multiple views and across multiple scenes. Fig. 5 shows the overview of the pipeline, and the

Figure 4: **In-the-Wild Capture Setup. (a) Hardware for capturing. _A_: chrome ball, _B_: magenta platform for object segmentation, _C_: object, _D_: printed patterns for camera registration, _E_: DSLR camera, _F_: cloth for hiding photographer, _G_: mobile cart. (b) An example of the image-envmap pair. The environment map is solved from the reflection image on the chrome ball.**

Figure 5: **Data Processing Pipeline. _Left_: Overview of the data processing pipelines for both studio and in-the-wild captures. Three individual modules painted blue are expanded on the right. _Middle-Top_: The semi-automatically segmentation module produces object masks for all images. _Middle-Bottom_: Environment maps are solved from the chrome ball images. _Right_: Accurate camera poses are obtained from COLMAP and refined using NVDiffRec , given the scanned mesh and (for in-the-wild images) the pseudo materials optimized from light box captures.**

details of each step are laid out below. Overall, for each object, it takes roughly \(3\) hours to capture the raw data (\(2.5\) hours for the in-the-wild capture and \(0.5\) hour for studio capture). The data processing pipeline takes roughly 10 hours on one machine for each object, which can be paralleled, and an additional \(0.5\) hour of human labor to manually adjust the annotations and alignments.

Object Masks.Getting high-quality inverse rendering results requires highly accurate object masks. We make use of the state-of-the-art object segmentation model, SAM , which is trained on over a billion curated masks. Given a sparse set of around 5 query points, the model produces reasonable object masks on our images. However, we observe that the resulting masks often appear smaller than the actual object. Hence, we further refine the masks using the classic GrabCut algorithm .

Environment Maps.Given the chrome ball images, we first apply the same object masking procedure described above to segment the chrome ball. We then fit a synthetic 3D sphere to the chrome ball by optimizing its 3D location w.r.t. the camera. With a differentiable Monte-Carlo-based renderer, we optimize the environment map to fit the reflection image on the chrome ball using the single-view light estimation method proposed in .

Pseudo Material Decomposition.For the purpose of refining the in-the-wild camera poses as detailed below, we optimize a set of pseudo surface materials for each object through an inverse rendering method NVDiffRec , making use of the 3D scan and the images captured in the light box. Specifically, we first register all light box images using COLMAP [56; 57] and SuperGlue  features to obtain a set of initial camera poses. In order to fit our 3D scanned mesh to the images, we optimize a coarse mesh with NVDiffRec, and roughly align the scanned mesh to the optimized mesh manually. The ground-truth environment map is also computed from the chrome ball images. Finally, we optimize the materials with a microfacet BRDF model  using NVDiffRec, jointly refining the COLMAP poses like , given the scanned mesh and ground-truth environment map.

Camera Pose Registration.For both training and evaluation, we need accurate relative camera poses both between different viewpoints of a scene and across different scenes for relighting. Similarly to the registration of light box images above, we make use of the 3D scans and the pseudo materials previously solved from the studio captures. For each scene, we solve for initial poses using COLMAP and SuperGlue, optimize a coarse mesh with NVDiffRec, and align the scanned mesh to it. The camera poses are further refined using NVDiffRec given the scanned mesh, pseudo materials, and ground-truth lighting. The relative pose between each source and target scenes for relighting is thus obtained through the same canonical scanned mesh.

## 4 Benchmarking State-of-the-Art Methods

This section provides an overview of baseline methods2, followed by evaluation details for the three benchmarks from Section 3.1 and discussion of the results.

### Baseline Methods

Existing inverse rendering methods can be roughly categorized into

* **Material Decomposition** methods from multi-view images, such as NeRD , NeuralPIL , PhySG , InvRender , NVDiffRec  and NVDiffRecMC , which typically recovers surface materials as BRDFs and environment lighting of a scene;
* **Novel View Synthesis and 3D Reconstruction** methods from multi-view images, such as NeRF  and IDR , which focus primarily on reconstructing 3D geometry and free-viewpoint appearance synthesis of a scene, without explicit material decomposition;
* **Single-View Prediction** methods, including SIRFS  and SI-SVBRDF , which focus primarily on predicting intrinsic images, _e.g._, depth, normal, albedo, and shading maps, and potentially full 3D reconstructions, from a single image.

Note that these methods can be either per-scene optimization-based, learning-based, or hybrid, where priors learned from large training datasets can be exploited for test time inference with limited views.

All methods assume access to images and camera poses during optimization or training. Inverse rendering methods that additionally assume access to the ground truth shape [36; 37] or ground truth lighting [48; 47; 78] are not included in this benchmark.

Furthermore, we design two strong baselines, using NVDiffRec  and NVDiffRecMC  with ground-truth 3D scans and pseudo materials optimized from light-box captures. As shown in Table 2, these two baselines achieve the best results in relighting as expected. For all methods, we use their public codebase and default configurations in all experiments, and feed in HDR images if possible by default. All models are trained and tested with \(512 512\) images, except SI-SVBRDF  which is trained and tested with \(256 256\) images (results are upsampled to \(512\) for consistency).

### Tasks

We consider three inverse rendering tasks: geometry estimation, novel scene relighting, and novel view synthesis.

Geometry Estimation.We evaluate geometry reconstruction quality by comparing the rendered depth maps, normal maps, and predicted 3D meshes to the ground truth obtained from 3D scans. For per-scene multi-view optimization methods, we evaluate the predictions on held-out test views of the same scene. For single-image prediction methods, we simply run the pre-trained model  or optimization  on all test images and evaluate the results. Since neither of [30; 2] predicts novel views or full 3D shapes, we only evaluate their depth and normal predictions and omit the other two benchmarks.

To evaluate depth maps, we use the Scale-Invariant Mean Squared Error (**SI-MSE**)  between predicted and ground-truth images masked with ground-truth object masks: \(_{}_{k=1}^{K}\|(I_{,k}- I _{,k}) M_{,k}\|_{2}^{2}\), where \(\) denotes the Hadamard product, and \(\) is the scale optimized for all \(K\) testing views of each scene. For normal evaluation, we compute the **Cosine Distance** between the predicted and ground-truth masked normal maps. For direct shape evaluation, we follow DeepSDF  to compute the bi-directional **Chamfer Distance** between \(30,000\) points sampled from the predicted mesh surface and ground-truth mesh vertices. For methods that do not produce explicit meshes [79; 43; 9; 85; 7; 88; 75], we run the marching cube algorithm  for mesh extraction. Following IDR , for all methods, we take the largest connected component as the final mesh prediction. In rare cases where a method fails to converge during the training, we simply sample \(30\)k dummy points at the origin.

Novel Scene Relighting.We evaluate the quality of material decomposition through the task of relighting under _novel_ scenes with the ground-truth environment lighting. To do this, we take

Figure 6: **Visualization of the Benchmark Comparisons. Metrics on geometry are visualized on the left, and metrics on relighting and novel view synthesis are on the top right and the bottom right. The black line on each bar indicates the standard deviation of the scores across all scenes.**

decomposed results on one training scene and render them in the other 2 scenes with the respective ground-truth lighting. Note that IDR, NeRF, and Neural-PIL use implicit lighting or appearance representations and cannot directly support relighting. Hence, we do not evaluate relighting for these methods.

We adopt three widely-used metrics to compare the rendered relit images against the ground-truth images: Peak Signal-to-Noise Ratio (**PSNR**), Structural Similarity Index Measure (**SSIM**) , and Learned Perceptual Image Patch Similarity (**LPIPS**) . Since there is an inherent scale ambiguity in material and lighting decomposition, we follow PhySG  and adapt all metrics to be scale-invariant by rescaling the predictions with an optimized scale for each RGB channel. Specifically, given the ground-truth image \(I_{}\) and the predicted image \(I_{}\), we rescale the prediction to \(I^{}_{}=\{_{s}(\|I^{i}_{}-sI^{i}_{ }\|^{2}) I^{i}_{}\}_{i=,,}\) before computing the metrics.

We report PSNR in both HDR and LDR (after the standard sRGB tone mapping), denoted as PSNR-H and PSNR-L, respectively. The HDR values are clamped to a maximum of \(4\) to suppress the effects of over-saturated pixels (_e.g._, mirror reflection of the sun). SSIM and LPIPS are computed with LDR values. In cases where the baselines fail to converge during training, we use a constant gray image to represent the prediction.

Novel View Synthesis.We also report the typical novel view synthesis metrics on the _same_ scene as training. Specifically, for each trained model, we render the object from test views under the same scene and compare the rendered images with ground-truth test images. The two single-view inverse rendering methods [30; 2] do not explicitly perform view synthesis, and therefore are not evaluated on this task. We use the same metrics as in relighting (without rescaling), namely PSNR-H, PSNR-L, SSIM, and LPIPS.

### Results

We briefly summarize the key findings of the results in Table 2 and Fig. 6. For the tasks of geometry reconstruction and novel view synthesis (NVS), IDR  outperforms all other baselines, including NeRF . A potential reason is that IDR focuses on surface reconstruction and represents the geometry using a signed distance function (SDF) , hence particularly advantageous in modeling these object-centric scenes. While NeRF is powerful in modeling large-scale scenes, it fails to reconstruct consistent geometry and texture details for certain scenes (see Fig. 3 in the Appendix). Inverse rendering methods that extend from IDR ([85; 75]) or NeRF ([9; 7; 88]) for further decomposition of lighting and surface materials tend to result in a degradation of performance in geometry reconstruction and NVS. In comparison, NVDiffRec  and NVDiffRecMC  recover better geometry, suggesting the advantages of the underlying DMTet -based shape representation over SDF or NeRF. NVDiffRecMC also significantly outperforms NVDiffRec and other material decomposition baselines on NVS thanks to the differentiable Monte Carlo-based (MC) renderer.

    &  &  &  \\   & Depth\(\) & Normal\(\) & Shape\(\) & PSNR-H\(\) & PSNR-L\(\) & SSIM\(\) & LPIPS\(\) & PSNR-H\(\) & PSNR-L\(\) & SSIM\(\) & LPIPS\(\) \\  NVDiffRecMC \(\) &  & \(\) & \(32.28\) & \(0.974\) & \(\) & &  \\ NVDiffRec \(\) &  & \(24.93\) & \(\) & \(\) & \(\) & &  \\  IDR  & \(0.35\) & \(0.05\) & \(\) & & N/A & & \(\) & \(\) & \(\) & \(\) \\ NeRF  & \(2.19\) & \(0.62\) & \(62.05\) & & N/A & & \(26.31\) & \(33.59\) & \(0.968\) & \(0.044\) \\  Neural-PIL  & \(0.86\) & \(0.29\) & \(4.14\) & & N/A & & \(25.79\) & \(33.35\) & \(0.963\) & \(0.051\) \\ PhySG  & \(1.90\) & \(0.17\) & \(9.28\) & \(21.81\) & \(28.11\) & \(0.960\) & \(0.055\) & \(24.24\) & \(32.15\) & \(0.974\) & \(0.047\) \\ NVDiffRec  & \(\) & \(0.06\) & \(0.62\) & \(22.91\) & \(29.72\) & \(0.963\) & \(0.039\) & \(21.94\) & \(28.44\) & \(0.969\) & \(0.030\) \\ NeRD  & \(1.39\) & \(0.28\) & \(13.70\) & \(23.29\) & \(29.65\) & \(0.957\) & \(0.059\) & \(25.83\) & \(32.61\) & \(0.963\) & \(0.054\) \\ NeRFactor  & \(0.87\) & \(0.29\) & \(9.53\) & \(23.54\) & \(30.38\) & \(0.969\) & \(0.048\) & \(26.06\) & \(33.47\) & \(0.973\) & \(0.046\) \\ InFender  & \(0.59\) & \(0.06\) & \(0.44\) & \(23.76\) & \(30.83\) & \(0.970\) & \(0.046\) & \(25.91\) & \(34.01\) & \(0.977\) & \(0.042\) \\ NVDiffRecMC  & \(0.32\) & \(\) & \(0.51\) & \(24.43\) & \(31.60\) & \(0.972\) & \(0.036\) & \(28.03\) & \(36.40\) & \(0.982\) & \(0.028\) \\  SI-SVBRDF  & \(81.48\) & \(0.29\) & N/A & & N/A & & & N/A & \\ SIRFS  & N/A & \(0.59\) & N/A & & N/A & & & N/A & \\   

Table 2: **Benchmark Comparison of Existing Methods. †denotes models trained with the ground-truth 3D scans and pseudo materials optimized from light-box captures. Depth SI-MSE \( 10^{-3}\). Shape Chamfer distance \( 10^{-3}\).**For the task of relighting, the two most recent inverse rendering methods [75; 24] achieve better performance than the earlier ones. In particular, NVDiffRecMC again improves upon its precursor NVDiffRec and consistently outperforms all other inverse rendering baselines across all metrics, further confirming the effectiveness of the MC-based renderer. Finally, as expected, the two baselines that have access to ground truth 3D scans and pseudo-materials (marked with \({}^{}\)) perform significantly better than methods without this information, and this gap suggests room for improvement for all existing methods. Here, the performance gain from using the MC renderer seems negligible.

Please refer to the Appendix for more results. In particular, we additionally evaluate and compare the albedo prediction quality of various methods using the albedo maps optimized from the studio capture data as pseudo ground-truth. The complete comparison results are summarized in Table 1 of the Appendix, and a few visual examples are provided in Fig. 3 of the Appendix.

## 5 Conclusion

We present Stanford-ORB, the first real-world evaluation benchmark for object inverse rendering tasks including shape reconstruction, object relighting, and novel view synthesis under in-the-wild environments. To build this benchmark, we capture a new dataset of 14 real-world objects under various in-the-wild scenes with ground-truth 3D scans, multi-view images, and ground-truth environment lighting. Using the dataset, we evaluate and compare a wide range of state-of-the-art inverse rendering methods. All data collected in this work are focused on daily objects and do not contain any personal information. Please refer to the website for more information on the dataset: https://stanfordorb.github.io/.

Acknowledgments.This work was in part supported by NSF CCRI #2120095, RI #2211258, ONR MURI N00014-22-1-2740, the Stanford Institute for Human-Centered AI (HAI), Adobe, Amazon, Ford, and Google.