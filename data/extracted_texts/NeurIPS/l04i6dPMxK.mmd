# Bandits with Abstention under Expert Advice

Stephen Pasteris\({}^{1*}\) Alberto Rumi\({}^{2,3}\) Maximilian Thiessen\({}^{4}\) Shota Saito\({}^{5}\)

**Atsushi Miyauchi\({}^{3}\) Fabio Vitale\({}^{3}\) Mark Herbster\({}^{5}\)**

\({}^{1}\)The Alan Turing Institute \({}^{2}\)University of Milan \({}^{3}\)CENTAI Institute

\({}^{4}\)TU Wien \({}^{5}\)University College London

\({}^{*}\)spasteris@turing.ac.uk

###### Abstract

We study the classic problem of prediction with expert advice under bandit feedback. Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the _confidence-rated bandits with abstentions_ (CBA) algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. Our problem can be construed as the aggregation of confidence-rated predictors, with the learner having the option to abstain from play. We are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving the previous bounds of SpecialistExp (treating abstention as another action). We discuss how CBA can be applied to the problem of adversarial contextual bandits with the option of abstaining from selecting any action. We are able to leverage a wide range of inductive biases, outperforming previous approaches both theoretically and in preliminary experimental analysis. Additionally, we achieve a reduction in runtime from quadratic to almost linear in the number of contexts for the specific case of metric space contexts.

## 1 Introduction

We study the classic problem of prediction with expert advice under bandit feedback. The problem is structured as a sequence of trials. During each trial, each expert recommends a probability distribution over the set of possible actions. The learner then selects an action and observes and incurs the (potentially negative) reward associated with that action on that particular trial. In practical applications, errors often lead to severe consequences, and consistently making predictions is neither safe nor economically practical. For this reason, the abstention option has gained a lot of interest in the literature, both in the batch and online setting (Chow, 1957, 1970; Hendrickx et al., 2021; Cortes et al., 2018). Similarly to previous works, this paper is based on the assumption that one of the actions always has zero reward: such an action is equivalent to an abstention of the learner from play. Besides the rewards being bounded, we make no additional assumptions regarding how the rewards or expert predictions are generated. In this paper, we present an efficient algorithm CBA (Confidence-rated \(}\)andits with \(}\)) which exploits the abstention action to get reward bounds that can be dramatically higher than those of Exp4 (Auer et al., 2002). In the worst case, our reward bound essentially matches that of Exp4 so that CBA can be seen as a strict improvement, since the time-complexities of the two algorithms are, up to a factor logarithmic in the time horizon, identical in the general case.

Our problem can also be seen as that of aggregating _confidence-rated predictors_(Blum and Mansour, 2007; Gaillard et al., 2014; Luo and Schapire, 2015) when the learner has the option of abstaining from taking actions. When the problem is phrased in this way, at the start of each trial, each predictor recommends a probability distribution over the actions (which now may not include an action withzero reward) but with a confidence rating. A low confidence rating can mean that either the predictor thinks that all actions are bad (so that the learner should abstain) or simply does not know which action is the best. Previous works on confidence-rated experts measure the performance of their algorithm in terms of the sum of _scaled_ per-trial rewards. In contrast to previous algorithms, our approach allows for the derivation of bounds on the expected cumulative reward of CBA.

This formulation enables us to extend our work to the problem of adversarial contextual bandits with the abstention option, which has not been studied before. Previous work has considered the abstention option in the standard (context-free) adversarial bandit setting or in stochastic settings (Cortes et al., 2018, 2020; Neu and Zhivotovskiy, 2020), but not in the contextual and adversarial case. Moreover, their results and methods cannot be applied to confidence-rated predictors. To get more intuition on this setup, we can think of any deterministic policy that maps contexts into actions. Any such policy can be viewed as a classifier, with _foreground_ classes associated with each action and a _background_ class associated with abstaining. Our learning bias is represented by a set of information we refer to as the _basis_, which we formally define later. It encodes contextual structural assumptions that hold exclusively for the foreground classes and are provided to the algorithm a priori. A particular type of basis is generated by a set of potential clusters that can overlap. Alternatively, a basis can also be created using balls generated by any kind of distance function, which groups contexts believed to be close together. For this latter family of basis, we can also achieve a significant speedup in the per-trial time complexity of CBA. This result is very different (and incomparable) to other results about adversarial bandits in metric spaces (Pasteris et al., 2023, 2020).

### Additional related work

The non-stochastic multi-armed bandit problem, initially introduced by Auer et al. (2002), has been a subject of significant research interest. Auer et al. (2002) also considered the multi-armed bandit problem with expert advice, introducing the Exp4 algorithm. Exp4 evolved the field of multi-armed bandits to encompass more complex scenarios, particularly the contextual bandit (Lattimore and Szepesvari, 2020). Contextual bandits are an extension of the classical multi-armed bandit framework, where an agent makes a sequence of decisions while taking into account contextual information. Our work is also related to the multi-class classification with bandit feedback, called _weak reinforcement_(Auer and Long, 1999). An action in our bandit setting corresponds to a class in the multi-class classification framework.

As discussed in the introduction, a key aspect of this work is the option to abstain from making any decision. In the batch setting (Chow, 1957, 1970), this option is usually referred to as "rejection". These works study whether to use or reject a specific model prediction based on specific requests (see Hendrickx et al. (2021) for a survey). In online learning, "rejection" can be the possibility of abstention by the learner. These works usually rely on a cost associated with the abstention action. Neu and Zhivotovskiy (2020) studied the magnitude of the cost associated with abstention in an expert setting with bounded losses. They state that if the cost is lower than half of the amplitude of the interval of the loss, it is possible to derive bounds that are independent of the time. In Cortes et al. (2018), a non-contextual and partial information setting with the option of abstention is studied. The sequel model (Cortes et al., 2020) regards this model as a special case of their stochastic feedback graph model. Schreuder and Chzhen (2021) studied the fairness setting when using the option of abstaining as it may lead to discriminatory predictions.

One specific scenario where prior algorithms can establish cumulative reward bounds is as follows: on any given trial, the predictors are _specialists_(Freund et al., 1997), having either full confidence (a.k.a. _awake_) or no confidence (a.k.a. _asleep_). The SpecialistExp algorithm by Herbster et al. (2021), a bandit version of the standard specialist algorithm, achieves regret bounds with respect to any subset of specialists where exactly one specialist is awake on each trial. We differ from this work as abstention is an algorithmic choice. Instead of sleeping in the rounds where the specialist is not active, the specialist will vote for abstention, which is a proper action of our algorithm. In Section 5.2, we present an illustrative problem involving learning balls in a space equipped with a metric. This example demonstrates our capability to significantly improve on SpecialistExp. For this problem, we also present subroutines that significantly speed up CBA.

Problem formulation and notation

We consider the classic problem of prediction with expert advice under bandit feedback. In this problem we have _\(K+1\) actions_, _\(E\) experts_, and _\(T\) trials_. On each trial \(t\):

1. Each expert suggests, to the learner, a probability distribution over the \(K+1\) actions.
2. The learner selects an action \(a_{t}\).
3. The reward incurred by action \(a_{t}\) on trial \(t\) (which is in \([-1,1]\)) is revealed to the learner.

We note that the experts' suggestions and the rewards (associated with each action) are chosen a-priori and hence do not depend on the learner's actions. The aim of the learner is to maximize the cumulative reward obtained by its selected actions. As discussed in Section 1, we consider the case in which there is an action (the abstention action) that incurs zero reward on every trial.

We denote our action set by \([K]\{\}\) where \(\) is the abstention action. For each trial \(t[T]\) we define the vector \(_{t}[-1,1]^{K}\) such that for all \(a[K]\,,r_{t,a}\) is the reward obtained by action \(a\) on trial \(t\). Moreover, we define \(r_{t,}:=0\) which is the reward of the abstention action \(\).

It will be useful for us to represent probability distributions over the actions by vectors in the set:

\[:=\{^{K}\,|\,\|\|_{1} 1\}\,.\]

Any vector \(\) represents the probability distribution over actions which assigns, for all \(a[K]\), a probability of \(s_{a}\) to action \(a\), and assigns a probability of \(1-\|\|_{1}\) to the abstention action \(\), where \(\|\|_{1}\) denotes 1-norm of \(\). We write \(a\) to represent that action \(a\) is drawn from the probability distribution \(\). We will refer to the elements of the set \(\) as _stochastic actions_.

A _policy_ is any element of \(^{T}\) (noting that any such policy is a matrix in \(^{T K}\)). Any policy \(^{T}\) defines a stochastic sequence of actions: on every trial \(t[T]\) an action \(a[K]\{\}\) being drawn as \(a_{t}\). Note that if the learner plays according to a policy \(^{T}\), then on each trial \(t\) it obtains an expected reward of \(_{t}_{t}\), where the operator \(\) denotes the dot product. Note that each expert is equivalent to a policy. Thus, for all \(i[E]\) we denote the \(i\)-th expert by \(^{i}^{T}\). Hence, at the start of each trial \(t[T]\), the learner views the sequence \(^{i}_{t}\,|\,i[E]\).

We can also view the experts as _confidence-rated predictors_ over the set \([K]\): for each \(i[E]\) and \(t[T]\), the vector \(^{i}_{t}\) can be viewed as suggesting the probability distribution \(^{i}_{t}/\|^{i}_{t}\|_{1}\) over \([K]\), but with confidence \(\|^{i}_{t}\|_{1}\). We denote this confidence by \(c_{t,i}:=\|^{i}_{t}\|_{1}\) and write \(_{t}:=(c_{t,1},,c_{t,E})\).

In this work, we will refer to the _unnormalized relative entropy_ defined by:

\[(,):=_{i[E]}u_{i}(}{v_{i}}) -\|\|_{1}+\|\|_{1}\]

for any \(,^{E}_{+}\). We will also use the Iverson bracket notation \([\![]\!]\) as the indicator function, meaning that it is equal to \(1\) if Pred is true, and \(0\) otherwise. All the proofs are in the Appendix.

## 3 Main result

Our main result is represented by a bound on the cumulative reward of our algorithm CBA. We note that any _weight_ vector \(^{E}_{+}\) induces a matrix \(()^{T K}_{+}\) defined by

\[():=_{i[E]}u_{i}^{i},\]

which is the linear combination of the experts with coefficients given by \(\). However, only some of such linear combinations generate valid policies. Thus, we define

\[:=\{^{E}_{+}\,|\,()^ {T}\}\]

as the set of all weight vectors that generate valid policies. Particularly, note that \(\) if and only if, on every trial \(t\), the weighted sum of the confidences \(_{t}\) is no greater than one. Given some \(\), we define

\[():=_{t[T]}_{t}_{t}()\,,\]which would be the expected cumulative reward of the learner if it was to follow the policy \(()\). We point out that the learner does not know \(\) or the function \(\) a-priori.

The following theorem (proved in Appendix A) allows us to bound the regret of CBA with respect to any valid linear combination \(\) of experts.

**Theorem 3.1**.: CBA _takes parameters \((0,1)\) and \(_{1}_{+}^{E}\). For any \(\) the expected cumulative reward of CBA is bounded below by:_

\[_{t[T]}[r_{t,a_{t}}][()]-,_{1})}{}-(12K+2)T\,,\]

_where the expectations are with respect to the randomization of CBA's strategy. The per-trial time complexity of CBA is in \((KE)\)._

We now compare our bound to those of previous algorithms. Firstly, Exp4 can only achieve bounds relative to a \(\) with \(\|\|_{1}=1\), in which case it essentially matches our bound but with \(12K+2\) replaced by \(8K+8\). Hence, for any \(\) the Exp4 bound essentially replaces the term \(()\) in our bound by \(()/\|\|_{1}\). Note that \(\|\|_{1}\) could be as high as the number of experts which implies we can dramatically outperform Exp41.

Secondly, when viewing our experts as confidence-rated predictors, we note that previous algorithms for this setting only give bounds on a weighted sum of the per-trial rewards where the weight on each trial is \(_{t}\) for some \(\). This is only a cumulative reward bound when \(_{t}=1\) for all \(t[T]\), and finding such a \(\) is typically impossible. When there does exist \(\) that satisfies this constraint, the reward relative to \(\) is essentially the same as for us (Blum and Mansour, 2007). However, there will often be another value of \(\) that will give us a much better bound, as we show in Section 5.2.

## 4 The CBA algorithm

The CBA algorithm is given in Algorithm 1. In this section, we describe its derivation via a modification of the classic _mirror descent_ algorithm.

Our modification of mirror descent is based on the following mathematical objects. For all \(t[T]\) we first define:

\[_{t}:=\{_{+}^{E}|\,\|_{t}()\|_{1}  1\}\,,\]

which is the set of all weight vectors that give rise to linear combinations producing valid stochastic actions at trial \(t\). Given some \(t[T]\), we define our _objective function_\(_{t}:_{t}[-1,1]\) as

\[_{t}():=_{t}() _{t}.\]

Like mirror descent, CBA maintains, on each trial \(t[T]\), a weight vector \(_{t}_{+}^{E}\). However, unlike mirror descent on the simplex, we do not keep \(_{t}\) normalized, but we will instead project it into \(_{t}\) at the start of trial \(t\), producing a vector \(}_{t}\). Also, unlike mirror descent, CBA does not use the actual gradient (which it does not know) of \(_{t}\) at \(}_{t}\), but (inspired by the Exp3 algorithm) uses an unbiased estimator instead. Specifically, on each trial \(t[T]\), CBA does the following:

1. Set \(}_{t}*{argmin}_{_{t}} (,_{t})\).
2. Randomly construct a vector \(_{t}^{E}\) such that \([_{t}]=_{t}(}_{t})\).
3. Set \(_{t+1}*{argmin}_{_{+}^{E}}( _{t}(}_{t}-)+(,}_{t }))\).

This naturally raises two questions: how is \(a_{t}\) selected and how is \(_{t}\) constructed? On each trial \(t[T]\) we define

\[_{t}:=_{i[E]}_{t,i}_{t}^{i}\,,\]

which is the stochastic action generated by the linear combination \(}_{t}\), and select \(a_{t}_{t}\). Note that:

\[[r_{t,a_{t}}]=_{t}(}_{t})\,,\] (1)which confirms that \(_{t}\) is our objective function at trial \(t\). Once \(r_{t,a_{t}}\) is revealed to us we can proceed to construct the gradient estimator \(_{t}\). It is important that we construct this estimator in a specific way. Inspired by Exp4 we first define a reward estimator \(}_{t}\) such that for all \(a[K]\) we have:

\[_{t,a}:=1- a=a_{t}(1-r_{t,a_{t}})/s_{t,a_{t}}\,.\]

This reward estimate is unbiased as:

\[[_{t,a}]=1-[a=a_{t}](1-r_{t,a})/s_{t,a}=r_{t,a}\,.\]

We then define, for all \(i[E]\), the component:

\[g_{t,i}:=_{t}^{i}}_{t}\,.\]

Note that for all \(i[E]\) we have:

\[[g_{t,i}]=_{t}^{i}[}_{t}]=_ {t}^{i}_{t}=_{i}_{t}(}_{t})\]

so that \([_{t}]=_{t}(}_{t})\) as required.

Now that we defined the process by which CBA operates we must show how to compute \(}_{t}\) and \(_{t+1}\). First we show how to compute \(}_{t}\) from \(_{t}\). If \(\|_{t}\|_{1} 1\) it holds that \(_{t}_{t}\) so we immediately have \(}_{t}=_{t}\). Otherwise we must find \(}_{t}_{+}^{E}\) that minimizes \((}_{t},_{t})\) subject to the constraint:

\[_{i[E]}}_{t,i}c_{t,i}=1\,,\]

which is equivalent to the constraint that \(\|(}_{t})\|_{1}=1\). Hence, by Lagrange's theorem there exists \(\) such that:

\[_{}_{t}}(}_{t},_{t})+ _{i[E]}_{t,i}c_{t,i}=0\]

which is solved by setting, for all \(i[E]\,\):

\[_{t,i}:=w_{t,i}(- c_{t,i})\,.\]The constraint is then satisfied if \(\) is such that:

\[_{i[E]}c_{t,i}w_{t,i}(- c_{t,i})=1\,.\]

Since this function is monotonic decreasing, \(\) can be found by interval bisection. For this computation step, we treat our numerical precision as a constant in our time complexity. In Appendix A.1, we show that, even if the numerical precision is unbounded, we incur a time complexity equal to that of Exp4, up to a factor logarithmic in \(T\), adding only 1 to the regret.

Turning to the computation of \(_{t+1}\,,\) since it is unconstrained it is found by the equation:

\[_{_{t+1}}(_{t}_{t+1}+ ^{-1}(_{t+1},}_{t}))=0\,.\]

which is solved by setting, for all \(i[E]\,\):

\[w_{(t+1),i}:=_{t,i}( g_{t,i})\,.\] (2)

## 5 Adversarial contextual bandits with abstention

One main application of CBA is in the problem of adversarial contextual bandits with a finite context set. In this problem, we have a finite set of _contexts_\(\). A-priori nature selects a sequence:

\[(x_{t},_{t})[-1,1]^{K}\,|\,t[T] \,,\]

but does not reveal it to the learner. For all \(t[T]\) we define \(r_{t,}:=0\). On each trial \(t[T]\) the following happens:

1. The context \(x_{t}\) is revealed to the learner.
2. The learner selects an action \(a_{t}[K]\{\}\).
3. The learner sees and incurs reward \(r_{t,a_{t}}[-1,1]\).

We will assume that we are given, a-priori, a set \( 2^{}\) that we call the _basis_. We call each element of \(\) a _basis element_ (which is a set of contexts). We will later introduce various potential bases, determined by the nature of the context's structure: points within a metric space, nodes within a graph, and beyond. Importantly, our method is capable of accommodating any type of basis and, thus, any potential inductive bias that might be present in the data.

Given our basis we run our algorithm CBA with each expert corresponding to a pair \((B,k)[K]\). The expert corresponding to each pair \((B,k)\) will deterministically choose action \(k\) when the current context \(x_{t}\) is in \(B\), and abstain otherwise.

**Corollary 5.1**.: _Given any basis \(\) of cardinality \(N\) and any \(M\) we can implement CBA such that for any sequence of disjoint basis elements \( B_{j}\,|\,j[M]\) with corresponding actions \( b_{j}[K]\,|\,j[M]\) we have:_

\[_{t[T]}[r_{t,a_{t}}]_{t[T]}_{j[M]}[\![x_{t } B_{j}]\!]r_{t,b_{j}}-\,.\]

_The per-trial time complexity of this implementation of CBA is in \((KN)\)._

Proof.: The choice of experts for CBA that leads to Corollary 5.1 is defined by the set of pairs so that \(E=NK\) and for each \(B\) and action \(a[K]\) there exists an unique \(i[E]\) such that for all \(t[T]\) and \(b[K]\) we have:

\[e_{t,b}^{i}:=[\![x_{t} B]\!][\![b=a]\!]\,.\]

By choosing \(w_{1,i}:=M/NK\) for all \(i[E]\,\), and choosing

\[:=(M(N)/(6K+1)T)^{-1/2}\,,\]

Theorem 3.1 implies the reward bound in Corollary 5.1. The per-trial time complexity of a direct implementation of CBA for this set of experts would be \((KN)\)We briefly comment on the term:

\[_{j[M]}[\![x_{t} B_{j}]\!]r_{t,b_{j}}\;,\]

that appears in the theorem statement. If \(x_{t}\) does not belong to any of the sets in \( B_{j}\,|\,j[M]\) then this term is equal to zero (which is the reward of abstaining). Otherwise, since the sets are disjoint, \(x_{t}\) belongs to exactly one of them and the term is equal to the reward induced by the action that corresponds to that set. In other words, the total cumulative reward is bounded relative to that of the policy that abstains whenever \(x_{t}\) is outside the union of the sets and otherwise selects the action corresponding to the set that \(x_{t}\) lies in.

Note the vast improvement of our reward bound over that of SpecialistExp with abstention as one of the actions. Let's assume our context set is a metric space and our basis is the set of all balls. In order to get a reward bound for SpecialistExp, the sets in which the specialists are awake must partition the set \(\). This means that we must add to our \(M\) balls a disjoint covering (by balls) of the complement of the union of the original \(M\) balls. Note that the added balls correspond to the sets in which the specialists predicting the abstention action are awake. Typically this would require a huge number of balls so that the total number of specialists is huge (much larger than \(M\)); this huge number of specialists essentially replaces the term \(M\) in our reward bound (we illustrate an example in Figure 1).

Furthermore, in Appendix D, we show that the same implementation of CBA is capable of learning a weighted set of _overlapping_ basis elements, as long as the sum of the weights of the basis elements covering any context is bounded above by one, which SpecialistExp cannot do in general.

As we will see below, the practical bases we propose have a moderate size of \(||=(||^{2})\) leading to a per-step runtime of \((K||^{2})\) for CBA in this contextual bandit problem. In Section 5.2, we show how to significantly improve the runtime for a broad family of bases.

### A lower bound

In this section, we show that CBA is, up to an \(((||))\) factor, essentially best possible on this contextual bandit problem:

**Proposition 5.2**.: _Take any learning algorithm. Given any basis \(\) and any \(M\), for any sequence of disjoint basis elements \( B_{j}\,|\,j[M]\) there exists a sequence of corresponding actions \( b_{j}[K]\,|\,j[M]\) such that an adversary can force:_

\[_{t[T]}_{j[M]}[\![x_{t}_{j}]\!]r_{t,b_{j}}-_{ t[T]}[r_{t,a_{t}}]()\;.\]

Figure 1: Illustrative example of abstention where we cover the foreground and background classes with metric balls. We consider two clusters (blue and orange) as the foreground and one background class (white), using the shortest path \(d_{}\) metric. Using abstention, we can cover two clusters with one ball for each and abstain the background with no balls required (Fig. 1(a)). In contrast, if we treat the background class as another class, it would require significantly more balls to cover the background class, as seen by the 10 gray balls in Fig. 1(b). If the number of balls to cover significantly increases like in this case, the bound involving the number of balls also gets significantly worse.

### Efficient learning with balls

In practice we can often quantify the similarity between any pair of contexts. That is, the contexts form a metric space, equipped with a _distance_ function \(d:_{+}\) known to the learner a-priori. For example, contexts could have feature vectors in \(^{m}\) (and the metric is the standard Euclidean distance or cosine similarity) or be nodes in a graph with the metric given by the shortest-path distance. A natural basis for this situation is the set of metric _balls_. Specifically, a ball is any set \(B\) in which there exists some \(x\) and \(_{+}\) with:

\[B=\{z\,|\,d(x,z)\}.\]

For this broad family of bases2 we can achieve the following speed-up, relying on a a sophisticated data structure based on binary trees.

**Theorem 5.3**.: _Let \(N:=||\). Given any \(M\) we can implement_ CBA _such that for any sequence of disjoint balls \( B_{j}\,|\,j[M]\) with corresponding actions \( b_{j}[K]\,|\,j[M]\) we have:_

\[_{t[T]}[r_{t,a_{t}}]_{t[T]}_{j[M]}[\![x_{t } B_{j}]\!]r_{t,b_{j}}-\,.\]

_The per-trial time complexity of this implementation of_ CBA _is in \((KN(N))\)._

As there are at most \((N^{2})\) metric balls, this improves the runtime of the direct CBA implementation from \((KN^{2})\) to \((KN(N))\), that is almost linear per step. All the details are in Appendix B.

## 6 Experiments

This section conducts preliminary experiments, the code is available at GitHub3. We evaluate our method to compare existing algorithms using graph data, since it is common to consider graph structures under the confidence-rated expert setting (Cesa-Bianchi et al., 2013; Herbster et al., 2021). As mentioned above, the bases used in our algorithm can be constructed arbitrarily, allowing to encompass different inductive biases based on applications. Thus, we consider some representative bases used on learning tasks on graphs before, each leading to different inductive priors on the contexts. We provide a short description of the bases here and refer to Appendix E for more details.

Figure 2: Results regarding the number of mistakes over time, the four main settings are presented from left to right: the Stochastic Block Model, Gaussian graph, Cora graph and LastFM Asia graph. In this context, D1, D2, and D-INF represent the \(p\)-norm bases, LVC represents the community detection basis, and INT represents the interval basis. The baselines, EXP3 for each context, Contextual Bandit with similarity, and GABA-II, are denoted as EXP3, CBSim, and GABA, respectively, and are represented with dashed lines. All the figures display the data with 95% confidence intervals over 20 runs, calculated using the standard error multiplied by the \(z\)-score 1.96.

**Effective \(p\)-resistance basis \(d_{p}\)**: Balls given by the metric

\[d_{p}(i,j):=(_{^{N}\\ u_{i}-u_{j}=1}_{s,t V}|u_{s}-u_{t}|^{p})^{-1/p}.\]

We use \(d_{1}\), \(d_{2}\), and \(d_{}\)(Herbster and Lever, 2009).

**Louvain method basis (LVC)**: Communities returned by the Louvain method (Blondel et al., 2008), processed by the greedy peeling algorithm (Lanciano et al., 2024).

**Geodesic intervals basis (INT)**: All sets of the form \(I(x,y):=\{z z\}\) for all \(x,y\)(Pelayo, 2013; Thiessen and Gartner, 2021).

Let \(N\) be the cardinality of \(||\). For all three basis types, we immediately get an \((KN^{2})\) runtime per step of CBA as there are \((N^{2})\) basis elements. Moreover, for \(d_{p}\) balls and the LVC basis we can use the more efficient \((KN N)\) implementation through Theorem 5.3. We empirically evaluate our approach in the context of online multi-class node classification on a given graph with bandit feedback. At each time step, the algorithm is presented with a node chosen uniformly at random and must either predict an action from the set of possible actions \([K]\) or abstain. The node can accept (resulting in a positive reward) or reject (resulting in a negative reward) the suggestion based on its preferred class with a certain probability. In a real-world application, this models a scenario where each user has a category preference (such as music genre or interest). When the item we decide to present matches their interest, there is a high probability of receiving a reward.

We compare our approach CBA using each of these bases on real-world and artificial graphs against the following baselines: an implementation of ContextualBandit from Slivkins (2011), the GABA-II algorithm proposed by Herbster et al. (2021), and an EXP3 instance for each data point. We use the following graphs for evaluation.

**Stochastic block model.** We use an established synthetic graph, _stochastic block model_(Holland et al., 1983). This graph is generated by spawning an arbitrary number of disjoint cliques representing the foreground classes. Then an arbitrary number of background points are generated and connected to every possible point with a low probability. Figure 2(a) are displayed the results for the case of \(F=160\) nodes for each foreground class and \(B=480\) nodes for the background class. Connecting each node of the background class with a probability of \(1/\).

**Gaussian graph.** The points on this graph are generated in a two-dimensional space using five different Gaussian distributions with zero mean. Four of them are positioned at the corners of the unit square, representing the foreground classes and having a relatively low standard deviation. Meanwhile, the fifth distribution, representing the background class, is centered within the square and is characterized by a larger standard deviation. The points are linked in a \(k\)-nearest neighbors graph. In Figure 2(b) are displayed the results for 160 nodes for each foreground class and a standard deviation of 0.2, 480 nodes for the background class with a standard deviation of 1.75, along with a 7-nearest neighbors graph.

**Real-world dataset.** We tested our approach on the Cora dataset (Sen et al., 2008) and the LastFM Asia dataset (Leskovec and Krevl, 2014). While both of these graphs contain both features and a graph, we exclusively utilized the largest connected component of each graph, resulting in 2485 nodes and 5069 edges for the Cora graph and 7624 nodes and 27806 edges for the LastFM Asia graph. Subsequently, we randomly chose a subset of three out of the original seven and eighteen classes, respectively, to serve as the background class. Additionally, we selected 15% of the nodes from the foreground classes randomly to represent noise points, and we averaged the results over multiple runs, varying the labels chosen for noise. Both in Figures 2(c) and 2(d) we averaged over 5 different label sets as noise. For the LastFM Asia graph, we exclusively tested the LVC bases, as it is the most efficient one to compute given the large size of the graph.

**Results.** The results from both synthetically generated tests (Figures 2(a) and 2(b)) demonstrate the superiority of our method when compared to the baselines. In particular, \(d_{}\)-balls delivered exceptional results for both graphs, implying that \(d_{}\)-balls effectively cover the foreground classes as expected. For the Cora dataset (Figure 2(c)), we observed that our method outperforms GABA-II only when employing the community detection basis. This similarity in performance is likely attributed to the dataset's inherent lack of noise. Worth noting that the method we employed to inject noise into the dataset may not have been the optimal choice for this specific context. However, it isessential to highlight that our primary focus revolves around the abstention criteria, which plays a central role in ensuring the robustness of our model in the presence of noise. For the LastFM Asia dataset, our objective was to assess the practical feasibility of the model on a larger graph. We tested the LVC bases as they were the most promising and most efficient to compute. We outperform the baselines in our evaluation as shown in Figure 2(d) and further discussed in Appendix F.

In summary, our first results confirm what we expected: our approach excels when we choose basis functions that closely match the context's structure. However, it also encounters difficulties when the chosen basis functions are not a good fit for the context. In Appendix F, the results for a wide range of different parameters used to generate the previously described graphs are displayed.