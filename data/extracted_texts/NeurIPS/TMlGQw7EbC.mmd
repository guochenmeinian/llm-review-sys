# Markov Equivalence and Consistency in

Differentiable Structure Learning

 Chang Deng\({}^{}\)1 Kevin Bello\({}^{,}\)   Pradeep Ravikumar\({}^{}\)   Bryon Aragam\({}^{}\)

\({}^{}\)Booth School of Business, University of Chicago, Chicago, IL 60637

\({}^{}\)Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213

Changdeng@chicagobooth.edu

###### Abstract

Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers (without resorting to approximations such as Gumbel-Softmax), thus paving the way for differentiable structure learning under general models and losses. Open-source code is available at https://github.com/duntrain/dagrad.

## 1 Introduction

Directed acyclic graphs (DAGs) are the most common graphical representation for causal models , where nodes represent variables and directed edges represent cause-effect relationships among variables. We are interested in the problem of structure learning, i.e. learning DAGs from passively observed data, also known as causal discovery. Our focus will mainly be on score-based approaches to DAG learning , where the structure learning problem is formulated as optimizing a given score or loss function \(s(B;)\) that measures how well the graph, represented as an adjacency matrix \(B\{0,1\}^{p p}\), fits the observed data \(\), constrained to the graphical structure \(B\) being acyclic. This combinatorial optimization problem is known to be NP-complete .

Recent advances in score-based methods have introduced a continuous representation of DAGs, transforming the combinatorial acyclicity constraint into a continuous constraint via a differentiable function that exactly characterizes DAGs . In this case, the discrete adjacency matrix \(B\{0,1\}^{p p}\) is first relaxed to the space of real matrices, i.e., \(B^{p p}\), and then a differentiable function \(h:^{p p}[0,)\) is devised so that \(h(B)=0\) if and only if \(B\) is a DAG . This results in the following optimization problem:

\[_{B^{p p}}s(B;) h (B)=0.\] (1)

Considering a differentiable score function \(s\), the differentiable program (1) facilitates the use of gradient-based optimization techniques along with the use of richer models, such as neural networks,for modeling the functional relationships among the variables [74; 67; 41; 29; 44; 28; 76]. One of the most attractive features of this approach is that it applies to general models, losses, and optimizers, in contrast to prior work. Moreover, it cleanly separates computational and statistical concerns, so that each can be studied in isolation, in the same spirit as the graphical lasso [36; 68; 17].

Looking back at the inception of the continuous DAG learning framework by Zheng et al. [73; 74], however, most developments in this framework have focused on the design of alternative differentiable acyclicity functions \(h\) with better numerical/computational properties [4; 32; 72; 67], placing little emphasis on which score function to use . In fact, and unfortunately, regardless of the modeling assumptions, it has become a rather standard practice [67; 74; 4; 13; 32; 28] to simply use the least squares (LS) loss (a.k.a. "reconstruction loss") as the score by default, following the original paper by Zheng et al. , despite its known statistical limitations [63; 33; 1].

As a result, Reisach et al.  flagged the empirical successes of continuous structure learning (CSL) methods as largely due to the high agreement between the order of marginal variances of the nodes and the topological order of the underlying simulated DAGs, a concept they describe as "varsortability". Then, Reisach et al.  empirically showed that the performance in structure recovery of CSL methods drops significantly after simple data standardization. More recently, Ng et al.  demonstrated that this phenomenon may not be explained by varsortability, and instead pointed out that the explanations are due to the score function, albeit without proposing which score function to use. These observations motivate a deeper consideration of the choice of score.

Unfortunately, despite the fact that several score functions have been proposed for learning Bayesian networks (such as BIC , BDeu , and MDL ), their application to CSL methods is not well understood. This paper is precisely concerned with finding a suitable and general score function with strong statistical properties for CSL methods. That is, our objective is to find a score function that is: (1) intrinsically differentiable so that it is amenable to gradient-based optimization without approximations; (2) applicable to general models; (3) scale-invariant; (4) capable of identifying the sparsest model under proper regularization; and (5) connects nicely with classical concepts from Bayesian networks such as faithfulness and Markov equivalence classes.

**Contributions.** The main contribution of our work is to show that a properly regularized, likelihood-based score function has the five properties outlined above. We begin with Gaussian models to convey the main ideas, and then discuss generalizations. In more detail:

1. (Section 4) Starting with Gaussian models, we show that using the log-likelihood with a quasi-MCP penalty (10) as the scoring function leads to optimal solutions of (1) that correspond to the sparsest DAG structure which is Markov to \(P(X)\) (Theorem 1). Furthermore, under the faithfulness assumption, all optimal solutions are the sparsest within the same Markov equivalence class (Theorem 2).
2. (Section 5) We provide general conditions on the log-likelihood under which similar results hold for general models (Theorem 4).
3. (Section 4.5) We show that for Gaussian models, the log-likelihood score is scale-invariant. This means that rescaling or standardizing the data does not change the DAG structure (Theorem 3), and hence is not susceptible to varsortability.
4. We conduct experiments to evaluate the advantages of using a likelihood-based score. The findings from these experiments are detailed in in Section 6. The empirical results support our theory: The likelihood-based score is robust and scale invariant. We also release open-source code to facilitate the implementation and reproduction of our results.

## 2 Related work

Most methods for learning DAGs fall into two primary categories: Constraint-based algorithms, which depend on tests of conditional independence, and score-based algorithms, which aim to optimize a specific score or loss function. As our focus is on score-based methods, we only briefly mention classical constraint-based methods [59; 34; 62]. Within the umbrella of score-based methods, the linear Gaussian models is covered in works such as [1; 2; 19; 20; 37; 49], while studies on linear non-Gaussian SEMs are found in [33; 57]. Regarding nonlinear SEMs, significant contributions have been made in additive models [9; 15; 64], additive noise models [24; 49; 39], generalized linear models [47; 46; 22], and broader nonlinear SEMs [38; 21].

Works that are more directly connected to our research include those developed in the continuous structure learning (CSL) framework (e.g. 73; 74; 13; 4; 14; 29; 76; 41; 40; 28; 44). Most of these papers focus on empirical and computational aspects, and only a few study the theoretical properties of the CSL framework in (1). These include: [65; 43] studied the optimization and convergence subtleties of problem (1);  studied optimality guarantees for more general types of score functions and proposed a bi-level optimization method to guarantee local minima;  designed an optimization scheme that converges to the global minimum of the least squares score in the bivariate case. Finally, among the few works that study score functions under this framework, we note:  studied the properties of the \(_{1}\)-regularized profile log-likelihood, which leads to quasi-equivalent models to the ground-truth DAG; and the authors in  claim that a family of likelihood-based scores reduce to the least square loss, although this only holds under knowledge of the noise variances . Perhaps most closely related to our work is , who proved a similar identifiability result under the likelihood score. However, they used an \(_{0}\) regularizer along with the faithfulness assumption, which leads to an inherently non-differentiable optimization problem that is much simpler to analyze but requires approximations (e.g. Gumbel-Softmax) to optimize. On the other hand, they also consider interventional data, which we do not pursue in this work. Extending our results to include interventional data and interventional Markov equivalence is an important direction for future work. In contrast to the aforementioned works, we also prove that the log-likelihood has desirable properties such as being scale invariant, and when regularized by nonconvex and differentiable approximations of the \(_{0}\) function, it provably leads to useful solutions that are minimal models and Markov equivalent to the underlying structure, without assuming faithfulness.

## 3 Preliminaries

We let \(G=(V,E)\) denote a directed graph on \(p\) nodes, with vertex set \(V=[p]\{1,,p\}\) and edge set \(E V V\), where \((i,j) E\) indicates the presence of a directed edge from node \(i\) to node \(j\). We associate each node \(i V\) to a random variable \(X_{i}\), and let \(X=(X_{1},,X_{p})\).

**Structural equation models (SEMs).** An SEM \((X,f,P(N))\) over the random vector \(X=(X_{1},,X_{p})\) is a collection of \(p\) structural equations of the form:

\[X_{j}=f_{j}(X,N_{j}),_{k}f_{j}=0k_{j},\] (2)

where \(f=(f_{j})_{j=1}^{p}\) is a collection of functions \(f_{j}:^{p+1}\), here \(N=(N_{1},,N_{p})\) is a vector of independent noises with distribution \(P(N)\), and \(_{j}\) denotes the set of parents of node \(j\). Here, \(_{k}f_{j}\) denotes the partial derivative of \(f_{j}\) w.r.t. \(X_{k}\), which is identically zero when \(f_{j}\) is independent of \(X_{k}\), i.e. \(f_{j}(X,N_{j})=f_{j}(_{j},N_{j})\). The graphical structure induced by the SEM, assumed to be a DAG, will be represented by the following \(p p\) weighted adjacency matrix \(B\):

\[B=B(f), B_{ij}=\|_{i}f_{j}\|_{2},\] (3)

and we use \(G(B)\) to denote the corresponding binary adjacency matrix. For any set \(\) of SEMs, let

\[()\{G(B(f)):(X,f,P(N))\},\] (4)

i.e. \(()\) is the collection of all the DAGs implied by \(\). If \(\) is a set of DAGs and \(\) is a set of SEM, we also abuse notation by writing \(=\) to indicate \(=()\).

The SEM (2) is general enough to include many well-known models, such as linear SEMs (e.g., 33; 49), generalized linear models [47; 45; 18], and additive noise models [24; 51], post-nonlinear models [70; 71] and general nonlinear SEM [38; 21; 26; 74]. To illustrate some of these models: In linear SEMs we have \(X_{j}=f_{j}(_{j})+N_{j}\), where \(f_{j}\) is a linear map; in causal additive models (CAM) we have \(X_{j}=_{k_{j}}f_{j,k}(X_{k})+N_{j}\), where \(f_{j,k}\) is a univariate function; in post-nonlinear models we have \(X_{j}=f_{j,1}(f_{j,2}(_{j})+N_{j})\). In fact, essentially any distribution can be represented as an SCM of the form (2); see Proposition 7.1 in Peters et al. .

**Faithfulness and sparsest representations.** It is well-known that the DAG \(G\) is _not_ always identifiable from \(X\), and there is a well-developed theory on what can be identified based on \(X\) under certain assumptions. This leads to the concepts of faithfulness and sparsest representations, which we briefly recall here; we refer the reader to [60; 48; 50] for details. Let \((P)\) denote the set of conditional independence relations implied by the distribution \(P\), and let \((G)\) denote the set of \(d\)-separations implied by the graph \(G\). Then \(P\) is _Markov_ to \(G\) if \((G)(P)\), and _faithful_ to \(G\) if \((P)(G)\). When both conditions hold, i.e. \((P)=(G)\), then \(G\) is called a _perfect map_ of \(P\). Following common convention, we will simply call \(P\) faithful when \((G)=(P)\). When \(P\) is faithful to \(G\), the Markov equivalence class (MEC) of \(G\) is identifiable and can be represented by a CPDAG.

**Definition 1**.: _For any DAG \(G\), the Markov equivalence class is \((G)=\{:()=(G)\}\)_

Since faithfulness may not always hold, there has been progress in understanding what can be identified under weaker conditions. One approach which we will use is the notion of a _sparsest (Markov) representation_ (SMR), introduced in . A sparsest representation of \(P\) is a Markovian DAG \(G\) that has strictly fewer edges than any other Markovian DAG \(G^{}\), and such sparsest representation is unique up to Markov equivalence class. Theorem 2.4 in  shows that if \(P\) is faithful to \(G\), then \(G\) must be a sparsest representation of \(P\). This notion is closely related to the notion of minimality we adopt in Definition 2 (cf. Lemma 4 in the Appendix). These ideas can be generalized and weakened even further; see  for details.

**Parameters and the negative log-likelihood (NLL).** For positive integers \(m,s\), we will use \(^{m}\) and \(^{s}\) to denote the model parameters for \(f=(f_{1},,f_{p})\) and \(N\), respectively.2 Then we denote the distribution of \(X\) by \(P(X;,)\). Let \(^{p}\) denote one observation of \(X\). Given \(n\) i.i.d. samples \(=(_{1},,_{n})^{}\) where \(_{i} P(X;,)\), the negative log-likelihood and expected negative log-likelihood can be written as:

\[_{n}(,)=-_{i=1}^{n} P(_{i};,), (,)=-[ P(;,)],\] (5)

where the subscript \(n\) in \(_{n}\) is used to indicate the sample version of the log-likelihood.

**Identifiability.** Let \(^{0}\) (resp. \(^{0}\)) denote the model parameters for the ground truth \(f^{0}\) (resp. \(N^{0}\)), let \(B^{0}=B^{0}(^{0})^{p p}\) denote the induced weighted adjacency matrix, and let \(G(B^{0})\{0,1\}^{p p}\) denote the induced binary adjacency matrix. For example, in the general linear Gaussian model (6), \(=B\) represents the adjacency matrix, and \(=\) denotes the variance of the Gaussian noise. In another case, if \(f_{j}\) is approximated by a multilayer perceptron (MLP), with \(N_{j}\) as Gaussian noise, then \(\) includes all the parameters of the MLP, while \(\) represents the variance of the Gaussian noise. Additionally, \((B)_{ij}=[B()]_{ij}=\|A_{j}^{(1)}\|\), where \(A_{j}^{(1)}\) is the first hidden layer in \(f_{j}\). Thus, by our definitions, \(P(X;^{0},^{0})\) is the true distribution. Here, there are two types of identifiability questions:

1. _Parameter identifiability_: Is it possible to uniquely determine the parameters \((^{0},^{0})\) based on observations from \(P(X;^{0},^{0})\)? Formally, is there any \((,)(^{0},^{0})\), such that \(P(X,^{0},^{0})=P(X,,)\) almost surely?
2. _Structural identifiability_: Is it possible to uniquely determine the DAG \(G(B^{0})\) based on observations from \(P(X;^{0},^{0})\)? In other words, is there any \((,)(^{0},^{0})\) such that \(P(X,^{0},^{0})=P(X,,)\) but \(G(B^{0}) G(B())\).

In general, parameter identifiability implies structural identifiability since the ability to uniquely determine parameter values often means that the structure they induce is also identifiable. However, the converse is not generally true, i.e. structural identifiability does not always imply parameter identifiability, as different parameter values can lead to the same structure. Classical results on identifiability of SEMs include: linear SEM with equal variance , linear SEM with non-Gaussian noises , causal additive models with Gaussian noises , additive models with continuous noise , and post-nonlinear models .

In models where parameter identifiability is possible, the population NLL \((,)\) serves as a natural choice for the score function because it attains a unique minimum at the true parameters \((^{0},^{0})\). However, this approach is not straightforward for nonidentifiable models, where multiple parameter sets can induce the same data distribution \(P(X;^{0},^{0})\), leading to ambiguities in parameter or structure estimation. In such cases, regularizing the log-likelihood can alleviate this issue. These regularizers enforce specific characteristics like sparsity, guiding the model towards more meaningful solutions (e.g. faithful or sparsest), despite the lack of identifiability.

## 4 General linear Gaussian SEMs: A nonidentifiable model

Although our results apply to general models, we begin by outlining the main idea with one of the simplest nonidentifiable models, the Gaussian model. Our goal in this section is to theoretically showhow the NLL with nonconvex differentiable regularizers can lead to meaningful solutions such as minimal-edge models and elements of Markov equivalent classes. We also discuss and prove the scale invariance of NLL, making it amenable to CSL approaches and addressing concerns raised in previous work [55; 42]. Then, in Section 5, we extend these results to general models.

### Gaussian DAG models

A linear SEM \((B,)\) over \(X\) with independent Gaussian noises \(N\), a special case of (2), is well-known to be nonidentifiable in terms of parameters and structure [see 2, for discussion]. We write the model as follows:

\[X=B^{}X+N,\] (6)

where \(B^{p p}\) is a matrix of coefficients with \(G(B)\) being a DAG, and \(N^{p}\) is the vector of independent noises with covariance matrix \(=(_{1}^{2},,_{p}^{2})\).3

Given the model (6) it is easy to see that the distribution \(P(X)\) is Gaussian and is fully characterized by the pair \((B,)\). That is:

\[X(0,),=_{f}(B,)(I-B)^{- }(I-B)^{-1},\] (7)

where \(\) is the covariance matrix of \(X\). In the sequel, we use the subscript \(f\) to refer to a function. In this case, \(_{f}\) denotes a function with arguments \((B,)\) and returns the covariance matrix. Moreover, we use \(\) to denote the corresponding precision matrix (inverse of the covariance matrix):

\[=_{f}(B,)(I-B)^{-1}(I-B)^{}.\] (8)

Let \(=(_{1},,_{n})^{}\) be \(n\) i.i.d. samples of \(X\). Then, let the sample covariance matrix be \(=_{i=1}^{n}_{i}_{i}^{}\). The sample NLL function is given by:

\[_{n}(B,)=-_{i=1}^{n}P(_{i};B,) =-(I-B)+( (B,))+.\]

The corresponding population NLL function is

\[(B,)=-_{X} P(X;B,)=- (I-B)+((B,))+.\]

The full derivation can be found in Appendix C.1. Here, it is important to note that the distribution of \(X\) is fully determined by either the precision matrix \(\) or the covariance matrix \(\).

### Equivalence and nonidentifiability

Our goal is to identify \((B,)\): Unfortunately, the model is inherently nonidentifiable in terms of both parameter and structure. This means that multiple pairs \((B,)\) for model (6) can induce the same data distribution \(P(X)\) given in (7), thus resulting also in the same precision matrix \(\). To address this, we define the equivalence class \(()\) as the set of all pairs \((B,)\) such that \(_{f}(B,)=\).

\[()\{(B,):_{f}(B,)=\}.\] (9)

It is worth noting that the size of \(()\) is finite and at most \(p!\), which corresponds to the number of permutations for \(p\) variables . For more comprehensive details on this class, see Appendix C.2.

This ambiguity naturally leads to the question: which pair \((B,)\) should we estimate? Since any pair would be indistinguishable based only on observational data, a natural objective is to estimate the "simplest" DAG, for example, a DAG that induces the precision matrix \(\) with the smallest number of edges. In other words, our goal is to estimate the matrix \(B\) that has the minimal number of nonzero entries in the equivalence class. Let \(s_{B}=|\{(i,j):B_{ij} 0\}|\).

**Definition 2** (Minimality).: \((B,)\) _is called a minimal-edge I-map4 in the equivalence class \(()\) if \(s_{B} s_{},(,)()\). The set of all minimal-edge I-maps in the equivalence class \(()\) is referred to as the minimal equivalence class \(_{}()\):_

\[_{}()=\{(B,):(B,),(B, )()\}.\]

[MISSING_PAGE_FAIL:6]

Ideally, we would like \(_{n,,}=_{}(^{0})\), however, it is unclear whether there exist values of \(\) and \(\) such that any optimal solution \((B^{*},^{*})\) lies within \(_{}()\). The following theorem provides an affirmative answer to this question. In the sequel, we say that a property \(S(x)\) holds for all sufficiently small \(x>0\) if there is some fixed \(>0\) such that for every \(x\), the property \(S(x)\) holds.

**Theorem 1**.: _Let \(X\) follow model (6) with \((B^{0},^{0})\) and \(^{0}=_{f}(B^{0},^{0})\). Let \(\) be \(n\) i.i.d. samples from \(P(X)\), and \(_{n,,}\) be defined as in (13). Then, for all sufficiently small \(,>0\) (independent of \(n\)), it holds that \(P(_{n,,}=_{}(^{0})) 1\) as \(n\)._

In other words, we can always guarantee that \(_{n,,}=_{}(^{0})\) by taking \(,\) sufficiently small, which is easily accomplished in practice. In the following, we use the superscript \(0\) to denote ground truth parameters. Additionally, we can assume that \(B^{0}\) always belongs to \(_{}(^{0})\), ensuring that our reference to the ground truth aligns with the simplest or minimal representation within the equivalence class. Moreover, by Lemma 1, under the faithfulness assumption, Theorem 1 can be interpreted as recovering the Markov equivalence class \((G^{0})\):

**Theorem 2**.: _Consider the setup in Theorem 1 and assume additionally that \(P(X)\) is faithful to \(G^{0}:=G(B^{0})\). Then, for all sufficiently small \(,>0\) (independent of \(n\)), it holds that \(P(_{n,,}=(G^{0})) 1\) as \(n\)._

Theorem 2 indicates with properly chosen hyperparameters, the optimal solution from optimization (12) will produce a graph that adheres to the same independence statements as \(G^{0}\). This implies that the structure learned through the optimization process accurately reflect the underlying causal or conditional independence structure of underlying data generating process.

**Remark 1**.: _Although we use quasi-MCP (mainly for its simplicity), it turns out MCP or SCAD can also be used. See Corollary 1 in Appendix A for details._

### Scale invariance and standardization

It is known that the LS loss is not _scale-invariant_, i.e. re-scaling the data (and in particular, standardizing it) can drastically change the structure , a fact which Reisach et al.  use to argue that differentiable DAG learning with the LS Loss is also not scale-invariant. Here we show that by using a different score--in this case the log-likelihood--fixes this and results in (provable) scale invariance. Thus, the choice of score function is crucial if certain properties such as scale invariance are desired. The following result restates the well-known fact that Gaussian DAGs are invariant to re-scaling (i.e. re-scaling does not change the support for any \((B,)()\)) using our notation:

**Lemma 2**.: _Let \(X(0,)\), suppose \(\) is a positive definite covariance matrix and let \(:=^{-1}\), suppose \(D\) is a diagonal matrix with positive diagonal entries. Then \((())=((D D))\)._

Lemma 2 has appealing consequences for standardization. Given raw data \(\), denote its standardized version by \(\) (cf. Appendix C.6). Ideally, structure learning algorithms will output the same structure whether \(\) or \(\) is used as input, and Lemma 2 suggests that re-scaling \(\) will not alter the structure of the DAG that is recovered from optimizing (12). The following theorem formalizes this:

**Theorem 3**.: _Under the same setting as Theorem 1, the solutions to (12) are scale-invariant. That is, for any \(n 0\), let_

\[_{n,,}()= \{(B^{*},^{*}):(B^{*},^{*}) with data }\},\] \[_{n,,}()= \{(B^{*},^{*}):(B^{*},^{*}) with data }\},\]

_where \(\) is the standardized version of \(\). Then, for all sufficiently small \(, 0\) and all \(n\), we have \((_{n,,}())=( _{n,,}())\). Moreover, for all sufficiently small \(,>0\) we have_

\[P[(_{n,,}())=( _{n,,}())=(_{}( _{f}(B^{0},^{0})))] 1n.\]

Thus, _even on finite samples_, the set of DAG structures \((_{n,,}())\) derived from the raw (unstandardized) data \(\) will always be the same as \((_{n,,}())\), which is derived from standardized data \(\). As a result, standardizing Gaussian data does not affect the recovered DAG structure if the optimization problem (12) can be solved exactly.

**Remark 2**.: _Theorem 3 applies to global optimization of the objective (12). Of course, in practice, algorithms can get stuck in local optima, but the global solutions (even for finite samples \(n\)) will always be scale invariant._Nonconvex regularized log-likelihood for general models

The results in the previous section are not specific to Gaussian models, although this helps with interpretability in a familiar setting. We now extend these results from linear Gaussian SEMs to more general SEMs. Here, we assume that \(X\) follows model (2) and the induced distribution is denoted by \(P(X;^{0},^{0})\). Let us define the equivalence class \((^{0},^{0})\),

\[(^{0},^{0})=\{(,):P(x;,)=P(x;^{0},^{0}),  x^{p}\}.\]

That is, \((^{0},^{0})\) is a set of pairs \((,)\) that induce the same distribution \(P(X;^{0},^{0})\). As a result, any pair \((,)\) within this equivalence class will be a minimizer of the NLL \((,)\). Analogously to Definition 2, we can also define the collection of minimal elements in the equivalence class \((^{0},^{0})\).

**Definition 3**.: \((,)\) _is called a minimal-edge \(I\)-map in the equivalence class \((^{0},^{0})\) if \(s_{B()} s_{B()},(,) (^{0},^{0})\). We further define_

\[_{}(^{0},^{0})=\{(,):(,)I(,)(^{0},^{0})\}.\]

Here, it is crucial that our concept of minimality concerns \(s_{B()}\), which is the number of nonzero entries in the weighted adjacency matrix \(B()\), rather than the number of nonzero entries in the parameter \(\) itself. Therefore, \(s_{B()}\) essentially counts the number of edges in the adjacency matrix.

**Assumption A**.: _(1) \(|(^{0},^{0})|\) is finite. (2) \(B()\) is L-Lipschitz w.r.t. \(\), i.e. \()-B(_{2})\|_{2}}{\|_{1}-_{2}\|_{2}} L\)._

**Assumption B**.: _For any \(\) such that \((^{0},^{0})<\), the level set \(\{(,):(,)\}\) is bounded, where \((,)\) is the expected NLL defined in (5)._

Assumption A(1) is relatively mild; it requires that the equivalence class contains only finitely many points. This assumption is satisfied by Gaussian models, generalized linear models with continuous output , binary output [74; 13], and most exponential families. It is also obviously satisfied by any identifiable model since \(|(^{0},^{0})|=1\). Assumption A(2) is a mild continuity requirement on \(B()\). Assumption B simply guarantees that the optimization problem has a minimizer, and is standard . More discussions about the assumptions are included in Appendix C.7. Without this type of assumption, score-based learning is not even well-defined.

Similar in spirit to Theorem 1, we can show that by combining the NLL with quasi-MCP for appropriate \(,\), solving the following problem, we recover elements of \(_{}(^{0},^{0})\):

\[_{,}_{n}(,)+p_{,}(B())  h(B())=0,\] (14)

where \(p_{,}()\) is quasi-MCP defined in (10). Next, define its set of global minimizers.

\[_{n,,}=\{(^{*},^{*}):(^{*},^{*})}\}.\]

**Theorem 4**.: _Let \(X\) follow model (2) with parameters \((^{0},^{0})\) and let \(\) be \(n\) i.i.d. samples from \(P(X;^{0},^{0})\). Under Assumptions A-B, for all sufficiently small \(,>0\) (independent of \(n\)), it holds that \(P(_{n,,}=_{}(^{0},^{0})) 1\) as \(n\)._

**Theorem 5**.: _Under the setting in Theorem 4 and assuming that \(P(X;^{0},^{0})\) is faithful with respect to \(G^{0} G(B(^{0}))\). Then, for all sufficiently small \(,>0\) (independent of \(n\)), it holds that \(P(_{n,,}=(G^{0})) 1\) as \(n\)._

## 6 Experiments

To solve (12) and (14), we employ the augmented Lagrangian algorithm  from NOTEARS [73; 74], modifying their least squares score with \(_{1}\) penalty into the log-likelihood with MCP (10). We compare our approach to relevant baselines, e.g. NOTEARS , GOLEM , DAGMA , VarSort , FGES  and PC . For our variation of NOTEARS that employs a score function based on the NLL with MCP, we name it as logll-notears. The suffixes 'population' and'sample' denote the use of the population and sample covariance matrix, respectively. Full details of the experiments are given in Appendix D. Open-source code implementing these methods is available at https://github.com/duntrain/dagrad.

Our primary empirical results are shown in Figures 1 and 2. We use the structural Hamming distance (SHD) as the main metric to evaluate the difference between the estimated graph and the ground truth graph. Lower SHD values indicate better estimation accuracy. Given that the model specified in (6) is nonidentifiable, we compare the CPDAGs of the estimated graph and the ground truth graph.

In Figure 1(a), we observe that using the NLL+MCP achieves the best performance for the different types of graphs and ranks second best for sparse graphs {ER1, SF1}. In Figure 1(b), standardizing \(\) significantly impacts the performance of GOLEM, NOTEARS, and DAGMA; the SHD values are not any better than an empty graph, exactly as predicted by prior theory. The performance of logll-notears-sample and logll-notears-population are also affected by standardization, but these methods remain robust and continue to make meaningful discoveries. It is important to note that this observation does not contradict our Lemma 2. The challenges arise because solving the optimization problems (12) and (14) to find global solutions becomes inherently difficult as \(p\) increases.To verify the scale invariance property in Theorem 3, we also conduct experiments on small graphs and include exact method that solve (12) and (14) to global optimal, see Figure 5.

In Figure 2, we replicate the Figure 1 in , providing a more direct comparison between various methods applied to raw data (\(\)) and standardized data (\(\) standardized). We include VarSort (referred to as sortnregress in ) as a baseline. Notably, for smaller graphs (\(p=10\)), both logll(-notears)-sample and logll(-notears)-population exhibit the scale-invariant property alongside PC and FGES, in alignment with Lemma 2. This contrasts sharply with other methods, which completely deteriorate. For larger graphs (\(p=50\)), standardizing the data mildly degrades the performance of logll(-notears)-sample and logll(-notears)-population. This can be attributed to the increased complexity of optimization as the size of the graph grows.

In Figure 3, we use a concrete toy example to investigate two key factors in the implementation: (1) the impact of random initialization, and (2) the upper limit for \(_{0}\) that can be applied according to Theorem 1. We generate \(10^{5}\) initializations \(B_{}\) with weight for each edge uniformly sampled within \([-5,5]\), and perform optimization using logll-notears starting from these points. The "maximal \(\)" is the theoretical maximum \(_{0}\) that ensures the validity of Theorem 1. We computed the SHD and the distances between the estimated \((B_{})\) and \((B^{0})\). The red line in Figure 3 represents the average SHD and distances. The distribution of these \(10^{5}\) estimated SHD and distances is visualized using dots of varying sizes, where larger dots indicate a higher frequency of points. In some cases where SHD takes a value of \(-1\), this value is used to indicate that the estimated \(B_{}\) does not form a valid DAG, which is an artifact of thresholding and affects \(<0.5\%\) of models. For the remaining models, the optimization (12) can typically be solved very close to a globally optimal, and according to Theorem 2, the SHD should ideally be zero, which is consistent with the figure.

Our results are not limited to the linear model with Gaussian noise. In Appendix E.3, we provide additional experiments on a logistic model (binary \(X_{j}\)) and neural networks. Further details on the experimental settings and additional experiments can be found in Appendix D and E.

## 7 Conclusion

Continuous score-based structure learning is a relative newcomer to the literature on causal structure learning, which goes back several decades. It has attracted significant attention due to its simplicity and generality, however, its theoretical properties are often misunderstood. We have sought to fill in this gap by studying its statistical aspects (to complement ongoing computational studies, e.g. ). To this end, we proposed a fully differentiable score function for structure learning, composed of log-likelihood and quasi-MCP. We demonstrated that the global solution corresponds to the sparsest DAG structure that is Markov to the data distribution. Under mild assumptions, we conclude that all optimal solutions are the sparsest within the same Markov equivalence class. Additionally, the proposed score is scale-invariant, producing the same structure regardless of the data scale under the linear Gaussian model. Experimental results validate our theory, showing that our score provides better and more robust structure recovery compared to other scores.

We hope that this work stimulates further statistical inquiry into the properties of CSL. For example, we have focused on parametric models, and left extensions to nonparametric models to future work. Certain assumptions such as the finiteness of the equivalence class and the boundedness of the level set of the log-likelihood become more interesting in this regime. We have mentioned already that extensions to richer data types including interventions is an important direction. It would be of great 

[MISSING_PAGE_EMPTY:10]