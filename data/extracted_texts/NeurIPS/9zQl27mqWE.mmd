# Mixed Dynamics In Linear Networks:

Unifying the Lazy and Active Regimes

 Zhenfeng Tu

Courant Institute

New York University

New York, NY 10012

zt2255@nyu.edu

&Santiago Aranguri

Courant Institute

New York University

New York, NY 10012

aranguri@nyu.edu

&Arthur Jacot

Courant Institute

New York University

New York, NY 10012

arthur.jacot@nyu.edu

###### Abstract

The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unifying formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the 'best of both worlds': it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task.

## 1 Introduction

Whether in linear networks or nonlinear ones, there has been a lot of interest in the distinction between the lazy regime  and the active regime [16; 43; 15; 52; 13] as the number of neurons grows towards infinity. In the lazy regime the training dynamics become linear, so that they can be easily described in terms of the Neural Tangent Kernel (NTK) [27; 9; 51; 36], while the active regime exhibits complex nonlinear dynamics. While our understanding of the active regime remains much more limited, it appears to be characterized by the emergence of feature learning[22; 52], and of a form of sparsity [3; 11; 2; 1] (the type of sparsity observed depends on the network type [11; 19; 25; 24], but we will focus on fully-connected linear networks which exhibit a rank sparsity in the learned linear map [7; 35; 28; 47]) which are both absent in the lazy regime.

Note that even though it is common to talk of the 'the' active regime, we do not know yet whether there is only one or multiple active regimes. Indeed the term active regime is usually used to describe any regime that differs from the lazy regime and exhibit some form of feature learning. Though we do not have an complete understanding of where the lazy regimes ends and the active regime(s) start, we know that the lazy regime requires extreme overparametrization (a large number of neurons in comparison to the number of datapoints) [5; 20], a 'large' initialization of the weights , a small learning rate, and early stopping when using a cross-entropy loss or weight decay. Indeed, active regimes have been observed by breaking either of these requirements: taking limits with mild or no overparametrization , taking smaller or even vanishingly small initializations [35; 28], using largelearning rates  or SGD , or studying the late training dynamics with the cross-entropy loss  or weight decay . Though each of these can lead to active regimes with significantly different dynamics, they often lead to similar types of feature learning and sparsity.

In this paper, we study this transition in the context of linear networks and focus mainly on the effects of the width \(w\) and the variance of the weights at initialization \(^{2}\), and give a precise and almost complete phase diagram, showing the transitions between lazy and active regimes. In this setting, we will show that there typically is only 'one' active regime, which is the same (up to approximation) as the already well-studied balanced regime .

But our result also paint a more subtle picture than the lazy/active dichotomy. We propose a more granular approach, where at a certain time some part of the network can be in the lazy regime, while others are in the active or balanced regime. More precisely the network is lazy along the singular values of the matrix represented by the network that are smaller than \(^{2}w\), and in the active regime along the singular values larger than \(^{2}w\).

### Contributions

We consider the training dynamics of shallow linear networks \(A_{}=W_{2}W_{1}\) and show that for large enough width \(w\) (the inner dimension), and a iid \((0,^{2})\) initialization of all weights, the dynamics of \(A_{(t)}\) as a result of training the parameters \(=(W_{1},W_{2})\) with GD/GF on the loss \(()=C(A_{})\) for a general matrix cost \(C\) with learning rate \(\) is approximately given by the self-consistent dynamics

\[_{t}A_{(t)}-A_{}^{T}+^{ 4}w^{2}I} C(A_{})- C(A_{})^{T}A_{ }+^{4}w^{2}I}.\] (1)

These dynamics contain as special cases both the lazy dynamics

\[_{t}A_{(t)}-2^{2}w C(A_{})\]

when \(^{2}w_{max}(A_{})\) and the balanced dynamics

\[_{t}A_{(t)}=-A_{}^{T}} C(A_{ })- C(A_{})^{T}A_{}}\]

when \(^{2}w_{min}(A_{})\). But it also reveals the whole spectrum of mixed dynamics in between, where some singular values of \(A_{}\) are below the \(^{2}w\) threshold and some are above it.

This suggests that the lazy/active transition is best understood at a more granular level, where at each time \(t\) every singular value of \(A_{}\) can either be lazy or active/balanced. The mixed regime is the best of both worlds: on one hand, since \(A_{}^{T}+^{4}w^{2}I}\) is always positive definite, the network can never get stuck at a saddle/local minimum as can happen in the balanced regime, on the other hand there is a momentum effect where the dynamics along large singular values is much faster than along the small ones, leading to an incremental learning behavior and a low-rank bias, which is absent in lazy learning. By choosing the threshold \(^{2}w\) adequately, one can best take advantage of these two phenomenon.

Finally, we focus on the task of recovering a low-rank \(d d\) matrix \(A^{*}\) from noisy observations \(A^{*}+E\), training on the MSE error \(}\|A_{}-(A^{*}+E)\|_{F}^{2}\) in the limit as the dimension \(d\), width \(w\) and variance \(^{2}\) scale together with scaling laws \(w=d^{_{w}}\) and \(^{2}=d^{_{^{2}}}\). We describe the training dynamics for almost all reasonable scalings \(_{w},_{^{2}}\) leading to a phase diagram with two main regimes:

* **Lazy (\(1<_{^{2}}+_{w}\))** where all singular values remain below the threshold \(^{2}w\) throughout training, and where the network fails to recover \(A^{*}\) due to the absence of low-rank bias.
* **Active (\(1>_{^{2}}+_{w}\))** where \(K=A^{*}\) singular values pass the threshold and fit \(A^{*}\) before the other singular values have time to fit the noise \(E\), leading to the recovery of \(A^{*}\).

There are two other degenerate regimes that we avoid: the underparametrized regime when \(w<d\) (or \(_{w} 1\)) where the rank is constrained by the network architecture rather than the training dynamics, and the noisy regime \(2_{^{2}}+_{w}+1>0\) where the variance of the entries of \(A_{(0)}\) at initialization is infinite.

### Previous Works

Linear networks have been used as a testing ground, a stepping stone on the way to understand nonlinear networks. Linear networks and their training dynamics are in many ways much simpler than nonlinear ones, but in spite of a long research history, our understanding remains limited.

The setting that is best understood is that of diagonal linear networks where the dynamics decouple along the diagonal entries leading to an incremental learning behavior and a sparsity bias [44; 3; 45; 23; 41], some of this analysis has been extended to include effects of initialization scale  and SGD . While the same decoupling happens in general linear with diagonal initializations and diagonal task, it remains an extremely strong assumption.

Some work has been done to prove similar incremental learning dynamics outside the diagonal case [35; 28; 31] where the incremental aspect can be understood as the parameters going from saddle to saddle. For shallow linear networks, the training dynamics with MSE can be explicitly solved  but remain very complex so that one needs to assume some form of alignment to guarantee convergence . For deeper networks there exists explicit formulas in the mean-field limit where the number of neurons grows to infinity , these results can of course be applied to the special case of shallow nets, our paper goes further by giving self-consistent dynamics for the full matrix, revealing the lazy/active transition, and also extends the analysis to finite widths.

A very powerful tool in the analysis of a linear network is its training invariants, and the balancedness condition which greatly simplifies the dynamics [6; 7]. Balanced networks exhibit a momentum effect, where the training dynamics along a singular value \(s_{i}\) have'speed' proportional to \(s_{i}\) itself (or \(s_{i}\) to some power), while this momentum effect seems to be key to understand the low-rank bias of linear networks , it also means that one needs to guarantee that the dynamics never approach zero, which is one the main hurdle towards proving convergence in balanced networks. To solve this issue, recent work has focused on initialization that slightly imbalanced [49; 37; 46; 38; 50]. This suggests that it is key to find the right balance between balancedness and imbalancedness to obtain both fast convergence and low-rank bias.

In a concurrent work  a similar transition between lazy and active regimes is observed, and the same mixed dynamics are derived for a specific initialization. In contrast, we prove that these dynamics are approximately true with high probability for random Gaussian initializations, which is the standard initialization scheme for neural networks.

Figure 1: For both plots, we train either using gradient descent or the self-consistent dynamics from equation (1), with the scaling \(_{^{2}}=-1.85\), \(_{w}=2.25\) which lies in the active regime. (Left panel): We plot train and test error for both dynamics. We observe that the train/test error for gradient descent is very close to the train/test error for the self-consistent dynamics. (Right panel): We plot with a solid line the singular values of \(A_{(t)}\) when running the self-consistent dynamics, and use a dashed line for the singular values from running gradient descent. In this experiment, \(A^{}=5\). We use different colors for the \(5\) largest singular values and the same color for the remaining singular values. We can see how the \(5\) largest singular values ‘speed up’ as they cross the \(^{2}w\) threshold, allowing them to converge earlier than the rest. The minimal test error is achieved in the short period where the large singular values have converged but not the rest.

### Setup

We will study shallow linear networks (or matrix factorization) where a \(d_{out} d_{in}\) matrix \(A_{}\) is represented as the product of two matrices \(A_{}=W_{2}W_{1}\), where the weight matrices \(W_{1}\) and \(W_{2}\) are respectively \(w d_{in}\) and \(d_{out} w\) dimensional, for some width \(w\). The parameters \(\) of the network are the concatenation of the entries of both submatrices \(=(W_{1},W_{2})\).

The parameters \(\) are learned in the following manner: they are initialized as i.i.d. Gaussian \((0,^{2})\), and then optimized with gradient descent to minimize a loss \(()=C(A_{})\). Though most of our analysis works for general convex costs \(C:^{d_{out} d_{in}}\) on matrices, we will in the second part focus on the task of recovering a low-rank matrix \(A^{*}\) from noisy observations \(A^{*}+E\), by training a linear network \(A_{}\) on the MSE loss

\[()=}\|A_{}-(A^{*}+E)\|_{F}^{2 }.\]

The width \(w\) allows us to control the over parametrization, indeed the set of matrices that can be represented by a network of width \(w\) is the set \(_{ w}\) of matrices of rank \(w\) or less. The overparametrized regime is when \(w\{d_{in},d_{out}\}\) because all matrices can be represented in this case.

### Lazy Dynamics

The evolution of the weight matrices during gradient descent with learning rate \(\) is given by

\[W_{1}(t+1) =W_{1}(t)- W_{2}^{T}(t) C(A_{(t)})\] \[W_{2}(t+1) =W_{2}(t)- C(A_{(t)})W_{1}^{T}(t)\]

where we view the gradient \( C(A_{(t)})\) of the cost \(C\) as a \(d_{out} d_{in}\) matrix, which for the MSE cost equals \( C(A_{(t)})=2d^{-2}(A_{(t)}-(A^{*}+E))\).

But we care more about the evolution of the complete matrix \(A_{(t)}=W_{2}(t)W_{1}(t)\) induced by the evolution of \(W_{1}(t),W_{2}(t)\), which can be approximated by

\[A_{(t+1)}=A_{(t)}- W_{2}(t)W_{2}^{T}(t) C(A_{(t)} )- C(A_{(t)})W_{1}^{T}(t)W_{1}(t)+O(^{2}).\] (2)

Thus we see that if we can describe the matrices \(C_{1}=W_{1}^{T}W_{1}\) and \(C_{2}=W_{2}W_{2}^{T}\) throughout training, then we can describe the evolution of \(A_{(t)}\).

When \(w\) is very large, we end up in the lazy regime where the parameters move enough up to a time \(t\) to change \(A_{(t)}\), but not enough to change \(C_{1},{C_{2}}\)1, allowing us to make the approximation \(C_{i}(t) C_{i}(0)\). Furthermore at initialization these matrices concentrate as \(w\) around their expectations \([C_{1}]=^{2}wI_{d_{in}}\), \([C_{2}]=^{2}wI_{d_{out}}\). The GD dynamics can then be approximated by the much simpler dynamics:

\[A_{(t+1)}=A_{(t)}-2^{2}w C(A_{(t)}),\]

which are equivalent to doing GD on the cost \(C\) directly with a learning rate of \(2^{2}w\).

One can then easily prove exponential convergence for any convex cost \(C\) following the convergence analysis of traditional linear models. But we can see the absence of feature learning from the fact that the covariance \(C_{1}\) of the 'feature map' \(W_{1}\) is (approximately) constant. More problematic in the context of low-rank matrix recovery is the absence of low-rank bias, indeed one can easily solve the dynamics to obtain

\[A_{(t)}=(A^{*}+E)+(1-4d^{-2}^{2}w)^{t}(A_{(0)}-(A^{*}+E)),\]

and since \(A_{(0)}=0\) we obtain

\[[A_{(t)}]=(1-(1-4d^{-2}^{2}w)^{t} )(A^{*}+E).\]The expected test error \(\|A_{(t)}-A^{*}\|^{2}\) is therefore lower bounded by

\[\|A_{(t)}-A^{*}\|^{2}=\|(1-4d^{-2}^{2 }w)^{t}A^{*}+(1-(1-4d^{-2}^{2}w)^{t})E\|^{2}\]

which never approaches zero.

In linear networks, there is no advantage to being in the lazy regime, as we simply recover a simple linear model at an additional cost of more parameters and thus more compute. But we will see that a short period of lazy regime at the beginning of training plays a crucial role in making sure that the subsequent active regime starts from an 'aligned' state.

### Balanced Dynamics

There has been much more focus on so-called balanced linear networks, which are networks that satisfy the balanced condition \(W_{1}W_{1}^{T}=W_{2}^{T}W_{2}\). If the network is balanced at initialization, it remains so throughout training, because, the difference \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\) is an invariant of GF (and an approximate invariant of GD with small enough learning rate).

First observe that the balanced condition implies the following shared eigendecomposition \(W_{1}W_{1}^{T}=W_{2}^{T}W_{2}=USU^{T}\). This implies the following shared SVD decompositions \(W_{1}=UU_{in}^{T}\), \(W_{2}=U_{out}U^{T}\) and \(A_{}=U_{out}SU_{in}^{T}\). Furthermore, we have \(C_{1}=U_{in}SU_{in}^{T}=^{T}A_{}}\) and \(C_{2}=U_{out}SU_{out}^{T}=A_{}^{T}}\), which leads to self-consistent dynamics for \(A_{(t)}\):

\[A_{(t+1)}=A_{(t)}-A_{(t)}^{T}} C (A_{(t)})- C(A_{(t)})^{T}A_{(t) }}+O(^{2}).\]

Now these dynamics are quite complex in general, and it remains difficult to prove convergence. Indeed one can easily find initializations \(A_{(0)}\) that will not converge, for example if \(A_{(0)}=0\) then GD will remain stuck there. A lot of work has been dedicated to finding conditions that guarantee the convergence of the above dynamics , but these assumptions are often quite strong.

The simplest initialization that guarantees convergence (and the one that will be most relevant to our analysis) is the positively aligned initialization. If at initialization \(A_{(0)}\) and \(A^{*}+E\) are 'aligned', i.e. shares the same singular vectors \(A_{(0)}=U_{out}SU_{in}^{T}\) and \(A^{*}+E=U_{out}S^{*}U_{in}^{T}\), then they will remain aligned throughout training \(A_{(t)}=U_{out}S(t)U_{in}^{T}\) and the dynamics decouple along each singular value

\[s_{i}(t+1)=s_{i}(t)+2|s_{i}(t)|(s_{i}^{*}-s_{i}(t))+O(^{2}).\]

Since we always have \(s_{i}^{*} 0\), then for small enough learning rates \(\), we see that if \(s_{i}(0)(0,s_{i}^{*}]\) it will grow monotonically and converge to \(s_{i}^{*}\); if \(s_{i}(0)>s_{i}^{*}\) it will decrease monotonically to \(s_{i}^{*}\), and if \(s_{i}(0) 0\) it will increase and converge to \(0\). Thus one can guarantee convergence if we further assume positive alignment \(s_{i}(0)>0\).

The advantage is that there is a momentum effect in the form of the prefactor \(|s_{i}(t)|\), which implies that the dynamics along large singular values are faster than along small ones. As a result, if all singular values are initialized with the same small value, then they will at first grow very slowly until they reach a critical size where the momentum effect will make them converge very fast. The singular values aligned with the top singular values of \(A^{*}+E\) will reach this threshold much faster, and they will therefore converge to approximately their final value \(s_{i}=s_{i}^{*}\) at a time when the other singular values are still basically zero. If we stop training at this time then the linear network will have essentially learned only the top \(K\) singular values of \(A^{*}+E\), which is a good approximation for \(A^{*}\), leading to a small test error (see  for details).

But this analysis relies on the very strong assumption of positive alignment at initialization. If we do not assume a positive alignment and assume that the \(s_{i}\) are random (i.i.d. w.r.t. a symmetric distribution), then each \(s_{i}\) has probability \(}{{2}}\) of starting with a negative alignment and getting stuck at zero, which means that with high probability training will fail to recover \(A^{*}\) and will recover only a random subset of the singular values of \(A^{*}\). The presence of these attractive saddles shows the complexity of the balanced dynamics.

A limitation of this approach is that it requires a quadratic cost and a very specific initialization, and in the case of positive alignment, an initialization that requires knowledge of the (SVD of the)true function \(A^{*}\). Nevertheless, the positively aligned and balanced dynamics seem to capture some qualitative phenomenon that has been observed empirically outside of this restricted setting. This is the phenomenon of incremental learning, where if the singular values are initialized as very small, they first grow very slowly, but the multiplicative momentum will lead to come up one by one in a very abrupt manner, and this leads to a low rank bias where the network first only fits the largest singular value, then two largest, and so on. More generally, this can be interpreted as the network performing a greedy low-rank algorithm .

Our analysis will confirm the fact that positive alignment happens naturally as a result of a short period of lazy training, allowing us to prove similar decoupling and incremental learning for a general random initialization.

_Remark_.: We can define the time dependent map \((G;t)=C_{2}(t)G+GC_{1}(t)\), so that the GD dynamics can be rewritten as \(A_{(t+1)}=A_{(t)}-( C(A_{(t)}),t)+O(^{2})\). The map \(\) is none other than the NTK for shallow linear networks, but it has also been called the preconditioning matrix in previous work . The lazy regime is then characterized by the NTK \(\) being approximately equal to the time-independent NTK \(^{}(G)=2^{2}wG\), whereas the balanced regime is characterized by the time-dependent \(^{}(G;t)=A_{(t)}^{T}}G+G^{T}A_{(t)}}\), with the distinction that the time dependence is only through \(A_{(t)}\).

## 2 Mixed Lazy/Balanced Dynamics

Both lazy and balanced dynamics have the surprising but very useful property that the evolution of the network matrix \(A_{}\) is approximately self-consistent: the evolution of \(A_{}\) can be expressed in terms of itself. The lazy approximation becomes correct for a sufficiently large initialization, while the balanced one is correct for a balanced initialization. However, for most initializations, neither of these approximations are correct.

We fill this gap by providing a self-consistent evolution of \(A_{}\) that applies for any initialization scale:

\[_{t}A_{(t+1)}-A_{(t)}^{T}+ ^{4}w^{2}I C(A_{t})}- C(A_{t})^{T}A_ {(t)}+^{4}w^{2}I}.\]

This approximation is formalized in the following theorem, denoting \(_{1}(t)=^{T}A_{(t)}+^{4}w^{2}I}\) and \(_{2}(t)=A_{(t)}^{T}+^{4}w^{2}I}\)

**Theorem 1**.: _For a linear net \(A_{}=W_{2}W_{1}\) with width \(w\), initialized with i.i.d. \((0,^{2})\) weights and trained with Gradient Flow, we have with high probability that for all time \(t\),_

\[\|C_{1}(t)-_{1}(t)\|_{op},\|C_{2}(t)-_{2}(t) \|_{op}\{O(^{2}w),O(} \|C_{1}(t)\|_{op})\}.\]

Proof.: (sketch) The quantity \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\) is invariant under GF (and approximately so under GD) and it is approximately equal to \(^{2}w(P_{1}-P_{2})\) for two orthogonal projections \(P_{1},P_{2}\) (at initialization and for all subsequent times because of the invariance). We therefore have

\[W_{1}^{T}(W_{1}W_{1}^{T}-W_{2}^{T}W_{2})^{2}W_{1}^{4}w^{2}W_{1}^ {T}(P_{1}+P_{2})W_{1}^{4}w^{2}C_{1}.\]

Thus the pairs \(C_{1},C_{2}\) approximately satisfy the following equations:

\[0  C_{1}^{3}-A_{}^{T}A_{}C_{1}-C_{1}A_{}^ {T}A_{}-^{4}w^{2}C_{1}+A_{}^{T}C_{2}A_{}\] \[0  C_{2}^{3}-A_{}A_{}^{T}C_{2}-C_{2}A_{}A_ {}^{T}-^{4}w^{2}C_{2}+A_{}C_{1}A_{}^{T}.\]

The pair \(_{1},_{2}\) is a solution of the above, and one can show that \(C_{1},C_{2}\) must approach them and not any of the other solutions. 

The takeaway from theorem 1 is the following.

1. In the lazy regime where \(\|C_{1}(t)\|_{op}+\|C_{2}(t)\|_{op} O(^{2}w)\), then \(\|C_{1}(t)-_{1}(t)\|_{op}\|C_{1}(t)\|_{op}<<\|C_{1}(t)\|_{op}\).

2. In the active regime where \(\|C_{1}(t)\|_{op}/^{2}w>d^{}>>1\), then \(\|C_{1}(t)-_{1}(t)\|_{op} O(^{2}w)<<d^{-}\|C_{1}(t) \|_{op}\).

It is true that the error does not vanish. However, for our purpose it suffices to show that \(\|_{1}-C_{1}\|_{op}\) is infinitely smaller than \(C_{1}\) for all times, regardless of the magnitude of \(\|C_{1}(t)\|_{op}\).

We see how both the lazy and balanced dynamics appear as special cases depending on how large the variance at initialization \(^{2}\) is in comparison to the singular values of the matrix \(_{(t)}\):

* **Lazy:** When \(^{2}w s_{max}(A_{(t)})\), then \(_{1}^{2}wI_{d_{out}}\) and \(_{2}^{2}wI_{d_{in}}\), recovering the lazy dynamics.
* **Balanced:** When \(^{2}w s_{min}(A_{(t)})\), then \(_{1}^{T}A_{(t)}}\) and \(_{2}A_{(t)}^{T}}\), recovering the balanced dynamics.

But clearly there can be times when neither conditions are satisfied, when some singular values of \(A_{(t)}\) are larger than the threshold \(^{2}w\) while others are smaller, in such cases we are in a mixed regime, where the network is lazy along the small singular values of \(A_{(t)}\) (\(s_{i}^{2}w\)) and active/balanced along the large ones (\(s_{i}^{2}w\)).

At initialization, the singular values are of size \(^{2}\). This implies that with overparametrization (\(w d\)), all singular values start in the lazy regime and follow the simple lazy dynamics, which may (or may not) lead to some singular growing and crossing the \(^{2}w\) threshold, at which point they will switch to balanced dynamics (after a short transition period when the singular value is around the threshold \(s_{i}^{2}w\)). Once a singular value is far past the threshold \(s_{i}^{2}w\), training along this singular value will be much faster than along the lazy singular values (this speed up can be seen in Figure 1). This allows the newly active singular values to converge while the lazy singular values remain almost constant. Once the active singular values have converged, the slow training of the remaining lazy singular values continues until some of these singular values reaches the threshold, or until GD converges.

This type of behavior is illustrated by the following formula, which describes the derivative in time of the \(i\)-th singular value \(s_{i,t}\) of \(A_{t}\), with singular vectors \(u_{i,t},v_{i,t}\):

\[s_{i,t+1}-s_{i,t}_{t}u_{i,t}^{T}_{t}A_{(t)}v_{i,t} -2_{t}^{2}+^{4}w^{2}}u_{i,t}^{T} C(A_{ (t)})v_{i,t},\]

where the prefactor \(2_{t}^{2}+^{4}w^{2}}\) describes the effective learning rate along the \(i\)-th singular value, which depends on the \(i\)-th singular value \(s_{i,t}\) itself.

This suggests that it is more natural to distinguish between the lazy and active regime at a much more granular level: at every time \(t\) a singular value can be either active or lazy (or very close to the transition but this typically only happens for a very short time). In contrast, the traditional definition of the lazy regime was defined for a whole network and over the whole training time. To avoid confusion, we call this the pure lazy regime, where all singular values remain lazy throughout training. This begs the question of whether a pure balanced regime also exists, but all singular values will always be lazy for at least a short time period (assuming \(w>d\)), and as we will see this short lazy period plays a crucial role in aligning the network so that the subsequent balanced regime can learn successfully. A pure balanced regime can only be obtained in the underparametrized regime, or by taking a balanced initialization instead of the traditional i.i.d. random initialization.

While this challenges the traditional lazy/active dichotomy, it also reinforces it, as it shows that there is no fundamentally different third regime, only lazy, active, and some mix of the two. Theorem 1 thus allows us to revisit previous descriptions of lazy and balanced dynamics and 'glue them together' to extend them to the general case. This simple strategy will allow to almost fully 'fill in the phase diagram', i.e. describe the dynamics, convergence and generalization properties of DLNs for almost all reasonable initialization scales \(^{2}\) and widths \(w\).

_Remark_.: The transition of a singular value \(s_{i}\) from lazy to active can be understood as a form of alignment happening in the hidden layer: the two vectors \(W_{1}v_{i}\) and \(W_{2}^{T}u_{i}\) for \(u_{i},v_{i}\) the left and right singular vectors of \(s_{i}\) are orthogonal in the lazy regime and become perpendicular in the balanced regime. Indeed the normalized scalar product of these two vectors satisfies

\[^{T}W_{2}W_{1}v_{i}}{\|W_{2}^{T}u_{i}\|\|W_{1}v_{i} \|}=}{^{T}C_{2}u_{i}}^{T}C_{1}u_{i}}} }{^{2}+^{4}w^{2}}}\]

which is close to zero for lazy singular values \(s_{i}^{2}w\) and close to one for active ones \(s_{i}^{2}w\).

### Phase Diagram for MSE

To illustrate the power of Theorem 1 we provide a phase diagram of the behavior of large shallow networks on a MSE task, for almost all (reasonable) choices of width \(w\) and variance \(^{2}\) scalings.

We want to recover a rank \(K\) and \(d d\)-dimensional matrix \(A^{*}\) with \(s_{i}(A^{*})=da_{i}\) for some \(a_{1} a_{2} a_{K}\) independent of the dimension \(d\). We however only observe a noisy version \(A^{*}+E\) for some \(E\) such that \(\|E\|_{op} c_{0}d^{}\). One could imagine \(E\) to have iid random Gaussian entries \((0,1)\) in which case \(\|E\|_{op} c_{0}\) with high probability.

Figure 2: As a function of \(_{^{2}},_{w},\) we run GD and plot different quantities. Our theoretical results only apply to the top left region for \(_{w}>1\) and below the red line, although these plots suggest that some results may extend to smaller \(_{w}\)s. (Top left panel): We plot the smallest test error \(}\|A_{(t)}-A^{*}\|_{F}^{2}\) in the whole run. The active region (below the black line) has a small error while the lazy region does not. (Top right panel): We plot the stable rank of \(A_{(t)}\) (defined as \(\|A_{(t)}\|_{F}^{2}/\|A_{(t)}\|_{}^{2}\)) at the time of minimal test error. In this experiment, we took \(A^{*}=5\). We see that the active region has approximately the correct rank while the lazy region overestimates it. (Bottom left panel): We plot the number of iterations until minimal test error, illustrating the trade-off between test error and training time. (Bottom right panel): We compute \((}\|A_{(t)}-_{(t)}\|_{F}^{2})\) where \(A_{(t)}\) comes from GD and \(_{(t)}\) from the self-consistent dynamics. We observe that this distance is not only small for the region where our theoretical results apply but also almost everywhere outside this region.

As the dimension \(d\) grows, the size of the network needs to scale too, as well as the initialization variance, but it is unclear what is the optimal way to choose \(w\) and \(^{2}\). We will therefore consider general scalings \(w=d^{_{w}}\) and \(^{2}=d^{_{^{2}}}\). We will now describe the \((_{w},_{^{2}})\)-phase diagram which features 4 regimes: underparametrized, infinite-noise, lazy and mixed/active.

We can identify a region of'reasonable' pairs \((_{^{2}},_{w})\) by ruling out degenerate behavior. First, the width \(w\) needs to be larger than the dimension \(d\), since a network of width \(w\) can only represent matrices of rank \(w\) or less, this means that we need \(_{w} 1\). Another constraint comes from the variance of \(A_{}\) at initialization: the entries \(A_{(0),ij}\) at initialization have variance \(^{4}w\). We want this variance to go to zero as \(d\) grows which implies that we need \(2_{^{2}}+_{w}<0\).

Now within this reasonable region we observe two regimes, the pure lazy regime for \(1<_{^{2}}+_{w}\) where the network simply fits \(A^{*}+E\) thus failing to learn \(A^{*}\) and the mixed regime for \(1>_{^{2}}+_{w}\) where the dynamics are lazy for a short amount of time until \(K\) singular values grow large enough to switch to the balanced dynamics and fit the true matrix \(A^{*}\).

**Theorem 2**.: _For pairs \(_{w},_{^{2}}\) such that \(_{w}>1\) and \(2_{^{2}}+_{w}<0\), we have two regimes:_

* _Lazy (\(1<_{^{2}}+_{w}\)): with a learning rate_ \(}{^{2}w}\) _we have that for all time_ \(t\)_,_ \(}\|A_{(t)}-A^{*}\|_{F}^{2} c\)_._
* _Active (_\(1>_{^{2}}+_{w}\)): with a learning rate_ \(}{s_{1}(A^{*})} d\)_, and at time_ \[t=(}+(1,2)}{c(a_ {1},,a_{K})}+(1,2)}{2a_{K}})d d+^{ -1}O(d d),\] _for_ \(=1-_{^{2}}-_{w}>0\)_, we have that_ \[}\|A_{(t)}-A^{*}\|_{F}^{2} O( ^{4}w+w^{2}^{2}d}{d^{2}}+d^{-}+w}{d}+^{2}d}{d^{2}}),\] _for_ \(c(a_{1},,a_{K})=_{k,j:a_{k}_{j}}|a_{k}-a_{j}| a_{K}^{2}}{_{k,j:a_{k} a_{j}}|a_{k}^{2}-a_{j}^{2}|}\)_._

Note that all the terms inside the final \(O()\) term vanish: \(^{4}w 0\) because \(_{^{2}}+_{w}<0\), \(w^{2}^{2}d}{d^{2}}+w}{d} 0\) since \(1>_{^{2}}+_{w}\), and \(^{2}d}{d^{2}} 0\) since we assumed \( d\).

This shows that the lazy regime only appears for very large widths \(_{w}>2\) (or at least the lazy regime with finite variance at initialization). Indeed the choice \(_{w}=2,_{^{2}}=-1\) is at the boundary of the lazy regime with the smallest \(_{w}\). This could explain why it is rare to observe the lazy regime in practice.

Our theoretical results applies to the overparametrized regime \(w d\), but actually we only want to fit \(A^{*}\) which has a much smaller rank \(r\), and so we might only need \(w r\). Figure 2, top left panel, confirms this, since we see a good generalization even for small widths \(w<d\), and in particular when \(wA^{*}\). But to leverage this underparametrized regime, one would need to know the rank of the true matrix \(A^{*}\) in advance, which is typically not the case in practice. Nevertheless, the interesting behavior we observe in the (mildly) underparametrized regime warrants further analysis, and the fact that our self-consistent dynamics remain a good approximation in this regime (Figure 2, bottom right panel), suggests that the analysis we present here could be extended to this regime too.

Finally, we observe a trade-off between generalization error and training time: on one hand the test error has terms that scale negatively with \(1-_{^{2}}-_{w}\), which is the distance to the lazy/active transition, on the other hand, the time it takes to reach the minimal loss point scales positively with the same term. This can be seen from Figure 2, bottom left panel, which plots the number of steps required to reach minimal test error, which increases as one goes further into the active regime.

_Remark_.: In general when trying to fit a matrix \(B\) (instead of the special case \(B=A^{*}+E\)), the transition between lazy and mixed regime is when \(^{2}w\|B\|_{op}\). Thus the exact location of the transition is task-dependent, so that the same variance \(^{2}\) and width \(w\) can lead to NTK or mixed regimes depending on the task. For example, let us assume that \(A^{*}\) is full-rank instead of finite rank, then we expect \(\|A^{*}\|_{op}\) instead of \(\|A^{*}\|_{op} d\), thus the transition would be at \(=_{^{2}}+_{w}\) instead of \(1=_{^{2}}+_{w}\). This suggests that linear networks are able to adapt themselves to the task:leveraging active dynamics when the true data is low-rank to get better generalization, or remaining in the lazy dynamics in the absence of low-rank structure, to take advantage of the faster convergence. Note also that in the absence of sparsity, the lazy regime can be attained with a smaller width (\(_{w}>1\) instead of \(_{w}>2\)), since the choice \(_{w}=1,_{^{2}}=-\) is already on the boundary of the lazy regime.

## 3 Conclusion

We prove a surprisingly simple self-consistent dynamic for the evolution of the matrix represented by a shallow linear network under gradient descent. This description not only unifies the already known lazy and balanced dynamics, but reveals the existence of a spectrum of mixed dynamics where some of the singular values are lazy while others are balanced.

Thanks to this description we are able to give an almost complete phase diagram of training dynamics as a function of the scaling of the width and variance at initialization w.r.t. the dimension.

A natural question that comes out of these results is whether nonlinear network also feature similar mixed regimes, and whether they could be the key to understand the convergence of general DNNs.