# Multi-view Masked Contrastive Representation Learning for Endoscopic Video Analysis

Kai Hu

Xiangtan University

kaihu@xtu.edu.cn

&Ye Xiao

Xiangtan University

yxiao@smail.xtu.edu.cn

&Yuan Zhang

Xiangtan University

yuanz@xtu.edu.cn

&Xieping Gao

Hunan Normal University

xpgao@hunnu.edu.cn

Corresponding authors.

###### Abstract

Endoscopic video analysis can effectively assist clinicians in disease diagnosis and treatment, and has played an indispensable role in clinical medicine. Unlike regular videos, endoscopic video analysis presents unique challenges, including complex camera movements, uneven distribution of lesions, and concealment, and it typically relies on contrastive learning in self-supervised pretraining as its mainstream technique. However, representations obtained from contrastive learning enhance the discriminability of the model but often lack fine-grained information, which is suboptimal in the pixel-level prediction tasks. In this paper, we develop a **M**ulti-view **M**asked **C**ontrastive **R**epresentation **L**earning (M\({}^{2}\)CRL) framework for endoscopic video pre-training. Specifically, we propose a multi-view masking strategy for addressing the challenges of endoscopic videos. We utilize the frame-aggregated attention guided tube mask to capture global-level spatiotemporal sensitive representation from the global views, while the random tube mask is employed to focus on local variations from the local views. Subsequently, we combine multi-view mask modeling with contrastive learning to obtain endoscopic video representations that possess fine-grained perception and holistic discriminative capabilities simultaneously. The proposed M\({}^{2}\)CRL is pre-trained on 7 publicly available endoscopic video datasets and fine-tuned on 3 endoscopic video datasets for 3 downstream tasks. Notably, our M\({}^{2}\)CRL significantly outperforms the current state-of-the-art self-supervised endoscopic pre-training methods, _e.g._, Endo-FM (3.5% F1 for classification, 7.5% Dice for segmentation, and 2.2% F1 for detection) and other self-supervised methods, _e.g._, VideoMAE V2 (4.6% F1 for classification, 0.4% Dice for segmentation, and 2.1% F1 for detection). 2

## 1 Introduction

Video endoscopy is a crucial medical examination and diagnostic tool widely used for inspecting various tissues and structures (the digestive tract, respiratory tract, and abdominal cavity, _etc._) . In clinical practice, endoscopic video analysis usually relies on the experience and expertise of physicians, which is not only time-consuming and labor-intensive but also prone to subjective errors. Computer-aided medical analysis [2; 3; 4] can automatically and efficiently identify and classifylesions, thereby assisting physicians in making more accurate diagnoses. In this paper, we focus on endoscopic videos with the aim of developing a robust pre-trained model for endoscopic video analysis to facilitate downstream tasks (_i.e._, classification, segmentation, and detection).

Yann LeCun has mentioned "the revolution will not be supervised " in multiple talks, emphasizing that the future development of artificial intelligence will increasingly rely on un-/self-supervised learning. Among them, self-supervised learning (SSL) aims to learn scalable visual representations from large amounts of unlabelled data for downstream tasks with limited annotated data. To learn meaningful representations for SSL, researchers crafted visual pretext tasks [6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16], which are summarized into two main categories: contrastive and generative . Contrastive methods, also known as discriminative methods, employ a straightforward discriminative idea that pulling closer representations from the same image and pushing away different images, _i.e._, contrastive learning (CL) [6; 7; 8]. By utilizing image-level prediction with global features, CL can naturally endow pre-trained models with strong instance discriminability, which has been proven to be effective in classification tasks. However, CL also presents the challenge that downstream dense prediction tasks, such as segmentation and detection, are not fully considered.

Generative methods aim to reconstruct the input data itself by encoding the data into features and then decoding it, including AE , VAE , GAN , _etc_. Recently, masked image modeling (MIM) [13; 14; 16] has demonstrated the strong potential in self-supervised learning. MIM masks a substantial portion of image patches during training and utilizes an autoencoder  to reconstruct the original signal of the image, which, unlike CL, can enhance the ability to capture the pixel-level information. Following the success of MIM, some works have tried to extend this new pre-training paradigm to the video domain for self-supervised video pre-training [21; 22; 23]. Actually, mask techniques [24; 25; 26] are crucial for the success of mask modeling. Endoscopic videos, in particular, have higher dimensions and redundancy compared to static images. They also exhibit unstable inter-frame variations due to the manual manipulation by the doctor. Additionally, lesions in endoscopic videos often have low contrast and appear in obscured regions or with subtle variations. Therefore, simply applying random mask not only requires extensive pre-training time but also easily leads to poor performance.

To address the aforementioned issues, in this paper, we develop a Multi-view Masked Contrastive Representation Learning framework named M\({}^{2}\)CRL. **First**, considering the characteristics of inter-frame instability and small inter-class differences in endoscopic videos, we propose a multi-view masking strategy. Specifically, we introduce a frame-aggregated attention guided tube masking strategy for the global views, which aggregates features from multiple frames to capture global spatiotemporal information. Simultaneously, a random tube masking strategy is employed from the local views, enabling the model to focus on local features. **Second**, to address the inadequacy of capturing pixel-level details in contrastive learning, we integrate multi-view masked modeling into contrastive approach, which not only encourages the model to learn discriminative representations but also forces it to capture more refined pixel-level features. Extensive experiments have verified that our M\({}^{2}\)CRL significantly enhances the quality of endoscopic video representation learning and exhibits excellent generalization capabilities in multiple downstream tasks (_i.e._, classification, segmentation and detection). Overall, our contributions are summarized as follows:

* We propose a novel multi-view masking strategy aimed at enhancing the capture of fine-grained representations in endoscopic videos by performing mask modeling on both global and local views. This strategy involves utilizing the frame-aggregated attention guided tube mask to capture global-level spatiotemporal contextual relationships from the global views, while employing the random tube mask to focus on local variations from the local views.
* We propose a multi-view masked contrastive representation learning framework that combines multi-view mask modeling with contrastive method to train endoscopic videos, which effectively addresses the limitation of contrastive method in capturing dense pixel dependencies by predicting the intensity of each pixel within masked patches.
* We conduct extensive experiments on 10 endoscopic video datasets to evaluate the performance of M\({}^{2}\)CRL in comparison to other methods. M\({}^{2}\)CRL achieves 94.2% top-1 accuracy on PolypDiag , 81.4% on CVC-12k , and 86.3% on KUMC , outperforming the state-of-the-art methods, _i.e._, Endo-FM  by +3.5%, +7.5%, and +2.2%, and VideoMAE V2 by +4.6%, +0.4%, and +2.1%, respectively.

## 2 Related Work

### Self-supervised video representation learning

Self-supervised learning is a machine learning technique that mitigates the reliance on manual labeling by exploiting the inherent structure or properties of the data itself, and its common goal in computer vision is to learn scalable and generalisable visual representations. In SSL, the key lies in designing an appropriate pretext task, which involves generating suitable supervision using only visual signals. Pretext tasks in images [9; 10; 11; 12] have been widely explore, and motivated by images, there has been great interest in exploring SSL pretext tasks for videos. Some early works [31; 32; 33] focused on extending image SSL methods for video, however video has a specific temporal dimension. The temporal information can be leveraged for representation learning, including future prediction [34; 35; 36; 37; 38; 39; 40], temporal ordering [41; 42; 43; 44; 45], object motion [46; 47; 48; 49], and temporal consistency [50; 51]. In the last few years, contrastive learning [52; 53; 54; 55; 56; 57] has made great progress in SSL. However, SSL based on contrastive methods typically focuses on discriminative features of holistic information, while lacking the ability to focus on fine-grained features.

### Masked visual modeling

Masked visual modeling is proven to be a simple and effective self-supervised learning paradigm [13; 14; 15; 16; 58; 21; 22] through a straightforward pipeline based on masking and reconstruction. This paradigm quickly expanded to the video domain, _e.g._, VideoMAE  and ST-MAE , which extend image MAE to video. As with images, the choice of masking strategy affects self-supervised representation learning [24; 25; 26; 59]. VideoMAE  employs random tube mask to model the same locations across frames to ensure spatial discontinuity while maintain temporal continuity. In contrast, ST-MAE  generates a random mask for each frame independently, which are discontinuous in both space and time. Nevertheless, mask video modeling using randomly masked tokens to reconstruct is inefficient because the tokens embedded in a video frame are not equally important. Several studies [60; 61; 62; 63] have proposed various motion-based video mask strategies. However, our focus is on endoscopic videos, which lack a strong motion counterpart compare to the above works.

Figure 1: The pipeline of the proposed M\({}^{2}\)CRL. For the generated global and local views with different frame rates and spatial sizes, we adopt two different mask strategies: the frame-aggregated attention guided tube mask and the random tube mask. These strategies are integrated with mask reconstruction and contrastive method, enabling the model to simultaneously learn both the pixel-level and discriminative features of the video.

### Self-supervised learning for endoscopy

In recent years, SSL has received increasing attention in the field of medical analysis, including endoscopy. Endo-FM  employs contrastive method to minimize the disparity in feature representations between different spatiotemporal views of the same video. Intrator  has proposed the use of contrastive learning to adapt video inputs for appearance-based object tracking. Hirsch  applies the SSL framework masked siamese networks (MSNs) to analyze endoscopic videos. While MSNs use a masking concept, it primarily serves as a data augmentation technique, with its essence still rooted in contrastive learning. Currently, most self-supervised pre-training for endoscopic videos relies on contrastive methods. While these methods have shown promise for endoscopic video pre-training, relying solely on them may not fully capture the fine feature expressions of endoscopic videos.

## 3 Method

To address the limitations of contrastive methods in fine-grained information perception in SSL for endoscopic videos, we propose a Multi-view Masked Contrastive Representation Learning (M\({}^{2}\)CRL) framework for endoscopic video pre-training, as shown in Fig. 1. Here, we first review masked prediction in Section 3.1. Then, we present our proposed multi-view masking strategy in Section 3.2 and introduce our M\({}^{2}\)CRL framework in Section 3.3.

### Preliminary

Masked prediction is a prevalent representation learning technique in natural language processing (NLP) [66; 67; 68; 69], and many researchers have explore its application to images [14; 16; 70; 71; 72] and videos [21; 22; 23]. MIM endeavors to learn image representations by solving a regression problem, where the model is tasked with predicting the pixel values in randomly masked patches of the image. Specifically, an image \(x^{H W C}\) is reshaped into \(N=HWP^{2}\) flattened patches as \(\{x_{i}\}_{i=1}^{N}\), where \((H,W)\) is the resolution of image, \(C\) is the number of channels and \(P\) is the patch size. Each patch is represented with token embedding. MIM constructs a random mask \(m\{0,1\}^{N}\) to indicate the masked tokens that correspond to \(m_{i}=1\). In MAE , only the visible tokens \(\{x_{i}\,|\,m_{i}=0\}_{i=1}^{N}\) are fed into the vision transformer to obtain the latent feature, and then the decoder uses the latent feature and the masked tokens as inputs to predict \(\). In SimMIM , visible and invisible tokens are fed into the encoder. The prediction loss is calculated as the loss between the normalized masked tokens and the reconstructed ones in the pixel space by:

\[=}\!\|_{m}-x_{m}\|_{p}\] (1)

where \(N_{m}\) is the number of masked tokens, \(x_{m}\) is the masked token, \(p\) is norm and its value is 1 or 2.

### Multi-view masking strategy

Masked video modeling (MVM) [21; 22; 23] employs random mask strategies (_i.e._, random, spatial-only, temporal-only) to capture meaningful representations from pre-training videos. Although these strategies are effective for general video datasets with well-curated and stable distributions, they do not account for the unique characteristics of medical data. We have summarized two key characteristics of endoscopic videos: (1) Instability of inter-frame variations is a prominent feature of endoscopic videos. These variations are driven by camera movement, instrument manipulation, and the uneven distribution of lesion areas, _e.g._, variations can range from drastic to minor, as the camera navigates from the intestinal wall to specific lesion sites. (2) Endoscopic video exhibits characteristics of small inter-class differences. The lesion tissues typically resemble surrounding the normal tissues in color, texture, or shape, which complicates the model's ability to accurately identify the lesion area. Therefore, we propose a multi-view masking strategy by considering the above two points, the details are as follows.

#### 3.2.1 Frame-aggregated Attention Guided Tube Mask

To address the challenge of instability between video frames, we propose a frame-aggregated attention guided tube masking strategy. We aggregate the attention of all frames of the video along the framedimension to generate a frame-aggregated attention map, which then dynamically guides the masking process. This way can capture the overall scene information in a video sequence from the global spatiotemporal information and ignores irrelevant spatiotemporal noise to some extent.

Semantic information extractionOur architecture consists of the teacher network \(f_{t}\) and the student network \(f_{s}\). Our network employs a self-attention mechanism known as divided space-time attention mechanism , which enhances the learning ability of the network while reducing the computational complexity. Specifically, we take an endoscopic video, from which we sample the global views \(\{v_{g}^{i}^{T_{g}^{s} 3 H_{g} W_{g}}\}_{i=1}^{G}\), where \(T_{g}\) is the number of frames in the sampled view. Each frame is then divided into \(N={{H_{g}}{W_{g}}}{P^{2}}}{P^{2}}}. -1.2pt}{P^{2}}}\) patches, which are mapped into patch tokens and fed into the transformer blocks of the teacher network. Thus, each encoder block processes \(N\) patch (spatial) and \(T_{g}\) temporal tokens. The network includes a learnable class token, \([cls]\), which represents the global features learned by the network along spatial and temporal dimensions. Given the intermediate token \(e^{u}^{(N+1) D}\) from block \(u\), the token in the next block is computed as follows:

\[e_{time}^{u+1}=MSA_{time}(LN(e^{u}))+e^{u},\\ e_{space}^{u+1}=MSA_{space}(LN(e_{time}^{u+1}))+e_{time} ^{u+1},\\ e^{u+1}=MLP(LN(e_{space}^{u+1}))+e_{space}^{u+1}\] (2)

where \(MSA\), \(LN\) and \(MLP\) denote the multi-head self-attention, layer normalization, and multi-layer perceptron, respectively. Each block utilizes \(MSA\) layer to project and divide \(e\) into \(n_{h}\) parts. Each part contains the query \(Q_{r}\), key \(K_{r}\) and value \(V_{r}\) for \(r\) = 1, 2,..., \(n_{h}\), where \(n_{h}\) denotes the number of heads. We can get the attention map of the last layer of blocks by calculating the correlation between the query embedding of class token \(Q^{cls}\) and key embeddings of all other patches \(K\). It is averaged for all heads as follows:

\[A=}_{r=1}^{n_{h}}Softmax(Q_{r}^{cls}}{}})\] (3)

where \(D_{h}=D/n_{h}\), and \(A\) is \(T_{g}\) spatial attention maps. Although single-frame spatial attention integrates temporal information, it still only considers the spatial information of the current frame, neglecting the global spatiotemporal dependencies in the video sequence. Thus, we aggregate the attention of \(T_{g}\) frames to the mean value to obtain a simplified and holistic attention distribution by:

\[A_{agg}=}_{t=1}^{T_{g}}A_{t}\] (4)

This attention mechanism is capable of obtaining an approximation of the critical region of the video that are being attended to from both the temporal and spatial dimensions, while reducing the impact of excessive variations in individual frames or regions. We will further exploit this attention dynamic to guide the generation of tube masking to help the model perform the reconstruction task more appropriately.

Visible tokens sampling and maskingThe traditional random masking strategy treats critical and non-critical areas of the video equally in each iteration, which may lead to excessive masking of key video regions at a high masking ratio, thereby affecting the learning ability of the model. Therefore, we utilize a frame-aggregated attention mechanism to guide the generation of tube mask. By sampling some reasonably visible tokens from the model-focused areas and masking the rest, it allows our method to efficiently perform the reconstruction task even at a high masking ratio, while improving pre-training efficiency. Specifically, we begin by ranking tokens in descending order of attention scores and subsequently select a proportion of high-attention tokens based on the threshold \(\). We randomly sample visible tokens to form a binary mask from selected tokens of attention, as depicted in Fig. 1(b). The number of sampled visible tokens \(N_{v}\) is determined by the predefined mask ratio \((0,1)\). Followed by SimMIM , we fill the mask tokens with learnable mask embeddings, which are subsequently fed together into the student network for feature mapping and finally into the prediction head for reconstruction.

In comparison to random sampling, which is inefficient in token allocation, our method selects visible tokens based on the global spatiotemporal information of the video sequence. This approach minimizes redundant background area sampling, as these backgrounds have minimal impact on the significance of mask reconstruction. Furthermore, the attention-guided mask generation is dynamic and allows for adjustments during model training. This adaptability enables the model to continuously optimize its focus, adapting to complex or changing data characteristics.

#### 3.2.2 Random Tube Mask

Global view mask modeling is primarily employed for a comprehensive understanding and spatiotemporal contextual awareness of the entire endoscopic video frame, which mitigates the effects of inter-frame variability instability by aggregating frame attention. However, due to the low contrast between lesions and normal tissues in endoscopic videos, global views may struggle to accurately capture local details. Hence, we apply random tube mask reconstruction on local views to learn more granular detail information, as shown in Fig. 1(c). Specifically, we obtain the local views \(\{v_{l}^{j}=^{T_{l}^{j} 3 H_{l} W_{l}}\}_{j=1}^{L} (T_{l}<T_{g})\) by random cropping and uniform sampling at different frame rates. In local views, we also implement a high masking ratio \(\) = 90% to reduce information leakage during the mask modeling process. By local view mask modeling with different frame rates and spatial cropping, it is possible to make the model proficient in capturing variations in time scale and spatial detail. Moreover, local view mask modeling focuses on specific regions in the video without interference from the global background, enables more targeted learning. This allows the model to finely capture local information within the video, enhancing its ability to recognize subtle differences between abnormalities and normal tissues.

### Multi-view Masked Contrastive Representation Learning

The pipeline of our proposed M\({}^{2}\)CRL is shown in Fig. 1(a), which introduces a multi-view masking strategy and combines multi-view mask modeling with contrastive learning to learn representations that possess fine-grained and discriminative capabilities simultaneously.

In DINO , self-distillation is proposed not from the posterior distribution but by a teacher-student scheme that extracts knowledge from the model's own past iterations. This self-distillation method of self-supervision is also considered a form of contrastive learning . The contrastive learning part of our M\({}^{2}\)CRL follows Endo-FM , which also employs self-distillation method to achieve representation learning. Given an endoscopic video, two types of views are created under random data augmentation (\(G\) global views \(\{v_{g}^{i}\}_{i=1}^{G}\) and \(L\) local views \(\{v_{l}^{j}\}_{j=1}^{L}\)). The model is encoded by two encoders, a teacher network \(f_{t}\) and a student network \(f_{s}\), which are respectively parameterized by \(_{t}\) and \(_{s}\). It should be noted that the two student networks depicted in Fig. 1(a) actually represent

   &  &  & Pretrain &  &  &  \\  & & & Time(h) & (Classificaton) & (Segmentation) & (Detection) \\  Scratch (Rand.init.) & - & - & N/A & \(83.5 1.3\) & \(53.2 3.2\) & \(73.5 4.3\) \\  TimeSformer  & ICML & 2021 & 104.0 & \(84.2 0.8\) & \(56.3 1.5\) & \(75.8 2.1\) \\ CORP  & ICCV & 2021 & 65.4 & \(87.1 0.6\) & \(68.4 1.1\) & \(78.2 1.4\) \\ FAME  & CVPR & 2022 & 48.9 & \(85.4 0.8\) & \(67.2 1.3\) & \(76.9 1.2\) \\ ProViCo  & CVPR & 2022 & 71.2 & \(86.9 0.5\) & \(69.0 1.5\) & \(78.6 1.7\) \\ VCL  & ECCV & 2022 & 74.9 & \(87.6 0.6\) & \(69.1 1.2\) & \(78.1 1.9\) \\ ST-Adapter  & NeurIPS & 2022 & 8.1 & \(84.8 0.7\) & \(64.3 1.9\) & \(74.9 2.9\) \\ VideoMAE  & NeurIPS & 2022 & 25.3 & \(91.4 0.8\) & \(80.9 1.0\) & \(82.8 1.9\) \\ Endo-FM  & MICCAI & 2023 & 20.4 & \(90.7 0.4\) & \(73.9 1.2\) & \(84.1 1.3\) \\ DropMAE  & CVPR & 2023 & 37.9 & \(88.2 0.8\) & \(80.9 0.3\) & \(81.7 2.6\) \\ VideoMAE V2  & CVPR & 2023 & 17.3 & \(89.6 1.4\) & \(81.0 0.4\) & \(84.2 1.0\) \\  M\({}^{2}\)CRL & Ours & - & 24.3 & \(\) & \(\) & \(\) \\  

Table 1: Comparison with other latest SOTA methods on 3 downstream tasks. We report F1 score (%) for PolypDiag, Dice (%) for CVC-12k, and F1 score (%) for KUMC, respectively.

a single student network. We illustrate two student networks in the figure to more clearly convey the data flow. During pre-training, the global views are input into both the teacher and student networks, while the local views are only input into the student network. The network output \(f\) is normalized by a softmax function with a temperature \(\) to obtain the probability distribution \(p\) = softmax(\(f\)/ \(\)). Subsequently, the cross-entropy loss function is used to compute the losses between the teacher's global views and the student's global views, as well as between the teacher's global views and the student's local views. The specific loss functions are as follows:

\[^{gg}_{CL}=_{i=1}^{G} _{k=1\\ k i}^{G}-p^{t}_{v^{i}_{g}} p^{s}_{v^{k}_{g}},\\ ^{gl}_{CL}=_{i=1}^{G}_{j=1}^{L}-p^{t}_{v^{ i}_{g}} p^{s}_{v^{i}_{l}}\] (5)

Contrastive learning utilizes class tokens  from global and local views to calculate matching losses, which offers strong capabilities for overall discriminative representation. However, this method overlooks the dense pixel dependencies that are crucial for dense prediction tasks like segmentation and detection. Therefore, we integrate the multi-view mask reconstruction pre-training task into the contrastive learning. In the multi-view mask modeling task, the video clips processed through masking are fed into the encoder to be mapped into the feature space. Subsequently, the prediction head utilizes the context of unmasked patches to regress the dense pixel intensities within masked patches. The losses for global and local view reconstruction are as follows:

\[^{g}_{MVM}=^{m}} _{i=1}^{G}^{i}_{g}-v^{i}_{g},\\ ^{l}_{MVM}=^{l}}_{j=1}^{L} ^{j}_{l}-v^{j}_{l}\] (6)

M\({}^{2}\)CRL exploits both contrastive loss and reconstruction loss in optimization, and the total loss is \(_{total}=^{gg}_{CL}+^{gl}_{CL}+^{ g}_{MVM}+^{l}_{MVM}\). The student network updates the student parameters \(_{s}\) by minimizing \(_{total}\), and the teacher network \(_{t}\) is updated using the exponential moving average (EMA) of the student weights, and \(_{t}_{t}+(1-)\,_{s}\). Here \(\) denotes the momentum coefficient. By combining contrastive learning and masked video modeling, the model is encouraged to learn the holistic discriminative representation and detailed pixel information, effectively improving the learning ability of the model in complex visual data of endoscopic videos.

## 4 Experiments

### Datasets and Experimental Settings

We conduct experiments on 10 publicly available endoscopic video datasets: Colonoscopic , SUN-SEG , LDPolpyVideo , Hyper-Kvasir , Kvasir-Capsule , CholecTriplet , Renji-Hospital , PolypDiag , CVC-12k , and KUMC . These datasets have a total of 33,231 videos with approximately 5,500,000 frames, covering 3 types of endoscopy examination protocols and \(10+\) different diseases. The videos are processed into 30fps clips with an average duration of 5 seconds. The first 7 datasets are used for pre-training and we sample \(G=2\) global views

Figure 2: Qualitative results of segmentation and detection tasks. The segmentation results on the left are from the CVC-12k dataset, while the detection results on the right are from the KUMC dataset.

and \(L=8\) local views, where \(T_{g}\), \(T_{l}\) and the spatial size set to \(224 224\) and \(96 96\), respectively. We take ViT-B/16  as the backbone and perform 30 epochs of pre-training. In downstream tasks, we perform classification task on PolypDiag, segmentation task on CVC-12k, and detection task on KUMC, respectively. Our implementation is based on Endo-FM , and more experimental details can be found in SS A.

### Comparison with Prior Work

We compare our method with the recent state-of-the-art (SOTA) endoscopic video pre-training model, Endo-FM , which is the first pre-training model on a large scale across various endoscopic videos. The results of other methods are taken from the comparative method of Endo-FM, including TimeSformer , CORP , FAME , ProViCo , VCL , and ST-Adapter . Additionally, we also compare the latest video self-supervised methods: VideoMAE , VideoMAE V2  and DropMAE . For a fair comparison, all methods are pretrained on the same union of 7 datasets as our M\({}^{2}\)CRL. All experimental settings are referred to those documented in the original papers or in the released code.

Quantitative evaluationWe observe that our method outperforms existing state-of-the-art methods, as shown in Table 1. Particularly, compared to Endo-FM , our M\({}^{2}\)CRL achieves improvements of 3.5% F1 in classification (PolyDiag), 7.5% Dice in segmentation (CVC-12k), and 2.2% F1 in detection (KUMC) tasks. These improvements are attributed to our pre-training approach, which integrates multi-view mask modeling with contrastive method, substantially enhancing the model's ability for representation learning. Clearly, compared to other methods (TimeSformer , CORP , FAME , _etc._), M\({}^{2}\)CRL achieves considerable advantages across three downstream tasks. Although the performance gains on extremely dense task (_i.e._, segmentation) is modest compared to the latest video self-supervised methods (_i.e._, VideoMAE , VideoMAE V2 , DropMAE ), our M\({}^{2}\)CRL makes great progress on classification task and reaches 94.2%. This shows that our M\({}^{2}\)CRL not only focuses on pixel details but also enhances discriminative capabilities.

Qualitative evaluationWe visualize segmentation and detection results in Fig. 2. Compared to other methods, our M\({}^{2}\)CRL demonstrates superior visual results in segmenting both large and small polyp regions (1\({}^{st}\) and 2\({}^{nd}\) rows on the left in Fig. 2). Despite potential issues such as blurry boundaries or lens glare caused by camera movement, M\({}^{2}\)CRL is still capable of accurately segmenting the target regions (3\({}^{rd}\) row on the left in Fig. 2). We attribute these results to our multi-view mask modeling, which encourages the model to learn more precise detail information from videos. Similarly, M\({}^{2}\)CRL also exhibits good performance in detection tasks, especially in detecting small target regions (2\({}^{nd}\) row on the right in Fig. 2). Although there are some differences between the predictions of our method and the ground truth, we achieve a high degree of overlap with the ground truth. This demonstrates that our M\({}^{2}\)CRL significantly improves the pre-training capability for endoscopic videos. See SS D for more segmentation and detection visual comparison.

### Ablation Studies

Multi-view maskTable 2 illustrates the impact of single-view mask and multi-view mask. All experiments use the same masking ratio. For the single-view, different masking strategies are employed for global and local views, respectively. The random tube mask  randomly samples masked tokens in the 2D spatial domain and then extends these tokens along the temporal axis. The random mask  randomly masks tokens in the spatiotemporal domain. Our approach samples visible tokens by leveraging frame-aggregated attention from global views, resulting in better results than above two strategies, particularly achieving 80.6% in segmentation task. Similarly, the random tube mask demonstrates some superiority on local views. However, from Table 2, it can be observed that the performance of solely conducting mask modeling on the single-view is inferior to that on multi-views. In multi-view mask, it is evident that employing the frame-aggregated attention guided tube mask on global views and the random tube mask on local views results in significant performance gains. These two complementary mask methods work together to learn richer details of video features.

Hyper-parameters of the FAGTMWe conduct an experiment to investigate the impact of the hyperparameters of the frame-aggregated attention guided tube mask (FAGTM) for global views,the results are shown in Table 3. We perform experiments by sorting each patch of the obtained frame-aggregated attention maps in descending order and selecting the top \(\) proportion of patches as candidate mask patches. Subsequently, visible tokens are random sampled in candidate mask patches. From Table 3, we can observe that the model performs best when \(\) is set to 0.6. A lower value indicates selecting visible patches from smaller high-attention regions, leading to excessive attention on non-critical areas during reconstruction, contradicting the setup of the self-supervised pre-text task. On the other hand, higher values of \(\) adversely affecting the learning efficacy of the model.

The teacher's block used for the FAGTMIn our study, we use the last layer block of the teacher ViT-B for the FAGTM. The higher layer block incorporates lower-level features and object-level semantic information, offering comprehensive and abstract features that are essential for the model. Thus, it effectively guides the student network in masking. Table 4 shows that it is most beneficial for the FAGTM to utilize the last layer block of the teacher network.

Masking ratioThe impact of different masking ratios is illustrated in Table 5. It can be observed that there is an improvement in results across three downstream tasks when the masking ratio increases from 75% to 90%. Due to the reundancy in videos, it shows that the 75% masking ratio utilized in ImageMAE is not suitable for videos. When the masking ratio in videos reaches 90%, the task becomes challenging due to the limited number of patches available for learning temporal correspondence, thus enhancing the learning capacity of the model. This observation is also validated in VideoMAE . Compared to the optimal masking ratio, a higher masking ratio increases the difficulty of pre-training, hindering the model's ability to learn effective representations. Although our ablation experiments have shown that a masking ratio of 95% can achieve comparable performance, its effectiveness in three downstream tasks is lower than that of the 90% masking ratio. This suggests that at a masking ratio of 95%, the model is placed in a relatively unfavorable learning situation, resulting in suboptimal results.

Analysis of componentsAs see from the first row in Table 6, although the performance of the contrastive learning framework on classification tasks is acceptable, its performance on pixel-level tasks, especially the segmentation task, is not very good. Similarly, within the single mask modeling

    &  &  &  &  \\    & global & local & & & \\   & random & - & \(90.2 1.5\) & \(78.6 1.6\) & \(83.8 1.9\) \\  & RTM & - & \(93.0 0.8\) & \(77.5 3.1\) & \(84.0 1.0\) \\ single-view & FAGTM & - & \(92.7 0.4\) & \(80.6 0.5\) & \(84.4 1.4\) \\  & - & random & \(91.1 0.7\) & \(76.1 1.5\) & \(83.7 0.7\) \\  & - & RTM & \(91.1 0.5\) & \(77.5 0.6\) & \(85.0 0.4\) \\   & random & random & \(91.3 0.3\) & \(77.7 0.4\) & \(84.9 1.0\) \\ multi-view & RTM & RTM & \(93.2 0.4\) & \(80.2 0.9\) & \(85.2 1.3\) \\  & FAGTM & RTM & \(\) & \(\) & \(\) \\   

Table 2: **Multi-view mask**. We compare multiple different masking strategies on different views. FAGTM = Frame-aggregated Attention Guided Tube Mask. RTM = Random Tube Mask.

   \(\) & cla. & seg. & det. & blocks & cla. & seg. & det. \\ 
0.5 & \(91.6 0.5\) & \(80.7 0.7\) & \(84.8 0.4\) & \(4\) & \(91.8 0.7\) & \(76.6 1.5\) & \(83.6 1.1\) \\
0.6 & \(\) & \(\) & \(\) & \(8\) & \(92.5 0.4\) & \(79.7 2.2\) & \(84.8 1.0\) \\
0.7 & \(93.7 1.1\) & \(81.0 0.1\) & \(85.9 2.6\) & \(10\) & \(93.9 1.0\) & \(80.6 0.9\) & \(85.9 1.7\) \\
0.8 & \(92.9 0.7\) & \(79.0 0.3\) & \(85.4 0.8\) & \(12\) & \(\) & \(\) & \(\) \\   

Table 3: **Hyper-parameters** of the FAGTM.

[MISSING_PAGE_FAIL:10]