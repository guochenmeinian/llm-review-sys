# Latent Representation Matters: Human-like Sketches

in One-shot Drawing Tasks

 Victor Boutin\({}^{1,2,3}\), Rishav Mukherji\({}^{3}\), Aditya Agrawal\({}^{3}\), Sabine Muzellec\({}^{2,3}\), Thomas Fel\({}^{1,3}\), Thomas Serre\({}^{1,3}\), Rufin VanRullen\({}^{1,2}\)

\({}^{1}\)Artificial and Natural Intelligence Toulouse Institute, Universite de Toulouse, Toulouse, France.

\({}^{2}\)Centre de Recherche Cerveau & Cognition CNRS, Universite de Toulouse, France

\({}^{3}\)Carney Institute for Brain Science, Brown University

victor_boutin@brown.edu

###### Abstract

Humans can effortlessly draw new categories from a single exemplar, a feat that has long posed a challenge for generative models. However, this gap has started to close with recent advances in diffusion models. This one-shot drawing task requires powerful inductive biases that have not been systematically investigated. Here, we study how different inductive biases shape the latent space of Latent Diffusion Models (LDMs). Along with standard LDM regularizers (KL and vector quantization), we explore supervised regularizations (including classification and prototype-based representation) and contrastive inductive biases (using SimCLR and redundancy reduction objectives). We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) - better mimicking human perception (as evaluated psychophysically). Overall, our results suggest that the gap between humans and machines in one-shot drawings is almost closed.

## 1 Introduction

For cognitive scientists, human drawings offer a window into the brain, providing tangible insights into its visual and motor internal processes . For instance, drawings have been used in clinical settings to screen for perceptual impairments following brain trauma or Alzheimer's disease [2; 3], to assess perceptual disorders in autistic individuals [4; 5; 6] or to investigate perceptual changes during child development [7; 8] (see  for a recent review). Drawing tasks have also proven instrumental for exploring how the brain generalizes to novel visual categories [9; 10; 11]. Cognitive psychologists routinely use the one-shot drawing task to understand how human observers can reliably form new object categories from just one exemplar [12; 13]. From a computational viewpoint, this task is ill-defined because of the infinite number of possible sets of samples that could be associated with that exemplar. Yet, humans can effortlessly produce drawings that are not only easily recognizable but also original (i.e., sufficiently distinct from the reference exemplar) . This remarkable capability suggests that the brain leverages powerful representational inductive biases - yet to be discovered - to form novel categories.

Computer scientists have started to make progress in identifying some of the inductive biases for machine learning algorithms to learn from limited data. For one-shot classification tasks, a particularly effective representational inductive bias is to design an embedding space where samples of the same category, whether seen during training or not, cluster closely. This approach spans a wide range of models ranging from representations learned via contrastive objective functions [14; 15; 16], prototype-based representations [17; 18] or metric matching losses [19; 20]. Conversely, for one-shot generation tasks, researchers have preferred architectural over representational inductive biases. For instance,novel architectures based on Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs) have incorporated forms of spatial attention  or contextual integration [22; 23; 24]. Recent advances in diffusion models [25; 26] make them particularly promising for one-shot generation tasks. Indeed, clever conditioning on a context vector  or directly using guidance from the exemplar  has led to powerful one-shot diffusion models . Such a guidance mechanism has also proven successful in Latent Diffusion Models (LDMs) , which use a Regularized AutoEncoder (RAE) to compress input data and a diffusion model to learn the RAE's latent distribution. These diffusion models have started to close the gap with humans in the one-shot drawing task  (see section 2 for related work on one-shot learning). While better conditioning mechanisms have driven improvements in one-shot generative models, the potential of shaping their input space with representational inductive biases inspired by one-shot classification remains largely unexplored. This raises the question: "Do representational inductive biases from one-shot classification help narrow the gap with humans in one-shot drawing tasks?"

In this article, we use Latent Diffusion Models (LDMs ) to address this question. LDMs combine the flexibility of the Regularized AutoEncoder (RAE), in which one can seamlessly include various representational inductive biases in the latent space via regularization, with the high expressivity of the diffusion model. Herein, we study the impact of \(6\) different regularizers corresponding to distinct representational inductive biases. They are categorized into \(3\) groups. The first group, which serves as a baseline, includes the KL and the vector quantization regularization approaches typically used in LDMs . The second group involves supervised regularizers: a classification loss that promotes discriminative features mapping with categorical training labels and a prototype-based objective function that clusters samples with their respective prototypes in an embedding space. The third group features contrastive learning regularization schemes with the SimCLR and Barlow losses. The SimCLR objective function keeps a sample and its augmented view close in the embedding space but far apart from other samples' views. In contrast, the Barlow loss ensures that features of similar samples are decorrelated from those of dissimilar ones.

We compare those regularized LDMs against humans on the one-shot drawing task. Such a task offers a leveled playfield in which humans and machines can create sketches that are directly comparable using established evaluation frameworks [31; 30; 12] (see section 2 for related work). More specifically, our comparison focuses on two metrics to evaluate the quality of sketches produced by humans and machines - based on how distinct from the exemplar and how recognizable they are  - and on the alignment between humans' and machines' perceptual strategies. For the latter, we describe a novel method to generate importance maps highlighting category-diagnostic features in LDMs. These maps are then directly compared against importance maps derived from human observers obtained through psychophysics experiments. Our results show that LDMs using prototype-based and redundancy-reduction (with the Barlow twin objective) regularization techniques are further closing the gap with humans. These results are supported by both the sample's similarity and the feature importance maps alignment. Overall, our contributions can be summarized as follows:

* We introduce novel representational inductive biases in Latent Diffusion Models. In particular, we draw inspiration from losses that have proven effective in one-shot classification tasks (with the prototype-based, Barlow and SimCLR objective functions) to regularize the latent space of LDMs.
* We derive a novel explainability method to generate LDMs' feature importance maps that highlight category diagnostic features.
* We systematically compare the sketches and feature importance maps derived from humans and machines, and we show that LDMs with prototype-based and Barlow regularization significantly narrow the gap with humans on the one-shot drawing task.

Our work underscores the critical role of well-designed representational inductive biases in achieving human-like performance in one-shot drawing tasks. It also sets the stage for developing generative models that are better aligned with humans.

## 2 Related work

**Representation learning for one-shot classification tasks:** Learning representations that bring unseen samples (from the query set) close to the exemplars (in the support set) has proven effective in one-shot classification. The historical approach, called metric learning, aims at creating a feature space in which the distances between the query and support sets are preserved [20; 19; 32; 33]. However, the limited number of samples in the support set restricts these networks' ability to recognize novel classes. This limitation becomes more pronounced in the one-shot setting as the support set contains only one sample (the exemplar). To address this, the field has shifted towards prototype-based representations. Rather than trying to preserve the distances between query and support samples, such networks learn an embedding space in which the query samples cluster near the support samples [34; 17; 35]. Contrastive learning, a self-supervised learning approach, offers another effective solution to mitigate sample scarcity by augmenting the training set. This method learns an embedding space where positive pairs (a sample and its augmented version) are close together, and distant from negative pairs (augmented views from different instances) [36; 37; 15; 38; 39]. Among alternative methods, the SimCLR algorithm  uses a cosine similarity between samples whereas the Barlow-twins network  leverages the correlation matrix between features to dissociate positive and negative pairs. In this article, we use the prototype-based , the SimCLR  and the Barlow twins  objectives to regularize RAEs latent space. For additional mathematical details, see section 4.1 for the prototype-based loss and section A.2.3 for SimCLR and Barlow.

**Generative models for one-shot image generation tasks:** Some of the main techniques involve including information from the support set into the generative process, a method known as conditioning. For instance, the Neural Statistician uses a context vector containing summary statistics from the support sets, which is then concatenated with a VAE latent space [22; 40; 24]. Similarly, GANs leverage a compressed representation of the support set as a conditioning mechanism . Such a mechanism has also been used successfully to either condition [41; 42; 43; 29] or guide the denoising process of diffusion models [27; 28] and latent diffusion models . Here, we leverage LDMs with classifier-free guided diffusion models . Such a diffusion process has been shown to well approximate human drawings in one-shot drawing tasks .

**Human-machine comparison in one-shot drawing tasks:** Cognitive scientists have developed various methods to compare the generalization abilities of machines and brains on drawing tasks. Lake et al.  introduced the Omniglot challenge in which both humans and machines are tasked with drawing symbols from categories represented by a single exemplar (see  for a review on the challenge). The authors evaluated the drawings' recognizability in a visual Turing test where humans (or classifiers) had to distinguish between human-drawn and machine-generated symbols . Additional metrics, including classification uncertainty and semantic similarity, were also used to compare drawings produced by humans and machines under different time constraints [46; 8]. While these evaluation frameworks provide useful insights into a sample's recognizability, they do not measure how the diversity of model-generated samples compares to those created by humans. The "originality vs. recognizability" framework  mitiges this issue by adding the originality metric. An originality score quantifies the similarity between the original exemplar and its corresponding variations (see section 5.1 for details on this evaluation framework). This evaluation framework has been used to benchmark the generalization performance of mainstream generative models - Diffusion models , GANs  and VAEs  - against humans in the one-shot drawing setting . Although Diffusion models come closest to human performance, a noticeable gap remained in this study. In this article, we use the "originality vs. recognizablility" framework from Boutin et al.  to evaluate representational inductive biases in Latent Diffusion Models. In particular, we demonstrate that effective biases in one-shot classification tasks also prove efficient in the one-shot drawing task.

## 3 Datasets

As done in previous work [31; 30; 11], we use the Omniglot  and the QuickDraw-FS  datasets to compare humans and machines on the one-shot drawing task. These datasets, made of handwritten symbols or drawings, offer a fair environment for comparing the generation abilities of humans and machines [11; 46; 31; 30]. It is important to note that natural images generation is a task beyond human capability, making it unsuitable for a fair comparison between humans and machines.

**Omniglot** contains \(1,623\) categories of handwritten characters from \(50\) different alphabets, with \(20\) samples per class . This article uses a downsampled version of the dataset (size: \(48 48\) pixels). We train the models on a training set composed of all available symbols minus \(3\) symbols per alphabet left aside for the test set (similar to ). All the results on the Omniglot dataset are in the Appendix (see A.6).

**QuickDraw-FS** is made from drawings of the _Quick, Draw!_ challenge . In this challenge, human participants are asked to produce drawings in less than \(20\) seconds when presented with an object name. The categories are, therefore, made with semantically consistent samples that do not necessarily represent the same visual concept (e.g., the "phone" object category might contain coded phones, smartphones, phones with rotary dials, etc). The Quickraw-FS dataset mitigates this issue with categories representing the same visual concepts (see A.1 for more details). This dataset is ideally suited for purely visual one-shot generation tasks . It contains \(665\) categories with \(500\) samples each. The training set is made of \(550\) randomly selected categories, and \(115\) are left aside for the testing set. We downsampled the drawings to \(48 48\) pixels to keep computational resources manageable.

For each category in both datasets, we extract a 'prototypical' sample, selected in the center of the category cluster to condition the one-shot generative models (see A.1 for more details on the exemplar selection).

## 4 One-shot Latent Diffusion Models

The one-shot image generation task involves synthesizing variations of a visual concept not seen during training. Let \(^{D}\) denote the image variation and \(^{D}\) the exemplar. Latent Diffusion Models (LDMs) are composed of \(2\) distinct stages: a first stage leverages a Regularized AutoEncoder (RAE) that extracts a latent representation \(^{d}\) (\(d D\)) for each image (see green boxes in Fig. 1), and a second stage consisting of a diffusion model that learns the latent distribution (orange boxes in Fig. 1). In the one-shot setting, the diffusion model is conditioned by \(}\), the latent representation of \(\). We call \(\) the category label of the training set (a one-hot vector).

### Regularized Auto-Encoders (RAEs)

To describe the RAE, we use a probabilistic formulation in which \(q_{}(|)\) is the recognition model (or the encoder), and \(p_{}(|)\) is the decoder. We train the RAEs by minimizing \(_{RAE}\) (Eq. 1). In this equation, the first term is a reconstruction loss (computed with a \(_{2}\) distance), and the second term (\(_{reg}\)) covers a wide range of regularization losses. \(_{reg}\) includes the representational inductive biases we study in this article. Those inductive biases fall into 3 groups: the standard LDM regularizers, the supervised regularizers, and the contrastive regularizers.

\[_{,}_{RAE}\ \ \ \ \ \ _{RAE}=-_{  q_{}(|)}[ p_{}(| )]+_{reg}()\] (1)

Standard regularizers (KL and VQ):The KL divergence in Eq. 2 forces each coordinate of the latent vector to be distributed following a pre-determined distribution (e.g Gaussian distribution, as in the VAE ). The vector quantized loss in Eq. 3 transforms the continuous latent code \(\) into a discrete code \(}\) using the nearest entry in a codebook \(=\{}\}_{i=1}^{K}\) with the quantization operator: \(}=n_{}()\) (s.t. \(n_{}:_{e_{i}}-}_{2}\) as in the VQ-VAE ). This quantization operation being non-differentiable, backpropagation is achieved using a stop-gradient operation \(sg[]\) to provide a gradient estimator. We provide an extensive mathematical description of the VQ-VAE in App. A.2.1.

\[_{KL} =(q_{}(|x)||p())\ \ (\ p()=(0,))\] VAE (2) \[_{VQ} =( sg[]-}_{2}^{2}- sg[ }]-_{2}^{2})\] **VQ-VAE** (3)

Supervised regularizers (Classif. and Proto.):The classification regularizer forces discriminative features by minimizing the cross-entropy between the true labels (\(\)) and the softmax of the logits. Here the logits are learned by a linear layer (\(h_{}^{CL}\)) stacked on the latent space (Eq. 4). While the classification loss is supervised by the true categorical labels, the prototype-based loss is supervised

Figure 1: Latent Diffusion Models stack a diffusion model (orange) on top of an Auto-Encoder (green).

by the exemplars themselves (as in the Prototypical Net ). The prototype-based loss learns a metric space in which classification can be performed by computing distances between the variations and their corresponding exemplars (i.e., the prototypes)(see Eq. 5). Here, the metric space is linked to the latent space of the RAE through a linear layer (\(h_{}^{PR}\)). Intuitively, the prototype-based loss finds an embedding space where the variations will be close (in terms of \(_{2}\) distance) from their prototypes. See A.2.2 for more details.

\[_{CL} =(h_{}^{CL}(),)\] (4) \[_{PR} =_{_{}_{}(.| )}-((h_{}^{PR}()- h_{}^{PR}(_{})_{2})\] **Proto.** (5)

**Contrastive regularizers** (\(\) **and Barlow): Contrastive learning algorithms learn representations that are invariant under different distortions (i.e., data augmentations). Here we define two data-augmentation operators, \(^{A}()\) and \(^{B}()\), that transform the variations \(\) into \(^{}=^{A}()\) and \(^{}=^{B}()\), respectively. We denote \(^{}=q_{}(|^{})\) and \(^{}=q_{}(|^{})\) the projection of \(^{}\) and \(^{}\) into the RAE latent space, respectively. The \(\) regularizer is based on the InfoNCE loss: it maximizes the similarity between the representation of a sample and its augmented view while minimizing the similarity with negative pairs (augmented views of different instances) . The Barlow regularizer (as in the Barlow twins ) forces the cross-correlation matrix between \(^{}\) and \(^{}\) to be as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of samples to be similar while minimizing the redundancy between the components of these vectors. Said differently, the \(\) loss shapes the space based on the samples' similarity, while the Barlow operates on the correlation between the features of the samples. For conciseness, we have included the mathematical derivations and details on the data augmentation we used in App. A.2.3.

We leverage standard convolutional architectures (from ) to parametrize both the encoder and the decoder. The resulting autoencoder has a 1D bottleneck (\(d=128\) for QuickDraw-FS and \(d=64\) for Omniglot). We refer the reader to App. A.3.1 for complete architectural and training details of the RAE. In the rest of the article, we evaluate the impact of these regularizations by exploring the effect of \(\) (see Eq. 1) on LDMs.

### Diffusion Model

The LDM second stage is a diffusion model that learns the data distribution in the latent space of the RAE. Diffusion models progressively denoise a pure noise \(_{}(0,)\) into a clean latent representation \(_{}:=\) through a sequence of partially denoised variables \(\{_{}\}_{i=1}^{T}\). The goal is then to learn a transition probability \(p_{}(_{-1}|_{})\) that approximates a noise injection operator \(_{t}(.)\) so that \(_{}=_{t}(_{})=} _{}+_{t}}\) (\(_{t}\) is an hyperparameter of the diffusion schedule, and \(\) a Gaussian noise). The Denoising Diffusion Probabilistic Model (DDPM)  reduces the learning of \(p_{}(_{-1}|_{})\) to the optimization of a simple autoencoder \(_{}\) trained to predict the noise from a degraded latent representation \(_{t}\) (see A.4 for mathematical justification):

\[*{arg\,min}_{}_{_{ _{}} q_{}(.|)\\ _{} q_{}(.|)} \|_{}_{t}(_{}),_{ },t-\|_{2}^{2}\] s.t. \[(0,)\] and \[t(1,T)\] (6)

In Eq. 6, \(_{}\) denotes the latent representation of the exemplar \(\). Eq. 6 could be interpreted as a denoising score matching objective , so the optimal model \(_{^{*}}\) matches the following score function:

\[_{_{t}} p_{^{*}}(_{t}|_{ })-_{t}}}_{^{*}}(_{t },_{})\] (7)

The autoencoder-like model \(_{}(.,_{},t)\) is a 1D Unet conditioned on the time variable \(t\) and \(_{}\) (see A.4.3 for details on the architecture and the training of the Unet). Herein, we use a classifier-free guided version of the DDPM  with the following score function:

\[_{_{t}} p_{^{*},}(_{t}|_{ })=(1+)_{_{t}} p_{^{*}}(_{t }|_{})-_{_{t}} p_{^{*}}( _{t})\] (8)

This formulation introduces a guidance scale \(\) (we use \(\)=1) to tune how much the conditioning signal influences the final score. Such a formulation has shown effective in one-shot settings [28; 30]. Note that each term on the RHS of Eq. 8 is computed with the same network \(_{}\) using Eq. 7. \(_{}\) is simply conditioned on a non-informative signal to compute \( p_{^{*}}(_{t})\). We remind the reader that the training of the diffusion model begins only after the RAE training is complete, and occurs exactly identically, regardless of the type of regularization used. The quality of images generated by the diffusion model thus directly serves to compare the different regularizations. The code to train all described models is available at http://anonymous.4open.science/r/LatentMatters-526B.

## 5 Results

### Originality vs. Recognizabilty

To compare humans and machines in the one-shot drawing task, we first use the originality vs. recognizability framework [31; 30]. This framework leverages \(2\) critic networks to evaluate the samples produced during the testing phase. The recognizability is quantified using the classification accuracy of a one-shot classifier , while the originality is measured using the average distance between the variations and their corresponding exemplars. This distance is computed in the feature space of a self-supervised model . Importantly, both human-drawn and machine-generated samples are evaluated using the same \(2\) critic networks. This ensures that any potential biases in the critic networks are minimized, leading to a more balanced comparative analysis. Note that the originality is normalized across all tested models to range between \(0\) and \(1\). Here, we use the same originality vs. recognizability framework setting as that used in Boutin et al. . Importantly, the originality vs. recognizability plots should be interpreted based on how close the models are to the human data point (grey star in Fig. 3), rather than focusing solely on their individual originality or recognizability scores. In simple terms, a model that effectively mimics human drawings should fall near the human data point. Note also that there is an inherent trade-off between originality and recognizability: while recognizability assesses how likely the data point falls within the classifier decision boundary, originality measures how 'diffuse' the sample distribution is. Therefore a very original agent (producing highly diverse samples) will tend to have a low recognizability as the samples are likely to fall outside of the classifier decision boundary.

In Fig. 3, we first evaluate how increasing the regularization weights (i.e. the \(\) in Eq. 1) for each regularizer (taken separately) affects the similarity of LDM samples to human drawings. To do so, we report the originality and the recognizability values for LDM samples trained with different \(\) values (see data points in Fig. 3). We use a parametric fit (least curve fitting methods ) to illustrate how increasing \(\) affects these scores (see A.5 for more details on the parametric fit computations). We observe a similar concave shape for all curves. As \(\) starts increasing, the recognizability improves while the originality decreases (except for VQ regularizer). Beyond a certain \(\) value, the recognizability declines, and the originality increases. In particular, the maximum recognizability values for KL and VQ (obtained with \(_{KL}=10^{-5}\) and \(_{VQ}=5\)) match those of a diffusion model trained in the pixel space and barely exceed those of a non-regularized LDM (see Fig. 3a). Increasing the weight of the prototype-based regularizer substantially reduces the distance to human compared to the classification regularizer (the minimal distance to human is \(0.04\) for \(_{PR}=5 10^{2}\) vs. \(0.15\) for \(_{CL}=5\), see Fig. 3b). Among the contrastive regularizers, Barlow regularization significantly reduces the distance to human compared to the SimCLR one (the minimal distance to human is \(0.08\) with \(_{BAR}=30\) vs. \(0.12\) with \(_{SimCLR}=10^{-2}\), see Fig. 3c). A visual inspection of the samples tends to corroborate these results (see Fig. 2 and A.7 for more samples). We observe similar trends for all tested regularizers on the Omniglot dataset (see A.6).

Overall, our findings indicate that not all regularizers are created equal. For supervised regularizers (see Fig. 3b), the prototype-based regularizer generates more recognizable samples compared to the classification regularizer. This is expected since the classifier focuses on separating categories in the training set, which may not be ideal for unseen categories in the one-shot setting [19; 17]. In contrast, the prototype-based regularizer clusters samples near their prototypes, leading to less

Figure 2: **Samples from LDMs w/ different regularizers.** The LDMs correspond to the larger data points in Fig. 3.

overfitting and better transferability, which is valuable for few-shot tasks . Our experiments confirm that the prototype-based regularizer generalizes better for one-shot drawing. In Fig. 3c, the **Barlov** regularization outperforms the SimCLR regularizer in recognizability, likely due to Barlow's effective feature disentangling . These features transfer well to new datasets, making Barlow more suitable for the one-shot drawing task. Overall, our results demonstrate that effective representational inductive biases in few-shot learning also enhance performance in one-shot drawing.

We now study the effect of the regularizers when they are used in combination. In particular, we have systematically explored the following combinations of regularizers **Barlow + Prototype** (Fig. 4a), SimCLR. **+ Prototype** (Fig. 4b), KL **+ Prototype** (Fig. 4c), VQ **+ Prototype** (Fig. 4d). We observe that the **Barlow + Prototype** and the KL + Prototype** combinations produced the most human-like samples. Those regularizer's combinations are particularly as in both cases the combined recognizability is significantly higher compared to using each regularizer alone. This suggests that clustering samples around their prototypes (using the Prototype regularizer) within a disentangled space (achieved via the KL or **Barlow** regularizer) enhances generalization. In contrast, the VQ **+ Prototype** and the SimCLR **+ Prototype** combinations show little to no improvements.

### Comparing humans and LDM perceptual strategies

While the originality vs. recognizability framework allows us to compare human and machine performances in the one-shot drawing task, it does not reveal the strategies each uses to generalize to new categories. To address this, we aim to compare the visual strategies more directly via feature importance maps. These maps emphasize the most salient features to recognize a drawing.

Figure 3: **Effect of increasing the regularization weights on the originality vs recognizability framework (QuickDraw-FS dataset).** Each data point represents an LDM trained with different values of regularization weights (\(\)). The curves represent the parametric fits, oriented in the direction of an increase of \(\). **a):** For the LDMs with “standard” regularizers, the \(\) is applied on the KL (\(_{KL}\) in Eq. 2) or on the VQ regularizers (\(_{VQ}\) in Eq. 3). **b):** For the supervised regularizers, the \(\) is applied on the CL (\(_{CL}\) in Eq. 4) or on the prototype-based regularizers (\(_{PR}\) in Eq. 5). **c):** For the contrastive regularizers, the \(\) is applied on the SimCLR (\(_{SimCLR}\) in Eq. 14) or on the **Barlow** regularizers (\(_{Bar}\) in Eq. 15). See A.5 for more information on the range of \(\) we have explored for each regularizer. Larger data points indicate models whose performance is closer to that of humans for each type of regularization. For comparison, we include an LDM leveraging a non-regularized RAE (hexagon marker) and a diffusion model trained directly on the pixel space (cross marker). The human performance corresponds to the recognizability and originality computed on human drawings (shown with a grey star).

Previous research has demonstrated that by summing the absolute values of the diffusion scores (\(_{z_{t}} p_{}(z_{t}|z_{y})\)) throughout all diffusion steps, one can create heatmaps that highlight salient features in a diffusion model's generation process . Here, we adapt this heuristic to make it compatible with LDMs. This involves projecting each intermediate noisy state (\(}\)) back to pixel space using the RAE's decoder (\(p_{}(|})\)). To do so, we use the chain rule, and we multiply each diffusion score by the Jacobian of the RAE decoder w.r.t \(}\) (denoted \(J_{ p_{}(|})}(})\)). For each variation \(\) and its corresponding exemplar \(\), we can therefore compute a heatmap using Eq. 9 (see A.8.1 for mathematical details). Then, we average \(10\) of these heatmaps, obtained with the same exemplar but for different variations belonging to the same category. This process allows us to mitigate intra-class variations while focusing on category-specific features. We call this average the feature importance map (see A.8.2 to visualize feature importance maps).

\[(,)=_{i=0}^{T}|J_{ p_{}(| })}(})_{}} p_{}(}|})|} q_{}(|)\] (9)

We derived human feature importance maps using psychophysical data from Boutin et al.  (data shared by the original authors). The authors collected human saliency maps through an online psychophysics experiment based on a similar protocol to the ClickMe experiment . In this experiment, participants were presented with drawings and were asked to draw on regions important for categorization (see App. S in  for more details on the experimental protocol). We averaged the heatmaps across participants and drawings within the same category to obtain the feature importance maps we compared with those of machines (see A.8.3 for visualizing feature importance maps).

In Fig 5, we compare humans and LDMs feature importance maps. For each regularizer, we select the LDMs that produce the most human-like sketches (highlighted with larger data points in Fig. 3). Note that we exclude the VQ-regularized LDM from this analysis because it produces irrelevant feature importance maps, possibly due to the non-differentiability of the quantization process (see Fig. A.15). In Fig. 5a, we showcase examples of the obtained feature importance maps for all other LDMs' regularizations (see also A.8.2) and for humans (see also A.8.3). We qualitatively observe that the LDMs regularized with the Barlow and the prototype-based objectives tend to focus on sparse features. This particular aspect seems to be shared with the human feature importance maps. We compute the Spearman rank correlation to quantify the similarity between human and machine

Figure 4: **Combined effect of the regularization weights on the originality vs recognizability framework (QuickDraw-FS dataset). Each data point represents an LDM trained with a combination of \(2\) different regularizers. All combinations include the prototype-based regularizers. The curves represent the parametric fits, oriented in the direction of an increase of \(\). a): Barlow and prototype-based regularizers applied either separately (plain lines) or in combination (dashed-line). When applied in combinations, only the weight of the prototype-based regularizer is modified (with \(=30\) for Barlow). b): SimCLR and prototype-based regularizers. When applied in combinations, only the weight of the prototype-based regularizer is modified, the SimCLR is set to \(=1\). c): KL and prototype-based regularizers. When applied in combinations, only the weight of the prototype-based regularizer is modified, the KL is set to \(=1e-3\). d): VQ and prototype-based regularizers. When applied in combinations, only the weight of the prototype-based regularizer is modified, the VQ is set to \(=20\). See caption in Fig. 3.**

feature importance maps (see Fig. 5b). To make sure that the correlation comparison between the different LDMs is significant, we have computed pairwise statistical tests (Wilcoxon signed-rank test, see A.8.4). Our results show that all considered regularizations correlate significantly more with human feature importance maps than non-regularized LDMs. In addition, the prototype-based regularizer produces the feature importance maps with the highest correlation with humans and is significantly above all other tested regularizations (\(p<10^{-3}\)). In the human-alignment ranking, the Barlow-regularized LDM follows the prototype-based LDM, also showing a significantly higher Spearman correlation coefficient than KL, classification, SimCLR regularizers (\(p<10^{-3}\)). All other pair-wise statistical tests (between KL, classification, SimCLR) are not significant enough to draw a meaningful ranking.

## 6 Conclusion

In this article, we used Latent Diffusion Models (LDMs) to study the effect of representational inductive biases for one-shot drawing tasks. We explore \(6\) different regularizers: KL, vector quantization, classification, prototype-based, SimCLR and Barlow regularizers. We analyzed the human/LDMs alignment from two (independent) perspectives: their performance relative to humans on the one-shot drawing task (with the recognizability vs. originality framework in section 5.1) and the similarity of the underlying visual strategies (with the feature importance maps in 5.2). Overall, we observe a clear alignment between the \(2\) analyses on the following points:

* All regularized LDMs have an optimal regularization weight (\(\)) where they are more aligned with humans than their non-regularized counterparts.

Figure 5: **Feature importance maps comparison.****a)** The visualizations include feature importance maps for humans (top row) and LDMs (six bottom rows). All the maps are overlaid on exemplars. Hot vs. cold pixels show image locations that are more vs. less important. Maps for humans were computed using psychophysical data from Boutin et al. . For the LDMs, they are obtained for each category by averaging \((,)\) (see Eq. 9) over \(10\) different image variations (\(\)) belonging to the same category. The models’ maps are computed on the more human-like LDMs for each regularization (larger data points in Fig. 5). **b)** Spearman’s rank correlation coefficient between humans and LDMs feature importance maps. The error bar is computed as the standard deviation of the Spearman coefficients over all categories (\(25\) in total). Stars indicate the p-value (\(:p<10^{-3}\) and \(:p<5.10^{-2}\)) of pair-wise statistical test between models (Wilcoxon signed-rank test, see A.8.4). The black line corresponds to an LDM without any regularization. The dashed line is the human consistency (\(0.88\)), it quantifies how much two populations of humans agree with each other on feature importance maps (see A.8.3 for details on the human consistency computation).

* The **prototype**-based regularizer is showing the best matches with human performance and attentional strategy.
* In the one-shot drawing tasks, the samples' human-likeness could be further improved by combining the prototype-based regularizer with either the KL or the Barlow regularizers.

In conclusion, we observe that all representational inductive biases "are not created equal". However, some of them (**prototype**-based and Barlow regularizers) do narrow the gap with humans in the one-shot drawing task.

## 7 Limitations

In this article, we tested six representational inductive biases, a small number considering the extensive range available in the representation-learning literature. This field encompasses hundreds of inductive biases that have proven successful in one-shot classification tasks. Therefore, other representational inductive biases might align better with human performance, both in terms of sample similarity and visual strategy. Our goal wasn't to test all possible biases but to demonstrate that some of them can significantly narrow the gap with humans in one-shot drawing tasks.

Another limitation of this article lies in the recognizability vs. originality framework we are using to evaluate the drawings. This framework leverages \(2\) critic networks to evaluate the sample's originality and recognizability. There's no guarantee these networks align with human perceptual judgments. Thus, the recognizability and originality scores might not reflect human perception accurately. However, since both human and model outputs are evaluated using the same pre-trained critic networks, the comparison remains fair.

## 8 Discussion

It is noteworthy that the KL and VQ regularizers, commonly used to train LDMs on natural images (as in StableDiffusion ) are not the best-performing regularizers in the one-shot drawing task. Our study indicates that the prototype-based and the Barlow regularizers, not tested yet on LDMs trained on natural images, hold a significant potential for enhancing their one-shot ability. From a single image of a new vehicle prototype or of a new fashion item design, a generative model trained with these regularizers could produce relevant variations - an ability that current commercial applications still struggle with (see Fig. A.8.5).

Interestingly, the \(2\) inductive biases that align most closely with humans are directly related to prominent neuroscience theories. The prototype-based objectives provide an instantiation of the prototype theory of recognition and memory , suggesting that humans use prototype similarity to recognize novel objects. Similarly, the Barlow regularization is inspired by Barlow's redundancy reduction theory , which posits that the brain encodes statistically independent features to eliminate redundancy (and minimize energy consumption). The effectiveness of these regularizations provides hints that the brain may use similar inductive biases to generalize to new categories. In terms of brain inspiration, although we use LDMs to model humans' one-shot generation abilities, we do not claim that these neural networks constitute a realistic model of brain processes. It is indeed unlikely that humans generate samples by iteratively denoising random noise. More biologically plausible generative models might further help to obtain better models of human behavior (e.g., see ).

With this paper, we highlight how specific representational inductive biases, included in the input space of generative models, can help bridge the gap with human capabilities. We believe these biases will allow advanced models to generalize and create as effectively as humans do, leading to exciting advancements in technology and creativity.

## Aknowledgement

This work was funded by the European Union (ERC, GLoW, 101096017), ANITI (Artificial and Natural Intelligence Toulouse Institute) and the French National Research Agency (ANR-19-PI3A-0004). Additional funding was provided by ONR (N00014-24-1-2026) and NSF (IIS-1912280, IIS-2402875 and EAR-1925481). Computing hardware supported by NIH Office of the Director grant S10OD025181 via the Center for Computation and Visualization (CCV).