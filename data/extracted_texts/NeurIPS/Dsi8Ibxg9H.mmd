# Language Model as Visual Explainer

Xingyi Yang Xinchao Wang

National University of Singapore

xyang@u.nus.edu, xinchao@nus.edu.sg

Corresponding author

###### Abstract

In this paper, we present Language Model as Visual Explainer (LVX), a systematic approach for interpreting the internal workings of vision models using a tree-structured linguistic explanation, without the need for model training. Central to our strategy is the collaboration between vision models and LLM to craft explanations. On one hand, the LLM is harnessed to delineate hierarchical visual attributes, while concurrently, a text-to-image API retrieves images that are most aligned with these textual concepts. By mapping the collected texts and images to the vision model's embedding space, we construct a hierarchy-structured visual embedding tree. This tree is dynamically pruned and grown by querying the LLM using language templates, tailoring the explanation to the model. Such a scheme allows us to seamlessly incorporate new attributes while eliminating undesired concepts based on the model's representations. When applied to testing samples, our method provides human-understandable explanations in the form of attribute-laden trees. Beyond explanation, we retrained the vision model by calibrating it on the generated concept hierarchy, allowing the model to incorporate the refined knowledge of visual attributes. To access the effectiveness of our approach, we introduce new benchmarks and conduct rigorous evaluations, demonstrating its plausibility, faithfulness, and stability.

## 1 Introduction

Unlocking the secrets of deep neural networks is akin to navigating through an intricate, ever-shifting maze, as the intricate decision flow within the networks is, in many cases, extremely difficult for humans to fully interpret. In this quest, extracting clear, understandable explanations from these perplexing mazes has become an imperative task.

While efforts have been made to explain computer vision models, these approaches often fall short of providing direct and human-understandable explanations. Standard techniques, such as attribution methods , mechanical interpretability  and prototype analysis , only highlight certain pixels or features that are deemed important by the model. As such, these methods often require the involvement of experts to verify or interpret the outputs for non-technical users. Natural language explanations , on the other hand, present an attractive alternative, since the produced texts are better aligned with human understanding. Nevertheless, these approaches typically rely on labor-intensive and biased manual annotation of textual rationales for model training.

In this study, we attempt to explain AI decision in a human-understandable manner, for example, tree-structured language. We call this task _visual explanatory tree parsing_. To implement this, we present a systematic approach, Language Model as Visual Explainer (LVX), for interpreting vision models structured natural language, without model training.

A key challenge is that vision models, trained solely on pixel data, inherently lack comprehension of textual concepts within an image. For example, if a model labels an image as a "_dog_", it is unclear whether it truly recognizes the features like the _wet nose_ or _floppy ear_, or if it is merely making ungrounded guesses. To address this challenge, we link the vision model with a powerful external knowledge provider, to establish connections between textual attributes and image patterns. Specifically, we leverage large language models (LLM) such as ChatGPT and GPT4 as our knowledge providers, combining them with the visual recognition system. Figure 1 (Left) describes a toy case, where the LLM is interacts with the vision model to explore its capability boundaries. By doing so, we gain insights into the what visual attributes can be recognized by the model.

The pipeline of our approach is illustrated in Figure 1, which comprises two main stages, the _construction phase_ and the _test phase_.

In the _construction phase_, our goal is to create an attribute tree for each category, partitioning the feature space of a visual model via LLM-defined hierarchy. We begin by extracting commonsense knowledge about each category and its visual attributes from LLMs using in-context prompting . This information is naturally organized as a tree for better organization and clarity. Utilizing a text-to-image API, we gather corresponding images for each tree node. These images are subsequently inputted into the vision model to extract prototype embeddings, which are then mapped to the tree.

Once created, the tree is dynamically adjusted, based on the properties of the training set. Specifically, each embedding of the training sample is extracted by the vision model.Such embedding then navigates the parse tree based on their proximity to prototype embeddings. Infrequently visited nodes, representing attributes less recognizable, are pruned. Conversely, nodes often visited by the model indicate successful concept recognition. Those nodes are growaned, as the LLM introduces more detailed concepts. Consequently, LVX yields human-understandable attribute trees that mirror the model's understanding of each concept.

In the _test phase_, we input a test sample into the model to extract its feature. The feature is then routed in the parse tree, by finding its nearest neighbors. The root-to-leaf path serves as a sample-specific rationale for the model, offering an explanation of how the model arrived at its decision.

To assess our method, we compiled new annotations and developed novel metrics. Subsequently, we test LVX on these self-collected real-world datasets to access its effectiveness.

Beyond interpretation, our study proposes to calibrate the vision model by utilizing the generated explanation results. The utilization of tree-structured explanations plays a key role in enhancing the model's performance, thereby facilitating more reliable and informed decision-making processes. Experimental results confirm the effectiveness of our method over existing interpretability techniques, highlighting its potential for advancing explainable AI.

To summarize, our main contributions are:

* The paper introduces a novel task, _visual explanatory tree parsing_, that interprets vision models using tree-structured language explanations.

Figure 1: **General workflow of LVX. (Left) A toy example that LLM interacts with vision model to examine its capability. (Mid) It combines vision, language, and visual-language APIs to create a parse tree for each visual model. (Right) In testing, embeddings navigate this tree, and the traversed path provides a personalized explanation for the modelâ€™s prediction.**

* We introduce the Language Model as Visual Explainer (LVX) to carry out this task, without model training. The proposed LVX is the first dedicated approach to leverage LLM to explain the visual recognition system.
* Our study proposes utilizing the generated explanations to calibrate the vision model, leading to enhanced performance and improved reliability for decision-making.
* We introduce new benchmarks and metrics for a concise evaluation of the LVX method. These tools assess its plausibility, faithfulness, and stability in real-world datasets.

## 2 Problem Definition

We first define our specialized task, called **visual explanatory tree parsing**, which seeks to unravel the decision-making process of a vision model through a tree.

Let us consider the trained vision model \(f\), defined as a function \(f\), where \(\) represents the input image space and \(\) denotes the output label space. In this study, our focus lies on the classification task, where \(f=g h\) is decomposed into a feature extractor \(g\) and a linear classification head \(h\). The output space is \(^{n}\), where \(n\) signifies the number of classes. The model is trained on a labeled training set \(D_{tr}=\{_{j},y_{j}\}_{j=1}^{M}\), and would be evaluated a test set \(D_{ts}=\{_{j}\}_{j=1}^{L}\).

The ultimate objective of our problem is to generate an explanation \(T\) for each model-input pair \((f,)\) on the test set, illuminating the reasoning behind the model's prediction \(=f()\). This unique explanation manifests as a tree of attributes, denoted as \(T=(V,E)\), comprising a set of \(N_{v}\) nodes \(V=\{v_{i}\}_{i=1}^{N_{v}}\) and \(N_{e}\) edges \(E=\{e_{i}\}_{i=1}^{N_{e}}\). The root of the tree is the predicted category, \(\), while each node \(v_{i}\) encapsulates a specific attribute description of the object. These attributes are meticulously organized, progressing from the holistic to the granular, and from the general to the specific. Figure 2 provides an example of the parse tree.

Unlike existing approaches [53; 3] that explaining visual-language models [48; 47; 51; 86; 93], we address the more challenging scenario, on explaining vision models trained solely on pixel data. While some models can dissect and explain hierarchical clustering of feature embeddings [67; 77], they lack the ability to associate each node with a textual attribute. It is important to note that our explanations primarily focus on examining the properties of the established network, going beyond training vision model or visual-language model [2; 44] for reasoning hierarchy  and attributes  from the image. In other words, visual-language model, that tells the content in the image, can not explain the inner working inside another model. Notably, our approach achieves this objective _without supervision_ and in _open-vocabulary_ manner, without predefined explanations for model training.

## 3 Language Model as Visual Explainer

This section dives deeper into the details of LVX. At the heart of our approach is the interaction between the LLM and the vision model to construct the parsing tree. Subsequently, we establish a rule to route through these trees, enabling the creation of coherent text explanations.

### Tree Construction via LLM

Before constructing our trees, let's take a moment to reflect how humans do this task. Typically, we already hold a hierarchy of concepts in our minds. When presented with visual stimuli, we instinctively compare the data to our existing knowledge tree, confirming the presence of distinct traits. We recognize familiar traits and, for unfamiliar ones, we expand our knowledge base. For example, when we think of a dog, we typically know that it has a _furry tail_. Upon observing a dog, we naturally check for the visibility of its tail. If we encounter a _hairless tail_, previously unknown

Figure 2: The illustration of visual explanatory tree parsing. Each input sample is interpreted as a parse tree to represent the modelâ€™s logical process.

to us, we incorporate it into our knowledge base, ready to apply it to other dogs. This process is typically termed Predictive Coding Theory  in cognitive science.

Our LVX mirrors this methodology. We employ LLM as a "knowledge provider" to construct the initial conceptual tree. Subsequently, we navigate through the visual model's feature space to assess the prevalence of each node. If a specific attribute is rarely observed, we remove the corresponding nodes from the tree. Conversely, if the model consistently recognizes an attribute, we enrich the tree by integrating more nuanced, next-level concepts. This iterative process ensures the refinement and adaptation of the conceptual tree within our pipeline, which gives rise to our LVX.

**Generating Textual Descriptions for Visual Concepts.** We leverage a large language model (LLM) as our "commonsense knowledge provider"  to generate textual descriptions of visual attributes corresponding to each category. The LLM acts as an external database, providing a rich source of diverse visual concept descriptions. The process is illustrated in Figure 3.

Formally, assume we have a set of category names, denoted as \(C=\{c_{i}\}_{i=1}^{n}\), where \(i\) represents the class index. For each of these classes, we prompt an LLM \(L\) to produce visual attribute tree. We represent these attributes as \(d_{i}=L(c_{i},)\), where \(d_{i}\) is a nested JSON text containing textual descriptions associated with class \(c_{i}\). To help generate \(d_{i}\), we use example input-output pairs, \(\), as in-context prompts. The process unfolds in two stages:

* **Initial Attribute Generation**: We initially generate keywords that embody the attributes of each class. This prompt follows a predefined template that instructs the LLM to elaborate on the attributes of a visual object. The template is phrased as (This is a <CLSUNAME> because). The output json contains four primary nodes: Concepts, Substances, Attributes, and Environments. As such, the LLM is prompted to return the attributes of that concept. Note that the initial attributes tree may not accurately represent the model; refinements will be made in the refinement stage.
* **Description Composition**: Next, we guide the LLM to create descriptions based on these attributes. Again we showcase an in-context example and instruct the model to output ("Generates sentences that describe a concept according to each attribute.").

Once the LLM generates the structured attributes \(d_{i}\), we parse them into an initial tree, represented as \(T_{i}^{(0)}=(V_{i}^{(0)},E_{i}^{(0)})\), using the key-value pairs of the json text. Those generated json tree is then utilized to query images corresponding to each factor.

**Visual Embeddings Tree from Retrieved Images.** In order to enable the vision model to understand attributes generated by the LLM, we employ a two-step approach. The primary step involves the conversion of textual descriptions, outputted by the LLM, into images. Then, these images are deployed to investigate the feature region that symbolizes specific attributes within the model.

The transition from linguistic elements to images is facilitated by the use of arbitrary text-to-image API. This instrumental API enables the generation of novel images or retrieval of existing images that bear strong relevance to the corresponding textual descriptions. An initial parse tree node, denoted by

Figure 3: Crafting text-image pairs for visual concepts. Through in-context prompting, we extract knowledge from the LLM, yielding visual attributes for each category. These attributes guide the collection of text-image pairs that encapsulate the essence of each visual concept.

\(v\), containing a textual attribute, is inputted into the API to yield a corresponding set of \(K\) support images, represented as \(\{}_{i}\}_{i=1}^{K}=(v)\). The value of \(K\) is confined to a moderately small range, typically between 5 to 30. The full information of the collected dataset will be introduced in Section 4.

Our research incorporates the use of search engines such as Bing, or text-to-image diffusion models like Stable-Diffusion , to derive images that correspond accurately to the provided attributes.

Following this, the images are presented to the visual model to extract their respective embeddings, represented as \(_{i}=g(}_{i})\). As such, each tree node contains a set of support visual features \(P=\{_{k}\}_{k=1}^{K}\). This procedure allows for the construction of an embedding tree, consisting of paired text and visual features. These pairs are arranged in a tree structure prescribed by the LLM. It is important to note that the collected images are not employed in training the model. Instead, they serve as a support set to assist the model in understanding and representing the disentangled attributes effectively. As such, the visual model uses these embeddings as a map to navigate through the vast feature space, carving out territories of attributes, and laying down the groundwork for further exploration and explanation of a particular input.

**Tree Refinement Via Refine Prompt.** Upon construction, the parse tree structure is refined to better align with the model's feature spaces. This stage, termed _Tree Refinement_, is achieved through passing training data as a query to traverse the tree. Nodes that are seldom visited indicate that the model infrequently recognizes their associated attributes. Therefore, we propose a pruning mechanism that selectively eliminates these attributes, streamlining the tree structure. For nodes that frequently appear during the traversal, we further grow the tree by introducing additional or more detailed attributes, enriching the overall context and depth of the tree. The procedure is demonstrated in Figure 4.

Initially, we treat the original training samples, denoted as \((_{j},y_{j}) D_{tr}\), as our query set. Each sample is passed to the visual model to extract a feature, represented as \(_{j}=g(_{j})\).

Next, the extracted feature traverses the \(y_{j}\)-corresponding tree. Its aim is to locate the closest semantic neighbors among the tree nodes. We define a distance metric between \(_{j}\) to support set \(P\) as the point-to-set distance \(D(_{j},P)\). This metric represents the greatest lower bound of the set of distances from \(_{j}\) to prototypes in \(P\). It is resilient to outliers and effectively suppresses non-maximum nodes.

\[D(_{j},P)=\{d(_{j},)| P\}\] (1)

In our paper, similar to , we set \(d(,)=-(1+-||^{2}})\)2. It emphasizes close points while moderating the impact of larger distances. Following this, we employ a Depth-First Search (DFS) algorithm to locate the tree node closest to the query point \(_{j}\). After finding this node, each training point \((_{j},y_{j})\) is assigned to a specific node of the tree. Subsequently, we count the number of samples assigned to a particular node \(v^{*}\), using the following formula:

\[C_{v^{*}}=_{j=1}^{M}\{v^{*}=*{argmin}_{v V_{y_{ j}}^{(0)}}D(_{j},P_{v})\}\] (2)

In this formula, \(\) is the indicator function and \(P_{v}\) denotes the support feature for node \(v\). Following this, we rank each node based on the sample counter, which results in two operations to update the tree architecture \(T_{i}^{(+1)}=((T_{i}^{()}))\), where \(t\) stands as the iteration number

* **Tree Pruning.** Nodes with the least visits are pruned from the tree, along with their child nodes.

Figure 4: Tree refinement by traversing the embedding tree and querying the LLM model.

* **Tree Growing**. For the top-ranked node, we construct a new inquiry to prompt the LLM to generate attributes with finer granularity. The inquiry is constructed with an instruction template "Add visual attributes for the <NodeName> of a <CLassName>, to the json".
* **Common Node Discrimination**. In cases where different categories share common nodes (e.g. "human" and "dog" both have "ear"), we execute a targeted growth step aimed at distinguishing between these shared elements. To achieve this differentiation, we utilize a contrasting question posed to the LLM (The <NodeName> of <CLassName> is different from <CLassName> because").

The revised concept tree generated by the LLM provides a comprehensive and detailed representation of the visual attribute. To refine the attribute further, we employ an iterative procedure that involves image retrieval and the extraction of visual embeddings, as illustrated in Figure 1. This iterative process enhances the parse tree by incorporating new elements. As each new element is introduced, the attribute areas within the feature space become increasingly refined, leading to improved interpretability. In our experiment, we performed five rounds of tree refinement.

### Routing in the Tree

Once the tree is established, the model predicts the class of a new test sample \(^{}\) and provides an explanation for this decision by finding the top-k nearest neighbor nodes.

Specifically, the model predicts the category \(\) for the test instance \(^{}\) as \(=f(^{})\). The extracted image feature \(^{}\) corresponding to \(^{}\) is routed through the tree. Starting from the root, the tree is traversed to select the top-k nearest neighbor nodes \(\{v_{i}\}_{i=1}^{k}\) based on the smallest \(D(^{},P_{v_{i}})\) values, representing the highest semantic similarity between \(^{}\) and the visual features in the tree's nodes. The paths from the root to the selected nodes are merged to construct the explanatory tree \(T\) for the model's prediction.

This parse tree structure reveals the sequence of visual attributes that influenced the model's classification of \(^{}\) as \(\). It facilitates the creation of precise, tree-structured justifications for these predictions. Importantly, the routing process involves only a few feature similarity computations per node and does not require queries to the large language model, resulting in exceptionally fast computation.

### Calibrating through Explaining

The created parse tree offers a two-fold advantage. Not only does it illustrates the logic of a specific prediction, but it also serves as a by-product to refine the model's predictions by introducing hierarchical regularization for learned representation. Our goal is to use the parse tree as pseudo-labels, embedding this hierarchical knowledge into the model.

To operationalize this, we employ a hierarchical multi-label contrastive loss (HiMulCon) , denoted as \(_{HMC}\), to fine-tune the pre-trained neural network. This approach enhances the model by infusing structured explanations into the learning process, thus enriching the representation.

Specifically, we apply the LVX on all training samples. The explanatory path \(_{j}\) provides a hierarchical annotation for each training sample \(_{j}\). The model is trained with both the cross-entropy loss \(_{CE}\) and \(_{HMC}\) as follows:

\[_{j=1}^{M}_{CE}f(_{j}),y_{j}+ _{HMC}g(_{j}),_{j}\] (3)

Here, \(\) is a weighting coefficient. The explanation \(_{j}\) is updated every 10 training epochs to ensure its alignment with the network's evolving parameters and learning progress. Notably, the support set isn't used in model training, maintaining a fair comparison with the baselines.

## 4 Experiment

This section offers an in-depth exploration of our evaluation process for the proposed LVX framework and explains how it can be utilized to gain insights into the behavior of a trained visual recognition model, potentially leading to performance and transparency improvements.

### Experimental Setup

**Data Annotation and Collection.** To assess explanation plausibility, data must include human annotations. Currently, no large-scale vision dataset with hierarchical annotations is available to facilitate reasoning for visual predictions. To address this, we developed annotations for three recognized benchmarks: CIFAR10, CIFAR100 , and ImageNet , termed as H-CIFAR10, H-CIFAR100, and H-ImageNet. These annotations, detailed in Table 8, serve as ground truth for model evaluation, highlighting our dataset's unique support for hierarchical attributes and diverse visual concepts. Note that, we evaluate on hierarchical datasets only, as our method is specifically designed for structured explanations.

As an additional outcome of our framework, we have gathered three support sets to facilitate model explanation. In these datasets, each attribute generated by the LLM corresponds to a collection of images that showcase the specified visual concepts. These images are either retrieved from Bing search engine 3 using attributes as queries or are generated using Stable-diffusion. We subsequently filter the mismatched pairs with the CLIP model, with the threshold of 0.5. Due to the page limit, extensive details on data collection, false positive removal, limitations, and additional evaluation of user study and on medical data, such as X-ray diagnoses, are available in the supplementary material.

**Evaluation Metrics.** In this paper, we evaluate the quality of our explanation from three perspectives: _Plausibility_, _Faithfulness_ and _Stability_.

* **Plausibility** measures how reasonable the machine explanation is compared to the human explanation. We measure this by the graph distance between the predicted and ground-truth trees, using two metrics: Maximum Common Subgraph (MCS) , and Tree Kernels (TK) . We calculate their normalized scores respectively. Specifically, given a predicted tree \(T_{pred}\) and the ground-truth \(T_{gt}\), the MCS score is computed as \(||T_{gt}|}}\), and the TK score is computed as \(,T_{gt}) 100}{,T_{pred})TK(T_{gt},T_{gt})}}\). Here, \(||\) represents the number of nodes in a tree, and \(TK(,)\) denotes the unnormalized TK score. We report the average score across all validation samples.
* **Faithfulness** states that the explanations should reflect the inner working of the model. We introduce Model-induced Sample-Concept Distance (MSCD) to evaluate this, calculated as the

 
**Dataset Name** & **No. Class** & **No. Att** & **No. Images** & **Age. Tree Depth** & **Rationalness** & **Hierarchy** & **Validation Only** \\  AWA2  & 50 & 85 & 37,322 & N/A & âœ“ & \(\) & \(\) \\ CUB  & 200 & 8,001 & 11,788 & N/A & âœ“ & \(\) & \(\) \\ BDU  & 906 & 1,668 & 26,000 & N/A & âœ“ & \(\) & \(\) \\ VAV  & N/A & 650 & 27,234 & N/A & \(\) & \(\) & \(\) \\ COCO  & 29 & 196 & 10,000 & N/A & \(\) & \(\) & \(\) \\ DR-CIFAR-10  & 10 & 60 & 2,201 & N/A & âœ“ & \(\) & \(\) \\ DR-CIFAR-100  & 100 & 540 & 18,318 & N/A & âœ“ & \(\) & \(\) \\ DR-ImageNet  & 1,000 & 5,310 & 217,016 & N/A & âœ“ & \(\) & \(\) \\  H-CIFAR-10 & 10 & 289 & 10,000 & 4.3 & âœ“ & âœ“ & âœ“ \\ H-CIFAR-100 & 100 & 2,393 & 10,000 & 4.5 & âœ“ & âœ“ & âœ“ \\ H-ImageNet & 1,000 & 26,928 & 50,000 & 4.8 & âœ“ & âœ“ & âœ“ \\  

Table 1: Data annotation statistics. The \(*\) indicates the number of video frames. We compare the statistics of category, attributes, image and tree depth across different explanatory datasets. Our dataset stands out as the first hierarchical dataset, offering a wide range of attributes.

Figure 5: _Plausibility_ comparison on three visual tree parsing benchmarks. We plot the mean\(\)std across all networks architectures. For both scores, higher values indicate better performance.

average of point-to-set distances \(}_{v V}D(_{j},P_{v})\) between all test samples and tree nodes, reflecting the alignment between generated explanation and model's internal logic. The concept is simple: if the explanation tree aligns with the model's internal representation, the MSCD is minimized, indicating high faithfulness.
* **Stability** evaluates the resilience of the explanation graph to minor input variation, expecting minimal variations in explanations. The MCS/TK metrics are used to assess stability by comparing explanations derived from clean and slightly modified inputs. We include 3 perturbations, including Gaussian additive noise with \(\{0.05,0.1\}\) and Cutout  augmentation.

**Baselines.** We construct three baselines for comparisons: Constant, using the full category template tree; Random, which selects a subtree randomly from the template; and Subtree, choosing the most common subtree in the test set for explanations. Additionally, we consider TrDec Baseline , a strategy utilizing a tree-topology RNN decoder on top of image encoder. Given the absence of hierarchical annotations, the CLIP model verifies nodes in the template trees, serving as pseudo-labels for training. We only update the decoder parameters for interpretation purposes. These models provide a basic comparison for the performance of LVX. More details are in the appendix.

For classification performance, we compare LVX-calibrated model with neural-tree based solutions, including a Decision Tree (DT) trained on the neural network's final layer, DNDF , and NBDT .

**Models to be Explained.** Our experiments cover a wide range of neural networks, including various convolutional neural networks (CNN) and transformers. These models consist of VGG , ResNet , DenseNet , GoogLeNet , Inceptionv3 , MobileNet-v2 , and Vision Transformer (ViT) . In total, we utilize 12 networks for CIFAR-10, 11 networks for CIFAR-100, and 8 networks for ImageNet. For each model, we perform the tree refinement for 5 iterations.

**Calibration Model Training.** As described in Section 3.3, we finetune the pre-trained neural networks with the hierarchical contrastive loss based on the explanatory results. The model is optimized with SGD for 50 epochs on the training sample, with an initial learning rate in \(\{0.001,0.01,0.03\}\) and a momentum term of 0.9. The weighting factor is set to 0.1. We compare the calibrated models with the original ones in terms of accuracy and explanation faithfulness.

### LLM helps Visual Interprebility

**Plausibility Results.** We evaluated LVX against human annotations across three datasets, using different architectures, and calculating MCS and TK scores. The results, shown in Figure 5, reveal LVX outperforms baselines, providing superior explanations. Notably, TrDec, even when trained on CLIP induced labels, fails to generate valid attributes in deeper tree layers--a prevalent issue in long sequence and structure generation tasks. Meanwhile, SubTree lacks adaptability in its explanations, leading to lower scores. More insights are mentioned in the appendix.

**Faithfulness Results.** We present the MSCD scores for ResNet-18(RN-18), ResNet-50(RN-50), and ViT-S, contrasting them with SubTree and TrDec in Table 3. Thanks to the incorporation of tree refinement that explicitly minimizes MSCD, our LVX method consistently surpasses benchmarks, demonstrating lowest MSCD values, indicating its enhanced alignment with model reasoning.

**Stability Results.** The stability of our model against minor input perturbations on the CIFAR-10 dataset is showcased in Table 2, where MCS/TK are computed. The "Clean" serves as the oracle baseline. Our method, demonstrating robustness to input variations, retains consistent explanation results (MCS\(>\)60, TK\(>\)85). In contrast, TrDec, dependent on an RNN-parameterized decoder, exhibits higher sensitivity to feature variations.

    &  &  &  &  \\  & & & \((=0.05)\) & \((=0.1)\) & \((n_{}=1)\) \\   & RN-18 & 100 & 100 & 65.3 & 86.4 & 56.2 & 82.5 & 65.4 & 86.0 \\  & RN-18 & 100 & 100 & **69.7** & **90.8** & **62.1** & **86.5** & **61.8** & **83.8** \\   & RN-50 & 100 & 100 & 68.3 & 88.5 & 59.3 & 84.2 & 66.2 & 86.9 \\  & RN-50 & 100 & 100 & **71.9** & **92.1** & **65.6** & **88.3** & **69.3** & **90.1** \\   

Table 2: _Stability comparison in CIFAR10 under input perturbations._

    &  &  &  \\   & & TrDecSubTree & LVX & TrDecSubTree & LVX & TrDecSubTree & LVX \\   & 0.224 & -0.393 & **-0.971** & -0.246 & -0.446 & **-0.574** & -0.298 & -0.548 & **-0.730** \\  & 0.236 & -0.430 & **-1.329** & -0.256 & -0.500 & **-1.170** & -0.317 & -0.588 & **-1.186** \\  & VIT-S & 16 & 0.244 & -0.467 & **-1.677** & -0.266 & -0.527 & **-1.073** & -0.330 & -0.626 & **-1.792** \\   

Table 3: _Faithfulness_ comparison by computing the MSCD score. Smaller the better.

**Model and Data Diagnosis with Explanation.** We visualize the sample explanatory parse tree on ImageNet validation set induced by ViT-B in Figure 6. The explanations fall into three categories: (1) correct predictions with explanations, (2) incorrect predictions with explanations, and (3) noisy label predictions with explanations. We've also displayed the 5 nearest neighbor node for each case.

What's remarkable about LVX is that, even when the model's prediction is wrong, it can identify correct attributes. For instance, in a case where a "white shark" was misidentified as a "killer whale" (b-Row 2), LVX correctly identified "fins", a shared attribute of both species. Moreover, the misrecognition of the attribute "wide tail flukes" indicates a potential error in the model, that could be later addressed to enhance its performance.

Surprisingly, LVX is able to identify certain noisy labels in the data, as shown in c-Row 2. In such cases, even experienced human observers might struggle to decide whether a "pig bank with band" should be classified "piggy bank" or "band aid". It again underscores the superior capabilities of our LVX system in diagnosing the errors beyond model, but also within the data itself.

**Calibration Enhances Interpretability and Performance.** Our approach involves fine-tuning a pre-trained model with the loss function outlined in Section 3.3, using parsed explanatory trees to improve model performance. Table 4 compares the classification performance of our model with that of other neural tree methods. Our model clearly outperforms the rest.

Neural tree models often face challenges in balancing interpretability with performance. In contrast, LVX achieves strong performance without relying on a strict decision tree. Instead, decisions are handled by the neural network, with concepts guided by the LLM through Equation 3. This approach enhances the model's ability to disentangle visual concepts while preserving high performance.

In addition, we compared the quality of the generated parsed tree with or without calibration, in Figure 5. The calibration process not only improved model performance, but also led to more precise tree predictions, indicating enhanced interpretability. We also test the calibrated model on OOD evaluations in Appendix, where we observe notable improvements.

## 5 Ablation Study and Analysis

In this section, we present an ablation study on the refinement stage of LVX. We also apply the method to different neural networks to observe variations in model's behavior.

**Ablation 1: No Refinement.** To study the impact of refinement stage, we present a baseline called _w/o Refine_. In this setup, the initial tree generated by LLMs is kept fixed. We evaluate the method using the MSCD for faithfulness and MCS for plausibility on the CIFAR-10 and CIFAR-100 datasets.

The results show in Table 6 that incorporating image model feedback indeed improves tree alignment with the classifier's internal representation, as reflected in higher MCS scores. The refined trees also better match human-annotations.

**Ablation 2: Refinement Criteria.** In our original method, tree refinement is based on feature similarity to the training set. To explore an alternative, we use _average activation magnitude_ on generated data as the criterion for concept familiarity. Concepts with activation magnitudes \(\) are pruned. This method, referred to as _ActMag_, is evaluated on CIFAR-10. We report the MCS, MSCD for performance, and average tree depth as an indicator of tree complexity.

Table 7 shows that feature similarity achieves better results than _ActMag_. Specifically, setting a threshold is challenging for _ActMag_, leading shallow trees (\(=0.3\)) or too deep ones (\(=0.01\)).

**Analysis: CNN vs. Transformer.** We use our LVX to compare CNN and Transformer models and identify which concepts they miss. We compared ConvNeXt-T (CNN) and DeiT-B (Transformer) on 26,928 concepts we collected on ImageNet, from sub-categories of _Concepts_, _Substances_, _Attributes_, and _Environments_. We measured accuracy across 4 sub-categories and tree depths.

Results show that ConvNeXt-T is better at local patterns (Attributes, Substances), while DeiT-B perform better on Environments which needs global semantics. Additionally, DeiT-B is more accurate at shallow depths, whereas ConvNeXt-T performs better at deeper levels. These findings aligns with earlier research showing that CNN are biased towards textures over shape [23; 84].

## 6 Conclusion

In this study, we introduced LVX, an approach for interpreting vision models using tree-structured language explanations without hierarchical annotations. LVX leverages large language models to connect visual attributes with image features, generating comprehensive explanations. We refined attribute parse trees based on the model's recognition capabilities, creating human-understandable descriptions. Test samples were routed through the parse tree to generate sample-specific rationales. LVX demonstrated effectiveness in interpreting vision models, offering potential for model calibration. Our contributions include proposing LVX as the first approach to leverage language models for explaining the visual recognition system. We hope this study potentially advances interpretable AI and deepens our understanding of neural networks.