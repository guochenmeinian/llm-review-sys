# Adversarially Robust Distributed Count Tracking via Partial Differential Privacy

Zhongzheng Xiong

School of Data Science

Fudan University

zzxiong21@m.fudan.edu.cn

&Xiaoyi Zhu

School of Data Science

Fudan University

zhuxy22@m.fudan.edu.cn

&Zengfeng Huang

School of Data Science

Fudan University

huangzf@fudan.edu.cn

Corresponding author

###### Abstract

We study the distributed tracking model, also known as distributed functional monitoring. This model involves \(k\) sites each receiving a stream of items and communicating with the central server. The server's task is to track a function of all items received thus far continuously, with minimum communication cost. For count tracking, it is known that there is a \(\) gap in communication between deterministic and randomized algorithms. However, existing randomized algorithms assume an "oblivious adversary" who constructs the entire input streams before the algorithm starts. Here we consider adaptive adversaries who can choose new items based on previous answers from the algorithm. Deterministic algorithms are trivially robust to adaptive adversaries, while randomized ones may not. Therefore, we investigate whether the \(\) advantage of randomized algorithms is from randomness itself or the oblivious adversary assumption. We provide an affirmative answer to this question by giving a robust algorithm with optimal communication. Existing robustification techniques do not yield optimal bounds due to the inherent challenges of the distributed nature of the problem. To address this, we extend the differential privacy framework by introducing "partial differential privacy" and proving a new generalization theorem. This theorem may have broader applications beyond robust count tracking, making it of independent interest.

## 1 Introduction

In the distributed tracking model there are \(k\) sites and a single central server. Each site \(i\) receives items over time in a streaming fashion and can communicate with the server. Let \(S_{i}(t)\) be the stream that site \(i\) observes up to time \(t\). The sever wants to track the value of a function \(f\) that is defined over the multiset union of \(\{S_{i}(t) i=1, k\}\) at all times. The goal is to minimize the communication cost, which is defined as the total number of words communicated between the server and all sites. Due to strong motivations from distributed system applications, this model has been extensively investigated, e.g., . The theoretical study of communication complexity was initiated by . Count tracking is the most basic problem in distributed tracking, where \(f\) is simply the total number of items received so far. Since exact tracking requires sites to communicate every time an item arrives, incurring too much communication, the objective is to trackan \((1+)\)-approximation. For this problem, there is a simple deterministic algorithm with \((k/)\)2 communication. Huang et al.  proposed a randomized algorithm that achieves \((/)\), but the correctness is under the assumption of an oblivious adversary, i.e., input streams are constructed in advance and are just given to sites one item at a time. In particular, the analysis assumes the input is independent of the algorithm's internal randomness. In interactive applications, this assumption is often unrealistic; the adversary can generate the next item based on previous answers from the server, making the independence assumption invalid. Moreover, the break of independence may occur unintentionally. For example, the tracking algorithm may be part of a larger system; the output of the algorithm can change the environment, from which the future input to the algorithm is generated. In such cases, we can no longer assume the independence between inputs and algorithm's internal state. The main question of this paper is: _Whether the \(\) advantage of randomized count tracking algorithms is from randomness itself or the oblivious adversary assumption?_

Designing robust randomized algorithms against adaptive adversaries has received much attention recently . Existing research focuses on centralized settings, and in this paper, we initiate the study of adversarial robustness for distributed tracking. We provide a new randomized algorithm with communication \((/)\), which provably tracks the count within an \(\) relative error at all times, even in the presence of an adaptive adversary. As in , we utilize differential privacy (DP) to construct robust tracking algorithms. The main idea is to use DP to protect the internal randomness, so that the adversary cannot learn enough information about it to generate bad inputs. However, due to the "event-driven nature" of distributed tracking algorithms, we cannot protect the randomness in the usual sense of DP (which will be elaborated in more details below). Thus, the DP framework of  is not directly applicable.

To address this difficulty, a relaxed version of differential privacy, called _partial differential privacy_, is introduced. Moreover, a new generalization theorem for partial DP is proved. In partial DP we allow an arbitrary small subset of the database to be revealed, and only require the privacy of the remaining dataset is protected. The power of the new definition comes from the fact that the privacy leaked set can be chosen by the algorithm after the interaction with the adversary and the set can depend on the actual transcript. Intuitively, an interactive mechanism satisfies partial DP as long as _after_ the interaction, we can always find a large subset whose privacy is protected. On the other hand, since the set we try to protect is not fixed in advance, it is subtle to give the right notion of "protecting the privacy of a large part of the data". Besides this new notion of DP, our algorithm deviates from the framework of  in many other details. For instance, our algorithm does not treat existing oblivious algorithms as a black box; instead, we directly modify oblivious algorithms and perform a more fine-grained privacy analysis. The contributions of this paper are summarized as follows:

1. We initiate the study of adversarially robust distributed tracking and propose the first robust counting tracking algorithm with near optimal communication.
2. To overcome the inherent challenges that arise from the distributed nature of the problem, we introduce a relaxed (and more general) version of differential privacy and prove a new generalization theorem for this notion. We believe that this new generalization theorem can be of independent interest and may have broader applications beyond count tracking.

### Problem Definitions and Previous Results

Throughout this paper, we use \(\) to denote the tracking algorithm/mechanism and \(\) to denote the adversary. \(N\) is used to denote the total number of items.

#### The model and its event-driven nature

We assume there exists a point-to-point communication channel between each site and the server, and communication is instantaneous with no delay. It is convenient to assume that the time is divided into discrete time steps. In each step, the adversary picks one site and sends it a new item. The adversary is also allowed to skip the step and do nothing, and because of this, algorithms that can trigger new events based on the global time do not have an advantage over purely event-driven ones. For example, the server may have wanted to wait a random number of time steps before updating the output, but the adversary can always skip a large number of steps before sending the next item, which makes the waiting meaningless. That being said, it is not a restriction to only consider _event-driven_ algorithms: the internal state of each site changes only when it receives a new item or a new message from the server, and the server's state changes only if a new message from sites arrives.

Distributed count trackingThe goal of a count tracking algorithm \(\) is to output an \((,)\)-approximation of the total number of items received by all sites. More specifically, with probability at least \(1-\), the output of \(\) is \((1)\)-accurate with respect to the true answer at all time steps simultaneously. We measure the complexity of the algorithm by the total communication cost between the server and all sites. Consistent with prior research, communication cost is expressed in terms of words unless otherwise stated. We assume that any integer less than \(N\) can be represented by a single word. To simplify the presentation, we assume \(k}\). The case \(k>}\) can be solved with the same technique, with an extra additive \(O(k N)\) term in the communication complexity3.

The adversarial modelThe setting can be viewed as a two-player game between the tracking algorithm \(\) and the adversary \(\). At each time step \(t\),

1. \(\) generates a pair \(u_{t}=(i,x)\), where \(x\) is the item and \(i\) is the site to send \(x\) to; and \(u_{t}\) depends on the previous items and previous outputs of \(\).
2. \(\) processes \(u_{t}\) and outputs its current answer \(a_{t}\).

The interaction between \(\) and \(\) generates a transcript \(=(u_{1},a_{1},u_{2},a_{2},)\). The objective of \(\) is to cause \(\) to output an incorrect answer at some step \(t\).

Existing results on count trackingPrevious results and their main ideas are discussed here.

_Deterministic complexity._ There is a simple deterministic solution to count tracking. Each site notifies the server every time their counter increases by a factor of \(1+\). Then, the server always maintains an \(\)-approximation to each site's counter, and their sum is an \(\)-approximation to the total count. It is easy to see the communication complexity of this algorithm is \(O( N)\). We note deterministic algorithms are trivially robust to adaptive inputs. A deterministic communication lower bound of \(( N)\) was proved in .

_Randomized complexity._ A randomized algorithm with \(O(}{} N)\) communication and constant error probability was proposed in , which was shown to be optimal in the same paper. The main idea of their algorithm is as follows. Let \(N\) be the current number of items. Unlike the above deterministic algorithm, in which each site notifies its local count according to deterministic thresholds, now the thresholds are set randomly. Let \(e_{i}\) be the discrepancy between the true local count on site \(i\) and its estimation on the server, and the total error is \(e=_{i}^{k}e_{i}\). For deterministic algorithms, all \(e_{i}\) could have the same sign in the worst case, so on average, \(e_{i}\) has to be less than \( N/k\). On the other hand, in the randomized algorithm, each \(e_{i}\) is a random variable. Suppose \([e_{i}]( N)^{2}/k\) for each \(i\), the total variance \([e]( N)^{2}\), and it is sufficient to obtain an \(\)-approximation with constant probability by standard concentration inequalities. Compared to deterministic estimators, now each local error \(e_{i}\) may far exceed \( N/k\).

_Robustness to adaptive inputs._ In the randomized approach described above, the analysis crucially relies on the independence assumption on \(e_{i}\)'s, since otherwise the variances do not add up and concentration inequalities cannot be applied. When the adversary is oblivious, the independence holds as long as each site uses independent random numbers. However, in the adaptive setting, this does not hold any more, and it becomes unclear whether the \(\) improvement is still achievable.

### Existing Robust Streaming Frameworks

Distributed tracking is a natural combination of streaming algorithms  and communication complexity . Robust streaming algorithms design has become a popular topic recently and several interesting techniques have been proposed. Next, we provide a brief overview on the existing frameworks for robust streaming algorithms. Let \(\) be the target function, for example, the number of distinct elements.

**Sketch switching** Given a stream of length \(N\) and an accuracy parameter \(\), the flip number, denoted as \(_{,N}\), is the number of times that the target function \(\) changes by a factor of \(1+\). For insertion-only streams and a monotone function \(\), \(= N\). In sketch switching, we initialize \(\) independent copies of an oblivious algorithm, and items in the stream are fed to all copies. The stream can be divided into \(O()\) phases; in each phase \(\) increases roughly by a factor of \(1+\). During the \(j\)th phase, the output remains the same, and the \(j\)th copy is used for tracking the value. When the estimate (from the \(j\)th copy) has become larger than the last released output by a factor of \((1+)\), the output is updated and \(j j+1\). The robustness holds because each copy is utilized no more than once, and once its randomness is revealed, the algorithm switches to a new copy. The space complexity is \(\) times the space of the oblivious algorithm. Applying sketch switching on the algorithm of  results in a robust count tracking algorithm. However, the communication complexity increases by a factor of \(\), which can be worse than the deterministic bound.

Difference estimatorWoodruff et al.  refined the sketch switching approach significantly and proposed the difference estimator (DE) framework. Informally, instead of using oblivious sketches as the switching unit, the DE framework divides the stream into blocks and uses sketches on each block as switching units. Consider a part of the stream, denoted by \(S\), in which the value of \(\) increases from \(F\) to \(2F\). A technique called difference estimator (DE) was proposed to estimate the difference between values of \(\) at current time \(t\) and some earlier time \(t_{0}\). The estimator is generated by maintaining \(=\) levels of DEs. In level \(1\), \(S\) is divided into \(\) blocks and the value of \(\) increases by \( F\) in each block. In the \(j\)th level, \(S\) is divided into \(}\) blocks, and the DEs produce estimators with additive error \( F\).  proved that for many important problems, the space complexity of such DEs is \(()2^{j-1}\), where \(()\) is the space complexity of \(\) in the oblivious setting. Since there are \(}\) DEs on the \(j\)th level, the total space of level \(j\) is \(()\) and the space is \(()\) over all levels. Since blocks from all levels form a dyadic decomposition, the final estimator is the sum of \(\) DEs, one from each level. Thus, the total error is \( F\), and by adjusting \(\) in the beginning by a factor of \(\), this produces the desired error. Applying the DE framework to distributed tracking, the communication bottleneck is from level \(1\), where there are \(1/\) DEs. It requires synchronization at the beginning of each block, so that all sites and the server are able to agree to start a new DE. A synchronization incurs \(k\) communication; thus, even ignoring other cost, the total cost is at least \(\), which is no better than the deterministic bound.

Differential privacyHassidim et al.  proposed a framework using tools from DP. Instead of switching to fresh sketches, this framework protects the randomness in the sketch using DP. The random bits in each copy of the oblivious sketch are viewed as a data point in the database, and the adversary generates an item (considered as a query in DP) at each time and observes the privatized output. By the generalization theorem of DP, if the interaction transcript satisfies DP w.r.t. the random bits, then the error in the output is close to the error of an oblivious algorithm in the non-adaptive setting (the closeness depends on the magnitude of the noise injected). Similar as in sketch switching, the output is updated only when it changes by a \((1+)\) factor, and thus there are only \(\) time steps in which the adversary observes "useful information". Therefore, it is not surprising that the sparse vector technique  is applied. By the advanced composition theorem of DP , \(()\) independent copies of the oblivious algorithm is enough for \(\) outputs. Therefore, compared with sketch switching, the space increases by only a factor of \(\). Attias et al.  gave an improvement by incorporating difference estimator to the DP framework.

However, there is a fundamental challenge in applying the DP framework to distributed tracking. As discussed in Section 1.1, all distributed tracking algorithms are essentially event-driven. Now let us focus on a time step \(t\) where the server updates its output. Because of the event-driven nature, this update is triggered by the event that some site \(i\) just sent a message. Similarly, site \(i\) sending the message is also triggered by another event, and so on and so forth. The start of this event chain must be that the adversary sends an item to some site \(j\), triggering \(j\) to send the first message. This causes additional privacy leakage. For example, suppose whether to send a message is indicated by a binary function \(f(n_{j},r_{j})\) where \(r_{j}\) is the random number used in the tracking algorithm and \(n_{j}\) is the local count on site \(j\). At time \(t\), the adversary knows \(f(n_{j},r_{j})=1\), which makes the algorithm have no privacy guarantee. This problem is attributed to the fact that the server can update the output only after it receives a message. So to achieve the desired level of privacy, one has to add noise to \(f\) locally on each site, but the total noise from all sites can be too large.

### Our Method

**Technical overview** In our algorithm, each site divides its stream into continuous blocks of size \(=(})\). For each block \(j\), the site draws a random integer \(r_{j}\) with uniform distribution in \([]\). The site sends a message to the server when the number of items in a block \(j\) reaches the threshold \(r_{j}\). The server output \(m\), where \(m\) is the number of messages received from all sites. By a similar analysis as in , the estimator has \( N\) additive error. To robustify this algorithm, we also use DP. To overcome the limitations of the existing DP framework, we make several critical changes. First, instead of running multiple independent copies of the oblivious algorithm, we run a single copy of the above oblivious algorithm. Secondly, we perform a more refined privacy analysis, in which each random number \(r_{j}\) is treated as the privacy unit. Therefore, the analysis framework is quite different from . Thirdly, and most importantly, we do not require the algorithm to have a privacy guarantee in the traditional sense; instead, we privatize the algorithm so that, at any time, we can always find a large set of random numbers whose privacy is protected. However, it is unclear how to change the original DP definition to capture the meaning of "protecting a large subset of the dataset", since this set depends on the current transcript, and may change at each time step. Moreover, for this weaker DP, we need to prove that the generalization theorem still holds. To this end, we introduce partial DP and prove a generalization theorem for it. We believe partial DP is quite general and will have more applications beyond robust distributed tracking. The main results of this paper is summarized in the next theorem.

**Theorem 1** (Main theorem).: _With probability \(1-\), our mechanism \(\) comprised of Algorithm 1 and 2 outputs an \(\)-approximate to the total count at all times in the adversarial setting. The communication complexity is \(O( N}{})\) where \(C=)^{1.5}+1)( N}{})}\)._

Compared to the optimal randomized bound in the oblivious setting, the cost of handling adaptive adversaries is at most an extra factor of \(C\).

## 2 Preliminaries

**Notation** Let \(\) be the space of all possible transcripts of the interaction between \(\) and \(\). We use \(\) to denote the transcript random variable, \(\) to denote a realization of \(\). The Laplace distribution with \(0\) mean and \(2b^{2}\) variance is denoted by \((b)\). We use the notation \(S^{m}\) to indicate that \(S\) is a dataset comprised of \(m\) i.i.d samples from distribution \(\). The conditional distribution of \(S\) given the transcript \(\) is represented by \(_{}\). The query function is denoted by \(q:^{m}\) and \(q_{t}\) denotes the query function at time step \(t\). If \(q_{t}\) is a linear query, then \(q_{t}(S)=_{i=1}^{d}q_{t,i}(S_{i})\), where \(q_{t,i}:\) is a sub-query function on a single sample. The expectation of \(q\) over the distribution \(^{m}\) is denoted by \(q(^{m})=_{S^{m}}[q(S)]\). And \(q(Q_{})=_{S Q_{}}[q(S)]\).

**Differential privacy** Let \(S^{m}\) be the database that \(\) needs to protect, for example in our case, the random numbers (thresholds) in \(\). Denote the interaction between \(\) and \(\) by \(I(,;S)\).

**Definition 1** (Differential Privacy).: \(\) _is \((,)\)-differentially private if for any \(\), any two neighboring database \(S S^{}^{m}\) differing only in one position, and any event \(E\), we have_

\[_{ I(,;S)}[ E] e^{ }_{ I(,;S^{})}[  E]+.\]

**Lemma 1** (Laplace Mechanism ).: _Let \(x,x^{}\) and \(|x-x^{}| l\). Let \((l/)\) be a Laplace random variable. For any measurable subset \(E\), \([x+ E] e^{}[x^{}+ E]\)._

**Private continual counting** Consider the _continual counting problem_: Given an input stream consists of \(\{0,1\}\), continual counting requires to output an approximate count of the number of \(1\)'s seen so far at every time step. Different techniques have been proposed to achieve differential privacy under continual observation [29; 30]. In this paper, we make use of the Binary Mechanism (BM)  (see appendix for its pseudo code).

**Theorem 2**.: () BM is \((,0)\)-differentially private with respect to the input stream. With probability at least \(1-\), the additive error is \(O(( T)^{1.5}())\) at all time steps \(t[T]\).

**Remark**.: _Note that although the input of BM is bits, it can directly extend to real numbers without any modification. The same privacy and utility guarantees hold._

**Generalization by differential privacy** The generalization guarantee of differential privacy arises from adaptive data analysis. Existing research  has shown that any mechanism for answering adaptively chosen queries that is differentially private and sample-accurate is also accurate out-of-sample.

**Definition 2** (Accuracy).: \(\) _satisfies \((,)\)-sample accuracy for adversary \(\) and distribution \(\) iff_

\[_{S^{m}, I(,;S)}[_{t} |a_{t}-q_{t}(S)|],\]

_where \(a_{t}\) is the output of \(\) and \(q_{t}\) is the query given by \(\) at time \(t\). \(\) satisfies \((,)\)-distributional accuracy iff_

\[_{S^{m}, I(,;S)}[_{t }|a_{t}-q_{t}(^{m})|].\]

Recently, Jung et al.  discovered a simplified analysis of the generalization theorem by introducing the posterior data distribution \(_{}\) as the key object of interest. Through a natural resampling lemma, they showed that a sample-accurate mechanism is also accurate with respect to \(_{}\).

**Lemma 2** ().: _Suppose that \(\) is \((,)\)-sample accurate. Then for every \(c>0\) it also satisfies:_

\[_{S^{m}, I(,;S)}[_{t }|a_{t}-q_{t}(_{})|>+c] .\]

Thus, to achieve distributional accuracy, it suffices to prove the closeness between \(_{}\) and \(^{m}\). Then they showed that this can be guaranteed by differential privacy.

**Lemma 3** ().: _If \(\) is \((,)\)-differentially private, then for any data distribution \(\), any analyst \(\), and any constant \(c>0\):_

\[_{S^{m}, I(,;S)}[_{t }|q_{t}(^{m})-q_{t}(_{})|>(e^{ }-1)+2c].\]

## 3 Partial Differential Privacy and Its Generalization Property

In the definition of partial DP, we specify the set of data whose privacy is leaked via a mapping \(f_{L}: 2^{[m]}\). Given a transcript \(\), partial DP guarantees privacy only on \(S f_{L}()\). Intuitively, this means that the transcripts on two database \(S,S^{}\) that differs only on a position \(i S f_{L}()\) have similar distributions. However, \(\) is not known in advance, which causes trouble to this direct definition. We remedy this by first fixing \(i\); then we only consider \(S,S^{}\) that differs only on \(i\) and only those events \(E\) whose elements do not contain \(i\) in their privacy leaked set.

**Definition 3** (Partial Differential Privacy).: \(\) _is \((,,)\)-partial differentially private, if there exists a privacy leak mapping \(f_{L}: 2^{[m]}\) with \(_{}|f_{L}()|\), the following holds: for any \(\), any \(i\), any \(S,S^{}^{m}\) that differs only on the \(i\)th position, and any \(E\) such that \(i_{ E}f_{L}()\),_

\[_{ I(,;S)}[ E] e^{ }_{ I(,;S^{})}[  E]+.\]

A generalization theorem for partial DP is presented below. Generalization for linear queries suffices for our application, but this can be extended to general low-sensitivity queries.

**Theorem 3**.: _For linear queries, if \(\) satisfies \((,,)\) partial differential privacy and \(\) is \((,)\)-sample accurate, then for any data distribution \(\), any adversary \(\), and any constant \(c,d>0\):_

\[_{S^{m}, I(,;S)}[_{t }|a_{t}-q_{t}(^{m})|>+(e^{}-1)++c+2d]+.\]

Compared with existing results, it has an extra term \(\) in the error. This is intuitive, as the privacy leaked set contributes at most \(\) error in the worst case. Following , it suffices to establish the low discrepancy between \(_{}\) and \(^{m}\). To this end, we prove the following key lemma in the appendix.

**Lemma 4**.: _If \(\) satisfies \((,,)\) partial differential privacy, then for any data distribution \(\), any adversary \(\), and any constant \(c>0\):_

\[_{S^{m}, I(,;S)}[_{t}|q _{t}(_{})-q_{t}(^{m})|>(e^{}-1)++2c].\]

The query function of interest in this paper only depends on \(\) data points, with \( m\), at each time step. For such queries, we expect the total error to be proportional to \(\) rather than \(m\), which is formalized in the following refinement of Theorem 3, the proof of which requires only a slight modification and is included in the appendix.

**Theorem 4**.: _For linear queries, if \(\) satisfies \((,,)\) partial differential privacy and \(\) is \((,)\)-sample accurate. Further, if each linear query depends on at most \(\) data points, then for any data distribution \(\), any adversary \(\), and any constant \(c,d>0\):_

\[_{S^{m}, I(,;S)}[_{t} |a_{t}-q_{t}(^{m})|>+}{m}(e^{}-1+c)+ +2d]+.\]

## 4 Robust Distributed Count Tracking Algorithm

Our algorithm has multiple rounds. In each round, \(N\) increases roughly by a factor of \(1+\). After a round ends, the true count is computed and sent to all sites, and then, the algorithm is reinitialized with fresh randomness. Thus, we only focus on one round, and let \(N_{0}\) be the true count at the beginning of the round. In the algorithm, \(C=)^{1.5}+1)(}{})}\) and \(=}{8C}\).

The algorithm on site \(i\) is presented in Algorithm 1. The site divides its own stream into blocks of size \(\), and exactly one bit will be sent to the server in each block. The actual time of sending the bit is determined by a random threshold \(r_{ij}\). Let \(N_{t,i}\) be the number of items received on site \(i\) from the beginning of the current round until time \(t\). Let \(d_{t,i}= N_{t,i}/\) and \(e_{t,i}=N_{t,i}\). Thus, \(j=d_{t,i}+1\) is the index of the current active block, and \(e_{t,i}\) is the offset in this block. As per Algorithm 1, the number of bits sent by site \(i\) up to time \(t\) is \(B_{t,i}=d_{t,i}+[r_{i,j}<e_{t,i}] d_{t,i}+1\). Since \(r_{i,j}(0,)\), \([B_{t,i}]=d_{t,i}+}{}\), meaning \( B_{t,i}\) is an unbiased estimate of \(N_{t,i}\). Let \(D(0,)^{m}\) (\(m=k k^{}\)) be the database comprised of all sites' random numbers, i.e., \(D_{ij}=r_{ij}\), considering the input generated by the adversary as queries, then the query at time \(t\) can be specified as:

\[q_{t}(D)=(_{i[k]}d_{t,i}+_{i[k]}[r_{i,j}<e_{t,i}])=}{m},\] (1)

where \(B_{t}=_{i=1}^{k}B_{t,i}\) denotes the total number of bits received by the server at time \(t\). The value of \(q_{t}\) at each time step depends on \(k\) random numbers corresponding to the active blocks, which is much less than the size of \(D\), which motivates the use of Theorem 4.

Let \(_{t}\) be algorithm's estimate of \(B_{t}\). Algorithm 2 consists of \(\) phases; \(_{t}\) remains constant in each phase and a new estimate \(_{j}\) is obtained at the end of the \(j\)th phase via the binary mechanism. The times that the phases end are denoted by \(H=\{t_{1},t_{2},,t_{}\}\), and for \(t[t_{j},t_{j+1})\), we have \(_{t}=_{j}\). Therefore, the transcript generated by \(\) and \(\) is of the form \(((,0),(,0),,(,_{1}),(,_{1}),,(, _{}))\). We note, in addition to noise in BM, the only noise added for the purpose of DP is the Laplace random variable added on \(T\), and an independent noise is used in each phase.

### Privacy Analysis

In this section, we analyze the privacy of \(\) for a single round, demonstrating that it satisfies \((,0,)\)-partial differential privacy with respect to the random numbers \(D\) used by all sites. To achieve this, we first provide the privacy leak mapping. Note that each time the output of \(\) updates, i.e, reporting \(\), \(\) knows the site \(i\) it just accessed has sent a bit to the server. Then the active randomnumber \(r_{ij}\) at this moment is exposed, which means there is no meaningful privacy guarantee4. Therefore, we need to relax the DP constraint. Given a transcript \(\), the privacy leaked set consists of those \(r_{ij}\) that are exposed during the execution.

**Definition 4** (Privacy Leaked Set).: _For a given transcript \(\), let \(A\) be the set of time steps when output updates, i.e. \(A=\{t_{t}=(,)\}\) where \(_{t}\) is the output at time \(t\). Let \(i_{t}\) be the site \(\) chooses at time \(t\). Then, \(f_{L}()=\{(i_{t},j_{t}) t A(),j_{t}= N_{t,i_{t}}/ +1\}\)._

Since there is one data point exposed in each phase, \(\). The privacy guarantee with this privacy leak mapping is presented below.

**Lemma 5**.: _For any two neighboring databases \(D D^{}\) that differ only on \((i,j)\), any transcript \(\) satisfying \((i,j) f_{L}()\), our mechanism (Algorithm 1 and 2) satisfies the following inequality,_

\[e^{-}_{ I(,;D^{})}[ =]_{ I(,;D)}[=] e^{ }_{ I(,;D^{})}[=],\]

_i.e., it satisfies \((,0,)\)-partial differential privacy._

### Accuracy and Communication

Observe that the size of \(D\) is \(m=k k^{}\), and each query \(q_{t}\) depends only on \(=k\) of them. In the algorithm \(q_{t}(D)\) is estimated by \(_{t}/m\). Suppose it is \((,)\)-sample accurate, then by Lemma 5 and Theorem 4, we have

\[_{D^{m}, I(,;D)}_{t }|_{t}}{m}-q_{t}(^{m})|>+( e^{}-1+c)+}{m}.\]

Note that \(=C/\). By setting \(c=1/\), we get the following lemma.

**Lemma 6**.: _For query function \(q_{t}\) defined in equation (1), if our mechanism \(\) is \((,)\)-sample accurate for \(q_{t}\), it is \((}{m}+,)\)-distributional accurate._

By Theorem 2, with probability \(1-\), for all \(t_{j} H\), we have that:

\[|_{t_{j}}-B_{t_{j}}|( )^{1.5}(}{}) C ,\] (2)

where the second inequality is from \(=C/\) and the definition of \(C\). Denote the Laplace variables used in Algorithm 2 as \(\{_{1},,_{}\}\). By the union bound, with probability \(1-\), \(|_{j}|/)}{}\) for all \(j[]\). Consider the \((j+1)\)th phase. For every time step \(t(t_{j},t_{j+1})\), since \(B_{t}-B_{t_{j}} T+_{j+1} 2C+/)}{ }\) and \(_{t}=_{t_{j}}\), we have

\[|_{t}-B_{t}|=|_{t_{j}}-B_{t}|=|_{t_{j}}-B_{t_{j}}+B_{t_{j}}-B_{t}| 3C+/ )}{} 4C.\] (3)

Then \(_{t}/m\) is \((4C/m,)\)-sample accurate with respect to \(q_{t}(D)\). By Lemma 6, it follows that \(_{t}/m\) is \((8C/m,)\)-accurate w.r.t. \(_{D^{m}}[q_{t}(D)]=N_{t}/m\). We establish the following lemma of the accuracy guarantee.

**Lemma 7**.: _With probability \(1-\), for all \(t\) in a round starting from \(N_{0}\), we have \(|_{t}+N_{0}-(N_{t}+N_{0})| 8C=  N_{0}\)._

For the Communication complexity, in one round, the total number of received bits by the server is \(c_{1}+c_{2}++c_{}\). By analysis above, with probability \(1-/2\), for all \(j[]\), we have

\[_{j=1}^{}c_{j}_{j=1}^{}(_{j}+1 )_{j=1}^{}(T+|_{j}|+1)_{j=1}^{}2C =2Ck,\] (4) \[_{j=1}^{}c_{j}_{j=1}^{} _{j=1}^{}(T-|_{j}|)_{j=1}^{}C Ck.\] (5)

Therefore, in one round, the communication cost is upper bounded by \(O(Ck)\). By (5), there are at least \(Ck= N_{0}/8\) items received in this round, which means \(N\) increases by an \(O(1+/8)\) factor after one round. It follows that there are at most \(O(})\) rounds. Combined this with the communication cost in one round, we can conclude the final communication complexity is \(O( N}{})\). Now we are ready to prove Theorem 1.

Proof of Theorem 1.: Since \(N_{0}\) is the exact count at the beginning of a round, the Lemma 7 guarantees an \(\)-relative error in the round. Since the lemma holds for any round, the correctness is established. We set the failure probability as \(=/( O(}))\). Since there are \(O(})\) rounds, by the union bound and Lemma 7, it can be concluded that with probability \(1-\), the output of \(\) is an \(\)-approximate to \(N\) at all times. The communication complexity is \(O( N}{})\)5. 

Conclusion

In this paper, we study the robustness of distributed count tracking to adaptive inputs. We present a new randomized algorithm that employs differential privacy to achieve robustness. Our new algorithm has near optimal communication complexity. Besides, we introduce a relaxed version of differential privacy, which allows privacy leak of some data points. Based on this definition, we prove a new generalization theorem of differential privacy, which we believe can be of independent interest and have broader applications.