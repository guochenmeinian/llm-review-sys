# Discrete-state Continuous-time Diffusion for Graph Generation

Zhe Xu Ruizhong Qiu

University of Illinois Urbana-Champaign. {zhexu3, rq5, zhichenz, htong}@illinois.edu

Yuzhong Chen

Visa Research. {yuzchen, hchen, xirafan, menpan, mahdas}@visa.com

Huiyuan Chen

Visa Research. {yuzchen, hchen, xirafan, menpan, mahdas}@visa.com

Xiran Fan

Visa Research. {yuzchen, hchen, xirafan, menpan, mahdas}@visa.com

Menghai Pan

Visa Research. {yuzchen, hchen, xirafan, menpan, mahdas}@visa.com

Zhichen Zeng

University of Illinois Urbana-Champaign. {zhexu3, rq5, zhichenz, htong}@illinois.edu

Mahashweta Das

Visa Research. {yuzchen, hchen, xirafan, menpan, mahdas}@visa.com

Hanghang Tong

University of Illinois Urbana-Champaign. {zhexu3, rq5, zhichenz, htong}@illinois.edu

###### Abstract

Graph is a prevalent discrete data structure, whose generation has wide applications such as drug discovery and circuit design. Diffusion generative models, as an emerging research focus, have been applied to graph generation tasks. Overall, according to the space of _states_ and _time_ steps, diffusion generative models can be categorized into discrete-/continuous-state discrete-/continuous-time fashions. In this paper, we formulate the graph diffusion generation in a discrete-state continuous-time setting, which has never been studied in previous graph diffusion models. The rationale of such a formulation is to preserve the discrete nature of graph-structured data and meanwhile provide flexible sampling trade-offs between sample quality and efficiency. Analysis shows that our training objective is closely related to the generation quality and our proposed generation framework enjoys ideal invariant/equivariant properties concerning the permutation of node ordering. Our proposed model shows competitive empirical performance against state-of-the-art graph generation solutions on various benchmarks and at the same time can flexibly trade off the generation quality and efficiency in the sampling phase.

## 1 Introduction

Graph generation has been studied for a long time with broad applications, based on either the one-shot (i.e., one-step)  or auto-regressive generation paradigm . The former generates all the graph components at once and the latter does that sequentially. A recent trend of applying diffusion generative models  to graph generation tasks attracts increasing attentions because of its excellent performance and solid theoretical foundation. In this paper, we follow the one-shot generation paradigm, the same as most graph diffusion generative models.

Some earlier attempts at graph diffusion models treat the graph data in a continuous state space by viewing the graph topology and features as continuous variables . Such a formulation departs from the discrete nature of graph-structured data; e.g., topological sparsity is lost and the discretization in the generation process requires extra hyper-parameters. DiGress  is one of the early efforts

Figure 1: A taxonomy of graph diffusion models.

applying discrete-state diffusion models to graph generation tasks and is the current state-of-the-art graph diffusion generative model. However, DiGress is defined in the discrete time space whose generation is inflexible. This is because, its number of sampling steps must match the number of forward diffusion steps, which is a fixed hyperparameter after the model finishes training. A unique advantage of the continuous-time diffusion models [70; 32] lies in their flexible sampling process, and its simulation complexity is proportional to the number of sampling steps, determined by the step size of various numerical approaches (e.g., \(\)-leaping [18; 8; 71]) and decoupled from the models' training. Thus, a discrete-state continuous-time diffusion model is highly desirable for graph generation tasks.

Driven by the recent advance of continuous-time Markov Chain (CTMC)-based diffusion generative model , we incorporate the ideas of CTMC into the corruption and denoising of graph data and propose the first discrete-state continuous-time graph diffusion generative model. It shares the same advantages as DiGress by preserving the discrete nature of graph data and meanwhile overcomes the drawback of the nonadjustable sampling process in DiGress. This Discrete-state Continuous-time graph diffusion model is named DisCo.

DisCo bears several desirable properties and advantages. First, despite its simplicity, the training objective has a rigorously proved connection to the sampling error. Second, its formulation includes a parametric graph-to-graph mapping, named backbone model, whose input-output architecture is shared between DisCo and DiGress. Therefore, the graph transformer (GT)-based backbone model  from DiGress can be seamlessly plugged into DisCo. Third, a concise message-passing neural network backbone model is explored with DisCo, which is simpler than the GT backbone and has decent empirical performance. Last but not least, our analyses show that the forward and reverse diffusion process in DisCo can retain the permutation-equivariant/invariant properties for its training loss and sampling distribution, both of which are critical and practical inductive biases on graph data.

Comprehensive experiments on plain and molecule graphs show that DisCo can obtain competitive or superior performance against state-of-the-art graph generative models and provide additional sampling flexibility. Our main contributions are summarized:

* **Model.** We propose the first discrete-state continuous-time graph diffusion model, DisCo. We utilize the successful graph-to-graph neural network architecture from DiGress and further explore a new lightweight backbone model with decent efficacy.
* **Analysis.** Our analysis reveals (1) the key connection between the training loss and the approximation error (Theorem 3.3) and (2) invariant/equivariant properties of DisCo in terms of the permutation of nodes (Theorems 3.8 and 3.9).
* **Experiment.** Extensive experiments validate the empirical performance of DisCo.

## 2 Preliminaries

### Discrete-State Continuous-time Diffusion Models

A \(D\)-dimensional discrete state space is represented as \(=\{1,,C\}^{D}\). A continuous-time Markov Chain (CTMC) \(\{_{t}=[x_{t}^{1}, x_{t}^{D}]\}_{t[0,T]}\) is characterized by its (time-dependent) rate matrix \(_{t}^{||||}\). Here \(_{t}\) is the state at the time step \(t\). The transition probability \(q_{t|s}\) between from time \(s\) to \(t\) satisfies the Kolmogorov forward equation, for \(s<t\),

\[q_{t|s}(_{t}|_{s})=_{}q_{ t|s}(|_{s})_{t}(,_{t}),\] (1)

The marginal distribution can be represented as \(q_{t}(_{t})=_{_{0}}q_{t|0}(_{ t}|_{0})_{}(_{0})\) where \(_{}(_{0})\) is the data distribution. If the CTMC is defined in time interval \([0,T]\) and if the rate matrix \(_{t}\) is well-designed, the final distribution \(q_{T}(_{T})\) can be close to a tractable reference distribution \(_{}(_{T})\), e.g., uniform distribution. We note the reverse stochastic process as \(}_{t}=_{T-t}\); a well-known fact (e.g., Section 5.9 in ) is that the reverse process \(\{}_{t}\}_{t[0,T]}\) is also a CTMC, characterized by the reverse rate matrix: \(}_{t}(,)=)}{q()}_{t}(,)\). The goal of the CTMC-based diffusion models is an accurate estimation of the reverse rate matrix \(}_{t}\) so that new data can be generated by sampling the reference distribution \(_{}\) and then simulating the reverse CTMC [16; 17; 18; 1]. However, the complexity of the rate matrix is prohibitively high because there are \(C^{D}\) possible states. A reasonable simplification is to factorize the process over dimensions [8; 71; 73; 2]. Specifically,the forward process is factorized as \(q_{t|s}(_{t}|_{s})=_{d=1}^{D}q_{t|s}(x_{t}^{d}|x_{s}^{d})\), for \(s<t\). Then, the forward diffusion of each dimension is independent and is governed by dimension-specific forward rate matrices \(\{_{t}^{d}\}_{d=1}^{D}\). With such a factorization, the goal is to estimate the dimension-specific reverse rate matrices \(\{}_{t}^{d}\}_{d=1}^{D}\).

The dimension-specific reverse rate is represented as \(}_{t}^{d}(x^{d},y^{d})=_{x_{0}^{d}}_{t}^{d}(y^{ d},x^{d})(y^{d}|x_{0}^{d})}{q_{t|0}(x^{d}|x_{0}^{d})}q_{0|t}(x_{0}^{d}| )\). Campbell et al.  estimate \(q_{0|t}(x_{0}^{d}|)\) via a neural network \(p_{}\) such that \(p_{}(x_{0}^{d}|,t) q_{0|t}(x_{0}^{d}|)\); Sun et al.  propose another singleton conditional distribution-based objective \((y^{d}|^{},t)}{p_{}(x^{d}|^{},t)}|^{},t)}{q(x^{d}| ^{},t)}\) whose rationale is Brook's Lemma [5; 49].

### Graph Generation and Notations

We study the graphs with _categorical_ node and edge attributes. A graph with \(n\) nodes is represented by its edge type matrix and node type vector: \(=(,)\), where \(=(e^{(i,j)})_{i,j_{ n}^{+}}\{1,,a+1\}^{n  n}\), \(=(f^{i})_{i_{ n}^{+}}\{1,,b\}^{n}\), \(a\) and \(b\) are the numbers of node and edge types, respectively. Notably, the absence of an edge is viewed as a special edge type, so there are \((a+1)\) edge types in total. The problem we study is graph generation where \(N\) graphs \(\{^{i}\}_{i_{ N}^{+}}\) from an inaccessible graph data distribution \(\) are given and we aim to generate \(M\) graphs \(\{^{i}\}_{i_{ M}^{+}}\) from \(\).

## 3 Method

This section presents the proposed discrete-state continuous-time graph diffusion model, DisCo whose overview is Figure 2. Section 3.1 introduces the necessity to factorize the diffusion process and Section 3.2 details the forward process. Our training objective and its connection to sampling are introduced in Sections 3.3 and 3.4, respectively. Last but not least, a specific neural architecture of the graph-to-graph backbone model and its properties regarding the permutation of node ordering are introduced in Sections 3.5 and 3.6, respectively. _All proofs are in Appendix._

### Factorized Discrete Graph Diffusion Process

The number of possible states of an \(n\)-node graph is \((a+1)^{n^{2}} b^{n}\) which is intractably large. Thus, we follow existing discrete models [2; 8; 71; 73] and formulate the forward processes on every node/edge to be independent. Mathematically, the forward diffusion process for \(s<t\) is factorized as

\[q_{t|s}(_{t}|_{s})=_{i,j=1}^{n}q_{t|s}(e_{t}^{(i,j) }|e_{s}^{(i,j)})_{i=1}^{n}q_{t|s}(f_{t}^{i}|f_{s}^{i})\] (2)

where the edge type transition probabilities \(\{q_{t|s}(e_{t}^{(i,j)}|e_{s}^{(i,j)})\}_{i,j_{ n}^{+}}\) and node type transition probabilities \(\{q_{t|s}(f_{t}^{i}|f_{s}^{i})\}_{i_{ n}^{+}}\) are characterized by their forward rate matrices \(\{_{t}^{(i,j)}\}_{i,j_{ n}^{+}}\) and \(\{_{t}^{i}\}_{i_{ n}^{+}}\), respectively. The forward processes, i.e., the forward rate matrices in our context, are predefined, which will be introduced in Section 3.2. Given the factorization of forward transition probability in Eq. (2), a question is raised: _what is the corresponding factorization of the forward rate matrix (\(_{t}\)) and the reverse rate matrix (\(}_{t}\))?_ Remark 3.1 shows such a factorization.

Figure 2: An overview of DisCo. A transition can happen at any time in \([0,T]\).

_Remark 3.1_.: (Factorization of rate matrices, extended from Proposition 3 of ) Given the factorized forward process Eq. (2), the overall rate matrices are factorized as

\[_{t}(},) =_{i}A_{t}^{i}+_{i,j}B_{t}^{(i,j)}\] (3) \[}_{t}(,}) =_{i}A_{t}^{i}_{f_{0}^{i}}(^{i}|f_{0} ^{i})}{q_{t|0}(f^{i}|f_{0}^{i})}q_{0|t}(f_{0}^{i}|)+_{i,j}B_{t}^ {(i,j)}_{e_{0}^{(i,j)}}(^{(i,j)}|e_{0}^{(i,j)})}{q_{t|0 }(e^{(i,j)}|e_{0}^{(i,j)})}q_{0|t}(e_{0}^{(i,j)}|)\] (4)

where \(A_{t}^{i}=_{t}^{i}(^{i},i)_{} f ^{i},} f^{i}}\), \(B_{t}^{(i,j)}=_{t}^{(i,j)}(^{(i,j)},e^{(i,j)})_{ }^{(i,j)},} e^{(i,j )}}\), the operator \(_{} f^{i},} f^{i}}\) (or \(_{}^{(i,j)},} e ^{(i,j)}}\)) checks whether two graphs \(}\) and \(\) are exactly the same except for node \(i\) (or the edge between nodes \(i\) and \(j\)).

Note that this factorization itself is not our contribution but a necessary part of our framework, so we mention it here for completeness. Its full derivation is in Appendix - Section A. Next, we detail the design of forward rate matrices.

### Forward Process

A proper choice of the forward rate matrices \(\{_{t}^{(i,j)}\}_{i,j_{ n}^{+}}\) and \(\{_{t}^{i}\}_{i_{ n}^{+}}\) is important because (1) the probability distributions of node and edge types, \(\{q(f_{t}^{i})\}_{i_{ n}^{+}}\) and \(\{q(e_{t}^{(i,j)})\}_{i,j_{ n}^{+}}\), should converge to their reference distributions within \([0,T]\) and (2) the reference distributions should be easy to sample (e.g., uniform distribution). We follow  to formulate \(_{t}^{(i,j)}=(t)_{e}^{(i,j)},\  i,j\) and \(_{t}^{i}=(t)_{f}^{i},\  i\), where \((t)\) is a corruption schedule, \(\{_{e}^{(i,j)}\}\) and \(\{_{f}^{j}\}\) are the base rate matrices. For brevity, we set all the nodes/edges to share a common node/edge rate matrix, i.e., \(_{e}^{(i,j)}=_{e}\) and \(_{f}^{i}=_{f}\), \( i,j\). Then, the forward transition probability for all the nodes and edges are \(q_{t|0}(f_{t}=v|f_{0}=u)=(e^{_{0}^{t}(s)_{f}ds})_{uv}\) and \(q_{t|0}(e_{t}=v|e_{0}=u)=(e^{_{0}^{t}(s)_{e}ds})_{uv}\), respectively. We omit the superscript \(i\) (or \((i,j)\)) because the transition probability is shared by all the nodes (or edges). The detailed derivation of the above analytic forward transition probability is provided in Appendix - Section B.

For categorical data, a reasonable reference distribution is a uniform distribution, i.e., \(_{f}=}{b}\) for nodes and \(_{e}=}{a+1}\) for edges. In addition, inspired by , we find that node and edge marginal distributions \(_{f}\) and \(_{e}\) are good choices as the reference distributions. Concretely, an empirical estimation of \(_{f}\) and \(_{e}\) is to count the number of node/edge types and normalize them. The following proposition shows how to design the rate matrices to guide the forward process to converge to uniform and marginal distributions.

**Proposition 3.2**.: _The forward processes for nodes and edges converge to uniform distributions if \(_{f}=^{}-b\) and \(_{e}=^{}-(a+1)\); they converge to marginal distributions \(_{f}\) and \(_{e}\) if \(_{f}=_{f}^{}-\) and \(_{e}=_{e}^{}-\). \(\) is an all-one vector and \(\) is an identity matrix._

Regarding the selection of \((t)\), we follow [23; 70; 8] and set \((t)=^{t}()\) for a smooth change of the rate matrix. \(\) and \(\) are hyperparameters. Detailed settings are in Appendix F.3.

### Parameterization and Optimization Objective

Next, we introduce the estimation of the reverse process from its motivation. The reverse process is essentially determined by the reverse rate matrix \(}_{t}\) in Eq. (4), whose computation needs \(q_{0|t}(f_{0}^{i}|)\) and \(q_{0|t}(e_{0}^{(i,j)}|)\), \( i,j\); their exact estimation is expensive because according to Bayes' rule, \(p_{t}()\) is needed, whose computation needs to enumerate all the given graphs: \(p_{t}()=_{_{0}}q_{t|0}(|_{0}) _{}(_{0})\).

Thus, we propose parameterizing the reverse transition probabilities via a neural network \(\) whose specific architecture is introduced in Section 3.5. The terms \(\{q_{0|t}(f_{0}^{i}|)\}_{i_{ n}^{+}}\) and \(\{q_{0|t}(e_{0}^{(i,j)}|)\}_{i,j_{ n}^{+}}\) in Eq. (4) are replaced with the parameterized \(\{p_{0|t}^{}(f^{i}|)\}_{i_{ n}^{+}}\) and \(\{p_{0|t}^{}(e^{(i,j)}|)\}_{i,j_{ n}^{+}}\). Thus, a parameterized reverse rate matrix \(}_{,t}(,})\) is represented as \(}_{,t}(,})=_{i}}_{,t}^{i}(f^{i},^{i})+_{i,j}}_{ ,t}^{(i,j)}(e^{(i,j)},^{(i,j)})\) where \(}_{,t}^{i}(f^{i},^{i})=A_{t}^{i}_{f_{0}^{ }}(f^{i}|f_{0}^{i})p_{0|t}^{}}{q_{i|0}(f^{i}|f_{0}^{i} )p_{0|t}^{}}(f_{0}^{i}|)\), \(}_{,t}^{(i,j)}(e^{(i,j)},^{(i,j)})=B_{t}^{(i,j)} _{e_{0}^{(i,j)}}(^{(i,j)}|e_{0}^{(i,j)})}{q_{i|0}(e^{(i,j)}|e_{0}^{(i,j)})}p_{0|t}^{}(e_{0}^{(i,j)}|)\), and the remaining notations are the same as Eq. (4). Note that all the terms \(\{p_{0|t}^{}(f^{i}|)\}_{i_{ n}^{+}}\) and \(\{p_{0|t}^{}(e^{(i,j)}|)\}_{i,j_{ n }^{+}}\) can be viewed together as a graph-to-graph mapping \(:\), whose input is the noisy graph \(_{t}\) and its output is the predicted clean graph probabilities, concretely, the node/edge type probabilities of all the nodes and edges.

Intuitively, the discrepancy between the groundtruth \(}_{t}\) (from Eq. (4)) and the parametric \(}_{,t}\) should be small. Theorem 3.3 establishes a cross-entropy (CE)-based upper bound of such a discrepancy, where the estimated probability vectors (sum is \(1\)) are notated as \(_{0}^{i}=[p_{0|t}^{}(f^{i}=1|_{t}),,p_{0|t}^{ }(f^{i}=b|_{t})]^{}^{b}\) and \(_{0}^{(i,j)}=[p_{0|t}^{}(e^{(i,j)}=1|_{t}),,p_{0|t }^{}(e^{(i,j)}=a+1|_{t})]^{}^{a+1}\).

**Theorem 3.3** (Approximation error).: _for \(}\)_

\[|}_{t}(,})- }_{,t}(,})|^{2}  C_{t}+C_{t}^{}_{_{0}}q_{t|0} (|_{0})_{i}_{}(f_{0}^{i}),_{0}^{i}\] \[+C_{t}^{}_{_{0}}q_{t|0}( |_{0})_{i,j}_{}(e_{0}^{(i,j)}),_{0}^{(i,j)}\] (5)

_where \(C_{t}\), \(C_{t}^{}\), and \(C_{t}^{}\) are constants independent on \(\) but dependent on \(t\), \(\), and \(}\);_ One-Hot _transforms \(f_{0}^{i}\) and \(e_{0}^{(i,j)}\) into one-hot vectors._

The bound in Theorem 3.3 is tight, i.e., the right-hand side of Eq. (5) is \(0\), whenever \(_{0}^{i}=q_{0|t}(f_{0}^{i}|_{t}), i\) and \(_{0}^{(i,j)}=q_{0|t}(e_{0}^{(i,j)}|_{t}), i,j\). Guided by Theorem 3.3, we (1) take expectation of \(t\) by sampling \(t\) from a uniform distribution \(t_{(0,T)}\) and (2) simplify the right-hand side of Eq. (5) by using the unweighted CE loss as our training objective:

\[_{}\;T_{_{0}}_{q_{t|0}( _{t}|_{0})}_{i}_{}( (f_{0}^{i}),_{0}^{i})+_{i,j}_{}((e_{0}^{(i,j)}),_{0}^{(i,j)})\] (6)

A step-by-step training algorithm is in Algorithm 1. Note that the above CE loss has been used in some diffusion models (e.g., [2; 8]) but lacks a good motivation, especially in the continuous-time setting. We motivate it based on the rate matrix discrepancy, as a unique contribution of this paper.

### Sampling Reverse Process

Given the parametric reverse rate matrix \(}_{,t}(,})\), the graph generation process can be implemented by two steps: (1) sampling the reference distribution \(_{}\) (i.e., \(_{f}\) for nodes and \(_{e}\) for edges) and (2) numerically simulating the CTMC from time \(T\) to \(0\). The exact simulation of a CTMC has been studied for a long time, e.g., [16; 17; 1]. However, their simulation strategies only allow one transition (e.g., one edge/node type change) per step, which is highly inefficient for graphs as the number of nodes and edges is typically large; once a(n) node/edge is updated, \(}_{,t}\) requires recomputation. Apractical approximation is to assume \(}_{,t}\) is fixed during a time interval \([t-,t]\), i.e., delaying the happening of transitions in \([t-,t]\) and triggering them all together at the time \(t-\); this strategy is also known as \(\)-leaping [18; 8; 71], and DisCo adopts it.

We elaborate on \(\)-leaping for transitions of node types; the transitions of edge types are similar. The rate matrix of the \(i\)-th node is fixed as \(}_{,t}^{i}(f^{i},^{i})=_{t}^{i}(^{i},f^{i})_{f_{0}^{i}}(^{i}f_{0}^{i})}{q_{10}(^{i} f_{0}^{i})}p_{0|t}^{}(f^{i}|_{t})\), during \([t-,t]\). According to the definition of rate matrix, in \([t-,t]\), the number of transitions from \(f^{i}\) to \(^{i}\), namely \(J_{f^{i},^{i}}\), follows the Poisson distribution, i.e., \(J_{f^{i},^{i}}(}_{,t}^{i}( f^{i},^{i}))\). For categorical data (e.g., node type), multiple transitions in \([t-,t]\) are invalid and meaningless. In other words, for the \(i\)-th node, if the total number of transitions \(_{^{i}}J_{f^{i},^{i}}>1\), \(f^{i}\) keeps unchanged in \([t-,t]\); otherwise, if \(_{^{i}}J_{f^{i},^{i}}=1\) and \(J_{f^{i},s}=1\), i.e., there is exact \(1\) transition, \(f^{i}\) jumps to \(s\). A step-by-step sampling algorithm (Algorithm 2) is in Appendix.

_Remark 3.4_.: The sampling error of \(\)-leaping is linear to \(C_{}\), the approximation error of the reverse rates: \(_{}|}_{}(,})-}_{,t}(,})| C_{}\). Interested readers are referred to Theorem 1 from . Our Theorem 3.3 shows the connection between our training loss and \(C_{}\), which further verifies the correctness of our training loss.

### Model Instantiation

As mentioned in Section 3.3, the parametric backbone \(p_{0|t}^{}(_{0}|_{t})\) is a graph-to-graph mapping whose input is the noisy graph \(_{t}\) and its output is the predicted denoised graph \(_{0}\). There exists a broad range of neural network architectures. Notably, DiGress  uses a graph Transformer (GT) as \(p_{0|t}^{}\), a decent reference for our continuous-time framework. We name our model with the GT backbone as DisCo-GT and its detailed configuration is in Appendix F.3. The main advantage of the GT is its long-range interaction thanks to the complete self-attention graph; however, the architecture is very complex and includes multi-head self-attention modules, leading to expensive computation.

Beyond GTs, in this paper, we posit that a regular message-passing neural network (MPNN)  should be a promising choice for \(p_{0|t}^{}(_{0}|_{t})\). It is recognized that the MPNNs' expressiveness might not be as good as GTs' [33; 7], e.g., in terms of long-range interactions. However, in our setting, the absence of an edge is viewed as a special type of edge and the whole graph is complete; therefore, such a limitation of MPNN is naturally mitigated, which is verified by our empirical evaluations.

Concretely, an MPNN-based graph-to-graph mapping is presented as follows, and DisCo with MPNN backbone is named DisCo-MPNN. Given a graph \(=(,)\), where \(\{1,,a,a+1\}^{n n}\), \(\{1,,b\}^{n}\), we first transform both the matrix \(\) and \(\) into one-hot embeddings \(_{}\{0,1\}^{n n(a+1)}\) and \(_{}\{0,1\}^{n b}\). Then, some auxiliary features (e.g., the # of specific motifs) are extracted: \(_{},_{}=(_{ })\) to overcome the expressiveness limitation of MPNNs . Here \(_{}\) and \(_{}\) are the node and global auxiliary features, respectively. Note that a similar auxiliary feature engineering is also applied in DiGress . More details about the Aux can be found in Appendix E. Then, three multi-layer perceptrons (MLPs) are used to map node features \(_{}_{}\), edge features \(_{}\), and global features \(_{}\) into a common hidden space as \(_{}=(_{} _{})\), \(_{}=(_{})\), \(_{}=(_{})\), where \(\) is a concatenation operator. The following formulas present the update of node embeddings (e.g., \(^{i}=(i,:)\)), edge embedding (e.g., \(^{(i,j)}=(i,j,:)\)), and global embedding \(\) in an MPNN layer, where we omit the subscript hidden if it does not cause ambiguity:

\[^{i} ^{i},_{j=1}^{n}^{(j,i)}/n ,,\ \ ^{(i,j)}(^{(i,j)},^{i}^{j}),,\] (7) \[+\{^{i} \}_{i=1}^{n}+\{^{(i,j)}\}_{i,j=1}^{n}.\] (8)

The edge embeddings are aggregated by mean pooling (i.e., \(_{j=1}^{n}^{(j,i)}/n\)); the node pair embeddings are passed to edges by Hadamard product (i.e., \(^{i}^{j}\)); edge/node embeddings are merged to the global embedding \(\) via the PNA module ; Some FiLM modules  are used for the interaction between node/edge/global embeddings. More details about the PNA and FiLM are in Appendix E. In this paper, we name Eqs. (7) and (8) on all nodes/edges together as an MPNN layer, \(,,(,, )\). Stacking multiple MPNN layers leads to larger model capacity.

Finally, two readout MLPs are used to project the node/edge embeddings into input dimensions, \(()^{n b}\) and \(()^{n n(a+1)}\), which are output after wrapped with softmax.

Both the proposed MPNN and the GT from DiGress  use the PNA and FiLM to merge embeddings, but MPNN does not have multi-head self-attention layers so that the computation overhead is lower.

### Permutation Equivariance and Invariance

Reordering the nodes keeps the property of a given graph, which is known as permutation invariance. In addition, for a given function if its input is permuted and its output is permuted accordingly, such a behavior is known as permutation equivariance. In this subsection, we analyze permutation-equivariance/invariance of the (1) diffusion framework (Lemmas 3.5, 3.6, and 3.7), (2) sampling density (Theorem 3.8), and (3) training loss (Theorem 3.9).

**Lemma 3.5** (Permutation-equivariant layer).: _The proposed MPNN layer (Eqs. (7) and (8)) is permutation-equivariant._

The auxiliary features from the Aux are also permutation-equivariant (see Appendix E). Thus, the whole MPNN-based backbone \(p^{}_{0|t}\) is permutation-equivariant. Note that the GT-based backbone from DiGress  is also permutation-equivariant whose proof is omitted as it is not our contribution. Next, we show the permutation invariance of the rate matrices.

**Lemma 3.6** (Permutation-invariant rate matrices).: _The forward rate matrix of DisCo is permutation-invariant if it is factorized as Eq. (3). The parametric reverse rate matrix of DisCo (\(}_{,t}\)) is permutation-invariant whenever the graph-to-graph backbone \(p^{}_{0|t}\) is permutation-equivariant._

**Lemma 3.7** (Permutation-invariant transition probability).: _For CTMC satisfying the Kolmogorov forward equation (Eq. (1)), if the rate matrix is permutation-invariant (i.e., \(_{t}(_{i},_{j})=_{t}(( _{i}),(_{j}))\), the transition probability is permutation-invariant (i.e., \(q_{t|s}(_{t}|_{s})=q_{t|s}((_{t})| (_{s}))\), where \(\) is a permutation._

Based on Lemmas 3.6 and 3.7, DisCo's parametric reverse transition probability is permutation-invariant. The next theorem shows the permutation-invariance of the sampling probability.

**Theorem 3.8** (Permutation-invariant sampling probability).: _If both the reference distribution \(_{}\) and the reverse transition probability are permutation-invariant, the parametric sampling distribution \(p^{}_{0}(_{0})\) is permutation-invariant._

In addition, the next theorem shows the permutation invariance of the training loss.

**Theorem 3.9** (Permutation-invariant training loss).: _The proposed training loss Eq. (6) is invariant to any permutation of the input graph \(_{0}\) if \(p^{}_{0|t}\) is permutation-equivariant._

## 4 Experiments

This section includes: an effectiveness evaluation on plain graphs (Section 4.1) and molecule graphs (Section 4.2), an efficiency study (Section 4.3), and an ablation study (Section 4.4). Detailed settings (Sections F.1-F.3), additional effectiveness evaluation (Sections F.4, additional ablation study (Section F.5), convergence study (Section F.6), and visualization (Section F.7) are in Appendix. Our code is released 3.

### Plain Graph Generation

**Datasets and metrics.** Datasets SBM, Planar , and Community  are used. The relative squared Maximum Mean Discrepancy (MMD) for degree distributions (Deg.), clustering coefficient distributions (Clus.), and orbit counts (Orb.) distributions (the number of occurrences of substructures with \(4\) nodes), Uniqueness(%), Novelty(%), and Validity(%) are chosen as metrics. Details about the datasets, metrics, baselines (Section F.2.2), and results on Community (Table 8) are in Appendix.

**Results.** Table 1 shows the effectiveness evaluation on SBD and Planar from which we observe:* DisCo-GT can obtain competitive performance against the SOTA, DiGress, which is reasonable because both models share the graph Transformer backbone. Note that DiGress's performance in terms of Validity is not the statistics reported in the paper but from their latest model checkpoint 4. In fact, we found it very hard for DiGress and DisCo-GT to learn to generate valid SBM/Planar graphs. These two datasets have only \(200\) graphs, but sometimes only after \(>10,000\) epochs training, the Validity percentage can be \(>50\%\). Additionally, DisCo-GT provides extra flexibility during sampling by adjusting the \(\). This is important: our models can still trade-off between the sampling efficiency and quality even after the model is trained and frozen. * In general, DisCo-MPNN has competitive performance against DisCo-GT in terms of Deg., Clus., and Orb. However, its performance is worse compared to DisCo-GT in terms of Validity, which might be related to the different model expressiveness. Studying the graph-to-graph model expressiveness would be an interesting future direction, e.g., generating valid Planar graphs.

   Dataset & Model & Deg.\(\) & Clus.\(\) & Orb.\(\) & Unique \(\) & Novel \(\) & Valid \(\) \\   & GraphRNN  & 6.9 & 1.7 & 3.1 & **100.0** & **100.0** & 5.0 \\  & GRAN  & 14.1 & 1.7 & 2.1 & **100.0** & **100.0** & 25.0 \\  & GG-GAN  & 4.4 & 2.1 & 2.3 & **100.0** & **100.0** & 0.0 \\  & MolGAN  & 29.4 & 3.5 & 2.8 & 95.0 & **100.0** & 10.0 \\  & SPECTCE  & 1.9 & 1.6 & **1.6** & **100.0** & **100.0** & 52.5 \\  & ConGress  & 34.1 & 3.1 & 4.5 & 0.0 & 0.0 & 0.0 \\  & DiGress  & 1.6 & 1.5 & 1.7 & **100.0** & **100.0** & **67.5** \\  & DisCo-MPNN & 1.8\(\)0.2 & **0.8\(\)0.1** & 2.7\(\)0.4 & **100.0\(\)0.0** & **100.0\(\)0.0** & 41.9\(\)2.2 \\  & DisCo-GT & **0.8\(\)0.2** & **0.8\(\)0.4** & 2.0\(\)0.5 & **100.0\(\)0.0** & **100.0\(\)0.0** & 66.2\(\)1.4 \\   & GraphRNN  & 24.5 & 9.0 & 2508.0 & **100.0** & **100.0** & 0.0 \\  & GRAN  & 3.5 & 1.4 & 1.8 & 85.0 & 2.5 & **97.5** \\  & GG-GAN  & 315.0 & 8.3 & 2062.6 & **100.0** & **100.0** & 0.0 \\  & MolGAN  & 4.5 & 10.2 & 2346.0 & 25.0 & **100.0** & 0.0 \\  & SPECTRE  & 2.5 & 2.5 & 2.4 & **100.0** & **100.0** & 25.0 \\  & ConGress  & 23.8 & 8.8 & 2590.0 & 0.0 & 0.0 & 0.0 \\  & DiGress  & 1.4 & **1.2** & **1.7** & **100.0** & **100.0** & 85.0 \\  & DisCo-MPNN & 1.4\(\)0.3 & 1.4\(\)0.4 & 6.4\(\)1.6 & **100.0\(\)0.0** & **100.0\(\)0.0** & 33.8\(\)2.7 \\  & DisCo-GT & **1.2\(\)**0.5 & 1.3\(\)0.5 & **1.7\(\)0.7** & **100.0\(\)0.0** & **100.0\(\)0.0** & 83.6\(\)2.1 \\   

Table 1: Performance (mean\(\)std) on SBM and Planar datasets.

   Model & Valid \(\) & V.U. \(\) & V.U.N. \(\) \\  CharacterVAE  & 10.3 & 7.0 & 6.3 \\ GrammarVAE & 60.2 & 5.6 & 4.5 \\ GraphVAE  & 55.7 & 42.0 & 26.1 \\ GT-VAE  & 74.6 & 16.8 & 15.8 \\ Set2GraphVAE  & 59.9 & 56.2 & - \\ GG-GAN  & 51.2 & 24.4 & 24.4 \\ MolGAN  & 98.1 & 10.2 & 9.6 \\ SPECTRE  & 87.3 & 31.2 & 29.1 \\ GraphNVP  & 83.1 & 82.4 & - \\ GDSS  & 95.7 & 94.3 & - \\ EDGE  & 99.1 & **99.1** & - \\ ConGress  & 98.9 & 95.7 & 38.3 \\ DiGress  & 99.0 & 95.2 & 31.8 \\ GRAPHARM  & 90.3 & 86.3 & - \\  DisCo-MPNN & 98.9\(\)0.7 & 98.7\(\)0.5 & **68.7\(\)0.2** \\ DisCo-GT & **99.3\(\)0.6** & 98.9\(\)0.6 & 56.2\(\)0.4 \\   

Table 2: Performance (mean\(\)std%) on QM9 dataset. V., U., and N. mean Valid, Unique, and Novel.

### Molecule Graph Generation

**Dataset and metrics.** The datasets QM9 , MOSES , and GuacaMol  are chosen. For MOSES, metrics including Uniqueness, Novelty, Validity, Filter, FCD, SNN, and Scaf are reported in Table 3. For QM9, metrics include Uniqueness, Novelty, and Validity. For GuacaMol, metrics include Valid, Unique, Novel, KL div, and FCD. Details about the datasets, metrics, and baseline methods are in Appendix F.2.3.

**Results.** Table 2 shows the performance on QM9 dataset. Our observation is consistent with the performance comparison on plain datasets: (1) DisCo-GT obtains slightly better or at least competitive performance against DiGress due to the shared graph-to-graph backbone, but our framework offers extra flexibility in the sampling process; (2) DisCo-MPNN obtains decent performance in terms of Validity, Uniqueness, and Novelty comparing with DisCo-GT.

Tables 3 and 4 show the performance on MOSES and GuacaMol which further verifies that (1) performance of DisCo-GT is on par with the SOTA general graph generative models, DiGress and (2) DisCo-MPNN has decent performance, but worse than DisCo-GT and DiGress.

### Efficiency Study

A major computation bottleneck is the graph-to-graph backbone \(p_{0|t}^{}\), which is GT or MPNN. We compare the number of parameters, the forward and back-propagation time of GT and MPNN in Table 5. For a fair comparison, we set all the hidden dimensions of GT and MPNN as \(256\) and the number of layers as \(5\). We use the Community  dataset and set the batch size as \(64\). Table 5 shows that GT has a larger capacity and more parameters at the expense of more expensive training.

   Model & Valid \(\) & Unique \(\) & Novel \(\) & KL div \(\) & FCD \(\) \\  LSTM  & 95.9 & 100.0 & 91.2 & 99.1 & 91.3 \\ NAGVAE  & 92.9 & 95.5 & 100.0 & 38.4 & 0.9 \\ MCTS  & 100.0 & 100.0 & 95.4 & 82.2 & 1.5 \\ ConGress  & 0.1 & 100.0 & 100.0 & 36.1 & 0.0 \\ DiGress  & 85.2 & 100.0 & 99.9 & 92.9 & 68.0 \\  DisCo-MPNN & 68.7 & 100.0 & 96.4 & 77.0 & 36.4 \\ DisCo-GT & 86.6 & 100.0 & 99.9 & 92.6 & 59.7 \\   

Table 4: Performance on GuacaMol. LSTM, NAGVAE, and MCTS are tailored for molecule datasets; ConGress, DiGress, and DisCo are general graph generation models.

   Model & Valid \(\) & Unique \(\) & Novel \(\) & Filters \(\) & FCD \(\) & SNN \(\) & Scaf \(\) \\  VAE  & 97.7 & 98.8 & 69.5 & 99.7 & 0.57 & 0.58 & 5.9 \\ JT-VAE  & 100.0 & 100.0 & 99.9 & 97.8 & 1.00 & 0.53 & 10.0 \\ GraphINVENT  & 96.4 & 99.8 & N/A & 95.0 & 1.22 & 0.54 & 12.7 \\ Confress  & 83.4 & 99.9 & 96.4 & 94.8 & 1.48 & 0.50 & 16.4 \\ DiGress  & 85.7 & 100.0 & 95.0 & 97.1 & 1.19 & 0.52 & 14.8 \\  DisCo-MPNN & 83.9 & 100.0 & 98.8 & 87.3 & 1.63 & 0.48 & 13.5 \\ DisCo-GT & 88.3 & 100.0 & 97.7 & 95.6 & 1.44 & 0.50 & 15.1 \\   

Table 3: Performance on MOSES. VAE, JT-VAE, and GraphINVENT have hard-coded rules to ensure high validity.

    & GT & MPNN \\  \# Parameters & \(14 10^{6}\) & \(7 10^{6}\) \\ Forward & \(0.065\) & \(0.022\) \\ Backprop. & \(0.034\) & \(0.018\) \\   

Table 5: Efficiency comparison in terms of number of parameters, forward and backpropagation time (second/iteration).

### Ablation Study

An ablation study on DisCo-GT for reference distributions (marginal vs. uniform), and sampling steps (\(1\) to \(500\)) is presented in Table 6. The number of sampling steps is \(()\) if \(T=1\). QM9 dataset is chosen. A similar ablation study on DisCo-MPNN is in Table 9 in Appendix. We observe that first, generally, the fewer sampling steps, the lower the generation quality. In some cases (e.g., the marginal distribution) with the sampling steps decreasing significantly (e.g., from \(500\) to \(30\)), the performance degradation is still very slight, implying our method's high robustness in sampling steps. Second, the marginal reference distribution is better than the uniform distribution, consistent with the observation from DiGress .

## 5 Related Work

Diffusion models  can be interpreted from both the score-matching [69; 70] or the variational autoencoder perspective [23; 35; 34]. Pioneering efforts on diffusion generative modeling study the process in continuous-state [67; 23; 68] whose typical reference distribution is Gaussian. Beyond that, some efforts propose discrete-state models  to. E.g., D3PM  designs the discrete diffusion process by multiplication of transition matrices; \(\)-LDR  generalizes D3PM by formulating a continuous-time Markov chain;  proposes a singleton conditional distribution-based objective for the continuous-time Markov chain-based model whose rationale is Brook's Lemma [5; 49].

Diffusion models are widely used in graph generation tasks [44; 13; 85; 86; 84; 15; 83; 14; 61; 74; 47; 79; 78; 59; 3] such as molecule design [65; 25; 27; 43]. Pioneering works such as EDP-GNN  and GDSS  diffuse graph data in a continuous state space . DiscDDPM  is an early effort to modify the DDPM architecture into a discrete state. In addition, DiGress  is also a one-shot discrete-state diffusion model, followed by a very recent work MCD , both in the discrete-time setting. Beyond the above-mentioned efforts, DruM  proposes to mix the diffusion process. EDGE  proposes an interesting process: diffusing graphs into empty graphs. Besides, GRAPHARM  proposes an autoregressive graph diffusion model, and  applies the diffusion models for molecule property prediction tasks. In addition to the above-mentioned general graph diffusion models, there are many other task-tailored graph diffusion generative models [48; 30; 41; 60; 77; 76; 4; 75], which incorporate more in-depth domain expertise into the model design. Interested readers are referred to this survey .

## 6 Conclusion

This paper introduces the first discrete-state continuous-time graph diffusion generative model, DisCo. Our model effectively marries continuous-time Markov Chain formulation with the discrete nature of graph data, addressing the fundamental sampling limitation of prior models. DisCo's training objective is concise with a solid theoretical foundation. We also propose a simplified message-passing architecture to serve as the graph-to-graph backbone, which theoretically has desirable properties against permutation of node ordering and empirically demonstrates decent performance against existing graph generative models in tests on various datasets.

  Ref. Dist. & Steps & Valid \(\) & V.U. \(\) & V.U.N. \(\) \\   & 500 & 99.3\(\)0.6 & 98.9\(\)0.6 & 56.2\(\)0.4 \\  & 100 & 98.7\(\)0.5 & 98.5\(\)0.4 & 58.8\(\)0.4 \\  & 30 & 97.9\(\)1.2 & 97.6\(\)1.1 & 59.2\(\)0.8 \\  & 10 & 95.3\(\)1.9 & 94.8\(\)1.6 & 62.1\(\)0.9 \\  & 5 & 93.0\(\)1.7 & 92.4\(\)1.3 & 64.9\(\)1.1 \\  & 1 & 76.1\(\)2.3 & 73.9\(\)1.6 & 62.9\(\)1.8 \\   & 500 & 94.1\(\)0.9 & 92.9\(\)0.5 & 56.6\(\)0.4 \\  & 100 & 91.5\(\)1.0 & 90.3\(\)0.9 & 54.4\(\)1.2 \\   & 30 & 88.7\(\)1.6 & 86.9\(\)1.0 & 58.6\(\)2.1 \\   & 10 & 84.5\(\)2.3 & 80.4\(\)1.7 & 59.8\(\)1.8 \\   & 5 & 77.0\(\)2.5 & 69.9\(\)1.5 & 56.1\(\)3.5 \\   & 1 & 44.9\(\)3.1 & 35.1\(\)3.4 & 29.6\(\)2.5 \\  

Table 6: Ablation study (mean\(\)std%) with GT backbone. V., U., and N. mean Valid, Unique, and Novel.