# Latent Functional Maps:

a spectral framework for representation alignment

 Marco Fumero

IST Austria

marco.fumero@ist.ac.at

&Marco Pegoraro

Sapienza, University of Rome

pegoraro@di.uniroma1.it

&Valentino Maiorca

Sapienza, University of Rome

maiorca@di.uniroma1.it

&Francesco Locatello

IST Austria

francesco.locatello@ist.ac.at

&Emanuele Rodola

Sapienza, University of Rome

rodola@di.uniroma1.it

Equal ContributionWork done while visiting ISTA

###### Abstract

Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge. By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks. To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces. We validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment.

## 1 Introduction

Neural Networks (NNs) learn to represent high-dimensional data as elements of lower-dimensional latent spaces. While much attention is given to the model's output for tasks such as classification, generation, reconstruction, or denoising, understanding the internal representations and their geometry is equally important. This understanding facilitates reusing representations for downstream tasks  and the comparison between different models , thereby broadening the applicability of NNs, and understanding their structure and properties. Moreover, recent studies have shown that distinct neural models often develop similar representations when exposed to similar stimuli, a phenomenon observed in both biological  and artificial settings . Notably, even when neural networks have different architectures, internal representations of distinct models can often be aligned through a linear transformation . This indicates a level of consistency in how NNs process information, highlighting the importance of exploring and understanding these internal representations.

In this paper, we shift our focus from characterizing relationships between samples in distinct latent spaces to modeling a map among function spaces defined on these representational spaces. To this end, we propose leveraging spectral geometry principles by applying the framework of _functional maps_ to the field of representation learning. Functional maps represent correspondences between function spaces on different manifolds, providing a new perspective on the problem of representational alignment. In this setting, many complex constraints can be easily expressed compactly . For instance, as shown in Figure 1, the mapping (\(C\)) between two spaces \(\) and \(\) can be represented in the functional space as a linear map with a sparse structure.

Originally used in 3D geometry processing and more recently on graphs applications [60; 46], we extend this framework to handle arbitrary large dimensional latent spaces. Our proposed method, _Latent Functional Map_ (LFM), is a flexible tool that allows (i) to compare distinct representational spaces, (ii) to find correspondences between them both in an unsupervised and weakly supervised way, and (iii) to transfer information between them. Our contributions can be listed as follows:

* We introduce the framework of Latent Functional Maps as a way to model the relation between distinct representational spaces of neural models.
* We show that LFM allows us to find correspondences between representational spaces, both in weakly supervised and unsupervised settings, and to transfer representations across distinct models.
* We showcase LFM capabilities as a meaningful and interpretable similarity measure between representational spaces.
* We validate our findings in retrieval and stitching tasks across different models, modalities and datasets, demonstrating that LFMs can lead to better performance and sample efficiency than other methods.

## 2 Related Work

Similarity between latent spaces.Comparing representations learned by neural models is of fundamental importance for a diversity of tasks , ranging from representation analysis to latent space alignment and neural dynamics. In order to do so, a similarity measure between different spaces must be defined . This can range from functional similarity (matching the performance of two models) to similarity defined in representational space , which is where our framework falls in. A classical statistical method is Canonical Correlation Analysis (CCA) , known for its invariance to linear transformations. Various adaptations of CCA aim to enhance robustness, such as Singular

Figure 1: **Framework overview:** given two spaces \(\), \(\) their samples lie on two manifolds \(\), \(\), which can be approximated with the KNN graphs \(G_{}\),\(G_{}\). We can optimize for a _latent functional map_\(C\) between the eigenbases of operators defined on the graphs. This map serves as a map between functions defined on the two manifolds and can be leveraged for (i) comparing representational spaces, (ii) solving correspondence problems, and (iii) transferring information between the spaces.

Vector Canonical Correlation Analysis (SVCCA) , or to decrease sensitivity to perturbations using methods like Projection-Weighted Canonical Correlation Analysis (PWCCA) . Closely related to these approaches, Centered Kernel Alignment (CKA)  measures the similarity between latent spaces while ignoring orthogonal transformations. However, recent research  reveals that CKA is sensitive to local shifts in the latent space.

We propose to leverage LFMs as a tool to measure the similarity, or how much two spaces differ from an isometry w.r.t. to the metric that has been used to construct the graph.

Representation alignment.Complementary to measuring the similarity of distinct representational spaces, several methods have been proposed to optimize for a transformation to align them . The concept of _latent space communication_, introduced by , builds on the hypothesis that latent spaces across neural networks, varying random seed initialization to architecture or even data modality, are intrinsically compatible. This notion is supported by numerous empirical studies [38; 30; 23; 6; 55; 3; 58; 27; 29; 4; 40; 9; 15; 8], with the phenomenon being particularly evident in large and wide models [51; 33]. The core idea is that relations between data points (i.e., distances according to some metric) are preserved across different spaces because the high-level semantics of the data are the same and neural networks learn to encode them similarly . With this "relative representation", the authors show that it is possible to _stitch_ together model components coming from different models, with little to no additional training, as long as a partial correspondence of the spaces involved is known. Indeed, [26; 32; 35; 37] show that a simple linear transformation is usually enough to effectively map one latent space into another, measuring the mapping performance via desired downstream tasks.

With LFMs, we change the perspective from merely relating samples of distinct latent spaces to relating function spaces defined on the manifold that the samples approximate, showing that processing information in this dual space is convenient as it boosts performance while also being interpretable.

Functional Maps.The representation we propose is directly derived from the functional maps framework for smooth manifolds introduced in the seminal work by . This pioneering study proposed a compact and easily manipulable mapping between 3D shapes. Subsequent research has aimed at enhancing this framework. For instance,  introduced regularization techniques to improve the informativeness of the maps, while  developed refinement methods to achieve more accurate mappings. The functional map framework has been extended as well outside the 3d domain, for example, in  and , who applied the functional framework to model correspondences between graphs, and in , who demonstrated its utility in graph learning tasks. In particular, they have shown that the functional map representation retains its advantageous properties even when the Laplace basis is computed on a graph.

Inspired by these advancements, our work leverages the functional representation of latent spaces of neural models. We demonstrate how this representation can be easily manipulated to highlight similarities and facilitate the transfer of information between different spaces, thereby extending the applicability of the functional maps framework to the domain of neural latent spaces.

## 3 Method

### Background

This section provides the basic notions to understand the framework of functional maps applied to manifolds. We refer to  for a comprehensive overview.

Consider two manifolds \(\) and \(\) equipped with a basis \(^{}\) (respectively \(^{}\)). Any squared integrable function \(f:\) can be represented as a linear combination of basis functions \(_{}\): \(f=_{i}a_{i}_{i}^{}=_{}\). Given a bijective correspondence \(T:\) between points on these manifolds, for any real-valued function \(f:\), one can construct a corresponding function \(g:\) such that \(g=f T^{-1}\). In other words, the correspondence \(T\) defines a mapping between two function spaces \(T_{F}:(,)(,)\). Such a mapping is _linear_ and can be represented as a matrix \(\) such that for any function \(f\) represented as a vector of coefficients \(\), we have \(T_{F}()=\).

The functional representation is particularly well-suited for map inference (i.e., constrained optimization problems). When the underlying map \(T\) (and by extension the matrix \(\)) is unknown, many natural constraints on the map become linear constraints in its functional representation. In practice, the simplest method for recovering an unknown functional map is to solve the following optimization problem:

\[*{arg\,min}_{}||-||^{2}+ ()\] (1)

where \(\) and \(\) are sets of corresponding functions, denoted as _descriptors_, expressed in the bases on \(\) and \(\), respectively, and \(()\) represents additional constraints deriving from the properties of the matrix \(\). When the manifolds \(\) and \(\) are approximately isometric and the descriptors are well-preserved by the (unknown) map, this procedure provides a good approximation of the underlying map. In cases where the correspondence \(T\) is encoded as a permutation matrix \(\), the functional map can be retrieved as \(=_{}^{}_{ }\) where \(_{}\) and \(_{}\) are the bases of the functional spaces \((,)\) and \((,)\), respectively, and \({}^{}\) denotes the Moore-Penrose pseudo-inverse.

### Latent Functional Maps

#### 3.2.1 Setting

We consider deep neural networks \(f:=f_{1} f_{2}...f_{n}\) where each layer \(f_{i}\) is associated to a representational space \(_{i}\) corresponding to the image of \(f_{i}\). In the following we're gonna drop the subscript \(i\), when the layer considered is clear form the context. We assume that elements \(x\) are sampled from a latent manifold \(\). Considering pairs of spaces \((,)\), and corresponding manifolds \(,\) our objective is to characterize the relation between them by mapping the space of functions \(()\) to \(()\). Our framework can be summarized in three steps, which we are going to describe in the following sections: (i) how to represent \(\) from a sample estimate \(X\), by building a graph in the latent space \(\) (ii) how to encode any known preserved quantity between the two spaces by defining a set of descriptor function for each domain (iii) optimizing for the latent functional map between \(\) and \(\). An illustrative description of our framework is depicted in Figure 1.

Building the graph.To leverage the geometry of the underlying manifold, we model the latent space of a neural network building a symmetric k-nearest neighbor (k-NN) graph . Given a set of samples \(X=\{x_{1},,x_{n}\}\), we construct an undirected weighted graph \(G=(X,E,)\) with nodes \(X\), edges \(E\), and weight matrix \(\). The weight matrix is totally characterized by the choice of a distance function \(d:\).Suitable choices for \(d\) include the \(L2\) metric or the angular distance. Edges \(E\) are defined as \(E=\{(x_{i},x_{j}) X X x_{i}_{k}x_{j}x_{j}_{k}x_{i}\}\), where \(x_{i}_{k}x_{j}\) indicates that \(x_{j}\) is among the \(k\) nearest neighbors of \(x_{i}\). The weight matrix \(_{ 0}^{n n}\) assigns a weight \((x_{i},x_{j})=(d(x_{i},x_{j}))\) to each edge \((x_{i},x_{j}) E\), and \(_{i,j}=0\) otherwise, with \(\) being some monotonic function. Next, we define the associated weighted graph Laplacian operator \(_{G}=-^{-1/2}^{-1/2}\), where \(\) is the diagonal degree matrix with entries \(_{i,i}=_{j=1}^{n}_{i,j}\). \(_{G}\) is a positive semi-definite, self-adjoint operator ), therefore, it admits an eigendecomposition \(_{G}=_{G}_{G}_{G}^{T}\), where \(_{G}\) is a diagonal matrix containing the eigenvalues, and \(_{G}\) is a matrix whose columns are the corresponding eigenvectors. The eigenvectors form an orthonormal basis for the space of functions defined on the graph nodes (i.e., \(_{G}^{T}_{G}=\)). We give comprehensive details on the choice of \(k\) in Appendix A.1.

Choice of descriptors.To define the alignment problem between pair of spaces \(,\) we will introduce the notion of _descriptor function_. We define as descriptor function any real valued function \(f:G^{k}\) defined on the nodes of the graph. Informally descriptors should encode the information which is _shared_ between \(\) and \(\), either explicitly or implicitly. We categorize descriptor functions into supervised, weakly supervised and unsupervised descriptors. For the former we assumed to have access to a partial correspondence between the domains \(,\), defined as a bijective mapping \(_{S}:_{}_{}\) where \(_{},_{} \). Then descriptors takes the form of distance functions \(d_{}:_{}^{+}\), \(d_{}:_{S}(_{}) ^{+}\). As an example considering the geodesic distance (shortest path distance on the k-NN graph) from each node of the graph to the nodes in the anchor set, is an instance of supervised descriptor. In the case of weakly supervised descriptor, we assume to have access to an equivalence relation defined \(_{W}:_{}_{}\{0,1\}\). Example of these are multi-to-multi mappings, like mappings between labels. Unsupervised descriptors are quantities that depends on the topology and the geometry of the graph itself and they don't rely on any pre-given correspondence or equivalence relation. An example of this is the Heat Kernel Signature  node descriptor, based on heat diffusion over the manifold. We give examples of descriptors and ablation in Appendix C.2.

**Computing LFMs**. We now describe the optimization problem to compute a latent functional map between \(\) and \(\). We model each space using a subset of training samples \(X=\{x_{1},,x_{n}\}\) and \(Y=\{y_{1},,y_{n}\}\) and build the k-NN graphs \(G_{X}\) and \(G_{Y}\) from these samples, respectively. For each graph, we compute the graph Laplacian \(_{G}\) and derive the first \(k\) eigenvalues \(_{G}\) and eigenvectors \(_{G}=[_{1},,_{k}]\), which serve as the basis for the function space defined on the latent spaces.

Given the set of descriptors \(_{G_{X}}=[f_{1}^{G_{X}},,f_{n_{f}}^{G_{X}}]\) and \(_{G_{Y}}=[f_{1}^{G_{Y}},,f_{n_{f}}^{G_{Y}}]\), we consider the optimization problem defined in Equation 1 and incorporate two regularizers which enforce commutativity for the Laplacian and the descriptors, respectively, with the map \(\):

\[*{arg\,min}_{}}_{G_{X}} -}_{G_{Y}}_{F}^{2}+_{}()+ _{f}()\] (2)

where \(}_{G}=_{G}^{T}_{G}\) are the spectral coefficients of the functions \(_{G}\), \(_{}\) and \(_{f}\) are the Laplacian and descriptor operator commutativity regularizers respectively, akin to . We give full details on the regularizers in Appendix A. Once we have solved the optimization problem defined in Equation 2, we refine the resulting functional map \(\) using the spectral upsampling algorithm proposed by , as detailed in Appendix A.3.

### LFMs as a similarity measure

Once computed, the functional map \(\) can serve as a measure of similarity between spaces. The reason is that for isometric transformations between manifolds, the functional map is volume preserving (see Thm 5.1 in ), and this is manifested in orthogonal \(\). By defining the inner product between functions \(h_{1},h_{2}(M)\) as \( h_{1},h_{2}=_{}h_{1}(x)h_{2}(x)(x)\), it holds that \( h_{1},h_{2}=_{1},_{2}\) when the map preserves the local area, where \(\) denotes the functional representation of \(h\). In other words, when the transformation between the two manifolds is an isometry, the matrix \(^{T}\) will be diagonal. By measuring the ratio between the norm of the off-diagonal elements of \(^{T}\) and the norm of the full matrix, we can define a measure of similarity \(sim(X,Y)=1-((^{T})_{ F}^{2}}{||^{T}||_{F}^{2}}\). In Appendix A.4 we prove that this similiarity measure is a proper distance on the space of functional maps, allowing to compare measurements from collections of spaces. Furthermore, this quantity is interpretable; the first eigenvector of \(^{T}\) can act as a signal to localize the area of the target manifold where the map has higher distortion , as we show in the experiment in Figure 2(a).

### Transfering information with LFM

The map \(\) computed between two latent spaces can be utilized in various ways to transfer information from one space to the other. In this paper, we focus on two methods: (i) Expressing arbitrary points in the latent space as distance function on the graph and transferring them through the functional domain; (ii) Obtaining a point-to-point correspondence between the representational spaces from the LFM (see the first step in Appendix A.3), starting from none to few known pairs, and leverage off-the-shelf methods to learn a transformation between the spaces. Additional strategies could be explored in future work.

Space of Functional Coefficients.The space of functional (or spectral) coefficients offers an alternative representation for points in the latent space \(\). Using the equation \(_{G}=_{G}^{T}f_{G}\), any function \(f_{G}(G,)\) can be uniquely represented by its functional coefficients \(_{G}\). We leverage this property to represent any point \(x\) as a distance function \(f_{d}(G,)\) from the set of points \(X_{G}\), which correspond to the nodes of the graph \(G\). The functional map \(\) between two latent spaces \(\) and \(\) aligns their functional representations, enabling the transfer of any function from the first space to the second. This functional alignment can be used similarly to the method proposed by  to establish a shared space where the representational spaces \(\) and \(\) are aligned.

Finding correspondences.As explained in Section 3, the functional map \(\) represents the bijection \(T\) in a functional form.  demonstrated that this bijection can be retrieved as a point-to-pointmap by finding the nearest neighbor for each row of \(_{G_{Y}}\) in \(_{G_{X}}\). This process can be efficiently implemented and scaled up using kd-tree structures or approximation strategies [21; 20]. Starting from an empty set or few known correspondences (anchors) between the two spaces \(\) and \(\), we can extend the correspondence to the entire set of nodes \(X\) and \(Y\). This extended set of anchors can then be used to fit an arbitrary transformation between the latent spaces, for example an orthogonal mapping . In our experiments, we demonstrate that by using a very small number of anchors (typically \( 50\)), we can retrieve optimal transformations that facilitate near-perfect stitching and retrieval tasks.

## 4 Experiments

In this section, we present a series of experiments designed to evaluate the effectiveness of the Latent Functional Map (LFM) framework. We explore its application across various tasks, including similarity evaluation, stitching, retrieval performance, and robustness to perturbations in latent space. By comparing LFM to existing methods under different conditions, we aim to demonstrate its versatility and superior performance in aligning and analyzing neural network representations. Additional ablations and qualitative visualizations on the choice of distance metric to construct the graph and descriptors selection are reported in the Appendix C.

### Analysis

We demonstrate the benefits of using latent functional maps for comparing distinct representational spaces, using the similarity metric \(sim(X,Y)\) defined in Section 3.3

Experimental settingIn order to validate experimentally if LFMs can serve as a good measure of similarity between distinct representational spaces, we run the same sanity check as . We train 10 CNN models (the architecture is depicted in Appendix B.1) on the CIFAR-10 dataset , changing the initialization seed. We compare their inner representations at each layer, excluding the logits and plot them as a similarity matrix, comparing with Central Kernel Alignment (CKA) measure  and Canonical Correlation Analysis (CCA) [18; 48]. We then measure the accuracy of identifying corresponding layers across models and report the results comparing with CKA and CCA as baselines. For CCA, we apply average pooling on the spatial dimensions to the embeddings of the internal layers, making it more stable numerically and boosting the results for this baseline compared to what was observed in .

Result analysisFigure 2 shows that our LFM-based similarity measure behaves correctly as CKA does. Furthermore, the similarities are less spread around the diagonal, favoring a slightly higher accuracy score in identifying the corresponding layers across models.

While CKA (Centered Kernel Alignment) is a widely used similarity metric in deep learning, recent research by  has shown that it can produce unexpected or counter-intuitive results in certain situations. Specifically, CKA is sensitive to transformations that preserve the linear separability of

Figure 2: **Similarity across layers Similarity matrices between internal layer representations of CIFAR10 comparing our LFM-based similarity with the CCA and CKA baselines, averaged across 10 models. For each method, we report the accuracy scores for matching the corresponding layer by maximal similarity.**

two spaces, such as local translations. Our proposed similarity measure is robust to these changes and demonstrates greater stability compared to CKA.

Experimental settingWe compute the latent representations from the pooling layer just before the classification head for the CIFAR10 train and test sets. Following the setup in , we train a Support Vector Machine (SVM) classifier on the latent representations of the training samples to find the optimal separating hyperplane between samples of one class and others. We then perturb the samples by translating them in a direction orthogonal to the hyperplane, ensuring the space remains linearly separable. We measure the CKA and LFM similarities as functions of the perturbation vector norm, as shown in Figure 2(a). In the accompanying plot on the right, we visualize the area distortion of the map by projecting the first singular component of the LFM \(\) into the perturbed space and plotting it on a 2d TSNE  projection of the space.

Result AnalysisWe start by observing that when the latent space is perturbed in a way that still preserves its linear separability, it should be considered identical from a classification perspective, as this does not semantically affect the classification task. Figure 2(a) shows that while CKA degrades as a function of perturbation intensity, the LFM similarity remains stable to high scores. To understand this difference, we can visualize the area distortion as a function of the samples by projecting the first singular component of \(\) onto the perturbed space. In Figure 2(b), we use t-SNE  to project the perturbed samples and the distortion function into 2D. The visualization reveals that distortion is localized to the samples corresponding to the perturbed class.

### Zero-shot stitching

In latent communication tasks, a common challenge is combining an encoder that embeds data with a decoder specialized in a downstream task, such as classification or reconstruction--this process is often referred to as _stitching_. Previous works, like  and , have used trainable linear projections, known as stitching layers, to enable the swapping of different network components. However,  introduced the concept of zero-shot stitching, where neural components can be merged without any additional training procedure. It's important to note that while  still required to trained a decoder module once for processing relative representations before stitching could be performed, our method is fully zero-shot, with no need for additional training. In the following experiments, stitching is conducted without any training or fine-tuning of the encoder or decoder, adhering strictly to a _zero-shot fashion_.

Experimental SettingWe consider four pre-trained image encoders (see Appendix B.2 for details) and stitch their latent spaces to perform classification using a Support Vector Machine (SVM) on five

Figure 3: **Robustness of LFM similarity**_Left:_ Similarity scores as a function of perturbation strength: while the CKA baseline degrades, our LFM similarity scores are robust to perturbations that preserve linear separability of the space. _Right:_ Visualization of area distortion of the map by projecting the first singular component of the LFM in the perturbed space: the distortion localizes on the samples of the perturbed class, making LFM similarity interpretable.

[MISSING_PAGE_FAIL:8]

when more than 50 anchors are used, at which point LFM's effectiveness is constrained by the number of eigenvectors. Comparing the CIFAR100 coarse and fine-grained labeling, the fine-grained task is more complex due to the larger number of classes, making the estimation of the map more challenging. This complexity is especially evident when aligning self-supervised vision models with classification-based ones (e.g., DINO vs. ViT), where the training strategies differs.

This experiment shows that the latent functional map is highly effective when few anchors are available (\( 50\)). It significantly enhances performance in zero-shot stitching tasks, outperforming direct orthogonal transformations at low or no anchor counts. This suggests that the latent functional map method provides a robust means of aligning latent spaces with minimal correspondence data, making it a valuable tool for tasks requiring the integration of independently trained models.

#### 4.2.1 Qualitative example

In Figure 5, we present qualitative experiments on the MNIST, FashionMNIST , and CIFAR-10 datasets, focusing on visualizing the reconstructions of stitched autoencoders. For these experiments, we trained convolutional autoencoders using two different seeds and stitched their encoder and decoder modules together. As a baseline, we used no transformation (Absol.), where the latent representation from the first encoder is directly input into the second decoder. We then compared this baseline with the orthogonal transformation from  (Ortho) and its LFM-augmented version (LFM). For both our method and , we used 10, 10 and 50 correspondences for the MNIST, FashionMNIST, and CIFAR-10 datasets, respectively. Our method consistently produced superior reconstruction quality compared to both the baseline and the method from .

### Retrieval

We extend our analysis to the retrieval task, where we look for the most similar embedding in the aligned latent space.

Experimental SettingWe consider two different English word embeddings, FastText  and Word2Vec . Following the approach of , we extract embeddings of 20K words from their shared vocabulary using pre-trained models. We use 2K random corresponding samples to construct the k-NN graphs and evaluate the retrieval performance on the remaining 18K word embeddings. We test two settings in our experiments:

1. Aligning functional coefficients (LFM Space).

Figure 6: **Retrieval of word embeddings. We compare the retrieval performance of the functional map framework with state-of-the-art models as the number of anchors increases. The left panel shows the Mean Reciprocal Rank (MRR) across different numbers of anchors. The right panels depict the first two components of PCA for a subsample of the latent space (b) and the functional space (c), both before and after alignment using the functional map.**

2. Computing an orthogonal transformation using the correspondences obtained by the functional map (LFM+Ortho).

For this experiment, we construct k-NN graphs with a neighborhood size of 300 and compute the functional map using the first 50 eigenvectors. We evaluate the methods' performance using the Mean Reciprocal Rank (MRR), as detailed in Appendix B.4. Our functional map methods are compared with the method proposed by  (Relatives) and the orthogonal transformation method proposed by  (Ortho). We choose to fit the same orthogonal transformation on top of the correspondence found by LFM to make the comparison the fairest possible, although in principle any off-the-shelf methods could be used to estimate the transformation once the new correspondence is found. We illustrate this versatility in Table 3 of the Appendix, where LFM is combined with other methods in the same task.

Result AnalysisFigure 6 shows the performance of these methods as the number of anchors increases. Numerical results are detailed in Table 2 in the Appendix. The functional map significantly improves performance with just 5 anchors, achieving an MRR of over 0.8. As the number of anchors increases, the performance of competing methods improves but still falls short of FMAP+Transform at 300 anchors, which reaches an MRR of 0.99. Interestingly, the performance of the functional map methods does not improve beyond 5 anchors, suggesting that this number of anchors is sufficient to achieve an optimal functional map between the spaces. These findings are further supported by Table 3 in the Appendix, where LFM consistently achieves superior MRR scores, particularly with fewer anchors, outperforming other baselines such as . In Appendix C.3, we analyze how the results improve as the number of eigenvectors used to compute the functional map increases. Notably, the MRR score drastically increases to over 0.6 with more than 25 eigenvectors.

These results confirm that the latent functional map is a valuable tool in settings with little knowledge about correspondences. It significantly enhances retrieval performance with a minimal number of anchors, making it an efficient approach for aligning latent spaces. Moreover, its performance can be improved using a higher number of eigenvectors. In particular, our framework comprises at the same time (i) an interpretable similarity measure, (ii) a way to find correspondences, and (iii) solving for an explicit mapping between different spaces in a single framework, differently from  and  which attempt to solve just the latter implicitly.

## 5 Conclusions

In this paper, we explored the application of latent functional maps (LFM) to model and analyze the relationships between different latent spaces. Our approach leverages the principles of spectral geometry, providing a robust framework for comparing and aligning representations across various models and settings. By extending the functional map framework to high-dimensional latent spaces, we offer a versatile method for comparing distinct representational spaces, finding correspondences between them in both unsupervised and weakly supervised settings (few or no anchors) and transferring information across different models.

Limitations and future workThe performance of LFM can be sensitive to the number of eigenvectors used, as observed in some of our experiments (see Appendix C.3). Finding the optimal number of eigenvectors without extensive experimentation remains an open problem. The current framework assumes a correspondence between the domains, and is not directly adapted in order to deal with partial mappings, where just a subset of one domain can be mapped to other. Extending the results in [49; 47] to our neural representation setting, is a promising way to overcome the full correspondence assumption. Future research can further explore the potential of LFM in other domains and tasks, and modalities. The versatility of the functional representation can be further explored to define new ad-hoc constraints and regularizers for different settings. In particular, ts performance in fully unsupervised settings, while promising is still an area requiring further research and improvement. In conclusion, the Latent Functional Map framework offers a novel and effective approach to understanding and utilizing neural network representations, promising significant advancements in both theoretical and practical aspects of machine learning.