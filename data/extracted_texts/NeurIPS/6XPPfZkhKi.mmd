# HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception

Junkun Yuan\({}^{1,2}\), Xinyu Zhang\({}^{2}\)\({}^{*}\), Hao Zhou\({}^{2}\), Jian Wang\({}^{2}\), Zhongwei Qiu\({}^{3}\), Zhiyin Shao\({}^{4}\), Shaofeng Zhang\({}^{5}\), Sifan Long\({}^{6}\), Kun Kuang\({}^{1}\), Kun Yao\({}^{2}\), Junyu Han\({}^{2}\), Errui Ding\({}^{2}\),

**Lanfen Lin\({}^{1}\), Fei Wu\({}^{1}\), Jingdong Wang\({}^{2}\)**

\({}^{1}\)Zhejiang University \({}^{2}\)Baidu VIS \({}^{3}\)University of Science and Technology Beijing

\({}^{4}\)South China University of Technology \({}^{5}\)Shanghai Jiao Tong University \({}^{6}\)Jilin University

{zhangxinyu14,wangjingdong}@baidu.com {yuanjk,kunkuang}@zju.edu.cn

Code: https://github.com/junkunyuan/HAP

Project Page: https://zhangxinyu-xyz.github.io/hap.github.io/

Equal contribution. This work was done when Junkun Yuan, Zhongwei Qiu, Zhiyin Shao, and Shaofeng Zhang were interns at Baidu VIS. \({}^{4}\)Corresponding author.

###### Abstract

Model pre-training is essential in human-centric perception. In this paper, we first introduce masked image modeling (MIM) as a pre-training approach for this task. Upon revisiting the MIM training strategy, we reveal that human structure priors offer significant potential. Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out. This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP simply uses a plain ViT as the encoder yet establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par result on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.

## 1 Introduction

Human-centric perception has emerged as a significant area of focus due to its vital role in real-world applications. It encompasses a broad range of human-related tasks, including person re-identification (ReID) [23; 32; 62; 90], 2D/3D human pose estimation [15; 78; 82], pedestrian attribute recognition [34; 36; 63], _etc_. Considering their training efficiency and limited performance, recent research efforts [8; 13; 16; 33; 64] have dedicated to developing general human-centric pre-trained models by using large-scale person datasets, serving as foundations for diverse human-centric tasks.

Among these studies, some works [8; 13; 33] employ contrastive learning for self-supervised pre-training, which reduces annotation costs of massive data in supervised pre-training [16; 64]. In this work, we take the first step to introduce masked image modeling (MIM), which is another form of self-supervised learning, into human-centric perception, since MIM [2; 9; 30; 77] has shown considerable potential as a pre-training scheme by yielding promising performance on vision tasks [44; 65; 78].

There is no such thing as a free lunch. Upon initiating with a simple trial, _i.e._, directly applying the popular MIM method  with the default settings for human-centric pre-training, it is unsurprising to observe poor performance. This motivates us to delve deep into understanding why current MIM methods are less effective in this task. After revisiting the training strategies, we identify three crucial factors that are closely related to human structure priors, including high scaling ratio2, block-wise mask sampling strategy, and intermediate masking ratio, have positive impacts on human-centric pre-training. Inspired by this, we further tap the potential of human structure-aware prior knowledge.

In this paper, we incorporate human parts , a strong and intuitive human structure prior, into human-centric pre-training. This prior is employed to guide the mask sampling strategy. Specifically, we randomly select several human parts and mask the corresponding image patches located within these selected regions. These part-aware masked patches are then reconstructed based on the clues of the remaining visible patches, which can provide semantically rich body structure information. This approach encourages the model to learn contextual correlations among body parts, thereby enhancing the learning of the overall human body structure. Moreover, we propose a structure-invariant alignment loss to better capture human characteristics. For a given input image, we generate two random views with different human parts being masked out through the proposed mask sampling strategy. We then align the latent representations of the two random views within the same feature space. It preserves the unique structural information of the human body, which plays an important role in improving the discriminative ability of the pre-trained model for downstream tasks.

We term the overall method as HAP, short for **H**uman structure-**A**ware **P**re-training with MIM for human-centric perception. HAP is simple in terms of modalities, data, and model structure. We unify the learning of two modalities (images and human keypoints) within a single dataset (LUPerson ). The model structure is based on the existing popular MIM methods, e.g., MAE  and CAE , with a plain ViT  as the encoder, making it easy to transfer to a broad range of downstream tasks (see Figure 0(a)). Specifically, HAP achieves state-of-the-art results on 11 human-centric benchmarks (see Figure 0(b)), _e.g._, 78.1% mAP on MSMT17 for person ReID, 68.05% Rank-1 on CUHK-PEDES for text-to-image person ReID, 86.54% mA on PA-100K for pedestrian attribute recognition, and 78.2% AP on MS COCO for 2D human pose estimation. In summary, our main contributions are:

* We are the first to introduce MIM as a human-centric pre-training method, and reveal that integrating the human structure priors are beneficial for MIM in human-centric perception.
* We present HAP, a novel method that incorporates human part prior into the guidance of the mask sampling process to better learn human structure information, and design a structure-invariant alignment loss to further capture the discriminative characteristics of human body structure.
* The proposed HAP is simple yet effective that achieves superior performance on 11 prominent human benchmarks across 5 representative human-centric perception tasks.

Figure 1: (a) Overview of our proposed HAP framework. HAP first utilizes human structure priors, _e.g._, human body parts, for human-centric pre-training. HAP then applies the pre-trained encoder, along with a task-specific head, for addressing each of the human-centric perception tasks. (b) Our HAP demonstrates state-of-the-art performance across a broad range of human-centric benchmarks.

## 2 Related work

**Human-centric perception** includes a broad range of human-related tasks. The goal of person Re-ID [24; 32; 51; 50; 81; 90; 84; 86] and text-to-image person ReID [59; 20; 62; 58] is to identify persons of interest based on their overall appearance or behavior. Pedestrian attribute recognition [35; 36] aims to classify the fine-grained attributes and characteristics of persons. 2D [78; 79; 82] and 3D [69; 14; 25] pose estimation learn to predict position and orientation of individual body parts.

Recent works make some efforts to pre-train a human-centric model. HCMoCo  learns modal-invariant representations by contrasting features of dense and sparse modalities. LiftedCL  extracts 3D information by adversarial learning. UniHCP  and PATH  design multi-task co-training approaches. SOLIDER  tries to balance the learning of semantics and appearances.

Compared with these works, this paper focuses on exploring the advantage of utilizing human structure priors in human-centric pre-training. Besides, instead of training with numerous modalities and datasets using a specially designed structure, we pursue a simple pre-training framework by employing two modalities (images and human keypoints) of one dataset (LUPerson ) and vanilla ViT structure with the existing popular MIM methods (_e.g._, BEiT , MAE , and CAE ).

**Self-supervised learning** has two main branches: contrastive learning (CL) and masked image modeling (MIM). CL [4; 7; 10; 11; 12; 28; 31] is a discriminative approach that learns representations by distinguishing between similar and dissimilar image pairs. Meanwhile, MIM [2; 9; 30; 56; 72; 73; 77; 85; 88] is a generative approach that learns to recover the masked content in the corrupted input images. It is shown that CL learns more discriminative representations [4; 12], while MIM learns finer grained semantics [2; 30]. To obtain transferable representations for the diverse tasks, our work combines both of them to extract discriminative and fine-grained human structure information.

Semantic-guided masking is presented in some recent MIM works [6; 29; 37; 40; 45; 68; 83]. Many methods [83; 29; 6; 37] utilize the attention map to identify and mask informative image patches, which are then reconstructed to facilitate representation learning. In comparison, this paper explores to directly employ the human structure priors to guide mask sampling for human-centric perception.

## 3 Hap

Inspired by the promising performance of MIM in self-supervised learning, we initially employ MIM as the pre-training scheme for human-centric perception. Compared with MIM, our proposed HAP integrates human structure priors into MIM for human-centric pre-training, and introduces a structure-invariant alignment loss. The overview of our HAP framework is shown in Figure 2.

### Preliminary: great potential of human structure priors

We use MAE  with its default settings as an example for human-centric pre-training. LUPerson  is used as the pre-training dataset following [8; 23; 51]. We designate this setting as our _baseline_. The results are shown in Figure 3. The baseline achieves poor performance on two representative human-centric perception tasks, i.e., person ReID and 2D human pose estimation. This phenomenon motivates us to delve deep into understanding the underlying reasons.

We start by visualizing the input images under different training strategies in Figure 4. Surprisingly, we observe that it is hard to learn information of human body structure from the input images of the previous baseline (see Figure 4 (b-c)). We conjecture that: i) the scaling ratio may be too small to preserve the overall human body structure, as shown in Figure 4 (b), and ii) the masking ratio may be too large to preserve sufficient human structure information, which makes the reconstruction task too difficult to address, as shown in Figure 4 (c).

Figure 4: **Visualization of training strategies.** For a given image (a), the baseline  uses 20% scaling ratio (b) and 75% masking ratio (c) with random mask sampling strategy, yielding a meaningless image with little human structure information. We adopt 80% scaling ratio and 50% masking ratio with block-wise mask sampling (d), maintaining the overall body structure.

Driven by this conjecture, we further study the impacts of scaling ratio and masking ratio in depth. Figure 3 illustrates that both _i) high scaling ratio_ (ranging from 60% to 90%) and _ii) mediate masking ratio_ (ranging from 40% to 60%) contribute positively to the performance improvement of human-centric tasks. Especially, an 80% scaling ratio can bring an approximately 3.5% improvement on MSMT17 of the person ReID task. This effectively confirms that information of human body structure is crucial during human-centric pre-training. Additionally, we also discover that _iii) block-wise mask sampling strategy_ performs sightly better than the random mask sampling strategy, since the former one masks semantic human bodies appropriately due to block regions. We refer to these three factors of the training strategy as _human structure priors_ since they are associated with human structure information. In the following exploration, we employ scaling ratio of 80%, masking ratio of 50% with block-wise masking strategy by default. See an example of our optimal strategy in Figure 4 (d).

### Human part prior

Inspired by the above analyses, we consider to incorporate human body parts, which are intuitive human structure priors, into human-centric pre-training. Here, we use 2D human pose keypoints , which are convenient to obtain, to partition human part regions by following previous works . Section 3.1 reveals that the mask sampling strategy plays an important role in human-centric pre-training. To this end, we directly use human structure information provided by human body parts as robust supervision to guide the mask sampling process of our HAP during human-centric pre-training.

Specifically, we divide human body into six parts, including head, upper body, left/right arm, and left/right leg according to the extracted human keypoints, following . Given a specific input image \(\), HAP first embeds \(\) into \(N\) patches. With a pre-defined masking ratio \(\), the aim of mask sampling is to mask \(N_{m}=\{n|n N\}\) patches, where \(\) represents the set of integers. Specifically, we randomly select \(P\) parts, where \(0 P 6\). Total \(N_{p}\) image patches located on these \(P\) part regions are masked out. Then we adjust the number of masked patches to \(N_{m}\) based on the instructions introduced later. In this way, unmasked (visible) human body part regions provide semantically rich human structure information for the reconstruction of the masked human body parts, encouraging the pre-trained model to capture the latent correlations among the body parts.

There are three situations for the masked patches adjustment: i) if \(N_{p}=N_{m}\), everything is well-balanced; ii) if \(N_{p}<N_{m}\), we further use block-wise sampling strategy to randomly select additional

Figure 2: **Our HAP framework.** During pre-training, we randomly select human part regions and mask the corresponding image patches. We then adjust the masked patches to match the pre-defined masking ratio. The encoder embeds the visible patches, and the decoder reconstructs the masked patches based on latent representations. For a given image, HAP generates two views using the mask sampling with human part prior. We apply the proposed structure-invariant alignment loss to bring the [CLS] token representations of these two views closer together. After pre-training, only the encoder with a plain ViT structure is retained for transferring to downstream human-centric perception tasks.

\(N_{m}-N_{p}\) patches; iii) if \(N_{p}>N_{m}\), we delete patches according to the sequence of human part selection. We maintain a total of \(N_{m}\) patches, which contain structure information, to be masked.

**Discussion.** i) We use human parts instead of individual keypoints as guidance for mask sampling. This is based on the experimental result in Section 3.1, _i.e._, the block-wise mask sampling strategy outperforms the random one. More experimental comparisons are available in Appendix. ii) HCMoCo  also utilizes keypoints during human-centric pre-training, in which sparse keypoints serve as one of the multi-modality inputs. Different from HCMoCo, we integrate two modalities, including images and keypoints, together for mask sampling. It should be noted that the keypoints in our method are pseudo labels; therefore they may not be accurate enough for supervised learning as in HCMoCo. However, these generated keypoints can be beneficial for the self-supervised learning.

### Structure-invariant alignment

Learning discriminative human characteristics is essential in human-centric perception tasks. For example, person ReID requires to distinguish different pedestrians based on subtle details and identify the same person despite large variances. To address this issue, we further propose a structure-invariant alignment loss, to enhance the representation ability of human structural semantic information.

For each person image \(\), we generate two different views through the proposed mask sampling strategy with guidance of human part prior. These two views, denoted as \(\) and \(}\), display different human part regions for the same person. After model encoding, each view obtains respective latent feature representations \(\) and \(}\) on the normalized [CLS] tokens. Our structure-invariant alignment loss \(_{}\) is applied to \(\) and \(}\) for aligning them. We achieve it by using the InfoNCE loss :

\[_{}=-( }/)}{_{i=1}^{B}( _{i}/)},\] (1)

where \(B\) is the batch size. \(\) is a temperature hyper-parameter set to 0.2 . In this way, the encoder is encouraged to learn the invariance of human structure, and its discriminative ability is improved.

Figure 3: **Study on training strategy.** We investigate three factors of training strategy, _i.e._, (a) scaling ratio, (b) masking ratio, and (a,b) mask sampling strategy, on MSMT17  (person ReID) and MPII  (2D pose estimation). The baseline is MAE  with default settings (20% scaling ratio, 75% masking ratio, random mask sampling strategy). The optimal training strategy for human-centric pre-training is with 80% scaling ratio, 50% masking ratio, and block-wise mask sampling strategy.

### Overall: HAP

To sum up, in order to learn the fine-grained semantic structure information, our HAP incorporates the human part prior into the human-centric pre-training, and introduces a structure-invariant alignment loss to capture discriminative human characteristics. The whole loss function to be optimized is:

\[=_{}+_{},\] (2)

where \(_{}\) is the MSE loss for reconstruction following . \(\) is the weight to balance the two loss functions of \(_{}\) and \(_{}\). We set \(\) to 0.05 by default, and leave its discussion in Appendix.

## 4 Experiments

### Settings

**Pre-training.** LUPerson  is a large-scale person dataset, consisting of about 4.2M images of over 200K persons across different environments. Following , we use the subset of LUPerson with 2.1M images for pre-training. The resolution of the input image is set to \(256 128\) and the batch size is set to 4096. The human keypoints of LUPerson, _i.e._, the human structure prior of HAP, are extracted by ViTPose . The encoder model structure of HAP is based on the ViT-Base . HAP adopts AdamW  as the optimizer in which the weight decay is set to 0.05. We use cosine decay learning rate schedule , and the base learning rate is set to 1.5e-4. The warmup epochs are set to 40 and the total epochs are set to 400. The model of our HAP is initialized from the official MAE/CAE model pre-trained on ImageNet by following the previous works [16; 78]. After pre-training, the encoder is reserved, along with task-specific heads, for addressing the following human-centric perception tasks, while other modules (_e.g._, regressor and decoder) are discarded.

**Human-centric perception tasks.** We evaluate our HAP on 12 benchmarks across 5 human-centric perception tasks, including _person ReID_ on Market-1501  and MSMT17 , _2D pose estimation_ on MPII , COCO  and AIC , _text-to-image person ReID_ on CUHK-PEDES , ICFG-PEDES  and RSTPReid , _3D pose and shape estimation_ on 3DPW , _pedestrian attribute

Figure 5: **Reconstructions of LUPerson (1st row), COCO (2nd row), and AIC (3rd row) images using our HAP pre-trained model. The pre-training dataset is LUPerson. For each triplet, we show (left) the original image, (middle) the masked image, and (right) images reconstructed by HAP. We highlight some interesting observations using red dotted ellipses. Although the reconstructed parts are not exactly identical to the original images, they are semantically reasonable as body parts. For example, in the first triplet of the 1st row, the reconstructed right leg of the player differs from the original, but still represents a sensible leg. It clearly shows that HAP effectively learns the semantics of human body structure and generalizes well across various domains. Zoom in for a detailed view.**recognition_ on PA-100K , RAP  and PETA . For person ReID, we use the implementation of  and report mAP. For 2D pose estimation, the evaluation metrics are PCKh for MPII and AP for COCO and AIC. The codebase is based on . Rank-1 is reported for text-to-image person ReID with the implementation of . mA is reported in the pedestrian attribute recognition task with the codebase of . The evaluation metrics are MPJPE/PA-MPJPE/MPVPE for 3D pose and shape estimation. Since there is no implementation with a plain ViT structure available for this task, we modify the implementation based on . More training details can be found in Appendix.

Table 1: **Main results. We compare HAP with representative task-specific methods, human-centric pre-training methods, and the baseline MAE . \({}^{}\) indicates using stronger model structures such as sliding windows in  and convolutions in . HAP\({}^{}\) follows the implementation of .**

### Main results

We compare HAP with both previous task-specific methods and human-centric pre-training methods. Our HAP is simple by using only one dataset of \(\)2.1M samples, compared with existing pre-training methods (Table1a), yet achieves superior performance on various human-centric perception tasks.

In Table 1, we report the results of HAP with MAE  on 12 datasets across 5 representative human-centric tasks. Compared with previous task-specific methods and human-centric pre-training methods, HAP achieves state-of-the-art results on **11** datasets of **4** tasks. We discuss each task next.

**Pedestrian attribute recognition.** The goal of pedestrian attribute recognition is to assign fine-grained attributes to pedestrians, such as young or old, long or short hair, _etc_. Table 1b shows that HAP surpasses previous state-of-the-art on all of the three commonly used datasets, _i.e_., +0.17% on PA-100K , +0.57% on RAP , and +0.36% on PETA . It can be observed that our proposed HAP effectively accomplishes this fine-grained human characteristic understanding tasks.

**Person ReID.** The aim of person ReID is to retrieve a queried person across different cameras. Table 1c shows that HAP outperforms current state-of-the-art results of MSMT17  by +5.0% (HAP 78.0% _vs._ MALE  73.0%) and +1.0% (HAP 78.1% _vs._ SOLIDER  77.1%) under the input resolution of \(256 128\) and \(384 128\), respectively. HAP also achieves competitive results on Market-1501 , _i.e_., 93.9% mAP with \(384 128\) input size. For a fair comparison, we also report the results of HAP with extra convolution modules following , denoted as HAP\({}^{}\). It brings further improvement by +1.3% and +2.0% on MSMT17 and Market-1501 with resolution of 384. Review that previous ReID pre-training methods [81; 23; 90; 51] usually adopt contrastive learning approach. These results show that our HAP with MIM is more effective on the person ReID task.

**Text-to-image person ReID.** This task uses textual descriptions to search person images of interest. We use the pre-trained model of HAP as the initialization of the vision encoder in this task. Table1e shows that HAP performs the best on the three popular text-to-image benchmarks. That is, HAP achieves 68.05%, 61.80% and 49.35% Rank-1 on CUHK-PEDES , ICFG-PEDES  and RSTPReid , respectively. Moreover, HAP is largely superior than MAE , _e.g_., 68.05% _vs._ 60.19% (+7.86%) on CUHK-PEDES. It clearly reveals that human structure priors especially human keypoints and structure-invariant alignment indeed provide semantically rich human information, performing well for human identification and generalizing across different domains and modalities.

**2D pose estimation.** This task requires to localize anatomical keypoints of persons by understanding their structure and movement. For fair comparisons, we also use multiple datasets as in  and larger image size as in [82; 8] during model training. Table 1d shows that HAP outperforms previous state-of-the-art results by +0.3%, +1.0%, +3.1% on MPII , COCO , AIC , respectively. Meanwhile, we find both multi-dataset training and large resolution are beneficial for our HAP.

**3D pose and shape estimation.** We then lift HAP to perform 3D pose and shape estimation by recovering human mesh. Considering that there is no existing method that performs this task on a plain ViT backbone, we thus follow the implementation of  and just replace the backbone to a plain ViT. The results are listed in Table 1f. It is interesting to find that despite this simple implementation, our HAP achieves competitive results for the 3D pose task even HAP does not see any 3D information. These observation may suggest that HAP improves representation learning ability on human structure from the 2D keypoints information, and transfers its ability from 2D to 3D.

**CAE  vs MAE .** HAP is model-agnostic that can be integrated into different kinds of MIM methods. We also apply HAP to the CAE framework  for human-centric pre-training. Table 2 shows that HAP with CAE  achieves competitive performance on various human-centric perception tasks, especially on discriminative tasks such as person ReID that HAP with CAE is superior than HAP with MAE  by 1.6% on Market-1501 (with the input resolution of \(256 128\)), and text-to-image person ReID that HAP with CAE performs better than HAP with MAE by 2.35% on RSTPReid. These observations demonstrate the versatility and scalability of our proposed HAP framework.

   & MSMT17 &  &  & MPII & COCO & RSTPReid \\   & 256 & 384 & 256 & 384 & & & & \\  MAE  & 76.4 & 76.8 & 91.7 & 91.9 & 86.54 & 91.8 & 75.9 & 49.35 \\ CAE  & 76.5 & 77.0 & 93.3 & 93.1 & 86.33 & 91.8 & 75.3 & 51.70 \\  

Table 2: HAP with MAE  and CAE . The evaluation metrics are the same as those in Table 1.

### Main properties

We ablate our HAP in Table 3. The analyses of main properties are then discussed in the following.

**Pre-training on human-centric data.** Our baseline, using default settings of  and LUPerson  as pre-training data, has achieved great improvement than original MAE , _e.g._, +6.9% mAP on MSMT17 and +6.30% Rank-1 on CUHK-PEDES. It shows that pre-training on human-centric data benefits a lot for human-centric perception by effectively bridging the domain gap between the pre-training dataset and downstream datasets. This observation is in line with previous works [23; 51].

**Human structure priors.** We denote HAP-0 to represent human structure priors, _i.e._, large scaling ratio and block-wise masking with intermediate masking ratio, introduced in Section 3.1. HAP-0 performs better than the baseline method, especially on MSMT17 with +5.2% improvement. It clearly reveals that human structure priors can help the model learn more human body information.

**Human part prior.** We further use human part prior to guide the mask sampling process. Compared with HAP-1 and HAP-0, part-guided mask sampling strategy brings large improvement, _e.g._, +1.0% on MSMT17 and +0.33% on PA-100K. It clearly shows that the proposed mask sampling strategy with the guidance of human part prior has positive impact on the human-centric perception tasks.

**Structure-invariant alignment.** The structure-invariant alignment loss is applied on different masked views to align them together. Compared with HAP-2 _vs._ HAP-0 and HAP _vs._ HAP-1, this alignment loss results in +1.5% and +1.3% improvement on MSMT17, and +0.7% and +0.7% improvement on MPII, respectively. It reflects that the model successfully captures human characteristics with this loss. We provide more discussions about the alignment choices as well as the weight in Appendix.

Overall, our HAP greatly outperforms the original MAE  and the baseline by large margins across various human-centric perception tasks, showing significant benefits of the designs of our HAP.

### Ablation study

**Number of samples.** We compare using different number of data samples during pre-training in Table 4 (left). Only using \(\)0.5M LUPerson data already outperforms original MAE  pre-trained on ImageNet . Meanwhile, more pre-training data brings larger improvement for our HAP.

**Training epochs.** Table 4 (right) shows that pre-training only 100 epochs already yields satisfactory performance. It suggests that HAP can effectively reduces training cost. These observations indicate that HAP, as a human-centric pre-training method, effectively reduces annotation, data, and training cost. Therefore, we believe HAP can be useful for real-world human-centric perception applications.

**Visualization.** To explore the structure information learned in the plain ViT encoder of HAP, we probe its attention map by querying the patch of a random body keypoint. See visualizations in Figure 6. Interestingly, we observe that HAP captures the relation among body parts. For example,

  samples & MSMT17 & MPII & epochs & MSMT17 & MPII \\  \(\)0.5M & 66.9 & 90.4 & 100 & 72.2 & 91.3 \\ \(\)1.0M & 71.9 & 91.2 & 200 & 73.9 & 91.6 \\ \(\)2.1M & 76.4 & 91.8 & 400 & 76.4 & 91.8 \\  

Table 4: Ablation on (left) **number of samples** and (right) **training epochs** for pre-training of HAP.

  method & HSP & HPM & SIA & MSMT17 & MPII & CUHK-PEDES & 3DPW \(\) & PA-100K \\  baseline & & & 68.9 & 90.3 & 66.49 & 57.4 & 83.41 \\ HAP-0 & ✓ & & 74.1 & 90.9 & 67.69 & 56.2 & 85.50 \\ HAP-1 & ✓ & ✓ & 75.1 & 91.1 & 67.95 & 56.2 & 85.83 \\ HAP-2 & ✓ & ✓ & ✓ & 75.6 & 91.6 & 68.00 & 56.1 & 86.38 \\ HAP & ✓ & ✓ & ✓ & **76.4** & **91.8** & **68.05** & **56.0** & **86.54** \\  

Table 3: **Main properties** of HAP. “HSP”, “HPM”, and “SIA” stand for human structure priors, human part prior for mask sampling, and structure-invariant alignment, respectively. We compare HAP with the baseline of MAE ImageNet pretrained model on the representative benchmarks.

when querying the right hip of the player in the first image pair (from LUPerson), we see that there are strong relations to the left leg, left elbow, and head. And when querying the left knee of the skateboarder in the second image pair (from COCO), it is associated with almost all other keypoints. These observations intuitively illustrate that HAP learns human body structure and generalizes well.

## 5 Conclusion

This paper makes the first attempt to use Masked Image Modeling (MIM) approach for human-centric pre-training, and presents a novel method named HAP. Upon revisiting several training strategies, HAP reveals that human structure priors have great benefits. We then introduces human part prior as the guidance of mask sampling and a structure-invariant alignment loss to align two random masked views together. Experiments show that our HAP achieves competitive performance on various human-centric perception tasks. Our HAP framework is simple by unifying the learning of two modalities within a single dataset. HAP is also versatility and scalability by being applied to different MIM structures, modalities, domains, and tasks. We hope our work can be a strong baseline with generative pre-training method for the future researches on this topic. We also hope our experience in this paper can be useful for the general human-centric perception and push this frontier.

**Broader impacts.** The proposed method is pre-trained and evaluated on several person datasets, thus could reflect biases in these datasets, including those with negative impacts. When using the proposed method and the generated images in this paper, one should pay careful attention to dataset selection and pre-processing to ensure a diverse and representative sample. To avoid unfair or discriminatory outcomes, please analyze the model's behavior with respect to different demographics and consider fairness-aware loss functions or post-hoc bias mitigation techniques. We strongly advocate for responsible use, and encourage the development of useful tools to detect and mitigate the potential misuse of our proposed method and other masked image modeling techniques for unethical purposes.

**Limitations.** We use human part prior to guide pre-training, which is extracted by existing pose estimation methods. Therefore, the accuracy of these methods could affect the pre-training quality. Besides, due to the limitations in patch size, the masking of body parts may not be precise enough. If there are better ways to improve this, it should further enhance the results. Nonetheless, the proposed HAP method still shows promising performance for a wide range of human-centric perception tasks.