# Memorization Detection Benchmark for Generative Image models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Generative models in medical imaging offer significant potential for data augmentation and privacy preservation, but they also pose risks of patient data memorization. This study presents a comprehensive, data-driven approach to evaluate and characterize the memorization behavior of generative models. We systematically compare various network architectures, loss functions, pretraining datasets, and distance metrics to identify optimal configurations for detecting potential privacy concerns in synthetic images. Our analysis reveals that self-supervised contrastive networks using Triplet Margin loss in models like DinoV2, DenseNet121, and ResNet50, when paired with Bray-Curtis or Standardized Euclidean distance metrics, demonstrate superior performance in detecting augmented copies of training images. We further apply our methodology to characterize the memorization behavior of a conditional diffusion image transformer model trained on mammography data. This work contributes a robust framework for evaluating generative models in medical imaging, offering a crucial tool for assessing the risk of patient data leakage in synthetic datasets.1

## 1 Introduction

The advent of generative models has a lot of potential in healthcare and medical imaging initiatives, promising enhanced data sharing, expanded datasets, and improved training data diversity . However, these advancements come with significant privacy implications, especially given the sensitive nature of patient information. A key concern is the phenomenon of model memorization [2; 3], where generative models inadvertently reproduce specific details from their training data, potentially compromising patient confidentiality and undermining the core purpose of synthetic data generation.

Recent research has demonstrated that a wide range of generative models, including GANs, VAEs, and diffusion models, are vulnerable to memorization [4; 5; 6; 7]. Of particular note, diffusion models , despite their impressive image quality, have shown a higher propensity for memorization . This finding underscores the intricate interplay between model sophistication, output quality, and data privacy. Furthermore, conventional evaluation metrics such as Inception Score (IS)  and Frechet Inception Distance (FID)  fall short in detecting these memorization issues, potentially masking critical privacy vulnerabilities in emerging image generation techniques.

A common misconception is that memorization can be effectively addressed by simply monitoring validation errors and preventing overfitting. However, this approach overlooks the fundamental differences between these two phenomena . While overfitting manifests as a global issue wheremodels excel on training data at the expense of generalization, memorization is a more nuanced problem. It involves the model assigning disproportionately high probabilities to specific training instances. Intriguingly, a model's tendency to memorize can actually increase even as its validation performance improves, particularly during the initial stages of training . This paradoxical relationship highlights the need for specialized strategies to identify and mitigate memorization, distinct from traditional overfitting prevention techniques.

Our research builds upon recent advances in self-supervised contrastive learning for memorization detection [5; 12], offering a comprehensive benchmark. We propose a novel approach to evaluate the efficacy and resilience of self-supervised networks through systematic image augmentations. Our study compares the performance of various state-of-the-art pretrained network architectures, including ResNet50  and DinoV2 . We also investigate the influence of different loss functions, including distance-based and entropy-based formulations, and examine the impact of pretraining on natural versus medical image datasets. By comparing a range of similarity, distance, and information-theoretic metrics, we aim to identify the most sensitive indicators for detecting and characterizing training data memorization. To demonstrate the practical application of our findings, we employ the best-performing method to analyze the memorization patterns in a diffusion model.

## 2 Related work

### Model Memorization

The phenomenon of model memorization has been extensively studied in machine learning, particularly in supervised learning contexts. Neural networks have demonstrated the capacity to memorize entire datasets, including those with random labels . This memorization is not uniform across all data points; outliers and mislabeled samples are more likely to be memorized . Memorization and generalization might also depend on network architecture and optimization procedure, but also on the data itself . Moreover, some level of memorization in supervised learning has been shown to be important for generalization in several standard benchmarks . In generative models, memorization presents unique challenges, as models that closely replicate training data may still achieve favorable scores on standard quality and diversity metrics. Recent work has demonstrated that GANs, VAEs, and diffusion models as well as vision language models are all susceptible to memorizing training data [4; 5; 6; 7; 19]. Therefore, creating a memorization metric to be monitored during training would enable a more comprehensive assessment of the generative model performance.

### Memorization Detection Methods

Various approaches have been proposed to detect and quantify memorization in generative image models. Correlation-based methods, such as the structural similarity index measure (SSIM) employed by [20; 21; 22], offer a straightforward approach to assessing similarity between generated and training images. However, these methods were initially developed to measure diversity not memorization behaviour, and may be sub-optimal to detect generated samples which are mere augmented versions of the training data (e.g., rotation or flipping).

More sophisticated approaches leverage self-supervised learning and contrastive methods. In  the authors introduced a framework that uses contrastive learning to map images to a lower-dimensional embedding space, allowing for the detection of copies that may include rotated or flipped variants of training images. This method was further explored in , which investigated the effects of various hyperparameters and training setups on memorization as well as mitigation strategies.

### Mitigation Strategies

Various approaches have been proposed to mitigate memorization in generative models. These include using exclusively augmented images during training , implementing Differentially Private Stochastic Gradient Descent (DP-SGD) , and applying standard regularization techniques like dropout and weight decay. Additionally, novel methods such as Privacy Distillation have been introduced . This two-step approach involves training an initial diffusion model on real data, generating and refining synthetic samples to exclude identifiable information, and then using these refined samples to train a second model. This method aims to reduce re-identification risk while maintaining downstream performance.

However, these mitigation strategies often involve trade-offs. DP-SGD can compromise image quality or lead to model divergence , while data augmentation may complicate similarity assessments between synthetic and original images. The Privacy Distillation approach, while promising, may result in reduced quality of the final synthetic samples. Finally, factors such as over-training, dataset size, and augmentation techniques also significantly influence memorization and should be carefully addressed [5; 6; 12].

## 3 Methods

### Problem Formulation

Let \(=\{x_{1},,x_{N}\}\) represent a set of \(N\) training images, \(_{v}=\{v_{1},,v_{K}\}\) denote a set of \(K\) validation images, and \(=\{g_{1},,g_{M}\}\) be a set of \(M\) generated images. We train a Self-Supervised Contrastive Network (SSCN) to learn an embedding function \(:^{d}\), where \(\) is the image space and \(d\) is the embedding dimension, by minimizing a contrastive loss function \(L(;)\).

Given a similarity metric \(s:^{d}^{d}\), we compute the similarity between training and generated images as \(S(x,g)=s((x),(g))\) for \(x,g\), and baseline similarities between training and validation images as \(S_{base}(x,v)=s((x),(v))\) for \(x,v_{v}\). To prevent memorization of synthetic data, we set a threshold \(\) as the \(p\)-th percentile of the \(S_{base}\) distribution.

For evaluation, we define a set of severely augmented images \(_{a}=\{a_{1},,a_{L}\}\), where each \(a_{i}\) is derived from \(\) using strong augmentations. We monitor the percentage of augmented images that match their corresponding original images in \(\) according to the similarity threshold \(\).

### Self-Supervised Contrastive Network

#### 3.2.1 Architecture

The SSCN comprises a backbone network \(f_{}:^{d}\), followed by a projection head \(g_{}:^{d}^{k}\). The backbone extracts features from the input images, while the projection head maps these features to a lower-dimensional embedding space. The complete network is represented as:

\[h_{,}(x)=g_{}(f_{}(x))\] (1)

We experiment with several backbone architectures, including ResNet50 , DenseNet121 , Inception V3 , CLIP Image Encoder , and DinoV2 . The projection head is a linear layer defined as \(g_{}(z)=Wz+b\), where \(W^{k d}\) and \(b^{k}\).

To explore the impact of domain-specific knowledge, we use backbones pretrained on both natural images (ImageNet ) and medical images (RadImageNet ). This comparison allows us to evaluate the transfer learning benefits of using medical-domain-specific pretraining.

#### 3.2.2 Loss Functions

To structure the embedding space, we employ and compare two popular contrastive losses: the Triplet Margin Loss  and InfoNCE Loss. Both losses aim to pull semantically similar data points closer while pushing dissimilar points farther apart.

Triplet Margin Loss.This loss function ensures that the distance between an anchor-positive pair is smaller than the distance between the anchor-negative pair, with a margin \(m\). Specifically, for an anchor \(a\), a positive example \(p\), and a negative example \(n\), the loss is defined as:

\[L_{}(a,p,n)=(0,m+d(a,p)-d(a,n))\] (2)

where \(d(,)\) is the Euclidean distance and \(m\) is the margin parameter. This encourages positive pairs to be closer together while keeping negatives farther apart in the embedding space.

InfoNCE Loss.InfoNCE (Information Noise-Contrastive Estimation) compares each anchor representation \(z_{i}\) with one positive sample \(z_{j}^{+}\) and \(N-1\) negative samples \(\{z_{j}^{-}\}\), using Cosine similarity between the embeddings. The objective is to maximize the probability that the positive pair is more similar than the negative ones. This probability is expressed as:

\[P(i|j)=,z_{j}^{+})/)}{(s(z_{i},z_{j}^{+})/)+_ {z_{j}^{-}}(s(z_{i},z_{j}^{-})/)}\] (3)

where \(\) is a temperature parameter that controls the smoothness of the distribution, and \(s(z_{i},z_{j})\) is the Cosine similarity between anchor \(z_{i}\) and positive or negative samples.

The InfoNCE loss is computed as the negative log-likelihood of the positive pair:

\[L_{}(z_{i},z_{j}^{+},\{z_{j}^{-}\})=- P(i|j)\] (4)

#### 3.2.3 Training Procedure

The training process is conducted over 100 epochs. For each epoch, mini-batches are sampled from the training set. Each batch undergoes a series of stochastic augmentations, including rotation, scaling, flipping, affine transformations, bias field distortion, gamma correction, noise addition, and blurring. These augmentations enhance the network's ability to learn invariant features and generalize better.

The model computes embeddings for both the original and augmented batches, then calculates the loss (either Triplet or InfoNCE) based on these embeddings. Network parameters are updated using the AdamW optimizer with an initial learning rate of \(10^{-4}\), which is decayed exponentially with a factor of 0.99 after each epoch.

We implemented the model using PyTorch and distributed the training across two NVIDIA RTX 4090 GPUs. A batch size of 128 was used for most experiments, except for CLIP and DinoV2 models, where it was reduced to 64 due to memory constraints. For the InfoNCE loss, we set the temperature \(=0.5\), while for the triplet margin loss, we used a margin \(m=0.05\) with hard negative mining. The backbone was frozen during the first 5 epochs to ensure proper warm-up of the linear layer.

### Embedding Similarity Analysis

To comprehensively evaluate the similarity between the learned embeddings, we employed and compared the following distance and similarity metrics: Bray-Curtis distance, Canberra distance, Chebyshev distance, City Block (Manhattan) distance, Correlation distance, Cosine similarity, Dice similarity coefficient, Euclidean distance, Jensen-Shannon divergence, Mahalanobis distance, Matching distance, Minkowski distance, Standardized Euclidean distance (SEuclidean), and Squared Euclidean distance.

#### 3.3.1 Similarity Distributions

For each trained model, we compute the similarity metrics between the training set and its adversarial (augmented) counterpart, the validation set and its adversarial counterpart, and for baseline similarity level assessment between the training and validation sets. These result in a distribution of the highest similarity score for each image enabling to test whether the contrastive model is capable of detecting augmented image copies and assess quantitatively the memorization degree by comparing with the train-val distribution. When aggregating over networks, losses, pretrainign and/or metrics we report the mean validation (augmented) detection with error bars representing 95 % confidence intervals, and significance test are calculated using two-tailed t-test.

Detection of Augmented CopiesTo evaluate the effectiveness of our similarity metrics in identifying augmented copies, we implement a threshold-based detection method. Let \(_{aug}=\{x_{1}^{},,x_{N}^{}\}\) and \(_{v,aug}=\{v_{1}^{},,v_{K}^{}\}\) represent the augmented versions of the training and validation sets, respectively. Given our similarity metric \(s\) and embedding function \(\), we compute the similarity \(S(x,x^{})=s((x),(x^{}))\) between each original image \(x\) and its augmented version \(x^{}_{aug}\).

We flag \(x^{}\) as a potential copy if \(S(x,x^{})>\) for any \(x\), where \(\) is set as the \(p\)-th percentile of the baseline similarity distribution \(S_{base}(x,v)=s((x),(v))\) for \(x,v_{v}\).

Our benchmark aims to detect all images in \(_{aug}\) and \(_{v,aug}\) as copies of their original counterparts when using a \(\) equal to the 5-th percentile of \(S_{base}(x,v)\). By comparing the detection rates between \(_{aug}\) and \(_{v,aug}\), the model's generalizability and robustness of our similarity metrics in identifying augmented copies can be assessed.

### Dataset

Our study utilized an anonymized X-ray mammography dataset comprising 7,184 scans from 1,718 unique patients. The images were obtained and stored in DICOM format with a median shape of 2800 x 2082 pixels and median spacing of 0.065 x 0.065 mm.

The dataset includes two primary classes of mammography scans: normal scans and scans with calcification. To ensure the integrity of our evaluation, we performed a patient-aware train-validation split, ensuring that scans from the same patient were not distributed across different sets.

For preprocessing, all images were resized to square resolutions. During model training, images were further resized to match the natural input resolution of the backbone networks, typically 224 x 224 pixels. This dataset provides a robust foundation for training and evaluating our self-supervised contrastive network and conditional diffusion model for medical image synthesis.

### Conditional Diffusion Model for Medical Image Synthesis

To enhance our dataset and evaluate the potential of generative models in medical imaging, we trained a class-conditional diffusion model using our medical imaging data. This model was designed to generate high-quality, synthetic medical images while preserving class-specific features.

Training ProcessWe utilized a Diffusion Image Transformer (DiT) architecture , specifically the DiT XL/2 variant (670M), comprising 28 Transformer layers with a hidden size dimension of 1152 and 16 attention heads. The model, was initially pretrained on ImageNet and then fine-tuned on our medical imaging dataset for 100.000 steps with a learning rate of 1e-4, batch size of 2, with horizontal flip as the only augmentation.

Inference and Dataset AugmentationAt inference time, we used the trained model to upsample our original dataset, effectively doubling its size. The resulting images were later processed via the best performing SSCN to showcase the usability of such privacy detector methods and their memorization characterization performance.

## 4 Results

In this study, we evaluated the performance of various deep learning models for a detection task, comparing different network architectures, pretraining datasets (ImageNet and RadImageNet), and loss functions (InfoNCE and Triplet). Our results reveal significant variations in performance across these factors, with some clear trends emerging.

### Network, Pretraining and Loss

The performance of self-supervised networks varied significantly across different architectures, loss functions, and pretraining datasets (Figure 1). Consistently across all network architectures, the Triplet loss outperformed InfoNCE, often by a substantial margin. This superiority of Triplet loss over InfoNCE was found to be statistically significant (\(p<0.05\)) for all tested network architectures and pretraining datasets, with many comparisons showing highly significant differences (\(p<0.001\)).

When comparing the best configurations of different network architectures, several significant differences emerged. DinoV2 with ImageNet pretraining and Triplet loss achieved the highest overall performance (0.722), closely followed by DenseNet121 (0.710) and ResNet50 with RadImageNet pretraining (0.660). The differences between these top-performing models were not statistically significant (\(p>0.05\)), suggesting that they perform comparably well.

However, significant differences were observed between the top-performing models and the Inception architecture. Inception, even in its best configuration (ImageNet, Triplet), performed significantly worse than ResNet50 (\(p=0.030\)), DinoV2 (\(p=0.014\)), and DenseNet121 (\(p=0.015\)). The CLIP model, with its best configuration (RadImageNet, Triplet), showed intermediate performance (0.600) that was not significantly different from the top models but was marginally better than Inception (\(p=0.059\)).

Interestingly, when focusing on the Triplet loss, the choice of pretraining dataset (ImageNet vs. RadImageNet) did not lead to statistically significant differences in performance for most architectures. This lack of significant difference in pretraining datasets for Triplet loss was consistent across all models, including ResNet50 (\(p=0.540\)), CLIP (\(p=0.953\)), Inception (\(p=0.875\)), DinoV2 (\(p=0.323\)), and DenseNet121 (\(p=0.060\)).

These findings indicate that while the choice of network architecture and loss function (Triplet vs. InfoNCE) has a significant impact on performance, the effect of pretraining dataset is more nuanced, particularly when using Triplet loss. The top-performing models (DinoV2, DenseNet121, and ResNet50) show comparable performance, significantly outperforming Inception, with CLIP falling in between. The robustness of Triplet loss to variations in pretraining data suggests it may offer more flexibility in the choice of pretraining dataset for self-supervised learning tasks across different network architectures.

### Impact of Distance Metrics on Triplet Loss Performance

In addition to comparing network architectures and pretraining datasets, we also evaluated the performance of various distance metrics when using the Triplet loss function. The results, as illustrated in Figure 2, reveal substantial variations in performance across metrics, with the mean validation detection ratios and their respective confidence intervals showing clear differences.

The Bray-Curtis distance metric demonstrated the highest mean validation detection ratio of 0.8094 (\(\)0.1036 CI), positioning it as the best performer. It was closely followed by the Jensen-Shannon divergence (0.7882 \(\)0.1107 CI) and a group of Euclidean-based metrics, including Euclidean, Minkowski, and Squared Euclidean, which all achieved 0.7871 (\(\)0.1235 CI). These metrics consistently performed well across various configurations, highlighting their robustness when applied with models trained on Triplet Margin loss.

A slightly lower performance was observed with metrics such as the City Block (Manhattan) distance (0.7813 \(\)0.1269 CI), the Canberra distance (0.7810 \(\)0.1101 CI), and the Standardized Euclidean distance (0.7708 \(\)0.1349 CI). Although these metrics exhibited detection ratios slightly below the top group, they still maintained strong performance, with detection ratios above 0.77. These

Figure 1: Comparison of network architectures performance with their best configurations.

results indicate that they are viable alternatives, particularly in situations where domain-specific considerations or computational efficiency play a role in metric selection.

On the other hand, the Chebyshev distance (0.7599 \(\)0.1213 CI) and the Mahalanobis distance (0.6099 \(\)0.1257 CI) displayed notably lower performance. The lower mean detection ratios for these metrics suggest that they may not be as effective in this task when paired with the Triplet loss function. Furthermore, the correlation-based metrics, including Correlation, Cosine, Dice, and Matching, performed significantly worse, with detection ratios falling below 0.06. Notably, the Matching distance exhibited extremely poor performance (0.0073 \(\)0.0025 CI), suggesting that correlation-based metrics are ill-suited for this particular detection task when using Triplet loss.

The statistical analysis of pairwise comparisons further reinforced these findings. The differences between the top-performing metrics--Bray-Curtis, Jensen-Shannon, and Euclidean-based--were not statistically significant (\(p>0.05\)), indicating that their performances are comparable. However, these top-performing metrics were significantly superior to the lower-performing and poor-performing metrics, with highly significant differences observed when compared to Mahalanobis and correlation-based metrics (\(p<0.001\)).

### Best Combinations for Each Network Architecture

We present the best-performing combinations of network architecture, pretraining dataset, loss function and metric. Table 1 highlights the maximum validation detection achieved and the distance metric that produced this maximum value for each network configuration.

  
**Model** & **Pretraining** & **Loss** & **Val. Detection** & **Metric** \\  DinoV2 & ImageNet & Triplet & 0.9971 & Bray-Curtis \\ DenseNet121 & ImageNet & Triplet & 0.9842 & SEuclidean \\ ResNet50 & RadImageNet & Triplet & 0.9568 & Bray-Curtis \\ ResNet50 & ImageNet & Triplet & 0.8863 & Bray-Curtis \\ CLIP & RadImageNet & Triplet & 0.8806 & City Block \\ CLIP & ImageNet & Triplet & 0.8791 & Euclidean \\ DinoV2 & RadImageNet & Triplet & 0.8604 & Euclidean \\ DenseNet121 & RadImageNet & Triplet & 0.7281 & Bray-Curtis \\ Inception & ImageNet & Triplet & 0.5813 & Canberra \\ ResNet50 & RadImageNet & InfoNCE & 0.5496 & Euclidean \\   

Table 1: Best combinations for each network architecture

Figure 2: Comparison of distance metrics performance with Triplet loss in terms of mean validation detection ratio. Error bars represent confidence intervals.

As shown in Table 1, the DinoV2 model pre-trained on ImageNet using the Triplet loss achieved the highest validation detection score (0.9971), with the Bray-Curtis distance metric. Similar trends are observed across other architectures, with DenseNet121 and ResNet50 also performing well with SEuclidean and Bray-Curtis metrics, respectively.

### Memorization Characterization of Diffusion Models

Using the best-performing combinations identified for our dataset, the fine-tuned DinoV2 model was employed to analyze the memorization behavior of a DiT trained to generate synthetic mammography images (Figure 3). The augmented images are easily distinguishable from the training data, while the generated samples exhibit a slight shift towards the left of the training distribution. This shift suggests a degree of memorization, as the synthetic samples appear to be closer to the training data than the training data is to the validation images.

## 5 Discussion

Our study presents a comprehensive, data-driven approach to evaluating and characterizing the memorization behavior of generative models in medical imaging. By systematically comparing various network architectures, loss functions, pretraining datasets, and distance metrics, we have identified optimal configurations for detecting potential privacy concerns in synthetic images. The results demonstrate that the developed method can identify all augmented images when using Triplet Margin loss with models like DinoV2, DenseNet121, and ResNet50, particularly when paired with the Bray-Curtis or Standardized Euclidean distance metrics. The ability to quantify the degree of memorization in generated images offers a method to assess the risk of patient data leakage in synthetic datasets. This approach can be integrated into the training pipeline of generative models, serving as an early warning system for memorization and potential privacy breaches.

LimitationsAs for limitations, our study is based on a private mammography dataset from various institutions. Although this dataset is substantial and diverse, the generalizability of our findings to other medical imaging modalities or natural image datasets remains to be validated. Future work should address these limitations by generating a foudnational model that serves for both 2D and 3D data, multi-institutional and multi-modality datasets to avoid having to fine-tune the model for each dataset. A comparative analysis of various generative model architectures and stronger conditioning forms (text or segmentation) would provide a more comprehensive understanding of memorization behavior across generative models.

Figure 3: Memorization characterization by the two best-performing self-supervised contrastive networks, DinoV2 (left) and DenseNet121 (right), for generated samples by a DiT model.