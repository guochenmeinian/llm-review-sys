# VisMin: Visual Minimal-Change Understanding

Rabiul Aval\({}^{*}\) Saba Ahmadi\({}^{*}\) Le Zhang\({}^{*}\) Aishwarya Agrawal

Mila - Quebec AI Institute

Universite de Montreal

{rabiul.aval,le.zhang,aishwarya.agrawal}@mila.quebec

denotes equal contribution

###### Abstract

Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). To evaluate VLMs' fine-grained understanding, existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar _captions_ given an image. In this paper, our focus is on evaluating VLMs' capability to distinguish between two very similar _images_ give a caption. To this end, we introduce a new, challenging benchmark termed **Vis**ual **M**inimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. Importantly, the image pair (as well as the caption pair) contains minimal-changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: _object_, _attribute_, _count_, and _spatial relation_. These four types of minimal-changes are specifically designed to test the models' understanding of objects, attributes of objects (such as color, material, shape), counts of objects and spatial relationship between objects. To curate our benchmark, we built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. Furthermore, leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model). Our findings show that both these models benefit significantly from fine-tuning on this data, as evident by marked improvements in fine-grained understanding across a wide range of benchmarks. Additionally, such fine-tuning improves CLIP's general image-text alignment capabilities too. We release all resources including the benchmark, the training data and the finetuned model checkpoints at https://vismin.net/.

## 1 Introduction

Fine-grained understanding of objects, attributes, and their relationships is critical for Visual-Language Models (VLMs) to generalize effectively to new, unseen scenes and compositions. Previous studies such as ARO  and Sugarcrepe , highlighting the deficiencies of VLMs in this domain predominantly focus on understanding fine-grained differences between two very similar _captions_ - a human-written caption and an automatically generated hard-negative2 caption, where the hard-negative caption differs from the original caption only with respect to an _object_, or an _attribute_ or a _relationship_ between two objects. While such hard-negative examples for _captions_ can be synthesized using rule-based approaches, synthesizing such hard-negative examples for images is verychallenging. Existing benchmarks presenting _visual_ hard-negatives suffer from two main limitations: 1) **Limited Difficulty:** In benchmarks such as Winoground , MMVP, the original images and their hard-negative counterparts differ in multiple aspects (objects, attributes of objects, image background, etc.). This multiplicity limits the difficulty of the benchmark and makes it challenging to precisely evaluate the models' fine-grained understanding of specific aspects. 2) **Limited Complexity:** Although benchmarks such as EQBEN , SPEC  have controlled hard-negatives, the visual domain is limited to graphic engines, a few video domains or reliance on purely synthetic images depicting simplistic scenes.

Motivated by these observations, we propose a new benchmark, **Vis**ual **Min**imal-Change Understanding (VisMin ), built on top of the images from the COCO  dataset that consists of complex everyday scene images. VisMin is designed to measure VLMs' ability to comprehend minimal changes, i.e., changes with respect to only one aspect (see Fig. 1), from among the following aspects: _object_, _attribute_, _count_, and _spatial relation_, while keeping other aspects unchanged as much as possible.

The evaluation task for a model is to predict the correct image-caption match given: 1) two images and one caption, 2) two captions and one image. To curate VisMin, we built an automated pipeline using large language models and diffusion models. To ensure the quality of our benchmark, the synthetic data generated using the automated pipeline undergoes a rigorous 4-step verification process by human annotators, with data retained in the benchmark only if it passes all four steps. We meticulously designed the benchmark ensuring uniformity across various categories to the extent possible. We conduct a detailed analysis of our benchmark, which enables a more transparent assessment of the various strengths and weaknesses of the models.

We conducted empirical tests on eight open-source VLMs, including foundational models like CLIP  and Multimodal Large Language Models (MLLMs) such as Llava and Idefics2. We also evaluated two closed-source APIs, GPT-4 and Gemini. Our findings suggest that both foundational models and MLLMs perform relatively well in understanding minimal changes in objects and attributes. Surprisingly, MLLMs underperform foundational VLMs in object and attribute understanding! For spatial relation understanding, although MLLMs perform better than VLMs, both families of models perform below random chance! Similarly, both families of models show considerable room for improvement in counting capabilities. Our results underscore the need for a emphasis on spatial reasoning and counting understanding over attribute/object recognition in VLM evaluations. We anticipate that our benchmark will catalyze advancements in these critical areas within the community.

Figure 1: Overview of our VisMin benchmark. VisMin consists of four types of minimal-changes – object, attribute, count and spatial relation – between two image-captions pairs. The evaluation task requires a model to predict the correct image-caption match given: 1) two images and one caption, 2) two captions and one image.

Lastly, owing to the automated nature of our synthetic data creation process, we generated a large-scale (64,392 samples) minimal-change image-text data for fine-tuning the VLMs to enhance their fine-grained understanding. Fine-tuning CLIP (a foundational VLM) and Idefics2 (a MLLM) on our minimal-change data, without any additional modifications to the model architecture or loss functions, results in significant improvements in fine-grained understanding across various benchmarks. Notably, such fine-tuning also enhances foundational VLMs' general image-text alignment capabilities as evident by marked improvements in CLIP's image-text retrieval performance on COCO. These observations suggest that our minimal-change dataset can serve as model-agnostic, general-purpose resource to enhance the capabilities of VLMs.

To summarize, our contributions are threefold: 1) **A controlled and challenging benchmark**. We introduce the VisMin benchmark, which challenges models to detect semantic differences between visually similar but semantically different images. Extensive testing on foundational VLMs and MLLMs reveals their difficulties with this task, highlighting areas for improvement. 2) **A pipeline for automated data creation and benchmark development**. We create an automated pipeline to generate visual minimal-change data at scale using large language models and diffusion models, with a rigorous four-step human verification system to ensure high data quality. 3) **Enhancement of VLMs' fine-grained understanding with fine-tuning on minimal-change data**. We improve the fine-grained understanding of CLIP and Idefics2 by fine-tuning them on our large-scale minimal-change image-text data, demonstrating improved image-text alignment and overall performance.

## 2 Related work

**Fine-grained understanding benchmarks:** Most existing benchmarks focus on understanding fine-grained textual differences, such as VL-checklist , ARO , and Sugarrepe . Benchmarks presenting visual hard-negatives, such as EQBEN , Winoground , ImageCode , SPEC , either lack minimal changes or have limited visual complexity - graphic engines, a few video domains or purely synthetic images depicting simplistic scenes. Our benchmark addresses these gaps by utilizing the advances in LLMs  and diffusion models [30; 20; 21] to achieve minimal changes in complex COCO-like scenes without compromising the naturalness of the images, thus providing a more robust evaluation of fine-grained visual understanding in VLMs. Detailed comparisons of benchmarks are provided in section 4.

**Automatic approach to generate visual hard negatives:** Existing approaches to automatically generate visual hard negatives fall into three broad categories: (i) using nearby video frames with semantic changes [14; 38], (ii) using graphic engines , (iii) using diffusion models [28; 38; 17]. Our proposed framework falls in the third category. DEMON is the closest to our work, creating training data using diffusion models to improve the learning of a given vision-language model. They use diffusion models to perform local editing on the images given the target object mask. However, this approach requires attention masks from the vision-language model being studied. SPEC  proposes a diffusion-based canvas-filling method for generating minimally-different image pairs limited to four types of minimal changes: size, position, count and existence. Compared to these existing methods, our automated pipeline to generate minimal-change data is more involved in order to achieve minimal-changes in complex scenes while maintaining the photo-realism of the scene and controlling changes across diverse categories. Our pipeline also has more a comprehensive automated filtering mechanism compared to previous pipelines that mainly rely on CLIP-based filtering.

**Enhancing fine-grained understanding in VLMs with hard negatives:** Most methods to enhance fine-grained understanding in VLMs like CLIP focus on fine-tuning with caption-based hard negatives and optimizing loss functions to better use these signals [42; 46; 33]. Common strategies for generating textual hard negatives include heuristic rules , language models [46; 5], scene-graph information [7; 33], and LLMs integrated with semantic segmentation . In contrast, fewer works explore visual hard negatives; methods like NegCLIP  and General Scene Difference  rely on nearest-neighbor images, which often differ too much or too little in context, limiting fine-grained learning. Our approach is closest to SPEC  and CounterCurate , which also fine-tune VLMs using minimal-change visual hard negatives. Unlike SPEC, we and CounterCurate extend this to multimodal large language models, but our work evaluates performance on 10 out-of-distribution benchmarks (compared to 1 or 2 in SPEC and CounterCurate) and outperforms baseline models in most cases, demonstrating the strength of our approach (see Tables 3 and 4).

Minimal-Change Image-Text Dataset Creation

We devised a framework to synthesize large-scale minimal-change data and introduce the VisMin benchmark (see overview fig. 2). The pipeline includes three stages: **Minimal-Change Pairs Synthesis**, where we minimally edit image and text pairs; **Automatic Filtering**, which verifies the faithfulness of the texts and synthesized images; and **Human Verification**, a four-step process to ensure that only data meeting all quality criteria is included. We will discuss each stage in detail.

**LLM-guided Edit Instructions Generation** To generate minimal-change text pairs, we start with source captions and then prompt an LLM (Mistral 47B ) to generate both the edit instructions specific to each edit category and the corresponding edited caption (see Appendix A.2.1 for the prompt used). For **Object and Attribute** edits, we use human-written captions from COCO  and VSR  datasets as our source captions. The LLM processes these captions to suggest edits targeting specific objects or attributes. For example, given the _source caption_ "A dog in the middle of the couch", the LLM generates the _edit instruction_ "change dog to doll" which contains both the _source phrase_ ("dog") and the _edited phrase_ ("doll"). The LLM also generates the _edited caption_ "A doll in the middle of the couch". We generate five plausible (based on the criteria outlined for LLM prompting) edit instructions and edited captions per source caption. To ensure the edited captions are minimally changed w.r.t the source caption and contain visually plausible changes, we prompt the LLM again for filtering, removing \(40\%\) of the total LLM outputs that do not meet those criteria (see appendix A.2.2 for details on the criteria). For **Counting and Spatial Relation** edits, we generate the source captions synthetically due to the absence of a suitable human-written captions dataset containing descriptions of counts and spatial relations of objects. We prompt the LLM to create captions and outline object layouts and bounding boxes. For instance, the LLM might generate a _caption_ like "A plate of cookies on a table and a cup of coffee to its right," with the corresponding _bounding boxes_: {"plate of cookies": \(\); "cup of coffee": \(\)}. The LLM generates a large pool of such synthetic captions. The edit instructions and the corresponding edited captions are generated using a rule-based method aimed at swapping the object positions for

Figure 2: Our dataset creation pipeline includes three stages: (i) **Minimal-Change Pairs Synthesis**: We develop methods for synthesizing minimal-change image-caption pairs involving Objects & Attributes and Counting & Spatial Relations. (ii) **Automatic Filtering**: An LLM generates questions and answers based on captions, and a VQA model predicts answers from images. Synthetically generated minimal-change data are excluded if answers don’t match. (iii) **Human Verification**: Synthetically generated minimal-change data undergoes a rigorous 4-steps human verification, and only examples passing all stages are included in the benchmark.

spatial relation edits (e.g. _edited caption_: "A cup of coffee on a table and a plate of cookies to its right", _swapped bounding boxes_: {"1st crop"; \(\); "2nd crop": \(\)) or adjusting the object counts for counting edits (e.g. _edited caption_: "A cup of coffee on a table"), _removed bounding boxes_: \(\{\}\), in this example removing the plate of cookies from the image.

**Diffusion-guided Image Synthesis** We modify images according to the edit instructions generated by the LLM in the previous step. For **Object and Attribute** edits, we first mask the object to be edited in the source image using the Grounding-DINO model . We obtain the source images from the COCO dataset. The object to be edited is specified in the source phrase of the edit instruction (e.g., "a dog" in the edit instruction "change dog to doll"). We then apply the SDXL inpainting model , using the _input image, masked region, and edited phrase_ (obtained from the edit instruction, e.g., "a doll" in the edit instruction "change dog to doll") to alter the masked image region to match the desired outcome, e.g., changing "a dog" to "a doll." For **Counting and Spatial Relation** edits, we create a synthetically generated source image dataset based on LLM-suggested layouts from the previous step, using the LLM-grounded Diffusion (LMD) model  for image synthesis. To create an edited image, for the spatial relation edits, we first reposition the source image's bounding boxes using a rule-based method. We then obtain image crops from the source image corresponding to the objects which we need to reposition w.r.t each other. Lastly, we use the GLIGEN layout-diffusion model  to smoothly insert the obtained crops into the source image at the repositioned bounding box locations. For counting edits, we obtain the edited image by always removing one or multiple objects from the source image. The object to be removed is specified by masking and we use the Lama model  to carry out the object removal. We employ layout-based diffusion models [21; 20] instead of using end-to-end diffusion models like Stable diffusion  as the layout-based model facilitates precise control over the object positions and counts and thus ensures the changes are faithful to the edit instruction as well as minimal. Unfortunately, end-to-end models such as Stable Diffusion are not good at precisely editing object positions and counts.

**Stage 2: Automatic Filtering** To ensure consistency of synthesized hard-negative images, we use a VQA-based filtering system, which is more effective than object detection (see Stage 2 in Figure 2). Questions generated by an LLM  based on the edit instruction and caption (following TIFA ) verify that edits align with the caption and that the positive caption no longer applies to the negative image. We use LLaVa 7B  to answer these questions, with region-specific questions for object/attribute edits and global questions for background consistency. This process removes \(75\)% of synthesized images, ensuring dataset quality.

**Stage 3: Human Verification** To ensure high-quality of the benchmark, on top of automated filtering, we conduct human verification using the Amazon Mechanical Turk platform. The images and captions undergo four steps of verification, requiring agreement from at least four out of five annotators at each step to pass the human verification. The steps are: **1) Naturalness and Image-Text Matching Verification**: Annotators assess if (a) the image looks natural, (b) the caption is sensical, and (c) the image matches the caption. Only 26% of synthetic images pass this step, mainly due to the criterion (a), where counting and spatial relation images often look unnatural. See Appendix Table 7 for detailed acceptance rates. **2) Visual Edit Verification**: Annotators assess if the images faithfully reflect the specified minimal edits without additional changes, with an acceptance rate of 80%. **3) Edit Instruction Verification**: Annotators assess if the LLM-generated edit instructions are minimal, i.e., the suggested edit modifies only one aspect of the sentence (out of object, attribute, counting, or spatial relation), with an 84% acceptance rate. **4) Textual Edit Verification**: Annotators assess if the edited sentence faithfully reflects the specified minimal edits without additional changes, with a 95% acceptance rate. Annotators also verify the LLM's categorization of the types of edits (object, attribute, counting, or spatial relation). These steps ensure precise, minimal changes in images and captions, delivering a high-quality benchmark for fine-grained visual understanding. See Appendix A.4.1 for annotator instructions.

## 4 Training and Benchmark sets

In our study, we create training and benchmark sets to improve and assess fine-grained understanding in VLMs. The training data is generated through a scalable pipeline with automatic filtering, while the benchmark data undergoes additional rigorous human verification to ensure high quality (as explained above). For object and attribute edit types, that make use of natural images, the training data is sourced from VSR (images sourced from COCO) and the COCO 2017 training split (118K images), while the benchmark data is sourced from the COCO 2017 validation split (5K images). This ensures benchmark images are unseen during training, maintaining evaluation reliability by community standards. The Training dataset has 64,392 samples (37,017 objects, 10,352 attributes, 10,050 counting, 6,973 relations), while the VisMin benchmark has 2,084 samples (579 objects, 294 attributes, 589 counting, 622 relations). We aimed for a balanced benchmark across categories. However, the number of attribute samples in VisMin is relatively low because the LLM suggested attribute edits for only 2000 samples in the COCO 5K validation set. Moreover, most of these suggested edits were color edits. So we further downsampled the color edit instances to balance the distribution of the types of attribute edits. Figure 3 shows subcategories of the changes in VisMin. For detailed training set subcategories, see Appendix15. For qualitative samples, refer to Appendix 13 and 14.

In table 1, we compare VisMin with related benchmarks. **Visual Minimal HN**: This criterion evaluates if the visual hard negatives contain minimal changes. In Winoground and MMVP, hard negatives differ across multiple aspects (object, attribute, background, etc.). In contrast, VisMin's hard negatives vary only in one aspect, keeping others unchanged as much as possible. This minimal-change property is also present in What'sUp, EQBEN, SPEC, and in a subset of images from ImageCoDe and CounterCurate. **Visual Complexity**: This criterion assesses the complexity of visual scenes. ImageCoDe and EQBEN mainly feature images from limited video domains and graphic engines, while What'sUp uses simple household and tabletop images. SPEC generates simplistic scenes using diffusion models. In contrast, Winoground uses expert-curated Getty Images, and MMVP uses ImageNet and LAIONAesthetics. VisMin and CounterCurate (concurrent work) stand out by using diverse, complex everyday scenes from COCO  and Flicker30K Entities , featuring common objects in natural contexts.

**Textual Complexity**: Benchmarks like ImageCoDe, Winoground, and MMVP use free-form human-written captions. In contrast, What'sUp (focused on spatial changes) and SPEC (focused on controlled changes) use ICD  and , respectively. The results show that the proposed methods are capable of producing a better performance on the COCO 2017 dataset. The results show that the proposed methods are capable of producing a better performance on the COCO 2017 dataset.

**Human Verification**: For benchmarks using synthetic images like EQBEN, SPEC, CounterCurate, and VisMin, human evaluation is essential to ensure images look natural. It's also crucial for benchmarks with automatically generated hard negative captions, as these may be nonsensical unless well-defined templates like What'sUp are used. Nonsensical captions make it easier for VLMs to identify them as incorrect . VisMin is notably the only benchmark with full human verification, ensuring both captions and images are high quality. CounterCurate also conducts human verification but only checks for image-caption consistency (on 300 examples), without verifying image naturalness or caption sensibility. **Size**: This criterion assesses the dataset's size. VisMin excels by combining **controlled minimal changes with complex, natural scenes and captions**, providing an optimal balance for robust evaluation.

## 5 Benchmarking VLMs on VisMin Benchmark

SetupWe have comprehensively benchmarked existing _state-of-the-art_ VLMs on VisMin, encompassing both foundational VLMs-such as CLIP , SigLip , BLIP , and Coca  and generative MLLMs including Llava , Idefics2  and InternVL1.5 . Additionally, closed-source MLLMs such as GPT4-o  and Gemini1.0 Pro  are also evaluated.

  
**TaskNetNet** & \(\) & \(\) & \(\) & \(\) & \(\) \\    **ImageCoDe** (**11**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**21**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**30**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**11**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**12**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**13**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**14**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**15**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**16**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   **ImageCoDe** (**17**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  
 **ImageCoDe** (**18**) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison of benchmarks offering visual hard negatives (HN) across: \(\) minimal HN, \(\) visual complexity, \(\) textual complexity, \(\)human-approved captions (\(\)) and images (\(\)), and \(\) size. \(\): criterion holds for a subset of the benchmark.

Figure 3: VisMin categories and subcategories.

For foundational models like CLIP, we conducted an image-text matching task using cosine similarity, following . The tasks involved two settings: choosing the correct image from two captions and selecting the correct caption from two images. In VisMin examples (see Fig. 1) with pairs \(\{(I_{1},C_{1}),(I_{2},C_{2})\}\), the **text score** is 1 if \((s(C_{0},I_{0})>s(C_{1},I_{0}))(s(C_{1},I_{1})>s(C_{0},I_{1}))\), and the **image score** is 1 if \((s(C_{0},I_{0})>s(C_{0},I_{1}))(s(C_{1},I_{1})>s(C_{1},I_{0}))\); the **group score** is 1 when both scores are 1. For MLLMs, we adapted these tasks to a visual question answering format with binary questions about the matching relationship between images and captions \(\{(I_{1},C_{1}),(I_{2},C_{2})\}\). To calculate the **text score**, we presented the model with one image and two captions, using the prompt _"Does this image depict: (\(C_{1}\) or \(C_{2}\))?"_.3 To calculate the **image score**, we presented the model with two images and one caption, using the prompt _"Which image better aligns with the description: '{C}'? The first or the second image?"_. The score is 1 if the predicted answer matches the ground truth. Once both scores are obtained, the **group score** is 1 if both individual scores are 1.

**Results** Insights from Table 2 highlight key capabilities and limitations of current models. Text scores generally surpass image scores, especially in MLLMs, where text scores are often two to three times higher. In contrast, foundational VLMs show a modest discrepancy between image and text scores. We hypothesize that for MLLMs, the image score is lower compared to the text score because they lack training with multiple images, and simple vertical concatenation does not provide sufficient visual signals, leading to suboptimal alignment with captions. Notably, Idefics2, which supports multi-image processing, performs similarly on text and image scores, underscoring the importance of multi-image data during pretraining. Foundational VLMs' higher text scores suggest that distinguishing between captions is easier than between images, highlighting the need for our visual minimal change benchmark. Interestingly, foundational VLMs generally outperform MLLMs due to the latter's lower image scores.

All models perform well on Object and Attribute splits, indicating that understanding semantic changes correlates strongly with recognition capabilities. Models excelling in image classification tend to perform better, reflecting a foundational understanding that does not require advanced reasoning. For instance, Idefics2, using the SigLip (ViT-L/16) vision encoder, performs worse with strong LLMs compared to its foundational VLM counterpart, likely due to limited multi-image understanding in MLLMs. Notably, CogVLM shows superior performance over other MLLMs in understanding object and attribute changes. This advantage likely stems from its integration of grounding tasks and high-quality human-annotated datasets like Flickr30K Entities , RefCOCO , and VisualGenome  in its pretraining. On the other hand, the Spatial Relation split relies heavily on reasoning capabilities, with MLLMs outperforming foundational models. This suggests that LLMs can parse object relationships through reasoning. However, existing VLMs struggle with spatial relations, often scoring below random chance, indicating potential biases in models and highlighting an area for further research.

    &  &  &  &  &  \\   & T & I & G & T & I & G & T & I & G & T & I & G & \\  Random Chance & 25 & 25 & 16.67 & 25 & 25 & 16.67 & 25 & 25 & 16.67 & 25 & 25 & 16.67 & 22.22 \\ MTurk Human & 86.87 & 95.50 & 83.07 & 82.31 & 91.15 & 76.87 & 81.67 & 92.76 & 76.20 & 88.96 & 96.77 & 86.41 & 86.54 \\  CLIP (ViT-B/32)  & 79.62 & 77.89 & 67.53 & 72.11 & 65.99 & 55.1 & 8.2 & 43.44 & 0.48 & 31.24 & 20.71 & 10.53 & 41.15 \\ CLIP (ViT-B/16) & 86.53 & 79.1 & 71.68 & 70.75 & 65.31 & 52.38 & 8.84 & 3.22 & 0.8 & 34.3 & 22.58 & 13.58 & 42.42 \\ CLIP (ViT-L/14) & 87.6 & 83.59 & 78.07 & 74.49 & 69.73 & 57.82 & 9.16 & 4.66 & 1.45 & 37.01 & 30.56 & 18.17 & 46.42 \\ NegCLIP  & 87.4 & 87.70 & 80.68 & 81.63 & 80.27 & 71.77 & 10.13 & 4.66 & 1.13 & 55.01 & 57.72 & 42.28 & 48.96 \\ SigLip (ViT-B/16) & 90.5 & 88.95 & 83.25 & 86.05 & 79.25 & 73.13 & 11.58 & 6.43 & 1.77 & 60.95 & 47.03 & 38.37 & 55.61 \\ SigLip (ViT-L/16) & 93.44 & 88.43 & 84.46 & 84.35 & 78.23 & 68.37 & 10.29 & 4.82 & 1.29 & 61.8 & 57.05 & **44.14** & **56.39** \\ Flava  & 81.69 & 75.12 & 66.66 & 74.49 & 60.54 & 52.01 & 6.75 & 5.79 & 0.96 & 35.99 & 27.5 & 15.45 & 41.91 \\ BLIP  & 92.4 & 92.57 & **87.05** & 88.44 & 86.73 & **78.57** & 11.25 & 4.98 & **2.09** & 52.97 & 46.01 & 33.28 & 56.36 \\ Coca  & 84.97 & 81.52 & 73.58 & 78.57 & 66.63 & 57.82 & 11.25 & 5.95 & 1.77 & 60.1 & 35.82 & 28.52 & 48.85 \\ 
1LaVal1 6.6 (7B)  & 93.0 & 32.8 & 32.2 & 92.2 & 34.4 & 33.3 & 91.8 & 7.8 & 7.4 & 73.6 & 25.0 & 20.2 & 38.28 \\ IdeiCE (8B)  & 95.4 & 69.4 & 67.6 & 89.1 & 71.4 & 67.0 & 18.6 & 18.8 & 4.8 & 72.2 & 50.6 & **47.0** & **55.99** \\ CopyLM (7B)  & 94.64 & 89.63 & **87.56** & 89.11 & 88.83 & **81.63** & 21.70 & 11.09 & 2.25 & 58.40 & 6.45 & 3.56 & 52.87 \\ InterVL1.5 (25B)  & 94.65 & 40.24 & 39.72 & 91.16 & 42.86 & 41.16 & 74.28 & 14.79 & **11.74** & 73.51 & 31.58 & 27.5 & 48.60 \\  Gemini1.0 Pro \(\) & 94.99 & 79.97 & 78.76 & 91.84 & 74.83 & 72.45 & 52.57 & 15.43 & 9.81 & 67.74 & 44.14 & 37.52 & 49.63 \\ GPT4+o \(\) & 95.51 & 96.2 & **93.44** & 92.18 & 90.48 & **87.07** & 89.07 & 50.48 & **46.78** & 77.42 & 78.27 & **68.42** & **73.93** \\   

Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns ‘I, ‘T,’ and ‘G’ denote Image, Text, and Group scores from Winoground . AVG denotes the average across columns. The best results are highlighted in **bold**.

We document human baseline performance on our benchmark via Amazon Mechanical Turk (see Appendix A.4.2). Humans generally outperform models on image scores, except in the attribute category where GPT4-o excels. Models typically surpass humans in text scores, especially with attributes and objects. However, in spatial relations and counting, humans significantly outperform models in group scores, highlighting areas for model improvement and the robustness of human scene comprehension.

## 6 Enhancing fine-grained understanding in VLMs

We use a synthetic minimal-change dataset to enhance fine-grained understanding through additional fine-tuning of VLMs. Training with pairs of images and captions with minimal differences provides a richer training signal, improving model performance in fine-grained understanding tasks. We demonstrate improvements on top of both foundational VLMs and MLLMs by conducting extensive evaluations across various benchmarks: (1) **Single image benchmarks** test the alignment between single images and multiple captions: VSR , CountBench , VALSE , SPEC , and Sugarcrepe . (2) **Multiple image benchmarks** test the alignment between multiple images and captions: ImageCode , MMVP , Whatsup , Winoground , EQBEN , and our VisMin benchmark.

### Fine-tuning Foundational VLMs

Enhancement with Minimal-Change DataOur approach uses a synthetic minimal-change dataset to improve visual representation without altering the training methodology. We construct training batches with both source and edited image-text pairs: In the original CLIP training, a mini-batch is \(=\{(C_{1},I_{1}),(C_{2},I_{2}),,(C_{n},I_{n})\}\), with pairs randomly sampled from the dataset as random negatives. With minimal-change data, we add edited image-text pairs as hard negatives, resulting in \(=\{(C_{1},I_{1}),(C_{1}^{},I_{1}^{}),(C_{2},I_{2}),(C_{2 }^{},I_{2}^{}),\}\), where \((C_{n}^{},I_{n}^{})\) is the edited pair of \((C_{n},I_{n})\). We use a total batch size of 128 with 4 A100 GPUs and retain other training protocols and hyperparameters as default from OpenCLIP , including a learning rate of 1e-05, weight decay of 0.2, Adam \(_{1}\) of 0.9, \(_{2}\) of 0.98, an eps of 1e-06, and a cosine scheduler. The training runs for \(5\) epochs, and we select checkpoints based on a separate VisMin validation set.

We fine-tuned pre-trained CLIP on our minimal-change data, calling it VisMin-CLIP. For comparison, we implemented three models using the same pre-trained CLIP: NegCLIP , CounterCurate-CLIP , and SPEC-CLIP . In NegCLIP, we fine-tuned CLIP with automatically generated hard-negative captions and nearest-neighbor images paired with human-written captions. For CounterCurate-CLIP, we used hard-negative data (attribute, position, counting) but trained one model on all three types, unlike the original approach of training separate models. SPEC-CLIP was fine-tuned on six combined category-specific splits (size, spatial, existence, count). Hard-negatives were included in all batch constructions, with excess negatives rolled into the next batch if needed. All models used ViT-L/14 as the backbone and the original CLIP loss, initialized from OpenAI checkpoints. Best checkpoints were selected from validation sets for NegCLIP and CounterCurate-CLIP, and from average benchmark performance for SPEC-CLIP. This controlled comparison evaluates the impact of different hard-negative data approaches on improving CLIP's fine-grained understanding.

**Results** We evaluated these models on the VisMin benchmark (results in Table 3). Fine-tuning with minimal-change data significantly improves CLIP's performance on the Object, Attribute, and Count categories, demonstrating the usefulness of our minimal-change data in enh

    &  &  &  &  &  \\    & T & I & G & T & &  & G & T & I & G & T & I & G \\  CLIP(ViT-L/14) & 87.56 & 83.59 & 78.07 & 74.49 & 69.73 & 57.82 & 9.16 & 4.66 & 1.45 & 37.01 & 30.56 & 18.17 & 46.02 \\ NegCLIP\({}^{}\) & 87.74 & 87.05 & 80.66 & 81.63 & 80.27 & 71.77 & 10.13 & 4.66 & 1.13 & 55.01 & 57.72 & 42.28 & 55.00 \\ CounterCurate-CLIP\({}^{}\) & 89.81 & 91.02 & 84.46 & 82.99 & 80.27 & 72.79 & 20.1 & 11.41 & **7.4** & 49.24 & 45.16 & 31.92 & 55.54 \\ SPEC-CLIP\({}^{}\) & 86.53 & 86.01 & 78.58 & 78.57 & 71.77 & 63.95 & 9.16 & 5.31 & 1.13 & 45.5 & 47.71 & 32.43 & 50.55 \\ VisMin-CLIP & 91.54 & 91.19 & **86.36** & 85.03 & 83.67 & **75.85** & 11.9 & 3.38 & 1.29 & 82.34 & 79.97 & **72.33** & **63.74** \\  Idefics2 & 95.4 & 69.4 & 67.6 & 89.1 & 71.4 & 67.0 & 18.6 & 18.8 & 4.8 & 72.2 & 50.6 & 47.0 & 55.99 \\ VisMin-Idefics2 & 96.5 & 95.7 & **93.3** & 91.2 & 91.8 & **86.7** & 83.0 & 76.6 & **69.3** & 85.4 & 87.8 & **80.5** & **86.43** \\   

Table 3: Performance of fine-tuned CLIP and Idefics2 across categories on the VisMin Dataset. The \({}^{}\) symbol indicates the reproduced model checkpoints based on their respective training data.

understanding of foundational VLMs such as CLIP. VisMin-CLIP consistently outperforms NegCLIP, CounterCurate-CLIP and SPEC-CLIP across all categories except spatial relations. This suggests that the visual minimal-change data is more helpful in improving the fine-grained understanding capabilities of the CLIP model compared to the nearest neighbor images in NegCLIP and not fully minimally-changed CounterCurate and SPEC data.

We further conduct a zero-shot evaluation of the fine-tuned CLIP models on other fine-grained understanding benchmarks (beyond our VisMin benchmark) to test their generalization capabilities (see Table 4). VisMin-CLIP performs best in \(11\) out of 18 tasks, while NegCLIP, CounterCurate-CLIP, and SPEC-CLIP lead in \(3\), \(1\), and \(3\) tasks, respectively. All models outperform the pre-trained CLIP model across benchmarks. For counting and spatial reasoning tasks, VisMin training data shows significant improvements. On CountBench, we observed \(9\%\), \(19\%\), and \(17\%\) gains over NegCLIP, CounterCurate-CLIP, and SPEC-CLIP, respectively. Similarly, on spatial reasoning benchmarks (SPEC, Whatsup, VSR), VisMin-CLIP showed an average \(7.79\%\) improvement over NegCLIP and \(5.21\%\) over CounterCurate-CLIP. While SPEC-CLIP performed best on the in-distribution SPEC benchmark, VisMin-CLIP still outperformed other models, indicating its effectiveness in improving fine-grained understanding. For VALSE and SugarCrepe, NegCLIP performed best, likely due to similarities in the textual hard-negative generation process used in those benchmarks and NegCLIP's fine-tuning data.

Furthermore, our minimal-change data significantly outperforms others in multi-image understanding benchmarks. Fine-tuning on this data enhances the model's ability to distinguish between similar images, showing improvements on challenging benchmarks like Winoground, MMVP, and EQBEN, which test compositional reasoning and fine-grained understanding. VisMin-CLIP improved Text scores by \(6\%\) on Winoground and \(18\%\) on EQBEN over baseline CLIP, demonstrating effective alignment of visual and textual feature spaces. VisMin-CLIP also outperforms other models on most tasks and achieves comparable results on remaining benchmarks (except SPEC), despite using fewer samples (e.g., \(65K\) in VisMin vs. \(637K\) in SPEC).

Additional FindingsFurther experiments reveal several key findings: (1) **Scalability**: As illustrated in fig. 4, we evaluated varying sizes of OpenAI's CLIP models\(-\) B/32 and L/16. Larger models demonstrated improved performance across both single-image and multi-image benchmarks after training on our synthetic data. This improvement is likely because understanding minimal changes is a complex task, demanding robust model capabilities. For instance, the smallest tested model,

   } &  &  &  &  &  &  \\   & **VSR** & **CB** & **VALSE** & **S** & **Viability** & **SPEC** & **IC** & **MMVP** & **Unground** & **EQBEN** & **Vails** & **Vails** \\  CLIP (ViT-L4) & - & 58.33 & 33.65 & 69.1 & 72.0 & 37.7 & 32.85 & 30.36 & 61.47 & 19.26 & 27.5 & 11.0 & 6.5 & 35.71 & 33.57 & 21.43 & 50.58 & 47.13 & 33.86 \\ NoCLIP+ CLIP & - & 11.86 & 86.40 & **101.01** & **85.32** & **14.22** & 37.23 & **14.38** & **102.93** & 26.25 & 22.20 & 12.0 & 72.46 & 42.00 & 30.60 & 34.63 & 57.42 & 48.96 \\ CompactCurate CLIP+ & 24.01 & 86.74 & 30.29 & 68.47 & 64.36 & **64.22** & **37.99** & 35.24 & 63.81 & 25.19 & 23.25 & 9.0 & 6.0 & 6.0 & 33.57 & 25.67 & 69.88 & 69.1 \\ SPEC-CLIP & 67.78 & **96.52** & 27.06 & 68.75 & 79.34 & 40.35 & **100.80** & 60.23 & 30.77 & 22.5 & 7.75 & 41.33 & 40.00 & 30.71 & 54.91 & 32.77 & 44.02 \\ VisMin-CLIP & 66.3 & 55.00 & **99.66** & 27.21 & 61.43 & 4.99 & 42.45 & 39.76 & 60.13 & 20.50 & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** \\  Media2 & - & 77.3 & 91.11 & **108.91** & 50.45 & 60.24 & 77.03 & 63.4 & 64.4 & 43.15 & **42.28** & 31.75 & **22.25** & 62.28 & 33.31 & 25.26 & 68.31 & 32.56 & 48.6 \\ Vain-Merge2 & - & **80.32** & **90.90** & 70.08 & **10.91** & **70.42** & **70.26** & **70.56** & **70.50** & **48.80** & 47.00 & 13.95 & **22.75** & **22.55** & **63.49** & **54.31** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** & **20.50** \\   

Table 4: Evaluation on other single and multi-image **visual** fine-grained understanding benchmarks. All models adopt **ViT-L4** as the vision encoder. **CB** refers CountBench, **SG** refers to SSTCrepe, **IC** refers to Imageocode. I2T and T2I indicate standard standard image-to-text and text-to-image retrieval metrics. Best-performing models in the CLIP-family are highlighted in blue, and best-performing MLLM models are highlighted in green,ViT-B/32 (149.62M parameters), exhibited improvements of \(2.37\) and \(3.24\) in single and multiple image benchmarks, respectively, when comparing VisMin-CLIP against the baseline CLIP. When the model's capacity was expanded to ViT-L/14 (427.62M parameters), the improvements increased to \(6.88\) and \(9.21\), respectively. These results highlight the scalability and efficacy of our data in enhancing model performance. (2) **Enhanced Original Capabilities**: In addition to improvements in fine-grained understanding tasks, training on our data also enhances performance in standard retrieval tasks, as shown in fig. 5. This suggests that models achieve better alignment from training on minimal change tasks, indicating that our data is generally applicable across various cross-modal tasks.

### Fine-tuning Multimodal Large Language Models (MLLMs)

We utilized Idefics2  to improve fine-grained understanding, employing our instruction-formatted dataset. Given its proficiency in multimodal interactions and advanced multi-image processing, Idefics2 was chosen for its open-source accessibility, model size and leading zero-shot performance.

**Dataset and QLoRa Fine-tuning** Our dataset, _VisMin Instruct-IT_, includes image-text pairs created using a rule-based approach (see A.2.3 for details). We reformulated these pairs for MLLMs, where the task is to select the correct image from two options based on a given caption or choose the appropriate caption for an image from two possibilities. While the base Idefics2 model was trained with a variable number of images in a sequence, we limited it to two images to include one positive and one hard negative example from VisMin. We fine-tuned the Idefics2-8B model using the QLoRa technique , updating adapters in the language model and modality connector including perceiver resampler with 1 A100 80GB GPU. We used \(4\)-bit quantization, with \(r=64\) and \(=16\) for LoRa, and a learning rate of \(1e-5\). The model was fine-tuned for one epoch with an accumulated batch size of \(64\).

**Results** The fine-tuned Idefics2 model shows significant improvement on VisMin (see Table 3) across all categories, comparable to GPT4-o (see Table 2), demonstrating the effectiveness of our minimal-change data in enhancing MLLM fine-grained understanding. Notably, in the Spatial Relation category, gains of \(64.4\%\), \(57.2\%\), and \(64.5\%\) were observed for Text, Image, and Group, respectively, unlike CLIP, where fine-tuning did not improve spatial understanding. Fine-tuning Idefics2 also transfers well to other fine-grained benchmarks, achieving over \(5\%\) overall improvement (see Table 4). To assess generalization, we evaluated its zero-shot performance on non-fine-grained tasks (MMMU  and POPE ; results in Fig. 5 (right)). The model maintained comparable performance on POPE but dropped on MMMU, likely due to the binary-choice format in our fine-tuning data. This suggests further gains could be made by combining our data with instruction-tuning datasets, though this wasn't pursued due to the GPU demands of fine-tuning an 8B model with additional data.

In addition to our main results, we provide further analysis in the Appendix A.3, including detailed evaluations on additional multimodal benchmarks, zero-shot image classification task, and video understanding tasks.

## 7 Conclusion and Limitations

We present VisMin, a benchmark for evaluating fine-grained visual understanding in VLMs such as CLIP, SigLIP, LLaVA, and Idefics2. While these models perform well in object and attribute recognition, they struggle with counting and spatial relationships. To address this, we fine-tuned CLIP and Idefics2 on our minimal-change dataset, yielding significant improvements in objects, attributes, and counting. For spatial relations, CLIP showed limited gains, while Idefics2 made notable progress. Fine-tuning also enhanced CLIP's image-text alignment, demonstrated in COCO retrieval tasks, underscoring our dataset's value as a training resource for VLMs. **Limitations:** Despite automatic filtering, the dataset contains noise, including image deformations and text-image mismatches due to diffusion model limitations. Future diffusion model advancements should improve minimal-change editing. Additionally, our model may inherit social biases from the base models (CLIP/Idefics2), as no specific mitigation measures were applied during fine-tuning. Our use of uniform prompts for evaluation may have influenced performance variably.