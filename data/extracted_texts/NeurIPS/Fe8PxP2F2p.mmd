# Gradient-Based Feature Learning under

Structured Data

Alireza Mousavi-Hosseini\({}^{1}\), Denny Wu\({}^{2}\), Taiji Suzuki\({}^{3}\), Murat A. Erdogdu\({}^{1}\)

\({}^{1}\)University of Toronto and Vector Institute,

\({}^{2}\)New York University and Flatiron Institute,

\({}^{3}\)University of Tokyo and RIKEN AIP

{mousavi,erdogdu}@cs.toronto.edu, dennywu@nyu.edu,

taiji@mist.i.u-tokyo.ac.jp

###### Abstract

Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their _information exponent_. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a _spiked covariance_ structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of _batch normalization_ can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.

## 1 Introduction

A fundamental feature of neural networks is their _adaptivity_ to learn unknown statistical models. For instance, when the learning problem exhibits certain low-dimensional structure or sparsity, it is expected that neural networks optimized by gradient-based algorithms can efficiently adapt to such structure via feature/representation learning. A considerable amount of research has been dedicated to understanding this phenomenon under various assumptions and to demonstrate the superiority of neural networks over non-adaptive methods such as kernel models .

A particular relevant problem setting for feature learning is the estimation of single index models, where the response \(y\) depends on the input \(^{d}\) via \(y=g(,)+\), where \(g:\) is the nonlinear link function and \(\) is the unit target direction. Here, learning corresponds to recovering the unknowns \(\) and \(g\), which requires the model to extract and adapt to the low-dimensional target direction. Recent works have shown that the sample complexity is determined by certain properties of the link function \(g\). In particular, the complexity of gradient-based optimization is captured by the _information exponent_ of \(g\) introduced by . Intuitively, a larger information exponent \(s\) corresponds to a more complex \(g\) (for gradient-based learning), and it has been proven that when the input is isotropic \((0,_{d})\), gradient flow can learn the single index model with \(}(d^{s})\) sample complexity .

In practice, however, real data always exhibits certain structures such as low intrinsic dimensionality, and isotropic data assumptions fail to capture this fact. In statistics methodology, it is known that the directions along which the input \(\) has high variance are often good predictors of the target \(\) ; indeed, this is the main reason principal component analysis is used in pretraining . A fundamental model that captures such a structure is the _spiked matrix model_ in which \((0,_{d}+^{})\) for some unit direction \(^{d}\) and \(>0\). Along the direction \(\), data has higher variability and predictive power. In single index models, such predictive power translates to a non-trivial alignment between the vectors \(\) and \(\) -- our focus is to investigate the effect of such alignment on the sample complexity of gradient-based training.

### Contributions: learning single index models under spiked covariance

In this paper, we study the sample complexity of learning a single index model using a two-layer neural network and show that it is determined by an interplay between

* **spike-target alignment:**\(, d^{-r_{1}},r_{1}[0,1/2]\),
* **spike magnitude:**\( d^{r_{2}}\), for \(r_{2}\).

Our contributions can be summarized as follows.

1. [leftmargin=*]
2. We show that even in the case of perfect spike-target alignment (\(r_{1}=0\)), the spherical gradient flow commonly employed in recent literature (see e.g. ) cannot recover the target direction for moderate spike magnitudes in the population limit. The failure of this covariance-agnostic procedure under anisotropic structure insinuates the necessity of an appropriate covariance-aware normalization to effectively learn the single index model.
3. We show that a covariance-aware normalization that resembles _batch normalization_ resolves this issue. Indeed, the resulting gradient flow can successfully recover the target direction \(\) in this case, and depending on the amount of spike-target alignment, the sample complexity can significantly improve compared to the isotropic case.
4. Under the spiked covariance model, we prove a three-stage phase transition for the sample complexity depending on the quantities \(r_{1}\) and \(r_{2}\). For a suitable direction and magnitude of the spike, the sample complexity can be made \(}(d^{3+})\) for any \(>0\) which is independent of the information exponent \(s\). This should be compared against the known complexity of \(}(d^{s})\) under isotropic data.
5. We finally show that preconditioning the training dynamics with the inverse covariance improves the sample complexity. This is particularly significant for the spiked covariance model where \(}(d^{3+})\) samples can be reduced to \(}(d^{1+})\) for any \(>0\), i.e. almost linear in \(d\). The three-stage phase transition also emerges, as illustrated in Figure 1: in the "hard" regime, the complexity remains \(}(d^{s})\) regardless of the magnitude and direction of the spike, while in the "easy" regime the complexity only depends on the spike magnitude and not its direction. The "intermediate" regime interpolates between these two; smaller \(r_{1}\) and larger \(r_{2}\) improve the sample complexity.

The rest of the paper is organized as follows. We discuss the notation and the related work in the remainder of this section. We provide preliminaries on the statistical model and the training procedure in Section 2, and provide a negative result on the covariance-agnostic gradient flow in Section 2.1. Our main sample complexity result on a single neuron is presented in Section 3.2. We provide our results on multi-neuron neural networks in Section 4 and also discuss extensions such as preconditioning and its implications. We provide a technical summary in Section 5 and conclude in Section 6.

**Notation.** We use \(,\) and \(\) to denote Euclidean inner product and norm. For matrices, \(\) denotes the usual operator norm, and \(_{}()\) and \(_{}()\) denote the largest and smallest eigenvalues respectively. We reserve \(\) for the standard Gaussian distribution on \(\), and let \(_{}\) denote the \(L^{2}()\) norm. \(^{d-1}\) is the unit \(d\)-dimensional sphere. For quantities \(a\) and \(b\), we will use \(a b\) to convey there exists a constant \(C\) (a universal constant unless stated otherwise, in which case may depend on polylogarithmic factors of \(d\)) such that \(a Cb\), and \(a b\) signifies that \(a b\) and \(b a\).

Figure 1: Sample complexity to learn \(\) and \(g\) under the spiked model. Smaller \(r_{1}\) denotes a better spike-target alignment, while larger \(r_{2}\) denotes a larger spike magnitude. The sample complexities are based on Corollary 8.

### Further related work

**Non-Linear Feature Learning with Neural Networks.** Recently, two popular scaling regimes of neural networks have emerged for theoretical studies. A large initialization variance leads to the _lazy training_ regime, where the weights do not move significantly, and the training dynamics is captured by the neural tangent kernel (NTK) . However, there are many instances of function classes that are efficiently learnable by neural networks and not efficiently learnable by the NTK . Under a smaller initialization scaling, gradient descent on infinite-width neural networks becomes equivalent to Wasserstein gradient flow on the space of measures, known as the mean-field limit , which can learn certain low-dimensional target functions efficiently .

As for neural networks with smaller width, recent works showed that a two-stage feature learning procedure can outperform the NTK when the data is sampled uniformly from the hypercube  or isotropic Gaussian . However, these results do not take into account the additional structure that might be present in the covariance matrix of the input data. Two notable exceptions are , where the authors analyzed a spiked covariance and Gaussian mixture data, respectively. Our setting is closer to , however, they do not provide optimization guarantees through gradient-based training. Furthermore, in a companion work , we zoom into the setting where the spike and target are perfectly aligned (\(r_{1}=0\)), and prove learnability in the \(n d\) regime for both kernel regression and two-layer neural network. Finally, we go over some results concurrent to our work in Appendix A.

**Learning Single Index Models.** The problem of estimating the relevant direction in a single index model is classical in statistics , with efficient dedicated algorithms ( among others). However, these algorithms are non-standard and instead, we are concerned with standard iterative algorithms like training neural networks with gradient descent. Recently,  considered an iterative optimization procedure for learning such models with a polynomial sample complexity that is controlled by the smoothness of the link function.  considered the effect of taking a single gradient step on the ability of a two-layer neural network to learn a single index model, and  considered training a special two-layer neural network architecture where all neurons share the same weight with gradient flow or online SGD. However, these works only consider the isotropic Gaussian input, and the effect of anisotropy in the covariance matrix when training a neural network to learn a single index model has remained unclear.

**Training a Single Neuron with Gradient Descent.** When training the first layer, we consider a setting where there is only one effective neuron. A large body of works exists on training a single neuron using variants of gradient descent. In the realizable setting (i.e. identical link and activation), the typical assumptions on the activation correspond to information exponent 1 as the activations are required to be monotone or have similar properties, see e.g. . In the agnostic setting,  considered initializing from the origin which is a saddle point for information exponent larger than 1.  also considered the agnostic learning of a ReLU activation, albeit their sample complexity is not explicit other than being polynomial in dimension.

## 2 Preliminaries: Statistical Model and Training Procedure

For a \(d\)-dimensional input \(\) and a link function \(g L^{2}()\), consider the single index model

\[y=g,}{\|^{1/2}\|} +\ \ \ \ (0,), \]

where \(\) is a zero-mean noise with \((1)\) sub-Gaussian norm and \(^{d-1}\). Learning the model (2.1) corresponds to approximately recovering the unknown link \(g\) and the unknown direction \(\). Note that a normalization is needed to make this problem well-defined; without loss of generality, we write \(,/\|^{1/2}\|\) to ensure that the input variance and the scaling of \(g\) both remain independent of the conditioning of \(\). For this learning task, we will use a two-layer neural network of the form

\[(;,,)_{i=1}^{m}a_{i}( _{i},+b_{i}), \]

where \(=\{_{i}\}_{i=1}^{m}\) is the \(m d\) matrix whose rows corresponds to first-layer weights \(_{i}\), \(=\{a_{i}\}_{i=1}^{m}\) denote the second-layer weights, \(=\{b_{i}\}_{i=1}^{m}\) denote the biases, and \(\) is the non-linearactivation function. We assume \(g\) and \(\) are weakly differentiable with weak derivatives \(g^{}\) and \(^{}\) respectively, and \(g,g^{},,^{} L^{2}()\). We are interested in the high-dimensional regime; thus, \(d\) is assumed to be sufficiently large throughout the paper. Our ultimate goal is to learn both unknowns \(g\) and \(\) by minimizing the population risk

\[R(,,)}( (;,,)-y)^{2}, \]

using a gradient-based training method such as gradient flow.

We follow the two-step training procedure employed in recent works : First, we train the first-layer weights \(\) to learn the unknown direction \(\); at the end of this stage, the neurons \(_{i}\) align with \(\). Here, the goal is to recover only the direction. Next, using random biases and training the second-layer weights, we obtain a good approximation for the unknown link function \(g\). In the majority of this work, we focus on the first part of this two-stage procedure as the alignment between \(_{i}\)'s and \(\) essentially determines the sample complexity of the overall procedure. This problem is somewhat equivalent to the simplified problem of minimizing (2.3) with \(m=1\), \(a_{1}=1\), \(b_{1}=0\), i.e., \((;,,)\) is replaced with \((;)(,)\) and we write \(R() R(,,)\) for simplicity. We emphasize that unless \(=g\) (i.e. the link function is known), the first stage of training only recovers the relevant direction \(\) and is not able to approximate \(g\). Indeed, \(m>1\) is often needed to learn the non-linear link function; this is the focus of Section 4.2 where we derive a complete learnability result for a two-layer neural network with \(m>1\).

Characteristics of the link function play an important role in the complexity of learning the model. As such, a central part of our analysis will rely on a particular property based on the Hermite expansion of functions in a basis defined by the normalized Hermite polynomials \(\{h_{j}\}_{j 0}\) given as

\[h_{j}(z)=e^{^{2}/2}}{}^{j}}{ z^{j}}e^{-z^{2}/2}. \]

These polynomials form an orthonormal basis in the space \(L^{2}()\), and the resulting expansion yields the following measure of complexity for \(g\), which is termed as _the information exponent_.

**Definition 1** (Information exponent).: _Let \(g=_{j 0}_{j}h_{j}\) be the Hermite expansion of \(g\). The information exponent of \(g\) is defined to be \(s\{j>0\,:\,_{j} 0\}\)._

This concept was introduced in  in a more general framework, and our definition is more in line with the setting in . We remark that the definition of  can be modified to handle anisotropy in which case one arrives at Definition 1. We provide a detailed discussion on this concept together with some properties of the Hermite expansion in Appendix B. Throughout the paper, we assume that the information exponent does not grow with dimension.

In the case where the \(d\)-dimensional input data is isotropic,  showed that learning a single index target with full-batch gradient flow requires a sample complexity of \(}(d^{s})\) for \(s 3\) where \(s\) is the information exponent of \(g\). We will show that this sample complexity can be improved under anisotropy. More specifically, if the input covariance \(\) has non-trivial alignment with the unknown direction \(\), we prove in Section 3 that the resulting sample complexity can be even made independent of the information exponent if we use a certain normalization in the training. In what follows, we prove that such a normalization in training procedure is indeed necessary.

### The spiked model and limitations of covariance-agnostic training

In practice, data often exhibit a certain structure which may have a profound impact on the statistical procedure. A well-known model that captures such a structure is the _spiked model_ for which one or several large eigenvalues of the input covariance matrix \(\) are separated from the bulk of the spectrum (see also ). Although our results hold for generic covariance matrices, they reveal interesting phenomena under the following spiked model assumption.

**Assumption 1**.: _The covariance \(\) follows the \((,)\)-spiked model if \(=_{d}+^{}}{1+}\) where \(\|\|=1\)._

In pursuit of the target (unit) direction \(\), the magnitude of the neuron \(\) is immaterial; thus, recent works take advantage of this and simplify the optimization trajectory by projecting \(\) onto unit sphere \(^{d-1}\) throughout the training process . In the sequel, we study the same dynamics which is agnostic to the input covariance in order to motivate our investigation of normalized gradient flow in Section 3. More specifically, we consider the spherical population gradient flow

\[^{t}}{t}=-^{S}R(^{t})\;\; \;\;^{S}R()= R()- R(), . \]

where \(^{S}\) is the spherical gradient at the current iterate. It is straightforward to see that when the initialization \(^{0}\) is on the unit sphere, the entire flow will remain on the unit sphere, i.e. \(^{t}^{d-1}\) for all \(t 0\). The flow (2.5) has been proven useful for learning the direction \(\) in the isotropic case \(=_{d}\) when the activation \(\) is ReLU. In contrast, when \(\) follows a spiked model, we show that it can get stuck at stationary points that are almost orthogonal to \(\). Indeed, when the input covariance \(\) has a spike in the target direction \(\), i.e. \(=\), one expects that the training procedure benefits from this as the input \(\) contains information about the sought unknown \(\) without even querying the response \(y\). The following result proves the contrary; for moderate spike magnitudes, the alignment between the first-layer weights and target \(^{t},\) will be insignificant for all \(t\).

**Theorem 2**.: _Let \(s>2\) be the information exponent of \(g\) with \([g]=0\), and assume \(\) follows the \((,)\)-spiked model with \((1)(d^{})\). For ReLU activation, let \(^{t}\) denote the solution to (2.5) initialized uniformly at random over \(^{d-1}\), then with probability at least \(0.99\),_

\[_{t 0}^{t}, 1/, \]

A non-trivial alignment between the first-layer weights \(^{t}\) and the target direction \(\) is required to learn the single index model (2.1). However, the above result implies that in high dimensions when \(d 1\), the alignment is negligible in the population limit (when the number of samples goes to infinity). We remark that when the spike magnitude is large, i.e. \((d)\), the flow (2.5) can achieve alignment as the problem essentially becomes one-dimensional, as we demonstrate in Appendix C.

To see why the flow (2.5) gets stuck at saddle points and fails to recover the true direction, notice that

\[R()=\,(, )-y^{2}=\,( ,)^{2}-[(,) y]+\,y^{2}. \]

If the input was isotropic, i.e. \((0,_{d})\), the first term in (2.7) would be equal to \(\|\|_{}^{2}\), which is independent of \(\). Thus, minimizing \(R()\) in this case is equivalent to maximizing the "correlation" term \([(,)y]\). However, under the spiked model, the alignment between \(\) and \(\) breaks the symmetry; consequently, the first term in the decomposition grows with \(,\), creating a repulsive force that traps the dynamics around the equator where \(\) is almost orthogonal to \(\).

## 3 Main Results: Alignment via Normalized Dynamics

Having established that the covariance-agnostic training dynamics (2.5) is likely to fail, we consider a covariance-aware normalized flow in this section and show that it can achieve alignment with the unknown target and enjoy better sample complexity compared to the existing results  in the isotropic case. We start with the population dynamics.

### Warm-up: Population dynamics

To simplify the exposition, we define \(^{-1/2}\), \(}^{1/2}/\|^{1/2}\|\) and similarly define \(}\), and consider the prediction function \((;}):=(},)\). Due to symmetry, the second moment of the prediction is \((;})^{2}=\|\|_{ }^{2}\) which is independent of \(\); thus, the population risk reads

\[()\,( ;})-y^{2}=\|\|_{} ^{2}+\,y^{2}-[( },)y]. \]

In (3.1), the only term that depends on the weights \(\) is the correlation term and the source of the repulsive force in (2.7) is eliminated; we have \(_{}()=-_{}\,[( },)y]\). Based on this, we use the following normalized gradient flow for training

\[^{t}}{t}=-(^{t})_{} (^{t})\;\;\;\;()=\|^{1/2}\|^{2}. \]We remark that, though not identical, this normalization is closely related to batch normalization which is commonly employed in practice . Under the invariance provided by the current normalization, minimizing \(}()\) corresponds to maximizing \([(},)y]\). Thus, instead of \(\), it will be more useful to track the dynamics of its normalized counterpart \(}\), which is made possible by the following intermediary result that follows from Stein's lemma; also see e.g. .

**Lemma 3**.: _Suppose we train \(^{t}\) using the gradient flow (3.2). Then \(}^{t}\) solves the following ODE_

\[}^{t}}{t}=-_{,g}( }^{t},})(_{d}-}^{t}}^{t^{}})(_{d}-}^{t}}^{t^{}})}, \]

_where \(_{,g}(},})- [^{}(},)g^{}( },)]\)._

We will investigate if the modified flow (3.3) achieves alignment; in this context, alignment corresponds to \(}^{t},} 1\). Towards that end, we make the following assumption.

**Assumption 2**.: _Let \(g=_{j 0}_{j}h_{j}\) and \(=_{j 0}_{j}h_{j}\) be the Hermite decomposition of \(g\) and \(\) respectively. Let \(s\) be the information exponent of \(g\). For some universal constant \(c>0\), we assume_

\[_{,g}()=-_{j>0}j_{j}_{j}\,^{j-1}-c\, ^{s-1},\ \ \ \ \ (0,1).\]

There are several important examples that readily satisfy Assumption 2. The obvious example is when the link function is known as in , i.e. \(=g\). A more interesting example is when \(\) is an activation with degree \(s\) non-zero Hermite coefficient (e.g. ReLU when \(s\) is even, see [1, Claim 1]) and \(g\) is a degree \(s\) Hermite polynomial, which for \(s=2\) corresponds to the phase retrieval problem. In this case, the assumption is satisfied if \(_{s}\) and \(_{s}\) have the same sign, which occurs with probability \(0.5\) if we randomly choose the sign of the second layer.

Under this condition, the following result shows that the population flow (3.3) can achieve alignment.

**Proposition 4**.: _Suppose Assumption 2 holds and consider the gradient flow given by (3.3) with initialization satisfying \(}^{0},}>0\). Then, we have \(}^{T},} 1-\) as soon as_

\[T}^{0},} +(1/)}{_{}()}\ \ \ _{s}(z)1&s=1\\ (1/z)&s=2\\ (1/z)^{s-2}&s>2. \]

We remark that the information exponent enters the rate in (3.4) through the function \(_{s}\), and time needed to achieve \(\) alignment gets worse with larger information exponent. Indeed, it is understood that this quantity serves as a measure of complexity for the target function being learned.

### Empirical dynamics and sample complexity

Given \(n\) i.i.d. samples \(\{(^{(i)},y^{(i)})\}_{i=1}^{n}\) from the single index model (2.1), we consider the flow

\[^{t}}{t}=-(^{t})}(^{t})\ \ \ }()-_{} _{i=1}^{n},^{(i)}}{\|}^{1/2}\|}y^{(i)}}, \]

where we estimate the covariance matrix \(\) using the sample mean \(}}_{i=1}^{n^{}}^{ (i)}{^{(i)}}^{}\) over \(n^{}\) i.i.d. samples; the above dynamics defines an empirical gradient flow. Notice that we ignored the gradient associated with the term \(^{2}\) since the population dynamics ensures that its gradient will concentrate around zero; thus, it is redundant to estimate this term. Below, we will use \(n^{}=n\) for smooth activations, i.e. the same dataset can be used for covariance estimation; For ReLU, we require a more accurate covariance estimator, thus, we use \(n^{} n^{2}\) by assuming access to an additional \(n^{}-n\) unlabeled data points. Similar to the previous section, we track the dynamics of normalized \(\) by defining \(}}^{1/2}/\|}^{1 /2}\|\) (and leave \(}\) unchanged from Section 3.1). The same arguments as in Lemma 3 allow us to track the evolution of \(}\), which ultimately yields the following alignment result under general covariance structure.

**Theorem 5**.: _Let \(s\) be the information exponent of \(g\), and assume it satisfies \(|g()| 1+||^{p}\) for some \(p>0\). For \(\) denoting either the ReLU activation or a smooth activation satisfying \(|^{}||^{}| 1\), suppose Assumption 2 holds. For any \(>0\), suppose we run the finite sample gradient flow (3.5) with \(()=\|}^{1/2}\|^{2}\), initialized such that \(}^{0},}>0\), and with number of samples_

\[n d()^{2}}^{0}, }^{2(1-s)}^{-2}},\]_where \(()\) is the condition number of \(\). Then, for \(T(}^{0},} )+(1/)}{_{}()}\), we have_

\[}^{T},} 1- , \]

_with probability at least \(1-c_{1}d^{-c_{2}}\) for some universal constants \(c_{1},c_{2}>0\) over the randomness of the dataset. Here, \(_{s}\) is defined in (3.4) and \(\) hides poly-logarithmic factors._

RemarkWe make the following remarks on the above theorem.

* The initial condition \(}^{0},}>0\) is required when we have odd information exponent. When \(^{0}\) is initialized uniformly over \(^{d-1}\), the condition holds with probability \(0.5\) over the initialization. See [1, Remark 1.8] for further discussion on this condition.
* Although \(}\) is defined using the empirical covariance unlike \(}\) which is defined by population covariance, this definition is the suitable choice to approximate the target function \(g\) (c.f. Theorem 9), since it ensures the arguments of \(\) and \(g\) are sufficiently close when \(}\) recovers \(}\).

The intuition behind the proof of Theorem 5 is presented in Section 5 with the complete proof in the appendix. We highlight that the improvement in the sample complexity compared to the isotropic setting occurs whenever the covariance structure induces a stronger initial alignment and consequently stronger signal. The following corollary demonstrates a concrete example of such improvement by specializing Theorem 5 for a spiked covariance model.

**Corollary 6**.: _Consider the setting of Theorem 5 with \(\) following the \((,)\)-spiked model, where \(, d^{-r_{1}}\) and \( d^{r_{2}}\) with \(r_{1}[0,1/2]\) and \(r_{2}\). Suppose \(^{0}\) is sampled uniformly from \(^{d-1}\). Then, when conditioned on \(}^{0},}>0\), the sample complexity in Theorem 5 reads_

\[nd^{1+2r_{2}}d^{s-1}^{-2}&0<r _{2}<r_{1}\\ d^{1+2r_{2}}d^{s-1)(1-2(r_{2}-r_{1}))}^{-2}&r_{1} <r_{2}<2r_{1}\,,\\ d^{1+2r_{2}}d^{s-1)(1-r_{2})}^{-2}&2r_{1}<r_{2}<1  \]

_where \(\) hides poly-logarithmic factors of \(d\)._

RemarkWe have the following observations on the above sample complexity.

* Corollary 6 demonstrates that structured data can lead to better sample complexity when the right normalization is used during training. This complements Theorem 2 where we recall that spherical training dynamics ignores the structure in data and the target direction cannot be recovered.
* When \(g\) is a polynomial of degree \(p\), the lower bound for rotationally invariant kernels (including the neural tangent kernel at initialization) implies a complexity of at least \(d^{((1-r_{2})p)}\). Thus the sample complexity of Corollary 6 can always outperform the kernel lower bound when \(p\) is sufficiently large and \(s\) remains constant.

Three-step phase transitionRecall that in the isotropic setting \(=_{d}\), the sample complexity of learning \(g\) with information exponent \(s\) using full-batch gradient flow is \(}(d^{s})\) for \(s 3\). The sample complexity in Corollary 6 is strictly smaller than \(}(d^{s})\) as soon as \((s-1)r_{1}/(s-2)<r_{2}\). Furthermore, for any \(>0\) it is at most \(}(d^{3+})\) as soon as \(r_{2} 1-/(s-3)\) and \(2r_{1}<r_{2}\), in which case the sample complexity becomes independent of the information exponent. Interestingly, the complexity becomes independent of \(r_{1}\) when \(r_{2}>2r_{1}\) or \(r_{2}<r_{1}\), i.e. the direction of the spike becomes irrelevant when the spike magnitude is sufficiently large or small.

The three-stage phase transition of Corollary 6 is due to the different behaviour of the inner product \(}^{0},}\) in different regimes of \(r_{1}\) and \(r_{2}\). When \(r_{2}<r_{1}\), we have \(}^{0},} ^{0},\), thus the initial alignment is just as uninformative as the isotropic case providing no improvement. Moreover, a potentially large condition number may hurt the sample complexity in this case. On the other hand, when \(r_{1}<r_{2}<2r_{1}\) we have \(}^{0},} ,^{0},\), and \(r_{2}>2r_{1}\) leads to \(}^{0},}^{0},\), thus large \(\) or \(,\) in this regime may improve the sample complexity.

Implications to Neural Networks and Further Improvements

### Improving Sample Complexity via Preconditioning

We now demonstrate that preconditioning the training dynamics with \(}^{-1}\) can remove the dependency on \(()\), ultimately improving the sample complexity. Consider the preconditioned gradient flow

\[^{t}}{t}=-(^{t})}^{-1 }}(^{t})\ \ \ \ ()=\|}^{1/2}\|^{2}. \]

We have the following alignment result.

**Theorem 7**.: _Consider the same setting as Theorem 5, and assume we run the preconditioned empirical gradient flow (4.1) with number of samples_

\[n d}^{0},} ^{2(1-s)}^{-2}},\]

_where \(\) hides poly-logarithmic factors of \(d\). Then, for \(T_{s}}^{0},} +(1/),\) we have_

\[}^{T},} 1- ,\]

_with probability at least \(1-c_{1}d^{-c_{2}}\) for some universal constants \(c_{1},c_{2}>0\)._

Preconditioning removes the condition number dependence, which is particularly important in the spiked model case where this quantity can be large.

**Corollary 8**.: _Consider the setting of Theorem 7, and assume we run the preconditioned empirical gradient flow (4.1) for the \((,)\)-spiked model where \(, d^{-r_{1}}\) and \( d^{r_{2}}\) with \(r_{1}[0,1/2]\) and \(r_{2}\). Suppose \(^{0}\) is sampled uniformly from \(^{d-1}\). Then, when conditioned on \(}^{0},}>0\), the sample complexity of Theorem 7 reads_

\[ndd^{s-1}^{-2}&0<r_{2}<r_{1} \\ dd^{(s-1)(1-2(r_{2}-r_{1}))}^{-2}&r_{1}<r_{2}<2r_ {1}\\ dd^{(s-1)(1-r_{2})}^{-2}&2r_{1}<r_{2}<1, \]

_where \(\) hides poly-logarithmic factors of \(d\)._

The above result improves upon Corollary 6; thus, making a case for preconditioning in practice. The complexity results also strictly improve upon the \(}(d^{s})\) complexity in the isotropic case  when \(r_{2}>r_{1}\). Further, for any \(>0\), we can obtain the complexity of \(}(d^{1+})\) (nearly linear in dimension) when \(r_{2}>1-/(s-1)\) and \(r_{2}>2r_{1}\) or \(r_{1}+1/2(1-/(s-1))<r_{2}<2r_{1}\). In addition to the remarks of Corollary 6, we note that the complexity is independent of both \(r_{1}\) and \(r_{2}\) when \(r_{2}<r_{1}\) (cf. Figure 1 hard regime), i.e. the spike magnitude and the spike-target alignment have no effect on the complexity unless \(r_{2} r_{1}\).

Under the spiked covariance model, one could improve the above results by instead using spectral initialization, i.e. initializing at \(\), which can be estimated from unlabeled data. Assuming perfect access to \(\), using the statement of Theorems 5 and 7, this initialization would imply a sample complexity of \(}(d^{1+2r_{2}+((s-1)(2r_{1}-r_{2}) 0)})\) without and \(}(d^{1+((s-1)(2r_{1}-r_{2}) 0)})\) with preconditioning.

### Two-layer neural networks and learning the link function

Our main focus so far was learning the target direction \(\). Next, we consider learning the unknown link function with a neural network, providing a complete learnability result for single index models.

We use Algorithm 1 and train the first-layer of the neural network with either the empirical gradient flow (3.5) or the preconditioned version (4.1). Then, we randomly choose the bias units and minimize the second layer weights using another gradient flow. Our goal is to track the sample complexity \(n\) needed to learn the single index target which we compare against the results of . We highlight that layer-wise training in Algorithm 1 is frequently employed in the literature  and in particular  also used gradient flow for training.

**Theorem 9**.: _Let \(g\) be twice weakly differentiable with information exponent \(s\) and assume \(g^{}\) has at most polynomial growth. Suppose \(\) is the ReLU activation, Assumption 2 holds and we run Algorithm 1 with \(^{0}\) initialized uniformly over \(^{d-1}\). For any \(>0\), let \(n\) and \(T\) be chosen according to Theorem 5 when we run the gradient flow (3.5) and Theorem 7 when we run the preconditioned gradient flow (4.1). Then, for \(\), some regime of \(\) given by (E.3) and sufficiently large \(T^{}\) given by (E.4), we have_

\[_{(,y)}(;^{T}, ^{T^{}},)-y^{2} C_{1}\, ^{2}+C_{2}(+1/m), \]

_conditioned on \(}^{0},}>0\) with probability at least \(0.99\) over the randomness of the dataset, biases, and initialization, where \(C_{1}\) is a universal constant and \(C_{2}\) hides \((m,n,d)\) factors._

The next result immediately follows from the previous theorem together with Corollaries 6 & 8.

**Corollary 10**.: _In the setting of Theorem 9, if \(\) follows the \((,)\)-spiked model, the sample complexity \(n\) is given by (3.7) if we use the empirical gradient flow and (4.2) if we use the preconditioned version._

We remark that for fixed \(\), the sample complexity to learn \(g\) in the isotropic case is \(}(d^{s})\). Under the spiked model, if we assume that \(r_{2}\) is sufficiently large and \(r_{1}\) is sufficiently small as discussed in the previous section, Corollary 10 improves this rate to either (3.7) when the empirical gradient flow is used without preconditioning or to (4.2) with preconditioning.

## 5 Technical Overview

In this section, we briefly discuss the key intuitions that lead to the proof of our main results. We first review the case \(=}_{d}\), where we have the following decomposition for population loss

\[R()\,((,)-y)^{2}=\|\|_{}^{2}+\, y^{2}-[(, )g(,)]. \]

Notice that the only term contributing to the population gradient is the last term which measures the correlation between \(\) and \(g\). Following the gradient flow and applying Stein's lemma yields

\[^{t},}{t}= ^{}(^{t},)g^{ }(,)(1-^{t },^{2})=(1-^{t},^{2} )_{j s}j_{j}_{j}^{t}, ^{j-1},\]

where the second identity follows from the Hermite expansion; see also . Assume \(_{s}_{s}>0\) to ensure that the population dynamics will move towards \(\) at least near initialization. When replacing the population gradient with a full-batch gradient, we need the estimation noise to be smaller than the signal existing in the gradient. When \(^{0}, 1\), this signal is roughly of the order \(^{0},^{s-1}\). As the uniform concentration error over \(^{d-1}\) scales with \(\), we need \(n d^{0},^{2(s-1)}\) to ensure the signal remains dominant and \(^{t}\) moves towards \(\). When \(^{0}\) is initialized uniformly over \(^{d-1}\) this translates to a sample complexity of \(n d^{s}\), which is indeed obtained by  via similar arguments.

However, the behavior of the spherical dynamics entirely changes when we move to the anisotropic case. Suppose \(\) follows a \((,)\)-spiked model and \(\) is ReLU. Using Lemma 12, it is easy to show that with the spherical gradient flow, the alignment obeys the following ODE

\[^{t},}{t}=^{}(}^{t},)g^{}( },)-_{, g}(^{t})}{1+}^{t},}(1- ^{t},^{2}),\]

where \(_{,g}(^{t})\) is introduced in Lemma 12. The additional \(_{,g}(^{t})\) term creates a repulsive force towards the equator \(^{t},=0\). The presence of this term is due to the fact that unlike (5.1), the term \((^{t},)^{2}\) is no longer independent of \(\) and cannot be replaced by \(\|\|_{}^{2}\). When \(^{0}\) is initialized uniformly over \(^{d-1}\) and \((1)(d)\), we have \(}^{0},}^{0},\). Furthermore, at this initialization \(_{,g}(^{0}) 1/2\). Therefore,

\[^{t},}{t}s _{s}_{s}(^{0},)^{s-1}-^{0},}{2}}.\]

Hence the dynamics is trapped at \(|^{t},|=(1/)\) for all \(t>0\) as long as \(=(d^{1-1/(s-1)})\).

To remove the repulsive force in the spherical dynamics, we can directly normalize the input of \(\). As demonstrated by (3.1), once again the only term that varies with \(\) would be the correlation loss. Specifically, using the result of Lemma 3, in the population limit we can track \(}^{t},}\) via

\[}^{t},} }{t}=[^{}(}, )g^{}(},)]}^{t}_{},}^{t}_{}, \]

where \(}^{t}_{}:=}-}^{t},}}^{t}\). Thus, the strength of the signal at initialization is of order \(}^{0},}^{s-1}/ ()\), which after controlling the error in the estimate of \(}\) and in the estimate of population gradient using finitely many samples, leads to the sample complexity \(n d()^{2}}^{0},}^{2(1-s)}\). Importantly, \(\) can include a much stronger initial alignment \(}^{0},}\) than the isotropic case \(^{0},\), which is emphasized in Corollary 6. Using preconditioning will further remove the dependency on the condition number of \(\).

## 6 Conclusion

We studied the dynamics of gradient flow to learn single index models when the input data covariance may contain additional structure. Under a spiked model for the covariance matrix, we showed that using spherical gradient flow, as an example of a covariance-agnostic training mechanism employed in the recent literature, is unable to learn the target direction of the single index model even when the spike and the target directions are identical. In contrast, we showed that an appropriate weight normalization removes this problem and successfully recovers the target direction. Moreover, depending on the alignment between the covariance structure and the target direction, the sample complexity can improve upon the isotropic setting, while also outperforming lower bounds for rotationally-invariant kernels. This phenomenon is due to the additional information about the target direction contained in the covariance matrix which improves the effective alignment at initialization. Additionally, we showed that a simple preconditioning of the gradient flow using the inverse empirical covariance can improve the sample complexity, achieving almost linear rate in certain settings.

We outline a few limitations of our current work and discuss directions for future research.

* While studying single index models provides a pathway to a general understanding of feature learning with structured covariance, considering multi-index models can provide a more complete picture , e.g. by establishing incremental learning dynamics . We leave the problem of learning multi-index models under structured input as an interesting future direction.
* Gradient flow under squared loss can be seen as an example of a Correlational Statistical Query (CSQ) algorithm , i.e. an algorithm that only accesses noisy estimates of expected correlation queries from the model. Understanding the limitations of learning single index models under a structured input through a CSQ lower bound perspective is another important direction that would complement our results in this paper.
* When training the first layer, we considered a somewhat unconventional initialization and relied on the symmetry it induces. It is interesting to consider cases where we train a network with multiple neurons starting from a more standard initialization which can help relax Assumption 2.