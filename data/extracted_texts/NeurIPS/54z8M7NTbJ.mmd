# GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies

Takahiro Mimori

Waseda University

RIKEN AIP

takahiro.mimori@aoni.waseda.jp

&Michiaki Hamada

Waseda University

AIST-Waseda CBBD-OIL

Nippon Medical School

mhamada@waseda.jp

###### Abstract

Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topological candidates. In experiments using real benchmark datasets, GeoPhy significantly outperformed other approximate Bayesian methods that considered whole topologies.

## 1 Introduction

Phylogenetic inference, the reconstruction of tree-structured evolutionary relationships between biological units, such as genes, cells, individuals, and species ranging from viruses to macro-organisms, is a fundamental problem in biology. As the phylogenetic relationships are often indirectly inferred from molecular observations, including DNA, RNA and protein sequences, Bayesian inference has been an essential tool to quantify the uncertainty of phylogeny. However, due to the complex nature of the phylogenetic tree object, which involve both a discrete topology and dependent continuous variables for branch lengths, the default approach for the phylogenetic inference has typically been an Markov-chain Monte Carlo (MCMC) method , enhanced with domain-specific techniques such as a mixed strategy for efficient exploration of tree topologies.

As an alternative approach to the conventional MCMCs, a variational Bayesian approach for phylogenetic inference was proposed in  (VBPI), which has subsequently been improved in the expressive powers of topology-dependent branch length distributions . Although these methods have presented accurate joint posterior distributions of topology and branch lengths for real datasets, they required reasonable preselection of candidate tree topologies to avoid a combinatorial explosion in the number of weight parameters beforehand. There have also been proposed variational approaches  on top of the combinatorial sequential Monte Carlo method (CSMC), which iteratively updated weighted topologies without the need for the preselection steps. However, the fidelity of the joint posterior distributions was still largely behind that of MCMC and VBPI as reported in .

In this work, we propose a simple yet effective scheme for parameterizing a binary tree topological distribution with a transformation of continuous distributions. We further formulate a novel differentiable variational Bayesian approach named GeoPhy 1 to approximate the posterior distribution of the tree topology and branch lengths without preselection of candidate topologies. By constructing practical topological models based on Euclidean and hyperbolic spaces, and employing variance reduction techniques for the stochastic gradient estimates of the variational lower bound, we have developed a robust algorithm that enables stable, gradient-based phylogenetic inference. In our experiments using real biological datasets, we demonstrate that GeoPhy significantly outperforms other approaches, all without topological restrictions, in terms of the fidelity of the marginal log-likelihood (MLL) estimates to gold-standard provided with long-run MCMCs.

Our contributions are summarized as follows:

* We propose a representation of tree topological distributions based on continuous distributions, thereby we construct a fully differentiable method named GeoPhy for variational Bayesian phylogenetic inference without restricting the support of the topologies.
* By considering variance reduction in stochastic gradient estimates and simple construction of topological distributions on Euclidean and hyperbolic spaces, GeoPhy offers unprecedently close marginal log-likelihood estimates to gold standard MCMC runs, outperforming comparable approaches without topological preselections.

## 2 Background

### Phylogenetic models

Let \(\) be represent an unrooted binary tree topology with \(N\) leaf nodes (tips), and let \(B_{}\) denote a set of evolutionary distances defined on each of the branches of \(\). A phylogenetic tree \((,B_{})\) represents an evolutionary relationship between \(N\) species, which is inferred from molecular data, such as DNA, RNA or protein sequences obtained for the species. Let \(Y=\{Y_{ij}\}_{1 i N,1 j M}\) be a set of aligned sequences with length \(M\) from the species, where \(Y_{ij}\) denote a character (base) of the \(i\)-th sequence at \(j\)-th site, and is contained in a set of possible bases \(\). For DNA sequences, \(\) represents a set of 4-bit vectors, where each bit represents 'A', 'T', 'G', or 'C'. A likelihood model of the sequences \(P(Y|,B_{})\) is determined based on evolutionary assumptions. In this study, we follow a common practice for method evaluations [42; 40] as follows: \(Y\) is assumed to be generated from a Markov process along the branches of \(\) in a site-independent manner; The base mutations are assumed to follow the Jukes-Cantor model . The log-likelihood \( P(Y|,B_{})\) can be calculated using Felsenstein's pruning algorithm , which is also known as the sum-product algorithm, and differentiable with respect to \(B_{}\).

### Variational Bayesian phylogenetic inference

The variational inference problem for phylogenetic trees, which seeks to approximate the posterior probability \(P(,B_{}|Y)\), is formulated as follows:

\[_{Q}D_{}(Q()Q(B_{}|)\|P(,B_{}|Y) ),\] (1)

where \(D_{}\), \(Q()\) and \(Q(B_{}|)\) denote the Kullback-Leibler divergence, a variational tree topology distribution, and a variational branch length distribution, respectively. The first variational Bayesian phylogenetic inference method (VBPI) was proposed by Zhang and Matsen IV , which has been successively improved for the expressiveness of \(Q(B_{}|)\)[39; 40]. For the expression of variational topology mass function \(Q()\), they all rely on a subsplit Bayesian network (SBN) , which represents tree topology mass function \(Q()\) as a product of conditional probabilities of splits (i.e., bipartition of the tip nodes) given their parent splits. However, SBN necessitates a preselection of the likely set of tree topologies, which hinders an end-to-end optimization strategy of the distribution over all the topologies and branch lengths.

### Learnable topological features (LTFs)

Zhang  introduced a deterministic embedding method for tree topologies, termed learnable topological features (LTFs), which provide unique signatures for each tree topology. Specifically, given a set of nodes \(V\) and edges \(E\) of a tree topology \(\), a node embedding function \(f_{ emb}:V^{d}\) is determined to minimize the Dirichlet energy defined as follows:

\[_{(u,v) E}\|f_{ emb}(u)-f_{ emb}(v)\|^{2}.\] (2)

When feature vectors for the tip nodes are determined, Zhang  showed that the optimal \(f_{ emb}\) for the interior nodes can be efficiently computed with two traversals of the tree topology. They also utilized \(f_{ emb}\) to parameterize the conditional distribution of branch length given tree topology \(Q(B_{}|)\) by using graph neural networks with the input node features \(\{f_{ emb}(v)|v V\}\).

### Hyperbolic spaces and probability distributions

Hyperbolic spaces are known to be able to embed hierarchical data with less distortion in considerably fewer dimensions than Euclidean spaces . This property has been exploited to develop various data analysis applications such as link predictions for relational data , as well as phylogenetic analyses . For representing the uncertainty of embedded data on hyperbolic spaces, the wrapped normal distribution proposed in  has appealing characteristics: it is easy to sample from and evaluate the density of the distribution.

The Lorentz model \(^{d}\) is a representation of the \(d\)-dimensional hyperbolic space, which is a submanifold of \(d+1\) dimensional Euclidean space. Denote \(^{d}\) and \(^{d d}\) be a location and scale parameters, respectively, a random variable \(z^{d}\) that follows a wrapped normal distribution \((,)\) is defined as follows:

\[u(0,), z=_{}_{^{o} }(u),\] (3)

where \(^{o}\), \(_{}:T_{}^{d}^{d}\), and \(_{^{o}}:T_{^{o}}^{d} T_{} ^{d}\) denote the origin, the exponential map, and the parallel transport defined on the Lorentz model. Also, the probability density \((,)\) has a closed form and is differentiable with respect to the parameters \(,\). More details and properties are summarized in Appendix A.

Figure 1: The proposed scheme for constructing a variational distribution \(Q(,B_{})\) of a tree topology and branch lengths by using a distribution \(Q(z)\) defined on the continuous space. **Left**: The marginal distributions of four tip nodes \(Q(z_{1}),,Q(z_{4})\). **Middle**: Coordinates of the tip nodes \(z=\{z_{1},,z_{4}\}\) sampled from the distribution \(Q(z)\), and a tree topology \((z)\) determined from \(z\). **Right**: A set of branch lengths \(B_{}\) (red figures) of the tree \(\) is sampled from a topology dependent distribution \(Q(B_{}|)\).

Proposed methods

### Geometric representations of tree topology ensembles

Considering the typically infeasible task of parameterizing the probability mass function of unrooted tree topologies \(\), which requires \((2N-5)!!-1\) degrees of freedom, we propose an alternative approach. We suggest constructing the mass function \(Q()\) through a transformation of a certain probability density \(Q(z)\) over a continuous domain \(\), as follows:

\[Q():=_{Q(z)}[[=(z)]],\] (4)

where \(:\) denotes a deterministic link function that maps \(N\) coordinates to the corresponding tree topology (Fig. 1). Note that we have overloaded \(\) to represent both a variable and function for notational simplicity. An example of the representation space \(\) is a product of the Euclidean space \(^{N d}\) or hyperbolic space \(^{N d}\), where \(d\) denotes the dimension of each tip's representation coordinate. For the link function, we can use \((z)=T_{j} D(z)\), where \(D:^{N N}\) denotes a function that takes \(N\) coordinates and provides a distance matrix between those based on a geometric measure such as the Euclidean or hyperbolic distance. \(T_{j}:^{N N}\) denotes a map that takes this distance matrix and generates an unrooted binary tree topology of their phylogeny, determined using the Neighbor-Joining (NJ) algorithm . While the NJ algorithm offers a rooted binary tree topology accompanied by estimated branch lengths, we only use the topology information and remove the root node from it to obtain the unrooted tree topology \(\).

### Derivation of variational lower bound

Given a distribution of tip coordinates \(Q(z)\) and an induced tree topology distribution \(Q()\) according to equation (4), the variational lower bound (1) is evaluated as follows:

\[[Q]=_{Q(z)}[_{Q(B_{}|(z))}[ |(z))}{Q(B_{}|(z))}]+].\] (5)

Thanks to the deterministic mapping \((z)\), we can obtain an unbiased estimator of \([Q]\) by sampling from \(Q(z)\) without summing over the combinatorial many topologies \(\). However, even when the density \(Q(z)\) is computable, the evaluation of \( Q()\) remains still infeasible according to the definition (4). We resolve this issue by introducing the second lower bound with respect to a conditional variational distribution \(R(z|)\) as follows:

\[[Q,R]=_{Q(z)}[_{Q(B_{}|(z))}[ |(z))}{Q(B_{}|(z))}]+].\] (6)

This formulation is similar in structure to the variational auto-encoders , where \(Q(|z)\) and \(R(z|)\) are viewed as a probabilistic decoder and encoder of data \(\), respectively. However, unlike typical auto-encoders, we design \(Q(|z)\) to be deterministic, and instead adjust \(Q(z)\) to approximate \(Q()\).

**Proposition 1**.: Assuming that \(R(z|)Q(z|)\), the inequality \( P(Y)[Q][Q,R]\) holds, where \(\) denotes the support of distribution. The first and the second equality holds when \(Q(,B_{})=P(,B_{}|Y)\) and \(R(z|)=Q(z|)\), respectively.

Proof.: The first variational lower bound of the marginal log-likelihood is given as follows:

\[ P(Y) P(Y)-D_{}[Q(,B_{})\|P(,B_{}|Y )]=_{Q(,B_{})}[)}{Q( ,B_{})}]:=[Q],\] (7)

where the equality condition of the first inequality holds when \(Q(,B_{})=P(,B_{}|Y)\). Since we have defined \(Q()\) in equation (4), we can further transform the lower bound as \([Q]\)

\[[Q]=_{Q(z)}[_{}[ =(z)]\,_{Q(B_{}|)}[|) }{Q(B_{}|)}+]],\] (8)

from which equation (5) immediately follows. Hence the inequality \( P(Y)[Q]\) and its equality condition \(Q(,B_{})=P(,B_{}|Y)\) have been proven.

Next, the entropy term \(-}_{Q(z)}[ Q((z))]=-}_{Q(z)} [ Q()]\) in equation (5) can be transformed to derive further lower bound as follows:

\[-}_{Q()}[ Q()] -}_{Q()}[ Q()+D_{}(Q(z|)\|R(z|))]\] \[=-}_{Q()Q(z|)}[]=-}_{Q(z)}[ ],\]

where the last equality is derived by using the relation \(}_{Q()Q(z|)}[]=} _{Q(z)}[_{}[=(z)]]\) and \(Q()Q(z|)=Q(z)[=(z)]\). The equality condition of the first inequality holds when \(R(z|)=Q(z|)\). Hence, the inequality \([Q][Q,R]\) and the equality condition is proven. 

Similar to Burda et al. , we can also derive a tractable importance-weighted lower-bound of the model evidence (IW-ELBO), which is used for estimating the marginal log-likelihood (MLL), \( P(Y)\), or an alternative lower-bound objective for maximization. The details and derivations are described in Appendix B.

### Differentiable phylogenetic inference with GeoPhy

```
1:\(,,\) Initialize variational parameters
2:while not converged do
3:\(_{z}^{(1:K)},_{B}^{(1:K)}\)Random samples from distributions \(p_{z}\), \(p_{B}\)
4:\(z^{(1:K)} h_{}(_{z}^{(1:K)})\)
5:\(B_{}^{(1:K)} h_{}(_{B}^{(1:K)},(z^{(1:K)}))\)
6:\(_{},_{},_{}\)Estimate the gradients \(_{}\), \(_{}\), \(_{}\)
7:\(,,\)Update parameters using an SGA algorithm, given gradients \(_{},_{},_{}\)
8:endwhile
9:return\(,,\) ```

**Algorithm 1** GeoPhy algorithm

Here, we introduce parameterized distributions, \(Q_{}(z)\), \(Q_{}(B_{}|)\), and \(R_{}(z|)\), aiming to optimize the variational objective \([Q_{,},R_{}]\) using stochastic gradient ascent (SGA). The framework, which we term GeoPhy, is summarized in Algorithm 1. While aligning with black-box variational inference  and its extension to phylogenetic inference , our unique lower bound objective enables us to optimize the distribution over entire phylogenetic trees within continuous geometric spaces.

Gradients of lower boundWe derive the gradient terms for stochastic optimization of \(\), omitting parameters \(,,\) for notational simplicity. We assume that \(Q_{}(z)\) and \(Q_{}(B_{}|(z))\) are both reparameterizable; namely, we can sample from these distributions as follows:

\[z=h_{}(_{z}), B_{}=h_{}(_{B},(z)),\] (9)

where the terms \(_{z} p_{z}(_{z})\) and \(_{B} p_{B}(_{B})\) represent sampling from parameter-free distributions \(p_{z}\) and \(p_{B}\), respectively, and the functions \(h_{}\) and \(h_{}\) are differentiable with respect to \(\) and \(\), respectively. Although we cannot take the derivative of \((z)\) with respect to \(z\), the gradient of \(\) with respect to \(\) is evaluated as follows:

\[_{} =}_{Q_{}(z)}[(_{}  Q_{}(z))(}_{Q_{}(B_{}|(z) )}[|(z))}{Q_{}(B_{}|(z))}]+  P((z))R_{}(z|(z)))]\] \[+_{}[Q_{}(z)],\] (10)

where \(\) denotes the differential entropy. The gradient of \(\) with respect to \(\) and \(\) are evaluated as follows:

\[_{}=}_{Q_{}(z)} }_{p_{B}()}[_{}=h_{}(,)|(z))}{Q_{}(B_{}=h_{}(, )|(z))}],_{}= }_{Q_{}(z)}[_{} R_{}(z|(z))],\] (11)

where we assume a tractable density model for \(R_{}(z|)\).

Gradient estimators and variance reductionFrom equation (10), an unbiased estimator of \(_{}\), using \(K\)-sample Monte Carlo samples, can be derived as follows:

\[_{}^{(K)}=_{k=1}^{K}(_{} Q_{ }(z^{(k)}) f(z^{(k)},B_{}^{(k)})-_{} Q_{}( h_{}(_{z}^{(k)}))),\] (12)

where we denote \(_{z}^{(k)} p_{z}\,z^{(k)}=h_{}(_{z}^{(k)})\), \(_{B}^{(k)} p_{B}\), \(B_{}^{(k)}=h_{}(_{B}^{(k)},(z^{(k)}))\), and

\[f(z,B_{}):=|(z))}{Q_{}(B_{}|(z))}+ P ((z))R_{}(z|(z)).\] (13)

Note that we explicitly distinguish \(z\) and \(h_{}(_{z})\) to indicate the target of differentiation with respect to \(\).

In practice, it is known to be crucial to reducing the variance of the gradient estimators proportional to the score function \(_{} Q_{}\) to make optimization feasible. This issue is addressed by introducing a control variate \(c(z)\), which has a zero expectation \(_{z}[c(z)]=0\) and works to reduce the variance of the term by subtracting it from the gradient estimator. For the case of \(K>1\), it is known to be effective to use simple Leave-one-out (LOO) control variates  as follows:

\[_{,}^{(K)}=_{k=1}^{K}[ _{} Q_{}(z^{(k)})(f(z^{(k)},B_{}^{(k)})- {f_{k}}(z^{( k)},B_{}^{( k)}))-_{}  Q_{}(h_{}(_{z}^{(k)}))],\] (14)

where we defined \(}(z^{(k)},B_{}^{( k)}):= _{k^{}=1,k^{} k}^{K}f(z^{(k)},B_{}^{(k)})\), which is a constant with respect to the \(k\)-the sample.

We can also adopt a gradient estimator called LAX , which uses a learnable surrogate function \(s_{}(z)\) differentiable in \(z\) to form control variates as follows:

\[_{,}=(_{} Q_{}(z) )(f(z,B_{})-s_{}(z))+_{}s_{}(h_{}( _{z}))-_{} Q_{}(h_{}(_{z})),\] (15)

where we write the case with \(K=1\) for simplicity. To optimize the surrogate function \(s_{}\), we use the estimator \(_{}=_{}_{}^{2}\)2, where \(\) denotes the average of the vector elements. We summarize the procedure with LAX estimator in Algorithm 2.

We also consider other options for the gradient estimators, such as the combinations of the LOO and LAX estimators, the gradient of IW-ELBO, and its variance-reduced estimator called VIMCO  in our experiments. For the remaining gradients terms \(_{}\) and \(_{}\), we can simply employ reparameterized estimators as follows:

\[_{}=_{}=h_{}(_{B}, )|(z))}{Q_{}(B_{}=h_{}(_{B},)|(z))}, _{}=_{} R_{}(z|(z)),\] (16)

where we denote \(_{B} p_{B}\) and \(z Q_{}\). More details of the gradient estimators are summarized in Appendix B.

Variational distributionsTo investigate the basic effectiveness of GeoPhy algorithm, we employ simple constructions for the variational distributions \(Q_{}(z)\), \(Q_{}(B_{}|)\), and \(R_{}(z|)\). We use an independent distribution for each tip node coordinate, i.e. \(Q_{}(z)=_{i=1}^{N}Q_{_{i}}(z_{i})\), where we use a \(d\)-dimensional normal or wrapped normal distribution for the coordinates of each tip node \(z_{i}\). For the conditional distribution of branch lengths given tree topology, \(Q_{}(B_{}|)\), we use the diagonal lognormal distribution whose location and scale parameters are given as a function of the LTFs of the topology \(\), which is comprised of GNNs as proposed in . For the model of \(R_{}(z|)\), we also employ an independent distribution: \(R_{}(z|)=_{i=1}^{N}R_{_{i}}(z_{i}|)\), where, we use the same type of distribution as \(Q_{_{i}}(z_{i})\), independent of \(\).

Related work

Differentiability for discrete optimizationDiscrete optimization problems often suffer from the lack of informative gradients of the objective functions. To address this issue, continuous relaxation for discrete optimization has been actively studied, such as a widely-used reparameterization trick with the Gumbel-softmax distribution [10; 20]. Beyond categorical variables, recent approaches have further advanced the continuous relaxation techniques to more complex discrete objects, including spanning trees . However, it is still nontrivial to extend such techniques to the case of binary tree topologies. As outlined in equation (4), we have introduced a distribution over binary tree topologies \(\) derived from continuous distributions \(Q(z)\). This method facilitates a gradient-based optimization further aided by variance reduction techniques.

Gradient-based algorithms for tree optimizationFor the hierarchical clustering (HC), which reconstructs a tree relationship based on the distance measures between samples, gradient-based algorithms [23; 2; 3] have been proposed based on Dasgupta's cost function . In particular, Chami et al.  proposed to decode tree topology from hyperbolic coordinates while the optimization is performed for a relaxed cost function, which is differentiable with respect to the coordinates. However, these approaches are not readily applicable to more general problems, including phylogenetic inference, as their formulations depend on the specific form of the cost functions.

Phylogenetic analysis in hyperbolic spaceThe approach of embedding phylogenetic trees into hyperbolic spaces has been explored for visualization and an interpretation of novel samples with existing phylogeny [21; 11]. For the inference task, a maximum-likelihood approach was proposed in , which however assumed a simplified likelihood function of pairwise distances. A recent study  proposed an MCMC-based algorithm for sampling \((,B_{})\), which were linked from coordinates \(z\) using the NJ algorithm . However, there remained the issue of an unevaluated Jacobian determinant, which posed a challenge in evaluating inference objectives. Given that we only use topology \(\) as described in equation (4), the variational lower bound for the inference can be unbiasedly evaluated through sampling, as shown in Proposition 1.

## 5 Experiments

We applied GeoPhy to perform phylogenetic inference on biological sequence datasets of 27 to 64 species compiled in Lakner et al. .

Models and trainingAs a prior distribution of \(P()\) and \(P(B_{}|)\), we assumed a uniform distribution over all topologies, and an exponential distribution \((10)\) independent for all branches, respectively, as commonly used in the literature [42; 16]. For the neural network used in the parameterization of \(Q_{}(B_{}|)\), we employed edge convolutional operation (EDGE), which was well-performed architecture in . For the stochastic gradient optimizations, we used the Adam optimizer  with a learning rate of 0.001. We trained for GeoPhy until one million Monte Carlo tree samples were consumed for the gradient estimation of our loss function. This number equals the number of likelihood evaluations (NLEs) and is used for a standardized comparison of experimental runs [34; 42]. The marginal log-likelihood (MLL) value is estimated with 1000 MC samples. More details of the experimental setup are found in Appendix C.

Initialization of coordinatesWe initialized the mean parameters of the tip coordinate distribution \(Q_{}(z)\) with the multi-dimensional scaling (MDS) algorithm when \(Q_{}\) was given as normal distributions. For \(Q_{}\) comprised of wrapped normal distributions, we used the hyperbolic MDS algorithm (hMDS) proposed in  for the initialization. For a distance matrix used for MDS and hMDS, we used the Hamming distance between each pair of the input sequences \(Y\) as similar to . For the scale parameters, we used \(0.1\) for all experiments. For \(R_{}(z)\), we used the same mean parameters as \(Q_{}(z)\) and 1.0 for the scale parameters.

### Exploration of stable learning conditions

To achieve a stable and fast convergence in the posterior approximations, we compared several control variates (CVs) for the variance reduction of the gradients by using DS1 dataset . Wedemonstrate that the choice of control variates is crucial for optimization (Fig. 2). In particular, adaptive control variates (LAX) for individual Monte Carlo samples and the leave-one-out (LOO) CVs for multiple Monte Carlo samples (with \(K=3\)) significantly accelerate the convergence of the marginal log-likelihood (MLL) estimates, yielding promising results comparable to VBPI-GNN. Although IW-ELBO was effective when \(Q(z)\) was comprised of diagonal Normal distributions, no advantages were observed for the case with full covariance matrices. Similar tendencies in the MLL estimates were observed for the other dimension \((d=3,4)\) and wrapped normal distributions (Appendix D.1).

### Comparison of different topological distributions

We investigated effective choices of tip coordinate distributions \(Q(z)\), which yielded tree topologies, by comparing different combinations of distribution types (normal \(\) or wrapped normal \(\)), space dimensions (\(d=2,3,4\)), and covariance matrix types (diagonal or full), across selected CVs (LAX, LOO, and LOO+LAX). While overall performance was stable and comparable for the range of configurations, the most flexible ones: the normal and wrapped normal distributions with \(d=4\) and a full covariance matrix, indicated relatively high MLL estimates within their respective groups (Table 2 in Appendix D), implying the importance of flexibility in the variational distributions.

### Performance evaluation across eight benchmark datasets

To demonstrate the inference performance of GeoPhy, we compared the marginal log-likelihood (MLL) estimates for the eight real datasets (DS1-8) [8; 6; 37; 9; 17; 43; 38; 30]. The gold-standard values were obtained using the stepping-stone (SS) algorithm  in MrBayes , wherein each evaluation involved ten independent runs, each with four chains of 10,000,000 iterations, as reported in . In Table 1, we summarize the MLL estimates of GeoPhy and other approximate Bayesian inference approaches for the eight datasets. While VBPI-GNN  employs a preselected set of tree topologies as its support set before execution, it is known to provide reasonable MLL estimates near the reference values. The other approaches including CSMC , VCSMC , \(\)-CSMC  and GeoPhy (ours) tackles the more challenging general problem of model optimization by considering all candidate topologies without preselection. We compared the GeoPhy for two configurations of \(Q(z)\): a wrapped normal distribution \(\) with a 4-dimensional full covariance matrix, and a 2-dimensional diagonal matrix, with three choices of CVs for optimization. Results for other settings, including those of the normal distributions in Euclidean spaces, are provided in Table 3 in Appendix D. There, we noted a slight advantage of wrapped normal distributions over their Euclidean counterparts. In

Figure 2: Comparison of marginal log-likelihood (MLL) estimates for DS1 dataset with different control variates. For a variational distribution, \(Q(z)\), an independent two-dimensional Normal distribution with a diagonal (diag) or full covariance matrix was used for each tip node. \(K\) stands for the number of Monte Carlo samples used for gradient estimation. For reference, we show the mean MLL value (gray dashed lines; \(-7108.42 0.18\)) estimated with the MrBayes stepping-stone (SS) method in . We also show MLL estimates obtained with VBPI-GNN (red dashed lines) using VIMCO estimator \((K=10)\). The iterations are counted as the number of likelihood evaluations (NLEs). Legend: NoCV stands for no control variates, LOO for leave-one-out CV, and IW for the case of using importance-weighted ELBO as the learning objective.

[MISSING_PAGE_FAIL:9]

progressively aligns with those obtained via MCMC (Fig. 3). Furthermore, we also observed that the Robinson-Foulds (RF) distance between the consensus trees derived from the topological distribution \(Q()\) and MCMC (MrBayes) are highly aligned with the MLL estimates (Fig. 4 First to Third), underscoring the validity of MLL-based evaluations. While GeoPhy provides a tree topology mode akin to MCMCs with a close MLL value, it may require more expressivity in \(Q(z)\) to fully represent tree topology distributions. In Fig. 4 Fourth, we illustrate the room for improvement in the expressivity of \(Q()\) in representing the bipartition frequency of tree samples, where VBPI-GNN aligns more closely with MrBayes. Further analysis is found in Appendix D.3.

In terms of execution time, we compared GeoPhy with VBPI-GNN and MrBayes, obtaining results that are promising and comparable to VBPI-GNN (Fig. 10 Left). Though we have not included CSMC-based methods in the comparison due to deviations in MLL values, they tend to capitalize on parallelism for faster execution as demonstrated in .

## 6 Conclusion

We developed a novel differential phylogenetic inference framework named GeoPhy, which optimized a variational distribution of tree topology and branch lengths without the preselection of candidate topologies. We also proposed a practical implementation for stable model optimization through choices of distribution models and control variates. In experiments conducted with real sequence datasets, GeoPhy consistently outperformed other approximate Bayesian methods that considered whole topologies.

## 7 Limitations and Future work

Although GeoPhy exhibits remarkable performance on standard benchmarks without preselecting topologies, it may be necessary to use more expressive distributions for \(Q(z)\) than independent parametric distributions to reach a level of performance comparable to state-of-the-art VBPI or gold-standard MCMC evaluations. Another limitation of our study lies in its exclusive focus on efficiency, measured in terms of the number of likelihood evaluations (NLEs), without paying significant attention to optimizing the computational cost per iteration. The general design of our variational distribution allows us to replace the tree link function such as UPGMA as presented in Fig. 9 Right. Future work should explore alternative design choices in continuous tree topology representations and the link functions to address these limitations. Extending our framework to include complex models remains crucial. Given the potential of scalable phylogenetic methods to enhance our understanding of viral and bacterial evolution, further exploration could substantially impact public health and disease control.

Figure 4: **First** to **Third**: Marginal log-likelihood (MLL) estimates and Robinson-Foulds (RF) distance between the consensus trees obtained from topology distribution \(Q()\) and MrBayes (MCMC) for datasets DS1, DS3, and DS7, respectively. **Fourth**: Comparison of bipartition frequencies derived from the posterior distribution of tree topologies for MrBayes, VBPI-GNN, and GeoPhy (ours).