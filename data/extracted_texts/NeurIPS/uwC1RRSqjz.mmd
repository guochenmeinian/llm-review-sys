# Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations

Richard Bergna\({}^{1,}\), Sergio Calvo-Ordonez\({}^{2,3}\), Felix L. Opolka\({}^{4}\)

Pietro Lio\({}^{4}\), Jose Miguel Hernandez-Lobato\({}^{1}\)

\({}^{1}\)Department of Engineering, University of Cambridge

\({}^{2}\)Mathematical Institute, University of Oxford

\({}^{3}\)Oxford-Man Institute of Quantitative Finance, University of Oxford

\({}^{4}\)Department of Computer Science and Technology, University of Cambridge

###### Abstract

We address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODE) are effective in learning node representations, they fail to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through Brownian motion to quantify uncertainty. We provide theoretical guarantees for LGNSDE and empirically show better performance in uncertainty quantification.

## 1 Introduction

Before the widespread of neural networks and the boom in modern machine learning, complex systems in various scientific fields were predominantly modelled using differential equations. Stochastic Differential Equations (SDEs) were the standard approach to incorporating randomness. These methods were foundational across disciplines such as physics, finance, and computational biology [Hoops et al., 2016, Quach et al., 2007, Mandelzweig and Tabakin, 2001, Cardelli, 2008, Buckdahn et al., 2011, Cvijovic et al., 2014].

In recent years, Graph Neural Networks (GNNs) have become the standard for graph-structured data due to their ability to capture relationships between nodes. They are widely used in social network analysis, molecular biology, and recommendation systems. However, traditional GNNs cannot reliably quantify uncertainty. Both aleatoric (inherent randomness in the data) and epistemic (model uncertainty due to limited knowledge) are essential for decision-making, risk assessment, and resource allocation, making GNNs less applicable in critical applications.

To address this gap, we propose Latent Graph Neural Stochastic Differential Equations (LGNSDE), a method that perturbs features during both the training and testing phases using Brownian motion noise, allowing for handling noise and aleatoric uncertainty. We also assume a prior SDE latent space and learn a posterior SDE using a GNN. This Bayesian approach to the latent space allows us to quantify epistemic uncertainty. As a result, our model can capture and quantify both epistemic and aleatoric uncertainties. More specifically, our contributions are as follows:

* We introduce a novel model class combining SDE robustness with GNN flexibility for handling complex graph-structured data, which quantifies both epistemic and aleatoric uncertainties.
* We provide theoretical guarantees demonstrating our model's ability to provide meaningful uncertainty estimates and its robustness to perturbations in the inputs.

* We empirically show that Latent GNSDEs demonstrate exceptional performance in uncertainty quantification, outperforming Bayesian GCNs (Hasanzadeh et al., 2020), and GCN ensembles (Lin et al., 2022).

## 2 Methodology

Inspired by Graph Neural ODEs (Poli et al., 2019) and Latent SDEs (Li et al., 2020), we now introduce our model: Latent Graph Neural SDEs \(-\) LGNSDEs (Figure 1), which use SDEs to define prior and approximate posterior stochastic trajectories for \((t)\)(Xu et al., 2022). Furthermore, LGNSDEs can be viewed as the continuous representations of existing discrete architectures (A.4).

### Model Definition

LGNSDEs are designed to capture the stochastic latent evolution of \((t)\) on graph-structured data. We use an Ornstein-Uhlenbeck (OU) prior process, represented by

\[\!(t)=_{}((t),t) \!t+_{}((t),t)\!(t),\]

where we set the drift and diffusion functions, \(_{}\) and \(_{}\), to constants and consider them hyperparameters. Moreover, \(d(t)\) is a Wiener process. The approximate posterior is defined as

\[\!(t)=_{}((t),t, )\!t+_{}((t),t) \!(t),\] (1)

where \(_{}\) is parameterized by a GCN with \(\) representing the learned weights of the neural network. The drift function mainly determines the dynamics of the evolution of the latent state, while the diffusion term \(_{}((t))\!(t)\) introduces stochastic elements. With the need to keep the Kullback-Leibler (KL) divergence bounded, we set the diffusion functions of the prior and posterior to be the same (Calvo-Ordonez et al., 2024; Archambeau et al., 2007).

Let \(\) be a collection of target variables, e.g., class labels, for some of the graph nodes. Given \(\) we train our model with variational inference, with the ELBO computed as

\[_{}()=}[ p(|(t_{1}))-_{t_{0}}^{t_{1}}\|v((u), ,,)\|_{2}^{2}\,\!u],\]

Figure 1: The diagram shows the evolution of one of the nodes of the input graph in latent space, \((t)\), through an SDE, with sample paths (purple) and confidence bands representing variance. At three timesteps, we visualize graph embeddings, where nodes (white and orange) become more separable over time due to the influence of the vector field. The inset axes represent latent dimensions, while the purple and yellow background highlights the magnitude and direction of the vector field guiding the latent dynamics.

where the expectation is approximated over trajectories of \((t)\) sampled from the approximate posterior SDE, and \(v=_{}((t))^{-1}[_{,}( (u),u)-_{,}((u),u)]\).

To sample \((t)\) from the approximate posterior, we integrate the SDE in Eq. 1:

\[(t_{1})=(t_{0})+_{t_{0}}^{t_{1}}_{,}((u),u)\,u+_{t_{0}}^{t_{1}}_{}((u),u)\,(u),\]

where \((t_{0})\) are the node-wise features \(_{}\) in the graph \(\). In practice, this is not feasible since the posterior drift \(_{,}\) is parametrised by a neural network. We numerically solve this integral with a standard Stochastic Runge-Kutta method (Rossler, 2010). We then use a Monte Carlo approximation to get the expectation of \((t_{0})\) and approximate the posterior predictive distribution as

\[p(^{*}|,_{},)_{n=1}^{N}p(^{*}|_{n}(t_{1}), ),\]

where \(_{1}(t_{1}),,_{N}(t_{1})\) are samples drawn from the approximate posterior \(p((t_{1})|,_{},)\).

Following Poli et al. (2019), we use a similar encoder-decoder setup. Our encoding focuses solely on the features of individual nodes, while the graph structure remains unchanged. Finally, we remark that the memory and time complexity are \((||d+L)\) and \((L)\) respectively, where \(L\) is the number of SDE solver steps, \(\) is the number of edges in the graph and \(d\) is the dimension of the features.

## 3 Theoretical Guarantees

In this section, we present key results on the stability and robustness of our framework under mild assumptions (Appendix A.2). Firstly, we address the fundamental question of whether our proposed models provide meaningful uncertainties. By showing that the variance of the latent representation bounds the model output variance, we highlight the ability of LGNSDEs to capture and quantify inherent uncertainty in the system. The latent representation is the underlying structure from which the model's output is generated, i.e. the uncertainty in the latent space directly influences the uncertainty in predictions. We formalize this in the following lemma:

**Proposition 1**.: _Under assumptions 1-3, there exists a unique mild\({}^{1}\) solution to an LGNSDE of the form_

\[d(t)=_{}((t),t,) \,dt+_{}((t),t)\,d(t),\]

_whose variance bounds the variance of the model output \(}(t)\) as:_

\[(}(t)) L_{h}^{2}((t)),\]

_where \(L_{h}^{2}\) is the Lipschitz constant of the readout layer. This ensures that the output variance is bounded by the prior variance of the latent space, providing a controlled measure of uncertainty._

We now demonstrate the robustness of our framework under small perturbations in the initial conditions. By deriving explicit bounds on the deviation between the perturbed and unperturbed solutions over time, we show that the model's output remains stable.

**Proposition 2**.: _Under assumptions 1-3, consider two initial conditions \(_{0}\) and \(}_{0}=_{0}+(0)\), where \((0)^{n d}\) is a small perturbation in the initial node features with \(\|(0)\|_{F}=\). Assume that \(_{0}\) is taken from a compact set \(^{n d}\). Then, the deviation between the solutions \((t)\) and \(}(t)\) of the LGNSDE with these initial conditions remains bounded across time \(t^{2}\), specifically_

\[[\|(t)-}(t)\|_{F}] e^{(L_{f}+ L_{g}^{2})t}.\]

In summary, we show analytically that our framework effectively quantifies uncertainty and maintains robustness under small perturbations of the input. First, we confirm that the model's output variance is controlled and directly linked to the variance of the latent state. Second, we provide a bound on the deviation between solutions with perturbed initial conditions, ensuring stability over time. The proofs can be found in Appendix A.

## 4 Experiments

We evaluate LGNSDEs on 5 datasets (see A.2 for details on these datasets and hyperparameters), we compare it to GNODE (Poli et al., 2019), GCN Kipf and Welling (2016), Bayesian GCN (BGCN) (Hasanzadeh et al., 2020), and an ensemble of GCNs (Lin et al., 2022).

The results in Table 3 demonstrate that LGNSDE consistently ranks as either the best or second-best model across most datasets in terms of Micro-AUROC (Area Under the Receiver Operating Characteristic), AURC (Area Under the Risk Coverage), and accuracy. This indicates that LGNSDE effectively handles model uncertainty, successfully distinguishing between classes (AUROC), maintaining low risk while ensuring confident predictions (AURC), and delivering high accuracy.

The top figure 2 shows the entropy distributions of the models for correct and incorrect predictions. Note that most models display similar mean entropy for both correct and incorrect predictions. Notably, our model stands out with the largest difference in entropy, with incorrect predictions having 35% more entropy compared to correct predictions, a larger gap than observed in other models.

We evaluate the models' ability to detect out-of-distribution (OOD) data by training them with one class left out of the dataset. This introduces an additional class in the validation and test sets that the models have not encountered during training. The goal is to determine if the models can identify this class as OOD. We analyze the entropy, \(H(y|_{i})=-_{c=1}^{C}p(y=c|_{i}) p(y=c|_ {i})\), where \(p(y=c|_{i})\) represents the probability of input \(_{i}\) belonging to class \(c\). Entropy quantifies the uncertainty in the model's predicted probability distribution over \(C\) classes for a given input \(_{i}\).

**Metric** & **Model** & **Cora** & **Citeseer** & **Computers** & **Photo** & **Pubmed** \\   & GCN & \(0.7063 0.0569\) & \(0.7937 0.0366\) & \(0.7796 0.0271\) & \(0.8578 0.0136\) & \(0.6127 0.0351\) \\  & GNODE & \(0.7398 0.0677\) & \(0.7828 0.0465\) & \(0.7753 0.0795\) & \(0.8473 0.0158\) & \(0.5813 0.0242\) \\  & BGCN & \(0.7193 0.0947\) & \(0.8287 0.0377\) & \(0.7914 0.1234\) & \(0.7910 0.0464\) & \(0.5310 0.0472\) \\  & ENSEMBLE & \(0.7031 0.0969\) & \(0.8190 0.0375\) & \(0.8292 0.0338\) & \(0.8352 0.0059\) & \(0.6130 0.0311\) \\  & **LGNSDE (Our)** & \(0.7614 0.0804\) & \(0.8258 0.0418\) & \(0.7994 0.0238\) & \(0.8707 0.0099\) & \(0.6204 0.0162\) \\   & GCN & \(0.0202 0.0049\) & \(0.0527 0.0075\) & \(0.0072 0.0013\) & \(0.0076 0.0006\) & \(0.3227 0.0266\) \\  & GNODE & \(0.0184 0.0053\) & \(0.0545 0.0110\) & \(0.0070 0.0029\) & \(0.0097 0.0015\) & \(0.3357 0.0309\) \\   & BGCN & \(0.0208 0.0091\) & \(0.0458 0.0071\) & \(0.0064 0.0047\) & \(0.0108 0.0034\) & \(0.3714 0.0317\) \\   & ENSEMBLE & \(0.0215 0.0061\) & \(0.0487 0.0072\) & \(0.0041 0.0011\) & \(0.0081 0.0003\) & \(0.3277 0.0265\) \\   & **LGSNDE (Our)** & \(0.0168 0.0070\) & \(0.0479 0.0109\) & \(0.0061 0.0011\) & \(0.0068 0.0008\) & \(0.3205 0.0135\) \\  

Table 1: AUROC (Mean \(\) Std) and AURC (Mean \(\) Std) for OOD Detection across datasets. Red denotes the best-performing model, and blue denotes the second-best-performing model.

Figure 2: **Top:** Entropy distributions comparing correct and incorrect model predictions on the CORA dataset. Higher entropy is expected for incorrect predictions. **Bottom:** Entropy distributions comparing OOD samples with in-distribution samples in the CORA dataset.

Figure 2 shows the test entropy distribution for in-distribution (blue) and out-of-distribution (red) data. For each test sample, predictions were made over \(C-1\) classes, excluding the left-out class. The OOD class exhibits higher entropy, indicating greater uncertainty. While most models show similar entropy distributions for both data types, our LGNSDE model achieves a clear separation, with a 50% higher mean entropy for OOD data compared to in-distribution data. Other models show less than a 10% difference between the two distributions.

Table 1 presents the AUROC and AURC scores for OOD detection across multiple datasets. AUROC evaluates the model's ability to differentiate between in-distribution and out-of-distribution (OOD) samples, with higher scores indicating better discrimination. AURC measures the risk of misclassification as coverage increases, where lower values are preferred. The LGNSDE model (ours) consistently achieves the best AUROC and AURC scores across most datasets, indicating its superior performance in accurately identifying OOD samples and minimizing the risk of misclassification.

## 5 Conclusions and Future Work

In conclusion, LGNSDEs outperform the tested models, opening a new avenue for uncertainty quantification in graph data. In future work, stronger benchmarks should be included for a more comprehensive evaluation. Additionally, Neural SDEs face challenges with time and memory complexity. Further work should explore more scalable sampling methods to address these limitations.