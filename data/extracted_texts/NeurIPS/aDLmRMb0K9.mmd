# Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning

Stefan Stojanovic

EECS

KTH, Stockholm, Sweden

stesto@kth.se

&Yassir Jedra

EECS

KTH, Stockholm, Sweden

jedra@kth.se

Alexandre Proutiere

EECS

kTH, Stockholm, Sweden

alepro@kth.se

###### Abstract

We study matrix estimation problems arising in reinforcement learning (RL) with low-rank structure. In low-rank bandits, the matrix to be recovered specifies the expected arm rewards, and for low-rank Markov Decision Processes (MDPs), it may for example characterize the transition kernel of the MDP. In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these methods further need to accommodate for inherent correlations in the available data (e.g. for MDPs, the data consists of system trajectories). We investigate the performance of simple spectral-based matrix estimation approaches: we show that they efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. These new results on low-rank matrix estimation make it possible to devise reinforcement learning algorithms that fully exploit the underlying low-rank structure. We provide two examples of such algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs. Both algorithms yield state-of-the-art performance guarantees.

## 1 Introduction

Learning succinct representations of the reward function or of the system state dynamics in bandit and RL problems is empirically known to significantly accelerate the search for efficient policies . It also comes with interesting theoretical challenges. The design of algorithms learning and leveraging such representations and with provable performance guarantees has attracted considerable attention recently, but remains largely open. In particular, significant efforts have been made towards such design when the representation relies on a low-rank structure. In bandits, assuming such a structure means that the arm-to-reward function can be characterized by a low-rank matrix . In MDPs, it implies that the reward function, the \(Q\)-function or the transition kernels are represented by low-rank matrices . In turn, the performance of algorithms exploiting low-rank structures is mainly determined by the accuracy with which we are able to estimate these matrices.

In this paper, we study matrix estimation problems arising in low-rank bandit and RL problems. Two major challenges are associated with these problems. (i) The individual entries of the matrix carry important operational meanings (e.g. in bandits, an entry could correspond to the average reward of an arm), and we seek estimation methods with low entry-wise error. Such requirement calls for afine-grained analysis, typically much more involved than that needed to only upper bound the spectral or Frobenius norm of the estimation error [22; 21; 12; 2; 54; 15; 53; 47]. (ii) Our estimation methods should further accommodate for inherent correlations in the available data (e.g., in MDPs, we have access to system trajectories, and the data is hence Markovian). We show that, essentially, spectral methods successfully deal with these challenges.

**Contributions.** 1) We introduce three matrix estimation problems. The first arises in low-rank bandits. The second corresponds to scenarios in RL where the learner wishes to estimate the (low-rank) transition kernel of a Markov chain and to this aim, has access to a generative model. The last problem is similar but assumes that the learner has access to system trajectories only, a setting referred to as the forward model in the RL literature. For all problems, we establish strong performance guarantees for simple spectral-based estimation approaches: these efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. To prove these results, we develop and combine involved leave-one-out arguments and Poisson approximation techniques (to handle the correlations in the data).

2) We apply the results obtained for our first matrix estimation problem to devise an efficient regret-minimization algorithm for low-rank bandits. We prove that the algorithm enjoys finite-time performance guarantees, with a regret at most roughly scaling as \((m+n)^{3}(T)/_{}^{2}\) where \((m,n)\) are the reward matrix dimensions, \(T\) is the time horizon, \(\) is the average of the reward gaps between the best arm and all other arms, and \(_{}\) is the minimum of these gaps.

3) Finally, we present an algorithm for best policy identification in low-rank MDPs in the reward-free setting. The results obtained for the second and last matrix estimation problems imply that our algorithm learns an \(\)-optimal policy for any reward function using only a number of samples scaling as \(O(nA/^{2})\) up to logarithmic factors, where \(n\) and \(A\) denote the number of states and actions, respectively. This sample complexity is mini-max optimal , and illustrates the gain achieved by leveraging the low-rank structure (without this structure, the sample complexity would be \((n^{2}A/^{2})\)).

**Notation.** For any matrix \(A^{m n}\), \(A_{i,.}\) (resp. \(A_{.,j}\)) denotes its \(i\)-th row (resp. its \(j\)-th column), \(A_{}=_{(i,j)}A_{i,j}\) and \(A_{}=_{(i,j)}A_{i,j}\). We consider the following norms for matrices: \(\|A\|\) denotes the spectral norm, \(\|A\|_{1}=_{i[m]}\|A_{i,.}\|_{1}\), \(\|A\|_{2}=_{i[m]}\|A_{i,.}\|_{2}\), and finally \(\|A\|_{}=_{(i,j)[m][n]}|A_{i,j}|\). If the SVD of \(A\) is \(U V^{}\), we denote by \((A)=UV^{}\) the matrix sign function of \(A\) (see Definition 4.1 in ). \(^{r r}\) denotes the set of \((r r)\) real orthogonal matrices. For any finite set \(\), let \(()\) be the set of distributions over \(\). The notation \(a(n,m,T) b(n,m,T)\) (resp. \(a(n,m,T)=(b(n,m,T))\)) means that there exists a universal constant \(C>0\) (resp. \(c,C>0\)) such that \(a(n,m,T) Cb(n,m,T)\) (resp. \(cb(n,m,T) a(n,m,T) Cb(n,m,T)\)) for all \(n,m,T\). Finally, we use \(a b=(a,b)\) and \(a b=(a,b)\).

## 2 Models and Objectives

Let \(M^{m n}\) be an unknown rank \(r\) matrix that we wish to estimate from \(T\) noisy observations of its entries. We consider matrices arising in two types of learning problems with low-rank structure, namely low-rank bandits and RL. The SVD of \(M\) is \(U V^{}\) where the matrices \(U^{m r}\) and \(V^{n r}\) contain the left and right singular vectors of \(M\), respectively, and \(=(_{1},,_{r})\). We assume without loss of generality that the singular values have been ordered, i.e., \(_{1}_{r}\). The accuracy of our estimate \(\) of \(M\) will be assessed using the following criteria:

* _Singular subspace recovery._ Let the SVD of \(\) be \(^{}\). To understand how well the singular subspaces of \(M\) are recovered, we will upper bound \(_{O^{r r}}\|U-O\|_{2}\) and \(_{O^{r r}}\|V-O\|_{2}\) (the \(_{O^{r r}}\) problem corresponds to the orthogonal Procrustes problem and its solution aligns \(\) and \(U\) as closely as possible, see Remark 4.1 in ).
* _Matrix estimation._ To assess the accuracy of \(\), we will upper bound the row-wise error \(\|-M\|_{1}\) or \(\|-M\|_{2}\), as well as the entry-wise error \(\|-M\|_{}\) (the spectral error \(\|-M\|\) is easier to deal with and is presented in appendix only).

We introduce two classical quantities characterizing the heterogeneity and incoherence of the matrix \(M\)[11; 48]. Let \(=_{1}/_{r}\), and let \((U)=\|U\|_{2}\) (resp. \((V)=\|V\|_{2}\)) denote the row-incoherence (resp. column-incoherence) parameter of \(M\). Let \(=\{(U),(V)\}\). Next, we specify the matrices \(M\) of interest in low-rank bandits and RL, and the way the data used for their estimation is generated.

**Model I: Reward matrices in low-rank bandits.** For bandit problems, \(M\) corresponds to the average rewards of various arms. To estimate \(M\), the learner has access to data sequentially generated as follows. In each round \(t=1,,T\), an arm \((i_{t},j_{t})[m][n]\) is randomly selected (say uniformly at random for simplicity) and the learner observes \(M_{i_{t},j_{t}}+_{t}\), an unbiased sample of the corresponding entry of \(M\). \((_{t})_{t 1}\) is a sequence of zero-mean and bounded random variables. Specifically, we assume that for all \(t 1\), \(|_{t}| c_{1}\|M\|_{}\) a.s., for some constant \(c_{1}>0\).

Model II: Transition matrices in low-rank MDPs.In low-rank MDPs, we encounter Markov chains whose transition matrices have low rank \(r\) (refer to Section 5 for details). Let \(P^{n n}\) be such a transition matrix. We assume that the corresponding Markov chain is irreducible with stationary distribution \(\). The objective is to estimate \(P\) from the data consisting of samples of transitions of the chain. More precisely, from the data, we will estimate the _long-term frequency matrix_\(M=()P\) (\(M_{ij}\) is the limiting proportion of transitions from state \(i\) to state \(j\) as the trajectory grows large). Observe that \(M\) is of rank \(r\), and that \(P_{i:}=M_{i:}/\|M_{i:}/\|M_{i:}\|_{1}\). To estimate \(M\), the learner has access to the data \((x_{1},,x_{T})[n]^{T}\) generated according to one of the following two models.

1. In the _generative_ model, for any \(t[T]\), if \(t\) is odd, \(x_{t}\) is selected at random according to some distribution \(_{0}\), and \(x_{t+1}\) is sampled from \(P_{x_{t},:}\).
2. In the _forward_ model, the learner has access to a trajectory \((x_{1},,x_{T})\) of length \(T\) of the Markov chain, where \(x_{1}_{0}\) and for any \(t 1\), \(x_{t+1} P_{x_{t},:}\).

## 3 Matrix Estimation via Spectral Decomposition

In the three models (Models I, II(a) and II(b)), we first construct a matrix \(\) directly from the data, and from there, we build our estimate \(\), typically obtained via spectral decomposition, i.e., by taking the best rank-\(r\) approximation of \(\). In the remaining of this section, we let \(^{}\) denote the SVD of \(\). Next, we describe in more details how \(\) is constructed in the three models, and analyze the corresponding estimation error.

### Reward matrices

For Model I, for \(t=1,,T\), we define \(_{t}=(M_{i_{t},j_{t}}+_{t})_{\{(i,j)=(i_{t}, j_{t})\}}_{i,j[m][n]}\) and \(=_{t=1}^{T}_{t}\). Let \(\) denote the best rank-\(r\) approximation of \(\).

**Theorem 1**.: _Let \(>0\). We introduce:_

\[=}()}+^{3/2}()).\]

_Assume that \(T c^{4}^{2}r^{2}(n+m)^{3}(e(m+n)T/)\) for some universal constant \(c>0\). Then there exists a universal constant \(C>0\) such that the following inequalities hold with probability at least \(1-\):_

\[(i) \|U-(^{}U)\|_{2 },\|V-(^{}V)\|_{2} C ^{2}r^{3/2})}{},\] \[(ii) \|-M\|_{2} C\, ^{2}r^{3/2})}{}\|M\|_{},\] \[(iii) \|-M\|_{} C(^{11/2}\,^{2}r^{1/ 2}+^{3} r^{3/2}})\|M \|_{}.\]

**Corollary 2**.: _(Homogeneous reward matrix) When \(m=(n)\), \(=(1)\), \(=(1)\), \(\|M\|_{}=(1)\), \(r=(1)\), we say that the reward matrix \(M\) is homogeneous. In this case, for any \(>0\), when \(T c(n+m)^{3}(e(m+n)T/)\) for some universal constant \(c>0\), we have with probability at least \(1-\):_

\[(\|U-(^{}U)\|_{2},\|V- (^{}V)\|_{2})}^{3/2}(),\] \[\|-M\|_{2}} ^{3/2}(),\] \[\|-M\|_{}}^{3/ 2}().\]

For a homogeneous reward matrix, \(\|U\|_{2}=(1/)\) and \(\|M\|_{}=(1)\), and hence, from the above corollary, we obtain estimates whose relative errors (e.g., \(\|-M\|_{}/\|M\|_{}\)) scale at most as \(\) up to the logarithmic factor.

We may also compare the results of the above corollary to those of Theorem 4.4 presented in . There, the data consists for each pair \((i,j)\) of a noisy observation \(M_{i,j}+E_{i,j}\). The \(E_{i,j}\)'s are independent across \((i,j)\). This model is simpler than ours and does not include any correlation in the data. But it roughly corresponds to the case where \(T=nm\) in our Model I. Despite having to deal with correlations, we obtain similar results as those of Theorem 4.4: for example, \(\|-M\|_{}\) (up to logarithmic terms) with high probability.

### Transition matrices under the generative model

For Model II(a), the matrix \(\) records the empirical frequencies of the transitions: for any pair of states \((i,j)\), \(_{i,j}=_{k=1}^{ T/2 }_{\{(x_{2k}-1,x_{2k})=(i,j)\}}\). \(\) is the best rank-\(r\) approximation of \(\) and the estimate \(\) of the transition matrix \(P\) is obtained normalizing the rows of \(\): for all \(i[n]\),

\[_{i,:}=(_{i,:})_{+}/\|(_{i,:})_ {+}\|_{1},&\|(_{i,:})_{+}\|_{1}>0,\\ _{n},&\|(_{i,:})_{+}\|_{1}=0.\] (1)

where \(()_{+}\) is the function applying \((0,)\) component-wise and \(_{n}\) is the \(n\)-dimensional vector of ones. The next theorem is a simplified version and a consequence of a more general and tighter theorem presented in App. B.2. To simplify the presentation of our results, we define

\[g(M,T,)=n(}{})\{^{}^{ 6}r^{3},}{})_{\{:T M_{ }, 1\}}}{(1+})}\}.\]

**Theorem 3**.: _Let \(>0\). Introduce \(=/T)(n/)}\). Assume that we have \((_{0})_{}=_{i[n]}(_{0})_{i}>0\). If (a) \(n c^{2}(nT^{3/2}/)\) and (b) \(T cg(M,T,)\) for some universal constant \(c>0\), then there exists a universal constant \(C>0\) such that the following inequalities hold with probability at least \(1-\):_

\[(i) \{\|U-(^{}U)\|_{2},\|V -(^{}V)\|_{2}\} Cr}{n\|M\|_{}},\] \[(ii) \|-M\|_{2} C,\ \ \|-P\|_{1} C}{(_{0})_{}} ,\] \[(iii) \|-M\|_{} Cr}{} ,\] \[(iv) \|-P\|_{} C}{(_{0})_{ }}[}{(_{0})_{}}+(1+}{\|M\|_{}}) r}{}],\]

_where (iv) holds if in addition \(T cn\|M\|_{}(_{0})_{}^{-2}r^{2}^{4}(n/)\)_

Note that in theorem, the condition (a) on \(n\) has been introduced just to simplify the expression of \(\) (refer to App. B.2 for a full statement of the theorem without this condition).

**Corollary 4**.: _(Homogeneous transition matrix) When \(=(1)\), \(=(1)\), \(r=(1)\), \(M_{}=(M_{})\), we say that the frequency matrix \(M\) is homogeneous. If \(T cn(nT)\) for some universal constant \(c>0\), then we have with probability at least \(1-\{n^{-2},T^{-1}\}\):_

\[\{\|U-(^{}U)\|_{2}, \|V-(^{}V)\|_{2}\}},\] \[\|-M\|_{2}},\;\|-M\|_{}},\] \[\|-P\|_{1}},\;\|-P\|_{}}.\]

For a homogeneous frequency matrix, \(\|U\|_{2}=(1/)\), \(\|M\|_{2}=(1/n)\), \(\|M\|_{}=(1/n^{2})\), \(\|P\|_{1}=1\), \(\|P\|_{}=(1/n)\). Thus for all these metrics, our estimates achieve a relative error scaling at most as \(\) up to the logarithmic factor.

### Transition matrices under the forward model

For Model II(b), we first split the data into \(\) subsets of transitions: for \(k=1,,\), the \(k\)-th subset is \(((x_{k},x_{k+1}),(x_{k+},x_{k+1+}),,(x_{k+(T_{}-1)},x_{k +1+(T_{}-1)}))\) where \(T_{}= T/\). By separating two transitions in the same subset, we break the inherent correlations in the data if \(\) is large enough. Now we let \(^{(k)}\) be the matrix recording the empirical frequencies of the transitions in the \(k\)-th subset: \(^{(k)}_{i,j}=}_{l=0}^{T_{}-1}_ {\{(x_{k+l},x_{k+1+l})=(i,j)\}}\) for any pair of states \((i,j)\). Let \(^{(k)}\) be the best \(r\)-rank approximation of \(^{(k)}\). As in (1), we define the corresponding \(^{(k)}\). Finally we may aggregate these estimates \(=_{k=1}^{T}^{(k)}\) and \(=_{k=1}^{}^{(k)}\). We present below the performance analysis for the estimates coming from a single subset; the analysis of the aggregate estimates easily follows.

For any \(>0\), we define the \(\)-mixing time of the Markov chain with transition matrix \(P\) as \(()=\{t 1:\;_{1 i n}\|P^{t}_{i, :}-^{}\|_{1}\}\), and its mixing time as \(^{}=(1/4)\). The next theorem is a simplified version and a consequence of a more general and tighter theorem presented in App. B.3. To simplify the presentation, we define:

**Theorem 5**.: _Let \(>0\). Assume that \(_{}=_{i[n]}_{i}>0\) and that \(/(^{}(T_{}^{-1}))[c_{1},c_{2}]\) for some universal constants \(c_{2}>c_{1} 2\). Introduce:_

\[=\|M\|_{}}{T}( }}{})(})}.\]

_If (a) \(n c^{}^{3/2}(nT^{3/2}/)^{1/2}(T_{}^{-1})\) and (b) \(T ch(M,T,)\) for some universal constant \(c>0\), then there exists a universal constant \(C>0\) such that the following inequalities hold with probability at least \(1-\):_

\[(i) \{\|U-(^{}U)\|_{2},\| V-(^{}V)\|_{2}\} Cr}{ \|M\|_{}},\] \[(ii) \|-M\|_{2} C,\;\;\;\| -P\|_{1} C}{_{}},\] \[(iii) \|-M\|_{} Cr}{} ,\] \[(iv) \|-P\|_{} C}{_{}}[ }{_{}}+(1+}{\|M\|_{}})r}{} ],\]

_where (iv) holds if in addition \(T cn\|M\|_{}_{}^{-2}^{}r^{2}^{4}(n/)(T_{}^{-1})\)._

Note that our guarantees hold when \(\) roughly scales as \(^{}(T_{}^{-1})\). Hence to select \(\), one would need an idea of the latter quantity. It can be estimated typically using \(^{}_{}^{-1}\) samples  (whichis small when compared to the constraint \(T ch(M,T,)\) as soon as \(_{}=(1/n)\)). Further observe that in the theorem, the condition (a) can be removed (refer to App. B.3 for a full statement of the theorem without this condition).

**Corollary 6**.: _(Homogeneous transition matrices) Assume that \(M\) is homogeneous (as defined in Corollary 4). Let \(=(Tn)\). If \(T cn^{2}(nT)\) for some universal constant \(c>0\), then we have with probability at least \(1-\{n^{-2},T^{-1}\}\):_

\[\{\|U-(^{}U)\|_{2},\| V-(^{}V)\|_{2}\}}(nT),\] \[\|-M\|_{2}}(nT ),\;\|-M\|_{}}(nT),\] \[\|-P\|_{1}}(nT ),\;\|-P\|_{}}(nT).\]

As for the generative model, for a homogeneous frequency matrix, our estimates achieve a relative error scaling at most as \(\) up to the logarithmic factor for all metrics. Note that up to a logarithmic factor, the upper bound for \(\|-P\|_{1}\) (and similarly for \(\)) matches the minimax lower bound derived in .

### Elements of the proofs

The proofs of the three above theorems share similar arguments. We only describe elements of the proof of Theorem 5, corresponding to the most challenging model. The most difficult result concerns the singular subspace recovery (the upper bounds (i) in our theorems), and it can be decomposed into the following three steps. The first two steps are meant to deal with the Markovian nature of the data. The third step consists in applying a leave-one-out analysis to recover the singular subspaces.

_Step 1: Multinomial approximation of Markovian data._ We treat the matrix \(^{(k)}\) arising from one subset of data, and for simplicity, we remove the superscript \((k)\), i.e., \(=^{(k)}\). Note that \(T_{}\) is a matrix recording the numbers of transitions observed in the data for any pair of states: denote by \(N_{i,j}\) this number for \((i,j)\). We approximate the joint distribution of \(N=(N_{i,j})_{(i,j)}\) by a multinomial distribution with \(n^{2}\) components and parameter \(T_{}M_{i,j}\) for component \((i,j)\). Denote by \(Z=(Z_{i,j})_{(i,j)}\) the corresponding multinomial random variable. Using the mixing property of the Markov chain and the choice of \(\), we establish (see Lemma 21 in App. C) that for any subset \(\) of \(\{z^{n^{2}}:_{(i,j)}z_{i,j}=T_{}\}\), we have \([N] 3[Z]\).

_Step 2: Towards Poisson random matrices with independent entries._ The random matrix \(Z\) does not have independent entries. Independence is however a requirement if we wish to apply the leave-one-out argument. Consider the random matrix \(Y\) whose entries are independent Poisson random variables with mean \(T_{}M_{i,j}\) for the \((i,j)\)-th entry. We establish the following connection between the distribution of \(Z\) and that of \(Y\): for any \(^{n^{2}}\), we have \([Z] e}[Y]\). Refer to Lemma 22 in App. C for details.

_Step 3: The leave-one-out argument for Poisson matrices._ Combining the two first steps provides a connection between the observation matrix \(\) and a Poisson matrix \(Y\) with independent entries. This allows us to apply a leave-one-out analysis to \(\) as if it had independent entries (replacing \(\) by \(Y\)). The analysis starts by applying the standard dilation trick (see Section 4.10 in ) so as to make \(\) symmetric. Then, we can decompose the error \(\|U-(^{}U)\|_{2}\) (see Lemma 32 in App. E) into several terms. The most challenging of these terms is \(\|(M-)(U-(^{}U))\|_{2}=_{ l[n]}\|(M_{l_{:}}-_{l_{:}})(U-(^{}U))\|_{2}\) because of inherent dependence between \(M-\) and \(U-(^{}U)\). The leave-one-out analysis allows us to decouple this statistical dependency. It consists in exploiting the row and column independence of matrix \(\) to approximate \(\|(M_{l_{:}}-_{l_{:}})(U-(^{}U))\|_{2}\) by \(\|(M_{l_{:}}-_{l_{:}})(U-^{(l)}((^{(l)})^{ }U)\|_{2}\) where \(^{(l)}\) is the matrix of eigenvectors of matrix \(^{(l)}\) obtained by zeroing the \(l\)-th row and column of \(\). By construction, \((M_{l_{:}}-_{l_{:}})\) and \(U-^{(l)}((^{(l)})^{}U)\) are independent, which simplifies the analysis. The proof is completed by a further appropriate decomposition of this term, combined with concentration inequalities for random Poisson matrices (see App. D).

## 4 Regret Minimization in Low-Rank Bandits

Consider a low-rank bandit problem with a homogeneous rank-\(r\) reward matrix \(M\). We wish to devise an algorithm \(\) with low regret. \(\) selects in round \(t\) an entry \((i_{t}^{},j_{t}^{})\) based on previous observations, and receives as a feedback the noisy reward \(M_{i_{t}^{},j_{t}^{}}+_{t}\). The regret up to round \(T\) is defined by \(R^{}(T)=TM_{i^{},j^{}}-[_{t=1}^{T}M_{i_{t}^{},j_{ t}^{}}]\), where \((i^{},j^{})\) is an optimal entry. One could think of a simple Explore-Then-Commit (ETC) algorithm, where in the first phase entries are sampled uniformly at random, and where in a second phase, the algorithm always selects the highest entry of \(\) built using the samples gathered in the first phase and obtained by spectral decomposition. When the length of the first phase is \(T^{2/3}(n+m)^{1/3}\), the ETC algorithm would yield a regret upper bounded by \(O(T^{2/3}(n+m)^{1/3})\) for \(T=((n+m)^{3}(n+m))\).

To get better regret guarantees, we present SME-AE (Successive Matrix Estimation and Arm Elimination), an algorithm meant to identify the best entry as quickly as possible with a prescribed level of certainty. After the SME-AE has returned the estimated best entry, we commit and play this entry for the remaining rounds. The pseudo-code of SME-AE is presented in Algorithm 1. The algorithm runs in epochs: in epoch \(\), it samples \(T_{}\) entries uniformly at random among all entries (in \(T_{}\), the constant \(C\) just depends on upper bounds of the parameters \(\), \(\), and \(\|M\|_{}\), refer to App. G); from these samples, a matrix \(^{()}\) is estimated and \(_{}\), the set of candidate arms, is pruned. The pruning procedure is based on the estimated gaps: \(_{i,j}^{()}=_{*}^{()}-_{i,j}^{( )}\) where \(_{*}^{()}=_{i,j}_{i,j}^{()}\).

``` Input: Arms \([m][n]\), confidence level \(\) \(=1\) ; \(_{1}=[m][n]\); while\(|_{}|>1\)do \(_{}=/^{2}\); \(T_{}=[C(2^{+2})^{2}(m+n)^{3}(2^{2+4}(m+n )/_{})]\) ;  Sample uniformly at random \(T_{}\) entries from \(_{1}\): \((M_{i_{t},j_{t}}+_{t})_{t=1,,T_{}}\) ;  Estimate \(^{()}\) via spectral decomposition as described in Section 3.1 ; \(_{+1}=\{(i,j)_{}:_{i,j}^{ ()} 2^{-(+2)}\}\); \(=+1\);  end while Output: Recommend the remaining pair \((i_{},j_{})\) in \(_{}\). ```

**Algorithm 1**Succesive **M**atrix **E**stimation and **A**rm **E**limination (**SME-AE**)

The following theorem characterizes the performance of SME-AE and the resulting regret. To simplify the notation, we introduce the gaps: for any entry \((i,j)\), \(_{i,j}=(M_{i^{},j^{}}-M_{i,j})\), \(_{}=_{(i,j):_{i,j}>0}_{i,j}\), \(_{}=_{(i,j)}_{i,j}\), and \(=_{(i,j)}_{i,j}/(mn)\). We define the function \((n,m,)=)}{_{}^{2}}^{3} ()}{_{}})\) for some universal constant \(c>0\).

**Theorem 7**.: _(Best entry identification) For any \((0,1)\), SME-AE(\(\)) stops at time \(\) and recommends arm \((i_{},j_{})\) with the guarantee \((i_{},j_{})=(i^{},j^{}),(n,m, ) 1-\). Moreover, for any \(T 1\) and \(>0\), the sample complexity \(\) of SME-AE\((1/T^{})\) satisfies \([ T](n,m,T^{-})+T^{1-}\). (Regret) Let \(T 1\). Consider the algorithm \(\) that first runs SME-AE(\(1/T^{2}\)) and then commits to its output \((i_{},j_{})\) after \(\). We have: \(R^{}(T)((n,m,T^{-2})+1)+} {T}\)._

The proof of Theorem 7 is given in App. G. Note that the regret upper bounds hold for any time horizon \(T 1\), and that it scales as \(O((m+n)^{3}(T)/_{}^{2})\) (up to logarithmic factors in \(m,n\) and \(1/_{}\)). The cubic dependence in \(^{3}(T)\) is an artifact of our proof techniques. More precisely, it is due to the Poisson approximation used to obtain entry-wise guarantees. Importantly, for any time horizon, the regret upper bound only depends on \((m+n)\) rather than \(mn\) (the number of arms / entries), and hence, the low-rank structure is efficiently exploited. If we further restrict our attention to problems with gap ratio \(_{}/_{}\) upper bounded by \(\), our regret upper bound becomes \(O((m+n)^{3}(T)/_{})\), and can be transformed into the minimax gap-independent upper bound \(O(((m+n)T)^{1/2}^{2}(T))\), see App. G. Finally note that \((((m+n)T)^{1/2})\) is an obvious minimax regret lower bound for our low-rank bandit problem.

A very similar low-rank bandit problem has been investigated in . There, under similar assumptions (see Assumption 1 and Definition 1), the authors devise an algorithm with both gap-dependent and gap-independent regret guarantees. The latter are difficult to compare with ours. Their guarantees exhibit a better dependence in \(T\) and \(_{}\), but worse in the matrix dimensions \(n\) and \(m\). Indeed in our model, \(b^{*}\) in  corresponds to \(\|M\|_{2}\) and scales as \(\). As a consequence, the upper bounds in  have a dependence in \(n\) and \(m\) scaling as \((n+m)\) in the worst case for gap-dependent guarantees and even \(nm\) (through the constant \(C_{2}\) in ) for gap-independent guarantees.

## 5 Representation Learning in Low-Rank MDPs

The results derived for Models II(a) and II(b) are instrumental towards representation learning and hence towards model-based or reward-free RL in low-rank MDPs. In this section, we provide an example of application of these results, and mention other examples in Section 7. A low-rank MDP is defined by \((,,\{P^{a}\}_{a},R,)\) where \(\), \(\) denote state and action spaces of cardinalities \(n\) and \(A\), respectively, \(P^{a}\) denotes the rank-\(r\) transition matrix when taking action \(a\), \(R\) is the reward function, and \(\) is the discount factor. We assume that all rewards are in \(\). The value function of a policy \(:\) is defined as \(V^{}_{R}(x)=[_{t=1}^{}^{t-1}R(x_{t}^{},_{t} (x_{t}^{}))|x_{t}^{}=x]\) where \(x_{t}^{}\) is the state visited under \(\) in round \(t\). We denote by \(^{}(R)\) an optimal policy (i.e., with the highest value function).

**Reward-free RL.** In the reward-free RL setting (see e.g. [36; 30; 66]), the learner does not receive any reward signal during the exploration process. The latter is only used to construct estimates \(\{^{a}\}_{a}\) of \(\{P^{a}\}_{a}\). The reward function \(R\) is revealed at the end, and the learner may compute \((R)\) an optimal policy for the MDP \((,,\{^{a}\}_{a},R,)\). The performance of this model-based approach is often assessed through \(=_{R}\|V^{^{}(R)}_{R}-V^{(R)}_{R}\|_{}\). In tabular MDP, to identify an \(\)-optimal policy for all reward functions, i.e., to ensure that \(\), we believe that the number of samples that have to be collected should be \((()A}{^{2}})\) (the exact degree of the polynomial in \(1/(1-)\) has to be determined). This conjecture is based on the sample complexity lower bounds derived for reward-free RL in episodic tabular MDP [30; 43]. Now for low-rank MDPs, the equivalent lower bound would be \((()})\) (this minimax lower bound is valid for Block MDPs, a particular case of low-rank MDPs).

Leveraging our low-rank matrix estimation guarantees, we propose an algorithm matching the aforementioned sample complexity lower bound (up to logarithmic factors) at least when the frequency matrices \(\{M^{a}\}_{a}\) are homogeneous. The algorithm consists of two phases: (1) in the model estimation phase, it collects \(A\) trajectories, each of length \(T/A\), corresponding to the Markov chains with transition matrices \(\{P^{a}\}_{a}\). From this data, it uses the spectral decomposition method described in SS3 to build estimates \(\{^{a}\}_{a}\). (2) In the planning phase, based on the reward function \(R\), it computes the best policy \((R)\) for the MDP \((,,\{^{a}\}_{a},R,)\). The following theorem summarizes the performance of this algorithm. To simplify the presentation, we only provide the performance guarantees of the algorithm for homogeneous transition matrices (guarantees for more general matrices can be derived plugging in the results from Theorem 5).

**Theorem 8**.: _Assume that for any \(a\), \(M^{a}\) is homogeneous (as defined in Corollary 4). If \(T cnA^{2}(nAT)\) for some universal constant \(c>0\), then we have with probability at least \(1-\{n^{-2},T^{-1}\}\): \(=_{R}\|V^{^{}(R)}_{R}-V^{(R)}_{R}\|_{} }}(nAT)\)._

Theorem 8 is a direct consequence of Corollary 6 and of the fact that for any reward function \(R\): \(\|V^{^{}(R)}_{R}-V^{(R)}_{R}\|_{}}_{a}\|P^{a}-^{}\|_{1}\), see App. A. The theorem implies that if we wish to guarantee \(\), we just need to collect \(O((1-)^{4}})\) samples up to a logarithmic factor.

This sample complexity is minimax optimal in \(n\), \(A\), and \(\) in view of the lower bound presented in .

## 6 Related Work

**Low-rank matrix estimation.** Until recently, the main efforts on low-rank matrix recovery were focused on guarantees w.r.t. the spectral or Frobenius norms, see e.g.  and references therein. The first matrix estimation and subspace recovery guarantees in \(_{2}\) and \(_{}\) were established in ,  via a more involved perturbation analysis than the classical Davis-Kahan bound. An alternative approach based on a leave-one-out analysis was proposed in , and further refined in [10; 12; 17], see  for a survey. Some work have also adapted the techniques beyond the independent noise assumption [39; 1; 5], but for very specific structural dependence. We deal with a stronger dependence, and in particular with Markovian data (an important scenario in RL).

The estimation of low-rank transition matrices of Markov chains has been studied in [63; 9] using spectral methods and in [40; 67] using maximum-likelihood approaches.  does not conduct any fine-grained subspace recovery analysis (such as the leave-one-out), and hence the results pertaining to the \(\|\|_{1}\)-guarantees are questionable; refer to App. H for a detailed justification. All these papers do not present entry-wise guarantees.

It is worth mentioning that there exist other methods for matrix estimation that do not rely on spectral decompositions like ours, yet enjoy entry-wise matrix estimation guarantees [51; 3; 50]. However, these methods require different assumptions than ours that may be too strong for our purposes, notably having access to the so-called anchor rows and columns. Moreover, we do not know if these methods also lead to guarantees for subspace recovery in the norm \(\|\|_{2}\), nor how to extend those results to settings with dependent noise.

**Low-rank bandits.** Low-rank structure in bandits has received a lot of attention recently [35; 37; 32; 57; 41; 7; 34; 27]. Different set-ups have been proposed (refer to App. H for a detailed exposition, in particular, we discuss how the settings proposed in [32; 7] are equivalent), and regret guarantees in an instance dependent and minimax sense have been both established.

Typically minimax regret guarantees in bandits scale as \(\), but the scaling in dimension may defer when dealing with a low rank structure [32; 34; 7]. In , the authors also leverage spectral methods. They reduce the problem to a linear bandit of dimension \(nm\) but where only roughly \(n+m\) dimensions are relevant. This entails that a regret lower bound of order \((n+m)\) is inevitable. Actually, in their reduction to linear bandits, they only use a subspace recovery in Frobenius norm, which perhaps explains the scaling \((n+m)^{3/2}\) in their regret guarantees. It is worth noting that in , the authors manage to improve upon the work  and obtain a scaling order \((m+n)\) in the regret. Our algorithm leverages entry-wise guarantees which rely on a stronger subspace recovery guarantee. This allows us to obtain a scaling \(\) in the regret. The work of  is yet another closely related work to ours. There, the authors propose an algorithm achieving a regret of order \((n+m)\) for a contextual bandit problem with low rank structure. However, their result only holds for rank 1 and their observation setup is different than ours because in their setting, the learner observes \(m\) entries per round while in ours the learner only observes one entry per round. In , the authors use matrix estimation with nuclear norm penalization to estimate the matrix \(M\). Their regret guarantees are already discussed in SS4.

Some instance-dependent guarantees with logarithmic regret for low rank bandits have been established in [35; 37; 57]. However, these results suffer what may be qualified as serious limitations. Indeed, [35; 57] provide instance dependent regret guarantees but only consider low-rank bandits with rank \(1\), and the regret bounds of  are expressed in terms of the so-called column and row gaps (see their Theorem 1) which are distinct from the standard gap notions.  extend the results in  to rank \(r\) with the limitation that they require stronger assumptions than ours. Moreover, the computational complexity of their algorithm depends exponentially on the rank \(r\); they require a search over spaces of size \(\) and \(\). Our proposed algorithm does not suffer from such limitations.

We wish to highlight that our entry-wise guarantees for matrix estimation are the key enabling tool that led us to the design and analysis of our proposed algorithm. In fact, the need for such guarantees arises naturally in the analysis of gap-dependent regret bounds (see Appendix G.1). Therefore, we believe that such guarantees can pave the way towards better, faster, and efficient algorithms for bandits with low-rank structure.

**Low-rank Reinforcement Learning.** RL with low rank structure has been recently extensively studied but always in the function approximation framework [29; 18; 20; 44; 24; 65; 56; 4; 46; 59; 60; 49]. There, the transition probabilities can be written as \((x,a)^{}(x^{})\) where the unknown feature functions \((x,a),(x^{})^{r}\) belong to some specific class \(\) of functions. The major issue with algorithms proposed in this literature is that they rely on strong computational oracles (e.g., ERM, MLE), see [33; 25; 64] for detailed discussions. In contrast, we do not assume that the transition matrices are constructed based on a given restricted class of functions, and our algorithms do not rely on any oracle and are computationally efficient. In [51; 50], the authors also depart from the function approximation framework. There, they consider a low rank structure different than ours. Their matrix estimation method enjoys an entry-wise guarantee, but requires to identify a subset of rows and columns spanning the range of the full matrix. Moreover, their results are only limited the generative models, which allows to actually rely on independent data samples.

## 7 Conclusion and Perspectives

In this paper, we have established that spectral methods efficiently recover low-rank matrices even in correlated noise. We have investigated noise correlations that naturally arise in RL, and have managed to prove that spectral methods yield nearly-minimal entry-wise error. Our results for low-rank matrix estimation have been applied to design efficient algorithms in low-rank RL problems and to analyze their performance. We believe that these results may find many more applications in low-rank RL. They can be applied (i) to reward-free RL in episodic MDPs (this setting is easier than that presented in SS5 since successive episodes are independent); (ii) to scenarios corresponding to offline RL  where the data consists of a single trajectory generated under a given behavior policy (from this data, we can extract the transitions \((x,a,x^{})\) where a given action \(a\) is involved and apply the spectral method to learn \(^{a}\)); (iii) to traditional RL where the reward function \(R\) has to be learnt (learning \(R\) is a problem that lies in some sense between the inference problems in our Models I and II); (iv) to model-free RL where we would directly learn the \(Q\) function as done in  under a generative model; (v) to low-rank RL problems with continuous state spaces (this can be done if the transition probabilities are smooth in the states, and by combining our methods to an appropriate discretization of the state space).