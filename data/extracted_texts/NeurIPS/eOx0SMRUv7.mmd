# Online Consistency of the Nearest Neighbor Rule

Sanjoy Dasgupta

Department of Computer Science

UC San Diego

La Jolla, CA 92023

dasgupta@ucsd.edu

&Geelon So

Department of Computer Science

UC San Diego

La Jolla, CA 92023

geelon@ucsd.edu

###### Abstract

In the realizable online setting, a learner is tasked with making predictions for a stream of instances, where the correct answer is revealed after each prediction. A learning rule is _online consistent_ if its mistake rate eventually vanishes. The nearest neighbor rule (Fix and Hodges, 1951) is a fundamental prediction strategy, but it is only known to be consistent under strong statistical or geometric assumptions--the instances come i.i.d. or the label classes are well-separated. We prove online consistency for all measurable functions in doubling metric spaces under the mild assumption that the instances are generated by a process that is _uniformly absolutely continuous_ with respect to a finite, upper doubling measure.

## 1 Introduction

In online classification, a learner faces a never-ending stream of prediction tasks. For all times \(n\):

* the learner is presented with an instance \(X_{n}\),
* the learner makes a prediction \(_{n}\),
* the ground-truth label \(Y_{n}\) is revealed.

When the instances come from some underlying metric space, one of the simplest prediction rules the learner can employ is the _nearest neighbor rule_(Fix and Hodges, 1951). This learner memorizes everything it sees, and when it comes time to make a prediction for a new instance \(X_{n}\), it looks for the most similar data point in memory, predicting with that nearest neighbor's label. In this work, we are interested in simple conditions under which the nearest neighbor rule is in fact a reasonable strategy, where the rate at which the learner makes mistakes eventually vanishes.

Let \((,,)\) be a metric measure space where \(\) is a separable metric and \(\) is a finite Borel measure. We study the _realizable_ setting, in which the ground-truth labels \(Y_{n}=(X_{n})\) are given by some measurable label function \(:\). Let \(=(X_{n})_{n 0}\) be any stochastic process. It induces a _nearest neighbor process_\(}=()_{n>0}\), which is any process satisfying:

\[_{n}*{arg\,min}_{x_{<n}}\,(X_{n},x).\]

The nearest neighbor rule predicts using the label \(_{n}=(_{n})\), and it is **online consistent** when the asymptotic mistake rate on the problem instance \((,)\) goes to zero:

\[_{N}\,_{n=1}^{N}(X_{n} )(_{n})}=0\] (1)

There are two representative results for the online consistency of the 1-nearest neighbor rule in this setting--both impose strong constraints on either \(\) or \(\). Cover and Hart (1967) assume thestatistical constraint that \(\) is an i.i.d. process. In contrast, Kulkarni and Posner (1995) allow for arbitrary processes. However, they assume the geometric constraint that points of different classes \((x)(x^{})\) are uniformly separated \((x,x^{})>c>0\) in a totally bounded space.

The result of Kulkarni and Posner (1995) turns out to be tight in the absence of further assumptions. As long as there are points belonging to different classes that are arbitrarily close together, then there are adversarial sequences on which the nearest neighbor rule is not online consistent (Proposition 2). Still, this negative result does not necessarily spell doom for the nearest neighbor rule in all non-i.i.d. settings; one of our aims is to understand just how pathological these worst-case sequences are.

The upshot of this work is that worst-case sequences on which the nearest neighbor rule fails to learn are in fact extremely rare--under quite mild constraints on \(\) and \(\), they almost never occur. The nearest neighbor rule is online consistent under much broader conditions than previously known.

### Main results

Consistency for functions with negligible boundaryWe first consider learning label functions with _negligible boundary_, where almost every point has a positive separation from other classes. As the separation may be instance-dependent, this relaxes the uniform separation condition of Kulkarni and Posner (1995), and it is equivalent to the assumption used by Cover and Hart (1967).

It turns out that if the label function has negligible boundary, we can cover essentially all of \(\) with _mutually-labeling_ balls (Definition \(8\)). On such a ball, the nearest neighbor rule makes at most one mistake (Lemma \(9\)). So, progress is monotonic: eventually, all mistakes must come from a remainder region with arbitrarily small \(\)-mass (roughly, points arbitrarily close to the decision boundary).

It follows that if we can limit the rate at which \(\) comes from regions with arbitrarily small mass, we can also limit the mistake rate of the nearest neighbor rule. To this end, we formalize the notion of an _ergodically dominated_ process (Definition \(3\)), which is a process where the asymptotic rate of landing in a region \(A\) is bounded as a function of \((A)\). In particular, these processes do not hit regions with arbitrarily small mass at a constant rate. With little ado, we can show that the nearest neighbor rule is consistent when \(\) is ergodically dominated and \(\) has negligible boundary (Theorem \(7\)). Stronger, quantitative assumptions also yield rates of convergence (Theorem F.2).

Universal consistencyThis first consistency result for functions with negligible boundaries is quite general and captures many settings of interest (e.g. classification on \(^{d}\) with smooth decision boundaries). But as not all functions have \(\)-negligible boundaries, we also study when the nearest neighbor rule is _universally consistent_ (i.e. consistent for any measurable \(\)). This question is trickier since boundary points, which are hard for our learner, need not be localized to a set of measure zero.

We proceed by giving more structure to \((,,)\) and \(\). First, we let \(\) be a \(d\)-doubling metric (Definition \(11\)). This is helpful because every measurable function \(\) can then be approximated arbitrarily well by a function \(^{}\) with negligible boundary (Proposition \(13\)). And so, it seems that we might be able to deduce universal consistency of the nearest neighbor rule almost directly from the previous result--learning \(\) is perhaps not so different from learning \(^{}\) when their disagreement region is made to be vanishingly small. However, this turns out not to be the case.

For example, Blanchard (2022) constructs a classification problem where the nearest neighbor rule is not consistent, but \(\) is a \(1\)-doubling space (the unit interval \(\) with the usual metric), \(\) is measurable, and \(\) is ergodically dominated. The problem is that, even if the disagreement region is made to be extremely small, its influence on nearest neighbor predictions is not limited to the times when instances land in it. These instances exert influence when they themselves are nearest neighbors of downstream instances. In other words, 'bad points' can accumulate in the memory of the nearest neighbor learner, and their influence grow and shrink with their Voronoi cells (regions where they are nearest neighbors). As the region on which the nearest neighbor learner is prone to make mistakes waves and wanes throughout time, we cannot argue that progress is monotonic in the same way as before. In short, a tail constraint on \(\) is no longer sufficient for consistency.

To show universal consistency, we constrain \(\) at every moment in time. Ergodic domination only ensured that the _time-averaged_ rate at which \(\) hits small regions is small. We now require the _time-uniform_ rate to also be small, a strictly stronger condition. Formally, a _uniformly dominated_ process (Definition 4) is one where the probability that the instance \(X_{n}\) lands in a region \(A\) is bounded as a function of \((A)\) at each point in time. Intuitively, ergodic domination is retrospective: looking back, how often do points land in \(A\)? In contrast, uniform domination is a generative constraint: at any point in time, how easily can the underlying mechanism generating \(\) select a point in \(A\)?

The basic argument then is that even though 'bad points' can accumulate in space over time, in a doubling space, their Voronoi cells also tend to shrink quickly when they are hit by further instances. If the mass of these Voronoi cells were to shrink as well, then it would become increasingly unlikely that these instances are nearest neighbors of downstream points whenever \(\) is uniformly dominated. So that having small metric entropy implies having small mass, we let the measure be _upper doubling_ (Definition 11), by which we mean that the mass of a ball of radius \(r\) is bounded by \(O(r^{d})\).

We first prove the following result, which may be of interest in its own right, about the behavior of nearest neighbor processes: when \(\) is a uniformly dominated process in an upper doubling space, any nearest neighbor process \(}\) is ergodically dominated (Theorem 14). Simply put, this means that if two functions \(\) and \(^{}\) rarely disagree, then the average rate at which nearest neighbor processes land in their disagreement region is also bounded. Universal consistency now follows fairly easily.

Let \(\) be closely approximated by some \(^{}\) with negligible boundary. The asymptotic mistake rate of the nearest neighbor rule on \(^{}\) is zero when \(\) is uniformly dominated. On the other hand, if the mistake rate on \(\) is large, this discrepancy must be due to the influence of their (very small) disagreement region. But, a large discrepancy is not possible as the nearest neighbor process is unable to significantly amplify the influence of very small regions. Thus, the nearest neighbor rule is universally consistent on upper doubling spaces for uniformly dominated processes (Theorem 12).

NotationGiven a sequence \(\), we let \(_{<n}\) denote the set \(\{X_{1},,X_{n-1}\}\). The symbol \(\) denotes the indicator function, which is equal to 1 when the event that follows it occurs and 0 otherwise. We let \(B(x,r)\) denote the open ball of radius \(r\) centered at \(x\). For any \(x\) and \(Z\), let:

\[(x,Z)=_{z Z}\ (x,z)(Z) =_{z,z^{} Z}(z,z^{}).\]

## 2 Non-convergence for worst-case sequences

To motivate the study of non-worst case sequences, let's first consider a worst-case example showing that the nearest neighbor rule can make a mistake in each round learning a threshold function:

**Example 1** (Failing to learn a threshold).: _Let \(=[-1,1]\) and let \((x)=\{x 0\}\) be a threshold function. Let \(\) be defined by \(X_{n}=(-1/3)^{n}\). The nearest neighbor rule makes a mistake every round \(n+1\): the nearest neighbor of \(X_{n+1}\) is \(X_{n}\), which has the opposite sign (see Figure 1)._

More generally, the hardness of a point for the nearest neighbor rule depends on its separation from points of different classes--hard sequences exist precisely whenever the classes are not separated:

**Proposition 2** (Non-convergence in the worst-case).: _Let \((,)\) be a totally bounded metric space. Given \(:\), there is a sequence of instances \((X_{n})_{n}\) on which the nearest neighbor rule is not online consistent on \(\) if and only if there is no positive separation between classes:_

\[_{(x)(x^{})}\ (x,x^{})=0.\]

While the nearest neighbor rule can fail to learn many functions of interest, in both the example and the proof of the proposition, the mode of failure depended on the ability of a worst-case adversary to select instances with arbitrary precision. This can be seen in the threshold example, where the interval on which the learner can make a mistake shrinks exponentially quickly.

However, this mode of failure may not always be present, especially when the ability of the adversary to select instances from small regions of space diminishes with the sizes of those regions. This may be the case if the adversary is limited in computation, information, or ill-will toward the learner. The question remains: what is the behavior of the nearest neighbor rule in such non-worst-case settings?

## 3 Two general classes of non-worst case sequences

To study this, let's formalize the generating process. At each time step \(n\), conditioned on the past outcomes \(_{<n}\), an adaptive adversary constructs a distribution over \(\) from which \(X_{n}\) is drawn. In this way, any particular choice of adaptive adversary corresponds to a stochastic process, which defines a probability measure over the space of all sequences of instances. We are interested in the almost-sure online consistency of the nearest neighbor rule under these measures.

In the standard i.i.d. setting, each \(X_{n}\) is drawn from the same underlying distribution. In the worst-case setting, the conditional distribution of \(X_{n}|\,_{<n}\) may be point masses and so \(\) may be an arbitrary sequence. We introduce two mildly-constrained classes of non-worst-case processes. Both are given with respect to an underlying reference measure \(\) (which we assumed to be finite).

Ergodically dominated processesThe first can be considered a class of _budgeted adversaries_. For any region \(A\), the asymptotic rate at which the adversary can select points in \(A\) is bounded by a function \((\,\,)\) of \((A)\). In particular, we require \(() 0\) as \(\) goes to zero. Instances from an ergodically dominated process do not concentrate in regions with small mass in retrospect.

**Definition 3** (Ergodic continuity).: A stochastic process \(\) is _ergodically dominated_ by \(\) if for any \(>0\), there exists \(>0\) such that when a measurable set \(A\) satisfies \((A)<\), then:

\[_{N}\,_{n=1}^{N}X_{n} A }<\] (2)

We say that \(\) is _ergodically continuous_ with respect to \(\) at rate \(()\).

As pointed out by Hanneke (2021), the set function \(A_{N}_{n=1}^{N}\{X_{n} A\}\) is a submeasure. Then, ergodic continuity requires it to be _absolutely continuous_ with respect to \(\). These processes are closely related to the \(_{1}\)-processes introduced by Hanneke (2021). There, the submeasures must be _exhaustive_, which Talagrand (2008) shows to be a strictly weaker condition than absolute continuity. Hanneke (2021) also demonstrates that there are \(_{1}\)-processes on the unit interval for which the nearest neighbor rule is not universally online consistent.

Uniformly dominated processesThe following can be thought of as a class of _bounded precision_ adversaries. Each time step, the probability that the adversary selects an instance from a region \(A\) is bounded by a function \((\,\,)\) of \((A)\). Thus, it provides a time-uniform condition:

**Definition 4** (Uniform absolute continuity).: A stochastic process \(\) is _uniformly dominated_ by \(\) if for any \(>0\), there exists \(>0\) such that when a measurable set \(A\) satisfies \((A)<\), then:

\[_{n}\,X_{n} A\,|\,_{<n}<.\] (3)

We say that \(\) is _uniformly absolutely continuous_ with respect to \(\) at rate \(()\).

Figure 1: Learning the threshold \(\{x 0\}\) on \(\). The nearest neighbor classifier makes a mistake every single round on the sequence \(X_{n}=(-1/3)^{n}\), where subsequent test points alternate sign.

This class can be seen as a strict generalization of the _\(\)-smoothed processes_ introduced by Haghtalab et al. (2020), which satisfy the Lipschitz rate \(()=/\) (Definition F.3). This stronger, quantitative condition allows us to give rates of convergence for smoothed processes in Appendix F.

Time-averaged behavior of uniformly dominated processesWe now show that ergodic continuity is a weaker condition than uniform absolute continuity. And intuitively, the former lets us prove consistency when the hard or atypical instances come from a small, fixed region of space. The latter will be needed when the hard regions of space evolve over time.

In the following, we can think of \((A_{n})_{n}\) as a sequence of hard regions (e.g. the region on which the learner can make mistakes). By the martingale law of large numbers, if the mass of these regions eventually remain small, then the average rate at which \(\) lands in hard regions also becomes small:

**Lemma 5**.: _Let \(\) be uniformly dominated by \(\) at rate \(()\), and let \((_{n})_{n}\) be its natural filtration. Let \(A_{n}\) be an \(_{n}\)-predictable sequence where \(_{n}(A_{n})<\) almost surely. Then:_

\[_{N}\,_{n=1}^{N}X_{n} A_ {n}}()\]

Setting \(A_{n}\) to \(A\) shows that ergodic continuity is weaker than uniform absolute continuity. In fact, as it only constrains the tail of \(\), it is strictly weaker (the ergodically-dominated adversary is stronger).

## 4 Consistency for functions with negligible boundaries

The inductive bias built into the nearest neighbor rule is that most points are surrounded by other points of the same class (though one might have to zoom in very close to the point). We first consider label functions for which this inductive bias is correct \(\)-almost everywhere. Ergodic continuity with respect to \(\) shall be enough for online consistency. To formalize these functions, we define:

**Definition 6** (Boundary point).: Let \(:\) be measurable. Let \(*{margin}_{}(x)\) be the distance from \(x\) to points of other classes, and let \(_{}\) denote the _boundary_ of \(\), points with no margin:

\[*{margin}_{}(x)=_{(x)(x^{})}\,(x, x^{})_{}=x :*{margin}_{}(x)=0}.\]

Let \(_{0}=(_{} )=0}\) denote the set of _functions with negligible boundaries_.

The class \(_{0}\) captures many label functions of interest. For example, when \(\) is a Euclidean space equipped with the Lebesgue measure, then label functions with smooth decision boundaries have negligible boundaries--the boundary forms a lower-dimensional manifold with zero measure.

**Theorem 7** (Online consistency for \(_{0}\)).: _Let \((,,)\) be a metric measure space, where \(\) is a separable metric and \(\) is a finite Borel measure. Let \(\) be ergodically dominated by \(\) and let \(\) have \(\)-negligible boundary. The nearest neighbor rule is online consistent with respect to \((,)\)._

To prove this, we introduce the notion of a _mutually-labeling set_, which are subsets \(U\) of \(\) that share a single label under \(\), and whose diameter is less than the distance to reach a different class:

**Definition 8** (Mutually-labeling set).: A set \(U\) is _mutually-labeling_ for \(\) if for all \(x U\),

\[*{diam}(U)<*{margin}_{}(x).\] (4)

See Figure 2 for a picture. This construct is useful because the nearest neighbor rule makes at most one mistake per mutually-labeling set--see Lemma 9. Moreover, it is easy to construct such sets; Lemma 10 shows that sufficiently small balls centered at non-boundary points are mutually-labeling.

**Lemma 9**.: _Let \(U\) be a mutually-labeling set for \(\). Let \(\) be an arbitrary process. Then:_

\[_{n=1}^{}X_{n} U\,(X_{n}) (_{n})} 1.\]

**Lemma 10**.: _For any \(0<r<*{margin}_{}(x)/3\), the ball \(B(x,r)\) is mutually-labeling for \(\)._Proof of Theorem 7.: Fix \(>0\). We first prove that the asymptotic mistake rate is bounded by \(\). Choose \(>0\) such that the ergodic domination condition, Equation (2), holds. We claim that we can cover all of \(\) by a finite number \(K_{}<\) of mutually-labeling sets, except for a region \(A_{}\) of small mass \((A_{})<\). By Lemma 9, at most one mistake can be made on each of the mutually-labeling set, so that all but finitely many mistakes come from \(A_{}\). Thus, the asymptotic mistake rate is bounded by the rate at which \(\) can hit \(A_{}\). By definition of ergodic continuity, this is less than \(\),

\[_{N}\,_{n=1}^{N}(X_{n}) (_{n})}_{N}\,_{n=1 }^{N}X_{n} A_{}}<\]

Online consistency follows by applying this simultaneously to a countable sequence of \(_{i} 0\).

To finish the proof, we construct the above collection of mutually-labeling sets. By Lemma 10, we can cover almost all of \(\) by the collection of mutually-labeling balls, since we only miss out on the boundary points, which is \(\)-negligible. This collection has a countable subcover \(=\{B_{1},B_{2},\}\) because \(\) is separable. By the finiteness of \(\) and the continuity of measures, when \(K_{}\) is sufficiently large, the first \(K_{}\) balls cover everything but a region \(A_{}\) of mass \((A_{})<\),

\[A_{}=_{k K_{}}B_{k}.\]

A picture is also given in Figure 2. 

## 5 Universal consistency on upper doubling spaces

We now show that the nearest neighbor rule is _universally online consistent_ (that is, consistent for any measurable function) whenever \(\) is uniformly dominated and \(\) is an _upper doubling_ space:

**Definition 11** (Upper doubling).: A metric space \((,)\) is _doubling_ with doubling dimension \(d\) if every ball \(B(x,r)\) can be covered by \(2^{d}\) balls of radii \(r/2\). A \(d\)-doubling space with measure \(\) is _upper doubling_ if there exists \(c>0\) such that for all \(B(x,r)\), we have \(B(x,r) cr^{d}\).

This notion was introduced under a somewhat more general form by Hytonen (2010), and it relaxes the condition of a doubling measure space. For example, \(^{d}\) with the \(_{}\)-distance and Lebesgue measure is readily seen to be upper doubling with doubling dimension \(d\).

**Theorem 12** (Universal consistency).: _Let \((,,)\) be an upper doubling metric measure space, where \(\) is a separable metric and \(\) is a finite Borel measure. Let \(\) be uniformly dominated by \(\). For any measurable \(\), the nearest neighbor rule is online consistent with respect to \((,)\)._

The doubling metric condition is helpful because the set \(_{0}\) of functions with negligible boundaries is then dense in \(L^{1}(;)\), as shown in Proposition 13. That is, any measurable \(\) can be arbitrarily well-approximated by \(_{0}\), for which we know the nearest neighbor rule is consistent.

**Proposition 13** (\(_{0}\) is dense in \(L^{1}\)).: _Let \((,,)\) be a metric measure spaces where \(\) is doubling and \(\) is a finite Borel measure. Then, the set \(_{0}\) is dense in \(L^{1}(,)\)._

To show universal consistency, it is not enough that \(\) can be well-approximated by a function \(_{0}\) with negligible boundary. We also need to know that their disagreement region \(\{_{0}\}\) cannot have excessive influence on the behavior of nearest neighbor predictions. The upper doubling condition allows us to show that when \(\) is uniformly dominated, then \(}\) is ergodically dominated. We will use this to limit the rate that nearest neighbors come from regions where \(\) is poorly approximated.

**Theorem 14** (Ergodic continuity of nearest neighbor processes).: _Let \((,,)\) be a upper doubling space with bounded diameter. Suppose that a process \(\) is uniformly dominated by \(\) at a rate \(()\). There exists constants \(c_{1},c_{2}>0\) such that for any measurable set \(A\) with \((A)<_{0}\),_

\[_{N}\,_{n=1}^{N}_{ n} A}<_{>0}\,\{(c_{1}+c_{2} )(_{0})+()\}\]

If we do not optimize the bound and just let \(=_{0}\), this shows that when \(\) is uniformly dominated at rate \(()\), then \(}\) is ergodically dominated at a slower rate \(O(())\). We can now show:Proof of Theorem 12.: Fix \(>0\). Let \(\) be uniformly dominated and let \(\) be measurable. We prove that the asymptotic mistake rate of the nearest neighbor rule for \((,)\) is bounded by \(3\) almost surely. The result follows by simultaneously applying this to a sequence \(_{i} 0\).

Let \(_{0}_{0}\) be a \(_{0}\)-accurate approximation of \(\) so that \((\{_{0}\})<_{0}\). We will set \(_{0}\) later. If at time \(n\), the nearest neighbor rule makes a mistake, then at least one of three events must occur:

1. The functions \(\) and \(_{0}\) disagree on \(X_{n}\), \((X_{n})\)\(}\)\((_{n})\)
2. The functions \(\) and \(_{0}\) disagree on \(_{n}\), (a)\(}\)\(}\)
3. The nearest neighbor rule errs at time \(n\) on \((,_{0})\). \(_{0}(X_{n})\)\(}\)\(_{0}(_{n})\)

Lemma 5 implies that (a) contributes at most \((_{0})\) to the asymptotic mistake rate, while Theorem 7 implies that (c) contributes nothing. We just need to bound the contribution of (b) for when the nearest neighbor process lands in the disagreement region of \(\) and \(_{0}\). For this, we can almost directly apply Theorem 14, except that it assumes that the process takes place in a bounded space.

It turns out that because \(\) is doubling and \(\) is finite, there is a bounded region \(_{}\) that captures the vast majority of \(\) and \(}\); Lemma D.4 bounds the rate at which either process escapes \(_{}\),

1. \(_{N}\;_{n=1}^{N}X_{n} _{}_{n}_{}}< \)

Having accounted for mistakes outside of \(_{}\), we can consider the amended event (b\({}^{}\)) that \(\) and \(_{0}\) disagree on \(A=_{}\{_{0}\}\). Since \((A)<_{0}\), we now apply Theorem 14; when \(c_{1}\) and \(c_{2}\) are the corresponding constants given by for the space \((_{},,)\), it suffices to set \(_{0}>0\) so that:

\[_{>0}\;\{(c_{1}+c_{2}) (_{0})+()\}<.\]

Then, to the asymptotic mistake rate, the events in (a) contribute at most \((_{0})<\), (b\({}^{}\)) contribute another \(\), (c) contribute nothing, and (d) contribute \(\). Together, they yield the target \(3\) bound. 

## 6 Ergodic continuity of nearest neighbor processes

The nearest neighbor rule does not forget; and so, a data point \(X_{n}\) can be the nearest neighbor of an unbounded number of downstream instances \(_{>n}\). In this section, we ask a trickier question: at what rate can a set of instances in \(\) contain nearest neighbors of downstream points? More intuitively, how much influence can a set of instances exert through the nearest neighbor process?

Theorem 14 gives one result of this form: informally, if a process \(\) can generate points from small regions only very rarely, then these points cannot make up a significantly larger fraction of \(}\). More generally, we consider the long-term influence of any _asymptotically rate-limited subsequence_ of \(\). Formally, to indicate the instances whose long-term influence we wish to bound, we define:

**Definition 15** (Indicator process).: An _indicator process_\(=(I_{n})_{n}\) is a sequence of \(\{0,1\}\)-random variables. It induces a _counter_\(k(n)\) and the sequence of _stopping times_\((_{k})_{k}\) where:

\[k(n)=I_{1}++I_{n}_{k}=n:k(n) k }.\]

That is, \(k(n)\) is the number of indications given by time \(n\), while \(_{k}\) is the time of the \(k\)th indication. We say that \(\) is _asymptotically rate-limited_ by \(>0\) if almost surely, \(_{n}k(n)/n<\).

**Notation 16** (Indicated instances).: Let \(\) be a process and \(\) be an indicator process. Let \(_{<n}\) denote the subset of instances in \(\) indicated by time \(n\) (not inclusive), so that:

\[_{<n}:=X_{m}:m<nI_{m}=1}.\] (5)

For uniformly dominated processes in upper doubling spaces, if this set of instances doesn't grow too fast, then its time-averaged influence is limited when filtered through the nearest neighbor process. For simplicity, in this section, we will also assume that the space is bounded.

**Theorem 17** (Long-term influence bound).: _Let \((,,)\) be a bounded, upper doubling space. There are constants \(c_{1},c_{2}>0\) so that the following holds. Let \(\) be uniformly dominated at rate \(()\) and let \(\) be an indicator process adapted to \(\) asymptotically rate-limited by \(>0\). For any \(>0\), the rate that the indicated instances \(_{<n}\) contain a nearest neighbor \(_{n}\) is at most:_

\[_{N}\,_{n=1}^{N}\{_{n} _{<n}\}<(c_{1}+c_{2 })+()\]

The ergodic continuity of \(}\) follows when we take \(I_{n}\) to be \(\{X_{n} A\}\) and optimize the bound.

### A metric bound for nearest neighbor events

To prove Theorem 17, we need to balance two opposing dynamics. On one hand, more and more indicated instances fill the space as time goes on. On the other, the Voronoi cells of these instances--regions in which they are nearest neighbors--tend to shrink as they are hit. Mathematically, this can be seen by decomposing the event that an indicated instance is a nearest neighbor like so:

\[\{_{n}_{<n}\}\;= _{x[_{<n}]}\{X_{n}_{<n}$}\}.\] (6)

The natural proof strategy following this would be to bound the probability that the left-hand side event occurs by arguing that the indicated Voronoi cells have small \(\)-mass, so that these cells would be rarely hit if \(\) is uniformly dominated. However, this decomposition does not seem to be very fruitful as it is difficult to directly control how the mass of Voronoi cells evolve over time.

Instead, our proof strategy makes use of both notions of size available to us in metric measure spaces. Besides the _measure_ of a set, recall the _packing number_ of a set, a notion of metric entropy:

**Definition 18** (Packing and packing number).: Let \(r>0\). A set \(Z\) is an \(r\)_-packing_ if all of its points are bounded away from each other by a distance \(r\),

\[_{z,z^{} Z}\,(z,z^{}) r.\]

The \(r\)_-packing number_\(_{r}(U)\) of \(U\) is the maximum possible size of an \(r\)-packing \(Z\) contained in \(U\).

The packing number bounds the number of times that the nearest neighbor distance \((X_{n},_{n})\) is large. In particular, for any \(r>0\), the nearest neighbor distance can exceed \(r\) at most \(_{r}()\) times. This is because such a set of instances must then form an \(r\)-packing. As a slight generalization:

**Definition 19** (\(r\)-separated event).: Let \(\) be a process and \(r>0\). The \(r\)_-separated event_ at time \(n\) is the event \(E_{n}^{r}\) that \(X_{n}\) is \(r\)-separated from all past instances \(_{<n}\),

\[E_{n}^{r}:=(X_{n},_{n}) r}.\]

Given a subset \(U\), the \((U,r)\)_-separated events_ are the events \(E_{n}^{U,r}:=E_{n}^{r}X_{n} U}\).

**Lemma 20** (Packing bound).: _Let \((,)\) be a metric space, \(U\) be a subset, and \(r>0\). For any process \(\), the number of \((U,r)\)-separated events is bounded by the \(r\)-packing number of \(U\),_

\[_{n=1}^{}E_{n}^{U,r}} _{r}(U).\]

Thus, we now have two ways of bounding how often an instance can appear in the nearest neighbor process: the number of times it can be a distant nearest neighbor is bounded by the packing number, while the rate it can be a close nearest neighbor can be limited by the measure of small balls. The basic proof idea for Theorem 17 will be to slowly trade off the measure bound for the metric bound via an alternative decomposition of the event \(_{n}_{<n}\) based on the cover tree data structure.

### The cover-tree decomposition

DefinitionsIn this section, let \(\) be a bounded metric space, which we may rescale to have unit diameter. Let us recall the _cover tree_ data structure, introduced by (Beygelzimer et al., 2006), which is an efficient multi-scale ball cover for a given dataset: each data point is covered by a ball of every radius \(2^{-}\) for all \(_{0}\{0\}\). First, we'll introduce the _dyadic cone_ as a multi-scale cover of a single data point. Then, we inductively construct the cover tree as a union of dyadic cones.

**Definition 21** (Dyadic cone).: Let \((,)\) have unit diameter and let \(x\). A _dyadic cone_ centered at \(x\) of rank \(L_{0}\) is the discrete collection of balls:

\[(x;L)=\{B(x,2^{-}): L_{0}\}.\]

When \(L^{} L\), we also refer to \((x;L^{})\) within \((x;L)\) as its rank-\(L^{}\)_tail_.

One possible way of constructing a multi-scale covering of a dataset is to take the union of all dyadic cones of rank zero centered all data points. However, this cover is not particularly efficient, with many redundant large-scale balls. Instead, the cover tree is defined as a union of dyadic cones with adaptive ranks. Each rank is chosen to be minimal while still covering each point at all scales:

**Definition 22** (Sequentially-constructed cover trees).: Let \((,)\) have unit diameter and \(=(a_{k})_{k}\) be a dataset in \(\) without duplicates. The _cover trees_\((_{k})_{k}\) with _insertion ranks_\((L_{k})_{k}\) are defined:

\[_{1} =(a_{1};L_{1}), L_{1} =0,\] \[_{k} =_{k-1}(a_{k};L_{k}), L_{k} =\,:2^{-}_{k-1}x}.\]

We say that \(a_{k}\) was _inserted_ into the cover tree at the \(L_{k}\)th rank.

Points in \(_{ k}\) are covered by the cover tree \(_{k}\) at all scales. All other points are also covered by balls in the doubled cover tree, where the radii are comparable to their distances to \(_{ k}\). In particular, we define a _cover-tree neighbor_ map \(_{k}:_{ k}_{k}\) as any one satisfying:

\[_{k}(x)=B(a,r) x B(a,2r) r/2(x,_{ k})<r.\] (7)

The following lemma shows that such a map always exist.

**Lemma 23**.: _The cover tree \(_{k}\) for \(_{ k}\) has a cover-tree neighbor map \(_{k}\)._

Proof.: Let \(x_{ k}\) and let \(a_{ k}\) be a nearest neighbor, so that \((x,a)=(x,_{ k})\). There exists \(\) such that \(2^{-(+1)}(x,a)<2^{-}\) since \(x\) is not in \(_{ k}\). As \(a\) is covered at all scales, there is a ball \(B_{k}\) of radius \(2^{-}\) that contains \(a\). By triangle inequality, \(x\) is contained in \(2B\). 

Later, we shall also make use of the rooted tree structure on \(\) induced by the cover trees:

**Definition 24** (Tree structure).: For the above sequence of cover trees and insertion ranks, we let \(a_{1}\) be the _root_ of \(\). For all \(k>1\), there is a ball \(B(a_{j},2^{-L_{k}+1})_{k-1}\) containing \(a_{k}\). We assign such an \(a_{j}\) to be the _parent_ of \(a_{k}\), and we say that \(a_{k}\) is its _child_. A set of instances inserted at the same rank to the same parent is called a _generation_ of children. The number of generations of children \(a_{k}\) has at time \(n\) defines the upper triangular array \((G_{k,n})_{k n}\):

\[G_{k,n}=L_{k^{}}:a_{k}a_{k^{}}k^{} n}.\]

Application to nearest neighbor analysisLet \((_{k})_{k}\) be a sequence of cover trees for the sequence of indicated instances \(=(X_{_{k}})_{k}\). Using cover-tree neighbors, we obtain a decomposition of the event that an indicated instance is a nearest neighbor (Lemma 25). Whereas the earlier Voronoi decomposition proposed in Equation (6) corresponds to the partition of \(\) induced by the nearest neighbor map, this one is induced by the cover-tree neighbor map given by Equation (7).

**Lemma 25** (Cover tree decomposition).: _Let \((,)\) have unit diameter, let \(\) be a process in \(\), and let \(\) be an indicator process. For any \(n\), let \(\) be a cover tree for \(_{<n}\) with a cover-tree neighbor map \(\). Assume that \(X_{n}_{<n}\) is not equal to one of the indicated instances. Then:_

\[_{n}_{<n}}\ _{B(a,r)}X_{n}r/2_{<n}(X_{n})=B(a,r)}.\]

_In particular, the event within the union indexed by \(B=B(a,r)\) is contained in \(E_{n}^{2B,r/2}\)._

Proof sketch of Theorem 17.: Fix \(N\). Let \(\) be a cover tree for \([_{<N}]\). A purely metric bound is:

\[_{n=1}^{N}\,_{n} _{<n}} }{{}}_{n=1}^{N}_{B_{r} }E_{n}^{2B_{r},r/2}}\] \[}{{}}_{B_{r}} _{n=1}^{}E_{n}^{2B_{r},r/2}} }{{}}2^{2d}||.\]Here, (i) follows from Lemma 25, where \(B_{r}\) denotes a ball of radius \(r\) in \(\), (ii) is a larger summation, and (iii) applies the metric bound Lemma 20 and the fact that \(\) is a \(d\)-doubling space.

Of course, this bound is vacuous since the cover tree has infinitely many balls. Instead of paying for every ball in \(\) via the metric bound, we can decompose \(\) into two pieces: a relatively slow-growing collection of large balls for which we apply the metric bound, and a collection of dyadic tails with small combined measure. In particular, we adaptively adjust the tail for each indicated instance so that the combined tail events always have \(\)-mass at most \(\). As \(\) is uniformly dominated, the asymptotic rate at which the tail events occur is no more than \(()\).

The basic idea for choosing the tails is this. First, when an indicated instance is inserted into the cover tree, we immediately shrink the tail of this new instance by removing \(O()\) of its largest balls, where \((c,d)\) are the parameters associated to the upper doubling condition. Secondly, if this new instance is the first of a new generation of children, we also remove the largest ball from the tail of its parent's dyadic cone. We account for the events corresponding to these removed balls via the packing bound, contributing \(2^{2d}\) to the finite bound per removed ball. The indicated instances appear at a rate asymptotically bounded by \(\), so the cumulative packing bound grows at a rate of \(O( 2^{2d})\). Treating \(c\) and \(d\) as constants, we obtain an overall bound of:

\[_{N}\,_{n=1}^{N}\{_{n }_{<n}\}=O( {}+()).\]

See Figure 3 for a picture. Mathematically, we select the rank \(T_{k,n}\) of the tail of the \(k\)th indicated instance at time \(n\) as follows. Let \(L_{k}\) be the insertion rank of the \(k\)th indicated instance and let \(k(n)\) be the number of indicated instances that have arrived by time \(n\). Set the rank to:

\[T_{k,n}=L_{k}+1++G_{k,k (n)}.\] (8)

Lemma E.1 shows that this choice ensures that the combined tail event has \(\)-mass at most \(\). 

## 7 Related work

Nearest neighbor ruleThe nearest neighbor rule and its generalizations form a fundamental and well-studied part of non-parametric estimation, where the vast majority of work considers the i.i.d. setting (Fix and Hodges, 1951; Cover and Hart, 1967; Stone, 1977; Devroye et al., 1994; Cerou and Guyader, 2006; Chaudhuri and Dasgupta, 2014). Also see the surveys Devroye et al. (2013); Dasgupta and Kpotufe (2021). Far less is known in the non-i.i.d. setting. For positive results, Holst and Irle (2001); Ryabko (2006) relax to conditional independence where there may be a distinct distribution over instances per class; Kulkarni and Posner (1995) study arbitrary processes. Blanchard (2022) shows a negative result: in some non-worst-case online settings where other learners succeed, the nearest neighbor learner may fail. In the batch learning setting, Dasgupta (2012) and Ben-David and Urner (2014), considered consistency when training and test data distributions differ.

Non-worst-case online learningOur results echo a motif of _smoothed analysis_(Spielman and Teng, 2004): worst-case analyses of algorithms yield safeguards by refraining from making difficult-to-test assumptions, but they can be overly-pessimistic and fail to explain the observed behavior of algorithms (Roughgarden, 2021). Recently, two independent lines of work have emerged, introducing new classes of constrained stochastic processes for non-worst-case online learning: _smoothed online learning_(Rakhlin et al., 2011; Haghtalab et al., 2020, 2022; Block et al., 2022), and _optimistic universal learning_(Hanneke et al., 2021; Blanchard and Cosson, 2022; Blanchard, 2022). We connect the classes in these work through the strictly increasing chain of stochastic processes:

i.i.d. \(\)smoothed \(\)uniformly dominated \(\)ergodically dominated \(\)\(_{1}\)\(\) arbitrary.

To summarize the behavior of the nearest neighbor rule in these settings, (a) we characterize when the nearest neighbor rule is consistent for arbitrary sequences (Proposition 2); (b) Blanchard and Cosson (2022) shows that the nearest neighbor process is not universally consistent for the \(_{1}\)-processes defined by Hanneke (2021); (c) we show, via a new and simple proof, that the original Cover and Hart (1967) \(_{0}\)-consistency result for i.i.d. processes actually holds for a much larger class of ergodically dominated processes (Theorem 7); (d) we show universal consistency in upper doubling spaces for uniformly dominated processes (Theorem 12); and (e) we prove rates of convergence for smoothed processes in doubling length spaces (Theorem F.2).