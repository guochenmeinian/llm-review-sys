# An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness

Xiaochuan Gong  Jie Hao  Mingrui Liu

Department of Computer Science

George Mason University

{xgong2, jhao6, mingruil}@gmu.edu

Corresponding Author.

###### Abstract

This paper investigates a class of stochastic bilevel optimization problems where the upper-level function is nonconvex with potentially unbounded smoothness and the lower-level problem is strongly convex. These problems have significant applications in sequential data learning, such as text classification using recurrent neural networks. The unbounded smoothness is characterized by the smoothness constant of the upper-level function scaling linearly with the gradient norm, lacking a uniform upper bound. Existing state-of-the-art algorithms require \((^{-4})\) oracle calls of stochastic gradient or Hessian/Jacobian-vector product to find an \(\)-stationary point. However, it remains unclear if we can further improve the convergence rate when the assumptions for the function in the population level also hold for each random realization almost surely (e.g., Lipschitzness of each realization of the stochastic gradient). To address this issue, we propose a new Accelerated Bilevel Optimization algorithm named AccBO. The algorithm updates the upper-level variable by normalized stochastic gradient descent with recursive momentum and the lower-level variable by the stochastic Nesterov accelerated gradient descent algorithm with averaging. We prove that our algorithm achieves an oracle complexity of \((^{-3})\) to find an \(\)-stationary point, when the lower-level stochastic gradient has a small variance \(O()\). Our proof relies on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated gradient descent algorithm under distribution drift with high probability for the lower-level variable, which is of independent interest and also plays a crucial role in analyzing the hypergradient estimation error over time. Experimental results on various tasks confirm that our proposed algorithm achieves the predicted theoretical acceleration and significantly outperforms baselines in bilevel optimization. The code is available here.

## 1 Introduction

Bilevel optimization receives tremendous attention recently in the machine learning community, due to its applications in meta-learning , hyperparameter optimization , data hyper-cleaning , continual learning , and reinforcement learning . The bilevel optimization problem has the following formulation:

\[_{x^{d_{x}}}(x):=f(x,y^{*}(x)),\ \ ,\ \ y^{*}(x) *{arg\,min}_{y^{d_{y}}}g(x,y),\] (1)

where \(f\) and \(g\) are upper-level and lower-level functions respectively. For example, in meta-learning , \(x\) denotes the layers of neural networks for shared representation learning, \(y\) denotes the task-specific head encoded in the last layer, and the formulation (1) aims to learn the acommon representation learning encoder \(x\) such that it can be quickly adapted to downstream tasks by only updating the task-specific head \(y\). In machine learning, people typically consider stochastic optimization setting such that \(f(x,y)=_{_{f}}[F(x,y;)]\) and \(g(x,y)=_{_{g}}[G(x,y;)]\), where \(_{f}\) and \(_{g}\) are the underlying unknown data distributions for \(f\) and \(g\) respectively, and one can access noisy observations of \(f\) and \(g\) based on sampling from \(_{f}\) and \(_{g}\).

There emerges a wave of studies for algorithmic design and analysis for solving the bilevel optimization problem (1) under different assumptions of \(f\) and \(g\). Most theoretical work assumes the upper-level function is smooth (i.e., gradient is Lipschitz) and nonconvex, and the lower-level function is strongly convex . However, as pointed out by , certain neural networks such as recurrent neural networks , long-short term memory networks  and transformers  have smoothness constants that scale with gradient norm, potentially leading to unbounded smoothness constants (i.e., gradient Lipschitz constant can be infinity). Motivated by this, Hao et al.  designed the first bilevel optimization algorithm to handle the cases where \(f\) is nonconvex with potentially unbounded smoothness and \(g\) is strongly convex. The algorithm in  achieves \((^{-4})\) oracle complexity for finding an \(\)-stationary point (i.e., a point \(x\) such that \(\|(x)\|\)). Gong et al.  proposed an single-loop algorithm under the same setting as in  and also achieved \((^{-4})\) oracle complexity. This complexity result is worse than the \((^{-3})\) oracle complexity under the relatively easier setting where \(f\) has a Lipschitz gradient, and each realization of the stochastic oracle calls is Lipschitz with respect to its argument (e.g., almost-sure Lipschitz oracle) . This naturally motivates us to study the following question:

**Is it possible to improve the \((^{-4})\) oracle complexity for bilevel optimization problems where the upper-level function is nonconvex with unbounded smoothness and the lower-level function is strongly convex, by assuming that the properties of the function at the population level also hold almost surely for each random realization?**

In this paper, we give a positive answer to this question by designing a new algorithm named AccBO with an improved oracle complexity of \((^{-3})\), when the lower-level stochastic gradient has a small variance \(O()\). Our algorithm is inspired by momentum-based variance reduction techniques used in nonconvex smooth optimization  under the almost-sure Lipschitz stochastic gradient oracle framework. The innovation of AccBO lies in its update rules: it employs normalized stochastic gradient descent with recursive momentum for the upper-level variable and stochastic Nesterov accelerated gradient descent with averaging for the lower-level variable. Our approach differs from existing accelerated bilevel optimization algorithms, such as those proposed by  in two key ways: (i) while these algorithms use recursive momentum for the upper-level variable update, AccBO utilizes normalized recursive momentum to address the unbounded smoothness of the upper-level function; (ii) for the lower-level variable update, we use stochastic Nesterov accelerated gradient descent with averaging, in contrast to the recursive momentum method used by the other algorithms. The primary challenge in analyzing the convergence rate of AccBO arises from the need to simultaneously control errors from both upper-level and lower-level variables, given the unbounded smoothness, large learning rate, and recursive momentum in the upper-level problem. Our main contributions are summarized as follows.

* We design a new algorithm named AccBO for solving bilevel optimization problems where the upper-level function is nonconvex with unbounded smoothness and the lower-level function is strongly convex. AccBO leverages normalized recursive momentum for the upper-level variable and Nesterov momentum for the lower-level variable under the stochastic setting to achieve acceleration. To the best of our knowledge, the simultaneous usage of these two techniques in stochastic bilevel optimization is novel and has not been previously explored in the bilevel optimization literature.
* We prove that the AccBO algorithm requires \((^{-3})\) oracle calls for finding an \(\)-stationary point, when the variance of the lower-level stochastic gradient is \(O()\). This complexity strictly improves the state-of-the-art oracle complexity for unbounded smooth nonconvex upper-level problem and strongly-convex lower-level problem as described in 2. To achieve this result, we introduce novel proof techniques for analyzing the dynamics of stochastic Nesterov accelerated gradient descent under distribution drift with high probability for the lower-level variable, which are crucial for analyzing the hypergradient error and also of independent interest.
* We empirically verify the effectiveness of our proposed algorithm on various tasks, including deep AUC maximization and data hypercleaning. Our algorithm indeed achieves the predicted theoretical acceleration and significantly outperforms baselines in bilevel optimization.

## 2 Related Work

**Relaxed Smoothness.** The concept of relaxed smoothness was initially introduced by , inspired by the loss landscapes observed in recurrent neural networks and long-short term memory networks. They show that techniques such as gradient clipping and normalization could improve the performance compared with gradient descent in these scenarios. It inspired further investigations that concentrate on various aspects, including improved analysis on gradient clipping and normalization [74; 45], adaptive algorithms [15; 51; 68; 24], federated algorithms [54; 16; 17], generalized assumptions [15; 14], and recursive momentum based methods with faster rates [60; 56]. The work of [38; 32] considered a relaxed smoothness condition for the upper-level problem in the bilevel optimization setting.

**Bilevel Optimization.** Bilevel optimization refers to a special kind of optimization where one problem is embedded within another. It was first introduced by . Early works developed specific bilevel optimization algorithms with asymptotic convergence analysis [67; 1; 69]. Ghadimi and Wang  initiated the study of non-asymptotic convergence for gradient-based methods in bilevel optimization where the upper-level problem is smooth and the lower-level problem is strongly convex. This field saw further advancements with improved complexity results [41; 44; 11; 21; 12] and fully first-order algorithms [48; 52]. There is a line of work which leverages almost-sure Lipschitz oracle (e.g., stochastic gradient) to obtain improved convergence rates of bilevel optimization algorithms [71; 46]. When the lower-level function is not strongly convex, several algorithmic framework and approximation schemes were proposed [61; 63; 49; 62; 55; 10]. The setting considered in this paper is very close to [38; 32], where the upper-level function is nonconvex and unbounded smooth, and the lower-level function is strongly convex. However, the work of [38; 32] do not have an accelerated rate \((^{-3})\) for finding an \(\)-stationary point as established in this paper.

**Nesterov Accelerated Gradient and Variants.** Nesterov Accelerated Gradient (NAG) method was introduced by  for deterministic convex optimization problems. The stochastic version of NAG (SNAG) was extensively studied in the literature [3; 5; 66; 13]. To the best of our knowledge, none of them provide a high probability analysis for SNAG. In the online learning setting, there is a line of work focusing on the perspective of sequential stochastic/online optimization with distributional drift [6; 70; 57; 20]. While these studies provide valuable insights into adaptive techniques and performance bounds under distributional drift, they do not explore the potential integration of such methods with bilevel optimization problems, nor do they consider the application of SNAG within this framework.

## 3 Problem Setup and Preliminaries

Define \(,\) and \(\|\|\) as the inner product and the Euclidean norm. Throughout the paper, we use asymptotic notation \(()\), \(()\), \(()\) to hide polylog factors in \(^{-1}\) and \(1/\). Denote \(f\): \(^{d_{x}}^{d_{y}}\) as the upper-level function, and \(g\): \(^{d_{x}}^{d_{y}}\) as the lower-level function. The hypergradient \((x)\) has the following form :

\[(x)=_{x}f(x,y^{*}(x))-_{xy}^{2}g(x,y^{*}(x))[ _{yy}^{2}g(x,y^{*}(x))]^{-1}_{y}f(x,y^{*}(x)).\] (2)

To avoid the Hessian inverse computation, we typically use the following Neumann series to approximate the hypergradient [30; 46; 41]. In particular, for the stochastic setting, define

\[f(x,y;)=_{x}F(x,y;)-}_{xy }^{2}G(x,y;^{(0)})_{i=1}^{(Q)}(I- ^{2}G(x,y;^{(i)})}{l_{g,1}})_{y}F(x,y;),\]

where \((Q)\{0,,Q-1\}\), \(:=\{,^{(0)},,^{((Q))}\}\) and we use the convention that \(_{i=1}^{j}A_{i}=I\) if \(j=0\). Then \(_{}[f(x,y;)]\) is a good approximation of \((x)\) if \(y\) and \(y^{*}(x)\) are close .

Throughout the paper, we make the following assumptions.

**Assumption 3.1** (\((L_{x,0},L_{x,1},L_{y,0},L_{y,1})\)-smoothness ).: _Let \(z=(x,y)\) and \(z^{}=(x^{},y^{})\), there exists \(L_{x,0},L_{x,1},L_{y,0},L_{y,1}>0\) such that for all \(z,z^{}\), if \(\|z-z^{}\| 1/^{2}+L_{y,1}^{2})}\), then \(\|_{x}f(z)-_{x}f(z^{})\|(L_{x,0}+L_{x,1}\|_{x}f(z) \|)\|z-z^{}\|\) and \(\|_{y}f(z)-_{y}f(z^{})\|(L_{y,0}+L_{y,1}\|_{y}f(z) \|)\|z-z^{}\|\)._

**Remark**: Assumption 3.1 is introduced by  for describing the bilevel optimization problems with recurrent neural networks. This assumption can be regarded as a block-wise relaxed smoothness assumptions for two blocks \(x\) and \(y\), which is a variant of the relaxed smoothness assumption  and the coordinate-wise relaxed smooth assumption .

**Assumption 3.2**.: _Suppose the followings hold for objective functions \(f\) and \(g\): (i) \(f\) is continuously differentiable and \((L_{x,0},L_{x,1},L_{y,0},L_{y,1})\)-smooth in \((x,y)\); (ii) For every \(x\), \(\|_{y}f(x,y)\| l_{f,0}\) for all \(y\); (iii) For every \(x\), \(g(x,y)\) is \(\)-strongly-convex in \(y\) for \(>0\); (iv) \(g\) is \(l_{g,1}\)-smooth jointly in \((x,y)\); (v) \(g\) is twice continuously differentiable, and \(_{xy}^{2}g,_{yy}^{2}g\) are \(l_{g,2}\)-Lipschitz jointly in \((x,y)\)._

**Remark**: Assumption 3.2 is standard in the bilevel optimization literature [48; 38; 30]. Assumption 3.2 (i) characterizes the unbounded smoothness of the upper-level function and is empirically observed in recurrent neural networks .

**Assumption 3.3**.: _The following stochastic estimators are unbiased and have the following properties:_

\[_{_{f}}[\|_{x}F(x,y;)- _{x}f(x,y)\|^{2}]_{f,1}^{2},_{_{f} }[\|_{y}F(x,y;)-_{y}f(x,y)\|^{2}]_{f,1}^{2},\] \[(\|_{y}G(x,y;)-_{y}g(x,y)\|) 2 (-2^{2}/_{g,1}^{2})>0,\] \[_{_{g}}[\|_{xy}^{2}G(x,y; )-_{xy}^{2}g(x,y)\|^{2}]_{g,2}^{2},_{ _{g}}[\|_{yy}^{2}G(x,y;)-_{yy}^{2}g(x,y)\|^{2}] _{g,2}^{2}.\]

**Remark:** Assumption 3.3 assumes the stochastic oracle for the upper-level problem has bounded variance, which is standard in nonconvex stochastic optimization [28; 29; 30]. It also assumes the stochastic oracle for the lower-level problem is light-tailed, which is common for the high probability analysis for the lower-level problem [50; 39]. Note that the same assumption is also made in [38; 32] for the bilevel problems with a unbounded smooth upper-level function.

**Assumption 3.4**.: \(F(x,y;)\) _and \(G(x,y;)\) satisfy Assumption 3.2 for every \(\) and \(\) almost surely._

**Remark:** Assumption 3.4 assumes that each random realization of the upper- and lower-level functions satisfies the same property as in the population level. Note that this condition is the key to achieve an improved \((^{-3})\) oracle complexity under various settings, including both single-level nonconvex smooth problems [23; 18; 64] and bilevel problems with nonconvex smooth upper-level objectives [71; 46]. Furthermore, this assumption is shown to be necessary for achieving improved oracle complexity in single-level problems .

## 4 Algorithm and Analysis

### Main Challenges and Algorithm Design

**Main Challenges.** We begin by explaining why existing bilevel optimization algorithms and their corresponding analysis techniques are insufficient in our setting. First, most algorithms developed for bilevel optimization require the upper-level function is smooth (i.e., the gradient of the upper-level function is Lipschitz) [30; 44; 41; 71; 46; 21; 48]. They characterize the estimation error of the optimal solution for the lower-level problem, utilize an approximate hypergradient descent approach and the descent lemma for \(L\)-smooth functions to prove the convergence. In particular, they demonstrate that a potential function, incorporating both the function value and the bilevel error from the lower-level problem, progressively decreases in expectation. However, when the upper-level function is \((L_{x,0},L_{x,1},L_{y,0},L_{y,1})\)-smooth as illustrated in Assumption 3.1, the previous algorithms and analyses relying on \(L\)-smoothness do not work. The reason is that the hypergradient bias depends on the approximation error of the lower-level variable as well as the hypergradient itself: these elements are statistically dependent and the standard potential function argument with an expectation-based analysis would not work. To address this issue, the work of [38; 32] requires a careful high probability analysis in the unbounded smoothness setting and obtains \((^{-4})\) oracle complexity. Such a requirement of high probability analysis prevents us from leveraging the momentum-basedvariance reduction technique for updating the lower-level variable. For example, the work  which has \((^{-3})\) oracle complexity in the smooth case leverages the momentum-based variance reduction technique  for updating the lower-level variable with an expectation-based analysis, but it seems difficult to establish a high probability analysis for the momentum-based variance reduction algorithm in terms of the lower-level variable. Second, the recent work of Hao et al.  and Gong et al.  considered that the upper-level function is unbounded smooth and addressed this issue by performing normalized stochastic gradient with momentum for the upper-level variable and periodic updates or stochastic gradient descent for the lower-level variable, but their oracle complexity is not better than \((^{-4})\). These facts indicate that we need new algorithm design and analysis techniques to get potential acceleration.

**Algorithm Design.** To obtain potential acceleration and enable a high probability analysis for the lower-level variable, our key idea is to update the upper-level variable by normalized stochastic gradient descent with recursive momentum and update the lower-level variable by the stochastic Nesterov accelerated gradient (SNAG) method. Different from [38; 32], the key innovation of our algorithm design is that we achieve acceleration for both upper-level and lower-level problems simultaneously but without affecting each other. The upper-level update rule can be regarded as a generalization of the acceleration technique (e.g., the momentum-based variance reduction technique) [18; 56] from single-level to bilevel problems. The main challenge is that we need to deal with the accumulated error of the recursive momentum over time due to the hypergradient bias, which is caused by the inaccurate estimation of the optimal lower-level variable. Therefore we require a very small tracking error between the iterate of the lower-level variable and the optimal lower-level solution defined by the upper-level variable (i.e., \(y^{*}(x)\)) at every iteration. This requirement is satisfied by executing SNAG method under the distribution drift for the lower-level problem, where the drift is caused by the change of the upper-level variable over time. Note that we can provide a high probability analysis of the SNAG method under distributional drift, which strictly improves the analysis of SGD under distributional drift in  in the small stochastic gradient noise regime.

The detailed description of our algorithm is illustrated in Algorithm 2. At the very beginning, we run a certain number of iterations of SNAG for the fixed upper-level variable \(x_{0}\) (line \(2\)) as the warm-start stage, and then update the lower-level variable by SNAG (line \(8 20\)) with averaging (line \(21\)) and update the upper-level variable by normalized stochastic gradient descent with recursive momentum (line \(23 24\)). Note that we have two options for implementing SNAG. In Option I (line \(8 9\)), the algorithm simply runs SNAG under distribution drift caused by the sequence \(\{x_{i}\}\). Option I is specifically designed for a particular subset of bilevel optimization problems where the lower-level function is a quadratic function. Option II (line \(11 20\)) is designed for a broader range of bilevel optimization problems, accommodating general strongly-convex lower-level functions. In Option II, we run SNAG with periodic updates: the lower-level update is performed for \(N\) iterations only when the iteration number \(t\) is a multiple of \(I\).

```
1:Input:\(x,_{-1},_{0},,T_{0}\) # SNAG\((x,_{0},,T_{0})\)
2:for\(t=0,1,,T_{0}-1\)do
3: Sample \(_{t}\) from distribution \(_{g}\)
4:\(_{t}=_{t}+(_{t}-_{t-1})\)
5:\(_{t+1}=_{t}-_{y}G(x,_{t}; _{t})\)
6:endfor ```

**Algorithm 1** Stochastic Nesterov Accelerated Gradient Method (SNAG)

### Main Results

We first introduce some useful notations. Let \(()\) be the \(\)-algebra generated by the random variables in the argument. We define the following filtrations for \(t 1\): \(^{}=(_{0},,_{T_{0}-1})\), \(_{t}=(_{0},,_{t-1})\), \(}_{t}^{1}=(_{0},,_{t-1})\), and we also define \(}_{t}^{2}=(_{t}^{0},,_{t}^{N-1})\) when \(t\) is a multiple of \(I\). We use \(_{t}\), \(_{_{t}}\) and \(\) to denote the conditional expectation \([_{t}]\), the expectation over \(_{t}\) and the total expectation over \(_{T}\) respectively.

**Theorem 4.1**.: _Suppose Assumptions 3.1 to 3.4 hold. Let \(\{x_{t}\}\) be the iterates produced by Algorithm 2. For any given \((0,1)\) and small enough \(\) (see exact choice in (54)), if \(_{g,1}=O()\) as defined in (55), and we set parameters \(^{},,,,,,I,N,S,Q,T_{0}\) (see exact choices in (56), (57), (58),(59), and (60)) as

\[^{ init}=(^{4}),= (^{2}), 1-=(^{2}),= (^{2}),=(), =O(1),\] \[T_{0}=(^{-2}), I=( ^{-1}), N=(^{-1}), Q=(1), S=(1).\]

Then with probability at least \(1-2\) over the randomness in \((^{ init}}_{T}^{1})\) (for Option I) or \((^{ init}(_{t T}}_{t}^{2}))\) (for Option II), Algorithm 2 guarantees \(_{t=1}^{T}\|(x_{t})\| 20\) within \(T=}{}=(^{-3})\) iterations, where \(d_{0}(x_{0})-_{x}(x)\) and the expectation is taken over the randomness over \(_{T}\). For Option I, it requires \(T_{0}+SQT=(^{-3})\) oracle calls of stochastic gradient or Hessian/Jacobian vector product. For Option II, it requires \(T_{0}++SQT=(^{-3})\) oracle calls of stochastic gradient or Hessian/Jacobian vector product._

```
1:Input:\(^{ init},,,,,,I,S,T_{0},T\), set \(x_{0},y_{0}^{ init}=0\)
2:\(y_{0}=(x_{0},y_{0}^{ init},^{ init},T_{0})\), and set \(y_{-1}=y_{0}=y_{0}\)# Warm-start
3:for\(t=0,1,,T-1\)do
4: Sample \((Q)\{0,,Q-1\}\) and \(\{_{t,s}^{(0)},,_{t,s}^{((Q))}\}_{s=1}^{S} _{g}\)
5: Sample \(\{_{t,s}\}_{s=1}^{S}_{f}\), denote \(_{t}_{s=1}^{S}\{(Q),_{t,s},_{t,s}^ {(0)},,_{t,s}^{((Q))}\}\)
6:# Lower-Level: Stochastic Nesterov Accelerated Gradient Descent with Averaging
7:# Option I: from Line \(8 9\) (for quadratic lower-level function)
8:\(z_{t}=y_{t}+(y_{t}-y_{t-1})\)
9:\(y_{t+1}=z_{t}-_{y}G(x_{t},z_{t};_{t})\), where \(_{t}_{g}\)
10:# Option II: from Line \(11 20\) (for general strongly convex lower-level function)
11:if\(t>0\) and \(t\) is a multiple of \(I\)then
12: Set \(y_{0}^{t}=y_{t}^{-1}=y_{t}\)
13:for\(j=0,1,,N-1\)do
14:\(z_{t}^{j}=y_{t}^{j}+(y_{t}^{j}-y_{t}^{j-1})\)
15:\(y_{t}^{j+1}=z_{t}^{j}-_{y}G(x_{t},z_{t}^{j};_{t}^{j})\), where \(_{t}^{j}_{g}\)
16:endfor
17:\(y_{t+1}=y_{t}^{N+1}\)
18:else
19:\(y_{t+1}=y_{t}\)
20:endif
21:\(_{t+1}=(1-)_{t}+ y_{t+1}\)# Averaging
22:# Upper-Level: Normalized Stochastic Gradient Descent with Recursive Momentum
23:\(m_{t}= m_{t-1}+(1-)f(x_{t},_{t};_ {t})+(f(x_{t},_{t};_{t})-f(x_{t-1},_{t-1};_{t}))\) if \(t 1\) else \(m_{0}= f(x_{0},_{0};_{0})\)
24:\(x_{t+1}=x_{t}-}{\|m_{t}\|}\)
25:endfor ```

**Algorithm 2** Accelerated Bilevel Optimization algorithm (AccBO)

**Remark:** Theorem 4.1 established an improved \((^{-3})\) oracle complexity for finding an \(\)-stationary point when the lower-level standard deviation \(_{g,1}=O()\). This complexity result strictly improves the \((^{-4})\) obtained by [38; 32] when the upper-level function is nonconvex and unbounded smooth. This complexity result also matches that in the single-level unbounded smooth setting  and is nearly optimal in terms of the dependency on \(\). The full statement of Theorem 4.1 is included in Theorem E.2.

### Proof Sketch

In this section, we provide a roadmap of proving Theorem 4.1 and the main steps. The detailed proofs can be found in Appendix D and E. The key idea is to prove two things: (1) the lower-level iterate is very close to the optimal lower-level variable at every iteration; (2) two consecutive iterates of the lower-level iterates are close to each other. In particular, define \(y_{t}^{*}=y^{*}(x_{t})\), and we aim to prove that \(\|_{t}-y_{t}^{*}\| O()\) and \(\|_{t+1}-_{t}\| O(^{2})\) for every \(t\). These two requirements are essential to control the hypergradient estimation error (i.e., \(\|m_{t}-(x_{t})\|\)) caused by inaccurate estimate of the lower-level problem. Lemma 4.7 provides the guarantee for the lower-level problem, and Lemma 4.8characterizes the hypergradient estimation error. Equipped with these two lemmas, we can adapt the momentum-based variance reduction techniques  to the upper-level problem and prove the main theorem.

The main technical contribution of this paper is to provide a general framework for proving the convergence of SNAG under distributional drift in Section 4.3.1, which can be leveraged as a tool to control the lower-level error in bilevel optimization and derive the Lemma 4.7, as illustrated in Section 4.3.2. In particular, we can regard the change of the upper-level variable \(x\) at each iteration as the distributional drift for the lower-level problem: the drift is small due to the normalization operator of the upper-level update rule and also the Lipschitzness of \(y^{*}(x)\). Once we have the general lemma for tracking the minimizer for any fixed distributional drift over time, this lemma can be applied to our algorithm analysis and establish guarantees for the bilevel problem.

#### 4.3.1 Stochastic Nesterov Accelerated Gradient Descent under Distributional Drift

In this section, we study the sequences of stochastic optimization problems \(_{w^{d}}_{t}(w)\) indexed by time \(t\). We denote the minimizer and the minimal value of \(_{t}\) as \(w_{t}^{*}\) and \(_{t}^{*}\), and we define the _minimizer drift_ at time \(t\) to be \(_{t}:=\|w_{t}^{*}-w_{t+1}^{*}\|\). With a slight abuse of notation 3, we consider the SNAG algorithm applied to the sequence \(\{_{t}\}_{t=1}^{T}\), where \(T\) is the total number of iterations:

\[z_{t} =w_{t}+(w_{t}-w_{t-1})\] (3) \[w_{t+1} =w_{t}+(w_{t}-w_{t-1})- g_{t},\]

where \(g_{t}=_{t}(z_{t};_{t})\) is the stochastic gradient evaluated at \(z_{t}\) with random sample \(_{t}\). Define \(_{t}=g_{t}-_{t}(z_{t})\) as the stochastic gradient noise at \(t\)-th iteration. Define \(_{t}=(_{1},,_{t-1})\) as the filtration, which is the \(\)-algebra generated by all random variables until \(t\)-th iteration. We make the following assumption, which is the same as Assumption 3 in  for high probability analysis.

**Assumption 4.2**.: _Function \(_{t}:^{d}\) is \(\)-strongly convex and \(L\)-smooth for constants \(,L>0\). Also, there exists constants \(,>0\) such that the drift \(_{t}^{2}\) is sub-exponential conditioned on \(_{t}\) with parameter \(^{2}\) and the noise \(_{t}\) is norm sub-Gaussian conditioned on \(_{t}\) with parameter \(/2\)._

**Lemma 4.3**.: _Suppose Assumption 4.2 holds and let \(\{w_{t}\}\) be the iterates produced by the update rule (3) with constant learning rate \( 1/25L\), and set \(=}{1+}\). Define \(_{t}=[(w_{t}-w_{t}^{*})^{},(w_{t-1}-w_{t}^{*})^{}]^{} ^{2d}\), and the potential function \(V_{t}\) as_

\[V_{t}=_{t}^{}_{t}+_{t}(w_{t})-_{t}(w_{t}^{*} ),=1&-1\\ -1&(1-)^{2}_{d}.\]

_Then for any given \((0,1)\) and all \(t 0\), the following holds with probability at least \(1-\) over the randomness in \(_{t}\) (here \(e\) denotes the base of natural logarithms):_

1. _(With drift) Let_ \(_{t}(w)\|w-w_{t}^{*}\|^{2}\)_, then_ \(V_{t}(1-}{4})^{t}V_{0}+(^{2}}{}+}{}) {}\)_._
2. _(Without drift) Let_ \(_{t}(w)(w)\) _be any general strongly convex functions with_ \(=0\)_, then_ \(V_{t}(1-}{4})^{t}V_{0}+^{2}}{}\)_._

**Remark**: When \(\{_{t}\}_{t=1}^{T}\) is a sequence of quadratic functions with moving minimizers, Lemma 4.3 provides a high probability tracking guarantee for SNAG with distributional drift, which is useful to provide guarantees for Option I in Algorithm 2. Note that this guarantee strictly improves the guarantee of stochastic gradient descent with distributional drift (e.g., [20, Theorem 6]) _in the small stochastic gradient noise regime_ and therefore is of independent interest. In particular, for small \(\), the decaying factor in the first term is improved from \(1-\) to \(1-}{4}\), the drift term is improved from \(}{^{2}}\) to \(}{}\), and the variance term becomes a bit worse (from \(^{2}\) to \(^{2}}{}\)). When \(\) is small enough, the variance term becomes insignificant compared with the drift term, then Lemma 4.3 provides an improved convergence rate with high probability. To the best of our knowledge, such an improved guarantee for SNAG with distributional drift is first shown in this work. When there is no drift, Lemma 4.3 also provides a high probability guarantee for SNAG. It holds for any smooth and strongly convex function \(\), and it is useful to provide guarantees for Option II of Algorithm 2.

#### 4.3.2 Application of Stochastic Nesterov Accelerated Gradient to Bilevel Optimization

Inspired by , we can regard \(_{t}()\) as \(_{t}():=g(x_{t},)\) in the bilevel setting, and then we have \(_{t}= l_{g,1}/\) for every \(t\) due to the upper-level update rule and the Lipschitzness of \(y^{*}(x)\). Therefore we can focus on the high probability analysis on the lower-level variable without worrying about the randomness from the upper-level. Throughout, we assume Assumption 3.1, 3.2, 3.3 and 3.4 hold. In addition, the failure probability \((0,1)\) and \(>0\) are chosen in the same way as in Theorem 4.1.

**Lemma 4.4** (Warm-start).: _Let \(\{y_{t}^{}\}\) be the iterates produced by line 2 of Algorithm 2. Set \(^{}=(^{4})\), \(_{g,1}=()^{1/4}_{g,1}\), and \(_{t}(y) g(x_{0},y)\). Then \(\|y_{T_{0}}^{}-y_{0}^{*}\|}}\) holds with probability at least \(1-\) over the randomness in \(}^{}\) (we denote this event as \(_{}\)) in \(T_{0}=(^{-2})\) iterations._

**Remark:** Lemma 4.4 shows that for fixed initialization \(x_{0}\), running SNAG for at most \(T_{0}=(^{-2})\) iterations can guarantee that the Euclidean distance between the lower-level variable \(y_{T_{0}}^{}\) and the optimal solution \(y^{*}(x_{0})\) is at most \(O()\), with high probability.

**Lemma 4.5** (Option I).: _Under event \(_{}\), let \(\{y_{t}\}\) be the iterates produced by Option I. Set \(=(^{2})\), \(_{g,1}=()^{1/4}_{g,1}\), and \(_{t}(y)=g(x_{t},y)=\|y-y_{t}^{*}\|^{2}\). Then for any \(t[T]\), Algorithm 2 guarantees with probability at least \(1-\) over the randomness in \(}_{T}^{1}\) (we denote this event as \(_{y}^{1}\)) that \(\|y_{t}-y_{t}^{*}\|/2L_{0}\)._

**Lemma 4.6** (Option II).: _Under event \(_{}\), let \(\{y_{t}\}\) be the iterates produced by Option II. Set \(=(^{2})\), \(N=(^{-1})\), \(I=(^{-1})\), \(_{g,1}=()^{1/4}_{g,1}\), and \(_{t}(y)=g(x_{t},y)\) when \(t\) is a multiple of \(I\) (i.e., \(x_{t}\) is fixed for each update round of Option II so \(g\) can be general functions). Then for any \(t[T]\), Algorithm 2 guarantees with probability at least \(1-\) over the randomness in \((_{t T}}_{t}^{2})\) (we denote this event as \(_{y}^{2}\)) that \(\|y_{t}-y_{t}^{*}\|/L_{0}\)._

**Remark:** Lemma 4.5 and Lemma 4.6 show that, under event \(_{}\) and both option I and option II, the algorithm guarantees that each iterate \(y_{t}\) is \(O()\)-close to the the optimal lower-level variable \(y_{t}^{*}\) at every iteration \(t\) with high probability.

**Lemma 4.7** (Averaging).: _Under event \(_{}_{y}^{1}\) (Option I) or \(_{}_{y}^{2}\) (Option II), set \(=\) in the averaging step (line 21 of Algorithm 2). Then for any \(t 0\) we have \(\|_{t}-y_{t}^{*}\|}\) and \(\|_{t+1}-_{t}\|}{24L_{0}^{2}_{g,1}}=:\)._

**Remark:** Lemma 4.7 shows that after performing averaging operations over the sequence \(\{y_{t}\}_{t=1}^{T}\), the averaged sequence enjoys stronger guarantees. First, each averaged iterate \(_{t}\) is still \(O()\)-close to the optimal lower-level variable \(y_{t}^{*}\); Second, two consecutive averaged iterates (i.e., \(_{t}\) and \(_{t+1}\)) is \(O(^{2})\)-close to each other. The stronger guarantees are crucial to control the hypergradient estimation error as described in Lemma 4.8.

**Lemma 4.8**.: _Under event \(_{}_{y}^{1}\) (Option I) or \(_{}_{y}^{2}\) (Option II), define \(_{t}=m_{t}-_{t}[ f(x_{t},_{t};_{t})]\), then we have the following averaged cumulative error bound:_

\[_{t=0}^{T-1}\|_{t}\|}{T( 1-)}++_{0}}{}+^{2})}{S}}+_{1}+ ^{2})}{S(1-)}}_{t=0}^{T-1}\|(x _{t})\|,\]

_where \(S\) denotes the batch size, and \(,_{0},_{1}\) are defined in Lemmas B.4 and B.6._

**Remark:** Lemma 4.8 characterizes the upper-level hypergradient estimation error under the good event that the lower-level error can be controlled. One can choose hyperparameters appropriately such that the cumulative error (i.e., LHS) grows only sublinearly in terms of \(T\), which is important for establishing the fast convergence of our algorithm.

## 5 Experiments

**Deep AUC Maximization with Recurrent Neural Networks**. AUC (Area Under the ROC Curve)  is a critical metric in evaluating the performance of binary classification models. It measures the ability of the model to distinguish between positive and negative classes, andit is defined as the probability that the prediction score of a positive example is higher than that is a negative example . Deep AUC maximization [53; 72] can be formulated as a min-max optimization problem : \(_{^{d},(a,b)^{2}}_{}f (,a,b,)_{}[F(,a,b,;)]\), where \(F(,a,b,;)=(1-r)(h(;)-a)^{2}_{[c=1]}+r( h(;)-b)^{2}_{[c=-1]}+2(1+)(rh(;) _{[c=-1]}-(1-r)h(;)_{[c=1]})-r(1-r)^{2}\), \(\) denotes the model parameter, \(=(,c)\) is the random data sample (\(\) denote the feature vector and \(c\{+1,-1\}\) denotes the label), \(h(,)\) is the score function defined by a neural network, and \(r=(c=1)\) denotes the ratio of positive samples in the population. This min-max formulation is an special case of the bilevel problem with \(g=-f\) in (1), which can be reformulated as the following:

\[_{^{d},(a,b)^{2}}_{}[F( ,a,b,^{*}(,a,b);)]^{*}( ,a,b)_{}\,-_{}[F(,a,b,;)]\] (4)

where \((,a,b)\) denotes the upper-level variable, and \(\) denotes the lower-level variable. In this case, the lower-level is a quadratic function in terms of \(\) and is strongly convex, and the upper-level function is non-convex function with potential unbounded smoothness when using a recurrent neural network as the predictive model.

We aim to perform imbalanced text classification task and maximize the AUC metric. The Deep AUC maximization experiment is performed on imbalanced Sentiment140  dataset (under the license of CC BY 4.0), which is a binary text classification task. Specifically, we follow  to make training set imbalanced with a pre-defined imbalanced ratio (\(r\)), and leave the test set unchanged. Given \(r\), we randomly discard the positive samples (with label 1) in original training set until the portion of positive samples equals to \(r\). The imbalance ratio \(r\) is set to 0.2 in our experiment, which means only \(20\%\) data is positive in the training set. We use a two-layer recurrent neural network with input dimension=300, hidden dimension=4096, and output dimension=2 for the model prediction.

We compare with some bilevel optimization baselines, including StocBio , TTSA , SABA , MA-SOBA , SUSTAIN , VRBO  and BO-REP . We show the training and test AUC result with 25 epochs in (a) (b) of Figure 1 and running time in (c), (d) of Figure 1. Our algorithm AccBO achieves highest AUC score among all the baselines over epochs and running time. The running time figure shows AccBO converges to a good result faster than other baselines. The detailed parameter tuning and selection are included in Appendix G.

**Data Hypercleaning.** The Data hypercleaning task tries to learn a set of weights \(\) for the corrupted training data \(_{tr}\), such that the model trained on the weighted corrupted training set can achieve good performance on the clean validation set \(_{val}\), where the corrupted training set \(_{tr}:=\{_{i},_{i}\}\) and the label \(_{i}\) is randomly flipped to one of other labels with probability \(0<p<1\). The data hyper-cleaning can be formulated as a bilevel optimization problem,

\[_{}_{}|}_{_{}}(^{*}();),\;\; ^{*}()*{arg\,min}_{}_{ }|}_{_{i}_{}}(_{i})(;_{i})+c\|\|^{2},\] (5)

where \(\) is the model parameter of a neural network, and \((x)=}\) is the sigmoid function. We perform bilevel optimization algorithms on the noisy text classification dataset Stanford Natural Language Inference (SNLI)  (under the license of CC BY 4.0) with a three-layer recurrent neural network with input dimension=300, hidden dimension=4096, and output dimension=3 for the label prediction. Each of sentence-pairs manually labeled as entailment, contradiction, and neutral.

Figure 1: Results of bilevel optimization on deep AUC maximization. Figure (a), (b) are the results over epochs, and Figure (c), (d) are the results over running time.

Specifically, the label of each training data is randomly flipped to one of the other two labels with probability \(p\). We set \(p=0.1\) and \(p=0.2\) in the experiments, respectively. We compare all the baselines used in the deep AUC maximization experiment. Different from the formulation (4) for the deep AUC maximization, the lower-level function in (5) is not quadratic function of the lower-level variable. Therefore we choose Option II in Algorithm 2, i.e., periodic updates for the lower-level variable. The results are presented in Figure 2 (\(p=0.1\) and \(p=0.2\)). Our algorithm AccBO exhibits the highest classification accuracy on training and test set among all the bilevel baselines, and also shows a high runtime efficiency. More detailed parameter tuning and selection can be found in Appendix G. All the experiments are run on the device of NVIDIA A6000 (48GB memory) GPU and AMD EPYC 7513 32-Core CPU.

## 6 Conclusion

In this paper, we propose a new algorithm named AccBO for solving bilevel optimization problems where the upper-level is nonconvex and unbounded smooth and the lower-level problem is strongly convex. The algorithm achieved \((^{-3})\) oracle complexity for finding an \(\)-stationary point when the lower-level stochastic gradient variance is \(O()\), which matches the rate of the state-of-the-art single-level relaxed smooth optimization  and is nearly optimal in terms of dependency on \(\).

**Limitations.** There are two limitations of our work. One limitation of our work is that the convergence analysis for the Option I of our algorithm relies on the lower-level problem being a quadratic function: only under this case the algorithm becomes a single-loop procedure. Another limitation is that we require the lower-level stochastic gradient has variance \(O()\). It remains unclear how to design single-loop algorithms for more general lower-level strongly convex functions and get rid of the small stochastic gradient variance assumption for the lower-level variable.