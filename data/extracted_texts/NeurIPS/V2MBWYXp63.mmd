# Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction

Haoran Luo\({}^{1}\), Haihong E\({}^{1}\), Yuhao Yang\({}^{2}\), Tianyu Yao\({}^{1}\),

**Yikai Guo\({}^{3}\), Zichen Tang\({}^{1}\), Wentai Zhang\({}^{1}\), Shiyao Peng\({}^{1}\), Kaiyang Wan\({}^{1}\),**

**Meina Song\({}^{1}\), Wei Lin\({}^{4}\), Yifan Zhu\({}^{1}\), Luu Anh Tuan\({}^{5}\)**

\({}^{1}\)School of Computer Science, Beijing University of Posts and Telecommunications, China

\({}^{2}\)School of Automation Science and Electrical Engineering, Beihang University, China

\({}^{3}\)Beijing Institute of Computer Technology and Application \({}^{4}\)Inspur Group Co., Ltd., China

\({}^{5}\)College of Computing and Data Science, Nanyang Technological University, Singapore

{luohaoran, ehaihong, yifan_zhu}@bupt.edu.cn, anhtuan.luu@ntu.edu.sg

###### Abstract

Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: _hyper-relational schema_, _event-based schema_, _role-based schema_, and _hypergraph-based schema_, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in \(F_{1}\) scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available1.

## 1 Introduction

Modern knowledge graphs (KGs), such as Freebase , Google Knowledge Vault , and Wikidata , utilize a multi-relational graph structure to represent knowledge. Because of the advantage of intuitiveness and interpretability, KGs find various applications in question answering , query response , logical reasoning , and recommendation systems . Traditional KGs are mostly composed of binary relational facts (\(subject,\ relation,\ object\)), which represent the relationship between two entities . However, it has been observed  that over 30% of real-world facts involve n-ary relation facts with more than two entities (\(n 2\)). As shown in Figure 1, an n-ary relational knowledge graph (NKG) is composed of many n-ary relation facts, offering richer knowledge expression

Figure 1: An example of NKG construction.

and wider application capabilities. As a key step of constructing NKGs, n-ary relation extraction (n-ary RE) is a task of identifying n-ary relations among entities in natural language texts. Compared to binary relational facts, n-ary relational facts in NKGs have more diverse schemas for different scenarios. For example, Wikidata utilizes n-ary relational facts in a _hyper-relational schema_[20; 10; 23], i.e., \((s,r,o,\{(k_{i},v_{i})\}_{i=1}^{n-2})\) which adds \((n-2)\) key-value pairs to the main triple to represent auxiliary information. In addition to the _hyper-relational schema_, the existing NKG schemas also include _event-based schema_ (\(r\), \(\{(k_{i},v_{i})\}_{i=1}^{n}\)) [11; 16], _role-based schema_ (\(\{(k_{i},v_{i})\}_{i=1}^{n}\)) [12; 15], and _hypergraph-based schema_ (\(r,\{v_{i}\}_{i=1}^{n}\)) [26; 8], as shown in Figure 2.

Currently, most existing NKGs in four schemas, such as JF17K , Wikipeople , WD50K , and EventKG , are manually constructed. Previous n-ary RE methods [13; 31] focus on extraction with a fixed number of entities in _hypergraph-based schema_ or _role-based schema_. Existing event extraction methods [16; 17; 9] can achieve n-ary RE in _event-based schema_. Recently, CubeRE  introduce a cube-filling method, which is the only n-ary RE method in _hyper-relational schema_.

However, there are still three main challenges in automated n-ary RE for NKG construction, which remains at a coarse-grained level: **(1) Diversity of NKG schemas.** Previous methods could only perform N-ary RE based on a specific schema, but currently, there is no flexible method that can perform n-ary RE for arbitrary schema with different number of relations. **(2) Determination of the order of entities.** N-ary RE involves more possible entity orders than binary RE, for example, as shown in Figure 2, in a _hyper-relational schema_, there is an order issue regarding which entity is the head entity, tail entity, or auxiliary entity. Previous methods often ignored the joint impact of different entity orders, leading to inaccurate extraction.**(3) Variability of the arity of n-ary RE.** Previous methods usually output a fixed number of entities and are not adept at determining the variable number of entities forming an n-ary relational fact.

To tackle these challenges, we introduce **Text2NKG**, a novel fine-grained n-ary RE framework designed to automate the generation of n-ary relational facts from natural language text for NKG construction. Text2NKG employs a **span-tuple multi-label classification** method, which transforms n-ary RE into a multi-label classification task for span-tuples, including all combinations of entities in the text. Because the number of predicted relation labels corresponds to the chosen NKG schema, Text2NKG is adaptable to all NKG schemas, offering examples with _hyper-relational schema_, _event-based schema_, _role-based schema_, and _hypergraph-based schema_, all of which have broad applications. Moreover, Text2NKG introduces a **hetero-ordered merging** method, considering the probabilities of predicted labels for different entity orders to determine the final entity order. Finally, Text2NKG proposes an **output merging** method, which is used to unsupervisedly derive n-ary relational facts of any number of entities for NKG construction.

In addition, we extend the only n-ary RE benchmark for NKG construction, HyperRED , which is in the _hyper-relational schema_, to four NKG schemas. We've done sufficient n-ary RE experiments on HyperRED, and the experimental results show that Text2NKG achieves state-of-the-art performance in \(F_{1}\) scores of hyper-relational extraction. We also compared the results of Text2NKG in the other three schemas to verify applications.

Figure 2: Taking a real-world textual fact as an example, we can extract a four-arity structured span-tuple for entities (Einstein, University of Zurich, Doctorate, Physics) with an answer label-list for relations accordingly as a 4-ary relational fact from the sentence through n-ary relation extraction.

Related Work

### N-ary relational Knowledge Graph

An n-ary relational knowledge graph (NKG) consists of n-ary relational facts, which contain \(n\) entities (\(n 2\)) and several relations. The n-ary relational facts are necessary and cannot be replaced by combinations of some binary relational facts because we cannot distinguish which binary relations are combined to represent the n-ary relational fact in the whole KG. Therefore, NKG utilizes a schema in every n-ary relational fact locally and a hypergraph representation globally .

Firstly, the simplest NKG schema is hypergraph-based.  found that over 30% of Freebase  entities participate facts with more than two entities, first defined n-ary relations mathematically and used star-to-clique conversion to convert triple-based facts representing n-ary relational facts into the first NKG dataset JF17K in _hypergraph-based schema_ (\(r\), \(\{v_{i}\}_{i=1}^{n}\)).  proposed FB-AUTO and M-FB15K with the same _hypergraph-based schema_. Secondly,  introduced role information for n-ary relational facts and extracted Wikipedia, the first NKG dataset in _role-based schema_ (\(\{(k_{i},v_{i})\}_{i=1}^{n}\)), composed of role-value pairs. Thirdly, Wikidata , the largest knowledge base, utilizes an NKG schema based on hyper-relation \((s,r,o,\{(k_{i},v_{i})\}_{i=1}^{n-2})\), which adds auxiliary key-value pairs to the main triple.  first proposed an NKG dataset in _hyper-relational schema_ WD50K. Fourthly, as  pointed out, events are also n-ary relational facts. One basic event representation has an event type, a trigger, and several key-value pairs . Regarding the event type as the main relation, the (trigger: value) as one of the key-value pairs, and the arguments as the rest key-value pairs, we can obtain an _event-based NKG schema_ (\(r\), \(\{(k_{i},v_{i})\}_{i=1}^{n}\)).

Based on four common NKG schemas, we propose Text2NKG, the first method for extraction of structured n-ary relational facts from natural language text, which improves NKG representation and application.

### N-ary Relation Extraction

Relation extraction (RE) is an important step of KG construction, directly affecting the quality, scale, and application of KGs. While most of the current n-ary relation extraction (n-ary RE) for NKG construction depends on manual construction [26; 12; 10] but not automated methods. Most automated RE methods target the extraction of traditional binary relational facts. For example,  proposes a table-filling method for binary RE, and [30; 27] propose span-based RE methods with levitated marker and packed levitated marker, respectively.

For automated n-ary RE, some approaches [13; 31] treat n-ary RE in _hypergraph-based schema_ or _role-based schema_ as a binary classification problem and predict whether the composition of n-ary information in a document is valid or not. However, these methods extract n-ary information in fixed arity, which are not flexible. Moreover, some event extraction methods [16; 17; 9] propose different event trigger and argument extraction techniques, which can achieve n-ary RE in _event-based schema_. Recently, CubeRE  proposes an automated n-ary RE method in _hyper-relational schema_, which extends the table-filling extraction method to n-ary RE with cube-filling. However, these methods can only model one of the useful NKG schemas with limited extraction accuracy.

In this paper, we propose the first fine-grained n-ary RE framework Text2NKG for NKG construction in four example schemas, proposing a span-tuple multi-label classification method with hetero-ordered merging and output merging to improve the accuracy of fine-grained n-ary RE extraction in all NKG schemas substantially.

## 3 Preliminaries

**Formulation of NKG.** An NKG \(=\{,,\}\) consists of an entity set \(\), a relation set \(\), and an n-ary fact (n\(\)2) set \(\). Each n-ary fact \(f^{n}\) consists of entities \(\) and relations \(\). For hyper-relational schema : \(f^{n}_{hr}=(e_{1},r_{1},e_{2},\{r_{i-1},e_{i}\}_{i=3}^{n})\) where \(\{e_{i}\}_{i=1}^{n}\), \(\{r_{i}\}_{i=1}^{n-1}\). For event-based schema : \(f^{n}_{ev}=(r_{1},\{r_{i+1},e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\), \(\{r_{i}\}_{i=1}^{n+1}\). For role-based schema : \(f^{n}_{ro}=(\{r_{i},e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\), \(\{r_{i}\}_{i=1}^{n}\). For hypergraph-based schema : \(f^{n}_{hg}=(r_{1},\{e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\), \(r_{1}\).

Problem Definition.Given an input sentence with \(l\) words \(s=\{w_{1},w_{2},...,w_{l}\}\), an entity \(e\) is a consecutive span of words: \(e=\{w_{p},w_{p+1},...,w_{q}\}_{s}\), where \(p,q\{1,...,l\}\), and \(_{s}=\{e_{j}\}_{j=1}^{m}\) is the entity set of all \(m\) entities in the sentence. The output of n-ary relation extraction, \(R()\), is a set of n-ary relational facts \(_{s}\) in given NKG schema in \(\{f_{hr}^{n},f_{rv}^{n},f_{ro}^{n},f_{hg}^{n}\}\). Specifically, each n-ary relational fact \(f^{n}_{s}\) is extracted by multi-label classification of one of the ordered span-tuple for \(n\) entities \([e_{i}]_{i=1}^{n}_{s}\), forming an answer label-list for \(n_{r}\) relations \([r_{i}]_{i=1}^{n_{r}}\), where \(n\) is the arity of the extracted n-ary relational fact, and \(n_{r}\) is the number of answer relations in the fact, which is determined by the given NKG schema: \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n-1}\), when \(f^{n}=f_{hr}^{n}\), \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n+1}\) when \(f^{n}=f_{ev}^{n}\), \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n}\) when \(f^{n}=f_{ro}^{n}\), and \(R([e_{i}]_{i=1}^{n})=[r_{1}]\) when \(f^{n}=f_{hg}^{n}\).

## 4 Methodology

In this section, we first introduce the overview of the Text2NKG framework, followed by the span-tuple multi-label classification, training strategy, hetero-ordered merging, and output merging.

### Overview of Text2NKG

Text2NKG is a fine-grained n-ary relation extraction framework built for n-ary relational knowledge graph (NKG) construction. The input to Text2NKG is natural language text tokens labeled with entity span in sentence units. First, inspired by , Text2NKG encodes the entities using BERT-based Encoder  with a packaged levitated marker for embedding. Then each arrangement of ordered span-tuple with three entity embeddings will be classified with multiple labels, and the framework will be learned by the weighted cross-entropy with a null-label bias. In the decoding stage, in order to filter the n-ary relational facts whose entity compositions have isomorphic hetero-ordered characteristics, Text2NKG proposes a hetero-ordered merging strategy to merge the label probabilities of \(3!=6\) arrangement cases of span-tuples composed of the same entities and filter out the output 3-ary relational facts existing non-conforming relations. Finally, Text2NKG combines the output 3-ary relational facts to form the final n-ary relational facts with output merging.

### Span-tuple Multi-label Classification

For the given sentence token \(s=\{w_{1},w_{2},...,w_{l}\}\) and the set of entities \(_{s}\), in order to perform fine-grained n-ary RE, we need first to encode a span-tuple (\(e_{1},e_{2},e_{3}\)) consisting of every arrangement of three ordered entities, where \(e_{1},e_{2},e_{3}_{s}\). Due to the high time complexity of training every

Figure 3: An overview of Text2NKG extracting n-ary relation facts from a natural language sentence in hyper-relational NKG schema for an example.

span-tuple as one training item, inspired by , we achieve the reduction of training items by using packed levitated markers that pack one training item with each entity in \(_{s}\) separately. Specifically, in each packed training item, a pair of solid tokens, [S] and [/S], are added before and after the packed entity \(e_{S}=\{w_{{}_{PS}},...,w_{{}_{PS}}\}\), and (\(|_{s}|-1\)) pairs of levitated markers, [L] and [/L], according to other entities in \(_{s}\), are added with the same position embeddings as the beginning and end of their corresponding entities span \(e_{L_{i}}=\{w_{{}_{P_{L_{i}}}},...,w_{{}_{M_{L_{i}}}}\}\) to form the input token \(\):

\[=&\{w_{1},...,[S],w_{{}_{PS} },...,w_{{}_{PS}},[/S],...,\\ & w_{{}_{P_{L_{i}}}}[L],...,w_{{}_{T_{L_{i}}}}[/L],...,w_{ {}_{l}}\}.\] (1)

We encode such token by the BERT-based pre-trained model encoder :

\[\{h_{1},h_{2},...,h_{t}\}=(),\] (2)

where \(t=||\) is the input token length, \(\{h_{i}\}_{i=1}^{t}^{d}\), and \(d\) is embedding size.

There are several span-tuples (\(A,B,C\)) in a training item. The embedding of first entity \(h_{A}^{2d}\) in the span-tuple is obtained by concat embedding of the solid markers, [S] and [/S], and the embeddings of second and third entities \(h_{B},h_{C}^{2d}\) are obtained by concat embeddings of levitated markers, [L] and [/L] with all \(A_{m-1}^{2}\) arrangement of any other two entities in \(_{s}\). Thus, we obtain the embedding representation of the three entities to form \(A_{m-1}^{2}\) span-tuples in one training item. Therefore, every input sentence contains \(m\) training items with \(mA_{m-1}^{2}=A_{m}^{3}\) span-tuples for any ordered arrangement of three entities.

We then define \(n_{r}\) linear classifiers, each of which consists of 3 feedforward neural networks \(\{_{i}^{k}\}_{i=1}^{n_{r}},k=1,2,3\), to classify the span-tuples for multiple-label classification. Each classifier targets the prediction of one relation \(r_{i}\), thus obtaining a probability lists \((_{i})_{i=1}^{n_{r}}\) with all relations in given relation set \(\) plus a null-label:

\[_{i}=_{i}^{1}(h_{A})+_{i}^{2}(h_{B})+ _{i}^{3}(h_{C}),\] (3)

where \(_{i}^{k}^{2d(||+1)}\), and \(_{i}^{(||+1)}\).

### Training Strategy

To train the \(n_{r}\) classifiers for each relation prediction more accurately, we propose a data augmentation strategy for span-tuples. Taking the _hyper-relational schema_ as an example, given a hyper-relational fact (\(A,r_{1},B,r_{2},C\)), we consider swapping the head and tail entities, and changing the main relation to its inverse (\(B,r_{1}^{-1},A,r_{2},C\)), as well as swapping the tail entities with auxiliary values, and the main relation with the auxiliary key (\(A,r_{2},C,r_{1},B\)), also as labeled training span-tuple cases. Thus \(R_{hr}(A,B,C)=(r_{1},r_{2})\) can be augmented with \(3!=6\) orders of span-tuples:

\[R_{hr}(A,B,C)=(r_{1},r_{2}),\\ R_{hr}(B,A,C)=(r_{1}^{-1},r_{2}),\\ R_{hr}(A,C,B)=(r_{2},r_{1}),\\ R_{hr}(B,C,A)=(r_{2},r_{1}^{-1}),\\ R_{hr}(C,A,B)=(r_{2}^{-1},r_{1}),\\ R_{hr}(C,B,A)=(r_{1},r_{2}^{-1}).\] (4)

For other schemas, we can also obtain 6 fully-arranged cases of labeled span-tuples in a similar way, as described in Appendix A. If no n-ary relational fact exists between the three entities of span-tuples, then relation labels are set as null-label.

Since most cases of span-tuple are null-label, we set a weight hyperparameter \((0,1]\) between the null-label and other labels to balance the learning of the null-label. We jointly trained the \(n_{r}\) classifiers for each relations by cross-entropy loss \(\) with a null-label weight bias \(_{}\):

\[=-_{i=1}^{n_{r}}_{}(_{i}[r_{i}])}{_{j=1}^{||+1}(_{ij})}),\] (5)

where \(_{}=[,1.0,1.0,...1.0]^{(||+1)}\).

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

the same BERT-large encoder, reflecting Text2NKG's excellent performance. Figure 4(a) and 4(b) intuitively show the changes of evaluation metrics and answers of facts in the dev set during the training of Text2NKG. It is worth noting that Text2NKG exceeds 90% in precision accuracy, which proves that the model can obtain very accurate n-ary relational facts and provides a good guarantee for the quality of fine-grained NKG construction.

### Results on Various NKG Schemas (RQ2)

As shown in Table 3, besides _hyper-relational schema_, Text2NKG also accomplishes the tasks of fine-grained n-ary RE in three other different NKG schemas on HyperRED, which demonstrates good utility. In the added tasks of n-ary RE for event-based, role-based, and _hypergraph-based schemas_, since no model has done similar experiments at present, we used event extraction or unified extraction methods such as Text2Event , UIE , and LasUIE  for comparison. Text2NKG still works best in these schemas, which demonstrates good versatility.

### Ablation Study (RQ3)

Data augmentation (DA), null-label weight hyperparameter (\(\)), and hetero-ordered merging (HM) are the three main components of Text2NKG. For the different Text2NKG variants as shown in Table 2, DA, \(\), and HM all contribute to the accurate results of our complete model. By comparing the differences, we find that HM is most effective by combining the probabilities of labels of different orders, followed by DA and \(\).

### Analysis of Null-label Weight Hyperparameters (RQ4)

We compared the effect for different null-label weight hyperparameters (\(\)). As shown in Figure 4(c), the larger the \(\), the greater the learning weight of null-label compared with other lables, the more relations are predicted as null-label. After filtering out the facts having null-label, fewer facts are extracted, so the precision is generally higher, and the recall is generally lower. The smaller the \(\), the more relations are predicted as non-null labels, thus extracting more n-ary relation facts, so the recall is generally higher, and the precision is generally lower. Comparing the results of \(F_{1}\) values for different \(\), it is found that \(=0.01\) works best, which can be adjusted in practice according to specific needs to obtain the best results.

### Analysis of N-ary Relation Extraction in Different Arity (RQ5)

Figure 5(a) shows the number of n-ary relational facts extracted after output merging and the number of the answer facts in different arity during training of Text2NKG on the dev set. We find that, as the training proceeds, the final output of Text2NKG converges to the correct answer in terms of the number of complete n-ary relational facts in each arity, achieving implementation of n-ary RE in indefinite arity unsupervised, with good scalability.

Figure 4: (a) Precision, Recall, and \(F_{1}\) changes in the dev set during the training of Text2NKG. (b) The changes of the number of true facts, the number of predicted facts, and the number of predicted accurate facts during the training of Text2NKG. (c) Precision, Recall, and \(F_{1}\) results on different null-label hyperparameter (\(\)) settings.

### Computational Efficiency (RQ6)

As mentioned in Section 4.2, the main computational consumption of Text2NKG is selecting every span-tuple of three ordered entities to encode them and get the classified labels in multiple-label classification part. If we adopt an traversal approach with each span-tuple in one training items, the time complexity will be O(\(m^{3}\)). To reduce the high time complexity of training every span-tuple as one training item, Text2NKG uses packed levitated markers that pack one training item with each entity in \(_{s}\) separately. We obtain the embedding representation of the three entities to form \(A^{2}_{m-1}\) span-tuples in one training item. Every input sentence contains \(m\) training items with \(mA^{2}_{m-1}=A^{3}_{m}\) span-tuples for any ordered arrangement of three entities for multiple-label classification. Therefore, the time complexity decreased from O(\(m^{3}\)) to O(\(m\)).

### Case Study (RQ7)

Figure 5(b) shows a case study of n-ary RE by a trained Text2NKG. For a sentence, "He was born in Skripenbeck, near York and attended Pocklin.", four structured n-ary RE can be obtained by Text2NKG according to the requirements. Taking the _hyper-relational schema_ for an example, Text2NKG can successfully extract one n-ary relational fact consisting of a main triple [He, educated at, Pocklington], and two auxiliary key-value pairs {start time:1936}, {end time:1943}. This intuitively validates the practical performance of Text2NKG on fine-grained n-ary RE to better contribute to NKG construction.

### Comparison with ChatGPT (RQ8)

As shown in Table 2 and Table 3, we compared the extraction effects under four NKG schemas of the supervised Text2NKG with the unsupervised ChatGPT and GPT-4. We found that these large language models cannot accurately distinguish the closely related relations in the fine-grained NKG relation repository, resulting in their F1 scores ranging around 10%-15%, which is much lower than the performance of Text2NKG. On the other hand, the limitation of Text2NKG is that its performance is confined within the realm of supervised training. Therefore, in future improvements and practical applications, we suggest combining small supervised models with large unsupervised models to balance solving the cold-start and fine-grained extraction, which is detailed in Appendix G.1.

## 6 Conclusion

In this paper, we introduce Text2NKG, a novel framework designed for fine-grained n-ary relation extraction (RE) aimed at constructing N-ary Knowledge Graphs (NKGs). Our extensive experiments demonstrate that Text2NKG outperforms all existing baseline models across a wide range of fine-grained n-ary RE tasks. Notably, it excels in four distinct schema types: hyper-relational, event-based, role-based, and hypergraph-based. Furthermore, we have extended the HyperRED dataset, transforming it into a comprehensive fine-grained n-ary RE benchmark that supports all four schemas.

Figure 5: (a) The changes of the number of extracted n-ary RE in different arity, where ”pred_n” represents the number of extracted n-ary facts with different arities by Text2NKG, and ”ans_n” represents the ground truth. (b) Case study of Text2NKG’s n-ary relation extraction in four schemas on HyperRED.