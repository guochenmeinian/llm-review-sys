# DDCoT: Duty-Distinct Chain-of-Thought Prompting

for Multimodal Reasoning in Language Models

 Ge Zheng\({}^{1}\)  Bin Yang\({}^{1}\)  Jiajin Tang\({}^{1}\)  Hong-Yu Zhou\({}^{2}\)  Sibei Yang\({}^{1}\)

\({}^{1}\)ShanghaiTech University  \({}^{2}\)The University of Hong Kong

Project Page: https://toneyaya.github.io/ddcot/

Equal contribution.Corresponding author. yangsb@shanghaitech.edu.cn

###### Abstract

A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: _"keeping critical thinking"_ and _"letting everyone do their jobs"_ in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.

## 1 Introduction

One of the fundamental aspirations of AI systems is to address complex tasks with reliability and efficiency that are aligned with human capabilities . In tackling such complex tasks, humans rely on multi-step reasoning that integrates information from various modalities. Recently, language models (LMs)  have shown remarkable progress in a range of multi-step reasoning tasks by prompting  or fine-tuning  with the chain of thought (CoT) that mimics the human reasoning process.

However, most studies on CoT reasoning have focused solely on the language modality , with minimal attention to multimodal contexts. UnifiedQA  and MM-CoT  are the pioneering works of eliciting CoT reasoning in vision-and-language multimodality. They employ multimodal inputs, as illustrated in Figure 1(a), necessitating training for the generation of the intermediate reasoning steps (_i.e._, rationales) either as an explanation accompanied by an answer or as reasoning prior to inferring the answer. Despite the benefits of these multimodal CoT reasoning methods in enhancing the ability of LMs to answer multimodal science questions, several significant challenges have impeded their deployment: (1) **Labor-intensive annotation**: manual annotation for rationales is time-consuming, costly, and challenging to ensure consistency and completeness inmulti-step CoT reasoning, thereby restricting the utilization of these approaches. (2) **Flexibility**: the effectiveness of multimodal rationales generated by existing methods  is limited to either zero-shot prompting or fine-tuning learning with LMs, as demonstrated in Figure 1(b). Our observations indicate that the rationales generated by MM-CoT fail to provide any benefits for zero-shot prompting, while the rationales generated solely by GPT-3 do not enhance the reasoning ability of the fine-tuning model. (3) **Generalizability**: the current multimodal CoT reasoning approaches  exhibit limited generalizability to problems that require novel and unseen reasoning paths. As shown in Figure 1(d) and (e), basic science questions that are out-of-distribution pose a challenge for these models. Moreover, models trained on a specific subset, which includes arbitrary two subjects of questions in natural science, social science, and language science, perform worse when applied to another subject of questions (see Section 4.1). (4) **Explainability**: the objective of multimodal CoT extends beyond inferring answers to include providing explanations, yet the interpretability of current generated rationales still requires further improvement.

This study aims to explore overcoming the aforementioned challenges and develop a _zero-shot_, _generalizable_ and _explainable_ approach for generating rationales to enhance the reasoning abilities of LMs in _both zero-shot and fine-tuning learning_. To achieve this, we first probe the the incorporation of rationales into multimodal reasoning and determine a two-step rationale generation and utilization process. Then we explore (1) the role of rationales in the utilization phase, where they serve as authentic knowledge providers, and (2) the challenges of intensified hallucinations in rationale generation phase. Based on these factors, we formulate the following approach.

To generate general multimodal rationales, we propose a novel multimodal CoT prompting approach called Duty-Distinct Chain-of-Thought Prompting (DDCoT), which generate multimodal rationales using language-only LLMs , considering the explored factors above. For the first factor, our key insight is _"critical thinking - keeping a healthy dose of skepticism"_: explicitly indicating the uncertainty in the rationale generation process is critical since it helps improve the correctness of the generated rationales, which serve as key inputs guiding LMs in the thinking process of inferring answers, particularly in the zero-shot setting. For the second factor, our key insight is to _"let everyone do their jobs - divide labor and work together"_: it is necessary to prompt LLMs to explicitly identify the reasoning and recognition responsibility of LLMs and off-the-shelf visual models, in order to overcome the language hallucination issue that arises when directly generating rationales from interleaved multimodal inputs. Based on this, we propose a sequential process of negative-space prompting, visual recognition, and joint reasoning to perform interleaved reasoning and recognition. More detailed analysis of the observed factors and motivation behind the two insights are elaborated in Section 3.1 and Section 3.2.

To utilize the generated rationales to facilitate multimodal question answering of LMs, we leverage them as inputs to explicitly guide the LMs' chain of thought and the attention to multimodal inputs in both zero-shot prompting and fine-tuning learning. We combine generated rationales with problem

Figure 1: Comparison of existing multimodal CoT methods and our DDCoT on (a), (c), (d) and (e) an out-of-distribution example to illustrate the generalizability and (b) performance of zero-shot and fine-tuning learning, demonstrating that we are the first to generate general multimodal rationales.

statements as inputs to the LLMs for zero-shot learning. For fine-tuning learning, we propose deep-layer prompting (DLP) and rational-compressed visual embedding (RCVE) to utilize the rationales better to filter, encode, and jointly infer over interleaved multimodal inputs.

To sum up, our contributions are three-fold: (1) This work is the first to study the zero-shot multimodal rationale generation. We deeply analyze the challenges and insights in multimodal CoT for the rationale generation: rationale sensitive in zero-shot prompting, the knowledge-required in fine-tuning due to catastrophic forgetting, and hallucinations intensified due to interleaved multimodal inputs. We hope these insights could benefit future research work. (2) We propose a novel DDCoT prompting to maintain a critical attitude and identify reasoning and recognition responsibilities through the combined effect of negative-space design and deconstruction. The resulting rationales can directly be part of multimodal inputs to improve the reasoning abilities of LMs in both zero-shot prompting and fine-tuning learning. (3) With the rationales, our methods consistently outperform the state-of-the-art LMs, improving both GPT-3 and UnifiedQA by +\(2.53\%\) and +\(8.23\%\) respectively on questions that have image context, while exhibiting impressive generalizability and explainability.

## 2 Related Work

**CoT Reasoning of LLMs.** LLMs have been demonstrated successful in natural language processing. Recently, zero-shot  and few-shot [60; 44] multi-step thinking prompts have been found to significantly improve LLMs' reasoning ability, thus such chain-of-thought (CoT) methods have attracted growing interest. Some works are interested in example selection in terms of similarity [44; 32], diversity , and complexity . Other methods optimize the reasoning pipeline, exploring to introduce programming approach , explicit decomposition of problems [72; 22] or calibration coordination across multiple rationales [59; 27; 10]. Inspired by these works, we focus on extending CoT reasoning to multimodality, whilst tackling inherent complications that emerge from therefrom.

**Transferring Specialized Reasoning Skills to Small Models.** In addition to research on CoT reasoning in LLMs, some studies conduct CoT on models with a smaller number of parameters. The works [35; 13] distill the CoT capabilities of LLMs into smaller models, facilitating the performance of smaller models in specific tasks. Unlike them to generate rationales in inference, our work utilizes the generated rationales via CoT reasoning from LLMs to explicitly guide the understanding of the image for multimodal reasoning.

**Cross-modal CoT Reasoning.** The pioneering work  for multimodal CoT proposes ScienceQA, a dataset consisting of scientific questions involving multimodality with annotated rationales. They perform zero-shot prompting on GPT-3 and fine-tuning learning on UnifiedQA  to generate rationales and answers simultaneously.

The following work MM-CoT  devises a two-stage framework in which the model initially learns to generate rationales based on the ground-truth annotations, and then utilizes all available information to generate the final answer. However, their generated rationales can only exclusively benefit either zero-shot or fine-tuning learning.

**Integrate Visual Modality to Language Models.** With the demonstrated capability of LLMs to incorporate open-world commonsense knowledge [40; 43], an increasing number of studies are focused on making visual modality available to well-established LLMs for solving complex visual and multimodal problems. Some approaches [16; 26; 67; 28; 69; 6; 12; 63] incorporate additional training data to align image features with linguistic space. Others [33; 62; 47] take advantage of the scheduling ability of LLMs to dynamically integrate off-the-shelf vision models, extracting image information into text form explicitly. Unlike them directly integrating multimodal inputs once, we explicitly identify the uncertainty in rationale generation and complement visual information step by step.

## 3 Method

Our work focus on how to best put visual information in the text to generate generalizable and explainable rationales. In this section, we first introduce the concepts and motivation in exploration (Sec 3.1), and then present our concrete method designs for rationale generation (Sec 3.2) and utilization (Sec 3.3).

### Motivation: Leverage Rationales for Multimodal Reasoning

The success of rationales in unimodal reasoning  motivates us to leverage rationales to enhance both reasoning capabilities and interpretability in multimodal reasoning. In this section, we first investigate the incorporation of rationales into multimodal reasoning in Section 3.1.1, and then examine the roles of rationales in enhancing multimodal reasoning in Sections 3.1.2, while also illuminating the challenges to generate general rationales with LLMs in Section 3.1.3.

#### 3.1.1 Two-step Reasoning Process: Multimodal Rationales Generation and Utilization

Following , we first prompt LLMs to generate answers accompanied by rationales and observe that it cannot improve multimodal reasoning capabilities, _i.e._, \(74.04\%\) accuracy of generating only answers and \(75.17\%\) accuracy of generating both rationales and answers .

Moreover, as Figure 2(a) illustrates, despite sufficient image information provided in the form of image caption, the LLM, GPT-3  in this case, fails to jointly reason the question and the caption to infer answer. The possible reason is that LLMs have difficulty understanding dense image information.

Motivated by that people usually extract key information from images in the context of questions, rather than indiscriminately focusing on all dense information, _we explore to provide the rationale as a structured logical chain to explicitly guide the understanding of the image_. As shown in Figure 2(a) and (d), incorporating the rationale as input facilitates the model's multimodal reasoning capabilities. Therefore, we employ a two-step reasoning process: multimodal rationale generation, and their subsequent utilization.

#### 3.1.2 Roles of Rationales Differ in Zero-shot and Fine-tuning Learning

In further exploration, we find that the effect of rationale varies in zero-shot and fine-tuning models.

**Rationale-sensitive reasoning for zero-shot prompting.** Upon conducting zero-shot prompting on LLMs, such as ChatGPT , we find that the model tends to reason in line with the input rationales. For example, without referring to additional rationales, the common sense knowledge encoded in ChatGPT allows it to answer the basic biological question "which nutrients are mainly provided by the foods in the image", as shown in Figure 3(c). However, the assertion "oranges are known to be a good source of fats" in misleading rationale results in a failure of ChatGPT to answer the same question (see Figure 3(a)). Given language models' tendency to reason primarily based on input rationales, the accuracy of rationales becomes crucial for zero-shot prompting on LLMs.

Figure 3: An example for disparate roles of rationales in zero-shot and fine-tuning scenarios, with question depicted in Figure 1.

Figure 2: An example shows the importance of input rationales in multimodal reasoning as well as the disparate roles of rationales in zero-shot and fine-tuning scenarios.

Knowledge-required reasoning for fine-tuning.In contrast to zero-shot prompting learning, the fine-tuning models not only require the correctness of rationales but also exhibit a strong reliance on comprehensive prior knowledge embedded in rationales. As shown in Figure 3(d) and 2(b), lacking common sense knowledge, such as the nutrient mainly provided by oranges and factors affecting magnetic force, the fine-tuning model fails to answer these two questions. Compared to zero-shot prompting, the fine-tuning model showcases enhanced error tolerance (as shown in Figure 2(c)), while concurrently showing increased susceptibility to knowledge deficiencies. This stems from the catastrophic forgetting during fine-tuning in LMs [14; 4; 65].

#### 3.1.3 Challenge Lies in LLMs with Multimodality: Hallucinations Intensified

To generate general rationales simultaneously fulfill the above two roles for assisting language models in understanding multimodal information, we first analyze the state-of-the-art method , which is trained to generate rationales relying on manual annotations. However, it generates rationales without considering their correctness, thereby leading to the risk of misleading language models. In addition, based on training with manually annotated rationales, it lacks the ability to generate rationales that encompass sufficient relevant knowledge for out-of-distribution questions, leading to a significant performance drop (see Figure 3(b) and the left table in Table 2). Unlike MM-CoT , we resort to LLMs to generate rationales with zero-shot prompting, leveraging their intrinsic capacity for generalization. We further explore the strategies to generate rationales.

Uni-modal rationales have limited effect.We start by directly prompting _"let's think step by step"_ without any image information, resulting in visually irrelevant rationale (see Figure 4(a)) and poor performance (see row "w/ naive R" in the right table in Table 2).

Interleaved information exacerbates hallucinations.To generate multimodal rationales involving image information, the challenge lies in mitigating the language hallucinations [24; 36; 17; 2], which is exacerbated by providing interleaved multimodal information at once. A naive attempt is to extract the image caption and combine it with the question as a joint prompt to generate rationales. Despite the integration of image information, the generated rationales remain suboptimal for image-related questions, which is not aligned with our initial expectations. Upon analyzing various cases, we observe that interleaved information predisposes LLMs towards hallucination, generating fabricated visual information. As illustrated in Figure 4, we extracted the caption "a food web" from the image, which lacks the necessary information. Consequently, the language model resorts to imagining image-related information such as "the primary consumers are the zooplankton and the kelp" (see Figure 4(b)), leading to difficulty in distinguishing reliable knowledge from the language hallucinations.

### Zero-shot DDCoT Prompting for Multimodal Rationale Generation

We generate general multimodal rationales based on the following key insights, which are derived from Section 3.1:

(1) Utilize two-step reasoning process, considering that LLM has difficulty jointly reasoning multimodal information directly (Section 3.1.1).

Figure 4: An example of the hallucination challenge which is exacerbated by providing interleaved multimodal information at once in generating rationales with LLMs.

(2) Generate rationales that meet the requirements of both zero-shot and fine-tuning learning, filled with knowledge and requiring fidelity (Section 3.1.2).

(3) Alleviate the intensified hallucinations from interleaved information (Section 3.1.3).

Accordingly, we propose Duty-Distinct Chain-of-Thought Prompting (DDCoT), which encompasses three steps: (1) We utilize LLMs' intrinsic knowledge to generate multimodal rationales. (2) We explicitly cue the LLMs to differentiate the responsibilities of reasoning and recognition step by step. (3) We explicitly mark the negative space for uncertain parts, emphasizing critical thinking in rationale generation. Our DDCoT jointly exploits the reasoning ability in LLMs and the image understanding capability of visual question-answering models for general multimodal rationale generation.

**Breaking Reasoning Down to Recognition Steps with Negative-Space Prompting.**

First, with the given question, context, and options, we prompt the LLM  to deconstruct the input question into a sequence of basic sub-questions, breaking the complex reasoning chain into simple steps, as shown in Figure 5. Diverging prior works in NLP community , we introduce the following prompting designs: (1) We utilize a single-stage deconstruction to simplify the problem-solving process. Specifically, we employ the instruction _"please think step-by-step and deconstruct the question down to necessary sub-questions"_ to obtain the sub-question sequence at once. (2) Then we explicitly prompt the LLMs to determine whether each sub-question can be answered without visual information. In cases where sub-questions involving visual recognition are unanswerable, the LLMs are instructed to answer "uncertainty" as a negative space. We provide the model with the following prompt: _"Assume that you do not have any information about the picture, try to answer the sub-question and formulate the corresponding sub-answer as 'Uncertain' if the sub-question cannot be determined"_. By adopting a critical stance towards sub-questions and introducing the explicit assumption of invisibility, we successfully mitigate hallucination in LLMs when dealing with sub-questions involving images, reducing factual errors.

**Visual Recognition to Obtain Visual Complements.**

However, the obstruction caused by negative space prevents LLMs from directly activating the chain-of-thought reasoning. In order to fill the negative space, we leverage the image understanding capability of the off-the-shelf models to acquire visual information as visual complements. Specifically, we employ the visual question answering (VQA) model  to individually answer the sub-questions with negative space, which correspond to simple visual recognition problems. Note that our approach can solely leverage the basic visual recognition capability of the VQA model and robust to its possible inference errors thanks to the following joint reasoning integration.

**Integrate to Joint Reasoning.**

With a sequence of complete sub-answers including visual recognition results, we resort to LLMs again to integrate the information and engage in reasoning processes

Figure 5: An overview of our DDCoT and its utilization to improve the multimodal reasoning of LLMs. Note that although errors encounter in the second sub-problem during visual recognition, the language model rectifies this error in the joint reasoning step with critical thought.

to derive the rationales. Incorporating our obtained sub-questions and corresponding sub-answers as supplementary information, we prompt LLMs _"think step by step"_ to perform joint reasoning with linguistic and visual information and generate multimodal rationales. Furthermore, we prompt explicitly with _"note that the supplementary information given may not always be valid"_ and _"select valid information to form the rationale"_ to encourage a critical attitude towards the supplementary information and thoughtful analysis. By explicitly highlighting uncertainty, LLMs are able to pay attention to the origin question information and intrinsic knowledge, effectively filtering and even rectifying supplementary knowledge to derive more reasonable answers, as shown in Figure 5.

### Utilization of Multimodal Rationales in General Scenarios

In this section, we introduce the utilization of our rationales to achieve multimodal reasoning, both for zero-shot and fine-tuning learning. For zero-shot learning, we consider prompting the model with rationales, including established knowledge and a reasoning chain of thought. Additionally, for fine-tuning, our proposed rationales, in combination with our proposed deep-layer prompting and rationale-compressed visual embedding, improve deep multimodal understanding and reasoning.

**Utilization for Zero-shot Prompting.** As shown in Figure 5, for LLMs like ChatGPT, we utilize zero-shot prompts that combine rationales generated by Section 3.2 with our problem statements as inputs to the model. Our rationales generated by explicitly questioning the uncertainty with improving reliability can facilitate LLMs to make more precise assessments and exhibit fewer hallucinations.

**Utilization for Fine-tuning Learning.** The framework of the fine-tuning process is shown in Figure 5. During fine-tuning, we introduce learnable prompts in both shallow and deep layers to align cross-modal information and facilitate multimodal reasoning. In addition, instead of directly inputting the entire image into the LM, we use the multimodal rationales to guide the filtering of key image features as visual input embeddings for the LM.

* **Deep-Layer Prompting (DLP)** is designed to assist in the alignment and joint reasoning of multimodal inputs at multiple levels. It employs not only learnable prompts to facilitate the alignment of visual and linguistic semantics at a shallow level [74; 73; 51; 34] but also utilizes explicit rationales to jointly encode multimodality by learning different prompts for each encoder layer [18; 21; 30; 29]. Specifically, we randomly initialize learnable prompts \(P^{L N_{p} C}\) for \(L\) encoder layers. For the \(l\)-th layer, we load the prompt \(P_{l}^{N_{p} C}\) at the begging and end of the visual input embeddings.
* **Rational-Compressed Visual Embedding (RCVE).** Instead of inputting visual features into the LM directly, we compress visual input embeddings according to our multimodal rationales. Specifically, we leverage the rationales to jointly comprehend the textual and visual contexts, serving as the prior knowledge to filtering visual features. Given the text embeddings \(T^{N_{t} C}\) that contains context and rationale, the global visual input \(V_{g}^{C}\), and the local visual inputs \(V_{g}^{N_{v} C}\), we first update the global visual feature based its similarity to text embeddings as follows, \[V_{t}=(V_{g},T),\] (1) where \(V_{t}^{C}\) is the updated visual feature and \((,)\) is the standard multi-head cross-attention module . Next, instead of directly using the updated visual feature \(V_{t}\) to capture relevant local visual inputs \(V_{l}\), we introduce low-rank intermediate vectors as crucial mediators for filtering local inputs as follows: \[V_{r}=((V_{t})),V=(V_{r},V_{l}),\] (2) where \(()\) denotes a three-layer linear layer with activation function, \(V_{r}^{N_{r} C_{r}}\) represents \(N_{r}\) low-rank vectors, and \(V\) is the final visual embeddings to input to the LM's encoder.

## 4 Experiment

**ScienceQA benchmark ** is the first multimodal science question-answer dataset comprising 21,000 questions with multiple choices and images. Following previous works [71; 31], we divide ScienceQA into training, validation, and test sets, which contain 12,726, 4,241, and 4,241 examples, respectively. Additionally, the questions in ScienceQA can be categorized into three domains: natural science (NAT), social science (SOC), and language science (LAN).

**Implementation.** We conduct zero-shot experiments on LLMs including ChatGPT  and GPT-3 . Since LLMs are unable to accept images as visual input, we convert images into captionsvia BLIP-2  as extra input to LLMs. Furthermore, following previous works , we adopt UnifiedQA  as our base model for fine-tuning. We use CLIP ViT-L/14  as the visual encoder to extract the global visual input \(V_{g}\) and the local visual input \(V_{l}\). The hyperparamters \(N_{p}\), \(N_{r}\) and \(C_{r}\) are \(3\), \(16\) and \(4\), respectively. We train our model for 30 epochs with a learning rate of 1e-4 and batch size of 16. All experiments are implemented by PyTorch  and HuggingFace  and conducted on NVIDIA Tesla A40 GPUs. We employ accuracy as our evaluation metric and furnish comprehensive results across each domain.

### Comparison with State-of-the-Arts in Zero-shot Prompting and Fine-tuning Learning

Table 1 shows the comparison of our DDoC\(\) with the state-of-the-art models  on zero-shot and fine-tuning benchmarks. Our approach consistently achieves superior performance compared to previous methods. Note that we further report the results of the recent works  which have not been published but have been preprinted in the research community.

**Zero-shot Prompting.** Regarding zero-shot prompting, our zero-shot approach outperforms the published state-of-the-art few-shot method  by \(2.92\%\) on GPT-3 and \(1.84\%\) on ChatGPT, respectively. Compared with the concurrent work Chameleon , which incorporates a vast array of external tools, our method maintains comparable performance, even attaining a \(1.73\%\) enhancement on the IMG split. These suggest that our DDoC\(\), integrated with negative-space prompting, effectively curtails factual errors, guaranteeing the accuracy of multimodal reasoning in LLMs. Furthermore, we observe that the performance of GPT-3 (\(67.43\%\)) and ChatGPT (\(67.92\%\)) in a few-shot manner is similar, and the amplification of our technique in the IMG branch escalates as the performance of the language models strengthens (\(2.53\%\) for GPT-3 and \(4.61\%\) for ChatGPT). The possible reason is language models rarely acquire an innate understanding of dense image information, while explicit rationales guidance triggers the inherent reasoning capability of such models.

**Fine-tuning Learning.** As shown in Table 1, our DDoC\(\) has achieved exceptional accuracy on fine-tuning benchmarks. DDoC\(\) significantly surpasses the base model UnifiedQA  by \(17.22\%\) and \(21.96\%\) on avg split and IMG split, respectively. These demonstrate that our rationales not only enhance the model's multimodal reasoning ability in a zero-shot setting but also help the fine-tuning of

   Model & Report & Size\(\) & GT-\(\) & NAT & SOC & LAN & TXT & IMG & NO & G1-6 & G7-12 & Avg \\   \\  Random  & NeurIPS\({}^{22}\) & - & - & 40.28 & 46.13 & 29.25 & 47.45 & 40.08 & 33.66 & 39.35 & 40.67 & 39.83 \\ Human  & NeurIPS\({}^{22}\) & - & - & **90.23** & **84.97** & **87.48** & **89.60** & **87.50** & **88.10** & **91.59** & **82.42** & **88.40** \\   \\ 
**GPT-3(CoT)** & NeurIPS\({}^{22}\) & 175B & ✓ & **75.44** & **70.87** & **78.09** & **74.68** & **67.43** & **79.93** & **78.23** & **69.68** & **75.17** \\ ChatGPT(CoT)  & arXiv\({}^{23}\) & 175B & ✓ & 78.82 & 70.98 & 83.18 & 77.37 & 67.92 & 86.13 & 80.72 & 74.03 & 78.31 \\ Chameleon(ChatGPT)  & arXiv\({}^{23}\) & 175B & ✓ & 81.62 & 70.64 & 84.00 & 79.77 & 70.80 & 86.62 & 81.86 & 76.53 & 79.93 \\   \\ 
**Ours(GPT-3)** & & 175B & & 78.60 & **73.90** & **80.45** & **77.27** & **69.96** & **82.93** & **80.65** & **73.50** & **78.09** \\ Ours(ChatGPT) & & 175B & & 80.15 & **76.72** & **82.82** & **78.89** & **72.53** & **85.02** & **82.86** & **75.21** & **80.15** \\   \\   \\  LMadapter  & arXiv\({}^{23}\) & 7B & & 84.37 & 88.30 & 84.36 & 83.72 & 80.32 & 86.90 & 85.83 & 84.05 & 85.19 \\   \\  UnifiedQA  & NeurIPS\({}^{22}\) & 223M & ✓ & 71.00 & **76.04** & **78.91** & **66.42** & **66.53** & **81.81** & **77.06** & **68.82** & **74.11** \\ MMCOT  & arXiv\({}^{23}\) & 223M & ✓ & 87.52 & 77.17 & 85.82 & 87.88 & 82.90 & 86.83 & 84.65 & 85.37 & 84.91 \\ UnifiedQA  & NeurIPS\({}^{22}\) & 223M & 68.16 & 69.18 & 74.91 & 63.78 & **61.38** & **77.84** & **72.98** & 65.00 & **70.12** \\ MMCoT\({}^{}\) & arXiv\({}^{23}\) & 223M & 82.51 & 77.73 & 82.82 & 81.09 & 75.11 & 85.30 & 81.64 & 80.95 & 81.40 \\ Ours & & & 223M & **88.72** & **86.84** & **84.91** & **87.59** & **83.34** & **88.08** & **88.58** & **85.10** & **87.34** \\  & & & & +6.21 & +9.11 & +2.09 & +6.50 & +8.23 & +2.78 & 46.94 & **+4.15** & +5.94 \\   

Table 1: Main results (%). Size = backbone model size. GT-\(\) means models are trained with ground truth rationales. Question classes: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. \({}^{}\) denotes implementation by removing ground truth rationales when fine-tuning.

LMs to achieve multimodal alignment and joint inference. In comparison to the CoT-based approach MM-CoT  which uses annotated rationales for training, our multimodal rationales, generated through zero-shot CoT prompting, still enhance performance by an average of \(2.43\%\).

**Generalization.** As shown in Table 2(a), we conduct a supplementary experiment to evaluate the model's generalization capabilities after fine-tuning. By designating two domains for visibility during training, we report the accuracy of the questions in the unseen remaining domain within the test set. Across all three divisions, our approach consistently surpasses MM-CoT by \(15.5\%\), \(9.6\%\), and \(12.2\%\), respectively. These results demonstrate the excellent scalability of our rationales when handling out-of-distribution data.

### Analysis Effects of Visual Modality, Rationale Generation, and Fine-tuning Components

**The effects of different modalities of visual information.** Table 2 (b) presents the effects of different forms of image information on the fine-tuning models: (1) We start by directly adding captions or image inputs to the language only T5 based baseline. The former results in an improvement of \(2.33\%\) on IMG split. However, the absence of additional components and the direct usage of images as input leads to poor performance. The result suggests that models with a confined number of parameters encounter difficulties aligning images with text on small datasets. (2) Nevertheless, with the aid of our well-designed components, including deep-layer prompting (DLP) and rational-compressed visual embedding (RCVE), the language model effectively harnesses the information gleaned from images, thereby achieving the comparable performance of \(75.16\%\) on IMG split with the model using captions as inputs. (3) Despite both methods enhancing performance on IMG split, the improvement remains unsatisfactory due to the language model's difficultly in comprehending image information without guidance, which is also observed in the zero-shot scenario delineated in Section 3.1. Simultaneously, we observe a slight degradation in the performance of the text split, which is potentially attributed to the disparity in inputs between IMG and TXT splits in training data.

**The effects of rationale generation components.** As shown in Table 2(b) and Table 3, we further demonstrate the effectiveness of the components of our DDCoT prompting for rationale generation. (1) With naive rationales obtained directly from prompting GPT-3 with _"let's think step by step"_, we noted that the IMG branch garners no benefits, even evidencing a slight degradation in performance. It indicates that introducing rationales can facilitate the language model in comprehending and reasoning within the context, while image-agnostic rationales do not contribute to the model's multimodal understanding. (2) Compared to the naive rationale , deconstruction [72; 22] and joint reasoning without specialized designs result in a tiny improvement (row 2 in Table 3). The limited improvement can be attributed to the fact that LLMs still perform a combined reasoning and recognition task, leading to a suboptimal execution of the recognition task. (3) Hence, our duty distinct design, implemented through negative-space prompting, achieves \(2.58\%\) and \(2.06\%\) performance gain on IMG and average splits (row 3 in Table 3). It alleviates the reliance on the reasoning capability of the off-the-shelf visual model and generate more reliable multimodal rationales. (4) Moreover, by explicitly emphasizing uncertainty (row 4 in Table 3), we observe a notable improvement of \(5.15\%\) on IMG split, while also contributing to an average enhancement of \(2.19\%\). It achieves remarkable enhancement of correctness, which also significantly contributes to fine-tuning learning.

  
**(a) Generalization** & NAT & SOC & LAN \\  MMCoT & 50.98 & 77.73 & 80.00 \\ Ours & **66.43** & 83.69 & 83.09 \\  MMCoT & 78.86 & 62.20 & 76.82 \\ Ours & 86.5 & **71.77** & 85.09 \\  MMCoT & 79.53 & 76.38 & 55.82 \\ Ours & 86.59 & 86.73 & **68.00** \\   

Table 2: The results of **(a) Generalization** and **(b)** Modality and Rationale **Analysis**.

    & IMG & Avg \\  naive \(\) & 75.06 & 82.96 \\ decomposed question \(\) & 75.61 & 83.09 \\ duty distinct w/o uncertainty \(\) & 78.19 & 85.15 \\ duty distinct w/ uncertainty \(\) & 83.34 & 87.34 \\  w/o integrate to joint reasoning & 77.49 & 83.75 \\   

Table 3: Ablation study on components of DDCoT.

**Quantitative Analysis of Hallucinations.** Additionally, we conduct a human evaluation of the authenticity to assess the phenomenon of hallucinations  in the rationale generation process. As shown in the Table 4, introducing interleaved visual information exacerbates the hallucinations and diminishes the authenticity of generated rationales by 28.1% compared with naive prompting devoid of any visual information input. Our duty-distinct design significantly mitigates the impact of hallucinations. Moreover, the further suppression of hallucinations is observed with the explicit emphasis on uncertainty. These feedbacks align with the findings in Section 3.1.3 and the results presented in Table 3.

**Explainability.** We further exhibit the explainability evaluation including the automatic metrics and human evaluations in Table 6. In automatic metrics, we adopt the BLEU-1/4 , ROUGE-L , and Sentence Similarity  metrics. Note that the automatic evaluation solely reflects the similarity between the generated rationales and the annotated ground truth. In the course of human evaluation, annotators are required to grade each rationale on the criteria of Relevance, Correctness, Completeness, Coherence, and Explainability. It is worth noting that although previous methods [31; 71] achieve decent results in terms of Relevant and Correct, they perform particularly poorly in other areas, especially Explainability. In contrast, despite modest automatic metric results, our rationales generated by DDoCT significantly surpass other methods across all aspects of human evaluation, which is more valuable for interpretable studies.

## 5 Conclusion

This paper proposes a novel DDoCT prompting for multimodal reasoning tasks with language models. Not only achieving state-of-the-art performance on zero-shot learning, our model, with the novel techniques of deep-layer prompting and rational-compressed visual embedding, also demonstrates significant reasoning ability on the ScienceQA benchmark. The experimental results demonstrate the superiority and generalization ability of our proposed DDoCT for both zero-shot learning and fine-tuning. **Ethical Statement.** Our model is based on currently available pre-trained language models. In our experiments, we have not utilized any information concerning personal privacy, data security, or ethical hazards. However, it is crucial to acknowledge that large language models may introduce certain biases, which can originate from the training datasets. We must recognize that entirely eliminating biases from the model is a challenging task, and current technology may not achieve it completely. Therefore, we encourage users to be mindful of these potential biases when utilizing our model and to comprehend that the model's outputs are not infallible but require interpretation and understanding in conjunction with real-world circumstances.

**Acknowledgment:** This work was supported by the National Natural Science Foundation of China (No.62206174), Shanghai Pujiang Program (No.21PJ1410900), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShanghaiAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), and Shanghai Engineering Research Center of Intelligent Vision and Imaging.

    &  &  &  \\   & & B-1 & B-4 & R-L & Sim. & Relevant & Correct & Complete & Coherent & Explainable \\  MMCOT & & 0.970 & 0.930 & 0.970 & 0.990 & **70.83**\% & 67.99\% & 64.81\% & 57.94\% & 58.73\% \\ GPT-3 & ✓ & 0.075 & 0.018 & 0.249 & 0.548 & **81.01**\% & 75.99\% & 65.54\% & 61.64\% & 60.32\% \\ Ours & ✓ & 0.147 & 0.041 & 0.287 & 0.601 & **92.00**\% & **86.38**\% & **85.71**\% & **84.33**\% & **83.26**\% \\   

Table 6: Automatic metrics and human evaluation of explainability.

   Method & Context & Authenticity \\  naive **R** & w/o visual & 0.883 \\  naive **R** & interleaved & 0.602 \\ duty distinct w/o uncertainty **R** & interleaved & 0.783 \\ duty distinct w uncertainty **R** & interleaved & 0.855 \\   

Table 4: Human evaluation on hallucinations.

    & IMG & Avg \\  Ours & **83.34** & **87.34** \\ w/o RCVE & 80.32 & 86.06 \\ w/o DLP\&RCVE & 79.33 & 85.57 \\   

Table 5: Ablation study on fine-tuning components.