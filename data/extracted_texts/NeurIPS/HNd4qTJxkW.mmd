# A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction

Guillaume Huguet\({}^{1}\)1 Alexander Tong\({}^{1}\)2 Edward De Brouwer\({}^{2}\)1

Yanlei Zhang\({}^{1}\) Guy Wolf\({}^{1}\) Ian Adelstein\({}^{2}\)3 Smita Krishnaswamy\({}^{2}\)4

\({}^{1}\)Universite de Montreal; Mila - Quebec AI Institute \({}^{2}\) Yale University

Equal contributionCo-senior authors

###### Abstract

Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call _heat geodesic embeddings_. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state-of-the-art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE).

## 1 Introduction

The advent of high throughput and high dimensional data in various fields of science have made dimensionality reduction and visualization techniques an indispensable part of exploratory analysis. Diffusion-based manifold learning methods, based on the data diffusion operator, first defined in , have proven especially useful due to their ability to handle noise and density variations while preserving structure. As a result, diffusion-based dimensionality reduction methods, such as PHATE , T-PHATE , and diffusion maps , have emerged as methods for analyzing high throughput noisy data in various situations. While these methods are surmised to learn manifold geodesic distances, no specific theoretical links have been established. Here, we establish such a link by using Varadhan's formula  and a parabolic Harnack inequality [17; 25], which relate manifold distances to heat diffusion directly. This lens gives new insight into existing dimensionality reduction methods, including when they preserve geodesics, and suggests a new method for dimensionality reduction to explicitly preserve geodesics, which we call _heat geodesic embeddings_3. Furthermore, based on our understanding of other methods [22; 5], we introduce theoretically justified parameter choices thatallow our method to have greater versatility in terms of distance denoising and emphasis on local versus global distances.

Generally, data diffusion operators are created by first computing distances between datapoints, transforming these distances into affinities by pointwise application of a kernel function (like a Gaussian kernel), and then row normalizing with or without first applying degree normalization into a Markovian diffusion operator \(\)[5; 9; 14; 21; 33]. The entries of \((x,y)\) then contain probabilities of diffusing (or random walk probabilities) from one datapoint to another. Diffusion maps and PHATE use divergences between these diffusion or random walk-based probability distributions \((x,)\) and \((y,)\) to design a diffusion-based distance that may not directly relate to manifold distance. Our framework directly utilizes a heat kernel based distance, and offers a more comprehensive perspective to study these diffusion methods. By configuring parameters in our framework, we show how we can navigate a continuum of embeddings methods from PHATE  to Stochastic Neighbor Embedding (SNE) .

In summary, our contributions are as follows:

* We define the _heat-geodesic_ dissimilarity based on Varadhan's formula and the two-sided heat kernel bounds.
* Based on this dissimilarity, we present a versatile geodesic-preserving method for dimensionality reduction which we call _heat geodesic embedding._
* We establish a relationship between diffusion-based distances and the heat-geodesic dissimilarity.
* We establish connections between our method and popular dimensionality reduction techniques such as PHATE and SNE, shedding light on their geodesic preservation and denoising properties based on modifications of the computed dissimilarity and distance preservation losses.
* We empirically demonstrate the advantages of Heat Geodesic Embedding in preserving manifold geodesic distances in several experiments showcasing more faithful manifold distances in the embedding space, as well as our ability to interpolate data within the manifold.

## 2 Preliminaries

First, we introduce fundamental notions that form the basis of our manifold learning methods: Varadhan's formula  on a manifold, diffusion processes on graphs, efficient heat kernel approximations, and multidimensional scaling [12; 4; 16].

Varadhan's formulaVaradhan's formula is a powerful tool in differential geometry that establishes a connection between the heat kernel and the shortest path (geodesic) distance on a Riemannian manifold. Its versatility has led to widespread applications in machine learning [27; 28; 29; 15; 6; 10]. Let \((M,g)\) be a closed Riemannian manifold, and \(\) the Laplace-Beltrami operator on \(M\). The heat kernel \(h_{t}(x,y)\) on \(M\) is the minimal positive fundamental solution of the heat equation \(= u\) with initial condition \(h_{0}(x,y)=_{x}(y)\). In a \(d\)-dimensional Euclidean space the heat kernel is \(h_{t}(x,y)=(4 t)^{-d/2}\ e^{-d(x,y)^{2}/4t}\) so that \(-4t h_{t}(x,y)=2dt(4 t)+d^{2}(x,y)\) and

Figure 1: Embeddings of the Swiss roll (top) and Tree (bottom) datasets for different manifold learning methods. Our HeatGeo method correctly unrolls the Swiss roll while t-SNE and UMAP create undesirable artificial clusters.

we observe the following limiting behavior:

\[_{t 0}-4t h_{t}(x,y)=d^{2}(x,y).\] (1)

Varadhan  (see also ) proved that eq. 1 (now Varadhan's formula) holds more generally on complete Riemannian manifolds \(M\), where \(d(x,y)\) is the geodesic distance on \(M\), and the convergence is uniform over compact subsets of \(M\). A related result for complete Riemannian manifolds that satisfy the parabolic Harnack inequality (which includes convex domains in Euclidean space and Riemannian manifolds with non-negative Ricci curvature) is the two-sided heat kernel bound , showing that for any \((0,1)\) there exist constants \(c()\) and \(C()\) such that

\[)}(-}{4(1+)t }) h_{t}(x,y))}(-}{4(1-)t}),\] (2)

where \(V(x,)\) is the volume of a ball or radius \(\) centered at x. We denote this relation by \(h_{t}(x,y) V(x,)^{-1}(-d(x,y)^{2}/t)\) and note that it again recovers eq. 1 in the \(t 0\) limit, which is unsurprising as Varadhan's result holds more generally. More important for our purposes is that \(h_{t}(x,y) V(x,)^{-1}(-d(x,y)^{2}/t)\) holds for \(t>0\) which will allow us to approximate geodesic distances \(d(x,y)\) from a diffusion based estimation of the heat kernel \(h_{t}(x,y)\) and the volume \(V(x,)\). In appendix C.3, we provide examples using inequality (2).

Graph construction and diffusionOur construction starts by creating a graph from a point cloud dataset \(\) of size \(n\). We use a kernel function \(:^{d}^{d}^{+}\), such that the (weighted) adjacency matrix is \(_{ij}:=(x_{i},x_{j})\) for all \(x_{i},x_{j}\). The kernel function could be a Gaussian kernel, or constructed from a nearest neighbor graph. The resulting graph \(\) is characterized by the set of nodes (an ordering of the observations), the adjacency matrix, and the set of edges, i.e. pairs of nodes with non-zero weights. The graph Laplacian is an operator acting on signals on \(\) such that it mimics the negative of the Laplace operator. The combinatorial graph Laplacian matrix is defined as \(:=-\) and its normalized version as \(=_{n}-^{-1/2}^{-1/2}\), where \(\) is a diagonal degree matrix with \(_{ii}:=_{j}_{ij}\). The Laplacian is symmetric positive semi-definite, and has an eigen-decomposition \(=^{T}\). Throughout the presentation, we assume that \(_{ii}>0\) for all \(i\{1,,n\}\). The Laplacian allows us to define the heat equation on \(\), with respect to an initial signal \(_{0}^{n}\) on \(\):

\[(t)+(t)=,\;s.t.( 0)=_{0} t^{+}.\] (3)

The solution of the above differential equation is obtained with the matrix exponential \((t)=e^{-t}_{0}\), and we define the heat kernel on the graph as \(_{t}:=e^{-t}\). By eigendecomposition, we have \(_{t}= e^{-t}^{T}\). The matrix \(_{t}\) is a diffusion matrix that characterizes how a signal propagate through the graph according to the heat equations.

Other diffusion matrices on graphs have also been investigated in the literature. The transition matrix \(:=^{-1}\) characterizing a random walk on the graph is another common diffusion matrix used for manifold learning such as PHATE  and diffusion maps . It is a stochastic matrix that converges to a stationary distribution \(_{i}:=_{ii}/_{i}_{ii}\), under mild assumptions.

Fast computation of heat diffusionExact computation of the (discrete) heat kernel \(_{t}\) is computationally costly, requiring a full eigendecomposition in \(O(n^{3})\) time. Fortunately, multiple fast approximations have been proposed, including using orthogonal polynomials or the Euler backward methods. In this work, we use Chebyshev polynomials, as they have been shown to converge faster than other polynomials on this problem .

Chebyshev polynomials are defined by the recursive relation \(\{T_{k}\}_{k}\) with \(T_{0}(y)=0\), \(T_{1}(y)=y\) and \(T_{k}(y)=2yT_{k-1}(y)-T_{k-2}(y)\) for \(k 2\). Assuming that the largest eigenvalue is less than two (which holds for the normalized Laplacian), we approximate the heat kernel with the truncated polynomials of order \(K\)

\[_{t} p_{K}(,t):=}{2}+_{k=1}^{K}b_{t,k}T_{k }(-_{n}),\] (4)where the \(K+1\) scalar coefficients \(\{b_{t,i}\}\) depend on time and are evaluated with the Bessel function. Computing \(p_{K}(,t)\) requires \(K\) matrix-vector product and \(K+1\) Bessel function evaluation. The expensive part of the computation are the matrix-vector products, which can be efficient if the Laplacian matrix is sparse. Interestingly, we note that the evaluation of \(T_{k}\) do not depend on the diffusion time. Thus, to compute multiple approximations of the heat kernel \(\{p_{K}(,t)\}_{t}\), only necessitates reweighting the truncated polynomial \(\{T_{k}\}_{k[1,,K]}\) with the corresponding \(||\) sets of Bessel coefficients. The overall complexity is dominated by the truncated polynomial computation which takes \(O(K(E+n))\) time where \(E\) is the number of non-zero values in \(\).

Another possible approximation is using the Euler backward method. It requires solving \(K\) systems of linear equations \((t)=(_{n}+(t/K))^{-K}(0)\), which can be efficient for sparse matrices using the Cholesky decomposition [10; 28]. We quantify the differences between the heat kernel approximations in Appendix C.

Metric multidimensional scalingGiven a dissimilarity function \(d\) between data points, metric multidimensional scaling (MDS)  finds an embedding \(\) such that the difference between the given dissimilarity and the Euclidean distance in the embedded space is minimal across all data points. Formally, for a given function \(d:^{d}^{d}^{+}\), MDS minimizes the following objective:

\[L()=_{ij}d(x_{i},x_{j})-\|(x_{i})-(x_{j})\|_ {2}^{2}^{1/2},\] (5)

In metric MDS the solution is usually found by the SMACOF algorithm , or stochastic gradient descent .

## 3 Related Work

We review state-of-the-art embedding methods and contextualize them with respect to Heat Geodesic Embedding. A formal theoretical comparison of all methods is given in Section 5. Given a set of high-dimensional datapoints, the objective of embedding methods is to create a map that embeds the observations in a lower dimensional space, while preserving distances or similarities. Different methods vary by their choice of distance or dissimilarity functions, as shown below.

Diffusion mapsIn diffusion maps , an embedding in \(k\) dimensions is defined via the first \(k\) non-trivial right eigenvectors of the t-steps random walk \(^{t}\) weighted by their eigenvalues. The embedding preserves the _diffusion distance_\(DM_{}(x_{i},x_{j}):=\|(_{}^{t}-_{}^{t})(1/)\|_{2}\), where \(_{i}\) is a vector such that \((_{i})_{j}=1\) if \(j=i\) and \(0\) otherwise, and \(\) is the stationary distribution of \(\). Intuitively, \(DM_{}(x_{i},x_{j})\) considers all the \(t\)-steps paths between \(x_{i}\) and \(x_{j}\). A larger diffusion time can be seen as a low frequency graph filter, i.e. keeping only information from the low frequency transitions such has the stationary distributions. For this reason, using diffusion with \(t>1\) helps denoising the relationship between observations.

PhatThis diffusion-based method preserves the _potential distance_\(PH_{}:=\|-_{i}^{t}+_{j}^{t}\|_{2}\), and justifies this approach using the \(\) transformation to prevent nearest neighbors from dominating the distances. An alternative approach is suggested using a square root transformation. Part of our contributions is to justify the \(\) transformation from a geometric point of view. The embedding is defined using multidimensional scaling, which we present below.

Sne, t-Sne, UmapWell-known attraction/repulsion methods such as SNE , t-SNE , and UMAP  define an affinity matrix with entries \(p_{ij}\) in the ambient space, and another affinity matrix with entries \(q_{ij}\) in the embedded space. To define the embedding, a loss between the two affinity matrices is minimized. Specifically, the loss function is \(D_{}(p||q):=_{ij}p_{ij} p_{ij}/q_{ij}\) in SNE and t-SNE, whereas UMAP is equivalent to adding \(D_{}(1-p||1-q)\) for unnormalized densities . While these methods preserve affinities, they do not preserve any types of distances in the embedding.

## 4 Heat-Geodesic Embedding

In this section, we present our Heat Geodesic Embedding which is summarized in Alg. 1. We start by introducing the heat-geodesic dissimilarity, then present a robust transformation, and a heuristic to choose the optimal diffusion time. Proofs not present in the main text are given in AppendixA.

We consider the discrete case, where we have a set of \(n\) points \(\{x_{i}\}_{i=1}^{n}=:\) in a high dimensional Euclidean space \(x_{i}^{d}\). From this point cloud, we want to define a map \(:^{d}^{k}\) that embeds the observation in a lower dimensional space. An important property of our embedding is that we preserve manifold geodesic distances in a low dimensional space.

Heat-geodesic dissimilarityInspired by Varadhan's formula and the Harnack inequalities, we defined a heat-geodesic dissimilarity based on heat diffusion on graphs. From observations (data-points) in \(^{d}\), we define an undirected graph \(\), and compute its heat kernel \(_{t}=e^{-t}\), where \(\) is the combinatorial or symmetrically normalized graph Laplacian (the heat kernel is thus symmetric). Following the inequality (2), we can rearrange the terms to isolate the geodesic distance, inspired by this observation, we define the following dissimilarity.

**Definition 4.1**.: For a diffusion time \(t>0\) and tunable parameter \(>0\), we define the **heat-geodesic dissimilarity** between \(x_{i},x_{j}\) as

\[d_{t}(x_{i},x_{j}):=[-4t(_{t})_{ij}- 4t(_{t})_{ij}]^{1/2}\]

where \(_{t}\) is the heat kernel on the graph \(\), and \((_{t})_{ij}:=2[(_{t})_{ii}+(_{t})_{jj}]^{-1}\).

Here the \(\) is applied elementwise, and the term \(-4t(_{t})_{ij}\) corresponds to the geodesic approximation when \(t 0\) as in Varadhan's formula. In practice one uses a fixed diffusion time \(t>0\), so we add a symmetric volume correction term as in the Harnack inequality, ensuring that \(d_{t}(x_{i},x_{j})\) is symmetric. From Sec. 2, we have \(h_{t}(x,x) V(x,)^{-1}\), and we use the diagonal of \(_{t}\) to approximate the inverse of the volume. With this volume correction term and \(=1\), the dissimilarity is such that \(d_{t}(x_{i},x_{i})=0\) for all \(t>0\). When \(=0\) or the manifold has uniform volume growth (as in the constant curvature setting) we show that the heat-geodesic dissimilarity is order preserving:

**Proposition 4.2**.: _When \(=0\) or the manifold has uniform volume growth, i.e. \((_{t})_{ii}=(_{t})_{jj}\), and the heat kernel is pointwise monotonically decreasing w.r.t. a norm \(||\) in ambient space, we have for triples \(x_{i},x_{j},x_{k}\) that \(|x_{i}-x_{j}|>|x_{i}-x_{k}|\) implies \(d_{t}(x_{i},x_{j})>d_{t}(x_{i},x_{k})\), i.e. the heat-geodesic dissimilarity is order preserving._

Proof.: When \(=0\) or the manifold has uniform volume growth we need only consider the \(-4t(_{t})_{ij}\) terms. The assumption of pointwise monotonicity of the heat kernel entails that \(|x_{i}-x_{j}|>|x_{i}-x_{k}|\) implies \(_{t}(x_{i},x_{j})<_{t}(x_{i},x_{k})\). We are able to conclude that \(-4t_{t}(x_{i},x_{j})>-4t_{t}(x_{i},x_{k})\) and thus \(d_{t}(x_{i},x_{j})>d_{t}(x_{i},x_{k})\). 

Denoising distances with triplet computationsWe note that both diffusion maps and PHATE compute a triplet distance between datapoints, i.e., rather than using the direct diffusion probability between datapoints, they use a distance between corresponding rows of a diffusion operator. In particular, diffusion maps uses Euclidean distance, and PHATE uses an M-divergence. Empirically, we notice that this step acts as a denoiser for distances. We formalize this observation in the following proposition. We note \(D_{}\) the triplet distance. The triplet distance compares the distances relative to other points. Intuitively, this is a denoising step, since the effect of the noise is spread across the entire set of points. For a reference dissimilarity like the heat-geodesic, it is defined as \(D_{}(x_{i},x_{j}):=\|d_{t}(x_{i},)-d_{t}(x_{j},)\|_{2}\). For linear perturbations of the form \(d_{t}(x_{i},x_{j})+\), where \(\), the effect of \(\) on \(D_{}(x_{i},x_{j})\) is less severe than on \(d_{t}(x_{i},x_{j})\). Our embedding is based on a linear combination between the heat-geodesic dissimilarity and its triplet distance \((1-)d_{t}+ D_{}\), where \(\).

**Proposition 4.3**.: _Denote the perturbed triplet distance by \(}}(x_{i},x_{j})=||_{t}(x_{i},)-_ {t}(x_{j},)||_{2}\) where \(_{t}(x_{i},x_{j}):=d_{t}(x_{i},x_{j})+\) and \(_{t}(x_{i},x_{k}):=d_{t}(x_{i},x_{k})\) for \(k j\). Then the triplet distance \(D_{}\) is robust to perturbations, i.e., for all \(>0\),_

\[(}}(x_{i},x_{j})}{D_{}(x_{i},x_{j})} )^{2}((x_{i},x_{j})+}{d_{t}(x_{i},x_{j})} )^{2}.\]Optimal diffusion timeVaradhan's formula suggests a small value of diffusion time \(t\) to approximate geodesic distance on a manifold. However, in the discrete data setting, geodesics are based on graph constructions, which in turn rely on nearest neighbors. Thus, small \(t\) can lead to disconnected graphs. Additionally, increasing \(t\) can serve as a way of denoising the kernel (which is often computed from noisy data) as it implements a low-pass filter over the eigenvalues, providing the additional advantage of adding noise tolerance. By computing a sequence of heat kernels \((_{t})_{t}\) and evaluating their entropy \(H(_{t}):=-_{ij}(_{t})_{ij}(_{t})_{ij}\), we select \(t\) with the knee-point method  on the function \(t H(_{t})\). We show in Sec. 6.1 that our heuristic for determining the diffusion time automatically leads to better overall results.

Weighted MDSThe loss in MDS (eq.5) is usually defined with uniform weights. Here, we optionally weight the loss by the heat kernel. In Sec. 5, we will show how this modification relates our method to the embedding defined by SNE. For \(x_{i},x_{j}\), we minimize \((_{t})_{ij}(d_{t}(x_{i},x_{j})-\|(x_{i})-(x_{j})\|_{2})^{2}\). This promotes geodesic preservation of local neighbors, since more weights are given to points with higher affinities.

Heat-geodesic embeddingTo define a lower dimensional embedding of a point cloud \(\), we construct a matrix from the heat-geodesic dissimilarity, and then use MDS to create the embedding. Our embedding defines a map \(\) that minimizes \((d_{t}(x_{i},x_{j})-\|(x_{i})-(x_{j})\|_{2})^{2}\), for all \(x_{i},x_{j}\). Hence, it preserves the heat-geodesic dissimilarity as the loss decreases to zero. In Alg. 1, we present the main steps of our algorithm using the heat-geodesic dissimilarity. A detailed version is presented in Appendix A.

```
1:Input:\(N d\) dataset matrix \(\), denoising parameter \(\), Harnack regularization \(>0\), output dimension \(k\).
2:Returns:\(N k\) embedding matrix \(\).
3:\(_{t} p_{K}(,t)\)\(\)Heat approximation
4:\(t\{H(_{t})\}_{t}\)\(\)Knee detection e.g. 
5:\([-4t(_{t})_{ij}- 4t(_{t})_{ij}]^{1/2}\)\(\)\(\)is applied elementwise
6:\((1-)+ D_{}\)\(\)Triplet interpolation step
7:Return \((,\|\|_{2},k)\) ```

**Algorithm 1** Heat Geodesic Embedding

## 5 Relation to other manifold learning methods

In this section, we elucidate theoretical connections between the Heat Geodesic Embedding and other manifold learning methods. We relate embeddings via the eigenvalues of \(_{t}\) or \(^{t}\) with Laplacian eigenmaps and diffusion maps. We then present the relation between our methods and PHATE and SNE. We provide further analysis in the Appendix A. In particular, we introduce a new definition of kernel preserving embeddings; either via kernel-based distances (diffusion maps, PHATE) or via similarities (e.g. t-SNE, UMAP).

Diffusion maps with the heat kernelDiffusion maps  define an embedding with the first \(k\) eigenvectors \((_{i})_{i}\) of \(\), while Laplacian eigenmaps  uses the eigenvectors \((_{i})_{i}\) of \(\). In the following, we recall the links between the two methods, and show that a rescaled Laplacian eigenmaps preserves the diffusion distance with the heat kernel \(_{t}\).

**Lemma 5.1**.: _Rescaling the Laplacian eigenmaps embedding with \(x_{i}(e^{-2t_{1}}_{1,i},,e^{-2t_{k}}_{k,i})\) preserves the diffusion distance \(DM_{_{t}}\)._

Relation to PHATEThe potential distance in PHATE (Sec. 3) is defined by comparing the transition probabilities of two \(t\)-steps random walks initialized from different vertices. The transition matrix \(^{t}\) mimics the heat propagation on a graph. The heat-geodesic dissimilarity provides a new interpretation of PHATE. In the following proposition, we show how the heat-geodesic relates to the PHATE potential distance with a linear combination of \(t\)-steps random walks.

**Proposition 5.2**.: _The PHATE potential distance with the heat kernel \(PH_{_{t}}\) can be expressed in terms of the heat-geodesic dissimilarity with \(=0\)_

\[PH_{_{t}}=(1/4t)^{2}\|d_{t}(x_{i},)-d_{t}(x_{j},)\|_{2}^{2},\]

_and it is equivalent to a multiscale random walk distance with kernel \(_{k>0}m_{t}(k)^{k}\), where \(m_{t}(k):=t^{k}e^{-t}/k!\)._

Proof.: We present a simplified version of the proof, more details are available in Appendix A. For \(=0\), we have \(d_{t}(x_{i},x_{j})=-4t(_{t})_{ij}\), the relation between the PHATE potential and the heat-geodesic follows from the definition

\[PH_{_{t}}(x_{i},x_{j})=_{k}-_{t}(x_{i},x_{k})+ _{t}(x_{j},x_{k})^{2}=(1/4t)^{2}\|d_{t}(x_{i},)-d_{t}(x_{j}, )\|_{2}^{2}.\]

Using the heat kernel \(_{t}\) with the random walk Laplacian \(_{rw}=^{-1}=_{n}-^{-1}\) corresponds to a multiscale random walk kernel. We can write \(_{rw}=^{-1}\), where \(:=^{-1/2}\). Since \(=_{n}-_{rw}\), we have \(^{t}=(_{n}-)^{t}^{-1}\). Interestingly, we can relate the eigenvalues of \(_{t}\) and \(\) with the Poisson distribution. The probability mass function of a Poisson distribution with mean \(t\) is given by \(m_{t}(k):=t^{k}e^{-t}/k!\). For \(t 0\), we have \(e^{-t(1-)}=_{k 0}m_{t}(k)^{k}\). With this relationship, we can express \(_{t}\) as a linear combination of \(^{t}\) weighted by the Poisson distribution. Indeed, substituting \(=1-\) in yields

\[_{t}=e^{-t}^{-1}=_{k=0}^{}m_{t}(k)( _{n}-)^{k}^{-1}=_{k=0}^{}m_{t}(k)^{k}.\]

_Remark 5.3_.: In the previous proposition, the same argument holds for the symmetric Laplacian and the affinity matrix \(:=^{-1/2}^{-1/2}\) used in other methods such as diffusion maps . This is valid since we can write \(_{sym}=^{-1/2}^{T}^{-1/2},\) and \(=_{n}-_{sym}\).

_Remark 5.4_.: This proposition shows that, as the denoising parameter \( 1\), Heat Geodesic Embedding interpolates to the PHATE embeddings with a weighted kernel \(_{k=0}^{}m_{t}(k)^{k}\).

Relation to SNEThe heat-geodesic method also relates to SNE , and its variation using the Student distribution t-SNE . In SNE, the similarity between points is encoded via transition probabilities \(p_{ij}\). The objective is to learn an affinity measure \(q\), that depends on the embedding distances \(\|y_{i}-y_{j}\|_{2}\), such that it minimizes \(D_{}(p||q)\). Intuitively, points that have a strong affinity in the ambient space, should also have a strong affinity in the embedded space. Even though the heat-geodesic minimization is directly on the embedding distances, we can show an equivalent with SNE. In Appendix A, we provide additional comparisons between SNE and our method.

**Proposition 5.5**.: _The Heat-Geodesic embedding with \(=0\) and squared distances minimization weighted by the heat kernel is equivalent to SNE with the heat kernel affinity in the ambient space, and a Gaussian kernel in the embedded space \(q_{ij}=(-\|y_{i}-y_{j}\|^{2}/4t)\)._

## 6 Results

In this section, we show the versatility of our method, showcasing its performance in terms of clustering and preserving the structure of continuous manifolds. We compare the performance of Heat Geodesic Embedding with multiple state-of-the-art baselines on synthetic datasets and real-world datasets. For all models, we perform sample splitting with a 50/50 validation-test split. The validation and test sets each consists of 5 repetitions with different random initializations. The hyper-parameters are selected according to the performance on the validation set. We always report the results on the test set, along with the standard deviations computed over the five repetitions. We use the following methods in our experiments: our _Heat Geodesic Embedding_, _diffusion maps_, _PHATE_, _shortest-path_ (used in Isomap ) which estimates the geodesic distance by computing the shortest path between two nodes in a graph built on the point clouds, _t-SNE_, _UMAP_, and metric MDS with Euclidean distance. Details about each of these methods, and results for different parameters (graph type, heat approximation, etc.) are given in Appendix C.

### Distance matrix comparison

We start by evaluating the ability of the different distances or dissimilarities to recover the ground truth distance matrix of a point cloud. For this task, we use the Swiss roll and Tree datasets, for which the ground truth geodesic distance is known. The Swiss roll dataset consists of data points sampled on a smooth manifold (see Fig. 1). The Tree dataset is created by connecting multiple high-dimensional Brownian motions in a tree-shape structure. In Fig. 1, we present embeddings of both datasets. Our method recovers the underlying geometry, while other methods create artificial clusters or have too much denoising. Because we aim at a faithful relative distance between data points, we compare the methods according to the Pearson and Spearman correlations of the estimated distance matrices with respect to ground truth. Results are displayed in Tab. 1. We observe that Heat Geodesic Embedding typically outperforms previous methods in terms of the correlation with the ground truth distance matrix, confirming the theoretical guarantees provided in Sec. 4 & 2. Additional results such as computation time and correlation for different noise levels are available in Appendix C.

Optimal diffusion timeIn Section 4, we described a heuristic to automatically choose the diffusion time based on the entropy of \(_{t}\). In Fig. 2, we show that the knee-point of \(t H(_{t})\), corresponds to a high correlation with the ground distance, while yielding a low approximation error of the distance matrix (measured by the Frobenius norm of the difference between \(\) and the ground truth).

### Preservation of the inherent data structure

A crucial evaluation criteria of manifold learning methods is the ability to capture the inherent structure of the data. For instance, clusters in the data should be visible in the resulting low dimensional representation. Similarly, when the dataset consists of samples taken at different time points, one expects to be able to characterize this temporal evolution in the low dimensional embedding . We thus compare the different embedding methods according to their ability to retain clusters and temporal evolution of the data.

Identifying clusters.We use the PBMC dataset, the Swiss roll, the Tree dataset, MNIST , and COIL-20  dataset. The PBMC dataset consists of single-cell gene expressions from 3000 individual peripheral blood mononuclear cells. Cells are naturally clustered by their cell type. For

    &  &  \\  Distance & Pearson & Spearman & Pearson & Spearman \\  Diffusion distance & \(0.476 0.226\) & \(0.478 0.138\) & \(0.656 0.054\) & \(0.653 0.057\) \\ PHATE potential & \(0.457 0.01\) & \(0.404 0.024\) & \(0.766 0.023\) & \(0.743 0.028\) \\ Shortest path & \(0.497 0.144\) & \(0.558 0.134\) & \(0.780 0.009\) & \(0.757 0.019\) \\ Euclidean & \(0.365 0.006\) & \(0.413 0.005\) & \(0.735 0.014\) & \(0.704 0.033\) \\ Heat-geodesic (ours) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Pearson and Spearman correlation between the inferred and ground truth distance matrices on the Swiss roll and Tree datasets (higher is better). Best models on average are bolded.

Figure 3: Embeddings of 2000 differentiating cells from embryoid body  over 28 days. UMAP and t-SNE do not capture the continuous manifold representing the cells’ evolution.

Figure 2: Evolution of the correlation between estimated and ground truth distance matrices in function of the diffusion time \(t\).

the Tree dataset, we use the branches as clusters. For the Swiss roll dataset, we sample data points on the manifold according to a mixture of Gaussians and use the mixture component as the ground truth cluster labels. The MNIST and COIL-20 datasets are clustered by digits or objects but may not respect the manifold hypothesis. For each method, we run k-means on the two-dimensional embedding and compare the resulting cluster assignments with ground truth. Tab. 10 reports the results in terms of homogeneity and adjusted mutual information (aMI). Heat Geodesic Embedding is competitive with PHATE and outperforms t-SNE and UMAP on all metrics except on the MNIST and COIL-20 datasets. Yet, we show in Appendix C that all methods tend to perform equally well when the noise level increases. In Fig. 4, we present the PBMC embeddings of PHATE and HeatGeo, showing that HeatGeo interpolates to PHATE for \( 1\).

Temporal data representation.For this task, we aim at representing data points from population observed at consecutive points in time. We use single cell gene expression datasets collected across different time points, including the Embryoid Body (EB), IPSC , and two from the 2022 NeurIPS multimodal single-cell integration challenge (Cite & Multi). To quantitatively evaluate the quality of the continuous embeddings, we first embed the entire dataset and obfuscate all samples from a particular time point (_e.g.,_\(t=2\)). We then estimate the distribution of the missing time point by using displacement interpolation  between the adjacent time points (_e.g.,_\(t=1\) and \(t=3\)). We report the Earth Mover Distance (EMD) between the predicted distribution and true distribution. A low EMD suggests that the obfuscated embeddings are naturally located between the previous and later time points, and that the generated embedding captures the temporal evolution of the data adequately. Results are presented in Tab. 3. Heat Geodesic Embedding outperforms other methods on the EB, Multi, and IPSC datasets and is competitive with other approaches on Cite. We show a graphical depiction of the different embeddings for the embryoid (EB) dataset in Fig. 3.

    & Swiss roll &  &  & COIL-Non Manifold \\  Method & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI \\  GMAP & 0.851 \(\) 0.025 & 0.726 \(\) 0.045 & 0.475 \(\) 0.056 & 0.051 \(\) 0.036 & 0.177 \(\) 0.047 & 0.148 \(\) 0.045 & 0.581 \(\) 0.016 & 0.862 \(\) 0.015 & 0.817 \(\) 0.039 & 0.854 \(\) 0.012 \\ i-SNE & 0.852 \(\) 0.025 & 0.726 \(\) 0.028 & 0.058 \(\) 0.014 & 0.071 \(\) 0.015 & 0.059 \(\) 0.049 & 0.544 \(\) 0.022 & 0.088 \(\) 0.063 & **0.087 \(\) 0.04** & **0.088 \(\) 0.02** \\ Huang & 0.856 \(\) 0.009 & 0.712 \(\) 0.005 & 0.027 \(\) 0.014 & 0.021 \(\) 0.020 & 0.224 \(\) 0.027 & 0.217 \(\) 0.017 & 0.072 \(\) 0.017 & 0.741 \(\) 0.012 & 0.367 \(\) 0.02 & 0.027 \\ HuetGeo & 0.859 \(\) 0.003 & 0.701 \(\) 0.003 & 0.028 \(\) 0.002 & 0.021 \(\) 0.021 & 0.028 \(\) 0.023 & 0.028 \(\) 0.021 & 0.028 \(\) 0.021 & 0.026 \(\) 0.011 & 0.366 \(\) 0.027 & 0.027 \\ HuetGeo & 0.859 \(\) 0.004 & 0.859 \(\) 0.003 & 0.028 \(\) 0.002 & 0.021 \(\) 0.021 & 0.028 \(\) 0.023 & 0.028 \(\) 0.021 & 0.028 \(\) 0.021 & 0.028 \(\) 0.022 & 0.021 \(\) 0.024 \\ HeatGeo (ours) & 0.852 \(\) 0.013 & 0.956 \(\) 0.013 & 0.941 \(\) 0.013 & 0.956 \(\) 0.013 & 0.781 \(\) 0.019 & 0.768 \(\) 0.017 & 0.756 \(\) 0.020 & 0.852 \(\) 0.010 & 0.859 \(\) 0.016 & 0.856 \(\) 0.022 \\   

Table 2: Clustering quality metrics for different methods. We report the homogeneity and the adjusted mutual information (aMI). Best models on average are bolded (higher is better).

Figure 4: Embeddings on PBMC using the triplet distance with the heat-geodesic for different regularization parameter \(\).

   Method & Cite & EB & Multi & IPSC \\  Euclidean & 0.978 \(\) 0.069 & 1.012 \(\) 0.039 & 1.212 \(\) 0.199 & 1.085 \(\) 0.234 \\ Isomap & 0.978 \(\) 0.105 & 0.993 \(\) 0.062 & 1.299 \(\) 0.307 & 1.026 \(\) 0.253 \\ Non-metric MDS & 0.81 \(\) 0.012 & 0.85 \(\) 0.014 & 0.806 \(\) 0.015 & 1.013 \(\) 0.067 \\ UMAP & **0.791 \(\) 0.045** & 0.942 \(\) 0.053 & 1.418 \(\) 0.042 & 0.866 \(\) 0.058 \\ t-SNE & 0.905 \(\) 0.034 & 0.964 \(\) 0.032 & 1.208 \(\) 0.087 & 1.006 \(\) 0.026 \\ PHATE & 1.032 \(\) 0.037 & 1.088 \(\) 0.012 & 1.254 \(\) 0.042 & 0.955 \(\) 0.033 \\ Diffusion Maps & 0.989 \(\) 0.080 & 0.965 \(\) 0.077 & 1.227 \(\) 0.086 & 0.821 \(\) 0.039 \\ HeatGeo (ours) & 0.890 \(\) 0.046 & **0.733 \(\) 0.036** & **0.958 \(\) 0.044** & **0.365 \(\) 0.056** \\   

Table 3: EMD between a linear interpolation of two consecutive time points \(t-1\), \(t+1\), and the time points \(t\). Best models on average are bolded (lower is better).

Conclusion and Limitations

The ability to visualize complex high-dimensional data in an interpretable and rigorous way is a crucial tool of scientific discovery. In this work, we took a step in that direction by proposing a general framework for understanding diffusion-based dimensionality reduction methods through the lens of Riemannian geometry. This allowed us to define a novel embedding based on the heat geodesic dissimilarity--a more direct measure of manifold distance. Theoretically, we showed that our methods brings greater versatility than previous approaches and can help gaining insight into popular manifold learning methods such as diffusion maps, PHATE, and SNE. Experimentally, we demonstrated that it also results in better geodesic distance preservation and excels both at clustering and preserving the structure of a continuous manifold. This contrasts with previous methods that are typically only effective at a single of these tasks.

Despite the strong theoretical and empirical properties, our work presents some limitations. For instance, our method is based on a similarity measure, which is a relaxation of a distance metric. Additionally, the Harnack equation suggests that our parameters for the volume correction could be tuned depending on the underlying manifold. We envision that further analysis of this regularization is a fruitful direction for future work.