# Pearls from Pebbles: Improved Confidence Functions

for Auto-labeling

 Harit Vishwakarma

hvishwakarma@cs.wisc.edu

University of Wisconsin-Madison

 Yi Chen

yi.chen@wisc.edu

University of Wisconsin-Madison

&Sui Jiet Tay

st5494@nyu.edu

Satya Sai Srinath Namburi

satya.namburi@gehealthcare.com

GE HealthCare

&Frederic Sala

fredsala@cs.wisc.edu

University of Wisconsin-Madison

&Ramya Korlakai Vinayak

ramya@ece.wisc.edu

University of Wisconsin-Madison

Work done while at University of Wisconsin-Madison.

###### Abstract

Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual annotation. A prominent variant, threshold-based auto-labeling (TBAL), works by finding thresholds on a model's confidence scores above which it can accurately automatically label unlabeled data. However, many models are known to produce overconfident scores, leading to poor TBAL performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, we show that such methods fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the _optimal_ TBAL confidence function. We develop a tractable version of the framework to obtain Colander (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of Colander and compare it against methods designed for calibration. Colander achieves up to 60% improvement on coverage over the baselines while maintaining error level below \(5\%\) and using the same amount of labeled data.

## 1 Introduction

The demand for labeled data in machine learning (ML) is perpetual. Obtaining it is expensive and time-consuming, creating a bottleneck in ML workflows. Threshold-based auto-labeling (TBAL) is a promising solution to obtain high-quality labeled data at low cost . A TBAL system (Fig. 1) takes unlabeled data as input and outputs a labeled dataset. It works iteratively: in each iteration, it acquires human labels for a small chunk of data to train a model, then auto-labels points using the model's predictions where its _confidence scores_ are above a certain threshold. The threshold is determined using validation data so that the auto-labeled points meet a desired _accuracy criteria_. The goal is to maximize _coverage_--the fraction of points automatically labeled (out of the total)--while maintaining accuracy. TBAL powers industry products like Amazon SageMaker Ground Truth .

The confidence function is critical to the TBAL workflow (Figure 1). Existing TBAL systems rely on common choices like softmax outputs from neural networks . These functions _are not well aligned with the objective of the auto-labeling system_. Using them results in substantially suboptimal coverage (Figure 2(a)). For this reason, we ask:An ideal confidence function for auto-labeling will achieve the maximum coverage at a given auto-labeling error tolerance and thus will bring down the labeling cost significantly. Finding such an ideal function, however, is difficult because of the _inherent tension_ between accuracy and coverage. The models used in auto-labeling are often highly inaccurate so achieving a certain error guarantee is easier when being conservative in terms of confidence--but this reduces coverage. Conversely, high coverage may appear to require lowering the requirements in confidence, but this may easily lead to overshooting the desired error level. This is compounded by the fact that TBAL is iterative, so even small deviations in error levels can cascade in future iterations.

_Overconfidence_ may further stymie hopes of balancing accuracy and coverage. While overconfidence is a challenge in general, it is exacerbated in TBAL: since models are trained on a small amount of labeled data, they are often inaccurate, making the problem of designing confidence functions even more challenging. Common choices produce overconfident scores, i.e., high scores for both correct and incorrect predictions [51; 38; 17; 16; 3]. Fig. 2(a) shows that softmax scores are overconfident, resulting in poor auto-labeling performance.

Several methods have been introduced to address overconfidence, including a variety of calibration techniques . Applying these can nevertheless miss out on significant performance (Figure 2(b)) since the calibration goal differs from auto-labeling. From the auto-labeling standpoint, we seek minimum overlap between the correct and incorrect model prediction scores. Other approaches [6; 35] brake the objective of separating scores into model training or use different optimization procedures  that encourage separation. We observe that these do not help TBAL either, since, after some point, the model is correct on almost all the training points, making it hard to train it to discriminate between its own correct and incorrect predictions.

We tackle these challenges by _proposing a framework to learn suitable confidence functions_ for TBAL. In particular, we express the auto-labeling objective as an optimization problem over the space of confidence functions and thresholds. Our framework subsumes existing methods, i.e., they are points in the space of solutions. The resulting method, Colander (Confidence functions for Efficient and Reliable Auto-labeling), relies on a practical surrogate to the framework that can be used to learn optimal confidence functions for auto-labeling. Using these learned functions in TBAL can achieve up to 60% improvement in coverage versus baselines like softmax, temperature scaling , CRL  and FMFP . We summarize our contributions as follows,

1. We propose a principled framework to study the choices of confidence functions suitable for auto-labeling and provide a practical method (Colander) to learn confidence functions for efficient and reliable auto-labeling.
2. We systematically study commonly used choices of scoring functions and calibration methods and demonstrate that they lead to poor auto-labeling performance.
3. Through extensive empirical evaluation on real-world datasets, we show that using the confidence scores obtained using our procedure boosts auto-labeling performance significantly in comparison to common choices of confidence functions and calibration methods.

## 2 Background and Motivation

We provide notation, background on TBAL and its relationship to other methods, and describe the importance of confidence functions.

Figure 1: High-level diagram of TBAL system.

**Notation.** Let \([m]:=\{1,2,,m\}\) for any natural number \(m\). Let \(X_{u}\) be a set of unlabeled points drawn from some instance space \(\). Let \(=\{1,,k\}\) be the label space. There is an unknown ground truth labeling function \(f^{*}:\). Let \(\) be a _noiseless_ oracle that provides the true label for any point \(\). Denote the model (hypothesis) class by \(\), where each \(\) is a function \(:\). Each classifier \(h\) also has an associated _confidence function_\(g:^{k}\) that quantifies the confidence of the prediction by model \(\) on any data point \(\). Here, \(^{k}\) is a \((k-1)\)-dimensional probability simplex. Let \([i]\) denote the \(i^{}\) component for any vector \(^{d}\). For any point \(\) the prediction is \(:=()\) and the associated confidence is \(g()[]\). The vector \(\) denotes scores over \(k\)-classes, and \([y]\) denotes its \(y^{}\) entry, i.e., score for class \(y\). Table 3 (in Appendix B.4) contains a summary of the notation.

**Threshold-based auto-labeling.** It seeks to obtain labeled datasets while reducing the labeling burden on humans (Figure 1). The input is a pool of unlabeled data \(X_{u}\). It outputs, for each \( X_{u}\), a label \(\). The output label could be either \(y\), from the oracle (human), or \(\), from the model. Let \(N_{u}\) be the number of unlabeled points, \(A[N_{u}]\) the set of indices of auto-labeled points, and \(X_{u}(A)\) be these points. Let \(N_{a}\) be the size of the auto-labeled set \(A\). The _auto-labeling error_, denoted by \(}(X_{u}(A))\), and the _coverage_, denoted by \(}(X_{u}(A))\), are defined as follows:

\[}(X_{u}(A)):=}_{i A}( {y}_{i} f^{*}(_{i})),}(X_{u}(A)):=|A|/N_{u}=N_{a}/N_{u}.\] (1)

The goal of an auto-labeling algorithm is to label the dataset so that \(}(X_{u}(A))_{a}\) while maximizing coverage \(}(X_{u}(A))\) for a _user-given error tolerance_ parameter \(_{a}\). As depicted in Figure 1, the TBAL algorithm proceeds iteratively. In each iteration, it queries labels for a subset of unlabeled points from the oracle. It trains a classifier from the model class \(\) on the oracle-labeled data acquired till that iteration. It then uses the model's confidence scores on the validation data to identify the region in the instance space, where the current classifier is confidently accurate and automatically labels the points in this region. The auto-labeled points are removed from the unlabeled pool. Similarly, to maintain parity between the validation and unlabeled data in the next round, the validation points in the auto-labeling region are removed as well. These steps are executed in a loop until all the data is labeled or the budget to query oracle labels is exhausted.

Fundamental differences between TBAL, self-training and active learning.** At first glance, TBAL may appear similar to active learning (AL) , self-training (ST) , and selective classification (SC) . However, as described in , it is a fundamentally different technique designed with different goals. Perhaps the most substantial difference is that TBAL's aim is to create accurately labeled datasets, while the goal in AL and ST is to learn the best (in terms of generalization error) possible classifier in a given model class with limited ground truth labels. This difference is most substantial in the settings where AL converges to a bad classifier, e.g., due to incorrect choice of the model class, sampling bias, etc.  illustrates this notion with a scenario where TBAL coverage is above 95% while the other techniques average around 20%. See Appendix A.1 for details.

Figure 2: Scores distributions (Kernel Density Estimates) of a CNN model trained on CIFAR-10 data. (a) softmax scores of the vanilla training procedure (SGD) (b) scores after post-hoc calibration using temperature scaling and (c) scores from our Colander procedure applied on the same model. For training the CNN model we use 4000 points drawn randomly and 1000 validation points (of which 500 are used for Temp. Scaling and Colander ). The test accuracy of the model is 55%. Figures (d) and (e) show the coverage and auto-labeling error of these methods. The dotted-red line corresponds to a user-given error tolerance of 5%.

Problems with confidence functions in TBAL.The success of TBAL hinges on the ability of the classifier's confidence scores to distinguish between correct and incorrect labels. Prior works on TBAL [56; 42] train the model with stochastic gradient descent (SGD) and use the softmax output of the model as confidence scores, which are known to be overconfident . A natural choice to mitigate this problem is to use post-hoc calibration techniques, e.g., temperature scaling . We evaluate these choices by running TBAL for a single round on the CIFAR-10  dataset with a SimpleCNN model with 5.8M parameters  with error threshold \(5\%\). Details are in Appendix A.2.

In Figures 2(d) and 2(e) we observe that using softmax scores from the classifier only produces \(2.9\%\) coverage while the error threshold is violated with \(10\%\) error. Using temperature scaling only increases the coverage marginally to \(4.9\%\) and still violates the threshold with error \(14\%\). Looking closer at the scores for correct versus incorrect examples on validation data, we observe a large overlap for softmax (Figure 2(a)) and a marginal shift with considerable overlap for temperature scaling (Figure 2(b)). To overcome this challenge, we propose a novel framework (Section 3) to learn such confidence functions in a principled way. Our method in this example can achieve \(50\%\) coverage with an error of \(3.4\%\) within the desired threshold (Figure 2(c)).

## 3 Proposed Method (Colander)

The observations in Figure 2(a) and 2(b) suggest that fixed choices of confidence functions can leave significant coverage on the table. To find a better choice in a principled manner, we develop a framework based on auto-labeling objectives--maximizing coverage while having bounded auto-labeling error. We instantiate it by using empirical estimates and easy-to-optimize surrogates. We use the overall TBAL workflow from  and introduce our method to replace the confidence (scoring) function after training the classifier.

### Auto-labeling optimization framework

In any iteration of TBAL, we have a model \(\) trained on a subset of data labeled by the oracle. This model may not be highly accurate. However, it could be accurate in some regions of the instance space, and with the help of a confidence function \(g\), we want to identify the points where the model is correct and auto-label them. As we saw earlier, arbitrary choices of \(g\) perform poorly on this task. Instead, we propose a framework to find the right function from a sufficiently rich family.

Figure 3: Threshold-based Auto-labeling with Colander: takes unlabeled data as input, selects a small subset of data points, and obtains human labels for them to create \(D^{(i)}_{}\) and \(D^{(i)}_{}\) for the \(i\)th iteration. Trains model \(_{i}\) on \(D^{(i)}_{}\). In contrast to the standard TBAL procedure, here we randomly split \(D^{(i)}_{}\) into two parts, \(D^{(i)}_{}\) and \(D^{(i)}_{}\). Colander kicks in, takes \(_{i}\) and \(D^{(i)}_{}\) as input and learns a coverage maximizing confidence function \(_{i}\) for \(_{i}\). Using \(D^{(i)}_{}\) and \(_{i}\) auto-labeling thresholds \(}_{i}\) are determined to ensure the auto-labeled data has error at most \(_{a}\) (a user-given parameter). After obtaining the thresholds the rest of the steps are the same as standard TBAL. The whole workflow runs until all the data is labeled or another stopping criterion is met.

Optimal confidence function.To find a confidence function aligned with our objective, we consider a space of thresholds \(T\) (e.g. \(\)) and functions \(@converttounder{}: T^{k}\), where \(T^{k}\) is the \(k-\)dimensional product space of \(T\). We express the auto-labeling objective as an optimization problem (P1):

\[, T^{k}}{} (g,)\ \ \ \ (g,)_{a}.\] (P1)

Here \((g,|)\) and \((g,)\) are the population level coverage and auto-labeling error which are defined as follows,

\[(g,):=_{}g()[][](g,):=_{}y  g()[][].\] (2)

The optimal \(g^{}\) and \(^{}\) that achieve the maximum coverage while satisfying the auto-labeling error constraint belong to the solution(s) of this optimization problem.

### Practical method to learn confidence functions

The framework provides a theoretical characterization of the optimal confidence functions and thresholds for TBAL. However, it is impractical since the distributions and \(/^{}\) are unknown. Next, we give a practical method based on the above framework to learn confidence functions for TBAL.

Empirical optimization problem.Since we do not know the distributions of \(\) and \(/^{}\), we use estimates of coverage and auto-labeling errors on a fraction of validation data to solve the optimization problem. Let \(D\) be some finite number of labeled samples, and then the empirical coverage and auto-labeling error are defined as follows,

\[}(g,,D):=_{(,y) D}g()[][] ,\] (3)

\[}(g,,D):=,y) D }y g()[][ ]}{_{(,y) D}g()[ ][]}.\] (4)

We randomly split the validation data into two parts \(D_{}\) and \(D_{}\) and use \(D_{}\) to compute \(}(g,,D_{})\) and \(}(g,,D_{})\). Using these estimates, we now seek to solve the following problem,

\[, T^{k}}{} }(g,,D_{})\ \ \ }(g,,D_{})_{a}.\] (P2)

Nevertheless, the presence of \(0\)-\(1\) variables means the problem remains challenging.

Surrogate optimization problem.To make the optimization (P2) tractable using gradient-based methods, we introduce differentiable surrogates for the \(0\)-\(1\) variables. Let \((,z):=1/(1+(- z))\) denote the sigmoid function on \(\) with scale parameter \(\). It is easy to see that, for any \(g,y\) and \(\), \(g()[y][y](,g()[y]-[y]) 1/2\). Using this fact, we define the following surrogates of the auto-labeling error and coverage:

\[}(g,|,D_{}):=}|}_{(,y) D_{}},g( )[]-[],\] (5)

\[}(g,,D_{}):=,y) D_{}}y\, ,g()[]-[]}{_{ (,y) D_{}},g()[] -[]},\] (6)

and the surrogate optimization problem as follows,

\[, T^{k}}{}- }(g,,D_{})+\, }(g,,D_{})\] (P3)

Here, \(^{+}\) is the penalty term controlling the relative importance of the auto-labeling error and coverage. We tune it with the procedure discussed in Section 4.3. The gap between the surrogate and actual coverage diminishes as \(\). We discuss this in the Appendix.

[MISSING_PAGE_FAIL:6]

are obtained by solving the relaxed optimization problem. Thus, it is crucial to estimate reliable thresholds \(}_{i}\) from the held-out data \(D_{}^{(i)}\) to ensure the auto-labeling error constraint is not violated.

## 4 Threshold estimation.

The scores from the new confidence function \(_{i}\) on \(D_{}^{(i)}\) are used to estimate auto-labeling thresholds in Algorithm 2. This procedure finds thresholds for each class separately. It first splits the points in \(D_{}^{(i)}\) according to the ground truth class into subsets \(D_{}^{(i,y)}\). Then, for each class \(y\), it finds the auto-labeling threshold \(}[y]\) by selecting the minimum threshold \(t\) such that the estimate of auto-labeling error for class \(y\), \(}_{y}(_{i},t|}}_{i},D_{ }^{(i,y)})\) plus a confidence interval \((}_{y}(_{i},t| }}_{i},D_{}^{(i,y)}))\) estimated on points in \(D_{}^{(i,y)}\) having scores above \(t\), is at most the given error tolerance \(_{a}\). Here \((z)=C_{1}\) for \(z\) and \(C_{1} 0\) is a hyperparameter.

**5. Auto-labeling.** This is a simple step. We compute the scores on the remaining unlabeled data \(X_{}^{(i)}\) using the function \(_{i}\) and any point \( X_{}^{(i)}\) having score above \(}[]\) is assigned auto-label \(=_{i}()\), and the points that did not meet this criterion remain unlabeled.

**6. Remove auto-labeled points.** The points that got auto-labeled in the previous steps are removed from the unlabeled pool. To make the validation data consistent with this unlabeled pool for the next round, the points in the validation data that fall into the auto-labeling region are also removed.

**7. Get more human-labeled data.** Lastly, it calls the procedure \((_{i},X_{u}^{(i)},n_{b})\) to select \(n_{b}\) points from the remaining unlabeled pool using an active learning strategy. This newly acquired human-labeled data is added to the training data \(D_{}^{(i)}\). The details of the querying strategy are in Algorithm 3 Appendix B. Note, that the TBAL procedure is flexible to work with any choice of active querying strategy. We pick a simple strategy based on random sampling from the points where the classifier is most uncertain. To avoid confounding in TBAL with other scores we use the softmax scores from the classifier to determine uncertainty here.

The procedure then moves to step 2 and runs the loop until there are no more unlabeled points left or it has queried the stipulated number of human labels \(N_{t}\).

## 4 Empirical Evaluation

We validate the following claims through extensive empirical evaluation,

**C1.** Colander learns better confidence functions for auto-labeling compared to standard training and common post-hoc methods that seek to mitigate the overconfidence problem. Using it in TBAL can boost the coverage significantly while keeping the auto-labeling error low.

**C2.** Colander is independent of any particular train-time method and thus should help improve the performance when coupled with different train-time methods.

### Baselines

We examine several train-time and post-hoc methods that improve confidence functions from the calibration and ordinal ranking perspectives. Details of these methods are in the Appendix C.7.

**Train-time methods.** We use the following methods for training the model \(\). _Vanilla_ neural networks are trained with the cross-entropy loss using stochastic gradient descent (SGD) [1; 4; 12]. _Squentropy_ adds the average square loss over the incorrect classes to the cross-entropy loss to improve the calibration and accuracy of the model. _Correctness Ranking Loss (CRL)_ aligns the confidence scores of the model with the ordinal rankings criterion via regularization. _FMFP_ aligns confidence scores with the ordinal rankings criterion by using sharpness-aware minimization (SAM)  in lieu of SGD.

**Post-hoc methods.** We use the following methods for learning (or updating) the confidence function \(\) after learning \(\). _Temperature scaling_ is a variant of Platt scaling . It rescales the logits by a learnable scalar parameter. _Top-Label Histogram-Binning_ builds on the histogram-binning method  and focuses on calibrating the scores of the predicted label assigned to unlabeled points. _Scaling-Binning_ applies temperature scaling and then bins the confidence function values. _Dirichlet Calibration_ models the distribution of predicted probability vectors separately on instances of each class and assumes Dirichlet class conditional distributions. _Adaptive Temperature Scaling_ builds on top of temperature scaling and considers that different samples contribute to the calibration error differently. Each train-time method is piped with a post-hoc method, yielding a total of \(4 6=24\) methods.

### Datasets and models

We evaluate the performance of auto-labeling on four datasets. Each is paired with a model for auto-labeling: _MNIST_ is a hand-written digits dataset. We use the LeNet  for auto-labeling. _CIFAR-10_ is an image dataset with 10 classes. We use a CNN with approximately 5.8M parameters  for auto-labeling. _Tiny-ImageNet_ is an image dataset comprising 100K images across 200 classes. We use CLIP  to derive embeddings for the images in the dataset and use an MLP model. _20 Newsgroups_ is a natural language dataset comprising around 18K news posts across 20 topics. We use the FlagEmbedding  to obtain text embeddings and use an MLP model.

### Hyperparameter search and evaluation

The complexity of TBAL workflow and lack of labeled data make hyperparameter search and evaluation challenging. Similar challenges have been observed in active learning . We discuss our practical approach and defer the details to Appendix C.10 and code2.

**Hyperparameter search.** We run only the first round of TBAL with each method using a hyperparameter combination 5 times and measure the mean auto-labeling error and mean coverage on \(D_{}\), which represents a small part of the held-out human-labeled data. We pick the combination that yields the lowest average auto-labeling error while maximizing the coverage. We first find the best hyperparameters for each train-time method, fix those, and then search the hyperparameters for the post-hoc methods. Note that the best hyperparameter for a post-hoc method depends on the training-time method that it pipes to. The hyperparameter search spaces are in the Appendix C; and the selected values used for each setting are in the supplementary material.

**Performance evaluation.** After fixing the hyper-parameters, we run TBAL with each combination of train-time and post-hoc method on full \(X_{u}\) of size \(N\), with a fixed budget of \(N_{t}\) labeled training samples and \(N_{v}\) validation samples. The details of these values for each dataset are in Table 1 in Appendix C. Here, we know the ground truth labels for the points in \(X_{u}\), so we measure the auto-labeling error and coverage as defined in (1) and report them in Table 2.

### Results and discussion

Our findings, shown in Table 2, are:

_C1: Colander improves TBAL performance._ Our approach aims to optimize the confidence function to maximize coverage while minimizing errors. When applied to TBAL, we expect it to yield substantial coverage enhancement and error reduction compared to vanilla training and softmax scores. Indeed, the results in Table 2 corresponding to the vanilla training match our expectations. We see _across all data settings_, our method achieves _significantly higher coverage_ while keeping auto-labeling error below the tolerance level of 5%. The improvements are even more pronounced when

  
**Dataset** & **Model \(\)** & \(N\) & \(N_{}\) & \(K\) & \(N_{}\) & \(N_{v}\) & \(N_{}\) & **Modality** & **Preprocess** & **Dimension** \\  MNIST & LeNet-5 & 70k & 60k & 10 & 500 & 500 & 500 & Image & None & \(1 28 28\) \\ CIFAR-10 & CNN & 50k & 40k & 10 & 10k & 8k & 2k & Image & None & \(3 32 32\) \\ Tiny-Imagenet & MLP & 110k & 90k & 200 & 10k & 8k & 2k & Image & CLIP & 512 \\
20 Newsgroup & MLP & 11.3k & 9k & 20 & 2k & 1.6k & 600 & Text & FlagEmb. & 1,024 \\   

Table 1: Details of the dataset and model we used to evaluate the performance of our method and other calibration methods. For the Tiny-Imagenet and 20 Newsgroup datasets, we use CLIP and FlagEmbedding, respectively, to obtain the embeddings of these datasets and conduct auto-labeling on the embedding space. For Tiny-Imagenet, we use a 3-layer perceptron with 1,000, 500, 300 neurons on each layer as model \(\); for 20 Newsgroup, we use a 3-layer perceptron with 1,000, 500, 30 neurons on each layer as model \(\).

the datasets are more complex than MNIST. Also consistent with our expectation and observations in Figure 2(b), the post-hoc calibration methods improve the coverage over using softmax scores but at the cost of slightly higher error. While they are reasonable choices to apply in the TBAL pipeline, they fall short of maximally improving TBAL performance due to the misalignment of goals.

_C2: Colander is compatible with and improves over other train-time methods._ Our method is compatible with various choices of train-time methods, and if a train-time method (Squentropy here) provides a _better_ model relative to another train-time method (e.g., Vanilla), then our method exploits this gain and pushes the performance even further. Across different train-time methods, we do not see significant differences in the performance, except for Squentropy. Using Squentropy with softmax improves the coverage by as high as 6-7% while dropping the auto-labeling error in contrast to using softmax scores obtained with other train-time methods for the Tiny-ImageNet setting. This is unexpected: Squentropy adds the average square loss over the incorrect classes as a regularizer, and it has offered better accuracy and calibration compared to training with cross-entropy loss.

_Train-time methods designed for ordinal ranking objective perform poorly in auto-labeling._ CRL and FMFP are state-of-the-art methods designed to produce scores aligned with the ordinal ranking criteria. Ideally, if the scores satisfy this criterion, TBAL's performance would improve. However, we do not see any significant difference from the Vanilla method. Similar to the other baselines, their

    &  &  &  &  &  \\   & & **Err (\(\))** & **Cov (\(\))** & **Err (\(\))** & **Cov (\(\))** & **Err (\(\))** & **Cov (\(\))** & **Err (\(\))** & **Cov (\(\))** \\   & Softmax & **4.1\(\)0.7** & 85.0\(\)0.25 & 4.8\(\)0.2 & 14.0\(\)2.1 & 6.0\(\)0.6 & 48.2\(\)1.6 & 11.1\(\)0.3 & 32.6\(\)0.5 \\   & TS & 7.8\(\)0.6 & 94.2\(\)0.5 & 7.3\(\)0.3 & 23.2\(\)0.7 & 9.7\(\)0.6 & 60.7\(\)2.3 & 16.3\(\)0.5 & 37.4\(\)1.5 \\   & Dirichlet & 7.9\(\)0.7 & 93.2\(\)2.2 & 7.7\(\)0.5 & 22.4\(\)1.2 & 9.4\(\)0.9 & 59.4\(\)1.8 & 17.1\(\)0.4 & 33.3\(\)2.0 \\   & SB & 6.7\(\)0.5 & 92.6\(\)1.5 & 6.1\(\)0.4 & 18.6\(\)1.1 & 8.1\(\)0.6 & 58.1\(\)1.8 & 15.7\(\)0.6 & 35.4\(\)1.2 \\   & Top-HB & 7.4\(\)1.4 & 93.1\(\)3.6 & 6.0\(\)0.7 & 15.6\(\)1.9 & 9.2\(\)1.0 & 59.0\(\)2.0 & 16.6\(\)0.5 & 37.6\(\)2.2 \\   & AdaTS & 7.5\(\)0.9 & 92.8\(\)2.0 & 8.6\(\)0.6 & 16.9\(\)1.0 & 9.6\(\)1.1 & 61.8\(\)3.3 & 15.9\(\)0.7 & 36.7\(\)1.9 \\   & **Ours** & 4.2\(\)1.5 & **95.6\(\)1.4** & **3.0\(\)0.2** & **78.5\(\)0.2** & **2.5\(\)1.1** & **80.6\(\)0.7** & **1.4\(\)2.1** & **59.2\(\)0.8** \\   & Softmax & 4.7\(\)0.4 & 86.0\(\)4.5 & 5.2\(\)0.3 & 15.9\(\)0.8 & 5.8\(\)0.5 & 48.3\(\)0.3 & 10.4\(\)0.4 & 32.5\(\)0.6 \\   & TS & 8.0\(\)0.8 & 94.8\(\)0.8 & 6.8\(\)0.8 & 20.3\(\)1.1 & 9.5\(\)0.1 & 61.7\(\)1.6 & 15.8\(\)0.6 & 37.4\(\)1.7 \\   & Dirichlet & 8.6\(\)0.6 & 93.1\(\)1.6 & 7.7\(\)0.2 & 20.9\(\)1.1 & 8.7\(\)0.9 & 58.0\(\)1.4 & 16.3\(\)0.4 & 33.1\(\)1.9 \\   & SB & 7.4\(\)0.8 & 93.1\(\)2.7 & 5.9\(\)0.9 & 17.9\(\)1.5 & 8.9\(\)1.1 & 57.9\(\)3.9 & 15.0\(\)0.4 & 35.5\(\)1.2 \\   & Top-HB & 7.7\(\)0.8 & 94.1\(\)1.5 & 4.4\(\)0.5 & 12.3\(\)0.4 & 8.8\(\)1.0 & 58.8\(\)2.7 & 16.5\(\)0.5 & 38.9\(\)1.6 \\   & AdaTS & 7.8\(\)0.7 & 94.3\(\)1.2 & 8.8\(\)0.4 & 17.1\(\)1.2 & 9.1\(\)0.8 & 60.8\(\)1.9 & 16.2\(\)0.4 & 38.9\(\)1.2 \\   & **Ours** & **4.5\(\)1.4** & **95.6\(\)1.3** & **2.2\(\)0.6** & **77.9\(\)0.2** & **1.8\(\)1.2** & **81.3\(\)0.5** & **2.8\(\)2.1** & **61.2\(\)1.4** \\   & Softmax & 4.8\(\)0.8 & 84.2\(\)4.1 & 4.9\(\)0.4 & 15.6\(\)1.7 & 5.4\( is focused on models trained on large amounts of data. But, in TBAL, we have less data for training. The training error goes to zero after some rounds, and no information is left for the CRL loss to distinguish between correct and incorrect predictions (i.e., count SGD mistakes). On the other hand, FMFP is based on a hypothesis that training models using Sharpness Aware Minimizer (SAM) could lead to scores satisfying the ordinal ranking criteria. However, this phenomenon is still not well understood, especially in settings like ours with limited training data.

## 5 Related Work

**Data labeling.** We briefly discuss prominent methods for labeling. Crowdsourcing [45; 50] uses a crowd of non-experts to complete a set of labeling tasks. Works in this domain focus on denoising the obtained information, modeling label errors, and designing effective labeling tasks [11; 22; 33; 53; 52; 54; 5]. Weak supervision (WS), in contrast, emphasizes labeling through multiple inexpensive but noisy sources, not necessarily human [44; 48; 55; 18; 49; 60; 63]. Works such as [44; 8] concentrate on binary or multi-class labeling, while [48; 55] extend WS to structured prediction tasks.

Auto-labeling occupies an intermediate position between weak supervision and crowdsourcing in terms of human dependency. It aims to minimize costs to obtain human labels while generating high-quality labeled data using a specific model.  use a TBAL-like algorithm and explore the cost of training for auto-labeling with large-scale model classes. Recent work  theoretically analyzes the sample complexity of validation data required to guarantee the quality of auto-labeled data.

**Overconfidence and calibration.** The issue of overconfidence [51; 38; 16; 3] is detrimental in several applications (such as robustness to out-of-distribution points [59; 57]), including ours. Many solutions have emerged to mitigate the overconfidence and miscalibration problems. Gawlikowski et al.  provide a comprehensive survey on uncertainty quantification and calibration techniques for neural networks. Guo et al.  evaluated a variety of solutions ranging from the choice of network architecture, model capacity, weight decay regularization , histogram-binning and isotonic regression [61; 62] and temperature scaling [41; 39] which they found to be the most promising solution. The solutions fall into two broad categories: train-time and post-hoc. Train-time solutions modify the loss function, include additional regularization terms, or use different training procedures [27; 37; 36; 19]. On the other hand, post-hoc methods such as top-label histogram-binning , scaling binning , Dirichlet calibration  calibrate the scores directly or learn a model that corrects miscalibrated confidence scores.

**Beyond calibration.** While calibration aims to match the confidence scores with a probability of correctness, it is not the precise solution to the overconfidence problem in many applications, including our setting. The desirable criteria for scores for TBAL are closely related to the ordinal ranking criterion . To get such scores, Corbiere et al.  add a module in the net for failure prediction, Zhu et al.  switch to sharpness aware minimization  to learn the model; CRL  regularizes the loss.

## 6 Conclusion

We studied issues with confidence scoring functions used in threshold-based auto-labeling (TBAL). We showed that the commonly used confidence functions and calibration methods can often be a bottleneck, leading to poor performance. We proposed Colander to learn confidence functions that are aligned with the TBAL objective. We evaluated our method extensively against common baselines on several real-world datasets and found that it improves the performance of TBAL significantly in comparison to the several common choices of confidence function. Our method is compatible with several choices of methods used for training the classifier in TBAL and using it in conjunction with them improves TBAL performance further. A limitation of Colander is that, similar to other post-hoc methods it also requires validation data to learn the confidence function. Reducing (or eliminating) this dependence on validation data could be an interesting future work.

## 7 Acknowledgments

This work was partly supported by funding from the American Family Data Science Institute. We thank Heguang Lin, Changho Shin, Dyah Adila, Tzu-Heng Huang, John Cooper, Aniket Rege, Daiwei Chen and Albert Ge for their valuable inputs. We thank the anonymous reviewers for their valuable comments and constructive feedback on our work.