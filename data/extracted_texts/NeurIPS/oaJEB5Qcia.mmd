# FGPrompt: Fine-grained Goal Prompting

for Image-goal Navigation

Xinyu Sun\({}^{1,3}\) Peihao Chen\({}^{1}\) Jugang Fan\({}^{1}\) Thomas H. Li\({}^{4}\)

Jian Chen\({}^{1}\) Mingkui Tan\({}^{1,2,5}\)1

\({}^{1}\)South China University of Technology, \({}^{2}\)Pazhou Laboratory,

\({}^{3}\)Information Technology R&D Innovation Center of Peking University,

\({}^{4}\)Peking University Shenzhen Graduate School,

\({}^{5}\)Key Laboratory of Big Data and Intelligent Robot, Ministry of Education,

csxinyusu@gmail.com, mingkuitan@scut.edu.cn

Project Page & Videos: https://xinyusun.github.io/fgprompt-pages

###### Abstract

Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems. The agent is required to reason the goal location from where a picture is shot. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (FGPrompt) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (_i.e.,_ Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8% with only 1/50 model size.

## 1 Introduction

We focus on the image-goal navigation (ImageNav) task  that requires an agent to navigate to an image-specified goal position and face the same orientation as where the photo is taken. In this task, the agent needs to explore the environment and try to find the objects with their surroundings that best match the ones specified in the goal image. Though humans prefer to share information using language, an image is a much clearer and more detailed description to specify a goal location or an intermediate landmark for some household robots  or self-driving vehicles.

Despite its wide applications, this task is still very challenging for the embodied agent due to the following two aspects. First, compared to object-goal navigation which assigns goal descriptions with specific semantic categories, it requires the agent to perceive the visual observation as well as the goal image and make a comprehensive understanding of the scene in order to identify goal-relevant objects. Second, objects share similar semantic meanings within one environment, making it challenging to accurately find out the desired object instance.

Previous methods  seek to solve this task by decomposing the navigation system into several modules in isolation. In general, they tend to adopt efficient exploration skills to build a map incrementally as the understanding of the scene, and further predict a waypoint to navigate to. However, these map-based methods require depth maps or the agent's GPS position to build the occupancy map or topological map. The latest methods  instead try to learn a navigation policy in an end-to-end manner using reinforcement learning. These methods set up two different encoders to obtain semantic embeddings from goal and observation images independently. Subsequently, a recurrent model takes these embeddings as input to predict a possible action sequence. However, they suffer from two major limitations: 1) As the details in the goal image are gradually overlooked as it goes through deeper network layers, it is harder to find useful cues for reasoning and finding the goal location. 2) Existing methods leave the goal image apart from the observation when performing encoding, it is hard for the agent to focus on the goal-relevant regions in the observation since there is no goal prompting to guide the agent to understand the observation.

When people try to find a place captured in an image, they must look for the contextual cues presented with objects, shapes, colors, and textures in both the goal images and current visual observation. Spatial reasoning based on this information plays a critical role in understanding the scene, as people always compare and identify similarities, in order to consider the relative position of various elements and gain insights into the current position and the target location. Motivated by this fact, instead of considering only semantic features of goal and observation images, we propose a novel fine-grained goal prompting (**FGPrompt**) architecture to learn observation embeddings conditioned on the fine-grained and high-resolution features of the goal image.

Specifically, we implement the goal prompting scheme as a fusion process between the goal and observation images and design a mid fusion (FGPrompt-MF) mechanism. This mechanism leverages fine-grained and high-resolution feature maps in the intermediate goal network layers as the prompts, which are proven to contain informative object details . Hereafter, conditioned on these feature maps, we utilize FiLM  layers to learn a transform function to adjust the observation activations to focus on goal-relevant objects. In addition, we also design an early fusion (FGPrompt-EF) mechanism by concatenating the goal and observation images at the pixel level. We then use a neural network to perform implicit information exchange. Experimental results show that our proposed method significantly outperforms state-of-the-art methods, as shown in Figure 1.

To sum up, our contributions are as follows: 1) We propose a fine-grained image goal prompting (FGPrompt) architecture to explicitly exchange fine-grained information between goal image and observation image, reaching a new SOTA of the ImageNet task and also showing great potential in some other embodied tasks including instance image navigation and visual rearrangement tasks. 2) We empower the agent with fine-grained information exchangeability through a simple channel concatenation technique. This scheme is parameter efficient yet shows an absolute advantage on the ImageNet task, even compared to some complex memory graph-based methods. 3) We dedicately design a mid-fusion scheme through a novel fine-grained FiLM mapping module to perform a more robust information exchange. This scheme shows superior performance in more practical scenarios where the goal image possesses different camera parameters from the observation.

Figure 1: Main results of our proposed FGPrompt on the image navigation task.

## 2 Related Work

Modular methods.Modular methods leverage strictly defined modules that are handcrafted [37; 23] or learnable [7; 19; 18; 8; 37; 6; 2; 10] to address the image-goal navigation task step by step. Classical modular methods typically combine the exploration  component, simultaneous localization and mapping (SLAM [16; 43]) component, and path planning component to achieve the navigation goal. In order to localize the agent in an unknown environment, some approaches build an explicit metric map of the environment [7; 19], while others propose to obtain an implicit latent map  like a topological map [8; 37] or simply adopt object detectors without mapping . Chaplot _et al._ and Avraham _et al._ train supervised deep models to tackle the sub-tasks, which require a lot of annotated data. Although off-the-shelf modules can be used with zero fine-tuning , they still heavily rely on pose and depth sensors, which greatly limits their applicability in the real world.

RL-based navigation.Another pipeline for ImageNet is to directly learn from interactions with the environment using reinforcement learning (RL). RL-based navigation tends to learn an end-to-end reward-driven policy that maps observation to action [47; 46; 52; 28; 29; 12] and shows great potential in this task. However, these methods still face the challenge of the sparse reward mechanism and poor generalization performance. To address these issues, previous works [15; 52; 28; 26] propose different methods to encourage the agent to explore more efficiently. Yu _et al._ combines RL policy and visual representation learning model in a min-max game way to incentivize the agent to explore its environment. Al-Halah _et al._ proposes a zero-shot transfer learning approach with a novel reward for its semantic search policy. Similarly, Majumdar _et al._ uses a CLIP model pre-trained in self-supervised manner [33; 41; 9] to enhance image embedding. To tackle the long-horizon planning problem, an external memory module has been proposed by [29; 17; 3; 37; 25; 22; 11] that learns a topological graph [17; 3; 37; 25; 22] or attention map  online. Self-supervised learning paradigm has also been explored by Yadav _et al._[47; 46] to endow the navigation model with better representation ability. Different from existing approaches, we proposed a goal-prompted observation understanding method that learns to focus on goal-relevant objects through fine-grained goal prompts.

Goal-conditioned learning.Existing RL-based navigation methods can be interpreted as learning a goal-conditioned policy, since they only perform fusion on the latent goal embedding and observation embedding. Only semantic-level information can be exchanged during fusing. Some embodied robot planning methods [4; 40; 21; 49] learn a goal-conditioned observation encoder by injecting the goal embedding into it. Stone _et al._ and Brohan _et al._ only consider the language as the goal description, while Jang _et al._ and Yu _et al._ try to fuse the goal image with the intermediate feature maps of observation encoder using an affine transformation proposed by FiLM . However, they still focus on the latent embedding of goal images and neglect the fine-grained information in high-resolution activation maps. In this paper, we propose to make use of the intermediate activations in the goal encoder as informative guidance to condition the learning of the observation encoder.

## 3 Image Goal Navigation using Fine-Grained Goal Prompting

### Task definition

Image-goal navigation (ImageNav) requires an agent to navigate to a goal position that matches where the goal image \(v_{g}\) was shot. Specifically, the agent starts at a random location \(p_{0}\) and only receives a goal image \(v_{g}\) from the environment. At each time step \(t\), the agent receives an egocentric RGB image \(v_{t}\) captured by a RGB sensor fixed on its body, and executes an action \(a_{t}\) conditioned on \(v_{t}\) and \(v_{g}\). In RL-based methods, the action \(a_{t}\) is selected based on the learned policy. After performing the action \(a_{t}\), the agent will be assigned a reward \(r_{t}\) that encourages the agent to reach the goal position as soon as possible. A more detailed definition of our setup can be found in Section 4.

Existing RL-based methods tackle the ImageNav problem by learning an observation encoder and a goal encoder separately, and then fusing their output embeddings together. As shown in Figure 2 (a), this fusion module is commonly equipped on most of the baseline methods. However, those embeddings preserve little detailed information, _e.g._, shape, texture, and spatial relationship, to promote finding and comparing objects relevant to the goal image [50; 20]. To tackle this challenge, we propose to leverage fine-grained information from lower-level goal image features as prompts to promote the agent's ability to focus more on goal-relevant objects.

### Fine-grained Goal Prompting

We design and explore three different fine-grained goal prompting methods that vary from fusion mechanism, namely **Early Fusion**, **Mid Fusion**, and **Skip Fusion**. For the first early fusion mechanism, we investigate injecting fine-grained information from the goal encoder through a simple but effective channel concatenation. After that, we delicately design an explicit information flow through a novel fine-grained FiLM mapping module. Finally, we replace the learnable modules with a heuristic one that injects goal-relative features using feature matching.

Early Fusion via Joint Modeling.A naive solution to exchange information in two images is to directly concatenate them together before input to the encoder. In this way, we are able to fuse fine-grained image details in the very early stage and jointly model them using the same encoder. In particular, we concatenate the goal image with the observation image on the RGB channel dimension, resulting in an input tensor shaped \(128 128 6\). This concatenated tensor is then fed into a ResNet encoder that takes the 6-channel image as input. In this case, the fusion mechanism can be written as:

\[z_{fusion}=f_{o}(v_{o} v_{g})\] (1)

This simple design yields a promising performance on the image navigation benchmark. Detailed ablation on this early fusion operation can be found in Section 4.2.

Mid Fusion via FiLM Layers.However, as the early fusion mechanism enables spatial reasoning between two images using an identical convolution kernel, it is difficult to handle the situation when the orientation of the goal camera is noisy. To alleviate this, we further propose an active fusion scheme, utilizing the adaptability of a novel fine-grained FiLM mapping module. Previous literature [21; 49] inputs the goal embedding into the ResNet visual backbone via FiLM  layers, which adapt a learnable affine transformation conditioned on the input embedding to the intermediate activation maps in each residual blocks. Through these layers, we can easily connect the intermediate layers in both the goal encoder and the observation encoder to perform mid fusion.

Different from the existing approaches that leverage abstract language embedding as a global condition for all layers, we propose to use the hierarchical representations from the intermediate goal encoder layers. This allows us to make good use of the fine-grained information in high-resolution feature maps. Specifically, we perform FiLM affine transformation on the resnet blocks of the observation encoder, where the affine factors \(^{i}_{,},^{i}_{,}\) in block \(i\) are conditioned on the shaped activation map \(z^{i}_{g}\) from the correspondent block of the goal encoder. This process can be formulated as:

\[^{i}_{c}=f_{c}(z_{g})^{i}_{c}=h_{c}(z_{g})\] (2) \[^{i}_{o}=^{i}_{c}z^{i}_{o}+^{i}_{c}\] (3)

Figure 2: **Illustration of baseline fusion (a) and our goal prompting (b, c, d) for image-goal navigation**. All these methods take observation and goal images as input and output fused features.

where \(_{o}^{i}\) denotes a transformed activation map in block \(i\) and \(c\) denotes the \(c^{th}\) feature of the feature map. The functions \(f\) and \(h\) learn to map the condition variable into the affine factors. In practice, we implement them as \(1 1\) convolutions to maintain the same resolution between the input and target activation map. Section 4.2 further investigates the choices of the mapping function and the number of FiLM layers. The output from the conditioned observation encoder \(f_{o}\) can then be viewed as the fused feature \(z_{fusion}\), as shown in Figure 2 (c). The fused feature can be written as:

\[z_{fusion}=f_{o}(v_{o}|v_{g})\] (4)

Our experiments in Section 4.3 reveal that the mid fusion scheme performs more robustly when the configuration of the goal camera and the observation camera is not perfectly matched.

Skip Fusion via Keypoint Matching.In order to evaluate the importance of the addition of the fusion modules, We finally replaced the aforementioned learnable modules with a heuristic one. To achieve this, we follow the idea of Wasserman _et al._ that attach an additional low-level fusion module using handcrafted keypoint matching methods [27; 36], as an improvement of the Late Fusion baseline. We name this mechanism Skip Fusion as it fuses the goal image and observation image in the both early and later stages but skip the others, as shown in Figure 2 (b).

Keypoint matching, which aims to discover representative keypoints in an image and then describe and match them with the most similar ones in another image. As these points are detected based on the low-level statistic [27; 13] of image pixels, we leverage them to play a role as low-level fusion. This scheme is handcrafted as it is not learnable during training. To enable batch inference, we leverage a deep learning-based keypoint detecting  and a matching  method to obtain matched keypoint between the goal image and the observation image. Hereafter, we select top-k matched points according to their matching score to compose a variable \(z_{k}\) and concatenate them together with \(z_{g}\) and \(z_{o}\) as the fusion result:

\[z_{fusion}=z_{g} z_{o}(z_{k})\] (5)

where \(z_{k}=(x_{1},y_{1},x_{1}^{},y_{1}^{},...,x_{k},y_{k},x_{k}^{},y_{k}^{})\) is a flattened vector of \(k\) keypoints and FC denotes to a fully connected layer. The default value of vector \(z_{k}\) is set to \(-1\) if the number of matched keypoints is less than k. In Section 4.2, we show the superiority of our proposed Early Fusion and Mid Fusion schemes against this heuristic fusion baseline.

### Navigation Policy

Based on the fused embedding \(z_{fusion}\) of the goal image and observation image, we train a navigation policy \(\) using reinforcement learning (RL):

\[s_{t}=(z_{fusion} a_{t-1}|h_{t-1})\] (6)

where \(s_{t}\) is the embedding of the agent's current state. \(h_{t-1}\) denotes hidden state of the recurrent layers in policy \(\) from previous step. Following previous methods [52; 28], we adopt an actor-critic network to predict state value \(c_{t}\) and action \(a_{t}\) using \(s_{t}\) and train it end-to-end using PPO . We utilize the ZER reward  to encourage the agent to not only reach the goal position but also face the goal orientation. More details can be found in Appendix.

## 4 Experiments

Datasets.As for image-goal navigation, we use the Habitat simulator [38; 42] and train our agent on the Gibson dataset with 72 training scenes and 14 testing scenes under the standard setting. We use the training episodes provided by  and train our agent for 500M steps. We report results under multiple datasets to allow direct comparison to various prior works. On the Gibson dataset, we validate our agent on split A generated by , and split B generated by . On the MP3D and HM3D, we use the test episodes collected by , as well as the instance image navigation dataset released by . We also extend our method to another embodied task named visual rearrangement, where we use the iTHOR simulator and ai2thor-rearrangement 2023 dataset with 80 training scenes, 20 validation scenes and 20 test scenes. Following , we train our agent for 75M steps and finally test the best validation checkpoint on the test set.

Agent configuration.We follow the recipe of previous trails [52; 28; 47] to initialize an agent equipped with only RGB cameras of \(128 128\) resolution and \(90^{}\) FOV. When compared with methods that use a panoramic input, we initialize four RGB sensors to the front, left, right, and back directions of the agent, following [29; 47]. The agent's action space is comprised of four discrete actions, including MOVE_FORWARD, TURN_LEFT, TURN_RIGHT, STOP. The minimum units of rotation and forward movement are \(30^{}\) and 0.25m respectively.

Evaluation metrics.We report the success rate (SR) and Success weighted by Path Length (SPL) , which takes into account path efficiency of the navigation process. An episode is considered successful if the agent stops within 1.0m Euclidean distance from the goal location and the maximum number of steps in an episode is set to 500 as the default setting.

### Comparison with State-of-the-art Methods

Evaluation on Gibson.In Table 1, we report the ImageNav results on Gibson averaged over three random seeds (the variances of all random seed results are less than 1e-4.). We compare our methods with state-of-the-art methods in two different settings, one takes only one RGB sensor as input following [52; 28; 47] and another one takes 4 RGB sensors to assemble a panoramic view following [29; 47]. For the SLAM-based methods in the first three rows, we report the number reproduced by Mezghani _et al_. . We found that our proposed FGPrompt-MF and FGPrompt-EF methods take an absolute advantage compared with all previous methods. Even compared to OVRL-V2 , a method that utilizes a much larger visual backbone (ViT-B) pre-trained on an in-domain image dataset, we still achieved large performance gains on both SR (92.3% vs. 82.0%) and SPL (68.5% vs. 58.7%) in the absence of additional pose sensor input. This finding reveals the effectiveness and efficiency of our proposed method.

   Method & Backbone & Pretrain & Sensor(s) & Memory & Split & SPL & SR \\  NTS  & ResNet9 & N/A & RGBD+Pose & ✗ & A & 43.0\% & 63.0\% \\ Act-Neur-SLAM  & ResNet9 & N/A & RGB+Pose & ✗ & A & 23.0\% & 35.0\% \\ SPTM  & ResNet9 & N/A & RGB+Pose & ✗ & A & 27.0\% & 51.0\% \\  ZER  & ResNet9 & N/A & RGB & ✗ & A & 21.6\% & 29.2\% \\ ZSON  & ResNet50 & OSD & RGB & ✗ & A & 28.0\% & 36.9\% \\ OVRL  & ResNet50 & OSD & RGB & ✗ & A & 27.0\% & 54.2\% \\ OVRL-V2  & ViT-Base & HGSP & RGB & ✗ & A & 58.7\% & 82.0\% \\
**FGPrompt-MF (Ours)** & ResNet9 & N/A & RGB & ✗ & A & 62.1\% & 90.7\% \\
**FGPrompt-EF (Ours)** & ResNet9 & N/A & RGB & ✗ & A & 66.5\% & 90.4\% \\
**FGPrompt-EF (Ours)** & ResNet50 & N/A & RGB & ✗ & A & **68.5\%** & **92.3\%** \\  Mem-Aug  & ResNet18 & N/A & 4 RGB & ✓ & A & 56.0\% & 69.0\% \\ VGM  & ResNet18 & N/A & 4 RGB & ✓ & A & 64.0\% & 76.0\% \\ OVRL  & ResNet50 & OSD & 4 RGB & ✗ & A & 62.5\% & 79.8\% \\ T5GM  & ResNet18 & N/A & 4 RGB & ✓ & A & 67.2\% & 81.1\% \\
**FGPrompt-EF (Ours)** & ResNet9 & N/A & 4 RGB & ✗ & A & **75.0\%** & **94.2\%** \\  NRNS  & ResNet18 & N/A & RGBD & ✗ & B & 12.4\% & 24.0\% \\
**FGPrompt-EF (Ours)** & ResNet9 & N/A & RGB & ✗ & B & **70.5\%** & **93.0\%** \\   

Table 1: **Comparison with state-of-the-art methods on Gibson**. All methods are trained and evaluated both on the Gibson dataset.

    &  &  &  \\   & & SPL & SR & SPL & SR \\  Mem-Aug  & Resnet18 & 3.9\% & 6.9\% & 3.5\% & 1.9\% \\ NRNS  & Resnet18 & 5.2\% & 9.3\% & 4.3\% & 6.6\% \\ ZER  & Resnet9 & 10.8\% & 14.6\% & 6.3\% & 9.6\% \\
**FGPrompt-MF (Ours)** & Resnet9 & **50.4\%** & **77.6\%** & **49.6\%** & **76.1\%** \\   

Table 2: **Cross-domain evaluation on MP3D and HM3D**. The agent is trained in Gibson environments and directly transferred to new environments for evaluation.

We extend our FGPrompt-EF to the panoramic view setting (4 RGB) for direct comparison with some memory-based methods [29; 25; 22] and pre-trained method . We found that our FGPrompt-EF outperforms these memory-based methods by at least 13.1% in success rate and 7.8% in SPL, even without additional external memory module and pre-training phase. Besides, we also provide a comparison result on the non-mainstream testing episodes (split B) following . Compared with the self-supervised method NRNS  that pretrained on passive videos, our FGPrompt-EF brings 58.1% improvement in success rate and 69.0% in SPL, which shows a great advantage by learning to understand the scene based on goal prompting through interacting with the environment.

Cross-domain evaluation on out-of-domain datasets.In Table 2, we report the cross-domain evaluation results on the unseen scenes in the Matterport3D (MP3D)  and HM3D  to verify the generalization ability. Following , we directly transfer our model trained on Gibson to these two new datasets, without any tuning. Since there exists a very large domain gap between these datasets (_e.g._more complex and larger scenes in MP3D and diverse scene types in HM3D), this setting is extremely challenging. We leverage the testing episodes released by ZER . Compared with the baseline method ZER, our fine-grained goal prompting method brings \(7\) improvements in the success rate, which shows the generalization ability of our method.

### Ablation Study

In this section, we first compare the effectiveness of different variants of our method on the ImageNav task. Then we present the detailed ablation of each method to empirically discover their best implementation. For convenience and fairness, all variants in the ablation study are trained for 50M steps on the Gibson dataset.

Comparing different goal prompting methods.We first compare the proposed goal prompting variants on the image-goal navigation task. As shown in Table 3, the Skip Fusion (FGPrompt-SF) variant, integrated fine-grained information by simply adding a keypoint matching-based fusion module to the baseline, performs significantly better (from 14.0% to 41.4%). This reveals that fine-grained goal prompting is important. However, this heuristic method does not work when there is no matched area in the observation. The other two variants further tackle this problem by learning a joint-modeling framework. In detail, the Mid Fusion (FGPrompt-MF) mechanism leverages the intermediate activation maps with varied resolutions to perform goal prompting. As a result, this variant further increases the navigation success rate by 27.2%. Besides, as a simplified version of our proposed Mid Fusion mechanism, the Early Fusion mechanism enables an implicit fusion process through jointly modeling the goal and observation images. In Table 3, this simple but ingenious design brings a further improvement (4.3% in SPL) compared to the Mid Fusion mechanism which is well-designed and ablated. We attribute this to its adaptive and learnable fusion fashion.

   Mapping Method & SPL & SR \\  N/A & 11.2\% & 13.0\% \\ Semantic Mapping & 24.0\% & 32.0\% \\ FG/HR Mapping & **50.4\%** & **77.3\%** \\   

Table 4: **How to map activation into affine factors?** Using Fine-grained High-resolution (FG/HR) mapping performs significantly better.

   Setting & SPL & SR \\  Later Fusion (baseline) & 11.2\% & 13.0\% \\  Skip Fusion via keypoint matching (FGPrompt-SF) & 24.7\% & 41.6\% \\ Mid Fusion via FiLM layers (FGPrompt-MF) & 50.4\% & 77.3\% \\ Early Fusion via joint modeling (FGPrompt-EF) & **54.7\%** & **78.9\%** \\   

Table 3: **Comparison of different goal prompting methods on Gibson ImageNav task**. Fusing the fine-grained goal prompts with the observation instead of directly concatenating their semantic embeddings yield significant improvement.

[MISSING_PAGE_FAIL:8]

distribution of these parameters in the instance imagenav paper , sampling goal camera height \(h(0.8m,1.5m)\), pitch delta from \((-5^{},5^{})\), and HFOV from \((60^{},120^{})\). In Table 8, we find that the mid-fusion mechanism performs the best in this scenario.

We further conduct experiments on the instance image navigation (iin) dataset collected by . The episodes in this dataset cover a wide range of object instances in the environment and are much harder to finish. We train three agents on the HM3D ImageNetv dataset and evaluate them on the test split of the in dataset. In Table 9, the baseline model performs poorly in this task with a very low success rate (less than 1%). The agents with our proposed fusion mechanisms both perform better. We also observed that the mid-fusion variant outperforms early fusion in this scenario, as its delicately designed activation deformation module yields explicit and adaptive guidance from the goal image. All these results reveal the robustness of mid-fusion in harder tasks.

From Table 9, the performance of our methods on the instance imagenav task is relatively low compared to the ImageNetv task. We speculate that the extremely different perspective of goal images that haven't been seen during training and a longer episode length undermine the performance of our method. This result hints that our method could make a further improvement when combined with memory-based methods [25; 22] to achieve more efficient large-scope exploration.

   Method & Success\(\) & FixedStrict\(\) & E\(\) \\  ResNet18+IL Basline  & 1.89 & 4.92 & 1.32 \\ Ours & **10.2** (+439\%) & **24.9** (+406\%) & **0.81** \\   

Table 10: **Results on AI2THOR 1-Phase Rearrangement Challenge. We apply our proposed FGPrompt method on an imitation learning baseline with a ResNet18 backbone. Surprisingly, we found that our inserted module significantly improved the agent’s performance on the 1-Phase track of the visual rearrangement task.**

Figure 3: **EigenCAM visualization of the activation map in the fusion layer of FGPrompt-MF. Images in different rows illustrate results in different testing episodes in Gibson. The Mid Fusion mechanism learns to focus on the objects that are relevant to the goal image.**

From the experimental results above, we observed a trade-off between two different fusion schemes. The early-fusion scheme is somehow an interesting finding in that it performs competitively and has a simpler architecture. However, though performs well on the default setting, it does not generalize well to other scenarios where the goal camera parameters don't match with the agent's one. In contrast, our delicately designed mid-fusion mechanism performs better in this case. These results indicate that a carefully designed mid-fusion scheme with more inductive bias is necessary.

Transfer to the visual rearrangement task.To see whether our FGPrompt have wide application scenarios, we extend our method to visual rearrangement, another embodied challenge, which aims to move the objects to a correct position in the environment according to unshuffled images. We conduct experiments on the 1-Phase track of the ai2thor-rearrangement challenge and find our method useful in this task. We start from a ResNet18+IL baseline that separately encodes the unshuffled image and agent's current observation without a fusion mechanism and learn from expert actions. Then we introduce our proposed FGPrompt-EF module into the baseline model by fusing the observation with the unshuffled image in an early stage, resulting in one jointly modeled ResNet encoder. We train and test both methods on 2023 dataset and follow  to report the testing metrics of the best checkpoints in Table 10. Our proposed module brings 400% relative improvement compared to the baseline. We believe it helps the agent to locate correspondent or inconsistent objects in the environment.

How does the fine-grained goal prompting work?We visualize the activation maps using Eigen-CAM  before and after the fusion layers of our mid fusion goal prompting method (FGPrompt-MF) to find out how it works in the image navigation task. Illustrations are presented in Figure 3. Prompted with the fine-grained and high-resolution activation map from the goal image, the agent is able to find out the relevant objects in the current observation and pay more attention to them, as shown in the activation visualization in the last column. Interestingly, we found that even though the agent is far away from the goal position, the mid fusion mechanism still guided the observation encoder to focus on relevant objects or explore some candidate regions that may contain the target objects (see the _kitchen bar_ in the last row). We also provide visualization and analysis of the other two goal prompting methods in Appendix.

Performance versus model size.To discuss the feasibility of application on real-world robot systems with resource-limited devices (_e.g._, household robots), we investigate and compare the model size of our models with previous ones. We report the agent's number of parameters, as well as the ImageNet success rate on Gibson, and visualize them on the same coordinate system. As shown in Figure 0(b), our FGPrompt-EF model outperforms existing methods by a large margin with a much smaller model size, indicating its promising ability on applying to real-world robot systems.

## 5 Discussion

Limitation and future workAlthough our proposed FGPrompt achieved great improvements on different ImageNet datasets, we still need a comprehensive study to find out if this method is applicable to real-world robots. In the future, we will investigate how to deploy this visual navigation methodology to a real-world robot system, to perform sim-to-real transformation.

ConclusionIn this paper, we propose a novel fine-grained goal prompting (FGPrompt) method for visual navigation. In particular, we design a Mid Fusion architecture via FiLM Layers conditioning (FGPrompt-MF), which leverages the high-resolution activation maps from the goal encoder to perform an affine transformation on the observation encoder. Furthermore, we rethink it and condense it into an Early Fusion mechanism via joint modeling (FGPrompt-EF), with implicit learning of the fusion process. Experimental results on the Image-goal Navigation task show our method has excellent performance, concise architecture design, and strong generalization ability to unseen environments.

#### Acknowledgments

This work was partially supported by the National Natural Science Foundation of China (Grant No. 62072190 & 62376099 & 62072186), the Guangdong Basic and Applied Basic Research Foundation (Grant No. 2019B1515130001), the Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, the CCF-Tencent Open Fund RAGR20220108.