# HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors

Panwang Pan\({}^{*}\)\({}^{1}\), Zhuo Su\({}^{*}\)\({}^{}\)\({}^{1}\), Chenguo Lin\({}^{*}\)\({}^{1,2}\), Zhen Fan\({}^{1}\), Yongjie Zhang\({}^{1}\), Zeming Li\({}^{1}\), Tingting Shen\({}^{3}\), Yadong Mu\({}^{2}\), Yebin Liu\({}^{}\)\({}^{4}\)

\({}^{*}\) Equal contribution \(\) Project lead \(\) Corresponding author

\({}^{1}\)ByteDance, \({}^{2}\)Peking University, \({}^{3}\)Xiamen University, \({}^{4}\)Tsinghua University

###### Abstract

Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we present **HumanSplat**, which predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner. Specifically, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction Transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is devised to achieve high-fidelity texture modeling and impose stronger constraints on the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis. Project page: [https://humansplat.github.io](https://humansplat.github.io).

## 1 Introduction

Realistic 3D human reconstruction is a fundamental task in computer vision with widespread applications in numerous fields, including social media, gaming, e-commerce, telepresence, etc. Previous works for single-image human reconstruction can be broadly categorized into explicit and implicit approaches. Explicit methods , such as those based on parametric body models like SMPL , estimate human meshes by directly optimizing parameters and clothing offset to fit the observed image. However, these methods often struggle with complex clothing styles and require lengthy optimization. Implicit methods  represent humans using continuous functions, such as occupancy , SDF , and NeRF . While they excel at modeling flexible topology, these methods are limited in scalability and efficiency due to the high computational cost associated with training and inference. Recent advances in 3D Gaussian Splatting (3DGS)  have provided a balance between efficiency and rendering quality for reconstructing detailed 3D human models, which however rely on multi-view image  or monocular video  input. Recent popular human reconstruction studies  focus on the score distillation sampling (SDS) technique  to lift 2D diffusion priors to 3D, but time-consuming optimization (e.g., 2 hours) is required for each instance. Some generalizable and large-reconstruction-model-based works  can directly generalize the regression of 3D representations but either disregard human priors or require multi-view inputs, limiting the stability and feasibility in downstream applications.

In this paper, we propose a novel generalizable approach **HumanSplat** for single-image human reconstruction by introducing a generalizable Gaussian Splatting framework with a fine-tuned 2D multi-view diffusion model and well-conceived 3D human structure priors. Different from existing human 3DGS methods, our approach directly infers Gaussian properties from a single input image,eliminating the requirement for per-instance optimization or densely captured images. This empowers our method to generalize effectively in diverse scenarios while delivering high-quality reconstructions.

The key insight behind our method is to reconstruct Gaussian properties from the diffusion latent space in a generalizable end-to-end architecture, integrating a 2D generative diffusion model as appearance prior and a human parametric model as structure prior. Specifically, facing this under-constraint reconstruction problem from single-view input with extensive invisible regions, we first leverage a 2D multi-view diffusion model (denoted as **novel-view synthesizer**) to hallucinate the unseen parts of clothed humans. Then a generalizable **latent reconstruction Transformer** is proposed to enable interaction among the generated diffusion latent embeddings and the geometric information of the structured human model, enhancing the quality of 3DGS reconstruction. During interactions, to mitigate the limitations of inaccurate human priors like the SMPL model, we devise a projection strategy to strike a balance between robustness and flexibility, projecting 3D tokens into the 2D space and conducting searches within adjacent windows using projection-aware attention. Another specific challenge of human reconstruction is to capture fine detail in visually sensitive areas, such as the face and hand. To address this problem, we exploit the semantic cues from the structure priors and propose **semantics-guided objectives** to further promote the fine details reconstruction quality. By synergistically amalgamating these crucial components into a unified framework, our method achieves state-of-the-art performance in striking the right balance between quality and efficiency.

In summary, the main contributions of our work are as follows:

* We propose a novel generalizable human Gaussian Splatting network for high-fidelity human reconstruction from a single image. To the best of our knowledge, it is the first to leverage latent Gaussian reconstruction with a 2D generative diffusion model in an end-to-end framework for efficient and accurate single-image human reconstruction.
* We integrate structure and appearance cues within a universal Transformer framework by leveraging human geometry priors from the SMPL model and human appearance priors from the 2D generative diffusion model. Geometric priors stabilize the generation of high-quality human geometry, while appearance prior helps hallucinate unseen parts of clothed humans.
* We enhance the fidelity of reconstructed human models by introducing semantic cues, hierarchical supervision, and tailored loss functions. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing methods.

## 2 Related Work

**Single-Image Human Reconstruction.** Single-image human reconstruction methods fall into explicit and implicit approaches. Explicit methods use parametric body models [5; 6] to estimate

Figure 1: Our method achieves state-of-the-art rendering quality while maintaining the fastest runtime. (a) Qualitative results: LGM  and GTA  are generalizable but in lower quality, TeCH  exhibits issues with multi-face rendering and is time-consuming. In contrast, our method achieves higher fidelity in a much shorter time. (b) Performance and runtime comparison: metrics are evaluated on the challenging Twindom dataset.

minimally clothed human meshes , adding clothing via 3D offsets  or garment templates . However, these methods are constrained by topology, particularly with loose clothing. Implicit methods , utilizing representations like occupancy, signed distance fields (SDF) and Neural Radiance Fields (NeRF), offer flexibility with topology, allowing for accurate depiction of 3D clothed humans. Recent methods  combine implicit representation with explicit human priors for better robustness. Among these, GTA  uses Transformers with fixed embeddings for translating image features into 3D tri-plane features. TeCH  employs diffusion-based models for visualizing unseen areas but needs extensive optimization and accurate SMPL-X models. Another approach, HumanSGD , uses diffusion models for texture inpainting but struggles with mesh inaccuracies from . NeRF-based methods like SHERF  generate human NeRF from single images using hierarchical features, while ELICIT  leverages CLIP  for contextual understanding. These NeRF-based methods produce high-quality images but often struggle with detailed 3D mesh generation and require extensive optimization time.

**Human Gaussian Splatting.** While implicit representations like SDF or NeRF struggle with balance optimization efficiency and rendering quality, the high efficiency of 3DGS  has advanced 3D human creation. Recent 3DGS methods have targeted multi-view videos, monocular video sequences, and sparse-view input. Multi-view video methods like D3GA , HuGS , and 3DGS-Avatar  exploit dense spatial and temporal information for detailed models. Additionally, HiFi4G  combine 3DGS with a dual-graph mechanism to preserve spatial-temporal consistency, ASH  utilizes mesh UV parameterization for real-time rendering, and Animatable Gaussians  improve garment dynamics via pose projection mechanism and 2D CNNs. The monocular video only provides sufficient observation as humans move, so the monocular human Gaussian Splatting methods  often resort to using LBS for mapping to a canonical space and introducing additional regularization terms. For sparse-view images, GPS-Gaussian  achieves high rendering speeds without per-subject optimization by using a feed-forward way with efficient 2D CNNs encoding from diverse 3D human scan data. In comparison, our work stands out as the first one-shot generalizable human GS method, using only single image input and achieving high-quality reconstruction without any optimization and fine-tuning.

**Generalizable Large Reconstruction Model.** Large reconstruction model (LRM)  presents that with sufficient parameters and training datasets, a deterministic feed-forward Transformer  is capable of learning a triplane feature for volumetric rendering from a single-view image. Targeting general 3D object, TriplaneGaussian  combines 3DGS with triplane as a hybrid 3D representation for fast rendering, where point clouds are first extracted from an image and then projected on triplane features to decode 3DGS attributes. Most recent methods further leverage 2D diffusion models that can generate multi-view images simultaneously  to provide plausible and abundant inputs for the following reconstruction, so they focus on the sparse multi-view reconstruction task. Instant3D  follows the technique of LRM and produces triplane features from 4 posed images by a Transformer encoder. CRM , MeshLRM  and InstantMesh  replaces triplane NeRF by FlexiCubes  to extract meshes directly. LGM , GRM  and GS-LRM  predict 3DGS attributes from each input pixel  and combine the Gaussians from multiple viewpoints as the final 3D output. For human task, CharacterGen  follows Instant3D  to adopt a two-stage approach: first, generating a 2D multi-view canonical images, then using multi-view SDF reconstruction for cartoon digital characters, without direct interaction between stages. A more compact and elegant method HumanLRM  replaces training data of LRM  from general objects with human data to predict triplane NeRF, showcasing the capacities of the large model. While it relies heavily on the dataset, often leading to issues like missing limbs due to a lack of prior knowledge. In contrast, our 3DGS-based method leverages the latent diffusion and reconstruction model in an end-to-end framework and introduces human priors by addressing their inaccuracy with specific projection strategies, achieving better generalization and stability with even less data.

## 3 Method

### Preliminary

**SMPL** is a widely-used parametric human body model that is created by skinning and blend shapes, and learned from thousands of 3D body scans. The SMPL model  utilizes shape parameters\(^{10}\) and pose parameters \(^{24 3}\) to parameterize the deformation of the human body mesh \((,): ^{6880 3}\).

**3D Gaussians Splatting** is a recently proposed 3D representation that formulates 3D content by a set of \(}\) colored Gaussians \(=\{_{i}\}_{i=1}^{_{p}}\). Each Gaussian \(_{i}=(-(-_{i})^{}_{i}^{-1}(-_{i}))\) has opacity \(_{i}\) and color \(_{i}^{3}\) attributes, where \(_{i}^{3}\) is its location and \(_{i}^{3 3}\) implies its scaling \(_{}^{3}\) and orientation quaternion \(_{}^{4}\) in 3D space. These attributes constitute \(^{} 14}\) can represent a 3D object, and by projecting to the image plane with opacity-based color composition in the depth order, it can be differentiably rendered into 2D images in real-time.

### Overview

Given a single input image of a human body \(}^{H W 3}\), our task is to reconstruct a 3D representation, i.e., 3DGS in this work, from which novel-view images can be rendered. As illustrated in Fig. 2, our method leverages a 2D diffusion model and a novel latent reconstruction Transformer that adeptly integrates 2D appearance priors, human geometric priors, and semantic cues within a universal framework. First, we use SMPL estimator [91; 92] to predict human structure priors \(\), and CLIP  to generate the image embedding of the input image \(\). A latent temporal-spatial diffusion model , referred to as the **novel-view synthesizer** (Sec. 3.3), is fine-tuned and produces multi-view latent features \(\{_{i}^{h w c}\}_{i=1}^{}\), where \(\) is the number of views, and \(h\), \(w\) and \(c\) are the height, width and channel of the latent features. Then, we utilize a novel **latent reconstruction Transformer** (Sec. 3.4) that adeptly integrates human geometric prior and latent features within a universal framework, which predicts Gaussian attributes \(\{(_{i},_{i},_{i},_{i},_{i})|i=1,...,}\}\) and rendered into new images, where \(}\) represents the number of Gaussian points. Furthermore, to achieve high-fidelity texture modeling and better constrain the estimated multiple views, we designed a hierarchical loss that incorporates human semantic priors (See 3.5). Note that during training, the network is trained using 3D scan data to ensure accurate supervision from various viewpoints, where the registered SMPL is fitted via the multi-view version of SMPLify . During inference, the model employs PIXIE  to predict human structure priors \(\) and synthesize novel views from only a single image input based on the trained model, without any fine-tuning and optimization.

### Video Diffusion Model as Novel-view Synthesizer

We take advantage of the pre-trained video diffusion model SV3D  as appearance prior. For the input image \(}\), we leverage a CLIP image encoder  to obtain the image embedding \(\) and a pre-trained VAE \(\) to acquire latent feature \(_{0}\) as conditions. Subsequently, we progressively denoises gaussian noises into temporal-continuous \(\) multi-view latent features \(\{_{i}\}_{i=1}^{}\) by a spatial-temporal UNet \(D_{}\) with the objective:

\[_{ p()}[()\|D_{}(\{ _{i}^{}\}_{i=1}^{};\{e_{i},a_{i}\}_{i=1}^{},,_{0},)-\{_{i}\}_{i=1}^{}\|_{2}^{2} ], \]

Figure 2: Overview of HumanSplat. (a) Multi-view latent features are first generated by a fine-tuned multi-view diffusion model (Novel View Synthesizer in Sec. 3.3). (b) Then, the Latent Reconstruction Transformer (Sec. 3.4) interacts global latent features (Sec. 3.4.1) and human geometric prior (Sec. 3.4.2). (c) Finally, the semantic-guided objectives (Sec. 3.5) are proposed to reconstruct the final human 3DGS.

where \(\{_{i}^{}\}_{i=1}^{}\) is the noise-corrupted multi-view latent features, \(\{e_{i},a_{i}\}_{i=1}^{}\) is the corresponding relative elevation and azimuth angles to \(_{0}\), \(p()\) is the noise probability distribution and \(()^{+}\) is the loss weighting term related to the noise level \(\). Since SV3D is merely pre-trained on the dataset of objects , we further fine-tune it using human datasets to improve its effectiveness in the human reconstruction task. More technical details are available in Appendix A.1.

### Latent Reconstruction Transformer

Latent reconstruction Transformer contains two parts: **latent embedding interaction** (Sec. 3.4.1) and **geometry-aware interaction** (Sec. 3.4.2), which seamlessly incorporate latent features from novel-view synthesizer and human structure priors into the reconstruction process. The two components together form a cohesive framework for accurately recovering intricate human details.

#### 3.4.1 Latent Embedding Interaction

We utilize a pretrained VAE  encoder \(\) to encode the input image \(_{0}\) into a latent representation \(_{0}=(_{0})^{h w c}\) and combine it with the generated \(\{_{i}\}_{i=1}^{}\) (Sec. 3.3). Following previous works , latent representations are concatenated with their corresponding Plucker embeddings  along the channel dimension, resulting in a dense pose-conditioned feature map. These latents are then divided into non-overlapping patches  and mapped to \(d\)-dimensional latent tokens by a linear layer. An intra-attention module is followed to thoroughly extract spatial correlations of latent tokens, as shown in Fig. 3, by a standard Transformer  with \(N_{}\) blocks of multi-head self-attention and feed-forward network (FFN):

\[}_{i}=[((_{i}))]_{  N_{}}. \]

#### 3.4.2 Geometry-aware Interaction

**Human Geometric Tokenizer.** Given a human geometric model \(^{6890 3}\), to obtain the feature representation, we project \(\) onto the input view and compute the feature vector through bilinear interpolation on the feature grids by

\[_{0}()=(_{0}+ _{0}), \]

where \(_{0}\), \(_{0}\) are camera extrinsic parameters of the input view, \(\) is the intrinsic parameters. After the projection operation, we obtain geometric human prior tokens by concatenating \(\) and \(_{0}[_{0}()]\), and mapping them to \(d\) dimension. \(_{0}[_{0}()]\) stands for querying the input latent feature \(_{0}\) from projected locations. Similar to Latent Embedding Interaction (Sec. 3.4.1), an intra-attention module is utilized to interact among these human prior tokens and get human geometric features \(}^{6890 d}\).

**Human Geometry-aware Attention.** Empirically, we find that incorporating human priors  (e.g., SMPL model) into human reconstruction faces a significant dilemma of the trade-off between robustness and flexibility . On the one hand, the SMPL model provides geometric priors that alleviate common issues (e.g., broken body parts and various artifacts). On the other hand, it struggles to accurately capture and represent a broad spectrum of complex clothing styles, particularly more fluid garments like dresses and skirts. This limitation highlights a fundamental shortcoming in the model's capacity to accurately depict diverse apparel. As shown in Fig. 3, HumanSplat addresses inaccuracies in human priors by projecting 3D tokens into 2D space and conducting local searches within adjacent windows, efficiently using priors while minimizing redundancy. Specifically, we introduce projection-aware attention within the inter-attention module, utilizing a window \((K_{} K_{})\) and employing masked multi-head cross-attention. Here, the latent features \(\{}_{i}\}_{i=1}^{}\) serve as queries, and the human prior tokens \(}\) serve as keys and values. The process can be formulated as follows:

\[\{}_{i}\}_{i=1}^{}=[( _{}(\{}_{i}\}_{i=1}^{},}))] _{ N_{}}, \]

where feature interactions occur when \((},,_{i},_{i})\) is within the window of \(}_{i}\). It is noteworthy that the complexity of the cross-attention operation is reduced from \((_{F}_{H})\) to \((_{F} K_{}^{2})\), compared to the vanilla multi-head cross-attention setting, where \(_{F}\) and \(_{H}\) represent the lengths of the latent and human geometric tokens respectively.

### Semantics-guided Objectives

From each output token \(}_{i}\), we decode the attributes of pixel-aligned Gaussians \(\) in the corresponding patch by a convolutional layer with a \(1 1\) kernel. These attributes are used to render novel-view images through 3D Gaussian Splatting rasterization. An ideal training objective should ensure that the rendered outputs closely match the supervised images, striving for overall consistency between the reconstructed renderings and the ground truth. However, human attention tends to be particularly focused on facial regions, where intricate details play a pivotal role in perception. Therefore, to enhance the fidelity of reconstructed human models, we optimize our objective functions to preferentially focus on the facial regions.

**Hierarchical Loss.** Traditional approaches often overlook the semantic richness inherent in human anatomy, leading to reconstructions that lack crucial details and accuracy. To address this, we propose a novel framework that harnesses semantic cues, hierarchical supervision, and tailored loss functions to guide the training process. This ensures that not only are the overall structures of the human body accurately represented, but also that critical areas such as the face are reconstructed with exceptional detail and fidelity. Specifically, by incorporating a human prior model equipped with semantic information, which adheres to the widely-used definition of 24 human body parts, such as "right hand", "left hand", "head", etc. The ground-truth body part segmentation is obtained from SMPL meshes with predefined semantic vertices . Therefore, we can utilize different attention weights for different body parts and render different levels of real images from different viewpoints to provide hierarchical supervision. This approach significantly facilitates the precise localization of essential body parts, including the head, hands and arms. Please refer to Appendix A.2 for more detailed procedural insights. The overall loss is defined as a weighted sum of part-specific losses:

\[_{H}=_{i=1}^{n}_{j=1}^{m}_{i} _{j}(_{}(_{i,j},}_{i,j}) +_{p}_{}(_{i,j},}_{i,j})), \]

where \(i\{1,...,n\}\) and \(j\{1,...,m\}\) denote different resolution levels and human parts. \(}_{i,j}\) and \(_{i,j}\) are the predicted and ground-truth data for part \(j\), \(_{i}\) and \(_{j}\) are weighting factors that reflect the relative importance of each resolution level and each body part, respectively. \(_{}\) measures the MSE loss and \(_{p}\) measures the perceptual loss .

**Reconstruction Loss.** Reconstruction loss \(_{}\) based on the \(_{}\) rendered multi-view images is defined as

\[_{}=_{i=1}^{_{}}_{ }(_{i},}_{i})+_{m}_{ }(,}_{i})+_{p}_{ }(_{i},}_{i}), \]

Figure 3: Illustration of latent reconstruction Transformer. It first divides \(_{0}\) and \(_{i}\) into non-overlapping patches, which are then processed through an intra-attention module (Sec. 3.4.1). Within the iter-attention module (Sec. 3.4.2), we introduce the projection-aware attention with a window \((K_{} K_{})\), and the attributes of 3D Gaussians are decoded with a Conv \(1 1\) layer.

where \(_{i}\) and \(}_{i}\) are the ground-truth images and render images via Gaussian Splatting, \(_{i}\) and \(}_{i}\) are original and rendered foreground mask. \(_{m}\) and \(_{}\) are hyperparameters for balancing three kinds of losses. The final training objective is a combination of the two loss terms: \(=_{H}+_{}\).

## 4 Experiments

### Implementation Details

**Training Details.** HumanSplat is trained on 500 THuman2.0 , 1500 2K2K , and 1500 Twindom  high-fidelity human scans. We evenly position 36 cameras across each of three hierarchical cycles to capture the full body, half body, and face, with rendering resolution set to \(512 512\). We conduct 200 epochs of \(256\)-res training with a learning rate of 1e-5 and a batch size of 32 over 2 days on 8 A100 (40G VRAM) GPUs, while \(512\)-res finetuning costs 2 additional days. We train our model with AdamW  optimizer, whose \(_{1}\), \(_{2}\) are set to 0.9 and 0.95 respectively. A weight decay of 0.05 is used on all parameters except those of the LayerNorm layers. We use a cosine learning rate decay scheduler with a 2000-step linear warm-up and the peak learning rate is set to 4e-4. For parts related to the head, hands, and arms, \(_{j}\) are set to 2, while the rest human part are set to 1. The parameters \(_{i}\), \(_{}\) and \(_{m}\) are set to 1. The model is trained for 80K iterations on \(256\)-res and then fine-tuned on \(512\)-res for another 20K iterations. To enable efficient training and inference, we adopt Flash-Attention-v2  in the xFormers  library, gradient checkpointing , and FP16 mixed-precision . Fine-tuning the novel view synthesizer takes about 18 hours. Please refer to Appendix A.1 for more details about novel view synthesizer fine-tuning.

**Inference Time.** The video diffusion model takes about 9s to generate multi-view latent features while the subsequent latent reconstruction for 3DGS takes only about 0.3s. Additionally, it can render novel views at a rate exceeding 150 FPS on a NVIDIA A100 GPU.

### Comparison

**Quantitative Comparison.** We conduct a quantitative comparison on 21 THuman2.0  and 51 Twindom  scans, rendering textured meshes from multiple angles and using PSNR, SSIM , and VGG-LPIPS  as evaluation metrics. To ensure fairness, we fine-tune LGM  on our datasets and fully optimize each TeCH  instance for comparison. As shown in Tab. 1, HumanSplat consistently outperforms previous generalizable methods across all datasets. Specifically, HumanSplat surpasses SIFU  by +1.92 (8.72\(\%\)) in PSNR and reduces LPIPS from 0.079 to 0.055. Notably, HumanSplat also excels on the more challenging Twindom dataset, outperforming TeCH  by +2.15 (10.16\(\%\)) in PSNR and reducing LPIPS from 0.188 to 0.125. Although the values can further decrease with model training, we control the convergence to the current extent in the dataset for better generalization under more general in-the-wild scenarios. Besides, we also report the reconstruction times for \(512 512\) input images on a NVIDIA A100 GPU. HumanSplat achieves a remarkably fast reconstruction time, approximately 9.3 seconds on a single NVIDIA A100 GPU, making it much more practical than TeCH's 4.5-hour reconstruction time.

**Qualitative Comparison.** As illustrated in Fig. 4 and Fig. 5, by incorporating semantics-guided objectives, Humansplat achieves more detailed and higher fidelity results against GTA  and

    &  &  &  &  \\  & Diffusion &  &  & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  PIFu  & ✗ & ✗ & 18.093 & 0.911 & 0.137 & - & - & - & 30.0s \\ LGM\({}^{*}\) & ✓ & ✗ & 20.013 & 0.893 & 0.116 & 19.840 & 0.851 & 0.292 & 9.5s \\ GTA  & ✗ & ✓ & 18.050 & - & - & 17.669 & 0.741 & 0.418 & 43s \\ SIFU  & ✗ & ✓ & 22.025 & 0.921 & 0.084 & 19.714 & 0.832 & 0.312 & 44s \\ SIFU\({}^{}\) & ✓ & ✓ & 22.102 & 0.923 & 0.079 & - & - & - & 6min \\  Magic123\({}^{}\) & ✓ & ✗ & 14.501 & 0.874 & 0.145 & - & - & - & 1h \\ HumanSplat\({}^{}\) & ✓ & ✓ & 17.365 & 0.895 & 0.130 & - & - & - & 7min \\ TeCH\({}^{}\) & ✓ & ✓ & **25.211** & **0.936** & 0.083 & 21.192 & 0.884 & 0.188 & 4.5h \\ }\) is set to 1 (dubbed "Projection Only"), employing human priors significantly enhances performance, improving the PSNR metric from 22.635 to 23.333. We further explore setting window size to \(K_{}=2\) in a latent space with a resolution of \(64 64\), which further improves the model's PSNR metric from 23.333 to 24.294. This improvement is attributed to Humansplat with \(K_{}=2\) is capable of handling loose clothing and tolerating imprecise SMPL parameters. In Tab. 3, we present results for the 2K2K dataset across various window sizes. Further refinement with a window size of \(K_{}=3\) continues to enhance the results, whereas using \(K_{}=4\) does not yield additional improvements.

**Semantics-guided Training Objectives.** Thanks to the incorporation of semantic-guided hierarchical losses, HumanSplat can generate higher-quality textures for both the face and hand, while also promoting the convergence of the training process, as depicted in Fig. 7 (a) and (b). Furthermore, Fig. 7 (c) demonstrates that HumanSplat further produces 3D coherent segmentation results, which have potential applications in editing and specific generation of individual parts of 3D humans.

Figure 5: Qualitative results showcasing reconstructions of humans in challenging poses, diverse identities, and varying camera viewpoints from in-the-wild images.

Figure 6: Qualitative Comparison of ours against HumanSGD  on in-the-wild images.

## 5 Conclusion

We present HumanSplat, a pioneering generalizable human reconstruction network that derives 3D Gaussian Splitting properties from a single image. This model integrates generative diffusion and latent reconstruction Transformer models with human structure priors, enhanced by a tailored semantic-aware hierarchical loss. These innovations achieve high-fidelity reconstruction results in a feed-forward manner without any optimization or fine-tuning, especially in the important focal areas such as the face and hands. Extensive experiments demonstrate that HumanSplat surpasses existing state-of-the-art methods in both quality and efficiency, including robust performance in handling challenging poses and loose clothing. This capability opens up a broad spectrum of potential applications.

**Limitations and Future Works.** Our method has some limitations that can be addressed in future works: (a). While effective with most attire, the model may struggle with intricate garments and accessories. Future enhancements should incorporate 2D data and innovative training strategies, which is a promising direction for improving the handling of diverse and unusual clothing styles. (b). Although already efficient, increasing the computational speed by constraining Gaussian's number or combining compact 3DGS with texture representation could further facilitate real-time applications, particularly on mobile devices. (c). Animating the reconstructed human models currently requires post-processing. Future work could introduce canonical space to reconstruct animatable avatars directly, streamlining the process and improving efficiency.

Table 2: Ablation study of several proposed designs on 2K2K  evaluation dataset.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  w/o Human Prior & 22.635 (-1.658) & 0.893 (-0.022) & 0.182 (+0.142) \\ \(K_{}=1\) & 23.333 (-0.960) & 0.916 (-0.001) & 0.057 (+0.017) \\ \(K_{}=3\) & 24.383 (+0.089) & 0.931 (+0.016) & 0.041 (+0.001) \\ \(K_{}=4\) & 24.013 (-0.280) & 0.922 (+0.007) & 0.048 (+0.008) \\  \(K_{}=2\) (**Ours**) & 24.293 & 0.915 & 0.040 \\   

Table 3: Evaluation of different window sizes \(K_{}\) on the 2K2K  evaluation dataset.

Figure 7: (a) Qualitative evaluation of human prior and hierarchical loss \(}\). PSNR/SSIM values are shown at the top of each image, presenting the effectiveness of the two strategies in the full pipeline. (b) LPIPS scores during training on the 2K2K validation set. (c) HumanSplat can provide 3D coherent segmentation results for potential applications.