# Regression with Cost-based Rejection

Xin Cheng\({}^{1}\) Yuzhou Cao\({}^{2}\) Haobo Wang\({}^{3}\) Hongxin Wei\({}^{4}\) Bo An\({}^{2}\) Lei Feng\({}^{2}\)

\({}^{1}\)College of Computer Science, Chongqing University, China

\({}^{2}\)School of Computer Science and Engineering, Nanyang Technological University, Singapore

\({}^{3}\)School of Software Technology, Zhejiang University, China

\({}^{4}\)Department of Statistics and Data Science, Southern University of Science and Technology, China

xincheng9215@gmail.com, yuzhou002@e.ntu.edu.sg, wanghaobo@zju.edu.cn

weihx@sustech.edu.cn, boan@ntu.edu.sg, lfengqaq@gmail.com

Corresponding author: Lei Feng.

###### Abstract

Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.

## 1 Introduction

In machine learning, the learned model from training data is expected to make predictions on unknown test data as accurately as possible. However, it would be unreasonable for the learned model to make predictions on all the test instances, as there may exist some difficult instances that the learned model cannot give an accurate prediction. Incorrect predictions can cause severe consequences and even can be life-threatening, especially in risk-sensitive applications [5; 19; 36; 11], such as healthcare management, autonomous driving, and product inspection. Therefore, the _learning with rejection_ (LwR) framework [10; 11; 36; 27; 31; 17; 8; 6; 38] was extensively investigated, which aims to provide a reject option to not make a prediction in order to prevent critical false predictions. In this case, the LwR model can be learned by balancing the rejection and the prediction.

So far, most of the existing studies on LwR have focused on the classification setting, i.e., _classification with rejection_[10; 3; 44; 45; 14; 19; 37; 17; 8; 6]. A well-known framework for classification with rejection that has been studied extensively is called the cost-based framework, i.e., _classification with cost-based rejection_[11; 12; 18; 39; 16; 36; 8; 6]. In the classification with cost-based rejection setting, there is a pre-determined rejection cost \(c\) for each instance, which must be smaller than the classification error \(1\). A typical approach for classification with cost-based rejection is the _confidence-based approach_[22; 3; 45; 39; 40; 8]. The main idea is to use the real-valued output of the classifier as the confidence score and decide whether to reject the prediction based on theconfidence score and the given rejection cost \(c\). Another effective approach is _classifier-rejector approach_[11; 12; 36], which simultaneously trains a classifier and a rejector, and this approach achieves state-of-the-art performance in binary classification .

Despite many previous studies on LwR, they only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In many real-world scenarios, regression tasks with continuous real-valued targets can be commonly encountered. However, even state-of-the-art regression models may make incorrect predictions, and blindly trusting the model results may lead to critical consequences, especially in risk-sensitive applications [7; 4; 28; 23; 48]. Therefore, it is necessary to consider adding a rejection option for the regression problem to not make predictions in order to avoid critical mispredictions. To this end, many studies have been conducted on _regression with rejection_. A widely studied framework in regression with rejection is called _selective regression_[26; 44; 20; 46; 27; 41] that trains a regression model with a reject option given a fixed reject rate of predictions. However, this selective regression setting fails to consider the cost-based rejection scenario where a certain cost could be incurred if the model chooses to refrain from making a prediction for a certain instance.

In this paper, we provide the first attempt to investigate a novel regression setting called _regression with cost-based rejection_ (RcR), where the model could reject to make predictions on some instances at certain costs to avoid critical mispredictions. To solve the RcR problem, we first formulate the expected risk and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the popular mean squared error is used as the regression loss. However, it is difficult to directly optimize the expected risk to derive the optimal solution, since the variance of the instances cannot be easily accessed. Therefore, we propose a surrogate loss function to train the model that considers the rejection behavior as a binary classification and we provide theoretical analyses to show that the Bayes optimal solution can be recovered by minimizing our surrogate loss under mild conditions. Our main contributions can be summarized as follows:

* We propose a surrogate loss function considering rejection as binary classification and we demonstrate the _regressor-consistency_ and the _rejector-calibration_ when the binary classification loss function is classification-calibrated and is always greater than 0.
* We also provide a relaxed condition that allows the classification-calibrated binary classification loss to be non-negative. In the relaxed condition, the regressor-consistency can still be satisfied for correctly accepted instances.
* We derive a regret transfer bound and an estimation error bound for our proposed method, and extensive experiments demonstrate the effectiveness of our method.

## 2 Preliminaries

In this section, we introduce preliminary knowledge of ordinary regression, classification with rejection, and selective regression.

### Ordinary Regression

For the ordinary regression problem, let the feature space be \(^{d}\) and the label space be \(\). Let us denote by \((,y)\) an example including an instance \(x\) and a real-valued label \(y\). Each example \((,y)\) is assumed to be independently sampled from an unknown data distribution with probability density \(p(,y)\). For the regression task, we aim to learn a regression model \(h:\) that minimizes the following expected risk:

\[R(L)=_{p(,y)}[L(h(),y)],\] (1)

where \(_{p(,y)}\) denotes the expectation over the data distribution \(p(,y)\) and \(L:_{+}\) is a conventional loss function (such as mean squared error and mean absolute error) for regression, which measures how well a model estimates a given real-valued label.

### Classification with Rejection

A widely studied framework in classification with rejection is the cost-based framework [10; 36; 8; 6] that aims to train a classifier \(f:^{}}\) that can reject to make a prediction, where denotes the reject option. The evaluation metric of this task is the zero-one-c loss \(_{01c}\) defined as follows:

\[_{01c}(f(),z)= c(),&f()= ,\\ _{01}(f(,z),&,\] (2)

where \(c()\) is the rejection cost associated with \(\). Then, the expected risk with \(_{01c}\) can be represented as follows:

\[R_{01c}(f)=_{p(,y)}[_{01c}(f(),y)],\] (3)

The optimal solution for classification with rejection \(f^{}=*{argmin}_{f}R_{01c}(f)\) given by Chow's rule  can be expressed as follows:

**Definition 1**.: _(Chow's Rule ) A classifier \(f:^{}\) is the optimal solution of expected risk (3) if and only if the following conditions are almost satisfied:_

\[f()=,&_{z}_{z}() 1-c(),\\ *{argmax}_{z}_{z}(),&,\] (4)

where \(_{z}()=p(z|)\). Chow's rule shows that classification with rejection can be solved when \(()\) is known. However, the estimation of the probability is difficult especially when using deep neural networks .

### Selective Regression

In selective regression, for a given instance \(\), the selective model can choose to make a prediction for it or reject to make a prediction without costs. Formally, the selective model is a pair \((h,r)\) where \(h:\) is a regression prediction model and \(r:\) is a selection model, which serves as the rejection rule as follows,

\[(h,r)()=h(),&r()>0,\\ ,&.\] (5)

Let us denote by \(\) the expected rejection rate and denote by \((r)=_{p(,y)}[r()>0]\) the coverage of selective regression. The purpose of selective regression is to derive a pair \((h,r)\) such that the risk \(R(h,r)=_{p(,y)}[L(h(),y)[r()>0]]\) is minimized with coverage \(1-(r)<\), where \(L(h(),y)\) is a conventional regression loss function. However, the selective regression setting fails to consider the cost-based rejection scenario but fixes the rejection rate \(\). In many real-world scenarios, rejection with costs is more common, and the cost \(c()\) is easier to provide compared with the rejection rate \(\).

## 3 Regression with Cost-based Rejection

Let \(^{d}\) be the \(d\)-dimensional feature space and \(\) be the label space. Suppose the training set is denoted by \(=\{(_{i},y_{i})\}_{i=1}^{n}\), and each training example \((_{i},y_{i})\) is assumed to be sampled from an unknown data distribution with probability density \(p(,y)\). In RcR setting, for a given instance \(\), the learner has the option \(\) to reject making a prediction or to make a regression prediction. If the learner rejects an instance, the cost is a non-negative loss \(c()\). The goal of RcR is to induce a pair \((h,r)\) where \(h:\) is a regressor to predict the accepted instance and \(r:\) is a rejector to determine whether to reject an instance or not. The evaluation metric of this task is the following loss function \((h,r,c,,y)\):

\[(h,r,c,,y)=L(h(),y),&r()>0,\\  c(),&,\] (6)

where \(L(h(),y)\) is a conventional regression loss function (e.g., mean squared error).

In what follows, we will present a Bayes optimal solution to the RcR problem and provide a surrogate loss function to train the regressor-rejector.

### Bayes Optimal Solution

In this paper, we only discuss the case where the loss function \(L(h(),y)\) is the mean squared error (MSE), which is the most widely used regression loss function. The expected risk of \((h,r,c,,y)\) over the data distribution can be represented as follows:

\[R_{}(h,r)=_{p(,y)}[(h,r,c,,y)].\] (7)

Let us denote by \((h^{},r^{})=_{(h,r)}R_{}(h,r)\) the optimal pair of the expected risk \(R_{}\) and we use \(_{p(y|)}[y]=_{}p(y|)yy\) and \(_{p(y|)}[y]=_{}p(y|)(y-_{p(y| )}[y])^{2}y\) denote the expectation and variance of \(y\) over the distribution \(p(y|)\). For a given cost function \(c()\), we have the following theorem:

**Theorem 2**.: _For a given instance \(\), a non-negative cost \(c()\) and the Bayes optimal pair \((h^{},r^{})\) of the risk \(R_{}\), the following equality holds:_

\[h^{}()=_{p(y|)}[y],\\ r^{}()=(c()-_{p(y|)}[y]). \] (8)

The proof of Theorem 2 is provided in Appendix A. It is worth noting that our derived Bayes optimal solution in Theorem 2 can be considered as a generalized version of Proposition 2.1 in Zaoui et al. , with an instance-dependent cost function. Theorem 2 shows the expected optimal pair \((h^{},r^{})\) of risk \(R_{}\) where the rejector \(r^{}\) should reject making a prediction if the variance of the distribution of labels \(y\) associated with \(x\) is so large that it exceeds a given rejection cost \(c()\). This is intuitive and easy to understand. Unfortunately the probability density function \(p(y|)\) is usually unknown, meaning that obtaining the variance \(_{p(y|)}[y]\) and expectation \(_{p(y|)}[y]\) is difficult or even impossible. Many previous studies adopted specific assumptions to avoid directly estimating the variance and the expectation (e.g., homoscedasticity [26; 43; 42] and heteroscedasticity [29; 30; 9; 32]), while all of them have certain constraints. Therefore, the key challenge of RcR is how to learn the optimal solution \((h^{},r^{})\) without the expectation and the variance.

### Surrogate Loss Function of Training Regressor-Rejector

From Theorem 2, we know how the optimal pair \((h^{},r^{})\) makes rejection and prediction for an unknown instance, but since the expectation and the variance are difficult to obtain, we cannot directly derive the optimal regressor and rejector. Let us reconsider the RcR loss function \((h,r,c,,y)\) by the following equation:

\[(h,r,c,,y)=(h()-y)^{2}[r()>0]+c() [r() 0],\] (9)

where \([]\) denotes the indicator function. We cannot directly derive a regressor \(h\) and a rejector \(r\) by the above loss since the loss function contains non-convex and discontinuous parts \([r()>0]\) and \([r() 0]\). In order to efficiently optimize the target loss, using surrogate loss is preferred. It is worth noting that the behavior of the rejector is similar to binary classification due to the only two options, rejection and acceptance. We may consider it directly as a binary classification \(\{+1,-1\}\), where \(+1\) means acceptance and \(-1\) means rejection. Then we have the following surrogate loss function:

\[(h,r,c,,y)=(h()-y)^{2}(r(),-1)+c()(r(),+1),\] (10)

where \(()\) is an arbitrary binary classification loss function such as hinge loss. Then the expected risk with our surrogate loss \(\) can be represented as follows:

\[R_{}^{}(h,r)=_{p(,y)}[(h,r,c,,y)].\] (11)

The intuition behind this is that when the squared error is less than the given cost, we expect its weight \((r(),-1)\) to be larger, i.e., \((r(),+1)\) to be smaller. However, not every binary classification loss is theoretically grounded.

## 4 Theoretical Analysis

In this section, we first introduce the definitions of regressor-consistency and rejector-calibration. Then, we show the condition that our method can result in the Bayes optimal solution. Furthermore, we provide a relaxed condition that the regressor-consistency is only satisfied for correctly accepted instances. Finally, We derive a regret transfer bound and an estimation error bound for our method.

### Regressor-Consistency and Rejector-Calibration

The rejector-calibration we are talking about here is related to the notion of classification calibration . The notion of classification calibration of surrogate losses is defined as the minimum requirement to ensure that a risk-minimizing classifier becomes the Bayes optimal classifier, which is a pointwise version of consistency. The definition of rejector-calibration is given below.

**Definition 3**.: _(Rejector-calibration) We say a rejector \(r:\) is calibrated if \(r\) always makes the same decisions as the Bayes optimal rejector \(r^{}\) in Theorem 2, i.e., \((r())=(r^{}())\) for all \(\) such that \(r^{}() 0\)._

The definition of rejector-calibration indicates that we do not need to obtain the exact optimal rejector due to the difficulty of obtaining the variance, therefore, we just need to ensure that our rejector makes the same decisions as the optimal rejector in Theorem 2.

On the other hand, we define the regressor-consistency as follows.

**Definition 4**.: _(Regressor-consistency) We say a regressor \(h:\) is consistent if \(h\) is equivalent to the Bayes optimal regressor \(h^{}\) in Theorem 2, i.e., \(h=h^{}\)._

Then we demonstrate that our method is regressor-consistent by the following theorem:

**Theorem 5**.: _Suppose the binary loss \(\) is always larger than 0. For a given non-negative cost function \(c()\), for any fixed rejector \(r\), the optimal regressor \(h^{}_{}=_{h}R^{}_{}(h,r)\) is equivalent to the Bayes optimal regressor \(h^{}\)._

The proof of Theorem 5 is provided in Appendix B.1. Theorem 5 shows that the regressor \(h^{}_{}\) learned from our method can be equivalent to the Bayes optimal regressor \(h^{}\).

It is worth noting that there is a special case \((r(),-1)=0\), where the regressor actually ignores the instance \(\). Here we show a relaxed condition for regressor-consistency by the following theorem:

**Theorem 6**.: _Suppose the binary loss function \(\) is classification-calibrated and is always non-negative. Given a non-negative cost function \(c()\), for any fixed rejector \(r\) and for the optimal regressor \(h^{}_{}=_{h}R^{}_{}(h,r)\), the regressor-consistency can be only satisfied for correctly accepted instances (i.e., \(,r^{}()>0\))._

The proof of Theorem 6 is provided in Appendix B.2. Theorem 6 gives a relaxed condition of consistency, where the regressor-consistency is still satisfied for correctly accepted instances.

Then, we demonstrate that our method is rejector-calibrated by the following theorem:

**Theorem 7**.: _Suppose the binary loss \(\) is classification-calibrated and is always larger than 0. For a given non-negative cost function \(c()\), the optimal rejector \(r^{}_{}=_{r}R^{}_{}(h^{}_{},r)\) is calibrated (i.e., \((r^{}_{}())=(r^{}( ))\)), where \(r^{}\) is the Bayes optimal rejector._

The proof of Theorem 7 is provided in Appendix B.3. Theorem 7 shows that our method is rejector-calibrated if the used binary loss \(\) is classification-calibrated.

### Regret Transfer Bound and Estimation Error Bound

In the previous section, we have given the Bayes consistency analysis of our method, i.e., if the minimizer of our proposed risk can be the optimal one in Theorem 2. However, such a result does not guarantee the performance of models that are close to but not the minimizer of the risk \(R^{}_{}\), which occurs commonly since we usually minimize the empirical risk in practice. We provide a theoretical guarantee for such cases by showing the following regret transfer bound:

**Theorem 8**.: _Suppose that \(, y\), \(_{p(y|)}[(h()-y)^{2}] M\) and \(c() C\) hold almost surely:_

\[R_{}(h,r)-R^{}_{}(R^{}_{}(h,r)-R^{*}_{})),\]

_This regret transfer bound holds for widely used binary losses, e.g., when \(\) is the sigmoid loss or the hinge loss, \((u)=|u|\). When \(\) is the logistic loss or the square loss, \((u)=\{2u,2\}\)._The proof of Theorem 8 is provided in Appendix C.1. This theorem guarantees that even if the obtained \((h,r)\) is not exactly the minimizer of \(R_{}^{}\), we can also expect them to achieve good performance as long as they have a low risk \(R_{}^{}\). Then we can further get the following estimation error bound:

**Theorem 9**.: _Suppose the binary loss is upper bounded by \(M_{1}>0\) and \(\)-Lipschitz continuous, \(|h|\), \(c()\), and \(|y|\) are bounded by \(M_{2}>0\). Given the empirical risk minimizers \(\) and \(\), the following bound holds with probability at least \(1-\):_

\[R_{}^{}(,)-R_{}^{*} 2L_{1}(_{n}()+_{n}())+C_{1} },\]

_where \(C_{1}=(4M_{2}^{2}+M_{2})M_{1}\), \(L_{1}=^{2}+M_{1})^{2}+16M_{1}^{4}M_{2}^{2}}\), \(n\) is the \(i.i.d.\) sample size, and \(_{n}\) is the Rademacher complexity ._

The proof of Theorem 9 is provided in Appendix C.2. Given the fact that the Rademacher complexity usually decays at the rate of \((1/)\), we can finally conclude that the performance of our model can approximate its optimal performance with the increasing size of the training set. In the following sections, we will demonstrate the effectiveness of our approach through experiments.

## 5 Experiments

In this section, we show the experimental results when our method is equipped with various binary classification losses and is compared with selective regression methods. In addition to the evaluation metrics commonly used for LwR, we propose additional evaluation metrics. Details of the experiment and the complete experiment can be found in Appendix D.

### Implementation Details

When using deep neural networks as the model and using the gradient descent optimization, we consider a possible scenario where the regressor \(h\) predicts any instance \(\) with such a large error that \((h(),y)>>c()\). In this case the rejector \(r\) expects to reject all instances to make the empirical risk minimal. However, when the rejector \(r\) converges quickly to reject all train instances, i.e., \((r(),-1) 0\) for all train instances, the surrogate loss \(\) will be constant equal to \(c()(r(),+1)\). At that point the gradient of the regressor \(h\) suffers from gradient vanishing. The main reason for this situation is that the regressor \(h\) has not learned the distribution of the label, but the rejector \(r\) has converged, which means that the regressor is not ready. Fortunately, we can avoid such a situation by training the rejector after the regressor is ready, and we name such a method Slow-Start. Specifically, Slow-Start prioritizes training the regressor \(h\) without training the rejector \(r\), and then co-trains the regressor \(h\) and rejector \(r\) when the regressor \(h\) is capable of making predictions.

### Datasets and Backbone Models

We conduct experiments on seven datasets, including one computer vision dataset (AgeDB ), one healthcare dataset (BreastPathQ ), and five datasets from the UCI Machine Learning Repository  (Abalone, Airfoil, Auto-mpg, Housing and Concrete). For each dataset, we randomly split the original dataset into training, validation, and test sets by the proportions of 60%, 20%, and 20%, respectively. It is worth noting that our approach has no restrictions on the regressor \(h\) and rejector \(r\), so \(h\) and \(r\) can be two separate parts or share parameters.

AgeDB is a regression dataset on age prediction  collected by . It contains 16.4K face images with a minimum age of 0 and a maximum age of 101. Age prediction is not an easy task, especially when only a single photo is available. Lighting, clothing, makeup, and facial expressions all tend to affect the intuitive age, and even friends can hardly say they can identify the age in a photo. Rejecting predictions for photos with complex environments can avoid large errors. We employ ResNet-50  as our backbone network for AgeDB, and the regressor \(h\) and rejector \(r\) share parameters. We use the Adam optimizer to train our method for 100 epochs where the Slow-Start is set to 40 epochs, the initial learning rate of \(10^{-3}\) and fix the batch size to 256.

BreastPathQ  is a healthcare dataset collected at the Sunnybrook Health Sciences Centre, Toronto. The dataset contains 2579 patch images, each patch has been assigned a tumor cellularity score score of 0 to 1 by 1 expert pathologist. Currently, this task is performed manually and relies upon expert interpretation of complex tissue structures. Moreover, cancer cellularity scoring is extremely risky and the use of automated methods could lead to irreversible disasters. Regression with rejection can improve this problem very well by predicting only the accepted samples and leaving the rejected samples back to the experts for evaluation. We use the same network as AgeDB and train 300 epochs using Adam optimizer where the Slow-Start is set to 50 epochs, the initial learning rate of \(10^{-3}\) and fix the batch size to 128.

We conducted experiments on five UCI benchmark datasets including Abalone, Airfoil, Auto-mpg, Housing and Concrete. All of these datasets can be downloaded from the UCI Machine Learning . Since our proposed method do not depend on a specific model, and we train two types of base models including the linear model and the multilayer perceptron (MLP) to support the flexibility of our method on choosing a model, where the MLP model is a five-layer (\(d\)-20-30-10-1) neural network with a ReLU activation function. For the rejector \(r\) and regressor \(h\), we consider them as two separate parts with the same structure. For both the linear model and the MLP model, we use the Adam optimization method with the batch size set to 1024 and the number of training epochs set to 1000 where the Slow-Start is set to 200 epochs. The learning rate for all UCI benchmark datasets is selected from \(\{10^{-1},10^{-2},10^{-3}\}\).

### Evaluation Metrics

For evaluation metrics, we use the RcR loss (RcRLoss) in Eq. (6) and the rejection rate (Rej). In order to further investigate how the model work, we propose additional metrics. Accepted losses

   Cost & Sup & RcRLoss & AL & RL & Rej & AR & RA \\   & & 59.80 & 54.25 & 156.81 & 95.40 & 93.13 & 2.51 \\  & & (0.31) & (4.41) & (23.21) & (2.88) & (4.30) & (1.56) \\   & & 69.00 & 61.56 & 151.04 & 86.22 & 81.41 & 8.12 \\  & & (0.39) & (4.10) & (12.05) & (2.94) & (3.07) & (2.49) \\   & & 77.10 & 67.32 & 150.52 & 76.00 & 70.63 & 16.11 \\  & & (1.72) & (2.21) & (12.36) & (15.71) & (16.36) & (13.20) \\   & & 85.36 & 73.07 & 162.44 & 73.38 & 67.33 & 17.20 \\  & (3.73) & (2.23) & (3.21) & (12.45) & (11.50) & (12.07) & (9.08) \\   & & 92.94 & 82.89 & 170.04 & 58.35 & 52.15 & 30.56 \\   & & (3.02) & (7.47) & (20.53) & (12.51) & (11.59) & (12.48) \\   & & 95.08 & 79.62 & 166.07 & 52.15 & 46.13 & 34.38 \\   & & (5.62) & (5.44) & (13.75) & (14.96) & (14.76) & (13.40) \\   & & 96.80 & 82.44 & 173.14 & 37.11 & 32.54 & 51.31 \\   & & (7.45) & (2.40) & (12.58) & (22.64) & (21.42) & (23.96) \\   

Table 2: Test performance (mean and std) of our surrogate loss equipped MAE on AgeDB. We repeat the sampling-and-training process 5 times. The metrics Rej, AR, RA are scaled to 0-100.

   Cost & Sup & RcRLoss & AL & RL & Rej & AR & RA \\   & & 59.80 & 54.25 & 156.81 & 95.40 & 93.13 & 2.51 \\  & & (0.31) & (4.41) & (23.21) & (2.88) & (4.30) & (1.56) \\   & & 69.00 & 61.56 & 151.04 & 86.22 & 81.41 & 8.12 \\  & & (0.39) & (4.10) & (12.05) & (2.94) & (3.07) & (2.49) \\   & & 77.10 & 67.32 & 150.52 & 76.00 & 70.63 & 16.11 \\  & & (1.72) & (2.21) & (12.36) & (15.71) & (16.36) & (13.20) \\   & & 85.36 & 73.07 & 162.44 & 73.38 & 67.33 & 17.20 \\  & (3.73) & (2.23) & (3.21) & (12.45) & (11.50) & (12.07) & (9.08) \\   & & 92.94 & 82.89 & 170.04 & 58.35 & 52.15 & 30.56 \\  & & (3.02) & (7.47) & (20.53) & (12.51) & (11.59) & (12.48) \\   & & 95.08 & 79.62 & 166.07 & 52.15 & 46.13 & 34.38 \\   & & (5.62) & (5.44) & (13.75) & (14.96) & (14.76) & (13.40) \\   & & 96.80 & 82.44 & 173.14 & 37.11 & 32.54 & 51.31 \\   & & (7.45) & (2.40) & (12.58) & (22.64) & (21.42) & (23.96) \\   

Table 2: Test performance (mean and std) of our surrogate loss equipped MAE on AgeDB. We repeat the sampling-and-training process 5 times. The metrics Rej, AR and RA are scaled to 0-100.

### Formulation of Surrogates and Setting of Rejection Costs

In our experiments, we consider a variety of binary classification loss functions, such as mean absolute error (MAE), mean square loss, logistic loss, sigmoid and hinge loss. The rejection cost

   Datasets & Cost & Sup & R\(\&\)RLoss & AL & RL & Rej & AR & RA \\   & 3 & & 2.41 & 1.99 & 8.13 & 42.04 & 32.82 & 33.33 \\  & & & (0.12) & (0.21) & (1.08) & (3.18) & (3.44) & (3.22) \\   & 4 & & 2.88 & 2.30 & 11.37 & 33.70 & 25.56 & 39.27 \\  & 4 & 4.44 & (0.13) & (0.21) & (1.70) & (2.47) & (2.81) & (3.71) \\   & 5 & (0.46) & 3.22 & 2.66 & 10.30 & 23.43 & 16.83 & 48.98 \\  & 5 & & (0.23) & (0.35) & (1.25) & (2.94) & (2.41) & (5.90) \\   & 6 & & 3.53 & 2.93 & 12.13 & 19.32 & 13.81 & 53.20 \\  & 6 & & (0.25) & (0.35) & (1.69) & (3.47) & (3.33) & (5.67) \\   & 9 & & 7.20 & 4.23 & 37.80 & 62.23 & 41.49 & 11.60 \\  & & & (0.35) & (0.86) & (2.95) & (3.73) & (5.73) & (3.29) \\   & 12 & & 8.11 & 5.39 & 51.51 & 40.33 & 23.37 & 25.88 \\  & 12 & & (0.36) & (0.86) & (10.51) & (7.95) & (7.20) & (9.04) \\   & 16 & & 9.15 & 6.84 & 72.80 & 24.92 & 11.92 & 38.17 \\  & 12.96 & & (0.43) & (0.70) & (20.79) & (5.67) & (6.93) & (5.02) \\   & 20 & (2.60) & 11.32 & 8.83 & 58.28 & 21.53 & 13.70 & 48.66 \\  & 25 & & (0.75) & (1.47) & (8.87) & (7.71) & (5.34) & (18.08) \\   & 25 & & 11.47 & 9.24 & 74.38 & 14.19 & 8.08 & 52.11 \\  & & & (1.54) & (1.35) & (16.07) & (5.11) & (3.60) & (12.32) \\   & 30 & & 11.68 & 11.17 & 96.55 & 2.52 & 1.38 & 86.55 \\  & 30 & & (3.07) & (3.20) & (16.60) & (3.81) & (1.78) & (20.06) \\   & 4 & & 3.64 & 2.99 & 13.98 & 56.92 & 46.80 & 28.74 \\  & 4 & & (0.29) & (0.83) & (4.16) & (13.00) & (15.49) & (10.51) \\   & 6 & & 4.83 & 3.83 & 18.04 & 37.31 & 29.01 & 42.42 \\   & 8 & (0.93) & (1.70) & (5.95) & (14.10) & (12.74) & (19.54) \\   & 8 & 8.34 & 6.75 & 6.14 & 25.59 & 22.95 & 19.26 & 64.99 \\  & (2.16) & (1.93) & (2.41) & (12.48) & (19.98) & (18.27) & (23.95) \\   & 10 & & 7.14 & 6.11 & 23.29 & 24.07 & 17.15 & 48.47 \\  & 10 & & (1.64) & (2.24) & (9.54) & (6.58) & (5.12) & (15.98) \\   & 13 & & 8.13 & 7.42 & 35.49 & 12.56 & 10.38 & 71.52 \\  & 12 & & (2.41) & (2.83) & (23.74) & (6.83) & (6.14) & (14.52) \\   & 9 & & 8.80 & 6.25 & 40.28 & 84.46 & 77.60 & 9.72 \\  & & & (0.34) & (3.22) & (17.30) & (11.67) & (15.88) & (5.91) \\   & 12 & & 9.52 & 7.40 & 58.94 & 44.65 & 33.30 & 31.25 \\   & 12 & & (0.75) & (1.48) & (25.98) & (8.69) & (9.99) & (8.64) \\   & 16 & (3.43) & 10.12 & 8.35 & 88.14 & 22.38 & 14.21 & 51.84 \\  & & & (1.84) & (1.58) & (44.53) & (8.90) & (6.81) & (14.41) \\   & 20 & & 10.50 & 9.59 & 184.24 & 8.51 & 5.81 & 73.40 \\  & & & (3.32) & (3.50) & (109.35) & (6.82) & (5.32) & (13.11) \\   & 20 & & 18.03 & 13.17 & 82.17 & 69.42 & 54.06 & 12.34 \\  & & & (1.32) & (4.91) & (14.58) & (6.92) & (9.37) & (4.47) \\   & 30 & & 24.20 & 19.29 & 112.13 & 44.08 & 27.43 & 26.80 \\   & & (1.85) & (3.85) & (30.32) & (8.81) & (8.55) & (7.90) \\   & 40 & 34.44 & 28.63 & 23.12 & 136.51 & 31.50 & 18.32 & 39.49 \\   & (3.05) & & (2.56) & (4.59) & (46.59) & (8.98) & (7.30) & (12.07) \\   & 50 & & 32.48 & 27.90 & 168.19 & 19.76 & 10.54 & 53.82 \\   & & (2.76) & (4.31) & (41.73) & (7.54) & (4.51) & (13.74) \\   & 60 & & 34.33 & 30.33 & 197.26 & 12.82 & 5.67 & 60.95 \\  ^{n}[l(_{i})-y_{i}]^{2}(_{i}) [l(_{i})-y_{i}]^{2}}{_{i=1}^{n}[l(_{i} )-y_{i}]^{2}}]^{(_{i}) 0}\) and \(^{n}[l(_{i})-y_{i}]^{2} c(_{i

[MISSING_PAGE_FAIL:9]

Table 4 shows the results of comparison experiments. Specifically, to be able to establish a connection between our studied RcR and selective regression, we set the expected rejection rate (Rj) of the selective regression method based on the results of RcR_MAE (our surrogate loss equipped MAE). It is important to note that there is no way to perfectly match the rejection rate, as the set Rj is "expected". In addition, we show plots of the variation in AL loss for all methods with different rejection rates in Appendix D.7. As can be seen Table 4, our proposed method outperforms (smaller AL loss with the same rejection rate) almost all compared methods, which validates the effectiveness of our method.

## 6 Conclusion

In this paper, we investigated a novel regression problem called regression with cost-based rejection, which aims to learn a model that can reject predictions to avoid critical mispredictions at a certain rejection cost. In order to solve this problem, we first formulated the expected risk for regression with cost-based rejection and derived the Bayes optimal solution for the expected risk, which shows that we should reject instances where the variance is greater than the rejection cost. Since the variance is difficult to obtain, we proposed a surrogate loss function that considers rejection as binary classification. Further, we provided conditions for the consistency of our method, implying that the optimal solution can be recovered by our method. Finally, we derived the regret transfer bound and the estimation error bound for our method and conducted extensive experiments on various datasets to demonstrate the effectiveness of our proposed method. We expect that our first study of a simple yet theoretically grounded method for regression with cost-based rejection can inspire more interesting studies on this new problem.

    &  &  &  \\   &  &  &  &  &  &  &  &  &  \\   & 3 & 2.41 & **1.99** & 42.04 &  &  &  &  &  &  \\  & & (0.12) & **(0.21)** & & & & & & & \\   &  & 2.88 & **2.30** & 33.70 &  &  &  &  &  &  \\  & & (0.13) & **(0.21)** & & & & & & & \\   &  & 3.22 & **2.66** & 23.43 &  &  &  &  &  &  \\  & & (0.23) & **(0.35)** & & & & & & & \\   &  & 3.53 & 2.93 &  &  &  &  &  &  &  \\  & & (0.25) & (0.35) & & & & & & & \\   & 4 & 3.64 & **2.99** & 56.92 &  &  &  &  &  \\  & & (0.29) & **(0.83)** & & & & & & & \\   &  & **4.83** & **3.83** & 37.31 &  &  &  &  &  &  &  \\  & & (0.93) & **(1.70)** & & & & & & & \\   &  & 6.75 & **6.14** & 22.95 &  &  &  &  &  &  \\  & & (1.93) & **(2.41)** & & & & & & & \\   &  & 8.13 & **7.42** & 12.56 &  &  &  &  &  &  &  \\  & & (2.41) & **(2.83)** & & & & & & & \\   & 9 & 8.80 & **6.25** & 84.46 &  &  &  &  &  &  &  &  \\  & & (0.34) & **(3.22)** & & & & & & & \\   &  & 9.52 & **7.40** & 44.65 &  &  &  &  &  &  \\  & & (0.75) & **(1.48)** & & & & & & & \\   &  & 10.12 & **8.35** & 22.38 &  &  &  &  &  &  \\  & & (1.84) & **(1.58)** & & & & & & \\   &  & 10.50 & 9.59 & 8.51 &  &  &  &  &  &  \\  & & (3.32) & (3.50) & (6.82) & & & & & & \\   & 20 & 18.03 & **13.17** & 69.42 &  &  &  &  &  &  \\  & & (1.32) & **(4.91)** & & & & & & & \\   &  & 24.20 & **19.29** & 44.08 &  &  &  &  &  &  &  \\   &  &