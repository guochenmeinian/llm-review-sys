# Paul Couairon\({}^{1,2}\)****Mustafa Shukor\({}^{1}\)

DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut**Jean-Emmanuel Haugeard\({}^{2}\)****Matthieu Cord\({}^{1,3}\)****Nicolas Thome\({}^{1}\)**

\({}^{1}\)Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France \({}^{2}\)Thales, TSGF, cortAlx Labs, France

\({}^{3}\)Valeo.ai

###### Abstract

Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised semantic segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we demonstrate that using these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation. Specifically, we leverage a _recursive Normalized Cut_ algorithm that regulates the granularity of detected objects and produces well-defined segmentation maps that precisely capture intricate image details. Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks. _Project page_: https://difficcut-segmentation.github.io

Figure 1: **Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based _recursive_ partitioning algorithm. Compared to DiffSeg , DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.**

Introduction

Foundation models have emerged as powerful tools across various domains, including language , vision , and multimodal tasks . Pretrained on extensive datasets, these models exhibit unparalleled generalization capabilities, marking a significant departure from training models from scratch to efficiently adapting pretrained foundation models . Utilizing pretrained models is particularly vital for dense visual tasks, alleviating the need for large annotated datasets specific to each domain. While prior works  have addressed unsupervised image segmentation, they significantly lag behind supervised models . Recently, SAM , proposed a model that can produce fine-grained class-agnostic masks which achieves outstanding zero-shot transfer to any images. Still, it requires a huge annotated segmentation dataset as well as significant training resources. Therefore, in this work, we investigate an alternative direction: unsupervised and zero-shot segmentation under the most constraining conditions, where no segmentation annotations or prior knowledge on the target dataset are available.

Recently, several methods have emerged to address unsupervised object detection by framing it as a graph partitioning problem, utilizing self-supervised ViT features . LOST  proposes to localize a unique object in a image by exploiting the inverse degree information to find a seed patch. TokenCut  splits the graph in two subsets given a bipartition. FOUND  and MaskCut  extend these approaches by addressing the single object discovery limitation. While being able to localize multiple objects, the latter methods remain constrained to identify a pre-determined number of objects, making them ill-suited for a task of unsupervised image segmentation which inherently requires to adapt the number of segment to uncover to the visual content.

Conversely, text-to-image diffusion models  can produce high-quality visual content from textual descriptions , indicating implicit learning of a wide range of visual concepts. Recent works have tried to leverage diverse internal representations of such models for localization or segmentation tasks. Several methods  opt to exploit image-text interactions within cross-attention modules but are ultimately constrained by the need for meticulous input prompt design. Concurrently,  identifies semantic correspondences between image pixels and spatial locations of low-dimensional feature maps by modulating cross-attention modules. This method proves to be computationally intensive as it requires numerous forward inferences. On the other hand, DiffSeg  segment images by iteratively merging self-attention maps which only depict local correlation between patches.

In this work, we introduce DiffCut, a new method for zero-shot image segmentation which solely harnesses the encoder features of a pre-trained diffusion model in a _recursive_ graph partitioning algorithm to produce fine-grained segmentation maps. Importantly, our method does not require any label from downstream segmentation datasets and its backbone has not been pre-trained on dense pixel annotations such as SAM . We observe in Fig. 1 that DiffCut produces sharp segments that nicely outline object boundaries. In comparison with the recent state-of-the-art unsupervised zero-shot segmentation method DiffSeg , the segments yielded by DiffCut, are better aligned with the semantic visual concepts, _e.g._ DiffCut is able to uncover the urban area as well as the boats in the middle row image. Our main contributions are as follows:

* We leverage the features from the final self-attention block of a diffusion UNet encoder, for the task of unsupervised image segmentation. In this context, we demonstrate that exploiting the inner patch-level alignment yields superior performance compared to merging self-attention maps as done in DiffSeg .
* Compared to existing graph based object localization methods _e.g._ TokenCut or MaskCut , we push further and take advantage of a _recursive Normalized Cut_ algorithm to generate dense segmentation maps. Via a partitioning threshold, the method is able to regulate the granularity of detected objects and consequently adapt the number of segments to the visual content.
* We perform extensive experiments to validate the effectiveness of DiffCut and show that it significantly outperforms state-of-the-art methods for unsupervised segmentation on standard benchmarks, reducing the gap with fully supervised models.

In addition, we exhibit the remarkable semantic coherence emerging in our chosen diffusion features by measuring their patch-level alignment, which surpasses other backbones such as CLIP  or DINOv2 . Our ablation studies further reveal the relevance of these diffusion features as well as the _recursive_ partitioning approach which proves to provide robust segmentation performance. Finally, we show that DiffCut can be extended to an open-vocabulary setting with a straightforward process leveraging a _convolutional_ CLIP, which even tops most dedicated methods on this task.

## 2 Related Work

Semantic segmentation.Semantic segmentation consists in partitioning an image into a set of segments, each corresponding to a specific semantic concept. While supervised semantic segmentation has been widely explored [45; 46; 47; 27], unsupervised and zero-shot transfer segmentation for any images with previously unseen categories remains significantly more challenging and much less investigated. For example, most works in unsupervised segmentation require access to the target data for unsupervised adaption [21; 20; 19; 18]. Therefore, these methods cannot segment images that are not seen during the adaptation. Recently, DiffSeg  moved a step forward by proposing an unsupervised and zero-shot approach that can produce quality segmentation maps without any prior knowledge on the underlying visual content.

Segmentation with Text Supervision.Recent works have shown that learning accurate segmentation maps is possible with text supervision, overcoming the cost of dense annotations. These works are mostly based on image-text contrastive learning [48; 49; 50; 51], and usually exploit the features of CLIP [52; 53; 54]. MaskCLIP  leverages CLIP to get pseudo labels used to train a typical image segmentation model. ReCO  uses CLIP for dataset curation and get a reference image embedding for each class that is used to obtain the final segmentation. CLIPpy  proposes minimal modifications to CLIP to get dense labels. SegCLIP  continues to train CLIP with additional reconstruction and superpixel-based KL loss to enhance localization. TCL  learns a region-text alignment to get precise segmentation masks. GroupViT  also learns masks from text supervision and is based on a hierarchical grouping mechanism. Similarly, ViewCo  proposes a contrastive learning between multiple views/crops of the image and the text.

Graph-based Object Detection.Built on top of self-supervised ViT features, various methods frame the problem of object detection as a graph partitioning problem. LOST  aims at detecting salient object in an image using the degree of the nodes in the graph and a seed expansion mechanism. Based on _Normalized Cut (NCut)_, FOUND  proposes to identify all background patches, hence discovering all object patches as a by-product with no need for a prior knowledge of the number of objects or their relative size with respect to the background. TokenCut  detects one single salient object in each image with a unique _NCut_ bipartition. In an attempt to adapt TokenCut to multi-objects localization, MaskCut  first localizes an object and disconnects its corresponding patches to the rest of the graph before repeating the process a pre-determined number of times. As these graph partitioning methods are only able to uncover a fixed number of segments, they are inadequate for a task of image segmentation.

Segmentation with Diffusion Models.Diffusion models can produce high-quality visual content given a text prompt, indicating implicit learning of a wide range of visual concepts and the ability of grounding these concepts in images. Therefore their internal representations appear as good candidates for visual localization tasks [56; 57; 58]. ODISE  is one of the first training-based approaches to build a fully supervised panoptic image segmentor on top of diffusion features. Several other methods [40; 41; 42] leverage attention modules for localization or segmentation tasks. DiffuMask  uses the cross-modal grounding between a text input and an image in cross-attention modules to segment the referred object in a synthetic image. However, DiffuMask can only be applied to a generated image. In a zero-shot setting,  harnesses the image-text interaction via cross-attention score maps to complete self-attention maps and segment grounded objects. EmerDiff  opts not to exploit image-text interactions in cross-attention modules. Instead, it identifies semantic correspondences between image pixels and spatial locations by modulating the values of a sub-region of feature maps in low-resolution cross-attention layers. These cross-attention based methods eventually prove to be highly computationally intensive as multiple forward inferences are often required. On the other hand, DiffSeg  proposes an iterative merging process based on measuring KL divergence among self-attention maps to merge them into valid segmentation masks. However, it appears that self-attention score maps only depict very local correlation between patches.

DiffCut

Diffusion Models.Diffusion models [60; 61; 62] are generative models that aim to approximate a data distribution \(q\) by mapping an input noise \(_{T}(0,I)\) to a clean sample \(_{0} q\) through an iterative denoising process. In latent text-to-image (T2I) diffusion models, _e.g._ Stable Diffusion , the diffusion process is performed in the latent space of a Variational AutoEncoder  for computational efficiency, and encode the textual inputs as feature vectors from pretrained language models. Starting from a noised latent vector \(_{}\) at the timestep \(t\), a denoising autoencoder \(_{}\) is trained to predict the noise \(\) that is added to the latent \(\), conditioned on the text prompt \(\). The training objective writes:

\[=_{}(), (0,1),t}[\|-_{}(_{t},t, ())\|_{2}^{2}]\] (1)

where \(t\) is uniformly sampled from the set of timesteps \(\{1,,T\}\).

### Features Extraction

An input image is encoded into a latent via the VQ-encoder of the latent diffusion model and a small amount of gaussian noise is added to it (not shown in Fig. 2). The obtained latent is passed to the diffusion UNet encoder, from which we only extract the output features, denoted \(\), from its last self-attention block. This choice design has several motivations:

Attention Limitations.In contrast to several methods that harness cross-attention modules for localization or segmentation tasks [39; 40; 41; 42], we deliberately choose not to depend on this mechanism. The accuracy of segmentation maps generated via attention modules heavily relies on the quality of the textual input which often requires an automatic captioning model combined with a meticulous prompt design to reach competitive performance. Besides being constrained by the maximum number of input tokens, such approach is proved to be inaccurate in the presence of cohyponyms  and is prone to neglect subject tokens as the number of objects to detect becomes large . The localization and segmentation capacity with a single forward inference is then constrained by the performance of the captioning model and the attention modules themselves. Exploiting only the intermediate diffusion features alleviate the computational cost of an additional captioning model and do not necessitate multiple forward inferences.

UNet Encoder Effectiveness.Previous works [66; 67; 37] have shown that diffusion features provide precise semantic information shared across objects from different domains. Building on this observation, we hypothesize that the pyramidal architecture of the UNet encoder capture semantically rich image representations that are well-suited for zero-shot vision tasks. To validate this assumption, we exhibit the _semantic coherence_ emerging in the UNet encoder, evidenced by a remarkable patch level alignment in the output features of the final self-attention block. We in fact demonstrate that these features are sufficient to reach state-of-the-art zero-shot segmentation performance.

Computational Efficiency.By solely exploiting the diffusion UNet encoder, our method offers a substantial computational gain, reducing the model size by \(70\%\) (400M _vs_ 1.3B parameters). In contrast, DiffSeg extracts every self-attention maps of the UNet which requires a full model inference.

Figure 2: **Overview of DiffCut. 1)** DiffCut takes an image as input and extracts the features of the last self-attention block of a diffusion UNet encoder. **2)** These features are used to construct an affinity matrix that serves in a _recursive normalized cut_ algorithm, which outputs a segmentation map at the latent spatial resolution. **3)** A high-resolution segmentation map is produced via a concept assignment mechanism on the features upsampled at the original image size.

### Recursive Feature Clustering

_Normalized Cut_ treats image segmentation as a graph partitioning problem . Given a graph \(=(,)\) where \(\) and \(\) are respectively a set of nodes and edges, we construct an affinity matrix \(\) such that \(_{ij}\) is the edge between node \(v_{i}\) and \(v_{j}\), and a diagonal degree matrix \(\), with \(d(i)=_{j}_{ij}\). _NCut_ minimizes the cost of partitioning the graph into two sub-graphs by solving:

\[(-)x=x\] (2)

to find the eigenvector \(x\) corresponding to the second smallest eigenvalue. In the ideal case, the clustering solution only takes two discrete values. Since the solution of Eq. (2) is a continuous relaxation of the initial problem, \(x\) contains continuous values and a splitting point has to be determined to partition it. To find the optimal partition, we examine \(l\) evenly spaced points in \(x\) and select the one resulting in the minimum _NCut_ value.

Graph Affinity.Based on our observations of the patch-level alignment of diffusion features illustrated in Fig. 4, we assume that the _normalized cut_ algorithm will produce sharp segments, each corresponding to a precise semantic concept as distinct objects would manifest as weakly connected components in a patch similarity matrix. Following this intuition, we construct an affinity matrix \(\), by computing the cosine similarity, normalized between \(0\) and \(1\), between each pair of patches. As the _NCut_ criterion evaluates both the dissimilarity between different segments and the similarity within each segment, we opt to emphasize inter-segments dissimilarity by raising each element to a positive integer power \(\):

\[_{ij}=(_{i}_{j}}{\|_{i}\|_{2}\|_{ j}\|_{2}})^{}\] (3)

Essentially, this process maintains a relatively high affinity for highly similar patches, while squashing the weights between dissimilar patches towards zero. This mechanism plays the role of a _soft thresholding_, offering a more gradual adjustment compared to setting a threshold to explicitly binarize the affinity matrix as done in  and .

Recursive Partitioning.Classical spectral clustering  requires setting a pre-defined number of clusters to partition the graph, which is a significant constraint in the context of zero-shot image segmentation where no prior knowledge on the visual content is available. We therefore adopt a _recursive_ graph partitioning , which adapts the number of uncovered segments to the visual content via a threshold, denoted \(\), on the maximum partitioning cost. This hyperparameter stops the recursive partitioning of a segment when its _NCut_ value exceeds it and thereby regulates the granularity of detected objects. We demonstrate that our soft thresholding process detailed above enhances the robustness of the method which delivers competitive performance across a wide range of \(\) values. This recursive clustering process is summarized in Supplementary A.

### High-Resolution Concept Assignment

Thus far, we have constructed segmentation maps (_e.g._\(32 32\)) which are \(32\) times lower in resolution than the original image (_e.g._\(1024 1024\)). The number of segments found in each image depends on the image and the value of the hyperparameter \(\). Our next goal is to upscale these low-resolution maps to build accurate pixel-level segmentation maps. We propose a high-resolution segmentation process that can be decomposed into the following steps:

1. **Masked Spatial Marginal Mean.** First, our objective is to extract a set of representations that embeds the semantics of each segment. As shown in , reducing the spatial dimension of diffusion features with a _Spatial Marginal Mean_ (SMM) effectively retains semantic information and provides accurate image descriptor. In light of this, we naturally propose to collapse the spatial dimension of each segment with a _Masked SMM_. This process yields a collection of semantically rich concept-embeddings, denoted \(\).
2. **Concept Assignment.** A naive approach to obtain segmentation maps at the original image resolution consists in performing a _nearest-neighbor_ upsampling. Despite its straightforwardness, this approach results in a blocky output structure as all pixels within the same feature patch are assigned to the same concept. Alternatively, we opt to first bilinearly upsample our low-resolution feature map \(\) to match the original image spatial size and then proceed with the pixel/conceptassignment. Specifically, for each concept \(c_{i}\), we compute its cosine similarity with the upsampled features \(_{up}\). This yields a similarity matrix of size \((H W K)\) where \(K=||\). Then, the assignment process simply consists in taking the argmax across the \(K\) channels. The obtained segmentation map \(\) is eventually refined with a pixel-adaptive refinement module .

## 4 Experiments

Datasets.Following existing works in image segmentation [21; 53; 52; 18], we use the following datasets for evaluation: **a)** Pascal VOC  (20 foreground classes), **b)** Pascal Context  (59 foreground classes), **c)** COCO-Object  (80 foreground classes), **d)** COCO-Stuff-27 merges the 80 things and 91 stuff categories in COCO-stuff into 27 mid-level categories, **e)** Cityscapes  (27 foreground classes) and **f)** ADE20K (150 foreground classes) . An extra background class is considered in Pascal VOC, Pascal Context, and COCO-Object. We ignore their training sets and directly evaluate our method on the original validation sets, at the exception of COCO for which we evaluate on the validation split curated by prior works [21; 19].

Metrics.For all datasets, we report the mean intersection over union (mIoU), the most popular evaluation metric for semantic segmentation. Because our method does not provide a semantic label, we use the Hungarian matching algorithm  to assign predicted masks to a ground truth mask. For datasets including a background class, we perform a _many-to-one_ matching to the background label (Supplementary H). As in , we emphasize _unsupervised adaptation_ **(UA)**, _language dependency_ **(LD)**, and _auxiliary image_ **(AX)**. **UA** means that the specific method requires unsupervised training on the target dataset. Methods without the **UA** requirement are considered zero-shot. **LD** means that the method requires text input, such as a descriptive sentence for the image, to facilitate segmentation. **AX** means that the method requires an additional pool of reference images or synthetic images.

Implementation details.DiffCut builds on SSD-1B , a distilled version of Stable Diffusion XL . The model takes an empty string as input and we set the timestep for denoising to \(t=50\). To ensure a fair comparison when evaluating our method against baselines, we set a unique value for \(\) and \(\) across all datasets, equal to \(0.5\) and \(10\) respectively. Following previous works, we make use of PAMR  to refine our segmentation masks. Our method runs on a single NVIDIA TITAN RTX (24GB) with input images of size \(1024 1024\) and can segment an image in one second.

### Results on Zero-shot Segmentation

Tab. 1 reports the mIoU score for each baseline across the 6 benchmarks. Note that the numbers shown for COCO-Stuff and Cityscapes are taken from . We complete ReCo  and MaskCLIP  scores with the results obtained in . Other numbers are taken from . We also note that DiffSeg tunes the sensible merging hyperparameter on a subset of images from the training set from the respective datasets. For a fair comparison, we evaluate the method fixing it to 1, as recommended in the original paper, and refine the obtained masks with PAMR. This baseline is denoted DiffSeg\({}^{}\).

   Model & LD & AX & UA & **VOC** & **Context** & **COCO-Object** & **COCO-Stuff-27** & **Cityscapes** & **ADE20K** \\  _Extra-Training_ & & & & & & & & & 6.7 & 6.4 & - \\ IIC  & ✗ & ✗ & ✓ & 9.8 & - & - & 9.8 & 7.1 & - \\ MPC  & ✗ & ✗ & ✓ & - & - & - & 13.8 & 12.3 & - \\ PiCIE  & ✗ & ✗ & ✓ & - & - & - & 14.4 & - & - \\ EAGLE  & ✗ & ✗ & ✓ & - & - & - & 27.2 & 22.1 & - \\ U2Seg  & ✗ & ✗ & ✓ & - & - & - & 30.2 & - & - \\  STEGO  & ✓ & ✗ & ✓ & - & - & - & 28.2 & 21.0 & - \\ ACSeg  & ✓ & ✗ & ✓ & 53.9 & - & - & 28.1 & - & - \\  _Training-free_ & & & & & & & & & \\ ReCO  & ✓ & ✓ & ✗ & 25.1 & 19.9 & 15.7 & 26.3 & 19.3 & 11.2 \\ MaskCLIP  & ✓ & ✗ & ✗ & 38.8 & 23.6 & 20.6 & 19.6 & 10.0 & 9.8 \\  MaskCut (\(k=5\))  & ✗ & ✗ & ✗ & 53.8 & 43.4 & 30.1 & 41.7 & 18.7 & 35.7 \\ DiffSeg  & ✗ & ✗ & ✗ & - & - & - & 43.6 & 21.2 & - \\ DiffSeg\({}^{}\) & ✗ & ✗ & 49.8 & 48.8 & 23.2 & 44.2 & 16.8 & 37.7 \\ 
**DiffCut (Ours)** & ✗ & ✗ & **65.2** & **56.5** & **34.1** & **49.1** & **30.6** & **44.3** \\   

Table 1: **Unsupervised segmentation results. Best method in **bold**, second is underlined.

With our set of default hyperparameters, DiffCut significantly outperforms all other baselines despite not relying on language dependency, auxiliary images or unsupervised adaptation. On average, our method achieves a gain of +7.3 mIoU over the second best baseline. Notably, DiffCut exceeds MaskCut with an average improvement of +9.4 mIoU. Additionally, it outperforms the previous state-of-the-art method in unsupervised segmentation, DiffSeg, by +5.5 mIoU on COCO-Stuff and +9.4 mIoU on Cityscapes. The superiority of DiffCut over these two methods demonstrates our two key contributions: the high quality of our visual features for semantic segmentation and the flexibility of the recursive NCut algorithm in adjusting the number of segments according to the visual content of each image. The effectiveness of our method is further corroborated by our qualitative results shown in Fig. 1. In comparison to DiffSeg, DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts. Additional examples can be found in Supplementary N.

We note here that, as the granularity of annotations varies across target datasets, our fixed set of hyperparameters can not be in the optimal regime on each of them. Therefore, relaxing the condition on prior knowledge about the target dataset, we report in Supplementary G results of DiffCut where \(\) is loosely tuned using a small subset of annotated images from the target training split.

### Semantic Coherence in Vision Encoders

As good candidates for a task of unsupervised segmentation are expected to be semantically coherent, we conduct a comparison between different families of foundation models on their internal alignment at the patch-level. Selected models include text-to-image DMs (SSD-1B ), text-aligned contrastive models (CLIP , SigLIP ) and self-supervised models (DINO , DINOv2 ). At the exception of DINO-ViT-B/16, evaluated models are of roughly similar size, approximately 300M parameters for DINOv2, CLIP-ViT-L/14 and SigLIP-ViT-L/16 and 400M for SSD-1B UNet encoder.

As in , we collect patch representations from various vision encoders and store their corresponding target classes using the segmentation labels. Given \(_{i}=(})_{i}^{D_{v}}\) and \(_{j}=(})_{j}^{D_{v}}\), the patch representations of images \(}\) and \(}\) at respectively index \(i\) and \(j\), we compute their cosine similarity and use this score as a binary classifier to predict if the two patches belong to the same class. Given \(l(})_{i}\) and \(l(})_{j}\), the labels associated to the patches, if \(l(})_{i,j}=l(})_{p,q}\), the target value for binary classification is 1, else 0. We present in Fig. 3 the ROC curve and AUC score for our candidate models. We observe that SSD-1B UNet encoder  demonstrates a greater patch-level alignment than any other candidate model with an AUC score of 0.83, even surpassing DINOv2 . We further exhibit the outstanding alignment between patch representations associated to semantically similar concepts with qualitative results in Fig. 4. We provide additional qualitative examples patch-level alignment in Supplementary M.

Figure 4: **Qualitative results on the semantic coherence of various vision encoders. We select a patch (red marker) associated to the dog in Ref. image. Top row shows the cosine similarity heatmap between the selected patch and all patches produced by vision encoders for Ref. image. Bottom row shows the heatmap between the selected patch in Ref. image and all patches produced by vision encoders for Target. image.**

Figure 3: **ROC curves revealing the semantic coherence of vision encoders.**

A potential rationale for this observation lies in the superior semantic information retention of a diffusion model compared to alternative backbones, attributed to its inherent capacity to set a structural image layout, internally acquired during the training phase. These results provides insight into the strong clustering results presented in previous section, as improved semantic coherence suggests that patches belonging to the same object are more effectively clustered.

### Ablation study

In this section, we perform ablation studies to validate the individual choices in the design of DiffCut.

DiffCut _vs_ DiffSeg.DiffSeg proposes their own clustering algorithm based on a self-attention map merging process. As the original implementation uses a different diffusion backbone as ours, we validate the benefit of our method by swapping the original SDv1.4 with our stronger SSD-1B. For a fair comparison between methods, we use the default set of hyperparameters recommended in  and set the default merging threshold of DiffSeg to \(0.5\) for all datasets. Tab. 2 clearly validates the superiority of using rich semantic features in a _recursive_ graph partitioning algorithm over the self-attention merging mechanism of DiffSeg. Qualitative results shown in Fig. 1 further display the edge of DiffCut in uncovering semantic clusters. Shown results do not make use of the mask refinement module, explaining the gap with Tab. 1.

Recursive Normalized Cut _vs_ Automated Spectral Clustering.In DiffCut, the hyperparameter \(\) corresponds to the maximum graph partitioning cost allowed. In contrast, classical spectral clustering requires to explicitly set the number of segments to be found in the graph. To validate the benefit of the recursive approach over spectral clustering, we introduce a simple yet effective baseline dubbed AutoSC.  proposes a heuristic that estimates the number of connected components in a graph with the largest _relative-eigen-gap_ between its Laplacian eigen-values. The larger the gap, the more confident the heuristic. In our context, the index of the eigen-value that maximizes this quantity can be interpreted as the number of clusters in an image. Thus, we define a set of exponents \(\{1,5,10,15\}\) and determine the value \(\) in this set such that its element-wise exponentiation of matrix \(\) yields the largest Laplacian _relative-eigen-gap_. Then, we use the index of the eigen-value maximizing the gap as the number of clusters in a \(k\)-way spectral clustering performed with the algorithm proposed in . As shown in Tab. 2, DiffCut consistently outperforms AutoSC on all datasets, with a gain up to \(+3.5\) on ADE20K, at the exception of COCO-Stuff where the latter yields slightly better results. Noting that AutoSC is already a state-of-the-art baseline on most benchmarks, this experiment confirms the relevance of the _recursive Normalized Cut_ to uncover arbitrary numbers of segments.

### Model Analysis

Hyperparameters Impact.In this section, we assess the impact of hyperparameters \(\) and \(\) over the segmentation performance. We report in Fig. 5 the mIoU for various \(\) values, with respect to partitioning threshold values \(\) ranging from 0.3 to 0.97 on Cityscapes validation set. As \(\) increases, we observe a dual effect. First, since a greater \(\) value shrinks the affinity matrix components towards 0, the partitioning cost corresponding to the _NCut_ value decreases, explaining the shift of the optimal threshold between the different curves. Second, as \(\) increases, the range of \(\) values for which the method yields competitive performance widens, contributing to the overall robustness of the method. For example, DiffCut outperforms our

   Model & **VOC** & **Context** & **COCO-Object** & **COCO-Stuff-27** & **Cityscapes** & **ADE20K** \\ 
**DiffSeg** & 48.2 & 41.2 & 31.7 & 35.4 & 22.3 & 39.9 \\
**AutoSC** & 61.5 & 53.3 & 29.8 & **46.9** & 25.3 & 38.9 \\ 
**DiffCut (w/o PAMR)** & **62.0** & **54.1** & **32.0** & 46.1 & **28.4** & **42.4** \\   

Table 2: **Ablation Study.** The _recursive_ partitioning of DiffCut yields superior results to both the self-attention merging process of DiffSeg and Automated Spectral Clustering.

Figure 5: **Sensitivity of DiffCut.** As \(\) increases, DiffCut shows competitive results for a broad range of \(\) values.

own competitive baseline AutoSC for any \(\) between 0.35 and 0.67 when \(=10\), whereas it only surpasses it between 0.92 and 0.96 when \(=1\). Qualitatively, we observe in Fig. 6 that as \(\) increases, the method uncovers finer segments in images.

Diffusion Features.Our chosen diffusion backbone uses a UNet-based architecture which consists of an encoder \(\), bottleneck \(\) and a decoder \(\). The hierarchical features of the encoder, with spatial resolution of \(128 128\), \(64 64\) and \(32 32\) respectively, are injected into the decoder \(\) via skip connections. Considering the final self-attention modules at resolution \(64 64\) and \(32 32\) in the encoder and decoder, we demonstrate in Tab. 3, that the encoder's features extracted at the lowest spatial resolution retain the most semantic information and are sufficient to reach optimal performance. In addition, combining different hierarchical features does not lead to any improvements and adds to the computational burden.

Open-Vocabulary Extension.To extend DiffCut to an open-vocabulary setting, we propose in Tab. 4, a straightforward approach that assigns a semantic label to each segmentation mask. After mask proposals are generated, an image is processed by a frozen _convolutional_ CLIP visual encoder, which produces visual representations aligned with text in a shared embedding space via a projection layer. The embedding of each predicted segment is obtained by mask-pooling CLIP visual features, allowing a classification against category text embeddings through contrastive matching. Specifically, let \(\) represent the embedding of a segment, and let \(\{t_{i}\}_{i=1}^{N}\) denote the text embeddings of category names generated by the pretrained text encoder, the predicted class for this segment is determined as follows: \(c=_{i\{1,,N\}}~{}softmax([cos(,t_{1}),cos(,t_{2}),,cos(,t_{N})])\). This proposed extension reaches competitive performance, even outperforming several baselines dedicated to the task of open-vocabulary zero-shot semantic segmentation.

## 5 Discussion

In this work, we tackle the challenging task of unsupervised zero-shot semantic segmentation by introducing DiffCut, a method that significantly narrows the performance gap with fully supervised

   Model & LD & **VOC** & **Context** & **COCO-Object** \\   \\ ViL-Seg  & ✓ & 37.3 & 18.9 & - \\ TCL  & ✓ & 55.0 & **30.4** & 31.6 \\ CLIPpy  & ✓ & 52.2 & - & 32.0 \\ GroupVIT  & ✓ & 52.3 & 22.4 & 24.3 \\ ViewCo  & ✓ & 52.4 & 23.0 & 23.5 \\ SegCLIP  & ✓ & 52.6 & 24.7 & 26.5 \\ OVSegmentor  & ✓ & 53.8 & 20.4 & 25.1 \\   \\ ReCO  & ✓ & 25.1 & 19.9 & 15.7 \\ MaskCLIP  & ✓ & 38.8 & 23.6 & 20.6 \\ CLIP-DIV  & ✓ & 59.9 & 19.7 & 31.0 \\ FreeSeg-Diff  & ✗ & 53.3 & - & 31.0 \\ 
**DiffCut** & ✗ & **63.0** & 24.6 & **36.0** \\   

Table 4: **Open-Vocabulary Segmentation. A straightforward open-vocabulary extension with a CNN-based CLIP yields competitive performance.**

Figure 6: **Effect of \(\). As \(\) corresponds to the maximum _Ncut_ value, a larger threshold loosens the constraint on the partitioning algorithm and allows it to perform more recursive steps to uncover finer objects. It can be interpreted as the level of granularity of detected objects.**

models. DiffCut leverages the diffusion features of a UNet encoder within a recursive graph partitioning algorithm to generate sharp segmentation maps, achieving state-of-the-art results on popular benchmarks. By reusing pre-trained models in a zero-shot manner, our approach can not only reduce computational resources, energy consumption, and human labor but also align with sustainable AI practices. However, because the diffusion backbone is not specifically trained for specialized domains, such as biomedical imaging, the method may struggle with out-of-distribution images. Fine-tuning the diffusion model on domain-specific data could mitigate this challenge. While fully supervised, end-to-end segmentation methods currently offer better efficiency and accuracy, further advancements could close this gap, positioning diffusion-based UNet encoders as foundation models for future vision tasks.