# VisAlign: Dataset for Measuring the Alignment

between AI and Humans in Visual Perception

 Jiyoung Lee\({}^{1}\), Seungho Kim\({}^{1}\), Seunghyun Won\({}^{2}\), Joonseok Lee\({}^{3}\), Marzyeh Ghassemi\({}^{4,5,6}\)

James Thorne\({}^{1}\), Jaeseok Choi\({}^{7}\), O-Kil Kwon\({}^{7}\), Edward Choi\({}^{1}\)

\({}^{1}\)KAIST, \({}^{2}\)Seoul National University Bundang Hospital, \({}^{3}\)Seoul National University,

\({}^{4}\)MIT, \({}^{5}\)University of Toronto, \({}^{6}\)Vector Institute, \({}^{7}\)Kangwon National University Hospital

\({}^{1}\){jiyounglee0523, shokim, thorne, edwardchoi}@kaist.ac.kr

\({}^{2}\)shwon0213@gmail.com, \({}^{3}\)joonseok2010@gmail.com, \({}^{4}\)mghassem@mit.edu

\({}^{7}\){gobibobia, okkwon}@kangwon.ac.kr

###### Abstract

AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. In this paper, we focus on the models' visual perception alignment with humans, further referred to as _AI-human visual alignment_. Specifically, we propose a new dataset for measuring _AI-human visual alignment_ in terms of image classification. In order to evaluate _AI-human visual alignment_, a dataset should encompass samples with various scenarios and have gold human perception labels. Our dataset consists of three groups of samples, namely _Must-Act_ (_i.e._, Must-Classify), _Must-Abstain_, and _Uncertain_, and further divided into eight categories. All samples have a gold human perception label; even _Uncertain_ (_e.g._, severely blurry) sample labels were obtained via crowd-sourcing. The validity of our dataset is verified by sampling theory, statistical theories related to survey design, and experts in the related fields. Using our dataset, we analyze the visual alignment and reliability of five popular visual perception models and eight abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign.

## 1 Introduction

AI alignment  seeks to align models to act towards human-intended goals , preferences , or ethical principles . Misaligned models may show unexpected and unsafe behaviors which can bring about negative outcomes, including loss of human lives . This is particularly true for high-capacity models like deep neural networks, where there is little manual control of feature interaction. In such cases, analyzing the alignment between models and humans can be a proxy measure for safe behavior . In this paper, we particularly focus on alignment in _visual_ perception, referred to as _AI-human visual alignment_, and propose a new dataset for measuring this alignment. Note that recent work in AI-human alignment tends to focus on societal topics with ethical implications, such as racial or gender bias . In this work, however, we use image classification as the target task, which is more fundamental to machine perception but is less contentious.

Image classification presents significant challenges for deployed visual AI systems. When confronted with an image lacking any object from the designated classes, humans typically abstain from making an incorrect decision. In contrast, machine learning models may still generate an output unless they are explicitly trained to abstain from making predictions under certain confidence levels. Similarly, when an image provides imperfect information (_e.g._, due to blurred vision or a dark environment), human decisions tend to waver between a correct prediction and abstention. Conversely, machines oftenmake overconfident predictions . Given this discrepancy between human and model behaviors, we focus on image classification as a foundational starting point.

As AI alignment aims to guide an AI to resemble human behaviors and values for a safe use of AI, _AI-human visual alignment_, being a subcategory of AI alignment, aims to guide the AI to resemble the aforementioned human behaviors in visual perception (_i.e._, abstaining from making incorrect decisions, waering between a correct prediction and abstention) to ensure safety across diverse use cases. Our dataset, VisAlign, encapsulates these behaviors across three distinct groups: _Must-Act_, _Must-Abstain_, and _Uncertain_. _Must-Act_ contains identifiable photo-realistic images that humans can correctly classify (see Figure 1 green box). _Must-Abstain_ includes images that most humans would abstain from classifying due to their lack of photo-realism or because they clearly contain no objects within the target classes (see Figure 1 red box). _Uncertain_ category hosts images that have been cropped or corrupted in diverse ways and at varying intensities (see Figure 1 orange box). For this last group, we provide gold human labels from multiple annotators via crowd-sourcing. In Section 3, we elaborate on requirements that a visual alignment dataset must meet and provide details about our survey design, which has been validated using relevant statistical theories. _Must-Act_ and _Must-Abstain_ have been addressed in previous studies under the purview of robustness [22; 72; 25] and Out-of-Distribution Detection (OOD) [50; 74], respectively. However, most studies overlook _Uncertain_ samples, which are frequently found in real-world scenarios where visual input can continuously vary in aspects such as brightness and resolution. To the best of our knowledge, VisAlign is the first dataset to explore the diverse aspects of visual perception, including _Uncertain_ samples, under the concept of _AI-human visual alignment_. Furthermore, all decisions regarding the construction of VisAlign were based strictly on statistical methods for survey design [64; 9] and expert consultations to maximize the validity of the alignment measure (see Section 3).

We benchmark various image classification methods on our dataset using two different metrics. Firstly, we measure the visual alignment between the gold human label distribution and the model's output distribution using the distance-based method (Section 4.1). Secondly, we evaluate the model's _reliability score_ (Section 4.2). We test models with various architectures, each combined with various ad-hoc abstention functions that endow the model with the ability to abstain. Our findings suggest that current robustness and OOD detection methods cannot be directly applied to _AI-human visual alignment_, thus highlighting the unique challenges posed by our task as compared to conventional ones.

Our contributions can be summarized as follows:

Figure 1: The overview of VisAlign. The example images are given with reference to the class Zebra. _Category 1_. A photo-realistic image of a zebra. _Category 2_. A zebra crossing a road. _Category 3_. A slight noise is added to the Category 1 image. _Category 4_. A picture of a truck. _Category 5_. A head and two limbs of an elephant with the remaining body of a zebra. _Category 6_. A donkey. _Category 7_. A zebra illustrated on a piece of clothing. _Category 8_. Two pictures, one with cropping and the other frosted glass blur, respectively, of a zebra.

* To the best of our knowledge, this is the first work to construct a test benchmark for quantitatively measuring the visual perception alignment between models and humans, referred to as _AI-human visual alignment_, across diverse scenarios (8 categories in total).
* We propose VisAlign, a dataset that captures varied real-world situations and includes gold human labels. The construction of our dataset was carried out meticulously, adhering to statistical methods in survey designs (_i.e_., the number of samples in a dataset , intra and inter-consistency in surveys , and the required minimum number of participants ) and expert consultations.
* We benchmarked visual alignment and reliability on VisAlign using five baseline models and seven popular abstention functions. The results underscore the inadequacy of existing methods in the context of visual alignment and emphasize the need for novel approaches to address this specific task.

## 2 Related Works

Related Datasets.Previous datasets only focus on one aspect or do not have human gold labels. Mazeika et al.  focus on subjective interpretations and collected human annotations on emotions (_e.g_., amusement, interest, adoration). Existing corruptions datasets [22; 43; 72] apply slight corruptions to study the robustness of deep neural networks. These works overlook the moderately or severely corrupted images that appear in the real world. Although the dataset by Park et al.  applied brightness corruptions on hand X-ray images with multiple severities, they do not have gold human labels. CIFAR10H  is a dataset that collects a distribution of soft human labels for CIFAR10 images  to represent human perceptual uncertainty. Similarly, Schmarje et al.  collected multiple annotations per image. There are three key differences that distinguish our dataset from prior works that focus on uncertainty in object recognition. First, we applied corruption and cropping with different intensities ranging from 1 to 10 to reflect the continuity of uncertainty. As uncertainty is continuous and it is critical to test models on samples where uncertainty may increase in stages. Second, we obtained 134 human annotations per image to obtain numerically robust annotations. Third, while previous dataset include soft labels distributed only among classes, we include soft labels distributed among classes and abstention, which can represent recognizability uncertainty (_i.e_.,, whether an image itself is recognizable or not). Visual perception includes not only object identification (predicting that it is an elephant) but also object recognizability (the object itself is recognizable). In this sense, we cover broader scenarios compared to previous works as we include object recognizability uncertainty in our uncertain category.

Visual Alignment with Humans.Alignment is more broadly studied, including the gap between data collection and model deployment , natural language modeling , and object similarity [29; 51]. For visual alignment, specifically, previous works [18; 19; 53; 77] use only corrupted or perturbed datasets to compare the humans' and models' decisions. Zhang et al.  and Bomatter et al.  show that both model and human have better object recognition when given more context information. Both papers provided human-model correlations to describe their relative trends across conditions. However, our study on visual perception alignment is not about following human trends, but about measuring how well the model replicates human perception sample-wise. Geirhos et al.  and Bhojanapalli et al.  test the robustness of models to perturbations that does not affect the object identity. Peterson et al.  only test their models on in-class (_i.e_., Category 1) and out-of-class samples (_i.e_., Category 4 and Category 6) and Schmarje et al.  only tested their models on in-class samples (_i.e_., Category 1). In order to thoroughly evaluate visual alignment, models should also be tested under various scenarios with out of distribution properties (_i.e_., Category 5 and Category 7). We prepared VisAlign to include these out of distribution properties, and if needed, generated the samples by ourselves, of which details are in Section 3.2. Furthermore, they showed only accuracy and cross entropy or KL divergence. (which is analogous to KL divergence) of the models. Therefore, they did not test their models on various possible scenarios and did not use proper measurement, as KL divergence is not an optimal choice for visual perception alignment as will be described in Section 4.1. Therefore, although previous works trained their models with the goal of achieving visual perception alignment, none of the works have thoroughly verified how much the models have actually achieved visual perception alignment under diverse situations with an appropriate measurement. In contrast, we quantitatively measured visual perception alignment across various scenarios with multiple human annotations on uncertain images. In addition, we borrowed Hellinger distance to precisely calculate the visual perception alignment after careful consideration of other distance-based metrics. More details of comparison to previous works are in Appendix J

## 3 Dataset Construction

We have carefully considered what conditions must be met in a visual alignment dataset during the process of selecting the classes and the contents of VisAlign. We define four requirements that a visual alignment dataset must satisfy:

* **Requirement 1: Clear Definition of Each Class.** Each class must be distinctly and precisely defined. This criterion proves more challenging to meet than initially anticipated, given that most everyday objects are defined in relatively vague terms and therefore do not lend themselves to rigorous classification. For example, the term "automobile," which is defined by the Cambridge Dictionary as a synonym for "car", is described as "a vehicle with an engine, four wheels, and seats for a few people."1 The phrase "seats for a few people" is ambiguous, and the definition is broad enough to encompass trucks. Despite this, certain parties may contend that "automobile" and "truck" are distinctly separate classes, a view reflected in datasets like CIFAR-10  and STL-10 , which treat automobiles and trucks as separate classes. * **Requirement 2: Class Familiarity to Average Individuals.** The classification target (_i.e._, each class) must be known to average people. This is because we employ hundreds of MTurk workers to derive statistically robust ground-truth labels for a subset of images.
* **Requirement 3: Coverage of Diverse and Realistic Scenarios.** Samples must cover a wide range of scenarios that are likely to occur in reality. This includes samples outside of defined classes, out of distributions (_i.e._, Category 5 or 7) and confusing samples where people might not able to recognize or identify. The test will fail to sufficiently evaluate the AI's alignment with human visual perception without this diversity.
* **Requirement 4: Ground Truth Label for Each Sample.** Each sample must have an indisputable or, at the very least, reasonable ground truth. Our dataset's ground truth is human-derived, as we aim to measure the degree of alignment between AI and human visual perception.

### Class Selection

For our dataset to serve as a universal benchmark that any model can be tested on, the classes should have clear definitions so that model developers can easily prepare their models and training strategy. To meet Requirement 1, we cannot choose under-specified class definitions. For example, the class definitions in CIFAR10  can be disputed, as shown in the example of 'automobile' and 'truck' in Requirement 1. MNIST  classes cannot be used since numbers are recognized via trivial geometric patterns. After careful consideration, we use the taxonomic classification in biology, which is the meticulous product of decades of effort by countless domain experts to hierarchically distinguish each species as accurately as possible. Following Requirement 2, familiarity is one of the critical criteria since we conducted an MTurk survey to build a subset of our dataset. Therefore, among animal species, we select mammals that are familiar to the average person.

In summary, animal species were selected that 1) can be grouped under one scientific name for clear definitions, 2) are visually distinguishable from other species to avoid multiple correct answers, 3) have characteristic visual features allowing them to be identified by a single image, and 4) are familiar to humans, facilitating participation in our survey. The final 10 classes are _Tiger_, _Rhinoceros_, _Camel_, _Giraffe_, _Elephant_, _Zebra_, _Gorilla_, _Kangaroo_, _Bear_, and _Human_. This selection was revised and verified by two zoologists according to the aforementioned criteria. The scientific names and subspecies for each class can be found in Table 4 of Appendix C.

### Sample Categories

Our dataset, depicted in Figure 1, is partitioned into three groups: _Must-Act_, _Must-Abstain_, and _Uncertain_. To avoid misclassifications due to background objects, all samples exclusively contain one object. The authors manually scrutinized all test samples to ensure this. In line with Requirement 3, these three groups are further subdivided into eight categories to account for as many real-worldscenarios as possible. Each category comprises 100 samples, with the exception of Category 8 comprising 2002, totaling 900 samples. To establish the reliability of the dataset as a valid benchmark, Cronbach's alpha  was used, a metric that evaluates the reliability of tests. The dataset was deemed reliable, with a minimum of 100 samples per category. The complete calculation for Cronbach's alpha is detailed in Appendix D.1.

* contains clearly identifiable photo-realistic samples belonging to only one of the 10 classes. We intentionally restricted our dataset to photo-realistic samples to avoid ambiguous boundaries between in-class and out-of-class, such as abstract paintings or sculptures (_e.g._,, claiming that a box with four sticks at the bottom and a sinusoidal line on the side is an elephant). Individuals with no visual impairments and familiarity with the 10 mammals can consistently classify these images correctly.
* Category 1: Unaltered samples from the designated classes are included. This category serves as the most basic step required for visual perception alignment. We sourced images from ImageNet1K  and images.cv3. * Category 2: Image classification models have been known to sometimes base decisions based on unrelated features, such as the background of an image . We aim to challenge the models by testing them with samples that feature incongruuous backgrounds, _i.e._,, images of animals in environments where they are not commonly seen. Well-aligned models should accurately classify objects regardless of the changes in the background. Samples were generated using Stable Diffusion . Examples of text prompts used for generating samples are provided in the Appendix D.2.
* Category 3: Another case of images that humans can easily identify but models cannot are perturbed images used for adversarial attacks . Well-aligned models would not be influenced by noise or adversarial attacks intentionally designed to deceive them. Here we include Category 1 samples with adversarial perturbation to test such cases. We use Fast Gradient Sign Method (FGSM)  to inject adversarial perturbations. The gradients are produced by pre-trained image classifiers available in PyTorch4. 
* are images that qualified individuals always abstain from classifying.
* Category 4: This category includes images that do not belong to any one of VisAlign's 10 mammals. Examples might include other animal species (e.g., birds, cats, dogs), textures (e.g., bubbly, banded), or objects (e.g., truck, inline skate, guitar). This category tests the model's ability to abstain from classifying objects outside its defined scope. Well-aligned models should be able to disregard infinitely diverse objects outside the target classes. The space of Category 4 is inexhaustible; thus, the authors use their best efforts to include as diverse samples as possible to represent this space. Samples were collected from ImageNet1K , Describable Textures Dataset , and Caltech 10 .
* Category 5: While Category 2 tests whether models focus on relevant features of the class definition, it is also important to assess if a model evaluates the object as a whole, rather than focusing on specific portions of a sample. Thus, we included images of creatures that incorporate features from two different animals (e.g., a creature with the head and two limbs of an elephant but the body of a zebra). Recent advances in text-to-image models  enable us to rapidly and easily generate images of objects that do not naturally exist. We used Stable Diffusion  to create these images. Details of prompts are in Appendix D.2.
* Category 6: An image may contain an object that does not belong to the target class but has features closely resembling those of the target classes. Given the challenging nature of these near-miss cases, we include Category 6, featuring mammals that are biologically close to the 10 target mammals according to scientific taxonomy (e.g., donkeys are close to zebras). The primary purpose of Category 6 is to test the model abstention ability on seemingly similar yet different samples. This category can be considered a more challenging version of Category 4. We have set aside this category as these samples can check the model visual alignment on samples near the natural evolutionary boundary. Samples are collected from ImageNet21K .
* Category 7: This category includes images in styles other than photo-realistic (e.g., a drawing of an elephant, a sculpture of a giraffe). Considering that MUST-ACT samples are photo-realistic images confirmed by humans, well-aligned models should be able to discern styles that deviate from photo-realism. The images were collected from DomainNet  and ImageNet-R .

* includes images that are cropped or corrupted in various styles in different intensities
* Category 8: This category includes images that are either cropped at varying sizes and regions or corrupted using one of the 15 corruption types5. The original samples were collected from ImageNet21K . Well-aligned models should be able to correctly classify slightly corrupted images while abstaining from making decisions on indistinguishably corrupted images. The corruption process follows the approach outlined in ImageNet-C , with corruption intensities varying from 1 to 10. 
### Uncertain Group Label Generation

One challenging aspect of the _Uncertain_ group is the variability of these samples' gold standard labels, which fluctuates depending on corruption types and intensities. For instance, it would be optimal to correctly classify images with slight corruptions when identifiable. However, given a severely darkened image, the object might resemble a tiger, a jaguar, or be entirely unrecognizable. In such scenarios, determining whether a human observer would classify it as a tiger or abstain from decision-making becomes challenging. Therefore, we derive a gold human ratio (_i.e._, the distribution over classes provided by human annotators), rather than assigning one label per image as in _Must-Act_ and _Must-Abstain_, because human perception of an image can vary, and approximating the ratio for each image offers the best test of alignment6. To derive the gold ratio across the 11 classes (10 mammals + abstention), we employ MTurk workers to classify images in the _Uncertain_ group.

Every MTurk worker is asked to classify 35 images, including Category 4 images corrupted with a severity between 1 to 10, with 10 being distractors. This is to minimize MTurk workers' potential biases; _e.g._, a severely dark image can be perceived as anything other than the 10 mammals. After reviewing the task description and image samples for each class, MTurk workers select either one of the 10 mammals or an option labeled "None of the 10 mammals, uncertain, or unrecognizable", which is equivalent to abstention. To ensure the quality of samples, we disregard MTurk results where anything other than abstention was chosen for the distractor images.

In accordance with Requirement 4, we ask 134 individuals per image to estimate the indisputable ground truth distribution within an error bound of 5%, following the survey sampling theory. Proofs are provided in Appendix F. Additionally, we calculate the Fleiss' Kappa  to assess two types of consistency among the MTurk workers' answers: intra-annotator and inter-annotator consistency. Intra-annotator consistency measures the consistency of a single worker's responses. To calculate this, we inserted two sets of identical images in random order. If a worker selects the same answers for these identical images, we consider the worker's responses to be consistent. Inter-annotator consistency, on the other hand, measures the agreement among different workers. Our results show an intra-annotator consistency value of \(=0.91\), indicating almost perfect agreement, and an inter-annotator consistency value of \(=0.80\), demonstrating substantial agreement. Details on survey instructions, response filtering process, and participant statistics are provided in Appendix F.

### Dataset

We prepare three datasets: the train set, the open test set, and the closed test set. The train set is a subset of ImageNet-21K , consisting only of Category 1 samples. By doing so, we ensure the trained models are tested on a variety of unseen categories, reflecting a real-world scenario. For each of our 10 classes, we randomly sample a uniform amount of images from all related ImageNet-21K classes. We collected a total of 1250 images per class, using one-tenth of this data for validation. The creation processes of both the open and closed test sets are identical, as described above. We provide the open test set to allow developers to evaluate their models' visual perception alignment. Developerswishing to evaluate their models on the closed test set can submit their models to us. Table 1 presents a comparison of VisAlign and other datasets in terms of fulfilling the four requirements.

## 4 Metrics

We introduce a distance-based metric to measure _AI-human visual alignment_. Furthermore, we present a reliability score table to explore the correlation between a model's visual perception alignment and model reliability.

### Distance-Based Visual Perception Similarity Metric

We propose a distance-based metric to measure the distance between two multinomial distributions: the human visual distribution and the model output distribution over 11 classes (10 mammals + abstention). We opt for a distance-based metric for two reasons: 1) it does not depend on additional hyperparameters such as abstention threshold, and 2) comparison across all classes, rather than solely on the true class, provides a more accurate measure of visual alignment. For example, consider a _Must-Act_ tiger sample with the gold human label as a one-hot vector for the label _tiger_. Suppose one model outputs a probability of 0.7 for _tiger_ and 0.3 for _abstention_, and another model yields a probability of 0.7 for _tiger_ and 0.1 for _zebra_, _elephant_, and _giraffe_ respectively. These two models differ in visual perception alignment: the former is uncertain between two classes, whereas the latter is indecisive among four classes. If we were to consider only the gold label's probability, both models would yield the same result, which would not accurately represent visual alignment. Hence, we employ a distance-based metric calculated across all 11 classes, as opposed to using the maximum or gold label probability.

Specifically, we employ the Hellinger distance  to measure the difference between the two probability distributions as summarized in Eq. 1. Compared to other metrics for comparing two multinomial distributions, Hellinger distance produces smooth distance values even for extreme (_e.g_., one-hot) distributions (unlike KL Divergence ) and considers all classes while calculating the distance (unlike Total Variation distance). For instance, given a human visual distribution of [1., 0., 0.] and two model output distributions [0.3, 0., 0.7] and [0.3, 0.4, 0.3], the two output distributions would have the same KL Divergences with the human distribution while they have different Hellinger distances. Hellinger distance accounts not only for the gold label probability but also for the probabilities of all other labels. Additionally, as its range lies between 0 and 1, it provides an intuitive indication of model alignment.

\[h(P,Q)=}_{i}_{i}-_{i}_{2}\] (1)

### Reliability Score with Abstention

We also assess the model's reliability based on its final action. This process involves two steps. First, a model abstains if the abstention probability surpasses an abstention threshold, \(\); otherwise, it makes a prediction. Next, if a model decides to act, its prediction is one of the 10 mammal classes

   Dataset & Req. 1 & Req. 2 & Req. 3 & Req. 4 \\  ImageNet-C  & ✗ & ✗ & \(\) & ✓ \\ ImageNet-A  & ✗ & ✗ & ✗ & ✓ \\ OpenOOD  & ✗ & ✗ & \(\) & ✓ \\ Background Challenge  & ✗ & ✗ & ✓ \\ MNIST  & ✗ & ✓ & ✗ & ✓ \\ CIFAR10  & ✗ & ✓ & ✗ & ✓ \\ CIFAR10H  & ✗ & ✓ & \(\) & ✓ \\ PLEX  & ✗ & ✗ & ✓ & ✓ \\ Park et al.  & ✓ & ✗ & \(\) & ✗ \\ DCDC  & ✗ & ✗ & \(\) & ✓ \\  VisAlign & ✓ & ✓ & ✓ \\   

Table 1: The comparison between VisAlign and other related datasets on the requirements we define. \(\) indicates that only a subset of our scenarios are covered.

with the highest prediction probability. Table 2 details the reliability scores for each case. We devise separate metrics for _Must-Act_ and _Must-Abstain_ instances. For _Uncertain_ samples, they are treated as _Must-Act_ if the probability of the original label exceeds a threshold \(\); otherwise, they are treated as _Must-Abstain_. We set an initial \(\) value at 0.5, but this can be adjusted according to the specific objective. We denote the reliability score as \(RS_{c}(x)\), where \(c\) is the cost of an incorrect prediction. The main criterion for assigning scores is the consequences of the model's decision. The model earns a score of 1 per prediction when it aligns best with human recognition: making a correct prediction in Must-Act and abstaining in Must-Abstain. On the other hand, if the model's decision is erroneous and could potentially result in significant cost--in our case, a wrong prediction--the model receives a score of \(-c\). A score of zero indicates that the prediction is neither beneficial nor detrimental. Original Label Prediction is a special case only applied for Uncertain samples treated as Must-Abstain. In this case, a model correctly classifies a corrupted image that most humans cannot recognize. Although most humans disagree with the model's decision, it does not have a negative impact since it is a correct answer. The total score, \(RS_{c}\), is the summation over all test samples, \(_{i}RS_{c}(x_{i})\).

The proper value of cost \(c\) depends on the industry and the use case. \(c\) can be seen as the "strictness criterion for a reliable model" and can also be interpreted as "how many misclassifications correspond to a single accurate classification." \(c\) can be set as an integer ranging from 0 to the total size of the test set. A value 0 for \(c\) implies a 0% strictness, while the maximum value of \(c\) implies a 100% strictness. This means that even a single mistake would result in a negative score, and abstaining from all decisions on Must-Act samples would be deemed more reliable than making even one incorrect prediction. We designed this metric to enable both absolute and relative reference points. As an absolute reference point, if the final score is at or above 0 (non-negative reliability score), it demonstrates that the model satisfies the user-defined minimum reliability. A relative reference point is between different models; a model with a higher score between two reliability scores is more reliable. In this paper, we set the value of \(c\) as 0, 450, or 900.

## 5 Experiment

### Experiment Settings

We perform experiments with Transformer-based , CNN-based , and MLP-based models. We use ViT  and Swin Transformer  for Transformer-based models, and DenseNet  and ConvNeXt  for CNN-based models. For the MLP-based model, we use MLP-Mixer . All models are trained on our train set and tested on the open test set.

We chose abstention functions that satisfy the following three conditions: 1) must be applicable on any model architecture, 2) do not require OOD or other Must-Abstain samples during training, and 3) do not require a supplementary model. We first calculate the abstention probability using each function, then re-normalize the 10-class prediction probability so that the sum over the 11 classes becomes 1. Since not every function outputs the abstention probability between 0 and 1, we designed a smaller version of the dataset with the identical gather process to test set to use for normalizing the abstention probability.

* Softmax Probability (SP) regards the entropy among the 10 classes as abstention probability.
* Adjusted Softmax Probability (ASP) acts the same as SP, but it applies temperature scaling and adds perturbations to the input image based on the gradients to decrease the softmax score. This method is inspired by ODIN .
* Mahalanobis detector (MD)  determines abstention probability based on the minimum Mahalanobis distance  calculated from each class distribution's mean and variance.

   Sample Type & Model Action & \(RS_{c}(x)\) \\   & Correct Prediction & \(+1\) \\   & Incorrect Prediction & \(-e\) \\   & Abstention & 0 \\   & Original Label Prediction* & \(0\) \\   & Other Prediction & \(-e\) \\    & Abstention & \(+1\) \\   

Table 2: Reliability score table. The optimal outcomes earn a score of 1. Abstention in _Must-Predict_ and Original Label Prediction in _Must-Abstain_ get 0. The worst case receives \(-c\), where \(c\) is the cost value. *Note that the original label prediction can only happen in Uncertain samples that fall under Must-Abstain.

[MISSING_PAGE_FAIL:9]

similar visual alignment performances, predominantly ranging from 0.5 and 0.6. We conjecture the reason comes from that all models are struggling in approximating the overall ratios across 11 classes compared to _Must-Act_ and _Must-Abstain_, where models only need to correctly predict a single class. The difficulty of achieving visual perception alignment in _Uncertain_ suggests that there is room for improvement. KNN  has the best visual alignment across all categories on average. This might be because KNN can capture more fine-grained features than other distance-based abstention functions, as it calculates the distance between samples, not clusters. We also compute three reliability scores with \(c\) set to 0 (\(RS_{0}\)), 450 (\(RS_{450}\)), and 900 (\(RS_{900}\)). The resulting ratios of each action type are shown in Appendix G.1. Here, \(c=0\) indicates no negative impacts from incorrect predictions, while \(c=900\) suggests that a single incorrect prediction outweighs the remaining correct predictions. It is worth noting that reliability scores in \(RS_{450}\) and \(RS_{900}\) are mostly negative, suggesting that current models and abstention functions are not perfectly safe to be deployed in the real world. Notably, visual alignment distance is correlated with reliability score as can be seen in Appendix G.2.

Methods based on the minimum distance from each class (MD, KNN, and TAPUDD) generally show a worse visual alignment on _Must-Abstain_ categories. We conjecture that the reason comes from using the shortest distance to in-class clusters. If an embedding contains one clear in-class feature, the distance to the corresponding class would be short, leading the model to make a prediction. On the other hand, methods based on entropy or uncertainty show weak alignment on _Must-Act_ categories. With these methods, the model has to be not only confident that its predicted class is correct but also that the remaining classes are incorrect. Considering the confidence in all classes makes it more challenging for visual alignment in _Must-Act_ categories. An abstention function which takes advantage of both distance-based and probability-based methods is needed to perform well on visual alignment. The distance should be sample-wise to capture the nuanced characteristics of the samples. Overall, our experiments show that no methods perform well across all categories. There is much room for improvement in visual alignment, a field in which our dataset will become an essential tool for benchmarking new methods.

## 6 Conclusion

To the best of our knowledge, this is the first work to construct a test benchmark for quantitatively measuring the visual perception alignment between models and humans, referred to as _AI-human visual alignment_, across diverse scenarios. Our dataset is divided into three main groups and eight categories, each representing unique and essential situations. Our dataset includes gold human labels for each image, with some of these labels collected via MTurk survey. We benchmarked five baseline models and seven popular abstention functions, and our experimental results show that no current methods perform well across all categories. This suggests there is room for improvement in visual alignment. We believe VisAlign can serve as a universal benchmark for testing visual perception alignment and that our work has potential applications in both social and industrial contexts.

Despite our best efforts to construct VisAlign, there are some limitations. First, the number of classes is relatively small compared to other datasets since we collected 134 annotations per image and chose classes that would be familiar to an average human. Note that it is always challenging to collect gold human labels in any domain. For example, in diagnosing chest X-rays, the typical number of diseases is 14. To collect the ground truth labels within a statistical error bound of 5%, one would need to consult at least 107 radiologists. Therefore, more practical solutions are required to measure alignment in specialized domains. Another limitation comes from the nature of uncertainty. We acknowledge that uncertainty is continuous and it is hard to distinguish between clear and uncertain images. Although we put significant effort to include only clear images in Must-Act and Must-Abstain and obtained human annotations on Uncertain images, there is a possibility of corner cases where at least one person disagrees. Furthermore, synthetic corruptions cannot cover all uncertainties arising in the real world. However, uncertainty is too broad to specify and difficult to collect or generate, thus for now we use corruptions. We put our best effort to reflect the continuity of uncertainty by varying corruption intensity from 1 to 10 and include some corruptions that can arise in the real world (_e.g._, pixelation). We detailed further discussions on uncertainty in Appendix I. Also, extending visual alignment to scenarios such as visual illusions may also be introduced. While our dataset focuses on the essential object identification and abstention task under _AI-human visual alignment_, future work can be expanded to potentially contentious but socially engaging topics such as gender or racial bias and other vision tasks such as object detection and segmentation.