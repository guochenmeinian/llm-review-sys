# RL on Incorrect Synthetic Data Scales

the Efficiency of LLM Math Reasoning by Eight-Fold

 Amrith Setlur\({}^{*,1}\) Saurabh Garg\({}^{1}\) Xinyang (Young) Geng\({}^{2}\) Naman Garg\({}^{3}\)

**Virginia Smith\({}^{1}\) Aviral Kumar\({}^{1,2}\)**

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Google DeepMind \({}^{3}\)MultiOn

###### Abstract

Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or _positive_ problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data **doubles** the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize _negative_ responses, _i.e._, model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this _per-step_ scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by \(\). We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.

## 1 Introduction

Training large language models (LLMs) relies on the ability to train on large amounts of high-quality data. It is predicted that we will run out of high-quality internet data by 2026 , necessitating training on model-generated data, or what is commonly referred to as _synthetic data_. Recent trends illustrate that scaling up synthetic data can lead to improvements  on hard reasoning problems, while other results illustrate that training on synthetic data can steer the performance of the model into a downward spiral --amplying biases, misinformation, and undesired stylistic properties. Thus while _in principle_, synthetic data could potentially address data scarcity, it must be designed in an appropriate manner to be effective. However, this has been hard due to a lack of an understanding of how synthetic data contributes to LLM behavior.

To provide clarity on how synthetic data contributes to performance, we aim to understand its impact on LLM capabilities via a study on math reasoning, a prevalent scenario where synthetic data is used. Typically, in this setting, synthetic data corresponds to correct or _positive_ model-generated responses for a novel set of initial problems synthesized by prompting capable models . The resulting model is then evaluated on a held-out set of problems drawn from a test set. Perhaps as expected, we find that performance improves when finetuning models on positive synthetic responses, though the scaling rates for performance improvement are often substantially slower than those observed during pretraining. Concretely, we find that under the scaling law of Zhang et al. , the error rate scales as \(\)\(D^{-0.05}\) to \(D^{-0.15}\) in the size \(D\) of synthetic dataset. Second, we observe that not all typesof positive synthetic data are equally effective: often positive responses self-generated by the learner itself are as effective as \(2\) synthetic data from bigger models in improving performance. This is because responses from a similar model are "easier-to-fit" than those from a more capable model, resulting in reduced memorization [26; 56] during finetuning. We also observe that if the positive response contains incorrect/irrelevant intermediate steps, training on such data often incentivizes the model to overfit on spurious correlations, leading to a flat or even inverse scaling with more data.

Perhaps surprisingly, we find that the aforementioned pathologies of training on positive data only can be addressed if we also utilize synthetic _negative_ responses: responses generated by the model that do not result in obtaining a correct final answer. One way to utilize negative responses is via methods such as direct preference optimization (DPO) . While performance of standard DPO  largely flatlines as the number of synthetic problems are scaled up (Figure 5), we are able to attain consistent improvements if the negative data is generated appropriately. A solution trace for a math problem typically comprises of multiple _reasoning steps_ corresponding to intermediate results. Our insight is that instead of contrasting arbitrary correct and incorrect responses, we should contrast those positive and negative responses that depict good and bad choices for the more "critical" intermediate steps: steps that the model must carefully produce so as to succeed at the problem. In other words, critical steps are those which the model is unable to recover from, and hence, must be emphasized. With this scheme, we are able to attain consistent gains over only positive data, **attaining performance similar to scaling up positive synthetic data by \(8\).** We show that training on this sort of negative data evades spurious steps amplified by training on positive data alone.

To theoretically understand our findings, we build a conceptual model of how training on this data benefits performance. Formally, we show that this construction of negative data, which emphasizes "critical" tokens (Figure 6) enables us to perform credit assignment, and is equivalent to training the model with per-step advantage-weighted reinforcement learning (RL)  on a mixture of positive and negative synthetic data. Specifically, these advantage values are computed under an optimal value function induced by sampling multiple responses under the SFT policy obtained by training on only the positive data. This reduction of using negative data to advantage-weighted RL enables us to conceptually compare it to training on positive data, which corresponds to imitation learning (_i.e._, behavioral cloning) on positive data. First, we are able to argue for the generalization gains of advantage-weighted RL through the lens of distribution robust objectives. Second, building on theoretical results in RL , we are also able to show that when advantages can be estimated reliably, advantage-weighted RL will be significantly more sample-efficient compared to imitation. Overall, this model explains the utility of negative data over only positive data.

Our contribution is a study of the role of synthetic data in improving math reasoning capabilities of LLMs. We derive scaling laws for positive and negative data on common reasoning benchmarks and observe that: **(a)** training on positive synthetic data from capable models results in scaling rates that are significantly slower than standard empirical risk minimization; **(b)** training on model-generated positive synthetic data can improve sample efficiency by \(2\) but also amplifies spurious correlations; **(c)** appropriate ways of constructing learner-specific negative data with emphasis on critical steps, results in a performance boost equivalent to scaling up positive data \(8\); **(d)** training with negative data provides a mechanism to unlearn spurious correlations; and **(e)** we present a conceptual model inspired from RL to explain our observations on synthetic data and the generalization benefits we see.

## 2 Related Work

A standard procedure to finetune a pretrained LLM is teacher-forcing on expert data, _i.e._, maximizing the likelihood of the next token given all previous tokens [7; 61]. In Appendix G we discuss some failure modes of this procedure for math reasoning that positive or negative synthetic data can address.

**Positive synthetic data.** Learning theory dictates that the SFT policy trained on more SFT data (_e.g._, 1.5M for DeepSeek-Math ) would have improved math reasoning capabilities. Thus, a

Figure 1: _Positive and negative synthetic data:_ Pictorial representation of positive/negative synthetic data definitions we use and how they are fed to SFT, RFT and DPO.

common goal for generating synthetic data as close as possible to the SFT data [29; 31; 32]. That said, generating high quality math data can be challenging, since verification can often be hard. When synthetic data is verified by larger models [50; 59], recent works [33; 66] observe scaling similar to finetuning LLMs on expert data [69; 71], while another work  notes the compositional gains from SFT data for code generation. Common sources of "good" synthetic data include responses from stronger teachers [29; 30], or data generated by the SFT policy itself, in the framework of reinforced self-training (ReST) and STaR [8; 52; 69; 70]. In our work, we study and compare the performance scaling with positive synthetic data from bigger models like GPT-4 and Gemini 1.5 Pro with self-generated positive data. We connect our findings to evidence showing "ease of learning" generalizable features on self-generated completions  which often prevents undesirable memorization . Finally, our work also sheds light on several concerns about training on synthetic positive data amplifying biases [48; 63], and leading to model collapse [13; 17], especially due to overfitting on"spurious" intermediate steps. We conceptually explain this phenomenon and also discuss how negative model-generated responses can help identify and unlearn those spurious steps.

**Benefits and nuances of negative synthetic data.** While most works on synthetic data [29; 32; 66; 69] train only on correct answers, our work also studies complementary gains from incorrect completions generated by the SFT policy [23; 38; 68; 39]. To leverage sub-optimal negative data, we adopt the framework of offline preference optimization [16; 41; 73], where a preference pair is constructed using correct and incorrect responses for the same problem . Despite numerous studies on preference data composition [8; 9; 10; 37; 54; 55; 60], its unclear how to pose a reasoning problem as a preference optimization problem. Randomly pairing correct and incorrect completions in a preference pair can lead to poor performance [21; 38; 39; 64] due to objective mismatch [55; 72] and requires auxilliary losses to perform well. Another option is to use negative data for training verifiers [22; 65] but this line of work still only trains the policy using positive data. We introduce a conceptual model of negative data, where we understand how certain choices of negative data can assign per-step credits, which we use to establish the equivalence of preference optimization to advantage weighted RL. Self-explore method in Hwang et al.  can be viewed as an special instance of our general framework. Other works [34; 59] exploit per-step credit assignment through tree-based sampling. They identify the reasoning subsequence that led to the most incorrect answers under the SFT policy for training a reward model. While this is related, our conceptual model and analysis also understands why assigning per-step credits can generalize better by unlearning spurious correlations, _e.g.,_ when the credits are given by the Q-function of the "best-of-K" SFT policy.

## 3 Problem Setup and Synthetic Data Generation Pipeline

Building on the recipe of Li et al. , Liu et al. , we use GSM8K  and MATH  to collect synthetic data consisting of both novel problems designed by capable models such as GPT4  and Gemini 1.5 Pro , and responses to these problems, obtained from the same models.

**Synthetic data pipeline.** First, given a dataset \(_{}=\{(_{i}^{r},_{i}^{r})\}\) of problems \(_{i}^{r} p_{}()\) and solution traces \(_{i}^{r} p_{}(_{i})\), we prompt one of the highly-capable models with a uniformly random sample \((_{i}^{r},_{i}^{r})_{}\) and ask the model to generate a new problem \(_{i}\) such that it is similar to the real problem \(_{i}^{r}\), in a way that a feasible solution exists. Second, we ask the model to provide a solution trace answer \(_{i}\) with step-by-step reasoning (exact prompts for \(_{i},_{i}\) are borrowed from Li et al. , shown in Appendix H). We assume that the answers generated via this process are accurate, and perform lightweight filtering step to remove duplicates, badly-formatted answer traces, and model failures. Based on the above, for any synthetic problem and solution pair \((,)\), we can define a binary reward function \(r(,})\{0,1\}\), which verifies if a new solution trace \(}\) is correct or not. This is implemented with a set of answer extraction and string matching tools borrowed from [29; 66]. We say that a new trace \(}\) is a _positive_ trace if it produces the correct final answer _i.e._, \(r(},)=1\), and _negative_ if it produces an incorrect final answer, _i.e._, \(r(},)=0\). By definition, \(r(,)=1\), and the original trace \(\) is always positive.

**Positive and negative datasets.** The above process induces a joint distribution \(p_{}(,)\), _iid_ samples from which yields positive synthetic dataset \(_{}\). We note that the sampling process for \(_{}\) is designed to ensure that the induced marginal distribution over synthetic problems \(p_{}()\) is close to \(p_{}()\). We will use \(_{}^{+}\) to denote the positive dataset of \((,+})\) where \(+}\) is a positive solution trace generated from some policy \(()\). For a positive \(+}\) and negative \(-}\) trace, sampled from the same policy \(()\), we denote a dataset over problems and solution pairs: \((,+},-})\) as \(_{}^{}\).

**Reasoning steps.** The trace \(_{i}\) consists of several intermediate steps, \(_{i}\) = \(_{i,1},,_{i,L}\). We assume each trace has at most \(L\) steps, and use \(_{1:t}\) to denote the subsequence of first \(t\) steps. Since mathematical reasoning problems require step-by-step computation, simply arriving at an incorrect final answer does not mean that all steps in a negative \(}\) are incorrect. Similarly, a positive \(}\) may also have incorrect reasoning steps. In fact, even the original answers generated by more capable models in \(_{}\) may also contain incorrect reasoning steps, and training on such traces may actually lead to unintended consequences (Section 5).

## 4 Learning from Synthetic Data

In this section, we discuss various algorithms for learning from the synthetic dataset \(_{}\) discussed in the previous section, as well as positive and negative solution traces generated using a model.

**Supervised and rejection finetuning (SFT and RFT).** Given positive synthetic \(_{}\), perhaps the most straightforward approach (and the most prevalent) is to learn \(_{}\) on this data via supervised next-token prediction: \(_{}(|)\) := \(_{}_{,_{}}[ (|)]\). Another option is to train via supervised next-token prediction on problems in \(_{}\), but when using a positive solution trace \(}\) sampled from \(_{}(|)\), instead of positive synthetic responses from the capable models in \(_{}\). Akin to rejection finetuning (RFT  or STaR ), sampling from \(_{}()\) once is not guaranteed to give a positive response, and we instead sample \(M\) times for each \(\) and construct the dataset \(^{+}_{_{}}\) of SFT policy generated positive responses. Then, we apply the next-token prediction loss on \(^{+}_{_{}}\).

**Preference optimization.** Beyond positive data, we can also learn from negative synthetic data generated from the SFT policy, especially when contrasted with positive responses. However, learning from negative data presents multiple open design questions pertaining to the construction of negative traces, and the choice of the loss function, and simple supervised fine-tuning will not be a good choice since it will incentivize the model to produce more errors. Therefore, we use a contrastive training approach, direct preference optimization (DPO ) for incorporating negative data from \(_{}\). In a nutshell, DPO trains a policy using the following preference optimization objective:

\[_{}\ \ _{}()\] (1)

We consider two objectives that construct negative data and subsequently optimize Equation 1. The first variant is _standard DPO_, which samples negative data \(-}\) from the \(_{}\) (with rejection sampling) and adds \((,,-})\) to \(^{}_{_{}}\). The second variant is _per-step DPO_, which first samples a complete solution trace \(}_{1:L}\) from \(_{}\) and then determines the "first pit" \(}_{c}\). The first pit \(}_{c}\) is the step where any completion following the step: \(}_{c+1:L}_{}(,}_{1:c})\) leads to incorrect answers in expectation under \(_{}\). The triplet \((,,}_{1:c})\) is added to the preference dataset \(^{}_{_{}}\).

## 5 Positive Data Improves Coverage, But Amplifies Spurious Correlations

We first analyze the influence of scaling up positive synthetic data on GSM8K and MATH. In this experiment, we fine-tune DeepSeek-Math-7B  and LLama2-7B  models (details in Appendix J) on varying sizes of \(_{}\), constructed out of a 5:1 mixture of GPT-4-turbo  and Gemini-1.5 Pro . We obtain a series of SFT policies on this data scaling ladder. We then train a series of models by running one iteration of RFT on data obtained from the SFT policies at each step.

**Scaling results with positive synthetic data GPT-4 and Gemini 1.5 Pro.** Since we assume that the more capable models generate correct solutions for new problems, by scaling \(_{}\) we are increasing _coverage_ under \(p_{}\), _i.e._, adding new \(,\) with non-zero probability under \(p_{}\). In Figures 2(a,b), we plot the test error rate of the SFT policy as \(_{}\) is scaled. As expected, we observe that the test error rate on both GSM8K and MATH improves with more positive data. Further, by simply fitting the parametric scaling law from , for \(D\) := \(|_{}|\), we find that the scaling trends decay as \(\!\!D^{-0.15}\) on GSM8K and \(\!\!D^{-0.05}\) on the harder MATH dataset, with similar trends for the corresponding pass@\(5\) error rates. Since these scaling trends are much more underwhelming than those for pre-training , this perhaps implies that samples in \(_{}\) are indeed improving coverage over samples in \(p_{}(,)\), but maybe not as efficiently as sampling _iid_ samples directly from it.

**Scaling results with positive synthetic data from 7B SFT policy.** Previously, we scaled problems in \(_{}\) by querying GPT-4 and Gemini-1.5. Now, for existing problems in \(_{}\) we generate new responses by sampling from the \(_{}\) trained on problems+solutions in \(_{}\). For any \((,)_{}\)we generate verified positive solution traces \(}_{}\) s.t. \(r(},)=1\). Following Yuan et al. , to ensure we sample enough correct responses, we sample \(100\) times from \(_{}\) and generate RFT datasets \(^{+}_{_{}}\), where each problem has almost \(4\) correct and diverse solutions. Next, we finetune the pretrained DeepSeek-Math-7B model on these new series of RFT datasets and plot the performance on GSM8K and MATH (Figure 2(a,b)). First, we observe that for any size of \(_{}\), the performance of the RFT model is better than the corresponding SFT model, and the difference remains consistent as we scale \(_{}\). Surprisingly, this indicates that training on positive answer traces from the 7B \(_{}()\) can lead to better performing policies than capable models.

**What is the value of positives from \(_{}()\)?** If sampling from \(_{}\) also improves coverage and performance, then should we scale problems and solutions in \(_{}\), or just solutions in \(^{+}_{_{}}\)? To answer this, we need to assign a value to the RFT dataset \(^{+}_{_{}}\) in terms of \(|_{}|\). We do this by training SFT policies on \(_{}\) of sizes 8k and 16k, and then generating RFT datasets from the corresponding SFT policies where we only add more correct solution traces (for the same problems) and scale RFT data from 10k to 128k (unlike RFT data in Figure 2(a,b) where both questions and answers scale). In Figure 2(c) we plot the error rate of DeepSeek-Math-7B finetuned on the different sizes of \(^{+}_{_{}}\). Comparing the lowest values of the curves in Figure 2(c) with \(_{}\) scaling in Figure 2(a,b), we note that performance from \(^{+}_{_{}}\) is \(2\) the size of \(_{}\) used to train \(_{}\). We also note that performance can plateau (or worsen in the case of GSM8K) as we scale up \(^{+}_{_{}}\) by a lot. This is because \(r(,)\) is unable to verify the correctness of each step in the positive solution traces in \(^{+}_{_{}}\). Later, we see how incorrect steps induce spurious correlations that get amplified as we scale positive data, explaining this drop. See Appendix C for more discussion.

**Why is self-generated positive data more sample-efficient?** From our result above, we find that solutions sampled from \(_{}\) (trained on \(_{}\)) yield better models, as good as those trained on \(2|_{}|\). This finding is surprising since one might expect more capable GPT-4/Gemini models to present better solutions, training on which should lead to good performance, akin to distillation , but this is not the case. Our results are consistent with the study of memorization in LLMs [18; 26; 56], which shows that pretrained (base) LLMs tend to memorize "hard-to-fit" and "out-of-pretraining-distribution" responses during finetuning, resulting in imperfect generalization. In contrast, correct response traces produced by \(_{}\) on problems from \(_{}\) are not as hard-to-fit or as out-of-distribution, since they are obtained from a model that is "close" to the base LLM. We confirm this hypothesis with a histogram of negative log-likelihood values of the SFT and RFT data under the base LLM (Figure 3). Hence, we expect STaR/RFT to alleviate the memorization problem on a large chunk of examples. This finding also corroborates Yuan et al. 's result that lower the perplexity of SFT data under the base model, the smaller the gap between SFT and RFT performance. Note that one may also attribute better performance of RFT to improved coverage from multiple answers in \(^{+}_{_{}}\) for each question in \(_{}\). But, we find that even when RFT data is restricted to one solution per question, LLM trained on it outperforms SFT consistently by > \(1\%\). Since verification is cheap, we can sample more solutions and also benefit from coverage.

Figure 3: Under base LLM, \(^{+}_{_{}}\) has higher likelihood than \(_{}\).

Figure 2: _Positive data scaling laws:_ On GSM8K (a) and MATH (b), we evaluate SFT trained on \(_{}\) and RFT that uses SFT policy generated positives (\(^{+}_{_{}}\)), as we scale \(_{}\), observing \(^{+}_{_{}}\) to be \(2\) as \(_{}\). In (c), we plot performance of RFT the number of correct solutions in \(^{+}_{_{}}\) are scaled, for a fixed set of 8k/16k problems from \(_{}\), observing that scaling model positives can amplify spurious correlations.

**SFT/RFT policy suffers from spurious correlations in positive synthetic data.** While RFT data maybe "easier-to-fit", in Figure 2(c) we also note that continuing to scale RFT data leads to test error saturation, or even worse test error. This is unlike scaling of problems and solutions in SFT data (in Figure 2(a,b)). This failure can be attributed to the presence of incorrect/irrelevant steps that are not detected by our verifier, since it only verifies the final answer (see Appendix J, K for examples). For a problem \(\), when the LLM is trained with supervised next-token prediction on some positive sub-optimal \(\) in the RFT data, with incorrect step \(_{k}\), it is likely to overfit on spurious correlations between the sub-optimal subsequence \(_{1:k}\), and the following valid step \(_{k+1}\), when trying to maximize \((_{k+1}|_{1:k},)\). To verify this hypothesis, we amplify the presence of these spurious steps. Specifically, for each question in \(_{}\) we sample "spurious steps" from \(_{}\) trained on it, _i.e._, steps which lead to the incorrect answer with high probability under \(_{}\) (we sample multiple completions conditioned on the same spurious step to check how likely it leads to the correct final answer). Then, we interleave the solution traces in the RFT data with these spurious steps. Note, that all traces in the RFT data are still positive since, they all lead to the correct answer eventually. We find that the LLM trained on this sub-optimal spurious RFT data performs worse than the \(_{}\) policy itself.

## 6 Negative Synthetic Data Enables Per-Step Credit Assignment

The spurious correlations from Section 5 correspond to intermediate irrelevant or incorrect steps that are able to still steer the model towards the correct response on some training problems, but derail it otherwise. In this section, we present a conceptual model for constructing negatives that enables us to perform _per-step credit assignment_, and show that this approach can help us address these failure modes of positive data. We show that per-step DPO from Section 3 is a variant of this more general approach. We will then analyze scaling laws with negative data and empirically demonstrate that carefully constructed negative data can address issues with memorization. Finally, we theoretically prove that negative data improves sample-efficiency of \(_{}\).

### Conceptual Model: Constructing Negatives to Enable Per-Step Credit Assignment

While naively contrasting an entire positive response \(+\) against an entire negative response \(-\) will increase the likelihood of _each_ step that appears in \(+\) (even when incorrect or irrelevant) and reduce likelihood on each step appearing in \(-\) (even when accurate and relevant), it does not account for the importance of each step. Formally, given a negative solution trace \(-\), we would want to identify the first _critical_ step where the model introduces a flaw \(-\), and emphasize alternate correct completions from this step that the model could have still produced. Likewise, given a positive solution trace, \(+\), we would like to identify if a given step \(+_{i}\) does not make progress towards the solution by identifying if there exist alternatives from its predecessor step, \(+_{1:i-1}\), which now presents a key decision-making point. **What are these critical steps and how can we identify them procedurally?**

**Value functions.** We can formalize this notion of a critical step under the notion of value functions from reinforcement learning (RL). Recall that both \(+\) and \(-\) are sampled from \(_{}\). For problem \(\), with correct solution \(\), a response \(}\) with a sequence of steps \(}_{1:i-1}\), and a candidate step \(}_{i}\), we define the value function for step \(_{i}\), and previous steps under some policy \(\) as:

\[Q_{}(,}_{1:i-1}}_{}, }_{i}}_{})=_{1:i-1} ^{}(,}_{1:i})}[r(\{}_{1:i},_{i+1:L}\},)]}_{$}}\] (2)

Intuitively, for any partial solution upto \(i\) steps, this Q-function evaluates the probability of succeeding at solving the problem given the remaining budget of \(L-i\) more steps, in expectation over all possible futures sampled from some policy \(\). Our conceptual model treats the policy \(\) as an algorithmic design choice that can differ for algorithms using negative data. As we see later, choosing \(\) as the Best-of-K distribution around \(_{}\) (denoted as \((_{})\)) enables a particularly interesting tradeoff between \(Q\)-value estimation and policy improvement. Another common choice is \(_{}\) itself. Now,

Figure 4: Spurious correlations in RFT data hurt performance.

for any given step \(}_{i}\), we can define its _advantage_ as the relative change in \(Q_{}\) when adding step \(}_{i}\) in comparison with other candidates for step \(i\) as follows:

\[A_{}(,}_{1:i-1};}_{i})=Q_{}( ,}_{1:i-1},}_{i})-Q_{}(,}_{1:i-2},}_{i-1}).\] (3)

Equation 3 is identical to the definition of advantage of an action (_i.e._, \(}_{i}\)) at a state (\(,}_{1:i-1}\)) from RL , in that it is the gap between the Q-value of a state-action pair and the value function of the state (which itself is equal to the Q-value of the _previous_ step due to deterministic dynamics).

**Critical steps, per-step DPO, and advantage-weighted RL.** We can use advantages (**Equation 3**) to characterize critical steps. Steps that attain a higher advantage value than others are **critical** since they need to be generated more precisely to solve the problem. In contrast, steps that with very low advantage values are likely worse and must be unlearned.

**Our definition of the advantage function implies that one can calculate advantages for each step in a response via additional Monte Carlo rollouts starting from prefixes defined by partial solutions. One could then use these advantage estimates (Equation 3) for training the model, for example, by running advantage-weighted reinforcement learning . An alternate option would be to skip the computation of advantage estimates but instead rely on implicit approaches that optimize the advantage-weighted objective without computing their values. Theorem 6.1 shows that DPO performed over a precise pair distribution contrasting positive and negative traces obtained via additional rollouts from \(\), on prefixes of a response sampled from \(_{}\) is equivalent to advantage-weighted RL. A proof of Theorem 6.1 is in Appendix E. Note that unlike the standard reduction of DPO to the RL objective under _some_ reward function , Theorem 6.1 is stronger in that it identifies the value function induced by per-step DPO.

**Theorem 6.1** (Equivalence of advantage-weighted RL and DPO with per-step pairs).: _The optimal policy from Equation 1 with \(^{}_{_{}}\) given by \((,[_{1:i}+_{i+1}],[_{1:i},-_{i+1}])\) where the positive and negative traces share prefix \(_{1:i}_{}\), and \(-_{i+1}_{}(|,_{1:i})\), \(+_{i+1}(A_{}(,_{1:i};)-A_{}(,_{1:i};-_{i+1}))\), is identical to the optima of the advantage-weighted RL objective:_

\[_{}\ _{-p_{}(),-_{ }(|)}_{i=1}^{L}_{i} ,_{0:i-1}A_{}(,_{0:i -1};_{i}).\] (4)

**Practical instantation of DPO with per-step pairs.** In most of our experiments, we instantiate a practical version of the above framework, following the scheme in Hwang et al. . This is a special case (Part 1) of the complete algorithm shown in Algorithm 1 (see Appendix B). Unless otherwise mentioned, we use "per-step DPO" to refer to this version (Part 1 only) in practice. We will also experiment with the complete version (parts 1 and 2) later in Section 6.3.3. Instead of computing

Figure 5: _Negative data scaling laws:_ We evaluate algorithms that consume negative data as we scale \(_{}\), and compare them with only positive training (SFT) on \(_{}\). On GSM8K (a) and MATH (b), we observe an 8\(\) gain from per-step DPO (Section 4) which aligns with our model of negative data that enables per-step credit assignment. In (c) we compare different negative data construction algorithms, and particularly note that naively pairing positives and negatives  leads to worse performance as we scale \(_{}\).

Figure 6: Illustration of advantage estimation from negative data on a didactic example in synthetic model generations. Critical steps are those with high advantage values.

advantage estimates for each step, and then sampling preference pairs, as described in Theorem 6.1, we approximate this by only Q-value estimates on \(8\) negative responses for each question in the synthetic dataset, with \(\) chosen to be the best-of-K policy, \((_{})\) where \(K\) = \(5\). There are two benefits associated with this choice of \(\), especially a higher value of \(K\): **(i)** estimating the advantage in Equation 3 with Monte Carlo rollouts exhibits lower variance when \(K\) is large, since a larger budget \(K\) would lead most steps to have higher Q-values and the variance of Bernoulli reduces as Q-value \( 1\); and **(ii)**\(Q_{(_{})}\) is a non-decreasing function in \(K\) for any state-action, which implies that the solution of advantage-weighted RL objective, in principle, can now improve over a better policy \((_{})\), compared to \(_{}\). Next, we discuss scaling results for negative data, and then in Section 6.3 show how per-step credit assignment improves generalization and suppresses irrelevant and incorrect steps appearing in a response, extracting more gains from the same synthetic data.

### Scaling Results for Negative Data

Observe in Figure 5(a,b), that for both DeepSeek-Math-7B and LLama2-7B models, per-step DPO improves performance beyond the SFT policy and the performance continues to scale favorably as data size increases. In fact, for any given size of \(_{}\), per-step DPO also substantially improves over RFT (Figure 2) on both datasets, and overall, while RFT improved effective data size of \(_{}\) by \(2\), additionally training on negative data extends the performance improvement to \(8\) the size of \(_{}\). Additionally, since per-step DPO estimates advantage of each step under the Best-of-5 policy, one might expect a saturation in the pass@5 performance of the per-step DPO solution. On the contrary, we find that pass@5 performance also improves consistently. In Appendix D we present results for a filtered version of RFT. Here, steps with high advantages from positive/negative data are cloned. This resolves the scaling issue seen when naively scaling positive data in Figure 2(c).

**Choice of negative data matters.** In Figure 5(c) we plot negative data scaling laws where the choice of negative data (thereby pairs for DPO in Equation 1) differs. Observe that standard pairing of positive and negative responses in \(_{_{}}^{}\) for DPO  does not improve over the SFT policy. As such, we tuned \(\) in Equation 1 for DPO but could not fully avoid performance degradation. Our conceptual model explains this result: contrasting arbitrary positives and negatives would result in an incorrect induced advantage function, training with DPO will exacerbate spurious correlations that maximize this induced advantage function [39; 46; 64]. In fact, Pal et al.  also find similar concerns with random pairing and instead pair positives and negatives with highest edit distance, which leads to some improvement, but still performs poorer than per-step DPO that accounts for credit.

**Tickways to scaling negative data.**

* Negative data can identify high-advantage (critical) steps in model-generated responses.
* We can construct negative data distribution that equates DPO to advantage-weighted RL. Negative data used in this way improves the sample efficiency of synthetic data by \(8\).

### Why Does Credit Assignment from Negative Data Improve Model Generalization?

Our conceptual model illustrates that per-step DPO can perform credit assignment, and identify critical steps over irrelevant ones via advantage estimates. We saw that this improves test performance and scaling. Now, we attempt to understand why per-step credit assignment should improve generalization by understanding the generalization properties of advantage-weighted RL. We present two empirical studies below, and a formal theoretical guarantee combining these insights is shown in Appendix F.

#### 6.3.1 Advantage-Weighted RL De-Emphasizes Spurious Steps and Emphasizes Critical Steps

Our key insight is that spurious correlations emerge in monolithic SFT or RFT due to the well-known issue of causal confusion  in imitation learning: by memorizing incorrect or irrelevant steps and associating them with the correctness of the final answer, the model fails to generalize on novel problems, as we saw in Figure 4. We now explain how _online_ model-specific interventions and advantage estimation would address this issue. Consider \(=_{}\). As we show later, in under-trained models memorized steps are imperfectly cloned under \(_{}\), implying that while teacher-forcing loss is low for some spurious, memorized step \(_{s}\), sampling paths from \(_{}\), conditioned on \(_{1;s}\) is likely to generate incorrect responses. This means \(_{s}\) attains a low advantage. On the other

Figure 7: Per-step DPO improves Q-values at each step, standard DPO only improves at irrelevant steps.

hand, for a correct step, _whp_ estimated advantage is higher. Thus, training the model with advantage weighted RL would de-emphasize spurious steps and emphasize critical steps. Running per-step DPO on data generated by the RFT model that has overfit on spurious correlations improves accuracy by >\(6\%\) (Figure 4). We visualize advantages in Appendix K. In Figure 7, we plot the average Q-value of a step for different negative data schemes, and note that only per-step DPO improves over SFT at each step, as expected based on the connection to advantage-weighted RL (Theorem 6.1). Standard DPO fails to improve performance since it has poor success rate at earlier (critical) steps.

#### 6.3.2 Why Does Generalization Improve?: Connecting Advantage-Weighted RL to DRO

In the previous section, we discussed how advantage-weighted RL preferentially weighs the next-token prediction loss at each step. Now, we attempt to conceptually understand why this could improve generalization. For this, we present an intuitive explanation by drawing a connection between advantage-weighted RL and a distributionally robust optimization (DRO) algorithm, named Group DRO, commonly used to improve worst-group robustness in supervised learning .

**Intuitive explanation.** During inference, the SFT policy can fail even on training problems, especially in scenarios where the SFT policy has failed to perfectly clone the next step at each intermediate step in the SFT data. As previously discussed, these steps also present with low advantage values. One way to reduce the chance of compounding inference time errors  is to preferentially minimize the negative log-likelihood loss _more_ for the critical step, i.e., those steps from where the model is more likely to arrive at a wrong answer. If we iteratively update the policy with gradient steps computed over a re-weighted next-step prediction objective where each step is weighted by its advantage estimate, then the resulting algorithm intuitively exhibits this characteristic similarly to distributionally robust optimizers (DRO) . Similar to how DRO solutions guarantee that all subpopulations - both majority and minority subpopulations - in the training data achieve low loss values, the solution for the advantage-weighted RL objective guarantees that the negative log-likelihood of the critical steps with high advantage estimates under \(\) (which of per-step DPO is \((_{})\)) is also low, to a similar extent as the other more prevalent non-critical steps.

In other words, **our insight** is that weighting steps using advantages in Equation 4 upweights the likelihood of the underrepresented critical states while down-weighting it for the spurious ones. The guarantees on the training data (\(_{}\)) also translate to the population level objective when the weights for on-policy samples (advantage estimates) are accurate  and the policy is sufficiently regularized . Since correct behavior at critical steps determine the correctness of the overall solution, an elevated degree of correctness at executing critical steps at the population level implies a higher test accuracy on the reasoning task.

#### 6.3.3 But, Attaining Low Generalization Error Requires Low Advantage Estimation Error

The practical efficacy of algorithms that use negative data for credit assignment requires the advantage estimation error to be low with fewer rollouts from \(\). For discussion, consider \(=_{}\). When the initial advantage of a spurious step is incorrectly over-estimated, negative data algorithms up-weight the likelihood further. This only leads to further memorization. Hence, most Monte-Carlo rollouts from \(_{}\) would rely upon the memorized feature. Since the model generates the correct answer from the memorized feature, it would estimate higher \(A_{_{}}\), and this downward spiral of training with increasing weights on the spurious step leads to test-time model collapse. On the other hand, when \(=(_{})\) for a higher value of \(K\), the Monte-Carlo advantage estimator has a lower variance (and error). This discussion also justifies the choice of \(K\)=\(5\), an intermediate value, in per-step DPO.

#### 6.3.4 Validating Claims About Generalization: Controlled Analysis on a Didactic Problem

With the above insights, we now study the influence of \(_{}\) on the generalization effects of per-step DPO. For our analysis, we consider a didactic star graph problem (Appendix I) from Bachmann and Nagarajan , where given a graph in the shape of a star and a query (center/end node), the model is asked to output the full path between the start/end nodes. This task highlights the failure of SFT at planning problems (akin to math reasoning). They show that \(_{}\) minimizes SFT loss by memorizing the "hard-to-predict" node adjacent to the center, and copying the rest from the input graph. It is clear that the failure stems from not being able to identify the critical adjacent token. We will show how credit assignment with negative data accurately upweights the critical token and unlearns the memorized token. To vary the choice of \(_{}\), we choose several intermediate checkpoints obtained during supervised finetuning for synthetic negative data generation. We consider three initializations: **(1)** an under-trained SFT model with a large training and test loss, and **(2)** an SFT model obtained by early-stopping based on a held-out validation set, where the validation loss is the lowest, and **(3)** an over-trained SFT checkpoint, with a low training but high validation loss.

**(1) & (2): Training on negative data from an under-trained or early-stopped \(_{}\) improves both training loss and test performance.** As shown in Figure 8(a,b), we find that when training with negative data from iteration 60 (under-trained \(_{}\)) and iteration 200 (early-stopped \(_{}\)), utilizing per-step DPO reduces the training loss very aggressively. These benefits translate to test losses and performance as well (Figure 8(b), orange and green). In contrast, supervised finetuning exhibits a nearly-flat test loss landscape, although the train loss reduces slowly. Upon a closer inspection, we find that training on positive data via SFT only tends to memorize the critical token in the training data using non-generalizable features, and hence, the resulting model does not generalize to novel problems. More training with SFT is unable to "unlearn" this spurious correlation and does not reduce the loss function. On the other hand, per-step DPO with negative data is able to unlearn this spurious feature and drives improvement, as evident by the drastic improvement on train and test.

**(3) Training on negative data from an over-trained SFT initialization leads to model collapse.** When training with negative data on an over-trained \(_{}\) (iteration 580) in Figure 8(c), we observe that both SFT and per-step DPO exhibit identical test errors since training with more negative data simply exacerbates the model's dependence on memorizing the critical token, which manifests in the form of lower test losses. This is also an example where Monte-Carlo samples from the over-trained checkpoint estimates a high advantage since Q-value is already high at iteration 500 (in (a)). This means that when the SFT policy has sufficiently memorized the training data using a spurious feature, training further is unable to unlearn this dependence. Hence, we find that in this regime, negative data leads to no improvement, capping performance at what was attained by fine-tuning on positive data.

## 7 Discussion and Conclusion

Our work studies the role of synthetic data for improving math reasoning capabilities of LLMs. We find that while the typical approach of collecting new questions and corresponding positive (correct) solutions from capable models like GPT-4/Gemini-1.5 presents underwhelming data scaling. The sample efficiency of the same data can be improved up to \(2\) by sampling more positive traces from the 7B sized models SFT-ed on the original data. However, training on positive self-generated synthetic data alone often amplifies the model's dependence on spurious steps, that erroneously appear to lead to a good solution but do not generalize to novel problems and hurt test performance. That said, surprisingly, we show that negative (incorrect) traces sampled from the same SFT model can be used to address the failure modes of training on only positive data. In particular, negative data can be used to estimate advantage values for every step, and using these advantage estimates via RL enables us to address this problem. We show how the advantages can be used implicitly by preference optimization objectives. We show how training on an instance of this objective leads to \(8\) improvements in sample efficiency of the synthetic data used.

Figure 8: _Didactic analysis on star graph:_ In (a) we plot the SFT loss and Q-value of the critical token (adjacent node) for SFT and per-step DPO (starting from iter 60). Indicative of memorization SFT loss decreases at a slow rate, matching the slow rate of increase in the Q-value. In contrast per-step DPO loss sharply decreases during training. In (b) we notice a corresponding phase transition in the test error of per-step DPO starting from different under-trained SFT checkpoints, which does not happen for an over-trained SFT checkpoint in (c).