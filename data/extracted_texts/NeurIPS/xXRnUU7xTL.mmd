# SelfCodeAlign: Self-Alignment for Code Generation

Yuxiang Wei1 Federico Cassano2,7 Jiawei Liu1 Yifeng Ding1 Naman Jain3 Zachary Mueller5 Harm de Vries4 Leandro von Werra5 Arjun Guha2,6 Lingming Zhang1

1University of Illinois Urbana-Champaign 2Northeastern University

3University of California, Berkeley 4ServiceNow Research 5Hugging Face 6Roblox 7Cursor AI {ywei40,lingming}@illinois.edu {cassano.f,a.guha}@northeastern.edu https://github.com/bigcode-project/selfcodealign

###### Abstract

Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. For programming tasks, most models are finetuned with costly human-annotated instruction-response pairs or those generated by large, proprietary LLMs, which may not be permitted. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. Overall, SelfCodeAlign shows for the first time that a strong instruction-tuned code LLM can result from self-alignment rather than distillation.

## 1 Introduction

Recent studies have demonstrated the outstanding performance of large language models (LLMs)  in various code-related tasks, _e.g.,_ program synthesis , program repair , code optimization , code completion , code translation , software testing , and software agents . The reason is that modern LLMs are pre-trained over trillions of code tokens in the wild using various training objectives (as such next-token prediction ), making the base models natively good at understanding and generating code snippets. Furthermore, to fully unleash the power of LLMs, the base models aretypically further fine-tuned on high-quality instruction-following data to boost their performance in following natural language instructions and solving more general software engineering tasks . This step is known as _instruction tuning_.

Curating high-quality data for instruction tuning is crucial yet challenging. One source of acquiring instruction data is to employ human annotation . For example, Llama-3  uses a corpus of 10 million human-annotated examples in instruction tuning. Due to the high cost of human annotation, knowledge distillation is widely adopted to train a weaker LLM with outputs generated by stronger LLMs . However, distillation may violate the terms of service  of proprietary LLMs and the prerequisite of using a stronger LLM limits its generalizability. Therefore, recent proposals focus on instruction tuning without relying on human annotation or distillation . One cornerstone work along this direction is Self-Instruct, which finetunes GPT-3 with self-generated instruction data using in-context learning.

There is a growing number of instruction-tuned open-source LLMs in the code domain. However, some models, such as DeepSeek-Coder , Llama-3 , and CodeQwen1.5 , either use proprietary data or do not disclose their instruction-tuning strategies. Others, including WizardCoder , Magicoder , WaveCoder , and OpenCodeInterpeter , rely on knowledge distillation. The only exception is OctoCoder , which is instruction-tuned over heavily filtered GitHub commits, with commit messages as instructions and the changed code as responses, as well as data from OpenAssistant, a human-generated corpus of user-assistant conversations . Despite its transparency and permissive licensing, OctoCoder's performance, at 32.9 HumanEval+ pass@1, lags behind other mainstream code LLMs. Meanwhile, previous attempts at applying Self-Instruct for code generation have resulted in performance degradation over training on natural instruction-response pairs . Our findings imply that effective self-alignment requires a combination of data diversity control and response validation, which is not present in the traditional Self-Instruct approach.

In this paper, we propose SelfCodeAlign, the first fully transparent pipeline to successfully self-align base code LLMs with purely self-generated instruction data. First, SelfCodeAlign extracts diverse coding concepts from high-quality seed functions in The Stack V1 , a large corpus of permissively licensed code. Next, using these concepts, we prompt the base model to generate new coding tasks through in-context learning. We then instruct the base model to produce multiple responses for each task, each paired with test cases for self-validation. Finally, we select only the instruction-response pairs that pass the test cases. This method ensures the model practices various coding concepts and validates the consistency between instructions and responses.

To evaluate our method, we train CodeQwen1.5-7B, a state-of-the-art open-source base LLM for code, on both a dataset generated with SelfCodeAlign and OctoPack, a naturally-generated and meticulously-filtered dataset used for training OctoCoder . We benchmark both, along with OctoCoder and other models, on a series of tasks: code generation (both function- and class-level) , data science programming , and code editing . On all tasks, training CodeQwen with SelfCodeAlign significantly improves performance over the base model and over training it on OctoPack. For instance, on HumanEval+, our model achieves a pass@1 score of 67.1, 21.4 points higher than CodeQwen1.5-7B and 16.5 points higher than CodeQwen1.5-7B-OctoPack. This highlights the effectiveness of our synthetic data generation method compared to natural data in enhancing the capabilities of code LLMs.

In the component analysis, we justify the different components of the pipeline. We demonstrate that SelfCodeAlign is general to different LLMs whose sizes range from 3B to 33B. In particular, we find that a base LLM could learn more effectively from data within its own distribution than a shifted distribution from a teacher LLM. Additionally, we show that seed selection, concept generation, and execution filtering all contribute positively to the pipeline. Furthermore, on HumanEval+, SelfCodeAlign (67.1 pass@1) outperforms state-of-the-art, GPT-3.5-Turbo-based distillation methods, including OSS-Instruct  (61.6) and Evol-Instruct  (59.1), as well as direct output distillation from GPT-4o  (65.9).

SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. We discuss StarCoder2-Instruct in Appendix A.

Overall, we make the following main contributions: _(i)_ We introduce SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs to follow instructions. Our method does not rely on extensive human annotations or distillation from larger models. _(ii)_ We generate a series of datasets using SelfCodeAlign and train multiple models on these datasets, which will all be released to the public. _(iii)_ We thoroughly evaluate our method on a multitude of tasks, showing strong performance across all the evaluated models. _(iv)_ Our experiments demonstrate that training models on their own data can be more effective than using data from stronger, but distributionally different, teacher models when they don't have a huge performance gap. _(v)_ Finally, we run a comprehensive component analysis that verifies the positive contribution of each component in SelfCodeAlign.

## 2 SelfCodeAlign: Self-Alignment for Code Generation

Figure 1 illustrates an overview of our SelfCodeAlign technique. It first generates diverse instructions by extracting coding concepts from high-quality seed snippets. This process resembles OSS-Instruct , which employs GPT-3.5-Turbo to convert random snippets into instructions. However, our method uses the base model exclusively and incorporates a separate concept generation phase that we prove beneficial in SS4.3. SelfCodeAlign then generates several responses for each task, pairing each with test cases for sandbox execution, and finally chooses passing examples for instruction tuning. Example outputs from each step are listed in Appendix D.1. In the following sections, we provide detailed explanations of these steps.

### Seed Snippets Collection

SelfCodeAlign starts by collecting a set of seed code snippets from The Stack V1. In this step, it's crucial to ensure that the seed snippets are diverse and high-quality, as they will be used as the starting point for generating instructions and responses. To collect the seed snippets, we extract all Python functions with docstrings from The Stack V1, and then apply a series of filtering rules to ensure the quality of the seed snippets. In total, we collect 250k Python functions from 5M functions with docstrings in The Stack V1, which were filtered by running the Pyright type checker, removing benchmark items, filtering out functions with poor documentation, and removing near-duplicates. Appendix B details this process in depth.

### Diverse Instruction Generation

After collecting the seed functions, we perform Self-OSS-Instruct, our adaptation of OSS-Instruct  for self-alignment, to generate diverse instructions. In detail, we employ in-context learning to let the base model self-generate instructions from the given seed code snippets. This process utilizes 21 carefully designed few-shot examples listed in Appendix E. The instruction generation procedure is divided into the following two steps:

* **Concepts extraction:** For each seed function, we prompt the base model to produce a list of code concepts present within the function. Code concepts refer to the foundational principles and techniques used in programming, such as pattern matching and data type conversion.
* **Instruction generation:** We then prompt the base model to self-generate a coding task conditioned on the identified code concepts and two additional attributes, difficulty (easy/medium/hard) and category (function/class/program implementation), which we randomly sample to enrich the diversity of the generated instructions.

### Response Generation and Self-Validation

Given the instructions generated from Self-OSS-Instruct, our next step is to match each instruction with a high-quality response. Prior practices commonly rely on distilling responses from stronger

Figure 1: Overview of SelfCodeAlign.

teacher models, such as GPT-4, which hopefully exhibit higher quality. However, distilling proprietary models leads to non-permissive licensing and a stronger teacher model might not always be available. More importantly, teacher models can be wrong as well, and the distribution gap between teacher and student can be detrimental.

We propose to self-align the base model by explicitly instructing the model to generate tests for self-validation after it produces a response interleaved with natural language. This process is similar to how developers test their code implementations. Specifically, for each instruction, the base model samples multiple outputs of the format _(response, tests)_, and we filter out those responses falsified by the test execution under a sandbox environment. We then randomly select one passing response per instruction to the final instruction tuning dataset.

## 3 Main Evaluation

In this section, we comprehensively evaluate SelfCodeAlign over a diverse set of coding tasks:

* **Function generation (SS3.1):** Given a natural-language description, LLMs are asked to generate a self-contained function whose correctness and efficiency is checked through test execution [8; 3; 21; 76; 39].
* **Class generation (SS3.2):** Given a code skeleton with both class- and method-level information, LLMs are asked to generate the class and its methods .
* **Data science programming (SS3.3):** Given a description of a data science task and a partial code snippet, LLMs are asked to complete the code snippet to pass corresponding tests .
* **File-level code editing (SS3.4):** Provided with the contents of a file, the model is asked to edit the program following a natural language instruction .

### Function-level Code Generation

**HumanEval+ and MBPP+.** HumanEval  and MBPP  are the two most widely-used benchmarks for function-level code generation. We use their test augmented versions, _i.e.,_ HumanEval+ and MBPP+, with 80\(\)/35\(\) more test cases for rigorous evaluation .

As baselines, we consider a diverse set of state-of-the-art instruction-tuned models over various dimensions, including weight openness, data openness, transparency, and performance. Table 1 compares the pass@1 of the self-aligned SelfCodeAlign-CQ-7B against other baseline models on

    &  &  &  \\   & & HumanEval+ & MBPP+ & Transparent & Non-proprietary & Non-distilled \\  GPT-4-Turbo  & Not disclosed & 81.7 & 70.7 & \(\) & \(\) & \(\) \\ Mistral Large  & Not disclosed & 62.8 & 56.6 & \(\) & \(\) & \(\) \\ Gemini Pro  & Proprietary & 55.5 & 57.9 & \(\) & \(\) & \(\) \\  Llama3-70B-Instruct  & Proprietary & 70.7 & 66.4 & \(\) & \(\) & \(\) \\ CodeLlama-70B-Instruct  & Proprietary & 65.2 & 61.7 & \(\) & \(\) & \(\) \\ WizardCoder-33B-v1.1  & GPT distillation & 73.2 & 66.9 & \(\) & \(\) & \(\) \\ OpenCodelInterpect-DS-33B  & GPT distillation & 73.8 & 67.7 & \(\) & \(\) & \(\) \\ Magicoder-SDS-6.7B  & GPT distillation & 70.7 & 65.4 & \(\) & \(\) & \(\) \\ DeepSeeKoder-33B-Instruct  & Not disclosed & 75.0 & 66.7 & \(\) & - & - \\ CodeQwen1.5-7B-Chat  & Not disclosed & 77.7 & 67.2 & \(\) & - & - \\ Snowflake Arctic (480B)  & Not disclosed & 64.3 & 64.3 & \(\) & - & - \\ Mixtral-8x22B-Instruct-v0.1  & Not disclosed & 70.1 & 62.9 & \(\) & - & - \\ Command-R+ (104B)  & Not disclosed & 56.7 & 58.6 & \(\) & - & - \\ Mixxtral-8x7B-Instruct-v0.1  & Not disclosed & 39.6 & 49.7 & \(\) & - & - \\ OctoCoder-16B  & Publicly available & 32.9 & 49.1 & \(\) & \(\) & \(\) \\ StarCoder2-15B  & - & 37.8 & 53.1 & \(\) & \(\) & \(\) \\ CodeQwen1.5-7B-Base  & - & 45.7 & 60.2 & \(\) & - & - \\ CodeQwen1.5-7B-OctoPack & Publicly available & 50.6 & 63.2 & \(\) & \(\) & \(\) \\ SelfCodeAlign-CQ-7B & Self-generated & 67.1 & 65.2 & \(\) & \(\) & \(\) \\   

Table 1: Pass@1 (%) of different LLMs on EvalPlus computed using greedy decoding.

HumanEval+ and MBPP+. Among those trained using a fully transparent pipeline without any proprietary data or distillation, SelfCodeAlign-CQ-7B stands out as the best LLM by drastically outperforming the base model, OctoCode-16B, StarCoder2-15B, and CodeQwen1.5-7B-OctoPack. Meanwhile, compared to much larger models, SelfCodeAlign-CQ-7B outperforms Arctic, Command-R+, and Mixtral-8x7B-Instruct, while closely matching Mixtral-8x22B-instruct. Even compared to LLMs trained using proprietary data (_e.g._, manually annotated), SelfCodeAlign-CQ-7B remains competitive, surpassing Gemini Pro, Mistral Large, and CodeLlama-70B-Instruct. Additionally, SelfCodeAlign-CQ-7B, fine-tuned on purely self-generated data, closely rivals models finetuned with distillation-based or non-transparent synthetic data.

**LiveCodeBench.** In subsequent evaluations, we benchmark our model against state-of-the-art open-source LLMs of similar sizes for a fair comparison. LiveCodeBench  is a benchmark for contamination-free evaluation. It features 400 recent Python algorithm challenges from May 2023 to February 2024. These tasks are curated from online judge websites such as Codeforce and LeetCode, each with over 20 test cases on average. While LiveCodeBench is a holistic benchmark covering four problem types, we focus on the code generation task for assessing LLM function generation.

Table 2 reports the pass@1 results for problem subsets created after three specific start dates. It shows that SelfCodeAlign-CQ-7B consistently outperforms most baseline models and closely matches CodeQwen1.5-7B-Chat. In addition, moving the start date forward has minimal impact on the pass@1 of SelfCodeAlign-CQ-7B, indicating that our pipeline is less likely to suffer from contamination.

**EvoEval.** To mitigate the impact of potential data contamination, EvoEval  includes 828 programming problems created by prompting GPT-4 to evolve original HumanEval tasks across 5 semantic-altering and 2 semantic-preserving benchmarks. Following the leaderboard of EvoEval, we use the 5 semantic-altering benchmarks, each of which has 100 problems.

Table 3 shows that SelfCodeAlign-CQ-7B achieves the best pass@1 score among all transparently finetuned models. Meanwhile, it also surpasses most open LLMs (except CodeQwen1.5-7B-Chat) trained on unknown instruction-tuning data.

    &  \\   & 2023-09-01 & 2023-07-01 & 2023-05-01 \\  DeepSeek-Coder-6.7B-Instruct & 19.2 & 20.8 & 21.6 \\ CodeGemma-7B-IT & 15.2 & 14.1 & 13.6 \\ Llama-3-8B-Instruct & 18.3 & 18.4 & 17.3 \\ CodeQwen1.5-7B-Base & 19.3 & 20.7 & 21.8 \\ CodeQwen1.5-7B-Chat & **23.2** & **24.1** & **25.0** \\  OctoCoder-16B & 12.6 & 11.2 & 9.8 \\ StarCoder2-15B & 14.5 & 14.7 & 15.4 \\ CodeQwen1.5-7B-OctoPack & 19.3 & 21.8 & 22.5 \\  SelfCodeAlign-CQ-7B & **22.4** & **22.8** & **23.4** \\   

Table 2: Pass@1 (%) of LLMs on LiveCodeBench. Newer start dates imply lower contamination risk.

   Model & Average & Difficult & Creative & Subtle & Combine & Tool use \\  DeepSeek-Coder-6.7B-Instruct & 41.4 & 40 & 37 & 61 & 18 & 51 \\ CodeGemma-7B-IT & 35.4 & 31 & 32 & 49 & 9 & 56 \\ Llama-3-8B-Instruct & 40.6 & 34 & 39 & 57 & 15 & 58 \\ CodeQwen1.5-7B-Base & 36.2 & 26 & 30 & 46 & 18 & 61 \\ CodeQwen1.5-7B-Chat & **48.0** & 39 & 38 & 71 & 31 & 61 \\  OctoCoder-16B & 30.6 & 19 & 26 & 43 & 11 & 54 \\ StarCoder2-15B & 25.8 & 16 & 19 & 41 & 5 & 48 \\ CodeQwen1.5-7B-OctoPack & 42.2 & 35 & 36 & 59 & 22 & 59 \\  SelfCodeAlign-CQ-7B & **43.6** & 33 & 40 & 60 & 20 & 65 \\   

Table 3: Pass@1 (%) of code LLMs on EvoEval.

**EvalPerf.** While the earlier benchmarks focus on code correctness, we use EvalPerf  to evaluate the efficiency of LLM-generated code. EvalPerf includes 118 performance-exercising tasks with computation-intensive test inputs to fully exercise the efficiency of LLM-generated code.

Since code efficiency only matters when the generated code is correct, in Table 4 we only evaluate baselines that can achieve a decent pass@1 (_i.e.,_ over 50%) on HumanEval+. Specifically, we run EvalPerf by following its default settings: _(i)_ Each model generates 100 samples per task at the temperature of 1.0; _(ii)_ We evaluate the efficiency of up to 20 correct samples per model for tasks where it can at least generate 10 passing samples; and _(iii)_ Finally we rank the models based on their win rates, where each model pair compares their differential performance score (DPS) over the common set of passing tasks. Notably, DPS is a LeetCode-inspired metric that indicates the overall efficiency ranking of submissions. For example, Table 4 shows that SelfCodeAlign-CQ-7B achieves a DPS of 79.9, indicating that its correctly generated solutions can overall outperform or match the efficiency 79.9% of reference solutions across various efficiency levels.

Table 4 shows that SelfCodeAlign-CQ-7B ranks second among the evaluated models of comparable size. Specifically, SelfCodeAlign-CQ-7B is only next to DeepSeek-Coder-6.7B-Instruct whose training data is not disclosed. Surprisingly, the efficiency of SelfCodeAlign-CQ-7B-generated code surpasses many recent open models trained using private data, including the latest Llama-3.1-8B-Instruct.

### Class-level Code Generation

We evaluate code LLMs on class-level code generation using ClassEval , a collection of 100 class-level Python code generation tasks, covering 100 classes and 410 methods, with an average of 33 tests per class and 8 tests per method.

Following the ClassEval paper , we set the maximum model context size as 2048 tokens and report the best class-level pass@1 (and corresponding method-level pass@1) of each model among three generation strategies: (i) _Holistic Generation_: generating the entire class given a class skeleton, (ii) _Incremental Generation_: generating class methods iteratively by putting earlier generated methods in the prompt, and (iii) _Compositional Generation_: generating each class method independently without looking at other methods. Specifically, class-level pass@1 in Table 5 refers to the pass rate of generated classes given _both_ the method- and class-level tests. In contrast, method-level pass@1 is computed by _only_ checking if the generated methods can pass the method-level tests. Table 5 shows, in terms of class-level performance, SelfCodeAlign-CQ-7B is the best transparently finetuned model, surpassing the second-best transparent model (_i.e.,_ CodeQwen1.5-7B-OctoPack) by 28%, while performing no worse than those using unknown or proprietary instruction-tuning data. For method generation, SelfCodeAlign-CQ-7B also stands out in transparently finetuned models.

### Data Science Programming

DS-1000  is a benchmark of 1000 realistic data science challenges across 7 popular Python data science libraries. In DS-1000, a model must complete a partial code snippet to solve the problem. The solution is then evaluated through test execution. Table 6 shows that SelfCodeAlign-CQ-7B, despite being trained on limited data science code, stands out as the best in the transparent model category, while remaining competitive among the other evaluated baselines.

   Model & DPS (\%) & pass@1 (\%) & Win-rate (\%) \\  DeepSeek-Coder-6.7B-Instruct & 83.6 & 73.6 & 63.9 \\ Llama-3.1-8B-Instruct & 80.9 & 64.3 & 52.1 \\ Llama-3-8B-Instruct & 77.0 & 43.7 & 51.5 \\ CodeQwen1.5-7B-Chat & 80.7 & 74.1 & 51.2 \\ CodeQwen1.5-7B-OctoPack & 74.0 & 49.1 & 26.9 \\  SelfCodeAlign-CQ-7B & 79.9 & 65.2 & 54.0 \\   

Table 4: Ranking of model code efficiency based on the EvalPerf win rates, which are computed over the common set of passing tasks for each model pair. Each model generates 100 samples per task at a temperature 1.0. To exemplify differential performance score (DPS) with SelfCodeAlign-CQ-7B, it means its generations if correct can match the efficiency of 79.9% LLM samples.

### Code Editing

We further evaluate LLMs on code editing tasks using the CanItEdit benchmark , comprised of 210 code editing tasks from three change kinds (70 tasks each): corrective (fixing bugs), adaptive (adding new features), and perceive (improving existing features). The tasks are evaluated based on the correctness of the generated code changes, according to a set of hidden test cases. For each task, the model is given as input the original code snippet and a natural-language instruction describing the desired code change; then it is expected to produce an updated code snippet that satisfies the instruction. We follow the setting from the original benchmark  to generate 20 completions per task at a temperature of 0.2. Table 7 reports the pass@1 for each change kind and the average pass@1 across all tasks. Despite not being specifically tuned for code editing, SelfCodeAlign-CQ-7B exhibits strong performance on CanItEdit, achieving a pass@1 of 39.0%, outperforming all other models except CodeQwen1.5-Chat, whose instruction tuning details are not disclosed.

   Model & Avg. & Pandas & NumPy & Matplotlib & TensorFlow & SciPy & Sklearn & PyTorch \\  DeepSeek-Coder-6.7B-Instruct & 44.6 & 34.0 & 51.1 & 58.4 & 45.9 & 34.2 & 45.8 & 50.6 \\ CodeGenma-7B-IT & 30.8 & 21.9 & 34.4 & 54.7 & 25.1 & 21.8 & 22.6 & 34.5 \\ Llama-3-8B-Instruct & 31.1 & 21.5 & 33.1 & 51.9 & 34.4 & 25.2 & 23.8 & 37.2 \\ CodeQwen1.5-7B-Base & 32.4 & 21.6 & 35.9 & 56.7 & 28.8 & 28.2 & 30.9 & 23.8 \\ CodeQwen1.5-7B-Chat & **47.1** & 34.4 & 51.7 & 67.2 & 46.0 & 38.9 & 47.9 & 52.8 \\ OctoCoder-16B & 28.3 & 13.1 & 34.0 & 53.8 & 22.4 & 22.8 & 30.0 & 25.9 \\ StarCoder2-15B & 38.9 & 26.2 & 45.8 & 61.4 & 38.1 & 36.0 & 40.5 & 22.5 \\ CodeQwen1.5-7B-OctoPack & 38.2 & 26.7 & 42.6 & 61.8 & 37.7 & 32.7 & 36.6 & 31.4 \\ SelfCodeAlign-CQ-7B & **39.1** & 28.2 & 42.6 & 57.2 & 38.3 & 35.6 & 42.8 & 33.3 \\   

Table 6: Pass@1 (%) on DS-1000 with temperature 0.2 and top-p 0.95 over 40 samples, following the same hyperparameter setting used in StarCoder2 .

   Model & Average & Corrective & Adaptive & Perfective \\  DeepSeek-Coder-6.7B-Instruct & 36.3 & 34.9 & 38.8 & 35.3 \\ CodeGenma-7B-IT & 34.2 & 30.9 & 39.3 & 32.5 \\ Llama-3-8B-Instruct & 36.0 & 34.9 & 39.1 & 34.0 \\ CodeQwen1.5-7B-Base & 38.4 & 34.7 & 45.6 & 34.9 \\ CodeQwen1.5-7B-Chat & **39.9** & 38.1 & 46.6 & 35.1 \\ OctoCoder-16B & 30.2 & 38.4 & 31.6 & 20.5 \\ StarCoder2-15B & 36.7 & 32.1 & 43.8 & 34.2 \\ CodeQwen1.5-7B-OctoPack & 36.5 & 36.9 & 40.6 & 31.9 \\  SelfCodeAlign-CQ-7B & **39.0** & 37.4 & 42.4 & 37.2 \\   

Table 7: Pass@1 (%) of code LLMs on CanItEdit.

   Model & Class-level & Method-level \\  DeepSeek-Coder-6.7B-Instruct & **27.0** & **57.2** \\ CodeGenma-7B-IT & 21.0 & 44.8 \\ Llama-3-8B-Instruct & 23.0 & 52.4 \\ CodeQwen1.5-7B-Base & 23.0 & 52.8 \\ CodeQwen1.5-7B-Chat & **27.0** & 54.6 \\ OctoCoder-16B & 19.0 & 38.0 \\ StarCoder2-15B & 9.0 & 24.9 \\ CodeQwen1.5-7B-OctoPack & 21.0 & 45.2 \\  SelfCodeAlign-CQ-7B & **27.0** & **52.6** \\   

Table 5: Pass@1 (%) of code LLMs on ClassEval using greedy decoding.

Component Analysis

In this section, we extensively study how different components contribute to the SelfCodeAlign pipeline. To make the comparison tractable, we fix a subset of seed code snippets by randomly sampling 37k examples from the 250k corpus and evaluate finetuned models on HumanEval+ .

### Self-Alignment with Different Models

To assess whether SelfCodeAlign is generalizable and how performance varies with finetuning data generated by different models, we run the same data generation pipeline end to end with different LLMs. We include four diverse state-of-the-art model architectures and sizes ranging from 3B to 33B to observe how SelfCodeAlign performs across small, medium, and large-scale LLMs.

Table 8 shows the comparison and guides us to reach the following findings. Looking at the diagonal cells, SelfCodeAlign consistently improves the performance of the base models with varying sizes, from 3B to 33B. Comparing each diagonal cell and the cell immediately to its right (i.e., using base models with slightly better HumanEval+ performance as the teacher models), we can see that a base model may benefit more from self-generated data than a stronger teacher model, when they don't have a large performance gap. However, when the teacher model is clearly stronger, the base model learns better by distilling the teacher's knowledge. For example, StarCoder2-3B achieves higher pass@1 trained on its own data (35.4) compared to Llama-3-8B data (34.1), but when tuned with stronger models, StarCoder2-3B further improves (_e.g.,_ 42.1 with DeepSeek-Coder-33B data). Also, the last row shows that a stronger model can still learn from a weaker model, but less effectively. We provide qualitative examples in Appendix D.2.

### Effectiveness of Execution-based Filtering

The SelfCodeAlign pipeline samples multiple responses for an instruction and each response is equipped with self-generated test cases. Responses with failing tests are filtered out and each instruction will be paired with a randomly selected passing response. To answer the question of "to what extent is execution information helpful", in Table 9, we conduct 4 controlled experiments by varying how responses are selected while keeping the other components unchanged:

* **Random selection (all)**: pair each instruction with a random response without response filtering.
* **Random selection (subset)**: 15.6k subset of "Random selection (all)" for a consistent data amount.
* **Failures only**: pair each instruction with a failing response.
* **Passes only**: pair each instruction with a passing response.

First, we can observe that random pairing performs worse than using only passing examples, both when data sizes are aligned and when they scale up by 1.8\(\). Meanwhile, the "Failure only" setting

    &  \\   & StarCoder2-3B & Llama-3-8B & StarCoder2-15B & DeepSeek-Coder-33B & CodeQwen1.5-7B \\  StarCoder2-3B (27.4) & 35.4 & 34.1 & 39.0 & **42.1** & 40.2 \\ Llama-3-8B (29.3) & - & 42.7 & 40.2 & 41.5 & **43.3** \\ StarCoder2-15B (37.8) & - & - & 55.5 & 53.0 & **57.3** \\ DeepSeek-Coder-33B (44.5) & - & - & - & **65.9** & 62.2 \\ CodeQwen1.5-7B (45.7) & 48.8 & 54.9 & 56.1 & 59.1 & **65.2** \\   

Table 8: HumanEval+ pass@1 when finetuning the base models on different data (37k seeds).

   Selection strategy & Data size & Execution pass rate & Pass@1 \\  Random selection (all) & 27.7k & 24.1\% & 61.6 \\ Random selection (subset) & 15.6k & 24.2\% & 61.6 \\ Failures only & 15.6k & 0\% & 57.9 \\ Passes only & 15.6k & 100.0\% & **65.2** \\   

Table 9: Pass@1 on HumanEval+ with different response selection strategies.

results in the worst performance where we deliberately use failing responses for each instruction. These results suggest the importance of execution filtering and code correctness for self-alignment.

### Importance of Seed Selection and Concepts Generation

For instruction generation, SelfCodeAlign applies Self-OSS-Instruct that first selects high-quality seed code snippets, then mines code concepts from the seeds, and finally generates the instructions. To validate the usefulness of concept generation and high-quality seeds, we compare two variants of SelfCodeAlign in Table 10: 1) directly generating instructions from seeds, where the model produces an instruction based solely on a seed snippet, and 2) using the default pipeline except for the initial seeds, where random snippets are sampled from different code documents in The Stack V1.

It is shown that directly generating instructions from seeds leads to the poorest performance. This is because a direct generation from seeds requires the seed snippet to be presented in the context, whose format is not well represented in the wild and may not be in distribution for the model. The generated instructions will then be distracted and thus be of lower quality. Concept generation neutralizes this effect and produces more realistic and natural instructions. Using random snippets produces a more diverse but less coherent set of concepts, leading to slightly worse performance compared to using high-quality seeds. Appendices D.3 and D.4 illustrate some qualitative examples.

### Comparing Self-Alignment to Distillation

To compare self-alignment with distillation, we evaluate SelfCodeAlign against two state-of-the-art distillation methods for code instruction tuning: OSS-Instruct  and Code Evol-Instruct . We use the official OSS-Instruct dataset. As the official implementation of Code Evol-Instruct is unavailable, we opt for the most popular open-source version  on Hugging Face. Both datasets are generated using GPT-3.5-Turbo  and we randomly select their subsets to match the 74k samples generated by SelfCodeAlign. Table 11 shows that SelfCodeAlign substantially outperforms both methods, indicating the strength and promising future of self-alignment for code. Additionally, SelfCodeAlign outperforms direct distillation, where we use the same set of SelfCodeAlign instructions but rely on GPT-4o  to generate each response at temperature 0. This suggests that weaker models, combined with more post-validation compute, can produce higher-quality responses.

## 5 Related Work

**Instruction tuning for code.** To build more powerful code assistants, pre-trained code models are fine-tuned over a small amount of high-quality instruction-response pairs that are either collected from real-world  or synthetically generated [7; 57; 41; 72]. This step is known as instruction tuning. OctoPack  compiles a large set of real-world Git commits which are partially used for code fine-tuning. Code Alpaca  applies Self-Instruct to the code domain, which prompts ChatGPT to generate synthetic instruction data for code. Similarly, the instruction data for CodeLlama 

   Source of seeds & Pipeline & Pass@1 \\  Filtered functions & Seed \(\) instruction & 59.8 \\ Random snippets & Seed \(\) concepts \(\) instruction & 64.0 \\ Filtered functions & Seed \(\) concepts \(\) instruction & **65.2** \\   

Table 10: Pass@1 on HumanEval+ using different seeds and pipelines.

   Method & Dataset size & Teacher model & Execution filtering & Pass@1 \\  Evol-Instruct & 74k & GPT-3.5-Turbo & \(\) & 59.1 \\ OSS-Instruct & 74k & GPT-3.5-Turbo & \(\) & 61.6 \\ Direct distillation & 74k & GPT-4o & \(\) & 65.9 \\ SelfCodeAlign & 74k & CodeQwen1.5-7B & \(\) & **67.1** \\   

Table 11: SelfCodeAlign versus distillation using CodeQwen1.5-7B as the base model.

includes coding problems generated by prompting Llama 2  and solutions and tests by prompting base CodeLlama. Code Evol-Instruct  uses harder programming challenges as instruction data to fine-tune more capable models. Specifically, Code Evol-Instruct prompts ChatGPT with heuristics to evolve existing instruction data to more challenging and complex ones. Besides data complexity, the widely-adopted [62; 71; 14] OSS-Instruct  looks at the data _diversity_ and _quality_ dimension. Specifically, given a source code snippet, OSS-Instruct prompts ChatGPT to get inspired and imagine potential instruction-response pairs, which inherit the diversity and quality of sampled code snippets. Besides instruction tuning, recent work on training code LLMs for performance improvement also explores multi-turn code generation , model merging , preference tuning [74; 36], and reinforcement learning . Recently, various strong instruction-tuned code models have been released by major organizations [19; 64]. However, their instruction-tuning recipes (_e.g.,_ data and strategies) are not fully disclosed. This lack of transparency underscores the need for fully transparent and permissive instruction-tuning methods to advance the field.

**Self-alignment.** Self-alignment is an approach to instruction tuning that utilizes an LLM to learn from its own output without depending on an existing well-aligned teacher LLM. Self-Instruct is one of the first endeavors that allow GPT-3 to improve itself by generating new instructions and responses for instruction-tuning using its in-context learning capability. Self-Align, based on in-context learning, utilizes topic-guided Self-Instruct for instruction generation and pre-defines principles to steer the LLM towards desired responses. These instruction-response pairs are used to fine-tune the base model, followed by a final refinement stage to ensure the model produces in-depth and detailed responses. Instruction backtranslation  offers an alternative self-alignment method by initially training a backward model to generate instructions from unlabeled web documents using limited seed data. It then iteratively produces new instructions from new web documents and selects high-quality data for self-training. Most code LLM work targets knowledge distillation. Haluptzok et al.  share a relevant idea to our work but only consider program puzzles specified in symbolic forms. This setting cannot be generalized to real-world tasks with natural language involved.

## 6 Limitations and Future Work

We limit our data generation within a \(\)3000 window, skewing our distribution towards medium-sized samples. Therefore, generating and training on long-context instruction-response pairs can be a promising avenue . Second, we gather several negative samples during response generation, which are currently filtered out. These negatives could be used in a reinforcement-learning loop to steer the model away from incorrect responses [31; 53]. Furthermore, the good responses are labeled by test execution, while the generated unit tests might be erroneous, calling for research to study and improve the generation of valid test cases. Finally, we plan to apply SelfCodeAlign to more challenging domains such as complex program generation  and agentic software engineering .

## 7 Conclusion

We introduce SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign-CQ-7B, finetuned from CodeQwen1.5-7B using SelfCodeAlign, outperforms the \(10\) larger CodeLlama-70B-Instruct on HumanEval+ and consistently surpasses CodeQwen1.5 trained with OctoPack on all studied benchmarks. We evaluate SelfCodeAlign across various model sizes, illustrating that stronger base models benefit more from self-alignment than distillation. We also examine the effectiveness of different components in the pipeline, showing that SelfCodeAlign is better than GPT-3.5 and GPT-4o distillation. Overall, we demonstrate for the first time that a strong instruction-tuned code LLM can be created through self-alignment, without expensive human annotations or distillation.