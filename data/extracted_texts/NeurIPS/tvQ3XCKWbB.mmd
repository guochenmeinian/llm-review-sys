# Enriching Disentanglement:

From Logical Definitions to Quantitative Metrics

 Yivan Zhang

The University of Tokyo, RIKEN AIP

Tokyo, Japan

yivanzhang@ms.k.u-tokyo.ac.jp&Masashi Sugiyama

RIKEN AIP, The University of Tokyo

Tokyo, Japan

sugi@k.u-tokyo.ac.jp

###### Abstract

Disentangling the explanatory factors in complex data is a promising approach for generalizable and data-efficient representation learning. While a variety of quantitative metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what properties these metrics truly quantify. In this work, we establish algebraic relationships between logical definitions and quantitative metrics to derive theoretically grounded disentanglement metrics. Concretely, we introduce a compositional approach for converting a higher-order predicate into a real-valued quantity by replacing (i) equality with a strict premetric, (ii) the Heyting algebra of binary truth values with a quantale of continuous values, and (iii) quantifiers with aggregators. The metrics induced by logical definitions have strong theoretical guarantees, and some of them are easily differentiable and can be used as learning objectives directly. Finally, we empirically demonstrate the effectiveness of the proposed metrics by isolating different aspects of disentangled representations.

## 1 Introduction

In _supervised learning_, we usually use a real-valued cost function \(:Y Y_{ 0}\) to measure how close an output \(f(x)\) of a function \(f:X Y\) is to a target label \(y\), i.e., \((f(x),)\), to quantify the cost of inaccurate prediction. Then, we can use the _total cost_ over a collection of input-output pairs to measure the performance of this function. From a functional perspective, this construction induces a quantitative metric \(L:[X,Y][X,Y]_{ 0}\) between functions:1

\[L(f,g):=_{x X}(f(x),g(x)),\] (1)

where \(g:X Y\) is a "_ground-truth function_" that maps each input \(x\) to its target label \(y\). This metric can be used as both _learning objective_ and _evaluation metric_ for the learning model \(f:X Y\). What does \(L(f,g)\) quantify? It quantifies the extent to which two functions \(f\) and \(g\) are _equal_:

\[(f=_{[X,Y]}g):= x X.\ f(x)=_{Y}g(x).@note{footnote}{$\{,\}$ denotes the set of binary truth values: _true_}.\] (2)

Considering the equality as a predicate, we can observe a parallel between

* (binary-valued) equality \(=_{Y}\): \(Y Y\{,\}\) and (real-valued) cost \(:Y Y_{ 0}\),3

* universal quantifier ("_for all_") \( x X\) and summation \(_{x X}\), and
* function equality \(=_{[X,Y]}\): \([X,Y][X,Y]\{,\}\) and total cost \(L:[X,Y][X,Y]_{ 0}\).

We would like to ask: _Is it possible to measure and optimize other properties in the same way?_In _representation learning_(Bengio et al., 2013), measuring and optimizing the performance of a learning model becomes a non-trivial task. The quality of a model cannot always be measured by how close it is to a fixed ground truth. Instead, we often need to consider the properties of the model architecture or learned representation itself, such as _convexity_(Amos et al., 2017), _uniformity_(Wang and Isola, 2020), _invariance_(Kvinge et al., 2022), and _equivariance_(Lee et al., 2019; Brehmer et al., 2023). A proper comprehension of what constitutes good representations and how to assess their quality is important for designing suitable models, learning objectives, and evaluation metrics.

Disentangled representation learning: definitions, metrics, and methods_Disentanglement_ is an important property in representation learning, which intuitively means that different explanatory factors in data should be encoded separately (Bengio et al., 2013). However, disentanglement has no universally agreed-upon formal definition (Higgins et al., 2018; Suter et al., 2019; Shu et al., 2020; Fumero et al., 2021), and it is typically viewed not as a single property but rather as a combination of several requirements (Ridgeway and Mozer, 2018; Eastwood and Williams, 2018; Do and Tran, 2020; Tokui and Sato, 2022). While many metrics for measuring disentanglement have been proposed (Carbonneau et al., 2022), it remains unclear what properties these metrics truly quantify and how they can be optimized directly. Often, a new evaluation metric is introduced along with a new learning method, but it is usually unproven that the method can optimize the new metric (Higgins et al., 2017; Kim and Mnih, 2018; Chen et al., 2018; Li et al., 2020). This lack of theoretical understanding makes it difficult to design learning models that can effectively learn disentangled representations.

A logical and algebraic approach to defining and measuring disentangled representationsRecently, Zhang and Sugiyama (2023) proposed a general and abstract definition of disentanglement, shedding light on the common structures underlying the algebraic, statistical, and topological definitions of disentanglement. It was shown that the abstract concept of _product_(Mac Lane, 1978) underlies an essential property of disentanglement called _modularity_(Ridgeway and Mozer, 2018), and other properties of learning models, such as _informativeness_(Eastwood and Williams, 2018), can also be defined abstractly using only the composition and identity of morphisms. Following this algebraic approach, we aim to derive theoretically grounded quantitative metrics of disentanglement from the logical definitions of the desired properties, extending the parallel between Eqs. (1) and (2).

ContributionsIn this paper, we focus on logically defined properties of disentangled representation learning, such as modularity and informativeness (Section 2). We introduce a compositional approach to converting a _higher-order equational predicate_ into a _real-valued quantity_ (Table 1), which serves as a quantitative metric of the extent to which a function satisfies the predicate (Section 3). Our analysis on the relationship between the logical definitions and the induced quantitative metrics provides theoretical guarantee on the properties of the optimal functions (Theorem 1). Then, we demonstrate the usefulness of this conversion method by deriving quantitative metrics for measuring properties of disentangled representations, and we analyze these metrics in terms of computation, optimization, and differentiability (Section 4). Lastly, we compare the derived metrics with several existing ones in a fully controlled experiment and demonstrate that the proposed metrics are able to isolate different aspects of disentangled representations (Section 5).

## 2 Logical definitions of disentangled representations

In this section, let us first take a closer look at the logical definitions of two properties of disentangled representation learning -- informativeness (Eastwood and Williams, 2018) and modularity (Ridgeway and Mozer, 2018), which are arguably more important than other properties (Carbonneau et al., 2022). We limit our discussion to sets and functions, but the generalization to other morphisms, such as equivariant, stochastic, or continuous functions, is straightforward.

### Informativeness: injectivity or retractability of a learning model

Being informative, expressive, faithful, or useful is a basic requirement for learned representations (Bengio et al., 2013). We want a representation learning model to preserve explanatory factors in data that are informative to the downstream tasks. For functions, this criterion could be formulated as follows: If two factors \(y\) and \(y^{}\) are different, then their representations \(m(y)\) and \(m(y^{})\) extracted by a function \(m:Y Z\) should be different too. This means that the function \(m\) should be _injective_:

**Definition 1** (Injective function).: A function \(m:Y Z\) is _injective_ if

\[p_{}(m:Y Z):= y Y.\; y^{} Y.\;(m(y)=_ {Z}m(y^{}))(y=_{Y}y^{}).\] (3)

Alternatively, because injective functions are precisely functions with _retractions_ (left inverses) [Lawvere and Rosebrough, 2003, Chapter 2], we can measure the _retractability_ instead:

**Definition 2** (Retractable function).: A function \(m:Y Z\) has a _retraction_\(h:Z Y\) if

\[p_{}(m:Y Z):= h:Z Y.\;h m=_{[Y,Y]} _{Y}.\] (4)

Note that these properties are _predicates_\(p_{},p_{}:[Y,Z]\{,\}\) on the set \([Y,Z]\) of all functions from \(Y\) to \(Z\). Analogous to using the total cost in Eq. (1) to measure function equality in Eq. (2), if we want to measure the _injectivity_ in Eq. (3) or _retractability_ in Eq. (4), we need to find quantitative counterparts of the **implication**\(\), **universal quantifier**\(\), and **existential quantifier**\(\) used in their logical definitions. Generally, it is desirable to extend the parallel between Eqs. (1) and (2) to other predicates by finding quantitative operations corresponding to logical connectives and quantifiers. This correspondence allows us to construct and analyze quantitative metrics for machine learning models in a _compositional_ manner [Boole, 1854].

### Modularity: product structure preserved by a learning model

_Modularity_[Ridgeway and Mozer, 2018] is an essential property of disentangled representation learning, which means that the explanatory factors in data, such as the color and shape of an object, are separated into independent components in the learned representation [Bengio et al., 2013].

As shown in Fig. 1, modularity can be defined as follows. We assume that data with multiple explanatory factors (e.g., color and shape) is generated via a function \(g:Y X\) from a product \(Y:=Y_{1} Y_{2}\) of _factors_. An encoder \(f:X Z\) is a function to a product \(Z:=Z_{1} Z_{2}\) of _codes_. Then, an encoder is said to be _modular_ if it can **reconstruct the product structure**, such that the composition \(m:=f g:Y Z\) of the generator \(g\) and the encoder \(f\) is a product function.4

Formally, being a product function is also a property that can be represented as a predicate:

**Definition 3** (Product function).: Let \(Y:=Y_{1} Y_{2}\) and \(Z:=Z_{1} Z_{2}\) be products of sets. A function \(m:Y Z\) is a _product function_ if

\[p_{}(m:Y Z):= m_{1,1}:Y_{1} Z_{1}.\; m_{2,2 }:Y_{2} Z_{2}.\;m=_{[Y,Z]}m_{1,1} m_{2,2}.\] (5)

**Example 1**.: Let us compare the following two functions from \(Y:=\{0,1\}^{2}\) to \(Z:=^{2}\):

\[m:=(0,0)(1,2)\\ (0,1)(3,4)\\ (1,0)(5,6)\\ (1,1)(7,8) m^{}:=(0,0) (a,c)\\ (0,1)(a,d)\\ (1,0)(b,c)\\ (1,1)(b,d)=0 a\\ 1 b\\ m_{1,1}0 c\\ 1 d\\ m_{2,2}}_{\\ 1 d\\ }\] (7)

where \(a\), \(b\), \(c\), and \(d\) are arbitrary real numbers. According to Definition 3, only \(m^{}=m_{1,1} m_{2,2}\) is a product function, whose first/second output depends only on the first/second input.

In Example 1, although \(m\) is not a product function, we want to address the following questions:

* (Metric) Can we quantify the extent to which it resembles a product function?
* (Approximation) Can we find a product function that is closest to it?
* (Differentiability) Can we make it slightly closer to a product function?

Answers to these questions will be given in the following sections.

Figure 1: Disentangled representation learning

## 3 Enrichment: from logic to metric

In Appendices A and B, we describe in detail the theory of converting a higher-order predicate into a real-valued quantity. In this section, we only introduce the conversion procedure using concrete examples and present the theoretical results. A summary of the conversion is given in Table 1.

First of all, let us clarify the terms predicate and quantity. In the realm of classical logic, a _predicate_\(p:A\{,\}\) on a set \(A\) is a function from the set \(A\) to the set \(\{,\}\) of binary truth values. For example, the predicates \(p_{}\), \(p_{}\), and \(p_{}\) in Definitions 1 to 3 are functions from the set \([Y,Z]\) of functions to the set \(\{,\}\). They are _logical definitions_ of some properties of functions. On the other hand, in this work, a _quantity_\(q:A[0,]\) on a set \(A\) is defined as a function to the set \([0,]\) of extended non-negative real numbers. The quantities associated with a predicate will serve as _quantitative metrics_ for the property defined by the predicate.

### From equality predicate to strict premetric

In this work, a predicate of central importance is the _equality predicate_\(=_{A}\): \(A A\{,\}\)[Mazur, 2008]. A quantity associated with the equality predicate should be a strict premetric:

**Definition 4** (Strict premetric).: A _strict premetric_ on a set \(A\) is a function \(d_{A}:A A[0,]\) that

\[ a A.\; a^{} A.\;(d_{A}(a,a^{})=0) (a=_{A}a^{}).\] (8)

### From logical operation to quantitative operation

Next, let us have a look at the logical connectives and quantifiers used in the definitions of properties. The product of sets and functions plays a significant role in this work. Two functions \(f,g:C A B\) to a product are equal if and only if all their component functions are equal:

\[(f=_{[C,A B]}g):=(f_{1}=_{[C,A]}g_{1})(f_{2}=_{[C,B]}g_{2}).@note{ footnote}{For a function $f:C A B$ to a product $A B$, its component functions $f_{1}:C A:=p_{1} f$ and $f_{2}:C A:=p_{2} f$ are denoted by numeric subscripts, where $p_{1}:A B A:=(a,b) a$ and $p_{2}:A B B:=(a,b) b$ are projection functions}.\] (9)

Note that the **conjunction**\(:\{,\}\{,\}\{,\}\), a logical connective, is used in Eq. (9). To obtain a corresponding quantity, we replace it with the _addition_\(+:[0,][0,][0,]\):

\[d_{[C,A B]}(f,g):=d_{[C,A]}(f_{1},g_{1})+d_{[C,B]}(f_{2},g_{2}).\] (10)

The **universal quantifier** on a set \(A\) is a specific (second-order) predicate \(_{A}:\{,\}^{A}\{,\}\) on the set \(\{,\}^{A}\) of predicates. We can replace it with the _supremum_\(:[0,]^{A}[0,]\). We can also choose a function from the (i) _maximum_, (ii) _sum_, (iii) _mean_, and (iv) _mean square_ when the set \(A\) is finite. More generally, we can replace it with a quantity \(_{A}:[0,]^{A}[0,]\) on the set \([0,]^{A}\) of quantities that satisfies some conditions, which we refer to as a (universal) _aggregator_. Intuitively, a universal aggregator should output \(0\) if and only if all inputs are \(0\). Therefore, the median, mode, and range are non-examples. Different choices of aggregators yield metrics with different characteristics in computation and optimization. For example, the function equality predicate

\[(f=_{[A,B]}g):= a A.\;f(a)=_{B}g(a)\] (11)

converts to a quantity whose aggregator \(\) is not limited to the sum (cf. Eqs. (1) and (2)):

\[d_{[A,B]}(f,g):=d_{B}(f(a),g(a)).\] (12)

Dually, we also need to consider the **disjunction**\(\) and the **existential quantifier**\(\). We replace them with the _minimum_ and the _infimum_, respectively. Lastly, we replace the **implication**\(a b\) with the (truncated) _subtraction_\(b}a:=\{b-a,0\}\). These operations are illustrated in Fig. 2.

   &  \\  truth values & \(\{,\}\) & real values & \([0,]\) \\ equality & \(=\) & strict premetric & \(d\) \\ conjunction & \(\) & addition & \(+\) \\ disjunction & \(\) & minimum & \(\) \\ implication & \(\) & subtraction\({}^{*}\) & \(\) \\ universal quantifier & \(\) & aggregator\({}^{**}\) & \(\) \\ existential quantifier & \(\) & infimum & \(\) \\   \({}^{*}\) truncated subtraction: \(b}a:=\{b-a,0\}\)

\({}^{**}\) e.g., maximum, sum, mean, and mean square

Table 1: From logic to metric

### From compound predicate to compound quantity

Following Table 1, we can convert any _compound predicate_ defined using equational predicates and logical operations into a corresponding _compound quantity_ defined using strict premetrics and quantitative operations. Our main result on their relationship is as follows:

**Theorem 1**.: _Let \(p:A\{,\}\) be a predicate on a set \(A\), and let \(q:A[0,]\) be a quantity converted from \(p\) according to Table 1. Then, for any \(a A\), \(q(a)=0\) implies \(p(a)=\). Conversely, for any \(a A\), \(p(a)=\) implies \(q(a)=0\) if and only if \(p\) does not contain the implication._

The implication is special because we must sacrifice logical equivalence for the sake of continuity, which is necessary for gradient-based optimization. We will explore this through a concrete example regarding injectivity in Section 4.3 and discuss it in detail in Appendices B and D.

### (Sub)homomorphism from metric to logic

Finally, for readers interested in the theoretical background, we briefly introduce the following algebraic concepts and a proof sketch underlying Table 1 and Theorem 1.

**Definition 5** (Zero predicate).: The _zero predicate_\(:[0,]\{,\}:=x(x=0)\) is a function that maps \(0\) to true \(\) and any positive value to false \(\).

**Definition 6** ((Sub)homomorphism from a quantity to a predicate).: Let \(A\) be a set. A quantity \(q:A[0,]\) on the set \(A\) is _homomorphic_ to a predicate \(p:A\{,\}\) via the zero predicate \(:[0,]\{,\}\) if \( q=p\), and is _subhomomorphic_ to \(p\) if \( q p\).6

**Definition 7** ((Sub)homomorphism from a quantitative operation to a logical operation).: Let \(n\) be a natural number. An \(n\)-ary quantitative operation \(:[0,]^{n}[0,]\) is _homomorphic_ to a logical operation \(:\{,\}^{n}\{,\}\) via the zero predicate \(:[0,]\{,\}\) if \(=^{n}\), and is _subhomomorphic_ to \(\) if \(^{n}\).7

**Definition 8** ((Sub)homomorphism from an aggregator to a quantifier).: Let \(A\) be a set. An aggregator \(_{A}:[0,]^{A}[0,]\) on the set \(A\) is _homomorphic_ to a quantifier \(_{A}:\{,\}^{A}\{,\}\) via the zero predicate \(:[0,]\{,\}\) if \(_{A}=_{A}^{A}\), and is _subhomomorphic_ to \(_{A}\) if \(_{A}_{A}^{A}\).8

Homomorphic quantities, quantitative operations, and aggregators can be illustrated as follows:

(13)

Figure 2: From predicates and logical operations to quantities and quantitative operationsBased on these algebraic concepts, we can say that strict premetrics are homomorphic to equality predicates, addition is homomorphic to conjunction (since the sum is zero if and only if both addends are zero), minimum is homomorphic to disjunction, truncated subtraction is _subhomomorphic_ to implication, and universal aggregators are homomorphic to the universal quantifier.

Theorem 1 means that any compound quantity is (sub)homomorphic to the corresponding compound predicate if each component (quantities, quantitative operations, aggregators) is (sub)homomorphic to the corresponding component (predicates, logical operations, quantifiers). For implication, we use the truncated subtraction, which is only subhomomorphic, since there is no _continuous_ operation that is homomorphic to implication (see also Appendix D).

More abstractly and concisely, we can say that we replace the **Heyting algebra** of truth values \(\{,\}\) with a _quantale_ of extended non-negative real numbers \([0,]\), and we replace the quantifiers \(\) and \(\) with aggregators \(\) and \(\) (see also Appendix B). In this way, we can derive quantitative metrics for any logically defined properties of learning models _compositionally_.

## 4 Quantitative metrics of disentangled representations

In this section, we demonstrate how to apply the conversion method introduced above to derive quantitative metrics for measuring the modularity (Definition 3) and informativeness (Definitions 1 and 2) of disentangled representations.

In Sections 4.1 and 4.2, we introduce modularity metrics based on two approaches and discuss their differences in terms of computation and optimization. We point out that the main obstacle lies in the optimization step, resulting from the existential quantifiers in the definition. Then, we show that we can derive easily computable and differentiable metrics from a logically equivalent definition. In Section 4.3, we introduce informativeness metrics and present a result of Theorem 1.

### Modularity metrics via product approximation

We begin with _modularity_, which is an essential property of disentangled representation learning. Recall that modularity can be defined using the _product function_ (Definition 3). For easier reference, we provide the following diagram, which shows the domains and codomains of the functions involved in the upcoming discussion:

(14)

From Definition 3, we can derive the following metric:

**Definition 9** (Product approximation).: Let \(m:Y Z\) be a function from a product \(Y:=Y_{1} Y_{2}\) of sets to another product \(Z:=Z_{1} Z_{2}\) of sets. The extent to which \(m\) resembles a _product function_ can be measured by a distance between \(m\) and its best product function approximation:

\[q_{}(m:Y Z):=_{m_{1,1}[Y_{1},Z_{1}]}_{m_{2,2}[Y _{2},Z_{2}]}d_{[Y,Z]}(m,m_{1,1} m_{2,2}).\] (15)

The derivation of \(q_{}\) from \(p_{}\) follows the conversion described in Table 1: replacing the equality \(=_{[Y,Z]}:[Y,Z][Y,Z]\{,\}\) with a strict premetric \(d_{[Y,Z]}:[Y,Z][Y,Z][0,]\) and the existential quantifiers \(\) with the infimum operators \(\).

This modularity metric can be interpreted as a distance from a point \(m[Y,Z]\) to a subset \(\{m_{1,1} m_{2,2} m_{1,1}[Y_{1},Z_{1}],m_{2,2}[Y_{2},Z_{2}]\} [Y,Z]\) of product functions (cf. the Hausdorff distance (Lawvere, 1986; Tuzhilin, 2016)). Following from Theorem 1, \(q_{}(m)=0\) if and only if \(p_{}(m)=\). This means that the minimizers of this metric are precisely product functions.

However, we still face two obstacles: the product operation and the minimization problem. For the product operation, we can employ Eqs. (10) and (12) to rewrite \(q_{}\) into a more computable form:

**Proposition 2**.: _The quantity \(q_{}(m:Y Z)\) equals_

\[_{y_{1} Y_{1}} _{y_{2} Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1,1}^{*}(y_{1}))+ _{y_{2} Y_{2}} _{y_{1} Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2,2}^{*}(y_{2})),\] (16)

_where the functions \(m_{1,1}^{*}:Y_{1} Z_{1}\) and \(m_{2,2}^{*}:Y_{2} Z_{2}\) are given by_

\[m_{1,1}^{*}:Y_{1} Z_{1} :=y_{1}_{z_{1} Z_{1}}_{y_{2} Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),z_{1}),\] (17) \[m_{2,2}^{*}:Y_{2} Z_{2} :=y_{2}_{z_{2} Z_{2}}_{y_{1} Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),z_{2}).\] (18)

A detailed derivation can be found in Appendix C. Note that we can obtain the optimal product function approximation \(m_{1,1}^{*} m_{2,2}^{*}\) explicitly via Eqs. (17) and (18). Intuitively, we need to find an approximation of a (multi)set of codes with one factor fixed and other factors varying, and then we use the aggregation of all the approximation errors as a modularity metric.

The second obstacle -- the minimization problem -- still needs to be addressed. Since the code spaces \(Z_{1}\) and \(Z_{2}\) can be infinite sets, the minimization problem may not have a closed-form minimizer or even an exact solver. Even if an exact solver exists, the solution may not be differentiable with respect to the inputs. Let us examine some concrete examples of \(q_{}\) by choosing different aggregators \(\) in Eqs. (17) and (18). In the following three examples, we assume that the code spaces \(Z_{1}\) and \(Z_{2}\) are Euclidean spaces equipped with the usual Euclidean distances.

**Example 2**.: If the aggregator \(\) is the _supremum_, the best approximation is the _center_ of the smallest bounding sphere (Megiddo, 1983), and the approximation error is the _radius_.

This metric has the advantage of being definable even when the factor spaces \(Y_{1}\) and \(Y_{2}\) are infinite sets, and it can be computed using either randomized (Welzl, 1991) or exact (Fischer et al., 2003) algorithms. However, it is not easy to calculate its gradient. Thus, we cannot use it as a learning objective and directly optimize it using gradient-based optimization.

**Example 3**.: If the aggregator \(\) is the _mean_, the best approximation is the _(geometric) median_(Weiszfeld, 1937), and the approximation error is the _mean absolute deviation around the median_.

It is known that there is no exact algorithm for obtaining the geometric median (Cockayne and Melzak, 1969), but it can be effectively approximated using convex optimization (Cohen et al., 2016). The geometric median has found applications in robust estimation in the fields of statistics and machine learning (Meer et al., 1991; Minsker, 2015; Pillutla et al., 2022; Guerraoui et al., 2023).

**Example 4**.: If the aggregator \(\) is the _mean square_, the best approximation is the _mean_, and the approximation error is the _variance_. In this case, \(q_{}(m)\) can be simplified to

\[}_{y_{1}_{1}} _{y_{2}_{2}}}m_{1}(y_{1},y_{2})+} _{y_{2} Y_{2}}_{y_{1} Y_{1}}}m_{2}( y_{1},y_{2}).\] (19)

The variance is easier to compute and differentiate than the radius of the smallest bounding sphere and the mean absolute deviation around the median, but it is also more susceptible to outliers and noise. Further work could explore the theoretical implications of these metrics, especially in cases where only partial combinations of factors or noisy annotations are available.

Then, let us revisit our motivating example in Example 1:

**Example 5**.: Let us consider the function \(m:\{0,1\}^{2}^{2}\) in Eq. (6). Its best product function approximation is

\[m^{*}:\{0,1\}^{2}^{2}:=(0,0)(2,4) \\ (0,1)(2,6)\\ (1,0)(6,4)\\ (1,1)(6,6)=0 2&\\ \\ }_{m_{1,1}^{*}}0 4 \\ \\ }_{m_{2,2}}\] (20)

because \(m_{1,1}^{*}(0)=2\) is the center/median/mean of the set \(\{m_{1}(0,0)=1,m_{1}(0,1)=3\}\), and so on. The modularity metric is a distance between \(m\) and \(m^{*}\).

### Modularity metrics via constancy

Upon analyzing the metrics above, it becomes evident that what we need is not the best approximation itself (e.g., the mean) but rather the approximation error (e.g., the variance) -- a measure of the _constancy_ of a set of codes. Following this insight, our next objective is to formulate a modularity metric that eliminates the need for an optimization step. Zhang and Sugiyama (2023) have proved that a function is a product function if and only if the _curried functions9_ of its component functions are constant, as shown in the following example:

**Example 6**.: Consider the functions \(m,m^{}:\{0,1\}^{2}^{2}\) in Eqs. (6) and (7) and the curried functions of their second component functions \(m_{2},m_{2}^{}:\{0,1\}^{2}\):

\[m_{2}=(0,0) 2\\ (0,1) 4\\ (1,0) 6\\ (1,1) 800 2 \\ 1 4\\ (0,1) d\\ 1 8 m_{2}^{}=(0,0 ) c\\ (0,1) d\\ (1,0) c\\ (1,1) d00  c\\ 1 d\\ 1 d\] (21)

The curried function \(}:\{0,1\}\{\{0,1\},\}\) is not constant, while \(^{}}\) is constant with value \(\{0 c,1 d\}\{\{0,1\},\}\) (and so is \(^{}}\)), indicating that \(m^{}\) is a product function.

Based on this fact, we propose an alternative approach for measuring modularity:

**Definition 10** (Constancy of curried function).: Let \(m_{1}\) and \(m_{2}\) be the component functions of a function \(m:Y Z\) from a product \(Y:=Y_{1} Y_{2}\) of sets to another product \(Z:=Z_{1} Z_{2}\) of sets. The extent to which \(m\) resembles a product function can be measured by the _constancy_ of the curried functions of \(m_{1}\) and \(m_{2}\):

\[q_{}(m:Y Z):=q_{}(})+q_{ {const}}(}),\] (22)

where \(q_{}\) is a quantity for constant functions.

To complete this construction, we adopt the following definition and metric of the constant function:

**Definition 11** (Constant function).: A function \(f:A B\) is _constant_ if

\[p_{}(f:A B):= a A.\; a^{} A.\;f(a)=_{ B}f(a^{}),\] (23)

which can be measured by

\[q_{}(f:A B):= A}{}d_{B}(f(a),f(a^{})).\] (24)

This constancy metric \(q_{}\) only needs to compute pairwise distances between the outputs, requiring \(|A|^{2}\) times distance computation but no optimization. Incorporating \(q_{}\) into \(q_{}\), we can get the following metric:

**Proposition 3**.: _The quantity \(q_{}(m:Y Z)\) equals_

\[ Y_{1}}{} Y_{2}}{}^{} Y_{2}}{}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1}(y_{1},y_{2}^{ }))\\ + Y_{2}}{} Y_{1}}{ }^{} Y_{1}}{}d_{Z_{2}}(m_{ 2}(y_{1},y_{2}),m_{2}(y_{1}^{},y_{2})).\] (25)

Here are two examples of \(q_{}\) and \(q_{}\) using different aggregators \(\) in Eqs. (24) and (25).

**Example 7**.: If the aggregator \(\) is the _maximum_, \(q_{}\) is the _diameter_ (the maximum pairwise distance) of the outputs. In this case, \(q_{}(m)\) can be simplified to

\[_{y_{1} Y_{1}}*{diam}_{y_{2} Y_{2}}m_{1}(y_{1},y_{2})+ _{y_{2} Y_{2}}*{diam}_{y_{1} Y_{1}}m_{2}(y_{1},y_{2}).\] (26)

**Example 8**.: If the aggregator \(\) is the _mean square_, \(q_{}\) is the mean pairwise squared distance, which equals the _variance_. In this case, \(q_{}(m)\) coincides with Eq. (19).

In summary, Eqs. (19) and (26) are easily computable and differentiable metrics, and their minimizers are precisely product functions. They do not contain any hyperparameters or stochastic components and thus can serve as both learning objectives and evaluation metrics.

### Informativeness metrics

If an encoder \(f:X Z\) is constant, mapping everything to the same value, according to Definition 3, it is perfectly modular. However, a constant encoder is also completely useless. In this subsection, we shift our focus to the property of _informativeness_ -- a measurement of usefulness.

Informativeness is not a unique requirement for disentangled representations. Other representation learning paradigms, such as contrastive learning (Jaiswal et al., 2020; Wang and Isola, 2020) and metric learning (Musgrave et al., 2020), also emphasize the importance of mapping dissimilar data to far-apart locations in the representation space. While one could integrate this requirement into a single disentanglement score (e.g., (Higgins et al., 2017; Kim and Mnih, 2018)), we argue that it is better to evaluate the usefulness of representations separately for a more fine-grained assessment (Carbonneau et al., 2022).

One straightforward way to measure informativeness is to measure how much we can invert the encoding process:

**Definition 12** (Retraction approximation).: Let \(m:Y Z\) be a function. The extent to which \(m\) is _retractable_ can be measured by a distance between the composition of \(m\) and its best retraction approximation and the identity function:

\[q_{}(m:Y Z):=_{h[Z,Y]}d_{[Y,Y]}(h m, _{Y})=_{h[Z,Y]}d_{Y}(h(m(y)),y).\] (28)

This metric \(q_{}\) is derived from Definition 2 following the conversion procedure in Table 1. This informativeness metric also involves an optimization step similar to the modularity metric \(q_{}\), potentially introducing randomness or higher computation costs. Note that we may use a parameterized subset of the set \([Z,Y]\) of all functions from codes \(Z\) to factors \(Y\), such as the set of linear functions. Then, the problem becomes a regression/classification problem, and the metric is the performance of the predictor. A number of existing works adopted this approach and used the accuracy, the area under the ROC curve (AUC-ROC), or the mean squared error (MSE) to measure the informativeness (Ridgeway and Mozer, 2018; Eastwood and Williams, 2018; Eastwood et al., 2023). However, such metrics necessitate additional hyperparameter tuning and are more likely to exhibit varying behavior across different implementations (Carbonneau et al., 2022).

It raises the question of whether we can measure the informativeness of an encoder without approximating its retraction. We propose to measure informativeness by directly measuring the injectivity of the encoding process:

**Definition 13** (Contraction).: Let \(m:Y Z\) be a function. The extent to which \(m\) is _injective_ can be measured by how much \(m\) contracts pairs of inputs:

\[q_{}(m:Y Z):= Y}{}d_{Y}(y,y^{})d_{Z}(m(y ),m(y^{})).\] (29)

This metric \(q_{}\) is derived from Definition 1 following the conversion procedure in Table 1. According to Theorem 1, we know that \(q_{}(m)=0\) if and only if \(m\) is retractable. However, \(q_{}(m)=0\) implies the injectivity of \(m\) but not the other way around:

\[&(p_{}(m)=)\\ }}{}(p_{}(m)=)\\ &}\] (30)

In other words, a minimizer of \(q_{}\) is required to be _non-contractive_, which is a stronger condition than being injective. For example, let us consider the function \(m::=y 0.01 y\). Although it is injective, its outputs are less distinguishable from each other in terms of the Euclidean distance. Therefore, \(q_{}\) still assign a non-zero value to this function.

Although not all injective functions necessarily minimize \(q_{}\), according to Theorem 1, we can guarantee that minimizing \(q_{}\) will not lead to non-injective functions. Moreover, \(q_{}\) does not require training regressors or classifiers to approximate the retraction. Consequently, it does not need any time-consuming hyperparameter tuning or cross-validation like existing informativeness metrics (Eastwood and Williams, 2018; Ridgeway and Mozer, 2018).

[MISSING_PAGE_FAIL:10]