# DeSparsify: Adversarial Attack Against

Token Sparsification Mechanisms

 Oryan Yehezkel\({}^{*}\), Alon Zolfi\({}^{*}\), Amit Baras, Yuval Elovici, Asaf Shabtai

Department of Software and Information Systems Engineering

Ben-Gurion University of the Negev, Israel

{oryanyeh,zolfi,barasa}@post.bgu.ac.il, {elovici,shabtaia}@bgu.ac.il

Equal contribution

###### Abstract

Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (_e.g._, image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. _Token sparsification_ mechanisms have been proposed to address this issue. These mechanisms employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present _DeSparsify_, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our evaluation demonstrates the attack's effectiveness on three token sparsification mechanisms and examines the attack's transferability between them and its effect on the GPU resources. To mitigate the impact of the attack, we propose various countermeasures. The source code is available online1.

## 1 Introduction

In the last few years, vision transformers have demonstrated state-of-the-art performance in computer vision tasks, outperforming traditional convolutional neural networks (CNNs) in various tasks such as image classification, object detection, and segmentation . While vision transformers have excellent representational capabilities, the computational demands of their transformer blocks make them unsuitable for deployment on edge devices. These demands mainly arise from the quadratic number of interactions (inter-/intra-calculations) between tokens . Therefore, to reduce the computational requirements, various techniques have been proposed to improve their resource efficiency. _Token sparsification_ (TS), in which tokens are dynamically sampled based on their significance, is a prominent technique used for this purpose. The TS approaches proposed include: ATS , AdaViT , and A-ViT , each of which adaptively allocates resources based on the complexity of the input image (_i.e._, input-dependent inference) by deactivating uninformative tokens, resulting in improved throughput with a slight drop in accuracy. Despite the fact that TS has been proven to be effective in improving the resource efficiency of vision transformers, their test-time dynamism and average-case performance assumption creates a new attack surface for adversaries aiming to compromise model availability. Practical implications include various scenarios, such as: attacks on cloud-based IoT applications (_e.g._, surveillance cameras) and attacks on real-time DNN inference for resource- and time-constrained scenarios (_e.g._, autonomous vehicles) .

Given the potential impact of availability-oriented attacks, the machine learning research (ML) community has increasingly focused its attention on adversarial attacks aimed at compromising model availability. Shumailov et al.  were at the forefront of research in this emerging domain, introducing sponge examples, which leverage data sparsity to escalate GPU operations, resulting in increased inference times and energy consumption.

Exploiting this technique, Cina et al.  deliberately poisoned models with sponge examples in the training phase to induce delays during the inference phase. The post-processing phase of deep neural networks (DNNs), particularly in the context of object detection  and LiDAR detection , has also been shown to be susceptible to availability-oriented attacks. In addition, dynamic neural networks (DyNNs) , which adapt their structures or parameters based on input during the inference phase, have been found to be vulnerable to adversarial attacks. For example, previous studies demonstrated that layer-skipping and early-exit mechanisms are also vulnerable to malicious inputs that aim to induce worst-case performance [8; 11; 24].

In this paper, we introduce the _DeSparsify_ attack, a novel adversarial attack that targets TS mechanisms, exploiting their test-time dynamism to compromise model availability. To perform our attack, we craft adversarial examples using a custom loss function aimed at thwarting the sparsification mechanism by generating adversarial examples that trigger worst-case performance, as shown in Figure 1. To increase the stealthiness of our adversarial examples in scenarios where anomaly detection mechanisms are employed (_e.g._, monitoring shifts in the predicted distribution), the attack is designed to preserve the model's original classification. The experiments performed in our comprehensive evaluation examine the attack's effect for: _(i)_ different sparsification mechanisms (_i.e._, ATS, AdaViT, and A-ViT); _(ii)_ different transformer models (_i.e._, DeiT , T2T-ViT ); _(iii)_ compare the performance of different attack variations (_i.e._, single-image, class-universal, and universal); and _(iv)_ investigate the adversarial examples' transferability between different TS mechanisms and the effect of ensembles. For example, the results of our attack against ATS show that it can increase the number of floating-point operations by 74%, the memory usage by 37%, and energy consumption by 72%.

Our contributions can be summarized as follows:

* To the best of our knowledge, we are the first to both identify TS mechanisms dynamism as a threat vector and propose an adversarial attack that exploits the availability of vision transformers while preserving the model's original classification.
* We conduct a comprehensive evaluation on various configurations, examining different TS mechanisms and transformer models, reusable perturbations, transferability, and the use of ensembles.
* We discuss countermeasures that can be employed to mitigate the threat posed by our attack.

## 2 Background

Vision transformers [33; 30; 34] consist of a backbone network, which is usually a transformer encoder comprised of \(L\) blocks, each of which consists of a multi-head self-attention layer (MSA) and feedforward network (FFN).

We consider a vision transformer \(f:^{M}\) that receives an input sample \(x\) and outputs \(M\) real-valued numbers that represent the model's confidence for each class \(m M\). The input image \(x^{C W H}\) is first sliced into a sequence of \(N\) 2D patches, which are then mapped into patch embeddings using a linear projection. Next, a learnable class token is appended, resulting in a sequence of size \(N+1\). Finally, positional embeddings are added to the patch embeddings to provide

Figure 1: Token depth distribution in terms of transformer blocks for a clean (top) and adversarial (bottom) image for three TS mechanisms (b)-(d). The colors indicate the maximum depth each token reaches before being discarded. The adversarial image is crafted using the single-image attack variant (Section 4.1), which results in worst-case performance.

positional information. A single-head attention is computed as follows:

\[(Q,K,V)=(}{}})V=AV\] (1)

where \(Q,K,V^{(N+1) d}\) represent the query, key, and value matrices, respectively. These matrices are derived from the output of the previous block of the transformer, denoted as \(Z_{l}\), where \(l\) indicates the \(l\)-th block. For the first block, _i.e._, \(Z_{0}\), the value corresponds to the flattened patches of the input image mentioned above.

## 3 Related Work

### Token sparsification (TS)

In this section, we provide an overview of the three TS mechanisms we focus on in this paper.

**Adaptive Token Sampling (ATS)**. ATS is a differentiable parameter-free module that adaptively downsamples input tokens. It automatically selects an optimal number of tokens in each stage (transformer block) based on the image content. The input tokens in each block are assigned significance scores by employing the attention weights of the classification token in the self-attention layer. A key advantage of ATS is its ability to be seamlessly integrated into pretrained vision transformers without the need for additional parameter tuning.

**AdaViT**. AdaViT is an end-to-end framework for vision transformers that adaptively determines the use of tokens, self-attention heads, and transformer blocks based on input images. A lightweight subnetwork (_i.e._, a decision network) is inserted into each transformer block of the backbone, which learns to make binary decisions on the use of tokens, self-attention heads, and the block's components (MSA and FFN). The decision networks are jointly optimized with the transformer backbone to reduce the computational cost while preserving classification accuracy.

**A-ViT**. A-ViT is an input-dependent spatially-adaptive inference mechanism that halts the computation of different tokens at different depths, reserving computation for discriminative tokens in a dynamic manner. This halting score-based module is incorporated into an existing vision transformer block by allocating a single neuron in the MLP layer to perform this task, _i.e._, it does not require any additional parameters or computation for the halting mechanism.

### Availability-oriented attacks

Confidentiality, integrity, and availability, collectively known as the CIA triad, serve as a foundational model for the design of security systems . In the DNN realm, a significant amount of research performed has been devoted to adversarial attacks, particularly those focused on compromising integrity [29; 6; 21; 22; 28; 32; 36; 37] and confidentiality [1; 12]. However, adversarial attacks targeting the availability of these models have only recently begun to receive attention by the ML research community.

Pioneers in the field of availability-oriented adversarial attacks, Shumailov et al. , introduced sponge examples, an attack designed to compromise the efficiency of vision and NLP models. The authors presented two attacks exploiting: _(i)_ data sparsity - the assumption of data sparsity, which enables GPU acceleration by employing zero-skipping multiplications, and _(ii)_ computation dimensions - the internal representation size of inputs and outputs (_e.g._, in transformers, mapping words to tokens). Both attacks aim to maximize GPU operations and memory accesses, resulting in increased inference time and energy consumption. Taking advantage of the data sparsity attack vector, Cina et al.  proposed sponge poisoning, which aims to compromise a model's performance by targeting it with a sponge attack during the training phase. Boucher et al.  introduced a notable extension of the sponge attack (computation dimension vulnerability), presenting an adversarial strategy for NLP models. This method employs invisible characters and homoglyphs to significantly manipulate the model's output, while remaining imperceptible to human detection.

Designed to enhance computational efficiency by adapting to input data during runtime, DyNNs  have been shown to be susceptible to adversarial attacks. For example, Haque et al.  targeted DNNs employing layer-skipping mechanisms, forcing malicious inputs to go through all of the layers. Hong et al.  fooled DNNs that employ an early-exit strategy (dynamic depth), causing malicious inputs to consistently bypass early exits, thereby inducing worst-case performance. In a unified approach, Pan et al.  proposed a method for generating adversarial samples that are capable of attacking both dynamic depth and width networks.

Another line of research focused on the post-processing phase of DNNs. Shapira et al.  demonstrated that overloading object detection models by massively increasing the total number of candidates input into the non-maximum suppression (NMS) component can result in increased execution times. Building on this, Liu et al.  extended the approach to LiDAR detection models.

In this paper, we present a novel attack vector that has not been studied before, an adversarial attack that targets the availability of efficient transformer-based models that employ TS mechanisms.

## 4 Method

### Threat model

**Adversary's Goals.** We consider an adversary whose primary goal is to generate an adversarial perturbation \(\) that causes TS mechanisms to use _all_ available tokens, _i.e._, no tokens are sparsified. Furthermore, as a secondary goal, to increase the stealthiness of the attack, the adversary aims to maintain the model's original classification.

**Adversary's Knowledge.** To assess the security vulnerability of TS mechanisms, we consider three scenarios: _(i)_ a white-box scenario in which the attacker has full knowledge about the victim model, _(ii)_ a grey-box scenario in which the attacker has partial knowledge about the set of potential models; _(iii)_ a black-box scenario in which the attacker crafts a perturbation on a surrogate model and applies it on a different victim model.

**Attack Variants.** Given a dataset \(\) that contains multiple pairs \((x_{i},y_{i})\), where \(x_{i}\) is a sample and \(y_{i}\) is the corresponding label, we consider three attack variants: _(i)_ single-image - a different perturbation \(\) is crafted for each \(x\), _(ii)_ class-universal - a single perturbation \(\) is crafted for a target class \(m M\), and _(iii)_ universal - a single perturbation \(\) is crafted for all \(x\).

### DeSparsify attack

To achieve the goals presented above, we utilize the PGD attack  with a modified loss function (a commonly used approach [26; 11]). The update of the perturbation \(\) in iteration \(t\) is formulated as:

\[^{t+1}=_{||||_{p}<}(^{t} +(_{}_{(x,y)^{}} (x,y)))\] (2)

where \(\) is the step size, \(\) is the projection operator that enforces \(||||_{p}<\) for some norm \(p\), and \(\) is the loss function. The selection of \(^{}\) depends on the attack variant: _(i)_ for the single-image variant, \(^{}=\{(x,y)\}\); _(ii)_ for the class-universal variant with a target class \(m M\), \(^{}=\{(x,y)|y=m\}\); and _(iii)_ for the universal variant, \(^{}=\).

Next, we describe the proposed custom loss function, which is formulated as follows:

\[=_{}+_{}\] (3)

where \(_{}\) is the attacking component, \(_{}\) is the classification preservation component, and \(\) is a scaling term which is empirically determined using the grid search approach.

The \(_{}\) component, set to achieve the secondary goal, is defined as follows:

\[_{}=_{m=1}^{M}_{}(f_{ m}(x+),f_{m}(x))\] (4)

where \(f_{m}\) denotes the score for class \(m\) and \(_{}\) denotes the cross-entropy loss.

The \(_{}\) component, set to achieve the main goal, will be described separately for each TS mechanism in the subsections below.

#### 4.2.1 Attack on ATS

**Preliminaries.** To prune the attention matrix \(A\), _i.e._, remove redundant tokens, ATS  uses the weights \(A_{\{1,2\}},...,A_{\{1,N+1\}}\) as significance scores, where \(A_{1}\) represents the attention weights of the classification token, and \(A_{1,j}\) represents the importance of the input token \(j\) for the output classification token. The significance score for a token \(j\) is thus given by: \(S_{j}=||V_{j}||}{_{i=2}^{N+1}A_{1,i}||V_{i}||}\) where \(\{i,j 2,,N+1\}\). For multi-head attention, the score is calculated for each head, and those scores are totaled over all the heads. Since the scores are normalized, they can be interpreted as probabilities, and the cumulative distribution function (CDF) of \(S\) can be calculated as \(_{i}=_{j=2}^{j=i}S_{j}\). Given the cumulative distribution function, the token sampling function is obtained by calculating the inverse of the CDF: \((r)=^{-1}(r)=n\), where \(r\) and \(n[2,N+1]\) (which corresponds to a token's index). To obtain \(R\) samples, a fixed sampling scheme is used by choosing: \(r=\{,,,\}\). If a token is sampled more than once, only one instance kept. Next, given the indices of the sampled tokens, the attention matrix is refined by only selecting the rows that correspond to the sampled tokens. For example, in the case in which a token \(j\) in \(S\) is assigned a high significance score, it will be sampled multiple times, which will result in less unique tokens in the final set.

**Attack Methodology.** Since our goal is to prevent the method from sparsifying any tokens, we want the sampling procedure to sample as many unique tokens as possible, _i.e._, \(R^{}=|\{(r^{})|r^{} r\}| R\). The number of unique sampled tokens \(R^{}\) depends on the distribution they are drawn from (the CDF of \(S\)). In an extreme case, when the scores are not balanced (\(S_{j} 1\)), only the dominant token \(j\) will be sampled, _i.e._, \(R^{}=1\). In another extreme case, in which the scores are perfectly balanced, each token will only be sampled once, and none of the tokens will be sparsified, resulting in \(R^{}=R\). Therefore, we want to push the vector \(S\) towards a uniform distribution, which will result in a balanced scores vector.

Formally, let \(\) be a vector representing a uniform distribution. The loss component we propose is formulated as:

\[_{}=_{l}^{L}_{}(S^{l},)\] (5)

where \(_{}\) denotes the Kullback-Leibler (KL) divergence loss and \(S^{l}\) denotes the scores vector of the \(l\)-th block. The use of KL divergence loss enforces the optimization to consider all the elements in the scores vector \(S^{l}\) as a distribution, as opposed to a simple distance metric (_e.g._, MSE loss) that only considers them as independent values.

#### 4.2.2 Attack on AdaViT

**Preliminaries.** In AdaViT , a decision network is inserted into each transformer block to predict binary decisions regarding the use of patch embeddings, self-attention heads, and transformer blocks. The decision network in the \(l\)-th block consists of three linear layers with parameters \(W_{l}=\{W_{l}^{p},W_{l}^{h},W_{l}^{b}\}\) to produce computation usage policies for patch selection, attention head selection, and transformer block selection, respectively. Formally, given the input to the \(l\)-th block \(Z_{l}\), the usage policy matrices for the block are computed as \((m_{l}^{p},m_{l}^{h},m_{l}^{b})=\{W_{l}^{p},W_{l}^{h},W_{l}^{b}\}Z_{l}\), where \(m_{l}^{p}^{N},m_{l}^{h}^{H},m_{l}^{b}^{2}\). Since the decisions are binary, the action of keeping/discarding is resolved by applying Gumbel-Softmax  to make the process differentiable \(M_{l}=GS(m_{l}^{b}),GS(m_{l}^{h}),GS(m_{l}^{p})\), where \(M_{l}\{0,1\}^{(2+H+N)}\) and \(GS\) is the Gumble-Softmax function. For example, the \(j\)-th patch embedding in the \(l\)-th block is kept when \(M_{l,j}^{p}=1\) and dropped when \(M_{l,j}^{p}=0\). It should also be noted that the activation of the attention heads depends on the activation of the MSA.

**Attack Methodology.** The output of the decision network in each block \(M_{l}\) provides a binary decision about which parts will be activated. Therefore, our goal is to push all the decision values towards the "activate" decision, which will result in no sparsification. Practically, we want the Gumbel-Softmax values to be equal to one, _i.e._, \(\{M_{l,i}=1| i\}\).

Formally, we define the loss component as follows:

\[_{}=_{l}^{L}(_{b}^{2} _{}(M_{l}^{b},_{l}^{b})+_{M_{l}^{0}=1}} {H}_{h}^{H}_{}(M_{l}^{h},_{l}^{h})+_{ p}^{N}_{}(M_{l}^{p},_{l}^{p}))\] (6)

where \(_{}\) denotes the MSE loss, \(_{l}\) denotes the target value (set at one), and \(M_{l}^{0}\) denotes the decision regarding the activation of the MSA in block \(l\). We condition the attention heads' term with \(M_{l}^{0}\), to avoid penalizing the activation of the attention heads when the MSA is deactivated (\(M_{l}^{0}=0\)). When \(M_{l}^{0}=0\), the attention heads in that block are also deactivated.

#### 4.2.3 Attack on A-ViT

**Preliminaries.** In A-ViT , a global halting mechanism that monitors all blocks in a joint manner is proposed; the tokens are adaptively deactivated using an input-dependent halting score. For a token \(j\) in block \(l\), the score \(h_{j}^{l}\) is computed as follows:

\[h_{j}^{l}=H(Z_{j}^{l})\] (7)

where \(H()\) is a halting module, and \(h_{j}^{l}\) is enforced to be in the range \(\). As the inference progresses into deeper blocks, the score is simply accumulated over the previous blocks' scores. A token is deactivated when the cumulative score exceeds \(1-\):

\[I_{j}=*{arg\,min}_{n L}_{l=1}^{n}h_{j}^{l} 1-\] (8)

where \(\) is a small positive constant that allows halting after one block and \(I_{j}\) denotes the layer index at which the token is discarded. Once a token is halted in block \(l\), it is also deactivated for all remaining depth \(l>I_{j}\). The halting module \(H()\) is incorporated into a single neuron in the token's embedding - specifically, the first neuron. The neuron is "spared" from the original embedding dimension, and thus no additional parameters are introduced, enabling halting score calculation as:

\[H(Z_{j}^{l})=(*Z_{j,0}^{l}+)\] (9)

where \(Z_{j,0}^{l}\) indicates the first dimension of token \(Z_{j}^{l}\), \(()\) denotes the logistic sigmoid function, and \(\) and \(\) are respectively learnable shifting and scaling parameters that are shared across all tokens.

**Attack Methodology.** As noted above, the decision whether to deactivate a token \(j\) in block \(n\) (and for all the remaining blocks) relies on the cumulative score \(_{l=1}^{n}h_{j}^{l}\). If the cumulative score exceeds the threshold \(1-\), then the token is halted. Therefore, we want the push the cumulative score for _all_ blocks beneath the threshold, and practically, we want to push it towards zero (the minimum value of the sigmoid function). This will result in the use of token \(j\) for all blocks, _i.e._, \(I_{j} L\). Formally, we define the loss component as:

\[_{}=_{j=1}^{N}(_{n=1} ^{L}_{I_{j}<n}_{}_{l=1}^{n}h_{j}^{l},0)\] (10)

where \(_{I_{j}<n}\) is used to avoid penalizing tokens in deeper blocks that have already been halted.

## 5 Evaluation

### Experimental setup

**Models.** We evaluate our attack on the following vision transformer models: _(i)_; data-efficient image transformer  (DeiT) small size (DeiT-s) and tiny size (DeiT-t) versions; _(ii)_ Tokens-to-Token ViT  (T2T-ViT) 19-block version. All models are pretrained on ImageNet-1K, at a resolution of 224x224, where the images are presented to the model as a sequence of fixed-size patches (resolution 16x16).

**Datasets.** We use the ImageNet  and CIFAR-10  datasets, and specifically, the images from their validation sets, which were not used to train the models described above. For the single-imageattack variant, we train and test our attack on 1,000 random images from various class categories. For the class-universal variant, we selected 10 random classes, and for each class we train the perturbation on 1,000 images and test them on unseen images from the same class. Similarly, for the universal variant, we follow the same training and testing procedure, however from different class categories.

**Metrics.** To evaluate the effectiveness of our attack, we examine the following metrics:

* **Token Utilization Ratio (TUR)**: the ratio of active tokens (those included in the computation pipeline during model inference) to the total number of tokens in the vision transformer model.
* **Memory Consumption**: the GPU memory usage during model inference.
* **Throughput**: the amount of time it takes the model to process an input and produce the output.
* **Energy Consumption**: the overall GPU power usage during inference. This metric provides insights into the attack's influence on energy efficiency and environmental considerations.
* **Giga Floating-Point Operations per Second (GFLOPS)**: the number of floating-point operations executed by the model per second.
* **Accuracy**: the performance of the model on its original task.

It should be noted that AdaViT and A-ViT only zero out the redundant tokens (_i.e._, the matrices maintain the same shape), as opposed to ATS which removes them from the computation (_i.e._, the matrices are reshaped). As a result, when evaluating the attack on AdaViT and A-ViT, the values of the hardware metrics (_e.g._, memory, energy) remain almost identical to those of the clean images. The attack's effectiveness will only be reflected in the GFLOPS and TUR values. The GFLOPS are manually computed to simulate the potential benefit (an approach proposed in AdaViT).

**Baselines.** We compare the effectiveness of our attack to that of the following baselines:

* **Clean**: a clean image without a perturbation. We report the results for a clean image processed by both the sparsified (referred to as _clean_) and non-sparsified model (referred to as _clean w/o_). The results for the sparsified model represent the lower bound of our attack, while the results for the non-sparsified model represent the upper bound, _i.e._, the best results our attack can obtain.
* **Random**: a random perturbation sampled from the uniform distribution \((-,)\).
* **Standard PGD**: an attack using the model's original loss function (proposed in ).
* **Sponge Examples**: an attack aimed at increasing the model's activation values.

**Implementation details.** In our attack, we focus on \(_{}\) norm bounded perturbations, and set \(=\), a value commonly used in prior studies . For the attack's step \(\), we utilize a cosine annealing strategy  that decreases from \(\) to 0. We set the scaling term at \(=8 10^{-4}\) (Equation 3). The results are averaged across three seeds. In the Appendix, we report results for other \(\) values and the ablation study we performed on the \(\) value. For the TS mechanisms' hyperparameter configuration, we use the pretrained models provided by the authors and their settings. In addition to the provided pretrained models, we trained the remaining models for AdaViT and A-ViT using the same configurations. For ATS, the sparsification module is applied to blocks 4-12, and the number of output tokens of the ATS module is limited by the number of input tokens, _i.e._, \(R=197\) in the case of DeiT-s. For AdaViT, the decision networks are attached to each transformer block, starting from the second block. For A-ViT, the halting mechanism starts after the first block. The experiments are conducted on a RTX 3090 GPU.

### Results

Here, we present the results for DeiT-s on the ImageNet images. In the Appendix, we report the results for DeiT-t and T2T-ViT; the results on CIFAR-10; the cost of the attacks; and provide some examples for perturbed samples; Overall, we observe similar attack performance patterns for the different models and datasets.

**Effect of adversarial perturbations.** Table 1 presents the results for the various metrics of the different baselines and attack variants for the DeiT-s model when used in conjunction with each of the TS techniques. As can be seen, the baselines are incapable of compromising the sparsification mechanism. The random perturbation performs the same as a clean image, while the standard PGDdoes not affect ATS and only slightly affects AdaViT and A-ViT. The sponge examples, on the other hand, generates perturbations that perform even worse than the clean images, _i.e._, additional tokens are sparsified. The single-image attack variant, in which a perturbation is tailored to each image, results in the greatest performance degradation, increasing the GFLOPS values by 74%, 44%, and 100% for the ATS, AdaViT, and A-ViT, respectively. Note that the crafted perturbations have just a minor effect on the model's classification accuracy.

In general, the A-ViT is the most vulnerable of the three modules, and in this case, our attack achieves near-perfect results, increasing the TUR from 72% to 100%, _i.e._, no tokens are sparsified. The attack's success can be attributed to the fact that A-ViT utilizes a single neuron for the sparsification mechanism, easily bypassed by our attack.

While the attack's performance against AdaViT is the least dominant among the examined TS mechanisms, further analysis reveals that this stems from the overall behavior of its decision network. Figure 1(a) presents the distribution of the tokens used in each of the transformer blocks. As can be seen, even on clean images, the AdaViT mechanism does not use any tokens in blocks 4, 10, and 12. The same behavior is seen in the attention heads in block 4 (Figure 1(b)) and in the block's components in block 10 (Figure 1(c)). This phenomena is also evident in the performance of our attack, which is unable to increase the number of tokens used in these blocks. In the remaining blocks, our attack maximizes the use of tokens. This phenomena could be attributed to the fact that the decision networks in these blocks are overfitted or that these transformer blocks are redundant and do not affect the classification performance even when no sparsification is applied (_i.e._, a vanilla model).

The distribution of the tokens in the different blocks of ATS is presented in Figure 3. When tested on clean images, the ATS module gradually decreases the number of tokens used as the computation progresses to deeper blocks. However, when tested on attacked images, the distribution's mean shifted towards a higher value compared to the clean case, resulting in a large number of tokens used across all blocks. Interestingly, we have seen a special trend in which clean images whose GFLOPS are on the lower end of the spectrum (_i.e._, "easy" images that require less tokens to be correctly classified) are affected by our attack

    &  &  &  \\   &  & GFLOPS & TUR & Accuracy & GFLOPS & TUR & Accuracy & GFLOPS & TUR \\    } & Clean & 88.5\% & 3.09 (0\%) & 0.54 (0\%) & 83.6\% & 2.25 (0\%) & 0.33 (0\%) & 92.8\% & 3.57 (0\%) & 0.72 (0\%) \\  & Random & 83.7\% & 3.10 (0\%) & 0.53 (0\%) & 76.5\% & 2.26 (0\%) & 0.54 (0\%) & 91.4\% & 3.36 (1\%) & 0.71 (3\%) \\  & Standard PGD & 1.1\% & 3.07 (1\%) & 0.54 (1\%) & 0.54 (1\%) & 0.5 (2\%) & 2.48 (10\%) & 0.59 (1\%) & 1.1\% & 3.64 (10\%) & 0.73 (6\%) \\  & Sponge Examples & 44.3\% & 3.06 (5\%) & 0.55 (10\%) & 32.6\% & 2.24 (1\%) & 0.54 (1\%) & 61.9\% & 3.27 (2\%) & 0.66 (1\%) \\  & Clean w/o & - & 3.6 (100\%) & 1.0 (100\%) & - & 4.6 (100\%) & 1.0 (100\%) & - & 4.6 (100\%) & 1.01 (100\%) \\    } & Single & **88.2\%** & **4.20 (74\%)** & **0.88 (75\%)** & **82.5** & **32.77 (44\%)** & **0.78 (46\%)** & **91.8** (46\%)** & **10.15 (100\%)** \\  & Ensemble (Single) & 85.6\% & 3.83 (0\%) & 0.7 (1\%) & 32.5\% & 79.5\% & 3.16 (0\%) & 0.74 (44\%) & 86.8\% & 4.52 (9\%) & 0.98 (94\%) \\  & Class-Universal & - & 33.7\% & 3.40 (21\%) & 0.63 (22\%) & 80.0\% & 2.94 (30\%) & 0.69 (33\%) & 79.8\% & 4.07 (49\%) & 0.85 (99\%) \\  & Universal & 84.4\% & 3.31 (14\%) & 0.62 (15\%) & 77.6\% & 2.71 (19\%) & 0.64 (20\%) & 85.6\% & 3.85 (25\%) & 0.83 (42\%) \\  & Universal Patch & 4.6\% & 3.68 (40\%) & 0.73 (42\%) & 20.0\% & 3.00 (32\%) & 0.70 (35\%) & 71.0\% & **4.6 (100\%)** & **1.0 (100\%)** \\   

Table 1: Evaluation of DeiT-s when used with different TS modules on various baselines and attack variations. Clean w/o denotes the performance of the clean images for the non-sparsified model, and ensemble denotes perturbations that were trained with the three TS mechanisms. The number in parentheses is the percentage change between the clean and clean w/o performance.

Figure 3: Distribution of activated tokens in each ATS block on clean and adversarial images.

Figure 2: Distribution of the (a) tokens, (b) attention heads, and (c) blocks for the AdaViT mechanism when tested on clean and adversarial (single-image variant) images.

more easily, while images whose GFLOPS are initially high are more hard to perturb. For example, for clean images that have an average of 2.7 GFLOPS, our attack is able to increase the GFLOPS to 4.2. On the other hand, clean images that have an average of 3.3 GFLOPS, the GFLOPS of the adversarial counterpart only increase to 3.9. This indicates that easily classified images tend to include more "adversarial space," in which an adversary could step in.

**Universal perturbations.** We also explore the impact of reusable perturbations - class-universal and universal perturbations (Section 4.1). In addition to the standard universal perturbation, which only differs from the single-image variant in terms of the dataset used, we explore another universal variant: a universal patch. The universal patch is trained on the same dataset used for the universal perturbation, however it differs in terms of its perturbation budget, size, and location. We place the patch on the left upper corner of the image and limit its size to \(64 64\), and we do not bound the perturbation budget. As seen in Table 1, the universal variants perform better than the random perturbation baseline against all sparsification modules, confirming their applicability. For example, with the class-universal perturbation, a 21%, 30%, and 49% increase in the GFLOPS values compared to the clean image was observed for the ATS, AdaViT, and A-ViT mechanisms, respectively, while maintaining relatively high accuracy. The universal patch performs even better in terms of attack performance, however its use causes a substantial degradation in the model's accuracy, resulting in a less stealthy attack. While universal and class-specific perturbations demand more resources than perturbations on single images, they possess a notable advantage. A universal perturbation has the ability to influence multiple images or an entire class of images, presenting an efficient means of executing wide-ranging adversarial attacks. This would prove to be beneficial in situations where the attacker seeks to undermine the model across numerous samples with minimal computational effort.

**Transferability and ensemble.** In the context of adversarial attacks, transferability refers to the ability of an adversarial example, generated on a surrogate model, to influence other models. In our experiments, we examine a slightly different aspect in which the effect of perturbations trained on a model with one sparsification mechanism are tested on the same model with a different sparsification mechanism. In Figure 4, we present the transferability results between the TS mechanisms. While perturbations that are trained on ATS and A-ViT, and tested on AdaViT work to some extent, other combinations do not fully transfer. We hypothesize that this occurs due to the distinct mechanism used by each model. Another strategy we evaluate is the ensemble training strategy. In this strategy, the adversarial example is trained concurrently on all of the sparsification mechanisms, and in each training iteration a different mechanism is randomly selected. The goal is to explore the synergistic advantages of leveraging the strengths of multiple models to generate adversarial perturbations that are more broadly applicable. The results of the ensemble training, which are also presented in Figure 4, show that when a perturbation is trained on all TS mechanisms, it is capable of affecting all of them, achieving nearly the same performance as when the perturbations are trained and tested on the same mechanism. In a realistic case in which the adversary has no knowledge or only partial knowledge of the TS mechanism used, the ensemble is an excellent solution.

**Attack's effect on hardware.** We also assess the effect of our attack on hardware, based on several GPU metrics. As noted in Section 5.1, we only report the hardware metric results for the ATS mechanism, as it is the only module that sparsifies the tokens in practice. As seen in Table 2, which presents the results for these metrics, we can see that none of the baselines have an effect on the GPU's performance. As opposed to the baselines, the single-image attack variant increases the memory usage by 37%, the energy consumption by 72%, and the throughput by 8% compared to the clean images. As noted by the authors , the activation of ATS introduces a small amount of overhead associated with I/O operations performed by the sampling algorithm, which translates to longer execution time compared to the vanilla model.

Figure 4: GFLOPS transferability results for the single-image variant attack. Ensemble refers to perturbations that were trained on all modules simultaneously.

## 6 Countermeasures

In response to the challenges posed by DeSparsify, we discuss potential mitigations that can be used to enhance the security of vision transformers that utilize TS mechanisms. In general, based on our evaluation, we can conclude that as the number of parameters involved in the computation of the TS mechanism increases, the model's robustness to the attack grows (_e.g._, a decision network in AdaViT compared to a single neuron in A-ViT). However, when the TS mechanism is based on parameters that were not optimized for this specific goal (_e.g._, ATS attention scores), the model is even less vulnerable. To actively mitigate the threats, an upper bound can be set to the number of tokens used in each transformer block, which can be determined by computing the average number of active tokens in each block on a holdout set. This approach preserves the ability to sparsify tokens while setting an upper bound, balancing the trade-off between performance and security. To verify the validity of this approach, we evaluated two different policies for the token removal when the upper bound is surpassed: random and confidence-based policy. The results show that the proposed approach substantially decreases the adversarial capabilities compared to a clean model, _i.e._, adversarial image GFLOPS are almost reduced the level of clean images. For example, the GFLOPS reduce from 4.2 to 3.17 when tested on ATS (clean images GFLOPS are 3.09). Moreover, we also verified that applying the defense mechanism does not degrade the accuracy on clean images. See the Appendix for details.

## 7 Conclusion

In this paper, we highlighted the potential risk vision transformers deployed in resource-constrained environments face from adversaries that aim to compromise model availability. Specifically, we showed that vision transformers that employ TS mechanisms are susceptible to availability-based attacks, and demonstrated a practical attack that targets them. We performed a comprehensive evaluation examining the attack's impact on three TS mechanisms; various attack variants and the use of ensembles were explored. We also investigated how the attack affects the system's resources. Finally, we discussed several approaches for mitigating the threat posed by the attack. In future work, we plan to explore the attack's effect in other domains (_e.g._, NLP).

**Limitations**. A key limitation of our work is the limited transferability of the attack across different TS mechanisms and models, as it only achieves marginal success. Although we address this by proposing an ensemble training approach, future research could investigate the development of a unified loss function that effectively targets all TS mechanisms.