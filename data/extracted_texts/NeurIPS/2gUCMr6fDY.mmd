# Appendix

TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models

Pum Jun Kim\({}^{1}\) Yoojin Jang\({}^{1}\) Jisu Kim\({}^{2,3,4}\) Jaejun Yoo\({}^{1}\)

\({}^{1}\)Ulsan National Institute of Science and Technology

\({}^{2}\)Seoul National University \({}^{3}\)Inria \({}^{4}\)Paris-Saclay University

{pumjun.kim,softjin,jaejun.yoo}@unist.ac.kr

jkin82133@snu.ac.kr

###### Abstract

We propose a robust and reliable evaluation metric for generative models called Topological Precision and Recall (TopP&R, pronounced "topper"), which systematically estimates supports by retaining only topologically and statistically significant features with a certain level of confidence. Existing metrics, such as Inception Score (IS), Frechet Inception Distance (FID), and various Precision and Recall (P&R) variants, rely heavily on support estimates derived from sample features. However, the reliability of these estimates has been overlooked, even though the quality of the evaluation hinges entirely on their accuracy. In this paper, we demonstrate that current methods not only fail to accurately assess sample quality when support estimation is unreliable, but also yield inconsistent results. In contrast, TopP&R reliably evaluates the sample quality and ensures statistical consistency in its results. Our theoretical and experimental findings reveal that TopP&R provides a robust evaluation, accurately capturing the true trend of change in samples, even in the presence of outliers and non-independent and identically distributed (Non-IID) perturbations where other methods result in inaccurate support estimations. To our knowledge, TopP&R is the first evaluation metric specifically focused on the robust estimation of supports, offering statistical consistency under noise conditions.

## 1 Introduction

In keeping with the remarkable improvements of deep generative models [1; 2; 3; 4; 5; 6; 7; 8; 9], evaluation metrics that can well measure the performance of generative models have also been continuously developed [10; 11; 12; 13; 14]. For instance, Inception Score (IS)  measures the Kullback-Leibler divergence between the real and fake sample distributions. Frechet Inception Score (FID)  calculates the distance between the real and fake supports using the estimated mean and variance under the multi-Gaussian assumption. The original Precision and Recall  and its variants [13; 14] measure fidelity and diversity separately, where fidelity is about how closely the generated samples resemble real samples, while diversity is about whether a generative model can generate samples that are as diverse as real samples.

Considering the eminent progress of deep generative models based on these existing metrics, some may question why we need another evaluation study. In this paper, we argue that we need more reliable evaluation metrics now precisely, because deep generative models have reached sufficient maturity and evaluation metrics are saturated (Table 8 in ). Even more, it has been recently reported that even the most widely used evaluation metric, FID, sometimes doesn't match with the expected perceptual quality, fidelity, and diversity, which means the metrics are not always working properly .

In addition, existing metrics are vulnerable to the existence of noise because all of them rely on the assumption that real data is clean. However, in practice, real data often contain lots of artifacts, such as mislabeled data and adversarial examples [17; 18], which can cause the overestimation of the data distribution in the evaluation pipeline. This error seriously perturbs the scores, leading to a false impression of improvement when developing generative models. See Appendix G.2 for possible scenarios. Thus, to provide more comprehensive ideas for improvements and to illuminate a new direction in the generative field, we need a more robust and reliable evaluation metric.

An ideal evaluation metric should effectively capture significant patterns (signal) within the data, while being robust against insignificant or accidental patterns (noise) that may arise from factors such as imperfect embedding functions, mislabeled data in the real samples, and other sources of error. Note that there is an inherent tension in developing metrics that meet these goals. On one hand, the metric should be sensitive enough so that it can capture real signals lurking in data. On the other hand, it must ignore noises that hide the signal. However, sensitive metrics are inevitably susceptible to noise to some extent. To address this, one needs a systematic way to answer the following three questions: 1) What is signal and what is noise? 2) How do we draw a line between them? 3) How confident are we on the result?

One solution can be to use the idea of Topological Data Analysis (TDA)  and statistical inference. TDA is a recent and emerging field of data science that relies on topological tools to infer relevant features for possibly complex data. A key object in TDA is persistent homology, which observes how long each topological feature would survive over varying resolutions and provides a measure to quantify its significance; _i.e._, if some features persist longer than others over varying resolutions, we consider them as topological signals and vice versa as noise.

In this paper, we combine these ideas to estimate supports more robustly and overcome various issues of previous metrics such as unboundedness, inconsistency, etc. Our main contributions are as follows: we establish 1) a systematic approach to estimate supports via Kernel Density Estimator (KDE) derived under topological conditions; 2) a new metric that is robust to outliers while reliably detecting the change of distributions on various scenarios; 3) a consistency guarantee with robustness under very weak assumptions that are suitable for high dimensional data; 4) a combination of a noise framework and a statistical inference in TDA. Our code is available at TopP&R-NeurIPS 2023.

## 2 Background

To lay the foundation for our method and theoretical analysis, we first explain the previous evaluation method Precision and Recall (P&R). Then, we introduce the main idea of persistent homology and its confidence estimation techniques that bring the benefit of using topological and statistical tools for addressing uncertainty in samples. For the sake of space constraints and to streamline the discussion of our main idea, we only provide a brief overview of the concepts that are relevant to this work and refer the reader to Appendix A or [20; 21; 22; 23] for further details on TDA.

Figure 1: **Illustration of the proposed evaluation pipeline. (a) Confidence band estimation in Section 2, (b) Robust support estimation, and (c) Evaluation via TopP&R in Section 3.**

**Notation.** For any \(x\) and \(r>0\), we use the notation \(_{d}(x,r)=\{y:d(y,x)<r\}\) be the open ball in distance \(d\) of radius \(r\). We also write \((x,r)\) when \(d\) is understood from the context. For a distribution \(P\) on \(^{d}\), we let \((P)\{x^{d}:P((x,r))>0 { for all }r>0\}\) be the support of \(P\). Throughout the paper, we refer to \(()\) as support of \(P\), or simply support, or manifold, but we don't necessarily require the (geometrical) manifold structure on \(()\). Note that, when the support is high dimensional, what we can recover at most through estimation is the partial support. For a kernel function \(K:^{d}\), a dataset \(=\{X_{1},,X_{n}\}^{d}\) and bandwidth \(h>0\), we let the kernel density estimator (KDE) as \(_{h}(x)}_{i=1}^{n}K(}{h})\), and we let the average KDE as \(p_{h}[_{h}]\). We denote by \(P\), \(Q\) the probability distributions in \(^{d}\) of real data and generated samples, respectively. And we use \(=\{X_{1},,X_{n}\}^{d}\) and \(=\{Y_{1},,Y_{m}\}^{d}\) for real data and generated samples possibly with noise, respectively.

**Precision and Recall.** There exist two aspects of generative samples' quality; fidelity and diversity. Fidelity refers to the degree to which the generated samples resemble the real ones. Diversity, on the other hand, measures whether the generated samples cover the full variability of the real samples. Sajidai et al.  was the first to propose assessing these two aspects separately via Precision and Recall(P&R). In the ideal case where we have full access to the probability distributions \(P\) and \(Q\), \(_{P}(Q) Q((P))\), \(_{Q}(P) P((Q))\), which correspond to the max Precision and max Recall in , respectively.

**Persistent homology and diagram.** Persistent homology is a tool in computational topology that measures the topological invariants (homological features) of data that persist across multiple scales, and is represented in the persistence diagram. Formally, let _filtration_ be a collection of subspaces \(=\{_{}^{d}\}_{ }\) with \(_{1}_{2}\) implying \(_{_{1}}_{_{2}}\). Typically, filtration is defined through a function \(f\) related to data. Given a function \(f^{d}\), we consider its sublevel filtration \(\{f^{-1}(-,]\}_{}\) or superlevel filtration \(\{f^{-1}[,)\}_{}\). For a filtration \(\) and for each nonnegative \(k\), we track when \(k\)-dimensional homological features (_e.g._, \(0\)-dimension: connected component, \(1\)-dimension: loop, \(2\)-dimension: cavity, \(\)) appear and disappear. As \(\) increases or decreases in the filtration \(\{_{}\}\), if a homological feature appears at \(_{b}\) and disappears at \(_{d}\), we say that it is born at \(b\) and dies at \(d\). By considering these pairs \(\{(b,d)\}\) as points in the plane \((\{\})^{2}\), we obtain a _persistence diagram_. From this, a homological feature with a longer life length, \(d-b\), can be treated as a significant feature, and a homological feature with a shorter life length as a topological noise, which lies near the diagonal line \(\{(,):\}\)(Figure 1 (b)).

**Confidence band estimation.** Statistical inference has recently been developed for TDA . TDA consists of features reflecting topological characteristics of data, and it is of question to systematically distinguish features that are indeed from geometrical structures and features that are insignificant or due to noise. To _statistically_ separate topologically significant features from topological noise, we employ confidence band estimation. Given the significance level \(\), let confidence band \(c_{}\) be the bootstrap bandwidth of \(_{h}-p_{h}_{}\), computed as in Algorithm 1 (see Appendix H.2). Then it satisfies \(_{n}(_{h}-p_{h}_{ }<c_{}) 1-\), as in Proposition D.2 (see Appendix D). This confidence band can simultaneously determine significant topological features while filtering out noise features. We use \(c_{}\) and \(c_{}\) to denote the confidence band defined under significance level \(\) according to the datasets \(\) and \(\). In later sections, we use these tools to provide a more rigorous way of scoring samples based on the confidence level we set.

## 3 Robust support estimation for reliable evaluation

Current evaluation metrics for generative models typically rely on strong regularity conditions. For example, they assume samples are well-curated without outliers or adversarial perturbation, real or generative models have bounded densities, etc. However, practical scenarios are wild: both real and generated samples can be corrupted with noise from various sources, and the real data can be very sparsely distributed without density. In this work, we consider more general and practical situations, wherein both real and generated samples can have noises coming from the sampling procedure, remained uncertainty due to data or model, etc. See Appendix G.2 for more on practical scenarios.

Overview of our metricWe design our metric to evaluate the performance very conservatively. Our metric is based on topologically significant data structures with statistical confidence above a certain level. Toward this, we apply KDE as a function \(f\) to define a filtration, which allowsus to approximate the support with data through \(\{f^{-1}[,)\}_{}\). Since the significance of data comprising the support is determined by the life length of homological features, we calculate \(c_{}\) that enables us to systematically separate short/long lifetimes of homological features. We then estimate the supports with topologically significant data structure via superlevel set \(f^{-1}[c_{},)\) and finally, we evaluate fidelity and diversity with the estimated supports. We have collectively named this process TopP&R. By its nature, TopP&R is bounded and yields consistent performance under various conditions such as noisy data points, outliers, and even with long-tailed data distribution.

### Topological precision and recall

To facilitate our discussion, we rewrite the precision in Section 2 as \(_{P}()=Q((P) (Q))/Q((Q))\) and define the precision of data points as

\[_{P}()^{m}1(Y _{j}(P)(Q))}{_{j=1}^ {m}1(Y_{j}(Q))},\] (1)

which is just replacing the distribution \(Q\) with the empirical distribution \(_{j=1}^{m}_{Y_{j}}\) of \(Y\) in the precision. Similarly,

\[_{Q}()^{n}1(X_{ i}(Q)(P))}{_{i=1}^{n}1 (X_{i}(P))}.\] (2)

In practice, \((P)\) and \((Q)\) are not known a priori and need to be estimated, and these estimates should be robust to noise since we allow it now. For this, we use the KDE \(_{h_{n}}(x)^{T}}_{i=1}^{n}K(}{h_{n}})\) of \(\) and the bootstrap bandwidth \(c_{}\) of \(_{h_{n}}-p_{h_{n}}_{}\), where \(h_{n}>0\) and a significance level \((0,1)\) (Section 2). Then, we estimate the support of \(P\) by the superlevel set at \(c_{}\)1 as \((P)=_{h_{n}}^{-1}[c_{},)\), which allows to filter out noise whose KDE values are likely to be small. Similarly, the support of \(Q\) is estimated: \((Q)=_{h_{m}}^{-1}[c_{},)\), where \(_{h_{m}}(x)^{T}}_{j=1}^{m}K(}{h_{m}})\) is the KDE of \(\) and \(c_{}\) is the bootstrap bandwidth of \(_{h_{m}}-q_{h_{m}}_{}\)

For the robust estimates of the precision, we apply the support estimates to \(_{P}()\) and \(_{Q}()\) and define the topological precision and recall (TopP&R) as

\[_{}() ^{m}1(Y_{j}(P) (Q))}{_{j=1}^{m}1(Y_{j} (Q))}\] \[=^{m}1(_{h_{n}}(Y_{j})>c_{},\ _{h_{m}}(Y_{j})>c_{})}{_{j=1}^{m}1(_{h _{m}}(Y_{j})>c_{})},\] (3) \[_{}() ^{n}1(_{h_{m}}(X_{i})>c_{ },\ _{h_{n}}(X_{i})>c_{})}{_{i=1}^{n}1(_{h _{n}}(X_{i})>c_{})}.\] (4)

The kernel bandwidths \(h_{n}\) and \(h_{m}\) are hyperparameters, and we provide guidelines to select the optimal bandwidths \(h_{n}\) and \(h_{m}\) in practice (See Appendix H.4).

### Bandwidth estimation using bootstrapping

Using the bootstrap bandwidth \(c_{}\) as the threshold is the key part of our estimator (TopP&R) for robustly estimating \((P)\). As we have seen in Section 2, the bootstrap bandwidth \(c_{}\) filters out the topological noise in topological data analysis. Analogously, using \(c_{}\) allows to robustly estimate \((P)\). When \(X_{i}\) is an outlier, its KDE value \(_{h}(X_{i})\) is likely to be small as well as the values at the connected component generated by \(X_{i}\). So those components from outliers are likely to be removed in the estimated support \(_{h}^{-1}[c_{},)\). Higher dimensional homological noises are also removed. Hence, the estimated support denoises topological noise and robustly estimates \((P)\). See Appendix C for a more detailed explanation.

Now that we are only left with topological features of high confidence, this allows us to draw analogies to confidence intervals in statistical analysis, where the uncertainty of the samples is treatedby setting the level of confidence. In the next section, we show that TopP&R not only gives a more reliable evaluation score for generated samples but also has good theoretical properties.

### Addressing the curse of dimensionality

As discussed in Section 3.2, getting the bootstrap bandwidth \(c_{}\) with a theoretical guarantee plays a key role in our metric, and the choice of KDE as a filtration function is inevitable, as in Remark 4.4. However, computing the support of a high-dimensional feature with KDE demands significant computation, and the accuracy is low due to low density values. This hinders an efficient and correct evaluation in practice. To address this issue, we apply a random projection into a low-dimensional space by leveraging the Johnson-Lindenstrauss Lemma (Lemma B.1). This lemma posits that using a random projection effectively preserves information regarding distances and homological features, composed of high-dimensional features, in a low-dimensional representation. Furthermore, we have shown that random projection does not substantially reduce the influence of noise, nor does it affect the performance of TopP&R under various conditions with complex data (Section 5, I.7, and I.8).

## 4 Consistency with robustness of TopP&R

The key property of TopP&R is consistency with robustness. The consistency ensures that, the precision and the recall we compute from the _data_ approaches the precision and the recall from the _distribution_ as we have more samples. The consistency allows to investigate the precision and recall of full distributions only with access to finite sampled data. TopP&R achieves consistency with robustness, that is, the consistency holds with the data possibly corrupted by noise. This is due to the robust estimation of supports with KDE with confidence bands.

We demonstrate the statistical model for both data and noise. Let \(P\), \(Q\), \(\), \(\) be as in Section 2, and let \(^{0},^{0}\) be real data and generated data without noise. \(\), \(\), \(^{0}\), \(^{0}\) are understood as multisets, _i.e._, elements can be repeated. We first assume that the uncorrupted data are IID.

**Assumption 1**.: _The data \(^{0}=\{X_{1}^{0},,X_{n}^{0}\}\) and \(^{0}=\{Y_{1}^{0},,Y_{m}^{0}\}\) are IID from \(P\) and \(Q\), respectively._

In practice, the data is often corrupted with noise. We consider the adversarial noise, where some fraction of data are replaced with arbitrary point cloud data.

**Assumption 2**.: _Let \(\{_{k}\}_{k}\) be a sequence of nonnegative real numbers. Then the observed data \(\) and \(\) satisfies \(|^{0}|=n_{n}\) and \(|^{0}|=m_{m}\)._

In the adversarial model, we control the level of noise by the fraction \(\), but do not assume other conditions such as IID or boundedness, to make our noise model very general and challenging.

For distributions and kernel functions, we assume weak conditions, detailed in Assumption A1 and A2 in Appendix D. Under the data and the noise models, TopP&R achieves consistency with robustness. That is, the estimated precision and recall are asymptotically correct with high probability even if up to a portion of \(1/\) or \(1/\) are replaced by adversarial noise. This is due to the robust estimation of the support with the kernel density estimator with the confidence band of the persistent homology.

**Proposition 4.1**.: _Suppose Assumption 1, 2, A1, A2 hold. Suppose \( 0\), \(h_{n} 0\), \(nh_{n}^{d}\), \(nh_{n}^{-d}_{n}^{2} 0\), and similar relations hold for \(h_{m}\), \(_{m}\). Then,_

\[|_{}()-_{ }()| =O_{}(Q(B_{n,m})+_{m}),\] \[|_{}()-_{ }()| =O_{}(P(A_{n,m})+_{n}),\]

_for fixed sequences of sets \(\{A_{n,m}\}_{n,m},\{B_{n,m}\}_{n,m}\) with \(P(A_{n,m}) 0\) and \(Q(B_{n,m}) 0\) as \(n,m\)._

**Theorem 4.2**.: _Under the same condition as in Proposition 4.1,_

\[|_{}()-_ {}(Q)| =O_{}(Q(B_{n,m})+_{m}),\] \[|_{}()-_{ }(P)| =O_{}(P(A_{n,m})+_{n}).\]

Since \(P(A_{n,m}) 0\) and \(Q(B_{n,m}) 0\), these imply consistencies of TopP&R. In fact, additionally under minor probabilistic and geometrical assumptions, \(P(A_{n,m})\) and \(Q(B_{n,m})\) are of order \(h_{m}+h_{n}\).

**Lemma 4.3**.: _Under the same condition as in Proposition 4.1 and additionally under Assumption A3, A4, \(P(A_{n,m})=O(h_{n}+h_{m})\) and \(Q(B_{n,m})=O(h_{n}+h_{m})\)._

_Remark 4.4_.: Consistency guarantees from Proposition 4.1 and Theorem 4.2 are in principle due to the uniform convergence of KDE over varying bandwidth \(h_{n}\) (Proposition D.2). Once we replace estimating the support with KDE by k-NN or something else, we wouldn't have consistency guarantees. Hence, using the KDE is an essential part for the theoretical guarantees of TopP&R.

Our theoretical results in Proposition 4.1 and Theorem 4.2 are novel and important in several perspectives. These results are among the first theoretical guarantees for evaluation metrics for generative models as far as we are aware of. Also, as in Remark D.1, assumptions are very weak and suitable for high dimensional data. Also, robustness to adversarial noise is provably guaranteed.

## 5 Experiments

A good evaluation metric should not only possess desirable theoretical properties but also effectively capture the changes in the underlying data distribution. To examine the performance of evaluation metrics, we carefully select a set of experiments for sanity checks. With toy and real image data, we check 1) how well the metric captures the true trend of underlying data distributions and 2) how well the metric resists perturbations applied to samples.

We compare TopP&R with several representative evaluation metrics that include Improved Precision and Recall (P&R) , Density and Coverage (D&C) , Geometric Component Analysis (GCA) , and Manifold Topology Divergence (MTD)  (Appendix F). Both GCA and MTD are the recent evaluation metrics that utilize topological features to some extent; GCA defines precision and recall based on connected components of \(P\) and \(Q\), and MTD measures the distance between two distributions using the sum of lifetimes of homological features. For all the experiments, linear random projection to 32 dimensions is additionally used for TopP&R, and the shaded area of all figures denotes the \( 1\) standard deviation for ten trials. For a fair comparison with existing metrics, we have utilized 10k real and fake samples for all experiments. For more details, please refer to Appendix H.1.

### Sanity checks with toy data

Following , we first examine how well the metric reflects the trend of \(\) moving away from \(\) and whether it is suitable for finding mode-drop phenomena. In addition to these, we newly design several experiments that can highlight TopP&R's favorable theoretical properties of consistency with robustness in various scenarios.

#### 5.1.1 Shifting the generated feature manifold

We generate samples from \((,I)\) and \((,I)\) in \(^{64}\), where \(\) is a vector of ones and \(I\) is an identity matrix. We examine how each metric responds to shifting \(\) with \([-1,1]\) while there are outliers at \(^{64}\) for both \(\) and \(\) (Figure 2). We discovered that GCA struggles to detect changes using its default hyperparameter configuration, and this issue persists even after performing an exhaustive hyperparameter sweep. Since empirical tuning of the hyperparameters is required for each dataset, utilizing GCA in practical applications proves to be challenging (Appendix F). Both improved P&R and D&C behave pathologically since these methods estimate the support via the k-nearest neighbor algorithm, which inevitably overestimate the underlying support when there are outliers. For example, when \(<0.5\), Recall returns a high-diversity score, even though the true supports of \(\) and \(\) are actually far apart. In addition, P&R does not reach 1 in high dimensions even when \(=\). D&C  yields better results than P&R because it consistently uses \(\) (the real data distribution) as a reference point, which typically has fewer outliers than \(\) (the fake data distribution). However, there is no guarantee that this will always be the case in practice [17; 18]. If an outlier is present in \(\), D&C also returns an incorrect high-fidelity score at \(>0.5\). On the other hand, TopP&R shows a stable trend unaffected by the outlier, demonstrating its robustness.

#### 5.1.2 Dropping modes

We simulate mode-drop phenomena by gradually dropping all but one mode from the fake distribution \(\) that is initially identical to \(\) (Figure 3). Here, we consider the mixture of Gaussians with sevenmodes in \(^{64}\). We keep the number of samples in \(\) constant so that the same amount of decreased samples are supplemented to the first mode which leads fidelity to be fixed to 1. We observe that the values of Precision fail to saturate, _i.e._, mainly smaller than 1, and the Density fluctuates to a value greater than 1, showing their instability and unboundedness. Recall and GCA do not respond to the simultaneous mode drop, and Coverage decays slowly compared to the reference line. In contrast, TopP performs well, being held at the upper bound of 1 in sequential mode drop, and TopR also decreases closest to the reference line in simultaneous mode drop.

#### 5.1.3 Tolerance to Non-IID perturbations

Robustness to perturbations is another important aspect we should consider when designing a metric. Here, we test whether TopR behaves stably under two variants of noise cases (see Section G.3); 1) **scatter noise**: replacing \(X_{i}\) and \(Y_{j}\) with uniformly distributed noise and 2) **swap noise**: swapping the position between \(X_{i}\) and \(Y_{j}\). These two cases all correspond to the adversarial noise model of Assumption 2. We set \((=0,I)^{64}\) and \((=1,I)^{64}\) where \(=1\), and thus an ideal evaluation metric must return zero for both fidelity and diversity. In the result, while the GCA precision is relatively robust to the scatter noise, GCA recall tends to be sensitive to the swap noise. In both cases, we find that P&R and D&C are more sensitive while TopP&R remains relatively stable until the noise ratio reaches \(15\%\) of the total data, which is a clear example of the weakness of existing metrics to perturbation (Figure 4).

Figure 4: Behaviors of evaluation metrics on Non-IID perturbations. We replace a certain percentage of real and fake data (a) with random uniform noise or (b) by switching some of real and fake data.

Figure 3: Behaviors of evaluation metrics for (a) sequential and (b) simultaneous mode-drop scenarios. The horizontal axis shows the concentration ratio on the distribution centered at \(=0\).

Figure 2: Behaviors of evaluation metrics for outliers on real and fake distribution. For both real and fake data, the outliers are fixed at \(3^{64}\), and the parameter \(\) is shifted from -1 to 1.

### Sanity check with Real data

Now that we have verified the metrics on toy data, we move on to real data. Just like in the toy experiments, we concentrate on how the metrics behave in extreme situations, such as outliers, mode-drop phenomena, perceptual distortions, etc. We also test different image embedders, including pretrained VGG16 , InceptionV3 , and SwAV .

#### 5.2.1 Dropping modes in Baby ImageNet

We have conducted an additional experiment using Baby ImageNet  to investigate the sensitivity of TopP&R to mode-drop in real-world data. The performance of each metric (Figure A4) is measured with the identical data while simultaneously dropping the modes of nine classes of Baby ImageNet, in total of ten classes. Since our experiment involves gradually reducing the fixed number of fake samples until nine modes of the fake distribution vanish, the ground truth diversity should decrease linearly. From the experimental results, consistent to the toy result of Figure 3, both D&C and P&R still struggle to respond to simultaneous mode dropping. In contrast, TopP&R consistently exhibit a high level of sensitivity to subtle distribution changes. This notable capability of TopP&R can be attributed to its direct approximation of the underlying distribution, distinguishing it from other metrics. In addition, we perform the experiments on a dataset with long-tailed distribution and find that TopP&R captures the trend well even when there are minority sets (Appendix I.1). This again shows the reliability of TopP&R.

#### 5.2.2 Robustness to perturbations

To test the robustness of our metric against the adversarial noise model of Assumption 2, we test both scatter-noise and swap noise scenarios with real data (see Section G.3). In the experiment, following Kynkaanniemi et al. , we first classify inliers and outliers that are generated by StyleGAN . For scatter noise we add the outliers to the inliers and for swap noise we swap the real FFHQ images with generated images. Under these specific noise conditions, Precision shows similar or even better robustness than Density (Figure 5). On the other hand, Coverage is more robust than Recall. In both cases, TopP&R shows the best performance, resistant to noise.

#### 5.2.3 Sensitiveness to the noise intensity

One of the advantages of FID  is that it is good at estimating the degrees of distortion applied to the images. Similarly, we check whether the F1-score based on TopP&R provides a reasonable evaluation according to different noise levels. As illustrated in Figure 6 and A5, \(\) and \(\) are sets

Figure 5: Comparison of evaluation metrics on Non-IID perturbations using FFHQ dataset. We replaced certain ratio of \(\) and \(\) (a) with outliers and (b) by switching some of real and fake features.

Figure 6: Verification of whether TopP&R can make an accurate quantitative assessment of noisy image features. Gaussian noise, gaussian blur, salt and pepper, and black rectangle noise are added to the FFHQ images and embedded with T4096.

of reference and noisy features, respectively. The experimental results show that TopP&R actually reflects well the different degrees of distortion added to the images while a similar topology-based method MTD shows inconsistent behavior to the distortions.

#### 5.2.4 Ranking between generative models

The alignment between FID (or KID) and perceptual evaluation has been well-established in prior research, and these scores are widely used as a primary metric in the development of generative models due to its close correspondence with human perception. Consequently, generative models have evolved to align with FID's macroscopic perspective. Therefore, we believed that the order determined by FID at a high level captures to some extent the true performance hierarchy among models, even if it may not perfectly reflect it. In other words, if the development of generative models based on FID leads to genuine improvements in generative performance and if there is a meaningful correlation, similar rankings should be maintained even when the representation or embedding model changes. From this standpoint, while other metrics exhibit fluctuating rankings, TopP&R consistently provides the most stable and consistent results similar to both FID and KID. To quantitatively compare the similarity of rankings across varying embedders by different metrics, we have computed mean Hamming Distance (MHD) (Appendix H.6) where lower value indicates more similarity. TopP&R, P&R, D&C, and MTD have MHDs of 1.33, 2.66, 3.0, and 3.33, respectively.

## 6 Conclusions

Many works have been proposed recently to assess the fidelity and diversity of generative models. However, none of them has focused on the accurate estimation of support even though it is one of the key components in the entire evaluation pipeline. In this paper, we proposed topological precision and recall (TopP&R) that provides a systematical fix by robustly estimating the support with both topological and statistical treatments. To the best of our knowledge, TopP&R is the first evaluation metric that offers statistical consistency under noisy conditions, which may arise in real practice. Our theoretical and experimental results showed that TopP&R serves as a robust and reliable evaluation metric under various embeddings and noise conditions, including mode drop, outliers, and Non-IID perturbations. Last but not least, TopP&R provides the most consistent ranking among different generative models across different embeddings via calculating its F1-score.

    & StyleGAN2 & ReACGAN & BigGAN & PDGAN & ACGAN & WGAN-GP \\   & **FID** (\(\)) & 3.78 (1) & 3.87 (2) & 4.16 (3) & 31.54 (4) & 33.39 (5) & 107.68 (6) \\  & **KID** (\(\)) & 0.002 (1) & 0.012 (3) & 0.011 (2) & 0.025 (4) & 0.029 (5) & 0.137 (6) \\  & **TopP&R** (\(\)) & 0.9769 (1) & 0.8457 (2) & 0.7751 (3) & 0.7339 (4) & 0.6951 (5) & 0.0163 (6) \\  & **D**\&C (\(\)) & 0.9626 (2) & 0.9409 (3) & 1.1562 (1) & 0.4383 (4) & 0.3883 (5) & 0.1913 (6) \\  & **P**\&R (\(\)) & 0.6232 (1) & 0.3320 (2) & 0.3278 (3) & 0.1801 (4) & 0.0986 (5) & 0.0604 (6) \\  & **MTD** (\(\)) & 2.3380 (3) & 2.2687 (2) & 1.4473 (1) & 7.0188 (4) & 8.0728 (5) & 11.498 (6) \\   & **TopP&R** (\(\)) & 0.9754 (1) & 0.5727 (3) & 0.7556 (2) & 0.4021 (4) & 0.3463 (5) & 0.0011 (6) \\  & **D**\&C (\(\)) & 0.9831 (3) & 1.0484 (1) & 0.9701 (4) & 0.9872 (2) & 0.8971 (5) & 0.6372 (6) \\  & **P**\&R (\(\)) & 0.6861 (1) & 0.1915 (3) & 0.3526 (2) & 0.0379 (4) & 0.0195 (5) & 0.0001 (6) \\  & **MTD** (\(\)) & 25.757 (4) & 25.826 (3) & 34.755 (5) & 24.586 (2) & 23.318 (1) & 41.346 (6) \\  & **TopP&R** (\(\)) & 0.9093 (1) & 0.3568 (3) & 0.5578 (2) & 0.1592 (4) & 0.1065 (5) & 0.0003 (6) \\  & **D**\&C (\(\)) & 1.0732 (1) & 0.9492 (3) & 1.0419 (2) & 0.6328 (4) & 0.4565 (5) & 0.0721 (6) \\  & **P**\&R (\(\)) & 0.5623 (1) & 0.0901 (3) & 0.1459 (2) & 0.0025 (4) & 0.0000 (6) & 0.0002 (5) \\  & **MTD** (\(\)) & 1.1098 (1) & 1.5512 (3) & 1.3280 (2) & 1.8302 (4) & 2.2982 (5) & 4.9378 (6) \\   

Table 1: Generative models trained on CIFAR-10 are ranked by FID, KID, MTD, and F1-scores based on TopP&R, D&C and P&R, respectively. The \(\) and \(\) are embedded with InceptionV3, VGG16, and SwAV. The number inside the parenthesis denotes the rank based on each metric.