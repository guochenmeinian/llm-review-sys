# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

obtaining labeled data is challenging (Mirsky et al., 2018; Li et al., 2022), and the computational capabilities of network devices are limited, making it difficult to deploy NNs.

Recently, Jiang & Zhao (2020) tries to combine the advantages of symbolic rules and neural networks. They transform multi-string matching patterns into a trainable recurrent neural network (RNN), which can benefit from the high accuracy of RNN while being deployable in cold-start scenarios. However, multi-string matching patterns can only match simple strings, which is not sufficient to represent complex text patterns. Regular expression matching is a more versatile tool than multi-string matching for matching complex text patterns, thanks to the representation capacity offered by its syntaxes such as Kleene star (e.g., '*'), counting constraint (e.g., "{N, M}"), character class (e.g., "[a-fA-F0-9]"), and hexadecimal numbers (e.g., "vff"). As such, jointly exploiting the advantages of REs and NNs offers new opportunities for network device affordable solutions with high accuracy and throughput. Nevertheless, two challenges need to be addressed to achieve this high-level goal: 1) how to preserve expert knowledge from REs and develop learning models with adequate accuracy in cold-start scenarios; and 2) how to produce learning models that fit in the network devices while retaining good performance. Since network devices have limited computation and memory resources, deploying complex learning models transformed from regular expressions, e.g., NNs, in these devices is often difficult or even impossible.

In this paper, we propose Metis1, an intelligent and general framework that converts RE-based systems into learning models, which offer superior accuracy and throughput when deployed on network devices. Our codes are available at Github2. Metis preserves expert knowledge in RE-based systems for adequate cold-start performance and exploits NNs' ability to further improve performance using rich labeled data. First, we extract bytes from network packets as input features and construct byte-level deterministic finite-state automata (DFAs) from network rules, i.e., REs. Second, the DFAs are transformed into byte-level RNNs (BRNNs). Without training data, the BRNNs preserve the accuracy of RE-based systems for adequate performance in cold-start scenarios. When enough labeled data is collected, BRNNs' performance can be further improved by training with data. Third, we train pooling soft random forests (PSRFs) under the instruction of BRNNs using semi-supervised knowledge distillation (SSKD). Through the SSKD, we transfer the superior learning capability of BRNNs to PSRFs, which can be easily deployed on network devices for line-speed processing, i.e., \(100\) Gbps. To the best of our knowledge, this is the first method to explore the use of model inference as an alternative to RE matching in network scenarios. The key contributions of this paper are as follows: 1) We propose Metis, an intelligent general framework that converts RE-based systems to learning models deployable on network devices. 2) We develop a method for converting complex REs into DFA and subsequently into trainable BRNNs. This method allows us to maintain the accuracy of the RE-based system while leveraging the flexibility and capabilities of NNs. 3) We propose a novel semi-supervised distillation algorithm SSKD, and a student model PSRF for RE matching to achieve lightweight deployment of RE-based systems in network devices. 4) We collect network traffic data from a large data center for three weeks that reflects real network conditions. Experiments on the datasets demonstrate that Metis outperforms original RE-based systems and other baselines.

## 2 Related Work

In this section, we introduce the network rules and modern network devices, highlight the problems of the state-of-the-art in these fields, and discuss the opportunities provided by knowledge distillation.

**Network rules.** Network rules serve as a fundamental building block for many network security applications, e.g., network intrusion systems (Roesch, 1999; Project, 2022), application identification systems (ntop, 2022), web firewalls (Trustwave, 2022) and some network censorship systems (Hoang & Niaki, 2021). REs are one of the most representative and useful forms of network rules. In many network security applications, such as Snort (Roesch, 1999) and Suricata (Foundation, 2022), REs are used to inspect whether the payload of a packet matches any predefined network rules. With high interpretability and no requirement for a training phase, RE-based systems can be quickly deployed with decent performance in network scenarios. However, the matching speed of regular expressions usually becomes a bottleneck since the pattern matching has to inspect every byte of a packet against a set of rules Wang and Zhang (2021). Moreover, RE-based systems cannot boost their accuracy by training on labeled data and thus often underperform learning-based models in data-rich scenarios.

Machine learning in-network.Learning algorithms, especially deep learning ones, have been employed in the network field to leverage their superior learning abilities, e.g., network traffic classification (Barradas and Santos, 2021), malicious traffic detection (Fu and Li, 2021) and flow size prediction (Poupart and Chen, 2016). As the network bandwidth grows dramatically, it has been increasingly difficult for network applications to keep up with the high traffic volume (Cisco, 2021). Unfortunately, existing model-based systems are unable to process high-throughput traffic, due to their processing overhead. Although deploying more servers could achieve higher throughput, doing so would increase the capital costs drastically, which is not symmetric to the rapid growth of network bandwidth and network traffic nowadays.

Modern commodity network hardware devices, e.g., programmable switches (PS) (Barefoot Networks, 2021; Yang et al., 2022) and smart network interface card (NIC) (Lin et al., 2020)) provide hardware programmability. They have comparable power consumption and capital costs as traditional fixed-function network devices, which enables orders of magnitude cost reduction compared to commodity CPU or other hardware alternatives (e.g., GPU and FPGA). However, as programmable network devices only come with limited computational and memory resources, deploying learning models directly on them is often infeasible.

Knowledge distillation.Knowledge distillation (KD) is widely adopted in model compression, transferring the knowledge of an unwieldy teacher model, which is hard to deploy on resource-constrained devices, into a lightweight student model (Gou et al., 2021). According to the definition of knowledge, existing KD methods can be classified into three types: response-based KD (Hinton et al., 2015; Chen et al., 2017), feature-based KD (Romero et al., 2014; Chen and Mei, 2021), and relation-based KD (Yim et al., 2017). Response-based KD methods have been applied in network scenarios since they support training a student model with a different structure from a teacher model to adapt to network devices. The main idea of the response-based KD is to let the student model mimic the prediction of the teacher model. Frosst and Hinton (2017) propose a soft decision tree that is transformed from NNs using response-based KD. Xie et al. (2022) propose Mousika that leverages the response-based KD methods to convert the deep learning models into decision trees, and extract the flow table entries for direct deployment on programmable switches. This opens up new horizons for bringing learning models into network devices. However, existing KD response-based methods rely on abundant labeled data, which is not available in network scenarios.

## 3 Metis

Figure 1: Metis framework.

In this section, we describe the Metis framework as shown in Figure 1. For the offline training, we first process the network traffic and construct DFAs with bytes from network rules. Then, we convert DFAs into BRNNs. Finally, we propose SSKD to train our PSRF under the instruction of BRNN. For the online inference, we deploy the PSRF on network devices to process traffic in line-speed.

### Rule to DFA

In the field of NLP, the input is usually natural language text. Often, the input text is naturally tokenized by word delimiters, e.g., spaces in English text. Then, by using pre-trained word embeddings, dimension reduction can be easily conducted on features extracted from the tokenized text. However, the inputs in network scenarios are bit streams extracted from packets of network traffic.

In Metis, we utilize byte-level tokenization for network bit streams and network rules. The benefits of byte-level tokenization are two-fold. First, we can reduce the input vocabulary size to \(2^{8}=256\). Second, byte-level tokenization facilitates the deployment of the final PSRF in network devices since network devices typically process packets in bytes. For the incoming bit streams, we divide them into bytes and then convert bytes to hexadecimal numbers.

Network security applications, e.g., Snort (Roesch, 1999) and Suricata (Foundation, 2022), usually adopt Perl-compatible regular expression syntax (PCRE) (Philip Hazel, 2022) to construct network rules. PCRE includes more representative features than REs, such as counting constraints, character classes, and hexadecimal numbers. The syntaxes supported in this paper are shown in Appendix A. Note that (Jiang & Zhao, 2020) only supports the first five syntaxes. Taking "\( x26cv x3d[0-9]\{3,4\}\)", a simple rule in Snort to identify policy, as an example. Hexadecimal numbers will only appear after "\( x\)". Note that this rule can appear anywhere in a bit stream. So we first convert the rule to "\(* x26cvv x3d[0-9]\{3,4\}\).\(*\)", where '\(\)' is the wildcard that can match any character. '\(*\)' is the Kleene star operator to match the preceding subexpression zero or more times. Characters inside the brackets (e.g., "\([0-9]\)") form a character class module which ORs the characters included in a character class. To make the rule consistent with the input data format (i.e., bytes), we convert each character in the rule except character classes, wildcard, and counting constraint operators into its corresponding ASCII code, using only one byte per character. So the final converted rule is "\(*\ 0x26\ 0x63\ 0x76\ 0x76\ 0x3d\ [0-9]\{3,4\}\).\(*\)".

Next, we construct deterministic finite-state automata (DFA) for the converted rules. Finite-state automata (FA) are mathematical models of computation characterizing transitions among a finite number of states. An RE can be converted into an FA using Thompson's construction algorithm (Thompson, 1968). We can construct a unique FA with a minimum number of states and deterministic transitions (DFA) for an RE by the DFA construction algorithm (Rabin & Scott, 1959) and DFA minimization algorithm (Hopcroft, 1971). Formally, a DFA is defined as a 5-tuple \(=(,,T,_{0},_{})\), whose elements are defined as: \(\): the input vocabulary. In the byte-level input process, \(||=V=2^{8}=256\); \(\): a finite set of states. \(||=K\); \(T^{V K K}\): transition weights. \(T[,s_{i},s_{j}]\) is the weight of transferring \(s_{i}\) to \(s_{j}\) according to the input \(\). In DFA, \(T[,s_{i},s_{j}]\) is \(1\) indicates \(s_{i}\) can transfer to \(s_{j}\) otherwise \(0\); \(_{0}^{K}\): initial weights of \(\). \(_{0}[i]\) is the initial weight of \(s_{i}\) when time \(t=0\); \(_{}^{K}\): final weights of \(\). \(_{}[i]\) is the final weight of \(s_{i}\) after reading the whole input. Consider an input sequence \(=\{x_{1},x_{2},...,x_{N}\}\) and a path \(p=\{u_{1},u_{2},...,u_{N+1}\}\), where \(u_{i}\) is the index of state considering \(x_{i}\). The score \((,p)\) of path \(p\) is defined as

\[(,p)=_{0}[u_{1}](_{i=1}^ {N}T[x_{i},u_{i},u_{i+1}])_{}[u_{N+1} ].\] (1)

Let \(()\) be the set of all possible paths starting from \(_{0}\) and ending at \(_{}\), where \(_{0}\) is the set of start states and \(_{}\) is the set of final states. The sum of path scores, \(_{fw}(,)\), can be computed by the Forward algorithm Baum & Petrie (1966):

\[_{}(,)=_{ ()}(,)=_{0}^{T}( _{i=1}^{N}[x_{i}])_{}.\] (2)

As mentioned above, we can build a DFA using the converted rules. Note that transitions in our DFA consist of bytes. To construct the DFA more concisely, when constructing the transformation matrix, we treat wildcards and character classes as special words in the input vocabulary for quick processing.

### DFA to BRNN

Our DFA is parameterized by \(=<T,_{0},_{}>\). Let \(h_{t}^{K}\) be the forward score considering the first \(t\) words \(\{x_{1},x_{2},...,x_{t}\}\) of \(\). We rewrite the forward score into a recurrent form:

\[h_{0} =_{0}^{T},\] (3) \[h_{t} =h_{t-1} T[x_{t}],1 t N,\] (4) \[_{}(,) =h_{N}_{}.\] (5)

Here, we treat matching on RE as a binary classification task, which can suit most network applications. To this end, we expand \(_{}(,)\) to a vector \(\) when \(_{}(,)\) is 0, and \(\) otherwise. It can be easily generalized to multi-classification tasks as well. Recall that the calculation of the hidden states in the forward propagation of the RNN is formulated by \(h_{t}=(Ux^{t}+Wh^{t-1}+b)\), where \(\) is the activation function. When \(U=0,b=0\), \(W=T\), and \(\) is the identity function, the forward score calculation of DFA (e.g. 4) is similar to the forward propagation of RNN. Therefore, we convert the DFA with byte-format transitions into an RNN with \(\), called BRNN. In practice, fine-grained classification may be desired, which considers the different combinations of final states, i.e., different terminal state combinations correspond to different output categories. In this case, we can use an MLP after BRNN as the aggregation layer. The BRNN converted from DFA can retain the performance of the original RE and be put into production immediately without waiting for data collection. When enough labeled data is collected, the performance of the BRNN can be further improved through training.

### BRNN to PSRF

The BRNN is often difficult to deploy on network devices directly. To promote easy deployment on network devices, we further convert BRNN into pooling soft random forest (PSRF) using semi-supervised knowledge distillation (SSKD).

Existing knowledge distillation approaches (e.g., Mousika) face two major problems when applied in network scenarios. First, labeled data only accounts for a very small proportion of the massive network traffic. It is challenging to distill a student model with high accuracy in such data-scarce scenarios. Second, due to the limited computation resources, memory, and supported operations of network devices, there are stringent restrictions on the selection of student models. Specifically, only tree-based models (e.g., the decision tree and the random forest) can be deployed. However, tree-based models are not suitable for unaligned and sequential features (payloads of network packets), making network tasks challenging, such as RE matching.

In Metis, we propose SSKD and PSRF, which overcome the above-mentioned problems with two key ideas. First, we introduce the semi-supervised learning strategy into the knowledge distillation. Attributing to the cold-start characteristic of our BRNN, the BRNN can effectively preserve the expert knowledge from the RE-based system and hence offers adequate accuracy without training data. As a result, we jointly utilize the ground truth of labeled data and inference results of BRNN on both labeled and unlabeled data to train the student model. Second, we select the PSRF as the student model instead of the soft decision tree. To exploit the context information within the sequence features, we introduce the pooling operation before training. Since the matched pattern in payloads may shift, leveraging context information can alleviate overfitting and boost accuracy.

SSKD aims to train the PSRF, consisting of several pooling soft trees (PST). The training phase of SSKD includes three steps. First, we perform \(t\) times of sampling with a replacement on all data (including both labeled data and unlabeled data) to obtain a sample set \(U=\{U_{1},U_{2},U_{3},,U_{t}\}\), where \(U_{i}=\{(x_{1},y_{1}),(x_{2},y_{2}),,(x_{n},y_{n})\}\), \(i=1,,t\), is the sample subset for the \(i\)-th tree, \(t\) is the number of trees in the random forest. Note that for labeled samples, \(y=(1,0)\) or \(y=(0,1)\), and for unlabeled samples \(y=(-1,-1)\). Although SSKD focuses on the binary classification task in this paper, it can be easily generalized to multi-classification tasks. The input feature \(x\) is composed of the decimal form of the bytes, i.e. integers ranging from \(0\) to \(255\). Second, we calculate the mixed label \(y_{i}^{mix}\), and modify each item's label in the sample subset \(U_{m}\), e.g., \((x_{i},y_{i})\) to \((x_{i},y_{i}^{mix})\), where \(0 m t\) and \(0 i n\). Specifically, the mixed label is calculated as

\[y^{mix}= y^{hard}+(1-) y^{soft},\] (6)

where \(\) is a hyperparameter ranging from \(0\) to \(1\), \(y^{hard}\) is the hard label of a sample (a tuple including true or false), and \(y^{soft}\) represents the soft label (a tuple composed of BRNN output probability). Forunlabeled data, \(y^{mix}\) is effectively the output of the BRNN. For labeled data, \(y^{mix}\) mixes the ground truth and the output of the BRNN, which helps the student model learn the classification ability of the teacher model. Finally, we use each sample subset \(S_{i}\) to train a PST. Our PSRF consists of all the trained PSTs and predicts output by majority vote. We can deploy PSRF on network devices by converting rules to general match action table entries.

Before training, we introduce the max pooling operation to leverage the context information. We do not apply mean pooling since mean pooling leads to an increase in feature complexity. We define the window as \(w\) and the stride as \(s\). For each incoming feature sequences \(x=[f_{1},f_{2},f_{3},...,f_{maxlen}]\), the feature sequence is transformed to \([max(f_{1},...,f_{w}),max(f_{1+s},...,f_{w+s}),...]\), where \(f\) represents the feature of \(x\) and \(maxlen\) is the maximum length of payload sequences. However, the implementation of pooling operation is non-trivial in network devices due to their lack of support for complex float computing. To implement Metis on network devices, we first extract the packet bytes to the packet header vector through the parser. Then in the ingress pipeline, we use the match-action unit to perform a pooling operation on the packet header vector based on the ALU in parallel. Details of technical work can be referred to the technique report. The training process of each PST is similar to the CART decision tree (Breiman et al., 2017) which utilizes purity to split nodes. The purity after splitting the node via feature \(A\) is defined as

\[P(D,A)=^{A}|}{D}Gini(D_{l}^{A})+^{A}}{D}Gini(D_{r}^{A}),\] (7)

where \(D\) is the sample set of the parent node, and \(D_{l}^{A}\) and \(D_{r}^{A}\) are the sample sets of the left child and right child split by feature A, respectively. The feature with the smallest \(P(D,A)\) will be used for node split. Note that different from the calculation method of the _Gini_ index for hard labels, we calculate \(Gini(D)\) as

\[Gini(D)=1-(y^{0}}{|D|})^{2}-(y^{ 1}}{|D|})^{2},\] (8)

where \(y=(y^{0},y^{1})\). The detailed training process and end conditions of the PST are consistent with the CART decision tree. We mainly consider two hyperparameters: the number of features to consider when looking for the best split (\(\#\) split features) and the minimum number of samples required to be at a leaf node (minimum samples). To deploy PSRF on network devices efficiently, we heuristically aggregates table entries into clusters based on the similarity among features represented by table entries so that each cluster only requires a more compact table.

## 4 Experiment

### Experiment Setup

**Dataset.** Although there are several existing datasets collecting the real traces, e.g., CAIDA (Caida, 2019) and MAWI (Kenjiro Cho, 2000), most of them do not contain the payload of network traffic to protect user privacy, which makes them unsuitable for evaluating Metis. To evaluate Metis in realistic and diverse network scenarios, we collect packet-level traces at gateways of a large data center, which belongs to one of the largest public cloud providers. The data center serves tens of Tbps traffic for customers with diverse cloud access requirements. The collected traces are labeled by the advanced attack detection system and application identification system deployed in the data center. We use 10 minutes of traffic traces collected in different time periods of three weeks. We construct 11 categories of the dataset based on Snort (Roesch, 1999) rules, including "chat", "ftp", "games", "malware", "misc", "netbios", "p2p", "policy", "telnet", "tttp", and "client". Each category of the dataset consists of a set of network rules and \(200,000\) labeled data. We label the data as \(0\) if it does not match any of the network rules in this category. Otherwise, we label it as \(1\). As such, our task is a binary classification task. We split each category of the dataset into the training set, test set, and validation set with a ratio of \(7:2:1\).

**Baselines.** For DFA2BRNN, we compare it with LSTM (Hochreiter & Schmidhuber, 1997), a 4-layer CNN (Kim, 2014) and a 4-layer DAN (Iyyer & Manjunatha, 2015) which are widely applied in text classification. For BRNN2PSRF, we compare it with a CART decision tree (DT) (Breiman et al., 2017), a random forest (RF) (Breiman, 2001), a hard DT, a hard RF, and a soft random forest (SRF). We use BRNN to tag unlabeled data and combine them with the ground truth to train models which 

[MISSING_PAGE_FAIL:7]

accuracy compared to models trained on original labeled data, like DT and RF, highlighting the significance of semi-supervised learning. Additionally, attributing to the pooling operation and soft labels introduced by SSKD, PSRF demonstrates the most exceptional accuracy, with an improvement of \(9\% 25\%\) over other baselines. Furthermore, when compared to RE-based systems, PSRF shows an improvement of around \(8\% 17\%\) in accuracy. Even when provided with full training data, PSRF maintains its superiority in terms of accuracy compared to other baselines. Our system can easily identify normal traffic, which comprises over 99% of total traffic because it does not match RE patterns. By analyzing the traffic, the PDF and CDF of matched abnormal packet segment lengths are illustrated in Figure 5 in Appendix B. Although our traffic is collected from the real world and sampled over different time periods, we find that the RE patterns in the traffic are relatively fixed, and are only a subset of the Snort. This is why the accuracy of the baseline scheme increases rapidly from 0% to 1% \(\#\) training data, but not so much from 1% to 100% \(\#\) training data. For abnormal traffic, PSRF can effectively detect abnormal traffic containing short RE patterns (shorter than 50), which takes up over 95% of abnormal traffic as shown in Figure 5 in Appendix B. Though PSRF may struggle with long RE patterns due to both its inherent design logic and hardware limitation (e.g., the input length of PSRF cannot exceed 128 bytes, as restricted by the maximum width supported by the switch matching table), such patterns rarely appear and thus has very limited impact: PSRF can still achieve an accuracy higher than 99%. We also show average F1-Scores of DFA2BRNN and BRNN2PSRF with normal/abnormal traffic ratios of 99%/1% and 50%/50% in Appendix C. Besides, we conduct experiments on network intrusion detection using UNB ISCX IDS 2012 dataset (Moustafa & Slay, 2015), and the results are shown in Appendix D.

### Sensitivity Analysis

Effect of \(s\) and \(w\).In Figure 2(a) and 2(b), the accuracy decreases when \(s\) increases, while the accuracy is stable when \(w\) exceeds 2. The increase of \(s\) significantly reduces the number of features and ignores partial context information, which impairs the accuracy. The change of \(w\) is related to the contextual perspective of the feature and slightly affects accuracy. Note that when \(w=1\), the pooling operation does not function effectively, resulting in a significant drop in accuracy. This highlights the importance and effectiveness of the pooling operation. Therefore, we set \(s\) to \(1\) and \(w\) to \(3\).

Effect of \(\#\) split features and minimum samples.As shown in Figure 2(c), when minimum samples rise, the accuracy of PSRF decreases slightly. However, decreasing the minimum samples will increase the depth of the tree, which will affect the speed of inference and may cause overfitting. Therefore, we set the minimum samples to \(15\). As shown in 2(d), we find that the accuracy grows slightly and then even decreases as \(\#\) split features grows. The decrease results from overfitting. To improve the training speed and accuracy, we set \(\#\) split features to \(8\).

   Method & \# TD & chat & ftp & games & malware & misc & netbions & p2p & policy & telnet & ttp & client & average \\   & 1\% & 10.33\(\)1.5 & 95.00\(\)0.5 & 55.74\(\)0.0 & 24.00\(\)0.0 & 28.80\(\)0.0 & 77.50\(\)0.0 & 55.22\(\)0.0 & 76.20\(\)0.0 & 62.90\(\)0.0 & 72.7 \\  & 10\% & 81.1\(\)1.0 & 84.33\(\)0.0 & 83.39\(\)0.0 & 83.70\(\)0.0 & 83.70\(\)0.0 & 75.51\(\)0.0 & 82.70\(\)0.0 & 75.51\(\)0.0 & 82.72\(\)0.0 & 81.00\(\)0.0 & 72.33\(\)0.0 & 81.2 \\  & 100\% & 88.68\(\)0.0 & 91.27\(\)0.0 & 93.03\(\)0.0 & 89.32\(\)0.0 & 89.32\(\)0.0 & 88.20\(\)0.0 & 84.24\(\)0.0 & 88.31\(\)0.0 & 86.54\(\)0.0 & 89.0 \\   & 1\% & 17.23\(\)0.0 & 83.00\(\)0.0 & 81.85\(\)0.0 & 78.36\(\)0.0 & 79.01\(\)0.0 & 82.00\(\)0.0 & 78.86\(\)0.0 & 80.00\(\)0.0 & 79.94\(\)0.0 & 12.44\(\)0.0 & 78.4 \\  & 100\% & 86.8\(\)0.0 & 89.38\(\)0.0 & 86.01\(\)0.0 & 87.74\(\)0.0 & 87.50\(\)0.0 & 82.00\(\)0.0 & 81.34\(\)0.0 & 88.27\(\)0.0 & 86.24\(\)0.0 & 82.21\(\)0.0 & 86.4 \\  & 100\% & 88.94\(\)0.0 & 91.14\(\)0.0 & 88.00\(\)0.0 & 90.00\(\)0.0 & 89.44\(\)0.0 & 89.00\(\)0.0 & 84.44\(\)0.0 & 84.40\(\)0.0 & 88.00\(\)0.0 & 85.44\(\)0.0 & 85.44\(\)0.0 \\   & 0\% & 78.80\(\)0.0 & 77.10\(\)0.0 & 84.70\(\)0.0 & 79.88\(\)0.0 & 75.61\(\)0.0 & 81.26\(\)0.0 & 79.00\(\)0.0 & 79.94\(\)0.0 & 78.00\(\)0.0 & 79.00\(\)0.0 & 79.00\(\)0.0 & 79.00\(\)0.0 \\  & 1

**Effect of \(t\).** As shown in Figure 2(e) and 2(f), with the increase of \(t\), the accuracy and the # entries of PSRF also increase. There exists a tradeoff between accuracy and efficiency. Therefore, in our implementation, we set \(t\) to \(9\), which can maintain both high accuracy and high efficiency.

**Effect of \(\).** We conduct experiments on \(\) using PSRF and the results are shown in Figure 3. In the zero-shot scenario, the soft labels are the same as the hard labels, thus the accuracy remains the same. For other scenarios, we find that PSRF performs well when \(\) is in the range [0.2, 0.4]. This is because PSRF considers both information inherited in the BRNN and the original label. When \(\) is 0, PSRF only considers the logits of the BRNN and when \(\) is 1, PSRF mainly considers the original label while ignoring the information in the logits of the BRNN. We found the PSRF performs best when \(\) is around 0.3, so we set \(\) to 0.3 in our experiments.

### Experiment on Network Devices

We first conduct experiments on # entries consumed by different student models on programmable switches (PS). As shown in Table 3, \(\#\) entries in PSRF are less than that in SRF and RF, indicating the lightweightness of the PSRF. Note that columns represent # TD. Although DT achieves fewer \(\#\) entries compared with SRF, it suffers from poor accuracy as illustrated in Table 2. Therefore, PSRF is the best choice for the student model in terms of accuracy and resource efficiency.

Since network devices have limited computation and memory resources, it is difficult to deploy RE-based systems on them directly. To demonstrate the superiority of Metis for processing network traffic in practical deployment, we deploy PSRF on PS and compared it with common RE-based systems deployed on CPUs. We use multi-core parallel processing to boost the throughput of RE-based systems. We compare the number of (processed) packets per second (PPS) achieved by PSRF on PS and by RE-based systems (with 1, 2, 4, 8, 16, and 32 cores) in Figure 4 to show the difference in their throughputs. It is obvious that PSRF deployed on PS achieves a significantly higher throughput, which is \(74\) times that of the RE-based system with 32 cores.

   Method & 0\% & 1\% & 10\% & 100\% \\  DT & - & 410 & 407 & 411 \\ RF & - & 2145 & 2162 & 2129 \\ SRF & 1973 & 1834 & 1756 & 1694 \\ PSRF & 1640 & 1513 & 1538 & 1474 \\   

Table 3: # entries consumed by different student models on PS.

Figure 2: Sensitive analysis on PSRF.

## 5 Disscussion

**Limitations.** 1) Due to the limitation of hardware resources (the maximum width supported by the switch matching table is limited, e.g., 128 bytes), the input length of PSRF cannot exceed 128 bytes. In future work, we will consider using feature compression (such as using an autoencoder) as an attempt to support long REs. 2) Though we introduce the pooling operation into our device-friendly model PSRF, its ability to model sequence features still has room for improvement (compared to BRNN, PSRF after SSKD drops 2%-3% in accuracy). In future work, we will try to use other student models, such as binary neural networks, to improve the accuracy. 3) We only test single-phrase RE matching. In practice, some rules may be composed of multi-phrase REs. We can implement multi-phase REs by adding aggregation operations on the results of single-phrase RE matching. This will be part of our future work.

**Societal impact on AI community.** In the AI community, though many amazing NN works have been proposed, NNs can only be deployed on powerful devices (CPU, GPU), which undoubtedly limits the scope of their practical applications. In many domains, there exist a large number of less-powerful devices, which cannot take advantage of the excellent performance of NNs, including switches, network cards, intelligent gateways, and IoT devices. In mainstream large-scale data centers, 80% of the switches have been replaced with programmable switches. However, NNs contain complex floating-point and nonlinear operations, making it impossible to directly deploy them on programmable switches. Therefore, our work endeavors to bring NNs into a wider community (e.g., network community, and security community), so that more devices can take advantage of NN's superior performance to improve the quantity of service (QoS), and facilitate people's daily lives.

## 6 Conclusion

In this paper, we propose Metis, an intelligent and general framework to understand and enhance regular expressions in-network by utilizing learning models. We utilize byte-level tokenization to extract RE from network rules and process bit streams of network traffic. Then we design SSKD to transform the BRNNs into PSRFs that can be deployed on network devices to process online network traffic in line-speed. We collect network traffic on a large data center for the evaluation of Metis. Experimental results show that Metis is more accurate than original REs and other baselines, while also achieving superior throughput when deployed on network devices. We contribute Metis source code and datasets to the AI community to stimulate the following research.