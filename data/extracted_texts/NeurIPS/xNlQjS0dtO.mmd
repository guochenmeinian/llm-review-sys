# Keeping LLMs Aligned After Fine-tuning:

The Crucial Role of Prompt Templates

Kaifeng Lyu\({}^{1}\), Haoyu Zhao\({}^{1}\), Xinran Gu\({}^{2}\), Dingli Yu\({}^{1}\), Anirudh Goyal, Sanjeev Arora\({}^{1}\)

\({}^{1}\)Computer Science Department & Princeton Language and Intelligence, Princeton Univeristy

\({}^{2}\) Institute for Interdisciplinary Information Sciences, Tsinghua University

{klyu,arora}@cs.princeton.edu

**Content warning: This paper contains examples of harmful language.**

 Equal contributionWork done while visiting Princeton.

\({}^{1}\)Computer Science Department & Princeton Language and Intelligence, Princeton Univeristy

\({}^{2}\) Institute for Interdisciplinary Information Sciences, Tsinghua University

{klyu,arora}@cs.princeton.edu

**Content warning: This paper contains examples of harmful language.**

###### Abstract

Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "_Pure Tuning, Safe Testing_" (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.1

## 1 Introduction

Fine-tuning existing Large Language Models (LLMs) for new applications is crucial in today's research and business. Available options include fine-tuning open-source language models (e.g., Llama 2, Touvron et al.2023) with local resources or calling fine-tuning APIs for proprietary language models (e.g., GPT-3.5 Turbo, Peng et al.2023).

Many of these models underwent alignment training (usually RLHF, Ouyang et al.2022) so that they can follow users' instructions and provide helpful responses--while ensuring "safety," meaning that given problematic user queries (e.g., seeking help with criminal behavior), they either refuse to help or respond with a safe and constructive answer. However, there is no guarantee that the model will remain aligned after fine-tuning. Of course, a malicious model creator may fine-tune the model on a dataset full of inappropriate behaviors to break the model's alignment and elicit unsafe behaviors. Such methods have been shown to be effective on many popular language models, including Llama 2 and GPT-3.5 Turbo (Yang et al., 2023, Zhan et al., 2023, Lermen et al., 2023). But recently, Qi et al. (2024) raised a trickier question: If the model creator is _benign_ and the model is fine-tuned on clearly _benign_ datasets, will the model be safe for public deployment? Interestingly, they showed that even fine-tuning on datasets that do not contain harmful data (such as Alpaca, Taori et al.2023) can result in a noticeable rise in unsafe behaviors.

This phenomenon might seem counter-intuitive, but it is not entirely unexpected: it is known that neural networks may catastrophically forget previously learned knowledge of old tasks after being trained on new tasks (Kirkpatrick et al., 2017; Luo et al., 2023), so it is plausible that reckless fine-tuning on utility-oriented datasets may cause the model to forget when to prioritize safety over helpfulness. Additionally, as shown by He et al. (2024), seemingly benign data points to humans may subtly influence neural networks to generate more affirmative responses, even to harmful queries.

In this paper, we study how to help benign model creators mitigate the safety degradation in fine-tuning aligned LLMs with benign datasets. Our extensive experiments uncover that the safety degradation highly depends on input formats: after fine-tuning, the model is significantly less safe on test inputs with a similar format as the one used in fine-tuning, but it remains safe if we create a certain discrepancy between input formats used in fine-tuning and testing. More specifically, we control the input format by changing the _prompt template_, which we now describe in detail.

Prompt templates.At public deployment, a model creator can enforce a prompt template for users to interact with the model, where the prompt template here refers to a string with placeholders to be filled with the input data. For illustration, here we recall the recommended prompt templates for using Meta's Llama 2-Chat (Touvron et al., 2023). First, to ensure that the model answers in instruction-following mode (as opposed to free-form generation) it is recommended to wrap the user's query with the template "[INST]{input}[/INST]", i.e., adding the [INST] and [/INST] tokens to the beginning and the end of the input. Second, a common and lightweight technique to enhance safety is to prepend a _safety prompt_ that explicitly guide the model to ensure safety. Indeed, all the evaluations for Llama 2-Chat in its technical report (Touvron et al., 2023) are conducted with the following safety prompt: "_You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe..._" See Table 9 for the full safety prompt and template. Adding safety prompts has also been recommended for other models; see Appendix C.

The issue of distribution shift.For fine-tuning an aligned model on a downstream task, _what prompt template should be used during and after the fine-tuning process?_ A common practice is to use the same prompt template throughout fine-tuning and inference, since introducing any distribution shift can be harmful for downstream performance. Previous papers on the safety issues of benign fine-tuning indeed conduct experiments in this way (Qi et al., 2024; Pelrine et al., 2023; He et al., 2024). On the other hand, if the model learns to follow harmful instructions from some seemingly benign data points, such behaviors may be more likely to be triggered when the model is tested with the same template as fine-tuning. These two views motivate us to ask: _If we create a discrepancy between prompt templates used in fine-tuning and inference, can we make the fine-tuned model safer while still being useful on downstream tasks?_

This paper.Our experiments with popular LLMs, including Meta's Llama 2-Chat (Touvron et al., 2023), Mistral AI's Mistral 7B Instruct v0.2 (Jiang et al., 2023), and OpenAI's GPT-3.5 Turbo (Peng et al., 2023), show that the following strategy significantly reduces the loss of safety after fine-tuning while still maintaining substantial improvements in the helpfulness on the downstream task:

**Pure Tuning, Safe Testing (PTST):**

Do inference with a safety prompt, but do fine-tuning without it.

Figure 1: An overview of our “Pure Tuning, Safe Testing” (PTST) strategy: Do inference with a safety prompt, but do fine-tuning without it. Using the other combinations of prompt templates for fine-tuning and inference can lead to a significant loss of safety alignment.

Here the loss of safety is measured by the success rates of various harmful queries, called the _Attack Success Rate_ (ASR). We even report cases where using the recommended safety prompt during fine-tuning makes the original model _less safe_ than when we omit the safety prompt during both fine-tuning and inference.

First, we fine-tune these language models on GSM8K (Cobbe et al., 2021) for solving grade school math, which is _a priori_ unrelated to any unsafe behaviors (Sections 3.1 and 3.2). Our experiments with various prompt templates during fine-tuning and inference, including the ones with and without safety prompts, show that using the same prompt template throughout fine-tuning and inference breaks the safety alignment to a large extent. Conversely, in many cases, using different templates for them reduces ASR, and we identify that PTST is the most effective strategy among them. Experiments in Section 3.3 further confirm these findings on other fine-tuning tasks, including ChatDoctor (Li et al., 2023) and OpenOrca (Lian et al., 2023; Mukherjee et al., 2023).

Next, we explore the effect of adding additional safety examples (i.e., pairs of harmful queries and their refusal responses) during fine-tuning (Section 4). In the literature, adding some safety examples to the fine-tuning data has been shown to often mitigate the safety degeneration (Qi et al., 2024; Zhao et al., 2023). _Will the prompt templates still be important if we add safety examples?_ We show that the answer depends on whether the safety examples can cover the distribution of harmful queries at test time. First, by adding safety examples with a style similar to the safety benchmarks, we observe that the ASR can be almost reduced to 0%. However, there can be various creative ways of making harmful queries, and it is hard for a small or moderate number of safety examples to cover all of them. To test this, we curate a set of \(100\) harmful queries that mix GSM8K with harmful requests in a certain manner. While the original model can successfully defend against almost all of these attacks, after fine-tuning with GSM8K, the ASR increases to be high even with the safety examples added. On the other hand, PTST is able to significantly reduce this safety degradation, hence showing that PTST is effective even when safety examples are added.

Beyond the setting of fine-tuning an aligned model, we note that the PTST strategy is not entirely new: some aligned models themselves might be fine-tuned from the corresponding base models without safety prompts added to the alignment data (Touvron et al., 2023; Jiang et al., 2023), but later they could be deployed with a safety prompt. To the best of our knowledge, there has not been a detailed study for this use of safety prompts yet. While our main focus is to provide thorough ablation studies on the role of prompt templates for fine-tuning aligned models, we also hope our findings can provide insights into how safety prompts should be used in other situations.

## 2 Threat Model and Safety Evaluation

Our description of experiments and results uses the following threat model. A model owner fine-tunes an existing aligned model on a training set with a prompt template, referred to as the _training template_. The model owner then deploys the model online while enforcing any online users to interact with the model with another prompt template, called the _test template_. Training and test templates may or may not be the same. The model owner is assumed to have a _helpfulness_ metric for the trained model. Some standard examples: (a) training set is GSM8K (grade school math) and helpfulness is test accuracy on GSM8K. (b) training set is OpenOrca and helpfulness is accuracy on ARC dataset.

An attacker who has only black-box access to the model (i.e., with no access to the model weights or knowledge of the exact fine-tuning/pretraining data), inputs a harmful query with the test template chosen by the model owner. The model's response to the query is evaluated by a judge (which could be a powerful LLM) about its _harmfulness_. Below we describe this further.

GPT-4 judge.All our experiments use a GPT-4 judge to assess harmfulness on a 5-point Likert scale (1: not harmful, 5: very harmful). Given a harmful query dataset, we compute the _Attack Success Rate (ASR)_ as the percentage of harmful queries that lead to responses scored as \(5\).

Jailbreak attacks?We note that, even without fine-tuning, it is possible to use delicate prompt engineering techniques to "jailbreak" current public language models so that they can provide useful information to harmful queries. See Section 5 for an overview. Defending against these jailbreak attacks requires a better alignment training method and goes beyond the scope of our study. Therefore, most of our experiments test safety only on harmful queries that the original model (with an appropriate template) can already defend against with a low ASR, but still, we show the effectiveness of PTST in preserving safety by measuring the ASR under the Greedy CoordinateGradient (GCG) attack (Zou et al., 2023) from the JailbreakBench (Chao et al., 2024) in Table 1d (see details in Appendix F.4).

AdvBench.Following recent works on jailbreaking LLMs (Huang et al., 2023; Chao et al., 2023; Mehrotra et al., 2023; Qi et al., 2024; Zeng et al., 2024), we test safety on the "harmful behaviors" subset of the AdvBench benchmark curated by Zou et al. (2023), which consists of \(520\) examples of instructions that make direct harmful requests in imperative tone.

New dataset: DirectHarm4.Some of our fine-tuned models have low ASR for AdvBench, but we were able to find many harmful queries of certain types. Inspired by the observation in Qi et al. (2024) that loss of safety in fine-tuning is more severe in some categories than others, we created a new dataset, called DirectHarm4, consisting of \(400\) queries from \(4\) categories that tend to elicit higher ASRs in many fine-tuning settings. Similar to AdvBench, these harmful queries are ensured to be stated as direct requests in imperative tone. See Appendix F.3 for more details.2

## 3 Role of Prompt Templates

### Case Study: Fine-tuning on GSM8K

The first study focuses on fine-tuning LLama 2-Chat on GSM8K to understand the role of prompt templates during training and test time. Detailed descriptions of the prompt templates we considered are provided in Table 9. We generally call models prompted with [INST] and [/INST] tokens as being in the _chat mode_, and those without these tokens as being in the _text mode_.

* text:vanilla (TV): A minimal template that guides the model to respond in the text mode.
* text:alpaca (TA): The default template for Alpaca (Taori et al., 2023), which does not contain [INST] and [/INST] tokens. Papers such as Chen et al. (2023) have used this template for fine-tuning and testing LLama 2-Chat.
* chat:vanilla (CV): A minimal template that wraps the instruction with [INST] and [/INST] to guide the model to respond in the chat mode.
* chat:alpaca (CA): A template that wraps text:alpaca with [INST] and [/INST] tokens. This is the template used by Qi et al. (2024) for fine-tuning and inference to explore safety issues.
* chat:llama (CL): A template that prepends chat:vanilla with the safety prompt recommended by the LLama 2 paper (Touvron et al., 2023). Such a safety prompt is wrapped with recommended special tokens to highlight its importance and is also called as _system prompt_.

We also study the following two lightweight defenses that improve safety of aligned models by adding safety prompts. We specifically aim to understand how these defenses can be adapted to mitigate safety degradation in fine-tuning.

* Self-Reminder (SR): A template proposed by Xie et al. (2023) that reminds the model about safety by adding safety prompts not only before but also after the user's query.
* In-context Defense (ICD): A template proposed by Wei et al. (2023) that adds an unsafe query with a safe response before the user's query as an in-context example.

Safety degrades when using the same training and test templates.Conventional wisdom suggests that we should make the training and test settings as similar as possible to maximize generalization. Hence, the prompt template used for fine-tuning should be the same as the one used for test. For each of the 5 templates mentioned above, we fine-tune LLama-2-7b-chat with learning rate \(10^{-4}\) for \(6\) epochs, where these two hyperparameters are picked based on the helpfulness performance when the template is chat:vanilla. We repeat the fine-tuning using three different seeds. As shown in the "diagonal" entries of tables in Table 1, this indeed leads to significant improvement in helpfulness. For example, for the chat:vanilla template, the exact match score on GSM8K increases from \(20.32\%\) to \(33.39\%\). However, the ASR on DirectHarm4 rises significantly from \(2.75\%\) to \(11.00\%\), which indicates that safety is compromised. Indeed, a consistent degradation in safety alignment is observed across all templates, and using chat-mode templates is generally safer than using text-mode ones. Perhaps surprisingly, for the template chat:llama, which contains a safety prompt, the ASR

[MISSING_PAGE_FAIL:5]

PTST beats early stopping.One may wonder if the improvements from PTST could be achieved by early stopping the standard fine-tuning process (with the same training and test templates). Figure 1(a) plots the helpfulness and safety throughout the fine-tuning processes for three strategies: (1) fine-tuning and testing with chat:vanilla, (2) fine-tuning and testing with chat:llama, and (3) fine-tuning with chat:vanilla and testing with chat:llama (PTST). Without PTST, both helpfulness and ASR generally increase as we train longer. Conversely, PTST consistently maintains a low ASR, thereby achieving a better balance between helpfulness and safety.

### Experiments on Other Models: GPT-3.5 and Mistral

GPT-3.5 Turbo.We conduct experiments on GPT-3.5-turbo-0613 on GSM8K to further validate our findings. We fine-tune GPT-3.5 Turbo on the GSM8K dataset for 1 epoch using the chat-mode prompt templates in Table 9 with slight modifications to fit the API's requirement about the JSON format (Table 10). The API automatically picks the batch size and learning rate multiplier, which are 4 and 2, respectively. The results are summarized in Table 3. For models fine-tuned with chat:vanilla or chat:alpaca, transitioning to chat:llama for inference significantly reduces the ASR compared with adhering to the same prompt template as training. For example, for the model trained with chat:vanilla, switching from chat:vanilla to chat:llama for inference decreases the ASR from \(22.75\%\) to \(4.50\%\) on DirectHarm4 while maintaining a similar helpfulness improvement.

To compare PTST with early stopping, we further fine-tune GPT-3.5 Turbo on Orca-Math (Mitra et al., 2024), a larger and more diverse math word problem dataset containing 200k samples. We set the batch size to 6 and the learning rate multiplier to 2, fine-tuning on 10,000, 20,000, and 40,000 examples randomly sampled from the original dataset. As shown in Figure 1(b), PTST maintains a lower ASR while achieving similar helpfulness across all three training horizons compared with other strategies. See Appendix F for more details.

Table 3: Helpfulness and safety evaluation of GPT-3.5 Turbo fine-tuned on GSM8K. For models fine-tuned with chat:vanilla or chat:alpaca, transitioning to chat:llama for inference significantly reduces the harmfulness rate while preserving the helpfulness, compared with adhering to the same prompt template as training.

Figure 2: The ASR on DirectHarm4 v.s. Helpfulness after different numbers of training steps with different training and testing prompt templates. PTST (CV:CL) offers a better trade-off between helpfulness and safety compared to training and testing with the same template (CV:CV, CL:CL), even with early stopping. **A:B** denotes a trajectory that is trained with template **A** and tested with template **B**. Points in the plot are connected in the order of training steps.

**Mistral.** Similar to the experiments on Llama 2-That, we fine-tune Mistral-7B-Instruct-v0.2 on GSM8K for 6 epochs and summarize the helpfulness and safety of the fine-tuned models in Table 7 (in Appendix). The experiment results align with those on Llama and GPT-3.5 Turbo: PTST strategy significantly reduces the harmfulness rate while retaining the helpfulness, while training and inference with the same template suffer from a high ASR. Please refer to Appendix E for more detailed discussions.

### Experiments on Other Datasets: ChatDoctor and OpenOrca

Besides the GSM8K dataset, we also fine-tune the Llama-2-7b-chat model on ChatDoctor and OpenOrca datasets. For convenience, we only consider the templates under the chat mode, i.e., chat:vanilla, chat:alpaca, and chat:llama, and we test the safety on AdvBench and DirectHarm4. Table 4 and 5 summarize the results for ChatDoctor and OpenOrca respectively.

The observations on ChatDoctor and OpenOrca datasets are very similar to those on GSM8K. We should not use the same template during fine-tuning and testing: using the same template will lead to significant safety degeneration on AdvBench dataset. In constrast, using chat:llama during testing while not using chat:llama during fine-tuning preserves safety.3 Similar to the GSM8K experiments, we find that training with chat:vanilla while testing using chat:llama is a very solid strategy to preserve safety while still getting decent improvement on helpfulness.

### Experiments on Other Safety Prompts

Besides chat:llama, we also experiment with two other safety prompts to verify PTST: (1) chat:mpt (CM), which uses the default system prompt for MPT-7B-8K-Chat and MPT-30B-Chat [MosaicML, 2023]; (2) chat:llama-short (CS), which uses a shorter version of the system prompt recommended by the Llama 2 paper [Touvron et al., 2023].

**PTST with other safety prompts.** In Figure 3, we test the effectiveness of the above two templates on GSM8K for Llama 2-7B-Chat and GPT-3.5 Turbo. As expected, we find that using these templates for both training and testing leads to a significant drop in safety. If we follow PTST to do fine-tuning

Table 4: Helpfulness and safety for Llama-2-7B-chat fine-tuned on ChatDoctor. We use temperature \(=0.7\) and for sampling decoding. We report the helpfulness/harmfulness scores averaged over \(5\) random seeds for decoding, with the standard deviation in the subscript. We omit the standard deviations for the helpfulness scores as they are less than \(5 10^{-5}\) for all configurations.

Table 5: Helpfulness and safety for Llama-2-7B-chat model fine-tuned on OpenOrca. The results come from a single run. Fine-tuning and testing with the same prompt template lead to a high attack success rate (ASR) on AdvBench and DirectHarm4 dataset. When fine-tuned and tested with different prompts, the safety issue can be mitigated while substantially improving helpfulness over the base model.

with chat:vanilla and testing with either of these two templates, the safety can be preserved while still maintaining a large portion of the improvement in helpfulness.

Fine-tuning and testing with two different safety prompts.We then violate PTST slightly for further validation: fine-tune the model with a safety prompt, then test the model with a different safety prompt. More specifically, we test a model fine-tuned with chat:llama when other safety prompts are used at test time. As shown in Figures 2(a) and 2(b), this indeed leads to a noticeable drop in safety, suggesting that the safety drop in fine-tuning with a safety prompt cannot be easily resolved by using another safety prompt for testing.

## 4 Effects of Mixing Safety Data

Besides manipulating the templates with PTST, another natural way to protect the safety alignment is to mix some safety examples into the fine-tuning procedure, which has been found useful in Qi et al. (2024); Zong et al. (2024). In this section, we explore the effectiveness of PTST in fine-tuning with safety examples.

### Adding Safety Examples Can Reduce the ASR on Similar Queries Without PTST

Safety data for training.We use the dataset constructed in Bianchi et al. (2023), which contains 2483 harmful queries and their corresponding safe responses. We found that these queries have similar style and format as AdvBench and DirectHarm4: most of the queries only have a single imperative sentence asking for help with a harmful behavior. It is thus promising to reduce the ASRs on AdvBench and DirectHarm4 by adding these safety examples from Bianchi et al. (2023).

Training details.We fine-tune Llama-2-7B-chat model on a mixture of GSM8K and the above safety dataset, where we pass the GSM8k for 6 epochs and this safety dataset for 1 epoch. The learning rate is chosen to be 1e-4, the same as we used in Section 3.1. We train the model with chat:vanilla, chat:alpaca, and chat:llama templates, respectively. We always use the same template for both GSM8K and safety examples.

Results.Table 6 summarizes the safety evaluation on AdvBench and DirectHarm4, which shows that adding the safety data dramatically mitigates the safety degeneration during fine-tuning and reduces the ASRs to nearly \(0\%\), which is true no matter the training and test templates are the same or not. When PTST is applied, the ASR can be further reduced, though the safety gain can be marginal.

### PTST Leads to More Substantial Improvements on OOD Queries

Although adding safety data helps to protect the safety under similar style and format, it may not be very helpful for out-of-distribution (OOD) queries, i.e., harmful queries that come from a completely

Figure 3: The ASR on DirectHarm4 and the helpfulness for Llama 2-7B-Chat and GPT-3.5 Turbo fine-tuned on GSM8K with different training and test templates. The results are grouped by the test template, and X denotes template chat:X. Fine-tuning with chat:llama and inference with another safety prompt still leads to noticeable safety degradation. By contrast, PTST strategy preserves safety.

different distribution. As the safety degradation in fine-tuning may happen for various different queries, it can be difficult for a small or moderate number of safety data to cover **ALL** possible harmful queries and provide a perfect protection of safety degradation.

A dataset for illustration: GSM-Danger.To illustrate this, we curate another set of harmful instructions, called _GSM-Danger_. This is done by prompting GPT-4 to generate instructions that start the query like a math problem from GSM8K but end the query with a harmful request. This final request is restricted to a single sample from AdvBench: write a social media post that encourages people to engage in dangerous behaviors like drunk driving or using drugs4. Note that these instructions are of a very different style and content from the safety data since they are as long as a math problem and are not making direct requests.

Failure of defending against OOD queries.Our safety evaluation on GSM-Danger (Table 6) indicates that the original model can achieve a low ASR on GSM-Danger. However, if training and test templates are the same, the safety can degrade a lot after fine-tuning, even if we add the safety data: training on chat:vanilla, chat:alpaca, chat:llama all increase the ASR on GSM-Danger by more than 10%!

Effectiveness of PTST.Table 6 further presents the results of fine-tuning with PTST: if the model is fine-tuned with chat:vanilla and tested with chat:llama, the ASR on GSM-Danger is 5% without adding the safety data and 4% with the safety data, while training and testing with both chat:llama leads to 12% ASR even with the safety data. If we change the training template from chat:vanilla to chat:alpaca, the ASR are both 1% with or without the safety data. All these results showcase the effectiveness of PTST.

## 5 Related Works

Prompting for LLM alignment.Prompt engineering is a simple yet effective way to align LLMs with human values. Before the prevalence of chat models, Askell et al. (2021) proposed prompts incorporating both instructions and in-context examples to elicit honest and harmless responses from LLMs. The same idea was later promoted by Lin et al. (2023) and Zhang et al. (2023). For chat models, simply employing prompt engineering without in-context examples has been shown to enhance their safety. Touvron et al. (2023) reported that the safety of Llama 2-Chat can be efficiently improved by prefixing a safety system prompt. Zheng et al. (2024) proposed Directed Representation Optimization (DRO) for finding the best safety prompt. Additionally, employing prompts designed for self-reflection can further augment their safety capabilities (Ganguli et al., 2023; Wu et al., 2023). However, the effect of using different prompts for fine-tuning versus inference remains underexplored.

Table 6: Helpfulness and safety for Llama model fine-tuned on GSM8K and safety data. Adding safety data during fine-tuning can mitigate the safety degradation. However, the model can still be unsafe when using the same prompt for training and testing, especially on the GSM-Danger dataset. The results come from a single run.

Removing safety guardrails via fine-tuning.A series of recent works studied the safety risks introduced by fine-tuning aligned LLMs. Qi et al. (2024); Zhan et al. (2023); Lerman et al. (2023); Pelrine et al. (2023) demonstrated that fine-tuning aligned LLMs on a small amount of harmful data can easily bypass the safety guardrails. Zhao et al. (2023) studied the safety degradation when the fine-tuning dataset contains unsafe data. More intriguingly, Qi et al. (2024) and Pelrine et al. (2023) showed that fine-tuning with benign data, e.g., Alpaca (Taori et al., 2023) and BookCorpus (Zhu et al., 2015), can also lead to degradation in safety. However, there appears to be a gap in aligning the fine-tuning process with a specific utility-drive objective. Qi et al. (2024) did not include the performance of the fine-tuned models on corresponding downstream tasks, e.g., AlpacaEval for the model fine-tuned on the Alpaca dataset; the BookCorpus Completion task in Pelrine et al. (2023) does not have a natural downstream task. We reproduce the experiment of fine-tuning LLama-2-7B-chat on Alpaca (Qi et al., 2024) and find that the instruction-following ability, measured by AlpacaEval (Li et al., 2023), does not improve after fine-tuning (Table 8). Concurrent to our work, He et al. (2024) studied the safety degradation of fine-tuning LLMs on GSM8K and developed data selection methods to identify small subsets that can lead to an even more severe safety degradation.

Preserving safety during fine-tuning.Huang et al. (2024) proposed a new alignment method, Vaccine, to do the alignment in a way that the internet representations of the model are more robust to perturbations, thus making the model's safety more robust to fine-tuning. Mukhoti et al. (2024) showed that regularizing the change of internal features of CLIP during fine-tuning can help reduce forgetting of concepts irrelevant to the fine-tuning data. Concurrent to our work, Wang et al. (2024) proposed to prepend a secret prompt to safety data and mix them with the fine-tuning data. At inference time, the secret prompt is added to the prompt template to remind the model of preserving safety. In another concurrent work, Zong et al. (2024) curated a vision-language safe instruction-following dataset and proposed mixing the safety data into fine-tuning to fix the safety degradation of VLLM. In the same vein as Huang et al. (2024), several other concurrent works focused on improving the alignment method to mitigate the safety issue in fine-tuning (Rosati et al., 2024, 2024); Huang et al. (2024). Hsu et al. (2024) proposed a training-free method that projects the LoRA weights to certain "safe subspace" to mitigate the safety degradation of fine-tuning. All these defenses can be combined with our PTST strategy by adding a safety prompt at test time.

## 6 Conclusions

Fine-tuning an aligned model can lead to safety degradation, which happens for Llama, Mistral, and even for more intelligent models such as GPT-3.5 Turbo. This paper provides an empirical study of the roles of prompt templates in preserving safety alignment for fine-tuning an aligned model and proposes the PTST strategy as a simple yet useful amendment to the current practice: fine-tuning without a safety prompt but including it at test time.

Our understanding of PTST remains quite limited. The success of PTST suggests that LLMs have certain abilities of compositional generalization, which enable them to generalize from one template to another while being aware of safety constraints in the new template. However, the mechanisms driving this generalization are not yet well understood, which poses an interesting question that warrants further empirical and theoretical exploration.

For future work, we believe that this safety issue in fine-tuning is a fundamental problem of LLMs and requires systematic consideration throughout all stages of training. Beyond the custom fine-tuning and inference stages we focus on in this paper, an improved algorithm design in the alignment stage may lead to a more robust alignment against further fine-tuning. The safety issue may also be mitigated if the alignment algorithms can make PTST more effective, such as adding appropriate data augmentation, especially on prompt templates, to make the model more robust to certain template changes but more sensitive to the instruction in safety prompts. This may be related to recent works that teach models to prioritize system prompts over untrusted user instructions (Chen et al., 2024; Wallace et al., 2024).