# MVGamba: Unify 3D Content Generation as State Space Sequence Modeling

Xuanyu Yi\({}^{1,5}\)1 Zike Wu\({}^{3}\)1 Qiuhong Shen\({}^{2}\)1 Qingshan Xu\({}^{1}\) Pan Zhou\({}^{4}\)

**Joo-Hwee Lim**\({}^{5}\) **Shuicheng Yan\({}^{6}\) **Xinchao Wang**\({}^{2}\) **Hanwang Zhang\({}^{1}\)

\({}^{1}\)Nanyang Technological University \({}^{2}\)National University of Singapore

\({}^{3}\)University of British Columbia \({}^{4}\)Singapore Management University

\({}^{5}\)Institute for Infocomm Research \({}^{6}\)Skywork AI

###### Abstract

Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (_e.g._, Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long

Figure 1: MVGamba is a unified 3D generation framework build on Gaussian Splatting, which can generate high-quality 3D contents in a feed-forward manner in sub-seconds.

sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only \(0.1\) of the model size. The codes are available at https://github.com/SkyworkAI/MVGamba.

## 1 Introduction

We address the challenge of crafting 3D content from a single image, sparse-view images, or text input, which can facilitate a broad range of applications, _e.g._, Virtual Reality, immersive filming, digital gaming and animation. Previous research on 3D generation has investigated distilling 2D diffusion priors into 3D representations via score distillation sampling (SDS) . Although these optimization-based approaches exhibit strong zero-shot generation capability with high-fidelity rendering quality [2; 3; 4; 5], they are extremely time- and memory-intensive, often requiring hours to produce a single 3D asset, thus not practical for a real-world scenario.

With the advent of large-scale open-world 3D datasets [6; 7; 8], recent 3D large reconstruction models (LRMs) [9; 10; 11; 12] integrate multi-view diffusion models [13; 14; 15] with scalable multi-view 3D reconstruction to regress a certain 3D representation (_e.g._ Triplane-NeRF [16; 17], mesh) in a feed-forward manner. Specifically, current LRMs [18; 19; 20] adopt a _one image (or text)_\(\)_multi-view images_\(\)_3D_ diagram to predict 3D Gaussian Splatting (3DGS)  parameters, thereby ensuring the rendering efficiency while preserving fine details. Given a single image or text prompt, they first generate a set of images using multi-view diffusion models, which are then fed into a multi-view reconstructor (_e.g._, U-Net  or Transformer ), mapping image tokens to 3D Gaussians with superior generation speed and unprecedented quality.

However, we observe that existing feed-forward Gaussian reconstruction models typically adopt powerful yet computationally intensive architectures [23; 24] to generate long sequences of Gaussians for intricate 3D modeling. Such approaches inevitably compromise the integrity of multi-view information propagation to manage computational costs. For instance, they use local  or mixed  attention on limited multi-view image tokens or even deal each view separately and simply merge the predicted Gaussians afterwards . Consequently, the generated 3D models often suffer from multi-view inconsistency and blurred textures, as illustrated in Figure 2(a). These issues indicate that current compromise strategies fail to translate into coherent, high-quality outputs in practice. This raises a crucial question: _How can we preserve the integrity of multi-view information while efficiently generating a sufficiently long sequence of Gaussians?_

To address this issue, in this paper, we introduce **M**ulti-**V**ew **G**aussian **M**amba (MVGamba), a general and lightweight Gaussian reconstruction model. At its core, MVGamba features a multi-view Gaussian reconstructor based on the recently introduced RNN-like architecture Mamba , which expands the given multi-view images into a long sequence of 3D Gaussian tokens and processes them recurrently in a causal manner. By adopting causal context propagation, our approach efficiently maintains multi-view information integrity and further enables cross-view self-refinement from earlier to current views. Additionally, our Gaussian reconstructor enables the fine-detailed generation of long Gaussian sequences with linear complexity [26; 27] in a single forward process, eliminating the need for any post hoc operations used in previous work.

More concretely, we first patchify the multi-view images into \(N\) tokens and rearrange them according to a cross-scan order [27; 28], resulting in \(4 N\) image tokens for selective scanning. These tokens are then processed through a series of Mamba blocks for state space sequence modeling. Subsequently, we feed the output Gaussian sequence into a lightweight Multi-Layer Perceptron (MLP) for channel-wise knowledge selection, followed by a set of linear decoders to obtain the Gaussian parameters representing high-quality 3D content (Sec. 3.2). Compared to previous LRMs [11; 29; 30], our MVGamba features many computationally efficient components: a single-layer 2D convolution image tokenizer replaces the pre-trained DINO  transformer encoder, a lightweight MLP combined with linear decoders replaces the deep MLP decoder, and most importantly, linear complexity Mamba blocks replace quadratic complexity Transformer blocks (Figure 2(b)). Together, these designs ensure efficient training and inference while achieving higher generation quality (Sec. 4). Moreover, to directly convert the generated Gaussians into smooth textured polygonal meshes, we alternatively incorporate a 3DGS variant -- 2DGS  -- for accurate geometric modeling and mesh extraction.

We conducted comprehensive qualitative and quantitative experiments to verify the efficacy of our proposed MVGamba. The experimental results demonstrate that MVGamba (49M parameters) outperforms other latest LRMs [19; 29; 33] and even optimization-based methods [34; 35] on the task of text-to-3D generation, single-view reconstruction and sparse-view reconstruction with roughly only \(0.1\) of the model size. The contributions and novelties of our paper are summarized as follows:

* We point out that directly generating a sufficiently long sequence of Gaussians with full multi-view information is crucial for consistent and fine-detailed 3D generation.
* We introduce MVGamba, a novel feed-forward pipeline that incorporates causal context propagation for cross-view self-refinement, allowing the efficient generation of long sequences of 2D/3D Gaussians for high-quality 3D content modeling.
* Extensive experiments demonstrate that MVGamba is a potentially _general_ solution for 3D content generation, including text-to-3D, image-to-3D and sparse-view reconstruction task.

## 2 Related Work

**3D Generation.** Previous approaches for generating high-fidelity 3D models predominantly used SDS-based optimization techniques [1; 36] and their variants [34; 2; 4; 37; 2; 5]. These methods yield high-quality 3D generations but require hours for the per-instance optimization process to converge. Pioneered by the large reconstruction model (LRM) , recent works [10; 38; 30] show that image tokens can be directly mapped to 3D representations, typically triplane-NeRF, in a feed-forward manner via a scalable transformer-based architecture  with large-scale 3D training data [6; 7; 8]. Among them, Instant3D  integrates LRM with multi-view image diffusion models [13; 14; 39; 40], using four generated images for better quality. To avoid inefficient volume rendering and limited triplane resolution, some concurrent works [18; 19; 20] follow Instant3D and introduce 3D Gaussian Splatting  into sparse-view LRM variants. Specifically, GRM  and GS-LRM  use pixel-aligned Gaussian with a pure transformer-based reconstruction model, increasing the number of Gaussians through image feature upsampling and per-pixel merge operations. LGM  combines the 3D Gaussians from different views using a convolution-based asymmetric U-Net . Our MVGamba, on the other hand, directly processes multi-view conditions causally, recurrently generating a long sequence of Gaussians for coherent and high-fidelity 3D modeling.

**Mamba model for visual applications.** Recent advancements in State Space Models (SSMs) [17; 41; 42], notably Mamba , have gained prominence in long sequence modeling for harmonizing computational efficiency and model versatility [43; 44; 45; 46]. Following Mamba's progress, there has been a surge in applying this framework to critical vision domains, including generic vision backbones [26; 47; 48; 27], multi-modal streams [49; 50], and vertical applications, especially in medical image processing [51; 52; 53; 54; 55; 56]. Specifically, VMamba  pioneers a purely Mamba-based backbone to handle intensive prediction tasks. Similarly, Vim  leverages bidirectional SSMs for data-dependent global visual context without image-specific biases. Subsequent works progress with advanced selective scanning algorithms [47; 48], integration with other networks [57; 58], and

Figure 2: **(a) Previous Gaussian reconstruction models sacrifice the integrity of multi-view information for computationally intensive architectures, resulting in multi-view inconsistency and blurred textures. (b) Comparison of FLOPs between self-attention in Transformers and SSM in Mamba. Detailed FLOPs data are provided in Table 3.**

adapted structural designs [59; 59; 60]. Concurrently, Gamba  marries Mamba with 3DGS for single-view reconstruction with limited texture quality and generalization capacity. In this paper, we explore and demonstrate the efficiency and long-sequence modeling capacity of Mamba in various 3D generation tasks with large-scale pre-training.

## 3 Method

In this section, we present our MVGamba, designed to efficiently generate 3D content through a two-stage pipeline. In the first stage, we utilize off-the-shelf multi-view diffusion models, including MVDream  and ImageDream , to generate multi-view images based on an input text prompt or a single image. In the second stage, equipped with this multi-view image generator, we introduce an SSM-based multi-view reconstructor to generate Gaussians from multi-view images. Specifically, we first provide a brief overview of 3D Gaussian splatting and its variants (Sec. 3.1). Next, we describe the core architecture of our multi-view Gaussian reconstructor (Sec. 3.2), followed by detailed elaboration of our robust training objectives (Sec. 3.3). With above large-scale pre-training, we are able to chain these two stages to produce high-fidelity 3D content in seconds (Sec. 3.4).

### Preliminary: Gaussian Splatting

Introduced by , vanilla 3D Gaussian Splatting (3DGS) fits a 3D scene from multi-view images with a collection of 3D Gaussians. Variants of Gaussian Splatting [62; 63], typically 2DGS, leverage 2D Gaussian primitives instead, excelling in the vanilla version for more accurate geometry reconstruction. Generally, each Gaussian is composed of its 3D center \(^{3}\), 3D scale \(s^{3}\) or 2D scale \({}^{*}\)\(s^{2}\), associated color \(c^{3}\), opacity \(\), and a rotation quaternion \(q^{4}\). These parameters can be collectively denoted by \(\), with \(_{i}=\{_{i},s_{i},c_{i},_{i},q_{i}\}\) denoting the parameter of the \(i\)-th Gaussian. These Gaussians can then be splatted onto the image plane and rendered in real time via the differentiable tiled rasterizer [21; 62].

### SSM-based Gaussian reconstructor

The core of MVGamba is a feed-forward multi-view Gaussian reconstructor. As depicted in Figure 3(a), our reconstructor transforms multi-view input images with camera embedding  into 3D contents represented by 3D Gaussians  or its variants [62; 63] in a feed-forward manner. This reconstructor comprises an SSM-based processor to expand and process multi-view image tokens as Gaussian sequences, and a light-weight Gaussian decoder to predict attributes for each Gaussian.

Figure 3: **(a) Multi-view Gaussian reconstructor (Sec. 3.2): Multi-view inputs with ray embedding are used for causal sequence modeling, predicting Gaussians rendered at novel views and supervised with ground truth images. (b) Unified inference pipeline (Sec. 3.4): MVGamba combines multi-view diffusion models and Gaussian reconstructor to generate high-quality 3D content in sub-seconds.**

Expanding multi-view images as sequences.Given a posed multi-view image set \(\{v_{i},_{i}\}\), we first densely embed the camera pose \(_{i}^{4 4}\) for each view \(v_{i}^{H W 3}\) using Plucker rays , denoted as \(_{i}^{H W 6}\). The pixel values and ray embeddings are concatenated into a 9-channel fused map, which is then tokenized using a non-overlapping convolution with a kernel size of \(p p\):

\[_{i}=((v_{i},_{i})),\] (1)

where \(_{i}^{h w C}\) is the tokenized feature map; \(h=H/p\), \(w=W/p\); \(C\) is the embedding dimension. Note that considering the light-weight architectural design, our image tokenizer is much simpler than the pre-trained DINO  utilized by previous LRMs, which we empirically find to be redundant for low-level 3D reconstruction. With the tokenized multi-view features, we then adopt a cross-scan order  to rearrange them as sequence. Specifically, we scan the image tokens sequentially along four different directions: \(\), \(\), \(\), and \(\), which allows each token to integrate information from all adjacent tokens. This cross-scan rearrangement results in a sequence that is \(4\) longer:

\[=_{}((\{_{i}\})),\] (2)

where \(^{4Nhw C}\) denotes the expanded Gaussian sequences, \(N\) denotes the view number, and \(_{}\) denotes the cross-scan operation on each view.

Causal sequence modeling with State Space Model.Inspired by , we model the Gaussian sequences via an adapted SSM-based processor. In detail, given the expanded Gaussian sequence \(\), we first add a learnable positional embedding \(\) element-wise to it and derive the initial Gaussian sequence \(_{0}\). Then, we feed \(_{0}\) into \(L\) stacked SSM layers for recurrent causal sequence modeling, formulated as:

\[_{k}=_{k}(_{k-1};A_{k},B_{k},_{k})\] (3)

where \(_{k}\) denotes Gaussian sequence output by the \(k\)-th layer, and \(_{k}\) denotes the \(k\)-th SSM layer with vanilla Mamba  structure; \(A_{k}\), \(B_{k}\) and \(_{k}\) are parameters of SSM layer dependent on the input sequence \(_{k-1}\). Note that we are modeling Gaussian sequences rather than integrating spatial information as in existing vision Mamba models . Therefore, we adopt 1D convolution instead of 2D convolution in the Mamba blocks, similar to other sequential modeling SSMs . Through state space sequence modeling, we successfully propagate the causal context containing the multi-view information from earlier states to later states with linear complexity. This approach efficiently incorporates multi-view information causally from the initial condition onward, thereby making full use of all Gaussian tokens through cross-view self-refinement. As discussed in Sec. 5, this causal sequential generation of Gaussian tokens provides the model with unprecedented robustness and self-correction abilities, even under inconsistent or noisy input conditions.

Decoding causal token sequences into Gaussians.Each token in the processed causal sequence \(_{L}\) is treated as a separate 3D Gaussian token. We first apply a single hidden layer MLP to \(_{L}\), where the width of the hidden layer is \(4C\) and the output channels revert to \(C\). This process is denoted as \(=(_{L})\), which aims for channel-wise knowledge selection . We then apply sub-heads to derive each attribute of 3DGS with separate linear projections. Specifically, we predict the position  by discretizing the coordinates where position \(_{i}\) is clamped to \([-1,1]^{3}\). The scale \(s_{i}\) is predicted with a learnable linear projection followed by a \(\) activation. The opacity \(_{i}\) is predicted with a linear projection followed by \(\) activation. Regarding the color attribute \(c_{i}\), we predict the RGB values instead of the spherical harmonics adopted by the original 3DGS, as our reconstructor is mainly trained on synthetic 3D datasets free of light variation.

However, unlike other Gaussian attributes, the rotation quaternions are quite sensitive and difficult to predict directly, and hence are often set canonical isotropic and fixed in several recent works . On the other hand, some works  predict the rotation without any constraints, but this often causes artifacts and corrupted generations in practice. To address this issue, we design a novel rotation decoder, dubbed RotNet, which balances prediction flexibility and restriction. Our RotNet consists of a set of \(32\) pre-defined rotation quaternions, denoted as \(\), which forms a canonical rotation space, and a learnable linear projection matrix \(\) to predict the logits of these quaternions. The predicted logits are then transformed into a probability distribution using the Gumbel-Softmax , enabling differentiation through the discrete selection process by adding noise sampled from the Gumbel distribution to the logits \(^{32}\) before applying the softmax function:

\[=(+), g _{k}=-(-(u_{k})), u_{k}(0,1).\] (4)In this way, we convert the rotation prediction into a \(32\)-class classification task in a fully differentiable way, which allows for direct selection via the \(*{argmax}\) operation during inference. We refer to _Appendix_ D for more detailed explanations. These decoded Gaussians are finally passed into the differentiable rasterization pipeline  for image-level supervision.

### Stable Training of MVGamba

**Bridging the training-inference gap.** In the training phase, multi-view images are collected from the ground-truth blending of 3D objects, while they are generated by diffusion models during inference. To mitigate such domain gap: (1) Following LGM , we leverage the grid distortion and orbital camera jitter as two data augmentations with a \(30\%\) probability to simulate inconsistent pixels and inaccurate camera poses, respectively. (2) We directly use ImageDream  as a synthetic data engine to generate multi-view images input and conduct a joint training with the ground-truth renderings from the 3D training dataset. In practice, with a \(5\%\) chance, we train MVGamba with synthetic input to mimic the inference pattern for more robust generation results.

**Overall training objective.** During the training phase, we differentiably render the RGB image \(v_{i}\) and alpha mask \(v_{i}^{}\) of the \(N=4\) input views and another six novel views for image-level supervision. Our final objective then comprises four key terms:

\[=_{v_{i}}||}_{}(v_{i},v_ {i}^{})+_{}_{}(v_{i}^{ },v_{i}^{})+_{}_{}( v_{i},v_{i}^{})+_{}_{},\] (5)

where \(_{}\) and \(_{}\) represent the mean square error loss in the RGB image and the alpha mask, respectively; \(_{}\) represents the well-adopted VGG-based perceptual loss  ; \(_{}\) is the opacity L1 regularization loss \(||1-_{i}||\) encourage more efficient use of each Gaussian by enforcing higher density. \(_{}\), \(_{}\) and \(_{}\) are the trade-off coefficients that balance each loss.

### Unified 3D Generation Inference

During inference (Figure 3(b)), the pre-trained reconstructor can be smoothly combined with any off-the-shelf multi-view diffusion models to efficiently predict a set of Gaussians, which facilitates both text-to-3D and image-to-3D generation. Typically, we leverage ImageDream  and MVDream  to produce \(4\) multi-view images with anchored poses  from a single image or text prompt, respectively. For mesh extraction, following Huang et al. , we utilize truncated signed distance fusion (TSDF)  that fuse the depth maps rendered from the output Gaussians to obtain a smooth polygonal mesh.

## 4 Experiment

### Experimental Settings

**Training dataset.** We obtain the multi-view images from Obijaverse  for MVGamba pre-training. Following [19; 72], we filtered 80\(k\) valid high-quality 3D objects. We then used Blender under uniform lighting to render 25 views of RGBA images with their alpha masks at a resolution of 512 \(\) 512, in the elevation range of \(5^{}\) to \(30^{}\) with rotation \(\{15^{} r|r[0,23],r\}\). To align with the camera configurations in ImageDream  and MVDream , at each training step, we select \(4\) images of a certain object as input views with the same elevations, while rotations separated by \(90^{}\), denoted as \(\{+90^{} k k 0,1,2,3\}\) and another random set of \(6\) views as supervision.

**Implementation details.** MVGamba is trained on 32 NVIDIA A100 (80G) with batch size 512 for about 2 days. We adopt gradient checkpointing and mixed-precision training with BF16 data type to ensure efficient training and inference. We use the AdamW optimizer with learning rate \(1 10^{-3}\) and weight decay \(0.05\), following a linear learning rate warm-up for \(15\) epochs with cosine decay to \(1 10^{-5}\). The output Gaussians are rendered at \(512 512\) resolution for mean square error loss and resized to \(256 256\) for LPIPS loss for memory efficiency. The trade-off coefficients that balancing each loss were set as \(_{}=1.0\), \(_{}=0.6\) and \(_{}=0.001\). We also follow the common practice  to clip the gradient with a maximum norm of \(1.0\). The detail of MVGamba model configuration is included in _Appendix_ D.

### Comparison against Baselines

In this section, we compare MVGamba with previous state-of-the-art instant 3D generation methods in image-to-3D, text-to-3D generation, and sparse-view reconstruction tasks. For each task, we first elaborate on the evaluation metrics and baseline methods, then perform an extensive qualitative and quantitative comparison.

**Single image-to-3D generation.** We make comparisons to recent methods, including optimization-based DreamGaussian , Wonder3D ; feed-forward methods LGM , TripoSR  and Triplane-Gaussian  and One-2345++ . We adopted the official codebase and pre-trained model weight for all the above methods and we are quite confident that the baselines presented are the finest re-implementations we have come across+. We evaluated the generation quality of MVGamba with a wide range of wild images in Figure 4. We use well-adopted PSNR, SSIM and LPIPS for quantitative measurement in the GSO  dataset following , with a total of 16 test views with equidistant azimuth and \(-10 10\) degree elevations. As illustrated in Figure 4, MVGamba maintains high fidelity and plausible generation in most scenarios. In contrast, Triplane-Gaussian severely suffers from flat and blurred views, which is a notoriously ill-posed challenge, as stated by Instant3D . Moreover, LGM frequently showcases multi-view inconsistency with a transparent surface, which may be attributed to its suboptimal parameter constraints and merge operation. In

Figure 4: Qualitative comparison in image-to-3D and text-to-3D generation. Please refer to _Appendix_ C for more generation results.

[MISSING_PAGE_FAIL:8]

during inference to test robustness, and the other on the effect of Gaussian sequence length for sparse-view reconstruction. **(1)** In Figure 7(a), we manually perturb one of the four input images with Gaussian noise to simulate the worst-case multi-view input inconsistency caused by the diffusion model. We then feed these manipulated images into the MVGamba reconstructor and merge-operation reconstructor for comparison. As expected, generating 3D Gaussians conditioned on each view separately and simply merging the outputs treats the perturbed input as faithfully as the other views, resulting in inconsistency and even corrupted results. In contrast, our Gaussian reconstructor generates the Gaussian sequence in a causal and self-refining manner, allowing it to mitigate the effects of the perturbation by leveraging propagated multi-view information. This experiment demonstrates that our MVGamba is highly resilient to inconsistencies in multi-view diffusion models used in the first stage due to its causal sequence modeling. **(2)** We also investigate the effect of Gaussian sequence length by varying the patch size to model different sequence lengths. As shown in Figure 7(b), our SSM-based Gaussian reconstructor can directly generate extremely long Gaussian sequences, and its performance improves with increasing sequence length. From these two aspects, we can conclude that the higher generation performance of MVGamba is indeed attributable to its self-refineable multi-view modeling and efficient utilization of sufficiently long Gaussian sequences.

**Q2: _What impacts performance of MVGamba in terms of component-wise contributions?_**

**A2:** In Table 2, we analyze the component-wise contributions of MVGamba by verifying our design choices of multi-view image encoder, Gaussian decoder structure and training strategy. Note that this ablation is conducted on the filtered subset of Human category in G-buffer Objayverse  using smaller model architecture for better energy efficiency. Considering symbol simplicity, we denote \(\) + \(\) as PC; Gaussian Decoder with RotNet as GD; stable training strategy as ST. Table 2 illustrates that the replacement or exclusion of any component from MVGamba resulted in a significant degradation

   Model & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  w/o PC & 25.20 & 0.827 & 0.102 \\ w/o GD & 25.41 & 0.788 & 0.096 \\ w/o ST & 26.69 & 0.910 & 0.065 \\  Ours & 27.13 & 0.925 & 0.057 \\   

Table 2: Performance comparison of different model configurations.

Figure 6: RGB images and normal maps rendered by MVGamba-2DGS.

Figure 7: (a) Worst-case simulation of the inconsistency introduced by the multi-view diffusion model. (b) The effect of sequence length on 3D reconstruction.

in performance. In particular, if we replace our designed Gaussian Decoder with the one in previous LRMs [11; 29] (10-layer, 64-width shared MLP), we notice a large performance drop due to degraded sequential modeling ability and a tendency for the deep MLP to overfit. Additionally, the lack of restrictions on position and rotation could cause more artifacts during both training and inference.

**Q3: _What is the limitation of MVGamba?_**

**A3:** Honestly, though MVGamba achieves promising results, it still has several limitations. **(1)** As we model the Gaussian sequence causally, MVGamba can sometimes fail if the depth of the front view is not estimated correctly (Figure 8 top). Fortunately, we empirically find that this limitation can be mitigated by manually changing the input order. For example, using a side-view as the first input allows our model to generate satisfactory 3D content, as the side-view contains sufficient depth information (Figure 8 bottom). In the future, we may explore automatic ways to optimize the input order to enhance robustness. **(2)** The generation quality of MVGamba is highly dependent on the four input views provided by off-the-shelf multi-view diffusion models [13; 14]. However, current multi-view diffusion models are far from perfect, known to exhibit 3D inconsistencies [9; 19; 83], and limited to a resolution of 256 \(\) 256. We expect that our model's performance can be seamlessly boosted with the advancements in multi-view diffusion models in future work.

## 6 Conclusion

In this paper, we introduce MVGamba, a general and lightweight Gaussian reconstruction model for unified 3D content generation. MVGamba features a novel multi-view Gaussian reconstructor based on state space sequence modeling, maintaining multi-view information integrity and enabling cross-view self-refinement. It generates long Gaussian sequences with linear complexity in a single forward process, eliminating the need for post hoc operations. Extensive experiments demonstrate that MVGamba (49M parameters) outperforms state-of-the-art LRMs in various 3D generation tasks with only \(0.1\) the model size. In general, MVGamba achieves state-of-the-art quality and high efficiency in parameter utilization, training, inference, and rendering speed. In the future, we aim to apply MVGamba to a wider range of 3D generation tasks, such as scene and 4D (dynamic) generation.

Figure 8: Ablation study on input order. Top: MVGamba may fail if the depth of the front-view is estimated incorrectly and the front-view is given first. Bottom: Manually changing the input order to provide the side-view first allows MVGamba to generate satisfactory 3D content, as the side-view contains sufficient depth information.

Acknowledgments

This project is supported by Kunlun 2050 Research, Skywork AI and Agency for Science, Technology AND Research, and by the National Research Foundation, Singapore, under its Medium Sized Center for Advanced Robotics Technology Innovation. Pan Zhou was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grants (project ID: 23-SIS-SMU-028 and 23-SIS-SMU-070).