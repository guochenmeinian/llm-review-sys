# Bayesian Optimization with Cost-varying Variable Subsets

Sebastian Shenghong Tay12, Chuan Sheng Foo23, Daisuke Urano4,

Richalynn Chiu Xian Leong4, Bryan Kian Hsiang Low1

1Department of Computer Science, National University of Singapore

2Institute for Infocomm Research (I2R), A*STAR, Singapore

3Centre for Frontier AI Research (CFAR), A*STAR, Singapore

4Temasek Life Sciences Laboratory, Singapore

sebastian.tay@u.nus.edu, foo_chuan_sheng@i2r.a-star.edu.sg, daisuke@tll.org.sg, richalynn@tll.org.sg, lowkh@comp.nus.edu.sg

###### Abstract

We introduce the problem of _Bayesian optimization with cost-varying variable subsets_ (BOCVS) where in each iteration, the learner chooses a subset of query variables and specifies their values while the rest are randomly sampled. Each chosen subset has an associated cost. This presents the learner with the novel challenge of balancing between choosing more informative subsets for more directed learning versus leaving some variables to be randomly sampled to reduce incurred costs. This paper presents a novel Gaussian process upper confidence bound-based algorithm for solving the BOCVS problem that is provably no-regret. We analyze how the availability of cheaper control sets helps in exploration and reduces overall regret. We empirically show that our proposed algorithm can find significantly better solutions than comparable baselines with the same budget.

## 1 Introduction

_Bayesian optimization_ (BO) is a powerful framework for the sample-efficient optimization of costly-to-evaluate black-box objective functions  and has been successfully applied to many experimental design problems of significance such as hyperparameter optimization [6; 39], chemical synthesis , and particle accelerator control , among others. Conventional BO assumes that the learner has full control over all query variables (i.e., all variables in the input to the objective function). However, in many real-world optimization problems, some of the query variables may be subject to randomness affecting their values. In some cases, the randomness affecting a specific variable can be eliminated (by allowing the learner to select its value), but at a cost. We illustrate with a few concrete scenarios: In precision agriculture, consider a farm aiming to find the optimal conditions for largest crop yield where the query variables are a set of soil nutrient concentrations (e.g., Ca, B, NH\({}_{3}\), K) and pH. The farm may rely on the naturally-occurring quantities of these nutrients in the available soil, but these quantities will be randomly sampled. Alternatively, they may control some subset of these quantities (via manufactured soil and fertilizers) at a higher cost. In advanced manufacturing where random variation occurs in every operation , certain specifications of a product may be left unspecified by the manufacturer and randomly determined, or specified but at a higher cost. In ad revenue maximization or crowdsourcing where information is gathered from a large number of individuals via ad platforms or crowdsourcing platforms such as Amazon Mechanical Turk, suppose that the query variables describe the demographics of the individual, such as country of origin or income level. The learner may allow the platform to randomly assign the task to any individuals, or the learner may demand a specific subgroup of individuals at a higher cost. In all these practical scenarios, the goal is to find the maximizer with as little incurred cost as possible. At each query iteration, the learner isfaced with the non-trivial problem of deciding which variables to specify (for more directed learning) vs. which variables to allow to be randomly sampled (to reduce incurred costs), in addition to the usual BO problem of deciding the specified variables' values.

To the best of our knowledge, there are no existing works that tackle this problem precisely. The work of Hayashi et al.  introduced the problem of _BO with partially specified queries_ (BOPSQ) in which the subset of deterministically selected variables (_control set_) and randomly sampled variables (_random set_) can also be chosen by the learner, but it does not consider the costs incurred by such choices. This is a non-trivial limitation as the presence of costs can significantly alter the learner's decisions. Under such a formulation, if a control set is a strict subset of another, then the former will never be chosen as there is no benefit to having variable values we randomly sampled instead of chosen by the learner. Consequently, if there exists a control set that includes all the variables in a query, then all other control sets will not be used and the problem reduces to conventional BO. In practice, however, the availability of other control sets confers an advantage if these other control sets are cheaper. Having access to cheaper but more random control sets allows the learner to explore the query space cheaply and then use costlier but more deterministic control sets to exploit high-value regions. BOPSQ in its current formulation excludes the analysis of such strategies and is akin to multi-fidelity BO  but without modeling the costs of the different information sources: In this case, the learner would simply choose the highest-fidelity information source all the time, thus making the problem setting trivial.

This paper introduces the problem of _BO with cost-varying variable subsets_ (BOCVS) that explicitly models the cost of each control set and is more useful in practical scenarios. Our work generalizes BOPSQ and argues that BOCVS problems are much richer when analyzed from a similar perspective as multi-fidelity BO, and the various control sets are treated as information sources with different levels of usefulness and costs. By using cheap control sets for exploration and expensive control sets for exploitation, we show that with an appropriately designed algorithm, a learner can find significantly better solutions with a lower cost expenditure. To achieve this, we leverage the _Gaussian process upper confidence bound_ (GP-UCB) acquisition function [7; 32] to design a novel _no-regret_ algorithm, i.e., its incurred simple regret tends to \(0\) as the number of iterations tends to infinity, and the algorithm's best chosen query converges to the optimal solution. We additionally analyze the impact of the availability of cheaper control sets on the regret incurred by the most expensive control set. We observe that our algorithm generally outperforms the non-cost-aware baselines, while simple extensions based on Thompson sampling, maximizing UCB or expected improvement-based acquisition scores per unit cost [31, Sec. \(3.2\)] either fail to converge or fail to utilize cheap control sets effectively. Concretely, the contributions of our work in this paper include the following:

* We introduce the BOCVS problem (Sec. 4) and solve it by designing a novel UCB-based algorithm (Sec. 4.1) with a theoretical analysis of its properties, including the conditions under which it is provably no-regret and the impact of the availability of cheaper control sets on the regret incurred by the most expensive control set, and discuss the practical considerations (Sec. 4.2);
* We empirically evaluate the performance of our proposed algorithm against the baselines under several experimental settings with synthetic and real-world datasets (Sec. 5), including a plant growth dataset and an airfoil self-noise dataset corresponding, respectively, to the precision agriculture and advanced manufacturing use cases motivated earlier in this section.

## 2 Related Work

The work of Hayashi et al.  introduced _BO with partially specified queries_ (BOPSQ) and tackled the problem with Thompson sampling. However, it fails to consider the relative costs of control sets, which hinders the learner's ability to take advantage of all control sets even in the presence of more deterministic control sets. The work of Oliveira et al.  proposed BO with uncertain inputs in which the executed query is sampled from a probability distribution depending on the proposed query. Though related, its problem setting is motivated more by uncertainty in the input query even post-observation and does not involve variable subset selection. These two works are part of a line of research investigating BO in situations where the learner may not have full control over all variables in a query, which includes BO for expected values , risk-averse BO [5; 21; 22], and distributionally robust BO [17; 24; 37]. These works also do not consider variable subset selection. Our treatment of the BOCVS problem is inspired by multi-fidelity BO in which the learner has access to cheap, low-fidelity surrogates of the true objective function [15; 27; 35; 36]. In such works (and in ours), modeling costs is crucial as the learner would simply choose the highest-fidelity information source (in ours, the maximally deterministic control set) otherwise. While the general idea of paying less for potentially less informative queries is similar, our problem setting is fundamentally different: The lack of informativeness comes from the uncertainty of the executed query as opposed to a bias in the observed function values.

The BOCVS setting may be viewed as a special case of causal BO as formulated by Aglietti et al.  and continued in several works [2; 4]. Specifically, our setting is a case in which there are no 'non-manipulative' variables and the causal DAG is such that all input variables have no parents and are parents of the output variable. Nevertheless, we believe our focus on this special case has value as it allows us to derive useful theoretical results such as algorithm regret bounds that, to the best of our knowledge, do not exist for the completely general causal BO setting at the time of writing. The work of Sussex et al.  includes a regret bound, but is also a special case of , and has little overlap with our work as it does not consider costs of control sets or explicit probability distributions over input variables. We believe that our work is sufficiently general to be useful for practical scenarios (where the full causal BO apparatus may be unnecessary), and is also a stepping stone towards theory for the general case.

## 3 BO and Gaussian Processes

We will first give a brief review of conventional BO . Given a query set \(\) and an objective function \(f:\), a learner wishes to find the maximizing query \(^{*}*{argmax}_{}f( )\). However, \(f\) is black-box (i.e., not available in closed form) and can only be learned by submitting a query \(_{t}\) in each iteration \(t\) for function evaluation and receiving a noisy observation \(y_{t} f(_{t})+_{t}\) where each \(_{t}\) is i.i.d. \(\)-sub-Gaussian noise with zero mean. Each function evaluation is assumed to be expensive in some way, such as in terms of money or time spent. So, the learner must be sample-efficient and find \(^{*}\) in as few iterations as possible. BO achieves sample efficiency by leveraging a Bayesian model to represent a probabilistic belief of the function values at unobserved regions of \(\) in a principled manner. While any Bayesian model may be used for BO, _Gaussian processes_ (GPs)  are a common choice as they enable exact posterior inference: The GP posterior belief of \(f\) at any query \(\) after \(t\) iterations is a Gaussian with posterior mean and variance given by

\[_{t}()_{t}()^{}(_{t} +)^{-1}_{t}\;,_{t}^{2}()  k(,)-_{t}()^{}( _{t}+)^{-1}_{t}()\] (1)

where \(_{t}(y_{j})_{j=1}^{t}^{t}\), \(k\) is a positive semidefinite _kernel_ (covariance function), \(_{t}()(k(,_{j}))_{j=1}^{t} ^{t}\), \(_{t}(k(_{j},_{j^{}}))_{j,j^{ }=1}^{t}^{t t}\), and \(\) is an algorithm parameter; if the noise is a Gaussian with variance \(^{2}\), then the true posterior is recovered with \(=^{2}\). The kernel \(k\) is an important modeling choice as the GP posterior mean will reside in the _reproducing kernel Hilbert space_ (RKHS) associated with \(k\). For simplicity, we assume w.l.o.g. that \(k(,^{}) 1\) for any pair of queries \(,^{}\). Kernel \(k\) affects the _maximum information gain_ (MIG) defined as

\[_{T}()_{\{_{t}\}_{t=1}^{T} }0.5|+^{-1}_{T}|.\]

The MIG characterizes the statistical complexity of a problem and plays an integral role in the theoretical analysis. For the commonly used squared exponential kernel, \(_{T}()=(( T)^{d+1})\), while for the Matern kernel with \(>1\), \(_{T}()=(T^{d(d+1)/(2v+d(d+1))}( T))\). Importantly, \(_{T}()\) is increasing in the volume of \(\)[32, Theorem 8].

## 4 BO with Cost-varying Variable Subsets (BOCVS)

The BOCVS problem consists of a compact query set \(^{d}\) and an objective function \(f:\) in the RKHS of \(k\) with the RKHS norm upper bounded by \(B\). For simplicity, assume w.l.o.g. that \(=^{d}\). Let \([d]\{1,2,...,d\}\). The learner is given a collection \( 2^{[d]}\) of _control sets_ indexed by \(1,2,,m:=||\). Each control set \(i[m]\), denoted by \(_{i}[d]\), indicates the variables in a query with values that can be chosen by the learner. The complement \(}_{i}[d]_{i}\) of \(_{i}\) is the corresponding _random set_ indicating the variables in a query with values that will be randomly sampled from some distribution. A query \(\) can be represented by a combination of _partial queries_\([^{i},^{-i}]\) comprising the _control partial query_\(^{i}(x_{})_{_{i}}\) (i.e., \(^{i}\) collects the variablesindexed by \(_{i}\)) and the _random partial query_\(^{-i}(x_{})_{}_{i}}\) where \(x_{}\) denotes the \(\)-th variable in the query vector \(\). Note that \([^{i},^{-i}]\) is not a simple vector concatenation as the variables may need to be reordered according to their indices. Furthermore, let \(^{i}\{^{i}\}\).

In iteration \(t\), the learner chooses control set \(i_{t}\) and specifies the values in control partial query \(^{i_{t}}\). The random partial query \(^{-i_{t}}\) will then be randomly sampled from the environment. For example, if \(d=4\) and \(_{i_{t}}=\{1,3\}\), then \(}_{i_{t}}=\{2,4\}\) and the learner will be able to choose the values in \(^{i_{t}}\) (i.e., the \(1^{}\) and \(3^{}\) variables) but not those in \(^{-i_{t}}\) (i.e., the \(2^{}\) and \(4^{}\) variables). The full query in iteration \(t\) is then \(_{t}=[^{i_{t}},^{-i_{t}}]=(x_{t,})_{[ ]}\). Each observed variable \(x_{t,}\) for \(}_{i_{t}}\) is a realization of a random variable \(X_{t,}_{}\). The observed \(^{-i_{t}}\) is then a realization of the random vector \(^{-i_{t}}(X_{t,})_{}_{i_{t}}}^{-i_{t}}\) where \(^{-i_{t}}\) is the product measure \(_{}_{i_{t}}}_{}\). In other words, each variable in a random partial query is independently sampled from a probability distribution that governs that variable. All distributions are assumed to be known. The learner then observes \(y_{t} f(_{t})+_{t}\) where each \(_{t}\) is i.i.d. \(\)-sub-Gaussian noise with a zero mean. Fig. 1 illustrates two iterations in a BOCVS problem setting.

The learner wishes to find the optimal control set \(i^{*}\) and specified values in control partial query \(^{i^{*}}\) that maximize the expected value of \(f([^{i},^{-i}])\) where the expectation is w.r.t. \(^{-i}^{-i}\):

\[(i^{*},^{i^{*}})*{argmax}_{(i,^{i}) [m]^{i}}f([^{i},^{-i} ])\,.\]

The learner has an initial budget \(C^{+}\) and every control set \(_{i}\) has an associated cost \(c_{i}>0\) for all \(i[m]\). Let the control set indices be defined such that \(c_{1} c_{2} c_{m}\).1

In every iteration \(t\), the learner pays \(c_{i_{t}}\). The learning procedure ends after \(T\) iterations when \(C-_{t=1}^{T}c_{i_{t}}<c_{i_{T+1}}\), i.e., the learner has not enough budget left to pay for the chosen control set. \(T\) will now be a random variable depending on the algorithm and the random outcomes of the learning procedure. The cost-varying cumulative regret is defined as

\[R_{T}_{t=1}^{T}c_{i_{t}}(f([^{i_{ t}},^{-i^{*}}])-f([^{i_{t}}, ^{-i_{t}}])).\]

The regret incurred by choosing a sub-optimal control set and specifying sub-optimal values in the control partial query is weighted by the cost of that control set. This naturally incorporates the notion that the penalty for sub-optimal plays is lower if the play was cheap, while also penalizing using the entire budget on sub-optimal plays, regardless of whether those plays are cheap or expensive. Intuitively, to minimize the cost-varying regret, a learner would attempt to use the cheap control sets (i.e., low \(c_{i}\), low \(f([^{i},^{-i}])\)) to explore the query space, and use the expensive control sets (i.e., high \(c_{i}\), high \(f([^{i},^{-i}])\)) to exploit control partial queries with high expected function values.1 When all \(c_{i}=1\), we recover the BOPSQ problem , and \(C\) is simply the number of iterations

Figure 1: Two iterations in a BOCVS problem setting. The grey boxes are isometric views of a query set \(^{3}\). The blue regions depict the probability densities of random vectors \([^{i_{t}},^{-i_{t}}]\) and \([^{i_{t+1}},^{-i_{t+1}}]\). In iteration \(t\), the learner chooses the control set \(i_{t}=1\) and specifies the value (of the first variable \(x_{t,1}\)) in control partial query \(^{i_{t}}\), while the last two variables \(X_{t,2},X_{t,3}\) in random partial query \(^{-i_{t}}\) will be randomly sampled. In iteration \(t+1\), the learner chooses the control set \(i_{t+1}=2\) and specifies the values (of the first two variables \(x_{t,1},x_{t,2}\)) in control partial query \(^{i_{t+1}}\), while the last variable \(X_{t,3}\) in random partial query \(^{-i_{t+1}}\) will be randomly sampled.

in the learning trajectory. In fact, BOPSQ reduces to a simpler problem if there exists a _full query control set_ that allows the learner to choose the values of all \(d\) variables. If \([d]\), then \(_{i^{*}}=[d]\) and \(f(\{.^{i^{*}},^{-i^{*}}. =_{}f()\) since expectations of a function are never greater than the maximum value of the function. In other words, the full query control set is guaranteed to be the optimal control set and the BOPSQ problem reduces to one of conventional BO. In general, under BOPSQ, any control set that is a strict subset of another will never be chosen.

### Ucb-Cvs

Alg. 1 describes our UCB-CVS algorithm for solving the BOCVS problem. In iteration \(t\), it uses the GP posterior belief of \(f\) to construct an _upper confidence bound_ (UCB) \(u_{t-1}\) of \(f\):

\[u_{t-1}()=_{t-1}()+_{t}_{t-1}()\]

where the sequence \((_{t})_{t 1}\) is an algorithm parameter that controls the tradeoff between exploration and exploitation. UCB-based algorithm design is a classic strategy in the stochastic bandits [19, Ch. 7] and BO literature  and makes use of the "_optimism in the face of uncertainty_" (OFU) principle : Queries with a large posterior standard deviation (i.e., high uncertainty) are given high acquisition scores as the function values at those queries may be potentially high. UCB-CVS adapts this strategy by taking the expectation of the UCB as part of the acquisition process. Due to the monotonicity of expectation, if \(u_{t-1}\) is an upper bound of \(f\) (i.e., \(u_{t-1}() f()\) for any \(\)), then \(u_{t-1}([^{i},^{-i}])\) is also an upper bound of \(f([^{i},^{-i}])\) for any \(i[m],^{i}^{i}\).

UCB-CVS also takes as input an \(\)-schedule \((_{t})_{t=1}^{}\) where \(_{t} 0\) for all \(t\). To choose the control set in iteration \(t\), it first computes \(g_{t}\) which is the expected UCB of the best control set and specified values in the control partial query (Step 3). It then collects every control set \(i\) that fulfills the condition \(_{^{i}^{i}}u_{t-1}([^ {i},^{-i}])+_{t} g_{t}\) into a set \(_{1}\) (Step 4). It further reduces this set \(_{1}\) to \(_{2}\) by retaining only the control sets with the lowest cost (Step 5). Finally, it chooses the control set from \(_{2}\) with the largest expected UCB value (Step 6). Each \(_{t}\) thus serves as a relaxation that enables exploration with cheaper control sets. Choosing many \(_{t}\) to be large results in many iterations of choosing cheaper control sets; conversely, choosing \(_{t}=0\) for all \(t\) ignores all costs.

Our first result upper bounds the cost-varying cumulative regret incurred by UCB-CVS. Define the _feasible set_\(}_{i}_{=1}^{d}[a_{t}^{i},b_{t}^{i}]\) for each control set \(i\) such that \(a_{}^{i}=0\), \(b_{}^{i}=1\) if \(_{i}\), and \(a_{}^{i}=\{a F_{}(a)=0\}\), \(b_{}^{i}=\{b F_{}(b)=1\}\) otherwise, where \(F_{}\) is the CDF of \(X_{}_{}\). \(}_{i}\) is a subset of \(\) in which any query chosen with control set \(i\) must reside. Define \(T_{i}\) as the total number of iterations in which control set \(i\) is chosen.

**Theorem 4.1**.: _With probability at least \(1-\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by_

\[R_{T}(B+()+})(_{i=1}^{m}\!c_{i}(_{T_{i}}( }_{i})}+))+c_ {m}{_{t=1}^{T}}_{t}\]

_by setting \(_{t}=B+()+1+((m+1)/ ))}\)._For any appropriately chosen kernel such that \(_{T}()<()\) (e.g., commonly used squared exponential kernel, see Sec. 3) and \(\)-schedule such that \(_{t=1}^{T}_{t}\) is sublinear in \(T\), the cumulative regret incurred will be sublinear in \(T\): \(_{T}R_{T}/T=0\). Since the mean of a sequence is no less than the minimum, and all \(c_{i}>0\), this further implies the desired no-regret property: \(_{T}_{1 t T}(f([^{i^{*}}, ^{-i^{*}}])-f([^{i_{t}}, ^{-i_{t}}])=0\), i.e., the best control set and specified values in control partial query in the algorithm's choices eventually converge to the optimal solution. The proof of Theorem 4.1 relies on choosing an appropriate sequence of \(_{t}\) such that \(u_{t-1}() f()\) for any \(,t 1\) with high probability [7, Theorem 2]. The cumulative regret is bounded by a sum of expectations of posterior standard deviations, which can then be bounded by a sum of posterior standard deviations plus some additional terms [16, Lemma 3] and in turn bounded in terms of the MIG [7, Lemma 4]. The proofs of all results in this paper are provided in Appendix A.

Since each \(_{T_{i}}(}_{i})\) is increasing in the volume of \(}_{i}\), Theorem 4.1 states that control sets with smaller feasible sets will incur less regret. If the size of a feasible set is taken to be a reasonable surrogate for the diffuseness of the probability distributions involved, Theorem 4.1 then suggests that control sets with corresponding random sets whose probability distributions are less diffuse will incur less regret.2 Theorem 4.1 also informs us that one sufficient condition on the \(\)-schedule for the cost-varying regret to be sublinear in \(T\) is that \(_{t=1}^{T}_{t}\) is sublinear in \(T\). Our next proposition provides an alternative condition (neither is more general than the other):

**Proposition 4.2**.: _If there exists a \(>0\) s.t. for all \(i i^{*}\), \(_{t}f([^{i^{*}},^{-i^{*}}]) -_{^{i}^{i}}f([^{i},^{-i}])-\) eventually (i.e., the inequality holds for all \(t q\) for some \(q 1\)), and \(_{T}()<()\), then, with probability at least \(1-\), \(_{T}T_{i}/T=0\) for all \(i i^{*}\) and UCB-CVS incurs a cost-varying cumulative regret that is sublinear in \(T\) by setting \(_{t}=B+()+1+((m+1)/) )}\)._

The above results have shown that with an appropriately chosen \(\)-schedule, UCB-CVS satisfies the no-regret property. However, ignoring all costs by setting \(_{t}=0\) for all \(t\) also achieves no-regret. This begs the question: _In what way does a good \(\)-schedule improve UCB-CVS?_ Supposing the most expensive control set is the full query control set, the presence of queries chosen with cheaper control sets should reduce the cost-varying regret incurred by the full query control set by ruling out low function value regions and directing the full queries towards high function value regions. Additionally, it is reasonable to conjecture that the more diffuse each variable's (indexed by \(\)) probability distribution \(_{}\) is, the more the cheaper control sets would explore the query space and thus, the lower the cost-varying regret incurred by the full query control set. To derive such a result, the plan of attack is to relate the variances (i.e., notion of diffuseness) of the probability distributions to the distances between queries chosen with the cheaper control sets, followed by analyzing the effect of these distances and the number of times cheaper control sets were played on the MIG term of the most expensive control set. Our next result relates the distance between pairs of queries chosen with control set \(i\) to the variance \([X_{}]\) of every probability distribution \(_{}\) for \(}_{i}\):

**Lemma 4.3**.: _Suppose that for each control set \(i\), the random variable \(Y_{i}:=[,_{1}^{-i}]-[,_{2}^{- i}]^{2}\) has a median \(M_{i}\) s.t. \([Y_{i}|Y_{i}>M_{i}] h_{i}M_{i}\) for some \(h_{i}>0\) where \(_{1}^{-i},_{2}^{-i}^{-i}\). With probability at least \(1-\), there will be at least \(N_{i}\) non-overlapping pairs of queries \(\) and \(^{}\) chosen by UCB-CVS (Alg. 1) with control set \(i\) s.t. \(\|-^{}\|^{2} M_{i}\) where_

\[N_{i}=(T_{i}-1)/4-/4)(1/)}  M_{i}(4/(h_{i}+1))_{}_{i} }[X_{}]\.\] (2)

From (2), the higher the variances of the distributions that govern the variables in the random set, the larger the lower bound \(M_{i}\) on the squared distance between at least \(N_{i}\) pairs of queries chosen with control set \(i\). As expected, the number \(N_{i}\) of pairs increases with \(T_{i}\) (i.e., the total number of iterations in which control set \(i\) is chosen). The assumption on \(Y_{i}\) is mild: As long as \(Y_{i}\) has at least \(1\) non-zero median, it will hold. The assumption excludes the case in which \(_{}\) for all \(}_{i}\) are degenerate with all probability mass on a single point. With Lemma 4.3, we now derive an alternative regret bound that depends on the variances of the distributions and the number of plays of cheaper control sets:

**Theorem 4.4**.: _Suppose that the following hold:_

* _Assumption of Lemma_ 4.3 _holds;_
* \(k(,^{})\) _is an isotropic kernel which only depends on distance between_ \(\) _&_ \(^{}\) _and can be written as_ \(k(\|-^{}\|)\)_;_
* _There exists an iteration_ \(r\) _s.t. for all_ \(t r,i_{t} m-1\)_, and for all_ \(t>r,i_{t}=m\) _._

_Then, with probability at least \(1-\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by_

\[R_{T} (B+()+})(c_{m}(()}-+ ).\] \[.+_{i=1}^{m-1}c_{i}(_{T_{i}}( }_{i})}+)\,)+c _{m}{_{t=1}^{T}}_{t}\] \[(_{i=1}^{m-1}N_{i} V_{i}-2k}-k}^{2})+W)\]

_by setting \(_{t}=B+()+1+((2m)/) )}\) where \(N_{i}\) and \(M_{i}\) are previously defined in Lemma 4.3, and \(V_{i}\) and \(W\) are residual terms defined in Appendix A.5._

Theorem 4.4 shows that the MIG term pertaining to the most expensive control set \(m\) is reduced by \(\) which increases as \(N_{i}\) increases, which in turn increases as \(T_{i}\) increases. This suggests that an \(\)-schedule that increases the number of times cheaper control sets are played can reduce the MIG term. \(\) also increases as \(k(})\) decreases. For common kernels such as the squared exponential or Matern kernel with \(>1\) (which satisfy the second assumption on isotropic kernel), \(k(})\) decreases as \(M_{i}\) increases, from which we may conclude that higher variance probability distributions governing each \(X_{}\) lead to a larger \(\) due to (2) and hence a larger decrease on the MIG term. In cases where \(c_{m} c_{i}\) for all \(i m\), a carefully chosen \(\)-schedule can thus lead to a large decrease in the regret bound via \(\). The third assumption is (informally) approximately true in practice due to the design of UCB-CVS: If a decreasing \(\)-schedule is used, the algorithm will choose the cheaper but sub-optimal control sets at the start. After \(_{t}\) has decreased past a certain value, the algorithm will only choose the optimal (and likely most expensive) control set. The proof sketch upper bounds the sum of posterior standard deviations of queries chosen with control set \(m\) with the MIG term minus the sum of posterior standard deviations of queries chosen with all other control sets. This latter sum is then lower bounded by a log determinant of the prior covariance matrix which is then decomposed into a sum of log determinants of pairs of queries. The dependence on the distances between the pairs can be made explicit in this form. Neither Theorems 4.1 nor 4.4 is more general than the other.

### Practical Considerations

UCB-CVS is presented with the \(\)-schedule formulation for generality and ease of theoretical analysis. In practice, however, the \(\)-schedule is a hyperparameter that is difficult to interpret and choose. We propose a simple _explore-then-commit_ (ETC) variant with which the learner only chooses the number of plays of each _cost group_ (i.e., defined as a collection of control sets with the same cost that is not the maximum cost). In each iteration, the algorithm will choose the cost group with the lowest cost and non-zero remaining plays, and then choose the control set within that cost group with the largest expected UCB (similar to Step 6 in Alg. 1). Once all cost groups have zero remaining plays, the algorithm chooses the control set with the largest expected UCB among all control sets. This algorithm is highly interpretable and is equivalent to UCB-CVS with a specific sublinear \(\)-schedule (that cannot be known _a priori_). Furthermore, the learner should choose the number of plays adaptively depending on the cost of each cost group. On computational considerations, UCB-CVS may be computationally expensive if the number \(m\) of control sets is large (e.g., if every subset of variables is available as a control set and \(m=2^{d}\)) as each control set requires a maximization of the expected UCB (which can be approximated with Monte Carlo sampling). In such cases, the learner has the option to simply ignore any number of control sets to reduce \(m\), as long as \(i^{*}\) is not ignored.

Figure 2: Mean and standard error (over \(10\) RNG seeds) of the simple regret (lower is better) incurred against cost spent (budget) \(C\) by **TS-PSQ**, **UCB-PSQ**, **ETC-50**, **ETC-100**, and **ETC-Ada** with varying objective functions, cost sets, and variances of distributions. A diamond indicates the average budget after which an algorithm only chooses the optimal control set.

Experiments and Discussion

This section empirically evaluates the performance of the tested algorithms with \(4\) objective functions: (a) function samples from a GP prior (\(3\)-D), (b) the Hartmann synthetic function (\(3\)-D), (c) a plant growth simulator built from real-world data where the variables are nutrients such as NH\({}_{3}\) and pH (\(5\)-D), and (d) a simulator built from the airfoil self-noise dataset (\(5\)-D) from the UCI Machine Learning Repository . For the first \(2\) objective functions, the control sets are all possible subsets of the \(3\) variables except the empty set, which leads to \(7\) control sets. For the plant growth objective function, we pick \(7\) control sets including the full query control set. For the airfoil self-noise objective function, similar to that of , we pick \(7\) control sets of \(2\) variables each that are not subsets of each other. We use \(3\) different sets of costs for the \(7\) control sets: **cheap** (\(\{0.01,0.01,0.01,0.1,0.1,0.1,1\}\)), **moderate** (\(\{0.1,0.1,0.1,0.2,0.2,0.2,1\}\)), and **expensive** (\(\{0.6,0.6,0.6,0.8,0.8,0.8,1\}\)). Using these sets of costs, the control sets are ordered such that \(c_{i}<c_{j}_{^{i}^{i}} f([^{i},^{-i}])_{^{j}^{j}} f([^{j},^{-j}])\). These cost sets have fixed the optimal (i.e., last) control set to have a cost of \(1\). While these cost sets may (at first glance) seem arbitrary, it is the algorithms' _relative performance across these cost sets_ rather than the absolute performance on a single cost set that allows us to understand the conditions under which particular algorithms perform better or worse. Real-world applications (unlike the experiments conducted here) will come with their own cost sets defined by real-world constraints. If the real costs can also be categorized in a similar relative way like the above cheap, moderate, and expensive cost sets, then the results are expected to be similar. Every probability distribution \(_{}\) is a truncated normal distribution with mean \(0.5\) and the same variance which is one of \(0.02\), \(0.04\), and \(0.08\) (the uniform distribution on \(\) has variance \(1/12\)).

We compare the performance of our algorithm against that of the baseline Thompson sampling (**TS-PSQ**) algorithm developed in . We test **UCB-PSQ** (\(\)-schedule with \(_{t}=0\) for all \(t\)) along with the ETC variant of UCB-CVS (Sec. 4.2) with \(3\) sets of hyperparameters: \(50\) plays per cost group (**ETC-50**), \(100\) plays per cost group (**ETC-100**), and a cost-adaptive version with \(4/c_{j}\) plays per cost group where \(c_{j}\) is the cost of the control sets in that cost group (**ETC-Ada**). We also investigated simple extensions of TS-PSO, UCB-PSQ, and expected improvement (adapted for BOPSQ) for the BOCVS problem by dividing the acquisition score of a control set by its cost in a manner similar to that in [31, Sec. \(3.2\)]. We observed that these naive methods generally do not work well; we defer the results and discussion of these methods to Appendix B. Refer to Appendix C for full descriptions of all experimental settings and algorithm hyperparameters. The code for the experiments may be found at https://github.com/sebtsh/bocvs.

Fig. 2 shows the mean and standard error (over \(10\) RNG seeds) of the simple regret \(_{1 t T(C)}f([^{i^{*}},^{-i^ {*}}])-f([^{i_{t}},^{-i_{t}}]) \) (lower is better) incurred against cost spent (budget) \(C\) by each algorithm with varying objective functions, cost sets, and variances of distributions where \((C)\) denotes the maximum iteration reached after spending \(C\). The simple regret encodes the value of the best solution an algorithm has chosen within a certain budget and is a measure of cost efficiency. We report the salient observations below:

**(1) UCB-CVS variants outperform TS-PSQ and UCB-PSQ under cheap/moderate costs when the full query control set is available.** With the GP sample, Hartmann, and plant growth objective functions, the full query control set is available. TS-PSQ and UCB-PSQ only choose the full query control set in every iteration and are very cost inefficient under cheap and moderate costs, while UCB-CVS variants are able to use the cheaper control sets for exploration, followed by using the full query control set for exploitation, and find much better solutions with the same budget. As expected, their performance advantage reduces as the costs increase and \(c_{m}\) gets closer to \(c_{i}\) for all \(i m\).

**(2) Cost-adaptive UCB-CVS (ETC-Ada) can maintain competitive performance under expensive costs.** The non-cost-adaptive variants, ETC-50 and ETC-100, perform worse than TS-PSQ and UCB-PSQ under expensive costs. In contrast, it can be observed that ETC-Ada generally performs well under all costs by tuning the number of plays of suboptimal cost groups according to their costs. We recommend practitioners to use adaptive algorithms to achieve good performance under any cost set. In particular, the results suggest that an \((c_{i}^{-1})\) threshold is likely to work well across different sets of costs and is a robust choice for practitioners that keeps the number of hyperparameters to a minimum.

**(3) TS-PSQ and UCB-PSQ perform relatively well when the control sets are not subsets of each other.** With the airfoil self-noise objective function, TS-PSQ and UCB-PSQ perform better as the control sets with this objective function are not subsets of each other and thus, they can also use the cheaper control sets during learning, while the UCB-CVS variants suffer worse performance here due to artificially selecting suboptimal control sets and queries with the \(\)-relaxations. This worse performance is encoded in Theorems 4.1 and 4.4 as the sum of \(_{t}\) terms.

**(4) Increasing the variance of the probability distributions has competing effects on the simple regret.** Of the \(42\) experimental settings (combinations of objective function, cost set, and algorithm) in which the variance makes a difference (excluding TS-PSQ and UCB-PSQ for all objective functions except airfoil), the settings with variance \(0.02\), \(0.04\), and \(0.08\) achieved the lowest mean simple regret by the end \(11\), \(6\), and \(25\) times, respectively. This generally supports Theorem 4.4's prediction that higher variances decrease the upper bound on regret. However, due to the looseness of the bound, this effect is not guaranteed and there are still cases where lower variances lead to a lower regret, as suggested by the argument about feasible sets when discussing Theorem 4.1; note that the same MIGs of the feasible sets for control sets \(1\) to \(m-1\) appear in Theorem 4.4. We observe competing effects and conclude that the effect of increasing variance is problem- and algorithm-dependent. While higher variances may lead to more exploration, they may also result in too much smoothing of function values which may hinder the learner's ability to focus on high-value query regions.

## 6 Conclusion

This paper introduces the BOCVS problem and describes the UCB-CVS algorithm that is provably no-regret in solving this problem. We show that our algorithm performs well across several different experimental settings and achieves the desired goal of finding significantly better solutions within the same budget. This work opens up avenues of future research: In particular, an entropy search-based algorithm [14; 23; 41] that chooses control sets and queries based on expected information gain per unit cost is a non-trivial and promising direction for alternative methods of solving BOCVS.