# Data-Faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables

Qiheng Sun\({}^{1,2}\), Haocheng Xia\({}^{3}\), Jinfei Liu\({}^{1,2}\)

\({}^{1}\)Zhejiang University

\({}^{2}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

\({}^{3}\)Siebel School of Computing and Data Science

University of Illinois Urbana-Champaign

{qiheng_sun,jinfeiliu}@zju.edu.cn, hxia7@illinois.edu

Jinfei Liu is the corresponding author.

###### Abstract

The state-of-the-art feature attribution methods often neglect the influence of unobservable confounders, posing a risk of misinterpretation, especially when it is crucial for the interpretation to remain faithful to the data. To counteract this, we propose a new approach, data-faithful feature attribution, which trains a confounder-free model using instrumental variables. The cluttered effects of unobservable confounders in a model trained as such are decoupled from input features, thereby aligning the output of the model with the contribution of input features to the target feature in the data generation. Furthermore, feature attribution results produced by our method are more robust when focusing on attributions from the perspective of data generation. Our experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches.

## 1 Introduction

The increasing complexity and opacity of machine learning (ML) models in real-world applications boost the demand for feature attribution . The feature attribution methods have been developed to help users understand why a model produces certain outputs from specific inputs. For instance, a loan applicant rejected by a bank's decision-making model might seek reasons behind the denial and what changes could potentially reverse the model's decision. Some recent studies  have shifted the focus of feature attribution from a traditional _model-centric_ perspective to a new perspective that is _data-centric_. Specifically, users may wish to assign importance values to features according to the _data generation process_, referring to the causal relationships through which features influence the target feature. For example, consider a medical application setting where a patient seeks to understand which personal features aggravated the illness and the cooperative impact of all features on the illness. What the patient really wants to know is how all features collaboratively contribute to the illness, rather than one diagnostic model's prediction output. These two aspects of feature attribution align with the concepts of _model fidelity_ and _data fidelity_, respectively. Here, model fidelity refers to the attribution being consistent with the output of the explained model, while data fidelity pertains to the attribution being consistent with the data generation process.

SHapley Additive exPlanations (SHAP)  and Integrated Gradients (IG)  are prevalent representatives of two distinct series of feature attribution methods, each uniquely satisfying critical axioms including sensitivity, implementation invariance, completeness, and symmetry . These properties are essential for ensuring reasonability and fairness in feature attribution. The SHAP-based methods, grounded in game theory and particularly the Shapley value , offer interpretations by evaluating all possible combinations of feature contributions in a discrete feature space. In contrast,the IG-based methods, which are an analog of the Aumann-Shapley method  from cost-sharing, focus on continuous feature spaces by using path integration over gradients. It is worth noting that even when we aim for interpretations that are faithful to the data, we still rely on a model to predict the target feature values when certain features are selected or excluded in the attribution process. However, when unobservable confounders exist, both SHAP-based and IG-based methods may lead to misunderstandings if applied directly to the widely used predictive model. This is because unobservable confounders, although impacting the output, are entirely overlooked from the process of attribution. Consequently, their influence is instead attributed to other correlated features.

**Motivation example.** As shown in Figure 1, suppose a model predicting personal income includes _Education_ as an input feature, and _Ability_ serves as a confounder if the model does not incorporate it as an input feature. Because _Ability_ has an indirect impact on _Income_ through its influence on _Education_ and a direct impact on _Income_ simultaneously, the existing feature attribution methods tend to incorrectly attach the direct impact of _Ability_ on _Income_ to the impact of _Education_ on _Income_ due to their correlation. This concealed correlation may lead to incorrect attribution on the role of educational level in personal income, resulting in an overestimation of the education returns, as demonstrated in our experiments using real datasets (SS 5).

In this paper, we develop a method to eliminate unobservable confounder effects in feature attribution in order to achieve a deeper understanding from the perspective of data fidelity. The instrumental variable method is widely used for causal analysis . It lies in identifying features that directly affect those influenced by confounders, while not having a direct impact on the outcome themselves. By using the instrumental variable to control confounders that influence specific features, any resulting changes in the outcome variable are driven solely by how the instrumental variable alters that feature. For example, in Figure 1, when examining the impact of _Education_ on _Income_, a suitable instrumental variable could be the variable _Parental Education_ as analyzed in appendix Section F.2. By observing the changes in _Education_ resulting from variations in _Parental Education_, we can then discern the true effect of _Education_ on _Income_, effectively isolating it from other confounders. Intuitively, the instrumental variable approach can help to mitigate the impact of unobservable confounders in feature attribution. However, the instrumental variable approach mainly focuses on evaluating the isolated impact of individual features influenced by unobservable confounders on the outcome variable, while feature attribution lies in considering the cooperative attribution of features. This means evaluating the combined contribution of _Education_ and _Other Features_ on _Income_.

To bridge this gap, we propose using a two-stage model training with the instrumental variable that disrupts the association between confounders and other features. Specifically, the model is trained using features re-estimated through instrumental variables and collaborative variables. This ensures that the influence of confounders remains consistent despite variations in feature coalitions. Therefore, the marginal contribution of each feature, which is determined by assessing the impact on the model's output with and without the feature, is not affected by any confounders. Furthermore, the attribution value of each feature, calculated as the average of its marginal contributions across different feature coalitions, remains influenced by confounders. This alignment allows the contribution of input features to the model output to mirror their contribution in data generation to the target feature, thereby facilitating the attribution to be faithful to the data. Feature attribution involves explaining a model output by assigning attribution scores to the input instance. However, our focus is on data-faithful feature attribution, i.e., we are not trying to explain the output of a specific model but trying to explain the target feature through a model.

Figure 1: Arrows indicate direct effects. Since _Ability_ s not directly observed and is correlated with education, the influence that should be attributed to _Ability_ (arrow 1) is erroneously attributed to _Education_ (arrow 2) in feature attribution. To fix this issue, _Parental Education_ can be used as an instrumental variable for investigating the true impact of _Education_ on _Income_.

**Contributions.** To the best of our knowledge, we are the first to identify a crucial issue: unobservable confounders compromise feature attribution, especially when data fidelity is essential. To tackle this challenge, we propose training models free of confounders using instrumental variables, ensuring the feature attribution will remain faithful to the data. We validate the effectiveness of our proposed methods using both real and synthetic datasets, observing that our method achieves up to a 67% relative improvement over the baseline methods in terms of the error of attribution ratio metric in the real dataset.

## 2 Preliminaries

### Problem Setup

We aim to quantitatively assess the influence of each input feature on the target feature. This assessment can be viewed as a contribution assignment problem in the context of cooperative game theory . Formally, given an explained input vector of \(d\) features \(^{*}=\{x_{1}^{*},,x_{d}^{*}\}\), a baseline input \(^{}\), and a model \(f:^{d}\) which approximates the data generation equation for the target feature, our objective is to explain the difference in target feature, i.e., \(y^{*}-y^{}\), conducting data-faithful attribution for the input features. We assume \(^{*}\) and \(^{}\) are of the same dimensionality \(d\), and each entry can be either discrete or continuous. We denote by \(X\) the set of input features and \(Y\) the target feature, partitioning \(X\) into two subsets: \(\), which is influenced by unobserved confounders (denoted as \(\)), and \(\), the set of other observable features. For clarity and convenience, we use \(\), \(y\), \(}\), \(\), \(\) to denote possible values within feature sets \(X\), \(Y\), \(\), \(\), \(\), respectively. For a given subset of features \(\), we denote the subset of the original vector of values by using \(\) as a subscript, e.g., \(_{}:=\{x_{i}\}_{i:i}\).

### SHAP-based Attribution

**Shapley Value.** Consider a set of players \(=\{1,,d\}\). A _coalition_\(\) is a subset of \(\) that cooperates to complete a task. A utility function \(()\) (\(\)) is the utility of a coalition \(\) for a task. The _marginal contribution_ of player \(i\) with respect to a coalition \(\) is \((\{i\})-()\). Shapley value is the unique metric that satisfies the properties of fair reward allocation, including balance, symmetry, additivity, and zero element . It measures the expectation of marginal contribution by \(i\) in all possible coalitions. That is,

\[_{i}=|}{||}_{ \{i\}}(\{i\})- ()}{|-1}{||}}.\]

Computing the exact Shapley value requires enumerating all utilities for all player subsets. Therefore, the computational complexity of exactly calculating the Shapley value is exponential .

SHAP  utilizes the concept of Shapley values to attribute the contribution of each feature in a model. In the existing SHAP-based methods [27; 19], the definition of utility functions for interpreting an input \(\) can be divided into two categories , condition expectation Shapley and intervention Shapley. In condition expectation Shapley, following the assumption that the features are generated according to a distribution \(D\), the utility function is defined by \(^{}()=E_{D}[f()|_{}= _{}^{*}]\) based on the condition expectation of the model prediction under feature set \(\). In intervention Shapley, the utility function is defined by \(^{}()=E_{D}[f()|do(_{} =_{}^{*})]\) where the operation \(do(_{}=_{}^{*})\) means we intervene on the features \(\) in variable \(\) to be the same as the features in \(^{*}\), while the features outside of \(\) in \(\) are influenced following the causal relationships of the features . For conciseness, we omit the subscript \(D\) in the expectation term in the rest of the paper.

### IG-based Attribution

IG is a pivotal method for model attribution , particularly well-suited for deep neural networks due to its prerequisite that the model be continuously differentiable. This approach calculates the cumulative gradients along a straight-line path extending from a baseline input \(^{}\) to the explained input \(^{*}\). Mathematically, the attribution \(_{i}\) assigned to a particular feature \(_{i}^{*}\) for a given inputand baseline \(^{}\) is defined as:

\[_{i}(^{*},^{},f)=(^{*}_{i}- ^{}_{i})_{=0}^{1}^{}+ (^{*}-^{}))}{^{*}_{i}}d. \]

Remarkably, IG shares similarities with the Aumann-Shapley approach  and satisfies several essential properties, including linearity, dummy attribution, Affine Scale Invariance (ASI), proportionality, and symmetry . Recent research has enhanced IG through its application to complex models and the refinement of the integrated path [1; 28].

## 3 Misattribution with Unobservable Confounders

In this section, we carry out a theoretical analysis to demonstrate how unobservable confounders mislead the feature attribution of both SHAP-based and IG-based methods. To show the influence of unobservable confounders in feature attribution, we first employ a simple structural equation to characterize the data-generating process, expressed as

\[y=g(},})+,\]

where \(g(},})\) can represent any linear or nonlinear continuous relationship involving both \(}\) and \(}\). The equation allows us to clearly recognize the individual contributions of \(}\) and \(}\) to \(y\), while also considering the unobserved effects encapsulated in the error term \(\). Given that \(\) is the set of features influenced by unobservable confounders \(\), it generally follows that given two data instances \(_{1}\) and \(_{2}\), \([|}_{1}][|}_ {2}]\) when \(}_{1}}_{2}\) since \(\) is influenced by \(\) while \([|}_{1}]=[|}_{2}]\) is valid as the feature set \(\) is not affected by \(\).

### Example of Errors for Data-Faithful Feature Attribution

We discuss how unobservable confounders introduce errors in data-faithful feature attribution with a toy example of Figure 1 in condition expectation Shapley. We can assume \(\) as ability, \(}\) as the measurement of education level, \(}\) the work time in a week (i.e., the other variable), and \(y\) the weekly income. \(}\), \(}\), and \(\) each represents a single numerical variable. The data generation equations are defined as follows:

\[(0,1),}(0,1) +,}(0,1), y=}}+.\]

In this case, ability influences education, and the three features all have a direct influence on income. Consider a specific data instance \(^{*}=[}^{*},}^{*}]=[1.5,1]\)  we are curious about the contributions of the individual's education level and work hours to their income compared to the features distribution. In this example, it means assigning a value to the individual's education level \(}^{*}=1.5\) and work time \(}^{*}=1\) to evaluate their contribution to weekly income in comparison to the distribution of education levels and work hours of the population.

First, we conduct feature attribution with a model \(f\) that is trained to fit \([y|]=[y|},}]=g(},})+[|},}]\) which simulates the widely used supervised machine learning model training paradigm in reality. The utility derived from the model is \(^{}()=[f()|_{}=^{*}_{}]=[g(},})|_{ }=^{*}_{}]+[|_{}=^{*}_{}]\). The condition expectation Shapley value of \(}^{*}\) is \(^{}_{}^{*}}=\{[^{ }(\{}^{*}\})-^{}()]+[ ^{}(\{}^{*},}^{*}\})- ^{}(\{}^{*}\})]\}\). Replacing the activities, we have \(^{}_{}^{*}}=\{[g( }^{*},})]+[|}^{*},}]-[g(},})]- [|},}]+[g(}^{*}, }^{*})]+[e|}^{*},}^{*} ]-[e|},}^{*}]\}\).

Then, we conduct feature attribution for \(}^{*}\) with the term which it really contribute to \(y\), i.e., \(g(},})\). The utility derived is \(}^{}()=[g(}, })|_{}=^{*}_{}]\). According to the definition of condition expectation Shapley, \(}^{}_{}^{*}}=\{[ }^{}(\{}^{*}\})-}^{}()]+[}^{}(\{}^{*}, }^{*}\})-}^{}(\{}^{*} \})]\}\). Replacing the accrrording utilities, we have \(}^{}_{}^{*}}=\{[ [g(}^{*},})]-[g(}, })]+[g(}^{*},}^{*})]- [g(},}^{*})]\}\).

Errors in Feature Attribution Values.The values of each expectation term computed according to the data generation equations are shown in Table 1. Since \(}\) is independent from \(\), we have \([|},}^{*}]=[| },}]\) and \([|}^{*},}^{*}]=[| }^{*},}]\). By substituting the values for each expected term, we can obtain that \(^{}_{}^{*}}=0.875\), \(}^{}_{}^{*}}=0.625\), \(^{}_{}^{*}}=0.325\), and \(}^{}_{}^{*}}=0.325\). It'sclear that attribution based on the model trained to fit \([y|]\), which is a common training paradigm in supervised learning, tends to give a wrong attribution value of \(}^{*}\), which is the excessive attribution value of education level in this example. The intuitive reason is that the model associates the direct effect of ability on income with the education level, i.e., the influence of \(\) is attached to \(}\). However, \([|}^{*},}]-[| },}]\) is not actually in the effect of \(}^{*}\) on \(y\) because \(}\) does not influence \(\) during the data generation process.

### Errors in Feature Attribution with Unobservable Confounders

We analyze the attribution errors when the feature attribution is conducted on a model \(f\) trained to fit \([y|]=[y|},}]\) in supervised learning for SHAP-based method (Propositions 1 and 2) and IG (Proposition 3), respectively.

**Proposition 1**.: _The expected error for marginal contribution of feature \(i\) in condition expectation Shapley with model \(f\) trained to fit \([y|]\) is \([|_{\{i\}}=^{*}_{ \{i\}}]-[|_{}=^{*}_{}]\), resulting an expected deviation of attribution value by \(_{i}=_{ \{i\}}|-1}{||}^{-1}\{[|_ {\{i\}}=^{*}_{\{i\}}]-[| _{}=^{*}_{}]\}\)._

Proof.: Due to the limited space, please see the appendix for detailed proof. The same to the following propositions. 

**Proposition 2**.: _The expected error for marginal contribution of feature \(i\) in intervention Shapley with mode \(f\) trained to fit \([y|]\) is \(_{D}[|do(_{\{i\}}=^{*}_{\{i\}})]-[|do(_{}=^{*}_{})]\), resulting an expected deviation of attribution value by \(_{i}=_{\{i\}} |-1}{||}^{-1}\{[|do(_{ \{i\}}=^{*}_{\{i\}})]-[| do(_{}=^{*}_{})]\}\)._

**Proposition 3**.: _The expected error for attribution value of feature \(i\) using IG with model \(f\) trained to fit \([y|]\) is \(_{i}=(^{*}_{i}-^{}_{i})_{=0}^{1} ^{}+^{*}-^{ }}{_{i}}-^{}+ ^{*}-^{}}{_{i}}d\)._

## 4 Mitigating Unobservable Confounders via Instrumental Variables

As demonstrated in Section 3, when it pertains to unobservable confounders, the prevalent feature attribution methods including SHAP and IG inevitably lead to misunderstandings that are not faithful to the data, since they rely on predictive model \(f\) trained to fit \([y|]\). This is fundamental because the trained predictive model has already associated the unobservable confounders with the input features. Therefore, it is tempting to ask: _how can we decouple the confounders from their correlations with other features in the used model \(f\)_?

### Motivation of Using Confounder-free Model

One may think a straightforward solution is directly training a model \(f\) to fit \(g(},})\). Unfortunately, it is nearly impossible since we cannot remove the influence of unobservable confounders in the target feature \(y\). To bridge the gap, we provide an alternative solution to train a model that gives the same attribution results for the input features as it is trained to fit \(g(},})\). Denote by \(=g(},})+[]\), and \(f\) is trained to fit \([|},}]\). The influence of \(}\) and \(}\) on \(\) is identical to their impact on \(y\), as both are encompassed within \(g(},})\).

**Example**. For the toy example in Section 3, the utility calculated with \(f\) trained to fit \([|},}]=g(}, })+[]\) is \(}^{}()=[g(}, })|_{}=^{*}_{}]+[]\). The condition expectation Shapley value of \(}^{*}\) is \(}^{}_{}^{*}}=\{[}(}^{*})-}(0)]+[}( }^{*},}^{*})-}(\{ }^{*}\})]\}\). By replacing the according utilities, we have \(}_{}^{*}}=\{[g(}^{*},})]+[]-[g(}, })]-[]+[g(}^{*}, }^{*})]+[]-[g(}, }^{*})]-[]\}=\{[g( }^{*},})]-[g(},})]+[g(}^{*},}^{*})]-[g( },}^{*})]\}=}^{}_{ }^{*}}\).

The advantage of attribution based on \(\) lies in the term \([]\) being constant, thereby breaking the association between \(\) and the input features.

**Proposition 4**.: _From the perspective of data generation, the contributions of features in \(}\) and \(}\) to \(y\) are equivalent to their contributions to \(\). When using the condition expectation Shapley, intervention Shapley, and Integrated Gradients (IG) methods, the attribution values of each feature in \(}\) and \(}\) are identical for both models \(f=g(},})\) and \(f=g(},})+[]\)._

### Confounder-free Model Building

With Proposition 4, the problem becomes how to train the model \(f\) to fit \([|},}]\) now. To achieve this, we introduce the instrumental variables.

**Instrumental Variable.** The features that are used as instrumental variables, denoted as \(\), can be effectively utilized in our model if they satisfy the following three key properties. 1) **relevance**: \(\) should correlate with \(\), ensuring that \(\) can serve as a reliable proxy for these features. 2) **exogeneity**: \(\) should be uncorrelated with the latent confounders \(\), ensuring that it is not influenced by these unobserved factors. 3) **exclusion restriction**: \(\) should influence the outcome \(Y\) solely through its effect on \(\). In other words, apart from its interaction with \(\), \(\) should not have any other direct or indirect pathways affecting \(Y\). This ensures that the effect of \(\) on \(Y\) can be unambiguously attributed to its relationship with \(\). The effectiveness of IV-SHAP and IV-IG may be reduced when the three assumptions of instrumental variables are violated. However, the extent of this reduction depends on how severely the assumptions are violated.

With the help of instrumental variables, we can establish the following equation by taking the expectation of \(y\) given \(}\) and \(\),

\[[y|},]=[g(}, })|},]+[]= g( },})+[]dM(}| },),\]

where \(\) is a possible value of \(\) and \(dM(}|},)\) is the conditional distribution of \(\). Given the \(T\) training data instances, the optimal parameters of model \(f\) trained to fit \(g(},})+[]\) within the function space \(\) are identified by minimizing the following objective:

\[_{f}_{t=1}^{T}(y_{t}- f (},}_{t})dM(}| }_{t},_{t})) \]

where \(\) represents the loss metric we used to evaluate model performance and \(t\) is the index of specific data instance. We provide the training methods for supervised neural network models which are extensively employed in the real world in Section 4.3. Specifically, we discuss their loss functions and gradient computations when the objectives are regression and classification problems. The training steps are inspired by the two-stage training in causal effect estimation  and counterfactual prediction . The re-estimated unconfounded values are sampled from the first-stage trained model, the sampling has little influence on the implementation and computation complexity of the second-stage model training. Therefore, the two-stage training process does not limit the method's practical usability. Due to the limited space, we provide details of model training with discrete input features and non-gradient model training in appendix Section D.

### Confounder-free Model Training

**Model Training for Continuous Feature Attribution.** In the regression task, where the model \(f\) is a neural network trained for forecasting continuous value, it is also denoted as \(f_{}(},})\), where \(\) represents the model parameters. As our objective, we adopt a \(l_{2}\) loss function. With the unknown conditional distribution of \(\) given \(X\) and \(\), we initially utilize a neural network model, denoted \(M_{}\), where \(\) is the model parameters, to approximate this distribution. The \(l_{2}\) loss function for determining the optimal model parameters \(\) subsequently approximates as per the following equation

\[(T;)=|T|^{-1}_{t}(y_{t}- f_{} (},}_{t})d_{}(}}_{t},_{t}))^{2}, \]where the integral term estimates the expected output of \(f_{}\) under the distribution approximated by \(_{}\). By employing the relevant calculations, we ascertain that the gradient of the loss function with respect to the \(t^{th}\) training data point is

\[_{}_{t}= -2_{_{}(}|}_{t}, _{t})}[y_{t}-f_{}(},}_{t} )]_{_{}(}|} _{t},_{t})}.[f^{}_{}(},} _{t})]. \]

In short, our training process comprises two fundamental steps: 1) an instrumental variable method is applied to re-estimate \(}\), mitigating the impact of unobservable confounders, and 2) this refined \(}\) is utilized to calculate the gradients for \(f\).

**Model Training for Discrete Feature Attribution.** In the classification task, where \(y\) is a discrete variable representing classes, we adapt the loss function to the multi-class cross-entropy

\[(T;)=|T|^{-1}_{t}_{r=1}^{R}( y_{t,r}  f_{,r}(},}_{t})d_{ }(}}_{t},_{t})). \]

In this formulation, \(y_{t,r}\) represents the true label of the \(t^{th}\) data point in the \(r^{}\) category. \(R\) denotes the total number of distinct classes into which the target variable y can be classified. The probability of the model classifying a data point into the \(r^{}\) category is given by \(f_{,r}(},}_{t})\). The gradient calculation for the \(t^{th}\) training data point, considering this loss function, is then

\[_{}_{t}= _{_{}(}|}_{t},z _{t})}[_{r=1}^{R}}{f_{,r}(}, }_{t})} f^{}_{,r}(}, }_{t})]. \]

This adaptation of the loss function for discrete target variables ensures that our model can handle classification tasks, effectively optimizing its performance across multiple categories.

**Feature Attribution Computation.** The exact computation of Shapley value and integrated gradients needs huge cost while approximation methods are widely used. Towards practical applications, we further propose a Shapley value approximation method and an integrated gradients approximation method for saving computation costs in Sections E.1 and E.2, respectively. The correlation of input features may affect the data-faithfulness of IV-SHAP and IV-IG. We can combine methods which deal with the correlated input features to the two-stage model to better capture these correlations. For example, on-manifold Shapley  can be used to account for feature correlations, while causal Shapley  can be applied if the causal structure of the input features is known.

## 5 Experiments

In this section, we present our empirical evaluation in detail. We employ synthetic and real-world datasets to evaluate the faithfulness and robustness against unobservable confounders of feature attributions given by our proposed data-faithful feature attribution methods to prevalent SHAP-based and IG methods. Comparisons of feature attribution for classification problems, feature attribution on non-gradient training models, and the approximation methods of SHAP and IG are given in appendix Sections F.3, F.4, and F.5, respectively. Our code can be found in the repository at [https://github.com/ZJU-DIVER/IV-SHAP](https://github.com/ZJU-DIVER/IV-SHAP).

### Experiments on Synthetic Datasets

We first conducted a data simulation experiment to validate our proposed methods' effectiveness. Synthetic datasets offer an advantage in studying feature attribution, as we can obtain the ground truth of attribution values according to the data generation equation which is unobtainable in most real datasets.

**Dataset Generation Process.** We generated two synthetic datasets, each containing four parts: an unobserved confounder \(\), a variable \(}\) influenced by the unobserved confounder, collaborative variables \(}=\{_{i}\}\)\((1 i 6)\), and the target feature \(y\). Notably, dataset A and dataset B share the same \(}\) and \(\). \(\) is formulated by a uniform variable \(v\) and a parameter \(\) which controls the noise level. The generation of these features adhered to specific functional relationships, as illustrated in the equations below.

\[v(0,1),_{i}(0,1) \ (1 i 6),(0,1),\] \[^{A}=v,\\ }^{A}=(}+^{A}+^{2} )/3,\\ y^{A}=}^{A}+_{1}^{2}+_{2}+_{3}}+_{4}^{2}+_{5}+_{6}}}{2}}{6}+^{A},\\ ^{B}=,\\ }^{B}=(}+^{B}+^{2} )/3,\\ y^{B}=}^{B}_{1})+_{2}+ {_{3}}+_{4})+_{5}+_{6}}}{2}}{6}+^{B}.\]

**Compared Methods.** We utilized two representative feature attribution algorithms, SHAP  and IG , as the baseline methods. Specifically, we trained a neural network model on synthetic datasets to fit \([y|]\) as the baseline model. Then we applied the two feature attribution methods to attribute contributions for input features. For our proposed approach, we employed the same neural network architecture but trained the model to fit \([|]\) with instrumental variables. Our attribution approaches, applied to the model trained with the instrumental variable, are referred to as IV-SHAP and IV-IG, corresponding to SHAP and IG, respectively. It is worth noting that for the model, the explicitly input data features have no causal relationships among them. Therefore, Causal SHAP , Asymmetric SHAP , BSHAP  and SHAP are equivalent in this context.

**Experimental Results.** We randomly generated 1000 data points based on the data generation equations. We then adjusted features \(}\) and \(}\) of each data point by subtracting a certain value as a baseline input. We conducted experiments with varied subtracted values set at 0.125, 0.25, 0.375, and 0.5. For each data point, we applied IV-SHAP, IV-IG, SHAP, and IG to attribute the contributions of the features. Subsequently, we compared the attribution values of feature \(}\) against the ground truth obtained directly from the data generation equations. We observe that the errors in attribution results provided by IV-SHAP and IV-IG are significantly smaller than those of SHAP and IG. The absolute errors in the attribution values of each algorithm for every data point, as compared to the benchmark, are illustrated in Figures 2 and 3.

Figure 3: Evaluation results on synthetic Dataset B.

Figure 2: Evaluation results on synthetic Dataset A.

[MISSING_PAGE_FAIL:9]

**Empirical Analysis.** The second real dataset we use is the Angrist and Krueger dataset , which is an American census dataset consisting of statistical data on people born in specific years, including variables such as age (AGE), an education level (EDUC), weekly wage (LWKLYWGE), marital status (MARRIED), and race (RACE). We employed this dataset to examine the confounder effects of the ability on the attribution of education for income. In this dataset, we used the quarter of birth as an instrumental variable for years of education. The rationale behind this is the compulsory education laws in various states, which typically mandate schooling until the age of 16. Students born early in the year often start school later, leading to systematic differences in educational duration based on birth quarter. However, this dataset lacks measures of intelligence or work capability to assess the factor of ability, so we cannot conduct the experiment like the previous one. Nevertheless, the average attribution ratio of one additional year, calculated by SHAP, IV-SHAP, IG, and IV-IG, are \(0.0218\), \(0.0206\), \(0.0218\), and \(0.0207\), respectively. This aligns with the observation that people may overestimate educational returns because of neglecting the confounder ability in .

## 6 Limitations

Despite the strengths of our approach, there are several limitations to consider which are shown as follows:

* **Dependence on the Availability of Instrumental Variables**: Our approach assumes the presence of suitable instrumental variables for features affected by unobserved confounders. However, in practical scenarios, finding appropriate instrumental variables can be challenging sometimes. For further information on identifying instrumental variables, refer to works such as , , and .
* **Linearity Assumption in Theoretical Derivations**: Our theoretical derivations are based on the assumption that the influence of unobserved confounders on the target features is linear. This assumption does not hold in all real-world situations. Nevertheless, in our experiments with real datasets in Section 5.2, our attribution method showed significant improvements over existing methods, even when the influence of unobservable confounders on features was non-linear.

These limitations highlight areas for future research, particularly in developing methods that do not rely on the availability of instrumental variables and that can give theoretical analysis of non-linear effects of unobserved confounders. Addressing these aspects can enhance the practicality and applicability of our methods.

## 7 Conclusion

In this paper, we focus on addressing the effects of unobservable confounders in feature attribution, emphasizing feature attribution being faithful to data. The proposed method improves the understanding of the causal factors driving an outcome variable, going beyond standard attribution scores that simply describe predictive models. Our approach of training confounder-free models using instrumental variables effectively isolates the impact of confounders, enhancing the robustness of data-faithful feature attribution results. Our validations using real and synthetic datasets confirm the effectiveness of the proposed methods. For future work, we intend to develop methods that do not rely on the availability of instrumental variables and that can provide a theoretical analysis of the non-linear effects of unobserved confounders. For the broader impacts of the paper, please see Section A in the appendix due to the limited space.