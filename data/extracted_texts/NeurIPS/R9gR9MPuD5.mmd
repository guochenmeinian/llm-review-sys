# InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques

Rohan Gupta

cybershiptrooper@gmail.com

&Ivan Arcuschin

University of Buenos Aires

iarcuschin@dc.uba.ar

Thomas Kwa

kwathomas0@gmail.com

&Adria Garriga-Alonso

FAR AI

adria@far.ai

###### Abstract

Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.

## 1 Introduction

The field of mechanistic interpretability (MI) aims to reverse-engineer the algorithm implemented by a neural network . The current MI paradigm holds that the neural network (NN) represents concepts as _features_, which may have their dedicated subspace [8; 31] or be in _superposition_ with other features [15; 16; 32]. The NN arrives at its output by composing many _circuits_, which are subcomponents that implement particular functions on the features [9; 20; 32]. To date, the field has been very successful at reverse-engineering toy models on simple tasks [7; 10; 11; 30; 47]. For larger models, researchers have discovered circuits that perform clearly defined subtasks [22; 23; 27; 43].

How confident can we be that the NNs implement the claimed circuits? The central piece of evidence for many circuit papers is _causal consistency_: if we intervene on the network's internal activations, does the circuit correctly predict changes in the output? There are several competing formalizations of consistency [10; 20; 43; 25] and many ways to ablate NNs, each yielding different results [12; 35; 46]. This problem is especially due for _automatic_ circuit discovery methods, which search for subgraphs with the highest consistency [21; 45] or faithfulness [12; 39] measurements1.

These results would be on much firmer ground if we had an agreed-upon protocol for thoroughly checking a hypothesized circuit. To declare a candidate protocol _valid_, we need to check whether, in practice, it correctly distinguishes _true_ circuits from false circuits. Unfortunately, we do not know thetrue circuits of the models we are interested in, so we cannot validate any protocol. Previous work has sidestepped this in two ways. One method is to rely on qualitative evidence [10; 33], perhaps provided by human-curated circuits [12; 39], which is expensive and possibly unreliable.

The second way to obtain neural networks with known circuits is to construct them. Tracr  is a tool for compiling RASP programs  into standard decoder-only transformers. By construction, it outputs a model that implements the specified algorithm, making it suitable for evaluating MI methods. Unfortunately, Tracr-generated transformers are quite different from those trained using gradient descent: most of their weights and activations are zero, none of their features are in superposition, and they use only a small portion of their activations for the task at hand. Figure 2 shows how different the weights of a Tracr-generated transformer are from those of a transformer trained with gradient descent. This poses a very concrete threat to the validity of any evaluation that uses Tracr-generated transformers as subjects: we cannot tune the inductive biases of circuit evaluation algorithms with such unrealistic neural networks.

### Contributions

In this work, we present InterpBench, a collection of \(86\) semi-synthetic yet realistic transformers with _known circuits_ for evaluating mechanistic interpretability techniques. We collected \(85\) Tracr circuits plus 1 circuit from the literature (Indirect Object Identification ), and trained new transformers to implement these circuits using Strict Interchange Intervention Training (SIIT).

SIIT is an extension of Interchange Intervention Training (IIT) . Under IIT, we predefine which subcomponents of a _low-level_ computational graph (the transformer to train) map to nodes of a _high-level_ graph (the circuit). During training, we apply the same interchange interventions [10; 18] to both the low- and high-level models, and incentivize them to behave similarly with the loss.

Our extension, SIIT, improves upon IIT by also intervening on subcomponents of the low-level model that are not mapped to any high-level node. This prevents the low-level model from using them to compute the output, ensuring the high-level model correctly represents the circuit the NN implements.

We make InterpBench models and the SIIT code used to train them all publicly available.2 In summary, the contributions of this article are:

* We present InterpBench, a benchmark of \(86\) realistic semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques.
* We introduce Strict Interchange Intervention Training (SIIT), an extension of IIT which also trains nodes not in the high-level graph. Using systematic ablations, we validate that SIIT correctly generates transformers with known circuits, even when IIT does not.

Figure 1: SIIT transformers implement a known ground-truth circuit, but their weights and activations are similar to the ones in naturally trained transformers, letting us measure, in a realistic setting, how accurate circuit discovery methods are at finding the true circuit.

* We show that SIIT-generated transformers are realistic enough to evaluate MI techniques, by checking whether circuit discovery methods behave similarly on SIIT-generated and natural transformers.
* We demonstrate the benchmark's usefulness by evaluating five circuit discovery techniques: Automatic Circuit DisCovery (ACDC, 12), Subnetwork Probing (SP, 35) on nodes and edges, Edge Attribution Patching (EAP, 39), and EAP with integrated gradients (EAP-ig, 29). On InterpBench, the results conclusively favor ACDC over Node SP, showing that there is enough statistical evidence (_p-value_\( 0.0004\)) to tell them apart, whereas the picture in Conmy et al.  was much less clear. Interestingly, the results also show that EAP with integrated gradients is a strong contender against ACDC. In contrast, regular EAP performs poorly, which is understandable given the issues that have been raised about it .

This article's evaluation was performed on \(16\) Tracr circuits generated by us (Section 4). Since then, InterpBench has been expanded with \(69\) new models: \(10\) trained on more Tracr circuits generated by us and \(59\) trained on TracrBench circuits  (Appendix H).

## 2 Related work

Linearly compressed Tracr models.Lindner et al.  compress the residual stream of their Tracr-generated transformers using a linear autoencoder, to make them more realistic. However, this approach does not change the model's structure, and components that are completely zero remain in the final model.

Features in MI.While this work focuses on circuits, the current MI paradigm also studies _features_: hypothesized natural variables that the NN algorithm operates on. The most popular hypothesis is that features are most of the time inactive, and many features are in _superposition_ in a smaller linear subspace [15; 36]. This inspired sparse autoencoders (SAEs) as the most popular feature extraction method [5; 6; 13; 34; 40]. SAEs produce many human-interpretable features that are mostly able to reconstruct the residual stream, but this does not imply that they are natural features for the NN. Indeed, some features seem to be circular and do not fit in the superposition paradigm . Nevertheless, circuits on SAE features can be faithful and causally relevant .

A benchmark that pairs NNs with their known circuits is also a good way to test feature discovery algorithms (like SAEs): the algorithms should naturally recover the values of computational nodes of the true circuit. Conversely, examining how SIIT-trained models represent their circuits' concepts could help us understand how natural NNs represent features. This article omits the comparison because its models only perform one task, and thus have too few features to show superposition.

Other MI benchmarks.Ravel is a dataset of prompts containing named entities with different attributes that can be independently varied. Its purpose is to evaluate methods which can causally isolate the representations of these attributes in the NN. Orion is a collection of retrieval tasks to investigate how large language models (LLMs) follow instructions. CasualGym is a benchmark of linguistic tasks for evaluating interpretability methods on their ability to find

Figure 2: A histogram of the weights for the MLP output matrix in Layer 0 of a Tracr, SIIT, and “natural” transformer, i.e. trained by gradient descent to do supervised learning. All these transformers implement the frac_prevs task . The weight distribution of an SIIT-trained transformer is much closer to the natural than the Tracr transformer. Yet, we know the ground-truth algorithm that the SIIT transformer implements. We provide the KL divergence between these histograms in Table 5.

specific linear features in LLMs. Find is a dataset and evaluation protocol for tools which automatically describe model neurons or other components . The test subject must accurately describe a function, based on interactively querying input-output pairs from it.

We see InterpBench as complementary to Orion, Ravel, and CausalGym, and slightly overlapping with Find. InterpBench is very general in scope: its purpose is to evaluate _any_ interpretability methods which discover or evaluate circuits or features. However, InterpBench is not suitable for evaluating natural language descriptions of functions like Find is, and its NNs are about as simple as Find functions.

## 3 Strict Interchange Intervention Training

An interchange intervention , or resample ablation , returns the output of the model on a _base_ input when some of its internal activations have been replaced with activations that correspond to a _source_ input. Formally, an interchange intervention \((,,,V)\) takes a model \(\), an input _base_, an input _source_, and a variable \(V\) (i.e., a node in the computational graph of the model), and returns the output of the model \(\) for the input _base_, except that the activations of \(V\) are set to the value they would have if the input were _source_. This same definition can be extended to intervene on a set of variables \(\), where the activations of all variables in \(\) are replaced. Geiger et al.  define Interchange Intervention loss as:

\[_{,}(^{H},,,V^{H}),(^{L}, ,,(V^{H}))\] (1)

where \(^{H}\) is the high-level model, \(^{L}\) is the low-level model, \(V^{H}\) is a high-level variable, \((V^{H})\) is the set of low-level variables that are aligned with (mapped to) \(V^{H}\), and Loss is some loss function, such as cross-entropy or mean squared error. We use the notation \(()\) to denote the output of the model \(\) when run without interventions on input _base_.

The main shortcoming of the above definition is that, by sampling only high-level variables \(V^{H}\) and intervening on the low-level variables that are aligned with it (i.e., \((V^{H})\)), IIT never intervenes on low-level nodes that are not aligned with any node in the high-level model. This can lead to scenarios in which the nodes that are not intervened during training end up performing non-trivial computations that affect the low-level model's output, even when the nodes that are aligned with the high-level model are correctly implemented and causally consistent.

As an example, suppose that we have a high-level model \(^{}\) such that \(^{}(x)=3x+2\), and we want to train a low-level model \(^{}\) that has three nodes, only one of which is part of the circuit. If we train this low-level model using IIT, we may end up with a scenario like the one depicted in Figure 3. In this example, even though the low-level model has perfect accuracy and the aligned

Figure 4: Circuit for Indirect Object Identification task in InterpBench. This circuit is a simplified version of the one manually discovered by Wang et al. . The _Duplicate token head_ outputs the first position of duplicated tokens, if there is any; otherwise it outputs \(-1\). The _S-Inhibition head_ copies the token from the previous position and outputs it to the _Name mover head_, which increases the logits of all names except the ones that are inhibited.

Figure 3: Example of a low-level model that has a perfect accuracy, with aligned low-level nodes (in yellow) that are causally consistent with the high-level model, but has non-aligned nodes (in grey) that affect the output.

nodes are causally consistent, the non-aligned nodes still affect the output in a non-trivial way. This shows some of the issues that arise when using IIT: aligned low-level nodes may not completely contain the expected high-level computation, and non-aligned low-level nodes may contain part of the high-level computation.

To correct this shortcoming, we propose an extension to IIT called _Strict Interchange Intervention Training_ (SIIT). Its pseudocode is shown in Algorithm 1 (Appendix A). The main difference between IIT and SIIT is that, in SIIT, we also sample low-level variables that are not aligned with any high-level variable. This allows us to penalize the low-level model for modifying the output when intervening on these non-aligned variables. We implement this modification as a new loss function (_Strictness loss_) that is included in the training loop of SIIT. Formally:

\[_{}y_{b},( ^{L},,V^{L})\] (2)

where \(y_{b}\) is the correct output for input \(b\) and \(V^{L}\) is a low-level variable that is not aligned with any high-level variable \(V^{H}\). In other words, this loss incentivizes the low-level model to avoid performing non-trivial computations for this task on low-level components that are not aligned with any high-level variable. This makes the non-aligned components constant for the inputs in the task distribution, but not necessarily for the ones outside of it. Notice however that under the _Srictness loss_ the non-aligned components can still contribute to the output in a constant way, as long as they do not change the output when intervened on. The extent of this effect is analyzed in Appendix B.

As proposed by Geiger et al. , we also include in Algorithm 1 a behavior loss that ensures the model is not overfitting to the IIT and _Strictness_ losses. The behavior loss is calculated by running the low-level model without any intervention and comparing the output to the correct output.

## 4 InterpBench

InterpBench is composed of \(85\) semi-synthetic transformers generated by applying SIIT to Tracr-generated transformers and their corresponding circuits, plus a semi-synthetic transformer trained on GPT-2 and a simplified version of its IOI circuit . This benchmark can be freely accessed and downloaded from HuggingFace (see Appendix E). We generated \(26\) RASP programs using few-shot prompts on GPT-4, and collected \(59\) RASP programs from TracrBench .

The architecture for the SIIT-generated transformers was made more realistic (compared to the original Tracr ones) by increasing the number of attention heads up to 4 (usually only 1 or 2 in Tracr-generated transformers), which lets us define some heads as not part of the circuit, and by halving the internal dimension of attention heads. The residual stream size on the new transformers is calculated as \(d_{} n_{}\), and the MLP size is calculated as \(d_{} 4\).

Using IIT's terminology, the Tracr-generated transformers are the high-level models, the SIIT-generated transformers are the low-level ones, and the variables are attention heads and MLPs (i.e., nodes in the computational graph). Each layer in the high-level model is mapped to the same layer in the low-level model. High-level attention heads are mapped to randomly selected low-level attention heads in the same layer. High-level MLPs are mapped to low-level MLPs in the same layer.

We train InterpBench's main \(16\) SIIT models by using Algorithm 1 as described in Section 3, fixing the Weight\({}_{SIIT}\) to values between \(0.4\) and \(10\), depending on the task. Both the Weight\({}_{IIT}\) and Weight\({}_{behavior}\) are set to 1. We use Adam as the optimizer for all models, with a fixed learning rate of \(0.001\), batch size of \(512\), and Beta coefficients of \((0.9,0.999)\). All models are trained until they reach \(100\%\) Interchange Intervention Accuracy (IIA) and \(100\%\)_Strict_ Interchange Intervention Accuracy (SIIA) on the validation dataset. IIA, as defined by Geiger et al. , measures the percentage of times that the low-level model has the same output as the high-level model when both are intervened on the same aligned variables. The _Strict_ version of this metric measures the percentage of times that the low-level model's output remains unchanged when intervened on non-aligned variables.

The training dataset is composed of 20k-120k randomly sampled inputs, depending on each task. The validation dataset is randomly sampled to achieve 20% of the training dataset size. The expected output is generated by running the Tracr-generated transformer on each input sequence. The specific loss function to compare the outputs depends on the task: cross-entropy for Tracr categorical tasks, and mean squared error for Tracr regression tasks.

To show that SIIT can also train transformers with non-RASP circuits coded manually, InterpBench includes a model trained on a simplified version of the IOI task and the circuit hypothesized by Wang et al. , shown in Figure 4. We train a semi-synthetic transformer with \(6\) layers and \(4\) heads per layer, \(d_{}=64\), and \(d_{}=16\). Each high-level node in the simplified IOI circuit is mapped to an entire layer in the low-level model. We train this transformer using the same algorithm and hyperparameters as for the Tracr-generated transformers, but with a different loss function. We apply the IIT and SIIT losses to the last token of the output sequence, and the cross-entropy loss to all other tokens. The final loss is a weighted average of these losses, with the IIT and SIIT losses upweighted by a factor of \(10\). The hyperparameters remained the same during the experiments.

The semi-synthetic transformers included in InterpBench were trained on a single NVIDIA RTX A6000 GPU. The training time varied depending on the task and the complexity of the circuit but was usually around 1 to 8 hours.

Appendix E explains how to download InterpBench and the license under which it is released. Appendix G contains a detailed description of the Tracr tasks included in the benchmark, and Appendix F provides instructions on how to use it. Appendix H provides the training details and task description for the models that were not included in this article's evaluation. Further documentation of each task (e.g., training hyperparameters) can be found in the structured metadata file on the HuggingFace repository3, and their source code is publicly available on the GitHub repository4.

## 5 Evaluation

To investigate the effectiveness of SIIT and the usefulness of the proposed benchmark, we conducted a evaluation on the 16 main models and IOI to answer the following research questions (RQs):

_RQ1 (IIT): Do the transformers trained using IIT correctly implement the desired circuits?_

_RQ2 (SIIT): Do the transformers trained using SIIT correctly implement the desired circuits?_

_RQ3 (Realism): Are the transformers trained using SIIT realistic?_

_RQ4 (Benchmark): Are the transformers trained using SIIT useful for benchmarking mechanistic interpretability techniques?_

### Results

RQ1 & RQ2.In this evaluation, we compare the semi-synthetic transformers trained using IIT and SIIT. Unless specified, the SIIT models are the \(16\) main ones from InterpBench (Section 4). We use the same setup for IIT models, except that we set the Weight\({}_{SIIT}\) to \(0\).

Figure 5: Average effect on accuracy for nodes in the circuit (green) and out of the circuit (red) for the models of \(7\) randomly sampled tasks in the benchmark. Boxplots display, for each task and model, the average proportion of model outputs that change when intervening on nodes. For all regression tasks, we deem an intervention to have an effect when the new scalar output differs by \(0.05\) or more from the original. We can see that for Tracr and SIIT models, nodes not in the circuit have much lower effects, but that is not the case for IIT models.

To understand if a trained low-level model correctly implements a circuit we need to check that (1) the low-level model has the same output as the high-level model when intervening on aligned variables, and that (2) the non-circuit nodes do not affect the output. As we mentioned in Section 4, all low-level models in our experiments are trained to achieve 100% IIA on the validation sets, which ensures that the first condition is always met.

We answer the second condition by measuring the _node effect_ and _normalised KL divergence_ after intervening on each node in the model. Node effect measures the percentage of times that the low-level model changes its output when intervened on a specific node. As mentioned before, a node that is not part of the circuit should not affect the output of the model and thus should have a low node effect. Formally, for a node \(V\) in a model \(\), and a pair of inputs \((x_{b},x_{s})\) with corresponding labels (\(y_{b}\), \(y_{s}\)), we define the node effect as follows:

\[_{V}(x_{b},x_{s},y_{b})=[(,x_{b},x_{s},V) y_{b}],\]

where \([]\) is the indicator function. The normalized KL divergence is:

\[d_{V}(x_{b},x_{s},y_{b})= ((,x_{b},x_{s},V),y_{b})-d_{ KL}((x_{b}),y_{b})}{d_{KL}((x_{s}),y_{b})-d_{KL}( (x_{b}),y_{b})}.\]

Figure 8: Correlation coefficients between the accuracy achieved by the SIIT and “natural” models, and the Tracr and “natural” models, for \(11\) randomly selected cases, after mean ablating the nodes rejected by \(\)CDC over different thresholds (see Appendix B). These coefficients are consistently higher when comparing the SIIT and “natural” models than when comparing the Tracr and “natural” models.

Figure 6: Normalized effect on KL divergence for nodes in the circuit (green) and out of the circuit (red) for the models of \(5\) randomly sampled categorical tasks in the benchmark. Boxplots display, for each task and model, the differences in KL divergence before and after intervening on each node. We can see that in Tracr and SIIT nodes are very well separated into in/out of the circuit by their effect size, whereas that is not the case for IIT models.

Figure 7: Scatter plot comparing the effect for nodes in the circuit (green) and not in the circuit (red) for IIT and SIIT transformers on the \(16\) main tasks. The \(x\) and \(y\) axes display the average node effect when resample ablating on IIT and SIIT models, respectively. For each task, both models have a one-one node correspondence. Some IIT nodes that are not in the circuit have much higher effects than they should have.

If a semi-synthetic transformer correctly implements a Tracr's circuit, the effect of all aligned nodes will be similar to their corresponding counterparts in the Tracr model. For the KL divergence, it is not always possible to have a perfect match with the Tracr-generated transformer, as Tracr does not minimize the cross-entropy loss in categorical programs but only fixes the weights so that they output the expected labels. Still, we expect a clear separation between nodes in and out of the circuit.

Figure 5 shows the node effect for nodes in and out of the circuit for \(7\) randomly sampled tasks in the benchmark, averaged over a test dataset. Each boxplot shows the analysis for a Tracr, IIT, or SIIT transformer on a different task. We can see that the boxplots for IIT and Tracr are different, with the IIT ones consistently having a high node effect for nodes that are not in the circuit (red boxplots). On the other hand, the SIIT boxplots are more similar to the Tracr ones, with a low node effect for nodes that are not in the circuit, and a high node effect for nodes that are in the circuit.

Similarly, Figure 6 shows the average normalized KL divergence for nodes in and out of the circuit for \(5\) randomly sampled categorical tasks in the benchmark. Again, most of the boxplots for IIT have high KL divergence for nodes that are not in the circuit, while the SIIT boxplots have low values for these nodes. We can see that even though the SIIT transformer does not exactly match the Tracr behavior, there is still a clear separation between nodes in the circuit and those not in the circuit, which does not happen for the IIT transformers. It is worth pointing out that the higher error bar across cases for KL divergence is due to the fact that we are optimizing over accuracy instead of matching the expected distribution over labels.

Finally, Figure 7 shows a scatter plot comparing the average node effect for nodes in and out of the circuit for IIT and SIIT transformers for the \(16\) main tasks in the benchmark. We can see that there are several nodes not in the circuit that have a higher node effect for IIT than for SIIT.

Appendix B extends Figure 5 to the main \(16\) tasks in InterpBench, for SIIT and the original circuit only. It also repeats the experiments but with mean and zero ablations . Using another type of ablation is a robustness check for InterpBench, which was trained with interchange interventions. Under mean ablations, only nodes in the circuit have an effect, but that is not the case under zero ablations. This may indicate that InterpBench circuits are not entirely true, but also matches the widely held notion that zero ablation is unreliable .

Rq3.To analyze the realism of the trained models, we run ACDC  on Tracr, SIIT, and "naturally" trained transformers (i.e., using supervised learning). We measure the accuracy of these models after mean-ablating  all the nodes rejected by ACDC, i.e. the ones that ACDC deems to not

Figure 9: (a) AUROCs of circuit discovery techniques on InterpBench’s \(16\) main models. ACDC’s AUROC is obtained by varying the threshold. SP and edgewise SP’s AUROC is obtained by varying the regularization coefficient (\(3000\) epochs). EAP with integrated gradients uses \(10\) samples. (b) Difference in Edge AUC ROC for all circuit discovery techniques against ACDC.

be in the circuit. This lets us check whether SIIT and "natural" models behave similarly from the point of view of circuit discovery techniques. A more realistic model should have a score similar to the transformers trained with supervised learning. Figure 8 displays the difference in correlation coefficients when comparing the accuracy of the SIIT and Tracr models to the "natural" models, showing that SIIT models have a higher correlation with "natural" models than Tracr ones. Figure 18 (Appendix D) suggests that circuits in SIIT models are harder to find than those in Tracr models.

Another proxy for realism is: do the weights of "natural" and SIIT models follow similar distributions? Figure 2 shows a histogram of the weights for the MLP output matrix in Layer 0 of a Tracr, SIIT, and "natural" transformer. The SIIT and "natural" weight distributions are very similar.

**RQ 3**: SIIT-generated transformers are more realistic than Tracr ones, with behavior similar to the transformers trained using supervised learning.

Rq4.To showcase the usefulness of the benchmark, we run ACDC , Subnetwork Probing (SP) , edgewise SP, Edge Attribution Patching (EAP) , and EAP with integrated gradients  on the SIIT transformers and compare their performance. Edgewise SP is similar to regular SP, but instead of applying masks over all available nodes, they are applied over all available edges. We compute the Area Under the Curve (AUC) for the edge-level ROC as a measure of their performance.

Figure 8(a) displays boxplots of the AUC ROCs, and Figure 8(b) shows the difference in AUC ROC for all circuit discovery techniques against ACDC. For measuring statistical significance, we rely on the well-established Wilcoxon-Mann-Whitney U-test and Vargha-Delaney \(A_{12}\) effect size . From these tests, we get that ACDC is statistically different (_p-value_\(<0.05\)) to all the other algorithms except EAP with integrated gradients, with an effect size \(A_{12}\) ranging from \(0.54\) to \(0.91\).

Interestingly, previous evaluations of performance between SP and ACDC on a small number of tasks, including Tracr ones, did not show a significant difference between the two - SP was about as good as ACDC, achieving very similar ROC AUC across tasks when evaluated on manually discovered circuits . On the other hand, results on InterpBench clearly show that ACDC outperforms SP on small models that perform algorithmic tasks (_p-value_\( 0.0004\) and large effect size \(_{12} 0.742\)).

One difference between ACDC and other techniques is that this method uses causal interventions to find out which edges are part of the circuit, while SP and EAP rely on the gradients of the model. After manual inspection, we found that the gradients of the SIIT models were very small, possibly due to these models being trained up to 100% IIA and 100% SIIA, which could explain why SP and regular EAP are not as effective as ACDC. This, however, does not seem to negatively affect EAP with integrated gradients, since the results show that this method is not statistically different from ACDC (_p-value_\( 0.05\)), which means that it is as good as ACDC for the tasks in the benchmark.

There are some cases where ACDC is not the best technique (Figure 8(b)). Notably, in Case 33, ACDC is outperformed by all the other techniques except EAP. We leave investigating why to future work.

Finally, there is not enough statistical evidence to say EAP with integrated gradients is different than edgewise SP (_p-value_\( 0.05\)), which means that the latter is a close third to ACDC and EAP with integrated gradients. Appendix D contains further details on the statistical tests and the evaluation of the circuit discovery techniques.

**RQ 4**: InterpBench can be used to evaluate mechanistic interpretability techniques, and has yielded unexpected results: ACDC is significantly better than SP and egewise SP, but statistically indistinguishable from EAP with integrated gradients.

## 6 Conclusion

In this work, we presented InterpBench, a collection of \(86\) semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques. We introduced Strict Interchange Intervention Training (SIIT), an extension of IIT, and checked whether it correctly generates transformers with known circuits. This evaluation showed that SIIT is able to generate semi-synthetic transformersthat correctly implement Tracr-generated circuits, whereas IIT fails to do so. Further, we measured the realism of the SIIT transformers and found that they are comparable to "natural" ones trained with supervised learning. Finally, we showed that the benchmark can be used to evaluate existing mechanistic interpretability techniques, showing that ACDC  is substantially better at identifying true circuits than node- and edge-based Subnetwork Probing , but statistically indistinguishable from Edge Attribution Patching with integrated gradients .

It is worth mentioning that previous evaluations of MI techniques  relied mostly on manually found circuits such as IOI  for which there is no ground truth. In other words, these circuits are not completely faithful, and thus they are not guaranteed to be the real circuits implemented. In contrast, InterpBench provides models with ground truth, which allows us to compare the results of different MI techniques in a more controlled way.

Limitations.InterpBench has proven useful for evaluating circuit discovery methods, but its models, while realistic for their size, are very small and have very little functionality - only one algorithmic circuit per model, as opposed to the many subtasks in next-token prediction. Therefore, results on InterpBench may not accurately represent the results of the larger models that the MI community is interested in. As an example, we have not evaluated sparse autoencoders, as the small true number of features and size of the SIIT models would make it difficult to extract meaningful conclusions. Still, InterpBench serves as a worst-case analysis for MI techniques: if they can not retrieve accurate circuits here, they will not give faithful results in SOTA language models.

Future work.There are many ways to improve on this benchmark. One is to train SIIT transformers at higher granularities, like subspaces instead of heads, which would allow us to evaluate circuit and feature discovery techniques such as DAS  and Sparse Autoencoders . One could also make the benchmark models more realistic by making each model implement many circuits. This would also let us greatly increase the number of models without manually implementing more tasks.

Societal impacts.If successful, this line of work will accelerate progress in mechanistic interpretability, by putting its results in firmer ground. Better MI makes AIs more predictable and controllable, which makes it easier to use (and misuse) AI. However, it also introduces the possibility of eliminating _unintended_ biases and bugs in NNs, so we believe the impact is overall good.

## Author contributions

RG implemented the SIIT algorithm, performed the experiments for the evaluation, and set up the IOI task. IA performed the statistical tests, set up the Tracr tasks, and wrote the initial draft of the manuscript. Both RG and IA helped setting up the circuit discovery techniques. TK provided the initial implementation of IIT. AGA proposed the initial idea for the project, provided feedback and advice throughout the project, and did the final editing of the manuscript.