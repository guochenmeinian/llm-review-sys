# Convergence of Mean-field Langevin dynamics:

Time-space discretization, stochastic gradient,

and variance reduction

 Taiji Suzuki\({}^{1,2}\),  Denny Wu\({}^{3,4}\),  Atsushi Nitanda\({}^{2,5}\)

\({}^{1}\)University of Tokyo, \({}^{2}\)RIKEN AIP, \({}^{3}\)New York University,

\({}^{4}\)Flatiron Institute, \({}^{5}\)Kyushu Institute of Technology

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu, nitanda@ai.kyutech.ac.jp

###### Abstract

The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient. To demonstrate the wide applicability of our framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution for \((i)\) a wide range of learning problems such as mean-field neural network and MMD minimization, and \((ii)\) different gradient estimators including SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the SGD and SVRG settings when specialized to the standard Langevin dynamics.

## 1 Introduction

In this work we consider the mean-field Langevin dynamics (MFLD) given by the following McKean-Vlasov stochastic differential equation:

\[X_{t}=-)}{}(X_{t})t+ W_{t},\] (1)

where \(_{t}=(X_{t})\), \(F:_{2}(^{d})\) is a convex functional, \(W_{t}\) is the \(d\)-dimensional standard Brownian motion, and \(\) denotes the first-variation of \(F\). Importantly, MFLD is the Wasserstein gradient flow that minimizes an entropy-regularized convex functional as follows:

\[_{_{2}}\{F()+()\}.\] (2)

While the above objective can also be solved using other methods such as double-loop algorithms based on iterative linearization (Nitanda et al., 2020, 2023; Oko et al., 2022), MFLD remains attractive due to its simple structure and connection to neural network optimization. Specifically, the learning of two-layer neural networks can be lifted into an infinite-dimensional optimization problem in the space of measures (i.e., the _mean-field limit_), for which the convexity of loss function can be exploited to show the global convergence of gradient-based optimization (Nitanda and Suzuki, 2017; Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020). Under this viewpoint, MFLD Eq. (1) corresponds to the continuous-time limit of the _noisy_ gradient descent update on an infinite-width neural network, where the injected Gaussian noise encourages "exploration" and facilities global convergence (Mei et al., 2018; Hu et al., 2019).

Quantitative analysis of MFLD.Most existing convergence results of neural networks in the mean-field regime are _qualitative_ in nature, that is, they do not characterize the rate of convergence nor the discretization error. A noticeable exception is the recent analysis of MFLD by Nitanda et al. (2022); Chizat (2022), where the authors proved exponential convergence to the optimal solution of Eq. (2) under a _logarithmic Sobolev inequality_ (LSI) that can be verified in various settings including regularized empirical risk minimization using neural networks. This being said, there is still a large gap between the ideal MFLD analyzed in prior works and a feasible algorithm. In practice, we parameterize \(\) as a mixture of \(N\) particles \((X^{i})_{i=1}^{N}\) -- this corresponds to a neural network with \(N\) neurons, and perform a discrete-time update: at time step \(k\), the update to the \(i\)-th particle is given as

\[X^{i}_{k+1}=X^{i}_{k}-_{k})}{} (X^{i}_{k})+}^{i}_{k},\] (3)

where \(_{k}\) is the step size at the \(k\)-th iteration, \(^{i}_{k}\) is an i.i.d. standard Gaussian vector, and \(\) represents a potentially inexact (e.g., stochastic) gradient.

Comparing Eq. (1) and Eq. (3), we observe the following discrepancies between the ideal MFLD and the implementable noisy particle gradient descent algorithm.

1. **Particle approximation.**\(\) is entirely represented by a finite set of particles: \(_{k}=_{i=1}^{N}_{X^{i}_{k}}\).
2. **Time discretization.** We employ discrete gradient descent update as opposed to gradient flow.
3. **Stochastic gradient.** In many practical settings, it is computationally prohibitive to obtain the exact gradient update, and hence it is preferable to adopt a stochastic estimate of the gradient.

The control of finite-particle error (point \((i)\)) is referred to as _propagation of chaos_(Sznitman, 1991) (see also Lacker (2021) and references therein). In the context of mean-field neural networks, discretization error bounds in prior works usually grow _exponentially in time_(Mei et al., 2018; Javanmard et al., 2019; De Bortoli et al., 2020), unless one introduces additional assumptions on the dynamics that are difficult to verify (Chen et al., 2020). Consequently, convergence guarantee in the continuous limit cannot be transferred to the finite-particle setting unless the time horizon is very short (e.g., Abbe et al. (2022)), which limits the applicability of the theory.

Very recently, Chen et al. (2022); Suzuki et al. (2023) established a _uniform-in-time_ propagation of chaos for MFLD, i.e., the "distance" between the \(N\)-particle system and the infinite-particle limit is of order \((1/N)\) for all \(t>0\). While this represents a significant step towards an optimization theory for practical finite-width neural networks in the mean-field regime, these results assumed the continuous-time limit and access to exact gradient, thus cannot cover points \((ii)\) and \((iii)\).

In contrast, for the standard gradient Langevin dynamics (LD) without the mean-field interactions (which is a special case of MFLD Eq. (1) by setting \(F\) to be a _linear_ functional), the time discretization is well-understood (see e.g. Dalalyan (2014); Vempala and Wibisono (2019); Chewi et al. (2021)), and its stochastic gradient variant (Welling and Teh, 2011; Ma et al., 2015), including ones that employ variance-reduced gradient estimators (Dubey et al., 2016; Zou et al., 2018; Kinoshita and Suzuki, 2022), have also been extensively studied.

The gap between convergence analyses of LD and MFLD motivates us to ask the following question.

_Can we develop a complete non-asymptotic convergence theory for MFLD that takes into account points \((i)\) - \((iii)\), and provide further refinement over existing results when specialized to LD?_

### Our Contributions

We present a unifying framework to establish uniform-in-time convergence guarantees for the mean-field Langevin dynamics under time and space discretization, simultaneously addressing points \((i)\)-\((iii)\). The convergence rate is exponential up to an error that vanishes when the step size and stochastic gradient variance tend to \(0\), and the number of particles \(N\) tends to infinity. Moreover, our proof is based on an LSI condition analogous to Nitanda et al. (2022); Chizat (2022), which is satisfied in a wide range of regularized risk minimization problems. The advantages of our analysis is summarized as follows.

* Our framework provides a unified treatment of different gradient estimators. Concretely, we establish convergence rate of MFLD with stochastic gradient and stochastic variance reduced gradient (Johnson and Zhang, 2013). While it is far from trivial to derive a tight bound on the stochastic gradient approximation error because it requires evaluating the correlation between the randomness of each gradient and the updated parameter distribution, we are able to show that a stochastic gradient effectively improves the computational complexity. Noticeably, despite the fact that our theorem simultaneously handles a wider class of \(F\), when specialized to standard Langevin dynamics (i.e., when \(F\) is linear), we recover state-of-the-art convergence rates for LD; moreover, by introducing an additional mild assumption on the smoothness of the objective, our analysis can significantly improve upon existing convergence guarantees.
* Our analysis greatly extends the recent works of Chen et al. (2022); Suzuki et al. (2023), in that our propagation of chaos result covers the discrete time setting while the discretization error can still be controlled uniformly over time, i.e., the finite particle approximation error does not blow up as \(t\) increases. Noticeably, we do not impose weak interaction / large noise conditions that are common in the literature (e.g., Delarue and Tse (2021); Lacker and Flem (2022)); instead, our theorem remains valid for _any_ regularization strength.

## 2 Problem Setting

Consider the set of probability measure \(\) on \(^{d}\) where the Borel \(\)-algebra is equipped. Our goal is to find a probability measure \(\) that approximately minimizes the objective given by

\[F()=U()+_{}[r],\]

where \(U:\) is a (convex) loss function, and \(r:^{d}\) is a regularization term. Let \(_{2}\) be the set of probability measures with the finite second moment. In the following, we consider the setting where \(F() C(1+_{}[\|X\|^{2}])\), and focus on \(_{2}\) so that \(F\) is well-defined.

As previously mentioned, an important application of this minimization problem is the learning of two-layer neural network in the _mean-field regime_. Suppose that \(h_{x}()\) is a neuron with a parameter \(x^{d}\), e.g., \(h_{x}(z)=(w^{}z+b)\), where \(w^{d-1},b\), and \(x=(w,v)\). The mean-field neural network corresponding to a probability measure \(\) can be written as \(f_{}()= h_{x}()(x)\). Given training data \((z_{i},y_{i})_{i=1}^{n}^{d-1}\), we may define the empirical risk of \(f_{}\) as \(U()=_{i=1}^{n}(f_{}(z_{i}),y_{i})\) for a loss function \(:\). Then, the objective \(F\) becomes

\[F()=_{i=1}^{n}(f_{}(z_{i}),y_{i})+_{1}\|x \|^{2}(x),\]

   Method (authors) & \# of particles & Total complexity & Single loop & Mean-field \\  PDA* & & & & \\ (Nintada et al., 2021) & \(^{-2}(n)\) & \(G_{}^{-1}\) & \(\) & ✓ \\  P-SDCA & & & & \\ (Oke et al., 2022) & & & & \\  GLD & & & & \\ (Vempala and Wibisono, 2019) & & & & \\  SVRG-LD & & & & \\ (Kinoshita and Suzuki, 2022) & & & & \\  F-MFLD **(ours)** & \(^{-1}\) & \(nE_{*})}{()}\) & ✓ & ✓ \\  SGD-MFLD* **(ours)** & \(^{-1}\) & \(^{-1}E_{*})}{()}\) & ✓ & ✓ \\  SGD-MFLD* (ii) **(ours)** & \(^{-1}\) & \(^{-1}(1+}))}{( )^{2}}\) & ✓ & ✓ \\  SVRG-MFLD **(ours)** & \(^{-1}\) & \(E_{*})}{()}+n\) & ✓ & ✓ \\  SVRG-MFLD (ii) **(ours)** & \(^{-1}\) & \((n^{1/3}E_{*}+^{1/4}E_{*}^{3/4}))}{( )}+n\) & ✓ & ✓ \\   

Table 1: Comparison of computational complexity to optimize an entropy-regularized finite-sum objective up to excess objective value \(\), in terms of dataset size \(n\), entropy regularization \(\), and LSI constant \(\). Label * indicates the _online_ setting, and the unlabeled methods are tailored to the _finite-sum_ setting. “Mean-field” indicates the presence of particle interactions. “Single loop” indicates whether the algorithm requires an inner-loop MCMC sampling sub-routine at every step. “(ii)” indicates convergence rate under additional smoothness condition (Assumption 4), where \(E_{*}=^{2}}{}+}{}\). For double-loop algorithms (PDA and P-SDCA), \(G^{*}\) is the number of gradient evaluations required for MCMC sampling; for example, for MALA (Metropolis-adjusted Langevin algorithm) \(G_{}=O(n^{-5/2}(1/)^{3/2})\), and for LMC (Langevin Monte Carlo) \(G_{}=O(n()^{-2}())\).

where the regularization term is \(r(x)=_{1}\|x\|^{2}\). Note that the same objective can be defined for expected risk minimization. We defer additional examples to the last of this section.

One effective way to solve the above objective is the _mean-field Langevin dynamics_ (MFLD), which optimizes \(F\) via a noisy gradient descent update. To define MFLD, we need to introduce the first-variation of the functional \(F\).

**Definition 1**.: _Let \(G:_{2}\). The first-variation \(\) of a functional \(G:_{2}\) at \(_{2}\) is defined as a continuous functional \(_{2}^{d}\) that satisfies \(_{ 0}= ()(x)(-)\) for any \(_{2}\). If there exists such a functional \(\), we say \(G\) admits a first-variation at \(\), or simply \(G\) is differentiable at \(\)._

To avoid the ambiguity of \(\) up to constant shift, we follow the convention of imposing \(()=0\). Using the first-variation of \(F\), the MFLD is given by the following stochastic differential equation:

\[X_{t}=-)}{}(X_{t})t+ W_{t},_{t}=(X_{t}),\] (4)

where \(X_{0}_{0}\), \((X)\) denotes the distribution of the random variable \(X\) and \((W_{t})_{t 0}\) is the \(d\)-dimensional standard Brownian motion. Readers may refer to Huang et al. (2021) for the existence and uniqueness of the solution. MFLD is an instance of _distribution-dependent_ SDE because the drift term \()}{}()\) depends on the distribution \(_{t}\) of the current solution \(X_{t}\)(Kahn and Harris, 1951; Kac, 1956; McKean, 1966). It is known that the MFLD is a Wasserstein gradient flow to minimize the following _entropy-regularized_ objective (Mei et al., 2018; Hu et al., 2019; Nitanda et al., 2022; Chizat, 2022):

\[()=F()+(),\] (5)

where \(()=-((z)/z)(z)\) is the negative entropy of \(\).

Reduction to standard Langevin dynamics.Note that the MFLD reduces to the standard gradient Langevin dynamics (LD) when \(F\) is a linear functional, that is, there exists \(V\) such that \(F()= V(x)(x)\). In this case, \(=V\) for any \(\) and the MFLD Eq. (4) simplifies to

\[X_{t}=- V(X_{t})t+W_{t}.\]

This is exactly the gradient Langevin dynamics for optimizing \(V\) or sampling from \((-V/)\).

### Some Applications of MFLD

Here, we introduce a few examples that can be approximately solved via MFLD.

_Example 1_ (Two-layer neural network in mean-field regime.).: Let \(h_{x}(z)\) be a neuron with a parameter \(x^{d}\), e.g., \(h_{x}(z)=(r(w^{}x))\), \(h_{x}(z)=(r)(w^{}x)\) for \(x=(r,w)\), or simply \(h_{x}(z)=(x^{}z)\). Then the learning of mean-field neural network \(f_{}()= h_{x}()(x)\) via minimizing the empirical risk \(U()=_{i=1}^{n}(f_{}(z_{i}),y_{i})\) with a convex loss (e.g., the logistic loss \((f,y)=(1+(-yf))\), or the squared loss \((f,y)=(f-y)^{2}\)) falls into our framework.

_Example 2_ (Density estimation via MMD minimization).: For a positive definite kernel \(k\), the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) between two probability measures \(\) and \(\) is defined as \(^{2}(,):=(k(x,x)-2k(x,y)+k(y,y))(x)(y)\). We perform nonparametric density estimation by fitting a Gaussian mixture model \(f_{}(z)= g_{x}(z)(x)\), where \(g_{x}\) is the Gaussian density with mean \(x\) and a given variance \(^{2}>0\). The mixture model is learned by minimizing \(^{2}(f_{},p^{*})\) where \(p^{*}\) is the target distribution. If we observe a set of training data \((z_{i})_{i=1}^{n}\) from \(p^{*}\), then the empirical version of MMD is one suitable loss function \(U()\) given as

\[}^{2}():=\!\!\!\!g_{x}(z)g_{x^{ }}(z^{})k(z,z^{})zz^{}\!( \!\!)(x,x^{})\!-\!2\!\!\!_{ i=1}^{n}\!\!\!g_{x}(z)k(z,z_{i})z\!(x),\]

Note that we may also choose to directly fit the particles to the data (instead of a Gaussian mixture model), that is, we use a Dirac measure for each particle, as in Chizat (2022, Section 5.2) and Arbel et al. (2019). Here we state the Gaussian parameterization for the purpose of density estimation.

_Example 3_ (Kernel Stein discrepancy minimization).: In settings where we have access to the target distribution through the score function (e.g., sampling from a posterior distribution in Bayesian inference), we may employ the kernel Stein discrepancy (KSD) as the discrepancy measure (Chwialkowski et al., 2016; Liu et al., 2016). Suppose that we can compute \(s(z)=(^{*})(z)\), then for a positive definite kernel \(k\), we define the _Stein kernel_ as

\[W_{^{*}}(z,z^{}):=s(z)^{}s(z^{})k(z,z^{})+s(z)^{} _{z^{}}k(z,z^{})+_{z}^{}k(z,z^{})s(z^{ })+_{z^{}}^{}_{z}k(z,z^{}).\]

We take \(U()\) as the KSD between \(\) and \(^{*}\) defined as \(()= W_{^{*}}(z,z^{})(z)(z^{})\). By minimizing this objective via MFLD, we attain kernel Stein variational inference with convergence guarantees. This can be seen as a "Langevin" version of KSD descent in Korba et al. (2021).

**Remark 1**.: _In the above examples, we introduce additional regularization terms in the objective Eq. (5) to establish the convergence rate of MFLD, in exchange for a slight optimization bias. Note that these added regularizations often have a statistical benefit due to the smoothing effect._

### Practical implementations of MFLD

Although the convergence of MFLD (Eq. (4)) has been studied in prior works (Hu et al., 2019; Nitanda et al., 2022; Chizat, 2022), there is a large gap between the ideal dynamics and a practical implementable algorithm. Specifically, we need to consider \((i)\) the finite-particle approximation, \((ii)\) the time discretization, and \((iii)\) stochastic gradient.

To this end, we consider the following space- and time-discretized version of the MFLD with stochastic gradient update. For a finite set of particles \(=(X^{i})_{i=1}^{N}^{d}\), we define its corresponding empirical distribution as \(_{}=_{i=1}^{N}_{X_{i}}\). Let \(_{k}=(X^{i}_{k})_{i=1}^{N}^{d}\) be \(N\) particles at the \(k\)-th update, and define \(_{k}=_{_{k}}\) as a finite particle approximation of the population counterpart. Starting from \(X^{i}_{0}_{0}\), we update \(_{k}\) as,

\[X^{i}_{k+1}=X^{i}_{k}-_{k}v^{i}_{k}+}^{i}_{k},\] (6)

where \(_{k}>0\) is the step size, \(^{i}_{k}\) is an i.i.d. standard normal random variable \(^{i}_{k} N(0,I)\), and \(v^{i}_{k}=v^{i}_{k}(_{0:k},^{i}_{k})\) is a stochastic approximation of \()}{}(X^{i}_{k})\) where \(_{k}=(^{i}_{k})_{i=1}^{N}\) is a random variable generating the randomness of stochastic gradient. \(v^{i}_{k}\) can depend on the history \(_{0:k}=(_{0},,_{k})\), and \(_{^{i}_{k}}[v^{i}_{k}|_{0:k}]=)}{}(X^{i}_{k})\). We analyze three versions of \(v^{i}_{k}\).

(1) Full gradient: F-MFLD.If we have access to the exact gradient, we may compute

\[v^{i}_{k}=)}{}(X^{i}_{k}).\]

(2) Stochastic gradient: SGD-MFLD.Suppose the loss function \(U\) is given by an expectation as \(U()=_{Z P_{Z}}[(,Z)],\) where \(Z\) is a random observation obeying a distribution \(P_{Z}\) and \(:\) is a loss function. In this setting, we construct the stochastic gradient as

\[v^{i}_{k}=_{j=1}^{B},z^{(j)}_{k})} {}(X^{i}_{k})+ r(X^{i}_{k}),\]

where \((z^{(j)}_{k})_{j=1}^{B}\) is a mini-batch of size \(B\) generated from \(P_{Z}\) in an i.i.d. manner.

(3) Stochastic variance reduced gradient: SVRG-MFLD.Suppose that the loss function \(U\) is given by a finite sum of loss functions \(_{i}\): \(U()=_{i=1}^{n}_{i}(),\) which corresponds to the empirical risk in a usual machine learning setting. Then, the variance reduced stochastic gradient (SVRG) (Johnson and Zhang, 2013) is defined as follows:

1. When \(k 0 m\), where \(m\) is the update frequency, we set \(}=(^{i})_{i=1}^{n}=(X^{i}_{k})_{i=1}^{n}\) as the anchor point and refresh the stochastic gradient as \(v^{i}_{k}=)}{}(X^{i}_{k})\).
2. When \(k 0 m\), we use the anchor point to construct a control variate and compute \[v^{i}_{k}=_{j I_{k}}((_{ _{k}})}{}(X^{i}_{k})+r(X^{i}_{k})-(_{ })}{}(^{i})+})}{ }(^{i})),\] where \(I_{k}\) is a uniformly drawn random subset of \(\{1,,n\}\) with size \(B\) without duplication.

Main Assumptions and Theoretical Tools

For our convergence analysis, we make the following assumptions which are inherited from prior works in the literature (Nitanda et al., 2020, 2022; Chizat, 2022; Oko et al., 2022; Chen et al., 2022).

**Assumption 1**.: _The loss function \(U\) and the regularization term \(r\) are convex. Specifically,_

1. \(U:\) _is a convex functional on_ \(\)_, that is,_ \(U(+(1-)) U()+(1-)U()\) _for any_ \(\) _and_ \(,\)_. Moreover,_ \(U\) _admits a first-variation at any_ \(_{2}\)_._
2. \(r()\) _is twice differentiable and convex, and there exist constants_ \(_{1},_{2}>0\) _and_ \(c_{r}>0\) _such that_ \(_{1}I^{}r(x)_{2}I\)_,_ \(x^{} r(x)_{1}\|x\|^{2}\)_, and_ \(0 r(x)_{2}(c_{r}+\|x\|^{2})\) _for any_ \(x^{d}\)_, and_ \( r(0)=0\)_._

**Assumption 2**.: _There exists \(L>0\) such that \(\|(x)-)}{ }(x^{})\| L(W_{2}(,^{})+\|x-x^{}\|)\) and \(|U()}{^{2}}(x,x^{})| L(1+c_{ L}(\|x\|^{2}+\|x^{}\|^{2}))\) for any \(,^{}_{2}\) and \(x,x^{}^{d}\). Also, there exists \(R>0\) such that \(\|(x)\| R\) for any \(\) and \(x^{d}\)._

Verification of this assumption in the three examples (Examples 1, 2 and 3) is given in Appendix A in the supplementary material. We remark that the assumption on the second order variation is only required to derive the discretization error corresponding to the uniform-in-time propagation of chaos (Lemma 8 in Appendix E.4). Under these assumptions, Proposition 2.5 of Hu et al. (2019) yields that \(\) has a unique minimizer \(^{*}\) in \(\) and \(^{*}\) is absolutely continuous with respect to the Lebesgue measure. Moreover, \(^{*}\) satisfies the self-consistent condition: \(^{*}(X)(-)}{ }(X)).\)

### Proximal Gibbs Measure & Logarithmic Sobolev inequality

The aforementioned self-consistent relation motivates us to introduce the _proximal Gibbs distribution_(Nitanda et al., 2022; Chizat, 2022) whose density is given by

\[p_{}(X)(-})}{}(X)),\]

where \(=(X^{i})_{i=1}^{N}^{d}\) is a set of \(N\) particles and \(X^{d}\). As we will see, the convergence of MFLD heavily depends on a _logarithmic Sobolev inequality_ (LSI) on the proximal Gibbs measure.

**Definition 2** (Logarithmic Sobolev inequality).: _Let \(\) be a probability measure on \((^{d},(^{d}))\). \(\) satisfies the LSI with constant \(>0\) if for any smooth function \(:^{d}\) with \(_{}[^{2}]<\), we have \(_{}[^{2}(^{2})]-_{}[^{2}]( _{}[^{2}])_{}[\| \|_{2}^{2}].\)_

This is equivalent to the condition that the KL divergence from \(\) is bounded by the Fisher divergence: \((/)\| (/)\|^{2},\) for any \(\) which is absolutely continuous with respect to \(\). Our analysis requires that the proximal Gibbs distribution satisfies the LSI as follows.

**Assumption 3**.: \(^{*}\) _and \(p_{}\) satisfy the LSI with \(>0\) for any set of particles \(=(X^{i})_{i=1}^{N}^{d}\)._

Verification of LSI.The LSI of proximal Gibbs measure can be established via standard perturbation criteria. For \(U()\) with bounded first-variation, we may apply the classical Bakry-Emery and Holley-Stroock arguments (Bakry and Emery, 1985a; Holley and Stroock, 1987) (see also Corollary 5.7.2 and 5.1.7 of Bakry et al. (2014)). Whereas for Lipschitz perturbations, we employ Miclo's trick (Bardet et al., 2018) or the more recent perturbation results in Cattiaux and Guillin (2022).

**Theorem 1**.: _Under Assumptions 1 and 2, \(^{*}\) and \(p_{}\) satisfy the log-Sobolev inequality with_

\[}{2}\!(\!-}{_{1} })\{\!}+e^{ }{2_{1}}\!(}\!+\!}})^{2}}[2\!+\!d\!+\!(}{_{1}})\!+\!4}{_{1}}]\}^ {-1}.\]

_Furthermore, if \(\|\|_{} R\) is satisfied for any \(_{2}\), then \(^{*}\) and \(p_{}\) satisfy the LSI with \(}{}(-)\)._

See Lemma 5 for the proof of the first assertion, and Section A.1 of Nitanda et al. (2022) or Proposition 5.1 of Chizat (2022) for the proof of the second assertion.

Main Result: Convergence Analysis

In this section, we present our the convergence rate analysis of the discretized dynamics Eq. (6). To derive the convergence rate, we need to evaluate the errors induced by three approximations: (i) time discretization, (ii) particle approximation, and (iii) stochastic gradient.

### General Recipe for Discretization Error Control

Note that for the finite-particle setting, the entropy term does not make sense because the negative entropy is not well-defined for a discrete empirical measure. Instead, we consider the distribution of \(N\) particles, that is, let \(^{(N)}^{(N)}\) be a distribution of \(N\) particles \(=(X^{i})_{i=1}^{N}\) where \(^{(N)}\) is the set of probability measures on \((^{d N},(^{d N}))\). Similarly, we introduce the following objective on \(^{(N)}\):

\[^{N}(^{(N)})=N_{^{(N)}}[F(_{ })]+(^{(N)}).\]

One can easily verify that if \(^{(N)}\) is a product measure of \(\), then \(^{N}(^{(N)}) N()\) by the convexity of \(F\). _Propagation of chaos_(Sznitman, 1991) refers to the phenomenon that, as the number of particles \(N\) increases, the particles behave as if they are independent; in other words, the joint distribution of the \(N\) particles becomes "close" to the product measure. Consequently, the minimum of \(^{N}(^{(N)})\) (which can be obtained by the particle-approximated MFLD) is close to \(N(^{*})\). Specifically, it has been shown in Chen et al. (2022) that

\[0_{^{(N)}^{(N)}}^{N}(^{(N)} )-(^{*})}{N},\] (7)

for some constant \(C_{}>0\) (see Section E for more details). Importantly, if we consider the Wasserstein gradient flow on \(^{(N)}\), the convergence rate of which depends on the logarithmic Sobolev inequality (Assumption 3), we need to ensure that the LSI constant does not deteriorate as \(N\) increases. Fortunately, the propagation of chaos and the _tensorization_ of LSI entail that the LSI constant with respect to the objective \(^{N}\) can be uniformly bounded over all choices of \(N\) (see Eq. (11) in the appendix).

To deal with the time discretization error, we build upon the one-step interpolation argument from Vempala and Wibisono (2019) which analyzed the vanilla gradient Langevin dynamics (see also Nitanda et al. (2022) for its application to the infinite-particle MFLD).

Bounding the error induced by the stochastic gradient approximation is also challenging because the objective is defined on the space of probability measures, and thus techniques in finite-dimensional settings cannot be utilized in a straightforward manner. Roughly speaking, this error is characterized by the variance of \(v_{k}^{i}\): \(^{2}_{v,k}:=_{_{k},_{0:k}}[\|v_{k}^{i}- )}{}\|^{2}]\)1. In addition, to obtain a refined evaluation, we also incorporate the following smoothness assumption.

**Assumption 4**.: \(v_{k}^{i}(_{0:k},_{k}^{i})\) _and \(D_{m}^{i}F(_{k})=)}{}(X_{k}^{ i})\) are differentiable w.r.t. \((X_{k}^{j})_{j=1}^{N}\) and, for either \(G(_{k})=v_{k}^{i}(_{0:k},_{k}^{i})\) or \(G(_{k})=D_{m}^{i}F(_{k})\) as a function of \(_{k}\), it is satisfied that \(\|G(_{k}^{})-G(_{k})-_{j=1}^{N}_{X_{k}^ {j}}^{}G(_{k})(X_{k}^{ j}-X_{k}^{j})\| Q\|X_{k}^{ j}-X_{k}^{j}\|^{2}+N\|X_{k}^{ i}-X_{k}^{i} \|^{2}}{2N}\) for some \(Q>0\) and \(_{k}=(X_{k}^{j})_{j=1}^{N}\) and \(_{k}^{}=(X_{k}^{ j})_{j=1}^{N}\). We also assume \(_{_{k}}\|_{X_{k}^{j}}v_{k}^{i}(_{0:k}, _{k}^{i})^{}-_{X_{k}^{j}}D_{m}^{i}F(_{k})^{} _{}^{2}N^{2}}{N^{2}}_{v,k}^{2}\), and \(\|v_{k}^{i}((_{k},_{0:k-1}),_{k}^{i})-v_{k}^{i}(( _{k}^{},_{0:k-1}),_{k}^{i})\| L(W_{2}( _{_{k}},_{_{k}^{}})+\|X_{k}^{i}-X_{k}^{ i }\|)\)._

Note that this assumption is satisfied if the gradient is twice differentiable and the second derivative is bounded. The factor \(N\) appears because the contribution of each particle is \(O(1/N)\) unless \(i=j\). In the following, we present both the basic convergence result under Assumption 2, and the improved rate under the additional Assumption 4.

Taking these factors into consideration, we can evaluate the decrease in the objective value after one-step update. Let \(_{k}^{(N)}^{(N)}\) be the distribution of \(_{k}\) conditioned on the sequence \(_{0:k}=(_{k^{}})_{k^{}=0}^{k}\). Define

\[^{2}:=[\|X_{0}^{i}\|^{2}]+}[( }{4_{2}}+})(R^{2}+_ {2}c_{r})+ d],\ \ \ _{}:=C_{1}^{2}(^{2}+),\]

where \(C_{1}=8[R^{2}+_{2}(c_{r}+^{2})+d]\) and \(=L+_{2}\).2

**Theorem 2**.: _Under Assumptions 1, 2 and 3, if \(_{k}_{1}/(4_{2})\), we have_

_where \(_{k}=4_{k}_{_{k}}+(R+_{2})(1+ })(_{k}^{2}_{v,k}_{v,k}+Q _{k}^{3}_{v,k}^{2})+_{k}^{2}(L+_{2})^{2}_{v,k} ^{2}\) with Assumption 4, and \(_{k}=_{v,k}^{2}_{k}\) without this additional assumption; the expectation is taken with respect to the randomness \((_{k^{}})_{k^{}=1}^{k}=((_{k^{}}^{i})_{i=1}^{N} )_{k^{}=1}^{k}\) of the stochastic gradient; and \(C_{}=2 L(1+2c_{L}^{2})+2^{2}L^{2}^{2}\)._

The proof can be found in Appendix B. We remark that to derive the bound for \(_{k}=_{v,k}^{2}_{k}\) is relatively straightforward, but to derive a tighter bound with Assumption 4 is technically challenging, because we need to evaluate how the next-step distribution \(_{k+1}^{(N)}\) is correlated with the stochastic gradient \(v_{k}^{i}\). Evaluating such correlations is non-trivial because the randomness is induced not only by \(_{k}\) but also by the Gaussian noise. Thanks to this general result, we only need to evaluate the variance \(_{v,k}\) and \(_{v,k}\) for each method to obtain the specific convergence rate.

Conversion to a Wasserstein distance bound.As a consequence of the bound on \(^{N}(_{k}^{(N)})-(^{*})\), we can control the Wasserstein distance between \(_{k}^{(N)}\) and \(^{*N}\), where \(^{*N}^{(N)}\) is the (\(N\)-times) product measure of \(^{*}\). Let \(W_{2}(,)\) be the 2-Wasserstein distance between \(\) and \(\), then

\[W_{2}^{2}(_{k}^{(N)},^{*N})(^{N}( _{k}^{(N)})-N(^{*})),\]

under Assumptions 1 and 3 (see Lemma 3 in the Appendix). Hence, if \(^{N}(_{k}^{(N)})-(^{*})\) is small, the particles \((X_{k}^{i})_{i=1}^{N}\) behaves like an i.i.d. sample from \(^{*}\). As an example, for the mean-field neural network setting, if \(|h_{x}(z)-h_{x^{}}(z)| L\|x-x^{}\|\)\(( x,x^{}^{d})\) and \(V_{^{*}}=(f_{^{*}}(z)-h_{x}(z))^{2}^{*}(x)<\) for a fixed \(z\), then Lemma 4 in the appendix yields that

\[_{_{k}_{k}^{(N)}}[(f_{_{_{k}}}(z)-f _{^{*}}(z))^{2}]}{N}W_{2}^{2}(_{k}^{(N)},^{*N})+ {2}{N}V_{^{*}},\]

which also gives \(_{_{k}_{k}^{(N)}}[(f_{_{_{k}}}(z)-f _{^{*}}(z))^{2}]}{}(N^{-1}^{N} (_{k}^{(N)})-(^{*}))+V_{^{*}}\). This allows us to monitor the convergence of the finite-width neural network to the optimal solution \(^{*}\) in terms of the model output (up to \(1/N\) error).

### F-MFLD and SGD-MFLD

Here, we present the convergence rate for F-MFLD and SGD-MFLD simultaneously. F-MFLD can be seen as a special case of SGD-MFLD where the variance \(_{v,k}^{2}=0\). We specialize the previous assumptions to the stochastic gradient setting as follows.

**Assumption 5**.:
1. \(_{x^{d}}\|(x)\| R\) _for all_ \(\) _and_ \(z\)_._
2. \(_{x}\|_{x}_{x}^{}(x)\|_ {},\ _{x,x^{}}\|_{x}_{x^{}}^{}(,z)}{^{2}}(x,x^{})\|_{} R\)_._

Note that point \((i)\) is required to bound the variance \(_{v,k}^{2}\) (i.e., \(_{v,k}^{2} R^{2}/B\)), whereas point \((ii)\) corresponds to the additional Assumption 4 required for the improved convergence rate. Let \(_{0}:=[^{N}(_{0}^{(N)})]-( ^{*})\). We have the following evaluation of the objective. Note that we can recover the evaluation for F-MFLD by formally setting \(B=\) so that \(_{v,k}^{2}=0\) and \(_{v,k}^{2}=0\).

**Theorem 3**.: _Suppose that \(_{k}=\)\(( k_{0})\) and \( 1/4\) and \(_{1}/(4_{2})\). Under Assumptions 1, 2, 3 and 5, it holds that_

\[[^{N}(_{k}^{(N)})]-(^{*}) (- k/2)_{0}+ ^{2}C_{1}(+^{2})+ +}{ N},\] (8)

_where \(=4_{}+[R+_{2}+(L+_{2})^{ 2}](1+})^{2}}{B}+(R+ _{2})R(1+})^{3}} {B}\) under Assumption 5-(ii), and \(=}{B}\) without this additional assumption._

The proof is given in Appendix C. This can be seen as a mean-field generalization of Vempala and Wibisono (2019) which provides a convergence rate of discrete time vanilla GLD with respect to the KL divergence. Indeed, their derived rate \(O((- k)_{0}+)\) is consistent to ours, since Eq. (26) in the Appendix implies that the objective \(([^{N}(_{k}^{(N)})]- (^{*}))\) upper bounds the KL divergence between \(_{k}^{(N)}\) and \(^{*N}\). Moreover, our result also handles the stochastic approximation which can give better total computational complexity even in the vanilla GLD setting as shown below.

For a given \(>0\), if we take

\[^{2}C_{1}}} {}} 1,\;\;k (2_{0}/),\]

with \(B 4[(1+R)(R+_{2})+(L+_{2})^{2}]R^{2}( +)/()\) under Assumption 5-(ii) and \(B 4R^{2}/()\) without such assumption, then the right hand side of (8) can be bounded as \([^{N}(_{k}^{(N)})]-(^{*})= +}{ N}\). Hence we achieve \(+O(1/N)\) error with iteration complexity:

\[k=O(^{2}}{}+}{})(^{-1}).\] (9)

If we neglect \(O(1/)\) as a second order term, then the above can be simplified as \(O^{2}}{^{2}})}{ }\).

Noticeably, the mini-batch size \(B\) can be significantly reduced under the additional smoothness condition Assumption 5-(ii) (indeed, we have \(O(+)\) factor reduction). Recall that when the objective is an empirical risk, the gradient complexity per iteration required by F-MFLD is \(O(n)\). Hence, the total complexity of F-MFLD is \(O(nk)\) where \(k\) is given in Eq. (9). Comparing this with SGD-MFLD with mini-batch size \(B\), we see that SGD-MFLD has better total computational complexity (\(Bk\)) when \(n>B\). In particular, if \(B=((+)/())(^ {-1}+})\) which yields the objective value of order \(O()\), SGD-MFLD achieves \(O(n/B)=O(n())\) times smaller total complexity. For example, when \(==1/\), a \(O()\)-factor reduction of total complexity can be achieved by SGD-MFLD, which is a significant improvement.

### Svrg-Mfld

Now we present the convergence rate of SVRG-MFLD in the fixed step size setting where \(_{k}=\)\(( k)\). Instead of Assumption 5, we introduce the following two assumptions.

**Assumption 6**.:
1. _For any_ \(i[n]\)_, it holds that_ \(\|()}{}(x)- (^{})}{}(x^{})\| L(W_{2}(,^{})+ \|x-x^{}\|)\) _for any_ \(,^{}_{2}\) _and_ \(x,x^{}^{d}\)_, and_ \(_{x^{d}}\|()}{}(x)\| R\) _for any_ \(\)_._
2. _Additionally, we have_ \(_{x}\|_{x}_{x}^{}()}{}(x) \|_{},_{x,x^{}}\|_{x}_{x^{}}^{} _{i}()}{^{2}}(x,x^{})\|_{}  R\)_._

Here again, point (i) is required to bound \(_{v,k}^{2}\) and point (ii) yields Assumption 4. We have the following computational complexity bound for SVRG-MFLD.

**Theorem 4**.: _Suppose that \( 1/4\) and \(_{1}/(4_{2})\). Let \(=\). Then, under Assumptions 1, 2, 3 and 6, we have the same error bound as Eq. (8) with different \(\):_

\[=4_{}+(1+}) \{(R+_{2})^{2} L^{2}m(^{2}+ )R^{2}}+[(R+_{2})R^{3}+(L+ _{2})^{2}^{2}]\},\]

_under Assumption 6-(ii), and \(=C_{1} L^{2}m^{2}(+)\) without Assumption 6-(ii)._The proof is given in Appendix D. Therefore, to achieve \([^{N}(_{k}^{(N)})]-(^{*}) O( )+}{ N}\) for a given \(>0\), it suffices to set

\[=C_{1}}}{40L}},\,\,k=/)}{ }=O(^{2}}{}+}{}))}{()},\]

with \(B[)^{2}}{} m )^{3}}{}] n\). In this setting, the total gradient complexity can be bounded as

\[Bk++n\{n^{}(1+})^{},()^{}(1+ })^{}\})}{}+n,\]

where \(m=(n/B)=([n^{2/3}(1+)^{-4/3}(1+ )^{-3/2}()^{-1/2})] 1)\) and \(B=O([n^{}(1+})^{ }()^{}(1+})^{}] n)\). When \(\) is small, the first term in the right hand side becomes the main term which is \(\{n^{1/3},()^{1/4}\}) }{^{2}}\). Therefore, comparing with the full batch gradient method (F-MFLD), SVRG-MFLD achieves at least \(\{n^{},()^{-}\}\) times better total computational complexity when \(\). Note that even without the additional Assumption 6-(ii), we still obtain a \(\)-factor improvement (see Appendix D). This indicates that variance reduction is indeed effective to improve the computational complexity, especially in a large sample size setting.

Finally, we compare the convergence rate in Theorem 4 (setting \(F\) to be linear) against prior analysis of standard gradient Langevin dynamics (LD) under LSI. To our knowledge, the current best convergence rate of LD in terms of KL divergence was given in Kinoshita and Suzuki (2022), which yields a \(O((n+}{}))}{( )^{2}})\) iteration complexity to achieve \(\) error. This corresponds to our analysis under Assumption 6-(i) (without (ii)) (see Appendix D). Note that in this setting, our bound recovers their rate even though our analysis is generalized to nonlinear mean-field functionals (the \(O()\)-factor discrepancy is due to the difference in the objective - their bound considers the KL divergence while our objective corresponds to \(\) times the KL divergence). Furthermore, our analysis gives an even faster convergence rate under an additional mild assumption (Assumption 6-(ii)).

## 5 Conclusion

We gave a unified theoretical framework to bound the optimization error of the _single-loop_ mean-field Langevin dynamics (MFLD) that is applicable to the finite-particle, discrete-time, and stochastic gradient algorithm. Our analysis is general enough to cover several important learning problems such as the optimization of mean-field neural networks, density estimation via MMD minimization, and variational inference via KSD minimization. We considered three versions of the algorithms (F-MFLD, SGD-MFLD, SGLD-MFLD); and despite the fact that our analysis deals with a more general setting (mean-field interactions), we are able to recover and even improve existing convergence guarantees when specialized to the standard gradient Langevin dynamics.