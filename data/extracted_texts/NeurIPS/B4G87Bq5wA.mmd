# Fast Approximation of Similarity Graphs

with Kernel Density Estimation

 Peter Macgregor

School of Informatics

University of Edinburgh

United Kingdom

&He Sun

School of Informatics

University of Edinburgh

United Kingdom

###### Abstract

Constructing a similarity graph from a set \(X\) of data points in \(^{d}\) is the first step of many modern clustering algorithms. However, typical constructions of a similarity graph have high time complexity, and a quadratic space dependency with respect to \(|X|\). We address this limitation and present a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed algorithm with the well-known implementations from the scikit-learn library and the FAISS library, and find that our method significantly outperforms the implementation from both libraries on a variety of datasets.

## 1 Introduction

Given a set \(X=\{x_{1},,x_{n}\}^{d}\) of data points and a similarity function \(k:^{d}^{d}_{ 0}\) for any pair of data points \(x_{i}\) and \(x_{j}\), the objective of clustering is to partition these \(n\) data points into clusters such that similar points are in the same cluster. As a fundamental data analysis technique, clustering has been extensively studied in different disciplines ranging from algorithms and machine learning, social network analysis, to data science and statistics.

One prominent approach for clustering data points in Euclidean space consists of two simple steps: the first step is to construct a _similarity graph_\(=(V,E,w)\) from \(X\), where every vertex \(v_{i}\) of \(\) corresponds to \(x_{i} X\), and different vertices \(v_{i}\) and \(v_{j}\) are connected by an edge with weight \(w(v_{i},v_{j})\) if their similarity \(k(x_{i},x_{j})\) is positive, or higher than some threshold. Secondly, we apply spectral clustering on \(\), and its output naturally corresponds to some clustering on \(X\). Because of its out-performance over traditional clustering algorithms like \(k\)-means, this approach has become one of the most popular modern clustering algorithms.

On the other side, different constructions of similarity graphs have significant impact on the quality and time complexity of spectral clustering, which is clearly acknowledged and appropriately discussed by von Luxburg . Generally speaking, there are two types of similarity graphs:

* the first one is the \(k\)-nearest neighbour graph (\(k\)-NN graph), in which every vertex \(v_{i}\) connects to \(v_{j}\) if \(v_{j}\) is among the \(k\)-nearest neighbours of \(v_{i}\). A \(k\)-NN graph is sparse by construction, but loses some of the structural information in the dataset since \(k\) is usually small and the added edges are unweighted.
* the second one is the fully connected graph, in which different vertices \(v_{i}\) and \(v_{j}\) are connected with weight \(w(v_{i},v_{j})=k(x_{i},x_{j})\). While a fully connected graph maintains most of the distance information about \(X\), this graph is dense and storing such graphs requires _quadratic_ memory in \(n\).

Taking the pros and cons of the two constructions into account, one would naturally ask the question:

_Is it possible to directly construct a sparse graph that preserves the cluster structure of a fully connected similarity graph?_

We answer this question affirmatively, and present a fast algorithm that constructs an approximation of the fully connected similarity graph. Our constructed graph consists of only \((n)\) edges1, and preserves the cluster structure of the fully connected similarity graph.

### Our Result

Given any set \(X=\{x_{1},,x_{n}\}^{d}\) and a kernel function \(k:^{d}^{d}_{ 0}\), a fully connected similarity graph \(=(V,E,w)\) of \(X\) consists of \(n\) vertices, and every \(v_{i} V\) corresponds to \(x_{i} X\); we set \(w(v_{i},v_{j}) k(x_{i},x_{j})\) for any different \(v_{i}\) and \(v_{j}\). We introduce an efficient algorithm that constructs a sparse graph \(\)_directly_ from \(X\), such that \(\) and \(\) share the same cluster-structure, and the graph matrices for \(\) and \(\) have approximately the same eigen-gap. This ensures that spectral clustering from \(\) and \(\) return approximately the same result.

The design of our algorithm is based on a novel reduction from the approximate construction of similarity graphs to the problem of Kernel Density Estimation (KDE). This reduction shows that any algorithm for the KDE can be employed to construct a sparse representation of a fully connected similarity graph, while preserving the cluster-structure of the input data points. This is summarised as follows:

**Theorem 1** (Informal Statement of Theorem 2).: _Given a set of data points \(X=\{x_{1},,x_{n}\}^{d}\) as input, there is a randomised algorithm that constructs a sparse graph \(\) of \(X\), such that it holds with probability at least \(9/10\) that_

1. _graph_ \(\) _has_ \((n)\) _edges,_
2. _graph_ \(\) _has the same cluster structure as the fully connected similarity graph_ \(\) _of_ \(X\)_._

_The algorithm uses an approximate KDE algorithm as a black-box, and has running time \((T_{}(n,n,))\) for \( 1/(6(n))\), where \(T_{}(n,n,)\) is the running time of solving the KDE problem for \(n\) data points up to \(a(1+)\)-approximation._

This result builds a novel connection between the KDE and the fast construction of similarity graphs, and further represents a state-of-the-art algorithm for constructing similarity graphs. For instance, when employing the fast Gauss transform  as the KDE solver, Theorem 1 shows that a sparse representation of the fully connected similarity graph with the Gaussian kernel can be constructed in \((n)\) time when \(d\) is constant. As such, in the case of low dimensions, spectral clustering runs as fast (up to a poly-logarithmic factor) as the time needed to read the input data points. Moreover, any improved algorithm for the KDE would result in a faster construction of approximate similarity graphs.

To demonstrate the significance of this work in practice, we compare the performance of our algorithm with five competing algorithms from the well-known scikit-learn library  and FAISS library : the algorithm that constructs a fully connected Gaussian kernel graph, and four algorithms that construct different variants of \(k\)-nearest neighbour graphs. We apply spectral clustering on the six constructed similarity graphs, and compare the quality of the resulting clustering. For a typical input dataset of 15,000 points in \(^{2}\), our algorithm runs in 4.7 seconds, in comparison with between 16.1 - 128.9 seconds for the five competing algorithms from scikit-learn and FAISS libraries. As shown in Figure 1, all the six algorithms return reasonable output.

We further compare the quality of the six algorithms on the BSDS image segmentation dataset , and our algorithm presents clear improvement over the other five algorithms based on the output's average Rand Index. In particular, due to its _quadratic_ memory requirement in the input size, one would need to reduce the resolution of every image down to 20,000 pixels in order to construct the fully connected similarity graph with scikit-learn on a typical laptop. In contrast, our algorithm is able to segment the full-resolution image with over 150,000 pixels. Our experimental result on the BSDS dataset is showcased in Figure 2 and demonstrates that, in comparison with SKLearn GK, our algorithm identifies a more detailed pattern on the butterfly's wing. In contrast, none of the \(k\)-nearest neighbour based algorithms from the two libraries is able to identify the wings of the butterfly.

### Related Work

There are a number of works on efficient constructions of \(\)-neighbourhood graphs and \(k\)-NN graphs. For instance, Dong et al.  presents an algorithm for approximate \(k\)-NN graph construction, and their algorithm is based on local search. Wang et al.  presents an LSH-based algorithm for constructing an approximate \(k\)-NN graph, and employs several sampling and hashing techniques to reduce the computational and parallelisation cost. These two algorithms  have shown to work very well in practice, but lack a theoretical guarantee on the performance.

Figure 1: Output of spectral clustering with different similarity graph constructions.

Figure 2: Comparison on the performance of spectral clustering with different similarity graph constructions. Here, SKLearn GK is based on the fully connected similarity graph construction, and (d) â€“ (g) are based on different \(k\)-nearest neighbour graph constructions from the two libraries.

Our work also relates to a large and growing number of KDE algorithms. Charikar and Siminelakis  study the KDE problem through LSH, and present a class of unbiased estimators for kernel density in high dimensions for a variety of commonly used kernels. Their work has been improved through the sketching technique , and a revised description and analysis of the original algorithm . Charikar et al.  presents a data structure for the KDE problem, and their result essentially matches the query time and space complexity for most studied kernels in the literature. In addition, there are studies on designing efficient KDE algorithms based on interpolation of kernel density estimators , and coresets .

Our work further relates to efficient constructions of spectral sparsifiers for kernel graphs. Quantrol  studies smooth kernel functions, and shows that an explicit \((1+)\)-approximate spectral approximation of the geometric graph with \((n/^{2})\) edges can be computed in \((n/^{2})\) time. Bakshi et al.  proves that, under the strong exponential time hypothesis, constructing an \(O(1)\)-approximate spectral sparsifier with \(O(n^{2-})\) edges for the Gaussian kernel graph requires \((n 2^{(1/)^{0.32}})\) time, where \(<0.01\) is a fixed universal constant and \(\) is the minimum entry of the kernel matrix. Compared with their results, we show that, when the similarity graph with the Gaussian kernel presents a well-defined structure of clusters, an approximate construction of this similarity graph can be constructed in nearly-linear time.

## 2 Preliminaries

Let \(=(V,E,w_{})\) be an undirected graph with weight function \(w_{}:E_{ 0}\), and \(n|V|\). The degree of any vertex \(v\) is defined as \(_{}(v)_{u v}w_{}(u,v)\), where we write \(u v\) if \(\{u,v\} E()\). For any \(S V\), the volume of \(S\) is defined by \(_{}(S)_{v S}_{}(v)\), and the conductance of \(S\) is defined by

\[_{}(S)}(S)}{_{}(S)},\]

where \(_{}(S)_{u S,v S}w_{}(u,v)\). For any \(k 2\), we call subsets of vertices \(A_{1},,A_{k}\) a \(k\)_-way partition_ if \(A_{i}\) for any \(1 i k\), \(A_{i} A_{j}=\) for any \(i j\), and \(_{i=1}^{k}A_{i}=V\). Moreover, we define the \(k\)_-way expansion constant_ by

\[_{}(k)_{A_{1},,A_{k}} _{1 i k}_{}(A_{i}).\]

Note that a lower value of \(_{}(k)\) ensures the existence of \(k\) clusters \(A_{1},,A_{k}\) of low conductance, i.e, \(\) has at least \(k\) clusters.

For any undirected graph \(\), the adjacency matrix \(_{}\) of \(\) is defined by \(_{}(u,v)=w_{}(u,v)\) if \(u v\), and \(_{}(u,v)=0\) otherwise. We write \(_{}\) as the diagonal matrix defined by \(_{}(v,v)=_{}(v)\), and the normalised Laplacian of \(\) is defined by \(_{}-_{}^{-1/2} _{}_{}^{-1/2}\). For any PSD matrix \(^{n n}\), we write the eigenvalues of \(\) as \(_{1}()_{n}()\).

It is well-known that, while computing \(_{}(k)\) exactly is NP-hard, \(_{}(k)\) is closely related to \(_{k}\) through the higher-order Cheeger inequality : it holds for any \(k\) that

\[_{k}(_{})/2_{}(k) O(k^{3}) (_{})}.\]

### Fully Connected Similarity Graphs

We use \(X\{x_{1}, x_{n}\}\) to represent the set of input data points, where every \(x_{i}^{d}\). Given \(X\) and some kernel function \(k:^{d}^{d}_{ 0}\), we use \(=(V_{},E_{},w_{})\) to represent the fully connected similarity graph from \(X\), which is constructed as follows: every \(v_{i} V_{}\) corresponds to \(x_{i} X\), and any pair of different \(v_{i}\) and \(v_{j}\) is connected by an edge with weight \(w_{}(v_{i},v_{j})=k(x_{i},x_{j})\). One of the most common kernels used for clustering is the Gaussian kernel, which is defined by

\[k(x_{i},x_{j})=(--x_{j}\|_{2}^{2}}{^{2}})\]

for some bandwidth parameter \(\). Other popular kernels include the Laplacian kernel and the exponential kernel which use \(\|x_{i}-x_{j}\|_{1}\) and \(\|x_{i}-x_{j}\|_{2}\) in the exponent respectively.

### Kernel Density Estimation

Our work is based on algorithms for kernel density estimation (KDE), which is defined as follows. Given a kernel function \(k:^{d}^{d}_{ 0}\) with \(n\) source points \(x_{1},,x_{n}^{d}\) and \(m\) target points \(y_{1},,y_{m}^{d}\), the KDE problem is to compute \(g_{[1,n]}(y_{1}), g_{[1,n]}(y_{m})\), where

\[g_{[a,b]}(y_{i})_{j=a}^{b}k(y_{i},x_{j})\] (1)

for \(1 i m\). While a direct computation of the \(m\) values \(g_{[1,n]}(y_{1}), g_{[1,n]}(y_{m})\) requires \(mn\) operations, there is substantial research to develop faster algorithms approximating these \(m\) quantities.

In this paper we are interested in the algorithms that approximately compute \(g_{[1,n]}(y_{i})\) for all \(1 i m\) up to a \((1)\)-multiplicative error, and use \(T_{}(m,n,)\) to denote the asymptotic complexity of such a KDE algorithm. We also require that \(T_{}(m,n,)\) is superadditive in \(m\) and \(n\); that is, for \(m=m_{1}+m_{2}\) and \(n=n_{1}+n_{2}\), we have

\[T_{}(m_{1},n_{1},)+T_{}(m_{2},n_{2},) T _{}(m,n,);\]

it is known that such property holds for many KDE algorithms (e.g., ).

## 3 Cluster-Preserving Sparsifiers

A graph sparsifier is a sparse representation of an input graph that inherits certain properties of the original dense graph. The efficient construction of sparsifiers plays an important role in designing a number of nearly-linear time graph algorithms. However, most algorithms for constructing sparsifiers rely on the recursive decomposition of an input graph , sampling with respect to effective resistances , or fast SDP solvers ; all of these need the explicit representation of an input graph, requiring \((n^{2})\) time and space complexity for a fully connected graph.

Sun and Zanetti  study a variant of graph sparsifiers that mainly preserve the cluster structure of an input graph, and introduce the notion of _cluster-preserving sparsifier_ defined as follows:

**Definition 1** (Cluster-preserving sparsifier).: Let \(=(V,E,w_{})\) be any graph, and \(\{A_{i}\}_{i=1}^{k}\) the \(k\)-way partition of \(\) corresponding to \(_{}(k)\). We call a re-weighted subgraph \(=(V,F E,w_{})\) a cluster-preserving sparsifier of \(\) if \(_{}(A_{i})=O(k_{}(A_{i}))\) for \(1 i k\), and \(_{k+1}(_{})=(_{k+1}(_{ }))\).

Notice that graph \(\) has exactly \(k\) clusters if (i) \(\) has \(k\) disjoint subsets \(A_{1},,A_{k}\) of low conductance, and (ii) any \((k+1)\)-way partition of \(\) would include some \(A V\) of high conductance, which would be implied by a lower bound on \(_{k+1}(_{})\) due to the higher-order Cheeger inequality. Together with the well-known eigen-gap heuristic  and theoretical analysis on spectral clustering , the two properties in Definition 1 ensures that spectral clustering returns approximately the same output from \(\) and \(\).2

Now we present the algorithm in  for constructing a cluster-preserving sparsifier, and we call it the SZ algorithm for simplicity. Given any input graph \(=(V,E,w_{})\) with weight function \(w_{}\), the algorithm computes

\[p_{u}(v)\{C}}(u,v)}{_{}(u)},1\}, p_{v}(u) \{C}}(v,u)}{_{}(v)},1\},\]

for every edge \(e=\{u,v\}\), where \(C^{+}\) is some constant. Then, the algorithm samples every edge \(e=\{u,v\}\) with probability

\[p_{e} p_{u}(v)+p_{v}(u)-p_{u}(v) p_{v}(u),\]

and sets the weight of every sampled \(e=\{u,v\}\) in \(\) as \(w_{}(u,v) w_{}(u,v)/p_{e}\). By setting \(F\) as the set of the sampled edges, the algorithm returns \(=(V,F,w_{})\) as output. It is shown in  that, with high probability, the constructed \(\) has \((n)\) edges and is a cluster-preserving sparsifier of \(\).

We remark that a cluster-preserving sparsifier is a much weaker notion than a spectral sparsifier, which approximately preserves all the cut values and the eigenvalues of the graph Laplacian matrices. On the other side, while a cluster-preserving sparsifier is sufficient for the task of graph clustering, the SZ algorithm runs in \((n^{2})\) time for a fully connected input graph, since it's based on the computation of the vertex degrees as well as the sampling probabilities \(p_{u}(v)\) for every pair of vertices \(u\) and \(v\).

## 4 Algorithm

This section presents our algorithm that directly constructs an approximation of a fully connected similarity graph from \(X^{d}\) with \(|X|=n\). As the main theoretical contribution, we demonstrate that neither the quadratic space complexity for directly constructing a fully connected similarity graph nor the quadratic time complexity of the SZ algorithm is necessary when approximately constructing a fully connected similarity graph for the purpose of clustering. The performance of our algorithm is as follows:

**Theorem 2** (Main Result).: _Given a set of data points \(X=\{x_{1},,x_{n}\}^{d}\) as input, there is a randomised algorithm that constructs a sparse graph \(\) of \(X\), such that it holds with probability at least \(9/10\) that_

1. _graph_ \(\) _has_ \((n)\) _edges,_
2. _graph_ \(\) _has the same cluster structure as the fully connected similarity graph_ \(\) _of_ \(X\)_; that is, if_ \(\) _has_ \(k\) _well-defined clusters, then it holds that_ \(_{}(k)=(k_{}(k))\) _and_ \(_{k+1}(_{})=(_{k+1}(_{ }))\)_._

_The algorithm uses an approximate KDE algorithm as a black-box, and has running time \((T_{}(n,n,))\) for \( 1/(6(n))\)._

### Algorithm Description

At a very high level, our designed algorithm applies a KDE algorithm as a black-box, and constructs a cluster-preserving sparsifier by simultaneous sampling of the edges from a _non-explicitly constructed_ fully connected graph. To explain our technique, we first claim that, for an arbitrary \(x_{i}\), a random \(x_{j}\) can be sampled with probability \(k(x_{i},x_{j})/_{}(v_{i})\) through \(O( n)\) queries to a KDE algorithm. To see this, notice that we can apply a KDE algorithm to compute the probability that the sampled neighbour is in some set \(X_{1} X\), i.e.,

\[[z X_{1}]=_{x_{j} X_{1}},x_{j})}{ _{}(v_{i})}=}(x_{i})}{g_{X}(x_{i})},\]

where we use \(g_{X}(y_{i})\) to denote that the KDE is taken with respect to the set of source points \(X\). Based on this, we recursively split the set of possible neighbours in half and choose between the two subsets with the correct probability. The sampling procedure is summarised as follows, and is illustrated in Figure 3. We remark that the method of sampling a random neighbour of a vertex in \(\) through KDE and binary search also appears in Backurs et al. 

1. Set the feasible neighbours to be \(X=\{x_{1},,x_{n}\}\).
2. While \(|X|>1\): * Split \(X\) into \(X_{1}\) and \(X_{2}\) with \(|X_{1}|=|X|/2\) and \(|X_{2}|=|X|/2\). * Compute \(g_{X}(x_{i})\) and \(g_{X_{1}}(x_{i})\); set \(X X_{1}\) with probability \(g_{X_{1}}(x_{i})/g_{X}(x_{i})\), and \(X X_{2}\) with probability \(1-g_{X_{1}}(x_{i})/g_{X}(x_{i})\).
3. Return the remaining element in \(X\) as the sampled neighbour.

Next we generalise this idea and show that, instead of sampling a neighbour of one vertex at a time, a KDE algorithm allows us to sample neighbours of every vertex in the graph "simultaneously". Our designed sampling procedure is formalised in Algorithm 1.

Finally, to construct a cluster-preserving sparsifier, we apply Algorithm 1 to sample \(O( n)\) neighbours for every vertex \(v_{i}\), and set the weight of every sampled edge \(v_{i} v_{j}\) as

\[w_{}(v_{i},v_{j})=,x_{j})}{(i,j)},\] (2)where \((i,j)_{i}(j)+_{j}(i)-_{i}(j) _{j}(i)\) is an estimate of the sampling probability of edge \(v_{i} v_{j}\), and

\[_{i}(j)\{6C n,x_{j})}{g _{[1,n]}(x_{i})},1\}\]

for some constant \(C^{+}\); see Algorithm 2 for the formal description.

```
1:Input: set \(S\) of \(\{y_{i}\}\)
2:set \(X\) of \(\{x_{i}\}\)
3:Output:\(E=\{(y_{i},x_{j})ij\}\)
4:if\(|X|=1\)then
5:return\(S X\)
6:else
7:\(X_{1}=\{x_{j}:j<|X|/2\}\)
8:\(X_{2}=\{x_{j}:j|X|/2\}\)
9: Compute \(g_{X_{1}}(y_{i})\) for all \(i\) with a KDE algorithm
10:\(S_{1}=S_{2}=\)
11:for\(y_{i} S\)do
12:\(r\)
13:if\(r\ \ g_{X_{1}}(y_{i})/(g_{X_{1}}(y_{i})\ +\ g_{X_{2}}(y_{i}))\)then
14:\(S_{1}=S_{1}\{y_{i}\}\)
15:else
16:\(S_{2}=S_{2}\{y_{i}\}\)
17:endif
18:endfor
19:return\((S_{1},X_{1})(S_{2},X_{2})\)
20:endif ```

**Algorithm 1**Sample

### Algorithm Analysis

Now we analyse Algorithm 2, and sketch the proof of Theorem 2. We first analyse the running time of Algorithm 2. Since it involves \(O( n)\) executions of Algorithm 1 in total, it is sufficient to examine the running time of Algorithm 1.

We visualise the recursion of Algorithm 1 with respect to \(S\) and \(X\) in Figure 4. Notice that, although the number of nodes doubles at each level of the recursion tree, the total number of samples \(S\) and data points \(X\) remain constant for each level of the tree: it holds for any \(i\) that \(_{j=1}^{2^{i}}S_{i,j}=S_{0}\) and

Figure 4: The recursion tree for Algorithm 1.

Figure 3: The procedure of sampling a neighbour \(v_{j}\) of \(v_{i}\) with probability \(k(x_{i},x_{j})/_{}(v_{i})\). Our algorithm performs a binary search to find the sampled neighbour. At each step, the value of two kernel density estimates are used to determine where the sample lies. Notice that the algorithm doesnâ€™t compute any edge weights directly until the last step.

\(_{j=1}^{2^{i}}X_{i,j}=X_{0}\). Since the running time of the KDE algorithm is superadditive, the combined running time of all nodes at level \(i\) of the tree is

\[T_{i}=_{j=1}^{2^{i}}T_{}(|S_{i,j}|,|X_{i,j}|,) T_{ }(_{j=1}^{2^{i}}|S_{i,j}|,_{j=1}^{2^{i}}|X_{i,j}|, )=T_{}(n,n,).\]

Hence, the total running time of Algorithm 2 is \((T_{}(n,n,))\) as the tree has \(_{2}(n)\) levels. Moreover, when applying the _Fast Gauss Transform_ (FGT)  as the KDE algorithm, the total running time of Algorithm 1 is \((n)\) when \(d=O(1)\).

Finally, we prove that the output of Algorithm 2 is a cluster preserving sparsifier of a fully connected similarity graph, and has \((n)\) edges. Notice that, comparing with the sampling probabilities \(p_{u}(v)\) and \(p_{v}(u)\) used in the SZ algorithm, Algorithm 2 samples each edge through \(O( n)\) recursive executions of a KDE algorithm, each of which introduces a \((1+)\)-multiplicative error. We carefully examine these \((1+)\)-multiplicative errors and prove that the actual sampling probability \((i,j)\) used in Algorithm 2 is an \(O(1)\)-approximation of \(p_{e}\) for every \(e=\{v_{i},v_{j}\}\). As such the output of Algorithm 2 is a cluster-preserving sparsifier of a fully connected similarity graph, and satisfies the two properties of Theorem 2. The total number of edges in G follows from the sampling scheme. We refer the reader to Appendix A for the complete proof of Theorem 2.

```
1:Input: data point set \(X=\{x_{1},,x_{n}\}\)
2:Output: similarity graph G
3:\(E=\), \(L=6C(n)/_{k+1}\)
4:for\([1,L]\)do
5:\(E=E(X,X)\)
6:endfor
7:Compute \(g_{[1,n]}(x_{i})\) for each \(i\) with a KDE algorithm ```

**Algorithm 2** FastSimilarityGraph

## 5 Experiments

In this section, we empirically evaluate the performance of spectral clustering with our new algorithm for constructing similarity graphs. We compare our algorithm with the algorithms for similarity graph construction provided by the scikit-learn library  and the FAISS library  which are commonly used machine learning libraries for Python. In the remainder of this section, we compare the following six spectral clustering methods.

1. SKLearn GK: spectral clustering with the fully connected Gaussian kernel similarity graph constructed with the scikit-learn library.
2. SKLearn \(k\)-NN: spectral clustering with the \(k\)-nearest neighbour similarity graph constructed with the scikit-learn library.
3. FAISS Exact: spectral clustering with the exact \(k\)-nearest neighbour graph constructed with the FAISS library.
4. FAISS HNSW: spectral clustering with the approximate \(k\)-nearest neighbour graph constructed with the "Hierarchical Navigable Small World" method .
5. FAISS IVF: spectral clustering with the approximate \(k\)-nearest neighbour graph constructed with the "Invertex File Index" method .
6. Our Algorithm: spectral clustering with the Gaussian kernel similarity graph constructed by Algorithm 2.

We implement Algorithm 2 in C++, using the implementation of the Fast Gauss Transform provided by Yang et al. , and use the Python SciPy  and stag  libraries for eigenvector computation and graph operations respectively. The scikit-learn and FAISS libraries are well-optimisedand use C, C++, and FORTRAN for efficient implementation of core algorithms. We first employ classical synthetic clustering datasets to clearly compare how the running time of different algorithms is affected by the number of data points. This experiment highlights the nearly-linear time complexity of our algorithm. Next we demonstrate the applicability of our new algorithm for image segmentation on the Berkeley Image Segmentation Dataset (BSDS) .

For each experiment, we set \(k=10\) for the approximate nearest neighbour algorithms. For the synthetic datasets, we set the \(\) value of the Gaussian kernel used by SKLearn GK and Our Algorithm to be \(0.1\), and for the BSDS experiment we set \(=0.2\). This choice follows the heuristic suggested by von Luxburg  to choose \(\) to approximate the distance from a point to its \(k\)th nearest neighbour. All experiments are performed on an HP ZBook laptop with an 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz processor and 32 GB RAM. The code to reproduce our results is available at https://github.com/pmacg/kde-similarity-graph.

### Synthetic Dataset

In this experiment we use the scikit-learn library to generate 15,000 data points in \(^{2}\) from a variety of classical synthetic clustering datasets. The data is generated with the make_circles, make_moons, and make_blobs methods of the scikit-learn library with a noise parameter of \(0.05\). The linear transformation \(\{\{0.6,-0.6\},\{-0.4,0.8\}\}\) is applied to the blobs data to create asymmetric clusters. As shown in Figure 1, all of the algorithms return approximately the same clustering, but our algorithm runs much faster than the ones from scikit-learn and FAISS.

We further compare the speedup of our algorithm against the five competitors on the two_moons dataset with an increasing number of data points, and our result is reported in Figure 5. Notice that the running time of the scikit-learn and FAISS algorithms scales roughly quadratically with the size of the input, while the running time of our algorithm is nearly linear. Furthermore, we note that on a laptop with 32 GB RAM, the SKLearn GK algorithm could not scale beyond 20,000 data points due to its quadratic memory requirement, while our new algorithm can cluster 1,000,000 data points in 240 seconds under the same memory constraint.

### BSDS Image Segmentation Dataset

Finally we study the application of spectral clustering for image segmentation on the BSDS dataset. The dataset contains \(500\) images with several ground-truth segmentations for each image. Given an input image, we consider each pixel to be a point \((r,g,b,x,y)^{}^{5}\) where \(r\), \(g\), \(b\) are the red, green, blue pixel values and \(x\), \(y\) are the coordinates of the pixel in the image. Then, we construct a similarity graph based on these points, and apply spectral clustering, for which we set the number of clusters to be the median number of clusters in the provided ground truth segmentations. Our experimental result is reported in Table 1, where the "Downsampled" version is employed to reduce the resolution of the image to 20,000 pixels in order to run the SKLearn GK and the "Full Resolution" one is to apply the original image of over 150,000 pixels as input. This set of experiments demonstrates our algorithm produces better clustering results with repsect to the average Rand Index .

Figure 5: Comparison of the running time of spectral clustering on the two moons dataset. In every case, all algorithms return the correct clusters.

## 6 Conclusion

In this paper we develop a new technique that constructs a similarity graph, and show that an approximation algorithm for the KDE can be employed to construct a similarity graph with proven approximation guarantee. Applying the publicly available implementations of the KDE as a black-box, our algorithm outperforms five competing ones from the well-known scikit-learn and FAISS libraries for the classical datasets of low dimensions. We believe that our work will motivate more research on faster algorithms for the KDE in higher dimensions and their efficient implementation, as well as more efficient constructions of similarity graphs.