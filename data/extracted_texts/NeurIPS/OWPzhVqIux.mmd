# Can Large Language Models Explore In-Context?

Akshay Krishnamurthy\({}^{1}\) Keegan Harris\({}^{2}\) Dylan J. Foster\({}^{1}\) Cyril Zhang\({}^{1}\) Aleksandrs Silvkins\({}^{1}\)

\({}^{1}\)Microsoft Research \({}^{2}\)Carnegie Mellon University

keeganh@cs.cmu.edu, {akshaykr,dylanfoster,cyrilzhang,slivkins}@microsoft.com

###### Abstract

We investigate the extent to which contemporary Large Language Models (LLMs) can engage in _exploration_, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple _multi-armed bandit_ environments, specifying the environment description and interaction history entirely _in-context_, i.e., within the LLM prompt. We experiment with Gpt-3.5, Gpt-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Only one configuration resulted in satisfactory exploratory behavior: Gpt-4 with chain-of-thought reasoning and an externally summarized interaction history; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. While these findings can be interpreted positively, they suggest that external summarization--which may not be possible in more complex settings--is essential for desirable LLM behavior. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.

## 1 Introduction

_In-context learning_ is an important emergent capability of Large Language Models (LLMs) whereby one can use a pre-trained LLM to solve a problem by specifying the problem description and relevant data entirely _in-context_, i.e., within the LLM prompt, with no updates to LLM parameters . For example, one can prompt an LLM with numeric covariate vectors and scalar targets and subsequently obtain regression-style predictions from the model by including new covariate vectors in the prompt . Perhaps surprisingly, LLMs are not explicitly trained for this behavior; instead the underlying algorithms employed for in-context learning are extracted from the training corpus and _emerge_ at scale.

Since its discovery in the Gpt-3 model , in-context learning has been actively studied, from theoretical investigations into the underlying mechanisms [e.g., 78, 7] to empirical probes [e.g., 28, 40] to leveraging in-context learning in applications [e.g., 79, 67, 25]. This literature predominantly concerns prediction or supervised learning tasks, and while theoretical progress is in its infancy, our understanding of how to use _in-context supervised learning_ (ICSL) in practice is rapidly taking shape.

While ICSL is an important capability, many applications demand the use of ML models for downstream _decision making_. Thus, _in-context reinforcement learning_ (ICRL) is a natural next frontier. LLMs are already being used as decision making agents in applications ranging from experimental design in the natural sciences  to game playing , but our understanding--theoretically and operationally--of ICRL is far less developed than for ICSL. To date, we lack a systematic understanding as to whether LLMs can be considered general-purpose decision-making agents.

Decision making agents must possess three core capabilities: _generalization_ (required for supervised learning), _exploration_ (making decisions that may be suboptimal in the short term for the sake of gathering more information) and _planning_ (to account for long-term consequences of decisions). In this paper, we focus on exploration, the capability to deliberately gather information in order to evaluate alternatives and reduce uncertainty. A recent series of papers [42; 44; 57] demonstrates in-context reinforcement learning behavior (including exploration) in transformer models when they are _explicitly trained_ to produce this behavior using data from reinforcement learning agents or expert demonstrations on related tasks. Such training tends to be laborious, expensive, and possibly task-specific. In particular, these findings do not shed light into whether exploratory behavior manifests in general-purpose LLMs obtained via standard training methods, which suggests the following basic question:

_Do contemporary LLMs exhibit the capability to explore in-context?_

**Contributions.** We investigate this question by deploying LLMs as agents in simple synthetic reinforcement learning problems, namely _multi-armed bandits (MABs)_[65; 43], specifying the environment description and interaction history entirely within the LLM prompt. MABs are a well-studied type of RL problem that isolates the tradeoff between exploration and _exploitation_, i.e., making the best decision given the available data. They are also fundamental in that the ability to solve MABs is a prerequisite for more challenging RL tasks. These considerations make MABs a natural choice for systematically studying the in-context exploration abilities of LLMs.

We evaluate the in-context exploration behavior of Gpt-3.5 , Gpt-4 , and Llama2  in MAB environments, using a variety of prompt designs. In our experiments, we find that only a single configuration (i.e., a prompt design and LLM pair) results in satisfactory exploratory behavior. All other configurations exhibit exploration failures, failing to converge to the best decision (_arm_) with significant probability. We find that this typically happens due to _suffix failures_, where the LLM fails to select the best arm even once after some initial rounds (i.e., in some "time suffix"). This scenario is reflected in Figure 1(a): in particular, Gpt-4 with our basic prompt design experiences a suffix failure in \(>60\%\) of the replicates. An alternative failure mode we identify is where the LLM behaves "uniformly", selecting all arms near-equally often and failing to narrow down to the better ones.

The single configuration that succeeds in our experiments involves a combination of Gpt-4 and an "enhanced" prompt that (a) provides a suggestive hint to explore, (b) externally summarizes the history of interaction into per-arm averages, and (c) asks the LLM to use zero-shot chain-of-thought reasoning [41; 74]. This configuration is visualized in Figure 1(b). One can interpret this finding positively: state-of-the-art LLMs _do_ possess the capability to robustly explore, provided that the prompt is carefully designed to elicit this behavior. On the other hand, the same configuration without external summarization fails, leading to a negative interpretation: LLMs may fail to explore in more complex environments, where external summarization is a non-trivial algorithmic problem.1

We conclude that while the current generation of LLMs can perhaps explore in simple RL environments with appropriate prompt engineering, training interventions --in the spirit of Lee et al. , Raparthy et al. -- may be required to endow LLMs with more sophisticated exploration capabilities required for more complex settings.

**Methodology.** An underlying technical challenge in assessing LLM capabilities and limitations is that one must search a combinatorially large space of prompt designs while obtaining statistically meaningful results, all while meeting the financial and computational constraints associated with LLMs. Assessing in-context bandit learning is even more challenging because (a) stochasticity in the environment demands a high degree of replication for statistical significance and (b) the sample complexity of learning/exploration demands that even a single experiment involve hundreds or thousands of LLM queries to obtain meaningful effect sizes (i.e., separation between successful and failing methods). To address these issues, our core technical contribution is to identify _surrogate statistics_ as diagnostics for long-term exploration failure. The surrogate statistics we consider characterize long-term exploration failure, yet can be measured at moderate scale with few replicates and short learning horizons, even when the standard performance measure (namely, reward) is too noisy to be useful.

## 2 Experimental setup

**Multi-armed bandits (MAB).** We consider a basic multi-armed bandit variant, _stochastic Bernoulli bandits_. There are \(K\) possible actions (_arms_), indexed as \([K]:=\{1,,K\}\). Each arm \(a\) is associated with mean reward \(_{a}\), which is unknown. An agent interacts with the environment for \(T\) time steps, where in each time step \(t[T]\) the agent selects an arm \(a_{t}[K]\) and receives a reward \(r_{t}\{0,1\}\) drawn independently from a Bernoulli distribution with mean \(_{a_{t}}\). Thus, the MAB instance is determined by the mean rewards \((_{a}:\ a[K])\) and the time horizon \(T\). The goal is to maximize the total reward, which roughly corresponds to identifying the _best arm_: an arm with the highest mean reward. A key feature of the MAB setup is that rewards for arms not chosen by the agent are not revealed, so exploration is necessary to identify the best arm.

We focus on MAB instances where the best arm has mean reward \(^{}=0.5+/2\) for a parameter \(>0\), while all other arms have mean reward \(=0.5-/2\) (so, \(=^{}-\) is the _gap_ between the best and the second-best arm). The main instance we consider has \(K=5\) arms and gap \(=0.2\). We call this the hard instance, as we also consider an easy instance with \(K=4\) and \(=0.5\).2

**Prompts.** We employ LLMs to operate as decision making agents that interact with MAB instances by prompting them with a description of the MAB problem (including the time horizon \(T\)) and the history of interaction thus far. Our prompt design allows several independent choices. First is a "scenario", which provides a grounding for the decision making problem, positioning the LLM either a) as an agent choosing _buttons_ to press, or b) as a recommendation engine displaying _advertisements_ to users. Second, we specify a "framing" as either a) explicitly _suggestive_ of the need to balance exploration and exploitation, or b) _neutral_. Third, the history can be presented as a) a _raw_ list over rounds, or it can b) be _summarized_ via number of plays and average rewards of each arm. Fourth, the requested final answer can be a) a single _arm_, or b) a _distribution_ over arms. Finally, we either a) request the answer only, or b) also allow the LLM to provide a "chain-of-thought" (CoT) explanation. Altogether, these choices lead to \(2^{5}=32\) prompt designs, illustrated in Figure 2. More details about the prompt design, including examples, are provided in Appendix B.

Figure 1: **Representative experiments:** Two prompt configurations for Gpt-4 on a \(5\)-armed bandit problem, with exploration failure (top) and success (bottom). The baselines are two standard bandit algorithms with performance guarantees, Upper Confidence Bound (UCB) and Thompson Sampling (TS), as well as the Greedy algorithm (see Footnote 5). Visualizations are: (Left) histogram over replicates of the number of times the best arm is chosen, (Center) for each \(t\), we plot the _suffix failure frequency_, the fraction of replicates for which the best arm is never chosen after time-step \(t\), and (Right) cumulative time-averaged rewards, averaged over replicates (\( 2\) standard errors).

The most basic prompt design from the options above uses the buttons scenario, neutral framing, and raw history, and requests the LLM to return only an arm with no CoT. Each of the five possible modifications to this prompt can potentially help the LLM, and our experiments evaluate this. For example, both the advertising scenario and suggestive framing might help invoke the LLM's knowledge of bandit algorithms (as bandit algorithms are commonly used in content recommendation). History summarization might help if the LLM cannot reliably summarize history itself (perhaps due to arithmetic errors3) and/or does not fully realize that it should. Returning a distribution might help if the LLM can identify a good distribution, but fails to correctly sample from it. Finally, chain-of-thought is known to help in a wide variety of LLM scenarios , even when used in a zero-shot manner  as we do here.

Prompts are presented to each LLM using both system and user messages (exposed by all three LLM APIs). The system message presents information about the scenario and framing and prompts the LLM about whether to use CoT and whether (and how) to return a distribution. The user message presents the history and reminds the LLM about how to format its response. For Gpt-4 only, we found that prompting the LLM to use CoT in the system prompt did not reliably elicit CoT outputs, so--for Gpt-4 only--we also consider a _reinforced CoT_ prompt design that additionally reminds the LLM to use CoT at the end of the user prompt. See Appendix B for examples.

LLM configurations and baselines.We experiment with three LLMs: Gpt-3.5, Gpt-4, and Llama2.4 In addition to the prompt variations above, we also consider two choices for the temperature parameter, \(0\) and \(1\). A temperature of \(0\) forces the LLM to be deterministic and therefore isolates the "deliberate" exploration behavior of the LLM itself. A temperature of \(1\) provides a source of external randomness in the LLM responses, which may or may not result in randomization among the arms. Allowing the LLM to return a distribution instead of a single arm also provides external randomness (as we sample from the returned distribution); to isolate sources of randomness, we do not consider temperature \(1\) with "return distribution" prompt designs.

We refer to the tuple (prompt design, temperature) as the _LLM configuration_. We identify each configuration with a 5-letter "code" \(L_{1}L_{2}L_{3}L_{4}L_{5}\), with letters \(L_{i}\) denoting the choices:

* \(L_{1}\): 'B' or 'A' for, resp., buttons or advertisements scenario;
* \(L_{2}\): 'N' or 'S' for, resp., neutral or suggestive framing;
* \(L_{3}\): 'R' or 'S' for, resp., raw or summarized history;
* \(L_{4}\): 'C' or 'C' or 'N' for, resp., chain-of-thought, reinforced CoT, or no CoT.
* \(L_{5}\): '0', '1' or 'D' for, resp., temperature and returning a distribution (with temperature \(0\)).

We refer to "BNRN0" as the _basic_ configuration going forward. Most of our experiments consider the "buttons" scenario, and we use the "advertisements" scenario primarily as a robustness check.

For Gpt-3.5 and Llama2, we do not consider reinforced CoT as it is not required to reliably elicit CoT outputs; thus, we have 48 configurations total. For Gpt-4, we primarily used reinforced CoT, but did experiment with some standard CoT prompt designs; thus, there are 72 configurations total.

For baselines, we consider two standard MAB algorithms, UCB  and Thompson Sampling (TS) , which are optimal in a certain theoretical sense and also reasonably effective in practice. We also consider the Greedy algorithm, which does not explore and is known to fail.5 While all three

Figure 2: Prompt designs; see Figure 9 for a more detailed view. A prompt is generated by traversing the graph from top to bottom.

baselines have tunable parameters, we perform no parameter tuning (see Section A.1 for a detailed description of each algorithm with parameter settings). In addition to these baselines, some of our experiments include the the \(\)-Greedy algorithm6 with various choices of \(\) to quantitatively demonstrate tradeoffs between exploration and exploitation. We ran \(1000\) replicates for each baseline and each MAB instance (with rewards realized independently across the replicates).

**Scale of the experiments.** Our main set of experiments has time horizon \(T=100\). To account for randomness in rewards (and possibly in the LLM, via temperature) we ran \(N\{10,20\}\) replicates for each LLM configuration and each bandit instance, with rewards generated independently across the replicates. As a robustness check, we ran a single experiment on Gpt-4 with the basic configuration for \(T=500\) rounds (with \(N=20\)), and obtained consistent/stronger conclusions, see Figure 1(a).

In more detail, for Gpt-3.5 we used \(N=20\) replicates across all \(48\) prompt configurations, resulting in \( 200K\) queries in total. Gpt-4 was an order of magnitude more expensive, considerably slower on throughput, and subject to unpredictable throttling. As such, we only used \(N=10\) replicates across \(10\) representative prompt configurations.7 For additional robustness checks, we ran four Gpt-4 configurations with \(T=200\), two for \(N=20\) replicates and two for \(N=40\) replicates. In total, this resulted in \(\)\(50K\) queries issued to Gpt-4. Llama2 was essentially free from our perspective (since it was locally hosted), but its performance was consistently sub-par; we limited our experiments to the hard MAB instance, \(32\) configurations, and \(N=10\) replicates.

We emphasize that bandit experiments with LLMs are quite costly in terms of money and time. They take \(N T\) LLM queries for each LLM configuration and each MAB instance being tested. Both \(N\) and \(T\) must be relatively large to obtain statistically meaningful results: \(N\) governs the significance level and must be large to overcome randomness in reward realizations, while \(T\) governs the effect size and must be large so that good algorithms have enough time to identify the optimal arm. Both issues are more pronounced in harder MAB instances (many arms \(K\) and/or small gap \(\)), but exploration failures also tend to be less frequent in (very) easy MAB instances. Further, we need to cover the space of possible prompt designs, which is essentially infinitely large, to ensure that our findings do not overfit to one particular design. Thus, ideally we would take \(N\), \(T\), the number of MAB instances, and the number of prompts to be rather large, but doing so is not practically feasible.8 Instead, we use moderately small gap \(=0.2\), moderately large choices for \(N\{10,20\}\) and \(T=100\), and the prompt design space as described above.

As we see below, these choices (\(N\{10,20\}\), \(T=100\), \(=0.2\)) do not provide enough statistical power to distinguish between successful and unsuccessful methods based solely on accumulated rewards. In lieu of further increasing the scale of the experiments, which is not practically feasible, we rely on _surrogate statistics_ which can be detected at our moderate scale, and are highly suggestive of long-term/persistent exploration failures. Our robustness checks with larger \(T\) and \(N\), as well as qualitative findings that we report below provide supporting evidence for this methodology.

## 3 Experimental results

In this section, we present our experimental findings, beginning with a summary. In Section 3.1 we investigate failing LLM configurations in detail. In Section 3.2, we focus on the single successful LLM configuration we identified. In Section 3.3, we attempt to diagnose root causes for failures.

**Overview.** All but one LLM configurations considered exhibit exploration failures, not converging to the best arm with significant probability. This happens either due to _suffix failures_, where the LLM never selects the best arm after a small number of initial rounds, or (in a few configurations) due to _uniform-like failures_, where the LLM selects all arms at an approximately uniform rate, failing to eliminate poorly performing arms. The one exception is Gpt-4 with the BSSC0 configuration, i.e., the buttons scenario, suggestive framing, summarized history, reinforced CoT, and temperature \(0\).

We summarize our key findings in Figures 3-4. Figure 3 summarizes the main set of experiments (on the hard MAB instance), mapping each LLM configuration to a single point on a scatter plot.

The axes correspond to two _surrogate statistics_, \(\) and \(K\), which represent the strength of the two failure modes (suffix failures and uniform-like failures), and are described in detail in the sequel. Figure 4 displays \(\), \(\), \(\) (which measures how similar a method is to Greedy), and additional summary statistics for each Gpt-4 configuration in the main set of experiments. These statistics reveal that all of the LLM configurations, except for Gpt-4-BSSC0 (the blue star in Figure 3), behave fundamentally differently from the baseline algorithms UCB and TS, and we find that these differences result in a large, persistent drop in performance. Conversely, we find that Gpt-4-BSSC0 successfully explores and (hence) converges to the best arm.

### Identifying failures

We now give a precise overview of the exploration failures illustrated in Figure 3 and Figure 4, and provide additional results and figures that illustrate failure in greater detail. We focus on Gpt-4, as Gpt-3.5 and Llama2 perform worse (and often _much_ worse) in all experiments; detailed results for Gpt-3.5 and Llama2 are included in Appendix C. We begin with detailed background on the surrogate statistics, \(\) and \(\), used to quantify failures in Figures 3 and 4 and beyond, providing evidence that exploration failure--as quantified by these statistics--results in a persistent drop in performance.

**Suffix failures.** Most of the LLM configurations we consider exhibit highly _bimodal_ behavior, whereby a large fraction of the replicates choose the best arm very rarely, and a few replicates converge to the best arm extremely quickly. Consistent with this bimodal behavior, we observe a large incidence of _suffix failures_, where the best arm is not selected even once after a small number initial of rounds (i.e., in some "time suffix"). Suffix failures are suggestive of a long-term failure to explore which cannot be improved by running the algorithm for longer, because, without playing the optimal arm, one cannot acquire information to learn that it is indeed optimal. Such behaviors are qualitatively similar to those of Greedy and qualitatively very different from those of UCB and Thompson Sampling.

Our surrogate statistic for measuring suffix failures is defined as follows: For an experiment replicate \(R\) and round \(t\), let \((t,R)\) be a binary variable that is \(1\) if the best arm is never chosen in rounds \([t,T]\). Then let \((t):=(\{(t,R):R\})\). Suffix failures manifest in most of our experiments at \(T=100\). In the scatter plot in Figure 3, the X-axis plots \((T/2)\) for each LLM configuration, and we find that all but five configurations

Figure 4: Gpt-4 for \(T=100\): a per-configuration **summary table** on the hard MAB instance with \(N=10\) replicates. Only three Gpt-4 configurations do not exhibit suffix failures; two of these (BNRND and BSSCD) exhibit uniform-like failures. The final configuration (BSSC0) succeeds.

Figure 3: Scatter plot summarizing all experiments with \(T=100\). We plot suffix failures (via \((T/2)\)) vs. uniform-like failures (via \(K(T)\)). Each LLM/configuration pair maps to a dot (some dots overlap). The only successful Gpt-4 configuration (BSSC0) is labeled with a star. We also plot \(\)-Greedy, tracing out the tradeoffs for different \(\).

have \((T/2) 15\%\). Recalling the definition of suffix failures, this means that \( 15\%\) of the time, these configurations do not pull the best arm _even once_ in the last half of the rounds.

A more detailed view of suffix failures and bimodal behavior can be obtained by focusing on individual LLM configurations. We visualize this for the basic configuration (Gpt-4-BNRN0) in Figure 1 (top) for \(T=500\), and in Figure 5 for Gpt-4 (BNRN0 and BNRN1) at \(T=100\). In these detailed views, the middle panels plot \((t)\) at each time \(t\) for the given LLM configurations, as well as UCB, TS, and Greedy. We find that these LLM configurations have much higher suffix failure rates than both UCB and TS. Bimodal behavior is visualized in the left panel of each plot, where for each configuration, a large fraction of replicates rarely pulls the best arm, while the remaining fraction almost always pulls the best arm. Because of this bimodal behavior (particularly because a constant fraction of replicates by chance almost always pull the best arm), suffix failures are not fully reflected in the total reward plots in the right panels of Figure 5, since the time horizon \(T=100\) is not large enough. However, as mentioned, suffix failures are suggestive of an irrecoverable failure to explore which leads to stark differences in reward for larger \(T\). This is precisely what we find at \(T=500\) in Figure 1, which suggests that suffix failures indeed lead to poor long-term performance.

**Uniform-like failures.** Returning to the left panel of Figure 3, we see that three Gpt-4 configurations avoid suffix failures. Two of these configurations exhibit a different type of failure, where the LLM selects arms in roughly equal proportions for the entirety of the \(T\) rounds and fails to exploit the acquired information to focus on the better arms. We call this a _uniform-like failure_.

Our surrogate statistic for measuring such failures is defined as follows: For a particular experiment replicate \(R\) and round \(t\), let \(f_{a}(t,R)\) be the fraction of rounds in \([1,t]\) in which a given arm \(a\) is chosen, \((t,R):=_{a}f_{a}(t,R)\), and \((t):=(\{(t,R):R\})\). Since \((t) 1/K,\  t[T]\), we always \(\ K(t)\), so as to rescale the range to \(\). Larger \((t)\) corresponds to a more uniform selection of arms at time \(t\). When an LLM's \((t)\) does not decrease over time and stays substantively larger than that of the baselines (especially as \(t\) approaches the time horizon \(T\)), we take it as an indication of a uniform-like failure.

The Y-axis of Figure 3 records \(K(T)\) for each configuration, where we see that of the three Gpt-4 configurations that avoid suffix failures, two configurations have very \(\,(T)\) relative to UCB and TS (the third configuration is Gpt-4-BSSC0, which is successful). These two

Figure 5: Bimodal behavior and suffix failures for Gpt-4 with \(T=100\), same visualizations as in Figure 1. Shown: the basic configuration (BNRN0) and the ablation with temperature \(1\) (BNRN1).

Figure 6: Detailed view of uniform-like failures for Gpt-4 (the BNRND and BNNSD configurations) with \(T=200\). Visualizations are: (Left) suffix failure frequency, (Center) \(K(t)\) as a function of \(t\) and (Right) cumulative time-averaged rewards. These configurations exhibit uniform-like failures but not suffix failures, and uniform-like failures are detrimental to long-term rewards.

configurations are Gpt-4-BNRND and Gpt-4-BSSCD, both of which use the _distributional_ output format. We provide a more detailed view of Gpt-4-BNRND (as well as Gpt-4-BNNND, which also exhibits uniform-like failures, but only differs from Gpt-4-BNRND in the use of summarized history) in Figure 6, which considers a longer horizon and more replicates (\(T=200\) and \(N=20\)). The middle panel reveals that \(K(t)\) does not decrease over time for these LLM configurations, while it does for the baselines. This behavior results in no suffix failures, but leads to much lower reward than the baselines. In particular, we obtain a clear separation in total reward, showing that uniform-like failures indeed result in poor long-term performance.

**Generality of the failures.** To summarize, Figure 3 shows that all LLM configurations except Gpt-4-BSSC0 exhibit either a suffix failure or a uniform failure for the hard MAB instance and the buttons scenario. Scatter plots for the other three experiments (i.e., the advertisements scenario and/or the easy MAB instance) are qualitatively similar and are deferred to Appendix C.

The same data, but with attributions to specific LLM configurations, are presented for _all_ Gpt-4 configurations in Figure 4; analogous tables for other LLMs and experimental settings are given in Appendix C. As it is not instructive to present detailed plots such as Figure 5 for every LLM configuration, Figure 4 summarizes the performance of each configuration with just a few statistics. We include: \((T/2)\) and \((T)\), defined above; \(\): the rescaled median (over replicates) of the time-averaged total reward;9 Greedyfrac: the fraction of _greedy rounds_ (where an arm with a largest average reward is selected), averaged over the replicates. Greedyfrac is one way to quantify the extent to which a configuration behaves like Greedy.

We now summarize further findings from the scatter plots (Figures 3 and 10) and the summary tables (Figures 11 to 17). First, Gpt-4 performs much better than Gpt-3.5, and Llama2 performs much worse (in particular, the suffix failure frequency for Llama2 ranges from that of Greedy to much larger). Second, we observe that all LLMs are sensitive to small changes in the prompt design. However, the different modifications we consider appear to interact with each other, and it is difficult to identify which individual modifications improve performance and which degrade it.

### Investigating successes

On the hard MAB instance, the only configuration in our experiments that avoids both suffix failures and uniform-like failures is Gpt-4 with the BSSC0 prompt design. As can be seen from Figure 4, at \(T=100\), this configuration has no suffix failures, the \(K\) value is only slightly larger than TS, and the reward is comparable to TS. These statistics suggest that this configuration succeeds.

For more statistically meaningful results supporting this claim, we run Gpt-4-BSSC0 on the hard MAB instance with \(T=200\) and \(N=40\). We also consider Gpt-4-BSRC0, which swaps summarized history for raw history, as an ablation. Figure 7 summarizes this experiment, while Figure 1(b) provides a detailed view of the BSSC0 configuration. We see that BSSC0 continues to avoid suffix failures and perform relatively well in terms of reward for larger \(T\). On the other hand, the ablation BSRC0 exhibits a non-trivial fraction of suffix failures, a fundamentally different behavior.

We provide additional visualizations with some qualitative evidence toward the success of BSSC0, as well as the failure of other configurations. In Figure 8, we plot the fraction of rounds in \([0,t]\) where the optimal arm was pulled; we plot this for individual replicates, as a function of \(t\). BSRC0 is

Figure 7: Summary statistics of two Gpt-4 configurations with reinforced CoT (BSRC0 and BSSC0), on the hard MAB instance with \(T=200\) and \(N=40\) replicates. BSRC0 shows suffix failures. BSSC0 has neither suffix nor uniform-like failures and reasonable reward.

visually similar to UCB, except that a non-trivial fraction of runs exhibit suffix failures (the curves that converge to \(0\) on the plot). Meanwhile, BSSC0 is visually similar to TS, with almost all replicates slowly converging to \(1\). Another visualization, presented in Appendix D, shows the arm chosen at each time step for particular replicates; this is for several LLM configurations ("successful" and not), as well as the baselines. These visualizations, along with the summary statistics, suggest that BSSC0 behaves most similarly to TS, which further suggests a similar convergence in the long run.

### Root causes

_Why_ do LLMs behave the way they do? Particularly, can one explain their failures via flaws in their _per-round_ decisions? Two natural hypotheses are that the failing LLM configurations are either a) too greedy, or b) too uniform-like. Indeed, most GPT-4 configurations behave much like Greedy on the easy MAB instance; yet, they avoid suffix failures and accrue large rewards, and so does Greedy. However, on the hard instance, most GPT-4 configurations seem to be doing something non-trivial.

A secondary experiment studies this further: Each agent (LLM or baseline) faces a "data source" (distribution of bandit histories) and makes a single decision. We used GPT-3.5 and several data sources. We find it difficult to separate LLMs from the baselines based on the per-round performance, as the latter is very sensitive to the data source. While a deeper investigation is needed, we report this difficulty as a non-trivial finding. All these results are discussed in Appendix E.

## 4 Discussion and open questions

Our investigation suggests that contemporary LLMs do not robustly engage in exploration required for very basic statistical RL and decision making problems, at least without further intervention. Let us identify several natural next steps. First, _experiment with other prompts:_ as in many other settings , small changes to our prompt template might improve performance; but sensitivity to prompt design is already concerning. Second, _experiment with few-shot prompting,_ where the prompt contains examples of exploratory behavior, or use such examples to _fine-tune_ the LLM. Third, _train the LLM to use auxiliary tools,_ such as a calculator for basic arithmetic or a "randomizer" to correctly sample from a distribution. We emphasize that cost, access to models, and compute pose significant barriers to further study, particularly because of the need to employ long horizons \(T\) and many replicates \(N\) to obtain statistically meaningful results. To this end, we believe that further methodological and/or statistical advancements to enable cost-effective diagnosis and understanding of LLM-agent behavior (e.g., our surrogate statistics) are essential.

**Implications for more complex problems.** Our focus on simple MAB problems provides a clean and controllable experimental setup to study the exploratory behavior of LLMs. Exploration failures here suggest that similar failures will also occur in more complex RL and decision-making settings. On the other hand, mitigations must be developed with caution, as solutions that succeed for the MAB setting may not generalize to more complex settings. For example, while GPT-4 with summarized interaction history and reinforced CoT seems to successfully explore in our MAB setting, it is not clear how one should externally summarize the history in settings with complex, high-dimensional observations such as contextual bandits (see Footnote 1). Indeed, even for linear contextual bandits, the approach may not be applicable without a substantial algorithmic intervention (such as, e.g., a linear regression computed externally and included in the prompt) and the many explicit modeling and algorithmic choices involved therein. We believe a deeper investigation of algorithmic interventions is essential to understand the extent to which LLMs can operate as decision-making agents.

Figure 8: Per-replicate behavior: two reinforced-CoT GPT-4 configurations & the baselines. For each algorithm, replicate and round \(t\), we plot the fraction of rounds in \([0,t]\) when the best arm was pulled.