# One-Layer Transformer Provably Learns One-Nearest Neighbor In Context

Zihao Li\({}^{1}\) Yuan Cao\({}^{2}\) Cheng Gao\({}^{1}\) Yihan He\({}^{1}\) Han Liu\({}^{3}\)

Jason M. Klusowski\({}^{1}\) Jianqing Fan\({}^{1}\)1 Mengdi Wang\({}^{1}\)1

\({}^{1}\)Princeton University \({}^{2}\)The University of Hong Kong \({}^{3}\)Northwestern University

{zihaoli,chenggao,yihan.he,jason.klusowski,jqfan,mengdiw}@princeton.edu

yuancao@hku.hk hanliu@northwestern.edu

Equal Contribution.

###### Abstract

Transformers have achieved great success in recent years. Interestingly, transformers have shown particularly strong in-context learning capability - even without fine-tuning, they are still able to solve unseen tasks well purely based on task-specific prompts. In this paper, we study the capability of one-layer transformers in learning one of the most classical nonparametric estimators, the one-nearest neighbor prediction rule. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we show that, although the loss function is nonconvex when trained with gradient descent, a single softmax attention layer can successfully learn to behave like a one-nearest neighbor classifier. Our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms, and sheds light on the role of softmax attention in transformer models.

## 1 Introduction

Transformers have emerged as one of the most powerful machine learning models since its introduction in Vaswani et al. (2017), achieving remarkable success in various tasks, including natural language processing (Devlin et al., 2018; Achiam et al., 2023; Touvron et al., 2023), computer vision (Dosovitskiy et al., 2020; He et al., 2022; Saharia et al., 2022), reinforcement learning (Chen et al., 2021; Janner et al., 2021; Parisotto et al., 2020), and so on. One intriguing aspect of transformers is their exceptional In-Context Learning (ICL) capability (Garg et al., 2022; Min et al., 2022; Wei et al., 2023; Von Oswald et al., 2023; Xie et al., 2021; Akyurek et al., 2022). It has been observed that transformers can effectively solve unseen tasks solely relying on task-specific prompts, without the need for fine-tuning. However, the underlying mechanisms and reasons behind the exceptional in-context learning capability of transformers remain largely unexplored, leaving a significant gap in our understanding of how and why transformers can be pretrained to exhibit such remarkable performance.

Several recent studies have attempted to understand in-context learning (ICL) through the lens of learning specific function classes. Notably, Garg et al. (2022) proposed a well-defined approach: the training data includes a demonstration prompt, consisting of a sequence of labeled data and a new unlabeled query. The in-context learning performance of a transformer is then evaluated based on its ability to successfully execute a machine-learning algorithm to predict the query data label using the prompt demonstration (i.e., the context). Based on such definition, several works such as Zhang et al. (2023); Huang et al. (2023); Chen et al. (2024) investigated ICL the optimization dynamicsof transformers under in-context learning from a theoretical lens, but their studies are limited to linear regression prediction rules, which is a significant simplification of the transformer in-context learning task. Another line of work including Bai et al. (2024); Akyurek et al. (2022) investigated the expressiveness of transformers in context, but no optimization result is guaranteed. Whether transformers can handle more complicated ICL tasks under regular gradient-based training is still, in general, unknown.

In this paper, we examine the ability of single-layer transformers to learn the one-nearest neighbor prediction rule. Our major contributions are as follows:

* We establish convergence guarantees as well as prediction accuracy guarantees of a single-layer transformer in learning from examples of one-nearest neighbor classification. Utilizing the softmax attention layer, we demonstrate that the training loss can be minimized to zero despite the highly non-convex loss function landscapes. We further justify our results with numerical simulations.
* Based on the optimization results, we further establish a behavior guarantee for the trained transformer, demonstrating its ability to act like a 1-NN predictor under data distribution shift. Our result thus serves as a concrete example of how transformers can learn nonparametric methods, surpassing the scope of previous literature focusing on linear regression.
* In our technical analysis, we make the key observation that although the transformer loss is highly nonconvex when learning from one-nearest neighbor, its optimization process can be controlled by a two-dimensional dynamic system when choosing a proper initialization. By analyzing the behavior of such a system, we establish the convergence result despite the curse of nonconvexity.

To summarize, our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms and sheds light on the role of softmax attention in transformer models. To our knowledge, this is the first paper that establishes a provable result in both optimization and consecutive behavior under distribution shift for a softmax attention layer beyond the scope of linear prediction tasks.

## 2 Preliminaries

In this section, we introduce the in-context learning data distribution based on the one-nearest neighbor data distribution and the setting of one-layer softmax attention transformers. Then, we discuss the training dynamics of transformers based on gradient descent.

### In-Context Learning Framework: One-Nearest Neighbor

In an In-Context Learning (ICL) instance, the model is given a prompt \(\{(_{i},_{i})\}_{i[N]}_{}\) and a query input \(_{N+1}_{}\) from some data distributions \(_{}\) and \(_{}\), where \(\{_{i}\}_{i[N]}\) are the input vectors, \(\{_{i}\}_{i[N]}\) are the corresponding labels (e.g. real-valued for regression, or \(\{+1,-1\}\)-valued for binary classification), and \(_{N+1}\) is the query on which the model is required to make a prediction. Given a prompt \(\{(_{i},_{i})\}_{i[N]}\), the prediction task is to predict an ground truth model \(f(_{N+1};\{(_{i},_{i})\}_{i[N]})\) that maps the query token \(_{N+1}\) to a real number.

In this work, we consider using transformers as the model to perform in-context learning. For a prompt \(\{(_{i},_{i})\}_{i[N]}\) of length \(N\) and a query token \(_{N+1}\), we consider use the following embedding:

\[=[_{1},_{2},,_{N+1}]= _{1}&_{2}&&_{N}&_{N+1}\\ _{1}&_{2}&&_{N}&0\\ 0&0&&0&1^{(d+2)(N+1)}.\] (2.1)

We use the notation of \(_{j}=[_{j},_{j},0]\) for \(j N\), and \(_{N+1}=[_{N+1},0,1]\). Here, \(\{_{i}\}_{i[N]}\) represents the input vectors, each associated with a corresponding label \(\{_{i}\}_{i[N]}\), where \(_{i}\) is the label. Throughout this paper, the sequence \(\{(_{i},_{i})\}_{i[N]}\) are referred to as the _context_ or _prompt_ exchangeably. The \((d+2)\)-th row serves as the indicator for the training token, which equals to \(0\) value for \(i[N]\) and \(1\) for \(i=N+1\), analogous to a positional embedding vector. Such an indicator allows the model to distinguish the query token from the context. Similar models have been studied in a line of recent works (Zhang et al., 2023; Huang et al., 2023; Chen et al., 2024; Bai et al., 2024; Akyurek et al., 2022) studying in-context learning of linear regression tasks.

Throughout this work, we focus on the case where the ground-truth prediction \(f(_{N+1};\{_{i},_{i}\}_{i[N]})\) of the training data is constructed based on a One-Nearest Neighbor (1NN) data distribution, defined by the following definition.

**Definition 1** (One-Nearest Neighbor Predictor).: _Given a prompt \(\{(_{i},_{i})\}_{i[N]}\) and a query \(_{N+1}\), we define the one-nearest neighbor predictor by_

\[_{i^{*}}:=_{i=1}^{N}(i=*{ argmin}_{j[N]}\|_{N+1}-_{j}\|_{2})_{i}.\]

_We also define \(i^{*}=*{argmin}_{i[N]}\|_{N+1}-_{i}\|_{2}\)._

Without loss of generality, we assume that \(*{argmin}_{j[N]}\|_{N+1}-_{j}\|_{2}\) is unique. Such assumption holds almost surely whenever \(\{_{i}\}_{i[N]}\) is sampled from a continuous distribution. Notably, for a fixed prompt \(\{(_{i},_{i})\}_{i[N]}\) and query \(_{N+1}\), Definition 1 is identical to the nonparametric one-nearest neighbor estimator (Peterson, 2009; Beyer et al., 1999), in which the algorithm outputs the label corresponding to the vector closest to the input, with the prompt \(\{(_{i},_{i})\}_{i[N]}\) as the training data in 1-NN.

Next, we discuss the distribution of the training dataset \(\{(_{i},_{i})\}\{_{N+1}\}\). Throughout the training process, we focus on the case in which \(\{_{i}\}_{i[N+1]}\) are independently sampled from a uniform distribution on a \(d-1\)-dimensional sphere \(^{d-1}\), \(\{_{i}\}_{i[N]}\) is a zero-mean binary noise taken value in \(\{+1,-1\}\), with \(\{_{i}\}_{i[N+1]}\) and \(\{_{i}\}_{i[N]}\) being independent. Our data distribution assumption can be summarized formally by the following assumption:

**Assumption 1** (Training Distribution).: _For an embedding \(\) defined by Eq. (2.1), we focus on the following underlying training distribution: (i) The sequence \(\{_{i}\}_{i[N+1]}\) are sampled independently from a uniform distribution on a \(d-1\) dimensional sphere \(^{d-1}^{d}\). (ii) The labels \(\{_{i}\}_{i[N]}\) satisfies \([_{i}_{j}|_{1:N}]=0\) and \([_{i}^{2}|_{1:N}]=1\) for all \(i j,i,j[N]\). (iii) We have \((_{1:N}|_{1:N})=(_{1:N}|- _{1:N})\)._

Note that the case when \(\{_{i}\}_{i[N]}\) and \(\{_{i}\}_{i[N]}\) being independent when \(\{_{i}\}_{i[N]}\), with \(\{_{i}\}_{i[N]}\) are uniformly sampled from the sphere and \(\{_{i}\}_{i[N]}\) are randomly sampled from \(\{ 1\}\) is an example of Assumption 1. We remark that by considering the training data distribution in Assumption 1, we aim to study the capability of transformers in learning one-nearest neighbor prediction rules starting from the cleanest possible setting. Despite the seemingly simple problem setting, we would like to point out that this data distribution is still challenging to study, especially because of the assumption that the second order moment of \(\{_{i}\}_{i[N]}\) and \(\{_{i}\}_{i[N+1]}\) are uncorrelated. Due to

Figure 1: Illustration of data distribution in Assumption 1 on \(^{2}\) and the corresponding ground-truth division of \(^{2}\) generated by one-nearest neighbor. (1) In the left panel, the red and blue points correspond to the \(_{i}\) with \(_{i}=1\) and \(-1\) for \(i[N]\), respectively, with \(N=500\). (2) In the right panel, the color of every point on the sphere is the same as its closest neighbor in \(\{_{i}\}_{i[N]}\). The sphere is thus split into divisions by the one-nearest-neighbor decision rule.

such uncorrelation, the classifier given by one-nearest neighbor models are rather complicated. For example, Fig. 1 illustrates the randomly generated context data and the corresponding one-nearest neighbor prediction regions with \(d=3\) and \(N=500\), which clearly demonstrate the complexity of the training task. This further leads to a highly nonconvex and irregular objective function landscape, illustrated by Fig. 2.

### One-Layer Softmax Attention Transformers

We consider a simplified version of the one-layer transformer architecture (Vaswani et al., 2017) that processes any input sequence \(\) defined by Eq. (2.1) and outputs a scalar value:

\[_{W}=(^{}_{K }^{}_{Q}),\] (2.2)

where \(()\) applies softmax operator on each column of the matrix \(\), i.e. \([()]_{ij}=(A_{ij})/_{i}(A_{ij})\). Our model is slightly different from the standard self-attention transformers, as we consider a frozen value matrix. However, we also claim that such practice is common in deep learning theory Fang et al. (2020); Lu et al. (2020); Mei et al. (2018). We also merge the query and key matrices into one matrix denoted as \(\), which is often taken in recent theoretical frameworks (Zhang et al., 2023; Huang et al., 2023; Jelassi et al., 2022; Tian et al., 2023). The output of the model is defined by the \((d+1)\)-th element of the last column of \(_{}\), with a closed form:

\[}_{}(_{N+1};\{_{i},_{i}\}_{i[N]}):=[_{}]_{(d+1,N+1)}=^{N} _{j}(_{j}^{}_{N+1})}{_{j=1 }^{N+1}(_{j}^{}_{N+1})}.\] (2.3)

which is the weighted mean of \(_{1},,_{N}\). Here and after, we may occasionally suppress dependence on \(\{_{i},_{i}\}_{i[N]}\) and write \(}_{}(_{N+1};\{_{i}, _{i}\}_{i[N]})\) as \(}_{}(_{N+1})\). Since the prediction takes only one entry of the token matrix output by the attention layer, actually only parts of \(\) affect the prediction. To see this, we denote

\[=_{11}&_{12}&_{13}\\ _{21}&_{22}&_{23}\\ _{31}&_{32}&_{33},\] (2.4)

with \(_{11}^{d d},_{21}^{1  d},_{31}^{1 d},_{12}^{d 1},_{13}^{1 d}\), \(_{22},_{23},_{32}\) and \(_{33}\). Then by Eq. 2.3, it is easy to see that \(_{i2}\) does not affect \(}_{}\) for \(i\), which means we can simply take all these entries as zero in the following sections. Notably, for a fixed prompt-query pair \(\{(_{i},_{i})\}_{i[N]}\) and \(\{_{N+1}\}\), such an architecture allows an arbitrarily close approximation to the 1-NN model: consider \(_{11}^{k}=_{1}^{k}I_{d}\) with \(_{1}^{k}\) goes to positive infinity, \(_{33}^{k}=_{2}^{k}\) such that \(_{2}^{k}-_{1}^{k}\) converges to infinity, with the rest of \(_{ij}^{k}\) bounded, then \(}_{^{k}}(_{N+1}))\) converges to \(_{i^{*}}\) as \(k\) goes to infinity.

Figure 2: Heatmap and landscape of loss function of single layer transformer when learning from one-nearest neighbor. The loss is defined in Eq. (2.5), generated by sampling 100 training sequences according to Assumption 1, with \(d=N=4\). We parametrize \(\) as \(\{_{1},,_{1},0,_{2}\}\).

### Training Dynamics

To train the transformer model over the 1-NN task, we consider the Mean-Square Error (MSE) loss function. Specifically, the loss function is defined by

\[L()=_{\{_{i},_{i}\}_{i[N]},_{N+1}}}(_{N+1 })-_{i^{*}}^{2}.\] (2.5)

Above, the expectation is taken with respect to the sampled prompt \(\{(_{i},_{i})\}_{i[N]}\) and the query \(_{N+1}\). Notably, when the underlying distribution for the prompt and query are defined by Assumption 1, this loss function is nonconvex with respect to \(\). Such nonconvexity makes the optimization hard to solve without further conditions such as PL condition or KL condition (Bierstone and Milman, 1988; Karimi et al., 2020). We leave the proof of nonconvexity in Appendix E.

We shall consider the behavior of gradient descent on the single-layer attention architecture w.r.t. the loss function in Eq. (2.5). The parameters are updated as follows:

\[^{k+1}-^{k}=_{}L( ^{k}).\] (2.6)

We shall consider the following initialization for the gradient descent:

**Assumption 2** (Initialization).: _Let \(>0\) be a parameter. We assume the following initialization:_

\[^{0}=0_{(d+1)(d+1)}&0_{d+1}\\ 0_{d+1}&-,\]

Here the parameter \(\) is similar to masking, which is widely applied in self-attention training process, and prevents the model from focusing on the zero-label for the query \(_{N+1}\), e.g. Vaswani et al. (2017); Baade et al. (2022); Chang et al. (2022). The reason we take the zero initialization for non-diagonal entries will be made clear when we describe the proof in Section 4. However, from a higher view, it is because we want to keep the model focusing on the inner product between different \(_{i}\), which largely reduces the complexity of the dynamic system under gradient descent and makes it tractable. We leave the question of convergence under alternative random initialization schemes for future work.

## 3 Main Results

In this section, we summarize the convergence of training loss and testing error respectively. In Section 3.1, we discuss the convergence of training loss under gradient descent. Specifically, we prove that with a proper initialization constant \(\), gradient descent is able to minimize the loss function \(L()\) despite the nonconvexity. In Section 3.2, we further discuss the testing error of the trained transformer under distribution shift. Specifically, we consider a distribution \(_{}\) for the prompt \(\{(_{i},_{i})\}_{i[N]}\{_{N+1}\}\), which is different from the training data distribution \(_{}_{}\), and discuss the difference between the trained transformer and 1-NN predictor under such distribution shift.

### Convergence of Gradient Descent

First, we prove that under suitable initialization parameter \(\), the loss function will converge to zero under gradient descent.

**Theorem 1** (Convergence of Gradient Descent).: _Consider performing gradient descent of the softmax-attention transformer model \(}_{}(_{N+1})\). Suppose the initialization satisfies Assumption 2 with \(>2(\{(Nd),-1-(N)^{},C_{d} 1-}\})\), where \(C_{d}=(d)\), and the number of context \(N O d\), then \(L(^{k})\) converges to 0._

We leave the detailed proof in Appendix C. Theorem 1 shows that for the 1-NN data distribution, with a large enough initialization constant \(\), the training loss of the transformer converges to zero under gradient descent. Here \(\) plays a role similar to the masking techniques in the self-attention training training process, in which \(\) is often set as infinity or an extremely large number. Such a technique has been widely accepted and shown to greatly accelerate the training process Vaswani et al. (2017); Devlin et al. (2018); Dosovitskiy et al. (2020). We also compare our results to existingworks. Zhang et al. (2023) studied linear prediction tasks under gradient flow, however, their analysis is limited to linear attention layers. Huang et al. (2023) was the first to study softmax attention optimization under gradient descent, but their prediction is limited to linear prediction tasks under a finite orthogonal dictionary. Chen et al. (2024) established optimization convergence results for one-layer multi-head attention transformers under gradient flow. On the contrary, our work studies gradient descent convergence for transformer under a nonparametric estimator, setting it apart from all previous studies.

### Results for New Task under Distribution shift

In this section, we discuss the behavior of trained transformers under distribution shifts, i.e., how the model _extrapolate_ beyond the training distribution. Following the definition in Garg et al. (2022), let us assume in the training process, the prompts \(\{(_{i},_{i})\}_{i[N]}_{}^{ }\), and the query \(_{N+1}_{}^{}\). During inference, the prompts and queries are sampled from a new distribution \(^{}\). We study the behavior of the trained transformers under possible prompt and query shift, i.e. \(^{}_{}^{} _{}^{}\). Our studies show that, under some mild conditions, the behavior of the trained model is still similar to a 1-NN predictor even under a distribution shift. Before formally stating our result, let us introduce the following assumption on the testing distribution:

**Assumption 3** (Testing Distribution).: _We make the following assumption on \(^{}\):_

1. _There exists a_ \(R 0\) _such that_ \(|_{i}| R\) _holds for all_ \(_{i}\) _sampled from_ \(^{}\)_._
2. _For all_ \(\{(_{i},_{i})\}_{i[N]}\{_{N+1}\} ^{}\)_, we have_ \(_{i}^{d-1}\) _for all_ \(i[N+1]\)_._

Note that Assumption 3 only requires the label \(_{i}\) is bounded and \(_{i}\) is supported on a sphere. We also remind the reader that we do not assume independence between different \(_{i}\) or \(\{_{i}\}_{i[N+1]}\) and \(\{_{i}\}_{i[N+1]}\). Now we are ready to summarize our result in the following theorem.

**Theorem 2** (Resemblance to 1-NN predictor under Distribution Shift).: _Suppose Assumption 1 and 3 hold for \(_{}^{}_{}^{ }\) and \(^{}\). If we define_

\[A_{}:=\{\|_{j}-_{N+1}\|_{2}^{2}\|_{i^{ *}}-_{N+1}\|_{2}^{2}+j i^{*}_{j}_{i^{*}}\},\]

_then, after \(K\)-iterations of gradient descent, we have_

\[_{\{(_{i},_{i})\}_{i[N]},_{N+1}} }_{^{k}}(_{N+1})-_{i^{*}}^{2} O_{}R^{2}N^{2}K^{- (N,d)}+R^{2}^{}(A_{}^{c}) },\]

_here the expectation is taken w.r.t \(\{(_{i},_{i})\}_{i[N]}\{_{N+1}\} ^{}\). Recall that \(_{i^{*}}\) is the 1-NN predictor of \(_{N+1}\), which we defined in Definition 1._

We leave the detailed proof in Appendix D. Let us discuss the implication of Theorem 2. The event \(A_{}\) describes the situation when the query \(_{N+1}\) is located at an "inner point" away from its decision boundary, in which its distance to the nearest neighbor \(_{i^{*}}\) is strictly larger than all other points. Such a quantity is similar to the margin condition in classification theory in deep learning Bartlett et al. (2017) and \(k\)-NN literature Chaudhuri and Dasgupta (2014), where the optimal choice probability is strictly larger than all suboptimal choices. Specifically, if \(^{}(A_{^{*}})=1\) for some \(^{*}>0\), i.e., the query \(_{N+1}\) is strictly bounded away from the decision boundary almost surely, then the \(L_{2}\) distance between \(}_{^{k}}\) and the 1-NN predictor will converge in a \(O(R^{2}K^{-(N,d)^{*}})\) even under a shifted distribution. We also introduce the following corollary, in which we show that when \(_{i}\) only takes value in a finite integer set, resembling a classification task, the trained transformer behaves like a 1-NN predictor under an additional rounding operation.

**Corollary 1** (Classification of Trained Transformer).: _Suppose \(_{i}[M]\) for some integer \(M 0\) under \(^{}\), then we have_

\[_{}} _{^{k}}(_{N+1})_{i^{*}} O _{}M^{2}N^{2}K^{-(N,d)}+M^{2 }^{}(A_{}^{c})}.\]

_Here we define_

\[(t):=_{[t]<} t+ _{[t]} t,\]i.e. the mapping from \(t\) to its closest integer, and \(A_{}\) is defined as in Theorem 2. Moreover, if there exists \(^{*}>0\) such that \(^{}(A_{^{*}})=0\), then we have_

\[^{}} _{^{k}}(_{N+1})_{i^{*}}=0\]

_whenever \(K O(N,d)^{*}}\)._

We leave the detailed proof in Appendix D. Corollary 1 provides a convergence rate for the classification difference between 1-NN and the pretrained transformer. Notably, when \(_{N+1}\) is well separated from the decision boundary in the testing distribution \(_{}\), the trained transformer will behave exactly the same as the 1-NN classifier in \(O(N,d)^{*}}\) gradient steps for the pretrained transformer. Theorem 2 and Corollary 1 show that the trained transformer under gradient descent is robust to both query and prompt distribution shift in the test distribution \(^{}\), in the sense that it will maintain its resemblance to a 1-NN predictor in both prediction and classification task, thus extended the results in Zhang et al. (2023); Huang et al. (2023); Chen et al. (2024) to a nonparametric estimator.

## 4 Sketch of Proof

In this section, we sketch the proof of Theorem 1 and highlight the techniques we used. The full proof is left to Appendix C.

Equivalence to a Two-Dimensional Dynamic System.Recall that \(\{_{i}\}_{i[N+1]}\) and the first and second moment of \(\{_{i}\}_{i[N]}\) are uncorrelated. Utilizing this uncorrelation between \(\{_{i}\}_{i[N+1]}\) and \(\{_{i}\}_{i[N]}\), we can eliminate the reliance of the gradient on \(\{_{i}\}_{i[N]}\) since we are considering a population loss. Moreover, utilizing the structure of the initialization, we can prove by induction that all \(_{ij}\) will remain zero except for \(_{11}\) and \(_{33}\). This shows that with a suitable initialization, the transformer model will only focus on the relationship between different tokens \(_{i}\) throughout the whole training process. Our findings can be summarized by the following lemma.

**Lemma 1** (Closed-Form Gradient).: _With the initialization in Assumption 2, the gradient of \(L(^{k})\) with respect to \(_{11}\) can be written in the following form for all \(k 0\):_

\[_{_{11}}L(^{k})=_{i=1}^{N}g_{i }^{k}(_{i}^{}_{N+1})_{i}_{N +1}^{}+g_{i^{*}}^{k}(_{i^{*}}^{}_{N+1}) _{i^{*}}_{N+1}^{}\] (4.1)

_where \(\{g_{i}^{k}(x)\}_{i[N]}\{g_{i^{*}}^{k}(x)\}:\) is a set of functions. Here the expectation is taken with respect to \(\{_{i}\}_{i[N+1]}\), with \(_{i^{*}}=_{\{_{i}\}_{i [N]}}\|-_{N+1}\|_{2}\) sampled i.i.d. from a uniform distribution on \(^{d-1}\). Moreover, we have \(_{_{ij}}L(^{k})=0\) for all \((i,j)\) and all \(k 0\) except for \(_{11}\) and \(_{33}\)._

Lemma 1 shows that we only need to consider \(_{11}\) and \(_{33}\) in our update since all other entries will remain zero during the whole learning process. Note that in Eq. (4.1), all nonlinearity comes from the inner product between \(_{i}^{}_{N+1}\) and \(_{i^{*}}^{}_{N+1}\). Recall that \(\{_{i}\}_{i[N+1]}\) are i.i.d. sampled from a uniform distribution supported on a \(d-1\)-dimensional sphere \(^{d-1}\), therefore, the distribution of \(\{_{i}\}_{i[N+1]}\) is rotational invariance, which means \(_{i[N+1]}_{_{i}}=_{i[N+1]} _{U_{i}}\) for all orthogonal matrix \(U^{d d}\). Since the rotation of \(\{_{i}\}_{i[N+1]}\) does not change the inner products \(\{_{i}^{}_{i}\}_{i[N]}\) and \(_{i^{*}}^{}_{N+1}\), from the structure of \(_{_{11}}L(^{k})\) illustrated by Eq. (4.1), we shall always have \(U_{_{11}}L(^{k})U^{}=_{_{11}}L( ^{k})\), which shows \(_{_{11}}L(^{k})=c_{k}I_{d}\) for some constant \(c_{k}\) by simple algebra. We summarize our result in the following lemma.

**Lemma 2** (Two-Dimensional System).: _With the initialization in Assumption 2, there exists two sets of real numbers \(\{_{1}^{k}\}_{k 0}\) and \(\{_{2}^{k}\}_{k 0}\), such that \(^{k}\) has the following form:_

\[^{k}=\{^{k},,_{1}^{k}} _{},0,-_{2}^{k}\}.\]With Lemma 2, we reduce the dimension of the original dynamic system in Eq. (2.6) from \((d+2)^{2}\) to \(2\). We now only need to focus on the evolution of \(_{1}^{k}\) and \(_{2}^{k}\) for all \(k 0\).

Convergence of the Dynamic System.Lemma 2 helps us largely reduce the dimension of the training dynamics. However, this does not make our question a trivial one, as the loss function is still highly nonconvex even when we only need to consider a two-dimensional subspace of \(^{(d+2)(d+2)}\). To see this, we introduce the following lemma:

**Lemma 3** (Nonconvexity of Transformer Optimization).: _When \(\) lie in a two-dimensional subspace of \(^{(d+2)(d+2)}\) defined by \(=\{,,_{1}}_{d},0,-_{2}\}\), the original loss function defined in Eq. (2.5) is equivalent to the following:_

\[L(_{1},_{2}):=^{N}(_{1 }_{j},_{N+1})_{j}}{_{i=1}^{N} (_{1}_{i},_{N+1})+(_{1}-_ {2})}-_{i^{*}}^{2}.\] (4.2)

_Such loss function is still nonconvex._

We leave the detailed proof in Appendix E. Nonconvexity shown by Lemma 3 implies attaining the global minimum could be hard. Previous works such as Zhang et al. (2023) utilize conditions such as Polyak-Lojasiewicz inequality to analyze such systems, however, those conditions are not applicable in our setting, and a more delicate analysis for the evolution of \(_{1}^{k}\) and \(_{2}^{k}\) is needed. We characterize their behavior by the following lemma.

**Lemma 4**.: _For \(_{1}^{k} 0\), there exists constants \(c_{1},c_{2},c_{3},c_{4}>0\), such that_

\[(_{1}^{k+1}-_{1}^{k}) c_{1}(-6_{1}^{k})-c _{2}(2_{1}^{k}-_{2}^{k}),\]

_and_

\[(_{1}^{k+1}-_{1}^{k}) c_{3}(N,d)_{1}^{k}-c_{4}2(_{1}^{k}-_{2}^{k}) \]

Lemma 4 shows that there exits a constant \(c_{b}(0,1)\), such that \(_{1}^{k}\) will keep increasing with a scale of \(( k)\) until \(_{1}^{k} c_{b}_{2}^{k}\). With this ratio, we obtain the following lemma for the increment of \(_{2}^{k}\).

**Lemma 5**.: _For \(_{1}^{k} 0\), there exits constant \(c_{1}^{}\), \(c_{2}^{}\), such that_

\[c_{1}^{}(-(N,d)_{2}^{k}) (_{2}^{k+1}-_{2}^{k}) c_{2}^{}(-(N,d) _{2}^{k}).\]

Lemma 5 shows that \(_{2}^{k}\) will monotonically increase with a scale of \(((_{2}^{k}))\), which implies \(_{2}^{k}=( k)\). Combining Lemma 4 and 5, we show that both \(_{1}^{k}\) and \(_{2}^{k}\) converge to infinity, with \(_{1}^{k}\) maintaining a slower speed, as its decreases when getting closer to \(_{2}^{k}\) from the below. Recall that the loss function is equivalent to Eq. (4.2) under the initialization specified in Assumption 2, which shows that \(L(_{1}^{k},_{2}^{k})\) will converge to zero as long as \(_{1}^{k}\) and \(_{2}^{k}-_{1}^{k}\) both converges to infinity. We thus conclude our proof of \(L(^{k})\) eventually converges to it global minimum.

## 5 Numerical Results

In previous sections, we have shown that with the initialization specified in Assumption 2, a single softmax attention layer transformer is able to learn the 1-NN predictor under gradient descent and remain robust under distribution shift. We now conduct experiments in a less restrictive setting and show that even without specific initialization and full-batch gradient descent, simple stochastic gradient descent updates with random parameter initialization for the parameters are still sufficient for the model to learn the 1-NN predictor. First, we investigate the convergence of single-head single-layer transformers (Vaswani et al., 2017) trained on 1-NN tasks. The training data are sampled from \(_{}^{}\) and \(_{}^{}\), defined in Assumption 1. We choose context length \(N\{16,32,64\}\) and input dimension \(d\{8,16\}\). The model is trained on a dataset with a size of 10000, and an epoch number of 2000. To ensure our training convergence result holds beyond the gradient descent scheme, we choose SGD as our optimizer, with a batch size of 128 and a learning rate of 0.1. We use the random Gaussian as our initialization. Our results for the training loss convergence are summarized in the left panel of Fig. 3. The results show that the model converges to 1-NN predictor on the training data even under SGD and random initialization. Moreover, as the dimension \(d\) and the length of contexts \(N\) becomes larger, the convergence speed becomes slower.

To verify our results on the distribution shift, we generate testing data sampled from a distribution difference from the training data, and report the mean square error between the model prediction and the 1-NN predictor. Furthermore, the testing data satisfies \((A_{}^{*})=1\), with \(^{*}\) specified as \(0.1\). Recall that we defined \(A_{}\) in Theorem 2 as the event where \(_{N+1}\) is separated from the decision boundary with a distance of at least \(\). We leave the details of our data-generating process in Appendix B. We test the trained transformer model on this dataset once every epoch throughout the training process. Our results are summarized in the right panel of Fig. 3. The results show that the testing error decreases much faster than the training loss, due to the boundary separation condition, which the uniformly-sampled training data do not enjoy. Our result also coincides with our theoretical result in Theorem 2, showing that the trained transformers are robust under distribution shift, and benefits greatly from staying away from the decision boundary.

## 6 Conclusion

We investigate the ability of single-layer transformers to learn the one-nearest neighbor prediction rule, a classic nonparametric estimator. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we demonstrate that, despite the non-convexity of the loss function during gradient descent training, a single softmax attention layer can successfully emulate a one-nearest neighbor predictor. We further show that the trained transformer is robust to the distribution shift of the testing data. As far as we know, this paper is the first to establish training convergence and behavior under distribution shifts for softmax attention transformers beyond the domain of linear predictors.