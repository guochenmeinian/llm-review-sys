# Steve-1: A Generative Model for

Text-to-Behavior in Minecraft

Shalev Lifshitz\({}^{1,2}\)

shalev.lifshitz@mail.utoronto.ca

&Keiran Paster\({}^{1,2}\)

keirp@cs.toronto.edu

Harris Chan\({}^{1,2}\)

hchan@cs.toronto.edu

&Jimmy Ba\({}^{1,2}\)

jba@cs.toronto.edu

&Sheila McIlraith\({}^{1,2}\)

sheila@cs.toronto.edu

\({}^{1}\)Department of Computer Science, University of Toronto, Toronto, Canada.

\({}^{2}\)Vector Institute for Artificial Intelligence, Toronto, Canada.

###### Abstract

Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces a methodology, inspired by unCLIP, for instruction-tuning generative models of behavior without relying on a large dataset of instruction-labeled trajectories. Using this methodology, we create an instruction-tuned Video Pretraining (VPT) model called Steve-1, which can follow short-horizon open-ended text and visual instructions in Minecraft(tm). Steve-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, reducing the need for costly human text annotations, and all for only $60 of compute. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, Steve-1 sets a new bar for open-ended instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research.

## 1 Introduction

The ability to use text instructions to control and interact with powerful AI models has made these models accessible and customizable for the masses. Such models include ChatGPT , which can respond to messages written in natural language and perform a wide array of tasks, and Stable Diffusion , which turns natural language into an image. While those models cost anywhere from hundreds of thousands to hundreds of millions of dollars to train, there has been an equally exciting trend whereby powerful open-source foundation models like LLaMA  can be finetuned with surprisingly little compute and data to become instruction-following (e.g., ).

In this paper, we study whether such an approach could be applicable to sequential decision-making domains. Unlike in text and image domains, diverse data for sequential decision-making is very expensive and often does not come with a convenient "instruction" label like captions for images. We propose to instruction-tune pretrained generative models of behavior, mirroring the advancements seen in recent instruction-tuned LLMs like Alpaca , without relying on a large dataset of instruction-labeled trajectories.

In the past year, two foundation models for the popular open-ended video game Minecraft(tm) were released: a foundation model for behavior called VPT  and a model aligning text and video clips called MineCLIP . This has opened up an intriguing avenue to explore fine-tuning for instruction-following in the sequential decision-making domain of Minecraft. VPT was trained on 70k hours of Minecraft gameplay, so the agent already has vast knowledge about the Minecraft environment. However, just as the massive potential of LLMs was unlocked by aligning them to follow instructions, it is likely that the VPT model has the potential for general, controllable behavior if it is finetuned to follow instructions. In particular, our paper demonstrates a method for fine-tuning VPT to follow short-horizon text instructions with only $60 of compute and around 2,000 instruction-labeled trajectory segments.

Our method draws inspiration from unCLIP , the approach used to create the popular text-to-image model DALL(r) 2. We decompose the problem of creating an instruction-following Minecraft agent into two models: a VPT model finetuned to achieve visual goals embedded in the MineCLIP latent space, and a prior model that translates text instructions into MineCLIP visual embeddings. We finetune VPT using behavioral cloning with self-supervised data generated with hindsight relabeling , avoiding the use of expensive text-instruction labels in favor of visual MineCLIP embeddings. We apply unCLIP with classifier-free guidance  to create our agent called Steve-1, which sets a new bar for open-ended instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming the baseline set by Baker et al. .

Our main contributions are as follows:

* We introduce a methodology, inspired by unCLIP , for instruction-tuning generative models of behavior without relying on a large dataset of expensive instruction labels.
* We apply this methodology to create Steve-1, a Minecraft agent that can follow short-horizon open-ended text and visual instructions with a high degree of accuracy, all for only $60 of compute. We perform extensive evaluations of our agent, showing that it can robustly complete 12 of 13 goal-conditioned control tasks in our early-game evaluation suite in Minecraft. For long-horizon tasks1 like crafting and building, we show that a basic version of prompt chaining can dramatically improve performance. * We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, data scaling, prompt-engineering, and other design choices. In particular, we show that unCLIP  and classifier-free guidance  translate well to sequential decision making and are essential for strong performance.
* We release model weights for Steve-1 as well as training scripts and evaluation code to help foster more research into instructable, open-ended sequential decision-making agents.2

Figure 1: Like unCLIP , our approach involves two models. First, we train the _policy_ by finetuning VPT to achieve goals given by pretrained MineCLIP  visual embeddings using our gameplay dataset. Second, for the _prior_ model, we train a CVAE  to sample MineCLIP visual embeddings given a text prompt. The combination of these two models enables our agent to follow text and visual instructions.

Related Work

Minecraft as a Test-bed for AIMinecraft has gained popularity as a benchmark for AI research due to its complex and dynamic environment, making it a rich test-bed for reinforcement learning and other AI methods (e.g., [26; 19; 17; 21; 40; 62; 38; 9]). We leverage the MineRL environment  to research the creation of agents that can follow open-ended instructions in complex visual environments using only low-level actions (mouse and keyboard). We build Steve-1 on top of two recent foundation models. In order to align text and videos, we use MineCLIP , a CLIP  model trained on paired web videos of Minecraft gameplay and associated captions. To train Steve-1's policy, we fine-tune VPT , a foundation model of Minecraft behavior that is pretrained on 70k hours of Web videos of Minecraft along with estimated mouse and keyboard actions. Several prior works [61; 62] have explored the use of LLMs in creating instructable Minecraft agents. These works typically use LLMs to make high-level plans that are then executed by lower-level RL [40; 62] or scripted  policies. Since Steve-1 is a far more flexible low-level policy, the combination of Steve-1 with LLMs is a promising direction for future work. Fan et al.  introduced an agent trained using RL with MineCLIP as a shaping reward on 12 different tasks and conditioned on MineCLIP-embedded text-prompts. However, this agent failed to generalize beyond the original set of tasks without further RL finetuning using the MineCLIP reward function. Cai et al.  proposed a Goal-Sensitive Backbone architecture for goal-conditioned control in Minecraft which is trained on a fixed set of goals, while Steve-1 learns goal-reaching behavior from a large dataset in a self-supervised way without training on an explicit set of tasks.

Foundation Models for Sequential Decision-MakingFoundation models which are pretrained on vast amounts of data and then finetuned for specific tasks have recently shown great promise in a variety of domains including language [8; 14; 59], vision [48; 10; 47], and robotics [7; 53; 25; 39; 65]. GATO  and RT-1  have demonstrated the potential of training transformers to perform both simulated and real-world robotic tasks. With the exception of Kumar et al. , which uses Q-learning, the vast majority of cases [32; 7; 49] where deep learning has been scaled to large, multitask offline-RL datasets have used supervised RL. Supervised RL (e.g., [42; 18; 12]) works by framing the sequential decision-making problem as a prediction problem, where the model is trained to predict the next action conditioned on some future outcome. While these approaches are simple and scale well with large amounts of compute and data, more work is needed to understand the trade-offs between supervised RL and Q-learning or policy gradient-based methods [43; 44; 6; 55]. Recent works explore the use of hindsight relabeling  using vision-language models [47; 2] to produce natural language relabeling instructions. DIAL  finetunes CLIP  on human-labeled trajectories, which is then used to select a hindsight instruction from a candidate set. Sumers et al.  uses Flamingo  zero-shot for hindsight relabeling by framing it as a visual-question answering (VQA) task. In contrast, Steve-1 relabels goals using future trajectory segment embeddings given by the MineCLIP  visual embedding.

Text-Conditioned Generative ModelsThere has been a recent explosion of interest in text-to-X models, including text-to-image (e.g., [48; 51; 50]), text-to-3D (e.g., [27; 35]), and even text-to-music (e.g., ). These models are typically either autoregressive transformers modeling sequences of discrete tokens [60; 8] or diffusion models . Most related to our work is unCLIP, the method used for DALL*E 2 . unCLIP works by training a generative diffusion model to sample images from CLIP  embeddings of those images. By combining this model with a prior that translates text to visual CLIP embeddings, unCLIP can produce photorealistic images for arbitrary text prompts. unCLIP and many other diffusion-based approaches utilize a technique called classifier-free guidance , which lets the model trade-off between mode-coverage and sample fidelity post-training. We utilize the basic procedure of unCLIP and classifier-free guidance for training Steve-1.

## 3 Method

Inspired by the rapid recent progress in instruction-tuning Large Language Models (LLMs), we choose to leverage the recently released Video Pretraining (VPT)  model as a starting point for our agent. Since VPT was trained on 70k hours of Minecraft gameplay, the agent already has vast knowledge about the Minecraft environment. However, just as the massive potential of LLMs was unlocked by aligning them to follow instructions, it is likely that the VPT model has the potential for general, controllable behavior if it is finetuned to follow instructions. In this work, we present a method for finetuning VPT to follow natural, open-ended textual and visual instructions, which opens the door for a wide range of uses for VPT in Minecraft.

Our approach is inspired by unCLIP, the method behind the recent text-to-image generation model, DALL*E 2 . Our goal is to create a generative model of behavior in Minecraft conditioned on text instructions \(y\). To do so, we utilize a dataset of Minecraft trajectory segments, some of which contain instruction labels \(y\): \([(_{1},y_{1}),(_{2},y_{2}),,(_{n},)]\) where \(\) is a trajectory of observations and actions. We also employ a pretrained CLIP model called MineCLIP , which generates aligned latent variables \(z_{_{t:t+16}},z_{y}\), where \(z_{_{t:t+16}}\) is an embedding of any \(16\) consecutive timesteps from the trajectory. MineCLIP is trained using a contrastive objective on pairs of Minecraft videos and transcripts from the web. For simplicity of notation, we refer to the MineCLIP embedding of the last 16 timesteps of a trajectory segment as \(z_{_{}}}\). Like unCLIP , we utilize a hierarchical model consisting of a prior and a policy:

* A _prior_\(p(z_{_{}}}|y)\) that produces a latent variable \(z_{_{}}}\) conditioned on a text instruction \(y\).
* A _policy_\(p(|z_{_{}}})\) that produces a trajectory conditioned on a latent variable \(z_{_{}}}\).

These two models can then be combined to produce a generative model of behaviors conditioned on text instructions:

\[p(|y)=p(,z_{_{}}}|y)=p(z_{_{ }}}|y)p(|z_{_{}}})\] (3.1)

### Policy

To learn our policy, we finetune VPT, a foundation model of Minecraft behaviors \(p_{}()\) trained on 70k hours of Minecraft gameplay videos. Specifically, VPT consists of a ResNet  that processes frames of dimension \(128 128 3\), and a Transformer-XL  which processes the frame representations and autoregressively predicts the next action using the joint hierarchical action space described in Baker et al. . In order to modify the architecture to condition on goal information, we add an affine transformation of \(z_{_{}}}\) to the output of the ResNet before passing it to the transformer:

Process Frames: \[_{}(o_{t}) x_{t}\] [+ Conditioning on MineCLIP Embedding Goal]: \[x_{t} x_{t}+W_{}z_{_{}}}+b_{}\] Predict Actions: \[_{}(x_{t},,x_{t+T}) a_{t+T}\]

In order to finetune VPT to condition on goals, we finetune the model using a method inspired by supervised RL approaches like Decision Transformer , GLAMOR , and GCSL . We use a modification of hindsight relabeling which we call **packed hindsight relabeling** (see Figure 2) to generate a new dataset of trajectories with goals pulled from future states that periodically switch. In contrast with hindsight relabeling, packed hindsight relabeling packs multiple relabeled sequences into a single sequence. Specifically, our method to generate this dataset consists of two steps:

1. Given a trajectory \(\) with \(T\) timesteps, randomly generate indices to select goals from: \(i_{1},i_{2},,i_{n}\). These indices are chosen by starting at the first timestep and repeatedly sampling a new timestep by adding a random value to the previous timestep. This ensures that the data reflects that some goals may take longer to achieve than others.
2. For each chosen goal at timestep \(i_{j}\), set the goals for timesteps \(i_{j-1}+1,,i_{j}\) to be the goal at timestep \(i_{j}\), denoted \(z_{_{i_{j}}}\).

Figure 2: To create goal-conditioned data for finetuning, we randomly select timesteps from episodes and use hindsight relabeling to set the intermediate goals for the trajectory segments to those visual MineCLIP embeddings. This self-supervised data teaches the agent which actions lead to which states.

Our final dataset \(_{}\) consists of observation sequences \((o_{1},,o_{T})\), action sequences \((a_{1},,a_{T})\), and packed hindsight relabeled goals \((z_{1},,z_{T})\). We then finetune VPT on this dataset using a supervised loss to predict each action autoregressively using a causal attention mask:

\[_{}()=_{_{ }}[- p_{}(a_{t}|o_{1 t},z_{1 t})]\] (3.2)

### Prior

In order to condition not only on embeddings of visual goals but on latent goals, we need the prior, a model that produces a latent variable \(z_{_{}}\) conditioned on a text instruction \(y\). Our model is a simple conditional variational autoencoder (CVAE)  with a Gaussian prior and a Gaussian posterior. Rather than learn to condition directly on text, we choose to condition on frozen text representations from MineCLIP \(z_{y}\). Thus, the prior learns a function to translate from a text embedding \(z_{y}\) to a visual embedding \(z_{_{}}\) (see Appendix C.5). Both the encoder and decoder of our CVAE are parameterized as two-layer MLPs with 512 hidden units and layer normalization . We train the model on our dataset, for which we have text labels \(_{}\) using the following loss:

\[_{}()=_{(z_{_{}},z_{y}) _{}}(q_{}(z_{_{}}|z_{y})\|p(z_{_{}}))-_{c q_{}(z_{_{ }}|z_{y})} p_{}(z_{_{}}|c,z_{y}) \] (3.3)

### Datasets

To train our policy, we gather a gameplay dataset with 54M frames (\( 1\) month at 20FPS) of Minecraft gameplay along with associated actions from two sources: contractor gameplay and VPT-generated gameplay. To train our prior, we use a small dataset of text-video pairs gathered by humans and augmented using the OpenAI API gpt-3.5-turbo model  and MineCLIP. See Appendix D for more detailed dataset information.

OpenAI Contractor DatasetWe use 39M frames sourced from the contractor dataset which VPT  used to train its inverse dynamics model and finetune its policy. The dataset was gathered by hiring human contractors to play Minecraft and complete tasks such as house building or obtaining a diamond pickaxe. During gameplay, keypresses and mouse movements are recorded. We use the same preprocessing as VPT, including filtering out null actions.

VPT-Generated DatasetWe generate an additional dataset of 15M frames by generating random trajectories using the various pretrained VPT agents. The diversity of this dataset is improved by randomly switching between models during trajectories , randomly resetting the agent's memory, and randomly turning the agent to face a new direction.

Text-Video Pair DatasetTo train our prior model, which learns a mapping between text embeddings and visual embeddings, we also manually gather a small dataset of 2,000 text instructions paired with 16-frame video segments (less than a second) from our gameplay dataset. This dataset corresponds to less than 30 minutes of gameplay and takes just a few hours to collect. We augment this dataset by using the alignment between text and video embeddings from MineCLIP. For each text instruction, we find the top \(k\) most similar gameplay segments in our dataset and use the corresponding 16-frame segment as additional training data. For augmentation, we also add 8,000 text-instructions generated by the OpenAI API gpt-3.5-turbo model , in addition to our 2,000 hand-labeled instructions.

### Inference

At inference time, we use the prior to sample a latent goal \(z_{_{}}\) from the text instruction \(y\). We then use the policy to autoregressively sample actions \(a_{t}\) conditioned on the observation history \(o_{1 t}\) and the latent goal \(z_{_{}}\). Similar to the observation in Appendix I of Baker et al. , even with conditioning the policy often fails to follow its instruction and simply acts according to its prior behavior. To mitigate this, we borrow another trick used in image generation models: classifier-free guidance. Specifically, during inference we simultaneously compute logits for the policy conditioned on the goal \(f(o_{t},,o_{t+1},z_{_{}})\) and for the unconditional policy \(f(o_{t},,o_{t+1})\). We then compute a combination of the two logits using a \(\) parameter to trade-off between the two:\[=(1+)(o_{t},,o_{t+1},z_{r_{}})}_{}-(o_{t},,o_{t+1})}_{}\] (3.4)

By setting a higher value of \(\), we can encourage the policy to follow actions that are more likely when conditioned on the goal and, as demonstrated in Section 4.5, this significantly improves performance. Also, in order to train the policy to generate these unconditional logits, we occasionally dropout the goal embedding \(z_{r_{}}\) from the policy's input (with probability 0.1). This lets us generate both the conditional and unconditional logits using the same model with batch processing at inference time.

### Evaluation

Evaluating the performance of our agent is a challenging task due to the wide variety of instructions that are possible and the difficulty of evaluating whether the agent has successfully achieved its task. We use a combination of programmatic evaluation metrics and automatic MineCLIP evaluation metrics to get a sense of the agent's capability level. We collectively refer to all of our evaluation tasks including the 11 evaluation tasks from Figure 3 and the two prompt chaining tasks from Section 4.3 as our _early-game evaluation suite_.

Programmatic EvaluationWe compute programmatic evaluation metrics by monitoring the MineRL  environment state throughout each evaluation episode. As done in VPT , we compute multiple programmatic metrics including travel distance and early-game item collection. The travel distance is the maximum displacement of the agent along on the horizontal (X-Z) plane, measured from the initial spawn point. For early-game inventory counts, we store the maximum number of log, seed, and dirt items seen in the agent's inventory during the episode.

MineCLIP EvaluationWe explore the use of text-visual alignment in MineCLIP latent space between trajectories and text or visual goals to evaluate our agent over a wider variety of tasks where programmatic evaluation isn't practical. To determine the degree to which a task has been completed at all during an evaluation episode, we record the minimum cosine distance between the (text or visual) goal embedding and the visual MineCLIP embedding at any timestep during an episode.

## 4 Results

In our experiments, we aim to answer the following questions:

1. How well does Steve-1 perform at achieving both text and visual goals in Minecraft?
2. How does our method scale with more data?
3. What choices are important for the performance of our method?

### Training Setup

We base our implementation off of the official VPT codebase3. The main Steve-1 is trained using Pytorch  distributed data parallel on four A40 GPUs for 160M frames, or just under three epochs of our gameplay dataset. Hyperparameters are selected to match those in Baker et al.  with the exception of learning rate, which we set to 4e-5. Our models are optimized using AdamW . See Table 1 for a full list of hyperparameters.

### Performance on Textual and Visual Goals

Due to the hierarchical nature of our model, we can evaluate the performance of our agent at achieving either text or visual goals simply by choosing whether to use the prior to condition on text or bypass the prior and condition on a MineCLIP video embedding directly. We first tested our model on a set of 11 tasks that are achievable within the first 2.5 minutes of gameplay and which do not require multiple steps to complete (e.g., chop a tree or dig a hole, but not build a house). A complete list of the tasks and prompts we used for evaluation can be found in Table 3 in the appendix. To select visual goals for testing each of the evaluation tasks, we implemented a tool that searches through 10% of our gameplay dataset by finding the closest 16-frame videos to a given text prompt. We then manually selected a 16-frame video that clearly demonstrates the task being completed and use the corresponding MineCLIP video embedding as the goal embedding for that task. Screenshots of these visual goals can be found in Figure 14 in the appendix.

In Figure 3, we compare the performance of our text and visual-conditioned agents with the unconditional VPT agent and text-conditioned VPT agent (Appendix I in ) across our programmatic tasks. We find that when given the relevant text instruction, Steve-1 collects \(75\) more dirt, \(4.9\) more wood, \(22\) more seeds, and travels \(4.3\) farther than the unconditional agent, and Steve-1 collects \(3.3\) more dirt, \(4.4\) more wood, \(8.1\) more seeds, and travels \(2.2\) farther than the text-conditioned VPT agent. This represents a significant improvement over the reported performance of text-conditioned VPT, which collects several times fewer resources despite having twice as long of an episode to do so. We also run an automatic evaluation using MineCLIP embedding distances by measuring the minimum distance of a goal embedding to any frame in the episode. As shown in Figure 2(b), the distance between the goal and the episode is significantly lower when the agent is conditioned on the corresponding visual goal than otherwise. Full results for Steve-1 with both text and visual goals can be found in Appendix F.

In addition to our evaluations of Steve-1, we also recorded several sample interactive sessions we had with the agent (controlling it in real-time by giving it written text instructions or specific visual goals). These sessions demonstrate Steve-1's ability to responsively follow instructions in real-time in a variety of situations. We believe that such use-cases, where humans give an agent natural instructions that it can follow to complete tasks, will become increasingly important and have practical uses in the creation of instructable assistants and virtual-world characters. These videos, as well as videos of our agent performing our evaluation tasks, can be found at https://sites.google.com/view/steve-1.

### Prompt Chaining

We also experiment with longer horizon tasks that require multiple steps, such as crafting and building. We explore two different prompting methods: directly prompting with the target goal, and a simple form of prompt chaining [11; 64; 16] where the task is decomposed into several subtasks and the

Figure 3: **Left:** In our programmatic evaluations, Steve-1 performed far better than the unconditional VPT agent early-game-2x and the text-conditioned VPT agent when prompted appropriately. The asterisk * in the “VPT (Text)*” indicates that this result was taken from Appendix I in , which had twice the episode length compared to our setting. On some tasks, visual outperforms text-based prompting, creating a gap that can likely be bridged through better prompt engineering. **Right:** Across our 11 MineCLIP evaluation tasks, Steve-1 achieves the shortest distance between the episode and the MineCLIP goal embedding when prompted appropriately except for in two cases, where it mixes up digging and dirt and swimming and going underwater. This shows the strong general performance of Steve-1 across a wide variety of short-horizon tasks. The dashed box marks minimum element along the row, and the diagonal number signifies the diagonal element’s rank (0 means it is the minimum row element). See Figure 14 for sample frames from each of the 11 visual goals and Figure 13 for a success-rate version of this matrix.

prompts are given sequentially for a fixed number of steps. We explore prompt chaining with visual goals for two tasks: 1) building a tower and 2) making wooden planks. When using prompt chaining, we first prompt Steve-1 to gather dirt before building a tower, and to gather wooden logs before crafting wooden planks. Figure 4 shows that directly prompting Steve-1 with the final tasks results in near-zero success rates. However, prompt chaining allows Steve-1 to build a tower 50% of the time and craft wooden planks 70% of the time. For the tower building task, Steve-1 immediately starts collecting dirt until the prompt switches, at which point its average height starts increasing rapidly and its dirt decreases as it builds a tower. Similarly, for the crafting wooden planks task, Steve-1 immediately starts collecting a large amount of wooden logs until the prompt switches and it rapidly converts these wooden logs into wooden planks (causing the amount of wooden logs in its inventory to immediately decrease and the number of wooden planks to increase as it crafts more). Figure 4 visualizes the average item counts and agent height for the prompt chaining episodes. See Figure 18 and Figure 19 in the appendix for visualizations of specific prompt chaining episodes.

### Scaling

Recent works in language modeling have found that scaling up pretraining FLOPs, by training on more data or by training a model with more parameters, can improve performance on downstream tasks . In certain cases when measuring performance with metrics such as exact-match , performance improvement may appear to be "emergent" , appearing suddenly as the model is trained with more compute. Here, we aim to gain a basic understanding of how the performance of Steve-1 on various tasks scales by training with more data (learning rate schedule is chosen appropriately).

To assess performance gain, we first isolated the performance of the policy from the prior, measuring performance of the agent through training on programmatic tasks (travel distance, seeds, logs, dirt) with visual goals. Due to compute constraints, we chose to use the 2x VPT model, which has 248M parameters. We found that both seed collection and travel distance did not improve significantly past 20M frames. From inspecting gameplay, we suspect that travel distance is a relatively easy task since it is close to VPT's default behavior of running around and exploring. For seed collection, performance remains suboptimal, suggesting that further scaling may be beneficial. This hypothesis is supported by the observation that performance on log and dirt collection remained roughly level until 60M frames when it began to rapidly improve. Figure 4 shows the scaling curves for Steve-1 on each programmatic task when conditioning on relevant vs. irrelevant visual prompts for that task. Since we do not observe regression on any tasks as we train the model with more compute, we expect the model to continue to perform better as we train larger models on larger datasets.

Figure 4: **Top left**: By sequentially chaining visual prompts like “get dirt” and “build a tower”, Steve-1 successfully gathers dirt and then uses this dirt to build a tower. The prompts switch at the dotted vertical line. **Bottom left**: The success rates of the chained prompts improve steadily as we train Steve-1 on more data. **Right**: The performance of Steve-1 in different tasks scales in different ways when conditioning on a relevant visual prompt for the metric versus other irrelevant visual prompts (e.g., the break wood prompt is the relevant prompt for the “Wooden Logs Collected” metric, while the other prompts are “irrelevant”). For instance, in the wood-collection and dirt-collection tasks, performance starts increasing after training on 60M frames of gameplay. See Figure 14 for sample frames from each visual prompt.

[MISSING_PAGE_FAIL:9]

## 5 Limitations and Conclusion

In this paper, we present a methodology for creating instruction-following foundation models of behavior. Specifically, by leveraging two existing pretrained foundation models: a behavioral prior (VPT ) and a domain-specific CLIP model (MineCLIP ), we create a powerful Minecraft agent that can follow short-horizon open-ended text and visual instructions, all for only $60 of compute. The resulting foundation model, Steve-1, sets a new bar for open-ended instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We note that generalist agents such as Steve-1 can have potential negative effects on society. We include a thorough discussion of these issues in Appendix A.

Steve-1 is a significant advancement in creating generative models of text-to-behavior, but it has several limitations, as described in Appendix B. First, Steve-1 is mostly proficient at achieving short-horizon tasks while struggling with longer-horizon tasks. While prompt chaining is a promising approach for improving performance on complex tasks, more can be done in future work to improve performance. Another limitation we observe is that prompt engineering, as with other generative models, can be unintuitive and time-consuming. Future work should investigate improving the steerability of Steve-1 through a better understanding of natural language prompts. Additionally, we note that evaluating and describing the capabilities of open-ended generalist agents is an open research problem itself since capability depends strongly on preconditions, prompt engineering, and our own ability to come up with varied and challenging tasks. Finally, since our approach is not specific to the Minecraft domain, we hope that the method used to create Steve-1 can inspire future work in creating powerful generalist agents in other domains and environments.

Figure 5: **Left: We trained Steve-1 on 100M frames starting from four different pretrained weights: random initialization (scratch), foundation-2x (fd), bc-early-game-2x (bc), and rl-from-foundation-2x (rl). The rl-from-foundation-2x agent is generally the most performant after fine-tuning. Using pretrained weights performs better than training from scratch, especially for more complicated tasks like collecting wood. Right: By using classifier-free guidance , Steve-1 collects \(7.5\) more dirt and \(15\) more wood than when \(=0\) (no guidance). See Figure 17 in the Appendix for similar results with other programmatic tasks.**

Figure 6: Similar to in image generation, switching to a longer, more specific prompt dramatically improves the performance of Steve-1. Values in parentheses are 95% confidence intervals.