# Revealing Distribution Discrepancy

by Sampling Transfer in Unlabeled Data

 Zhilin Zhao\({}^{1,2}\)  Longbing Cao\({}^{1}\)  Xuhui Fan\({}^{1}\)  Wei-Shi Zheng\({}^{2,3}\)

\({}^{1}\) School of Computing, Macquarie University, Australia

\({}^{2}\) School of Computer Science and Engineering, Sun Yat-sen University, China

\({}^{3}\) Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

zhaozhl7@hotmail.com, {longbing.cao,xuhui.fan}@mq.edu.au, wszheng@ieee.org

Corresponding author

###### Abstract

There are increasing cases where the class labels of test samples are unavailable, creating a significant need and challenge in measuring the discrepancy between training and test distributions. This distribution discrepancy complicates the assessment of whether the hypothesis selected by an algorithm on training samples remains applicable to test samples. We present a novel approach called Importance Divergence (I-Div) to address the challenge of test label unavailability, enabling distribution discrepancy evaluation using only training samples. I-Div transfers the sampling patterns from the test distribution to the training distribution by estimating density and likelihood ratios. Specifically, the density ratio, informed by the selected hypothesis, is obtained by minimizing the Kullback-Leibler divergence between the actual and estimated input distributions. Simultaneously, the likelihood ratio is adjusted according to the density ratio by reducing the generalization error of the distribution discrepancy as transformed through the two ratios. Experimentally, I-Div accurately quantifies the distribution discrepancy, as evidenced by a wide range of complex data scenarios and tasks.

## 1 Introduction

The assumption that data are independently and identically distributed (IID) is staple in statistical machine learning. It suggests that a hypothesis selected by an algorithm, after observing several training samples, should perform effectively on test samples from the same unknown distribution. However, this assumption often oversimplifies the intricate and diverse nature of real-world data, particularly in non-IID scenarios . Thus, if training samples are considered in-distribution (ID), there is a risk that test samples may deviate from this distribution, characterized as out-of-distribution (OOD) . This distribution discrepancy between training and test distributions poses a critical and challenging non-IID learning  question: _How to quantify the applicability of a hypothesis derived from training samples to test samples?_. This question is generally applicable to areas like OOD generalization , OOD detection , domain adaptation , transfer learning , semi-supervised learning , robust learning , and adversarial learning .

The applicability of a hypothesis can be determined by the distribution discrepancy between training and test distributions. When the two distributions align, meaning both training and test samples are ID, the hypothesis derived from training samples can be seamlessly applied to test samples. However, in reality, particularly when test samples fall OOD, this scenario rarely holds. Under such circumstances, decisions must be based on the extent of the distribution discrepancy. This may require enhancing the hypothesis generalization capability towards test samples or rejecting thesesamples outright. These actions are fundamental to the principles of OOD generalization and OOD detection, respectively .

Evaluating the distribution discrepancy between training and test distributions for a selected hypothesis presents a significant challenge, as training samples are typically labeled, whereas test samples often are not . This discrepancy means that conventional statistical distances, such as F-divergences , integral probability metrics , and total variation , are not suitable for this situation. Furthermore, density ratio methods [18; 19] offer a partial solution by disregarding label information and estimating the ratio between input distributions. Moreover, more performance prediction techniques  aim to navigate this challenge by examining the uncertain outcomes of the hypothesis, potentially leading to unreliable results. A detailed discussion of related work is provided in Appendix A.

To tackle the issue of unavailable test labels, we introduce the concept of _Importance Divergence_ (I-Div), which measures the training-test distribution discrepancy w.r.t. the difference between the expected risks of the selected hypothesis on training and test distributions. To estimate the expected risk on test distributions without label access, the core strategy involves importance sampling to transfer the sampling patterns from the test distribution to the training distribution. This process requires the estimation of density and likelihood ratios. Specifically, the density ratio, informed by the selected hypothesis, is obtained by minimizing the Kullback-Leibler divergence between the actual and estimated input distributions. Simultaneously, the likelihood ratio is adjusted according to the density ratio by reducing the generalization error related to the distribution discrepancy as transformed through the two ratios. As a result, I-Div leverages the estimated density and likelihood ratios to quantitatively measure the distribution discrepancy between training and test distributions, eliminating the need for test class labels, and thus quantifying the applicability of the hypothesis across different datasets.

## 2 Preliminaries

Let \(\) denote the input space, and \(\) represent the set of labels. The joint probability distributions are represented by \(\) for training samples and \(\) for test samples. Assume we observe a labeled training dataset \(}\) and a unlabeled test dataset \(}\) containing \(N\) IID samples from \(\) and \(\), respectively. \(_{}\) denotes the marginal distribution of \(\) over \(\). \(}\) and \(}\) are described as

\[}=\{(_{i},y_{i})\}_{i=1}^{N}}}{{}},}=\{_{i} \}_{i=1}^{N}}}{{}}_{},\] (1)

where the label space \(\) encompasses all labels of samples from both \(\) and \(\). We also define \(\) as the hypothesis space and \((,)(0,B_{})\) as the bounded loss function. The expected and empirical risks  for a hypothesis \(h:\) on distribution \(\) and the dataset \(}\) are defined as \(_{}(h)\) and \(_{}(h)\), respectively, i.e.,

\[_{}(h)=_{}[ (h(),y)]&=_{}_{ }(h(),y)\,(,y)\,d \,dy,\\ _{}(h)=}_{ }[(h(),y)]&=}|}_{(,y)}}(h( ),y),\] (2)

where \(\) and \(}\) represent the expectation with respect to a data distribution and the sample average over a dataset, respectively. Accordingly, an algorithm \(\) aims to select the empirical risk minimizer \(_{}}\) after observing the samples from the training dataset \(}\) by

\[_{}}_{h}_{}(h),\] (3)

to approximate the optimal hypothesis \(h^{*}_{}\) selected from the distribution \(\) through \(h^{*}_{}_{h}_{}(h)\).

Our central research question is formulated as follows: _How can we quantify the applicability of the minimizer \(_{}\), originated from the training samples \(}\), to the unlabeled test samples \(}\)?_ To address this question quantitatively, we delve into methodologies for assessing the distribution discrepancy between training and test distributions, particularly focusing on doing so without the need to access class labels from test samples.

Importance divergence

For a hypothesis chosen by an algorithm after observing training samples, to assess its applicability to test samples in the absence of ground truth labels, we introduce the concept of Importance Divergence (I-Div). I-Div estimates the distribution discrepancy between training and test distributions for the given hypothesis. To address the issue of unavailable ground truth labels, it leverages importance sampling, density ratios, and likelihood ratios, facilitating the sampling transfer in the test distribution back to the training distribution.

In this section, we first present the distribution discrepancy with importance sampling, which transfers the data sampling from the test distribution to the training distribution. Following this, we discuss the methodologies for estimating the hypothesis-oriented density and adaptive likelihood ratios, essential components of this discrepancy, to facilitate this sampling transfer. The hypothesis-oriented density ratio is specifically tailored to each hypothesis, as it assesses the suitability of a specific hypothesis based on the resulting distribution discrepancy. The adaptive likelihood ratio is adjusted according to the density ratio to expedite the convergence of the distribution discrepancy. Lastly, we utilize an empirical estimator of the distribution discrepancy to evaluate the applicability of a hypothesis selected from training samples to test samples.

### Distribution discrepancy with importance sampling

For the hypothesis \(_{}\), I-Div evaluates the distribution discrepancy between training and test distributions without requiring the ground truth labels of the test samples. A smaller discrepancy implies that the training and test samples could be considered as drawn from the same distribution with respect to the given hypothesis, and vice versa. By using variational divergence, we can express this discrepancy as the difference between the expected risks of the hypothesis on training and test distributions by

\[d(,_{})=|_{ }(_{})-_{}(_{ })|.\] (4)

If the training and test distributions are aligned, i.e., \(=\), then the expected risks for the hypothesis \(_{}\) are similar, resulting in a minimal distribution discrepancy. This condition suggests that the test samples are likely ID for the hypothesis \(_{}\). Conversely, a notable difference between \(\) and \(\) indicates a larger distribution discrepancy, implying that the hypothesis \(_{}\) perceives the training and test samples as originated from distinct distributions, thus categorizing the test samples as OOD.

To make the expected risk difference in Eq. (4) more pronounced, I-Div employs Jensen's inequality to consider its upper bound by

\[d(,_{})_{} _{}|(,y)-( ,y)|(_{}(),y)\,d\,dy,\] (5)

thereby more distinctly highlighting the differences between the training and test distributions.

Recall that class labels in test samples are inaccessible, thus, the principal challenge is evaluating the expected risk for the test distribution with respective to the given hypothesis without access to ground truth labels. Since direct sampling from the test distribution is not feasible, an alternative is sampling from the training distribution. To overcome this limitation, we employ the importance sampling technique , converting the data sampling from test to training distributions, i.e.,

\[(,y)=(,y)}{( ,y)}(,y)=( )}{()}}_{r()}( y)}{(y)}_{v(,y)} (,y),\] (6)

where \(r\) and \(v\) denote a density ratio and a likelihood ratio, respectively. Thus, I-Div estimates distribution discrepancy between training and test distributions by merely sampling from the training one without accessing the class labels of the test samples. According to Eq. (5) and Eq. (6), we have

\[d(,_{},r,v) _{}[|r()v(,y)-1| (_{}(),y)].\] (7)

Accordingly, for a given labeled training dataset \(\) and a unlabeled test dataset \(\), we can then construct an empirical estimator for estimating the distribution discrepancy in Eq. (7) by

\[(},}_{ },r,v)}_{}[ |r()v(,y)-1|(_ {}(),y)].\] (8)However, to estimate the distribution discrepancy I-Div, the emerging challenge involves determining the density ratio \(r\) and likelihood ratio \(v\) from observed samples, which are discussed in Section 3.2 and Section 3.3, respectively.

### Hypothesis-oriented density ratio

The density ratio in the distribution discrepancy should be hypothesis-oriented. That is, it should depend on the specific hypothesis \(_{}\) selected by an algorithm \(\) on \(}\). This is because the criteria for judging the discrepancy between distributions \(\) and \(\) vary across different algorithms. For instance, whether two datasets of cats and dogs respectively come from the same distribution depends on whether the algorithm aims to identify if the subjects are biological entities or to distinguish between these two species. Accordingly, we apply a deep neural network to model a density ratio \(r\) from the hypothesis space \((_{})\) depending on the specific hypothesis \(_{}\). Then, we select a density ratio \(r(_{})\) to minimize the Kullback-Leibler divergence between the actual distribution and the estimated distribution based on this density ratio.

To construct the hypothesis space \((_{}})\), we utilize the output representations from \(_{}\) with a learnable component \(\) to model a density ratio \(r\). Specifically, we decompose the hypothesis into a backbone \(\) and a softmax layer \(\), represented as \(_{}()=()()\), where the output dimension of the backbone is \(O_{}\). Using a learnable component \(\), \(r\) is constructed as \(r()=()()\), where the learnable component \(()=(_{}_{})()\) contains an adapter \(_{}\) to introduce learnable parameters and a Softplus layer \(_{}\) to ensure strictly positive outputs. Specifically, the adapter \(_{}\), which follows the bottleneck \(\), comprises two fully connected layers with a Gaussian Error Linear Units (GELU)  activation layer in between. Furthermore, the Softplus layer is adopted to the adapter output, effectively mapping it to the range \((0,+]\). Furthermore, weight matrices are \(_{1}_{1}^{O_{} O_{m}}\) and \(_{2}_{2}^{O_{m} O_{}}\) in the fully connected layers. The activation functions GELU and Softplus are \(_{1}\)- and \(_{2}\)-Lipschitz, respectively. Additionally, we assume \(_{1}=_{_{1}_{1}}\|_{1}\|_{1,}\) and \(_{2}=_{_{2}_{2}}\|_{2}\|_{1,}\). Thus, the density ratio \(r(_{})\) can be modeled as

\[r()=(_{}_{}) ()=(1+(|}_{i[O_{}]} (_{})()_{i})),\] (9)

where \(O_{}\) represents the output dimensionality of the adapter. Without loss of generality, we further assume that \(r()(b_{r},B_{r})\) for any \(r(_{})\) and \(\).

To select a density ratio \(r(_{})\), we use it to estimate the density \(\) and \(\) by

\[}()=()/r(), }()=() r().\] (10)

Drawing on the inspiration of importance estimation methods [26; 27], we construct objectives and constraints around \(}()\) and \(}()\). The estimated probability distributions are designed to approximate their actual counterparts, suggesting the minimization of the two KL divergences with normalization constraints

\[_{r(_{})}& (()}( ))+(()}()),\\ &}()\,d =1,}()\,d=1.\] (11)

For convenience, we assume \(=/2+/2\) and define \(=\{1,-1\}\) as labels for training and test samples, respectively. A label \(c\), corresponding to a sample from \(\), indicates its distribution origin. The assignment \(c=1\) indicates that a sample originates from distribution \(\), while \(c=-1\) signifies that a sample comes from distribution \(\). We can then obtain the following objective function for learning the density ratio

\[f(r)=_{(,c)}[c r()+ \|(r())^{c}-1\|^{2}],\] (12)

where \( 0\) balances the KL divergence and normalization constraints, as the detailed derivation shown in Appendix B. We further assume that \(f(r)\) is \(L_{f}\)-lipschitz continuous with respect to \(r(_{})\). This objective function Eq. (12) can be estimated by

\[(r)=}_{(,c)}} [c r()+\|(r())^{c}-1 \|^{2}],\] (13)

where \(}=}}\). An empirical risk minimizer \(\) is selected by

\[_{r(_{})}(r),\] (14)

which aims to approximate the population risk minimizer \(_{r(_{})}f(r)\). The convergence rate of \(\) can be guaranteed by the following theorem.

**Theorem 3.1**.: _Let \( 1\) and \(\) be a constant related to the function \(f\). With a probability of at least \(1-\),_

\[_{}|()-( )|^{2}(_{1}_{2}_{1}_ {2}+1}+B_{r})}{}+_{2}L _{f}}{ N}:=(,N).\]

The presence of \(N\) in the denominators of both terms suggests that the bound tightens with an increasing sample size, which aligns with the general understanding that more data can lead to more accurate estimates in statistical learning.

### Adaptive likelihood ratio

Without making further assumptions, it is infeasible to estimate the likelihood ratio \(v\) due to its dependence on the unknown joint distribution \((,y)\). Instead of pursuing the true likelihood ratio, our goal is to approximate an adaptive likelihood ratio \(v()\) that enables a swift convergence of the distribution discrepancy, guided by the hypothesis-oriented density ratio \((_{})\). This strategy is valid since the density ratio captures the input distribution discrepancy between training and test distributions. It indicates that utilizing even a basic form of the covariate shift assumption, i.e., \(v(,y)=1\) for all \((,y)\), allows the distribution discrepancy, as calculated by Eq. (8), to approximate the difference between distributions to a reasonable degree. Furthermore, since the density ratio serves primarily to gauge the distribution discrepancy, the corresponding likelihood ratio must be specifically adapted to this density ratio for precisely assessing the distribution discrepancy.

Accordingly, we reveal the generalization error bound of the distribution discrepancy, leveraging both density and likelihood ratios, through the convergence rate of the hypothesis-oriented density ratio, Rademacher complexity  and Talagrand's contraction lemmas , which is shown as follows.

**Theorem 3.2**.: _Based on the conditions and results outlined in Theorem 3.1, with a probability of at least \(1-\), \(|d(,_{},,v)-(},} _{},,v)|\) is bounded by_

\[B_{},y)}}|()v(,y)-1|^{2}}{N}}+B_{}_{}[v(,y)]( /2,N)}{}}.\]

The result shows that the estimated distribution discrepancy converges quickly with increasing the sample size. Additionally, the bound is associated with the values of the likelihood ratio. Moreover, as per Eq. (6), for any \(r(_{})\), we have

\[(,y)\,d\,dy=_{}[r ()v(,y)]=1.\] (15)

Considering the terms in Theorem 3.2 related to \(v\) and the average error over the samples from \(\), we have

\[_{v()} }_{}[v(,y)+(()v(,y)-1)^{2}],\] (16) s.t. \[}_{}[()v( ,y)]=1,\]

where \(>0\) acts as a regularization parameter, influencing the trade-off, and the optimal solution is \(\). By using an proximal algorithm , we can obtain the following approximate solution

\[(,y)=()-1)}{( ())^{2}}}_{} }}[(})) ^{2}}{(})-1}].\] (17)We observe that in the two boundary cases where \( 0\) and \(+\), \(\) consistently equals \(1\), thus adhering to the covariate shift assumption. Even under these extreme conditions, it is feasible to calculate the distribution discrepancy using both the hypothesis-oriented density ratio and adaptive likelihood ratio to overcome the challenge of unlabelled test samples. This approach is viable because the hypothesis-oriented density ratio \(\) quantifies the distribution differences without relying on class labels, while employing \((0,+)\) utilizes the class labels of training samples. Although this method may not precisely determine the likelihood ratio for each instance, it is designed in accordance with a hypothesis-oriented density ratio such that the estimated distribution discrepancy aligns with the actual value, fulfilling our primary objective.

### Hypothesis applicability evaluation

I-Div quantifies the applicability of the hypothesis \(_{}\) selected by the algorithm \(\) from the training dataset \(}\) to the test dataset \(}\). Specifically, I-Div employs the empirical estimator in Eq. (8) with hypothesis-oriented density ratio \(\) in Eq. (14), and adaptive likelihood ratio \(\) in Eq. (17) to estimate the distribution between training and test distributions by

\[(},}_ {},,)=}_{}[|()(,y)-1 |(_{}(),y)],\] (18)

where \(\) is chosen from the hypothesis space \((_{})\) based on \(_{}\), and \(\) is selected from the space \(()\) based on \(\). A smaller discrepancy indicates that the training and test samples are likely drawn from the same distribution relative to \(_{}\). Since \(_{}\) minimizes the empirical risk on \(}\), a reduced distribution discrepancy improves the transferability of the hypothesis from training to test samples. On the other hand, a greater discrepancy suggests a reduced likelihood of hypothesis applicability. The I-Div methodology is detailed in Algorithm 1.

```
1:Input:
2:- Training samples \(}=\{(_{i},y_{i},c_{i}=1)\}_{i=1}^{N} \)
3:- Test samples \(}=\{(_{i},c_{i}=-1)\}_{i=1}^{N}\)
4:- Empirical minimizer \(_{}\), Hyperparameters \(\) and \(\)
5:Merge datasets: \(}=}}\)
6:Estimate the hypothesis-oriented density ratio on \(}\): \[_{r(_{})}}_{(,c)}}[c r() +\|(r())^{c}-1\|^{2}]\]
7:Estimate the adaptive likelihood ratio on \(}\): \[(,y)=(N()-N)/ ((())^{2})}_{ }}}[((( }))^{2})/((} )-1)]\]
8:Estimate the distribution discrepancy with importance sampling on \(}\): \[(},}_ {},,)=}_{}[|()(,y)-1 |(_{}(),y)]\]
9:Output: empirical estimator \((},}_ {},,)\) ```

**Algorithm 1** Importance divergence

## 4 Experimental results

This section presents a comparative analysis of I-Div 2 against existing methods for evaluating the distribution discrepancy between training and test samples. The detailed experimental setups are presented in Appendix D.1.

### Experiments on different classes

Our initial experiments focus on a relatively straightforward task: assessing the applicability of a hypothesis obtained on the training dataset to the test dataset with distinctly different class labels. We utilize two datasets, CIFAR10  and SVHN , each comprising ten semantically unique classes. For our experiments, we select samples from one class to serve as the test dataset, with the samples from the remaining nine classes forming the training dataset. This setup clearly illustrates that the knowledge learned in the training dataset cannot be transferred to the test dataset.

The results for CIFAR10 and SVHN are detailed in Table 1 and Table 6 (Appendix D.2), respectively. Our proposed I-Div algorithm consistently achieves perfect scores (\(100\%\)) in both AUROC and AUPR metrics across all classes of both datasets. This demonstrates its exceptional capability in distinguishing between training and test datasets, aligned with our initial hypothesis. The results unequivocally support the premise that the knowledge transfer from the training to the test datasets is ineffective, as evidenced by the flawless performance of I-Div. This starkly contrasts with the varying effectiveness of other algorithms, including NNBD and MMD-D. Notably, I-Div, MSP, and R-Div all yielded similarly impressive results. A key commonality among these algorithms is their reliance on a specific hypothesis to calculate distribution discrepancy, as opposed to NNBD and MMD-D, which use independent hypotheses. This highlights the significance of considering a particular hypothesis when evaluating distribution discrepancies. The rationale is that the hypothesis applicability depends on the specific design and its intended task.

### Experiments on different datasets

We now turn to a more complex scenario where the training and test datasets may share semantic similarities in class labels, indicating an overlap in the class label spaces. In cases where semantics differ significantly, we expect the algorithm to clearly differentiate the two kinds of samples. Conversely, if their semantics are similar, the algorithm may find it challenging to distinguish them. This outcome would suggest that the knowledge acquired from the training dataset is transferable to the test dataset, or it may indicate potential pathways to enhance the hypothesis generalization for the test distribution. We leverage CLIP  to align class labels between the training and test datasets, using the prompt template "A photo of a {label}." This helps adapt class labels across domains and captures semantic relationships between different datasets.

    &  &  &  &  &  &  \\   & & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\    & Airplane & **100.0** & **100.0** & 93.1 & 93.4 & 97.5 & 97.6 & **100.0** & **100.0** & **100.0** \\  & Automobile & **100.0** & **100.0** & 96.5 & 96.2 & 93.6 & 94.5 & **100.0** & **100.0** & **100.0** \\  & Bird & **100.0** & **100.0** & 90.4 & 90.0 & 97.2 & 97.6 & **100.0** & **100.0** & **100.0** \\  & Cat & **100.0** & **100.0** & 94.0 & 93.9 & 86.9 & 87.9 & **100.0** & **100.0** & **100.0** \\  & Dcer & **100.0** & **100.0** & 90.9 & 90.8 & 91.7 & 92.1 & **100.0** & **100.0** & **100.0** \\  & Dog & **100.0** & **100.0** & 95.5 & 93.3 & 95.9 & 96.4 & **100.0** & **100.0** & **100.0** \\  & Foo & **100.0** & **100.0** & 91.7 & 91.6 & 96.0 & 96.5 & **100.0** & **100.0** & **100.0** \\  & Horse & **100.0** & **100.0** & 91.9 & 91.8 & 82.8 & 83.4 & **100.0** & **100.0** & **100.0** & **100.0** \\  & Ship & **100.0** & **100.0** & 95.6 & 95.3 & 98.7 & 98.9 & **100.0** & **100.0** & **100.0** & **100.0** \\  & Teuck & **100.0** & **100.0** & 96.9 & 96.7 & 90.8 & 91.9 & **100.0** & **100.0** & **100.0** & **100.0** \\   

Table 1: Distribution discrepancy of different classes in CIFAR10. The larger the values of AUROC and AUPR, the better the performance.

    &  &  &  \\   & & & MSP & NNBD & MMD-D & R-Div & L-Div \\    & p & 94.7 & 100.0 & 99.4 & 95.8 & 100.0 & **39.7** \\  & A & 77.4 & 100.0 & 98.3 & 96.9 & 100.0 & **42.1** \\  & C & 74.3 & 100.0 & 98.2 & 95.2 & 100.0 & **41.5** \\  & S & 78.9 & 100.0 & 99.6 & 94.6 & 100.0 & **49.5** \\    & p & 76.1 & 100.0 & 97.2 & 94.6 & 100.0 & **44.8** \\  & A & 58.6 & 100.0 & 98.7 & 96.5 & 100.0 & **48.0** \\   & C & 48.5 & 100.0 & 98.8 & 95.8 & 100.0 & **51.4** \\   & R & 74.1 & 100.0 & 97.8 & 95.8 & 100.0 & **49.5** \\   

Table 2: Distribution discrepancy of domain adaptation data.

We conduct experiments using classic domain adaptation datasets: PACS  and Office-Home , each containing four domains. We designate one domain as the training dataset and merge the remaining three as the test dataset. The results, presented in Table 2, indicate that each hypothesis selected from a dataset performs significantly better than randomly selected hypotheses, demonstrating its applicability. Our I-Div algorithm aptly reflects this, in contrast to other algorithms that overly emphasize distribution discrepancies, thereby rigidly categorizing the difference between training and test datasets.

We use CIFAR10  as the training dataset and evaluate on diverse test datasets including Randomly Generated Images (RGI), SVHN , DTD , Flowers102 , OxfordIIITPet , SEMEION , Caltech256 , CIFAR100 , CIFAR10.1 , and STL10. Since CIFAR10.1 and STL10 share similar category spaces with CIFAR10, the model shows minimal differentiation for these datasets, with ACC values of \(73.4\%\) and \(42.1\%\), respectively. This suggests partial knowledge transferability. However, I-Div demonstrates lower AUROC values of \(43.7\%\) and \(37.2\%\), indicating reduced discrimination. Other algorithms like MSP and R-Div show higher AUROC values, near \(100\%\) across all datasets, but these results suggest an overemphasis on distribution discrepancy rather than semantic similarity. I-Div, in contrast, better captures semantic relationships between datasets, providing a more nuanced view of class label semantics.

We use ImageNet  as the training dataset and evaluate on diverse test datasets using ResNet50  and ViT-B/16 . The test datasets include the Open Images Dataset v4 (OIDv4) , Caltech256 , Flowers102 , and DTD . The experimental results presented in Table 4 show that the AUROC values of I-Div effectively capture the semantic similarity between ImageNet and the test datasets, yielding results that closely align with human intuition. For example, I-Div demonstrates lower AUROC values for OID and Caltech256, reflecting their semantic overlap with ImageNet, as these datasets share common object categories and scene types. In contrast, datasets such as Flowers102 and DTD, which focus on more specialized object categories and textures, show higher AUROC values with I-Div, indicating greater divergence from ImageNet. On the other hand, algorithms like MSP, NNBD, and MMD-D show consistently high AUROC values across most datasets, implying they emphasize distribution discrepancies over semantic relationships. This limits their effectiveness in distinguishing nuanced semantic differences compared to I-Div, which provides a more human-aligned understanding of dataset relationships.

   Training & Network & Test & ACC (CLIP) & MSP & NNBD & MMD-D & H-Div & R-Div & I-Div \\   & OIDv4 & 43.9 & 100.0 & 91.7 & 94.6 & 100.0 & 94.6 & **69.3** \\  & ResNet50 & Caltech256 & 36.6 & 100.0 & 91.4 & 95.6 & 100.0 & 100.0 & **72.4** \\  & Flowers102 & 5.1 & 100.0 & 98.6 & 100.0 & 100.0 & 100.0 & **100.0** \\  & DTD & 11.9 & 100.0 & 98.7 & 100.0 & 100.0 & 100.0 & **100.0** \\   &  & OIDv4 & 50.6 & 100.0 & 88.6 & 92.6 & 100.0 & 92.6 & **62.6** \\  & Caltech256 & 40.4 & 100.0 & 94.8 & 100.0 & 100.0 & 100.0 & **71.9** \\   & Flowers102 & 5.1 & 100.0 & 98.1 & 100.0 & 100.0 & 100.0 & **100.0** \\   & & DTD & 13.9 & 100.0 & 99.7 & 100.0 & 100.0 & 100.0 & **100.0** \\   

Table 4: Distribution discrepancy between ImageNet and other test datasets.

    & ACC &  &  &  &  &  \\   & (CLIP) & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\   RGI & 0.0 & **100.0** & **100.0** & **100.0** & **100.0** & 99.2 & 99.3 & **100.0** & **100.0** & **100.0** & **100.0** \\ SVHN & 17.0 & **100.0** & **100.0** & **100.0** & **100.0** & 94.8 & 95.7 & **100.0** & **100.0** & **100.0** & **100.0** \\ DTD & 1.9 & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\ Flowers102 & 1.6 & **100.0** & **100.0** & **100.0** & 97.0 & 98.8 & **100.0** & **100.0** & **100.0** & **100.0** \\ COVIDIITPET & 2.3 & **100.0** & **100.0** & **100.0** & 90.8 & 97.0 & 100.0 & **100.0** & **100.0** & **100.0** \\ SEMEION & 8.7 & **100.0** & **100.0** & **100.0** & **100.0** & 98.5 & 99.0 & **100.0** & **100.0** & **100.0** & **100.0** \\ Caltech256 & 2.4 & **100.0** & **100.0** & **100.0** & **100.0** & 92.3 & 92.4 & **100.0** & **100.0** & 99.9 & 99.9 \\ CIFAR100 & 2.2 & **100.0** & **100.0** & 99.9 & 90.2 & 91.5 & **100.0** & **100.0** & 94.6 & 94.7 \\ CIFAR10.1 & **73.4** & 100.0 & 100.0 & 92.1 & 93.7 & 92.9 & 93.1 & 100.0 & 100.0 & **43.4** & **45.2** \\ STL10 & **63.0** & 100.0 & 100.0 & 94.0 & 93.3 & 90.4 & 91.1 & 100.0 & 100.0 & **37.2** & **41.9** \\   

Table 3: Distribution discrepancy of different datasets on ResNet18. For CIFAR10.1 and STL10, smaller values of AUROC and AUPR indicate better performance. However, for other test datasets, larger values are better.

### Experiments on corrupted data

This section discusses experimental results on corrupted datasets. We progressively introduce noise into a dataset that serves as the training one, treating the resultant corrupted samples as the test dataset. Intuitively, as the noise level increases, the hypothesis performs worse, which indicates the hypothesis becomes less applicable to the corrupted data. To conduct this experiment, CIFAR10 serves as the training dataset, with incremental addition of noises to the original dataset to create the test dataset. The types of noises  used include Gaussian, Salt & Pepper, Uniform, and Speckle, with the noise rate increasing from \(0.1\) to \(0.9\) with a \(0.1\) interval. The methods for comparison include Hypothesis-oriented Density Ratio (HDR) in I-Div and R-Div . Fig. 1 presents our experimental findings, showing key performance metrics as influenced by varying noise rates. Notably, the classification performance declines with increasing noise, impacting the hypothesis predictive accuracy. Interestingly, our proposed I-Div algorithm demonstrates robustness against these challenges, with its discrimination power inversely related to the classification accuracy of the standard network in noisy conditions. A brief comparative analysis hints at the superior performance of I-Div over HDR, especially in relation to hypothesis applicability in noisy test datasets. Fig. 4 in Appendix D.3 shows the results when noise is added to the training data instead of the test data. The results are consistent with the above, as the classification accuracy decreases with increasing noise, and I-Div becomes more effective in distinguishing between clean training data and noisy test data. For a comprehensive discussion and full experimental results, please see Appendix D.3.

### Experiments on adversarial data

In this experiment, we delve into a specific scenario involving adversarial samples . We designate one dataset as training and its corresponding adversarial samples as the test dataset. It is a well-known phenomenon that a minimal adversarial perturbation, though visually imperceptible, can drastically alter the classification performance of a network. This suggests potential issues with the direct applicability of the hypothesis selected for a training dataset to a test dataset. However, based on human perception, which fails to distinguish original and adversarial samples visually, we would expect a negligible distribution discrepancy between the distributions of the original and adversarial samples. This outcome could guide us in enhancing network robustness against adversarial attacks and in generalizing the hypothesis to adversarial contexts. For this purpose, we use the CIFAR10 dataset to train a standard network, with adversarial perturbation magnitudes selected from the set \(\{0.001,0.01,0.1,0.3\}\). The results in Fig. 2 indicate a marked decrease in standard network

Figure 1: Distribution discrepancy between original data and its corrupted variants with different noise rate. (a) shows the classification performance of the standard network for the test datasets containing corrupted samples. (b)(c)(d) present the distribution discrepancy in terms of AUROC.

Figure 2: Distribution discrepancy between original data and adversarial data.

accuracy against adversarial perturbations, while the consistent AUROC of I-Div suggests its limited differentiation capability. Detailed results and analyses are provided in Appendix D.4.

### Experiments with different sample sizes and network architectures

We examine the impact of different sample sizes on the performance of the I-Div algorithm, focusing on its ability to generalize hypotheses from training to test datasets. Fig. 3 show that I-Div tends to maintain low AUROC values for semantically similar datasets like CIFAR10.1 and STL10, indicating effective hypothesis applicability. Conversely, for datasets with significant semantic differences, the performance of I-Div improves with larger sample sizes, highlighting its capacity to recognize non-transferable knowledge. Additionally, we investigate the effect of varying network architectures as shown in Table 5. Detailed results are provided in Appendix D.5.

## 5 Limitations

I-Div relies on density and likelihood ratios to achieve the sampling transfer in unlabeled test data, allowing for the estimation of distribution discrepancies between training and test datasets with labeled training samples. Although the density ratio can be accurately estimated using inputs from both training and test samples, the likelihood ratio cannot be estimated precisely due to the unavailability of class labels of test samples. The strategy used by I-Div targets the estimation of distribution discrepancies between training and test distributions. It optimizes a likelihood ratio that adapts to this density ratio to ensure a rapid convergence of the distribution discrepancy, by minimizing the upper bound of the generalization error based on the density ratio. Our future research includes refining the estimation methods for likelihood ratios and exploring distribution discrepancy estimation methods that can bypass the likelihood ratio.

## 6 Conclusion

In the realm of complex data and machine learning tasks, a crucial question arises regarding the applicability of a hypothesis derived from a training dataset to a test dataset. This uncertainty, especially challenging when test samples lack class labels, significantly determining the hypothesis generalization. To address this, we introduce the I-Div measure for estimating the distribution discrepancy between training and test distributions. I-Div involves the hypothesis-oriented density ratio and adaptive likelihood ratio in expected risk difference to shift the sampling problem from test to training distributions. Experimentally, we validate that I-Div can effectively assess the hypothesis capability of handling test samples, yielding results consistent with prior human knowledge.

    &  &  &  &  \\   & ACC & AUROC & AUPR & ACC & AUROC & AUPR & ACC & AUROC & AUPR & ACC & AUROC & AUPR \\   RGI & 0.0 & 100.0 & 100.0 & 0.0 & 73.0 & 70.0 & 0.0 & 100.0 & 100.0 & 100.0 & 99.6 & 98.2 \\ SVIN & 17.0 & 100.0 & 100.0 & 15.6 & 74.5 & 69.8 & 20.0 & 100.0 & 100.0 & 19.8 & 98.1 & 93.7 \\ DTD & 1.9 & 100.0 & 100.0 & 2.3 & 76.3 & 71.6 & 2.5 & 100.0 & 100.0 & 1.8 & 97.0 & 90.8 \\ FlowNet102 & 1.6 & 100.0 & 100.0 & 2.0 & 78.3 & 73.5 & 2.4 & 100.0 & 100.0 & 2.7 & 96.7 & 89.8 \\ OgoDIIIFT & 2.3 & 100.0 & 100.0 & 0.9 & 69.8 & 67.3 & 1.6 & 100.0 & 100.0 & 1.7 & 97.4 & 91.8 \\ SEIMCNN & 8.7 & 100.0 & 100.0 & 7.8 & 78.4 & 73.3 & 9.6 & 100.0 & 100.0 & 10.4 & 98.1 & 93.5 \\ CATre10256 & 2.4 & 99.9 & 99.9 & 2.5 & 72.4 & 67.0 & 2.3 & 99.2 & 99.2 & 2.0 & 96.8 & 94.3 \\ CIFAR100 & 2.2 & 94.6 & 94.7 & 2.6 & 66.0 & 60.3 & 2.9 & 83.3 & 83.0 & 1.9 & 89.5 & 79.7 \\ CIFAR101 & 73.4 & 43.4 & 45.2 & 82.5 & 34.8 & 40.7 & 70.0 & 45.8 & 47.1 & 74.2 & 44.4 & 49.8 \\ STL10 & 63.0 & 37.2 & 41.9 & 72.0 & 50.1 & 48.5 & 60.6 & 43.8 & 31.1 & 63.9 & 39.5 & 47.5 \\   

Table 5: Effect of different network architectures.

Figure 3: Effect of different sample sizes.