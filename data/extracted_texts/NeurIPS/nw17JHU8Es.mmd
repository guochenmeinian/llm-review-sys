# MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis

Sagnik Anupam

MIT

sanupam@mit.edu

&Maddy Bowers

MIT

mlbowers@mit.edu

&Omar Costilla-Reyes

MIT

costilla@mit.edu

&Armando Solar-Lezama

MIT

asolar@csail.mit.edu

###### Abstract

We present MathDSL, a Domain-Specific Language (DSL) for mathematical equation solving, which, when deployed in program synthesis models, outperforms state-of-the-art reinforcement-learning-based methods. We also introduce a quantitative metric for measuring the conciseness of a mathematical solution and demonstrate the improvement in the quality of generated solutions compared to other methods. Our system demonstrates that a program synthesis system (DreamCoder) using MathDSL can generate programs that solve linear equations with greater accuracy and conciseness than using reinforcement learning systems. Additionally, we demonstrate that if we use the action spaces of previous reinforcement learning systems as DSLs, MathDSL outperforms the action-space-DSLs. We use DreamCoder to store equation-solving strategies as learned abstractions in its program library and demonstrate that by using MathDSL, these can be converted into human-interpretable solution strategies that could have applications in mathematical education.

## 1 Introduction

Building machine learning models that can replicate human reasoning abilities in symbolic domains, such as algebra or arithmetic, is a challenging problem that researchers continue to face today . Even large models that exhibit state-of-the-art performance on language modelling datasets, like GPT-4, exhibit much poorer performance on mathematical exams involving reasoning tasks . Improving these systems to perform well on reasoning tasks often requires extensive usage of techniques such as chain-of-thought reasoning and training on large datasets of mathematical data, which is expensive in both time and cost.

Improving machine learning models' mathematical reasoning ability may yield significant interpretability that can provide educational benefits, as studies have shown that automated tutor systems can yield similar or greater educational gains than human tutors . Additionally, such improvements, when extended to more complex mathematical domains, can aid in developing software for researchers by helping describe the behavior of previously unknown functions . Thus, an important research goal is to develop machine-learning systems that write step-by-step solutions to mathematical problems while being as efficient as possible regarding training data and compute.

For simple algebraic domains like linear equations, many step-by-step solvers rely on manually written heuristics. However, those have been shown to have surprising blind spots that fail to account for solving rare equation cases . In recent years, attempts to help models learn neurosymbolic reasoning in mathematical domains have led to the development of specific reinforcement learningtechniques, like the Contrastive Policy Learning (ConPoLe) algorithm . When presented with an equation, ConPoLe uses reinforcement-learning techniques to query the search space and find the most promising next step until it synthesizes a complete step-by-step solution.

However, in the linear equations domain, the solutions generated using ConPoLe often tend to be elaborate and unwieldy , and often contain unnecessary steps that may confuse program users, as demonstrated by the ConPoLe solution in Figure 1. Recent research has demonstrated that humans find solutions simplified using skill-based, higher-level abstractions very useful . One approach that uses this idea is a theorem-proving language, Peano , which changes ConPoLe's action space to a finite set of valid axioms and discovers tactics by analyzing a batch of ConPoLe-generated solutions. However, solution discovery in Peano is limited to a few types of equations due to the limitations of the tactic language, and in practice, the discovered tactics are quite simple . Another approach, Lemma , leverages the idea of abstraction learning by examining several ConPoLe solutions generated on a training dataset (using its original action space) and building abstractions that ConPoLe can use to solve equations on a test dataset. However, Lemma requires training on a large dataset of previously generated ConPoLe solutions to build high-quality abstractions. Hence, it is not able to leverage the power of the abstractions the first time it solves tasks in a new domain.

## 2 Methods

Previous research on developing machine-learning models for mathematical equation solving has shown that neural models perform poorly on arithmetic tasks unless task-specific components are used . To improve the performance of neural models, recent approaches have examined if it is possible to use these models in sequence-to-sequence contexts by first converting mathematical expressions into Abstract Syntax Trees (ASTs), and then into prefix sequences derived from these trees . Extending this idea, MathDSL modifies the problem domain from algebra to _algebraic manipulation_ and uses primitives that describe operations performed on the prefix form of equations.

Our approach involves searching the space of all possible programs in MathDSL to find the correct program that converts the input from the prefix form of the equation to the solution state string. Thus, the entire solution to the equation is synthesized at once, unlike previous reinforcement learning systems, which attempt to find the next step in the solution by choosing from the existing action space. For searching the space of MathDSL programs, we use DreamCoder , a library-learning framework designed for use on inductive program synthesis problems . We use DreamCoder with the Stitch library learning algorithm  (as described in ). A complete description of the program synthesis system and its components has been provided in Appendix A.

We provide DreamCoder with MathDSL, our DSL that encodes a set of mathematical axioms, along with some elementary equation manipulation operations, as basic programming primitives. These primitives can then be composed to form more complex operations that can fully convert a linear

Figure 1: Comparison of ConPoLe solutions with more concise DreamCoder+Stitch+MathDSL and Lemma solutions. The ConPoLe solution contains unnatural subroutines, while DreamCoder+MathDSL and Lemma offer more human-like strategies.

equation into its corresponding solution state. MathDSL performs manipulations on equations in prefix form and comprises three types of primitive operations: _tree operations_, _arithmetic operations_, and _index operations_. _Tree operations_, such as tree rotations and distributive operations, are defined as operations performed on the equation's tree structure that rearrange the equation tree to make future simplifications easier without introducing additional nodes. _Arithmetic operations_, like dividing or multiplying both sides of the equation by a term, introduce additional nodes in the equation tree. _Index operations_ are helper operations used to help determine the indices for accessing the specific subtrees of an equation AST on which tree and arithmetic operations act. In MathDSL, arithmetic operations can only accept arguments that are subtrees of the current tree, and tree operations can only be performed on the current equation tree's subtrees. This constraint helps us avoid having to discretize the space of integers or real numbers in the DSL, which would make the search space much larger and make program synthesis prohibitively expensive. In our experiments, the index operations in MathDSL can generate integer constants from 0-110, a range much larger than the number of subtrees in any equation in our dataset. The complete list of MathDSL's primitive operations is provided in Appendix B, and an example program for solving an equation is provided in Figure 2.

## 3 Conciseness Metric

Here, we describe a metric to measure the conciseness of solutions generated by different equation-solving systems. We use an AST-based metric to ensure that large expressions were not added or transposed to different sides of the equations. We define the metric function as follows, for a solution \(s\) with \(n\) steps of an equation \(e\):

\[f(s)=_{i=1}^{n-1}max(|s_{i}.left-s_{i+1}.left|,|s_{i}.right-s_{i+1}.right|,1)\]

Here, \(s_{i}\) refers to the equation tree of the equation at the \(i\)-th step of the solution, while \(x.left\) and \(x.right\) refer to the **size** of the left and right subtrees of \(x\), respectively, where the size is defined as the number of subtrees of a tree (including the original tree). For a given equation, a solution with a smaller value of \(f(s)\) indicates a more concise solution.

As demonstrated in Figure 1, solutions that introduce large terms into the equation tree without prior simplification make the solution more difficult to interpret, especially in a pedagogical context. Hence, this metric function penalizes solutions that introduce complicated expressions in a solution step. Since this function assumes a cost of at least 1 occurring per step and does not normalize for length, it rewards shorter solutions over longer solutions. As a result, the function penalizes solutions that take many steps to simplify expressions without changing tree size (such as the ConPoLe solution in Figure 1). However, it can also reward overly-compressed solutions. A description of metric function limitations can be found in Appendix C.

For a given equation \(e\), a target model \(A\), and a baseline model \(B\), we use the metric function to compare two solutions to the same equation by measuring the relative improvement or decline in the metric compared to the baseline model. Let \(s_{A}\) and \(s_{B}\) denote the solutions generated by the target model and the baseline model, respectively. Then, the relative improvement or decline in performance is measured by the score:

\[C(s_{A},s_{B}|s_{A},s_{B}e)=)-f(s_{A})}{f(s_{B})}\]

We refer to this score as the C-score of \(A\) and \(B\) on \(e\). We report the mean C-score over all equations solved by both the target and the baseline models. Positive mean C-scores indicate that the target model generates more concise solutions than the baseline model on average. In contrast, negative mean C-scores indicate that the target generates less concise solutions than the baseline on average. To measure conciseness in our experiments, we take ConPoLe as our baseline model and compute the C-scores of Lemma and DreamCoder (considering each DSL separately) with respect to ConPoLe.

## 4 Experiments and Results

For evaluating the performance of MathDSL in our program synthesis system and compare its performance to Lemma and ConPoLe in terms of accuracy and conciseness, the models were evaluated on a variant of the Cognitive Tutor Algebra dataset . We note that it is extremely difficult to normalize the performance of DreamCoder and its reinforcement learning alternatives,since DreamCoder has previously shown the ability to learn useful abstractions from a few carefully chosen tasks , while both ConPoLe and Lemma rely on generating \( 10^{6}\) equations from the equation templates during training. Instead of training Lemma and ConPoLe from scratch on our training set, which may reduce their efficacy by training them on a smaller set of equations, we simply evaluated the final trained models on the infix-notation form of our train and test datasets to observe if the models can discover the equation solutions. Our evaluation also allowed Lemma to use all the abstractions it had discovered during its original training. Additional details about the experiments are in Appendix D, and some abstractions discovered by DreamCoder are described in Appendix E.

Additionally, to demonstrate that the improved performance is due to using the MathDSL and not due to DreamCoder being an inherently more powerful system, we also run experiments with DreamCoder using ConPoLe and Lemma's action spaces as DSLs. In our first experiment, we treat each axiom listed by  in Appendix A as a separate primitive in a new DSL (titled ConPoLeDSL). In our second experiment, in addition to the aforementioned axioms, we use the 15 abstractions discovered by  as primitives in another DSL (titled LemmaDSL). The performance of the different model experiments (DreamCoder (with MathDSL, ConPoLeDSL, and LemmaDSL), ConPoLe, and Lemma) in terms of accuracy and the conciseness metrics (evaluated by comparing against ConPoLe as the baseline model) are presented in Table 1.

As expected, both Lemma, DreamCoder+Stitch+MathDSL, and DreamCoder+Stitch+LemmaDSL's conciseness metrics have positive values, since they generate shorter solutions than ConPoLe does. A ConPoLe target with a ConPoLe baseline will give C-scores of 0 for all equations as \(s_{A}=s_{B} f(s_{A})=f(s_{B})\). Since ConPoLeDSL and LemmaDSL have a large number of primitives performing small modifications to the equation, solution programs in those DSLs are long and difficult to discover, leading to lower overall accuracy. Thus, we can conclude that DreamCoder, when used with MathDSL, is able to generate solutions to a large number of equations in the dataset using much fewer training examples as compared to ConPoLe and Lemma. Additionally, the solutions generated by composing DreamCoder's MathDSL abstractions together tend to be much more concise in nature as evidenced by the average percentage decrease in the metric function value when compared with baseline models such as ConPoLe and Lemma. We also observe that after further post-processing of DreamCoder solutions to remove identical steps (caused by intermediate abstractions that do not change the state of the program), the mean C-Scores of MathDSL increase to 0.6237 for the training set and 0.5521 for the test set respectively. Further details on the solution generation procedure and the step de-duplication are provided in Appendix D. These results confirm that MathDSL, when combined with DreamCoder, facilitates the discovery of concise, reusable abstractions, leading to more efficient, human-interpretable and accurate equation-solving strategies.

## 5 Conclusions

In this paper, we introduced MathDSL, a domain-specific language designed for solving linear equations via program synthesis. Our results demonstrate that DreamCoder combined with MathDSL significantly outperforms existing models like ConPoLe and Lemma in terms of both accuracy and solution conciseness. Specifically, DreamCoder+MathDSL achieved a testing set accuracy of **90.70%** and produced solutions that were on average **48.59%** more concise than those generated by ConPoLe, as evidenced by the C-score metric. Furthermore, MathDSL proved to be highly efficient in terms of training data, requiring only **198** equation templates compared to the \( 10^{6}\) equations required by ConPoLe and Lemma. The system also produced human-interpretable solution strategies, unlike ConPoLe, which lacks a structured abstraction mechanism to yield human-interpretability.

  Model Name & Training Set & Testing Set & Training & Testing \\  & Accuracy & Accuracy & Set Mean & Set Mean \\  & & & C-Score & C-Score \\  DreamCoder + Stitch + MathDSL & **0.9192** & **0.9070** & **0.5921** & **0.4859** \\ Lemma\({}^{*}\) & \(0.7980\) & \(0.8488\) & 0.5758 & 0.4752 \\ ConPoLe\({}^{*}\) & \(0.8182\) & \(0.8372\) & _0.0_ & _0.0_ \\ DreamCoder + Stitch + ConPoLeDSL & 0.0707 & 0.1047 & -0.7932 & -1.3611 \\ DreamCoder + Stitch + LemmaDSL & 0.3182 & 0.3488 & 0.5074 & 0.3829 \\  

Table 1: Accuracy and C-Scores (out of 198 randomly sampled training set problems and 86 test set problems). The \({}^{*}\) indicates that the model was not trained on this exact training set. Italicized text indicates that the ConPoLe solutions were the baseline solutions, and are expected to have a Mean C-Score of 0.

Our experiments using ConPoLeDSL and LemmaDSL confirmed that MathDSLs design, rather than the underlying DreamCoder framework alone, was responsible for the improved performance. The experiments showed that neither ConPoLeDSL nor LemmaDSL enabled DreamCoder to generate concise solutions with comparable accuracy.

In conclusion, MathDSL offers a novel approach for generating interpretable and concise solutions to mathematical equations, with broader applications in educational tools and automated reasoning systems. Future work can explore extending MathDSL to more complex mathematical domains, such as calculus or discrete mathematics, and its potential applications in automated tutoring systems where human-like solution strategies are valuable.