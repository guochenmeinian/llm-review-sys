# Perplexity-aware Correction for

Robust Alignment with Noisy Preferences

 Keyi Kong\({}^{1,*}\)  Xilie Xu\({}^{2,*}\)  Di Wang\({}^{3}\)  Jingfeng Zhang\({}^{4,5,}\)  Mohan Kankanhalli\({}^{2}\)

\({}^{1}\)Shandong University \({}^{2}\)National University of Singapore

\({}^{3}\)King Abdullah University of Science and Technology \({}^{4}\)The University of Auckland

\({}^{5}\)RIKEN Center for Advanced Intelligence Project (AIP)

luxinyayaya@mail.sdu.edu.cn, xuxilie@comp.nus.edu.sg

di.wang@kaust.edu.sa, jingfeng.zhang@auckland.ac.nz, mohan@comp.nus.edu.sg

 Equal contribution.Corresponding author.

###### Abstract

Alignment techniques are critical in ensuring that large language models (LLMs) output helpful and harmless content by enforcing the LLM-generated content to align with human preferences. However, the existence of noisy preferences (NPs), where the responses are mistakenly labelled as chosen or rejected, could spoil the alignment, thus making the LLMs generate useless and even malicious content. Existing methods mitigate the issue of NPs from the loss perspective by adjusting the alignment loss based on a clean validation dataset. Orthogonal to these loss-oriented methods, we propose perplexity-aware correction (PerpCorrect) from the data perspective for robust alignment which detects and corrects NPs based on the differences between the perplexity of the chosen and rejected responses (dubbed as PPLDiff). Intuitively, a higher PPLDiff indicates a higher probability of the NP because a rejected/chosen response which is mistakenly labelled as chosen/rejected is less preferable to be generated by an aligned LLM, thus having a higher/lower perplexity. PerpCorrect works in three steps: (1) PerpCorrect aligns a surrogate LLM using the clean validation data to make the PPLDiff able to distinguish clean preferences (CPs) and NPs. (2) PerpCorrect further aligns the surrogate LLM by incorporating the reliable clean training data whose PPLDiff is extremely small and reliable noisy training data whose PPLDiff is extremely large after correction to boost the discriminatory power. (3) Detecting and correcting NPs according to the PPLDiff obtained by the aligned surrogate LLM to obtain a denoised training dataset for robust alignment. Comprehensive experiments validate that our proposed PerpCorrect can achieve state-of-the-art alignment performance under NPs. Notably, PerpCorrect demonstrates practical utility by requiring only a modest amount of validation data and being compatible with various alignment techniques. Our code is available at PerpCorrect.

## 1 Introduction

Alignment enables the safe utilization of the remarkable capabilities acquired by large language models (LLMs) through self-supervised learning on vast corpora [6; 24; 4]. It refers to the process of ensuring that the contents generated by LLMs are helpful, harmless, and aligned with human values and preferences . Reinforcement Learning from Human Feedback (RLHF)  has emerged as a primary technique for achieving alignment. Current technical routes [39; 40; 29] require a reward model to simulate human preference and use it to optimize the policy model outputs withProximal Policy Optimization (PPO) . Current offline techniques such as Direct Preference Optimisation (DPO) , Sequence Likelihood Calibration with Human Feedback (SLiC)  and Identity-Preference Optimisation (IPO) , could directly align LLMs without intensively training a reward model as employed in RLHF.

Recent studies [34; 8] have shown there exist noisy preferences (NPs) that may lead to significant degradation in alignment performance. The issue of NPs, where the label of the actually chosen/rejected responses in training datasets is flipped as rejected/chosen, can arise from the biases of annotators  and malicious noise injection . As shown in Figure 1, when NPs are randomly injected into the training dataset, the conventional alignment method (e.g., DPO  and PPO ) will have significantly degraded alignment performance measured by the reward accuracy. Such performance degradation could result in the generation of useless and even malicious content . Therefore, it necessitates developing robust alignment methods that can utilize datasets with NPs to effectively align the LLMs with human preferences.

Existing robust alignment methods are proposed from the loss perspective, which adjust the alignment loss using a clean validation dataset to mitigate the issue of NPs. In particular, the conservative DPO (cDPO)  and robust-DPO (rDPO)  both estimate the proportion of NPs using the clean validation data via cross-validation and then adjust the original DPO loss based on the estimated proportion of NPs. However, Mitchell  and Chowdhury et al.  overlooked the essential differences between noisy and clean preferences, which is critical for mitigating the issue of NPs.

To this end, we propose **Perplexity-aware Correction** (PerpCorrect) for robust alignment from the data perspective by leveraging the differences between noisy and clean preferences for robust alignment. PerpCorrect detects and corrects NPs based on the difference between the perplexity of the chosen response and that of the rejected counterparts (dubbed as PPLDiff) obtained by an aligned surrogate LLM using the clean validation set. If an NP is detected, PerpCorrect will correct it by flipping the label of the rejected/chosen responses as chosen/rejected. Intuitively, rejected responses which are mistakenly labelled as chosen have a higher perplexity since they are less consistent with human preferences and thus have a lower probability of being generated after alignment. Therefore, a higher value of PPLDiff indicates a higher probability of the preferences being noisy. In this way, PerpCorrect leverages the differences between noisy and clean preferences (CPs) identified by PPLDiff to detect NPs.

To make the PPLDiff able to distinguish CPs and NPs, PerpCorrect requires an aligned surrogate LLM for calculating PPLDiff. The density of PPLDiff obtained on the noisy training dataset using

Figure 1: We evaluated various robust alignment methods under different proportions of noisy preferences using the Llama2-7B model, on the Golden HH dataset. The reward accuracy of both the vanilla DPO and PPO method significantly decreases as the proportion of noisy preferences increases. Our method, perplexity-aware correction (PerpCorrect), outperforms both the DPO and PPO series baselines across different proportions of noisy preferences.

an unaligned surrogate LLM, which can be fitted as a normal distribution centered around zero (evidenced in Figure 1(a)), cannot discriminate CPs and NPs. Therefore, we align a surrogate LLM using the clean validation data. The density of PPLDiff obtained by the aligned surrogate LLM in Figure 1(b) can be fitted into two distinguishable normal distributions, thus being able to differentiate CPs and NPs.

However, there still exists a large overlap between two normal distributions after aligning only on the clean validation dataset, which could result in an unsatisfactory accuracy of NP detection. To this end, we iteratively align the model using more reliable clean training data with extremely low PPLDiff (located in the green area in Figure 1(c)) and reliable noisy training data with extremely large PPLDiff (located in the red area in Figure 1(c)) sampled from noisy training datasets. Finally, the two normal distributions are significantly separated as shown in Figure 1(d), which indicates that PPLDiff has an enhanced discriminatory power.

Benefiting from the strong discriminatory power of PPLDiff calculated by the aligned surrogate LLM, PerpCorrect outputs a denoised training dataset for robust alignment by first detecting NPs based on a PPLDiff threshold and then correcting them. The data, whose PPLDiff is below a certain threshold (i.e., the black dotted line in Figure 1(d)) selected as the x-coordinate of the two normal distributions' intersection, are identified as NPs and thus corrected by flipping the response's label. Notably, our proposed PerpCorrect is compatible with various alignment methods as well as robust alignment methods [21; 8] since the metric PPLDiff is agnostic to training algorithms and only requires an arguably small number of clean validation data (~50), thus making it practical.

Comprehensive empirical results, evaluated using the Llama2-7B  and phi-2  models on the OpenAssistant Conversations (OASST1)  and Golden HH  datasets, validate the effectiveness of our proposed PerpCorrect method in robustifying alignment with NPs. We empirically validate that PerpCorrect consistently obtains state-of-the-art performance among various proportions of NPs. Besides, we empirically demonstrate that PerpCorrect can effectively robustify various alignment techniques and robust alignment methods, validating its compatibility.

## 2 Literature Review and Preliminary

In this section, we introduce the related work about LLM alignment and provide preliminaries about the noisy preferences, perplexity, as well as various alignment methods.

### LLM Alignment

In the domain of aligning LLMs with human preferences, pairwise preference methods are favored due to their lower cognitive burden on evaluators. Traditional online alignment approaches [32; 24; 29] involve training reward models from these preferences to provide signals in reinforcement learning. Recent offline alignment methods like Direct Preference Optimization (DPO) , Sequence Likelihood Calibration (SLiC) , and Identify Preference Optimization (IPO)  streamlined this process by directly using preference pairs to train LLMs, thus enhancing performance and reducing computational costs. Additionally, methods like RRHF  align LLMs using multiple ranked preferences, Kahneman-Tversky Optimization (KTO)  align LLMs using a single preference labeled as good or bad, and Rejection Sampling Optimization (RSO)  address DPO's limitation in sampling preference pairs from the optimal policy through rejection sampling. However, NPs, arising from the biased human feedback, can determine the alignment performance [24; 34]. Robust alignment methods like conservative DPO (cDPO) , robust DPO (rDPO)  have been proposed to address these issues from the loss perspective. Our approach focuses on the data perspective to address these issues of NPs and is orthogonal to these robust alignment methods.

### Preliminary

Noisy preferences (NPs).NPs refer to preference data in training datasets, whose label of the actually chosen/rejected responses is flipped as rejected/chosen. Let \(=\{(x^{(i)},y_{w}^{(i)},y_{l}^{(i)})\}_{i=1}^{N}\) be the preference dataset consisting of \(N\) preference data points. For each preference data point \((x,y_{w},y_{l})\), \(x\) is the prompt input to LLMs, \(y_{w}\) is the chosen response, and \(y_{l}\) is the rejected response. We let \(}=\{(x^{(i)},_{w}^{(i)},_{l}^{(i)})\}_{i=1} ^{N}\) be the noisy preference dataset (i.e., preference datasetconsisting noisy preferences) and denote preference data points that are not noisy as clean preferences (CPs). Following Chowdhury et al. , we obtain the noisy preference dataset \(}\) using the standard random noise model  with the probability \((0,50\%)\) to change the data point into noisy preferences, i.e.

\[_{(x^{(i)},_{w}^{(i)},_{l}^{(i)})}}[(x^{(i)},_{w}^{(i)},_{l}^{(i)})=(x^{(i)},y_{l}^{(i)},y_{w}^{(i)})]=.\] (1)

**Perplexity (PPL).** PPL  measures the probability that the LLM generates a sentence. A lower PPL of a sentence indicates that the LLM has generated this sentence with a high probability. PPL is defined as the average negative log-likelihood of a sequence, i.e.,

\[(s;)=(-_{i=1}^{t}_{}(s_{i}|s _{<i})),\] (2)

where \(s\) is a sequence composed of \(t\) tokens and \(_{}(s_{i}|s_{<i})\) denotes the log-likelihood of the \(i\)-th token given the preceding tokens \(s_{<i}\) calculated by an LLM \(_{}\).

Technical details of alignment methods.There are usually three phases in RLHF pipeline : (1) supervised fine-tuning (SFT); (2) reward modeling; (3) reinforcement learning (RL) optimization. In the SFT phase, an LLM is fine-tuned via supervised learning on high-quality task-related data.

Figure 2: We visualized the PPLDiff under the entire PerpCorrect process using Llama2-7B on Golden HH dataset with 20% noisy preferences. We use the green dotted line to represent the normal distribution formed by clean data, the red dotted line represents the normal distribution formed by noisy data, and the black dotted line represents the threshold.

We denote the LLM after the SFT phase as \(_{}\). In the reward modeling phase, the reward model is introduced to simulate human preferences. Given a preference dataset, a reward model \(r_{}(x,y)\) parameterized by \(\), which takes prompt \(x\) and response \(y\) as input and outputs a real number representing the reward score, can be optimized via minimizing the following loss function:

\[_{R}(r_{},)=-_{(x,y_{w},y_{l}) }[(r_{}(x,y_{w})-r_{}(x,y_{l}))],\] (3)

where \(\) is the logistic function. In the RL optimization phase, the objective function is as follows:

\[_{}_{x,y_{}(y|x)}[r_{}( x,y)-(_{}(y|x)-_{}(y|x))],\] (4)

where \(_{}(y|x)\) represents the probability that the LLM parameterized by \(>0\) generates the response \(y\) given the prompt \(x\), \(_{}\) is a reference LLM to maintain the generation ability of the aligned model, and \(\) is a hyper-parameter to ensure the similarity between \(_{}(y x)\) and \(_{}(y x)\). We take \(_{}\) as the reference LLM \(_{}\) following Ouyang et al. .

Recently, offline alignment methods directly leverages preferences in preference datasets, bypassing the need to learn a reward model in RLHF. The LLM parameters are optimized by minimizing the following loss function:

\[(_{};_{})=_{(x,y_{w},y_{l}) }[(x,y_{w},y_{l};)],\] (5)

where the function \(\) changes with the alignment method. To be specific, DPO  uses a BCE loss, SLiC  uses a hinge loss, and IPO  uses a square loss:

\[_{}(x,y_{w},y_{l};)=-( (y_{w} x)}{_{}(y_{w} x)}- (y_{l} x)}{_{}(y_{l} x)} ),\] (6) \[_{}(x,y_{w},y_{l};)=0,1- ((y_{w} x)}{_{}(y_{w} x )}-(y_{l} x)}{_{}(y_{l} x) })},\] (7) \[_{}(x,y_{w},y_{l};)=( (y_{w} x)}{_{}(y_{w} x)}- (y_{l} x)}{_{}(y_{l} x)}- )^{2}.\] (8)

To mitigate the issue of NPs, cDPO  and rDPO  adjust the DPO loss based on the estimated proportion of NPs \(^{}\) using a clean validation dataset \(_{}=\{(x^{(i)},y_{w}^{(i)},y_{l}^{(i)})\}_{i=1}^{N_{ }}\) consisting of \(N_{}\) clean preference data points, i.e.

\[_{}(x,_{w},_{l};) =(1-^{})_{}(x,_{w}, _{l};)+^{}_{}(x,_{l},_{w};),\] (9) \[_{}(x,_{w},_{l};) =)_{}(x,_{w},_{l};)-^{}_{}(x, _{l},_{w};)}{1-2^{}}.\] (10)

## 3 Perplexity-aware Correction for Robust Alignment

This section introduces **Perplexity-aware Correction** (PerpCorrect) for robust alignment with NPs. In Section 3.1, we introduce a novel metric called PPLDiff and then illustrate the pipeline of PerpCorrect to detect and correct NPs based on PPLDiff. In Section 3.2, we demonstrate how to adapt our proposed PerpCorrect with various alignment methods to achieve robust alignment.

### Perplexity-aware Correction (PerpCorrect)

In this subsection, we introduce PerpCorrect which employs a novel metric called PPLDiff as the foundation for detecting and correcting NPs. The algorithm of PerpCorrect is described in Algorithm 2.

PPLDiff.PPLDiff measures the difference between the PPL of chosen response and that of the rejected response. Given a preference data point \((x,_{w},_{l})}\) sampled from the noisy training dataset \(}\) and an LLM \(_{}\), PPLDiff is defined as follows:

\[(x,_{w},_{l};)=([x; _{w}];)-([x;_{l}];),\] (11)

where \([x;y]\) indicates the concatenation of the prompt \(x\) and the response \(y\). Intuitively, if a data point is a clean preference, the \(([x;_{w}];)\) will be lower than \(([x;_{l}];)\) because the sequence \([x;_{w}]\) is more aligned with human values and thus has a higher probability of being generated by aligned LLMs. As a result, it PPLDiff will be lower compared to NPs, which \(([x;_{w}];)\) is higher than \(([x;_{l}];)\). This difference allows us to distinguish CPs and NPs based on PPLDiff.

Aligning a surrogate LLM only using clean validation data.Here, we utilize a clean validation dataset \(_{}\) to obtain an aligned surrogate LLM to make PPLDiff able to distinguish CPs and NPs. We empirically find that the PPLDiff values of CPs and NPs calculated by an unaligned LLM in the noisy training dataset were initially indistinguishable as shown in Figure 1(a), making it impossible to differentiate the NPs from CPs. This is because an unaligned LLM lacks the necessary preferences to distinguish NPs and CPs.

Therefore, we introduce a surrogate LLM \(_{^{}}\) parameterized by \(^{}\) to replace the unaligned LLM and use it for calculating PPLDiff. We optimize the surrogate LLM \(_{^{}}\) using the clean validation dataset \(_{}\) as follows:

\[_{^{}}_{(x,y_{w},y_{l})_{}}[_{}(x,y_{w},y_{l};^{})].\] (12)

After aligning the surrogate LLM, the PPLDiff values of NPs calculated by the surrogate LLM \(_{^{}}\) are significantly increased and those of CPs are significantly decreased, forming two distinct distributions as shown in Figure 1(b). This is because the aligned surrogate LLM is trained to generate responses that align with human preferences, enhancing its ability to distinguish between NPs and CPs based on PPLDiff.

To separate CPs and NPs in the noisy training dataset without knowing the oracle preferences, we leverage the Levenberg-Marquardt (LM) algorithm to find two normal distributions that fit the density of PPLDiff calculated by the aligned surrogate LLM. Specifically, the LM algorithm returns the constants \(,,\) that satisfies the following condition:

\[h(x|,,)=(1-)f_{}(x|,^{2})+ f _{}(x|-,^{2}),\] (13) \[ f(x|,^{2})=}}(-}{2^{2}}).\] (14)

Note that \(x\) is the PPLDiff value and \(h(x|,,)\) is the superposition of these two normal distribution. We denote \(f_{}(x|,^{2})\) as the normal distribution fitting the PPLDiff of CPs and \(f_{}(x|-,^{2})\) as the normal distribution fitting the PPLDiff of NPs since the PPLDiff of NPs is intuitively higher than that of CPs. In this way, we can obtain two distinguishable normal distributions to separate NPs and CPs as shown in the green and red dotted lines of Figure 1(b) without knowing the oracle preferences.

Further aligning the surrogate LLM using extra reliable training data from noisy training datasets.After aligning only using the clean validation datasets, the discriminatory power of the PPLDiff is still far from satisfactory because of the large overlap between the two normal distributions. Therefore, we align the surrogate LLM with more reliable training data to make the PPLDiff of CPs and that of NPs more separable. We iteratively align the surrogate LLM \(_{^{}}\) using more reliably clean training data whose PPLDiff is extremely small and reliably noisy training data whose PPLDiff is extremely large after correction by flipping the label of the response.

Specifically, at epoch \(t\), we select \((t-1)|}|\) of the training data along with the clean validation data for further alignment where \((0,1)\) is the selection ratio and \(|}|=N\) is the number of data points in noisy training dataset. As shown in Lines 33-45 of Algorithm 2, the selected reliable training dataset \(^{}_{t}\) consists of \((t-1)(1-)|}|\) reliably clean training data whose PPLDiff values are smallest \((t-1)(1-)\) percent and \((t-1)|}|\) reliably noisy training data after correction. Note that the reliably clean training data are the data points whose PPLDiff values are smallest \((t-1)(1-)\) percent (located in the green area of Figure 1(c)), and the reliably noisy training data whose PPLDiff values are largest \((t-1)\) percent (located in the red area of Figure 1(c)) among all the training data points.

Detecting and correcting NPs based on PPLDiff to output a denoised training dataset.Based on the PPLDiff calculated by the aligned surrogate LLM, PerpCorrect detects and corrects NPs whose PPLDiff value is lower than a certain threshold. We take the x-coordinate of the intersection of the two normal distributions as the threshold (the black dotted line in Figure 1(d)). As shown in Lines 23-31, data points whose PPLDiff values are larger than this threshold are identified as CPs (the green area in Figure 1(d)), and other data points are identified as NPs requiring correction (the red area in Figure 1(d)). In this way, we can obtain a denoised training dataset for robust alignment.

Further, we select an optimal denoised training dataset to further enhance the performance of robust alignment according to the intersection area of the two normal distributions. We denote the intersection area of two normal distributions as the estimated NP proportion of the denoised training dataset, i.e.,

\[^{}_{PC}=_{-}^{+}\{(1-)f_{ }(x|,^{2}),f_{}(x|-,^{2})\}x,\] (15)

where \(^{}_{PC}\) calculates the ratio of noisy data points which are not detected by PerpCorrect (i.e., the green area enclosed by the black and red lines in Figure 1(d)) and the clean data points which are mistakenly detected by PerpCorrect (i.e., the red area enclosed by the black and green lines in Figure 1(d)). In this way, \(^{}_{PC}\) can efficiently calculate the NP proportion of the denoised training dataset. We take the denoised training dataset with the smallest \(^{}_{PC}\) among multiple iterations as the optimal one for robust alignment to boost alignment performance.

### Robust Alignment

Here, we introduce how to adapt PerpCorrect to robustify various alignment methods and demonstrate the algorithm of robust alignment via PerpCorrect in Algorithm 1. In general, the pipeline of the robust alignment based on PerpCorrect contains three stages: SFT, PerpCorrect, and alignment. We will first conduct SFT, following Christiano et al. , to boost the performance of a pre-trained LLM by boosting its skills for specific tasks. Next, we will conduct PerpCorrect to detect and correct NPs and output an optimal denoised training dataset \(}_{}\) the smallest \(^{}_{PC}\) in Eq. 15. Finally, we can obtain an aligned LLM from the SFT model using the denoised training dataset \(}_{}\) via alignment (i.e., Line 8 in Algorithm 1).

Because our proposed PerpCorrect is agnostic to alignment methods and model structures, PerpCorrect is applicable to robustify both online alignment methods such as RLHF (PPO)  and offline alignment methods including DPO , SLiC , and IPO . Besides, our proposed PerpCorrect is compatible with existing loss-oriented robust alignment methods, such as cDPO  and rDPO , based on the estimated proportion of NPs. Note that cDPO and rDPO require conducting computationally expensive cross-validation to tune the estimated proportion of NPs. We can efficiently estimate the proportion of NPs by utilizing the fitted normal distributions during PerpCorrect, i.e., \(^{}_{PC}\) in Eq. 15. Therefore, we can combine PerpCorrect with a wide range of existing alignment methods to achieve robust alignment with NPs.

```
1:Input: Noisy training dataset \(}\), clean validation dataset \(_{}\), and pre-trained LLM \(_{}\) parameterized by \(\)
2:Output: Robust alignment model \(_{}\)
3:// Stage I: Supervised fine-tuning (SFT)
4:\(_{}\) Supervised fine-tuned LLM \(_{}\). (Details in Appendix C.3)
5:// Stage II: Perplexity-aware correction using the surrogate LLM
6:\(}_{}\), \(^{}_{}\) Perplexity-aware Correction (\(_{}\), \(}\), \(_{}\)) (Details in Algorithm 2)
7:// Stage III: Alignment with denoised dataset
8:\(_{}\) Aligned LLM \(_{}\) using \(}_{}\) and \(^{}_{}\) (Details in Appendix C.3) ```

**Algorithm 1** Robust Alignment via Perplexity-aware Correction (PerpCorrect)

## 4 Experiments

In this section, we demonstrate that our proposed PerpCorrect achieves state-of-the-art alignment performance under different proportion of NPs and have good compatibility with other alignment methods. In Section 4.1, PerpCorrect combined with DPO  achieves state-of-the-art alignment performance than existing baselines (Section 4.1), including DPO , cDPO , and rDPO . In Section 4.2, we further analyze the impact of the number of validation data and verified the compatibility of PerpCorrect with online and offline alignment methods and robust alignment methods. The training details and compute resources are reported in Appendix C.1.

[MISSING_PAGE_FAIL:8]

The empirical results reveal a significant discrepancy in average reward accuracy between the more complex OASST1 dataset and the Golden HH dataset. The performance of other robust alignment methods is found to be unsatisfactory on the OASST1 dataset, often not surpassing the vanilla DPO. In contrast, our method PerpCorrect consistently maintains strong alignment performance across varying proportions of noisy preferences. In general, our method PerpCorrect can achieve better alignment performance than baselines across different datasets.

### Ablation Study

Impact of the number of clean validation data.Table 5 illustrates the impact of the number of clean validation data points. We conducted experiments on the Golden HH dataset using Llama2-7B with a proportion of NPs \(=40\%\). The empirical results indicate that as the number of clean validation data points increases, the performance of our method, PerpCorrect, also improves. However, when the number is too large, the improvement in performance is not obvious, and the cost of manual annotation significantly increases.

Compatibility with online alignment method RLHF (PPO).We adopt vanilla PPO , cPPO [21; 34], and rPPO  as baselines. Table 2 shows the alignment performance of PPO series alignment methods on the Golden HH  dataset using Llama2-7B. Although vanilla PPO has good performance when the proportion of NPs is low, it still declines significantly when the proportion is high. PerpCorrect maintains desirable alignment performances when the proportion of NPs is high. Our empirical results show that PerpCorrect has desirable compatibility with online alignment method RLHF (PPO).

Compatibility with various offline alignment methods.Table 6 presents the average reward accuracy and improvements of original offline alignment methods compared to those combined with PerpCorrect. Our experiments, conducted on the Golden HH dataset using Llama2-7B, reveal that the reward accuracy of SLiC  and IPO  both significantly decrease as the proportion of NPs increases, similar to vanilla DPO . However, our method PerpCorrect enhances their alignment performance across different proportions of NPs. Notably, IPO combined with PerpCorrect achieves

    &  \\   & 10 & 20 & 30 & 40 \\  DPO & 92.53\% & 82.62\% & 68.50\% & 53.15\% \\ PerpCorrect-DPO & 97.51\% & 96.24\% & 95.53\% & 94.92\% \\ \(\) & **+4.98**\% & **+13.62**\% & **+27.03**\% & **+41.77**\% \\  SLiC & 96.70\% & 87.75\% & 76.17\% & 58.59\% \\ PerpCorrect-SLiC & 96.95\% & 95.02\% & 95.38\% & 94.61\% \\ \(\) & **+0.25**\% & **+7.27**\% & **+19.21**\% & **+36.02**\% \\  IPO & 98.07\% & 92.73\% & 79.17\% & 61.64\% \\ PerpCorrect-IPO & **98.73\%** & **97.66\%** & **97.82\%** & **97.56**\% \\ \(\) & **+0.66**\% & **+4.93**\% & **+18.65\%** & **+35.92**\% \\  cDPO & 96.04\% & 90.85\% & 83.23\% & 65.60\% \\ PerpCorrect-cDPO & 98.12\% & 97.31\% & 94.97\% & 88.36\% \\ \(\) & **+2.08**\% & **+6.46**\% & **+11.74**\% & **+22.76**\% \\  rDPO & 96.65\% & 95.22\% & 93.90\% & 90.45\% \\ PerpCorrect-rDPO & 95.99\% & 95.02\% & 94.77\% & 95.73\% \\ \(\) & -0.66\% & -0.20\% & **+0.87**\% & **+5.28**\% \\   

Table 6: Average reward accuracy and improvements of the offline and robust alignment methods, as well as those combined with PerpCorrect, using Llama2-7B on the Golden HH dataset. The standard deviation of reward accuracy and improvements is reported in Table 12.

   Number & 10 & 20 & 30 & 40 & 50 & 100 & 200 \\  Reward accuracy & 81.40\% & 88.26\% & 94.21\% & 94.21\% & 95.43\% & 95.43\% & 96.04\% \\   

Table 5: Impact of the number of clean validation data evaluated on the Golden HH dataset using Llama2-7B with a proportion of NPs \(=40\%\).

the best alignment performance. We conjecture the main reason is that the proportion of NPs in the denoised dataset is very low and IPO performs better than other methods under a low proportion of NPs. These empirical results demonstrate that our method has good compatibility with various offline alignment methods.

Compatibility with robust alignment methods.Table 6 shows the average reward accuracy and improvements of robust alignment methods compared to those combined with PerpCorrect. Our method, PerpCorrect, can significantly enhance the performance of cDPO , and provide a modest improvement for rDPO  under almost all proportion of NPs. The empirical results show that our method has good compatibility with robust alignment methods.

## 5 Conclusions

This paper proposes a method called perplexity-aware correction (PerpCorrect), as an effective approach for robust alignment with noisy preferences (NPs). PerpCorrect utilizes a surrogate LLM to calculate a novel metric, PPLDiff, and further detects and corrects NPs from clean preferences (CPs) based on it. PerpCorrect consists of three steps: (1) PerpCorrect aligns a surrogate LLM using the clean validation dataset, enabling PPLDiff to distinguish between CPs and NPs. (2) PerpCorrect enhances the discrimination power of PPLDiff by aligning the surrogate LLM with more reliable training data. (3) PerpCorrect detects and corrects NPs from CPs based on a calculated threshold and obtains a denoised training dataset. The paper further proposes a robust alignment pipeline, consisting of three stages SFT, PerpCorrect, and alignment, to achieve robust alignment with NPs. The experimental results validate that PerpCorrect achieves state-of-the-art alignment performance and has good compatibility with other online, offline, and robust alignment methods. Therefore, PerpCorrect can be an effective method to mitigate the impact of NPs and can be used for robust alignment. Future research directions include: (1) Improving the time efficiency of PerpCorrect and (2) Reducing the amount of clean validation data required to achieve the same alignment performance.

## Limitations

We discuss some limitations of this work to stimulate further research in this direction. Our limitations mainly stem from two aspects: time efficiency issues caused by multiple calculations of PPLDiff and repeated training of a surrogate LLM, and the need for a validation dataset.

Time efficiency.Iteratively calculating the PPLDiff value for each data point and aligning a surrogate LLM is time-consuming. Selecting reliably training data and denoising the training dataset requires that the PPLDiff value be calculated for each data point during each epoch, which may cause unnecessary calculations for CPs and NPs that can already be clearly distinguished. Besides, aligning a surrogate LLM with same size as the LLM for alignment multiple times is time-consuming. The detailed discussion is in the Appendix B.

Validation dataset.PerpCorrect requires a validation dataset for aligning a surrogate LLM. However, manually annotating a validation dataset is complex and labor-intensive in practice. As shown in Table 5, there is a significant disparity in alignment performance when comparing the use of 10 clean samples to 50 clean samples. Exploring how to use fewer clean samples or even no clean samples to achieve the same or better performance is a problem worth further investigation.

## Appendix B Time Efficiency Analysis

The additional computational overhead is primarily attributed to PerpCorrect (Section 3.1). Table 7 presents both theoretical and empirical runtime comparisons, where \(X\) represents the theoretical time required for Alignment (Section 3.2) or other baselines.

In theory, during the PerpCorrect process, we need to calculate PPLDiff and train the surrogate model in each epoch. The computation time introduced by PerpCorrect is approximately \(\) that of the Alignment or other baselines.

The calculation of PPLDiff in each epoch requires only \(\)of the time needed for robust alignment. The primary computational load in robust alignment arises from the complexity of forwarding and back-propagation, while the complexities of gradient updates and parameter updates are relatively low. Additionally, back-propagation takes twice as long as forwarding. In addition, the calculation of PPLDiff only requires forwarding.

For surrogate model training, PerpCorrect utilized data points that represented \(t\) of the total dataset during epoch \(t\). Since both \(t\) and \(\) are small, the time required for surrogate model training can be approximately ignored.

In practice, Our entire robust alignment pipeline (\(\)24 hours) takes only twice as long as the baseline (\(\)12 hours). We set \(T=5\) and \(=2\), and used the AdamW optimizer. The practical efficiency of the PerpCorrect is due to the use of fp32 precision by the AdamW optimizer, which increases the GPU's calculation time during the robust alignment process.

## Appendix C Implementation details

### Training details and compute resources.

We utilized the Qlora method  for fine-tuning the LLMs, executed on RTX 4090 GPUs with 24 GB of memory. Hyperparameters were set as follows: \(=32\), \(=0.1\), and \(=16\). For SFT, we use the alpaca dataset  and set \(=2e-4\) and \(=20\). For our PerpCorrect stage II, we set \(=0.1\), \(=1e-3\), \(=4\), \(T=5\), and \(=0.02\). For our PerpCorrect stage III and all other alignment methods, we set \(=0.1\), \(=3e-4\), and \(=20\). Other details not mentioned, we follow the default setting in TRL library. Each experiment, involving a specific method and proportion of NPs, could be completed using a single RTX 4090 GPU within 24 hours on the Golden HH dataset and within 72 hours on the OASST1 dataset.

  Stage & Theoretical Running Time & Practical Running Time \\  PerpCorrect & \( X\) & \(\)12 hours \\  Alignment (or baselines) & \(X\) & \(\)12 hours \\  Total & \((1+)X\) & \(\)24 hours \\  

Table 7: Comparison of theoretical and practical running times for PerpCorrect and baselines.

[MISSING_PAGE_FAIL:16]

   &  \\   & 10 & 20 & 30 & 40 \\  vanilla DPO & 0.81\% & 0.40\% & 2.52\% & 2.60\% \\ cDPO & 1.15\% & 0.81\% & 1.76\% & 1.64\% \\ rDPO & 0.26\% & 1.53\% & 0.95\% & 1.92\% \\ PerpCorrect-DPO & 0.63\% & 0.87\% & 1.73\% & 0.63\% \\  

Table 8: Standard deviation of reward accuracy for DPO series alignment methods using Llama2-7B on the Golden HH dataset. The average reward accuracy is reported in Table 1.

   &  \\   & 10 & 20 & 30 & 40 \\  vanilla DPO & 0.92\% & 0.63\% & 1.28\% & 0.98\% \\ cDPO & 0.35\% & 0.54\% & 0.49\% & 1.84\% \\ rDPO & 0.30\% & 0.15\% & 1.67\% & 7.82\% \\ PerpCorrect-DPO & 0.81\% & 1.27\% & 0.63\% & 1.07\% \\  

Table 10: Standard deviation of reward accuracy for DPO series alignment methods using phi-2 on the Golden HH dataset. The average reward accuracy is reported in Table 3.

   &  \\   & 10 & 20 & 30 & 40 \\   & 0.81\% & 0.40\% & 1.28\% & 0.98\% \\ PerpCorrect-DPO & 0.63\% & 0.87\% & 1.73\% & 0.63\% \\ \(\) & 0.98\% & 0.89\% & 0.97\% & 1.98\% \\  SLiC & 1.91\% & 1.33\% & 6.77\% & 8.02\% \\ PerpCorrect-SLiC & 1.76\% & 1.45\% & 1.16\% & 1.50\% \\ \(\) & 0.63\% & 0.38\% & 5.62\% & 6.52\% \\  IPO & 0.84\% & 0.63\% & 3.48\% & 0.32\% \\ PerpCorrect-IPO & 0.32\% & 1.68\% & 0.23\% & 1.25\% \\ \(\) & 0.98\% & 2.29\% & 3.26\% & 1.10\% \\  cDPO & 1.15\% & 0.81\% & 1.76\% & 1.64\% \\ PerpCorrect-cDPO & 0.69\% & 0.75\% & 0.81\% & 0.72\% \\ \(\) & 0.47\% & 1.31\% & 1.85\% & 2.36\% \\  rDPO & 0.26\% & 1.53\% & 0.95\% & 1.92\% \\ PerpCorrect-rDPO & 0.63\% & 0.23\% & 1.84\% & 1.19\% \\ \(\) & 0.84\% & 1.48\% & 2.19\% & 2.31\% \\  

Table 11: Standard deviation of reward accuracy for DPO series alignment methods using phi-2 on the OASST1 dataset. The average reward accuracy is reported in Table 4.

   &  \\   & 10 & 20 & 30 & 40 \\  vanilla DPO & 0.70\% & 1.30\% & 2.46\% & 1.64\% \\ cPPO & 0.46\% & 0.87\% & 0.56\% & 0.91\% \\ rPPO & 0.96\% & 2.29\% & 1.24\% & 6.61\% \\ PerpCorrect-PPO & 0.35\% & 1.89\% & 1.35\% & 1.06\% \\  

Table 12: Standard deviation of reward accuracy and improvements of the offline and robust alignment methods, as well as those combined with PerpCorrect, using Llama2-7B on the Golden HH dataset. The average reward accuracy is reported in Table 6.

   &  \\   & 10 & 20 & 30 & 40 \\  vanilla PPO & 0.15\% & 1.30\% & 4.05\% & 0.77\% \\ cPPO & 0.15\% & 1.53\% & 4.61\% & 5.89\% \\ rPPO & 0.62\% & 1.38\% & 1.55\% & 5.29\% \\ PerpCorrect-PPO & 0.35\% & 1.15\% & 1.34\% & 1.57\% \\  

Table 9: Standard deviation of reward accuracy for PPO series alignment methods using Llama2-7B on the Golden HH dataset. The average reward accuracy is reported in Table 2.

    &  &  \\    & HH-RLHF  & SafeRLHF  & SHP  & WebGPT  & \\  Qwen2-1.5B  & -0.140 & -0.002 & 0.040 & -0.018 & -0.030 \\ Qwen2-1.5B-Instruct  & -0.149 & -0.009 & 0.046 & -0.021 & -0.033 \\ Yi-1.5-6B  & -0.158 & -0.103 & 0.105 & -0.036 & -0.048 \\ Yi-1.5-6B-Chat  & -0.159 & -0.054 & 0.069 & -0.040 & -0.046 \\ gamma-2-2b  & -0.140 & -0.051 & 0.001 & -0.024 & -0.053 \\ gamma-2-2b-it  & -0.163 & -0.053 & 0.063 & -0.028 & -0.045 \\ falcon-7b  & -0.113 & -0.001 & 0.037 & -0.016 & -0.023 \\ falcon-7b-instruct  & -0.121 & 0.021 & 0.039 & -0.017 & -0.019 \\ Mistral-7B-v0.3  & -0.133 & -0.045 & 0.048 & -0.026 & -0.039 \\ Mistral-7B-Instruct-v0.3  & -0.201 & -0.058 & 0.065 & -0.038 & -0.058 \\ glm-4-9b  & -0.134 & -0.019 & 0.045 & -0.027 & -0.034 \\ glm-4-9b-chat-1m  & -0.135 & -0.019 & 0.049 & -0.028 & -0.033 \\ Llama-2-7b-hf  & -0.133 & -0.052 & 0.051 & -0.028 & -0.040 \\ Llama-2-7b-chat-hf  & -0.139 & -0.041 & 0.063 & -0.032 & -0.037 \\   

Table 13: Average PPLDiff values of randomly selected data points across datasets calculated by different LLMs. ”Avg.” refers to the average PPLDiff value over all the datasets.

```
1:Input: Noisy training dataset \(}\), clean validation dataset \(_{}\), LLM \(_{}\) parameterized by \(\)
2:Output: Denoised training dataset \(}_{}\) and estimated proportion of NPs \(^{}_{}\)
3:\(_{^{}}_{}\), \(^{}_{0}\), \(^{}_{} 1\), \(}_{}}\),
4:for epoch \(t=0,,T\)do
5: // Aligning the surrogate LLM
6:\(_{^{}}\) Alignment (\(_{^{}}\), \(^{}_{t}_{}\))
7: // Calculating the PPLDiff values for each data point
8:\(\)
9:for\((,_{w},_{l})}\)do
10:\(z(x+_{w};^{})- (x+_{l};^{})\)
11:\(\{(,_{w},_{l},z)\}\)
12:endfor
13: // Fitting PPLDiff density of noisy training dataset
14:\(\), \(\), \(\) Fitted parameters using Levenberg-Marquard algorithm with \(\)
15: // Estimating NPs proportion of the denoised training dataset
16:\(^{}_{PC}\) Estimated proportion of NPs using the Eq.15 based on \(\), \(\), \(\)
17: // Keeping denoised training dataset with the smallest \(^{}_{}\)
18:if\(^{}_{PC}<^{}_{}\)then
19:\(^{}_{}^{}_{PC}\)
20: // Calculating the Threshold \(\)
21:\(\) x-coordinate of the intersection of the two normal distributions(\(\), \(\), \(\))
22: // Distinguishing CPs and NPs based on the threshold \(\) and correcting NPs
23:\(}_{}\), \(}_{}\)
24:for\((,_{w},_{l},z)\)do
25:if\(z>\)then
26:\(}_{}}_{} \{(,_{w},_{l})\}\)
27:else
28:\(}_{}}_{} \{(,_{l},_{w})\}\)
29:endif
30:endfor
31:\(}_{}}_{}}_{}\)
32:endif
33:\(_{}\), \(_{}\)
34: // Calculating the left bound \(_{l}\) and the right bound \(_{r}\)
35:\(_{l}(t-1)(1-)|}|\)-th smallest PPLDiff value in \(\)
36:\(_{r}(t-1)|}|\)-th largest PPLDiff value in \(\)
37: // Finding extra reliable training data
38:for\((,_{w},_{l},z)\)do
39:if\(z<_{l}\)then
40:\(_{}_{}\{( ,_{w},_{l})\}\)
41:endif
42:if\(z>_{r}\)then
43:\(_{}_{}\{( ,_{l},_{w})\}\)
44:endif
45:endfor
46:\(^{}_{t+1}_{}_{}\)
47:endfor ```

**Algorithm 2** Perplexity-aware Correction (PerpCorrect)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our introduction covers our contributions, main methods and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the efficiency issues and data volume requirements of our method PerpCorrect in the Conclusions section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We show all the experiment detail in the Experiments section and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to our code using Github. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We show the entire experimental details in the Experiments section and Appendix and provide open access to the code using Github. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard deviation in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide detailed sufficient information on the computer resources in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential impacts in Appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our robust alignment method does not have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The models and datasets the we used are open-sourced, and we follow their license and terms of use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our training code are open-source on GitHub. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.