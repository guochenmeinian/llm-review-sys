# Truly Scale-Equivariant Deep Nets

with Fourier Layers

 Md Ashiqur Rahman Raymond A. Yeh

Department of Computer Science, Purdue University

{rahman79, rayyeh}@purdue.edu

###### Abstract

In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.

## 1 Introduction

Consider the task of image classification; if an object in the image is scaled (resized), then its corresponding object label should remain the same, _i.e._, scale-invariant. Similarly, for semantic segmentation, if an object is scaled, then its corresponding mask should also be scaled accordingly, _i.e._, scale-equivariant. Similarly, one would expect the features extracted to be scale-equivariant; see Fig. 1 for illustration. These invariant and equivariant properties are important to many computer vision tasks due to the nature of images. A photo of the same scenery can be taken from different distances, and objects in the scenes may come in different sizes. Developing representations that effectively capture this multi-resolution aspect of images has been a long-standing quest .

Recently, there has been a line of work on developing scale-equivariant convolutions networks  to more effectively learn multi-resolution features. At a high level, these works achieve scale-equivariant convolution layers through weight-sharing and kernel resizing, _i.e._, use the "same" but resized kernel across all scales . The innovation of these works is how to properly resize the kernel. For example, Bekkers  and Sosnovik et al.  formulate kernel resizing as a continuous operation and then discretize the kernel when implemented in practice. However, this discretization leads to non-negligible equivariance error. On the other hand, Worrall and Welling  and Sosnovik et al.  directly formulate kernel resizing in the discrete domain, _e.g._, using dilation or solving for the best kernel given a fixed scale set, and achieve low equivariance-error.

Despite these successes, we point out that the aforementioned works are not truly scale-equivariant in practice. Specifically, these works are derived using a continuous domain down-scaling operation, _i.e._, there is no need to consider anti-aliasing. However, when performing a down-scaling on discrete space, the Nyquist theorem  tells us that an anti-alias filter is necessary to avoid high-frequency content to alias into lower frequencies. The canonical example of aliasing is the "wagon-wheel effect", where a wheel in a video appears to be rotating slower or even in reverse from its true rotation. To

[MISSING_PAGE_FAIL:2]

with numerous applications applied to various domains, _e.g._, sets [10; 26; 32; 34; 48; 51], graphs [6; 7; 15; 19; 20; 25; 28; 39; 49], etc. Moreover, several recent studies have also identified and tackled the issue of aliasing generated from the pooling layer to attain finer translation equivariance [35; 47; 52] and image generation .

**Fourier transform in neural networks.** Fourier transforms have been previously used in deep learning. For example, Mathieu et al.  proposes to use Fast Fourier Transform (FFT) to speed up CNN training. Fourier transform has also been used to develop network architectures, including various convolutional neural networks operating in the Fourier space [16; 31]. Recently, Fourier layers are capable of handling inputs of varying resolution, which have been employed in neural operators, facilitating applications in partial differential equations and in state space models [18; 29]. Fourier convolutions have also found success in low-level image processing tasks, _e.g._, inpainting , deblurring . Different from these works, we focus on developing truly scale-equivariant deep nets and leverage Fourier layers to achieve this goal.

## 3 Preliminaries

We briefly introduce and review the definition of Fourier transform, ideal downsampling, and scale-equivariance. For readability, we use 1D data to define these concepts. These ideas are extended to 2D data with multiple channels when implemented in practice.

**Discrete Fourier Transform (DFT).** Given an input vector \(^{N}\), we consider \(:^{N}^{N}\) be the discrete Fourier Transform (DFT) which has the form

\[=()\;[k] _{n=0}^{N-1}[n]e^{-jkn},\] (1)

where \(j\) denotes the unit imaginary number, _i.e._, \(j^{2}=-1\). The index \(k\) in Eq. (1) is commonly within the domain of \([0,N]\). Note that as Eq. (1) is \(N\)-periodic, for readability, we will use \(k\) from \([-,]\) where \(k=0\) corresponds the lowest frequency.

The corresponding inverse DFT (IDFT) \(^{-1}:^{N}^{N}\) is defined as

\[=^{-1}()\;[n]=_{k=0}^{N-1} [k]e^{jkn}.\] (2)

By the convolution property of DFT, the circular convolution between \(\) and a kernel \(^{N}\) can be represented as the element-wise multiplication in the Fourier domain, _i.e._,

\[(}}})=()( )=,\] (3)

where denotes the circular convolution and denotes element-wise multiplication. Unless explicitly mentioned, we will represent the input vector with lowercase letters (e.g., \(\)) and its corresponding DFT with uppercase letters (e.g., \(\)).

**Down-scaling operation.** To reduce the scale (or resolution) of a signal \(^{N}\), one could perform a subsampling \(_{R}\) by a factor of \(R\)

\[_{R}()[n]=[Rn].\] (4)

However, naively subsampling leads to aliasing. Hence, anti-aliasing is performed in a multi-rate system. In signal processing, the analysis commonly uses the ideal anti-aliasing filter \(\), which zeros out all the high-frequency content, _i.e._, its DFT \(()\) is defined as:

\[[k]=1\;|k|0.\] (5)

See Fig. 1(a) for an illustration of the ideal anti-aliasing filter.

In this work, we define the overall down-scaling operation to be the ideal downsampling \(_{R}\) by a factor of \(R\), which performs anti-aliasing followed by a subsampling operation:

\[_{R}()_{R}(}}})\;\; R<N,\] (6)where their DFT are related by

\[(_{R}())=()[-N/2R:N/2R ].\] (7)

**Scale-equivariance.** With the down-scaling operation defined, a deep net \(:\{^{1},^{2},,^{N}\}\{ ^{1},^{2},,^{N}\}\) is scale-equivariant if:

\[(_{R}())=_{R}(() )\{^{1},^{2},,^{N }\}R<(),\] (8)

where \(\{^{1},^{2},,^{N}\}\) represents the space of input/output signals at different scales. In this paper, we are interested in designing a family of deep nets that satisfies the equality in Eq. (8). Scale-invariance can be defined in a similar manner as

\[(_{R}())=() \{^{1},^{2},,^{N}\}R<().\] (9)

**Fourier layer.** Given a multi-channel input vector, \(^{C_{} N}\) and kernel \(^{C_{} C_{} N}\), where \(C_{/}\) is the number of input/output channels, the circular convolution layer is defined as

\[(())[c^{}]=_{c=1}^{C_{}}[c][c^{},c],\] (10)

where \(\) and \(\) denotes the DFT of \(\) and \(\) applied independently for each channel.

## 4 Approach

Our goal is to design truly scale-equivariant deep nets. To accomplish this goal, we propose scale-equivariant versions of CNN modules, including, the convolution layer, non-linearities, and pooling layers. In Sec. 4.1, we detail the operation for each of the proposed modules. In Sec. 4.2, we demonstrate how to build a classifier that is suitable for image classification with scale-equivariant features. We now explain our overarching design principle for the scale-equivariant modules.

From a frequency perspective, as reviewed in Eq. (6), the ideal downsampling operation results in the loss of higher frequency terms of the signal. In other words, if a feature's frequency terms depend on any higher frequency terms of the input, then it is not scale-equivariant, as the information will be lost after downsampling. We now formally state this observation in Claim 1.

**Claim 1**.: _Let \(g\) denote a deep net such that \(=g()\). If this deep net \(g\) can be equivalently represented as a set of functions \(_{k}:^{2k+1}\) such that_

\[[k]=_{k}([-k:k])\;\; k\] (11)

_then \(g\) is scale-equivariant as defined in Eq. (8). In other words, an output's frequency terms can only have dependencies on the terms in \(\) that are_ **even lower** _in frequencies. We illustrate this structure with a linear function in Fig. 1(b)._

Figure 2: In (a), we illustrate an ideal low-pass filter showing that it zeros out the high frequencies. In (b), we illustrate the structure described in Claim 1 for a linear \(\). The gray regions correspond to the value being zero.

[MISSING_PAGE_EMPTY:5]

\(|k|\) and \(\) denotes the DFT of the input, _i.e._, \(()\). In practice, we choose \(\) to be ReLU in our implementation.

Next, it is generally computationally expensive to achieve equivariance over all scales. In practice, we only enforce a set of scales for which we want to achieve equivariance, which can be denoted in terms of corresponding resolutions as \(=(m, N)\) with \([i]<[i+1]\). To achieve scale-equivariant non-linearity over the scales of \(\), \(_{}=^{-1}()\) can be efficiently computed as

\[[k]=^{-1}( -^{}[i]}{2}:^{}[i]}{2}) [k]\ \ \ ^{}[i-1]}{2}<|k|^{}[i]}{2}.\] (17)

Here, the ordered set \(^{}=\{0\}\). By Eq. (17), all the Fourier coefficients \(k\) between any two consecutive resolutions in \(\), _i.e._, \([i-1]/2<|k|[i]/2\) can be computed by a single Fourier transform pair.

**Scale-equivariant pooling.** Pooling operation is crucial for deep nets' scalability to larger images and datasets as they make the network more memory and computationally efficient. Commonly used pooling operations are max/average pooling, which reduces the input size by the factor of its window size \(w\) and is not scale-equivariant. To address this, we propose scale-equivariant pooling \(^{w}_{}\).

Let \(^{w}\) denote a max/average pooling operation with a window size \(w\) and \(^{w}:^{d}^{}\). We define scale-equivariant pooling operation \(^{w}_{}:^{d}^{}\) mapping from \(\) to \(\) where \(=^{-1}()\) follows

\[[k]=^{}(^{-1}( -w|k:w|k|))[k]\ \ \ \  k.\] (18)

Observe that this pooling layer satisfies Claim 1 by construction. Similar to non-linearity, we can enforce the equivariance over the set \(\) following the same formulation in Eq. (17).

Note that as pooling reduces the size of the output by a factor \(w\), the operation is only scale-equivariant at every \(w^{}\) resolution. When the input size is not a multiple of \(w\) there is a truncation of the input.

**Time Complexity.** We now provide the time complexity of our scale-equivariant Fourier layer and compare it with standard group convolutions. Let's consider a 1D signal of length \(N\) and a kernel of length \(K\). Our proposed model involves:

* A transformation of local filter to global with time complexity \(O(KN)\)
* A convolution using Fourier transform with time complexity \(O(Nlog(N))\)
* Our scale equivariant non-linearity depends on the size of the group. Let \(A\) be the set of group actions. The time complexity of the proposed scale-equivariant non-linearity is \(O(|A|N(N))\), where \(|A|\) denotes the cardinality of the set \(A\).

So, the time complexity for each layer becomes

\[O(|A|N(N)+KN).\]

As a comparison, the time complexity of regular group convolution is \(O(KN|A|)\) in the first layer and \(O(KN|A|^{2})\) for all intermediate layers, assuming the cost of group action is a negligible constant .

Considering the time complexity of the intermediate layers of group convolutions, our proposed method is more efficient when

\[|A|N(N)+KN<|A|^{2}KN(N)+<|A|K.\]

So, when \(K<<|A|\) and \(log(N)<|A|K\), i.e., assuming the set of group actions of moderate size, then our method is faster than group convolutions.

Modern GPUs are specifically optimized for regular convolution operations that can be performed in place. In contrast, the FFT algorithm does not fully capitalize on GPUs' advantages, primarily due to unique memory access patterns and moderate arithmetic intensities. Consequently, our approach is unable to harness the full potential of GPUs. When executed on a GPU, regular group convolutions implemented as standard convolutions might exhibit comparable or even shorter running times than our approach.

### Classifier for equivariant features

A truly scale-invariant, defined in Eq. (9), the model's performance is limited by the lowest resolution as the prediction needs to be the same. In the extreme, the prediction can only depend on a single mean pixel. Instead of invariance, we believe that it is more desirable to ensure that a high-resolution image achieves a better performance than its down-scaled version, _i.e._, the performance is scale "consistent". To achieve this property, we propose a suitable classifier architecture and training scheme.

**Classifier.** In order to enforce scale-consistency, we need a classifier that outputs a prediction per scale. This motivated the following proposed architecture. Let \(c\) be a classifier with \(M\) classes where \(}=c g()^{|()| M}\). \(()\) is defined as the set of resolutions smaller than the input resolution in the considered scales \(\). _i.e._, \(()=\{k:k()k \}\). Here, \(g\) is a scale-equivariant deep-net that extracts features \(=g()\) with corresponding DFT of \(\). Our proposed classifier has the form:

\[}[k]=^{-1} _{}([-:])~{} ~{} k(x)\] (19)

where \(_{}\) is a Fourier padding operation that symmetrically pads zero to either side of the DFT to a fixed size N, Pool is a spatial pooling operation and MLP maps the pooled feature to the predicted logits \(}[k]\) for each scale; Note the MLP is shared across all scales. As we are sharing the MLP, we need to ensure that the input sizes are identical. Hence, we padded the features \(\) to a fixed size. Finally, at test-time, we use the output from \(}[()]\) to make a prediction.

**Training.** Given a dataset \(=\{(,y)\}\), we train our model using the sum of two losses. The first term is a standard sum of cross entropy loss \(\) over the scales:

\[_{k()}(}[k],y).\] (20)

The second term is a consistency loss to encourage the performance of high-resolution to be better than the low-resolution:

\[_{k()}(}[k],y) -(}[k-1],y),0.\] (21)

This is a hinge loss that penalizes the model when the cross entropy loss \(\) on high-resolution features (larger \(k\)) is greater than that of the low-resolution features (smaller \(k\)).

## 5 Experiments

To study the effectiveness of our model, we conduct experiments on two benchmark datasets, MNIST-scale  and STL10 , following our theoretical setup using ideal downsampling. In this case, the theory exactly matches practice, and our approach achieves perfect scale-equivariance. We also conduct experiments comparing the models' generalization to unseen scales and data efficiency. Finally, we conduct experiments using a non-ideal anti-aliasing filter in down-scaling. Under this setting, our model no longer achieves zero scale equivariance-error. However, we are interested in how the models behave under this mismatch in theory and practice.

**Evaluation metrics.** To evaluate task performance, we report classification accuracy. Next, we introduce a metric to measure the scale-consistency. Given a sample from the test set, we check whether the cross entropy loss is less than or equal to the classification loss of its down-scaled version. We compute this as a percentage over the dataset and report the scale-consistent rate defined as:

\[=|}_{(,y)} _{r}(,y)(_{r}(),y),\] (22)

where \(r\) is uniformly sampled over the set of scales for which we want to achieve equivariance and \(\) denotes the indicator function.

Finally, we quantify the equivariance-error over the final feature map given by a fully trained model on the dataset. The equivariance-error (Equi-Err.) is defined as

\[=|||}_{ }_{r}_{r}())- _{r}(g())\|_{2}^{2}}{\|g(_{r}( ))\|_{2}^{2}}.\] (23)Here, \(\) is the set of all scales over which we enforce equivariance. We report the average equivariance error over the samples of the test set \(\). We note that this equivariance-error _differs_ from the one reported by Sosnovik et al.  where they measured the error for the "scale-convolution with weights initialized randomly." Contrarily, we measure the equivariance error from _end-to-end_ over _trained_ models, which more closely matches how the models are used in practice.

**Baselines.** Following prior works in scale-equivariant neural networks  we compare to baselines: DISCO , SI-ConvNet , SS-CNN , DSS , and SESN . For the baseline, we follow the architecture and training scheme provided by Sosnovik et al. . We also prepared three additional baseline models: (a) standard CNN, (b) Per Res, CNN where we train a separate CNN for each resolution in the training set, and (c) Fourier CNN  which utilizes Fourier layers.

### MNIST-scale (Ideal downsampling)

**Experiment setup.** We create the MNIST-scale dataset following the procedure in prior works . Each image in the original MNIST dataset is randomly downsampled with a factor of \([-1]\), such that every resolution from \(8 8\) to \(28 28\) contains an equal number of samples. As the baseline models (except the Fourier CNN) can not handle images of different resolutions, following prior works, lower-resolution images are zero-padded to the original resolution. We do not need to pad the input for our model and Fourier CNN. We used 10k, 2k, and 50k for training, validation, and test set samples. For this experiment, we enforce equivariance over scales that correspond to the discrete resolutions of \(=\{8,,28\}\).

**Implementation details.** For the baselines and CNN, we follow the implementation, hyper-parameters, and architecture provided in prior works . For Per Res. CNN, we train a separate CNN for each resolution. Each of these CNNs uses the architecture of baseline CNN. For Fourier CNN, we use the Fourier block introduced in the Fourier Neural operator . Inspired by their design, we use \(1 1\) complex convolution in the Fourier domain along with the scale-equivariant convolution. We follow the baseline for all training hyper-parameters, except we included a weight decay of \(0.01\).

**Results.** In Tab. 1, we report the accuracy of the MNIST-scale dataset. We observe that our approach achieved zero equivariance error and the highest accuracy. While all models achieve similar accuracy, there is a more notable difference in the scale consistency rate. This means that our model properly captures the additional information that comes with increased resolution.

**Generalization to unseen scales.** We study the generalization capabilities of the scale-equivariant modes to unseen scales; we train them on a dataset with 10k full resolution \((28 28)\) MNIST images and test on 50k samples of MNIST-scale, _i.e._, containing different scales. For the baselines, we added random scaling argumentation during training. In Tab. 2, we observe that our model can guarantee zero equivariance error even for the unseen scales and achieves comparable performance to baselines trained with data augmentation.

**Data efficiency.** We also conduct experiments studying the data efficiency of the different models. Following the same setup as MNIST-scale, we train the models on limited training examples, 5k, 2.5k, and 1k, of different resolutions and test on 50k samples across all resolutions. In Tab. 3, we observe that our model is more data efficient than the baselines. DISCO achieves the second-best

   Models & Acc.\(\) & Scale-Con.\(\) & Equi-Err.\(\) \\  CNN & 0.9737 & 0.6621 & - \\ Per Res. CNN & 0.9388 & 0.0527 & - \\  SESN & 0.9791 & 0.6640 & - \\ DSS & 0.9731 & 0.6503 & - \\ SI-CovNet & 0.9797 & 0.6425 & - \\ SS-CNN & 0.9613 & 0.3105 & - \\ DISCO & 0.9856 & 0.5585 & 0.44 \\  Fourier CNN & 0.9713 & 0.2421 & 0.28 \\ Ours & **0.9889** & **0.9716** & **0.00** \\   

Table 1: Accuracy of different models on MNIST-scale (ideal downsampling) with all scales.

   Models & Acc.\(\) & Scale-Con.\(\) & Equi-Err.\(\) \\  CNN & 0.9737 & 0.6621 & - \\ Per Res. CNN & 0.9388 & 0.0527 & - \\  SESN & 0.9791 & 0.6640 & - \\ DSS & 0.9731 & 0.6503 & - \\ SI-CovNet & 0.9797 & 0.6425 & - \\ SS-CNN & 0.9870 & 0.3593 & - \\ DISCO & **0.9914** & 0.5371 & 0.35 \\  Fourier CNN & 0.9713 & 0.2421 & 0.28 \\ Ours & **0.9889** & **0.9716** & **0.00** \\   

Table 2: Accuracy of different models on MNIST-scale (ideal downsampling) with missing scales.

performance. We also see that Per Res. CNN suffers the most when trained with fewer data points, as it trains a separate CNN for each scale and does not share parameters across different scales.

**Ablation.** We perform an ablation on the consistency loss in Eq. (21) over different training set sizes. From Tab. 5, we can observe that the consistency loss improves the accuracy of our model as well as the scale-consistency. This result validates the effectiveness of the proposed consistency loss.

### STL10-scale (Ideal downsampling)

**Experiment setup.** Following the same procedure as the MNIST-scale dataset, we create the STL10-scale dataset. Each image of the dataset is randomly scaled with a randomly chosen downsampling factor between \([1-2]\) such that every resolution from 48 to 97 contains an equal number of samples. We use 7k, 1k, and 5k samples in our training, validation, and test set. For the baseline models, we again zero-pad the downsampled images to the original size.

**Implementation details.** For the baseline models, we use the Wide ResNet as the CNN baseline following prior work . For Fourier CNN, we use six Fourier blocks followed by a two-layered MLP. For our model, we use six scale-equivariant Fourier blocks followed by a two-layer MLP. All of the models are trained for \(250\) epochs with Adam optimizer with an initial learning rate of \(0.01\). The learning rate is reduced by a factor of \(0.1\) after every \(100\) epoch. For scalability, we consider achieving equivariance over scales that correspond to the discrete resolutions in the set \(=\{48 48+i 8 97\}\ \  i\{0,1,2,\}\).

**Results.** In Tab. 4, we observe that our model achieves zero equivariance error with higher accuracy and scale consistency over the baselines. As the baseline models accept a fixed-sized input, the downsampled images are zero-padded following prior work's preprocessing on MNIST-scale. Note, MNIST images have a uniform black background, and zero-padding does not create artifacts. However, for colored images with diverse backgrounds, such as STL-10, any padding scheme to resize the image will cause artifacts. We believe this artifact hurts the performance of baseline models on the STL10-scale dataset. However, it is unclear whether there is a more suitable padding strategy.

### MNIST-scale (Non-ideal downsampling)

Ideal interpolation suffers from artifacts known as the ringing effect caused by _Gibbs phenomenon_; see the down-scaled image in Fig. 2(a). In practice, a non-ideal low-pass filter will be used instead. Taking this into consideration, we conduct the experiments using a more commonly used anti-aliasing scheme with a Gaussian blur instead of the ideal low-pass filter.

   Models & Acc.\(\) & Scale-Con.\(\) & Equi-Err.\(\) \\  Wide ResNet & 0.5596 & 0.2916 & 0.16 \\ SESN & 0.5525 & 0.4166 & 0.04 \\ DSS & 0.5347 & 0.1979 & 0.02 \\ SI-CovNet & 0.5588 & 0.2187 & 0.03 \\ SS-CNN & 0.4788 & 0.1979 & 1.82 \\ DISCO & 0.4768 & 0.3541 & 0.06 \\  Fourier CNN & 0.5844 & 0.2812 & 0.19 \\ Ours & **0.7332** & **0.6770** & **0.00** \\   

Table 4: The classification accuracy of different models on STL10-scale dataset.

   Models / \# Samples & 5000 & 2500 & 1000 \\  CNN & 0.9432 & 0.9389 & 0.8577 \\ Per Res. CNN & 0.9118 & 0.8392 & 0.5815 \\  DISCO & 0.9794 & 0.9665 & 0.9457 \\ SESN & 0.9638 & 0.9402 & 0.9207 \\ SI-CovNet & 0.9641 & 0.9437 & 0.9280 \\ SS-CNN & 0.9477 & 0.9259 & 0.9176 \\ DSS & 0.9654 & 0.9401 & 0.9281 \\  Fourier CNN & 0.9567 & 0.9419 & 0.8910 \\ Ours & **0.9835** & **0.9767** & **0.9606** \\    
   Models & Acc.\(\) & Scale-Con.\(\) & Equi-Err.\(\) \\  Wide ResNet & 0.5596 & 0.2916 & 0.16 \\ SESN & 0.5525 & 0.4166 & 0.04 \\ DSS & 0.5347 & 0.1979 & 0.02 \\ SI-CovNet & 0.5588 & 0.2187 & 0.03 \\ SS-CNN & 0.4788 & 0.1979 & 1.82 \\ DISCO & 0.4768 & 0.3541 & 0.06 \\  Fourier CNN & 0.5844 & 0.2812 & 0.19 \\ Ours & **0.7332** & **0.6770** & **0.00** \\   

Table 3: MNIST-scale accuracy with different numbers of training samples.

**Experiment details.** We follow the same experimental setup and training scheme as in MNIST-scale with the ideal downsampling experiment. The only difference is that we use a Gaussian kernel to perform anti-aliasing.

**Results.** From Tab. 6, we observe that our model achieves higher classification accuracy and Scale consistency. Importantly, our model achieves lower equivariance error than the baseline despite the gap in the theory of non-ideal downsampling.

## 6 Conclusion

We propose a family of scale-equivariant deep nets that achieve zero equivariance error measured from end to end. We formulate down-scaling in the discrete domain with proper consideration of anti-aliasing. To achieve scale-equivariance, we design novel modules based on Fourier layers, enforcing that the lower frequency content of output does not depend on the higher frequency content of the input. Furthermore, we motivated the scale-consistency property that the performance of higher-resolution input should be better than that of the lower resolution and designed a suitable classifier architecture. Empirically, our approach achieves competitive accuracy on image classification tasks, with improved scale consistency and lower equivariance-error compared to baselines. Similar to other equivariant methodologies, defining consistent scales or group actions to achieve equivalence before constructing the model is crucial. Moreover, a common challenge all equivariant and invariant techniques face is the significant demands on memory and computational resources. In our upcoming research, we plan to enhance our approach by applying it to high-resolution image datasets and dense prediction tasks, such as instance segmentation.

   Models & Acc.\(\) & Scale Con.\(\) & Equi. Err.\(\) \\  CNN & 0.9642 & 0.1033 & - \\ Per Res. CNN & 0.9450 & 0.0742 & - \\  SESN & 0.9710 & 0.6666 & - \\ DSS & 0.9772 & 0.5716 & - \\ SI-CovNet & 0.9694 & 0.4453 & - \\ SS-CNN & 0.9670 & 0.3144 & - \\ DISCO & 0.9830 & 0.4500 & 0.63 \\  Fourier CNN & 0.9745 & 0.1716 & 0.29 \\ Ours & **0.9880** & **0.9760** & **0.05** \\   

Table 6: The accuracy of different models on MNIST-scale (non-ideal downsampling).

Figure 3: Feature visualization for ideal and non-ideal downsampling settings. In both settings, our model seems to learn spatially local features such as digit contour and edges.