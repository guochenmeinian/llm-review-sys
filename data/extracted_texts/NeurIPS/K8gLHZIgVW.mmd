# Regularization properties of

adversarially-trained linear regression

 Antonio H. Ribeiro

Uppsala University

antonio.horta.ribeiro@it.uu.se &Dave Zachariah

Uppsala University

dave.zachariah@it.uu.se &Francis Bach

PSL Research University, INRIA

francis.bach@inria.fr &Thomas B. Schon

Uppsala University

thomas.schon@it.uu.se

###### Abstract

State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For \(_{}\)-adversarial training--as in square-root Lasso--the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.

## 1 Introduction

Adversarial attacks generated striking examples of the brittleness of modern machine learning. The framework considers inputs contaminated with disturbances deliberately chosen to maximize the model error and, even for small disturbances, can cause a substantial performance drop in otherwise state-of-the-art models . Adversarial training  is one of the most effective approaches for deep learning models to defend against adversarial attacks . It considers training models on samples that have been modified by an adversary, with the goal of obtaining a model that will be more robust when faced with new adversarially perturbed samples. The training procedure is formulated as a min-max problem, searching for the best solution to the worst-case attacks.

Despite its success in producing state-of-the-art results in adversarial defense benchmarks , there are still major challenges in using adversarial training. The min-max problem is a hard optimization problem to solve and it is still an open question how to solve it efficiently. Moreover, these methods still produce large errors in new adversarially disturbed test points, often much larger than the adversarial error obtained during training.

To get insight into adversarial training and try to tackle these challenges, a growing body of work  studies fundamental properties of adversarial attacks and adversarial training in linear models. Linear models allow for analytical analysis while still reproducing phenomena observed in state-of-the-art models. Consider a training dataset \(\{(_{i},y_{i})\}_{i=1}^{n}\) consisting of \(n\) data points of dimension \(^{p}\), adversarial training in linear regression corresponds to minimizing (in \(^{p}\))

\[R^{}(;,\|\|)=_{i=1}^{n}_{ \|_{i}\|}|y_{i}-(_{i}+_{i})^ {}|^{2}.\] (1)

Restricting the analysis to linear models allows for a simplified analysis: the problem is convex and the next proposition allows us to express \(R^{}\) in terms of the dual norm, \(\|\|_{*}=_{\|\| 1}|^{}|\).

**Proposition 1** (Dual formulation).: Let \(\|\|_{*}\) be the dual norm of \(\|\|\), then

\[R^{}(;,\|\|)=_{i=1}^{n}(|y _{i}-_{i}^{}|+\|\|_{*})^{2}.\] (2)

This simple reformulation removes one of the major challenges: it removes the inner optimization problem and yields a model that is more amenable to analysis. Similar reformulations can be obtained for classification. This simplification made the analysis of adversarial attacks in linear models fruitful to get insights into the properties of adversarial training and examples. From giving a counterexample to the idea that deep neural networks' vulnerabilities to adversarial attacks were caused by their nonlinearity , to help explaining how implicitly regularized overparametrized models can be robust, see .

In this paper, we further explore this reformulation and _provide a thorough characterization of adversarial training in linear regression problems._ We do this in a comparative fashion, establishing when the solution of adversarially-trained linear models coincides with the solution of traditional regularization methods. For instance, a simple inspection of (2) hints at the similarities with other parameter shrinking methods. This similarity is confirmed in Figure 1,1 which illustrates how the solutions of \(_{}\)-adversarial training can be almost indistinguishable from Lasso. We explain the similarities and differences with other methods. Our contributions are:

1. In the overparametrized region, we prove that the _minimum-norm interpolator is the solution to adversarial training for \(\) smaller than a certain threshold_ (Section 4).
2. We establish _conditions under which the solution coincides with Lasso and ridge regression_, respectively (Section 5).
3. We show that adversarial training can be framed in the _robust regression_ framework, and use it to establish connections with the _square-root Lasso_. We show that adversarial training (like the _square-root Lasso_) can obtain bounds on the prediction error that do not require the noise variance to be known (Section 6).
4. We prove a more _general version of Proposition 1_, valid for general lower-semicontinuous and convex loss functions (Section 8).

Figure 1: **Regularization paths** estimated in the Diabetes dataset . Regularization paths are plots showing the coefficient estimates for varying regularization parameter \(\) or, in adversarial training, perturbation radius \(\). This type of plot is commonly used in analysing Lasso and variants, i.e. . On the horizontal axis, we give the \(_{1}\)-norm of the estimated parameter. On the vertical axis, the coefficients obtained for each method. The dataset has \(p=10\) baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements), which were obtained for \(n=442\) diabetes patients. The model output is a quantitative measure of the disease progression. See Appendix B.5 for the relationship between \(\|}\|_{1}\) and \(\) and \(\).

Related work

**Generalization of minimum-norm interpolators.** The study of minimum-norm interpolators played a key role in explaining why overparametrized models generalize--an important open question for which traditional theory failed to explain empirical results --and for which significant progress has been made over the past few years . These estimates provide a simple scenario where we can interpolate noisy data and still generalize well. Minimum-norm interpolators have indeed been quite a fruitful setting to study the phenomenon of _benign overfitting_, i.e., when the model interpolates the training data but still generalizes well to new samples. In  the authors use these estimates to prove consistency, a development that later had several extensions . From another angle, in a series of insightful papers, Belkin _et al._ explore the phenomena of _double-descent_: where a double-descent curve subsumes the textbook U-shaped bias--variance trade-off curve, with a second decrease in the error occurring beyond the point where the model has reached the capacity of interpolating the training data. Interestingly, minimum-norm interpolators are also basic scenarios for observing this phenomena . _Our research connects the research on robustness to adversarial attacks to the study of generalization of minimum-norm interpolators._

**Adversarial training in linear models.** The generalization of adversarial attacks in linear models is well-studied. Tsipras _et al._ and Ilyas _et al._ use linear models to explain the conflict between robustness and high-performance models observed in neural networks; Ribeiro _et al._ use these models to show how overparameterization affects robustness to perturbations; Taheri _et al._ derive asymptotics for adversarial training in binary classification. Dan _et al._ and Dobriban _et al._ study adversarial robustness in Gaussian classification problems. Javanmard _et al._ provide asymptotics for adversarial training in linear regression, Javanmard _et al._ in classification settings and Hassani _et al._ for random feature regressions. Min _et al._ investigate how the dataset size affects adversarial performance. Yin _et al._ provide an analysis of \(_{}\)-attack on linear classifiers based on Rademacher complexity. These works, however, focus on the generalization properties. _We provide a fresh perspective on the problem by clarifying the connection of adversarial training to other regularization methods._

**Robust regression and square-root Lasso.** The properties of Lasso  for recovering sparse parameters in noisy settings have been extensively studied, and bounds on the parameter estimation error are well-known . However, these results rely on choices of regularization parameters that depend on the variance of additive noise. Square-root Lasso  was proposed to circumvent this difficulty. We show that for \(_{}\)-adversarial, _similarly to the square-root Lasso_, bounds can be obtained with the adversarial radius set without knowledge about the variance of the additive noise. We also show that both methods fit into the robust regression framework . The work on robust classification  is also connected to our work, there they show that certain robust support vector machines are equivalent to SVM. Our results for classification in Section 8 can be viewed as a generalization of their Theorem 3.

**Dual formulation.** Variations of Proposition 1 are presented in . Equivalent results in the context of classification are presented in . We use the reformulation extensively in our developments and also present a generalized statement for it.

## 3 Background

Different estimators will be relevant to our developments and will be compared in this paper.

**Adversarially-trained linear regression.** Our main object of study is the minimization of

\[R^{}(;,\|\|)=_{i=1}^{n} _{\|_{i}\|}|y_{i}-(_{i}+ _{i})^{}|^{2}=}{n} _{i=1}^{n}(|y_{i}-_{i}^{}|+ \|\|_{*})^{2},\]

where equality (a) follows from Proposition 1. We use \(\|\|_{*}=_{\|\| 1}|^{ }|\) to denote the dual norm of \(\|\|\). We highlight that the \(_{2}\)-norm, \(\|\|_{2}=_{i}|_{i}|^{2}\) is dual to itself. The \(_{1}\)-norm, \(\|\|_{1}=_{i}|_{i}|\), is the dual norm of the \(_{}\)-norm, \(\|\|_{}=_{i}|_{i}|\). When the adversarial disturbances are constrained to the \(_{p}\) ball: \(\{:\|\|_{p}\}\) we will call it \(_{p}\)-adversarial training. Our focus will be on \(_{}\) and \(_{2}\)-adversarial training.

**Parameter-shrinking methods.** Parameter-shrinking methods explicitly penalize large values of \(\). Regression methods that involve shrinkage include Lasso  and ridge regression, which minimize,respectively

\[R^{}(;)=_{i=1}^{n}|y_{i}-_{i}^{ }|^{2}+\|\|_{1} R^{}(;)=_{i=1}^{n}|y_{i}-_{i}^{}|^{ 2}+\|\|_{2}^{2}.\]

**Square-root Lasso:** Setting (with guarantees) the Lasso regularization parameters requires knowledge about the variance of the noise. Square-root Lasso  circumvent this difficulty by minimizing:

\[R^{}}(,)=_{i=1}^{n}  y_{i}-_{i}^{}^{2}}+\|\|_{1}.\] (3)

**Minimum-norm interpolator:** Let \(^{n p}\) denote the matrix of stacked vectors \(_{i}\) and \(^{n}\) is the vector of stacked outputs. When matrix \(\) has full row rank, the linear system \(=\) has multiple solutions. In this case the minimum \(\|\|_{*}\)-norm interpolator is the solution of

\[_{}\|\|_{*}=.\] (4)

## 4 Adversarial training in the overparametrized regime

Our first contribution is to provide conditions for when the minimum-norm interpolator is equivalent to adversarial training in the overparametrized case (we will assume that \(()=n\) and \(n<p\)). On the one hand, our result gives further insight into adversarial training. On the other hand, it allows us to see minimum-norm interpolators as a solution of the adversarial training problem.

**Theorem 1.** Assume the matrix \(^{n p}\) to have full row rank, let \(}\) denote the solution of the dual problem \(_{\|^{}\| 1}^{}\), and \(\) denote the threshold

\[=(n\|}\|_{})^{-1}.\] (5)

The minimum \(\|\|_{*}\)-norm interpolator minimizes the adversarial risk \(R^{}(,,\|\|)\) if and only if \((0,]\).

_Remark 1_ (Bounds on \(\)).: The theorem allows us to directly compute \(\), but it does require solving the dual problem to the minimum-norm solution. In Appendix A.3, we provide bounds on \(\) that avoid solving the optimization problem. For instance, for \(_{}\)-adversarial attacks, we have the following bounds depending on the singular values of \(\): \(}_{n}() n_{1}( )\). Where \(_{1}\) and \(_{n}\) denote the largest and smallest _positive_ singular values of \(\), respectively.

Proof of Theorem 1.: Let \(}\) and \(}\) be the minimum \(\|\|_{*}\)-norm solution and the solution of the associated dual problem. Throughout the proof we denote \(R^{}()=R^{}(;,\|\|)\), dropping the last two arguments. The subgradient of \(R^{}()\) evaluated at \(}\) is given by2

\[(_{0})=\{^{p}:()- (_{0})(-_{0}),\;^{p}\}.\]

 See  for properties. We use \(\|}\|_{*}\) to denote the subgradient of \(()=\|\|_{*}\) evaluated at \(}\).

\[ R^{}(})=_{i=1}^{n} \|}\|_{*}(_{i}_{i}+\|}\|_{*}),\] (6)

for any \(=(_{1},,_{n})^{n}\) that satisfies \(\|\|_{} 1\). We have that any element of \(\|}\|_{*}\) can be written as \(^{}}\) (we prove that in Lemma 1 in the Appendix). Hence, we can rewrite

\[ R^{}(})=_{i=1}^{n} \|}\|_{*}(_{i}_{i}+^{T}})=2\|}\|_{*}^{}}{n}+^{}}.\]

If \(=1/(n\|}\|_{})\), we can take \(=-n}\) and the subderivative contains zero. On the other hand, if \(>1/(n\|}\|_{})\) then \(^{}}{n}+^{}}\) is not zero for \(\|\|_{} 1\). 

The minimum \(_{1}\)-norm interpolator is well studied in the context of 'basis pursuit' and allows the recovery of low-dimensional representations in sparse signals . The interest in the minimum \(_{2}\)-norm is more recent. It played an important role in studying the interplay between interpolation and generalization, being used in many recent papers where the double-descent ,  and the benign overfitting phenomena  were observed.

Theorem 1 gives a new perspective on the robustness of the minimum-norm interpolator, allowing us to see it as a solution to adversarial training with adversarial radius \(\). Figure 2 shows that the radius \(\) of the adversarial training problem corresponding to the minimum-norm interpolator increases with the ratio \(p/n\). It is natural to expect that training with a larger radius would yield more adversarially robust models on new test points. The next result formalizes this intuition by establishing an upper bound on the robustness gap: the difference between the square-root of the expected adversarial squared error \(^{}_{*}(;_{},\|\|)= _{y_{0},_{0}}[_{\|_{i}\|_{ }}(y_{0}-(_{0}+_{0})^{})^{2}]\) and the expected squared error \(_{*}()=_{y_{0},_{0}}[(y_{0}-_{0 }^{})^{2}]\). The upper bound depends on the in-training adversarial error and on the ratio \(}}{}\) which are quantities that you can directly compute.

**Proposition 2**.: Assume \(\) to have full row rank and let \(}\) be the minimum \(\|\|_{*}\)-norm interpolator, than

\[^{}_{*}(};_{}, \|\|)}-_{*}(})}}}{}}(};,\|\|)}.\] (7)

_Remark 2_.: The example in Figure 2 is one case where adding more features makes the minimum-norm interpolator more robust. Examples showing the opposite and illustrating that adding more features to linear models can make them less adversarially robust are abound in the literature , . Indeed, examples that consider the minimum \(_{2}\)-interpolator subject to \(_{}\)-adversarial attacks during test-time result in this scenario, as discussed in . Proposition 7 in the Appendix is the equivalent of Proposition 2 for this case (mismatched norms in train and test). There, the upper bound grows with \(\), which explains the vulnerability in this scenario.

_Remark 3_.: A pitfall of analyzing the minimum-norm solution properties in linear models is that it requires the analysis of nested problems: different choices of \(p\) require different covariates \(\). The random projection model proposed by Bach  avoids this pitfall by considering the input \(\) is fixed, but only a projected version of it \(\) is observed, with the number of parameters being estimated changing with the number of projections observed. Adversarially training this model consists in finding a parameter \(}\) that minimizes: \(_{i=1}^{n}_{\|_{i}\|}|y_{i}-( _{i}+_{i})^{}^{}|^{2}\). We generalize our results so that we can also study this case (See Appendix A.4).

## 5 Adversarial training and parameter-shrinking methods

Theorem 1 stated an equivalence with the minimum-norm solution when \(\) is small. The next result applies to the other extreme of the spectrum, characterizing the case when \(\) is large.

**Proposition 3** (Zero solution of adversarial training).: The zero solution \(}=\) minimizes the adversarial training if and only if \(^{}\|}{\|\|_{1}}\).

In the remaining of this section, we will characterize the solution between these two extremes. We compare adversarial training to Lasso and ridge regression.

### Relation to Lasso and ridge regression

Proposition 1 makes it clear by inspection that adversarial training is similar to other well-known parameter-shrinking regularization methods. Indeed, for \(_{}\)-adversarial attacks, the cost function \(R^{}(;,\|\|_{})\) in its dual form is remarkably similar to Lasso  and \(R^{}(;,\|\|_{2})\), to ridge regression (the cost functions were presented in Section 3). In Figures 1 and 3, we show the regularization paths of these methods (the dataset is described in ). We observe that \(_{}\)-adversarial training produces sparse solutions and that Lasso and \(_{}\)-adversarial training have extremely similar regularization paths. There are also striking similarities between ridge regression and \(_{2}\)-adversarial training, with the notable difference that for large \(\) the solution of \(_{2}\)-adversarial

Figure 2: **Threshold \(\)_vs._ number of features.** We fix \(n=60\) and show the value of \(\) (defined in (5)) as a function of the number of features \(p\). The dotted lines give the reference \(_{}=0.01[\|\|]\) for comparison.

training is zero (which is explained by Proposition 3). The next proposition justifies why we can observe such similarities in part of the regularization path. It applies to data that has been normalized and for positive responses \(y\) (as it is the case in our example).

**Proposition 4.** Assume the output is positive and the data is normalized: \( 0\) and \(^{}=\).

The solution of adversarial training \(}\), for \(\|}\|_{*}_{i}|}{\|x_{i}\|}\) is also the solution of

\[_{}_{i=1}^{n}|y_{i}-_{i}^{}|^{ 2}+\|\|_{*}+\|\|_{1}^{2}.\]

The proposition is proved in Appendix B. It establishes the equivalence between \(_{}\)-adversarial training and Lasso (even though there is not a closed-formula expression for the map between \(\) and \(^{}\)). Indeed, _under the assumptions of the propositions, \(}\) is the solution of \(_{}\)-adversarial problem only if it is the solution of_

\[_{}_{i=1}^{n}(y_{i}-_{i}^{})+ ^{}||||_{1},\] (8)

_for some \(^{}\)._ We can prove this statement using the proposition: under the appropriate assumptions, the \(_{}\)-adversarial problem solution is also a solution to the problem

\[_{}_{i=1}^{n}(y_{i}-_{i}^{})^ {2}+||||_{1}+||||_{1}^{2},\]

which in turn, is the Lagrangian formulation of the following constrained optimization problem

\[_{}_{i=1}^{n}(y_{i}-_{i}^{})^ {2}||||_{1}+||||_{1} ^{2},\]

for some \(||||_{1}\). The constraint can be rewritten as \(||||_{1}-||||_{1}\). And, the result follows because (8) is the Lagrangian formulation of this modified problem. A similar reasoning could be used to connect the result for \(_{2}\)-adversarial training with ridge regression.

More generally, even when the condition on \(}\) is not satisfied and if \(\) is negative, we show in Appendix B that for zero-mean and symmetrically distributed covariates, i.e., \([]=0\) and \((-)\), adversarial training has approximately the same solution as

\[_{}_{i=1}^{n}|y_{i}-_{i}^{}|^ {2}+\|\|_{*}+^{}^{2},\]

for \(=(-^{}})\). This explains the similarities in the regularization paths.

### Transition into the interpolation regime

The discussion above hints at the similarities between adversarial training and parameter-shrinking methods. Interestingly, Lasso and ridge regression are also connected to minimum-norm interpolators. The ridge regression solution converges to the minimum-norm solution as the parameter vanishes

Figure 3: **Regularization paths** in the Diabetes dataset  (see dataset description in Figure 1). On the horizontal axis, we give the inverse of the regularization parameter (in log scale). On the vertical axis, the coefficients.

i.e., \(}^{}}()}^{-_{2}}\) as \( 0^{+}\). Similarly, there is a relation between the minimum \(_{1}\)-norm solution and Lasso. The relation requires additional constraints because, for the overparameterized case, Lasso does not necessarily have a unique solution. Nonetheless, it is proved in [41, Lemma 7] that the Lasso solution by the LARS algorithm satisfies \(}^{}()}^{-_{1}}\) as \( 0^{+}\).

For a sufficiently small \(\), the solution of \(_{2}\)-adversarial training equals the minimum \(_{2}\)-norm solution; and, the solution of \(_{}\)-adversarial training equals the minimum \(_{1}\)-norm solution. There is a notable difference though: while this happens _only in the limit_ for ridge regression and Lasso, for adversarial training this happens _for all values \(\) smaller than_ the threshold \(\). We illustrate this phenomenon next. Unlike ridge regression and Lasso which converge towards the interpolation solution, adversarial training goes through abrupt transitions and suddenly starts to interpolate the data. Figure 4 illustrates this phenomenon in synthetically generated data (with isotropic Gaussian feature, see Section 7).

**Intuitive explanation for abrupt transitions.** To get insight into why the abrupt transition occurs we present the case where \(n=1\), i.e., the dataset has a single data point \((,y)\), where \(y=1\) and \(\|\|_{2}=1\). We can write \(R^{}(;,\|\|_{2})=(( ))^{2}\), where

\[()=|y-^{}|+\|\|_{2}.\]

The function \(\) is necessarily minimized along the subspace spanned by the vector \(\). Now, along this line, the function \(\) is piecewise linear with three segments, see Figure 5. One of the three segments changes the slope sign as \(\) decreases and the minimum of the function changes abruptly, thus explaining why abrupt transitions occur.

## 6 Relation to robust regression and square-root Lasso

In this section we establish that both adversarial training and square-root Lasso can be integrated into the robust regression framework. We further investigate the similarities between the methods by analyzing the prediction error upper bounds.

**Robust regression framework.** Robust linear regression considers the minimization of the following cost function

\[R^{}(;)=_{ }\|-(+) \|_{2},\] (9)

where the disturbance matrix \(\) is constrained to belong to the 'disturbance set' \(\). We connect robust regression, square-root Lasso and adversarial training. The next proposition gives the equivalence between robust regression for a row-bounded disturbance sets \(_{p,}\) and \(_{p}\)-adversarial training, see the appendix for the proof.

Figure 4: **Training mean squared error _vs_ inverse adversarial radius. _Left:_ for ridge and \(_{2}\)-adversarial training. _Right:_ for Lasso and \(_{}\)-adversarial training. The error bars give the median and the 0.25 and 0.75 quantiles from 5 realizations. The vertical black lines show \(\) in (5). Figure S.9 (appendix) shows the test MSE. For ridge regression or Lasso (see Section 3), the \(x\)-axis should be read as \(1/\), rather than \(1/\). We generate the data synthetically using an isotropic Gaussian feature model (see Section 7) with \(n=60\) training data points and \(p=200\) features.

**Proposition 5.** For a disturbance set with the rows bounded by \(\):

\[_{p,}=\{[_{1}\\ _{n} _{i}}}:\|_{i}\| , i\},\]

we have that \(_{}R^{}(, _{p,})=_{}R^{}(; ,\|\|)\).

On the other hand, there is an equivalence between square-root Lasso and robust regression for column-bounded disturbance sets \(_{2,}\). This was established by Xu _et al._[33, Theorem 1] and we repeat it below.

**Proposition 6.** For a disturbance set with columns bounded by \(\):

\[_{2,}=\{[||&&|\\ _{1}.&&_{m} |]:\|_{i}\|_{2}, i\},\]

we have that \(_{}R^{}(;_{2,})=_{}R^{}}( {},)\).

The above discussion hints at the similarities between adversarial training and square-root Lasso, as instances of robust regression under different constraints. Square-root Lasso minimizes \(R^{}}(,)=n^{-1/2}\|- \|_{2}+\|\|_{1}\) (See Section 3). The main motivation for the square-root Lasso is that it is a 'pivotal' method for sparse recovery: that is, it attains near-oracle performance without knowledge of the variance levels to set the regularization parameter . As we will show in the next section, a similar property applies to \(_{}\)-adversarial training.

**Fixed-design analysis and similarities with square-root Lasso.** In this section, we assume that the data was generated as: \(y_{i}=_{i}^{}^{*}+_{i}\) where \(^{*}\) is the parameter vector used to generate the data. Under these assumptions, we can derive an upper bound for the (in-sample) prediction error:

**Theorem 2.** Let \(>^{*}=3^{}\|_{ }}{\|\|_{1}}\), the prediction error of \(_{}\)-adversarial training satisfies the bound:

\[\|(}-^{*})\|_{2}^{2} 8\|^{*}\|_{1}( \|\|_{1}+10\|^{*}\|_{1} ).\] (10)

For comparison, we also provide the result for Lasso: (Adapted from [31, Thm. 7.13, p. 210])

**Theorem 3.** Let \(>^{*}=3\|^{}}{n }\|_{}\), the prediction error of Lasso satisfies the bound:

\[\|(}-^{*})\|_{2}^{2} 8\|^{*}\|_{1}.\] (11)

Without additional constraints (see Remark 5) it can be shown that for Lasso it is not possible to improve the above bound. To satisfy this bound, however, Lasso requires knowledge of the magnitude of the noise \(\). In Theorem 3, \(^{*}=\|^{}\|\). Hence, if we rescale \(\) (i.e., \(\)) then a correspondent change in magnitude follows in \(^{*}\) (i.e., \(^{*}^{*}\)). Square-root Lasso  avoids this problem and achieves a similar rate even without knowing the variance: i.e., it is a 'pivotal' method. Our method has similar properties and allows us to set \(\) without estimating the variance. This can be seen in Theorem 2, where re-scaling \(\) does not alter the value of \(^{*}=3\|^{}\|/\|\|_{1}\) because it affects the numerator and denominator simultaneously.

For instance, if we assume \(\) has i.i.d. \((0,^{2})\) entries and that the matrix \(\) is fixed with \(_{j=1,,m}\|_{j}\|_{} M\). For \( M\), we (with high-probability) satisfy the condition in Theorem 3, obtaining: \(\|(}-^{*})\|_{2}^{2} M\). For adversarial training, we can set: \( M\), and (with high-probability) satisfy the theorem condition, obtaining the same bound. Notice that the choice of \(\) is not dependent on \(\). We provide a full analysis in Appendix C.3.

_Remark 4_ (On the relation between \(\) and \(^{*}\)).: For sufficiently large \(n\), we have \(^{*}>\) in the scenario above--the noise \(\) has i.i.d. normal entries \((0,^{2})\) and the matrix \(\) is fixed. We can prove it by contradiction: if \(^{*}\) then we could apply the bound from Theorem 2 to the minimum \(_{1}\)-norminterpolator (due to Theorem 1). Hence, \(^{2}\|\|_{2}^{2}=\|( }-^{})\|_{2}^{2} M^{}\) and we can always choose a sufficiently large value of \(n\) for which the inequality is false.

_Remark 5_.: In the original square-root Lasso paper , a bound is obtained for the \(_{2}\) parameter distance: \(\|}-^{}\|_{2}^{2}\) for the case \(\) satisfy the restricted eigenvalue condition and \(^{}\) is sparse. Under these more strict assumptions (restricted eigenvalue condition) we can also obtain a faster convergence rate for the prediction error. Here we focus on the fixed-design prediction error without additional assumptions on \(\). For other predictors, these other proofs follow similar steps and yield similar requirements on \(^{}\), see [31, Chapter 7]. However, we leave these and other analysis (such as variable selection consistency) of adversarial training for future work.

## 7 Numerical Experiments

We study five different examples. The main goal is to experimentally confirm our theoretical findings. For each scenario, we compute and plot: train and test MSE for different choices of \(p\), \(n\) and \(\). For comparison purposes, we also compute and plot train and test MSE for Lasso and ridge regression and minimum-norm interpolators. Finally, in line with our discussion in Section 4, we compute and plot \(\) as a function of \(p/n\). In all the numerical examples the adversarial training solution is implemented by minimizing (2) using CVXPY . The scenarios under consideration are described below (see Appendix D for additional details)

1. **Isotropic Gaussian feature model**. The output is a linear combination of the features plus additive noise: \(y_{i}=_{i}^{}+_{i}\), for Gaussian noise and covariates: \(_{i}(0,^{2})\) and \(_{i}(0,_{p})\).
2. **Latent-space feature model**[26, Section 5.4]. The features \(\) are noisy observations of a lower-dimensional subspace of dimension \(d\). A vector in this _latent space_ is represented by \(^{d}\). \(=+\). The output is a linear combination of the latent space plus noise.
3. **Random Fourier features model**. We apply a random Fourier feature map to inputs of the Diabetes dataset . Random fourier features are obtained by the transformation \(_{i}=}(_{i}+)\) where the entries of \(\) and \(\) are independently sampled. It can be seen as a one-layer untrained neural network and approximates the Gaussian kernel feature map .
4. **Random projection model**. The data is generated as in the isotropic Gaussian feature model, but we only observe \(_{i}\), i.e., a projection of the inputs.
5. **Phenotype prediction from genotype.** We illustrate our method on the Diverse MAGIC wheat dataset  from the National Institute for Applied Botany. We use a subset of the genotype to predict one of the continuous phenotypes.

**Results.** We experimentally corroborate Theorem 1 in all scenarios: In Figures 4 and S.8 (Appendix) we show the train MSE as we change the adversarial radius \(\), confirming the abrupt transitions into the interpolation regime and also that \(\) is the transition point. In Figures S.9 and S.10 we show the corresponding test MSE without and in the presence of an adversary, respectively. Interestingly, the adversarial radius that yields the best results is not always equal to the radius \(_{}\) the model will be evaluated on. Figures 2 and S.5 (Appendix) display \(\) as a function of the ratio \(p/n\). We observe that \(/[\|\|]\) is growing in all examples and we would still expect improved robustness in light of Proposition 2. The Random Fourier features model is the only case where \(\) seems to decrease (in absolute value). However, \([\|\|_{1}]\) is also decreasing (and at a faster rate). Figures S.6 and S.7 give test MSE without and in the presence of an adversary for the minimum-norm interpolator as a function of the ratio \(p/n\), confirming improved adversarial robustness as \(p\) grows.

Figure 6 provides a comparison of the test error of the different methods under study. For Lasso, ridge and adversarial training, we use the best \(\) or \(\) available for each method (obtained via grid search). We note that while for \(p=1000\) optimally tuned Lasso and \(_{}\)-adversarial training significantly outperform the corresponding minimum \(_{1}\)-norm interpolator. As \(p\) increases, the performance of the three different methods becomes quite similar. It is also an example where the minimum \(_{1}\)-norm outperforms the minimum \(_{2}\)-norm interpolator (for large \(p\)). In the same setup, Figure S.12

Figure 6: **Test normalized MSE (NMSE) in the MAGIC dataset.**

study a choice of adversarial radius inspired by Theorem 2. We use \(\|\|_{}/\|\|_{1}\) for \(\) a vector with zero-mean normal entries. We use a random \(\), since we do not know the true additive noise. Even with this approximation, \(_{}\)-adversarial training performs comparably with Lasso with the regularization parameter set using 5-fold cross-validation doing a full search in the hyperparameter space. The figure also provides a comparison with square-root Lasso under a similar setting.

## 8 Results for general loss functions

The following theorem can be used to generalize Proposition 1.

**Theorem 4.** Let \(:\) be a convex and lower-semicontinuous function, for every \( 0\),

\[_{\|\|}((+)^{})=_{s\{-1,1\}}( ^{}+ s\|\|_{*}).\] (12)

We can find a closed-formula expression for \(s_{*}=_{s\{-1,1\}}(^{}x+ s\|\|_{*})\) in many cases of interest. For **regression problems**, given any non-decreasing function \(:^{+}^{+}\), and \((^{})=(|^{}-y|)\). If \(\) is lower semicontinuous and convex then so will \(\) and the result of the theorem holds (the squared loss \((z)=z^{2}\) is a special case). Then \(s_{*}=-(y-^{})\) and

\[_{\|\|}(|(+)^{ }-y|)=(|y-x^{}|+\|\|_{*}).\]

For **classification**, let \(y\{-1,1\}\) and \((^{})=(y(^{}))\) where \(\) is a non-increasing function. If \(\) is lower semicontinuous and convex then so will \(\) and the result of the theorem holds. Then \(s_{*}=-y\) and we obtain:

\[_{\|\|}(y((+)^{ }))=(y(^{})-\|\|_{*}).\]

The above result can also be applied to unsupervised learning. In the Appendix, we illustrate how Theorem 4 can be used for **dimensionality reduction**. We consider the problem of finding \(\) that minimizes the reconstruction error \(\|+^{}\|_{2}^{2}\) (that yields PCA algorithm) and derive an adversarial version of it.

_Remark 6_.: Over this paper we consider \(,^{p}\), but Theorem 4 holds generaly for \(\) a vector in a Banach space endowed with norm \(\|\|\) and \(^{*}\) a continuous linear map \(:\). Just define \(^{}:=()\) and take \(\|\|_{*}\) to be the norm of the dual space \(^{*}\).

## 9 Conclusion

We study adversarial training in linear regression. Adversarial training allows us to depart from the traditional regularization setting where we can decompose the loss and the regularization terms, minimizing \(_{i=0}^{n}(_{i}^{},y_{i})+()\) for some penalization function \(\). We show how it provides new insights into the minimum-norm interpolator, by proving a new equivalence between the two methods (Section 4). While adversarially-trained linear regression arrives at similar solutions to parameter shrinking methods in some scenarios (Section 5), it also has some advantages, for instance, the adversarial radius might be set without knowing the noise variance (Section 6). Unlike, squared-root Lasso it achieves this while still minimizing a sum of squared terms. We believe a natural next step is to provide a tailored solver that can allow for efficient solutions for models with many features, rendering adversarially-trained linear regression useful, for instance, in modeling genetics data (see one minimal example of phenotype prediction from the genotype in Section 7).

Another interesting direction is generalizing our results to classification and nonlinear models. Indeed, adversarial training is very often used in the context of neural networks and it is natural to be interested in the behavior of this class of models. Our work allows for the analysis of simplified theoretical models commonly used to study neural networks. Random Fourier feature models can be analyzed using our theory and are they studied in the numerical examples. These models can be seen as a simplified neural network model (one-layer neural network where only the last layer weights are adjusted). In Appendix A.4, we provide adversarial training after linear projections, and could, for instance, be used to analyze deep linear networks, as the ones studied in . Finally, Section 8 provides a result for infinite spaces, and one could attempt to use it to analyze kernel methods and infinitely-wide neural networks . Overall we believe that our results provide an interesting set of tools also for the analysis of nonlinear models, and could provide insight into the empirically observed interplay (see ) between robustness and regularization in deep neural networks.