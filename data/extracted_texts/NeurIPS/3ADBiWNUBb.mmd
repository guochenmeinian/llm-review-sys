# Graph Structure Inference with BAM:

Neural Dependency Processing via Bilinear Attention

Philipp Froehlich &Heinz Koeppl

Department of Electrical Engineering and Information Technology

Technische Universitat Darmstadt

{philipp.froehlich, heinz.koeppl}@tu-darmstadt.de

###### Abstract

Detecting dependencies among variables is a fundamental task across scientific disciplines. We propose a novel neural network model for graph structure inference, which aims to learn a mapping from observational data to the corresponding underlying dependence structures. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. Central to our approach is a novel bilinear attention mechanism (BAM) operating on covariance matrices of transformed data while respecting the geometry of the manifold of symmetric positive definite (SPD) matrices. Inspired by graphical lasso methods, our model optimizes over continuous graph representations in the SPD space, where inverse covariance matrices encode conditional independence relations. Empirical evaluations demonstrate the robustness of our method in detecting diverse dependencies, excelling in undirected graph estimation and showing competitive performance in completed partially directed acyclic graph estimation via a novel two-step approach. The trained model effectively detects causal relationships and generalizes well across different functional forms of nonlinear dependencies.

## 1 Introduction

The discovery and understanding of dependencies among variables are fundamental to scientific inquiry across diverse disciplines, such as biology , climate science , economics , and social studies . These dependencies are commonly represented as edges within graphical models, particularly directed acyclic graphs (DAGs), first introduced by Wright  to study genetic inheritance, and later advanced for probabilistic inference by Lauritzen and Spiegelhalter . Graph structure inference, the process of deriving such graphical representations from observational data, is essential for gaining insights into complex systems and their underlying causal relationships . A prime example is the estimation of gene regulatory networks from experimental data , where identifying dependencies between genes is crucial for understanding biological processes and mechanisms.

Graph structure inference typically employs unsupervised learning methods to estimate the underlying graph through either score-based approaches, which rank graphs by predefined metrics, or constraint-based approaches that identify edges between variable pairs via conditional independence tests . However, these methods face challenges. Score-based approaches encounter computational burdens due to the superexponential growth of potential graph structures with node count, and the necessity to balance fit and structural sparsity . Constraint-based methods usually require a large sample size , rely on an elusive optimal threshold hyperparameter, and Shah and Peters  proved that the failure of Type I error control in underlying conditional independence tests is unavoidable, which can have significant consequences in downstream analyses.

Supervised causal learning techniques, as presented by Lopez-Paz et al. [39; 40; 41], Li et al. , Ke et al. , Lorch et al. , Dai et al. , have recently emerged as an appealing alternative to unsupervised methods. In this rising approach, a neural network is typically trained on simulated matrix-shaped data, with corresponding graph structures serving as ground-truth labels for supervised learning. The paradigm capitalizes on the strengths of deep learning to discern complex patterns in data, thereby enabling accurate graph structure inference.

Supervised causal learning strongly relies on neural networks' ability to extract dependency information from observational data matrices. Current approaches for processing observational data, whether in causal learning [31; 42], or related fields [34; 54], typically embed the data into expanded observational data spaces, attempting to implicitly capture dependencies within the Euclidean space in transformed data matrices. After marginalizing over the sample dimension, existing supervised causal learning methods [31; 42] then predict edges separately from each other via compressed observational representation vectors, rather than estimating a single graph object as in graphical lasso (GLASSO) algorithms [72; 7; 17]. GLASSO methods effectively optimize inverse covariance matrices in SPD space, where matrix entries directly encode the graph's conditional dependencies. This classical, highly influential approach is particularly powerful in the Gaussian case, where the covariance matrix is a sufficient statistic for the distribution. We extend this geometric insight from classical optimization to the neural network era by introducing SPD layers that learn and process dependency information directly. Instead of using pre-derived mathematical operations as in GLASSO-based algorithms, our network learns these shape-invariant transformations from data. Combined with learned transformations in the observational space, our approach generalizes beyond the Gaussian assumption of classical methods while preserving both the theoretical guarantees and computational efficiency of GLASSO approaches. In contrast to GLASSO methods which must solve an optimization problem for each new dataset, our approach requires only a single forward pass.

Analogous to GLASSO-based methods, the symmetry of covariance matrices constrains our approach to undirected graph estimation. We extend beyond this limitation via a novel two-phase procedure. We first estimate the graph's skeleton and immoralities, then identify edge directions to obtain a completed partially directed acyclic graph (CPDAG) . While this follows the general principle of the PC-algorithm , our deep learning approach distinguishes between skeleton edges and moralized edges through joint optimization in SPD space, instead of relying on sequential conditional independence tests over all possible immoralities for edge identification. By efficiently identifying immoralities in the first step, our method drastically reduces the number of required independence tests, minimizing both computational complexity and the propagation of false positive orientation errors.

Figure 1: Neural network architecture: An input of arbitrarily shape \((M,d)\) is provided, which is then embedded into \(C\) channels. Attention between attributes and attention between datapoints are applied alternately. Covariance matrices are calculated, followed by alternating applications of bilinear attention and the custom activation function in the Riemannian manifold of SPD matrices. The matrices are then transformed back into Euclidean space using the \(\)-\(\) layer. Output probabilities for each pair of variables being in the classes “no edge”, “skeleton edge”, and “moralized edge” are calculated using dense layers and applying a softmax layer on the channel axis.

## 2 Method

### Supervised approach for learning graph structures

In supervised graph learning, simulated training data is generated in the form of input/label pairs \((,)\). Each pair consists of an observational data matrix \(^{M d}\), where \(M\) denotes the number of samples and \(d\) represents the number of attributes, and its corresponding graph structure \(=(V,E)\). The graph \(\) is defined by a set of nodes \(V=\{v_{1},,v_{d}\}\) and a set of edges \(E V V\). Alternatively, graphs can be represented as adjacency matrices \(\{0,1\}^{d d}\), where \(_{ij}=1\) if an edge exists between nodes \(v_{i}\) and \(v_{j}\), and \(_{ij}=0\) otherwise. The process begins by creating a random graph, followed by generating observational data matrices using a structural equation model (SEM) \(X_{v}=f_{v}(X_{_{}(v)},_{v})\), where \(f_{v}\) is a measurable coupling function of parent nodes \(()\) and zero-mean error \(_{v}\). The random coupling in the SEM is designed to approximate a broad class of functions, enabling the model to perform classification of the presence or absence of causal relationships, irrespective of their specific functional forms. A neural network then learns the relationship between the graph and the generated data matrix as a mapping \(\), enabling the inference of graph structures from observational data. The supervised causal learning procedure is shown in Figure 2.

### Generation of training data

For training data generation, we utilize Erdos-Renyi (ER) graphs, denoted as \(ER(d,q)\), where the number of nodes \(d\) and the expected degree \(q\) are sampled from discrete uniform distributions \(d(\{,,\})\) and \(q U([q,])\), respectively. The shape-invariant design of our model enables it to train without restriction to a fixed \((d,q)\) pair, which proves advantageous when the graph density is unknown. For each graph \(_{i}\), we generate a data matrix \(_{i}^{M d}\) using an SEM, where the sample size \(M\) is drawn from a discrete uniform distribution, \(M(\{\{,,\}\})\). Although ER graphs are used for training, we demonstrate that our approach maintains its performance when tested on graphs generated using different random graph models, as detailed in Appendix G.6.

As coupling functions in the SEM, we employ random multivariate Chebyshev polynomial functions to generate diverse continuous training data. Chebyshev polynomials effectively approximate real-world functions, showing factorial decay in their coefficients given that higher order derivatives are bounded , i.e., for the \(n\)-th coefficient \(c_{n}\) it holds \(|c_{n}|\) for a constant \(C\), making higher-degree terms negligible, such that all such smooth functions can be efficiently approximated with a

Figure 2: Overview of our supervised graph learning approach. We generate synthetic training data by first creating random ground truth graphs. Then, observational data matrices are generated using SEMs with randomly parameterized polynomials. Employing our novel neural network architecture, we learn the mapping \(\) from observational data matrices to the corresponding graph structures.

low polynomial degree Chebyshev polynomial. This choice of parameterization avoids unnatural, non-smooth coupling functions where a higher-order derivative is much larger than a lower-order one, and where there is a hard cutoff in the approximated order of the derivative. Chebyshev polynomials have been widely used in various applications due to their excellent approximation properties and computational efficiency [44; 53]. Scatterplots of input/output values of four example Chebyshev polynomials are shown on the left side of Figure 2. The error terms \(_{v}\) in the SEM are modeled using Gaussian mixture models, which provide a flexible and expressive framework for capturing a diverse range of error distributions . Details about the parameterization of the SEM can be found in Appendix E.

### Prediction task: three-class edge classification

Our neural network architecture is designed to process SPD matrices, which inherently contain exclusively symmetrical relationship information, thus excluding non-symmetrical information necessary for estimating edge directionality. While this configuration prevents the estimation of a DAG or CPDAG in a single run, it enables efficient estimation of symmetrical structures such as the graph skeleton. Additionally, it facilitates the identification of moralized edges. We differentiate between the following edge types:

* **Skeleton edges**: Undirected edges found in the underlying DAG.
* **Moralized edges**: Not present in the DAG but emerge due to conditional dependencies among nodes sharing a common child without a connecting edge between the parents.
* **No edge**: Conditionally independent variables given all other variables.

Skeleton and moralized edges are both represented as undirected edges in models that use partial correlations or conditional independence tests, in which the conditioning set includes all other variables, to estimate the moral graph, as e.g., in [17; 59].

Using the assumption of faithfulness and the Markov condition enable us to identify the Markov equivalence class  and thus uniquely solve the three-class classification problem. Appendix C provides an explicit proof outlining the conditional independence relations that lead to a unique solution.

The prediction targets are an extension of the binary adjacency matrix \(\{0,1\}^{d d}\) to a set of one-hot encoded adjacency matrices, denoted as \(}\{0,1\}^{d d 3}\). Here, for each \(i,j\), the vector \(}_{i,j,\,\,}\{0,1\}^{3}\) represents a one-hot encoded classification among the three classes: _skeleton edge_, _moralized edge_, and _no edge_.

### Neural network architecture

We use the multi-dimensional analogue of matrix multiplication: For a tensor \(^{I J K}\) and a matrix \(^{K L}\) we denote

\[=^{I J L}_{ijl}=_{k}_{ijk}_{kl}.\] (1)

The network architecture is depicted in Figure 1. In the following, we introduce the individual layers.

Channel embedding.We perform an embedding of the input \(^{M d}\) to obtain a hidden representation with \(C\) channels. For this, one axis for \(\) is extended to \(}^{M d 1}\) and then trainable weights \(_{1}^{1 C}\), \(_{2}^{C C}\) are used to obtain

\[=}+(}_{1} )_{2}^{M d C},\]

where broadcasting is used for the addition.

A shape- and permutation-invariant neural network is achieved by employing \(C\) weight matrices in conjunction with attention mechanisms for fixed channel dimensions \(C\) and \(\) of the current and the next layer. This enables information flow among elements both within each matrix and across the \(C\) matrices, analogous to sequence models like the Transformer , where a sequence of length \(l\) is embedded into an \(l C\) matrix and processed via self-attention with \(C\) trainable weights.

Observational data self-attention.Self-attention for \(C\) observational \(M d\) data matrices, stored in a tensor \(^{M d C}\), proposed by Kossen et al. , is based on axial attention  and commonly used for processing observational data . The attention-between-attributes layer (Figure 3, left) computes \(M\) attention matrices \(\) of shape \(d d\), allowing attributes to attend to each other in parallel for each sample. The attention-between-samples layer interchanges \(M\) and \(d\), resulting in \(d\) attention matrices of shape \(M M\), enabling samples to attend to each other. These layers capture information flow along both axes of the observational data matrices. A mathematical description is provided in Appendix B.1.

Transition to SPD space via covariance calculation.Let a transformed data matrix \(^{M d C}\) be given. To obtain explicit dependency information, we calculate channelwise covariance matrices, i.e.:

\[=((-})^{T(3,2,1)}(- })^{T(3,1,2)})^{T(2,3,1)}^{d d C},\]

where we define for tensors \(^{I J K}\), \(^{I K L}\) the \(I\)-parallel matrix multiplication \(:=(_{i,},_{i,},)_{i=1,,I} ^{I J L}\), and \(}=_{M}^{T}^{1 d C}\) is a tensor of sample means. We denote by \(_{}^{d d}\) the cone of \(d d\) symmetric positive semi-definite (SPD) matrices, and by \(_{}^{d d C}=_{}^{d d} _{}^{d d}\) the \(C\)-ary Cartesian power of \(_{}^{d d}\). It holds \(_{}^{d d C}\).

The transpose notation \({}^{T(i,j,k)}\) permutes the dimensions of a tensor according to the specified order \((i,j,k)\), following the commonly used notation in deep learning . For example, \(^{T(2,3,1)}\) rearranges the dimensions of \(\) such that the second dimension becomes the first, the third becomes the second, and the first becomes the third.

Introducing the bilinear attention mechanism.To effectively process dependency information stored in the covariance matrices, which is crucial for causal discovery, we propose a novel bilinear attention mechanism. Inspired by the success of SPD networks in image processing, our bilinear attention layer is designed to be shape-invariant and permutation-invariant. In contrast to existing SPD architectures  that typically parameterize weights \(^{d d_{}}\) to be applied as \(^{T}_{}^{d d}\) to a matrix \(_{}^{d d}\), we parameterize weights \(^{C C}\) to act as linear combinations \(_{}^{d d C}\) on a set of covariance matrices \(_{}^{d d C}\). We employ an attention mechanism to generate attention matrices \(^{d d C}\), serving as adaptable \(d d\) weights on each \(d d\) matrix of \(^{d d C}\). Our bilinear attention mechanism is shown in (Figure 3, right).

The Riemannian manifold of SPD matrices has nonpositive sectional curvature, and its geometric properties are not preserved under Euclidean operations , leading to issues when using traditional neural networks . Specialized architectures respecting the SPD manifold's geometry are necessary. Bilinear1 matrix multiplication \(^{T}\), analogous to a dense layer in Euclidean space, preserves the space of SPD matrices \(_{}^{d d}\) and serves as a primary tool for SPD layers .

Figure 3: Left: Observational data self-attention layer across attributes. Gray denotes non-trainable tensors, and red represents trainable weights. Matrix multiplication is performed after necessary transposition to match axis dimensions. Right: Bilinear self-attention layer. The double arrow signifies the use of the matrix as a bilinear operator.

For an input \(_{}^{d d C}\), we obtain keys \(=_{K}_{}^{d d C}\) and queries \(=_{Q}_{}^{d d C}\), using non-negativity constraints on the weights \(_{K},_{Q}_{+}^{C C}\).

Keys and queries are now combined in a bilinear fashion, parallel over the \(C\) channels by calculating

\[:=_{,c}_{,c}_{,,c}_{c=1,,C}_{}^{d d C},\] (2)

resulting in a tensor of \(C\) SPD matrices.

We replace the softmax function for traditional attention with a custom softmax function \(}\) to preserve positive definiteness, i.e.,

\[}:_{}^{d d}_{ }^{d d}}():= ()}[]()}():=]_{d}},\] (3)

where \([]\) denotes the elementwise application of the exponential function, \(_{d}\) is a vector of length \(d\) with all entries being \(1\), and \(\) transforms a vector of length \(d\) into a \(d d\) diagonal matrix. The quotient and root are also taken elementwise.

Using the definition of the bilinear parallel tensor product in 2,we obtain the attention matrix \(\) by applying \(}\) channelwise, i.e.,

\[:=}(()_{,,c}) _{c=1,,C}_{}^{d d C},\]

and we obtain the output of the bilinear layer as

\[=_{}^{d d C}.\]

The elementwise exponential function preserves positive definiteness because SPD matrices are closed under addition and the Hadamard product , and \([]=_{n=0}^{}[]^{n}\), with elementwise exponentiation \([]^{n}\). Theorems regarding the eigenvalue regularization and stability properties of our modified softmax function \(}\) can be found in Appendix A.

Log-Eig layer and output softmax.Data representation transitions from the SPD space to Euclidean space through the Log-Eig layer, as proposed by . This transformation, given an input matrix \(=^{T}\) via eigendecomposition can be expressed as

\[l:_{}^{d d}^{d d}, l(): =():=()^{T}.\]

The final layer, equipped with a softmax activation function, consists of \(C=3\) output units for generating the probabilities.

Interpretation.In standard attention, the \((i,j)\)-th entry of the attention matrix \(\) directly represents a scalar influence of element \(i\) on element \(j\). In our bilinear attention framework, however, interdependencies emerge. Specifically, for an output pair \((i,j)\), the corresponding output value is given by the bilinear form \(_{k,l}A_{i,k}\ S_{k,l}\ A_{l,j}\). Hence, the influence on the \((i,j)\)-th entry depends not only on \(A_{i,j}\) but on the entire \(i\)-th row and \(j\)-th column of \(\). This produces a cross-shaped attention pattern within \(\) and allows columns of \(}\) to attend to each other, capturing more complex interactions. The learned SPD representations can be interpreted as end-to-end learned kernel matrices encoding different kinds of dependencies across multiple channels. A detailed discussion of the intuition behind our bilinear attention mechanism is provided in Appendix I.

Implementation details.Additional details regarding the activation function, residual connections and normalization, the use of multiple heads in attention layers and the initialization of the \(C C\) weights in the BAM layers are provided in Appendices B.2, B.3, B.4, B.5, respectively.

CPDAG estimation from the graph skeleton and the set of moralized edges.To derive the CPDAG from the graph skeleton and identified immoralities, we train a second neural network to infer v-structures from two parent nodes together with potential common child nodes that have edges to both parents, as well as other neighbor nodes related to these parents. We iterate over all inferred immoralities, treating the two nodes involved in each immorality as parent nodes. By applying distinct layers to the data corresponding to the parent nodes, potential common children, and neighbors, we can break the symmetry among nodes and enable role-specific learning, thus facilitating edge directionality inference. The resulting CPDAG is further refined using Meek rules . Details about the CPDAG estimation step can be found in Appendix D.

Related Work

Causal discovery primarily relies on unsupervised learning methods, including constraint-based approaches that infer conditional independencies [61; 16] and score-based methods that optimize a score function under acyclicity constraints [13; 71]. Some studies have estimated relations from multi-sequence alignment (MSA) data matrices, such as Rao et al.  using axial attention  and Li et al.  employing a convolutional neural network on precision matrices. However, these models are designed for the specific conditions of the MSA prediction task and its fixed data dimension of \(20\) naturally-occurring residue types.

Our work closely aligns with Ke et al.  and Lorch et al. , which also use attention mechanisms between samples and attributes to estimate adjacency matrices. The methodologies differ in their strategies for deriving an adjacency matrix from observational data representations.  employ an autoregressive transformer approach, whereas  utilize the dot product of embedding vectors derived from max-pooling across the sample axis of the observational data matrices. We demonstrate that deriving output adjacency matrices can be efficiently achieved through covariance matrices, allowing for direct processing of dependency information and enhanced learning efficiency through geometric learning on the SPD manifold.

Unlike , our model can be trained across different shapes of sample and attribute numbers, eliminating the need for re-training on different datasets. While 's model can evaluate data with varying dimensions, their implementation has practical limitations in training across variable sample sizes, and it is restricted to handling a limited range of dimensions for training. Our model addresses identifiability challenges by first estimating an undirected graph, then proceeding to CPDAG estimation by testing immoralities and using Meek's rules. In contrast,  and  deduce a DAG using both observational and interventional data such that these models dependent on the availability of interventional data or making random guesses for edges that cannot be directly inferred.

Closely related to our approach, Li et al.  used permutation-invariant models for supervised causal inference, but their method directly computes correlation matrices without learning a high-dimensional representation of the observational data, which may limit its efficacy when covariance matrices are not sufficient. Despite preserving invariances, the small number of \(5\) free parameters per layer could restrict its representational power.

[39; 40] proposed a supervised learning framework using kernel mean embeddings.  address identifiability with independence tests and cascade classifiers trained on vicinal graphs specific to observational data. Our method adopts a more general approach, training a network that does not require re-training for new evaluation samples. 's work on immoralities aligns with our second CPDAG estimation step.

## 4 Experiments

Training data hyperparameter settings.The hyperparameters for sampling training data, which are defined in section 2.2, are set to \(=10\), \(=100\), \(=1\), \(=(,5)\), \(=50\), and \(=1000\). This configuration allows for denser graphs than those explored in other studies [15; 31; 71].

Baseline algorithms.We evaluate BAM's performance against other algorithms: PC , PCBISIC , root and rcit , ccdr , GES , GIES , LiNGAM , MMPC , CAM , SAM , all implemented in the causal discovery toolbox , GLASSO , as implemented by , along with DAG-GNN . Default hyperparameters as in  are used. For GLASSO, cross-validation for the sparsity parameter is performed as proposed by . As a supervised causal discovery baseline, we use the AVICI model , both a pre-trained version (SCM-v0) and a version trained from scratch on the same Chebyshev data as our method, using default hyperparameters and data dimensions \(d\{2,5,10,20,30,40,60,80,100\}\). For the number of samples \(M\), which can only be chosen as a single hyperparameter value for AVICI  due to memory allocation constraints, we selected \(M\) = 150. GLASSO and MMPC are excluded from CPDAG estimation as they focus on undirected graphs. For algorithms estimating directed edges, the skeleton is computed for undirected prediction. We also compare SHD and accuracy results against a naive zero graph without edges.

Performance indicators.We assess the performance of both undirected and CPDAG graph estimations using the Area Under the Precision-Recall Curve (AUC), which is well-suited for imbalanced binary classification tasks like sparse graph detection . For CPDAG estimations, we also employ the Structural Hamming Distance (SHD), a standard metric in structure learning . In the undirected graph estimation task, SHD is equivalent to accuracy, defined as the percentage of correctly inferred edges, which we also report. Additional Structural Intervention Distance (SID)  results for CPDAG estimation are provided in Appendix G.5.

Graph estimation results.Figure 4 showcases BAM's efficacy in undirected graph prediction. Trained on synthetic Chebyshev polynomial data, BAM consistently outperforms other methods

Figure 4: Undirected graph estimation results arranged from worst (left) to best (right) performance. (A, B) AUC values for different dependencies, with (A) \(d=50\), \(M=200\), and (B) \(d=100\), \(M=50\). (C) Accuracy for the same dependencies as in (B). (D, E) AUC vs. \(M\) for \(d=100\) with (D) Chebyshev and (E) cosine dependencies. (F) SHD vs. \(M\) for \(d=100\) with Chebyshev dependency.

Figure 5: CPDAG estimation results arranged from worst (left) to best (right) performance. (A-C) SHD for different dependencies, with (A) \(d=20\), \(M=200\), (B) \(d=50\), \(M=200\), and (C) \(d=100\), \(M=500\). (D) AUC vs. \(M\) for \(d=100\) with Chebyshev dependency. (E, F) SHD vs. \(M\) for \(d=100\) with (E) Chebyshev and (F) sine dependencies.

across various dependency relations, dimensions, and sample sizes (Figure 4 (A - C)). It excels in capturing intricate non-monotonic dependencies (Figure 4 (A, B, E) and demonstrates superior performance over Avici on the training set (Figure 4 (D, F)).

Figure 5 displays the results for CPDAG estimation tasks. In high-dimensional scenarios (\(d=100\), \(M=50\)), no algorithm surpassed the baseline of a zero-graph in SHD, as shown in Appendix G.4. Therefore, our SHD analysis focuses on the low-dimensional setting shown in (A) and two moderate-dimensional (\(M>d\)) settings in (B) and in in (C) across various dependencies. Across these scenarios, the two-step method of our algorithm remains competitive, though its advantage is less pronounced than in the task of undirected graph estimation. Panels (D) and (E) depict the AUC and SHD across varying \(M\) values for the training datasets of BAM and the re-trained Avici, illustrating BAM's competitive performance in learning CPDAG-structure. Specifically, panel (E) presents SHD values for sine dependency across different \(M\) values for \(d=100\), further demonstrating the robustness of BAM in non-linear settings. Additional evaluation results for more complex multivariate dependencies using random Fourier features and MLPs are provided in Appendix G.3 and Figure 9, respectively. Additional SID results for CPDAG estimation in Appendix G.5 demonstrate that our method achieves the highest performance for the best DAG of the estimated equivalence class. Compared to competing algorithms, it infers fewer edge orientations.

Graph distribution robustness.To demonstrate that our architecture generalizes beyond the Erdos-Renyi graphs it was trained on, we evaluate its performance on graphs generated using various random graph models. Our results, shown in Figure 6, demonstrate consistent performance across all graph distributions, suggesting that our method is robust to variations in underlying graph structure. Details about the configuration of the distributions can be found in Appendix G.6.

Ablation study.Table 1 presents our ablation study results, underscoring the crucial role of the bilinear layer, whose removal significantly increases loss metrics. Omitting the LogEig layer leads to a substantial deterioration in loss, indicating that direct predictions from the SPD-space are problematic. Notably, bilinear data processing alone yields relatively good results, possibly due to the embedding layer's ability to

   model & loss \(\) & \(\)Param \\  FULL & \(0.173 0.007\) & \\ \(-\)BAM & \(0.202 0.005\) & 80 K \\ \(-\)BAM \(-\)LogEig & \(0.271 0.012\) & 100 K \\ \(-\)obs. att. & \(0.189 0.006\) & 120 K \\ \(-\)obs. att. \(-\)Dense & \(0.205 0.006\) & 160 K \\   

Table 1: Ablation study results featuring loss values. Data generated under Chebyshev dependencies. ”\(-\)” indicates the removal of a corresponding layer. \(\)Param quantifies the difference in the number of parameters to the full model.

Figure 6: Heatmaps of mean AUC values with standard deviations across graph distributions and dependency functions for expected degrees 2, 3, and 4 obtained from 10 independent simulation runs. Graph distributions include Erdős-Renyi (ER), Watts-Strogatz (WS) , Exponential Random Graph Model (ERGM) , configuration models with both homogeneous (Conf) and heterogeneous (Conf2) degree distributions , and geometric random graphs (Geom) .

decode non-linearities in the data, which are lost when solely relying on covariance matrices. Further details are provided in Appendix F.2.

Effectiveness of novel two-step approach for CPDAG estimation.To validate our approach of identifying immoralities through the first network step, we ran additional experiments where we maintained the undirected edge estimation network but modified the second step. Specifically, we first obtained an undirected graph as the union of skeleton and moralized edges, then trained another neural network with identical hyperparameters to test each possible immorality in this graph, not just those identified by our first network. Table 2 shows that our selective testing strategy consistently outperforms this exhaustive approach across different settings, particularly for Chebyshev dependencies used in training. This suggests that conditional dependencies are more accurately identified when leveraging information from the entire graph structure, rather than examining estimated local Markov blankets independently.

Efficiency.Training our neural network takes approximately 6 hours on an A-100 GPU with 81,920 MiB of graphical memory, while inference typically requires less than a few seconds and can be run on a normal computer. The overall memory complexity of our proposed method is \(O(CMd+Cd^{2}+Md^{2}+M^{2}d+C^{2})\). The runtime complexity of our approach is \(O(C^{2}Md+CMd^{2}+CM^{2}d+C^{2}d^{2}+Cd^{3})\). We present a detailed breakdown of the single components in Appendix H. Empirical evaluations show that our model achieves significantly lower computational times compared to most unsupervised approaches, as shown in Appendix G.7.

## 5 Conclusion

In this study, we introduced a novel neural network model for supervised graph structure learning that addresses identifiability issues in observational data through a two-step approach. Our approach extends the classical GLASSO paradigm to the neural network era by incorporating a novel bilinear attention mechanism operating in both Euclidean and SPD spaces, effectively learning shape-invariant transformations directly from data rather than relying on pre-derived mathematical operations. The trained model effectively detects causal relationships and generalizes well across different functional forms of nonlinear dependencies, while requiring only a single forward pass for inference. Comprehensive empirical evaluations demonstrate that our approach consistently outperforms state-of-the-art methods across diverse scenarios. Future directions include extending our approach to handle more complex settings and investigating its performance under various noise and confounding conditions, as well as scaling through local attention mechanisms. Limitations and future directions are further discussed in Appendix J.

  
**Dependency Function** & **Model** & \(d=10\) & \(d=20\) & \(d=50\) & \(d=100\) \\   & BAM & \(0.79 0.09\) & \(0.85 0.05\) & \(0.78 0.05\) & \(0.76 0.03\) \\  & AM & \(0.73 0.03\) & \(0.69 0.02\) & \(0.71 0.02\) & \(0.70 0.01\) \\   & BAM & \(0.71 0.06\) & \(0.72 0.02\) & \(0.69 0.06\) & \(0.70 0.02\) \\  & AM & \(0.67 0.03\) & \(0.63 0.06\) & \(0.63 0.04\) & \(0.68 0.03\) \\   & BAM & \(0.69 0.16\) & \(0.68 0.09\) & \(0.62 0.07\) & \(0.66 0.04\) \\  & AM & \(0.63 0.08\) & \(0.62 0.07\) & \(0.60 0.06\) & \(0.64 0.04\) \\   & BAM & \(0.71 0.04\) & \(0.74 0.05\) & \(0.65 0.11\) & \(0.59 0.04\) \\  & AM & \(0.67 0.07\) & \(0.64 0.05\) & \(0.62 0.07\) & \(0.59 0.05\) \\  \)} & BAM & \(0.77 0.05\) & \(0.77 0.09\) & \(0.71 0.08\) & \(0.63 0.11\) \\  & AM & \(0.68 0.06\) & \(0.64 0.05\) & \(0.64 0.05\) & \(0.61 0.05\) \\  \)} & BAM & \(0.56 0.03\) & \(0.55 0.17\) & \(0.54 0.09\) & \(0.59 0.06\) \\  & AM & \(0.58 0.05\) & \(0.50 0.10\) & \(0.56 0.09\) & \(0.57 0.05\) \\   

Table 2: CPDAG estimation performance (AUC scores) with \(M=200\) samples. The baseline (‘All Moralizations’, AM) tests all possible moralizations, while BAM only tests those identified by the first network. Mean \(\) standard deviation over 10 runs.