# GTSinger: A Global Multi-Technique Singing Corpus

with Realistic Music Scores for All Singing Tasks

 Yu Zhang Changhao Pan Wenxiang Guo Ruiqi Li Zhiyuan Zhu Jialei Wang Wenhao Xu Jingyu Lu Zhiqing Hong Chuxin Wang LiChao Zhang Jinzheng He Ziyue Jiang Yuxin Chen Chen Yang Jiecheng Zhou Xinyu Cheng Zhou Zhao Zhejiang University

{yuzhang34,panch,guowx314,zhaozhou}@zju.edu.cn

Equal contributionCorresponding Author

###### Abstract

The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present **GTSinger**, a large **G**lobal, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at http://gtsinger.github.io. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger.

## 1 Introduction

Traditional singing tasks, typically singing voice synthesis (SVS) , aim to generate high-quality singing voices using lyrics and musical notations, attracting broad interest in the industry and academic communities. As deep learning technology advances, there is a growing demand for more controllable and personalized singing experiences. This burgeoning demand has catalyzed the emergence of various new singing tasks like technique-controllable SVS, technique recognition, style transfer, and speech-to-singing (STS) conversion . These tasks have been progressively developed and applied in real life, like short videos and professional composition .

Despite the significant progress made in multiple singing tasks, the scarcity of publicly available high-quality and multi-task singing datasets has become a major bottleneck in their development due to the high cost of recording songs and manual annotations. The primary limitations of existing open-source singing datasets are as follows: 1) The **low quality** may lead to singing modelsproducing off-pitch, unpleasant, or noisy results. 2) A limited variety in **languages** and **singers** restricts personalized singing models to learn diverse timbres and styles. 3) The absence of the controlled comparison and annotations for multiple **singing techniques** (like falsetto) , constrains the technique modeling and control for singing models. 4) The lack of **realistic music scores** hinders human composers from using singing models in real-world musical composition. 5) Poor **task suitability** forces multiple emerging singing tasks to customize new datasets with high cost.

To address these challenges, we introduce **GTSinger**, a large **G**lobal, multi-**T**echnique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks. Our dataset contains 80.59 hours of high-quality singing voices without accompaniment, delivered by 20 professional singers covering nine widely spoken languages, including **Chinese, English, Japanese, Korean, Russian, Spanish, French, German, and Italian**. Moreover, GTSinger integrates phoneme-level annotations for six commonly used singing techniques, namely **mixed voice, falsetto, breathy, pharyngeal, vibrato, and glissando**. As shown in Figure 1, each song includes a control group for the natural singing voice and a technique group that intensively applies specific techniques under the same lyrics. Additionally, GTSinger furnishes realistic music scores for real-world musical composition. We also incorporate manual phoneme-to-audio alignments, global style labels (including singing method, emotion, range, and pace), and 16.16 hours of paired speech for various singing tasks. Overall, GTSinger boasts several advantages for multiple singing tasks over existing singing datasets:

* **80.59 hours of singing voices** in GTSinger are recorded in professional studios by skilled singers, ensuring **high quality and clarity**, forming the largest recorded singing dataset.
* Contributed by **20 singers** across **nine widely spoken languages** and all four vocal ranges, GTSinger enables zero-shot SVS and style transfer models to learn diverse timbres and styles.
* GTSinger provides **controlled comparison** and **phoneme-level annotations** of **six singing techniques** for songs, thereby facilitating singing technique modeling, recognition, and control.
* Unlike fine-grained music scores, GTSinger features **realistic music scores** with regular note duration, assisting singing models in learning and adapting to real-world musical composition.
* The dataset includes **manual phoneme-to-audio alignments**, **global style labels**, and **16.16 hours of paired speech**, ensuring comprehensive annotations and broad task suitability.

The rest of the paper is organized as follows. In Section 2, we briefly review and compare with current singing datasets. In Section 3, we provide the construction details and data statistics of GTSinger. In Section 4, to demonstrate the use and validate the quality of GTSinger, we conduct extensive experiments and establish benchmarks for four different singing tasks, including **technique-controllable singing voice synthesis, technique recognition, style transfer, and STS conversion**, employing recently published state-of-the-art methods for each task. In Section 5, we make the conclusion and discuss some potential risks along with the limitations of GTSinger.

Figure 1: The composition of each song in GTSinger. There are 1,366 songs recorded by 20 singers using six singing techniques across nine languages. Each song contains a technique group and a control group for the controlled comparison, along with a paired speech for STS tasks. Alignments, style labels, technique annotations, and realistic music scores are manually created for each group.

Related Work

The advancement of deep learning has enabled singing models, like singing voice synthesis (SVS) models, to achieve remarkably high-quality vocal results [11; 29; 19; 27; 15; 7]. Fueled by the growing demand for controllable and personalized singing experiences, diverse new singing tasks have emerged, like technique-controllable SVS, technique recognition, style transfer, and speech-to-singing (STS) conversion [9; 21; 13; 16; 28]. Unlike traditional datasets [19; 6; 22] designed for a singular task, a high-quality and multi-task singing dataset has higher demands. A high-quality dataset not only requires recordings by professional singers with manual alignments but also needs to include multiple singers and languages to broaden timbres and styles. Furthermore, a multi-task dataset also needs to feature controlled comparisons and phoneme-level annotations of singing techniques for technique modeling, realistic music scores for real-world musical composition, global style labels for global control, and paired speech for STS tasks. These requirements significantly elevate the recording and annotation costs, explaining the scarcity of high-quality and multi-task datasets.

As delineated in Table 1, several datasets endeavor to mitigate specific challenges to cater to designated tasks. VocalSet  provides the controlled comparison of various singing techniques, albeit constrained by its reliance solely on a range of vowels rather than linguistic content in singing voices. CSD  features recordings in both English and Korean, yet a limited number of vocalists constrains its diversity. KVT  annotates some types of global style labels in K-pop songs but uses existing songs and does not separate vocals from accompaniments. PopBuTFy  provides singing voices in both English and Chinese, but without annotations. OpenSinger  encompasses a substantial volume of vocal recordings across numerous singers, yet it does not contain any annotation. NHSS  introduces paired speech for STS tasks but falls short in providing manual phoneme-level alignments and other annotations. Tohoku Kiritan  provides manual alignments but is limited by its small scale. Opencpop  and M4Singer  mark significant advancements with their manual alignments and music scores. However, they only provide fine-grained music scores, which disrupt the regularity of note duration and thus, hinder the application to real-world musical composition. Moreover, they lack other annotations and paired speech for more tasks. In this paper, we construct a large multi-lingual, multi-singer, free-to-use, high-quality singing corpus with controlled comparison and phoneme-level annotations of multiple techniques, along with manual phoneme-to-audio alignments, realistic music scores, global style labels, and paired speech. We seek to comprehensively address the limitations in previous singing datasets and cater to all current singing tasks.

## 3 Dataset Description

In this section, we formally introduce GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, which aims to support all current singing tasks and can be used under license CC BY-NC-SA 4.0. Figure 2 depicts the pipeline of the creation of GTSinger, with detailed explanations of each step provided in the following subsections. Then, we present the necessary dataset statistics to enhance the understanding of our GTSinger. At last, we also provide the instructions to use our dataset and codes.

    &  &  &  &  &  \\  & & & Singing & Speech & Align & RMS & Tech & Style & **Comparison** \\  VocalSet  & 1 & 20 & 10.1 & 0 & ✗ & ✗ & ✗ & ✗ & ✓ \\ CSD  & 2 & 1 & 4.86 & 0 & ✗ & ✗ & ✗ & ✗ & ✗ \\ KVT  & 1 & 114 & 18.85 & 0 & ✗ & ✗ & ✗ & ✓ & ✗ \\ PopBuTFy  & 2 & 34 & 50.8 & 0 & ✗ & ✗ & ✗ & ✗ & ✗ \\ OpenSinger  & 1 & 66 & 50 & 0 & ✗ & ✗ & ✗ & ✗ & ✗ \\ NHSS  & 1 & 10 & 4.75 & 2.25 & ✗ & ✗ & ✗ & ✗ & ✗ \\ Tohoku Kiritan  & 1 & 1 & 1 & 0 & ✓ & ✗ & ✗ & ✗ \\ OpenCpop  & 1 & 1 & 5.25 & 0 & ✓ & ✗ & ✗ & ✗ & ✗ \\ M4Singer  & 1 & 20 & 29.77 & 0 & ✓ & ✗ & ✗ & ✗ & ✗ \\ 
**GTSinger (Ours)** & **9** & **20** & **80.59** & **16.16** & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: The information table of existing open-source singing datasets. Align and RMS mean manual phoneme-to-audio alignment and realistic music scores. Style denotes global style labels.

### Songs and Singers

To construct GTSinger, we first select nine widely spoken languages: Chinese, English, Japanese, Korean, Russian, Spanish, French, German, and Italian. Then, we also choose six commonly used singing techniques: mixed voice, falsetto, breathy, pharyngeal, vibrato, and glissando. After rigorous auditions, we select 20 professional singers, covering all four vocal ranges (alto, soprano, tenor, bass), and each singer is proficient in all six techniques and one of the widely spoken languages. Before recording, all singers agree to make their vocal performances open-source for academic research. We carefully select songs based on the representativeness of each language, the vocal range of each singer, and the suitability of singing each technique densely. As shown in Table 2, all singers are listed and anonymized by their languages along with vocal ranges.

### Recording

Singers perform a multitude of songs, each selected to highlight a specific singing technique (like falsetto). For each song, they maintain a consistent rhythm, lyrics, and key, recording twice: once

    &  &  &  \\  & & Singing & Speech &  Mixed Voice \\ and Falsetto \\  &  Breathy \\  &  Pharyngeal \\  &  Vibrato \\  & 
 Glissando \\  \\   & ZH-Tenor-1 & 8.45 & 1.82 & 3.6 & 1.26 & 1.18 & 1.18 & 1.23 \\  & ZH-Alto-1 & 8.14 & 1.49 & 3.7 & 1.13 & 1.06 & 1.13 & 1.12 \\   & EN-Tenor-1 & 4.76 & 0.87 & 2.06 & 0.69 & 0.65 & 0.7 & 0.66 \\  & EN-Alto-1 & 3.47 & 0.67 & 1.6 & 0.52 & 0.51 & 0.28 & 0.56 \\  & EN-Alto-2 & 4.9 & 1.04 & 2.05 & 0.74 & 0.67 & 0.73 & 0.71 \\   & JA-Tenor-1 & 2.13 & 0.29 & 1.01 & 0.33 & 0.34 & 0.15 & 0.3 \\  & JA-Soprano-1 & 4.32 & 0.87 & 2.24 & 0.56 & 0.41 & 0.53 & 0.58 \\   & KO-Tenor-1 & 4.61 & 1.32 & 1.19 & 0.87 & 0.88 & 0.83 & 0.84 \\  & KO-Soprano-1 & 0.95 & 0.24 & 0.19 & 0.16 & 0.2 & 0.21 & 0.19 \\  & KO-Soprano-2 & 2.72 & 0.61 & 1.12 & 0.37 & 0.42 & 0.42 & 0.39 \\   & RU-Alto-1 & 4.32 & 0.76 & 1.81 & 0.63 & 0.55 & 0.7 & 0.63 \\   & ES-Bass-1 & 4.45 & 0.9 & 2.01 & 0.61 & 0.61 & 0.61 & 0.61 \\  & ES-Soprano-1 & 3.48 & 0.82 & 1.4 & 0.59 & 0.4 & 0.53 & 0.56 \\   & FR-Tenor-1 & 4.58 & 0.58 & 1.27 & 0.9 & 0.84 & 0.66 & 0.91 \\  & FR-Soprano-1 & 3.96 & 0.59 & 1.75 & 0.58 & 0.58 & 0.57 & 0.48 \\   & DE-Tenor-1 & 4.54 & 0.9 & 2.19 & 0.56 & 0.59 & 0.59 & 0.61 \\  & DE-Soprano-1 & 4.54 & 0.82 & 1.9 & 0.64 & 0.63 & 0.67 & 0.7 \\   & IT-Bass-1 & 3.21 & 0.82 & 0.86 & 0.76 & 0.17 & 0.68 & 0.74 \\  & IT-Bass-2 & 1.61 & 0.4 & 0.32 & 0.32 & 0.3 & 0.33 & 0.34 \\  & IT-Soprano-1 & 1.45 & 0.35 & 0.98 & 0.11 & 0.1 & 0.05 & 0.21 \\   & **All** & **80.59** & **16.16** & **33.25** & **12.33** & **11.09** & **11.55** & **12.37** \\   

Table 2: The information table of languages, singers, techniques, and duration. Singing hours for technique ID count time for singing voices in both control groups and technique groups.

Figure 2: The pipeline of data collection of GTSinger. Human double-checks exist in each process.

densely applying the specific technique (technique group) and once for the natural singing voice without the specific technique(control group). We especially manage falsetto and mixed voice techniques due to their correlations. They form a distinct group, recording a natural singing voice (control group), and two technique groups, for both falsetto and mixed voice. Furthermore, each song includes an additional spoken lyric sentence recorded by the same singer, providing paired speech for STS tasks. All recordings are carried out in a professional studio, with singers listening to the song's accompaniment through headphones, ensuring clean vocal tracks devoid of accompaniment yet preserving rhythm and timing. Each audio is recorded at a 48kHz sampling rate with 24 bits in WAV format, ensuring high-quality data for further statistics and research. Table 2 presents the duration of 1,366 final recorded songs. For more details, please refer to Appendix A.1.

### Annotation

**Alignment:** We initially use the Montreal Forced Aligner (MFA)  for a coarse alignment of the original lyrics and audio and store the results in TextGrid format. Chinese phonemes are extracted using pypinyin, English phonemes follow the ARPA standard, Italian phonemes follow the Epitran standard, and others follow the MFA standard. These are the most effective and suitable phoneme standards for these languages. Next, annotators with a musical background use Praat  to correct the rough annotation results, focusing on the following areas: (1) Boundary correction: Annotators correct the boundaries of words and phonemes by listening to the audio and observing the mel-spectrogram, which forms the bulk of this step. (2) Word and phoneme correction: In cases of missing or incorrect lyrics, annotators are required to correct the words and corresponding phonemes based on their auditory perception. This is because singers may mispronounce words, or there may be homophones in Chinese that cause phoneme errors. (3) Unvoiced labeling: The unvoiced region, including breathing and silent sections, is marked by annotators who identify the boundaries respectively. In this step, we perform alignment for both the singing voice and paired speech.

**Technique and Style Annotation:** Following the alignment process, we instruct our annotators to perform phoneme-level annotations of six singing techniques on the TextGrid, including mixed voice, falsetto, breathy, pharyngeal, vibrato, and glissando. Annotators continue to use Praat  for annotations based on their auditory perception, indicating the presence or absence of each technique for every phoneme. Notably, in technique groups, singing voices employ densely specific techniques but not exclusively, as other techniques may also be used. In control groups, specific techniques are excluded but other techniques can be present as singers are asked to sing naturally. Next, annotators also label the singing method (pop and bed canto), emotion (happy and sad), pace (slow, moderate, and fast), and range (low, medium, and high) as global style labels for each group.

**Realistic Music Score Composing:** The difference in F0, fine-grained music scores, and realistic music scores are depicted in Figure 3. To compose realistic music scores, we initially employ RMVPE  to extract F0 for each singing voice. Then, we use ROSVOT  to derive the MIDI form of the music scores. The MIDI is obtained by referring to the F0 curve for determining the note pitch and duration. Subsequently, we engage music experts to listen to the recorded songs, refer to original accompaniments, and carry out the following steps: 1) Determine the actual tempo, cleft, and key. 2) Adjust the music scores to match the true note pitch. 3) Modify the note duration following regular realistic music score rules. 4) Annotate the note type to be rest, lyric, or slur. The outcome is realistic music scores in the muxicxmI format. More annotation details can be found in Appendix A.2.

Figure 3: The comparison between F0, fine-grained music scores, and realistic music scores. Score pitches are converted to frequencies and are very different from F0. Fine-grained music scores disrupt the regularity of note duration, resulting in fragmented notes that are unsuitable for composing.

### Post-Processing

**Data Check:** For each language with fully annotated data, we employ an additional music expert proficient in that language to randomly inspect 25% of the annotations. Their primary tasks include: (1) Checking alignment, including word and phoneme boundaries, incorrect characters, polyphonic phonemes in Chinese data, and annotations of unvoiced sections. (2) Examining technique and style annotations, focusing on annotations of techniques outside the specific group. (3) Reviewing realistic music scores, paying attention to key, tempo, and clef, and correcting note pitch and duration.

**Segmentation:** After completing the data annotation and inspection, we segment the audio into smaller fragments to facilitate training for singing tasks. For the same song, the control group, technique group, and paired speech are synchronously segmented into sentence-level segments, with their alignments, annotations, and scores correspondingly segmented. By leveraging the manual alignment results, we set a threshold for the unvoiced region and established maximum and minimum lengths for the voiced region as the conditions for performing the segmentation process. As shown in Figure 4 (a), we ensure more than 95% sentences are between 5 and 20 seconds in duration. Finally, we get 29,261 singing utterances and 12,373 speech utterances.

### Statistics:

To illustrate the diverse range of recorded singing voices, Figure 4 (b) presents the distribution of six singing techniques. The prevalence of mixed voice can be attributed to its widespread use in both pop and bel canto singing methods. Conversely, vibrato is the least frequent because it can only be used on sustained notes, resulting in a very low distribution density. Figure 4 (c) presents the distribution of beats per minute (BPM) across 3,145 groups (each song contains technique and control groups). The majority of groups fall within the 80 to 150 BPM range, encompassing the primary range for both pop and bel canto singing methods. Figure 4 (d) illustrates that note pitches of realistic music scores are primarily distributed between MIDI note numbers 50 (D3, 146.83 Hz) and 70 (B4, 494 Hz), covering four vocal ranges. This broad spectrum of languages, singers, techniques, BPM, and pitch suggests the potential of models trained on GTSinger to effectively handle a diverse range of singing styles. Refer to Appendix A for further detailed statistics.

### Instructions for Use

The dataset can be freely downloaded at https://huggingface.co/datasets/GTSinger/GTSinger and noncommercially used under license CC BY-NC-SA 4.0. Users can define additional tasks under this license. We also provide the code for processing data at https://github.com/GTSinger/GTSinger. For suggestions, please contact us via email. Regular updates will be provided on our GitHub repository.

Figure 4: The statistical distribution of segment duration, techniques, BPM, and pitch.

Benchmarks

In this section, to assess the quality and versatility of GTSinger, we conduct a comprehensive evaluation across four singing tasks: technique-controllable SVS, technique recognition, style transfer, and speech-to-singing (STS) conversion. We evaluate GTSinger with recently published state-of-the-art methods, utilizing four NVIDIA 2080Ti GPUs for all experimental protocols. Hifi-GAN  is employed as the vocoder for audio synthesis. We conduct the MOS (mean opinion score), FFE, MCD, and Cos for evaluation of these tasks on the test set. For more details about the evaluation, please refer to Appendix B.1 and B.2. For more detailed results, please refer to Appendix B.

### Technique-Controllable Singing Voice Synthesis

Previous SVS models are limited by datasets and cannot achieve technique-controllable SVS. Technique-controllable SVS can provide more controllable and personalized singing experiences for real-world applications, allowing even those who cannot sing to customize a variety of professional techniques. We randomly choose five songs from each singer, totaling 100 songs as the test set, with the remainder as the training set. We conduct experiments on the following systems: (1) GT: The ground truth singing voice; (2) GT (vocoder): The audio generated by the pre-trained HiFi-GAN; (3) DiffSinger : A popular SVS system based on the diffusion model; (4) RMSinger : An outstanding SVS system using the diffusion model to predict F0. (5) StyleSinger : the current state-of-the-art SVS system using the residual style encoder to model both global and detailed styles. We enhance both models with phoneme-level technique embedding to achieve technique control.

We employ both objective and subjective evaluation metrics for evaluation. For subjective evaluation, MOS-Q indicates the quality, naturalness, and clarity of the synthesized audio, while MOS-C reflects the expressiveness and accuracy of technique controllability. Both metrics are rated on a scale from 1 to 5 and reported with 95% confidence intervals. For objective evaluation, we use F0 Frame Error (FFE) to measure the accuracy of F0 and UV prediction, and Mean Cepstral Distortion (MCD) for audio quality measurement. We conduct both parallel and non-parallel experiments according to the target technique sequence. In the parallel experiments, we use the GT technique sequence as the target. In the non-parallel experiments, six techniques are randomly yet appropriately assigned to each target phoneme (zero, one or more techniques on each phoneme).

As shown in Table 3, we can observe the following: (1) Leveraging the diffusion decoder, DiffSinger achieves reasonable sound quality (MOS-Q). However, it struggles to model and control techniques effectively (MOS-C). Additionally, the high FFE and VDE indicate that the generated styles deviate significantly from the actual curve. (2) By using a diffusion model to model F0, RMSinger significantly improves its ability to handle techniques (MOS-C) and achieves better synthesis quality (MOS-Q). This highlights the impact of pitch modeling on technique representation. (3) StyleSinger integrates multi-level style information from the reference mel-spectrograms, and renders techniques more naturally and expressively, outperforming all other baseline models across all metrics. This demonstrates the complexity involved in modeling techniques across different singing styles. However, these results indicate that there is still a significant gap between the model's performance and GT (vocoder), highlighting ample room for improvement in the technique controllability of singing tasks. Future work can explore using more advanced generation models to incorporate realistic music scores and leverage phoneme-level technique information for better F0 and mel-spectrogram generation, as well as higher technique controllability. For more detailed and visualized results about technique-controllable SVS, please refer to Appendix B.3.

    &  &  \\  & FFE \(\) & MCD \(\) & MOS-Q \(\) & MOS-C \(\) & MOS-Q \(\) & MOS-C \(\) \\  GT & - & - & 4.54 \(\) 0.06 & - & - & - \\ GT (vocoder) & 0.05 & 1.33 & 4.21 \(\) 0.07 & 4.42 \(\) 0.03 & - & - \\  DiffSinger  & 0.29 & 3.58 & 3.81 \(\) 0.06 & 3.83 \(\) 0.07 & 3.77 \(\) 0.05 & 3.78 \(\) 0.07 \\ RMSinger  & 0.27 & 3.43 & 3.94 \(\) 0.07 & 3.95 \(\) 0.05 & 3.86 \(\) 0.06 & 3.89 \(\) 0.06 \\ StyleSinger  & 0.25 & 3.27 & 4.01 \(\) 0.09 & 4.15 \(\) 0.06 & 3.95 \(\) 0.08 & 4.10 \(\) 0.05 \\   

Table 3: Technique-controllable SVS performance in both parallel and non-parallel experiments. We use FFE, MCD, MOS-Q, and MOS-C for comparisons.

### Technique Recognition

Technique recognition aims to predict the techniques in unseen audio samples, facilitating the augmentation of existing singing datasets with technique annotations and aiding the real-world learning of singing techniques. We design a technique recognition model based on ROSVOT  and change the loss to the cross entropy loss of each technique label. The inputs of the technique recognition model include the mel-spectrogram, pitch, and phoneme boundaries, with the output being the predicted probabilities of six techniques in each phoneme. We conduct both overall and cross-lingual experiments to evaluate our model's performance and the annotation quality of GTSinger. We categorize the languages into two groups: Asian (Chinese, Japanese, and Korean) and European (Italian, Spanish, English, French, German, and Russian). In overall experiments, We reuse the rule in Section 4.1 to split training and test sets. In cross-lingual experiments, models are trained on one group of languages (like Asian) and tested on the other language group (like European) to assess their generalization capabilities. For evaluation, we provide F1 and Accuracy.

As shown in Table 4, our model demonstrates good F1 and Accuracy of all six techniques in both overall and cross-lingual experiments, highlighting the quality of our technique recognition model, as well as the merit of designing controlled comparison and phoneme-level annotation of six techniques in GTSinger. However, it is evident that the overall performance still surpasses that of cross-lingual cases. This indicates ample room for improvement in the technique recognition model, suggesting the potential to enhance generalization, thus handling out-of-domain technique recognition better. For more details about the model and results, please refer to Appendix B.4.

### Style Transfer

Style transfer aims to generate high-quality singing voices with the timbre and styles (like singing methods, rhythm, techniques, and pronunciation) of the reference audio. This technology can be applied in the dubbing of entertainment short videos, offering personalized experiences. We reuse the rule in Section 4.1 to split training and test sets. For baseline models, we reuse StyleSinger, as it is the first singing style transfer model, and enrich RMSsinger with additional singer and emotion embedding for conducting style transfer like previous works . For subjective evaluation, we use MOS-Q for synthesis quality and MOS-S for singer similarity in terms of timbre and styles. For objective evaluation, we employ FFE to measure pitch accuracy, MCD to assess synthesis quality, and Cos for evaluation of singer similarity. We conduct both parallel and cross-lingual style transfer experiments. For parallel experiments, we use another singing voice by the same singer as the reference audio. For cross-lingual experiments, we also split languages into Asian and European groups like Section 4.2. Then we randomly select reference audio from one language group (like Asian), transferring singing styles to target lyrics in the other language group (like European).

    &  &  \\  & & mixed voice & falsetto & breathy & pharyngeal & vibrato & glissando \\   & F1 & 0.78 & 0.96 & 0.99 & 0.85 & 0.70 & 0.70 \\  & Accuracy & 0.78 & 0.84 & 0.78 & 0.80 & 0.89 & 0.85 \\   & F1 & 0.72 & 0.94 & 0.96 & 0.84 & 0.66 & 0.64 \\  & Accuracy & 0.75 & 0.79 & 0.72 & 0.77 & 0.84 & 0.78 \\   

Table 4: F1 and Accuracy of each technique in overall and cross-lingual technique recognition.

    &  &  \\  & FFE \(\) & MCD \(\) & Cos \(\) & MOS-Q \(\) & MOS-S \(\) & MOS-Q \(\) & MOS-S \(\) \\  GT & - & - & - & 4.53 \(\) 0.03 & - & - & - \\ GT (vocoder) & 0.05 & 1.34 & 0.96 & 4.18 \(\) 0.04 & 4.26 \(\) 0.03 & - & - \\  RMSsinger & 0.31 & 3.47 & 0.88 & 3.70 \(\) 0.04 & 3.79 \(\) 0.06 & 3.66 \(\) 0.04 & 3.76 \(\) 0.08 \\ StyleSinger & 0.26 & 3.29 & 0.93 & 3.95 \(\) 0.06 & 4.01 \(\) 0.05 & 3.89 \(\) 0.07 & 3.92 \(\) 0.09 \\   

Table 5: Style Transfer performance in both parallel and cross-lingual experiments. We use FFE, MCD, Cos, MOS-Q, and MOS-C for comparisons.

As shown in Table 5, we can observe that StyleSinger achieves impressive results in both synthesized quality (MOS-Q) and singer similarity (MOS-S), which suggests that GTSinger's extensive style collection facilitates modeling and transfer, enabling the model to achieve high performance. Additionally, StyleSinger performs well in cross-lingual tasks, showcasing its ability to sing any music scores and lyrics, regardless of the singer's identity. However, there is still ample room for improvement in the cross-lingual style transfer performance. Future research can explore specialized models for handling singing style transfer tasks involving significant style differences. For more detailed and visualized results about style transfer, please refer to Appendix B.5.

### Speech-to-Singing Conversion

Speech-to-singing (STS) conversion aims to transform speech into the corresponding singing voice preserving the timbre and phoneme information. STS can be applied to automatic music production or personalized entertainment. We randomly select five songs (including paired speech) from each singer, totaling 100 pairs as the test set, and others as the training set. Besides GT and GT (vocoder), we use AlignSTS  as a baseline model, and design another model based on StyleSinger, which inputs paired speech as the reference audio to conduct STS conversion. StyleSinger uses realistic musical scores (RMS), which are more practical, while AlignSTS requires GT singing F0 as input. We reuse evaluation metrics in Section 4.3 for evaluating synthesized quality and singer similarity.

As shown in Table 6, we observe that StyleSinger outperforms AlignSTS in both synthesis quality (MOS-Q) and singer similarity (MOS-S). This also demonstrates the quality of our annotations of realistic music scores. Using realistic music scores and speech to synthesize expected singing voices allows for more controllable and personalized singing experiences in real-world applications. We can observe the potential for improvement in speech-to-singing performance, indicating that there is ample room for enhancement in pitch modeling based on realistic music scores. Future work can explore using realistic music scores for better pitch modeling, as well as designing specialized intermediate models to better convert styles between speech and singing styles. For more detailed and visualized results about the speech-to-singing conversion, please refer to Appendix B.6.

## 5 Conclusion and Discussion

In this paper, we propose a novel dataset GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, comprehensively addressing the limitations of existing singing datasets. Furthermore, we provide the construction process and statistical analysis for GTSinger. In addition, we have conducted extensive experiments and established four benchmarks, thereby contributing further to future singing research.

**Limitations and Future Directions:** (1) Our dataset currently lacks comprehensive coverage of widely spoken languages, like Arabic, and does not include several commonly used singing techniques, such as vocal fry. Future efforts will be directed towards expanding the diversity of the singing data. (2) Although our annotation process is performed by professionals with musical expertise, accurately segmenting phoneme durations within words and identifying subtle singing techniques remains challenging for human ears. Future models that better utilize word-level annotations may mitigate some of the errors introduced by manual labeling. (3) While our dataset addresses various singing tasks, extending its utility to the broader music field may require integration with vocal-to-accompaniment models like SingSong  to assist in generating music that includes vocals.

**Negative Societal Impact:** The presence of sensitive biometric data in our dataset inherently carries potential risks. Therefore, We first perform data desensitization and consider using techniques such as vocal watermarking to further protect personal privacy.

  
**Method** & FFE \(\) & MCD \(\) & Cos \(\) & MOS-Q \(\) & MOS-S \(\) \\  GT & - & - & - & 4.53 \(\) 0.03 & - \\ GT (vocoder) & 0.05 & 1.34 & 0.95 & 4.17 \(\) 0.05 & 4.20 \(\) 0.04 \\  AlignSTS & 0.35 & 3.52 & 0.85 & 3.68 \(\) 0.12 & 3.73 \(\) 0.09 \\ StyleSinger & 0.28 & 3.38 & 0.92 & 3.83 \(\) 0.09 & 3.88 \(\) 0.08 \\   

Table 6: Speech-to-singing performance in FFE, MCD, Cos, MOS-Q, and MOS-S metrics.