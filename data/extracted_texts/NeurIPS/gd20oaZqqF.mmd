# On Optimal Caching and Model Multiplexing for Large Model Inference

Banghua Zhu

Department of EECS

UC Berkeley

banghua@berkeley.edu

&Ying Sheng

Computer Science Department

Stanford University

Ying.Sheng@stanford.edu

&Lianmin Zheng

Department of EECS

UC Berkeley

lmzheng@berkeley.edu

&Clark Barrett

Computer Science Department

Stanford University

barrett@cs.stanford.edu

&Michael I. Jordan

Department of EECS

UC Berkeley

jordan@berkeley.edu

&Jiantao Jiao

Department of EECS

UC Berkeley

jiantao@berkeley.edu

###### Abstract

Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study two approaches for mitigating these challenges: employing a cache to store previous queries and learning a model multiplexer to choose from an ensemble of models for query processing.

Theoretically, we provide an optimal algorithm for jointly optimizing both approaches to reduce the inference cost in both offline and online tabular settings. By combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we achieve optimal rates in both offline and online settings. Empirically, simulations show that the combination of our caching and model multiplexing algorithms greatly improves over the baselines, with up to \(50\) improvement over the baseline when the ratio between the maximum cost and minimum cost is \(100\). Experiments on real datasets show a \(4.3\) improvement in FLOPs over the baseline when the ratio for FLOPs is \(10\), and a \(1.8\) improvement in latency when the ratio for average latency is \(1.85\).

## 1 Introduction

The recent emergence of Large Language Models (LLMs) and foundation models has significantly increased the capabilities of AI systems (Bubeck et al., 2023; Nori et al., 2023; Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching et al., 2023; Chowdhery et al., 2022; Wei et al., 2022; Google, 2023). This progress comes at a cost, however, of increased resource consumption and latency during both training and inference, presenting challenges not only in real-world deployment but also in terms of environmental impact and energy usage (Sharir et al., 2020; Patterson et al., 2021; Bommasani et al., 2022). For instance, LLM-based chatbots typically consist of large transformer-based networks with parameter counts ranging from one to several hundred billion (Zhou et al., 2023). Moreover, the auto-regressive nature of LLMs exacerbates the issue of latency and resource consumption because the model can only generate one token at a time. Thus, compared to traditional AI-powered services, language model inference costs are much higher and the latency is significantlylonger, making it nearly impossible to process each query using LLMs in high-throughput query systems such as search engines.

In this paper, we explore two simple yet effective strategies to mitigate this problem: (1) employing a caching system to store previous queries, and (2) developing a model multiplexer to choose the most appropriate model from a set of models for processing the queries. The general workflow of our proposed LLM-based inference system is shown in Figure 1: upon receiving a query or prompt, we initially check if it can be retrieved from the cache. If the query is not found in the cache, we employ the model multiplexer to determine which model should be used for processing it first, based on the estimated cost for both models.

The choice of cost function and models can vary based on the goal. One measure of cost, for example, could be floating point operations (FLOPs). Other alternatives could include the number of API calls as a measure of resource consumption, latency as a measure of time consumption, or a score provided by a user as a measure of user satisfaction. The cost could also be a weighted sum of multiple factors. For the models, a natural choice would be to have a small and a large model, where the small model costs less and is also less accurate, and the large model has a higher cost and also provides higher accuracy. Another alternative would be to have models with expertise in different areas, i.e., each model has high accuracy in its own area of expertise. We provide more discussion in Appendix A.

There is a long history of existing literature on caching algorithms, with prominent applications including computer architecture and web retrieval (Smith, 1982; Wang, 1999; Kumar and Singh, 2016). Existing caching algorithms deal with queries with different frequencies and cost, and must also provide guidelines for choosing the cache size. In addition to these well-known difficulties, the use of caching for LLMs raises new challenges, including:

* **The need for fuzzy search.** Since the prompt lies in a discrete space that is exponentially large with respect to the token size, it is impossible to match and save all distinct queries. Thus, to be at all useful, approximate matching and grouping is required when retrieving queries saved in the cache.
* **The randomness of the cost.** The cost for processing each query is a random variable that depends on the query and has a large variance due to the auto-regressive generation procedure and the difference in the length and quality of generated responses. When combined with the long-tailed distribution of the query frequency, the estimation of the cost requires a non-trivial algorithm design.
* **The effect of model multiplexing.** When the cache system is combined with the model multiplexer, the estimation of cost must change accordingly to take into consideration the different costs induced by various models.

For the fuzzy search problem, semantic search or vector-embedding-based ideas provide a systematic solution that includes embedding extraction and matching algorithms (Bast et al., 2016; Chang et al., 2020; Kamalloo et al., 2023). To simplify the problem, we assume that there exists some semantic search oracle that can group the prompts with the same semantic meaning and that the total cache size is limited by the number of queries, ignoring the difference in cache size between each individual query and response.

The remainder of this paper is organized as follows. In Section 2, we formally define the pipeline of caching and model multiplexing. In Section 3, we study the optimality of the Least Expected Cost (LEC) caching strategy, which estimates the frequency and cost of processing each query, and evicts the one with the least estimated expected cost when there is only one model to call. In section 4, we

Figure 1: A workflow for LLM-based inference with caching and model multiplexing.

consider the case when we have access to two models, and jointly design optimal caching and model multiplexer. In both sections, we start by assuming there are infinite samples and then analyze the offline and online learning cases where the cost and frequency need to be learned from data. The experimental results are presented in Section 5. We discuss the potential choices of cost, model, and output in the real world in Appendix A. We provide a brief discussion of the generalization to variable cache sizes in Appendix B and of the generalization to multi-model multiplexing in Appendix C.

### Related work

Cache replacement algorithmsTraditional cache replacement algorithms investigate optimal ways to cache queries with different frequencies, costs, and cache sizes. To address varying frequencies, a standard approach is to use a Least Frequently Used (LFU) or Least Recently Used (LRU) cache eviction strategy (Lee et al., 2001). These have been proven to be optimal for both adversarial and stochastic queries (Stallings and Paul, 2012; Bura et al., 2021). Caching has also been combined with machine learning advice and online learning analysis in the literature (Chang et al., 2018; Shuja et al., 2021; Jiang et al., 2019; He et al., 2017; Mukhopadhyay and Sinha, 2021; Faizal et al., 2023). When varying costs and varying frequencies exist simultaneously, Jin and Bestavros (2000); Arlitt et al. (2000) propose and study the Greedy Dual-Size with Frequency (GDSF) replacement algorithm, which takes both frequency and cost into consideration. Bahn (2005) proposes the Least Expected Cost (LEC) algorithm, which is similar to GDSF, except that it estimates frequency from data. Our work extends this idea by attempting to learn a model for both frequency and cost from data. Moreover we explore the statistical optimality of these algorithms in both offline and online settings. We also investigate combining caching algorithms with model multiplexing in order to boost performance.

Acceleration of LLM inferenceMuch effort has been devoted to reducing the cost and latency of LLMs during inference. For example, post-training quantization-based approaches aim to compress the model size by using lower-precision arithmetic without losing too much accuracy (Gholami et al., 2021; Frantar et al., 2023). Early-exit frameworks aim to utilize the output in the middle decoder blocks so that only a small fraction of decoder blocks are called when processing a query (Bakhtiarnia et al., 2022; Schuster et al., 2022). The Mixture of Experts approach designs a gating function that only assigns a small fraction of the network for each query (Fedus et al., 2022). Embedding recycling caches activations from an intermediate layer of a pre-trained model to accelerate the training and inference procedure (Du et al., 2020; Wei et al., 2022; Saad-Falcon et al., 2023). LLM cascade starts with the smallest model and continues to call larger models if the output is not acceptable (Chen et al., 2023). The big little transformer decoder framework uses a smaller model to generate a draft response and calls the large model to identify the unreliable tokens and perform correction (Kim et al., 2023). Similar ideas have been combined with speculative sampling to guarantee that the output remains the same in distribution as that of the large models (Chen et al., 2023; Leviathan et al., 2022).

## 2 Formulation

We formalize the workflow in Figure 1. Consider the set of (finite) prompts / queries \(^{d}\). In the \(t\)-th round, a query \(q_{t}\) is sampled from a fixed population distribution \(P()\). We maintain a small set of cache \(_{t}\) with \(|_{t}| L\). We say the query hits the cache if the query satisfies \(q_{t}_{t}\). When the query hits the cache, the incurred cost is zero. When the query does not hit the cache, we choose among the existing models to process the query.

In the processing stage, we first describe the setting of caching without model multiplexing, and extend it to the case of caching with model multiplexing.

### Caching without model multiplexing

In the case when we only have one model, let \(C_{l}(q)\) denote the random variable of the cost when processing the query with the model. Assume that \(C_{l}(q)\) is supported on \([B_{1},B_{2}]\) with \(B_{2}>B_{1}>0\) being the upper and lower bounds for the cost. Let \(c_{l}^{}(q)=[C_{l}(q)]\) be the expected true cost of processing the query \(q\). The cost for a given query \(q\) and cache \(\) can be written as:

\[(q,)=(q)[C_{l}(q )]=(q)c_{l}^{}(q).\]

By taking the expectation over the distribution \(q\), we have the expected cost as

\[()=_{q}P(q)(q)c_{l}^ {}(q).\]

In the offline learning setting, we collect an offline dataset and hope to learn a caching policy \(}\) such that \((})\) is minimized.

In the online setting, the query comes in a streaming fashion. At the beginning of each round, we receive a query \(q_{t}\). If the query misses the current cache \(_{t}\), we let the model process the query and receive a cost \(c_{t}_{C_{l}}\). Then we can choose to update the cache \(_{t}\) by adding the current query and response to the cache, and replacing one of the existing cached items if the cache \(_{t}\) is full. If the query hits the cache \(q_{t}_{t}\), then the cost for this round is set to zero with no more observations. In this case, we are interested in characterizing the average difference in the cost throughout the execution of the online learning process. This can be characterized by the regret:

\[_{}(T)=_{t=1}^{T}[(q_ {t},_{t})-(q_{t},^{})].\]

### Caching with model multiplexing

For the simplicity of the notation, we focus on the case of selecting from a small model and a large model,1 and discuss how it can be generalized to the case of selecting from multiple models in Appendix C. Let \(C_{s}(q)\) denote the random variable of the cost when processing the query with the small model, and \(C_{l}(q)\) denote the random variable of the cost when processing the query with the large model. We assume that both random variables are supported on \([B_{1},B_{2}]\). We observe \(i.i.d.\) draws of the random variables \(C_{s}(q)\) when executing the small model, and \(C_{l}(q)\) when executing the large model. Denote the expected cost as \(c_{s}^{}(q)=[C_{s}(q)]\) and \(c_{l}^{}(q)=[C_{l}(q)]\).

Let \(:\) be the (possibly random) model multiplexing policy that maps the query \(q\) to values in \(\), where \((q)=1\) represents that the query is always sent to the small model, and \((q)=0\) represents the query is always sent to the large model. The randomness in the policy \(\) is independent of the cost \(C_{s}(q),C_{l}(q)\). The total cost can be written as the following function of the query \(q\), cache \(\) and policy \(\):

\[(q,,) =(q)[C_{s}(q)(q)+C_{l} (q)(1-(q))]\] \[=(q)(c_{s}^{}(q)(q)+c_{l}^{ }(q)(1-(q))).\]

By taking the expectation over \(q\), we have the expected cost as

\[(,)=_{q}P(q)(q)(c _{s}^{}(q)(q)+c_{l}^{}(q)(1-(q))).\]

In the offline learning setting, we collect an offline dataset and hope to learn a caching policy \(}\) and a multiplexer \(\) such that \((},)\) is minimized. In the online setting, we get to update the cache in each round by adding the current query into the cache and evicting the ones in the cache if full. When the query \(q_{t}\) misses the cache in round \(t\), we will observe a sample from \(C_{s}(q_{t})\) if it is processed by the small model, or a sample from \(C_{l}(q_{t})\) if it is processed by the large model. There will be no observations of cost if \(q_{t}\) hits the cache. We aim at minimizing the regret:

\[_{}(T)=_{t=1}^{T}[(q_{t },_{t},_{t})-(q_{t},^{},^{ })].\]Optimal Caching without Model multiplexing

### Population setting

We start with the population setting where the probability distribution \(P\) and the cost \(c_{l}^{}\) are both known. In the case with only one model, the optimal caching strategy is the Least Expected Cost (LEC) or Greedy Dual Size with Frequency (GDSF) algorithm:

\[^{}=_{}=*{arg\,min}_{ :|| L}()=*{arg\, min}_{:|| L}_{q}P(q)(q )c_{l}^{}(q).\]

The traditional frequency-based caching strategy, including Least Recent Used (LRU) and Least Frequently Used (LFU), aims at caching the most frequent queries:

\[_{}=*{arg\,min}_{:||  L}_{q}P(q)(q).\]

We show in Appendix D that the ratio between the cost of LFU and LEC can be as high as \(}c_{l}^{}(q)}{_{q}c_{l}^{ }(q)}\) in the worst case, which shows that LFU can be highly suboptimal when the cost varies significantly.

### Finite sample setting: Offline learning

The previous section characterizes the optimal caching strategy in the population setting. We now consider the finite-sample offline learning setting, where we hope to produce a cache \(\) based on prior data such that the introduced cost is minimized. Denote \(_{N}=\{(q_{1},c_{1}),,(q_{N},c_{N})\}\), where \(q_{i}\) is sampled from the distribution \(P()\), and \(c_{i}\) is a sample from random variable \(C_{l}(q_{i})\). We consider estimating \(P,c_{l}^{}\) from oracles \(=(q_{1},,q_{N})\), \(_{l}(q)=(_{N})\). In practice, one may remove the last layer of the pre-trained language model and concatenate it with a linear head and fine-tune the model as the estimator. For theoretical analysis, we focus on the tabular case, where we set both \(\) and \(_{l}(q)\) to be the plug-in estimator:

\[(q) =^{N}(q_{i}=q)}{N},\] (1) \[_{l}(q) =^{N}(q_{i}=q)c_{i}}{ _{i=1}^{N}(q_{i}=q)},&_{i=1}^{N}(q_{i}=q )>0\\ B_{1},&_{i=1}^{N}(q_{i}=q)=0.\] (2)

In practice, the distribution of \(q\) may have a long tail. Although the estimation of \(P(q)\) is uniformly good for all \(q\), the estimation of \(c^{}(q)\) can be bad for the queries that are visited less. To select the maximum \(L\) elements from the imbalanced samples, we compensate the plug-in estimator by introducing pessimism (Rashidinejad et al., 2021; Jin et al., 2021)2. As we show in Lemma 1, the true frequency for any query \(q^{}\) is lower bounded by some constant that depends on \(B_{1},B_{2},||\). Thus the pessimism helps eliminate those less visited queries in the long tail of the distribution and encourages caching the queries in \(^{}\). The lower-confidence-bound based estimator is:

\[}=*{arg\,min}_{:|| L} _{q}(q)(q) (B_{1},(_{l}(q)-(B_{2}-B_{1})| /)}{2_{n=1}^{N}(q_{n}=q)}})).\]

We show how the cost for the caching from the empirical estimate differs from the optimal cost.

**Theorem 1**.: _Assume that \(N||(3L/)}{B_{1}}\) and taking \(=1/N\). We have_

\[[(})-(^{})]  C(B_{2}-B_{1})L||(N||)}{ NB_{1}}}.\]

The proof is deferred to Appendix E, where we prove a stronger high-probability bound rather than a bound in expectation. From the theorem, we know that the cost of the finite-sample caching policy converges to the cost of the optimal policy at a rate of \(1/\), which achieves the optimal dependence on \(N\). The insights from the tabular case also indicate that the cost needs to be estimated in a conservative fashion when considered for the cache replacement algorithm.

### Finite sample setting: Online learning

We summarize the caching algorithm pipeline in 1, which relies on the two estimation oracles, \(\) and \(\), which estimate both the frequency and cost of models from data.

```
1:Initialize the set of cache \(_{1}=\{\}\), past observations \(_{1}=\{\}\), \(_{l,0}(q)=B_{1}, q\).
2:For iteration \(t=1,2,T\)
3: Receive query \(q_{t}\).
4: Update the density estimation \(_{t}=(q_{1},,q_{t})\).
5:If\(q_{t}_{t}\):
6: Output the cached result, set \(_{l,t}=_{l,t-1}\), update the past observation \(_{t}=_{t-1}(q_{t},)\), and continue.
7: Use the large model to process the query, and observe a cost \(c_{t}_{C_{l}(q)}\).
8: Update the past observation \(_{t}=_{t-1}(q_{t},c_{t})\).
9: Update \(_{l,t}=(_{t})\).
10:If\(|_{t}|<L\):
11: Let \(_{t+1}\) be the union of \(_{t}\) and \(q_{t}\).
12:Elseif\(_{t}(q_{t})_{l,t}(q_{t})>_{q_{t}}_{t}(q) _{l,t}(q):\)
13:Replace the minimizer element of \(_{t}(q)_{l,t}(q)\) in the cache \(_{t}\) with \(q_{t}\) to get \(_{t+1}\). ```

**Algorithm 1** Caching in Online Learning

For theoretical analysis, we focus on the tabular case and define the oracles as follows:

\[_{t}(q) =^{t}(q_{i}=q)}{t},\] (3) \[_{l,t}(q) =B_{1},&_{i=1}^{t}(c_{i} ,q_{i}=q)=0,\\ (B_{1},^{t}(c_{i},q_{i}=q)c_{i}}{ _{i=1}^{t}(c_{i},q_{i}=q)}-(B_{2}-B_{1})|/)}{2_{i=1}^{t}(c_{i},q_{i} =q)}}),&\] (4)

For the estimation of density, we use plug-in estimator since there is no imbalance in the sampling process. For the estimation of the cost, we subtract the confidence bound to include pessimism. We have the following regret guarantee.

**Theorem 2**.: _When substituting the \(\) and \(\) with Equation (3) and (4) and set \(=1/T\), we have for some universal constant \(C\):_

\[_{}(T)-B_{1})B_{2}| |L^{2}(T||)}{B_{1}}.\]

_On the other hand, for any caching policy \(\{_{t}\}_{t=1}^{T}\), there exist some cases of \(P(q),c_{l}^{}(q)\) such that for some universal constant \(C^{}\),_

\[_{}(T) C^{}.\]

The proof is deferred to Appendix F. Different from the offline case, one interesting feature of the online case is the _partial observation phenomenon_: when the query hits the cache, it will not be processed by the model, and thus we cannot observe the sample from \(C_{l}(q)\) in this round. This is different from the traditional bandit literature where the selected arm is always observed in each round. Thus the partial observation thus requires new upper and lower bound analysis.

## 4 Optimal Caching and Model multiplexing

### Population setting

In the case when we have access to two models, we need to design a good caching and model multiplexing strategy jointly. We can compute the optimal caching and model multiplexing policy as\(^{},^{}=_{,}(,)\), which gives the following solution:

\[^{}(q) =(c_{s}^{}(q) c_{l}^{}(q)),\] \[^{} =*{arg\,min}_{:|| L}_{ q}P(q)(q)(c_{s}^{}(q),c_{l}^{ }(q)).\]

Such optimal strategies are straightforward: \(^{}\) always assigns the query to the model with a smaller cost, and \(^{}\) saves the \(L\) queries with the largest \(P(q)(c_{s}^{}(q),c_{l}^{}(q))\).

For the model multiplexing algorithm, we consider two baselines: (a) one always uses large model \(_{l}(q) 0\); (b) one always uses the small model \(_{s}(q) 0\). This is related to the LLM cascade idea in the concurrent work of Chen et al. (2023b). We provide more discussion in Appendix A, and present comparisons between baselines and \(^{}\) in Appendix D.

### Finite sample setting: Offline learning

We now consider the finite sample case. Let \(_{N}=\{(q_{1},c_{s,1},c_{l,1}),,(q_{N},c_{s,N},c_{l,N})\}\), where \(c_{s,n}\) is a sample from random variable \(C_{s}(q_{n})\), the observed cost for processing query \(q_{n}\) with the small model in round \(n\). And \(c_{l,n}\) is a sample from random variable \(C_{l}(q_{n})\), the observed cost for processing query \(q_{n}\) with the large model in round \(n\). We consider estimating \(P,c_{s}^{},c_{t}^{}\) with some oracles \(=(q_{1},,q_{N})\), \(_{s}(q),_{t}(q)=(_{N})\). We focus on the tabular case for theoretical analysis, where we set \(\), \(_{s}(q)\) and \(_{l}(q)\) to be the plug-in estimator:

\[(q)=^{N}(q_{i}=q)}{N},_ {l}(q) =^{N}(q_{i}=q)c_{i,1}}{ _{i=1}^{N}(q_{i}=q)},&_{i=1}^{N}(q_{i}=q )>0\\ B_{1},&_{i=1}^{N}(q_{i}=q)=0,\] \[_{s}(q) =^{N}(q_{i}=q)c_{s,i}}{ _{i=1}^{N}(q_{i}=q)},&_{i=1}^{N}(q_{i}= q)>0\\ B_{1},&_{i=1}^{N}(q_{i}=q)=0.\]

Similar to the case of caching without model multiplexing, for a long-tailed distribution \(P(q)\), the estimation of \(c_{s}^{}(q),c_{l}^{}(q)\) can be bad for the queries that are visited less. To select the maximum \(L\) elements from the plug-in estimator, we introduce pessimism to the estimate of \(_{l}\) and \(_{s}\). This leads to the following design of caching and model multiplexer \(\) and \(\):

\[}=*{arg\,min}_{:|| L} _{q}(q)(q)(B_{1},(_{s}(q),_{l}(q))-(B_{2}-B_{1})|/)}{2_{n=1}^{N}(q_{n}=q)}}).\]

We now show the cost for the caching and model multiplexer obtained from the empirical estimate is close to the optimal cost. The proof is deferred to Appendix G.

**Theorem 3**.: _Assume that \(N||(4L/)}{B_{1}}\) and take \(=1/N\). We have_

\[[(},)-( ^{},^{})] CL(B_{2}-B_{1})|| (8||N)}{B_{1}N}}.\]

### Finite sample setting: Online learning

We turn to the online case. We first propose a meta-algorithm in Algorithm 2. We provide a theoretical analysis of the meta-algorithm for the tabular case, with \(\)\(_{t}(q)=^{t}(q_{i}=q)}{t}\), and the \(\) defined as follows:

\[_{l,t}(q) =B_{1},&_{i=1}^{t}(s_{i}=0,q_ {i}=q)=0\\ (B_{1},^{t}(s_{i}=0,q_{i}=q)c_{l,i}}{ _{i=1}^{t}(s_{i}=0,q_{i}=q)}-(B_{2}-B_{1})|/)}{2_{i=1}^{t}(s_{i}=0,q_{i}=q)}}), ,\\ _{s,t}(q) =B_{

**Theorem 4**.: _Substituting the oracles in Algorithm 2 with the oracles above and \(=1/T\), we have_

\[_{}(T)-B_{1})B_{2}||L ^{2}(T||)}{B_{1}}.\]

The proof is deferred to Appendix H. Compared with the lower bound in Theorem 2, we see that the dependency on \(T\) is tight. The pessimism plays two different roles here: on the one hand, it encourages the exploration for model multiplexing to choose the ones with more uncertainty in the cost; on the other hand, it encourages the exploitation to be conservative about which query to save into the cache.

For the model multiplexer to work well, one needs to have a small yet accurate model multiplexer. In the case when the model multiplexer is not accurate, the small model always comes with a much smaller cost, and we are allowed to regenerate the responses and make corrections for the output, one may combine LEC with cascade (Chen et al., 2023b) to achieve better performance.

## 5 Experiments

We conduct both simulations and real-world experiments with our proposed methods. The code is available at https://github.com/Ying1123/llm-caching-multiplexing.

### Simulations for algorithm analysis

We conduct synthetic online and offline experiments for joint optimization of caching and model switching. In Figure 2, we plot the cumulative cost and regret in online learning for LFU and LEC caching algorithms. For LFU, we consider model switchers which always select the small or large models as the baselines. We consider \(20\) distinct prompts and set the cache size to be \(10\). We set the frequency distribution as power distribution with \(=0.9\). The ground truth cost for each query processed by both models is set as a sample from \(100X+1\), where \(X\) is a random variable generated from a Bernoulli distribution with the parameter \(0.5\). We repeat the simulation \(100\) times and plot the mean and standard deviation in the figure. Our simulation suggests that LEC with model switcher greatly improves the two baselines by a factor of \(50\) when the cost ratio is \(100\). We include additional results on the synthetic datasets for both online and offline settings with different \(\) values, cost ratios, and switcher accuracy in Appendix I.1.

### Experiments on real datasets

We evaluate our algorithms on two tasks: next-token prediction on the Lambada (Paperno et al., 2016) dataset and chat assistant on the OpenAssistant (Kopf et al., 2023) dataset.

For the next-token prediction task, we run the offline algorithm with two models: OPT-1.3B and OPT-13B (Zhang et al., 2022) and use FLOPs as the cost. The target performance metric is the number of correct tokens predicted, where we get the ground-truth token from the Lambada dataset.

For a given query, an algorithm can choose to run the small model or the large model. If the small model is chosen but its result is wrong, the large model must be run and it will incur an additional penalty. We fine-tune a BERT base model with \(2000\) samples as the model switcher by predicting whether the small model can give the correct result and achieve 80.2% accuracy. We work with \(100\) unseen distinct prompts in the offline setting with total queries \(10000\) and cache size \(40\). We compare our offline caching and switcher algorithms against LFU, large-model-only, and cascade (which always calls the small model first). As shown in Table 1, LEC is better than LFU in all cases. Combining LEC and switcher brings up to \(4.3\) cost reduction compared to the baseline "LFU + Large." However, as the predictor accuracy is limited, the model switcher may not be as good as the cascade algorithm in some cases. We leave the training of a better switcher as future work.

On the chat assistant task, we run the online algorithm with two models: FastChat-T5-3B and Vicuna-13B (Chiang et al., 2023), and use the inference latency as the cost. The quality of response is evaluated by GPT4 evaluation (Liu et al., 2023). We say a response is satisfying if the score is larger than 6 out of 10, and unsatisfying otherwise. If the response from the small model is unsatisfying, we will call the large model again and incur an additional cost in latency. The ratio between the average latency of the large model and the small model is 1.85. We work with \(100\) distinct prompts in the online setting with total queries \(10000\) and cache size \(40\). After a sufficient number of online learning steps, the switcher learns the accurate costs of two models on this finite prompts set, so "LEC + switcher" outperforms other algorithms in all cases on Table 2 with up to \(1.8\) latency reduction compared to "LFU + large" baseline.

We provide more experiments in Appendix I, where we evaluate both FLOPs and latency for both offline and online setting on both synthetic and real dataset, with varying cache size, query size and distinct prompts.

   \(\) & selector & LFU+ & LFU+ & LFU+ & LEC+ & LEC+ & LEC+ \\  & accuracy & large & cascade & selector & large & cascade & selector \\ 
0.2 & 80\% & 3.49 & 3.81 & 2.60 & 3.44 & **1.50** & 2.00 \\
0.8 & 80\% & 10.81 & 11.80 & 8.06 & 10.36 & **4.11** & 4.76 \\ 
0.2 & 100\% & 3.49 & 3.81 & 1.91 & 3.44 & 1.50 & **0.99** \\
0.8 & 100\% & 10.81 & 11.80 & 5.90 & 10.36 & 4.11 & **2.50** \\   

Table 1: Evaluation of offline algorithms on the Lambada dataset with OPT-1.3B and OPT-13B, \(100\) distinct prompts, total query size \(10000\) and cache size \(40\). \(\) is the parameter of the power distribution of the prompts. The table lists cumulative costs (\(10^{3}\)) for different algorithms.

Figure 2: Comparisons between LFU with either small or large model switching and LEC with model switcher. Both the \(x\)-axis and \(y\)-axis are logarithmic scales. The shaded regime represents the standard deviation calculated from the repeated experiments.

## 6 Conclusions

We have studied the joint optimization of caching and model multiplexing and proposed an optimal algorithm for the tabular case. There are a variety of further work that can be pursued in this vein, including:

* Designing the optimal caching and model multiplexing algorithm when there is a query queue, such that the query arrives at a random interval rather than a fixed interval. A more complicated serving pattern also needs to take batching strategies into consideration.
* Understanding the scaling law of the predictors. We hope to use a small yet accurate model for prediction to reduce overhead introduced by the predictor. It is important to understand the trade-off between prediction accuracy, model size, and training data size.
* Designing optimal caching algorithm when the responses generated in each round have diverse qualities.