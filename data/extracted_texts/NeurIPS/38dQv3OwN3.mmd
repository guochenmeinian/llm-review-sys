# Fairness Aware Counterfactuals for Subgroups

Loukas Kavouras

IMSI / Athena RC

kavouras@athenarc.gr &Konstantinos Tsopelas

IMSI / Athena RC

k.tsopelas@athenarc.gr &Giorgos Giannopoulos

IMSI / Athena RC

giann@athenarc.gr &Dimitris Sacharidis

ULB

dimitris.sacharidis@ulb.be &Eleni Psaroudaki

NTUA & IMSI / Athena RC

epsaroudaki@mail.ntua.gr &Nikolaos Theologitis

IMSI / Athena RC

n.theologitis@athenarc.gr &Dimitrios Rontogiannis

IMSI / Athena RC

dronto@gmail.com &Dimitris Fotakis

NTUA & Archimedes / Athena RC

fotakis@cs.ntua.gr &Ioannis Emiris

NKUA & IMSI / Athena RC

emiris@athenarc.gr

###### Abstract

In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e., receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation of different benchmark datasets.

## 1 Introduction

Machine learning is now an integral part of decision-making processes across various domains, e.g., medical applications , employment , recommender systems , education , credit assessment . Its decisions affect our everyday life directly, and, if unjust or discriminative, could potentially harm our society . Multiple examples of discrimination or bias towards specific population subgroups in such applications  create the need not only for explainable and interpretable machine learning that is more trustworthy , but also for auditing models in order to detect hidden bias for subgroups .

Bias towards protected subgroups is most often detected by various notions of _fairness of prediction_, e.g., statistical parity, where all subgroups defined by a protected attribute should have the same probability of being assigned the positive (favorable) predicted class. These definitions capture the _explicit_ bias reflected in the model's predictions. Nevertheless, an _implicit_ form of bias is thedifficulty for, or the _burden_ of, an individual (or a group thereof) to achieve _recourse_, i.e., perform the necessary _actions_ to change their features so as to obtain the favorable outcome . Recourse provides explainability (i.e., a counterfactual explanation ) and actionability to an affected individual, and is a legal necessity in various domains, e.g., the Equal Credit Opportunity Act mandates that an individual can demand to learn the reasons for a loan denial. _Fairness of recourse_ captures the notion that the protected subgroups should bear equal burden .

To illustrate these notions, consider a company that supports its promotion decisions with an AI system that classifies employees as good candidates for promotion, the favorable positive class, based on various performance metrics, including their cycle time efficiency (CTE) and the annual contract value (ACV) for the projects they lead. Figure 0(a) draws ten employees from the negative predicted class as points in the CTE-ACV plane and also depicts the decision boundary of the classifier. Race is the protected attribute, and there are two protected subgroups with five employees each, depicted as circles and triangles. For each employee, the arrow depicts the best _action_ to achieve recourse, i.e., to cross the decision boundary, and the number indicates the _cost_ of the action, here simply computed as the distance to the boundary . For example, \(x_{1}\) may increase their chances for promotion mostly by acquiring more high-valued projects, while \(x_{2}\) mostly by increasing their efficiency. Burden is defined as the mean cost for a protected subgroup . For the protected race 0, the burden is 2, while it is 2.2 for race 1, indicating thus unfairness of recourse against race 1. In contrast, assuming there is an equal number of employees of each race in the company, the classifier satisfies fairness of prediction in terms of statistical parity (equal positive rate in the subgroups).

While fairness of recourse is an important concept that captures a distinct notion of algorithmic bias, as also explained in , we argue that it is much more nuanced than the mean cost of recourse (aka burden) considered in all prior work , and raise three issues.

First, the mean cost, which comprises a _micro viewpoint_ of the problem, does not provide the complete picture of how the cost of recourse varies among individuals and may lead to contrasting conclusions. Consider again Figure 0(a), and observe that for race 1 all but one employee can achieve recourse with cost at most 2, while an outlier achieves recourse with cost 6. It is this outlier that raises the mean cost for race 1 above that of race 0. For the two race subgroups, Figure 0(b) shows the cumulative distribution of cost, termed the _effectiveness-cost distribution_ (ecd) in Section 2. These distributions allow the fairness auditor to inspect the _tradeoff_ between cost and recourse, and define the appropriate fairness of recourse notion. For example, they may consider that actions with a cost of more than 2 are unrealistic (e.g., because they cannot be realized within some timeframe), and thus investigate how many employees can achieve recourse under this constraint; we refer to this as _equal effectiveness within budget_ in Section 2.3. Under this notion, there is unfairness against race 0, as only 60% of race 0 employees (compared to 80% of race 1) can realistically achieve recourse.

There are several options to go beyond the mean cost. One is to consider fairness of recourse at the individual level, and compare an individual with their counterfactual counterpart had their protected attribute changed value . However, this approach is impractical as, similar to other causal-based definitions of fairness, e.g., , it requires strong assumptions about the causal structure in the

Figure 1: (a) An example of affected individuals, the decision boundary, actions, and a subpopulation (in the shaded region), depicted in the feature space; (b) Cumulative distribution of cost of recourse for the individuals in (a); (c) Comparison of two actions to achieve recourse for two individuals.

domain . In contrast, we argue that it's preferable to investigate fairness in subpopulations and inspect the trade-off between cost and recourse.

Second, there are many cases where the aforementioned micro-level aggregation of individuals' costs is not meaningful in auditing real-world systems. To account for this, we introduce a _macro viewpoint_ where a group of individuals is considered as a whole, and an action is applied to and assessed _collectively_ for all individuals in the group. An action represents an external horizontal intervention, such as an affirmative action in society, or focused measures in an organization that would change the attributes of some subpopulation (e.g., decrease tax or loan interest rates, increase productivity skills). In the macro viewpoint, the cost of recourse does not burden the individuals, but the external third party, e.g., the society or an organization. Moreover, the macro viewpoint offers a more intuitive way to _audit_ a system for fairness of recourse, as it seeks to uncover systemic biases that apply to a large number of individuals.

To illustrate the macro viewpoint, consider the group within the shaded region in Figure 0(a). In the micro viewpoint, each employee seeks recourse individually, and both race subgroups have the same distribution of costs. However, we can observe that race 0 employees, like \(x_{1}\) achieve recourse by actions in the ACV direction, while race 1 employees, like \(x_{2}\) in the orthogonal CTE direction. This becomes apparent when we take the macro viewpoint, and investigate the effect of action \(a_{1}\), depicted on the border of the shaded region, discovering that it is disproportionally effective on the race subgroups (leads to recourse for two-thirds of one subgroup but for none in the other). In this example, action \(a_{1}\) might represent the effect of a training program to enhance productivity skills, and the macro viewpoint finds that it would perpetuate the existing burden of race 0 employees.

Third, existing notions of fairness of recourse have an important practical limitation: they require a cost function that captures one's ability to modify one's attributes, whose definition may involve a learning process , or an adaptation of off-the-shelf functions by practitioners . Even the idea of which attributes are actionable, in the sense that one can change (e.g., one cannot get younger), or "ethical" to suggest as actionable (e.g., change of marital status from married to divorced) hides many complications .

Conclusions drawn about fairness of recourse crucially depend on the cost definition. Consider individuals \(x_{1}\), \(x_{2}\), and actions \(a_{1}\), \(a_{2}\), shown in Figure 0(c). Observe that is hard to say which action is cheaper, as one needs to compare changes _within_ and _across_ very dissimilar attributes. Suppose the cost function indicates action \(a_{1}\) is cheaper; both individuals achieve recourse with the same cost. However, if action \(a_{2}\) is cheaper, only \(x_{2}\) achieves recourse. Is the classifier fair?

To address this limitation, we propose definitions that are _oblivious to the cost function_. The idea is to compare the effectiveness of actions to the protected subgroups, rather than the cost of recourse for the subgroups. One way to define a cost-oblivious notion of fairness of recourse is the _equal choice for recourse_ (see Section 2.3), which we illustrate using the example in Figure 0(c). According to it, the classifier is unfair against \(x_{1}\), as \(x_{1}\) has only one option, while \(x_{2}\) has two options to achieve recourse among the set of actions \(\{a_{1},a_{2}\}\).

ContributionOur aim is to showcase that fairness of recourse is an important and distinct notion of algorithmic fairness with several facets not previously explored. We make a series of conceptual and technical contributions.

Conceptually, we distinguish between two different viewpoints. The _micro viewpoint_ follows literature on recourse fairness  in that each individual chooses the action that is cheaper for them, but revisits existing notions. It considers the trade-off between cost and recourse and defines several novel notions that capture different aspects of it. The _macro viewpoint_ considers how an action collectively affects a group of individuals, and quantifies its effectiveness. It allows the formulation of _cost-oblivious_ notions of recourse fairness. It also leads to an alternative trade-off between cost and recourse that may reveal systemic forms of bias.

Technically, we propose an efficient, interpretable, model-agnostic, highly parametrizable framework termed FACTS (Fairness-Aware Counterfactuals for Subgroups) to audit for fairness of recourse. FACTS is _efficient_ in computing the effectiveness-cost distribution, which captures the trade-off between cost and recourse, in both the micro or macro viewpoint. The key idea is that instead of determining the best action for each individual independently (i.e., finding their nearest counterfactual explanation ), it enumerates the space of actions and determines how many and which individualsachieve recourse through each action. Furthermore, FACTS employs a systematic way to explore the feature space and discover any subspace such that recourse bias exists among the protected subgroups within. FACTS ranks the subspaces in decreasing order of recourse bias it detects, and for each provides an _interpretable_ summary of its findings.

Related WorkWe distinguish between _fairness of predictions_ and _fairness of recourse_. The former aims to capture and quantify unfairness by comparing directly the model's predictions [27; 2] at: the individual level, e.g., individual fairness , counterfactual/causal-based fairness [21; 19; 26]; and the group level, e.g., demographic parity , equal odd/opportunity .

Fairness of recourse is a more recent notion, related to _counterfactual explanations_, which explains a prediction for an individual (the factual) by presenting the "best" counterfactual that would result in the opposite prediction, offering thus _recourse_ to the individual. Best, typically means the _nearest counterfactual_ in terms of a distance metric in the feature space. Another perspective, which we adopt here, is to consider the _action_ that transforms a factual into a counterfactual, and specify a _cost function_ to quantify the effort required by an individual to perform an action. In the simplest case, the cost function can be the distance between factual and counterfactual, but it can also encode the _feasibility_ of an action (e.g., it is impossible to decrease age) and the _plausibility_ of a counterfactual (e.g., it is out-of-distribution). It is also possible to view actions as interventions that act on a structural causal model capturing cause-effect relationships among attributes . Hereafter, we adopt the most general definition, where a cost function is available, and assume that the best counterfactual explanation is the one that comes from the minimum cost action. Counterfactual explanations have been suggested as a mechanism to detect possible bias against protected subgroups, e.g., when they require changes in protected attributes .

Fairness of recourse, first introduced in  and formalized in , is defined at the group level as the disparity of the mean cost to achieve recourse (called burden in subsequent works) among the protected subgroups. Fairness of recourse for an individual is when they require the same cost to achieve recourse in the actual world and in an imaginary world where they would have a different value in the protected attribute . This definition however only applies when a structural causal model of the world is available. Our work expands on these ideas and proposes alternate definitions that capture a macro and a micro viewpoint of fairness of recourse.

There is a line of work on auditing models for fairness of predictions at the subpopulation level [17; 18]. For example,  identifies subpopulations that show dependence between a performance measure and the protected attribute.  determines whether people are harmed due to their membership in a specific group by examining a ranking of features that are most associated with the model's behavior. There is no equivalent work for fairness of recourse, although the need to consider the subpopulation is recognized in  due to uncertainty in assumptions or to intentionally study fairness. Our work is the first that audits for fairness of recourse at the subpopulation level.

A final related line of work is global explainability. For example, recourse summaries [31; 24; 25] summarizes individual counterfactual explanations _globally_, and as the authors in [31; 25] suggest can be used to manually audit for unfairness in subgroups of interest.  aims to explain how a model behaves in subspaces characterized by certain features of interest.  uses counterfactuals to unveil whether a black-box model, that already complies with the regulations that demand the omission of sensitive attributes, is still biased or not, by trying to find a relation between proxy features and bias. Our work is related to these methods in that we also compute counterfactual explanations for all instances, albeit in a more efficient manner, and with the goal to audit fairness of recourse on subpopulation level.

## 2 Fairness of Recourse for Subgroups

### Preliminaries

We consider a **feature space**\(X=X_{1} X_{n}\), where \(X_{n}\) denotes the **protected feature**, which, for ease of presentation, takes two protected values \(\{0,1\}\). For an instance \(x X\), we use the notation \(x.X_{i}\) to refer to its value in feature \(X_{i}\).

We consider a binary **classifier**\(h:X\{-1,1\}\) where the positive outcome is the favorable one. For a given \(h\), we are concerned with a dataset \(D\) of **adversely affected individuals**, i.e., those who receive the unfavorable outcome. We prefer the term instance to refer to any point in the feature space \(X\), and the term individual to refer to an instance from the dataset \(D\).

We define an **action**\(a\) as a set of changes to feature values, e.g., \(a=\{, 12\}\). We denote as \(A\) the set of possible actions. An action \(a\) when applied to an individual (a factual instance) \(x\) results in a **counterfactual** instance \(x^{}=a(x)\). If the individual \(x\) was adversely affected (\(h(x)=-1\)) and the action results in a counterfactual that receives the desired outcome (\(h(a(x))=1\)), we say that action \(a\) offers recourse to the individual \(x\) and is thus **effective**. In line with the literature, we also refer to an effective action as a **counterfactual explanation** for individual \(x\).

An action \(a\) incurs a **cost** to an individual \(x\), which we denote as \((a,x)\). The cost function captures both how _feasible_ the action \(a\) is for the individual \(x\), and how _plausible_ the counterfactual \(a(x)\) is .

Given a set of actions \(A\), we define the **recourse cost**\((A,x)\) of an individual \(x\) as the minimum cost among effective actions if there is one, or otherwise some maximum cost represented as \(c_{}\):

\[(A,x)=\{(a,x)|a A:h(a(x))=1\},&  a A:h(a(x))=1;\\ c_{},&.\]

An effective action of minimum cost is also called a **nearest counterfactual explanation**.

We define a **subspace**\(X_{p} X\) using a **predicate**\(p\), which is a conjunction of feature-level predicates of the form "_feature-operator-value_", e.g., the predicate \(p=(=)( 9)\) defines instances from the US that have more than 9 years of education.

Given a predicate \(p\), we define the subpopulation **group**\(G_{p} D\) as the set of affected individuals that satisfy \(p\), i.e., \(G_{p}=\{x D|p(x)\}\). We further distinguish between the **protected subgroups**\(G_{p,1}=\{x D|p(x) x.X_{n}=1\}\) and \(G_{p,0}=\{x D|p(x) x.X_{n}=0\}\). When the predicate \(p\) is understood, we may omit it in the designation of a group to simplify notation.

### Effectiveness-Cost Trade-Off

For a specific action \(a\), we naturally define its **effectiveness** (eff) for a group \(G\), as the proportion of individuals from \(G\) that achieve recourse through \(a\):

\[(a,G)=|\{x G|h(a(x))=1\}|.\]

Note that effectiveness is termed correctness in  and coverage in . We want to examine how recourse is achieved for the group \(G\) through a set of possible actions \(A\). We define the **aggregate effectiveness** (aeff) of \(A\) for \(G\) in two distinct ways.

In the _micro viewpoint_, the individuals in the group are considered independently, and each may choose the action that benefits itself the most. Concretely, we define the **micro-effectiveness** of set of actions \(A\) for group \(G\) as the proportion of individuals in \(G\) that can achieve recourse through _some_ action in \(A\), i.e.,:

\[_{}(A,G)=|\{x G| a A,(a, x)=1\}|.\]

In the _macro viewpoint_, the group is considered as a whole, and an action is applied collectively to all individuals in the group. Concretely, we define the **macro-effectiveness** of set of actions \(A\) for group \(G\) as the largest proportion of individuals in \(G\) that can achieve recourse through _the same_ action in \(A\), i.e.,:

\[_{}(A,G)=_{a A}|\{x G|\, {eff}(a,x)=1\}|.\]

For a group \(G\), actions \(A\), and a cost budget \(c\), we define the **in-budget actions** as the set of actions that cost at most \(c\) for any individual in \(G\):

\[A_{c}=\{a A| x G,(a,x) c\}.\]

We define the **effectiveness-cost distribution** (ecd) as the function that for a cost budget \(c\) returns the aggregate effectiveness possible with in-budget actions:\[(c;A,G)=(A_{c},G).\]

We use \(_{}\), \(_{}\) to refer to the micro, macro viewpoints of aggregate effectiveness. A similar concept, termed the coverage-cost profile, appears in .

The value \((c;A,G)\) is the proportion of individuals in \(G\) that can achieve recourse through actions \(A\) with cost at most \(c\). Therefore, the \(\) function has an intuitive probabilistic interpretation. Consider the subspace \(X_{p}\) determined by predicate \(p\), and define the random variable \(C\) as the cost required by an instance \(x X_{p}\) to achieve recourse. The function \((c;A,G_{p})\) is the empirical cumulative distribution function of \(C\) using sample \(G_{p}\).

The **inverse effectiveness-cost distribution** function \(^{-1}(;A,G)\) takes as input an effectiveness level \(\) and returns the minimum cost required so that \(|G|\) individuals achieve recourse.

### Definitions of Subgroup Recourse Fairness

We define recourse fairness of classifier \(h\) for a group \(G\) by comparing the \(\) functions of the protected subgroups \(G_{0}\), \(G_{1}\) in different ways.

The first two definitions are _cost-oblivious_, and apply whenever we can ignore the cost function. Specifically, given a set of actions \(A\) and a group \(G\), we assume that all actions in \(A\) are considered equivalent, and that the cost of any action is the same for all individuals in the group; i.e., \((a,x)=(a^{},x^{})\), \( a a^{} A,x x^{} G\). The definitions simply compare the aggregate effectiveness of a set of actions on the protected subgroups.

**Equal Effectiveness** This definition has a micro and a macro interpretation, and says that the classifier is fair if the same proportion of individuals in the protected subgroups can achieve recourse: \((A,G_{0})=(A,G_{1})\).

**Equal Choice for Recourse** This definition has only a macro interpretation and claims that the classifier is fair if the protected subgroups can choose among the same number of sufficiently effective actions to achieve recourse, where sufficiently effective means the actions should work for at least \(100\%\) (for \(\)) of the subgroup:

\[|\{a A|\,(a,G_{0})\}|=|\{a A|\,(a,G_{1}) \}|.\]

The next three definitions assume the \(\) function is specified, and have both a micro and a macro interpretation.

**Equal Effectiveness within Budget** The classifier is fair if the same proportion of individuals in the protected subgroups can achieve recourse with a cost at most \(c\):

\[(c;A,G_{0})=(c;A,G_{1}).\]

**Equal Cost of Effectiveness** The classifier is fair if the minimum cost to achieve aggregate effectiveness of \(\) in the protected subgroups is equal:

\[^{-1}(;A,G_{0})=^{-1}(;A,G_{1}).\]

**Fair Effectiveness-Cost Trade-Off** The classifier is fair if the protected subgroups have the same effectiveness-cost distribution, or equivalently for each cost budget \(c\), their aggregate effectiveness is equal:

\[_{c}|\,(c;A,G_{0})-(c;A,G_{1})|=0.\]

The left-hand side represents the two-sample Kolmogorov-Smirnov statistic for the empirical cumulative distributions (\(\)) of the protected subgroups. We say that the classifier is fair with confidence \(\) if this statistic is less than \(}|+|G_{,1}|}{2|G_{,0}||G_{,1}|}}\).

The last definition takes a micro viewpoint and extends the notion of _burden_ from literature to the case where not all individuals may achieve recourse. The **mean recourse cost** of a group \(G\),

\[}(A,G)=_{x G}(A,x)\]considers individuals that cannot achieve recourse through \(A\) and have a recourse cost of \(c_{}\). To exclude them, we denote as \(G^{*}\) the set of individuals of \(G\) that can achieve recourse through an action in \(A\), i.e., \(G^{*}=\{x G| a A,h(a(x))=1\}\). Then the **conditional mean recourse cost** is the mean recourse cost among those that can achieve recourse:

\[}^{*}(A,G)=|}_{x G^{*}}(A,x).\]

If \(G=G^{*}\), the definitions coincide with burden.

**Equal (Conditional) Mean Recourse** The classifier is fair if the (conditional) mean recourse cost for the protected subgroups is the same:

\[^{*}(A,G_{0};A)=}^{*}(A,G_{1}).\]

Note that when the group \(G\) is the entire dataset of affected individuals, and all individuals can achieve recourse through \(A\), this fairness notion coincides with fairness of burden [35; 32].

## 3 Fairness-aware Counterfactuals for Subgroups

This section presents FACTS (Fairness-aware Counterfactuals for Subgroups), a framework that implements both the micro and the macro viewpoint, and all respective fairness definitions provided in Section 2.3 to support auditing of the "difficulty to achieve recourse" in subgroups. The output of FACTS comprises population groups that are assigned (a) an unfairness score that captures the disparity between protected subgroups according to different fairness definitions and allows us to rank groups, and (b) a user-intuitive, easily explainable counterfactual summary, which we term _Comparative Subgroup Counterfactuals_ (CSC).

Figure 2 presents an example result of FACTS derived from the adult dataset . The "if clause" represents the subgroup \(G_{p}\), which contains all the affected individuals that satisfy the predicate \(p=(=)\ \ (=)\ \ (=)\). The information below the predicate refers to the protected subgroups \(G_{p,0}\) and \(G_{p,1}\) which are the female and male individuals of \(G_{p}\) respectively. With blue color, we highlight the percentage \((G_{p,i})=|G_{p,i}|/|D_{i}|\) which serves as an indicator of the size of the protected subgroup. The most important part of the representation is the actions applied that appear below each protected subgroup and are evaluated in terms of fairness metrics. In this example, the metric is the _Equal Cost of Effectiveness_ with effectiveness threshold \(=0.7\). For \(G_{p,0}\) there is no action surpassing the threshold \(=0.7\), therefore we display a message accordingly. On the contrary, the action \(a=\{, \}\) has effectiveness \(0.72>\) for \(G_{p,1}\), thus allowing a respective \(72\%\) of the male individuals of \(G_{p}\) to achieve recourse. The unfairness score is "inf", since no recourse is achieved for the female subgroup.

**Method overview** Deploying FACTS comprises three main steps: (a) _Subgroup and action space generation_ that creates the sets of groups \(\) and actions \(A\) to examine; (b) _Counterfactual summaries generation_ that applies appropriate actions to each group \(G\); and (c) _CSC construction and fairness ranking_ that applies the definitions of Section 2.3. Next, we describe these steps in detail.

**(a) Subgroup and action space generation** Subgroups are generated by executing the fp-growth  frequent itemset mining algorithm on \(D_{0}\) and on \(D_{1}\) resulting to the sets of subgroups \(_{0}\) and \(_{1}\) and then by computing the intersection \(=_{0}_{1}\). In our setting, an item is a feature-level predicate of the form "feature-operator-value" and, consequently, an itemset is a predicate \(p\) defining a subgroup \(G_{p}\). This step guarantees that the evaluation in terms of fairness will be performed

Figure 2: CSC for a highly biased subgroup in terms of _Equal Cost of Effectiveness_ with \(=0.7\).

between the common subgroups \(\) of the protected populations. The set of all actions \(A\) is generated by executing fp-growth on the unaffected population to increase the chance of more effective actions and to reduce the computational complexity. The above process is parameterizable w.r.t. the selection of the protected attribute(s) and the minimum frequency threshold for obtaining candidate subgroups. We emphasize that alternate mechanisms to generate the sets \(A\) and \(\) are possible.

**(b) Counterfactual summaries generation** For each subgroup \(G_{p}\), the following steps are performed: (i) Find valid actions, i.e., the actions in \(A\) that contain a subset of the features appearing in \(p\) and at least one different value in these features; (ii) For each valid action \(a\) compute \((a,G_{p,0})\) and \((a,G_{p,1})\). The aforementioned process extracts, for each subgroup \(G_{p}\), a subset \(V_{p}\) of the actions \(A\), with each action having exactly the same cost for all individuals of \(G_{p}\). Therefore, individuals of \(G_{p,0}\) and \(G_{p,1}\) are evaluated in terms of subgroup-level actions, with a fixed cost for all individuals of the subgroup, in contrast to methods that rely on aggregating the cost of individual counterfactuals. This approach provides a key advantage to our method in cases where the definition of the exact costs for actions is either difficult or ambiguous: a misguided or even completely erroneous attribution of a cost to an action will equally affect all individuals of the subgroup and only to the extent that the respective fairness definition allows it. In the setting of individual counterfactual cost aggregation, changes in the same sets of features could lead to highly varying action costs for different individuals within the same subgroup.

**(c) CSC construction and fairness ranking** FACTS evaluates all definitions of Section 2.3 on all subgroups, producing an unfairness score per definition, per subgroup. In particular, each definition of Section 2.3 quantifies a different aspect of the difficulty for a protected subgroup to achieve recourse. This quantification directly translates to difficulty scores for the protected subgroups \(G_{p,0}\) and \(G_{p,1}\) of each subgroup \(G_{p}\), which we compare accordingly (computing the absolute difference between them) to arrive at the unfairness score of each \(G_{p}\) based on the particular fairness metric.

The outcome of this process is the generation, for each fairness definition, of a ranked list of CSC representations, in decreasing order of their unfairness score. Apart from unfairness ranking, the CSC representations will allow users to intuitively understand unfairness by directly comparing differences in actions between the protected populations within a subgroup.

## 4 Experiments

This section presents the experimental evaluation of FACTS on the Adult dataset ; more information about the datasets, experimental setting, and additional results can be found in the appendix. The code is available at: [https://github.com/AutoFairAthenaRC/FACTS](https://github.com/AutoFairAthenaRC/FACTS). First, we briefly describe the experimental setting and then present and discuss Comparative Subgroup Counterfactuals for subgroups ranked as the most unfair according to various definitions of Section 2.3.

Experimental SettingThe first step was the dataset cleanup (e.g., removing missing values and duplicate features, creating bins for continuous features like age). The resulting dataset was split randomly with a 70:30 split ratio and was used to train and test respectively a logistic regression model (consequently used as the black-box model to audit). For the generation of the subgroups and the set of actions we used fp-growth with a 1% support threshold on the test set. We also implemented various cost functions, depending on the type of feature, i.e., categorical, ordinal, and numerical. A detailed description of the experimental setting, the models used, and the processes of our framework can be found in the supplementary material.

    &  &  &  \\   &  &  &  &  &  &  &  &  \\  Equal Effectiveness & 2950 & Male & 0.11 & 10063 & Female & 0.0004 & 275 & Female & 0.32 \\ Equal Choice for Recourse (\(o=0.3\)) & Fair & 0 & 12 & Female & 2 & Fair & - & 0 \\ Equal Choice for Recourse (\(o=0.7\)) & 6 & Male & 1 & 4 & Female & 6 & Fair & - & 0 \\ Equal Effectiveness within Budget (\(c=5\)) & Fair & 0 & 2966 & Female & 0.056 & 70 & Female & 0.3 \\ Equal Effectiveness within Budget (\(c=10\)) & 2380 & Male & 0.11 & 2818 & Female & 0.0004 & 276 & Female & 0.3 \\ Equal Effectiveness within Budget (\(c=10\)) & 2575 & Male & 0.11 & 9221 & Female & 0.0004 & 272 & Female & 0.3 \\ Equal Cost of Effectiveness (\(o=0.3\)) & Fair & - & 0 & Fair & 0 & 1 & Female & 1 \\ Equal Cost of Effectiveness (\(o=0.7\)) & 1 & Male & 1 & 12 & Female & 2 & Fair & - & 0 \\ Fair Effectiveness-Cost Trade-Off & 4005 & Male & 0.11 & 3579 & Female & 0.13 & 306 & Female & 0.32 \\ Equal Confidential Mean Recourse & Pair & - & 0 & 3145 & Female & 0.35 & Fair & - & 0 \\   

Table 1: Unfair subgroups identified in the Adult dataset.

**Unfair subgroups** Table 1 presents three subgroups which were ranked at position 1 according to three different definitions: _Equal Cost of Effectiveness (\(\) = 0.7), Equal Choice for Recourse (\(\) = 0.7)_ and _Equal Cost of Effectiveness (\(\) = 0.3)_, meaning that these subgroups were detected to have the highest unfairness according to the respective definitions. For each subgroup, its _rank_, _bias against_, and _unfairness score_ are provided for all definitions presented in the left-most column. When the unfairness score is 0, we display the value "Fair" in the rank column. Note that subgroups with exactly the same score w.r.t. a definition will receive the same rank. The CSC representations for the fairness metric that ranked the three subgroups of Table 1 at the first position are shown in Figure 3.

Subgroup 1 is ranked first (highly unfair) based on _Equal Cost of Effectiveness with \(=0.7\)_, while it is ranked much lower or it is even considered as fair according to most of the remaining definitions (see the values of the "rank" column for Subgroup 1). The same pattern is observed for Subgroup 2 and Subgroup 3: they are ranked first based on _Equal Choice for Recourse with \(=0.7\)_ and _Equal Cost of Effectiveness with \(=0.3\)_ accordingly, but much lower according to the remaining definitions. This finding provides a strong indication of the utility of the different fairness definitions, i.e., the fact that they are able to capture different aspects of the difficulty in achieving recourse.1

What is more, these different aspects can easily be motivated by real-world auditing needs. For example, out of the aforementioned definitions, _Equal Cost of Effectiveness with \(=0.7\)_ would be suitable in a scenario where a horizontal intervention to support a subpopulation needs to be performed, but a limited number of actions is affordable. In this case, the macro viewpoint demonstrated in the CSC Subgroup 1 (top result in Figure 3) serves exactly this purpose: one can easily derive single, group-level actions that can effectively achieve recourse for a desired percentage of the unfavored

Figure 3: Comparative Subgroup Counterfactuals for the subgroups of Table 1.

subpopulation. On the other hand, _Equal Choice for Recourse (\(=0.3\))_, for which the CSC of a top result is shown in the middle of Figure 3, is mainly suitable for cases where assigning costs to actions might be cumbersome or even dangerous/unethical. This definition is oblivious to costs and measures bias based on the difference in the number of sufficiently effective actions to achieve recourse between the protected subgroups.

Another important observation comes again from Subgroup 1, where bias against the _Male_ protected subgroup is detected, contrary to empirical findings from works employing more statistical-oriented approaches (e.g., ), where only subgroups with bias against the _Female_ protected subgroup are reported. We deem this finding important, since it directly links to the problem of gerrymandering , which consists in partitioning attributes in such way and granularity to mask bias in subgroups. Our framework demonstrates robustness to this phenomenon, given that it can be properly configured to examine sufficiently small subgroups via the minimum frequency threshold described in Section 3.

## 5 Limitations

There exist numerous challenges and constraints when attempting to determine costs associated with specific counterfactuals. The cost functions involved are intricate, dataset-specific, and frequently necessitate the expertise of a domain specialist. These cost functions may depend on individual characteristics or human interpretation of the perceived difficulty of specific changes, potentially giving rise to concerns like breaches of user privacy (as the expert may require access to all individual user characteristics ) or the risk of malicious specialists manipulating cost functions to conceal existing biases.

We acknowledge the difficulties of finding the dataset-dependent cost functions and proceed to implement various "natural" cost functions tailored to different feature types (e.g., \(L_{1}\) norm, exponential distances). These are only suggestions to the expert who is going to use our framework, are susceptible to change, and can be tailored for the specific datasets. Our primary focus is to identify potential biases towards specific subpopulations and recognizing the aforementioned inherent difficulties in defining costs, we introduce fairness metrics that remain independent of cost considerations. We recognize that a single metric, whether cost-dependent or not, cannot identify all forms of bias that may be present within a model's predictions. We do not aim to replace the existing statistical measures for bias detection, rather than complement them by focusing on the _bias of achieving recourse_.

It is essential to note that FACTS, like other resource generation algorithms, may be susceptible to errors, as demonstrated in prior research . Outcomes produced by FACTS can vary based on the chosen configuration, including hyperparameters and metric selection. These variations can either obscure biases present within a classifier or indicate their absence. It is crucial to emphasize that since FACTS is an explainable framework for bias assessment within subgroups, its primary purpose is to serve as a guidance tool for auditors to identify subgroups requiring in-depth analysis of fairness criteria violations. Responsible usage of our framework necessitates transparency regarding the chosen configuration. Therefore, we recommend running FACTS with different hyperparameters and utilizing as many metrics as possible. Appendix G provides a comprehensive discussion of FACTS' limitations and responsible disclosure guidelines.

## 6 Conclusion

In this work, we delve deeper into the difficulty (or burden) of achieving recourse, an implicit and less studied type of bias. In particular, we go beyond existing works, by introducing a framework of fairness notions and definitions that, among others, conceptualizes the distinction between the micro and macro viewpoint of the problem, allows the consideration of a subgroup as a whole when exploring recourses, and supports _oblivious to action cost_ fairness auditing. We complement this framework with an efficient implementation that allows the detection and ranking of subgroups according to the introduced fairness definitions and produces intuitive, explainable subgroup representations in the form of counterfactual summaries. Our next steps include the extension of the set of fairness definitions, focusing on the comparison of the effectiveness-cost distribution; improvements on the exploration, filtering, and ranking of the subgroup representations, via considering hierarchical relations or high-coverage of subgroups; a user study to evaluate the interpretability of fairness judgments, and the development of a fully-fledged auditing tool.