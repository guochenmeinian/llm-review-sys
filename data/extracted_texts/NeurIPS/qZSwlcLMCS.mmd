# Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling

Jiatao Gu\({}^{}\)\({}^{*}\), Ying Shen\({}^{}\)\({}^{*}\), Shuangfei Zhai\({}^{}\), Yizhe Zhang\({}^{}\), Navdeep Jaitly\({}^{}\), Josh Susskind\({}^{}\)

\({}^{}\)Apple \({}^{}\)University of Illinois Urbana-Champaign \({}^{*}\) equal contribution

\({}^{}\){jgu32, szhai, yizzhang,njaitly, jsusskind}@apple.com \({}^{}\)ying22@illinois.edu

###### Abstract

Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latents, demonstrating its capability to effectively control the image generation process.

## 1 Introduction

Diffusion models have become pervasive in many text-to-image generation tasks for their ability to generate high-quality images based on textual descriptions. A pivotal mechanism in these models is

Figure 1: Comparison of the generated image samples given the caption “a cat sat on the mat”. Our models generate more diverse images with the help of autoregressive latent modeling.

classifier-free guidance (CFG) (Ho and Salimans, 2021), which effectively steers the sampling process towards better alignment to textual prompts and improved sampling quality at the same time. CFG can be interpreted as tuning the temperature of the conditional distribution, whereas increasing the guidance scale sharpens the conditional distribution. This guides the generation to focus on regions of high conditional probability, effectively reducing sampling noise which is typically of lower density. However, while high CFG improves sampling quality, it simultaneously narrows the diversity in the generated samples. This manifests in the models' inability to produce diverse images from the same caption, even when there are variations in the initial noise that seeds the generation process. For instance, given a fixed textual description, "a cat sits on a mat", existing text-to-image diffusion models predominantly produce image samples depicting cats with similar colors and patterns, as illustrated in Figure 1. Such limited visual diversity hinders the practical application of diffusion models in scenarios where a wide range of creative and diverse visual interpretations are desired from identical textual inputs. It also poses challenges in scenarios demanding the representation of underrepresented data or accommodating a wide range of user preferences. Therefore, enhancing diversity in diffusion models without compromising the quality remains a critical research problem.

To tackle this, we introduce Kaleido, a general framework that improves diffusion models with autoregressive priors. Kaleido first defines a discrete encoding of images (eg, detailed captioning, bounding boxes), which captures desirable abstractions of images that's not included in the default text prompts. Next, Kaleido integrates an encoder-decoder language model that encodes the original text caption and autoregressively predicts the discrete latent tokens. Lastly, the diffusion model is conditioned on both the original text prompt and the autoregressively generated discrete latents and generates an image. This enriched conditioning allows Kaleido to produce a more diverse array of high-quality images, even at high guidance scales. We explore various forms of latents, including textual descriptions, detection bounding boxes, object blobs, and abstract visual tokens - all designed to refine and guide the conditional image generation process.

We experiment on both class and text conditioned image generation benchmarks 1. We show that Kaleido not only outperforms standard diffusion models in terms of diversity but also maintains the high quality of the generated image. Additionally, the generated latents effectively control the characteristics of the generated images, ensuring that the image samples closely align with the intended latent variables. This modeling of latent tokens not only increases the diversity of image outputs but also provides a degree of interpretability and control over the image generation process.

To summarize, Kaleido exhibits the following advantages:

1. Kaleido promotes the diversity in generated image samples even with high CFG, allowing the image generation of both high quality and diversity.

2. The generated latent variables are interpretable, offering an explainable mechanism behind the image generation process, and facilitating an understanding of how different latents affect the outputs.

3. Kaleido provides a fine-grained, editable interface that allows users to adjust the discrete latent codes before final image production, granting greater flexibility and control over the output.

## 2 Preliminaries

Autoregressive Image GenerationThe success of large language models (LLMs) in NLP has demonstrated their _scalability_ and _universality_ of modeling any complex data, motivating the development of using autoregressive models for image generation. Typically, autoregressive image generation operates on discrete image tokens obtained from vector-quantization (VQ) (Van Den Oord et al., 2017). More precisely, given an image \(^{3 H W}\), we first obtain a sequence of discrete tokens \(_{1:N}=()\) which approximately reconstructs the input with a learned decoder \((_{1:N})\). Then, an autoregressive model is learned to predict the discrete tokens one after another, mirroring the sequential language modeling:

\[_{}^{}=_{n=1}^{N} P_{}(_{n}| _{0:n-1},),\] (1)

where \(\) is the condition (e.g., class, text prompt, etc.), and \(_{0}\) is a special start token. At inference time, we first sample from the learned distribution, and then pass the sampled latents to the decoder(\(\)) to get the final output. Such VQ-based paradigm has been the foundation for various text-to-image (Esser et al., 2021; Yu et al., 2021; Zheng et al., 2022; Yu et al., 2022) and multi-modal generation (Team et al., 2023; Team, 2024).

However, these methods share a common limitation: they primarily rely on discretization, which struggles to capture all the nuances of an image when using a limited length of discrete image token sequence. To generate higher-resolution images, a longer sequence of image tokens is necessary. Yet, this inherently leads to increased capacity demands. For instance, Yu et al. (2022) requires \(20\)B parameters to work properly. Additionally, the left-to-right properties of these autoregressive models prevent the rewriting of previously generated image tokens, resulting in suboptimal image quality.

Diffusion-based Image GenerationDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are latent variable models with a pre-determined posterior distribution and are trained using a denoising objective, which has quickly become the new _de-facto_ approach for image generation. Unlike autoregressive models which predict images as a sequence, diffusion-based models iteratively generate the whole image in a non-autoregressive fashion. Specifically, given an image \(^{3 H W}\) and a signal-noise schedule \(\{_{t},_{t}\}\) where the signal-to-noise ratio (SNR) \((_{t}^{2}/_{t}^{2})\) decreases monotonically with \(t\), we define a series of latent variables \(_{t},t=0,,T\) that adhere to:

\[q(_{t}|)=(_{t};_{t},_{t}^{2}I), q(_{t}|_{s})=(_{t};_{t|s}_{s}, _{t|s}^{2}I),\] (2)

where \(_{0}=\), \(_{t|s}=_{t}/_{s}\), and \(_{t|s}^{2}=_{t}^{2}-_{t|s}^{2}_{s}^{2}\) for \(s<t\). The model then learns to reverse this process using a backward model \(p_{}(_{s}|_{t},)\), which reformulates a denoising objective:

\[_{}^{}=_{t[1,T],_{t} q(_{t}|)}[_{t}||_{}(_{t},)- ||_{2}^{2}],\] (3)

where \(_{}(_{t},)\) is a neural network (typically a UNet (Ronneberger et al., 2015) or Transformer (Peebles and Xie, 2022)) that maps the noisy input \(_{t}\) to its clean version \(\), based on the time step \(t\) and conditional input \(\); \(_{t}^{+}\) is a loss weighting factor. In practice, \(_{}\) can be re-parameterized with noise- or v-prediction (Salimans and Ho, 2022) for enhanced performance, and can be applied on raw pixel space (Saharia et al., 2022; Gu et al., 2023) or latent space (Rombach et al., 2022).

Classifier-free GuidanceAn intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality. For instance, Ho and Salimans (2021) introduced _Classifier-free Guidance (CFG)_, which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination:

\[}_{}(_{t},)=(_{}(_{t},)-_{}(_{t}))+_{}(_{t}),\] (4)

where \(\) is the guidance weight, and \(_{}(_{t})=_{}(_{t},=)\) is the unconditional denoising output. During training, we drop the condition \(\) with certain probability \(p_{}\) to facilitate unconditional prediction. When \(>1\), CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation.

Compared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies. Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models. However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.

## 3 Kaleido Diffusion

We propose Kaleido, a general framework that integrate an autoregressive prior with diffusion model to enhance image generation. As illustrated in Fig. 2, Kaleido comprises two major components: an AR model that generates latent tokens as abstract representations, and a latent-augmented diffusion model that iteratively synthesizes images based on these latents together with the original condition. For following sections, we first describe the importance to introduce additional latents in standard diffusion models (SS 3.1), and show how we can model them with AR models (SS 3.2). The training and inference procedure are described in SS 3.3 and SS 3.4.

### Latent-augmented Diffusion Models

As demonstrated in Ho and Salimans (2021), diffusion with CFG (Eq. (4)) is equivalent to follow

\[_{}_{}(|)=[_{}( p_{}(|)- p_{}())]+ _{} p_{}(),\] (5)which can be interpreted as sampling from a "temperature-adjusted" distribution:

\[_{}(|) p_{}()[p_{ }(|)]^{}, p_{}( |) p_{}(|)/p_{}().\] (6)

Here \(\) can be seen as inverse temperature, which sharpens the conditional distribution \(p_{}(|)\) when \(>1\). That is to say, CFG is crucial as it guides the generation to only focus on high-probability regions, avoiding sampling noise (which tends to have low density). However, sharpening the distribution also reduces the diversity, causing undesirable phenomena like "mode collapse". This is because \(\) (e.g., class label, text prompt, etc.) normally does not contain all the information that describes \(\). Suppose we introduce a hypothetical variable \(\) to represent the "modes" of \(\) which we care most \(-p_{}(|)\), and leave \(p_{}(|,)\) to model other variations including local noise. In this case, CFG will simultaneously sharpen both distributions, considering:

\[p_{}(|)=_{}(| )}}_{}(| ,)}}_{},\] (7)

where standard diffusion models implicitly learn mode selection step together with generation.

Therefore, a natural solution is to **explicitly** model "mode selection" before applying diffusion steps so that the mode distribution will not be distorted by guidance. In this way, the sampling procedure (Eq. (6)) is modified as two steps: \( p_{}(|),_{}(| {z},)\), where CFG can be applied after \(\) is sampled. From the perspective of score function, we rewrite \(_{}(|)\) as \(_{}(|,)\) in Eq. (5):

\[_{}_{}(|,)=[ _{}( p_{}(|)+( |,)}- p_{}())]+_{}  p_{}().\] (8)

Compared to standard diffusion process, the highlighted term above pushes the updating direction towards the sampled modes at each step. This ensures diverse generation as long as \(p_{}(|)\) is diverse.

A Toy ExampleWe visualize the effect of explicitly introducing latent priors using a toy dataset with two main classes, each containing two modes. We compare two models: a standard diffusion model conditioned on the major class ID, and a latent-augmented model incorporating subclass ID as priors. Fig. 3 shows that while the standard diffusion model tends to converge to one mode (subclass) with increased guidance, the latent-augmented model captures all modes, showing the benefit of latent priors for improving diversity under high guidance. In practice, given the challenge of identifying all "modes" in real-world data distribution, we next propose to employ an autoregressive model to universally model various latent modes.

### Autoregressive Latent Modeling

To capture the complex distribution of real images, it is clearly impossible to assign classes for each mode. However, it is non-trivial to determine (1) the best representations for modes \(\); (2) the suitable generative model that can model \(p_{}(|)\). Fortunately, the modes that humans can perceive from an image are largely abstract, and such abstract semantics are easily represented in discrete symbols. For example, we can easily describe content differences through natural language, create composite

Figure 2: Training pipeline of the proposed Kaleido diffusion.

[MISSING_PAGE_FAIL:5]

## 4 Experiments

### Experimental Setups

DatasetWe validate our approach on both class- and text-conditioned image generation benchmarks. For the former, we use ImageNet , and we learn the text-to-image models on CC12M , a large image-text pair dataset where each image is accompanied by a descriptive alt-text. All models are trained to synthesize at \(256 256\). We generate all four types of latents as discussed in Appendix A for both datasets.

Evaluation MetricsTo assess the performance of our models, we employ Frechet Inception Distance (FID)  to capture the overall performance (considering both quality and diversity) of the generated images, and use Recall  to specifically measure the diversity of the generated images. Furthermore, we employ two additional quantitative assessments of diversity: Mean Similarity Score (MSS) and Vendi scores . We use SSCD  as the pretrained feature extractor for calculating both MSS (SSCD) and Vendi (SSCD). Additionally, we utilize DiNOv2  as the feature extractor for Vendi (DiNOv2), based on evidence from  that suggests DiNOv2 provides a richer evaluation of generative models.

Implementation Details and BaselineWe implement Kaleido with Matryoshka Diffusion Models (MDM) , a recently proposed approach that generates images directly in the raw pixel space with efficient training. The default MDM consists of a frozen T5-XL  context encoder and a nested UNet-based denoiser. We initialize the additional autoregressive decoder with the decoder of T5-XL, and make the parameters trainable. The vocabulary is resized to adapt special visual tokens. For fair comparison, we use MDM with the same hyper-parameters as our baseline model, and train both types in almost identical settings on \(64\) A100 GPUs. Additionally, we compare Kaleido with the Condition Annealed Diffusion Sampler (CADS) , a general sampling strategy that enhances the diversity of diffusion models by annealing the conditioning signal during inference. Given that CADS is applicable to different model architectures, we also evaluate CADS integrated with both baseline model MDM (MDM + CADS) and our model (ours + CADS).

### Quantitative Results

Fig. 5 quantitatively compares Kaleido with the baseline diffusion models (MDM) with various guidance scales on ImageNet. Both metrics are evaluated with \(50\)K samples against the full training set, where both our models and the baseline use DDPM sampling with \(250\) steps. Our findings reveal that Kaleido consistently enhances the diversity of samples without compromising their quality across different CFG, evidenced by the general improvement in both FID and Recall. Moreover, while the baseline's FID increases and Recall decreases significantly with higher CFG, Kaleido demonstrates a stead

Figure 4: **A Variety of Discrete Tokens. Original caption: “Dog laying on a human’s lap”**

Figure 5: **Comparison with guidance weights.**To further investigate, we examine image quality and diversity between Kaleido and baseline models. As shown in Table 1, Kaleido outperforms the MDM + CADS combination in terms of FID-50K and precision, demonstrating that our method more effectively maintains high image quality while generating diverse samples. Furthermore, integrating CADS with our model yields the best FID-50K results. Note that precision cannot accurately evaluate models with diverse outputs since a model producing high-quality but non-diverse samples could artificially achieve high precision [Sadat et al.].

Moreover, we assess the diversity of the generated images using \(10\)K samples. Following CADS, we select \(1,000\) random classes from ImageNet and generate \(10\) samples per class. Table 1 shows that both Kaleido and CADS significantly enhance sample diversity. While CADS achieves better performance in diversity, our model maintains superior image quality. Additionally, the methodologies used in CADS are complementary to ours, suggesting potential benefits from integrating CADS with our Kaleido. In fact, incorporating CADS into our model not only further improves image quality but also improves diversity, achieving the best scores in FID-50K, MSS (SSCD), and Vendi (DiNOv2).

Lastly, we provide visual comparisons for class- and text-conditioned image generation in Fig. 11. Notably, we observe that MDM + CADS fails to generate cats of diverse breeds from the prompt "a cat sleeping on the bed." In contrast, Kaleido can produce images of cats from various breeds with more diverse surrounding environments, showcasing its superior diversity capabilities. This observation contrasts with the trend of diversity scores in Table 1, suggesting that these diversity metrics may not fully capture certain aspects of diversity.

### Qualitative Results

Diversity of Generated ImagesWe present a comparative analysis of the images generated by Kaleido against baseline models (MDM). Fig. 7 demonstrates the comparison between baseline models and Kaleido on two conditional generation tasks: the class-conditioned image generation and the text-to-image generation. In both tasks, Kaleido consistently produces more diverse images from identical condition (class or textual description) across varying CFG scales. For instance, in the task of class-to-image generation, the baseline diffusion models generate predominantly frontal views of a "husky" at high CFG, while Kaleido produces diverse images depicting huskies in various poses and numbers. A similar improvement in diversity is observed in the text-to-image generation as well, highlighting the robustness of Kaleido in generating diverse images under identical conditions.

Control from Latent TokensWe show the efficacy of latent variables in guiding the image generation process in Fig. 6. Fig. 6 demonstrates images generated with different types of latent variables: (a) textual descriptions, (b) object blobs, (c) detection bounding boxes, (d) visual tokens, and (e) combined latents, which integrate textual descriptions, detection bounding boxes and visual tokens. We visualize the generated latents tokens alongside the resulting images, showing how closely the images generated by Kaleido align with the latent tokens. Such alignment is evident in fine-grained visual information - such as object appearance, background, and atmosphere -, spatial location and orientation of different objects, and the stylistic elements of generated images. This alignment confirms that Kaleido can effectively interpret and utilize generated latent variables to guide and refine the image generation process.

Latent EditingFig. 8 showcases the impact of latent editing in image generation. The first row displays images generated using autoregressively produced latent tokens. In the second row, we demonstrate the effect of manual modifications to the textual descriptions: changing "log" to "cobblestones" and "a body of water" to "forest". These changes result in a modified image where a frog is now positioned on cobblestones with a forest background. Additionally, by further augmenting the bounding box of a cup to a different position, we observe that the cup's position in the image changes accordingly, while most other visual elements remain unchanged. The precise control of image characteristics via latent editing underscores Kaleido's flexibility and controllability, offering

  
**Model** & **FID-50K \(\)** & **Precision \(\)** & **Recall \(\)** & **MSS (SSCD) \(\)** & **Vendi (SSCD) \(\)** & **Vendi (DiNOv2) \(\)** \\  MDM & 15.5 & **0.93** & 0.22 & 0.21 & 8.42 & 3.04 \\ MDM + CADS & 10.6 & 0.60 & **0.62** & **0.12** & **9.28** & 4.72 \\ Ours & 9.0 & 0.85 & 0.42 & 0.16 & 8.82 & 3.79 \\ Ours + CADS & **5.9** & 0.76 & 0.52 & **0.12** & 9.21 & **4.83** \\   

Table 1: **Comparison of quality and diversity on ImageNet.** FID-50K, Precision, and Recall are evaluated on 50K samples, while MSS and Vendi scores assess diversity on 1K \(\) 10 samples.

a powerful interactive interface for users to customize the generated images. Furthermore, the high fidelity of the re-generated images to their original versions indicates Kaleido's potential for applications requiring personalization or customizations.

## 5 Related Work

Augmenting Diffusion ModelsVarious enhancements have been proposed to improve the versatility and controllability of diffusion models with augmented latents. Innovations such as Diffusion AE (Preechakul et al., 2022) integrates diffusion models with a learnable encoder that extracts high-level semantics and enables the diffusion model to add details directly in image space. Further efforts have focused on incorporating specific control signals, such as bounding boxes, layout, and segmentation masks to guide and control the image generation process. (Balaji et al., 2022; Li et al., 2023; Zheng et al., 2023; Hu et al., 2023). Recently, BlobGen (Nie et al., 2024) proposes to ground existing text-to-image diffusion models on object blobs - tilted ellipses that capture spatial details of the objects - for compositional generation. While these approaches improve the models' capacity to

Figure 6: **Example of generation with various latents.**. This figure showcases images generated with different types of latents: (a) textual descriptions, (b) object blobs, (c) detection bounding boxes, (d) visual tokens, and (e) combined latents (textual descriptions + detection bounding boxes + visual tokens). Each row shows two sets of generated images sampled with one type of latents. Each set displays a visualization of the generated latents tokens (left) and a collage of images (right) sampled using the same latent tokens but different noises. The image tokens capture visual details difficult to convey through text, such as artistic style.

adhere to specified spatial layouts, they often necessitate modifications to the attention mechanism, potentially limiting their generality. In contrast, our method enhances the generative capabilities of diffusion models without altering the model architecture.

Connecting Diffusion Models with LLMsThe remarkable success of Large Language Models (LLMs) and diffusion models has spurred interest in connecting these models, aiming to leverage the capabilities of LLMs in understanding and generating complex data and combine it with the powerful image synthesis capabilities of diffusion models (Ge et al., 2023; Zheng et al., 2023; Sun et al., 2023). Ge et al. (2023); Zheng et al. (2023) propose image tokenizers that encodes images into

Figure 7: **Diversity comparison to standard diffusion model. Images sampled under varying CFG scales (\(\)). Panels (a) and (c) display images from the baseline models, while panels (b) and (d) show images from Kaleido. From top to bottom, as the CFG increases, the standard diffusion models exhibit reduced diversity, while Kaleido consistently maintains diversity across guidance scales.**

visual tokens, enabling multimodal language modeling. This line of work focuses on empowering LLM with image generation ability by aligning its output embedding space with the pre-trained diffusion models. Our work leverages the LLMs' robust capabilities in textural understanding and generation to model the generation of abstract latents from the original text. These latents are then integrated with latent-augmented diffusion model, enabling a more interpretable and diverse image generation process.

Our approach also distinguishes itself from the re-captioning method introduced in DALL-E 3 (Betker et al., 2023). Unlike re-captioning, which typically replaces the original captions with more descriptive captions, our method retains the original condition and supplements it with latent variables of various forms (beyond textual captions like bbox, blob and "vokens"). The sampled latents serves as a unifying interface for various types of inputs, and introduce diversity compared to recaptioning where no sampling is involved at inference time.

## 6 Conclusion

In this work, we address the challenge of improving sample diversity under high CFG in diffusion models. We introduce Kaleido Diffusion, which combines an autoregressive prior with a latent-augmented diffusion model. Results show Kaleido increases diversity without compromising quality, even at high CFG. With human interpretable latent tokens, Kaleido offers an explainable mechanism behind the image generation process and provides a fine-grained editable interface, enabling precise user control over the generated images.

Figure 8: **Effect of sequential latent editing. The top row displays images generated with autoregressively produced latent tokens. The middle row shows the re-generated images after applying latent editing to the textural description, and the bottom row presents re-generated images after further edits to the bounding box, showing the impact of step-by-step latent editing.**