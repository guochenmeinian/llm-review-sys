# Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning

Di Ming, Peng Ren, Yunlong Wang, Xin Feng

School of Computer Science and Engineering, Chongqing University of Technology

Chongqing, China

diming@cqut.edu.cn, misterr_2019@163.com, {ylwang, xfeng}@cqut.edu.cn

Corresponding Author: Xin Feng

###### Abstract

Vision transformers (ViTs) perform exceptionally well in various computer vision tasks but remain vulnerable to adversarial attacks. Recent studies have shown that the transferability of adversarial examples exists for CNNs, and the same holds true for ViTs. However, existing ViT attacks aggressively regularize the largest token gradients to exact zero within each layer of the surrogate model, overlooking the interactions between layers, which limits their transferability in attacking black-box models. Therefore, in this paper, we focus on boosting the transferability of adversarial attacks on ViTs through adaptive token tuning (ATT). Specifically, we propose three optimization strategies: an adaptive gradient re-scaling strategy to reduce the overall variance of token gradients, a self-paced patch out strategy to enhance the diversity of input tokens, and a hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples. In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability. On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches. Extensive experiments conducted on ViTs, undefended CNNs, and defended CNNs validate the superiority of our proposed ATT attack method. On average, our approach improves the attack performance by 10.1% compared to state-of-the-art transfer-based attacks. Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs. Code is available at https://github.com/MisterRpeng/ATT.

## 1 Introduction

The Vision Transformer (ViT)  was the first to apply the transformer architecture to computer vision models, demonstrating excellent performance in image classification tasks. Since then, various ViT-based transformer structures have shown comparable success in a range of computer vision tasks, including object detection [2; 3], semantic segmentation [4; 5], and human pose estimation . However, similar to convolutional neural networks (CNNs), ViT models are also vulnerable to adversarial attacks [7; 8; 9; 10]. For ViT to be widely adopted in real-world applications, particularly in secure systems, it is crucial to identify model weaknesses [11; 12; 13] and develop more robust ViT models. Specifically, adversarial attacks generate perturbations that can cause incorrect model classifications, which are often too subtle for humans to notice. The transferability of the adversarialperturbations leads to the fact that the perturbations can directly deceive the unknown target model. Therefore, it is imperative to study transferable ViT attacks to further uncover the vulnerabilities of ViT models, thereby providing valuable insights for adversarial defense or building robust ViTs.

Gradient regularization-based adversarial attack adjusts the gradient after back-propagation to update the perturbation more stably and effectively. However, attack methods designed specifically for CNNs are less effective when directly applied to ViTs. Token gradient regularization (TGR)  addresses the shortcomings of existing gradient regularization methods and designs a technique that regularizes the gradient of intermediate ViT layers, leading to a significant improvement of the transferability of adversarial attacks on ViTs. However, we observe that TGR simply sets the largest part of token gradients in each module to zero, thereby losing feature information to a certain extent. To solve this problem, we propose an adaptive variance reduction method, not only correcting the gradient variance but also preserving the feature information (see the bottom-right of Fig. 1). Thus, the perturbation can learn more features while reducing overfitting and improving the transferability of adversarial examples across ViT models with various network structures.

Another ViT attack approach, proposed by Wei _et al._, inspired us to incorporate input diversity with gradient regularization to enhance the transferability further. This transfer-based ViT method proposes the PatchOut Attack to exploit the input diversity . PatchOut is a straightforward and effective way to address the overfitting. However, this approach does not account for update efficiency and difficulty, leading to potential over-discarding of updates. As a result, the finite number of iterations may end with a perturbation that has not reached its optimal state. To address this, we propose a discard strategy that combines feature importance-based sampling and self-paced learning. As shown on the left side of Fig. 1, this approach allows for a more rational implementation of patch out strategy and improves the update efficiency.

Influenced by [15; 17], we found out that the attention mechanism specific to ViTs tends to cause the perturbation overfitting in surrogate models. To resolve this, we propose an attention weakening strategy to further adaptively tune the token gradients both within and across ViT layers. Specifically, as indicated in the top-right of Fig. 1, we truncate the gradient back-propagation in the attention module of deep ViT layers while preserving the token gradients of QKV and MLP modules, to reduce the overfitting caused by excessive global attention. In addition, for purpose of balancing the interactions between different modules in the remaining shallow ViT layers, we also adjust the token gradients of the attention, QKV and MLP modules accordingly.

In summary, the main contributions of our paper include:

* We propose an adaptive token gradient re-scaling method to improve the transferability of adversarial attacks. This method aims to reduce the overall variance of token gradients in each ViT layer and smooth token gradient variance changes throughout all the ViT layers. Meanwhile, we introduce a hybrid token gradient truncation method to weaken the effectiveness of attention mechanism. In order to achieve a balance between different modules, we further adjust the truncation factors of each module accordingly.
* We propose a self-paced patch out strategy for learning the input diversity. During the perturbation training, a sparse mask is constructed under the semantic guidance of patch-level feature importance. As the iterations increase, self-paced learning strategy is employed to generate a progressively more sampled mask. These sparse masks are then superimposed with perturbations as input samples, and the perturbation updates are incrementally enhanced through this self-paced training process, which not only mitigates the overfitting issue but also enhances the attack efficiency.
* Extensive experiments are conducted to verify the effectiveness of our proposed adaptive token tuning attack method, which improves the attack success rate by an average of 10.1% over state-of-the-art methods under black-box attack setting, including an average of 4.9% for attacking ViTs and 12.8% for attacking CNNs. Additionally, we achieves the highest average attack rate of 58.3% on defended CNNs, further demonstrating the superior transferability of our ATT attack approach in both undefended and defended black-box models.

## 2 Related Work

Current adversarial attacks can be broadly divided into two main categories: white-box and black-box attacks. White-box attack refers to adversarial attack under the condition that all the model information is known. On the contrary, black-box attacks do not have unrestricted access to all model's information. Specifically, black-box attacks consist of two types: query-based black-box attacks, which could access the output of the target model, and transfer-based black-box attacks, where no knowledge of the target model is available. In practice, it is often impossible to obtain any information about the target model, making transfer-based black-box attacks a more serious security threat. Therefore, this paper focuses on transferable black-box attacks on ViT models.

**Adversarial Attacks on CNNs.** Adversarial attacks on CNNs can be categorized into gradient optimization-based and feature information-based attacks. In gradient optimization-based adversarial attacks, [18; 19; 20] incorporated a momentum term into the existing iterative attack update process  to stabilize the update direction and avoid the local optima. Towards improving the backpropagation path, Skipping Gradient Method (SGM)  allowed gradients to backpropagate more through skip connections, while Backward Propagation Attack (BPA)  mitigated the negative impact of gradient truncation in ReLU and max-pooling to improve the relevance between the gradient and the input. Besides, [24; 25] introduced variance adjustment methods that modify the current gradient based on the variance of gradients from the previous iteration and from ensemble surrogate models respectively, resulting in more stable gradient updates. In comparison, feature information-based adversarial attacks, such as FIA  and FDA , aim to degrade model performance by disrupting important features that play a critical role in the model's decisions. Since inaccurate feature importance guidance could lead to a decrease in transferability, [28; 29] proposed the neuron attribution-based attack methods that utilized more accurate neuron importance for feature-level attacks. Furthermore, there were also other approaches [30; 31; 32] that utilized feature importance to create masks that guide perturbation updates, enhancing transferability by focusing on more critical features during attacks. In addition to that, [33; 34] specifically targeted feature information from shallow layers to craft more transferable perturbation against the fine-tuned models.

**Adversarial Attacks on ViTs.** However, attack methods designed for CNNs have very low transferability to ViT models. To improve the attack performance against ViTs, Nasser _et al._ proposed the self-integration (SE) method and token refinement module (TR). Specifically, SE leverages the classification header at each layer of the ViT model to produce adversarial perturbations, while TR further refines the classification token based on SE to enhance the attack transferability. Wei _et al._ approached the problem from the perspective of attention, proposing a method to skip the attention module, which significantly improves the transferability of adversarial examples. Additionally, they proposed a random extraction of perturbation patch strategy from the perspective of image transformation to further enhance the transferability. Zhang _et al._ introduced a virtual dense connectivity approach, enabling the backpropagation of gradients through jump connections in a deep network. Gao _et al._ developed a feature diversity-based attack that uses adversarial perturbations to accelerate feature collapse due to the ViT attention module. Zhang _et al._ proposed a token gradient regularization approach to reduce gradient variance during backpropagation in a token-wise manner, based on the structural features of ViTs, to improve the transferability of adversarial perturbations. By contrast, our adaptive token tuning method not only considers the token gradient variance but also aims to reduce the destruction of feature diversity to preserve the original feature information. Additionally, we design a self-paced patch out strategy to enhance both the transferability and efficiency of adversarial attacks.

Figure 1: The overall framework of our **A**daptive **T**oken **T**uning (**ATT**) attack method.

## 3 Methodology

### Preliminary

**Notations and Definitions.** A benign sample is defined as the original clean image \(^{C H W}\) along with its corresponding ground-truth label \(y\{1,2,,K\}\), where \(C\), \(H\), \(W\), and \(K\) represent the number of channels, height, width of the image, and the number of label categories, respectively. In ViT networks, the image \(\) is evenly partitioned into a set of patches \(_{p}=\{_{p}^{1},_{p}^{2},,_{p}^{n}\}\), where \(_{p}^{i}^{C P P}\) denotes the \(i\)-th patch and \(n=HW/P^{2}\) denotes the number of patches. The set of ViT modules is defined as \(=\{,,\}\), and the total number of ViT layers is \(L\). Given a surrogate ViT model \(f\), the predicted label for the input \(\) is represented by \(=f()\), where \(=y\) for benign sample \(\). Consistent with previous works, we focus on fooling ViT models in the untargeted attack setting. To guarantee the effectiveness and imperceptibility of adversarial example \(_{adv}=+\), adversarial attack maximizes the following optimization problem:

\[*{arg\,max}_{}\ (f(+),y), \ _{}\] (1)

where \(\) is the loss function, _e.g._, cross-entropy, and \(\) is a constant to constrain the adversarial perturbation \(\) in the \(_{}\)-norm bound, thereby resulting in a change of label that \(f(_{adv}) y\).

**Patch Out.** Motivated by previous work , Wei _et al._ proposed the PatchOut attack to alleviate the overfitting phenomenon by creating diverse input patterns. Specifically, a portion of patches in the adversarial perturbation are randomly discarded using a binarized attack mask \(\), where the values of selected/unselected patches are set to ones/zeros respectively. During the feed-forward process, the input to surrogate model \(f\) becomes as \(+\), thus enhancing the input diversity.

**Token Gradient Regularization.** As compared to gradient variance tuning  applied directly to the input, Zhang _et al._ regularized the gradient variance of tokens in the intermediate ViT blocks. Through the back-propagation in ViT layers, token gradients are multiplied by a scaling factor \(s\) and \(k\) extreme values are further reset to zeros. As a result, adversarial perturbation is updated by a regularized input gradient, _i.e._, \(TGR(_{},s,k)\), resulting in a more stable optimization process.

### Adaptive Variance Reduced Token Gradient

During the training, a large gradient variance tends to overfit the surrogate model and causes the update of the perturbations to fall into a local optimal solution, which lowers the transferability of adversarial attack. Wang _et al._ propose a variance tuning method to reduce the gradient variance in the input perturbation space, but ignoring the gradient variance in intermediate layers. Thus, we aim to reduce the overall gradient variance across all ViT layers to improve training effectiveness.

**Variance Reduction in a Single ViT Layer.** Given the module \(m\) in an intermediate \(l\)-th ViT layer, the latent representation of \(i\)-th token is defined as \(_{i}^{(l,m)}\) and its corresponding gradient _w.r.t._ the loss function \(\) in Eq. 1 is defined as \(_{i}^{(l,m)}=/_{i}^{(l,m)}\). During the back-propagation, TGR  searches the largest gradient \(_{j}^{(l,m)}\) out of \(n\) token gradients via \(j=*{arg\,max}_{i\{1,,n\}}_{i}^{(l,m)}\) for each module \(m\), and simply sets \(_{j}^{(l,m)}=0\) to reduce the variance of token gradients in \(l\)-th ViT layer. However, this largest gradient could be highly correlated with important features. As a consequence, iterative optimization updates may overlook this portion of semantic information, undermining the quality of the perturbations.

To simultaneously reduce the token gradient variance and preserve the feature information from the largest token gradient, we decrease the largest token gradient mildly by a gradient penalty factor \((0,1)\). For \(m=\) or \(m=\), \(_{i}^{(l,m)}=_{i}^{(l,m)}\). Following the TGR  method, for \(m=\), we also search for the set \(\) of extreme token gradients located in the same row or column as the largest token gradient. Thus, all the token gradients in \(\) are re-scaled by \(_{i}^{(l,m)}=_{i}^{(l,m)}\) for \(i\), since they are highly correlated with the largest token gradient. As can be seen, this mild re-scaling strategy allows some important feature information in the largest or extreme token gradients (_i.e._, nonzero gradient values) to back-propagate between consecutive ViT layers in certain degree. The detailed analysis of the change in the token gradient variance (before and after mild re-scaling by \(\)) is provided in Appendix A.

**Adaptive Variance Reduction Throughout ViT Layers.** If re-scaling the token gradient of each intermediate ViT layer independently, after back-propagation, the gradient _w.r.t._ the input perturbation could be deviated from the original updating direction. Inspired by [38; 39], we utilize the gradient information across ViT layers to adaptively re-scale the token gradients in each ViT layer. To be specific, the last ViT layer is selected as the anchor, since it is most relevant to the classification task. Then, the largest token gradient (or the set of extreme token gradients) in each ViT layer is adaptively re-scaled to ensure its gradient variance remains consistent with previous ViT layer. Thus, for \(t\)-th iteration, we define the adaptive variance reduce token gradient method as follows:

\[_{i,t}^{(l,m)}=_{i,t}^{(l,m)}+1- _{t}^{(l,m)}/_{t}^{(l+1,m)}},\] (2)

where \(_{t}^{(l,m)}\) and \(_{t}^{(l+1,m)}\) denote the variance of token gradients for module \(m\) in \(l\)-th and \((l+1)\)-th layers respectively, _i.e._, \(_{t}^{(l,m)}=(^{(l,m)})\) and \(_{t}^{(l+1,m)}=(^{(l+1,m)})\), and \(\) is the adaptive factor balancing the relative importance between the gradient penalty factor and the ratio of gradient variances. With this adaptive updating strategy, the variances of token gradients between consecutive ViT layers become smoother compared to using a fixed constant value for re-scaling token gradients. Appendix B provides further details regarding the analysis of adaptive variance reduced token gradient throughout ViT layers with different parameter settings.

### Self-Paced Patch Out under Semantic Guidance

Undoubtedly, random discarding  is a simple and effective method to alleviate the overfitting and improve the transferability. However, discarding patches inappropriately raised by randomness could lead to the difficulty in updating the perturbation, especially when limiting the total number of iterations to a small value, _e.g._, only 10 iterations are commonly used in the literature of ViT attack. Thus, within a limited training budget, we propose a self-paced patch out strategy under semantic guidance to prevent the improper discarding of important patches to a certain degree. In addition to that, the number of discarded patches are dynamically controlled by a scheduled pace to further stabilize the perturbation training.

**Generating Semantic Guided Sparse Mask.** Instead of generating a completely random mask, we leverage the rich semantic information to mitigate the optimization instability caused by discarding perturbation patches. Based on Grad-CAM , we construct the feature importance matrix \(^{H W}\) by fusing gradients and features from an intermediate ViT layer, _i.e._, \(=_{i=1}^{C^{(l)}}_{i}^{(l)}_{i}^{(l)}\), where \(C^{(l)}\) is the number of channels in \(l\)-th layer and \(l(0,L)\). According to the partition of \(_{p}\), we further define the patch version of \(\) as \(_{p}=\{_{p}^{1},,_{p}^{n}\}\), where \(_{p}^{i}^{P P}\). Thus, the feature importance of \(i\)-th patch \(_{p}^{(i)}\) can be measured by the Frobenius norm \(||_{p}^{i}||_{F}\).

Furthermore, patch-level feature importance can be combined with random sampling to discard a portion of patches under semantic guidance. First, we define \(_{p}\) as a \(C P P\) tensor with all ones and the scaled patch-level feature importance as \(^{C H W}\), where \(_{p}^{i}=(||_{p}^{i}||_{F}-_{j}(||_{p}^{j}||_{F}))/( _{j}(||_{p}^{j}||_{F})-_{j}(||_{p}^{j}||_{F}))_ {p}\) for \(i,j\{1,,n\}\). To control the number of discarded patches, we further introduce \( 0\) and \( 0\) as scaling and offset coefficients to shift the distribution of \(\). Then, the semantic guided sparse mask \(\) can be generated by:

\[=(<-),\] (3)

where \(^{C H W}\) is a random variable sampled from the patch-level uniform distribution \(_{p}(0,1)\). In detail, for \(i\)-th patch of \(\), we have \(_{p}^{i}=_{p}\) where \((0,1)\). As can be seen, the elements within a patch will be discarded or preserved altogether with the same probability. Additionally, patches with lower feature importance are more likely to be discarded, while those with higher feature importance are less likely to be discarded.

**Self-Paced Patch Out via Progressive Sparse Mask.** To further improve both the efficiency and the effectiveness of training adversarial perturbations, a self-paced patch out strategy is introduced to control the number of discarded patches for each iteration at a dynamic pace. For \(t\)-th iteration, we define the progressive sparse mask \(_{t}\) based on feature importance \(_{t}\) as follows:

\[&_{t}=|=(_{t}< _{t}-),\;_{t}_{p}(0,1),\;_{t}=1- (1-)},\\ & N_{p}(_{1})<N_{p}(_{2})<<N_{p}(_{T}), \] (4)where \(T\) is the number of iterations, \(_{t}\) is the random variable sampled from \(_{p}(0,1)\), and \(N_{p}(_{t})\) is the number of discarded patches in \(_{t}\), approximately equal to \([1-_{t}+)]/(C P^{2})\).

To be specific, as the iteration \(t\) increases, \(_{t}\) becomes to \(\) gradually, and more perturbation patches are discarded such as \(N_{p}(_{t})<N_{p}(_{t+1})\) to prevent the overfitting. In the meanwhile, training samples with varying patterns are fed into the surrogate ViT model for purpose of enhancing the diversity of inputs. On the other hand, the coefficients \(\) and \(\) can be adjusted correspondingly to guarantee that the strength (_i.e._, the magnitude) of adversarial perturbations increases step by step, _e.g._, \(||_{t}_{t}||_{1}||_{t+1}_{t+1} ||_{1}\). For a fair comparison, we also set appropriate values for \(\) and \(\) to generate progressive sparse masks, ensuring that the total number of discarded patches throughout the training process is greater than or at least equal to that of other methods such as PNA . Thus, we have \(_{t=1}^{T}[1-_{t}+)]/(C P^{2}) T  N_{dpatch}\), where \(T\) is the number of iterations and \(N_{dpatch}\) is the fixed number of patches discarded by comparison methods per iteration. To summarize, our proposed self-paced patch out effectively integrates self-paced learning into training process and maximizes the adaptive loss function \(_{t}=(f(+_{t}),y)\) at each iteration, which can further improve the transferability of crafted adversarial perturbations across various models.

### Weakening the Effectiveness of Attention Mechanism

Feature-based adversarial attacks [26; 29; 33; 34] have shown that not all the features in DNNs contribute positively to the generation of adversarial perturbations, despite their significances in classification tasks. On the other hand, it is demonstrated in previous ViT studies [15; 17] that attention modules in certain ViT layers are redundant for image classification and adversarial perturbation generation, which could cause the overfitting phenomenon. Building upon this, we propose to reduce the impact of some attention modules during the training of adversarial perturbations.

**Truncated Attention Layers.** According to [41; 35], shallow layers exploit generic properties of attention. whereas deep layers exploit highly model-specific properties of attention. To mitigate the overfitting caused by excessive global attention, we introduce a hard truncation strategy for deep ViT layers. During back-propagation, we multiply the token gradient \(g_{i}^{(l,m)}\) with a truncation factor \(\) for module \(m=\). To be specific, we set \(^{(l,m)}\) to \(0\) for \(l\{l^{}+1,,L\}\), while setting \(^{(l,m)}\) to non-zero value for \(l\{1,,l^{}\}\). As a result, intermediate gradients can be back-propagated to each token individually via skipping attention mechanism, thereby reducing the adversarial perturbation's dependence on complex interactions between tokens in deep ViT layers.

**Hybrid Token Gradient Truncation.** For purpose of effectively balancing the influence of different modules on the perturbation training, we further introduce a hybrid token gradient truncation method to constrain the token gradient in each module \(m\) throughout all ViT layers using the predefined set \(_{}^{(m)}\) of \(L\) truncation factors. For \(m=\), we set \(_{}^{(m)}=\{^{(1,m)},,^{(l^{},m)},0,,0\}\), where deep ViT layers are processed via the hard truncation (_i.e._, \(^{(l,m)}=0\), \(l>l^{}\)), and shallow ViT layers are processed via the soft truncation (_i.e._, \(^{(l,m)}>0\), \(l l^{}\)). For either \(m=\) or \(m=\), we set \(_{}^{(m)}=\{^{(1,m)},,^{(l^{},m)},^{(l^ {}+1,m)},,^{(L,m)}\}\), where all the intermediate ViT layers are processed via the soft truncation (_i.e._, \(^{(l,m)}>0\), \(l\{1,,L\}\)). In addition to that, for any \(l\)-th ViT layer (\(1 l l^{}\)), by setting \(^{(l,)}<(^{(l,)},^{(l,)})\) between different ViT modules, we can continue to weaken the effectiveness of attention mechanism during the perturbation training process.

In sections 3.2-3.4, we introduced the details of our proposed adaptive token tuning (ATT) attack method from three perspectives. On the one hand, both variance reduction and hybrid truncation aim to adaptively tune the token gradient throughout the back-propagation path. On the other hand, self-paced patch out focuses on adaptively tuning the token diversity in the input perturbation space. The overall optimization algorithm for training adversarial example is provided in Appendix C.

## 4 Experiments

We utilized different surrogate models for comparison, demonstrating versatility and effectiveness of our ATT attack. Ablation experiments were conducted to verify the effectiveness of each component of our approach. Additionally, we analyzed experimental results by examining gradient variance and feature information, providing a statistical explanation for the effectiveness of our method.

### Experiment Setup

**Dataset.** We followed the baseline approach  by selecting 1,000 random images from different categories in the ILSVRC2012  validation set. All surrogate models can classify the images in our chosen dataset with near-perfect accuracy.

**Models.** We chose four representative ViT models as surrogate models to produce adversarial samples: ViT-B/16 , PiT-B , CaiTS/24 , and Visformer-S . Considering structural differences between ViTs and CNNs, we divided attack scenarios into two categories: attacking ViTs and attacking CNNs. For ViT attack, we used four ViT models as target models to test the transferability of adversarial samples: DeiT-B , TNT-S , LeViT-256 , and ConViT-B . Similarly, we selected four CNN models as target models to verify the transferability of adversarial samples: Inception-v3 (Inc-v3) , Inception-v4 (Inc-v4) , Inception-ResNet-v2 (IncRes-v2) , and ResNet-v2-152 (Res-v2) . To validate the effectiveness against defense models, we further selected three adversarially trained defense models: Inception-v3-ens3 (Inc-v3ens3), Inception-v4-ens4 (Inc-vdens4), and Inception-ResNet-v2 (IncRes-v2adv) .

**Baseline Methods.** Since our approach takes into account the optimization of the gradient, we selected several methods closely related to ours as baselines, including MI-FGSM (MIM) , VMI-FGSM (VMI) , and SGM . To demonstrate the superiority of our approach over state-of-the-art ViT attacks, we used PNA  and TGR  for comparison. Our approach also proposes a superior self-paced input diversity method; therefore, we used PatchOut  as the baseline.

**Evaluation Metrics.** Consistent with the baseline methodology , we used the attack success rate (ASR) as the evaluation metric in all transferability comparison experiments. Additionally, we defined the number of iterations that led to the first-time misclassification by the model as \(t\), which we used as a metric for efficiency comparison within a limited training budget \(T\). For ASR, higher values (\(\)) indicate better transferability, while for \(t\), lower values (\(\)) indicate better efficiency.

**Parameters.** We kept all known parameter settings consistent with . The maximum perturbation amplitude was set to \(=16\), and the number of training iterations was set to \(T=10\), resulting in a step size of \(==1.6\) for each perturbation update. All comparison methods used momentum as the stabilization update strategy with decay factor \(=1.0\). Hyperparameters specific to each method were kept the same as those set by original methods. The penultimate ViT layer (_i.e._, \(l=L-1\)) is selected to generate patch-level feature importance. We set appropriate values for \(\) and \(\) to ensure the expected value of the number of discards in our method was greater than or equal to the number of PatchOut discards, where its optimal \(N_{detach}\) is \(130\). We adjusted the images of the whole dataset to \(224 224\) and set the patch size to \(16 16\). For the adaptive gradient variance reduction strategy, we set the gradient penalty factor to \(=0.5\) and the adaptive factor to \(=0.01\). Truncation factor \(\) is finetuned with appropriate values to balance different modules for each surrogate model.

### Evaluating the Transferability

In this section, we verified the transferability of adversarial perturbations on four ViTs, four undefended CNNs, and three defended CNN models. Specifically, we produced adversarial samples using the four ViT surrogate models, tested the attack success rate on all black-box models, and calculated the average attack success rate across all black-box models (abbreviated as \(_{bb}\)).

Firstly, we focus solely on the ViT attack using "Token Gradient-Based Optimization". Here, all comparison methods and "Ours" indicate the pure gradient-based attack "Without Input Diversity Enhancements". In Table 1, we verified the transferability of our method on ViTs. The experimental results indicated that our method achieved nearly 100% attack success rate under white-box settings. In addition, the transferability of our method performed significantly better than all other baseline methods, with an average increase of 6.4% in the attack success rate of black-box models. All baseline methods optimized the gradient and achieved good attack performance, especially TGR, improving the transferability by a large margin. However, TGR directly set the token with the maximum gradient to zero, preventing the perturbation from learning potentially important feature information. In contrast, our method greatly preserved the original feature information while avoiding the overfitting phenomenon caused by excessively large gradients.

Furthermore, we validated the transferability of our method from ViT models to undefended and defended CNN models. Experimental results on CNN models are shown in Table 2, where the

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

### The Analysis of Attack's Effectiveness and Efficiency

We utilized ViT-B/16 to generate adversarial examples and retained intermediate results for 10 iterations. Fig. 2 shows the process of discarding perturbation patches for "Ours+SPPO" versus "TGR+PO", and the classification results (label and probability). For each iteration, class activation map of adversarial example was generated by GradCAM on CaiT-S/24 with an unlabeled setup. The results showed that our method required only 5 iterations for the model to determine the error, while TGR required 9 iterations. Class activation maps also reflected this change. After 10 iterations, the confidence level of wrong label due to our attack was higher, indicating that our attack was not only more efficient but also stronger. Additionally, considering that each iteration increases a fixed perturbation magnitude, our method required a smaller perturbation to mislead the model. More efficiency results tested in different surrogate models are provided in Appendix D.1 and D.3.

## 5 Conclusion

In this paper, we propose an adaptive token tuning attack method to enhance the transferability of ViT attacks. Unlike previous gradient-based attacks, our method puts more emphasis on smoothing the token gradient variance between different layers and preserving important features along with the back-propagation. Guided by patch-level feature importance, we introduce a self-paced discarding strategy from the perspective of input diversity, where the number of discarded perturbation patches is gradually increased. To further improve transferability, we propose a hybrid truncation strategy to reduce overfitting in the attention mechanism. Extensive experiments show that our adaptive token tuning attack method has superior transferability and efficiency.

## 6 Limitations and Broader Impacts

Although our experimental results verified the effectiveness of the proposed adaptive token tuning strategy in enhancing the transferability of ViT attacks, the relationship between gradient variance reduction and transferability still lacked theoretical support. Existing research suggested that more generalized feature information could improve perturbation's transferability, and our work focused on leveraging this by reducing token gradient variance. In future work, we will continue exploring from a theoretical perspective to provide valuable insights into adversarial attacks.

If the proposed ATT attack method is maliciously used in real-world applications, it could lead to security concerns, representing one of its potential negative social impacts. Due to the superior performance of pretrained models and the significant time cost associated with training from scratch, many applications opt to fine-tune pretrained models. As a result, this undoubtedly exposes applications to a risky environment vulnerable to attacks. Our research aims to encourage deep learning practitioners to further explore security concerns related to model vulnerabilities, and in return, offering constructive guidance for adversarial defense.

Figure 2: The attack efficiency of “Ours+SPPO” versus “TGR+PO”. Our ATT attack makes the model predict wrong faster and ends up generating error labels with a higher level of confidence.