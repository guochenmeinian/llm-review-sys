# Large-Scale Distributed Learning via Private

On-Device Locality-Sensitive Hashing

 Tahseen Rabbani

Department of Computer Science

University of Maryland

trabbani@umd.edu

&Marco Bornstein

Department of Computer Science

University of Maryland

marcob@umd.edu

&Furong Huang

Department of Computer Science

University of Maryland

furongh@umd.edu

These authors contributed equally to this work.

###### Abstract

Locality-sensitive hashing (LSH) based frameworks have been used efficiently to select weight vectors in a dense hidden layer with high cosine similarity to an input, enabling dynamic pruning. While this type of scheme has been shown to improve computational training efficiency, existing algorithms require repeated randomized projection of the full layer weight, which is impractical for computational- and memory-constrained devices. In a distributed setting, deferring LSH analysis to a centralized host is (i) slow if the device cluster is large and (ii) requires access to input data which is forbidden in a federated context. Using a new family of hash functions, we develop one of the first private, personalized, and memory-efficient on-device LSH frameworks. Our framework enables privacy and personalization by allowing each device to generate hash tables, without the help of a central host, using device-specific hashing hyper-parameters (_e.g._ number of hash tables or hash length). Hash tables are generated with a compressed set of the full weights, and can be serially generated and discarded if the process is memory-intensive. This allows devices to avoid maintaining (i) the fully-sized model and (ii) large amounts of hash tables in local memory for LSH analysis. We prove several statistical and sensitivity properties of our hash functions and experimentally demonstrate that our framework is competitive in training large-scale recommender networks compared to other LSH frameworks which assume unrestricted on-device capacity.

## 1 Introduction

Locality-sensitive hashing (LSH) has proven to be a remarkably effective tool for memory- and computationally-efficient data clustering and nearest neighbor search . LSH algorithms such as SimHash  can be used to search for vectors in collection \(W^{d}\) of massive cardinality which will form a large inner product with a reference vector \(x^{d}\). This procedure, known as maximum inner product search (MIPS) , has been applied to neural network (NN) training. In NN training, the weights of a dense layer that are estimated to produce a large inner product with the input (thereby, a large softmax, for example) are activated while the remainder is dropped out.

While LSH-based pruning greatly reduces training costs associated with large-scale models, popular frameworks such as SLIDE  and Mongoose  cannot be deployed in distributed settings overmemory-constrained devices such as GPUs or mobile phones for the following reasons: **(a)** required maintenance of a large target layer in memory and **(b)** access to the input is needed to conduct LSH.

With many modern NN architectures reaching billions of parameters in size, requiring resource-constrained devices to conduct LSH analysis over even part of such a large model is infeasible as it requires many linear projections of massive weights. The hope of offloading this memory- and computationally-intensive task to a central host in the distributed setting is equally fruitless. LSH-based pruning cannot be conducted by a central host as it requires access to either local client data or hashed mappings of such data. Both of these violate the fundamental host-client privacy contract, especially in a federated setting . Therefore, in order to maintain privacy, devices are forced to conduct LSH themselves, returning us back to our original drawback in a vicious circle. We raise the following question then:

_Can a resource-constrained device conduct LSH-like pruning of a large dense layer without ever needing to see the entirety of its underlying weight?_

This work makes the following contributions to positively resolve this question:

**(1)** Introduce a novel family of hash functions, PGHash, for the detection of high cosine similarity amongst vectors. PGHash improves upon the efficiency of SimHash by comparing binarized random projections of _folded_ vectors. We prove several statistical properties about PGHash, including angle/norm distortion bounds, and that it is an LSH family.

**(2)** Present an algorithmic LSH framework, leveraging our hash functions, which allows for private, personalized, and memory-efficient distributed/federated training of large-scale recommender networks via dynamic pruning.

**(3)** Showcase experimentally that our PGHash-based framework is able to efficiently train large-scale recommender networks. Our approach is competitive against a distributed implementation of SLIDE  using full-scale LSH. Furthermore, where entry-magnitude similarity is desired over angular similarity (training over Amazon-670K, for example), we empirically demonstrate that using our DWTA  variant of PGHash, PGHash-D, matches the performance of using full-scale DWTA.

## 2 Related work

**LSH Families.**  Locality-sensitive hashing families have been used to efficiently solve the approximate nearest neighbors problem [16; 15; 2]. SimHash , based on randomized hyperplane projections, is used to estimate cosine similarity. Each SimHash function requires a significant number of random bits if the dimensionality of each target point is large. However, bit reduction using Nisan's pseudorandom generator  is often suggested [11; 14]. MinHash , a competitor to SimHash , measures Jaccard similarity between binary vectors and has been used for document classification. The Winner Take All (WTA) hash  compares the ordering of entries by magnitude (corresponding to Kendall-Tau similarity); such comparative reasoning has proven popular in vision applications . However, it was observed that WTA was ineffective at differentiating highly sparse vectors leading to the development of Densified WTA (DWTA) . Since MinHash, WTA, and DWTA are better suited for binary vector comparison, and we require comparison over real-valued vectors, PGHash is founded on SimHash.

**Hash-based Pruning.**  One of the earlier proposals of pruning based on input-neuron angular similarity via LSH tables is in , where a scheme for asynchronous gradient updates amongst multiple threads training over a batch along with hashed backpropagation are also outlined. These principles are executed to great effect in both the SLIDE  and Mongoose  frameworks for training extreme-scale recommender networks. Mongoose improves SLIDE by using an adaptive scheduler to determine when to re-run LSH over a layer weight, and by utilizing learnable hash functions. Both works demonstrated that a CPU using LSH-based dynamic dropout could achieve competitive training complexity against a GPU conducting fully-dense training. Reformer  uses LSH to reduce the memory complexity of self-attention layers.

**Distributed Recommender Networks.**  Several works which prune according to input-neuron angular similarity estimations via LSH utilize multiple workers on a single machine [29; 24; 7; 8]. Federated training of recommender networks is an emerging topic of interest, with particular interest in personalized training [18; 26] malicious clients [30; 36], and wireless unreliability . D-SLIDE , which is the federated version of SLIDE, eases local on-device memory and computational requirements by sharding the network across clients. However, in the presence of low client numbers,the proportion of the model owned per device can still be taxing, whereas our compression is independent of the number of federated agents. In , clients query the server for weights based on the results of LSH conducted using server-provided hash functions. We regard this complete server control over the hashing family, and therefore access to hash-encoding of local client data, as non-private and potentially open to honest-but-curious attacks.

## 3 Preliminaries

Let \(W=\{w_{1},w_{2},,w_{n}\}^{d}\) be the weights of a dense hidden layer and \(x^{d}\) be the input. For brevity, we refer to \(W^{d n}\) as the _weight_ of the layer. In particular, \(w_{i}^{d}\) corresponds to the \(i^{}\) neuron of the layer. We assume that the layer contains \(dn\) parameters. Within our work, we perform MIPS, as we select weights that produce large inner products with \(x\). Mathematically, we can begin to define this by first letting \(p= w_{i}^{}x\) for \(1 i n\). For \(0<<1\) we are interested in selecting \( W\), such that for \( w_{i} S\), \(w_{i}^{}x> p\). The weights of \(\) will pass through activation while the rest are dropped out, reducing the computational complexity of the forward and backward pass through this layer. As detailed in Section 4 and illustrated in Figure 1, we will determine \(\) by estimating angles with a projection \(BW=\{Bw_{i}\}_{i=1}^{n}\), with \(B^{c d}\) such that \(c<<d\).

**Locality-sensitive Hashing (LSH).** LSH  is an efficient framework for solving the \(\)-approximate nearest neighbor search (NNS) problem:

**Definition 1** (\(\)-Nns).: _Given a set of points \(P=\{p_{1},p_{2},,p_{n}\}\) in a metric space \(\) and similarity funcion \(Sim:\) over this space, find a point \(p P\), such that for a query point \(q X\) and all \(p^{} P\), we have that \(Sim(p,q)(1+)Sim(p^{},q)\)._

It is important to note that the similarity function \(Sim()\) need not be a distance metric, but rather any general comparison mapping. Popular choices include Euclidean distance and cosine similarity, the latter of which is the primary focus of this paper. The cosine similarity for \(x,y^{d}\) is defined as \((x,y) x^{}y/||x||_{2}||y||_{2}\). We can frame the MIPS problem described previously as an \(\)-NNS one if we assume that the weights \(w_{i} W\) are of unit length. Thus, we are searching for \(\)-nearest neighbors in \(W\) of the query \(x\) according to cosine similarity.

Consider a family \(\) containing hash functions of the form \(h:\), where \(\) is a co-domain with significantly lower feature dimensionality than \(\). We say that \(\) is locality-sensitive if the hashes of a pair of points \(x,y\) in \(\), computed by an \(h\) (selected uniformly at random), have a higher collision (matching) probability in \(S\) the more similar \(x\) and \(y\) are according to \(Sim\). We now formally define this notion following .

**Definition 2** (Locality-sensitive Hashing).: _A family \(\) is called \((S_{0}, S_{0},p_{1},p_{2})\)**-sensitive** if for any two points \(x,y^{d}\) and \(h\) chosen uniformly at random from \(\) satisfies the following, 1. if \(Sim(x,y) S_{0} Pr(h(x)=h(y)) p_{1}\), 2. if \(Sim(x,y) S_{0} Pr(h(x)=h(y)) p_{2}\)._

For an effective LSH, \(p_{1}<p_{2}\) and \(<1\) is required. An LSH family allows us to conduct a similarity search over a collection of vectors through a comparison of their hashed mappings. Of course, locality loss is inevitable if \(Sim\) is a dimension-lowering projection. Through a mixture of increased precision

Figure 1: **Hash-based dropout. Two procedures for conducting cosine similarity estimation between full weight matrix \(W^{d n}\) (column \(w_{i}\) denotes the weight of neurons \(i\)) and an input \(x^{d}\). (a) SimHash generates a hash table via left-multiplication by a randomized rectangular Gaussian matrix \(S(0,I_{d})\) onto the _fully-sized_\(W\). (b) PGHash generates a hash table via left-multiplication by a randomized square Gaussian matrix \(S(0,I_{c})\) onto a base projection \(BW^{c n}\) of \(W\). In both procedures, weight \(w_{i}\) is selected for activation if its signed hash matches the signed hash of \(x\).**

(raising the output dimension of \(\)) and repeated trials (running several trials over independently chosen \(h\)), we may tighten the correspondence between \(Sim\) and matches over \(\), following the spirit of the Johnson-Lindenstrauss Lemma .

**SimHash.** A popular LSH algorithm for estimating cosine similarity is SimHash, which uses signed random projections  as its hash functions. Specifically, for a collection of vectors \(W R^{d}\), the SimHash family \(^{Sim}\) consists of hash functions \(h_{v}\), each indexed by a random Gaussian vector \(v(0,I_{n})\), i.e., an \(n\)-dimensional vector with iid entries drawn from \((0,1)\). For \(x R^{n}\), we define the hash mapping \(h_{v}(x):=(v^{}x)\). Here, we modify \((x)\) to return 1 if \(x>0\), else it returns 0. For Gaussian \(v\) chosen uniformly at random and fixed \(x,y^{d}\), we have \(Pr(h_{v}(x)=h_{v}(y))=1-)}{}\). This hashing scheme was popularized in  as part of a randomized approximation algorithm for solving MAX-CUT. Notice that the probability of a hashed pair matching is monotonically increasing with respect to the cosine similarity of \(x\) and \(y\), satisfying Definition 2. More precisely, if we set \(S_{0}=(x,y)\) then \(^{Sim}\) is \(S_{0}, S_{0},1-(}{}), 1-(}{})\)-sensitive . The above discussion considers the sign of a single random projection, but in practice, we will perform multiple projections.

**Definition 3** (Hash table).: _Let \(X=\{x_{1},,x_{n}\}^{d}\) and \(V=\{v_{1}^{},,v_{k}^{}\}(0,I_{d})\), where \(k\) is the **hash length**. Define \(h_{V}:^{d}^{k}\) by \([h_{V}(x)]_{i}=h_{v_{i}}(x)\) where \(h_{v_{i}}^{Sim}\) for \(1 i k\). For fixed \(V\), the **hash table**\(h_{V}(X)^{k n}\) is a binary matrix with columns \(h_{V}(x_{j})\) for \(1 j n\)._

Following the notation above, we may estimate the similarity between an input \(q\) and a collection of vectors \(X^{d}\) by measuring the Hamming distances (or exact sign matches) between \(h_{V}(q)\) and columns of \(h_{V}(X)\). SimHash is now more discriminatory, as \(^{Sim}\) can separate \(R^{d}\) into \(2^{k}\) buckets corresponding to all possible length \(k\) binary vectors (which we refer to as **hash codes**). Finally, counting the frequency of exact matches or computing the average Hamming distance over several independently generated hash tables further improves our estimation of closeness. Implementations of the well-known SLIDE framework , which utilize SimHash for LSH-based weight pruning, require upwards of 50 tables.

**Dwta.** Another popular similarity metric is to measure how often high-magnitude entries between two vectors occur at the exact same positions. The densified winner-take-all (DWTA) LSH family  estimates this similarity by uniformly drawing \(k\) random coordinates over \(W\) and recording the position of the highest-magnitude entry. Similar to SimHash, this process is repeated several times, and vectors with the highest frequency of matches are expected to have similar magnitude ordering. This type of comparative reasoning is useful for computer vision applications .

## 4 PGHash

In this section, we develop a family of hash functions \(^{PG}\) which allow for memory-efficient serial generation of hash tables using a single dimensionally-reduced sketch of \(W\). This is in contrast to traditional LSH frameworks, which produce hash tables via randomized projections over the entirety of \(W\). We first present an overview of the Periodic Gaussian Hash (PGHash) followed by its algorithm for distributed settings, an exploration of several statistical properties regarding the local sensitivity of \(^{PG}\).

**PGHash Motivation.** As detailed in Section 3, our goal is to efficiently estimate cosine similarity between an input to a layer \(x^{d}\) and the columns of a large weight matrix \(W^{d n}\). SimHash performs hash table generation by first multiplying a matrix of uniformly drawn Gaussian hyperplanes \(T_{V}^{c d}\) with \(W\). The full hash table is computed as \(h_{V}(W)=(T_{V}W)\). Then, the \(j^{}\) neuron is activated if \((T_{V}x)=h_{V}(w_{j})\) for a layer input \(x\).

One can immediately notice that generation of a new hash table \(h_{}(W)\) requires both **(i)** computation of \(T_{}W\) which requires access to the fully-sized weights and **(ii)** the storage of \(T_{}\) to compute \((T_{}x^{})\) for further inputs \(x^{}\). _This is problematic for a memory-constrained device, as it would need to maintain both \(W\) and \(T_{}\) to generate further tables and perform dynamic pruning._ To solve this issue, we introduce a family of hash functions generated from a single projection \(BW\) of \(W\).

### PGHash theory

**Definition 4** (Periodic Gaussian Hash).: _Assume **sketch dimension**\(c<<d\) divides \(d\) for simplicity. Let \(B=[I_{c}|I_{c}||I_{c}]^{c d}\), where \(I_{c}\) is the \(c c\) identity matrix and \(|\) denotes \(\) concatenations. Let \(S^{k c}\) be a random Gaussian matrix with iid entries drawn from \((0,1)\). We may define a **Periodic Gaussian Hash (PGHash)** function \(h_{S}^{PG}:^{d}^{k}\) by \([h_{S}^{PG}(x)]_{i}=[(SBx)]_{i}\) for \(1 i k\). We denote the family of all such hash functions as \(^{PG}(c,d)\)._

We use the term "periodic" to describe the hash functions described in Definition 4, since unlike SimHash which projects a point via a fully random Gaussian vector as in SimHash, our projection is accomplished using a repeating concatenation of a length \(c\) Gaussian vector. Furthermore, for \(h_{S}^{PG} H^{PG}(c,d)\), the matrix representation \(SB\) is a tiling of a single Gaussian matrix. Notice that we may easily extend the notion of a PGHash of one vector to an entire hash table over multiple vectors following Definition 3. In this manner, we may generate a sequence of hash tables \(\{h_{S_{i}}^{PG}(W)\}_{i=1}^{}\) over a weight matrix \(W\) simply by drawing random Gaussian matrices \(S_{i}^{k c}\) for \(1 i\) (where \(k\) is the hash length) and computing \((S_{i}BW)\).

**Extension to DWTA (PGHash-D) Remark.**  When DWTA (described in Section 3) is preferred over SimHash for similarity estimation, we may modify Definition 4 as follows: \(B=D_{1}P\) where \(P\) is a \(d d\) random permutation matrix, and \(D_{1}\) is a \(c d\) rectangular diagonal matrix with \(D_{ii}=1\) for \(1 i c\). We denote our hash functions as \(h_{S}^{PG-D}:^{d}^{k}\) by \(h_{S}^{PG-D}(x)=_{i}[SBx]_{i}\) for \(1 i k\), with \(k c\), where \(S\) is now a rectangular permutation matrix which selects \(k\) rows of \(Bx\) at random. We refer to this scheme as PGHash-D, whereas PGHash refers to Definition 4.

**Local Memory Complexity Remark.**  When generating a new table using PGHash, a device maintains \(S\) and needs access to just \(BW\), which costs \((kc)\) and \((cn)\) space complexity respectively. This is much smaller than the \((kd)\) and \((dn)\) local memory requirements of SimHash.

**Sensitivity of \(^{PG}\).**  In this section, we will explore the sensitivity of \(^{PG}(c,d)\).

**Definition 5**.: _Let \(x^{d}\) and \(c\) such that \(c|d\). Define the \((d,c)\)**-folding**\(x_{c}^{c}\) of \(x\) as \([x_{c}]_{i}=_{j=1}^{}[x]_{i+j,}\). Equivalently, \(x_{c}=Bx\), with \(B\) as specified in Definition 4._

**Theorem 1**.: _Let \(x,y R^{d}\). Define the following similarity function \(Sim_{c}^{d}(x,y)(x_{c},y_{c})\), where \(x_{c},y_{c}\) are \((d,c)\)-foldings of \(x,y\). \(^{PG}(c,d)\) is an LSH family with respect to \(Sim_{c}^{d}\)._

Proof.: Let \(h_{v}^{PG}\). This means that for a randomly chosen \(v^{}(0,I_{})\), \(v\) is a \(c\)-times concatenation of \(v\). We see that \((v^{}x)=x}{||v||||x||} =}||}{||x||}x_{c} }{||v^{}||||x||}\). Since sgn is unaffected by the positive multiplicative factors, we conclude that \((v^{}x)=(v^{}x_{c})\). Through symmetric argument, we find \((v^{}y)=(v^{}y_{c})\). Since \(v^{}(0,I_{c})\), comparing the sign of \(v^{}x\) to \(v^{}y\) is equivalent to a standard SimHash over \(x_{c}\) and \(y_{c}\), i.e., estimation of \((x_{c},y_{c})\). 

**Corollary 1**.: _Let \(x,y^{d}\), then \(^{PG}(c,d)\) is \(S_{c}, S_{c},1-(}{}),1 -(}{})\)-sensitive where \(S_{c}=(x_{c},y_{c})\)._

Proof.: This follows directly from the well-known sensitivity of SimHash . 

We see that \(^{PG}\) is LSH with respect to the angle between \((d,c)\)-foldings of vectors. The use of periodic Gaussian vectors restricts the degrees of freedom (from \(d\) to \(d/c\)) of our projections. However, the usage of pseudo-random and/or non-iid hash tables has been observed to perform well in certain regimes. _Although \(^{PG}(c,d)\) is LSH, is \((x_{c},y_{c})\) necessarily an acceptable proxy for \((x,y)\), in particular, for high angular similarity?_ Heuristically, yes, for highly-cosine similar vectors: assuming \(x\) and \(y\) are both unit (since scaling does not affect angle) then we have that \(||x-y||^{2}=2-2(x,y)\). If \(_{2}\) similarity between \(x\) and \(y\) is already high, then the \(_{2}\) similarityof their (normalized) \((d,c)\)-foldings will also be high, and thus their cosine similarity as well. We now provide a characterization of the angle distortion of a \((d,c)\)-folding.

**Theorem 2**.: _Let \(x,y^{d-1}\). Assume that neither \(x\) nor \(y\) vanish under multiplication by \(B\) and define the set \(S_{x,y}:=\{v(x,y)\ :||v||=1\}\). We denote the following quantities: \(=d/c\), \(:=((x,y))\), \(=\{c>0\ |\ |||Bv|| c, v S_{x,y}\}\), and \(=/\). Assume \(>0\). Then \((x_{c},y_{c})\) lives between \(^{2}}{1+^{2}^{2}}\) and \(-^{2}-^{2}}{^{2}^{2}+^{2}}\)._

Proof sketch.: Consider the unit circle \(S_{x,y}\) contained in \((x,y)\) (Let us assume \(x\) and \(y\) are unit, WLOG). The linear distortion \(BS_{x,y}\) is an ellipse containing \(x_{c}=Bx\) and \(y_{c}=By\). The length of the axes of this ellipse is determined by the eigenvalues of \(B^{}B\). The bounds follow from further trigonometric arguments, by considering when the axes of \(BS_{x,y}\) are maximally stretched and shrunk respectively. These distortions are strongly related to \(\) and \(\).

We can see that as \((x,y) 0\) we have \((x_{c},y_{c}) 0\). It is natural to consider the distribution of \(\) in Theorem 2 as how extreme shrinking by \(B\) (the folding matrix) can greatly distort the angle. We can characterize this statistical distribution exactly.

**Proposition 1**.: _Let \(u S^{d-1}\), be drawn uniformly at random. Then \(||Bu||^{2}(,,0,)\), the four parameter Beta distribution with pdf \(f(x)=(1-2x/c)^{(d-c)/2-1}-1}{(d/c)(c/2,(d-c)/2}\) and \(||Bu||^{2}=1\)._

We defer proof of Proposition 1 to the Appendix D. Since the folded magnitude \(||Bu||^{2}\) is unit in expectation, the distortion term \(^{2}/^{2}\) in Theorem 2 will often be close to 1, greatly tightening the angle distortion bounds.

### PGHash algorithm

Below, we detail our protocol for deploying PGHash in a centralized distributed setting (presented algorithmically in Algorithm 1). Over a network of \(N\) devices, the central host identifies a target layer \(P\) whose weight \(W_{t}^{P}^{d n}\) (at iteration \(t\)) is too expensive for memory-constrained devices to fully train or host in local memory. Neurons (columns of \(W_{t}^{P}\)) are pruned by devices according to their estimated cosine similarity to the output \(x^{P-1}\) of the previous layer.

The central host begins each round of distributed training \(t\) by sending each device **(1)** all weights \(\{W_{t}^{t}\}_{t=1}^{P-1}\) required to generate the input \(x^{P-1}\) and **(2)** the compressed target layer \(BW_{t}^{P}\). Using these weights, each device conducts PGHash analysis (via Algorithm 2) using its current batch of local data to determine its activated neurons. The central host sends each device \(i\) their set of activated neurons \(W_{t}^{P}(_{i})\), and each device performs a standard gradient update on their new model \(\{W_{t}^{t}\}_{t=1}^{P-1} W_{t}^{P}(_{i})\{W_{t}^{t}\}_{ t=P+1}^{L}\). Finally, the central host receives updated models from each device and averages only the weights that are activated during training.

```
0: weights \(\{W_{0}^{}\}_{=1}^{L}\), objective \(\), \(N\) devices, target layer \(P\), \(T\) iterations, folding matrix \(B\), \(\) hash tables, hash length \(k\), sketch dim \(c\), comp. ratio \(CR\).
1:Server Executes:
2:for\(t=1,2,,T\)do
3: Compile pre-target weights \(W_{A}=\{W_{t}^{}\}_{=1}^{P-1}\)
4: Compile post-target weights \(W_{B}=\{W_{t}^{}\}_{=P+1}^{L}\)
5:for each device \(i\)in paralleldo
6:\(_{i}\)DeviceLSH\((i,W_{A},\)\(BW_{t}^{P},CR,,k,c)\)
7:\(W_{t+1,i}\)DeviceUpdate(i, \(W_{t}^{P}(_{i}) W_{B}\))
8:endfor
9:\(W_{t+1}\) Average the active weights \(W_{t+1,i}\) across all devices
10:endfor
11:return\(\{W_{T}^{}\}_{=1}^{L}\) ```

**Algorithm 1**Distributed PGHash

The on-device PGHash analysis (Algorithm 2) consists of first running a forward pass up to the target layer to generate the input \(x^{P-1}\). Devices generate personal hash tables by performing left-multiplication of \(BW_{t}^{P}\) and \(x^{P-1}\) by a random Gaussian matrix \(S(0,I_{c})\), as described in Section 3. The number of tables \(\) is specified by the user. A neuron \(j\) is marked as active if the input hash code \((Sx^{P-1})\) is identical to \(j\)-th weight's hash code \((SB[W_{t}^{P}]_{:,j})\). In Appendix A.2, we detail how Hamming distance can also be used for neuron selection.

**Computational Complexity Remark.**

Through the use of dynamic pruning, PGHash significantly reduces both the forward and backward training computational complexities. PGHash activates _at most_\(CR n\) neurons per sample as opposed to \(n\) for full training. In practice, PGHash activates only a fraction of the \(CR n\) neurons (as shown in Figure 5(a)). Therefore, the number of floating point operations within forward and backward training is dramatically reduced.

**Communication Complexity Remark.** By reducing the size of the model needed in local memory and subsequently requesting a pruned version of the architecture we improve communication efficiency. For a fixed number of rounds \(T\) and target weight size \(dn\), the total communication complexity, with respect to this data structure, is \((T CR dn)\), which is significantly fewer bits than the vanilla \((Tdn)\) communication cost of vanilla federated training. In Section 5, we show that PGHash achieves near state-of-the-art results with only \(CR=0.1\) (10% of a massive weight matrix).

```
0: batched input \(X^{d M}\), projected weight \(BW^{c n}\), compression rate \(CR\), \(\) hash tables, hash length \(k\), sketch dim \(c\).
1: Set \(=[\ \ ]\)
2:for\(i=1,2,,\)do
3: Draw random Gaussian \(S^{k c}\)
4:for each sample \(x\) in \(X\)do
5:for\(j=1,2,,n\)do
6:if\((SBx)=[(SBW)]_{:,j}\)then
7:\(.(j)\)
8:endif
9:endfor
10:endfor
11:if\(||> CR n\)then
12: Randomly remove selected neurons from table-\(i\) until \(||= CR n\)
13:return\(\)
14:endif
15:endfor
16:return\(\) ```

**Algorithm 2**PGHash

Figure 2: **Correlation between angle and Hamming distance. We plot the average Hamming distance (x-axis) between a PG/SimHash hashed fixed vector \(x\) and collection of vectors \(\) versus their true angles (y-axis). Vectors are unit, length 100, and hashed down to dimension \(k=25\) binary vectors according to \(=10\) or \(100\) hash tables. PGHash has a sketch dimension of \(c=25\). Both PGHash and SimHash show strong correlation between Hamming distance and angular similarity.**

## 5 Experiments

In this section, we **(1)** gauge the sensitivity of PGHash and **(2)** analyze the performance of PGHash and our own DWTA variant (PGHash-D) in training large-scale recommender networks. PGHash and PGHash-D require only 6.25% (\(c=8\)) of the final layer sent by the server to perform on-device LSH in our experiments. In PGHash, devices receive the compressed matrix \(BW^{c n}\) via the procedure outlined in Section 4. In PGHash-D, devices receive \(c\) out of \(d\) randomly selected coordinates for all \(n\) neurons in the final layer weight. Using \(k\) of the \(c\) coordinates (ensuring privacy since the server is unaware of the coordinates used for LSH), PGHash-D selects neurons that, within the \(k\) coordinates, share the same index of highest-magnitude entry between the input and weight. We employ PGHash for Delicious-200K and PGHash-D for Amazon-670K and WikiLSHTC-325K.

**PGHash Sensitivity Analysis.** Our first experiment measures the ability of \(^{PG}(c,d)\) to estimate cosine similarity. We produce a fixed unit vector \(x^{100}\) and set of 180 vectors \(\{v_{i}\}_{i=1}^{180}\) of the same dimension. Both the Gaussian vector \(x\) and collection of vectors \(V\) are fed through varying numbers of SimHash and PGHash tables. We produce a scatter plot measuring the correlation between the angle and average Hamming distance. PGHash, as seen in Figure 2, is an effective estimator of cosine similarity. We observe that PGHash, like SimHash, successfully produces low average Hamming distances for vectors that are indeed close in angle. This provides evidence that selecting neurons with exact hash code matches (vanilla sampling) is effective for choosing neurons that are close in angle to the input vector. Finally, we find increasing the number of hash tables helps reduce variance.

**Large-Scale Recommender Network Training.** Our second experiment tests how well PGHash(-D) can train large-scale recommender networks. We train these networks efficiently by utilizing dynamic neuronal dropout as done in . We use three extreme multi-label datasets for training recommender networks: Delicious-200K, Amazon-670K, and WikiLSHTC-325K. These datasets come from the Extreme Classification Repository . The dimensionality of these datasets is large: 782,585/205,443 (Delicious-200K), 135,909/670,091 (Amazon-670K), and 1,617,899/325,056 (WikiLSHTC-325K) features/labels. Due to space, Wiki results are found in Appendix A.3.

The feature and label sets of these datasets are extremely sparse. Akin to [8; 7; 34], we train a recommender network using a fully-connected neural network with a single hidden layer of size 128. Therefore, for Amazon-670K, our two dense layers have weight matrices of size \((135,909 128)\) and \((128 670,091)\). The final layer weights output logits for label prediction, and we use PGHash(-D) to prune its size to improve computational efficiency during training.

Unlike [8; 7; 34], PGHash(-D) can be deployed in a federated setting. Within our experiments, we show the efficacy of PGHash for both single- and multi-device settings. Training in the federated setting (following the protocols of Algorithm 1) allows each device to rapidly train portions of the entire neural network in tandem. We partition data evenly (in an IID manner) amongst devices. Finally, we train our neural network using TensorFlow. We use the Adam  optimizer with an initial learning rate of 1e-4. A detailed list of the hyper-parameters we use in our experiments can be found in Appendix A.1. Accuracy in our figures refers to the \(P@1\) metric, which measures whether the predicted label with the highest probability is within the true list of labels. These experiments are run on a cloud cluster using Intel Xeon Silver 4216 processors with 128GB of total memory.

Figure 3: **Compressed PGHash. We record model accuracy of a large recommendation system on an extreme classification task (Delicious-200K) using PGHash for varying compression rates (\(CR\)). Compressed PGHash, even at 90% compression, is competitive with full training (_without even including effects of sparsity-induced neuronal drop out_). Hyperparameters are in Appendix A.1.**

[MISSING_PAGE_FAIL:9]

PGHash(-D) and Federated SLIDE smartly train portions of the network related to each batch of local device data, via LSH, in order to make up for the lack of a full output layer. However, unlike Federated SLIDE, PGHash(-D) can perform on-device LSH _using as little as 6.25% of the full weight \(W\)_ (\(c=8\)) for both Delicious-200K and Amazon-670K experiments. Furthermore, for Delicious-200K, PGHash generates a dense Gaussian that is only 6.25% (\(c=8\)) the size of that for Federated SLIDE. In summary, PGHash(-D) attains similar performance to Federated SLIDE while storing less than a tenth of the parameters.

**Induced Sparsity.** PGHash(-D) induces a large amount of sparsity through its LSH process. This is especially prevalent in large-scale recommender networks, where the number of labels for each data point is a minuscule fraction of the total output layer size (_e.g._ Delicious-200K has on average only 75.54 labels per point). PGHash(-D) performs well at identifying this small subset of neurons as training progresses. As one can see in Figure 5(a), even when PGHash is allowed to select all possible neurons (_i.e._, no compression \(CR=1\)), it still manages to _select fewer than 1% of the total neurons after only 50 iterations of training over Delicious-200K_. For Amazon-670K, PGHash-D requires less than 30% of the total neurons for the majority of training. Therefore, PGHash(-D) greatly increases sparsity within the NN, improving the computational efficiency of the algorithm by reducing the number of floating point operations required in the forward and backward training.

## 6 Conclusion

In this work, we present a new hashing family, PGHash, which enables the generation of multiple LSH hash tables using a single base projection of a massive target weight. These hash tables can be used to dynamically select neurons that are similar to the layer input. This alleviates memory, communication, and privacy costs associated with conventional LSH-training approaches. As a proof of concept, we demonstrate that (i) the PGHash family is effective at mimicking SimHash and (ii) our framework is competitive against other, memory-inefficient, LSH-based federated training baselines of large-scale recommender networks. For future work, we intend to explore how multi-layer PGHash pruning affects model performance and incorporate learnable hashes as in the Mongoose  pipeline.

**Limitations.** Our theory indicates that PGHash is useful for detecting high angular similarity, but could prove unreliable for differentiating between intermediately dissimilar vectors. Additionally, LSH-based pruning has only shown success on large classification layers or attention layers in transformers . When considering broader impacts, large-scale recommender networks, and any subsequent improvements to their design, can be used for strategically negative advertising purposes.

Figure 6: **PGHash(-D) Computational Efficiency.** In Figure 5(a), we showcase that PGHash(-D) activates only a fraction of the total final layer neurons _even without compression_. Through this induced sparsity, PGHash(-D) greatly reduces the computational complexity of forward and backward training compared to full training. In Figures 5(b) and 5(c), we compare PGHash(-D) with the Sampled Softmax heuristic  (randomly sampling 10% of the total neurons) for efficiently training recommender systems. PGHash(-D) outperforms the Sampled Softmax baseline, as it selects a better set of activated neurons via LSH to more efficiently train the recommender system.