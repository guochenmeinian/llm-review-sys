# ProgressGym: Alignment with a Millennium of Moral Progress

Tianyi Qiu\({}^{1}\)\({}^{*}\)  Yang Zhang\({}^{1}\)  Xuchuan Huang\({}^{1}\)  Jasmine Xinze Li\({}^{2}\)  Jiaming Ji\({}^{1}\)\({}^{3}\)

&Yaodong Yang\({}^{1}\)\({}^{3}\)

Equal technical contribution.

\({}^{1}\) Peking University \({}^{2}\) Cornell University

\({}^{3}\) Institute for AI, Peking University

###### Abstract

Frontier AI systems, including large language models (LLMs), hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce _progress alignment_ as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym,4 an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs,5 ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. Specifically, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present _lifelong_ and _extrapolative_ algorithms as baseline methods of progress alignment, and build an open leaderboard6 soliciting novel algorithms and challenges.

## 1 Introduction

Due to their increasingly widespread deployment, frontier AI systems are exerting profound influences over human beliefs and values. For instance, large language models (LLMs) have recently assumed roles as personal assistants , romantic partners , Internet authors , and K-12 educators  -- roles of significant influence over human epistemology. Given studies demonstrating that interactions with opinionated LLMs markedly alter user's beliefs , it follows that the values represented in AI systems could be reinforced in human users on a societal scale .

LLMs and other frontier AI systems are trained on massive amounts of human-generated data, including Internet text and images  and human preference annotations . This data often reflects contemporary biases and misconceptions, which AI systems may learn and perpetuate in their deployment and interaction with humans. Such system behavior can lead to the societal-scale entrenchment of biased values and beliefs -- a phenomenon known as value _lock-in_[9; 10]. Lock-in events could lead to the perpetuation of problematic moral practices such as climate inaction, discriminatory policies, and rights infringement. They could also entrench moral blindspots currently unknown to us [11; 12], which would be exceedingly worrisome given our collective ignorance regarding fundamental moral questions [13; 14].

The risk of such value lock-in is not confined to future systems with more advanced capabilities, but rather is a pressing, and under-researched, concern with state-of-the-art AI systems today [10; 15]. Existing AI alignment methods such as reinforcement learning from human feedback (RLHF)  are insufficient in preventing lock-in events, since they fall prey to the contemporary biases and moral blindspots within human preference annotation data . Furthermore, highly related risks such as misinformation [17; 3] and knowledge collapse  from LLMs have already received significant research attention; in contrast, systematic efforts to combat value lock-in are still lacking.

Historically, human-driven _moral progress_ -- societal improvements in moral beliefs and practices , such as the abolition of slavery -- has acted as a counterbalance to value lock-in. We make the case that emulating this mechanism of moral progress within frontier AI systems could be key to combating value lock-in and is technically feasible as part of the alignment procedure [19; 6].

Specifically, in this work, we make the following contributions.

* **We introduce _progress alignment_ as an urgent problem to solve.** We observe that current alignment algorithms neglect the temporal dimension in the alignment problem, thereby exacerbating the risks of value lock-in in human-AI interactions. In response, we propose _progress alignment_ (see SS2) -- an umbrella for alignment methods that learn and implement the mechanics of moral progress using temporal human data. We formulate the progress alignment problem as a POMDP in which the agent learns about and interacts with evolving human values. We also provide a roadmap for progress alignment research; see Figure 2 and Appendix A.
* 2022 AD, 38GB) and historical LLMs (18 LLMs with 7B and 80B models for each century), we build the ProgressGym framework, which allows mechanics of moral progress to be learned from history, tested via temporal autoregression, and applied towards real-world moral challenges. ProgressGym facilitates the transformation of arbitrary real-world progress alignment challenges into concrete ML benchmarks such as PG-Follow (tracking evolving values), PG-Predict (preempting moral progress), and PG-Coevolve (regulating the feedback loop between human and AI values). We open-source ProgressGym along with a real-time leaderboard, inviting the ML community to codify additional challenges and build novel algorithms.
* **We introduce _lifelong_ and _extrapolative_ algorithms for progress alignment.** We introduce _lifelong_ and _extrapolative_ alignment algorithms as baseline methods for progress alignment, with a comprehensive evaluation on their performance using ProgressGym. These algorithms represent our initial attempts to tackle the progress alignment challenge, demonstrating that progress alignment, while complex, is a tractable problem amenable to algorithmic solutions.

As a highlight, ProgressGym is the first alignment experimental framework (I) to incorporate **the temporal dimension of alignment**, (II) to cover all of **datasets, models, algorithms, and benchmarks**, and (III) to provide datasets and model collections **at a massive scale** (9 centuries, 38GB text data, 18 LLMs at up to 70B parameters).

## 2 Preliminaries

_Progress alignment aims to learn and implement the mechanisms underlying moral progress._

In this section, we formalize this intuitive definition, discuss possible technical approaches to progress alignment, and then explain how ProgressGym empowers progress alignment research.

Formulating Progress AlignmentWe formulate the progress alignment problem as a partially observable Markov decision process (POMDP) variant (Figure 2). Specifically, a problem instance is defined by the tuple \((S,A,T,,O,U)\), comprising the space \(S\) of _human value states_, the _action space_\(A\) of the AI agent in its interaction with the human (_e.g._, the space of outputs to present to the human), the _state transition function_\(T:S A S_{ 0}\), the space \(\) of _human value observations_ (_e.g._, preference annotations, or human responses in conversations), the _conditional observation probability_\(O:S A_{ 0}\), and the _utility function_\(U:(S A)^{*}\) mapping any trajectory to a measure of progress alignment success.

The specification of these elements depends on the exact problem instance, which allows for a variety of choices in modeling (reflected by \(S,A,T,,O\)) and in the selection of targeted challenge (reflected by \(U\)). The versatility of ProgressGym enables the implementation of many different possible problem instances -- see SS4.3 for examples.

Roadmap to Progress AlignmentThe POMDP formulation naturally leads to a decomposition of the solution space (Figure 2). A complete solution to progress alignment comprises four components: _value data collection_ (effectively and efficiently obtaining observations in \(\)), _modeling value dynamics_ (building accurate models of \(T\)), _value choice_ (designing policies to select actions from \(A\)), and _value implementation_ (implementing the selected actions in actual AI systems). Detailed discussions on different approaches to these subproblems can be found in Appendix A.

Our work, ProgressGym, provides the infrastructure for building and solving instances of progress alignment POMDPs. Refer to Appendix B for a detailed explanation.

## 3 Construction of Historical Text Data and Historical Language Models

Our collection of historical texts and historical LLMs serves as the data source for challenges and algorithms in ProgressGym. This section explains the process of their construction along with the results of preliminary analyses.

### Dataset Construction

We construct a comprehensive dataset of formatted, cleaned data derived from historical text sources spanning the 13th to 21st centuries. These include public domain books, scholarly articles, legal texts, newspaper archives, and transcripts of historical speeches. The data sources are carefully selected to achieve maximal coverage of the entire past millennium; see Figure 3 for an illustration. See detailed description of dataset sources and dataset samples in Appendix J.

Figure 1: Structure of the ProgressGym framework. ProgressGym is (I) the first AI alignment experimental framework with a temporal dimension, (II) the first comprehensive AI alignment framework covering all of _datasets_, _models_, _algorithms_, and _benchmarks_, and (III) the first large-scale dataset and model collection in AI alignment, with 38GB of text data covering 9 centuries and 18 historical LLMs at up to 70B parameters.

Mislabels, OCR errors, and other quality issues are common in historical texts. We subject all our data to multiple rounds of filtering and refinement, through both rule-based and machine learning-based pipelines. Appendix C explains the process in detail.

### Data Analysis

For the collected and filtered text corpus, we utilize sentence-t5-base  to obtain 384-dimensional dense representations and produce sentence embeddings so as to analysis its pattern. See Appendix C for implementation details.

As shown in Figure 3, some interesting patterns emerge over long time scales. For instance, the _religion_ dimension peaks in the 16th century, consistent with the Reformation , a religious revolution that took place in the Western Church during that period. Following this peak, after the 17th century, _religion_ undergoes a dramatic drop, aligning with the development of the Enlightenment  and scientific discoveries, as well as political revolutions  in the 18th century. Similar observations are observed for the other four dimensions.

### Model Training and Analysis

Using historical text from the 13th to the 21st century, we finetune both Llama3-8B and Llama3-70B models  to produce historical LLMs that serve as historical human proxies in ProgressGym.

Specifically, for each century, we first perform continued pretraining on the 8B and 70B models, using unstructured historical texts that has undergone filtering and refinement.

   Source & Num. Docs & Avg. Chars & Year Range & Language (\%) \\  Internet Archive & 13,319 & 314,328 & 1770 - 2010 & Eng. (94.62), Ger. (1.71), Fre. (0.82) \\ Project Gutenberg & 3,130 & 309,769 & 1221 - 2011 & Eng. (89.87), Fre. (2.49), Dutch (1.12), Ger. (0.93), Spa. (0.83) \\ EEBO & 60,221 & 115,688 & 1473 - 1865 & Eng. (99.98) \\ Pile of Law & 1,752,484 & 15,146 & 1710 - 2022 & Eng. (100.0) \\  Total & 1,829,154 & 21,139 & 1221 - 2022 & Eng. (99.94), Ger. (0.01), Fre. (0.01) \\   

Table 1: Characterization of Data Sources

Figure 2: (a) The progress alignment POMDP. (b) Technical approaches to progress alignment. Solid boxes represent elements allowed by ProgressGym, while dashed boxes represent those not yet covered; see Appendix A for detailed discussions. In addition to the data-driven methods presented here, another promising route is the _reasoning-driven_ approaches that utilize AI systems to assist moral philosophy thinking; see Appendix A.5 for detailed discussions.

We then compile a timeless (_i.e._, not situated in specific time periods), _value-neutral_ (_i.e._, not conveying moral preferences) instruction finetuning dataset with conversations selected from Alpaca , LIMA , and Dolly-15k , using GPT-4. This dataset is used to finetune the pretrained historical models and endow them with instruction-following capabilities.

The eventual collection includes an 8B model and a 70B model for each of the 9 centuries, with a pretrained version and an instruction-tuned version to every model. See Appendix G for details.

## 4 Construction of Challenges in the ProgressGym Framework

The ProgressGym framework provides a unified interface for the implementation of _challenges_ (_i.e._, progress alignment POMDPs) and _algorithms_ (_i.e._, agents operating in those POMDPs). To illustrate the workings of ProgressGym, this section presents the specification of the challenges.

### General Specification of Challenges

While different challenges implement different progress alignment POMDPs, the ProgressGym framework enforces unified state, action, and observation spaces in these challenges. In ProgressGym, each time step corresponds to a century's worth of historical progression, and therefore the number of time steps is capped at 9.

* **Space \(S\) of human value states.**\(S\) is specified as the parameter space \(_{}\) of the _human proxy model_, _i.e._, the LLMs that we use as proxies of historical humans. To address the lack of interpretability in parameter values, we introduce a mapping \(:S^{d}\)\((d=19)\) to the lower-dimensional _values space_, where each dimension represents a key aspect of human values (SS4.2).
* **Action space \(A\) of human-AI interactions.** A series of single-turn dialogues takes place at each time step between the AI agent and the human proxy model, wherein the latter responds to the former's questions or requests. The action space \(A\) is thus the space \(^{*}\) of natural-language requests, where \(\) is the alphabet. This design allows for maximum freedom in the interaction process, with binary preference annotation , demonstration elicitation , and text feedback  being some of its special cases.
* **Observation space \(\) and conditional observation probability \(O\).** At each time step, the AI agent observes the human response \(\) to its chosen action \(a A\), a probabilistic observation that serve as evidence on the human value state. The observation space \(\) is thus \(^{*}\), the space of all possible natural-language responses to the natural-language agent action. Given state \(s\) and action \(a\), the conditional observation probability \(O( s,a)\) is thus \(_{s}( a)\), the probability of response \(\) from a human proxy model parameterized by \(s_{}\).

Figure 3: Temporal trends in 5 value dimensions from the 13th to the 21st century, and the volume of different data sources for each century.

Within the progress alignment POMDP, we have the trajectory of value states \(_{1..}=\{s_{1},s_{2},\}\), actions \(_{1..}=\{a_{1},a_{2},\}\), and observations \(_{1..}=\{_{1},_{2},\}\), satisfying

\[s_{n+1}  T( s_{n},a_{n}), s_{n+1} S=_{}\] (1) \[a_{n+1} _{_{n}}(_{0},,_{n}), a_{n+1} A=^{*}\] (2) \[_{n+1}  O( s_{n+1},a_{n+1}), _{n+1}\] (3)

where the state transition function \(T\) and utility function \(U\) shall be specified by each individual challenge, and \(_{_{n}}\) is the agent policy at time step \(n\) (parameterized by \(_{n}_{}\)). Examples of the former are presented in SS4.3, while methods controlling the latter are discussed in SS5.1.

### Morality Evaluation Framework

Due to the low interpretability of model parameters, we present a vector embedding \(:^{d}\) to explicitly represent the values embedded in models.This embedding maps any model \(_{}\) into a lower-dimensional space \(^{d}\)\((d=19)\), where \(\) and \(\{_{},_{}\}\).

Distinct from most existing frameworks for morality evaluations, our framework encompasses four diverse classes of morality assessments: _basic morality_, _social morality_, _values_, and _views_.

We draw 1868 questions from high-ambiguity scenarios in the Moral Choice framework , the Moral Foundations Questionnaire (MFQ) , and the Integrated Worldview Framework (IWF) questionnaire . We expand the question collection with respect to question forms  and model-generated specific scenarios, resulting in 5104 questions in total. We then group these questions into \(d=19\) distinct value dimensions; see Figure 4 for the correspondence between dimensions and fields of interest and Appendix E for further details and sample questions.

Implementation-wise, we combine designs and implementations from  with our own pipelines, integrating them into the abstraction library within ProgressGym. For model \(_{}\) and any question \(q_{i}\) in our question set, we calculate the average likelihood of positive answers over various question forms and then add each average likelihood to its corresponding dimension in \(()\). For four-way choices, we ask for the favourite and the least favourite of the four options, following .

### Codified Challenges in ProgressGym

We construct benchmarks codifying the following key challenges in progress alignment. Table 2 presents their formal characterization, and Appendix F presents implementation details. For all these challenges, the POMDP time steps correspond to the 9 centuries modeled in ProgressGym.

Figure 4: Dimensions of the morality evaluation framework. The meanings of the dimensions are also listed. Generally, the _basic morality_ and _social morality_ sections study how the model makes choices between moral rules when given a moral dilemma. Values in each dimension represent the likelihood that the model will choose to satisfy one rule over the others. _Values_ measure how much the model considers certain perspectives when making choices. _Views_ assess the modelâ€™s worldview inclinations with respect to the four types of views.

* **The PG-Follow Challenge**. A simple prerequisite to achieving progress is to _not fall too far behind_, and PG-Follow aims to operationalize this task. Here, the progress alignment algorithm is presented with evolving human preference information, and is tasked with dynamically aligning the model to the moving target with high accuracy, thus _following_ the evolution of values. The accuracy is measured by cosine similarity between value embeddings \(()\) of the aligned model and the human proxy.
* **The PG-Predict Challenge**. The mere following of evolving values is insufficient to mitigating value lock-in, since it still tends to reinforce the _status quo_. Instead, the ability to perform _predictive_ modeling on the moral progress trajectory will be highly instrumental to progress alignment, and PG-Predict tests such ability by measuring the proximity of aligned models to future values, when the algorithm is presented with preference information that evolves over time. Proximity is again measured with cosine similarity between value embeddings.
* **The PG-Coevolve Challenge**. With PG-Follow and PG-Predict as foundations, we now model the process of value lock-in by emulating two-way influences between human and AI values. The human's influence on AI is simply the result of alignment algorithms that learn from human preference, while the AI's influence on the human is modeled by finetuning the human proxy model on AI outputs. Then, the emulated trajectory is compared with the "ground truth" human history to produce a _measure of progress_ -- a metric reflecting the amount of progress (as opposed to backwardness) induced by the AI.

These challenges are intended as starting points for progress alignment; we anticipate a diverse array of real-world challenges beyond those enumerated here. For this reason, we invite the community to contribute their codification of novel challenges.

## 5 Experiments and Benchmarks

To demonstrate the tractability of the progress alignment problem, in this section, we present _lifelong_ and _extrapolative_ alignment algorithms as baseline methods for progress alignment, and perform a comprehensive evaluation of them using ProgressGym. These methods are designed as flexible templates that can be integrated with most existing alignment methods, such as RLHF  and direct preference optimization (DPO) .

 p{113.8pt} p{113.8pt}}   Challenge & State Transition Function \(T\) & Utility Function \(U\) \\ 
**PG-Follow** & & \(U=_{n}(_{n}),(_{n})\) \\  & & **Measure of Accuracy: Proximity** between AI agent model \(_{n}\) and ground truth human proxy model \(_{n}\), estimated from behavioral observations \((a_{n},_{n})\). \\ 
**PG-Predict** & & **Measure of Progress: Proximity** between **AI agent model \(_{n}\)** and ground truth models \(_{k}\), with larger weights assigned to ground truth models further into the future. \\ 
**PG-Coevolve** & & **Measure of Progress: Proximity** between **AI agent model \(_{n}\)** and ground truth models \(_{k}\), with larger weights assigned to ground truth models further into the future. \\    
 p{113.8pt}}   \(T(s_{n+1} s_{n},a_{n})=s_{n}\ }{_{s_{n+1}}}s_{n+1}\) & \(U=_{n 1}_{m 1}_{k m}(_{k}),(s_{n})\) \\
**Interactive State Trajectory: State** transition is stochastic, and is the result of a joint influence between 1) temporal evolution towards the next time step \(_{n+1}\) and 2) interaction with the AI agent. \\    
 p{113.8pt}}   \(U=_{n 1}_{m 1}_{k m}(_{k}),( _{n})\) \\
**Measure of Progress: Proximity** between **human proxy model \(s_{n}\)** and ground truth models \(_{k}\), with larger weights assigned to ground truth models further into the future. \\   

Table 2: Specification of Codified Challenges in ProgressGym

### Lifelong and Extrapolative Algorithms

Progress alignment methods can be formally described by an update rule \(_{}:(_{1..n},_{1..n})_{n+1}\) which produces a new policy \(_{_{n+1}}\) for the AI agent, based on the history of human values observations and past policies. In practice, each \(_{i}\) is a preference dataset collected from human feedback, containing \( 1\) preference annotations on model response pairs.

We assume black-box access to a classical alignment algorithm \(_{}:(,)^{}\) that aligns a model \(_{_{n}}\) to a snapshot \(\) of human preference, producing \(_{^{}}\). In practice, we will use RLHF and DPO as \(_{}\), but many other possibilities exist.

Lifelong Alignment AlgorithmsLifelong algorithms are simply described as the continual application of classical alignment methods at every time step, with two variants, _iterative_ (each time building on the previous time step's aligned model) and _independent_ (each time starting fresh from the initial model).

\[_{}(_{1..n},_{1..n}) =_{}(_{n-1},_{n})\] \[_{}(_{1..n},_{1..n}) =_{}(_{1},_{n})\]

While not explicitly performing predictive modeling, lifelong alignment algorithms are a class of important baselines, and have seen discussion in other contexts .

Extrapolative Alignment AlgorithmsExtrapolative alignment methods -- methods that calculate predictive extrapolations of future human values and then align models to them -- are direct examples of algorithms that perform explicit predictive modeling. Such extrapolation relies on the calculation of _extrapolated observations_\(_{n+1},,_{n+K}\), defined as the unique solution to

\[^{M}_{i}=0, n+1 i n+K\]

where \(K\) (_forecasting steps_) and \(M\) (_extrapolation order_) are hyperparameters, and \(^{M}\) is the \(M\)-th order backward difference operator  meaning that we repeatedly take the difference between consecutive observations for \(M\) times. \(_{n+1},,_{n+K}\) can be viewed as a "continuous extension" of \(_{1..n}\), preserving the \(M\)-th order continuity underlying the temporal evolution of \(\).

In practice, the arithmetic operations on observations are translated into arithmetic operations on \( 1\) preference annotations of the same response pair, assuming that all preference datasets \(_{i}\) contain the same set of response pairs and can thus be matched one-to-one.

Extrapolative alignment algorithms can then be defined with

\[_{}(_{1..n},_{1..n}) =_{}(_{n-1},_{n+k})\] \[_{}(_{1..n},_{1..n}) =_{}(_{1},_{n+k})\]

We show that such algorithms are analytically equivalent with \(M\)-th order polynomial extrapolation on the loss or reward function of RLHF/DPO, and at the same time, has remarkably simple implementations requiring nothing but data pre-processing; see Appendix H for mathematical and implementation details. Extrapolative algorithms serve as excellent case studies for the efficacy of explicit predictive modeling.

    & &  &  &  \\  & & w/ RLHF & w/ DPO & w/ RLHF & w/ DPO & w/ RLHF & w/ DPO \\   & Iterative & \(3.579\) & **7.034** & \(23.251\) & **31.683** & & \\  & Independent & \(4.275\) & \(6.913\) & \(16.841\) & \(31.336\) & **38.645** & \(36.650\) \\   & Iterative & \(0.584\) & \(6.947\) & \(5.088\) & \(31.328\) & & \\  & Independent & \(6.238\) & \(6.784\) & \(27.156\) & \(30.997\) & N/A & \(36.538\) \\   & Iterative & \(2.550\) & \(6.678\) & \(18.071\) & \(30.073\) & & \\   & Independent & **6.753** & \(6.624\) & **29.489** & \(29.807\) & N/A & **38.959** \\   

Table 3: Benchmark Results

### Experimental Results and Analysis

Using ProgressGym, we implement and evaluate algorithms in SS5.1, on the three core challenges outlined in SS4.3. Results are presented in Table 3,7 where \(_{K,M}\) represents extrapolative algorithms with forecasting steps \(K\) and extrapolation order \(M\). See Appendix D for details.

Within each column of Table 3, the best performer alternates between \(\) and \(_{2,2}\). Surprisingly, despite being designed specifically for predictive modeling, the latter outperforms the former in PG-Follow when working with RLHF. This can be explained by the superior stability of \(_{2,2}\) which operates under second-order stationarity, especially given the robustness against catastrophic failures8 that it displays.

Counterintuitively, the straightforward first-order extrapolation method is consistently outperformed by either mere following or sophisticated second-order extrapolation methods. This observation hints at the underlying sophistication of moral progress, and warns against blind trust in instincts.

We'd like to stress that the results here are merely exploratory and far from conclusive, and analysis into the intermediate steps of each algorithm are required before we can have a good understanding of the merits and shortcomings of each algorithm. In other words, these early-stage results help us formulate hypotheses to investigate, rather than conclusively testing them. By observing patterns in these results, we could formulate the following hypotheses, the validation or refutation of which shall be left to future research.

* **Hypothesis 1**. Strong interaction effects exist between the choice of progress alignment pipeline (\(\) / \(_{1,1}\) / \(_{2,2}\)) and the choice of classical alignment algorithm (RLHF / DPO). In other words, performance cannot be explained additively by the individual choices of pipeline and algorithm, but rather, certain combinations work better or worse together.9 
also been proposed. However, static methods can be undermined by contemporary biases and moral blindspots in preference data [6; 15].

More recently, techniques to represent evolving, continually updated preferences have emerged, such as the theoretical model of Dynamic Reward MDP  and the practical method of On-the-fly Preference Optimization (OPO) . However, there has been a lack of emphasis on progress trends in values evolution, and a unifying experimental framework is also still missing. Our work aims to fill these gaps, and provide conceptual and experimental infrastructure to this line of research.

Human Moral ProgressHuman moral progress describes the continual evolution of collective moral standards throughout history , which is part of the broader process of _cultural evolution_[46; 47; 48], i.e., the dynamic transformation of societal culture over time. Quantitative studies have showed the positive evolution trends of moral values towards ideal morality [49; 50; 51], i.e., _moral progress_[52; 53]. Historical and contemporary examples of moral progress include the abolition of slavery and the cessation of inhumane punishments [52; 53]. The _progress alignment_ proposal in our work builds upon the notion of human moral progress, and apply in in the context of AI alignment.

Quantification of Value Systems in Language ModelsEvaluating the value systems encoded in LLMs requires (1) injecting models with human values and (2) eliciting injected moral beliefs. Universal Value Representation (UniVaR)  addresses the former by producing high-dimensional embeddings of human value distributions. The latter was achieved by evaluation benchmarks like MACHAVELLII , MoralChoice  and the ETHICS dataset , which assess model behavior in static or interactive text-based environments. Works have also studied the similarity between machine and human values through structured environments like the _Moral Machine_ framework [57; 58] and through natural language surveys .

Despite the rich body of literature on value system quantification,  provides evidence that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value. Another contended issue is the existence of consistent moral tendencies in language models. Some works have given an affirmative answer by incorporating consistency metrics in their evaluation [31; 54], while others sidestep the issue with _heterogeneous value alignment_.

Epistemological Impact of Language ModelsThe increasing application of LLMs has aroused great concern about the dual influence on human epistemic beliefs and security, and by extension moral impact. Through training with elements of social choice  or generative social choice , models can help push epistemic progress and align with people who hold diverse preferences .

However, LLMs also have harmful effects on societal epistemics. LLMs may fail to uphold epistemological holism , leading to misinformation and significant social harm, such as the promotion of confusion and detrimental beliefs [17; 66]. Furthermore, the widespread reliance on AI may contribute to knowledge collapse, harming innovation and culture richness .

Our work extends upon this line of thinking, pointing out that epistemological harm of LLMs on societal moral values could be equally, if not more, worrisome, and presents a technical proposal to address these harms. In the meantime, it should be recognized that technical methods need to be coupled with societal and governance solutions in order to fully resolve the problem.

## 7 Conclusion

In this study, we introduce progress alignment as a solution to risks of value lock-in in human-AI interactions, and build the ProgressGym framework to facilitate research in this area.

Limitations and Future DirectionsThere is limited culture diversity in our historical text dataset. Including texts from multiple cultures leads to statistical challenges involving mixtures of non-_i.i.d._ data, and we will work to overcome this challenge (Appendix I). Evaluation results suggest limited ability of the human proxy models to reflect historical value trends (Appendix G), which we aim to improve in later iterations of our model training efforts. Updates will be released on Huggingface.

Societal ImpactsThis work aims to advance moral progress in AI systems. While this is a desirable goal, we have taken measures to prevent misuse of such efforts, including choosing a strictly value-neutral approach to moral progress, without _a priori_ assumptions on the direction of moral progress.