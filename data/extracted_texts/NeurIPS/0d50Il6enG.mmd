# Non-parametric classification via expand-and-sparsify representation

Kaushik Sinha

School of Computing

Wichita State University

Wichita, KS 67260

kaushik.sinha@wichita.edu

###### Abstract

In _expand-and-sparsify_ (EaS) representation, a data point in \(^{d-1}\) is first randomly mapped to higher dimension \(^{m}\), where \(m>d\), followed by a sparsification operation where the informative \(k m\) of the \(m\) coordinates are set to one and the rest are set to zero. We propose two algorithms for non-parametric classification using such EaS representation. For our first algorithm, we use _winners-take-all_ operation for the sparsification step and show that the proposed classifier admits the form of a locally weighted average classifier and establish its consistency via Stone's Theorem. Further, assuming that the conditional probability function \(P(y=1|x)=(x)\) is Holder continuous and for optimal choice of \(m\), we show that the convergence rate of this classifier is minimax-optimal. For our second algorithm, we use _empirical \(k\)-thresholding_ operation for the sparsification step, and under the assumption that data lie on a low dimensional manifold of dimension \(d_{0} d\), we show that the convergence rate of this classifier depends only on \(d_{0}\) and is again minimax-optimal. Empirical evaluations performed on real-world datasets corroborate our theoretical results.

## 1 Introduction

Given a training set \(x_{1},,x_{n}\) of size \(n\) and a test point \(x\) sampled _i.i.d._ from a distribution \(\) over some sample space \(^{d}\), the basic idea of non-parametric estimation (e.g., density estimation, regression or classification) is to construct a function \(x f_{n}(x)=f_{n}(x,x_{1},,x_{n})\) that approximates the true function \(f\) (which could be a density/regression/classification function as appropriate) as \(n\)Tsybakov (2008). For any \(x\), \(f_{n}(x)\) typically depends on a small subset of the training set lying within a small neighborhood of (thus close to) \(x\). For example, in case of \(k\)-nearest neighbor, \(f_{n}(x)\) depends on the \(k\) points from the training set that are closest to \(x\); in case of random forest, for each tree constituting the forest, \(f_{n}(x)\) depends on the points from the training set that are routed to the same leaf node to which \(x\) is also routed to; in case of kernel methods, \(f_{n}(x)\) depends on the points from training set defined by an appropriate kernel. In this paper, we propose a new non-parametric classification method where, for any \(x^{d-1}\), the appropriate neighborhoods are obtained using expand-and sparsify (EaS) representation of \(x\).

On a high level, expand-and-sparsity representation is essentially a transformation from a low-dimensional dense representation of sensory inputs to a much higher-dimensional, sparse representation. Such representation has been found, for instance, in the olfactory system of the fly (Wilson (2013)) and mouse (Stettler and Axel (2009)), the visual system of the cat (Olshausen and Field (2004)), and the electrosensory system of the electric fish (Chacron et al. (2011)). For example, in the olfactory system of Drosophila (Turner et al. (2008), Masse et al. (2009), Wilson (2013), Caron et al. (2013)), the primary sense receptors of the fly are the roughly 2,500 odor receptor neurons(also known as, ORNs), which can be clustered into 50 types, based on their odor responses, leading to a dense, 50-dimensional sensory input vector. In fact, all ORNs of a given type converge on a corresponding glomerulus in the antennal lobe; there are 50 of these in a topographically fixed configuration. This information is then relayed via projection neurons to a collection of roughly 2000 Kenyon cells (KCs) in the mushroom body, with each KC receiving signal from roughly 5-10 glomeruli (Dasgupta and Tosh (2020). The pattern of connectivity between the glomeruli and Kenyon cells appears random (Chacron et al. (2011)). The output of the KCs is integrated by a single anterior paired lateral (APL) neuron which then provides negative feedback causing all but the 5% highest-firing KCs to be suppressed (Lin et al. (2014)). The result is a random sparse high-dimensional representation of the sensory input, that is the basis for subsequent learning. The primary motivation of this paper is to study the benefit of this type of naturally occurring representation in the context of supervised classification.

In our setting, the EaS representation is a mapping from the \(d\)-dimensional unit sphere \(^{d-1}\) to \(\{0,1\}^{m},m d\), where a data point \(x^{d-1}\) is first randomly mapped to higher dimension \(^{m}\) using a random projection matrix \(\), and is followed by a sparsification operation, where the informative \(k m\) of the \(m\) coordinates of the resulting vector \( x^{m}\) are set to one and the rest of the coordinates are set to zero. Rows of \(\) are typically drawn independently from some distribution \(\) over \(^{d-1}\) or \(^{d}\). Let \(C_{j},j=1,,m\), be the set of all examples from the input space \(^{d-1}\) whose \(j^{th}\) coordinate in respective EaS representations are set to 1. We call \(C_{j}\) the _response region_ of \(_{j}\) (the \(j^{th}\) row of \(\)). Note that for any \(x\), there are \(k\)_activated_ response regions corresponding to the \(k\) non-zero bits in the EaS representation of \(x\). These \(k\) activated response regions serves as \(k\) neighborhoods of \(x\) for our proposed non-parametric classifier, where, for any \(x\) and each activated response region \(C_{j}\), we estimate \((y=1|C_{j})\) - expected value of \(y\) when \(x\) is restricted to \(C_{j}\) - and take their average to be the estimate of \((y=1|x)\). In a toy example, we visually show the EaS representation and the activated response regions \(C_{j}\) of a data point in Fig. 1. Comparing whether this conditional probability estimate exceeds \(1/2\), we predict the class label of \(y\). In particular, we present two algorithms using different sparsification schemes and analyze their theoretical properties.

One may find similarity between our proposed algorithm and a random forest classifier - for any \(x\), \(k\) activated response regions \(C_{j},x C_{j}\), may be interpreted as the leaf nodes (containing \(x\)) of \(k\) decision trees in a random forest. However, unlike random forest, we can not simply increase \(k\) (number of trees) without changing other hyper-parameters (such as \(m\)) hoping to achieving better prediction performance. Therefore, even-though the consistency property of random forest is studied under different settings Biau et al. (2008); Scornet et al. (2015); Scornet (2016); Tang et al. (2018), those ideas can not be applied for our theoretical analysis.

We summarize our contributions below:

* We present an interesting connection between non-parametric estimation and EaS representation, and propose two algorithms for non-parametric classification via EaS representation, and present empirical evaluations on benchmark machine learning datasets.
* For our first algorithm (using \(k\)-WTA sparsification), we establish its universal consistency and prove that it achieves minimax-optimal convergence rate that depends on dimension \(d\)

Figure 1: _Top:_ A point \(x^{2}\) (coordinate-wise values are different shades of gray) and its projection \(y= x^{15}\) (coordinate-wise values are different shades of red). The sparsification step sets the largest 5 values of \(y\) to 1 (black squares) and the rest to zero. _Bottom:_ Activated response regions \(C_{j},x C_{j}\), (\(x\) is a red dot), are shown using different colors. The points from the training set that intersects with these activated response regions are shown using black dots.

* When data lie on a low dimensional manifold having dimension \(d_{0} d\), we present a second algorithm (using empirical \(k\)-thresholding sparsification) and prove that its convergence rate is minimax-optimal and depends only on \(d_{0}\).

The rest of the paper is organized as follows. We discuss related work in 2. We present our first algorithm and its theoretical analysis including consistency and convergence rate in section 3. We present our second algorithm and derive its convergence rate in section 4. We present our empirical results in section 5 and conclude discussing limitations and future work in section 6.

## 2 Related work

Non-parametric estimation is an important branch of statistics with a rich history and classical results. Interested readers may refer to Tsybakov (2008); Gyorfi et al. (2002), which provide excellent treatment of this subject. Here we briefly review consistency and convergence rates of well-known non-parametric methods such as, partitioning estimates (histograms), \(k\)-nearest neighbors, kernel methods, and random forests. In the non-parametric literature, it is typical to estimate the regression function \((x)=(y|x)\) from data, and use the resulting estimate \(_{n}\) (using a sample of size \(n\)) to construct plug-in decision function (classification rule). Under mild conditions, such regression estimates and the resulting plug-in classification rules are consistent for histograms, \(k\)-nearest neighbors, and kernel methods Gyorfi et al. (2002); Devroye et al. (1996). Under mild conditions, different variations of random forest methods are also known to be consistent Biau et al. (2008); Scornet et al. (2015); Scornet (2016); Tang et al. (2018). Under the assumption that the regression function \((x)\) is Lipschitz continuous, the \(L_{2}\) error of the regression estimate of histogram convergences at a rate \(O(n^{-1/(d+2)})\) (Theorem 4.3 Gyorfi et al. (2002), the \(L_{2}\) error of the regression estimate of the kernel method convergence at a rate \(O(n^{-1/(d+2)})\) (Theorem 5.2 Gyorfi et al. (2002)), and the \(L_{2}\) error of the regression estimate of \(k\)-nearest neighbors method convergences at a rate \(O(n^{-2/(d+2)})\) (Theorem 6.2 Gyorfi et al. (2002)). For random forest, Gao and Zhou (2020) established finite-sample rate \(O(n^{-1/(8d+2)})\) on the convergence of pure random forests for classification, which was be improved to be of \(O(n^{-1/(3.87d+2)})\) by considering the midpoint splitting mechanism. They introduced another variant of random forests, which follow Breiman's original random forests but with different mechanisms for splitting dimensions and positions, to get a convergence rate \(O(n^{-1/(d+2)}( n)^{1/(d+2)})\), which reaches the minimax rate, except for a factor \(( n)^{1/(d+2)}\).

For EaS representation, when \(\) is the uniform distribution over \(^{d-1}\) and the sparsification scheme is a \(k\)-winners-take-all (\(k\)-WTA) operation that sets the \(k\) largest entries of a vector in \(^{m}\) to one the rest to zero, Dasgupta and Tosh (2020) proposed a function approximation scheme using such EaS representation that can approximate any Lipschitz continuous function \(f:^{d-1}\) in the \(L_{}\) norm, where the error decays exponentially slowly with \(d\). Further, they showed when the data lie on a low dimensional submanifold of \(^{d-1}\) having dimension \(d_{0} d\), using a different sparsification step, termed as \(k\)-thresholding, any Lipschitz continuous function defined on this manifold can be approximated in the \(L_{}\) norm, where the error decays exponentially slowly with \(d_{0}\). A different EaS representation, where the projection matrix is a sparse binary matrix and the sparsification step is \(k\)-WTA, was proposed in Dasgupta et al. (2017) that effectively hash input data points and such hashing has been shown to provide improved performance in accurately solving the similarity search problems compared to the state-of-the-art locality sensitive hashing (LSH) based techniques (Gionis et al. (1999); Andoni and Indyk (2008); Datar et al. (2004)). Such EaS representation has also been used to summarize data in the form of a bloom filter, termed as fly bloom filter (FBF), and has been successfully used in solving the novelty detection problems in Dasgupta et al. (2018) and classification problem in Sinha and Ram (2021); Ram and Sinha (2021, 2022).

While our work is inspired by the results of Dasgupta and Tosh (2020), there are key differences. First, we explicitly expand upon and apply the idea of Dasgupta and Tosh (2020) to the _supervised_ binary classification setting, and derive the rate at which the error probability of our proposed classifier converges to that of the Bayes optimal classifier. In comparison, the main motivation of Dasgupta and Tosh (2020) was to prove the existence of a weight vector that results in arbitrarily well function approximation in the _unsupervised learning_ setting using EaS representation. Because of this existential nature of their result, the effect of sample size was neither needed nor considered in their result. Second, the \(k\)-thresholding sparsification scheme proposed in Dasgupta and Tosh for function approximation result assumed that the coordinate-wise thresholds were known. We make no such assumption and explicitly describe how to compute such thresholds from data-resulting in realizable algorithm and derive convergence rate of our proposed classifier that takes care of the uncertainly associated with random natures of these thresholds. Third, while Dasgupta and Tosh  only considers LipSchitz continuous functions, we consider that conditional probability function \((x)=(y=1|x)\) to be Holder continuous (a broader function class), and prove that our proposed classifier achieves minimax-optimal convergence rate - whether such optimal convergence rate was achievable in the proposed setting was, unknown prior to our result.

Finally, we note that _random Fourier features_ Rahimi and Recht  are generated using a construction similar to EaS, however the choice of random directions there are chosen using a kernel function that measures a notion of similarity in the input space. EaS representation can also be though of an opposite process of _compressed sensing_ Candes et al. , Donoho , where the goal is to recover a sparse vector given random projections to it, while random projection is used to generate a sparse representation in case of EaS.

## 3 Algorithm 1

```
\(\)EsClassifier\((D_{n},m,k,R)\)  Sample \(\) with seed \(R\)  Initialize \(w[i]\), \([i] 0,\  i[m]\) for\((x,y) S\)do \( h_{1}(x)\) \(w[i] w[i]+y,\  i[m]:[i]=1\) \([i][i]+1,\  i[m]:[i]=1\)  end for \(w[i] w[i]/[i],\  i[m]\) return\(,w\) end for InferEsClassifier\((x,,k,w)\) \( h_{1}(x)\) return\([( w)/k]\)  end for
```

**Algorithm 1** Training set \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}^{d-1}\{0,1\}\), Projection dimensionality \(m\), \(k m\) non-zeros in the EaS representation, random seed \(R\), and inference with test point \(x^{d-1}\).

```
\(\)EsClassifier\((D_{n},m,k,R)\)  Sample \(\) with seed \(R\)  Initialize \(w[i]\), \([i] 0,\  i[m]\) for\((x,y) S\)do \( h_{1}(x)\) \(w[i] w[i]+y,\  i[m]:[i]=1\) \([i][i]+1,\  i[m]:[i]=1\)  end for \(w[i] w[i]/[i],\  i[m]\) return\(,w\) end for InferEsClassifier\((x,,k,w)\) \( h_{1}(x)\) return\([( w)/k]\)  end for
```

**Algorithm 2** Learning Algorithm 1

For \(x^{d-1}\), the EaS representation of \(x\), that uses \(k\)-WTA sparsification step, is given by the function \(h_{1}^{d-1}\{0,1\}^{m}\) defined as,

\[h_{1}(x)=_{k}( x), \]

where \(\) is a \(m d\) random projection matrix whose rows \(_{1},,_{m}\) are sampled i.i.d from the uniform distribution over \(^{d-1}\) and \(_{k}^{m}\{0,1\}^{m}\) is the \(k\)-WTA function converting a vector in \(^{m}\) to one in \(\{0,1\}^{m}\) by setting the largest \(k m\) elements of \( x\) to \(1\) and the rest to zero. For any \(j[m]\), let \(C_{j}=\{x:h_{1}(x)[j]=1\}\). We note that the subsets (response regions) \(C_{1},,C_{m}\) does not form a partition since they can be overlapping. We summarize our first algorithm for binary classification using \(h_{1}\) in Alg. 1. During its learning/training phase, a vector \(w^{m}\) summarizes the average \(y\) value over \(m\) response regions \(C_{j},j[m]\) using the training set. In particular, \(w[j],j[m]\) learned during the training phase is precisely given by

\[_{j}=^{n}y_{i}[x_{i} C_{j}]}{_{i=1} ^{n}[x_{i} C_{j}]} \]

Using (2),for any \(x\), we further define

\[(x)=_{j:x C_{j}}_{j} \]During the inference phase, for any test point \(x\), Alg. 1 first computes \((w h_{1}(x))/k\), which is an average of average \(y\) values over \(k\) response regions \(\{C_{j}:h_{1}(x)[j]=1\}\), which can be interpreted as an estimate of the conditional probability \((x)\) and is precisely the quantity \((x)\) given in (3). Alg. 1 then makes its prediction based on whether \((x)\) is greater than or equal to \(1/2\). We can rewrite this conditional probability estimate as follows.

\[(x) = (x)}{k}=_{j:x C_{j}}w[j]= _{j:x C_{j}}_{j}=_{j:x C_{j}} ^{n}y_{i}[x_{i}_{j}]}{_{i=1}^{n} [x_{i}_{j}]} \] \[= _{i=1}^{n}_{j:x C_{j}} [x_{i}_{j}]}{_{i=1}^{n}[x_{i} _{j}]})}_{w_{n,i}(x)}y_{i}=_{i=1}^{n}w_{n,i}(x)y_{i}\]

Using viewpoint (4), Alg. 1 can be interpreted as a "plug-in" classifier Devroye et al. (1996) where prediction is based on whether the estimated conditional probability \(\) exceeds \(1/2\) or not. In particular, classifier in Alg. 1 can be represented as \(g:\{0,1\}\) described as

\[g(x)=1,&(x) 1/2,\\ 0,&. \]

In comparison, the Bayes optimal classifier \(g_{*}:\{0,1\}\) is defined as

\[g_{*}(x)=1,&(x) 1/2,\\ 0,&. \]

In equation 4, the weights \(w_{n,i}(x)=w_{n,i}(x,x_{1},,x_{n})\) depends on \(x_{1},,x_{n}\). Next we show that for sufficiently large \(n\) sum of these weights is 1. Therefore, \(\) is simply a weighted average estimator of \(\) and is a non-parametric classifier Devroye et al. (1996).

**Lemma 3.1**.: _For any \(x\), suppose \(n\) is sufficiently large such that \(\{x_{i},,x_{n}\} C_{j}\) for each \(j\) satisfying \(x C_{j}\). Then, \(_{i=1}^{n}w_{n,i}(x)=1\)._

Indeed, we show in Lemma D.7 (in the Appendix), that for any \(x\), whenever \(n\) and \(m^{k}/n 0\) as \(n\), \(|\{x_{1},,x_{n}\}_{j}|\) for all \(j\) such that \(x C_{j}\).

### Consistency of Algorithm 1

In this section we prove that Alg. 1 is universally consistent. We start with the definition of consistent and universally consistent classification rule Devroye et al. (1996). Let \(D_{n}=\{(x_{1},y_{1}),,((x_{n},y_{n})\}\) be a training set sampled _i.i.d._ from a certain distribution of \((x,y)\) and let \(g_{n}:\{0,1\}\) be a classification rule learned using \(D_{n}\). Then \(g_{n}\) is consistent (or asymptotically Bayes-risk efficient) for a certain distribution of \((x,y)\) if the expected error probability \(L_{n}=\{g_{n}(x,D_{n}) y\} L^{*}\) as \(n\), where \(L^{*}\) is the Bayes error probability. A sequence of decision rules is called universally consistent, if it is consistent for any distribution of the pair \((x,y)\). It is well known that a general theorem by Stone Stone (1977) (presented below) provides a recipe for establishing universal consistency of any classification rule of the form

\[g_{n}(x)=1,&_{n}(x) 1/2,\\ 0,&. \]

based on an estimate \(_{n}(x)\) of the conditional probability \((x)\), satisfying \(_{n}(x)=_{i=1}^{n}w_{n,i}(x)y_{i}\) where weights \(w_{n,i}(x)=w_{n,i}(x,x_{1},,x_{n})\) are non-negative and \(_{i=1}^{n}w_{n,i}(x)=1\).

**Theorem 3.2**.: _[_Stone's Theorem (Theorem 6.3 (Devroye et al., 1996)]__]_ _Assume that for any distribution of \(x\), the weights satisfy the following three conditions: (i) There is a constant \(c\) such that, for every non-negative measurable function \(f\) satisfying \(f(x)<\), \((_{i=1}^{n}w_{n,i}(x)f(x_{i})) cf(x)\). (ii) For all \(a>0\), \(_{n}(_{i=1}^{n}w_{i,n}(x)_{ \{\|x_{i}-x\|>a\}})=0\). (iii) \(lim_{n}(_{1 i n}w_{n,i}(x))=0\). Then \(g_{n}\) is universally consistent._

[MISSING_PAGE_FAIL:6]

With this definition, we introduce an intermediate quantity

\[(x)=_{j:x C_{j}}_{j} \]

where, \(_{j}\) is defined as,

\[_{j}=)}_{C_{j}}(x)(dx)=(C_{j}) \]

Using triangle inequality, this allows us to write: \(_{x,D_{n}}|(x)-(x)|=_{x,D_{n}}|(x)- (x)+(x)-(X)|_{x,D_{n}}|(x)- {}(x)|+_{x,D_{n}}|(x)-(x)|\), and we show how to individually bound each term on the right-hand side of this inequality next.

_Remark 3.7_.: Note that, once \(m\) and \(k\) are fixed, our hypothesis space is the set of all linear models on the \(k\)-sparse \(m\) dimensional binary vectors. The two terms on the right-hand side of the above inequality correspond to _estimation error_ (the error of our proposed classifier with respect to the best hypothesis from the hypothesis space) and _approximation error_ (the error difference between the best hypothesis from the hypothesis space and the target classifier, i.e., Bayes optimal), respectively.

### Bounding \(_{x,D_{n}}|(x)-(x)|\)

Note that there is an inherent randomness in our proposed algorithm associated with the choice of \(\). In particular, the response regions \(C_{j}\) are random quantities that depend on the choice of \(\). In this section, we fix \(\) and conditioned on this, we bound \(_{x,D_{n}}|(x)-(x)|\). This ensures that for any \(>0\), the same bound holds with probability at least \(1-\) over the choice of \(\).

**Lemma 3.8**.: _Fix any \(\). Then we have, \(_{x,D_{n}}|(x)-(x)|}\)._

Sketch of proof:.: From the definition of \((x)\) and \((x)\) given in (3) and (10) and applying Jensen's inequality, crux of the proof is to focus on the expected value of the random quantity \(_{j:x C_{j}}(^{n}y_{i}[x_{i} C_{j}]}{ _{i=1}^{n}[1[x_{i} C_{j}]}-(C_{j}))^{2}\). Note that, for \(j[m]\), \(_{i=1}^{n}[x_{i} C_{j}]=n_{n}(C_{j})\) is the number of the points from \(D_{n}\) that fall in \(C_{j}\), where \(_{n}(C_{j})\) is the empirical probability estimate. We bound the quantity of interest in Lemma 3.8 as a sum of two quantities corresponding to the following two cases:

**Case 1: when \((C_{j})=0}\).** Using the notation \(0/0=0\), the quantity of interest becomes \(_{j:x C_{j}}(^{2}(C_{j})[_{n}(C_{j})=0 ])\). Utilizing the properties of the response regions established in Lemma 3.4, we show in Lemma E.3 that the expected value of the quantity of interest at most \(\).

**Case 2: when \((C_{j})>0}\).** Here the quantity of interest becomes \(_{j:x C_{j}}(^{n}(y_{i}-(C_{j})[|x_ {i} C_{j}]}{n_{n}(C_{j})})^{2}[n_{n}(C_{j})>0].\). Conditioned on \(x,x_{1},,x_{n}\), we first show in Lemma E.1 that expected value (w.r.t. \(y_{1},,y_{n}\)) of this quantity becomes at most \(_{j:x C_{j}}((C_{j})>0]}{n_{n}( C_{j})})\). Next, conditioned on \(x_{1},,x_{n}\), and utilizing the properties of the response regions established in Lemma 3.4, we show in Lemma E.2 that expected value (w.r.t. \(x\)) of this quantity is at most \(_{j=1}^{m}()[1|n_{n}(C_{j})>0]}{n_{n} (C_{j})})\). Finally, using standard Binomial bound (Lemma E.4) we show that expected value (w.r.t. \(x_{1},,x_{n}\)) of this quantity is at most \(\).

### Bounding \(_{x,D_{n}}|(X)-(x)|\)

In order to bound the expected value of \(|(x)-(x)|\), we need to impose certain smoothness condition on \(\). We consider a general form of smoothness, known as Holder continuity for \(\).

**Definition 3.9**.: We say that \(:\) is \((L,)\) smooth if for all \(x,x^{}\), we have \(|(x)-(x^{})| L\|x-x^{}\|^{}\).

Using Holder continuity assumption above, we first show the following:

**Lemma 3.10**.: _Suppose \(\) is \((L,)\) smooth. Then, \(_{x}|(x)-(x)| L_{j[m]}((C_{j}))^{}\)._

We have already shown in Lemma 3.5 how to bound the diameters of \(C_{j}\). Combining these two results we have 

**Lemma 3.11**.: _Let \(d 3\) and pick any \(0<<1\). Assume \(\) to be \((L,)\) smooth. There is an absolute constant \(c_{0}>0\) such that the following holds. If \(k c_{0}(d m+(1/))\) then with probability at least \(1-\), \(_{x,D_{n}}|(x)-(x)| 8L(2k/m)^{ {d-1}}.\)_

Combining Lemma 3.8, 3.11 and 3.6 we are present the main result of this section.

**Theorem 3.12**.: _Let \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\{0,1\}\) be the training data and consider the EaS representation given in (1). Let \(d 3\) and pick any \(0<<1\). Assume \(\) to be \((L,)\) smooth. There is an absolute constant \(c_{0}>0\) such that the following holds. If \(k c_{0}(d m+(1/))\) then with probability at least \(1-\) over the random choice of \(\),_

\[(g(x) y)-L^{*} 2(}+8L( )^{})\]

**Corollary 3.13**.: _In Theorem 3.12, set \(m=kn^{}\). Then, with probability at least \(1-\), over the choice of \(\),_

\[(g(x) y)-L^{*}=O(n^{-}).\]

_Remark 3.14_.: Since \(^{d-1}\), the effective dimension in our setting is \(d^{}=(d-1)\) and the convergence rate of Corollary 3.13 can be rewritten as \(O(n^{-}})\) which is minimax-optimal for plug-in classifiers under the assumption that \(\) is \((L,)\)-smooth (Audibert and Tsybakov, 2007).

### Inability to adapt to manifold structure

In Theorem 3.12, we derived the convergence rate of the classifier presented in Alg. 1 by bounding \(_{x,D_{n}}|(x)-(x)|\), which upper bounds the excess Bayes risk, from above and the resulting convergence rate decays exponentially slowly with the dimension \(d\). We now show that even if the data lie on a low dimensional manifold having dimesnion \(d_{0} d\), there exists a smooth \(\) such that the quantity \(_{x,D_{n}}|(x)-(x)|\) decreases at a rate no faster than \(n^{-}\). To prove claim, we assume data to lie on the following one-dimensional manifold:

\[_{1}=\{(x_{1},x_{2},0,0,,0)^{d}:x_{1}^{2}+x_{2}^ {2}=1\} \]

We further assume that \(k==1\) which implies that \(\) is \(L\)-Lipschitz from the definition of \((L,)\)-smoothness. Our lower bound result is as follows.

**Theorem 3.15**.: _For any \(d>3\), let input space \(_{1}\) be the one-dimensional sub-manifold of \(^{d}\) given in (12). Take \(k=1\) and \(=1\). Suppose the random matrix \(\) has rows chosen from a distribution that is uniform over \(^{d-1}\). Then there exists a \(\)-Lipschitz function \(:_{1}\) such that the following holds with probability at least \(1/2\) over the choice of \(\)._

\[_{x,D_{n}}|(x)-(x)|=(n^{-})\]

## 4 Algorithm 2

In this section we present an alternate EaS representation and an associated classification algorithm that adapts to intrinsic dimension \(d_{0}\) when data lie on a low dimensional manifold with dimension \(d_{0} d\). Here, we assume that the rows \(_{i},i[m]\) of matrix \(\) are sampled _i.i.d._ from a multivariate Gaussian distribution \(N(0,1/I_{d})\), denote by \(\), where \(I_{d}\) is \(d d\) identity matrix. This EaS representation is denoted by \(h_{2}:\{0,1\}^{m}\), where for any point \(x\), the \(j^{th}\) coordinate of \(h_{2}(x)\) is set to \(1\), as given in (14), using a data dependent threshold \(_{n}\). In particular, given a training set of size1\(2n\) sampled _i.i.d._from \((,)\), using the first half of it, namely, \(D^{}_{n}=\{(x^{}_{1},y^{}_{1}),,(x^{}_{n},y^{ }_{n})\}\), define \(_{n}:^{d}\) to be:

\[_{n}(){=}{}\!\!\{\!\!:_{i=1}^{n}[ x^{}_{i}]\!\} \]\[h_{2}(x)[j]=[_{j} x_{n}(_{j})] \]

We call the sparsification scheme given in (14)_empirical-k-thresholding_. For \(j[m]\), the \(j^{th}\) response region \(C_{j}\) is defined as:

\[C_{j}=C(_{j})=\{x:_{j} x_{n}(_{j})\} \]

Using the second half of the training data, namely \(D_{n}=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\), average \(y\) values over different \(C_{j}\) are estimated and summarized in vector \(w\) in the same way as in Alg. 1 implying that \(w[j]=_{j}, j[m]\), where \(_{j}\) is given in (2). This completes the training phase of our proposed algorithm summarized in Alg. 2.

```
TrainEsClassifier\((D_{n},D^{}_{n},k,t,R)\)  Sample \(\) with seed \(R\)  Initialize \(w[i],[i] 0,\; i[m]\) for\(j[m]\)do  Compute \(_{n}(_{j})\) using (13) and \(D^{}_{n}\)  end for for\((x,y) D_{n}\)do \( h_{2}(x)\) \(w[i] w[i]+y,\; i[m]:[i]=1\) \([i][i]+1,\; i[m]:[i]=1\)  end for \(w[i] w[i]/[i],\; i[m]\) return\(,w\)  end for InferEsClassifier\((x,,t,w)\) \( h_{2}^{n}(x)\) \(_{x}\{_{i}:h_{2}(x)[i]=1\}\) \(\) \([i] 0, i[m]:i A_{t}(x)\) return\([( w)/t]\)  end for \(h_{2}(x)[j]=[_{j} x_{n}(_{j})]\) (14) \(\) \([i] 0, i[m]:i A_{t}(x)\) return\([( w)/t]\)  end for \(h_{2}(x)[j]=[_{j} x_{n}(_{j})]\) (15)
```

**Algorithm 2** Training set \(D^{}_{n}=\{(x^{}_{i},y^{}_{i})\}_{i=1}^{n},D_{n}=\{(x_{i},y _{i})\}_{i=1}^{n}\{0,1\}\), Projection dimensionality \(m\), integer \(k m\), integer \(t\), random seed \(R\), and inference with test point \(x^{d-1}\).

The inference phase of Alg. 2 is slightly different from Alg. 1. While \(\) representation using \(h_{2}\) is not \(k\)-sparse anymore, we show that for large enough sample size, with high probability, it is at least \(k/2\)-sparse and at most \(2k\)-sparse in expectation (see Lemma F.1 in Appendix F). Let \(t\) be an integer passed as an argument to Alg. 2. For any \(x\), let \((x)=\{_{j}:x C_{j}\}\) and we define \(A_{t}(x)\) to be the set containing the indices \(j[m]\), such that \(_{j}\) is one of the \(t\) closest points to \(x\) from \((x)\). To make an inference for any \(x\), Alg. 2 first computes \(( w)/t\) and makes it prediction based on whether this quantity is greater than \(1/2\). Clearly, \(( w)/t\) is the average of \(w[j]\) for \(j A_{t}(x)\) and therefore, the conditional probability estimate \((x)\) of Alg. 2 can be represented as,

\[(x)=_{j}_{j}[j A_{t}(x)] \]

**Remark 4.1**.: Note that \(h_{1}\) and \(h_{2}\) are different in a specific way. When the support of data does not cover the whole unit sphere and is concentrated possibly in a small region and \(m\) is large, many of the \(m\) coordinates in \(\) representation will never be activated (set to \(1\)) for any data point as the corresponding projection direction \(_{j}\) may not be one of the \(k\) closest ones to any data point. Thus, many of the \(_{j}\) will be unused. This problem is the respective response region \(C_{j}\) may not be local to the manifold. For this purpose we need to identify "good" projection directions. Later in Lemma B.4 we show that, for any \(>0\), with probability at least \(1-2\) over the choice of \(\) and \(D^{}_{n}\), the number of "good" projection directions \(t\) is linear in \(k\).

Due to space limitation, manifold assumptions and other important details of analysis of Alg. 2 are presented in Appendix B and we present the main theoretical results below.

**Theorem 4.2**.: _Let \(D_{n}=\{(x_{i},y_{i})\}_{i=1}^{n} D^{}_{n}=\{(x^{}_{i},y^{ }_{i})\}_{i=1}^{n}\{0,1\}\) be the training data where the data lies on a low dimensional manifold satisfying manifold assumption presented in section B.1 and suppose the \(\) representation is given as in (14). Pick any \(0<<1\). Assume \(\) to be \((L,)\) smooth. If \(k c^{}_{d}(m/)\), where \(c^{}_{d}\) is a constant that depend on \(d\), then with probability at least \(1-2\),_

\[(g(x) y)-L^{*} 2(kn}}+4L( {2k}{c_{1}m})^{}})\]

_where \(_{d}\) is a constant that depends on \(d\)._

**Corollary 4.3**.: _In Theorem 4.2, setting \(m=kn^{}{2+d_{0}}}\) ensure that with probability at least \(1-\),_

\[(g(x) y)-L^{*}=O(n^{-}}).\]

_Remark 4.4_.: The convergence rate of Corollary 4.3 depends only on \(d_{0}\) and is minimax-optimal for plug-in classifiers under the assumption that \(\) is \((L,)\)-smooth (Audibert and Tsybakov, 2007).

## 5 Empirical evaluations

We investigate the effectiveness of our proposed method by evaluating it on eight benchmark datasets, details of which are provided in appendix A. We address the following questions:

1. Does performance our proposed classifier improve with increasing \(m\) as suggested by the theory?

2. How does our proposed classifier perform compared to other non-parametric classifiers?

For each dataset, we generate train and test set using scikit-learn'strain_test_split method (\(80:20\) split). We compare our proposed method against two non-parametric classifiers - scikit-learn's implementation of \(k\)-nearest neighbor classifier (\(k\)-NN) and random forest (RF). For \(k\)-NN, we used two values of \(k\): \(k=1\) and \(10\). For RF we use a grid search over the number of estimators (trees) from the set \(\{250,500,750,1000\}\) and perform a \(3\) fold cross validation to choose the final model. We preset our experimental results in Fig. 2 where we plot test accuracy of Alg. 1, \(k\)-NN (for \(k=1\) and \(10\)) and RF by varying expansion factor, where we define expansion factor to be \(m/d\). As per Theorem 3.12, we set \(k\) to be \(d m\). As can be seen from Fig. 2, with increasing \(m\) test accuracy of Alg. 1 increases in all eight datasets and becomes comparable to that of \(k\)-NN and RF for large \(m\), thus corroborating our theoretical findings.

## 6 Conclusions, limitations and future work

In this paper, we present an interesting connection between non-parametric estimation and expansion-and-sparsify representation. We presented two non-parametric classification algorithms using EaS representation and proved that both algorithms yield minimax-optimal convergence rates. The convergence rate of the first algorithm depends on the ambient dimension \(d\), while the convergence rate of the second algorithm, under manifold assumption, depends only on the intrinsic dimension \(d_{0} d\). In both algorithms, the projection directions are chosen in a data-independent manner. One limitation of our current work is that, even though the second algorithm adapts to the manifold structure, there is a large constant, possibly depending exponentially on \(d\), involved in bounding the excess Bayes risk, that is hidden under the Big Oh notation. In the future, we plan to investigate various data-dependent projection direction choices for a sparse representation, that would adapt to a manifold structure, and the constant involved in bounding of the excess Bayes risk from above, would be independent of ambient dimension \(d\).

**Acknowledgements:** We thank the anonymous reviewers for their constructive feedback. This work is supported by funding from the "NSF AI Institute for Foundations of Machine Learning (IFML)" (FAIN:2019844).

Figure 2: Empirical evaluation of Alg. 1, \(k\)-NN (for \(k=1\) and \(10\)) and RF on eight datasets Here expansion factor is \(m/d\). An error bar in the form of a shaded graph is provided for Alg. 1 over 10 independent runs.