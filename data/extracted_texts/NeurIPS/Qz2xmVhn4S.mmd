# Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?

Ruisheng Cao\({}^{}\)\({}^{}\)\({}^{}\)  Fangyu Lei \({}^{}\)  Haoyuan Wu \({}^{}\)  Jixuan Chen \({}^{}\)  Yeqiao Fu \({}^{}\)  Hongcheng Gao \({}^{}\)

Xinzhuang Xiong \({}^{}\)  Hanchong Zhang \({}^{}\)  Yuchen Mao \({}^{}\)  Wenjing Hu \({}^{}\)  Tianbao Xie \({}^{}\)  Hongshen Xu \({}^{}\)

Danyang Zhang \({}^{}\)\({}^{}\)  Sida Wang  Ruoxi Sun \({}^{}\)  Pengcheng Yin \({}^{}\)  Caiming Xiong \({}^{}\)  Ansong Ni \({}^{}\)

Qian Liu \({}^{}\)  Victor Zhong \({}^{}\)  Lu Chen \({}^{}\)  Kai Yu \({}^{}\)  Tao Yu \({}^{}\)

\({}^{}\) The University of Hong Kong \({}^{}\)  Shanghai Jiao Tong University

\({}^{}\)  Google Cloud AI Research \({}^{}\)  Google DeepMind \({}^{}\)  Salesforce Research

\({}^{}\)  Yale University \({}^{}\)  Sea AI Lab \({}^{}\)  University of Waterloo

ruishengcao@gmail.com  tyu@cs.hku.hk

Work done while interning at the University of Hong Kong.

###### Abstract

Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring \(494\) real-world tasks in authentic computer environments and incorporating \(20\) enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (\(14.0\%\) success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (\(16.2\%\)) and involve remote cloud-hosted workspaces (\(10.6\%\)). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.

## 1 Introduction

Data science and engineering pipelines usually rely on professional data software systems such as BigQuery, dbt, and Airbyte to acquire, process, and orchestrate large-scale data. Utilizing these enterprise systems involves writing SQL and Python code, as well as frequent and repetitive graphical user interface (GUI) controls, which can be complex even for experienced data scientists and engineers. With rapid advances in large language models (LLMs) and vision language models (VLMs), LLM/VLM-based autonomous agents have the potential to automate these workflows , enhancing productivity for data scientists and engineers  while democratizing access to large-scale data .

Previous studies on data agents focused mainly on daily life data processing and analysis by generating code or API calls , neglecting other crucial stages of data science and engineering (_e.g.,_ data ingestion and integration) using enterprise applications (_e.g.,_ Snowflake, Airflow, and Dagster). Additionally, to complete data workflows, data scientists and engineers often need to navigate multiple professional data systems, combining code writing with intensive GUI controls, such as navigating web pages and clicking buttons . However, there is currently no benchmark that integrates both code generation and GUI controls for professional data science and engineering.

To address this gap, we propose Spider2-V, the first multimodal agent benchmark covering the entire data science and engineering workflow, involving \(494\) real-world tasks in a real-time executable computer environment and \(20\) professional enterprise data software. Spider2-V aims to evaluate a multimodal agent's ability to perform professional data-related tasks by writing code and managing the GUI in enterprise data software systems, including data warehousing (_e.g.,_ BigQuery), data ingestion and integration (_e.g.,_ Airbyte), data transformation (_e.g.,_ dbt), data analysis and visualization (_e.g.,_ Superset), and data orchestration (_e.g.,_ Dagster). These tasks are derived from real-world practices, such as official tutorials on professional applications and open-source data engineering projects (with two task examples presented in Figure 1). We also supplement retrieval-augmented agents with official documentation and tutorials of these software systems to assess their capability to generalize and learn from these resources.

Each task in Spider2-V is defined within an executable computer environment based on OSWorldD , which allows multimodal agents to simulate human actions (_e.g.,_ typing code or clicking buttons) in a realistic setting. Specifically, a multimodal agent can observe real-time image-style screenshots and text-style accessibility tree of professional data applications in the current workflow and execute its predicted actions in dynamic multi-round interaction with the computer. This environment is connected to the real-world Internet, allowing the inclusion of professional software requiring authentic user accounts (_e.g.,_ Snowflake). To ensure reproducible and reliable experiments with this enterprise data software, \(10\) authors with computer science backgrounds developed \(170\) automatic task setup configurations and \(151\) customized evaluation metrics in total.

We experiment with state-of-the-art LLMs and VLMs including closed-source ones GPT-4 series , Gemini-Pro-1.5 , Claude-3-Opus , QWen-Max  and open-source representatives Mixtral

Figure 1: Spider2-V is a multimodal agent benchmark spanning across complete data science and engineering workflows (_e.g.,_ two task examples in the Figure above). It involves various professional enterprise-level applications and includes intensive GUI controls apart from code writing throughout the real-time multi-turn interaction with an executable computer environment.

8x7B  and Llama-3-70B . Performances reveal that even the top-tier VLM (GPT-4V ) achieves only \(14.0\%\) success rate. In the most challenging subset, with action steps exceeding \(15\), the performance drops to \(1.2\%\). And for those open-source LLMs, the success rate is less than \(2\%\). This indicates that existing LLMs or VLMs are still far away from achieving full data workflow automation. Even provided with an oracle step-by-step plan, the overall performance only increases to \(16.2\%\). This observation uncovers the poor capability of action grounding (_e.g._, identifying the precise coordinates of elements in the current focused application window) for multimodal agents. Furthermore, extensive analysis (SS 4.3) on Spider2-V demonstrate that these strategies remarkably promote the final performance, which include enhancing the alignment between different observation modalities, introducing feedback on action execution, integrating retrieved document context and enlarging the history trajectory length. These findings lay the groundwork for developing practical multimodal agents that can revolutionize the automation of data science and engineering workflows.

## 2 Executable Computer Environment of Spider2-V

In this section, we introduce the real-time executable computer environment of Spider2-V, which is built upon virtual machines (VMs) and adapted from OSWorld.

### Task Definition

Generally, an autonomous data agent is modeled as a partially observable Markov decision process (POMDP). Given the current observation \(o_{t}\) which includes a natural language instruction and a screenshot, accessibility tree (al1ytree), or their combination, an agent generates an executable action \(a_{t}\). This action can be clicking on a certain pixel of the screen (CLICK(560, 200)), or writing code through keyboard (TYPE("ls -lh")). The execution of \(a_{t}\) results in a new state \(s_{t+1}\) (_e.g._, the updated computer state) and a new partial observation \(o_{t+1}\). The al1ytree is a text-style representation of the desktop environment, which describes the status, position, and text content of each element (e.g., windows, buttons, and input boxes). The interaction loop repeats until an action that marks termination (DONE or FAIL) is generated or the agent reaches the max number of steps. See App. D for more details about the observation space and action space.

### Environment Setup

To ensure that an agent starts from a consistent initial state, we invoke a series of function calls from a pre-stored virtual machine (VM) snapshot to reset the environment. These function calls vary among tasks, resulting in \(170\) initial states. And we summarize \(5\) universal categories (see Figure 2), namely: 1) _File Transfer_: transfer files or project archives (either from local or cloud storage) into the VM; 2) _Application Launch_: open software on the desktop, _e.g.,_ Visual Studio Code and Chromium; 3) _Remote API Calls_: invoke tool-specific API calls for professional applications, especially those requiring authentic user accounts, to reset and configure cloud workspaces; 4) _Script Execution_: execute a shell script in VM to set up the initial state, _e.g.,_ run a Docker container to start a localhost webserver for Superset; 5) _Playwright Automation_: run web browser simulation with Playwright, _e.g.,_ sign into an account or click a specific button and redirect to the target web page.

Figure 2: Five common operations to reset the initial environment.

### Task-specific Evaluation

After the interaction terminates, we only have access to the open-ended resulting state of the computer. Thus, to measure whether the goal of each task is accomplished, we write task-specific functions to retrieve the desired result from the open-ended resulting state and return the success flag (\(0/1\)). All evaluation methods (in total, \(151\) evaluation scripts across the entire benchmark) can be classified into \(3\) generic categories, also shown in Figure 3:

1. [label=)]
2. _File-based comparison_: this method finds and copies the target files from VM to the host, and resorts to file-type based metrics (e.g.,.json,.csv, etc.) to compare the specified aspect of the generated file with ground truth. Sometimes, the ground truth may be updated over time. In this case, we will fetch the latest labels from the Internet during evaluation.
3. _Information-based validation_: this scheme is usually utilized to extract and check desired information from the computer. For example, in Figure 3(b), we want to confirm whether the time schedule of the data transportation is correctly configured in Airbyte. We can invoke Airbyte APIs to retrieve, or Chromium Playwright to locate the target value.
4. _Execution-based verification_: to verify whether an expected goal is achieved, we may also need to first execute a complicated Shell script in the final VM. For example, in Figure 3(c), we manually trigger the target Airflow DAG 2 and check the eventual status through running logs. 
## 3 Benchmark Construction

In this section, we introduce the general annotation pipeline, document warehouse construction, and dataset statistics for Spider2-V. For concrete examples, refer to App. H.

### Annotation Pipeline

To construct tasks in different categories, we find that official tutorials of enterprise applications serve as an excellent starting point. The \(6\)-step annotation pipeline is illustrated in Figure 4(a), and we elaborate it with a concrete and real example "_Orchestrate dbt Core jobs with Airflow and Cosmos_" 3:

1. [label=0)]
2. **Collect tutorials:** firstly, we find tutorials from official websites for each professional tool in Figure 5. In total, \(10\) annotators collected \(217\) source URLs. Note that these tutorials may utilize other professional software, e.g., MySQL. All involved professional tools are listed in App. B.
3. **Learn tutorials:** the annotator selects one tutorial, learns and realizes it in the VM. After that, they can summarize key knowledge points from this tutorial. For example, in Figure 4(b), five key steps in integrating a dbt project into an Airflow task are extracted.

Figure 3: Three generic methods for task evaluation.

3. **Write instructions:** since the chosen tutorial is extremely complicated, the annotator can select a few key points to construct the task instruction. In Figure 4, we only select key steps _iv)_ and _v)_ to write two versions of instructions, _abstract_ and _verbose_, indicating different levels of proficiency. Note that, to avoid potential data contamination and make the task more realistic, we ask the annotator to introduce at least two modifications to the raw tutorial. In this example, we a) replace the original "my_simple_dbt_project" into an open-source dbt project called "jaffle-shop" 4, and b) add one extra requirement on the time schedule (10:00 a.m. daily). 4. **Write environment setup functions:** the next step is to write initialization functions using operations defined in SS 2.2. In the example above, we need to: a) Upload an unfinished Airflow project into the VM. b) Execute a Shell script to launch the web server (via Docker containers) for Airflow under the project folder. c) Open all relevant applications on the desktop to simulate real user scenarios. d) Use Playwright to auto-login to the default Airflow account.
5. **Write task-specific evaluation functions:** In this step, annotators are required to programmatically obtain results from the open-ended states of VM and assess whether the task is completed using methods in SS 2.3. In this example, the evaluator contains: a) manually run the target Airflow DAG and verify the final status is "success"; b) using Airflow CLIs to retrieve details of the target Airflow DAG, and compare dbt sub-tasks, status and schedule with ground truth.
6. **Cross-validate on VM:** to ensure correctness, we go through strict cross-validation. Each annotated task is sent to two other annotators to check: a) whether the chosen task reflects a real-world use case; b) whether verbose instruction accurately fulfills the task and its requirements in the abstract instruction; c) whether the environment can be reset to the same state in different trials; d) whether the evaluation is robust when we exactly follow the verbose instruction or modify some inconsequential steps; e) whether the evaluation score is \(0\) if we deliberately make some mistakes (red-teaming). The task is preserved only if it withstands all these tests.

### Document Warehouse

Even senior data scientists query official documentation of professional applications when completing a complicated data engineering task. To compensate for the deficiencies of the data agents in utilizing enterprise professional software (e.g., unaware of coding specifications or APIs), we build a document warehouse for Spider2-V. Concretely, we recursively crawl the web pages from the root websites of the professional applications in Figure 5. After pre-processing through heuristics (refer to App. C), raw HTML web pages are convert into \(3\) different formats for retrieval, namely a) pure text, b) markdown, and 3) simplified HTML. Eventually, we obtain \(11,231\) documents in total.

Figure 4: The annotation pipeline of one selected demonstration from the Airflow tutorial: _Orches-rate dbt Core jobs with Airflow and Cosmos_. On average, annotating each task costs roughly \(4\) hours.

### Dataset Statistics

TasksWe classify all \(494\) tasks in Spider2-V into \(7\) categories and \(11\) software sub-categories with main statistics in Figure 5 and Table 1. Specifically, most (\(280\) tasks, \(56.7\%\)) involve CLI and GUI operations. And \(34\%\) examples request registering authentic software accounts. Since each task is associated with a detailed, step-by-step tutorial (verbose instruction), the entire task set can be categorized into three distinct levels based on the number of actions in these instructions. The proportion of easy, medium, and hard tasks is approximately \(1:2:1\). According to the rightmost distribution depicted in Figure 6, most tasks necessitate the coordinated utilization of multiple professional applications, thereby establishing Spider2-V as a particularly challenging benchmark.

Comparison with existing benchmarksIn Table 2, we compare Spider2-V with other agent benchmarks. Spider2-V incorporates generic computer control commands into the field of data science and engineering and is distinguished by these salient features: 1) a real-time executable environment. Instead of providing static input-output pairs, Spider2-V is equipped with a dynamic computer desktop such that agents can proactively explore it; 2) multiple enterprise software. We integrate \(20\) professional applications into the benchmark, which include not only tools installed on local hosts but also cloud-based enterprise services; 3) intensive GUI operations. Unlike traditional coding or data science domains, experienced data scientists frequently manipulate the UIs of those professional software to simplify the data workflow (_e.g._, enabling a specific function on the UI page or visualizing the graph view of data inputs). In summary, Spider2-V focuses on the use of professional enterprise software with visual interface in an interactive computer environment.

## 4 Experiments and Analysis

In this section, we introduce the experiment settings, experimental results, and ablation study to assess the proficiency of current LLM or VLM based agents on Spider2-V benchmark.

  
**Statistics** & **Number** \\ 
**Total Tasks** & **494 (100\%)** \\ - Pure CL1 & 28 (5.7\%) \\ - Pure GUI & 186 (37.7\%) \\ - CLI + GUI & 280 (56.7\%) \\  - w. Authentic User Account & 170 (34.4\%) \\ - w.o. Authentic User Account & 324 (65.6\%) \\ 
**Level (Action Steps)** &  \\ - Easy (\( 5\)) & 98 (19.8\%) \\ - Medium (\(6 15\)) & 310 (62.8\%) \\ - Hard (\(>15\)) & 86 (17.4\%) \\ - Avg. Action Steps & 4.0 / 9 / 6 / 22.0 \\   & 37.1 \\  & 191.5 \\  & 2.5 \\   

Table 1: Statistics of Spider2-V.

Figure 5: Task categories with professional tools.

Figure 6: Distribution of action steps, instruction length, and related applications per task.

### Environment Settings

Agent baselinesThe baseline method includes \(3\) schemes in zero-shot prompt learning: 1) Set-of-Mark (SoM, ): following OSWorld and VisualWebArena, we adopt heuristic methods to retrieve coordinates of visible elements from allytree (a text-format observation type) and draw indexed bounding box for these elements on the screenshot. We further insert these indexes into the pruned allytree to enhance the alignment between screenshot and allytree. 2) Execution Feedback (EF, ): we append execution feedback messages of those actions which failed to be grounded in the environment due to unexpected errors. The two techniques mentioned above are elaborated in App. D.3.1. 3) Retrieval-Augmented Generation (RAG, ): we leverage the task instruction as the query vector, bge-large-en-v1.5 as the embedding model, and LlamaIndex framework as the retrieval to generate document context for each task example. Documents are pre-chunked into segments with maximum length \(512\) and tokens overlapping size \(20\). Top \(4\) segments are selected as additional context in the task prompt (detailed in App. I.3).

LLMs and VLMsWe experiment with state-of-the-art LLMs and VLMs, including open-source representatives such as Mixtral-8x7B  and Llama-3-70B , and closed-source ones including Qwen-Max , Gemini-Pro-1.5 , Claude-3-Opus  and GPT  families (GPT-4o and GPT-4V 5). With respect to the two open-source LLMs and QWen-Max, we utilize pure text-format allytree as the observation type on account of their incapability of image processing. For the remaining \(4\) VLMs which support vision input, we use aligned text and image (that is Set-of-Mark) as the observation type in main experiments. Unless otherwise specified, we set the temperature to \(0.5\) and top_p to \(0.9\), the history trajectory window size to \(3\), the maximum length of allytree to \(5000\) tokens, and the maximum output tokens to \(1500\) in each turn. Heuristically, we require the agent to complete the tasks within both \(15\) interaction turns and one hour, which suffices for most tasks 6.

### Main Results

In Table 3, we compare performances of different LLMs and VLMs. All results above integrate techniques of both execution feedback (EF) and retrieval-augmented generation (RAG) in SS 4.1. Accordingly, we can summarize that:

1. **Existing data agents are far from satisfactory in completing real-world data science and engineering tasks.** Even state-of-the-art VLMs (GPT-4o and GPT-4V) perform terribly on

    &  & **Exec.** & **Ent.** & **GUI** & **\# Apps/** & **\# Exec.-based** &  \\  & & **Env?** & **Serv?** & **Support?** & **Sites** & **Eval. Func.** & \\   Spider  & Text-to-SQL & & & & 1 & 0 & 1034 \\ DS-1000  & Data Science & & & & 1 & 0 & 1000 \\ Arcade  & Data Science & & & & 1 & 0 & 1082 \\ MLAgentBench  & Machine Learning & & & & 4 & 13 & 13 \\ SWE-Bench  & Software Engineering & & & & 12 & 1 & 2294 \\  Mind2Web  & Web & & & & 137 & 0 & 2000 \\ WEBLINX  & Web & & & & 155 & 0 & 2337 \\ WorkArena  & Web & & & & 1 & 7 & 29 \\ AppAgent  & Android & & & & 10 & 0 & 50 \\ AndroidWorld  & Android & & & & 20 & 6 & 116 \\ WebArena  & Web & & & & 5 & 5 & 812 \\ OSWorld  & Computer Control & & & & 9 & 134 & 369 \\  Spider2-V & Data Science \& Engineering & & & & 20 & 151 & 494 \\   & w/ Computer Control & & & & & & \\   

Table 2: Comparison with existing agent benchmarks. Columns include the research field (Field), whether an executable environment is provided (Exec. Env.?), whether enterprise service is utilized (Ent. Serv.?), whether GUI actions are supported (GUI Support?) and some other statistics.

Spider2-V, achieving at best \(14.0\%\) overall success rate. As for their strongest competitors, Gemini-Pro-1.5  and Claude-3-Opus , they attain worse performances, even less than \(10\%\) percents. There is still ample room for improvement in future work.
2. **Closed-source models are much more superior than open-source ones**. For those open-source LLMs, the success rate is less than \(2\%\), with some categories approaching zero. On one hand, it can be attributed to the fact that closed-source VLMs are pre-trained and fine-tuned on data of higher quality. On the other hand, closed-source VLMs support inputs with longer contexts and integrate both vision and text modalities (further analyzed in SS 4.3).
3. **Performances of data agents exhibit high variance, especially in categories "_data ingestion_" and "_data visualization_".** The majority of these two partitions are pure GUI tasks, which means agents mostly interact with the environment through time-dependent GUI operations. However, a minor error in one intermediate step can be amplified, resulting in the entire sequence of actions being wasted. Through error analysis on trajectories, we discover that once agents mispredict the coordinates of the correct button, they will open the wrong window and become trapped in the incorrect area, unable to return.
4. **Across \(7\) data categories, the partitions "_data warehousing_" and "_traditional data processing_" are challenging, both less than \(10\%\) success rates._ The reasons for this observation are two-fold: a) _data warehousing_ tasks mostly involve authentic user accounts (_e.g._, BigQuery and Snowflake). Compared to other tasks which can be accomplished in a local host, these dynamic real-world scenarios incur extra burden on data agents, such as network connection delay and pop-up windows. Multimodal agents need to deal with these unexpected situations in real-time interaction with the computer. b) As for _traditional data processing_, the bottleneck is that spreadsheets in Excel contain many cells, and it is particularly difficult for data agents to accurately locate the coordinates of cells. For example, applying the same math formula to the entire column requests multimodal agents to firstly pinpoint the right corner of a specific cell, wait for the mouse to become a cross, press and drag the mouse towards the target cell. This series of actions requires precise and fine-grained GUI controls which are difficult to implement.

### Analysis

In this section, we delve into different factors which influence the eventual success rates, and analyze the underlying logics. The following analyses are based on our agent baseline with VLM GPT-4o unless otherwise specified. Firstly, we split the overall results into different subsets in Table 4.

1. **Tasks with more inherent action steps are more difficult.** Each task is associated with one verbose task instruction which gives a step-by-step guidance on how to complete it. We count the number of actions in the verbose instruction and split the entire task set into \(3\) difficulty levels: \( 5\) steps (Easy), \(5 15\) steps (Medium), and \(>15\) steps (Hard). Not surprisingly, as the number of intrinsic action steps increases, the average performance decreases significantly. And for tasks with steps more than \(15\), existing VLM-based data agents can hardly accomplish the goal.

   &  &  \\   & & _ware._ & _trans._ & _inges._ & _visual._ & _orches._ & _proc._ & _serv._ & **Overall** \\   Mistral-8x7B & & \(1.2\) & \(0.0\) & \(0.0\) & \(0.0\) & \(2.6\) & \(0.9\) & \(0.0\) & \(0.8\) \\ Llama-3-70B & a11ytree & \(2.4\) & \(0.0\) & \(0.0\) & \(2.5\) & \(3.9\) & \(2.8\) & \(0.0\) & \(2.0\) \\ Qwen-Max & & \(1.2\) & \(0.0\) & \(0.0\) & \(0.0\) & \(2.6\) & \(0.0\) & \(0.0\) & \(0.6\) \\  Claude-3-Opus & & \(2.4\) & \(2.5\) & \(10.4\) & \(15.0\) & \(11.5\) & \(3.8\) & \(12.1\) & \(8.1\) \\ Gemini-Pro-1.5 & & \(3.6\) & \(2.5\) & \(14.6\) & \(15.0\) & \(10.3\) & \(2.8\) & \(\) & \(9.1\) \\ GPT-4o & & \(7.2\) & \(7.5\) & \(\) & \(14.1\) & \(\) & \(\) & \(13.8\) & \(13.8\) \\ GPT-4V & & \(\) & \(\) & \(12.0\) & \(\) & \(18.4\) & \(8.5\) & \(12.1\) & \(\) \\  

Table 3: Success rates of baseline agents on Spider2-V grouped by \(7\) task categories (see Figure 5), namely data warehousing (_ware._), transformation (_trans._), ingestion (_inges._), visualization (_visual._), orchestration (_orche._), traditional data processing (_proc._), and IT service management (_manag._). For the first three LLMs, since they do not support visual information, we only utilize the text-based allytree as the observation. For the remaining four VLMs, we adopt Set-of-Mark (see § 4.1).

2. **Tasks involving authentic user accounts are much more challenging.** One salient feature of Spider2-V is the integration of professional applications that require authentic user accounts. We also split the entire task set accordingly (w/o or w/ account). Notably, data agents struggle to complete tasks involving authentic user accounts (\(10.6\%\) success rate). These tasks deal with real-world scenarios and incorporate cloud-hosted enterprise services. Compared with Web servers which are launched locally in the VM (_e.g._, from Docker containers), the cloud Web UIs 1) generally integrate more comprehensive functionalities or options in their menu panel, and 2) potentially suffer from emergency situation, such as extended network response delay due to bandwidth limitation or server overload. We conjecture these two causes collectively contribute to the inferior performances.
3. **Incorporating GUI operations typically lead to improved performances.** We split the task set by interfaces. If the task can be completed with pure CLIs (e.g., code editor or bash terminal), we classify it as \(\). If the task only requires the agent to manipulate the GUI (usually on the Web page), we classify it into \(\). For the remaining cases (\(\)), an agent must write code or scripts, and control the UI screen. We observe that pure \(\) tasks are much easier than \(\) tasks. This conclusion can be explained by the following two reasons: 1) GUIs of professional applications are designed to simplify the original coding task. Clicking buttons or typing values on UIs can avoid handling the rigorous and complex coding specification. 2) Both observation types, namely the screenshot and \(\), are naturally proposed for GUI tasks. For pure \(\) tasks, data agents must perform extra actions to locate and switch to the target panel before writing code.
4. **Providing a step-by-step guideline in task instructions results in remarkable performance gains.** The key difference between abstract and verbose instructions (the third step in SS 3.1) is whether a detailed step-by-step guidance is offered. With such stepwise oracle tutorials, data agents do not need to reason and plan, thus dramatically simplifying the original task. And the \(4.8\) points improvement in Table 4 consolidates this hypothesis. Nevertheless, the low success rate with verbose instructions (\(16.2\%\)) indicates that current VLMs still yield unsatisfactory results when purely grounding actions in real-world contexts. And significant potential remains for further enhancement.

In Table 5, we analyze the influence of different combinations of action space, observation types, and the \(3\) techniques described SS 4.1. The findings include: 1) **Regarding action space,**yautogui **code slightly outperforms self-customized JSON dict (\(\) v.s. \(\)).** This can be attributed to the advantage that agents can also generate functional Python code like file traversal apart from the limited GUI control operations using the first action space. And it improves the efficiency of action grounding. 2) **As for observation types, single screenshot leads to very low performances (\(\)) on account of the agent's failure in pinpointing concrete elements.** When inserting \(\) into the observation which contains precise coordinates, the agent capability of locating target pixels is remarkably promoted. 3) **All 3 methods we integrate into the agent baseline (namely SoM, EF and RAG) will boost eventual performances.** It is interesting that if we do not adopt Set-of-Mark (that is, enhancing the alignment between two modalities of observations), the result of screenshot+\(\) is even worse than that using pure \(\). This emphasizes the significance of modal alignment when handling state observations.

 
**Task Splits** & **Ratio (\(\%\))** & **SR (\(\%\))** \\   Easy & 19.8 & **38.8** \\ Medium & 62.8 & 9.7 \\ Hard & 17.4 & 1.2 \\  w/o account & 66.0 & **15.6** \\ w/ account & 34.0 & 10.6 \\  \(\) & 5.7 & 7.1 \\ GUI & 37.7 & **20.1** \\ \(\) & 56.7 & 10.6 \\  Abstract & 50 & 11.3 \\ Verbose & 50 & **16.2** \\  

Table 4: Success rate of GPT-40 with agent base-line SoM+EF+RAG across different partitions.

 
**Action** & **Observation** & **SR (\%)** \\
**Space** & **Types** & **4.2** \\   JSON dict & screenshot & 4.2 \\ pyautogui & \(\) & 10.5 \\ pyautogui & \(\) & **12.6** \\   & screenshot+\(\) & 11.4 \\   & w/ Set-of-Mark & 15.6 \\ pyautogui & w/ exec. feedback & 13.6 \\  & w/ retrieval aug. & 14.4 \\  & w/ all tricks & **16.3** \\  

Table 5: Ablation study on action space, observation types and \(3\) methods in § 4.1 on task subset.

A moderate temperature and longer history window size improve performances.In Figure 7, we investigate the influences of two hyper-parameters on a task subset: 1) The top-ranked performance is achieved with sampling temperature \(0.5\). 2) With the history window size enlarges, from \(0\) (no history, only the current observation) to \(3\), the performance increases stably. However, due to constraints on input length and considerations of cost-effectiveness, we are unable to extend the history trajectories any further. This also points out that the interaction efficiency is a serious issue and promising research direction.

## 5 Related Work

Benchmarks for data science and engineeringIn the field of data science and engineering, several recent works propose novel benchmarks to evaluate the capabilities of LLM agents in manipulating Excel spreadsheets [18; 4], common data science libraries (_e.g._, SQL and pandas) [45; 17; 11; 43], machine learning  or software engineering  projects. They are usually confined to a single stage within the entire data pipeline, predominantly data processing and analysis, thus overlooking other stages such as data warehousing and orchestration from a broader perspective. Besides, like other coding-related datasets [41; 32; 44], they merely focus on the command line interface, neglecting the fact that enterprise software usually has rich graphical user interfaces (GUIs). And data scientists often combine code programming with intensive GUI operations to fulfill a data workflow. To this end, Spider2-V is proposed as the first-of-its-kind multimodal agent benchmark in the field of data science and engineering, which covers the entire data workflow and integrates visual interfaces.

Benchmarks for multimodal agentsExisting works on GUI interaction mainly encompass web navigation [29; 19; 42; 5; 16], mobile device [46; 47; 26; 27; 33], and computer desktop [37; 35; 7; 15]. One trend of recent advanced benchmarks is to provide an executable simulation environment. Multimodal agents can explore and interact with this platform through keyboard, mouse, gesture and touch screen actions in a more realistic and complex scenario. However, previous literature mostly focuses on daily life applications (_e.g._, Web browser and calendar) [38; 25] or workflows of non-specialized business tasks . Few works [6; 37; 34] investigate the capability of multimodal agents to manipulate enterprise-level software. GUIs of professional applications often contain abundant domain-specific terminologies (_e.g._, "_materialization_" in Dagster), which requires multimodal agents to understand the specialized knowledge. Spider2-V incorporates \(20\) professional tools into a real-time computer environment to test the proficiency of agents in data science and engineering. Furthermore, we supplement a large volume of documents for retrieval to compensate for deficiencies of agents in domain knowledge.

## 6 Conclusion

In this work, we propose Spider2-V, the first data science and engineering benchmark which integrates enterprise professional applications and supports intensive GUI operations besides code writing across the full data pipeline. It contains \(494\) tasks, involves \(20\) professional tools, and provides a real-time executable computer environment. The most advanced VLM (GPT-4V) still performs poorly on Spider2-V (achieving \(14.0\%\) success rate), rendering it a very challenging benchmark. Although current multimodal agents are still far from automating data workflows, Spider2-V presents an easily accessible benchmark and lays the foundation for future research.

Figure 7: Ablation study on hyper-parameters.