# TopoFR: A Closer Look at Topology Alignment on Face Recognition

Jun Dan\({}^{*}\)1,2, Yang Liu\({}^{*}\)2,3, Jiankang Deng\({}^{4}\), Haoyu Xie2,5, Siyuan Li\({}^{2}\), Baigui Sun\({}^{1,2,5}\), Shan Luo\({}^{3}\)

\({}^{1}\)Zhejiang University \({}^{2}\)FaceChain Community \({}^{3}\)King's College London

\({}^{4}\)Imperial College London \({}^{5}\)Alibaba Group

danjun@zju.edu.cn, {yang.15.liu, shan.luo}@kcl.ac.uk, j.deng16@imperial.ac.uk

xiehaoyu.xhy@alibaba-inc.com, sunbaigui85@gmail.com

 Equal Contribution, \({}^{}\) Corresponding Author.

###### Abstract

The field of face recognition (FR) has undergone significant advancements with the rise of deep learning. Recently, the success of unsupervised learning and graph neural networks has demonstrated the effectiveness of data structure information. Considering that the FR task can leverage large-scale training data, which intrinsically contains significant structure information, we aim to investigate how to encode such critical structure information into the latent space. As revealed from our observations, directly aligning the structure information between the input and latent spaces inevitably suffers from an overfitting problem, leading to a structure collapse phenomenon in the latent space. To address this problem, we propose TopoFR, a novel FR model that leverages a topological structure alignment strategy called PTSA and a hard sample mining strategy named SDE. Concretely, PTSA uses persistent homology to align the topological structures of the input and latent spaces, effectively preserving the structure information and improving the generalization performance of FR model. To mitigate the impact of hard samples on the latent space structure, SDE accurately identifies hard samples by automatically computing structure damage score (SDS) for each sample, and directs the model to prioritize optimizing these samples. Experimental results on popular face benchmarks demonstrate the superiority of our TopoFR over the state-of-the-art methods. Code and models are available at: https://github.com/modelscope/facechain/tree/main/face_module/TopoFR.

## 1 Introduction

Face recognition (FR) is a critical biometric authentication technique that is widely applied in various applications. In recent years, convolutional neural networks (CNNs) have achieved remarkable success in FR task, thanks to their powerful ability to autonomously extract face features from images. Existing studies on FR primarily focuses on constructing more discriminative face features by developing margin-based loss functions  and powerful network architectures . Recently, the success of unsupervised learning  and graph neural networks  has demonstrated the importance of data structure information in improving model generalization. However, to the best of our knowledge, how to effectively mine the potential structure information in large-scale face data has not investigated. Thus, in this paper, we extend our interests on building a cutting-edge FR framework through exploiting such powerful and substantial structure information.

First, we use Persistent Homology (**PH**) , a mathematical tool used in topological data analysis  to capture the underlying topological structure of complex point clouds, to investigatethe evolution trend of structure information in existing FR framework and illustrate 3 interesting observations: **(1)** as the amount of data increases, the topological structure of the input space becomes more and more complex, as verified in Figures 1a-1d; **(2)** as the amount of data increases, the topological structure discrepancy between the input space and the latent space becomes increasingly larger, as verified in Figure 2a; **(3)** The results in Figure 2b demonstrate that as the depth of the network increases, the topological structure discrepancy becomes progressively smaller. This finding also provides an explanation for why models with more complex structure achieve higher FR accuracy. Based on the above observations, we can infer that in FR tasks with large-scale datasets, the structure of face data will be severely destroyed during training, which limits the generalization ability of FR models in practical application scenarios. To this end, we propose to improve the generalization performance of FR models by preserving the structure information.

However, we experimentally find that directly using PH to align the topological structures of the input space and the latent space may cause the model to suffer from **structure collapse phenomenon**. Concretely, under this experimental setting, we have 2 following quantitative results: **(1)** As shown in Figure 2c, the topological structure discrepancy drops to 0 dramatically during early training. **(2)** As depicted in Figure 2d, when evaluating on the IJB-C benchmark , there exists a significant structure information gap between the input space and the latent space. These typical overfitting phenomena indicate the latent space fails to preserve the structure information of input space accurately.

To remedy this issue, we propose a superior FR model named **TopoFR** that leverages a Perturbation-guided Topological Structure Alignment (**PTSA**) strategy to adequately preserve the topological structure information of the input space in corresponding latent face features. PTSA first employs a random structure perturbation (**RSP**) mechanism perturb the latent space and effectively increase its structure diversity. Then PTSA utilizes an invariant structure alignment (**ISA**) mechanism to align the topological structures of the original input space and the perturbed latent space, resulting in face features with stronger generalization ability

Moreover, in practical FR scenarios, the training dataset typically includes some low-quality face samples (_i.e._, hard samples) that are prone to being encoded into abnormal positions close to the decision boundary in the latent space , significantly destroying the topological structure of the latent space and affecting the alignment of structure. To address this issue, we propose a novel hard sample mining strategy named Structure Damage Estimation (**SDE**). SDE adaptively assigns structure damage score (**SDS**) to each sample based on its prediction uncertainty and prediction

Figure 1: We sample 1000 (a), 5000 (b), 10000 (c) and 100000 (d) face images from the MS1MV2 dataset respectively, and compute their persistence diagrams using PH, where \(H_{j}\) represents the \(j\)-th dimension homology. Persistence diagram  is a mathematical tool to describe the topological structure of space, where the \(j\)-th dimension homology \(H_{j}\) in persistence diagram represents the \(j\)-th dimension hole in space. In topology theory, if the number of high-dimensional holes in the space is more, then the underlying topological structure of the space is more complex . As shown in Figure 1(a)-1(d), as the amount of face data increases, the persistence diagram of the input space contains more and more high-dimensional holes (_e.g._, \(H_{3}\) and \(H_{4}\)). Therefore, this phenomenon demonstrates a growing complexity in the topological structure of the input space.

probability. By prioritiizing the optimization of hard samples with significant structure damage, SDE can gradually guide these samples back to their reasonable positions, thereby improving the generalization ability of FR model.

In summary, the main contributions are listed as follows:

**1)** To the best of knowledge, we are the first to explore the topological structure alignment in FR task. We propose a novel topological structure alignment strategy called PTSA to effectively align the structures of the original input space and the perturbed latent space.

**2)** A novel hard sample mining strategy named SDE is introduced to mitigate the adverse impact of hard samples on the latent space structure.

**3)** Experimental results show that the proposed method outperforms SOTA methods on various face benchmarks. Notably, our TopoFR has secured **the second place** in the ICCV21 MFR-Ongoing challenge  until the submission of this work (May 22 '24, academic track): http://iccv21-mfr.com/#/leaderboard/academic, indicating the robustness and generalization of our method.

## 2 Related Works

**Face Recognition (FR).** Convolutional Neural Networks (CNNs) [28; 29] have achieved remarkable advancements in tasks related to facial recognition [30; 31; 32; 1; 33; 34]. Notably, the extraction of robust deep facial embeddings has raised considerable interest within the research community. Among them, CNNs framework are representative methods, using two primary methods: metric learning-based and margin-based softmax approaches. The former utilizes loss functions like Triplet loss , Tuplet loss , and Center loss  to learn discriminative face features, while the latter aims to incorporate margin penalty into the softmax loss framework, including methods such as ArcFace , CosFace , AM-softmax , and SphereFace . Recent studies have explored various techniques, including adaptive parameters [3; 5], mining [4; 39; 40], learning acceleration [41; 42; 43], vision transformer architecture [9; 8], and data uncertainty [25; 6; 26] to further enhance models' performance on large-scale datasets.

**Persistent Homology (PH).** Over the past decade, PH has shown significant advantages in multiple various such as signal processing, video analysis [45; 46], neuroscience [47; 48], disease diagnosis  and evaluation of embedding strategies [50; 51]. In the field of machine learning, some studies [52; 53; 54] have shown that integrating topological representations into network can enhance model's recognition/segmentation performance.  proposes a topology distance for the evaluation of GANs.

Figure 2: **(a): We investigate the relationship between the amount of data and the topological structure discrepancy by employing ResNet-50 ArcFace model  to perform inferences on MS1MV2 training set. Inferences are conducted for 1000 iterations with batch sizes of 256, 1024, and 2048, respectively. Histograms are used to approximate these discrepancy distributions. (b): We investigate the relationship between the network depth and the topological structure discrepancy by performing inference on MS1MV2 training set (batch size=128) using ArcFace models with different backbones. (c): We investigate the trend of topological structure discrepancy during training (batch size=128) and found that i) directly using PH to align the topological structures will cause the discrepancy to drops to 0 dramatically; ii) whereas using our PTSA strategy promotes a smooth convergence of structure discrepancy. (d): Aligning the topological structures directly using PH will lead to significant discrepancy when evaluating on IJB-C benchmark. Our PTSA strategy effectively mitigates this overfitting issue, resulting in smaller structure discrepancy during evaluation.**

## 3 Background: Persistent Homology

PH is a computational topology method that quantifies the changes in the topological invariants of a Vietoris-Rips complex as a scale parameter \(\) is varied. In this section, we introduce some key concepts of PH. Further details on PH can be found in Refs. [20; 56].

**Notation.**\(:=\{x_{i}\}_{i=1}^{n}\) represents a point cloud and \(:\) denotes a distance metric over \(\). Matrix \(\) represents the pairwise distances (_i.e._, Euclidean distance) between points in \(\).

**Vietoris-Rips Complex.** The Vietoris-Rips complex  is a special simplicial complex constructed from a set of points in a metric space, and it can be used to approximate the topology of the underlying space. For \(0<\), we represent the Vietoris-Rips complex of point cloud \(\) at scale \(\) as \(_{}()\), which contains all simplices (_i.e._, subsets) of \(\), and each component of point cloud \(\) satisfies a distance constraint: \((x_{i},x_{j})\) for any \(i,j\). Moreover, the Vietoris-Rips complex satisfies a nesting relation, _i.e._, \(_{_{i}}_{_{j}}\) for any \(_{i}_{j}\), which allows us to track the evolution progress of simplical complex as the scale \(\) increases. It is worth noting that \(_{}()\) and \(_{}()\) are equivalent because constructing the Vietoris-Rips complex only requires distance.

**Homology Group.** The homology group  is an algebraic structures that analyzes the topological features of a simplicial complex in different dimension \(j\), such as connected components (\(H_{0}\)), cycles (\(H_{1}\)), voids (\(H_{2}\)), and higher-dimensional features (\(H_{j},j 3\)). By tracking the changes in topological features (\(H_{j}\)) of the Vietoris-Rips complex as the scale \(\) increases, it is possible to gain insight into the multi-scale topological information of the underlying space.

**Persistence Diagram and Persistence Pairing .** The persistence diagram \(\) is a multi-set of points \((b,d)\) in the Cartesian plane \(^{2}\), which encodes information about the lifespan of topological features. Specially, it summarizes the birth time \(b\) and death time \(d\) of each topological feature, where birth time \(b\) signifies the scale at which the feature is created and death time \(d\) refers to the scale at which it is destroyed. The persistence pairing \(\) contains indices \((i,j)\) corresponding to simplices \(r_{i},r_{j}_{}()\) that create and destroy the topological features identified by \((b,d)\), respectively.

## 4 Methodology

In this paper, we propose a novel framework named **TopoFR** for constraining the FR model to preserve the topological structure information of the input space in their latent features. The architecture of our TopoFR model is depicted in Figure 3. It consists of two components: a feature extractor \(\) and an image classifier \(\). Mathematically, given an input image \(x\), the latent feature extracted by \(\) is denoted as \(f=(x)^{l}\), and the classification probability predicted by \(\) is denoted as \(g=(f)^{K}\), where \(l\) represents the feature dimension and \(K\) denotes the number of classes. The entropy of the classification prediction probability \(g\) can be represented as \(E(g)=-_{k=1}^{K}g^{k} g^{k}\), where \(g^{k}\) is the probability of predicting a sample to class \(k\).

Figure 3: Global overview of our proposed TopoFR. \(\) represents the multiplication operation. \(\) denotes the probability of applying RSP to each training sample.

### Perturbation-guided Topological Structure Alignment

As mentioned in Section 1, directly applying PH to align the topological structures of the input space and the latent space can cause the FR model to encounter structure collapse phenomenon. To remedy this problem, we propose a Perturbation-guided Topological Structure Alignment (**PTSA**) strategy that includes two mechanisms: random structure perturbation and invariant structure alignment.

**Random Structure Perturbation (RSP).** PTSA first utilizes the RSP mechanism to randomly perturb the structure of the latent space. Specially, it utilizes a data augmentation list \(=\{_{1},_{2},_{3},_{4}\}\) that includes four common data augmentation operations, namely Random Erasing \(_{1}\), GaussianBlur \(_{2}\), Grayscale \(_{3}\) and ColorJitter \(_{4}\). For each training sample \(x_{i}\), RSP will randomly select an operation \(_{r}\) from \(\) to perturb it, _i.e._, \(_{i}=_{r}(x_{i})\). Then the perturbed sample \(_{i}\) will be fed into the model for supervised learning, which effectively increases the structure diversity of the latent space. In our model, we adopt ArcFace loss  as the basic classification loss:

\[_{arc}(_{i},y_{i})=-^{n}+m)\}}}{e^{s((_{i}^{y}+m))}+_{k=1,k y }^{K}e^{s_{i}^{k}}},\] (1)

where \(y_{i}\) is the class label of the original image \(x_{i}\), \(s\) is a scaling parameter, \(_{i}^{k}\) is the angle between the \(k\)-th class center and feature, and \(m\) denotes an additive angular margin. During training, we apply the RSP mechanism to each sample \(x_{i}\) with a probability \(\) of 0.2.

**Invariant Structure Alignment (ISA).** Given a mini-batch of original training samples \(=\{x_{i}\}_{i=1}^{n}\), we denote the perturbed batch samples as \(}=\{_{i}\}_{i=1}^{n}\). For the perturbed samples, we denote the latent features extracted by \(\) as \(}=\{_{i}\}_{i=1}^{n}\). During forward propagation, we can construct the Vietoris-Rips complexes \(_{}()\) and \(_{}(})\) for point clouds \(\) and \(}\) respectively, based on their respective pairwise distance matrix \(^{}\)and \(^{}}\). Then we can utilize persistent homology to analyze the topological structures of \(_{}()\) and \(_{}(})\), and obtain their corresponding persistence diagrams \(\{^{},^{}}\}\) and persistence pairings \(\{^{},^{}}\}\), respectively.

Ideally, no matter how the face image is perturbed, the position of the encoded face feature in the latent space should remain unchanged, and the topological structure of the perturbed latent space should also be consistent with the original input space. To this end, we choose to align the original input space \(\) with the perturbed latent space \(}\) to achieve this goal. Prior studies usually utilize bottleneck distance or Wasserstein distance to measure the topological structure discrepancy [60; 18] between two spaces by comparing the differences in persistence diagrams. However, these two metrics are sensitive to outliers in persistence diagrams [55; 61] and will significantly increase models' training time, rendering them unsuitable for FR tasks with extremely large-scale datasets, as verified in Table 8 in the Appendix.

To mitigate this issue, we turn to retrieve the persistence diagrams values by subsetting the corresponding pairwise distance matrix with edge indices provided by the persistence pairings [62; 63; 64], _i.e._, \(^{}^{}[^{}]\) and \(^{}^{}}[^{ }}]\). By comparing the difference between two topologically relevant distance matrices from both spaces, we can quickly and stably compute the discrepancy between their persistence diagrams, providing an efficient solution for structure alignment of FR models driven by large-scale datasets. We formulate the ISA loss as follows:

\[_{sa}(^{},^{}})=(\|^{}[^{}]- ^{}}[^{}]\|^{2}+ \|^{}}[^{}} ]-^{}[^{}}]\|^{2})\] (2)

**Notably**, in the field of FR, most existing works do not include any data augmentation operations, as this would introduce more unidentifiable face images (_i.e.,_ destroying the fidelity of each face), which generally hurts the FR model's generalization ability, as verified in Refs. [3; 65]. In this work, we do not employ these data augmentations to simply augment data scale. Instead, we use them to increase the latent space's structure diversity, effectively addressing the structure collapse problem. As a result, our PTSA strategy can reap the benefits of data augmentations while mitigating their potential negative effects (see Figure 2, Table 3, and Figure 5 for further analysis.).

### Structure Damage Estimation

In practical FR scenarios, low-quality face samples, also known as "hard samples", are commonly included in the training set. These hard samples tend to be encoded in abnormal positions near the decision boundary in the latent space [25; 6; 66; 67], which will disrupt the latent space's topological structure and further hinder the alignment of structures. To address this issue, we propose a novel hard sample mining strategy called Structure Damage Estimation (**SDE**). SDE is specifically designed to identify hard samples with serious structure damage within the training set accurately. By prioritizing the learning of these hard samples and guiding them back to the reasonable positions during optimization, SDE aims to mitigate the adverse impact of hard samples on the latent space's topological structure.

**Prediction Uncertainty.** Hard samples are typically distributed near the decision boundary, thus have a high prediction uncertainty (_i.e._, large entropy of the classifier prediction) and are more likely to be misclassified by the classifier [68; 69; 70; 71]. Conversely, easy samples are usually located far from the decision boundary and have relatively low prediction uncertainty. To model the difficulty of each sample, we introduce a binary random variable \(u_{i}\{0,1\}\) for each sample \(_{i}\) to indicate whether the sample is hard or easy by values of 1 and 0, respectively. Then the probability that sample \(_{i}\) belongs to hard samples (_i.e._, with large prediction uncertainty) can be defined as \(h_{}(_{i})=P_{}(u_{i}=1|_{i})\), where \(\) represents the parameter set. According to the cluster assumption [72; 73], we believe that samples with higher prediction entropy are more disruptive to the latent space's topological structure. Therefore, we propose to model the distribution of the entropy \(E(_{i})\) for each training sample \(_{i}\) using the Gaussian-uniform mixture (**GUM**) model, a statistical distribution that is robust to outliers [74; 75; 76]:

\[p(E(_{i})|_{i})=^{+}(E( _{i})|0,)+(1-)(0,),\] (3)

where

\[^{+}(a|0,)=2(a|0,),&a 0.\\ 0,&a<0.\] (4)

\((0,)\) is a uniform distribution defined on \([0,]\), \(\) is a prior probability, and \(\) is the variance of Gaussian distribution \((a|0,)\). In this mixed model, the uniform distribution term \(\) and the Gaussian distribution term \(^{+}\) respectively model the hard samples and easy samples. Then the posterior probability that the sample \(_{i}\) to be hard (_i.e._, high-uncertainty) can be computed as follows:

\[h_{}(_{i})=P_{}(u_{i}=1|_{i} )=(0,)}{^{+}(E(_{i})|0,)+(1-)(0,)}.\] (5)

In Equation (5), when the classifier prediction is close to uniform distribution or when the prediction probabilities for multiple classes are nearly equal, the posterior probability of a sample belonging to hard data will be very high, _i.e._, (\(_{i}[,,,],h_{ }(_{i}) 1\)), otherwise it is relatively low. Hence, the prediction uncertainty of sample \(x_{i}\) can be measured by a quantitative probability \(h_{}(_{i})\).

Assume \((_{i})=(-1)^{_{i}}E(_{i})\), \(_{i} B(1,0.5)\), where \(B\) is a Bernoulli distribution , then the variable \((_{i})\) obeys the following statistical distribution:

\[p((_{i})|_{i})=((_{i})|0,)+(1-)(-, ).\] (6)

In this way, the maximum likelihood model of Equation (6) can be formulated as: \(_{,,}_{i=1}^{n}p(( _{i})|_{i})\). Then, the parameter set \(=\{,,\}\) of GUM can be estimated via the Expectation-Maximization (EM) algorithm  with the following iterative formulas:

\[h_{}^{(t+1)}(_{i})=)(- ^{(t)},^{(t)})}{^{(t)}((_{ i})|0,^{(t)})+(1-^{(t)})(-^{(t)},^{(t)})},^{(t+1)}= ^{n}(1-h_{}^{(t+1)}(_{i}))}{n},\]

\[^{(t+1)}=^{n}(1-h_{}^{(t+1)}(_{i})) ((_{i}))^{2}}{_{i=1}^{n}(1-h_{}^{(t+1)}( _{i}))},^{(t+1)}=-_{1}^{2})},\] (7)where

\[_{1}=^{n}^{(t+1)}(_{i})}{1-^ {(t+1)}}(_{i})}{_{i=1}^{n}(1-h_{}^{(t+1)}( _{i}))},_{2}=^{n}^{(t+1)}( _{i})}{1-^{(t+1)}}(_{i}))^ {2}}{_{i=1}^{n}(1-h_{}^{(t+1)}(_{i}))}.\]

Specifically, at each iteration, EM alternates between evaluating the expected log-likelihood (E-step) and updating the parameter set \(\) (M-step). In Equation (7), the **E-step** aims to evaluate the posterior probability \(h_{}^{(t+1)}\) of an sample \(_{i}\) to be hard sample using the iterative formula \(h_{}^{(t+1)}(_{i})\), where \((t+1)\) denotes the EM iteration index. The **M-step** updates the parameter set \(\) using the iterative formulas \(^{(t+1)}\), \(^{(t+1)}\) and \(^{(t+1)}\).

**Structure Damage Score (SDS).** Compared to correctly classified samples, misclassified samples usually have larger difficulty and have greater destructive effects on the latent space's topological structure. Therefore, misclassified samples need to receive more attention during training. Inspired by the Focal loss , we design a probability-aware scoring mechanism \((_{i})\) that combines prediction uncertainty \(h_{}()\) and prediction accuracy to adaptively compute SDS for each sample \(_{i}\):

\[(_{i})=_{1}(_{i})_{2}( _{i})=(1+h_{}(_{i}))^{}(1- _{i}^{gt}),\] (8)

where \(\) is a temperature coefficient, and \(_{i}^{gt}\) represents the prediction probability of ground truth. Specifically, SDE assigns higher SDS to hard samples and lower SDS to easy samples, which effectively balances the contribution of each sample to the objective. By assigning higher scores to hard samples, the model is encouraged to focus more on learning these challenging samples, boosting the FR system's generalization. Formally, the SDS weighted classfication loss \(_{cls}\) is defined as:

\[_{cls}=(_{i})_{arc}( _{i},y_{i})\] (9)

During training, to minimize the objective \(_{cls}\), the model needs to optimize both the SDS \(\) and the loss \(_{arc}\), which brings two benefits: **(1)** Minimizing \(_{arc}\) can encourage the model to capture face features with greater generalization ability from diverse training samples. **(2)** Minimizing SDS \(\) can alleviate the damage of hard samples to the latent space's topological structure, which is beneficial to the preservation of topological structure information and the construction of clear decision boundary.

### Model Optimization

To summarize, the overall objective of TopoFR can be formulated as follows:

\[_{,}_{cls}+_{sa}\] (10)

where \(\) is hyper-parameter that balances the contributions of the loss \(_{cls}\) and the loss \(_{sa}\). Detailed parameter sensitivity analysis can be found in Figure 6 in the Appendix.

## 5 Experiments

### Datasets.

**i) For training**, we employ three distinct datasets, namely MS1MV2  (5.8M facial images, 85K identities), Glint360K  (17.1M facial images, 360K identities), and WebFace42M  dataset (42.5M facial images, 2M identities). **ii) For evaluation**, we adopt LFW , AgeDB-30 , CFP-FP , CPLFW , CALFW , IJB-C , IJB-B  and the ICCV-2021 Masked Face Recognition Challenge (**MFR-Ongoing**)  as the benchmarks to test the performance of our models.

Notably, the MFR-Ongoing  is the most authoritative and comprehensive competition for evaluating FR models' generalization performance. It contains not only the existing popular test sets, such as IJB-C, but also its own MFR benchmarks, such as Mask, Children, and Multi-Racial test sets. Due to page size limitation, more training settings and experimental results are placed on **Appendix**.

### Results on Mainstream Benchmarks

**Results on MFR-Ongoing.** We employ WebFace42M as training set, and compare our TopoFR with SOTA competitors on MFR-Ongoing challenge, as reported in Table 1. For a fair comparison, all compared models adopt ResNet-200 as the backbone. Specially, our TopoFR surpasses the SOTA competitors UniFace and Partial FC in multiple metrics, implying the superiority of our method. Until the submission of this work (May 22 '24), the proposed TopoFR **ranks second place** on the academic track of the MFR-Ongoing leaderboard: http://iccv21-mfr.com/#/leaderboard/academic.

**Results on LFW, CFP-FP and AgeDB-30.** We adopt MS1MV2 and Glint360K to train our models, respectively. The results are reported in Table 2. To showcase the universality of our method, we also provide detailed experimental results of **TopoFR\({}^{}\)** model trained by CosFace . As stated in Refs., the performances of existing FR models on these three benchmarks have reached saturation. **1)** On MS1MV2 training set, we note that our TopoFR and TopoFR\({}^{}\) models still obtain accuracy improvement and outperform SOTA competitors (_e.g._, AdaFace  and TransFace ). **2)** On Glint360K training set, our TopoFR become SOTA models and surpass AdaFace and TransFace.

**Results on IJB-C and IJB-B.** We train our TopoFR on MS1MV2 and Glint360K respectively, and compare with SOTA methods on IJB-C and IJB-B benchmarks, as reported in Table 2. **1)** On MS1MV2 training set, our models obtain the best results under different backbones. For instance, our R50 TopoFR and R50 TopoFR\({}^{}\) models greatly surpass SOTA method AdaFace and even beat most R100-based competitors. **2)** On Glint360K training set, all our models significantly outperform the cutting-edge competitor AdaFace and achieve SOTA performance. More importantly, our TopoFR even works better than ViT-based SOTA method TransFace, implying the superiority of our method.

   &  &  &  &  \\   & & & Mask & Children &  &  &  &  & 1e-5 & 1e-4 \\   R200, Partial FC  &  & CVPR22 & 91.87 & - & 97.79 & 98.70 & 98.54 & 89.52 & 97.70 & 96.93 & 97.97 \\ R200, UniFace  & & & 92.43 & 93.11 & **98.14** & **98.98** & 98.84 & 90.01 & 97.92 & 96.68 & 97.91 \\
**R200, TopoFR** & & & NeurIPS24 & **93.96** & **93.57** & 97.97 & 98.71 & **98.98** & **92.85** & **98.13** & **97.10 & **98.01** \\  

Table 1: Verification accuracy (%) on the MFR-Ongoing benchmark.

   &  &  &  &  &  &  &  \\    & & & & & & & & & & & & & & \\    & R50, ArcFace  & CVPR19 & 99.68 & 97.11 & 97.53 & 88.36 & 92.52 & 91.66 \\  & R50, MagFace  & CVPR21 & 99.74 & 97.47 & 97.70 & 88.95 & 93.34 & 91.47 \\  & R50, AdaFace  & CVPR22 & 99.82 & 97.86 & 97.85 & - & 96.27 & 94.42 \\  & R50, **TopoFR\({}^{}\)** & NeurIPS24 & **99.83** & **98.24** & 98.23 & **94.79** & 96.42 & 95.13 \\  & R50, **TopoFR** & NeurIPS24 & **99.83** & **98.24** & **98.25** & 94.71 & **96.49** & **95.14** \\   & R100, CosFace  & CVPR18 & 99.78 & 98.26 & 98.17 & 92.68 & 95.36 & 94.01 \\  & R100, ArcFace  & CVPR19 & 99.77 & 98.27 & 98.15 & 92.69 & 95.74 & 94.09 \\  & R100, MV-Softran  & AAAI20 & 99.80 & 98.28 & 97.95 & - & 95.20 & 93.60 \\  & R100, URL  & CVPR20 & 99.78 & 98.64 & - & 95.00 & 96.60 & - \\  & R100, Broadex  & ECCV20 & **99.85** & 98.63 & 98.38 & 94.59 & 96.38 & 94.97 \\  & R100, CurricularFace  & CVPR20 & 99.80 & 98.37 & 98.32 & - & 96.10 & 94.80 \\  & R100, MagFace+  & CVPR21 & 99.83 & 98.46 & 98.17 & 94.08 & 95.97 & 94.51 \\  & R100, SCF-ArcFace  & CVPR21 & 99.82 & 98.94 & 98.30 & 94.04 & 96.09 & 94.74 \\ R100, DAM-CurricularFace  & ICCV21 & - & - & - & - & - & 96.20 & 95.12 \\ R100, ElasticFace-Cos+  & CVPR22 & 99.80 & 98.73 & 98.28 & - & 96.65 & 95.43 \\ R100, AdaFace  & CVPR22 & 99.82 & 98.49 & 98.05 & - & 96.89 & 95.67 \\  & TransFace-B  & ICCV23 & 99.82 & 98.39 & 98.27 & 94.15 & 96.55 & \\  & R100, **TopoFR\({}^{}\)** & NeurIPS24 & **99.85** & **98.83** & **98.94** & **95.28** & **96.96** & **95.70** \\  & R100, **TopoFR** & NeurIPS24 & **99.85** & 98.71 & **98.42** & 95.23 & 96.95 & **95.70** \\   & R200, ArcFace  & CVPR19 & 99.79 & 98.44 & 98.19 & 94.67 & 96.53 & 95.18 \\  & R200, AdaFace  & CVPR22 & 99.83 & 98.76 & 98.28 & 94.88 & 96.93 & 95.71 \\   & TransFace-L  & ICCV23 & 99.83 & 98.65 & 98.23 & 94.55 & 96.59 & - \\   & R200, **TopoFR\({}^{}\)** & NeurIPS24 & **99.85** & **99.09** & **98.54** & **95.19** & **97.12** & 95.77 \\   & R200, **TopoFR** & NeurIPS24 & **99.85** & 99.05 & 98.52 & 95.15 & 97.08 & **95.82** \\    & R50, ArcFace  & CVPR19 & 99.78 & 98.77 & 98.28 & 95.29 & 96.81 & 95.30 \\   & R50, AdaFace  & CVPR22 & 99.82 & 99.07 & 98.34 & 95.58 & 96.90 & 95.66 \\   & R50, **TopoFR** & NeurIPS24 & **99.85** & **99.28** & **98.47** & **95.99** & **97.27** & **95.96** \\    & R100, ArcFace  & CVPR19 & 99.81 & 99.04 & 98.31 & 95.38 & 96.89 & 95.69 \\   & R100, AdaFace  & CVPR22 & 99.82 & 99.20 & 98.58 & 96.24 & 97.19 & 95.87 \\   & TransFace-B  & ICCV23 & 99.85 & 99.

### Analysis and Ablation Study

Due to the limitation of page size, more ablation experiments and analysis are placed on **Appendix**.

**1) Contribution of Each Component:** To investigate the contribution of each component in our model, we employ MS1MV2 as the training set, and compare ArcFace (baseline), and four variants of TopoFR on the IJB-C benchmark. The variants of TopoFR are as follows: (1) **TopoFR-R**, the variant only adds RSP mechanism to ArcFace. (2) **TopoFR-A**, based on ArcFace, the variant simply aligns the structure of input space and latent space without using RSP. (3) **TopoFR-P**, the variant fully introduces the PTSA strategy into ArcFace. (4) **TopoFR-G**, based on TopoFR-P, the variant only uses prediction uncertainty \(_{1}\) modeled by GUM to re-weight each sample. (5) **TopoFR-F**, based on TopoFR-P, the variant simply applies Focal loss \(_{2}\) to re-weight each sample.

The results gathered in Table 3 reflect some observations: (1) Compared with ArcFace, the accuracy of TopoFR-R is clearly reduced due to the addition of more unidentifiable face images, which hurts the FR model's generalization ability. (2) TopoFR-A outperforms ArcFace, indicating that directly aligning the two spaces can slightly boost model's performance, but it inevitably encounters structure collapse issue. (3) TopoFR-P greatly surpasses TopoFR-R and TopoFR-A, implying that preserving the structure information can greatly improve FR model's generalization. (4) TopoFR outperforms TopoFR-F and TopoFR-G, which not only demonstrates the effectiveness of SDE strategy, but also indicates that the prediction uncertainty \(_{1}\) is complementary to Focal loss \(_{2}\) in mining hard samples.

**2) Effectiveness of GUM:** To visually demonstrate the effectiveness of GUM in mining hard samples, we present the estimated Gaussian density of the prediction entropy during training in Figure 4. These curves show that the entropy of misclassified face samples (represented by black crosses) usually have rather low Gaussian density (_i.e._, high posterior probability \(h_{}\)), thus can be easily detected.

Figure 4: The estimated Gaussian density (blue curve) _w.r.t_ the entropy of classification prediction. Green marker \(\) and black marker \(\) represent the entropy of correctly classified sample and misclassified sample, respectively.

   Training Data & Method & IJB-C(1e-4) \\    & R50, ArcFace & 92.52 \\   & R50, TopoFR-R & 92.44 \\ MS1MV2 & R50, TopoFR-A & 93.26 \\ MS1MV2 & R50, TopoFR-P & 95.34 \\  & R50, TopoFR-F & 95.40 \\  & R50, TopoFR-G & 96.23 \\  & R50, TopoFR & **96.49** \\   

Table 3: Ablation study.

Figure 5: The topological structure discrepancy of TopoFR and variant TopoFR-A under different backbones and training datasets (_i.e._, **[Backbone, Training dataset]**). Variant TopoFR-A directly utilizes PH to align the topological structures of two spaces. Notably, our TopoFR models trained with Glint360K dataset almost perfectly align the topological structures of the input space and the latent space on the IJB-C benchmark (_i.e._, the blue histogram almost converges to a straight line).

Note that even if some misclassified samples have small entropy (_i.e._, high Gaussian density and low posterior probability \(h_{}\)), their SDS \(\) can still be corrected by the Focal loss \(_{2}\).

**3) Generalization of PTSA:** To show the superior generalization ability of our PTSA strategy in preserving structure information, we investigate the topological structure discrepancy between the input and the latent spaces of TopoFR and its variant TopoFR-A on IJB-C benchmark. Note that TopoFR-A directly utilizes PH to align the topological structures of two spaces. The results in Figure 5 indicate that: 1) Directly using PH to align the topological structures of two spaces does not effectively reduce the structure discrepancy, as the model encounters the structure collapse issue; 2) PTSA strategy can effectively align the topological structures of two spaces and address this structure collapse issue. Remarkably, Figures 4(c) and 4(d) show that our TopoFR models trained on Glint360K almost perfectly preserve structure information of input spaces in their latent features, thereby verifying the generalization of PTSA strategy.

## 6 Conclusion

This paper proposes a novel FR framework called TopoFR that aims to encode the critical structure information in large-scale face dataset into the latent space. Specially, TopoFR leverages a structure alignment strategy PTSA and a hard sample mining strategy SDE. PTSA employs PH to reduce the topological structure discrepancy between the input and latent spaces, effectively mitigating structure collapse phenomenon and preserving structure information. SDE accurately identifies hard samples and guides the model to prioritize optimizing these samples, mitigating their adverse impact on the latent space's structure. Comprehensive experiments validate the superiority of our TopoFR.

## 7 Broader Impacts

It would be good to mention that the utilization of face images do not have any privacy concern given the datasets have proper license and users consent to distribute biometric data for research purpose. We address the well-defined face recognition task and conduct experiments on publicly available face datasets. Therefore, the propose method does not involve sensitive attributes and we do not notice any negative societal issues.