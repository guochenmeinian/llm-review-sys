# FABind: Fast and Accurate Protein-Ligand Binding

Qizhi Pei\({}^{1,5}\), Kaiyuan Gao\({}^{2}\), Lijun Wu\({}^{3}\), Jinhua Zhu\({}^{4}\), Yingce Xia\({}^{3}\),

**Shufang Xie\({}^{1}\)**, **Tao Qin\({}^{3}\)**, **Kun He\({}^{2}\)**, **Tie-Yan Liu\({}^{3}\)**, **Rui Yan\({}^{1,6}\)**

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) School of Computer Science and Technology, Huazhong University of Science and Technology

\({}^{3}\) Microsoft Research AI4Science

\({}^{4}\) School of Information Science and Technology, University of Science and Technology of China

\({}^{5}\) Engineering Research Center of Next-Generation Intelligent Search

and Recommendation, Ministry of Education

\({}^{6}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

{qizhipei,shufangxie,ruiyan}@ruc.edu.cn, {im_kai,brooklet60}@hust.edu.cn

{lijuwu,yinxia,taoqin,tyliu}@microsoft.com,teslazhu@mail.ustc.edu.cn

Equal contribution. This work was done during their internship at Microsoft Research AI4Science.Corresponding authors: Rui Yan (ruiyan@ruc.edu.cn) and Lijun Wu (lijuwu@microsoft.com).

###### Abstract

Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose FABind, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. FABind incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference. Through extensive experiments on benchmark datasets, our proposed FABind demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at Github3.

## 1 Introduction

Biomolecular interactions are vital in the human body as they perform various functions within organisms . For example, protein-ligand binding , protein-protein interaction , protein-DNA interaction , and so on. Among these, drug-like small molecules (ligands) binding to protein is important and widely studied in interactions as they can facilitate drug discovery. Therefore, molecular docking, which involves predicting the conformation of a ligand when it binds to a target protein, serves as a crucial task, the resulting docked protein-ligand complex can provide valuable insights for drug development.

Though important, fast and accurately predicting the docked ligand pose is super challenging. Two families of methods are commonly used for docking: sampling-based and regression-based prediction. Most of the traditional methods lie in the sampling-based approaches as they rely on physics-informed empirical energy functions to score and rank the enormous sampled conformations , even with the use of deep learning-based scoring functions for conformation evaluation , these methods still need a large number of potential ligand poses for selection and optimization. DiffDock  utilizes a deep diffusion model that significantly improves accuracy. However, it still requires a large number of sampled/generated ligand poses for selection, resulting in high computational costs and slow docking speeds. The regression-based methods  that use deep learning models to predict the docked ligand pose bypass the dependency on the sampling process. For instance, TankBind  proposes a two-stage framework that simulates the docking process by predicting the protein-ligand distance matrix and then optimizing the pose. In contrast, EquiBind  and E3Bind  directly predict the docked pose coordinates. Though efficient, the accuracy of these methods falls behind the sampling-based methods. Additionally, the variation in protein sizes often requires the use of external modules to first select suitable binding pockets, which can impact efficiency. Many of these methods rely on external modules to detect favorable binding sites in proteins. For example, TankBind  and E3Bind  take P2Rank  as priors for generating the pocket center candidates, which results in the need for a separate module (e.g., affinity prediction in TankBind and confidence module in E3Bind) for pocket selection, increasing the training and inference complexity.

To address these limitations, we propose FABind, a Fast and Accurate docking framework in an end-to-end way. FABind unifies pocket prediction and docking, streamlining the process within a single model architecture, which consists of a series of equivariant layers with geometry-aware updates, allowing for either pocket prediction or docking with only different configurations. Notably, for pocket prediction, we utilize lightweight configurations to maintain efficiency without sacrificing accuracy. In contrast to conventional pocket prediction, which only uses protein as input and forecasts multiple potential pockets, our method incorporates a specific ligand to pinpoint the unique pocket that the ligand binds to. Integrating the ligand into pocket prediction is crucial as it aligns with the fundamental characterization of the docking problem. In this way, we achieve a quick (without the need for external modules such as P2Rank) and precise pocket prediction (see Section 5.1).

Several strategies are additionally proposed in FABind to make a fast and accurate docking prediction. (1) Our pocket prediction module operates as the first layer in our model hierarchy and is jointly trained with the subsequent docking module to ensure a seamless, end-to-end process for protein-ligand docking prediction. We also incorporate a pocket center constraint using Gumbel-Softmax. This way assigns a probabilistic weighting to the inclusion of amino acids in the pocket, which helps to identify the most probable pocket center and improve the precision of the docking prediction. (2) We incorporate the predicted pocket into the docking module training using a scheduled sampling approach . This way ensures consistency between the training and inference stages with respect to pocket leverage, thereby avoiding any mismatch that may arise from using the native pocket during training and the predicted pocket during inference. In this way, FABind is trained on a variety of possible pockets, allowing it to generalize well to new docking scenarios. (3) While directly predicting the ligand pose  and optimizing the coordinates based on the protein-ligand distance map  are both widely adopted in the sampling-based methods to ensure efficiency, we integrate both predictions in FABind to produce a more accurate pose prediction.

To evaluate the performance of our method, we conducted experiments on the binding structure prediction benchmark and compared our work with multiple existing methods. Our results demonstrate that FABind outperforms existing methods, achieving a mean ligand RMSD of \(6.4\). This is a significant improvement over previous methods and demonstrates the effectiveness of our approach. Remarkably, our approach also demonstrates superior generalization ability, performing surprisingly well on unseen proteins. This suggests that our model can be applied to a wide range of docking scenarios and has the potential to be a useful tool in drug discovery. In addition to achieving superior performance, our method is also much more efficient during inference (e.g., \(170\) faster than DiffDock), making it a fast and accurate binding framework. This efficiency is critical in real-world drug discovery scenarios, where time and resources are often limited.

## 2 Related Work

**Protein/Molecule Modeling.** Learning effective protein and molecule representations is fundamental to biology and chemistry research. Deep learning models have made great progress in protein and molecule presentation learning. Molecules are usually represented by graph or string formats, and the graph neural networks (e.g., GAT , GCN ) and sequence models (e.g., LSTM , Transformer ) are utilized to model the molecules. For proteins, the sequence models are the common choices since the FASTA sequence is the most widely adopted string format . Considering the 3D representations, incorporating the geometric information/constraint, e.g., SE(3)-invariance or equivariance, is becoming promising to model the protein/molecule structures, and there are multiple equivariant/invariant graph neural networks proposed [39; 20; 3; 16]. Among them, AlphaFold2  is almost the most successful geometry-aware modeling that achieves revolutionary performance in protein structure prediction. In our work, we also keep the equivariance when modeling the protein and ligand.

**Pocket Prediction.** Predicting binding pocket is critical in the early stages of structure-based drug discovery. Different approaches have been developed in the past years, including physical-chemical-based, geometric-based, and machine learning-based methods . Typical geometric-based methods are SURFNET  and Fpocket , and many alternatives have been proposed [47; 6; 42]. Recently, machine learning-based, especially deep learning-based methods, have been promising, such as DeepSite  and DeepPocket  which directly predict the pocket using deep networks, and some works utilize the deep scoring functions (e.g., affinity score) to score and rank the pockets [36; 50]. Among them, P2Rank  is an open-sourced tool that is widely adopted in existing works [51; 31] to detect potential protein pockets. In this work, we distinguish ourselves from previous methods through the introduction of a novel equivariant module and the combination of two separate losses. Besides, we jointly train the pocket prediction module and the docking module, leveraging the knowledge gained from the docking module to improve the performance of pocket prediction.

Figure 1: An overview of FABind. _Left_: The pocket prediction module takes the whole protein and the ligand as input and predicts the coordinates of the pocket center, where the ligand is randomly placed at the center of the protein. After determining the pocket center, a pocket is defined as a set of amino acids within a fixed radius around the center. Subsequently, the docking module moves the ligand to the pocket center and the ligand-pocket pair iteratively goes through the FABind layers to obtain the final pose prediction. \(M\) and \(N\) are the number of layers in pocket prediction and docking. _Right_: Architecture of FABind layers. Each layer contains three modules: independent message passing takes place within each component to update node embeddings and coordinates; cross-attention captures correlations between residues and ligands and updates embeddings only; and interfacial message passing focuses on the interface, attentively updating coordinates and representations.

**Protein-Ligand Docking.** Protein-ligand docking prediction is to predict the binding structure of the protein-ligand complex. Traditional methods usually take the physics-informed energy functions to score, rank, and refine the ligand structures, such as AutoDock Vina , SMINA , GLIDE . Recently, geometric deep learning has been attractive and greatly advances docking prediction. There are two distinct approaches in docking research. Regression-based methods, such as EquiBind , TankBind , and E3Bind , directly predict the docked ligand pose. On the other hand, sampling-based methods, like DiffDock , require extensive ligand pose sampling and optimization, but often yield more accurate predictions. Our work lies in the research of regression-based methods with much less computational costs, but achieves comparable performance to sampling-based approaches.

## 3 Method

An overview of our proposed FABind is presented in Fig. 1. We first clarify the notations and problem definition in Section 3.1. In Section 3.2, we illustrate our FABind layer in detail. The specific design of the pocket prediction module is explained in Section 3.3, while the docking module is introduced in Section 3.4. Furthermore, the training pipeline is comprehensively described in Section 3.5.

### Preliminaries

**Notations.** For each protein-ligand complex, we represent it as a graph and denote it as \(=(:=\{^{l},^{p}\},:=\{ ^{l},^{p},^{lp}\})\). Specifically, the ligand subgraph is \(^{l}=(^{l},^{l})\), where node \(v_{i}=(_{i},_{i})^{l}\) is an atom, \(_{i}\) is the pre-extracted feature by TorchDrug , and \(_{i}^{3}\) is the corresponding coordinate. The number of atoms is denoted as \(n^{l}\). \(^{l}\) is the edge set that represents chemical bonds in the ligand. The protein subgraph is \(^{p}=(^{p},^{p})\), where node \(v_{j}=(_{j},_{j})^{p}\) is a residue, \(_{j}\) is initialized with the pre-trained ESM-2  feature following DiffDock , and \(_{j}^{3}\) is the coordinate of the \(C_{}\) atom in the residue. The number of residues is denoted as \(n^{p}\). \(^{p}\) is the edge set constructed by a cut-off distance 8A. The input of the pocket prediction module is \(\). We further denote the pocket subgraph as \(^{p*}=(^{p*},^{p*})\), with \(n^{p*}\) residues in the pocket. The pocket and ligand form a new complex as the input to the docking module: \(^{lp*}=(:=\{^{l},^{p*}\},:=\{^{l},^{p*},^{lp*}\})\), where \(^{lp*}\) defines edges in the external contact surface. Detailed edge construction rule is in Appendix Section A.4. For clarity, we always use indices \(i\), \(k\) for ligand nodes, and \(j\), \(k^{}\) for protein nodes.

**Problem Definition.** Given a bounded protein and an unbounded ligand as inputs, our goal is to predict the binding pose of the ligand, denoted as \(\{_{i}^{l*}\}_{1 i n_{l}}\). Following previous works, we focus on _blind docking_ scenario in which we have zero knowledge about protein pocket.

### FABind Layer

In this section, we provide a comprehensive description of the FABind layer. For clarity, we use FABind in the pocket prediction module for a demonstration.

**Overview.** Besides node-level information, we explicitly model pair embedding for each protein residue-ligand atom pair \((i,j)\). We follow E3bind  to construct a pair embedding \(_{ij}\) via an outer product module (OPM): \(_{ij}=(((_ {i})(_{j})))\). For the initial pair embedding, OPM is operated on the transformed initial protein/ligand node embedding \(_{i}/_{j}\). As illustrated in Figure 1, each FABind layer conducts three-step message passing: (1) Independent message passing. The independent encoder first passes messages inside the protein and ligand to update node embeddings and coordinates. (2) Cross-attention update. This block operates to exchange information across every node and updates pair embeddings accordingly. (3) Interfacial message passing. This layer focuses on the contact surface and attentively updates coordinates and representations for such nodes. The fundamental concept behind this design is the recognition of distinct characteristics between internal interactions within the ligand or protein and external interactions between the ligand and protein in biological functions. After several layers of alternations, we perform another independent message passing for further adjustment before the output of pocket prediction/docking modules. Notably, the independent and interfacial message passing layers are E(3)-equivariant, while cross-attention update layer is E(3)-invariant since it does not encode structure. These ensure each layer is E(3)-equivariant.

**Independent Message Passing.** We introduce a variant of Equivariant Graph Convolutional Layer (EGCL) proposed by EGNN  as our independent message passing layer. For simplicity, here we only illustrate the detailed message passing of ligand nodes, while the protein updates are in a similar way. With the ligand atom embedding \(_{i}^{l}\) and the corresponding coordinate \(_{i}^{l}\) in the \(l\)-th layer, we perform the independent message passing as follows:

\[_{ik}=_{e}(_{i}^{l},_{k}^{l},\| _{i}^{l}-_{k}^{l}\|^{2}),\]

\[_{i}^{l}=_{i}^{l}+_{h}(_{i}^{l}, _{k(i|^{l})}_{ik})\!, _{i}^{l}=_{i}^{l}+(i|^{l})|}_{k(i|^{l})}( _{i}^{l}-_{k}^{l})_{x}(_{ik} ),\]

where \(_{e},_{x},_{h}\) are Multi-Layer Perceptrons (MLPs)  and \((i|^{l})\) denotes the neighbors of node \(i\) regarding the internal edges \(^{l}\) of the ligand.

**Cross-Attention Update.** After independent message passing, we enhance the node feature with cross-attention update over all protein/ligand nodes by passing the messages from all ligand/protein nodes. The pair embeddings are also updated accordingly. We also take the ligand embedding update for clarity. Given the ligand atom representations and the pair embeddings, we first perform multi-head cross-attention over all protein residues:

\[a_{ij}^{(h)}=_{j}(}{_{i}^{(h) ^{}}}_{j}^{(h)}+b_{ij}^{(h)}),_{i}^{l}= _{i}^{l}+(_{1 h H}( _{j=1}^{n^{p*}}a_{ij}^{(h)}_{j}^{(h)})),\]

where \(_{i}^{(h)},_{j}^{(h)},_{j}^{(h)}\) are linear projections of the node embedding, and \(b_{ij}^{(h)}=(_{ij}^{l})\) is a linear transformation of pair embedding \(_{ij}^{l}\). The protein embeddings \(_{j}^{l}\) are updated similarly. Based on updated node embeddings \(_{i}^{l}\) and \(_{j}^{l}\), the pair embeddings are further updated by \(_{ij}^{l}=_{ij}^{l}+(_{i}^{l}, _{j}^{l})\).

**Interfacial Message Passing.** With the updated protein and ligand representations, we perform an interfacial message passing to update the included node features and the coordinates on the contact surface. Our interfacial message passing derives from MEAN  with an additional attention bias. The detailed updates are as follows:

\[_{ij}=_{i}^{}_{ij}+b_{ij} )}{_{j(i|^{l e})}( _{i}^{}_{ij}+b_{ij})},\]

\[_{i}^{l+1}=_{i}^{l}+_{j(i| ^{l e})}_{ij}_{ij},_{i}^{l+1}= _{i}^{l}+_{j(i|^{l e})} _{ij}(_{i}^{l}-_{j}^{l})_{xv}( _{ij}),\]

where \(_{i}=_{q}(_{i}^{l})\), \(_{ij}=_{k}(\|_{i}^{l}-_{j}^{l} \|^{2},_{j}^{l})\), \(_{ij}=_{v}(\|_{i}^{l}-_{j}^{l} \|^{2},_{j}^{l})\), and \(b_{ij}=_{b}(_{ij}^{l})\), and \(_{q},_{k},_{v},_{b},_{xv}\) are MLPs. \(^{l e}\) denotes the external edges between ligand and protein contact surface constructed by cut-off distance 10A.

### Pocket Prediction

In the pocket prediction module, given the protein-ligand complex graph \(\), our objective is to determine the amino acids of the protein that belong to the pocket. Previous works such as TankBind and E3Bind both use P2Rank  to produce multiple pocket candidates. Subsequently, either affinity score (TankBind) or self-confidence score (E3Bind) is utilized to select the most appropriate docked pose. Though P2Rank is faster and better than previous tools, it is based on numerical algorithms and traditional machine learning classifiers. Furthermore, the incorporation of P2Rank necessitates the selection of candidate poses following multiple poses docking. These factors could potentially restrict the performance and efficiency of fully deep learning-based docking approaches.

In our work, we propose an alternative method by treating pocket prediction as a binary classification task on the residues using the FABind layer, where each residue in the protein is classified as belonging to the pocket or not. Hence, the pocket prediction is more unified with the deep learning docking. Specifically, we use a binary cross-entropy loss to train our pocket classifier:

\[_{p}^{c}=-}_{j=1}^{n_{p}}[y_{j}(p_{j})+(1-y_{j} )(1-p_{j})],\]where \(n_{p}\) is the number of residues in the protein, and \(y_{j}\) is the binary indicator for residue \(j\) (i.e., \(1\) if it belongs to a pocket, \(0\) otherwise). \(p_{j}=((\{()\}_{j})\)) is the predicted probability of residue \(j\) belonging to a pocket, \(\) is the protein-ligand complex graph, and \(\) is sigmoid function.

Besides the direct classification of each residue to decide the pocket, the common practice of leveraging P2Rank pocket prediction is to first predict a pocket center coordinate, then a sphere near the pocket center under a radius 20A. Therefore, we add a constraint about the pocket center to make a more accurate prediction.

Constraint for Pocket Center.To constrain the predicted pocket center, we introduce a pocket center regression task over the classified pocket residues. Given \(n^{p^{}}\) predicted pocket residues \(^{p^{}}=\{v_{1}^{p^{}},v_{2}^{p^{}},...,v_{n^{p^{ }}}^{p^{}}\}\) from our classifier, the pocket center coordinate is \(^{p^{}}=}}_{j=1}^{n^{p^{}}} _{j}^{p^{}}\). Then we can add a distance loss between the predicted pocket center \(^{p^{}}\) and the native pocket center \(^{p*}\). This pocket center computation inherently involves discrete decisions - selecting which amino acids contribute to the pocket. Hence, we apply Gumbel-Softmax  to produce a differentiable approximation of the discrete selection process. It provides a probabilistic "hard" selection, which more accurately reflects the discrete decision to include or exclude an amino acid in the pocket.

\[_{j}^{p}=)+g_{j})/_{e})}{_{k^{}=1}^{ n^{p}}(((p_{k^{}})+g_{k^{}})/_{e})},\]

where \(g_{j}\) is sampled from Gumbel distribution \(g_{j}=-(- U_{m})\), \(U_{m}\) Uniform(0, 1), and \(_{e}\) is the controllable temperature. Then we use Huber loss  between the predicted pocket center \(^{p}=}_{j=1}^{n^{p}}_{j}^{p}_{j}^ {p}\) and the native pocket center \(^{p*}\) as the constraint loss,

\[_{p}^{c2r}=l_{Huber}(^{p},^{p*}).\]

Training Loss.The pocket prediction loss is comprised of classification loss \(_{p}^{c}\) and pocket center constraint loss \(_{p}^{c2r}\), with a weight factor \(=0.2\),

\[_{pocket}=_{p}^{c}+_{p}^{c2r}.\]

Pocket Decision.Our pocket classifier can output the classified residues in a pocket. Following previous works [31; 51], we do not directly take these predicted residues as the pocket. Instead, we calculate the center of these classified residues and take it as the predicted pocket center, and the predicted pocket is in a sphere near the predicted center under a radius 20A. However, for some proteins, our classification model may predict each residue as negative for a pocket, making it impossible to determine the pocket center. This is likely due to an imbalance between the pocket and non-pocket residues. For these rare cases, we take the Gumbel-softmax predicted center \(x^{p}\) as the pocket center to address this problem.

### Docking

In the docking task, given a pocket substructure \(^{p*}\), our docking module predicts the coordinate of each atom in \(^{l}\). The docking task is challenging since it requires the model to preserve E(3)-equivariance for every node while capturing pocket structure and chemical bonds of ligands.

Iterative refinement.Iterative refinement  is adopted in docking FABind layers to refine the structures by feeding the predicted ligand pose back to the message passing layers several rounds. During refinement iterations, new graphs are generated and the edges are also constructed dynamically.

After \(k\) iterations (iterative refinement) of the \(N\) FABind layer alternations, we obtain the final coordinates \(^{L}\) and node embeddings \(_{i}^{L}\) and \(_{j}^{L}\). Besides directly optimizing the coordinate loss, we additionally add distance map constraints to refine the ligand pose better. We reconstruct distance matrices in two ways. One is to directly compute based on the predicted coordinates: \(_{ij}=\|_{i}^{L}-_{j}^{L}\|\). The other is to predict from the pair embeddings \(_{ij}^{L}\) by an MLP transition: \(_{ij}=(_{ij}^{L})\), where each vector outputs a distance scalar.

Training Loss.The docking loss is comprised of coordinate loss \(_{coord}\) and distance map loss \(_{dist}\):

\[_{docking}=_{coord}+_{dist}.\]\(_{coord}\) is computed as the Huber distance between the predicted coordinates and ground truth coordinates of the ligand atoms. \(_{dist}\) is comprised of three terms, each of which is \(_{2}\) loss between different components of the ground truth and the two reconstructed distance maps. Formally,

\[_{dist}= n^{p*}}[_{i=1}^{n^{l}}_{j =1}^{n^{p*}}(D_{ij}-_{ij})^{2}+_{i=1}^{n^{l}} _{j=1}^{n^{p*}}(D_{ij}-_{ij})^{2}+_{i= 1}^{n^{l}}_{j=1}^{n^{p*}}(_{ij}-_{ij})^{ 2}],\]

where \(D_{ij}\) is ground truth distance matrix. In practice, we set \(==1.0\).

### Pipeline

We first predict the protein pocket and then predict the docked ligand coordinates in a hierarchical unified framework. However, the common approach is to only use the native pocket for docking in the training phase, which is known as teacher-forcing  training. Therefore, there is a mismatch in that the training phase takes the native pocket while the inference phase can only take the predicted pocket since we do not know the native pocket in inference. To reduce this gap, we incorporate a scheduled training strategy to gradually involve the predicted pocket in the training stage instead of using the native pocket only. Specifically, our training pipeline consists of two stages, (1) in the initial stage, since the performance of pocket prediction is poor, we only use the native pocket to perform the docking training; (2) In the second stage, with the improved pocket prediction ability, we then involve the predicted pocket into docking, where the native pocket is still kept in docking. The ratio between the predicted pocket and the native pocket is \(1:3\).

**Comprehensive Training Loss.** Our comprehensive training loss comprises two components: the pocket prediction loss and the docking loss,

\[=_{pocket}+_{docking}.\] (1)

## 4 Experiments

### Setting

**Dataset.** We conduct experiments on the PDBbind v2020 dataset , which is from Protein Data Bank (PDB)  and contains 19,443 protein-ligand complex structures. To maintain consistency with prior works [41; 31], we follow similar preprocessing steps (see Appendix Section A.1 for more details). After the filtration, we used \(17,299\) complexes that were recorded before 2019 for training purposes, and an additional \(968\) complexes from the same period for validation. For our testing phase, we utilized \(363\) complexes recorded after 2019.

    &  &  \\   &  &  &  &  &  \\ 
**Methods** & 25\% & 50\% & 75\% & Mean & 2\(\) & 5\(\) & 25\% & 50\% & 75\% & Mean & 2\(\) & 5\(\) & **Runtime (s)** \\  QVina-W & 2.5 & 7.7 & 23.7 & 13.6 & 20.9 & 40.2 & 0.9 & 3.7 & 22.9 & 11.9 & 41.0 & 54.6 & 49* \\ GNINA & 2.8 & 8.7 & 22.1 & 13.3 & 21.2 & 37.1 & 1.0 & 4.5 & 21.2 & 11.5 & 36.0 & 52.0 & 146 \\ SMIINA & 3.8 & 8.1 & 17.9 & 12.1 & 13.5 & 33.9 & 1.3 & 3.7 & 16.2 & 9.8 & 38.0 & 55.9 & 146* \\ GLIDE & 2.6 & 9.3 & 28.1 & 16.2 & 21.8 & 33.6 & 0.8 & 5.6 & 26.9 & 14.4 & 36.1 & 48.7 & 1405* \\ Vina & 5.7 & 10.7 & 21.4 & 14.7 & 5.5 & 21.2 & 1.9 & 6.2 & 20.1 & 12.1 & 26.5 & 47.1 & 205* \\  EquiBind & 3.8 & 6.2 & 10.3 & 8.2 & 5.5 & 39.1 & 1.3 & 2.6 & 7.4 & 5.6 & 40.0 & 67.5 & **0.03** \\ TankBind & 2.6 & 4.2 & 7.6 & 7.8 & 17.6 & 57.8 & 0.8 & 1.7 & 4.3 & 5.9 & 55.0 & 77.8 & 0.87 \\ E3Bind & 2.1 & 3.8 & 7.8 & 7.2 & 23.4 & 60.0 & 0.8 & 1.5 & 4.0 & 5.1 & 60.0 & 78.8 & 0.44 \\ DiffDock (1) & 2.4 & 4.9 & 8.9 & 8.3 & 20.4 & 51.0 & 0.7 & 1.8 & 4.5 & 5.8 & 54.1 & 76.8 & 2.72 \\ DiffDock (10) & 1.6 & 3.8 & 7.9 & 7.4 & 32.4 & 59.7 & 0.6 & 1.4 & 3.6 & 5.2 & 60.7 & 79.8 & 20.81 \\ DiffDock (40) & **1.5** & 3.5 & 7.4 & 7.4 & **36.0** & 61.7 & **0.5** & **1.2** & **3.3** & 5.4 & **62.9** & **80.2** & 82.83 \\  FABind & 1.7 & **3.1** & **6.7** & **6.4** & 33.1 & **64.2** & 0.7 & 1.3 & 3.6 & **4.7** & 60.3 & **80.2** & 0.12 \\   

Table 1: Flexible blind self-docking performance. The top half contains results from traditional docking software; the bottom half contains results from recent deep learning based docking methods. The last line shows the results of our FABind. The number of poses that DiffDock samples is specified in parentheses. We run the experiments of DiffDock three times with different random seeds and report the mean result for robust comparison. The symbol ”#” means that the method operates exclusively on the CPU. The superior results are emphasized by bold formatting, while those of the second-best are denoted by an underline.

**Model Configuration.** For pocket prediction, we use \(M=1\) FABind layer with \(128\) hidden dimensions. The pocket radius is set to 20A. As for docking, we use \(N=4\) FABind layers with \(512\) hidden dimensions. Each layer comprises three-step message passing modules (refer to Fig. 1). We perform a total of \(k=8\) iterations for structure refinement during the docking process. Additional training details can be found in the Appendix Section A.2.

**Evaluation.** Following EquiBind , the evaluation metrics are (1) Ligand RMSD, which calculates the root-mean-square deviation between predicted and true ligand atomic Cartesian coordinates, indicating the model capability of finding the right ligand conformation at atom level; (2) Centroid Distance, which calculates the Euclidean distance between predicted and true averaged ligand coordinates, reflecting the model capacity to identify the right binding site. To generate an initial ligand conformation, we employed the ETKDG algorithm  using RDKit , which randomly produces a low-energy ligand conformation.

### Performance in Flexible Self-docking

In the context of flexible blind self-docking, where the bound ligand conformation is unknown and the model is tasked with predicting both the bound ligand conformation and its translation and orientation, we observe the notable performance of our FABind. Across most metrics, FABind ranking either as the best or second best, as presented in Table 1. Our FABind exceeds the DiffDock with \(10\) sampled poses. Although FABind may not achieve the best performance in terms of the <2A metric, it demonstrates exceptional proficiency with a mean RMSD score of \(6.4\)A. The comparatively lower performance in the <2A metric can be attributed to the optimization objective of DiffDock, which primarily focuses on optimizing ligand poses towards achieving <2A accuracy. FABind also gets comparative performance on Centroid Distance metrics. Overall, the experimental results showcase the remarkable efficacy of FABind.

### Performance in Self-docking for Unseen Protein

To perform a more rigorous evaluation, we apply an additional filtering step to exclude samples whose UniProt IDs of the proteins are not contained in the data that is seen during training and validation. The resulting subset, consisting of \(144\) complexes, was then used to evaluate the performance of FABind. The results of this evaluation are presented in Table 2, demonstrating that FABind surpasses other deep learning and traditional methods by a significant margin across all evaluation metrics. These findings strongly indicate the robust generalization capability of FABind.

### Inference Efficiency

We conducted an inference study to demonstrate the efficiency of FABind. The statistics regarding the average time cost per sample are presented in Table 1. Comparing our method to others, we observe that the inference time cost of FABind ranks second lowest. Notably, when compared to DiffDock with \(10\) sampled poses, our FABind achieves better docking performance while being over \(170\) times faster. The efficiency of our method can be attributed to eliminating the need for an external

    &  &  \\   &  &  &  &  &  &  \\ 
**Methods** & 25\% & 50\% & 75\% & Mean & 2Å & 5Å & 25\% & 50\% & 75\% & Mean & 2Å & 5Å & **Runtime (s)** \\  QVina-W & 3.4 & 10.3 & 28.1 & 16.9 & 15.3 & 31.9 & 1.3 & 6.5 & 26.8 & 15.2 & 35.4 & 47.9 & 49* \\ GNINA & 4.5 & 13.4 & 27.8 & 16.7 & 13.9 & 27.8 & 2.0 & 10.1 & 27.0 & 15.1 & 25.7 & 39.5 & 146 \\ SMINA & 4.8 & 10.9 & 26.0 & 15.7 & 9.0 & 25.7 & 1.6 & 6.5 & 25.7 & 13.6 & 29.9 & 41.7 & 146* \\ GLIDE & 3.4 & 18.0 & 31.4 & 19.6 & **19.6** & 28.7 & 1.1 & 17.6 & 29.1 & 18.1 & 29.4 & 40.6 & 1405* \\ Vina & 7.9 & 16.6 & 27.1 & 18.7 & 1.4 & 12.0 & 2.4 & 15.7 & 26.2 & 16.1 & 20.4 & 37.3 & 205* \\  EquiBind & 5.9 & 9.1 & 14.3 & 11.3 & 0.7 & 18.8 & 2.6 & 6.3 & 12.9 & 8.9 & 16.7 & 43.8 & **0.03** \\ TanRbind & 3.4 & 5.7 & 10.8 & 10.5 & 3.5 & 43.7 & 1.2 & 2.6 & 8.4 & 8.2 & 40.9 & 70.8 & 0.87 \\ EBiBind & 3.0 & 6.1 & 10.2 & 10.1 & 6.3 & 38.9 & 1.2 & 2.3 & 7.0 & 7.6 & 43.8 & 66.0 & 0.44 \\ DiffDock (1) & 4.1 & 7.2 & 18.2 & 12.5 & 8.1 & 33.1 & 1.4 & 3.7 & 16.7 & 10.0 & 33.6 & 58.3 & 2.72 \\ DiffDock (10) & 3.2 & 6.4 & 16.5 & 11.8 & 14.2 & 38.7 & 1.1 & 2.8 & 13.3 & 9.3 & 39.7 & 62.6 & 20.81 \\ DiffDock (40) & 2.8 & 6.4 & 16.3 & 12.0 & 17.2 & 42.3 & 1.0 & 2.7 & 14.2 & 9.8 & 43.3 & 62.6 & 82.83 \\  FABind & **2.2** & **3.4** & **8.3** & **7.7** & **19.4** & **60.4** & **0.9** & **1.5** & **4.7** & **5.9** & **57.6** & **75.7** & 0.12 \\   

Table 2: Flexible blind self-docking performance on unseen receptors.

pocket selection module, and the sampling and scoring steps used by other methods. By doing so, our FABind produces significantly faster docking. While EquiBind exhibits superior inference speed, it performs poorly in the docking task. These compelling findings strongly support the claim that our FABind is not only highly fast but also accurate.

## 5 Further Analysis

### Pocket Prediction Analysis

In this study, We conduct an independent analysis of our pocket prediction module. To evaluate its performance, we compared it with two existing methods, TankBind and E3Bind, both of which use P2Rank to first segment the protein to multiple pocket blocks, and then apply a confidence module for ranking and selection. The selected pockets are used to compare with our predicted pockets. The pocket prediction performance is measured by the distance between the predicted pocket center and the center of the native binding site (DCC) metric, which is a widely-used metric in pocket prediction task . We use thresholds of \(3\)A, \(4\)A, and \(5\)A, to determine successful predictions. From Table 3, we can see that: (1) removing the center constraint (classification only) significantly hurts the prediction performance. (2) The integration of ligand information into the pocket prediction task also enhances performance and efficiency. Previous two-stage methods involve initial docking for multiple pocket candidates, followed by the selection of the optimal candidate. Nevertheless, the majority of potential binding sites are irrelevant to the particular ligand . Our FABind further improves the inference efficiency as our pocket prediction is essentially designed for predicting the most probable pocket for the specific ligand. (3) Combined with center constraint, even without incorporating ligand information, our pocket prediction performance outperforms TankBind and E3Bind, which shows our rational design of the pocket prediction module is effective.

### Ablation Study

In this subsection, we perform several ablation studies to analyze the contribution of different components, the results for selected metrics are reported in Table 4. We examine four specific settings: removal of the scheduled training strategy (teacher forcing); removal of the distance map related losses; removal of the iterative refinement; and removal of the cross-attention update. From the table, we can see that: (1) each of the components contributes to the good performance of our FABind. (2) The combination of distance map losses impacts most of the docking performance. (3) The remaining components primarily contribute to improvements in fine-grained scales (<2A). A full ablation study can be found in Appendix Section C.1.

### Case Study

**FABind can identify the right binding pocket for a new large protein.** In Fig. 2(a), the large protein target (PDB 6NPI) is not seen during training, thus posing a significant challenge to accurately locate the binding pocket. Though predicted poses of other deep learning models (DiffDock, E3Bind, TankBind, and EquiBind) are all off-site, FABind can successfully identify the native binding site and predict the binding pose (in green) with low RMSD (\(3.9\)A), showing the strong generalization ability.

**FABind generates ligand pose with better performance and validity.** Fig. 2(b) shows the docked results of another protein (PDB 6G3C). All methods find the right pocket, but our FABind aligns best RMSD (\(0.8\)A) with the ground truth ligand pose. In comparison, E3Bind produces a knotted pose where rings are knotted together which is not valid. DiffDock also produces the same accurate ligand

    & **RMSD** & **RMSD** \\
**Methods** & **Mean (Å)** & **\% Below 2Å**\(\) \\  FABind & 6.4 & 33.1 \\  no scheduled sampling & 6.4 & 28.7 \\ coord loss only & 6.9 & 16.3 \\ no iterative refinement & 6.6 & 22.5 \\ no cross-attention & 6.4 & 21.4 \\   

Table 4: Results of ablation study.

    &  \\ 
**Methods** & 3Å & 4Å & 5Å \\  TankBind & 18.2 & 32.0 & 39.9 \\ E3Bind & 26.7 & 35.8 & 50.1 \\  P2Rank & 36.4 & 50.1 & 57.0 \\  FABind & **42.7** & **56.5** & **62.8** \\ - ligand information & 36.9 & 51.5 & 59.0 \\ - center constraint & 8.8 & 22.9 & 31.7 \\   

Table 3: Results of pocket prediction.

pose but is much slower. These show that FABind can not only find good pose with high speed but also maintain structural rationality.

## 6 Conclusion

In this paper, we propose an end-to-end framework, FABind, with several rational strategies. We propose a pocket prediction mechanism to be jointly trained with the docking module and bridge the gap between pocket training and inference with a scheduled training strategy. We also introduce a novel equivariant layer for both pocket prediction and docking. For ligand pose prediction, we incorporate both direct coordinate optimization and the protein-ligand distance map-based refinement. Empirical experiments show that FABind outperforms most existing methods with higher efficiency. In future work, we aim to better align the pocket prediction and docking modules and develop more efficient and effective tools.

## 7 Acknowledgments

We would like to thank the reviewers for their insightful comments. This work was partially supported by National Natural Science Foundation of China (NSFC Grant No. 62122089), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China.