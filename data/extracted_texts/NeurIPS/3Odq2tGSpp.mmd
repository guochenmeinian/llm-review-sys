# Stylus:

Automatic Adapter Selection for Diffusion Models

 Michael Luo\({}^{1}\)  Justin Wong\({}^{1}\)  Brandon Trabucco\({}^{2}\)  Yanping Huang\({}^{3}\)

Joseph E. Gonzalez\({}^{1}\)  Zhifeng Chen\({}^{3}\)  Ruslan Salakhutdinov\({}^{2}\)  Ion Stoica\({}^{1}\)

\({}^{1}\)UC Berkeley \({}^{2}\)CMU MLD \({}^{3}\)Google Deepmind

{michael.luo,wong.justin,jegonzal,istoica}@berkeley.edu

{btrabucc,rsalakhu}@cs.cmu.edu

{huangyp,zhifengc}@google.com

###### Abstract

Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters--most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a _set_ of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.

## 1 Introduction

In the evolving field of generative image models, finetuned adapters  have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different

Figure 1: **Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Stable Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO .**adapters and model checkpoints, fueling the proliferation of creative AI art [28; 51]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA)  emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.

In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts (see Fig. 1). However, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings . Specifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data--a common issue on open-source platforms. Furthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks. For instance, the prompt "two dogs playing the snow" suggests that there are two tasks: generating images of "dogs" and "snow". This necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems . Finally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).

We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the _refiner_ plugs in an adapter's model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods , the _retriever_ scores the relevance of each embedding against the user's entire prompt to retrieve a set of candidate adapters. Finally, the _composer_ segments the prompt into disjoint tasks, further prunes irrelevant candidate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that introduce biases detrimental to image generation (SS 4.3). Finally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.

To evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus's _refiner_. Our results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints--shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators. As a system, Stylus is practical and does not present large overheads to the batch image generation process. Finally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.

Figure 2: **Stylus algorithm.** Stylus consists of three stages. The _refiner_ plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding. The _retriever_ fetches candidate adapters that are relevant to the entire user prompt. Finally, the _composer_ prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.

Related Works

Adapters.Adapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar performance to full fine-tuning [7; 11; 14].

Our study focuses on retrieving and merging multiple Low-Rank adapters (LoRA), the popular approach within existing open-source communities [28; 29; 51].

Adapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications [19; 36; 40; 45; 46]. For large language models (LLM), the linear combination of multiple adapters improves in-domain performance and cross-task generalization [3; 15; 16; 48; 49; 55]. In the image domain, merging LoRAs effectively composes different tasks--concepts, which are then ranked against a user prompt based on similarity metrics [4; 9; 21; 27; 37; 39]. Similarly, our work draws inspiration from RAG to encode adapters as vector embeddings: leveraging visual-language foundational models (VLM) to generate semantic descriptions of adapters, which are then translated into embeddings.

A core limitation to RAG is limited precision, retrieving semi-relevant documents that do not exactly answer the prompt. This leads to a "needle-in-the-haystack" problem, where more relevant documents are buried further down the list . Recent work introduce _reranking_ step; this technique uses cross-encoders to assess both the raw user prompt and the ranked set of raw texts individually, thereby discovering texts based on actual relevance [27; 38]. Rerankers have been successfully integrated with various LLM-application frameworks [2; 24; 35].

## 3 Our Method: Stylus

Adapter selection presents three distinct challenges compared to existing methods for retrieving text documents, as outlined in Section 2. First, computing embeddings for adapters is a novel task, made more difficult without access to training datasets. Furthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks. This challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt. Finally, composing multiple adapters can degrade image quality and inject foreign biases into the model. Our three-stage framework below--**R**efine, **R**etrieve, and **C**ompose--addresses the above challenges (Fig. 2).

### Refiner

The _refiner_ is a two-stage pipeline designed to generate textual descriptions of an adapter's task and the corresponding text embeddings for retrieval purposes. This approach is analagous to pre-computed embeddings over an external database of texts in retrieval-based methods .

Given an adapter \(A_{i}\), the first stage is a vision-language model (VLM) that takes in the adapter's model card--a set of randomly sampled example images from the model card \(_{i}\{I_{i1},I_{i2},...\}\), the corresponding prompts \(_{i}\{p_{i1},p_{i2},...\}\), and an author-provided description,2\(D_{i}\)--and returns an improved description \(D_{i}^{*}\). Optionally, the VLM also recommends the weight for LoRA-based adapters,

Figure 3: **Number of Adapters.** Civit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face. Low-Rank Adaptation (LoRA) is the dominant approach for finetuning.

as the adapter weight is usually specified either in the author's description \(D_{i}\) or the set of prompts \(P_{i}\), a feature present in popular image generation software . We denote this weight/coefficient as \(_{i}\). If information cannot be found, the LoRA's weight is set to \(_{i}=0.8\). In our experiments, these improved descriptions were generated by Gemini Ultra  (see SS A.1 for prompt). We chose the Gemini class of models since it has mature safety guardrailing. Specifically, Google's VertexAI API provides stringent safety settings to block explicit content for the input prompt. Safety filters helped us filter out around 30% of original adapters that were tagged as non-explicit by other model repositories. The second stage uses an embedding model (\(\)) to embed the text description \(D^{*}_{i}\) for each adapters to yield embeddings, \(e_{i}=(D^{*})\). In our experiments, we create embeddings from OpenAI's text-embedding-3-large model . We store pre-computed embeddings in a vector database, formally notated by the matrix, \(V\).

### Retriever

The _retriever_ fetches the most relevant adapters over the entirety of the user's prompt using cosine similarity. Precisely, the retriever employs the same embedding model (\(\)) to process the user prompt, \(q\), generating embedding \(e_{q}=(q)\). Using the vector database, we calculate exact cosine similarity scores between the prompt's embedding \(e_{s}\) and the embedding of each adapter in the matrix \(V\). The similarity vector, \(s_{q}=^{T}V}{|e_{q}||V|}\), scores the adapter descriptions by similarity. The retriever simply returns indices of the top-k adapters \(_{k}=(s_{q})\). In our experiments, we find \(k=150\) is effective for StylusDocs. We denote the set of \(k\) descriptions of the adapters, \(_{k}\) as \(D^{*}_{k}\).

### Composer

The _composer_ serves a dual purpose: segmenting the prompt into tasks from a prompt's keywords and assigning retrieved adapters to tasks. This implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the prompt through keyword grounding. For example, if the prompt is "pandas eating bamboo", the composer may discard an irrelevant "grizly bears" adapter and a biased "panda mascots" adapter.

The composer (\(\)) is a function of the prompt (\(q\)), the top \(K\) adapters (\(_{K}\)) from the retriever. Formally, denote the tasks identified by the composer as \((q)=\{t_{1},t_{2},,t_{n}\}\). The composer produces a mapping from task to adapters:

\[(s,_{K})=\{(t_{i},_{k_{i}})\,|\,t_{i} (q),_{k_{i}}_{K}, j _{k_{i}},Align(_{j},t_{i})\} \]

where \(_{k_{i}}\) is the subset of adapters per task \(t_{i}\), \(Align(A_{j},t_{i})\) is a predicate that holds if the adapter, \(A_{j}\) is aligned with the task, \(t_{i}\).

While the composer can be further improved by fine-tuning with human-labeled data , we find that prompting a long-context Large Language Model (LLM) suffices. The LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters. In practice, the alignment function is determined in the LLM's chain-of-thought procedure before it outputs the final mapping of adapters to tasks. In our implementation, we choose Gemini 1.5, with a 128K context window, as the composer's LLM (see App. A.3 for the full prompt).

Stylus's composer is similar to _reranking_. Rerankers employ cross encoders (\(\)) that compare the retriever's individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: \((,D^{*})\). This prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment. Our experimental ablations (SS 4.3) show that our composer outperforms existing rerankers (Cohere, rerank-english-v2.0) .

Figure 4: **Qualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.** Stylus produces highly detailed images that correctly depicts keywords in the context of the prompt. For the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.

### Masking

The composer maps tasks to corresponding sets of highly relevant adapters. To further mitigate sensitivity to low-quality adapters, Stylus reduces the number of selected adapters with a straightforward masking scheme. Specifically, for each task, candidate masks are generated, and one is randomly selected to be applied over the set of adapters. Formally, for a given task, \(m_{i}\{0,1\}^{|_{k}|}\), is either a one hot encoding, \(\), or \(\), forming a set of possible masks, \(M_{i}\). Across all tasks, masks are combined by taking the cross-product, \(G=M_{1} M_{2}... M_{n}\). The combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (SS 4.2.3). This approach also curtails the number of final adapters merged into the base model, minimizing the risk of composing low-quality adapters that may introduce undesirable effects to the image .

### Merging

Stylus employs two key insights for effectively merging adapter weights. First, when applied to a single task, large adapter weights can introduce notable visual artifacts, such as over-saturation (Fig. 13(a)). Second, across multiple tasks, adapters tend to be orthogonal in the weight space, as they are designed to modify distinct, orthogonal concepts . Hence, Stylus computes the final adapter weights by _averaging_ weights per task and _summing_ weights across tasks. This approach ensures that the adapter weights per task remain appropriately scaled.

We mathematically illustrate our merging scheme below. Recall, the refiner outputs \(_{i}\), the recommended weight/coefficient, for each adapter. (SS 3.1). As shown in recent work , multiple LoRAs can be merged with the base model weights (\(W_{base}\)). We arrive at our final merged model weights by a summing the adapter weights normalized by task. For a mapping, \(\{(t_{1},_{k_{1}}),(t_{2},_{k_{2}}),(t_{n}, _{k_{n}})\}\), and \(g=(m_{1},m_{2},,m_{n}) G\), the final model weight is:

\[W^{}=W_{base}+_{i n}_{j x_{i}}_{j}_ {j}/|x_{i}| \]

where \(x_{i}=Mask(m_{i},_{i})\) and \(_{j}\) is the LoRA's weight. We set \(=0.8\) to mitigate image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4). For batch inference, Stylus returns images sorted by CLIP score.

## 4 Results

### Experimental Setup

**Adapter Testbed.** Adapter selection requires a large database of adapters to properly evaluate its performance. However, existing methods  only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters. To bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace . This dataset contains precomputed OpenAI embeddings  and improved adapter descriptions from Gemini Ultra-Vision , the output of Stylus's refiner component (SS 3.1). We further characterize the distribution of adapters in App. A.3.

**Generation Details.** We assess Stylus against Stable-Diffusion-v1.5  as the baseline model. Across experiments, we employ two well-known checkpoints: Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images. Our image generation process integrates directly with Stable-Diffusion WebUI  and defaults to 35 denoising steps using the default DPM Solver++ scheduler . To replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024

Figure 5: **Human Evaluation.** Stylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.

from 512x512 images, with the default latent upscaler  and denoising strength set to 0.7. For images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint's original style. To counteract this, we introduce a _debias prompt_ injected at the end of a user prompt to steer images back to the checkpoint's style3. We launched 16 replicas of Stylus and Stable Diffusion on 8 A100-80GB GPUs for 4 weeks to generate images for evaluation.

### Main Experiments

#### 4.2.1 Human Evaluation.

To demonstrate our method's general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO  and PartiPrompts , and two checkpoints, which generate realistic and anime-style images respectively. Examples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.

To conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination. These raters were asked to indicate their preference for Stylus or Stable-Diffusion-v1.5. In Fig. 5, users generally showed a preference for Stylus over existing model checkpoints. Although preference rates were consistent across datasets, they varied significantly between different checkpoints. Adapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.

#### 4.2.2 Automatic Benchmarks.

We assess Stylus using two automatic benchmarks: CLIP , which measures the correlation between a generated images' caption and users' prompts, and FID , which evaluates the diversity and aesthetic quality of image sets. We evaluate COCO 2014 validation dataset, with 10K

Figure 6: **Automatic Evaluation Metrics. Figure (a) plots the CLIP/FID pareto curve. We observe Stylus shifts the curve down (improved visual fidelity, FID) and to the right (improved textual alignment, CLIP score) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12]. Table (b) evaluates Stylus against different retrieval methods. Stylus outperforms existing retrieval-based methods, attains the best FID score, and achieves similar CLIP score to Stable Diffusion.**

Figure 7: **Image Diversity. Given the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right). Stylus’s diversity comes from its masking scheme and the composer LLM’s temperature parameter.**

sampled prompts, and the Realistic-Vision-v6 checkpoint. Fig. 5(a) shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment. This improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.

#### 4.2.3 VLM as a Judge

We use _VLM as a Judge_ to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments . For visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects. When asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented. To combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie. In Fig. 7(a), we assess evaluate 100 randomly sampled prompts from the PartiPrompts dataset . Barring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation. For textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint. As most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts. We provide the full prompt in Appendix A.5.

Figure 8: Figure **(a)** and **(b)** evaluate the preference win rate using GPT-4V as a judge. Stylus achieves higher preference scores as judged by GPT-4V for visual quality and image diversity. Figure **(c)** shows that Stylus achieves higher diversity scores than Stable Diffusion when prompt length increases.

Figure 9: **Different Retrieval Methods. Stylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.**

#### 4.2.4 Diversity per Prompt

Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes. Qualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles. To quantitatively assess this diversity, we use two metrics:

_dFID:_ Previous evaluations with FID  show that Stylus improves image quality and diversity _across prompts4_. We define _d_FID specifically to evaluate diversity per prompt, calculated as the variance of latent embeddings from InceptionV3 . Mathematically, _d_FID involves fitting a Normal distribution \((,)\) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, _d_FID \(=\ \).

**GPT-4V:** We use _VLM as a Judge_ to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts. Five images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V's positional bias . Similar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity . Full prompt and additional details are provided in App A.5.

Fig. 7(b) displays preference rates and defines a win when Stylus achieves higher _d_FID or receives a higher score from GPT-4V for a given prompt. Across 200 prompts, Stylus prevails in approximately 60% and 58% cases for _d_FID and GPT-4V respectively, excluding ties. Figure 7(c) compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images. Additional results measuring diversity per keyword are presented in Appendix A.6.

### Ablations

#### 4.3.1 Impact of Refiner

Table 10 evaluates the impact of different refiner pipelines on Stylus's end-to-end performance. Below, we describe each refiner baseline:

**No-Refiner**: Stylus uses baseline adapter descriptions sourced from popular repositories such as HuggingFace . These descriptions are often low-quality and underspecified. Hence, Stylus chooses the wrong adapters and attains lower CLIP and FID scores relative to SDv1.5.

**Gemini-Ultra Refiner**: This refiner,

We benchmark Stylus's performance relative to different retrieval methods. For all baselines below, we select the top three adapters and merge them into the base model.

**Random**: Adapters are randomly sampled without replacement from StylusDocs.

**Retriever**: The retriever emulates standard RAG pipelines , functionally equivalent to Stylus without the composer stage. Top adapters are fetched via cosine similarity over adapter embeddings.

**Reranker**: An alternative to Stylus's composer, the reranker fetches the retriever's adapters and plugs a cross-encoder that outputs the semantic similarity between adapters' descriptions and the prompt. We evaluate with Cohere's reranker endpoint .

As shown in Tab. 5(b), Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model. First, both the retriever and reranker

    & **CLIP** (\(\)) & **FID** (\(\)) \\  No-Refiner & 24.91 (-2.31) & 24.26 (+0.30) \\ Gemini-Ultra Refiner & 27.25 (+0.03) & 22.05 (-1.91) \\ GPT-4o Refiner & 28.04 (+0.82) & 21.96 (-2.00) \\ SD v1.5 & 27.22 & 23.96 \\   

Table 10: **Refiner’s impact on End2End performance.** Without a refiner, Stylus performs worse than SD v1.5 due to the poor quality of author-provided descriptions. Annotating adapters with GPT-4o significantly improves adapter descriptions and achieves higher CLIP/FID scores than Stylus’s default refiner VLM, Gemini-Ultra.

significantly underperform compared to Stable Diffusion. Each method selects adapters that are _similar_ to the prompt but potentially introduce unrelated biases. In Fig. 9, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants. Furthermore, both methods incorrectly assign weights to adapters, causing adapters' tasks to overshadow other tasks within the same prompt. In Fig. 9, both the reranker and retriever generate images solely focused on singular items--beds, chairs, suitcases, or trains--while ignoring other elements specified in the prompt. We provide an analysis of failure modes in A.4.

Conversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion. The random baseline chooses adapters that are orthogonal to the user prompt. Thus, these adapters alter unrelated concepts, which does not affect image generation. In fact, we observed that the distribution of random policy's images in Fig. 9 were nearly identical to Stable Diffusion.

#### 4.3.3 Breakdown of Stylus's Inference Time

This section breaks down the latency introduced by various components of Stylus. We note that image generation time is independent of Stylus, as adapter weights are merged into the base model .

Figure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts. Specifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds. The composer's large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens. Finally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process. However, Stylus's latency remains consistent across all batch sizes, as the composer and retriever run only once. Hence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.

#### 4.3.4 Image-Domain Tasks

Beyond text-to-image, Stylus applies across various image-to-image tasks. Fig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting.

**Image translation:** Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt's definition. Stylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style. We present examples in Fig 12a. For a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits. For a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.

**Inpainting:** Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask. Stylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity. We provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).

Figure 11: **Comparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS). At BS=1, Stylus accounts for \(75\%\) of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions. However, Stylus’s overhead decreases when batch size increases.**

## 5 Discussion

The strategic composition and routing of adapters in Stylus introduce a new dimension of model performance, broadening the scope of potential applications. One such application is the automatic creation of agentic workflows [54; 57]. For instance, Stylus's composer can decompose a complex task into a graph of subtasks and assign them to specialized agents to improve end-to-end performance. Additionally, routing can extend beyond adapters to encompass different models, allowing Stylus to optimize the cost-performance tradeoff by dynamically selecting between high-performing, resource-intensive models and more efficient, lower-cost models [31; 33]. Finally,for fact verification, adapters have shown significant potential in reducing hallucinations [10; 44]. Stylus can selectively use domain-specific, fine-tuned models to enhance factual accuracy and better verify claims.

As demonstrated in Sec. 4, Stylus demonstrates significant potential for improvement, as adapter composition introduces future research challenges beyond the scope of this work. A summary of Stylus's failure cases are provided in Fig. 14. Specifically, adapters can _restrict certain concepts_ from appearing in an image and _limit diversity_ among multiple subjects within a scene. While Stylus does not fundamentally solve these challenges, Stylus reduces the likelihood of these problems occurring by reducing the number of adapters through its masking algorithm. Lastly, Stylus introduces noticeable overheads to the inference pipeline, primarily stemming from the composer's long context prompts, which can be accelerated with various sequence parallel techniques [20; 23].

## 6 Conclusion

We propose Stylus, a flexible algorithm that automatically selects and composes adapters to generate better images. Our method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts' keywords. To evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings. Our evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.