# Canonical normalizing flows for manifold learning

Kyriakos Flouris

Department of Information Technology

and Electrical Engineering

ETH Zurich

kflouris@vision.ee.ethz.ch &Ender Konukoglu

Department of Information Technology

and Electrical Engineering

ETH Zurich

kender@vision.ee.ethz.ch

###### Abstract

Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such a manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used to optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that the density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an _efficient representation_ of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis, with degenerate information being stored in each dimension. Alternatively, if a locally orthogonal and/or sparse basis is to be learned, here coined canonical intrinsic basis, it can serve in learning a more compact latent space representation. Toward this end, we propose a canonical manifold learning flow method, where a novel optimization objective enforces the transformation matrix to have few prominent and non-degenerate basis functions. We demonstrate that by minimizing the off-diagonal manifold metric elements \(_{1}\)-norm, we can achieve such a basis, which is simultaneously sparse and/or orthogonal. Canonical manifold flow yields a more efficient use of the latent space, automatically generating fewer prominent and distinct dimensions to represent data, and consequently a better approximation of target distributions than other manifold flow methods in most experiments we conducted, resulting in lower FID scores. 1

## 1 Introduction

Many emerging methods in generative modeling are based on the manifold hypothesis, i.e., higher-dimensional data are better described by a lower-dimensional sub-manifold embedded in an ambient space. These methods replace the bijective normalizing flow in the original construction  (NF) with an injective flow. NFs are constructed as a smooth bijective mapping, i.e. a homeomorphism, where the learned distribution density is supported on the set of dimension \(D\) equal to the data dimension, where \(^{}\) is the data space. In order to fulfill the manifold hypothesis, an emerging class of methods, namely manifold learning flow models (MLF) , model the latent distribution as a random variable in \(^{d}\) where \(d<D\), i.e. realize injective flows. Consequently, the density lives on some d-dimensional manifold \(_{d}\) embedded in \(^{D}\) as required. If \(_{d}\) is known a-priori, , the density transformation to \(^{D}\) in MLF is a relatively straight forward affair. If the manifold is to belearned, the change-of-variable formula, i.e. the volume element, can be computationally inhibiting to calculate. Caterini et al  have recently solved this problem and established tractable likelihood maximization.

MLF methods have thus evolved to the point where the general manifold hypothesis is properly incorporated and the likelihood is efficiently calculated. However, it has been observed that the learned manifold can be suboptimal; for example, due to a nonuniform magnification factor  and the inability to learn complex topologies . Furthermore, pushforward models such as MLFs, i.e., \(d<D\), are designed to avoid over-fitting, but \(d\) in \(^{d}\) is still arbitrary unless determined by some meta-analysis method. As well as, there is often degeneracy in the information stored in each of these latent dimensions \(d\). For instance, as we show in Section 4.1, consider modeling a two-dimension line with considerable random noise, i.e. a 'fuzzy line'. If \(d=2\), both latent dimensions will attempt to encompass the entire manifold in a degenerative use of latent dimensions. Particularly, this behavior of storing degenerate representations is inefficient and can lead to over-fitting, topological, and general learning pathologies. Alternatively, we postulate that motivating sparsity  and local orthogonality during learning can enhance the manifold learning process of MLFs.

To this end, we propose an optimization objective that either minimizes or finds orthogonal gradient attributions in the learned transformation of the MLF. Consequently, a more compact  and efficient latent space representation is obtained without degenerate information stored in different latent dimensions, in the same spirit as automatic relevance determination methods  or network regularization . The relevant dimensions correspond to locally distinct tangent vectors on the data manifold, which can yield a natural ordering among them in the generations. Borrowing the term 'canonical basis', i.e., the set of linearly independent generalized eigenvectors of a matrix, we introduce Canonical manifold learning flows (CMF). In CMF, the normalizing flow is encouraged to learn a sparse and canonical intrinsic basis for the manifold. By utilizing the Jacobians calculated for likelihood maximization, the objective is achieved without additional computational cost.

## 2 Theoretical background

### Normalizing and rectangular normalizing flows

First, we define a data space as \(\) with samples \(\{x_{1},...,x_{n}\}\), where \(x_{i}^{D}\), and a latent space \(^{d}\). If, \(d=D\), then a normalizing flow is a \(\) parameterized diffeomorphism \(q_{}:^{D}^{D}\), i.e., a differentiable bijection with a differentiable inverse, that transforms samples from the latent space to the data space, \(:=q_{}()\). Optimization of \(\) is achieved via likelihood maximization, where due to the invertibility of \(q_{}\) the likelihood, \(p_{}(x)\), can be calculated exactly from the latent probability distribution \(p_{}(z)\) using the transformation of distributions formula ,

\[p_{}(x)=p_{}(q_{}^{-1}(x))|_{ q_{}}(q_{}^{-1}(x))|^{-1}.\] (1)

\(\) denotes the Jacobian functional such that \(_{q_{}}= x/ z\), i.e., the Jacobian of \(q_{}\) at \(x=q_{}(z)\). Here, \(\) is modelled by a random variable \(z p_{}\) for a simple density \(p_{}\), e.g., a normal distribution. Therefore, for large \(n\), a normalizing flow approximates well the distribution that gave rise to the samples in a dataset \(\{x_{i}\}_{i=1}^{n}\) by transforming the samples \(z p_{}\) with a trained \(q_{^{*}}\) if \(^{*}:=*{argmax}_{}_{i=1}^{n} p_{}(x_{ i})\).

In the context of manifold learning flows, or rectangular normalizing flows, \(d<D\) implies there is a lower dimensional manifold \(^{d}\) that is embedded into the \(R^{D}\) with a smooth and injective transformation \(g_{}:^{d}^{D}\). In practice, \(g_{}=h_{} f_{}\), where \(h_{}:^{d}^{d}\), \(f_{}:^{D}^{D}\) and \(:^{d}^{D}\). Padding can be used for trivial embedding, i.e., \(()=(_{0}_{d-1},0 0)\). The rectangular Jacobian of \(_{g_{}}\) is not invertible and thus, \(_{g_{}}^{T}_{g_{}}_{g_{}}^{T}\), which implies that the volume element needs to be calculated fully, i.e., the square root of the determinant of \(_{g_{}}^{T}_{g_{}}\) which is an equivalent expression for the general metric tensor in this context - see the appendix. This leads to a more generic formulation

\[p_{}(x)=p_{}(g_{}^{-1}(x))| _{g_{}}^{T}(g_{}^{-1}(x))_{g_{}}(g_{}^{-1} (x))|^{-1/2}.\] (2)

However, note that maximizing \(p_{}(x)\) on its own would only maximize the likelihood of the projection of \(x\) on the latent space, not ensuring that the model can reconstruct \(x\) from \(z\), i.e., the modeled manifold may not be aligned with the observed data. To encourage \(x\), namely to align the learned manifold to the data space, a reconstruction error needs to be minimized during optimization. The following loss is thus added to the maximum-likelihood loss function, as explained in :

\[_{i=1}^{n}\|x_{i}-g_{}(g_{}^{-1}(x))\|_{2}^{2}.\] (3)

Combining the logarithm of Equation (2) and using the hyperparameter \(>0\) to adjust Equation (3) we arrive at the total Lagrangian to be maximized:

\[^{*}=*{argmax}_{}_{i=1}^{ n} p_{Z}(g_{}^{-1}(x_{i}))-|_{h_{ n}}^{T}(g_{}^{-1}(x_{i}))|\\ -|_{f_{}}^{T}(f_{ }^{-1}(x_{i}))_{f_{}}(f_{}^{-1}(x_{i}))|-\|x_ {i}-g_{}(g_{}^{-1}(x_{i}))\|_{2}^{2}.\] (4)

### Riemannian geometry and the metric tensor

A \(d\) dimensional curved space is represented by a Riemannian manifold \(^{d}\), which is locally described by a smooth diffeomorphism \(\), called the chart. The set of tangential vectors attached to each point \(\) on the manifold is called the tangent space \(T_{}\). All the vector quantities are represented as elements of \(T_{}\). The derivatives of the chart \(\) are used to define the standard basis \((_{1},...,_{d})=}{ z_{1}},...,}{ z_{d}}\).

The metric tensor \(g\) can be used to measure the length of a vector or the angle between two vectors. In local coordinates, the components of the metric tensor are given by

\[G_{ij}=_{i}_{j}=}{ z _{i}}}{ z_{j}},\] (5)

where \(\) is the standard Euclidean scalar product.

The metric tensor \(G_{ij}\) describing the manifold \(^{D}\) learned in manifold learning flows is equivalent to the section of the rectangular Jacobian-transpose-Jacobian \(J^{T}J\) that describes \(\). As explained in  and the appendix, the \(J^{T}J\) can be very efficiently approximated in the context of MLFs. Thus, it can be seen that no additional computational cost is need to calculate the metric tensor.

## 3 Related Work and motivation

Generative models with lower-dimensional latent space have long being established as favorable methods due to their efficiency and expressiveness. For instance, variational autoencoders  have been shown to learn data manifolds \(\) with complex topologies [20; 21]. However, as seen in , variational approaches exhibit limitations in learning the distribution \(p_{}\) on \(\). Manifold learning flows  (Mflow) form a bridge between tractable density and expressiveness. Their support \(^{d}\) can inherently match the complex topology of \(^{D}\), while maintaining a well-defined change of variables transformation. Furthermore, state-of-the-art methods can efficiently calculate the determinant of the Jacobian dot product, as in rectangular normalizing flow  (RNF). Denoising normalizing flow  (DNF) represents a progression beyond Mflow, where density denoising is used to improve on the density estimation. RNFs can be seen as a parallel approach to DNF but with a more direct methodology, circumventing potential ambiguities stemming from heuristic techniques like density denoising.

Despite the success of manifold learning flows in obtaining a tractable density whilst maintaining expressivity, the learned manifold is not optimized within its latent space representation , an outstanding problem for most generative models. For example, due to lack of constrains between latent variables it can be the case that some of them contain duplicate information, i.e., form a degenerate intrinsic basis to describe the manifold, also demonstrated in Section 4.1. Sparse learning  was an early attempt to limit this degeneracy and encourage learning of global parameters, introducing solutions such as the relevance vector machine [24; 25] (RVM).The RVM attempts to limit learning to its minimum necessary configuration, while capturing the most relevant components that can describe major features in the data.

Even stricter approaches are principal [26; 27] and independent component analyses [28; 29], PCA and ICA, respectively. PCA and ICA can obtain a well-defined and optimized representation when convergent; however, PCA and ICA are linear in nature and their non-linear counterparts require kernel definitions. Here, the network-based approach is more general, reducing the need for hand-crafting nonlinear mappings. Furthermore, pure PCA and ICA can be limited in modelling some data manifold topologies where strict global independent components are not necessarily a favorable basis . Furthermore,  presents a method for efficient post-learning structuring of the latent space, showcasing the advantages of model compactness. We propose a canonical manifold learning flow where the model is motivated to learn a compact and non-degenerate intrinsic manifold basis.

Furthermore, it has been shown that tabular neural networks can show superior performance when regularized via gradient orthogonalization and specialization . They focus on network regularization, which is not specific to one method but requires certain network structure and costly attribution calculations.

Recently, NF methods with flow manifested in the principal components have been proposed [31; 32]. Cranmer et al.  focus on datasets that have distinct PCAs that can be efficiency calculated and, therefore, utilize the computed PCAs for efficient density estimation. Cunningham et al.  (PCAflow) relies on a lower-bound estimation of the probability density to bypass the \(J^{T}J\) calculation. This bound is tight when complete principal component flow, as defined by them, is achieved. The fundamental distinction here is that our method does not confine itself to a pure PCAflow scenario, which has the potential to restrict expressivity. In contrast, our method only loosely enforces orthogonality, while encouraging sparsity, i.e. promoting learning of a canonical manifold basis.

## 4 Method

### Canonical intrinsic basis

In this section, in order to motivate the proposed development, we first demonstrate how rectangular manifold learning flows assign a latent space representation in comparison to canonical manifold learning flow. We generated synthetic data on a tilted line with noise in the perpendicular and parallel directions, i.e., a fuzzy line. We sample 1000 \(x_{i}\)'s such that \(x_{1}(-2.5,2.5)\) and \(x_{2}=x_{1}+\), where \((-0.5,0.5)\). In Figure 1(a), a two-dimensional manifold \(\) is learned with the vanilla-RNF method using the samples by minimizing the loss given in Equation 4. As expected and seen on the left of Figure 1(a), the probability density is learned satisfactorily, i.e., the model fit well the samples and can generate samples with accurate density. However, when samples on the manifold are generated from the two latent dimensions, \(z_{i}\), individually, shown in Figure 1 (ii) and (iii), the sampling results are very similar. The implication being that the intrinsic basis is almost degenerate and information is duplicated amongst the latent variables. In contrast, when the same \(\) is learned with the proposed CMF, shown in Figure 2, the sampled latent dimensions are close to perpendicular to each other. Each basis is capturing non-degenerate information about \(\), see Figure 1(b), i.e., the two major axes of the dataset.

### Canonical manifold learning

As canonical manifold is not a standard term in literature, let us start by defining a canonical manifold for manifold learning flows.

**Definition 4.1**.: _Here, a canonical manifold, \(\), is a manifold that has an orthogonal and/or sparse basis \(_{i}\) such that \(_{i}_{j}=0\ \) and whenever \(i j\)._

The name in Definition 4.1 is inspired from the "canonical basis", i.e., the set of linearly independent generalized eigenvectors of a matrix, as defined in linear algebra. We hypothesize that enforcing learning a canonical manifold as defined in Definition 4.1 during manifold flows will lead to a less degenerate use of the latent space, and consequently a better approximation of the data distribution. As in sparse learning , during training necessary dimensions in the latent space will be automatically determined, and those that are necessary will be used in such a way to model a manifold with an orthogonal local basis to fit the data samples. This, we believe, will enhance the manifold learning process, avoiding overfitting and other learning pathologies associated with correlated attributions to dimensions of the latent space.

In order to obtain a canonical basis we first realize that, as described in Section 2 and the appendix, the metric tensor can be used as concise language to describe the manifold i.e. the transformation of the chart.

\[G_{ij}=_{k}^{-1,k}(x)}{ z^{i}}^{-1,k}(x)}{ z^{j}}=_{k}}{  z^{i}}}{ z^{j}}.\] (6)

This provides an opportunity to regularize the learned representation directly with a single optimization objective. The dependencies on \(G\) are dropped for brevity.

Leveraging the metric tensor representation, we propose to minimize the off-diagonal elements of the metric tensor, enforcing learning of a canonical manifold

\[\|G_{i j}\|_{1}^{1}_{i}_{j i}\|G_{ ij}\|_{1}^{1},\] (7)

where \(\|\|_{1}^{1}\) is the \(_{1}\) norm. Using \(\|G_{i j}\|_{1}^{1}\) as a cost to minimize serves both the sparsity and generating orthogonal basis. While the \(_{2}\) norm could be used to enforce a specific embedding , it is not in the scope of this work. In order to minimize this cost, the network will have to minimize the dot products \( x/ z^{i} x/ z^{j}\). There are two ways to accomplish this. The first is to reduce the magnitudes of the basis vectors \( x/ z^{i}\). Minimizing the magnitudes with the \(_{1}\) norm would lead to sparser basis vectors. The second is to make the basis vectors as orthogonal as possible so the dot product is minimized, which serves towards learning a canonical manifold as well. As detailed in  and the appendix, the Jacobian product, the bottleneck for both RNF and our method, can be efficiently approximated. With complexity \((id^{2})\), it is less than \((d^{3})\) when \(i<<d\), where \(i\) is the conjugate gradients method iterations. Thus, encouraging a canonical manifold through \(G_{i j}\) incurs minimal extra computational cost.

One can imagine an alternative optimization schemes. For instance, the diagonal elements of the metric tensor \(G_{kk}\) can be used to access the transformation of individual \(z_{i}\). Minimizing the \(_{1}\) norm \(_{k}\|G_{kk}\|_{1}^{1}\) will encourage sparse learning, akin to a relevance vector machine  or network specialization . Nevertheless, minimizing the diagonal elements will clearly not motivate perpendicular components and notably there is no mixing constraint on these components. Consequently, only the magnitude of the transformation can be affected.

Furthermore, one can imagine minimizing the cosine similarity between the basis vectors to promote orthogonality. However, this would not serve towards automatically determining the necessary dimensions in the latent space. Combining the \(_{1}\) of the diagonal elements and the cosine similarity may also be a solution; however, this would bring an additional weighing factor between the two

Figure 1: Comparison of density plots for a fuzzy line learned with RNF (**a**) and CMF (**b**). Sampled 1000 \(x_{i}\)’s such that \(x_{1}(-2.5,2.5)\) and \(x_{2}=x_{1}+\) with \((-0.5,0.5)\). The black dots represent samples from the real data manifold. (i) Samples from the model using the full latent space where colors correspond to \( p(x)\). (ii), (iii) Samples from the model using only the first and the second components \(z_{1/2}\) while setting the other to zero, respectively. Colors correspond to \( p(x)\) once more. All densities have been normalized to \([-1,1]\) for visualization purposes. Note that compared to RNF, with CMF different latent variables could capture almost an orthogonal basis for this manifold.

losses. \(_{1}\) loss of the off-diagonal elements of the metric tensor brings together these two objectives elegantly.

Note that the injective nature of the flow, the arbitrary topology of the image space and the finite dimensionality of the chart imply that a canonical solution may not exist for the complete learned manifold. However, this is not prohibitive as the local nature of the proposed method should allow even for isolated, sparse and independent component realizations. Even such solutions can be more preferable when representing complex multidimensional image dataset manifolds. Additionally, the canonical solution is not absolutely but rather statistically enforced, i.e. it is not a strict constraint. This is similar to encouraging orthogonality and specialization of gradient attributions for network regularizations .

To this end, combining Equation (7) with the established manifold learning flow log likelihood and reconstruction loss from Section 2.1, i.e., Equation (4), we arrive at the following total optimization objective of the canonical manifold learning flow loss:

\[^{*}=_{}_{i=1}^{n} p _{}(g_{}^{-1}(x_{i}))-|_{h_{ }}^{T}(g_{}^{-1}(x_{i}))|\] (8) \[-|_{f_{}}^{T}(f_{} ^{-1}(x_{i}))_{f_{}}(f_{}^{-1}(x_{i}))|- \|x_{i}-g_{}(g_{}^{-1}(x_{i}))\|_{2}^{2} -\|G_{j k}\|_{1}^{1},\]

where \(\) is a hyperparameter. In summary, for the transformation parameters \(\), the log-likelihood of NFs is maximized, while taking into consideration the rectangular nature of the transformation. Additionally, a reconstruction loss with a regularization parameter \(\) is added to ensure that the learned manifold, \(^{d}\), is aligned to the data manifold, \(^{D}\). The \(_{1}\) loss of the off-diagonal metric tensor ensures that a canonical manifold \(\) is learned.

## 5 Experiments

We compare our method with the rectangular normalizing flow (RNF) and the original Brehmer and Cranmer manifold learning flow (MFlow). No meaningful comparison can be made to linear PCA/ICA as they are not suitable for data on non-linear manifolds. While non-linear PCA/ICA can be used, they require a specific feature extractor or kernel. Our method, a type of manifold learning, directly learns this non-linear transformation from data. We use the same architectures as , namely real NVP  without batch normalization, for the same reasons explained in the nominal paper 2. Further experimental details can be found in the appendix. In short, we use the same hyperparameters for all models to ensure fairness. For example, we use a learning rate of \(1 10^{-4}\) across the board, and we have only carried out simple initial searches for the novel parameter \(\) from Equation (8). Namely, from trying \(=[0.001,0.01,0.1,1,1,10,50]\), we concluded in using \(==1\) for the low-dimensional datasets, \(=5\) and \(=0.1\) for the tabular datasets and \(=5\) and \(=0.01\) for the image datasets to ensure stable training. \(=1\) also produced good results in our experiments with different datasets, but it was not always stable and long training was required. Overall, the method is not overly sensitive to the hyperparameter \(\). Likelihood annealing was implemented for all image training runs. Approximate training times can be found in the appendix.

### Simulated data

We consider some simulated datasets. Specifically, we implement uniform distributions over a two-dimensional sphere and a Mobius band embedded in \(^{3}\). Then we generate 1000 samples from each of these distributions and use the samples as the simulated data to fit both RNF and CMF. Such datasets have a straightforward and visualizable canonical manifold representation. The samples generated by the trained networks are shown in Figure 2(a) and Figure 2(b) for the RNF and CMF methods, respectively. From the fully sampled density plots Figure 2(a)(i) and Figure 2(b)(i), both methods learn the manifold and the distribution on it. However, the manifold learning of CMF is superior to RNF as the sphere is fully encapsulated, and the density is more uniform. Furthermore, when the three \(z_{i}\) dimensions are sampled individually in Figure 2(b) (ii),(iii) and (iv), the canonical intrinsic basis learned by CMF can be seen. Specifically, two distinct perpendicular components are obtained while the third component approaches zero, i.e. reduced to something small as it is not necessary. In contrast, from Figure 2(a) (ii),(iii) and (iv) it can be seen that in RNF all three components attempt to wrap around the sphere without apparent perpendicularity, and the second component shrinks partially. Even more striking results can be seen for the Mobius band, Figure 2(c) and Figure 2(d). Additionally, the non-trivial topology of the Mobius manifold, the twist and hole, is proving harder to learn for the RNF as compared to CMF. These findings are in direct alignment with the expected canonical manifold learning, where the network is motivated to learn an orthogonal basis and/or implement sparse learning, as explained in Section 4.2. For CMF and RNF methods, the log likelihoods are 1.6553 vs. 1.6517 and KS p-values are 0.17 vs. 0.26. On the sphere, the log likelihoods are 1.97 vs. 1.16. CMF demonstrates better quality than RNF, both quantitatively and qualitatively.

Notably, for these simulated datasets we employ full dimensional latent representations, \(D=d\). First, it allows us to visualize what all the latent dimensions are doing in relation to what we expect them to do. In particular, CMF correctly uses only 2 latent dimensions for representing 2D surfaces even though they have 3 latent dimensions. Second, it clearly shows the advantage to RNFs for the case where dimensions of the embedded manifold are known. Such scenario is equivalent to a standard NF. RNF is based on , which is an NF that already tries to solve the learning pathologies of complicated manifolds. Therefore, there is no practical limitation in choosing \(D=d\), as also seen empirically. Considering the above, the CMF method is showcased to outperform these previous methods, which also aligns with the theoretical intuition.

### Image Data

First, Mflow, RNF and CMF are also compared using image datasets MNIST, Fashion-MNIST and Omniglot \(^{784}\). As mentioned, the hyperparameters were set to \(=5\) and \(=0.01\) for all CMF runs, \(=5\) and \(=0\) for RNF and Mflow methods, and the network architectures are all equivalent. Furthermore, to emphasize the ability and importance of CMF, we train for different latent dimensions \(d\). The test FID scores of the trained models are shown in Table 1. We use \(^{d=20}\) initially as in  for consistency but also run for lower latent dimensions, \(^{d=10}\), in order to showcase the advantages of CMF relative to the established methods. Namely, a learned canonical intrinsic basis can lead to a more efficient latent space representation and therefore advantageous performance when the latent space is limited. From Table 1, CMF appears to outperform the other methods.

Then, we also experimented using the CIFAR-10 and SVHN datasets with \(^{3072}\), and CelebA with \(^{12288}\). Again we set \(=5\) and \(=0.01\), however, this time with latent dimensions, \(^{d=30}\), and \(^{d=40}\). The extra dimensions were necessary for the higher dimensional data. The table Table 1

Figure 2: Density plot for a uniform distribution on a sphere (left) and uniform distribution on a Möbius (right) band learned with RNF (top) and CMF (bottom), the black dots are the data samples, similar to Figure 1. RNF and CMF are learned via Equations 4 and Equation (8) (i) samples from the model using the full latent space where colors correspond to \( p(x)\). (ii), (iii), (iv) Samples from the model using only the first, the second and third components \(z_{1/2/3}\) while setting the others to zero, respectively. Colors correspond to \( p(x)\) once more. All densities have been normalized to \([-1,1]\) for visualization purposes.

presents the FID scores. CMF outperforms the other alternatives, consistent with the results obtained on lower dimensional imaging data sets.

In order to visualize the sparsity and orthogonalization encouraged by the CMF, the pair-wise absolute cosine similarity between the basis vectors, i.e., \(\{ x/ z^{i}\}_{i=1}^{d}\) (bottom) and the diagonal of the metric tensor (top) are visualized in Figure 3. The CMF clearly induces more sparsity as compared to the RNF, seen from the sparse diagonal elements of the metric tensors, see Figure 3(top), for both the Fashion-MNIST and Omniglot examples. Similarly, the cosine similarity for the CMF is evidently lower for both data sets, see Figure 3(bottom); the MACS stands for the mean absolute cosine similarity. Similar results were observed for the tabular datasets, see appendix.

Additionally, in order to demonstrate the more efficient use of latent space by CMF, we trained both CMF and RNF using latent-dimension \(d=40\) on both the MNIST and the Fashion-MNIST datasets. Then we choose to vary the number of prominent latent dimensions from \(^{40}\) as those with the largest \(|G_{kk}|\). We first generate samples from the models using only the prominent dimensions by sampling from the prominent dimensions and setting all others to zero. We compute FID scores for the generated samples. Later, we also reconstruct real samples using only the prominent dimensions and compute reconstruction loss using mean squared error. Here, we take the dimensions, the larger \(G_{ii}\) as the ones with the highest weights; this simple interpretation is used for analysis. We repeat the experiment for different number of prominent latent dimensions \((1,8,16,40)\) non-zero \(z_{i}\). In Figure 4(a) and Figure 4(b) the FID score and the mean-squared-error for the different number of prominent dimensions are plotted respectively. From Figure 4, it is evident that the CMF (solid lines) shows better results as compared with RNF (dotted lines) consistently, when using fewer prominent latent dimensions. This suggests that the CMF is able to use the latent space more efficiently. Furthermore, the RNF lines exhibit some oscillation whereas the CMF lines are smoother, indicating that there is also some additional inherent importance ordering in the latent dimensions as expected from sparse learning. Samples generated with different numbers of latent dimensions can be found in the appendix.

### Tabular data

The standard normalizing flow benchmarks of tabular datasets from  are used to compare the RNF, MFlow and CMF methods. We fit all models with latent dimension \(d\)=2, 4, 10 and 21 to POWER, GAS, HEMPMASS and MINIBOONE datasets, respectively. The experiments here are similar to those in , however, we keep the latent dimension the same for all the methods. Further details can be found in the appendix. We then computed FID-like scores, as was done in compared works  to compare the three models. The results are shown in Table 2. CMF appears to have lower FID-like metric scores for POWER, HEMPMASS and MINIBOONE datasets. The GAS dataset

Figure 3: Sparsity and orthogonalization encouraged by CMF. Average diagonal elements of the metric tensor (top) and average cosine similarity between the basis vectors, \(\{ x/ z^{i}\}_{i=1}^{d}\) (bottom), with \(^{d=10}\). Trained for different datasets with RNF and CMF methods. The sparse metric tensor indicates specialization, and cosine similarity closer to zero indicates orthogonalization. MACS stands for mean absolute cosine similarity. The top plots show the CMF method yielding a sparser transformation, while the bottom ones demonstrate its ability to learn more orthogonal transformations, evidenced further by the reduced MACS values.

showed mixed results in the second run set, as seen in Table 2. The performance ordering has shifted, suggesting challenges in learning this dataset, possibly leading to random outcomes. However, the other tabular datasets showed consistent performance rankings across run sets. See the appendix for further investigation into the tabular dataset training pathologies. Training on tabular datasets with doubled latent dimensions yielded no significant differences, Table 2.

## 6 Limitations

Our method optimizes the learned manifold \(^{d}\) of manifold learning flows by constraining the learning to a canonical manifold \(\) as in Definition 4.1 at no significant additional computational cost as compared to RNFs . A table of indicative training times can be found in the appendix. Nevertheless, the nature of likelihood-based training, where the full \(J^{T}J\) needs to be calculated, dictates that such a class of methods is computationally expensive for higher dimensions, despite the efficient approximate methods implemented here and in RNFs. Therefore, higher dimensional datasets can be prohibitively computationally expensive to train with this class of methods. Further computational efficiency advancements are expected to be achieved in future works. A notable fact is that the original Mflow method  relies on the fundamental approximation such that the full \(J^{T}J\) calculation can be avoided, resulting in inferior log-likelihood calculation and therefore generations, albeit it being noticeably more computationally efficient.

Additionally, CMF can only model a manifold that is a homeomorphism to \(R^{d}\), and when such mapping preserves the topology. Therefore, if, for example, there is a topological mismatch between

   Method & MNIST & FMNIST & OMNIGLOT & SVHN & CIFAR10 & CELEBA \\  ^{d=10}\)} & ^{d=30}\)} \\  MFlow & 77.2 & 663.6 & 174.9 & 102.3 & 541.2 & N/A \\ RNF & 68.8 & 545.3 & 150.1 & 95.6 & 544.0 & 9064 \\ CMF & **63.4** & **529.8** & **147.2** & **88.5** & **532.6** & **9060** \\  ^{d=20}\)} & ^{d=40}\)} \\  Mflow & 76.211 & 421.401 & 141.0 & 110.7 & 535.7 & N/A \\ RNF & 49.476 & 302.678 & 139.3 & 94.3 & 481.3 & 9028 \\ CMF & **49.023** & **297.646** & **135.6** & **72.8** & **444.6** & **8883** \\   

Table 1: Image datasets FID scores, lower is better.

Figure 4: FID test **(a)** score and mean-squared-error **(b)** for fully trained CMF (solid line) and RNF (dotted line) methods on Fashion-MNIST and MNIST with \(^{d=40}\). The \(x-\)axis corresponds to sampling from the models using a different number of latent dimensions, while setting others to zero. The chosen latent dimensions are the most prominent ones, defined as those with the largest \(|G_{kk}|\) values.

the components of the image space \(R^{D}\) described directly by \( R^{d}\), it could lead to pathological training. This drawback, however, is shared across all manifold learning flow methods. We also acknowledge the possibility that an orthogonal basis may not always be an optimal representation and difficult to learn: enforcing strict complete orthogonality can restrict the expressivity of the transformation.

Furthermore, other families of state-of-the-art generative modeling methods working in full dimensional representations, for example denoising diffusion models , can obtain superior FID scores overall as compared to the class of methods described in this work. However, by default they use a latent space as large as the image space. Notably, for any future use that entails isolating prominent components, it would be beneficial to perform a proper intrinsic dimensionality estimation.

## 7 Summary and Conclusions

In this work, we attempt to improve on the learning objective of manifold learning normalizing flows. Having observed that MLFs do not assign always an optimal latent space representation and motivated from compact latent space structuring  and sparse learning , while taking advantage of the Riemannian manifold nature of homeomorphic transformations of NFs, we propose an effective yet simple to implement enhancement to MLFs that does not introduce additional computational cost.

Specifically, our method attempts to learn a canonical manifold, as defined in Section 4.1, i.e., a compact and non-degenerate intrinsic manifold basis. Canonical manifold learning is made possible by adding to the optimization objective the minimization of the off-diagonal elements of the metric tensor defining this manifold. We document this learned basis for low-dimensional data in Figure 2 and for image data in Figure 3. Furthermore, we showcase for image data that higher quality in generated samples stems from the preferable, compact latent representation of the data, Table 1.

Distinctly, the off-diagonal manifold metric elements when minimized by an \(_{1}\) loss facilitates sparse learning and/or non-strict local orthogonality. This means that the chart's transformation can be significantly non-linear while retaining some of the benefits of orthogonal representations. This idea can be used in other optimization schemes, which mitigates many of the drawbacks of previous methods in a simple yet theoretically grounded way. Indeed, standard approaches often focus solely on the diagonal elements or strictly enforcing the off-diagonals to be zero, as in , limiting expressivity. For instance, in , an isometric embedding, essentially a predefined constrained transformation, is presented. Although it allows for direct density estimation and is a form of PCA, it is a strict constraint. Furthermore, as indicated by the synthetic data examples, our present method can be implemented in principle as a full dimensional flow, naturally assigning dimensions accordingly.

The learned manifold representations can be useful as a feature extraction procedure for a downstream task. For example, improved out-of-distribution detection could be a consequence of the feature extraction. Additionally, data that require orthogonal basis vectors, like solutions of a many-body-physics quantum Hamiltonian, can show improved learning performance with the current method.

Manifold learning flows are noteworthy due to their mathematical soundness, particularly in the case of methods such as RNF and CMF, which do not implement any fundamental approximations in the density calculations using lower dimensional latent space.We anticipate that the method will stimulate further research into the nature and importance of the latent space representation of MLFs, enhancing our understanding of generative modelling and paving the way to general unsupervised learning.

   Method & power & gas (1st run) & gas (2nd run) & Hempass & miniboone \\  Mflow & 0.258 \(\) 0.045 & **0.219 \(\) 0.016** & 0.433 \(\) 0.071 & 0.741 \(\) 0.052 & 1.650 \(\) 0.105 \\ RNF & 0.074 \(\) 0.012 & 0.283 \(\) 0.031 & 0.470 \(\) 0.101 & 0.628 \(\) 0.046 & 1.622 \(\) 0.121 \\ CMF & **0.053 \(\) 0.005** & 0.305 \(\) 0.059 & **0.373 \(\) 0.065** & **0.574 \(\) 0.044** & **1.508 \(\) 0.082** \\   \\  RNF & 0.432 \(\) 0.022 & 0.386 \(\) 0.044 & 0.608 \(\) 0.023 & \\ CMF & **0.205 \(\) 0.098** & **0.367 \(\) 0.007** & **0.519 \(\) 0.063** & \\   

Table 2: Tabular data FID-like metric scores, lower is better.