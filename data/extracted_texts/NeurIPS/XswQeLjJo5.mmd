# Unraveling the Gradient Descent Dynamics of Transformers

Bingqing Song

University of Minnesota, Twin Cities

song0409@umn.edu

&Boran Han

Amazon Web Services

boranhan@amazon.com

Shuai Zhang

Amazon Web Services

shuaizs@amazon.com

&Jie Ding

University of Minnesota, Twin Cities

dingj@umn.edu

&Mingyi Hong

University of Minnesota, Twin Cities

mhong@umn.edu

The work of B. Song was partially done while interning at Amazon Web Services.

###### Abstract

While the Transformer architecture has achieved remarkable success across various domains, a thorough theoretical foundation explaining its optimization dynamics is yet to be fully developed. In this study, we aim to bridge this understanding gap by answering the following two core questions: (1) Which types of Transformer architectures allow Gradient Descent (GD) to achieve guaranteed convergence? and (2) Under what initial conditions and architectural specifics does the Transformer achieve rapid convergence during training? By analyzing the loss landscape of a single Transformer layer using Softmax and Gaussian attention kernels, our work provides concrete answers to these questions. Our findings demonstrate that, with appropriate weight initialization, GD can train a Transformer model (with either kernel type) to achieve a global optimal solution, especially when the input embedding dimension is large. Nonetheless, certain scenarios highlight potential pitfalls: training a Transformer using the Softmax attention kernel may sometimes lead to suboptimal local solutions. In contrast, the Gaussian attention kernel exhibits a much favorable behavior. Our empirical study further validate the theoretical findings.

## 1 Introduction

Transformer model architectures have become popular in machine learning, delivering remarkable performance across a wide array of tasks. From natural language processing (Vaswani et al., 2017; Beltagy et al., 2020) to computer vision (Dosovitskiy et al., 2020), these models have set new standards in performance and efficiency. Popular models include BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2020), GPT models (Radford et al., 2019; Brown et al., 2020) and ViT (Dosovitskiy et al., 2020). Despite their empirical success, a comprehensive understanding of their optimization process remains elusive. As highlighted in Liu et al. (2020), the training of large Transformers can sometimes result in deteriorated performance. It is therefore critical to develop theoretical insights for researchers and practitioners to better understand the practical performance of Transformers. However, the complexity of their architectures, coupled with the non-convex natureof the associated optimization problems, has made the theoretical analysis of these models very challenging.

The optimization landscape can be pivotal for understanding a certain type of neural network and providing the practical guidance (Liu et al., 2020). Existing literature offers numerous studies on achieving zero-loss solutions in networks with ReLU activation. These studies encompass various network structures, including fully-connected, convolutional, and residual networks, as explored in (Jain et al., 2017), (Jin et al., 2021), and (Danilova et al., 2022). They delve into the analysis of network optimization landscapes and provide assurances of rapid global convergence when using gradient descent (GD) or stochastic gradient descent (SGD) algorithms. For instance, in Du et al. (2019), the authors focus on fully-connected networks and ResNets with smooth activation functions, and they have demonstrated that global convergence can be achieved using GD with a network size proportional to \((N)\), where \(N\) is the sample size. Similarly, (Allen-Zhu et al., 2019) show that ReLU fully-connected networks with at least \((N)\) neurons can achieve global convergence using GD or SGD. From a statistical perspective, (Li et al., 2023) have shown that for two-layer ReLU neural networks (with input dimension \(p\)) that admit a sparse subnetwork representation, a sample size of \(O(^{4}(p/))\) can guarantee the global convergence with probability at least \(\) using GD. Despite this extensive body of work on traditional architectures, it is not clear what conditions we need (e.g. network size, optimizer, initialization) to ensure training Transformer models to find high-quality solutions.

Compared to traditional deep learning architectures, Transformers incorporate a unique level of intricacy through their attention kernel (Vaswani et al., 2017), which is designed to effectively handle sequence inputs. This mechanism incorporates Softmax activation to the inner products of query and key vectors, and this inherently non-convex operation poses considerable challenges to theoretical analysis. Consequently, existing frameworks for analyzing the convergence of classical deep learning models are not directly applicable to Transformers. Further, many recent works have pointed out that the performance of Transformers depends on a number of factors such as the choice of kernel function, initialization, choice of optimizers, and forms of token embeddings (Huang et al., 2020; Pan and Li, 2023; Shazeer, 2020; Li et al., 2018; Tian et al., 2023). In deep learning, these factors have been studied in a line works. For example, Li et al. (2018) show that the good training performance is not universal ; skip connections have the effect of smoothing the training landscape, and the Adam algorithm tends to follow a more direct trajectory towards optimal solutions compared to SGD. Therefore, it is imperative to understand what kind of conditions, including initialization, network structure, data properties, and optimizer choices, will lead to high-performing Transformers.

In this work, we will delve into the intricacies of attention kernels, discussing both their advantages and limitations in the context of model optimization. The main contributions of this work are threefold.

* We derive the conditions that will make the one-layer Softmax attention Transformer reach global optimality with vanilla gradient descent. The convergence guarantee is largely attributed to the linear layer (\(W^{}\)) in the attention mechanism.
* We investigate the attention kernel's effectiveness, revealing Gaussian attention achieves zero training loss, while Softmax can lead to non-optimal stationary points.
* Our experiments validate that Softmax attention Transformers converge slower and present more challenging training landscapes than Gaussian counterparts, potentially leading to more local optimal solutions.

## 2 Related Work

A number of research works have focused on the theoretical analysis and interpretation of Transformer models, revealing crucial insights into their practical performance.

Liu et al. (2020) showed that heavy reliance on the residual branch in multi-layer Transformer models can lead to training instability, which amplifies small parameter perturbations, causing significant disturbances in the model's output. In Bhojanapalli et al. (2020), the authors illustrated the existence of a low-rank bottleneck in Transformer models with sufficiently large embedding and hidden size (\(D=d\)). However, this work focuses on the representation ability of large size attention, while falling short of analyzing Transformer models from an optimization perspective. In Noci et al. (2022),the authors explored rank collapse issues in token representations and their impact on training. The authors discussed the origin of the phenomenon of rank collapse and proposed depth-dependent scaling of residual branches as a potential solution. They specifically investigated scenarios where token rank equals one, which can hinder Transformer training. Their findings demonstrate the occurrence of the vanishing gradient issue, however, this work does not comprehensively characterize the vanishing gradient problem throughout the entire training process.

A recent work Wu et al. (2024) analyzes the convergence behavior of shallow Transformer, which builds a convergence theory of shallow Transformer with realistic structure and initialization, but they do not provide roles for different matrices in the convergence. However, the focus of our paper is different from Wu et al. (2024). We not only derive the global convergence analysis (Our Theorem 2), but also investigates the role of different variables in optimization.

Some other works focus on improving the optimization of Transformers empirically. Huang et al. (2020) have proposed an initialization strategy such that no warm-up or layer normalization is needed to train Transformers efficiently; in Shazeer (2020), the GLU variant of token embedding has been showed to be better than plain embedding in the optimization of Transformer models with Softmax attention kernel. It is worth noting that the above works all primarily focus on empirical investigations into the training of Transformer models, lacking a comprehensive theoretical analysis of the underlying mechanisms.

Some recent research has focused on the convergence analysis of Transformer-based models within the in-context learning (ICL) framework. For instance, Huang et al. (2023); Zhang et al. (2023) explores the learning dynamics of a one-layer Transformer with Softmax attention trained via gradient descent to learn linear function classes in-context. However, this line of study primarily addresses the general convergence performance of Transformers within the ICL setting and does not delve into the role of individual variables. More specifically, these works analyze the convergence of in-context training, where a prompt is constructed with all the training samples and a single test sample. The goal of these works is to achieve the zero test loss (in expectation) by optimizing over the loss function modeled by the prompt. On the other hand, our analysis is based on standard empirical loss minimization, which does not involve any prompt construction.

## 3 Notations and Problem Description

In this section, we define the structure of the Transformer model and describe the training problem. We consider a one-layer attention Transformer model with multiple heads and a dataset with \(N\) samples. Each data sample consists of \(n\) discrete tokens, each with embedding dimension \(D\). We denote the dataset as \(\{(X_{i},y_{i})\}_{i=1}^{N}\), where \(X_{i}^{n D}\), and \(y_{i}^{n}\) is the label of the dataset. The output from the Transformer model is the prediction of the label. The Transformer structure is formulated as follows:

\[(W_{h}^{Q},W_{h}^{K},W_{h}^{V};X_{i}):=S(W_{h}^{ Q},W_{h}^{K};X_{i}W_{h}^{V}\] (1) \[(W^{Q},W^{K},W^{V};X_{i}):=(_{1},,_{H}) W^{O},\] \[_{h}:=(W_{h}^{Q},W_{h}^ {K},W_{h}^{V};X_{i}),h=1,,H.\] (2)

In the above notation, \(W_{h}^{Q},W_{h}^{K}^{D d}\) is the query weight matrix and key weight matrix, respectively; \(W_{h}^{V}^{D d}\) is the value weight matrix; these matrices are the main optimization variables throughout the paper. Further \(W^{O}^{Hd 1}\) is a fixed matrix, representing the weight of the output layer; \(H\) is the number of attention heads; \(S()\) is a kernel function of variables \(W^{Q},W^{K}\) and input \(X_{i}\). \(()\) is the attention head function; \(()\) represents the multi-head attention function. For example, with the Softmax attention Vaswani et al. (2017), \(S()\) can be written as:

\[S(W_{h}^{Q},W_{h}^{K};X_{i}):=( W_{h}^{Q}(X_{i}W_{h}^{K})^{}}{})\] (3)

where for a given \(n n\) matrix \(Z\), \((Z):=[(Z_{1}),,(Z_{n})]\). Throughout, let us denote \(S()_{kj}\) as the element of \(k\)-th row and \(j\)-th column in matrix \(S()\). Let \(X_{ik}\). \(^{D}\) denote the embedding of the \(k\)-th token in data \(X_{i}\), which is the \(k\)-th row of matrix \(X_{i}\). The structure of Transformer model can be found in Fig 1, where we denote \(S_{ih}:=S(W_{h}^{Q},W_{h}^{K};X_{i})\).

Based on the above Transformer model, we consider minimizing the following empirical \(_{2}\) loss function for the entire data set \(\{X_{i},y_{i}\}_{i=1}^{N}\):

\[_{M}_{i=1}^{N}\|(M;X_{i})-y_{i}\|^{2},\] (4)

where \(M:=(W^{Q},W^{K},W^{V})\) is the set of variables that can be optimized.

For notation simplicity, next we define the vector version of the Transformer model given in Equation (1), for the entire dataset \(\{(X_{i},y_{i})\}_{i=1}^{N}\). Towards this end, let \(X^{Nn D}\) denote the column-stacked matrix of each single data \(X_{i}\). Similarly, define the stacked label \(y^{Nn}\). Then we can define:

\[(M;X):=S_{11}X_{1}&&S_{1H}X_{1}\\ &&\\ S_{N1}X_{N}&&S_{NH}X_{N}(W_{1}^{V}, ,W_{H}^{V}) W^{O},\] (5)

\(i=1,2,,N,h=1,2,,H\) for simplicity.

Thus the empirical loss function given in Equation (4) can be simplified as

\[_{M}\|(M;X)-y\|^{2}.\] (6)

For more notations in the following sections, we will use subscript \(t\) to represent the variables in \(t\)-th iteration, e.g, \(M_{t}:=\{W_{t}^{Q},W_{t}^{K},W_{t}^{V}\}\). Similarly, we denote \(B_{t}\) as the matrix \(B\) at \(t\)-th iteration.

It is important to note that, in the above description and throughout the paper, we model the Transformer training problem by using a single-layer Transformer, with a regression loss. In practice Transformer models can exhibit greater complexity (different loss functions, multiple layers, etc). For example, the text classification task has an additional mean pooling layer followed by the output of the Transformer structure. Further, they usually contain downstream MLP modules. However, we choose to use the simplified version due to the following reasons:

First, the primary objective of this work is to understand how different attention kernels affect the training dynamics of the Transformers, so we do not include the layer normalization in our model. In fact, in the literature, many works that analyze popular network structures also do not consider layer normalization. For example, in (Huang et al., 2023; Zhang et al., 2023), both analyze the convergence performance of Transformers but normalization is not considered.

Second, we do not include the downstream MLP module in our work since we are interested in the role of self-attention layer in convergence analysis, and the single-attention model is also the standard model used in (Huang et al., 2023; Zhang et al., 2023). Further, the analysis of MLP is standard in literature (Allen-Zhu et al., 2019; Du et al., 2019; Nguyen and Mondelli, 2020). And it is worth noting that our choice to focus on a one-layer Transformer is consistent with other works that similarly aim to investigate the core training dynamics of Transformers, e.g, in (Tian et al., 2023), a single-layer Transformer is considered as a basic model.

Figure 1: One head in Transformer architecture with Soft-max Attention.

Convergence Analysis

In this section, we present our theoretical analysis for solving problem (6). We focus on the behavior of the vanilla GD algorithm for optimizing the variable set \(M\), where \(M\{W^{Q},W^{K},W^{V}\}\). Below we summarize our results.

**Common convergence conditions with Softmax Attention**: When the activation function \(S()\) is either the Softmax or Gaussian function, and the embedding dimension \(D\) is at least \((Nn)\), optimizing Equation (6) can achieve a global optimal solution when \(M=\{W^{V}\}\) and \(M=\{W^{Q},W^{K},W^{V}\}\).

**Different behavior between Softmax and Gaussian Kernel Attention**. When \(S()\) is Gaussian and the embedding dimension \(D\) is at least \((Nn)\), convergence to global optimal is also ensured for \(M=\{W^{Q}\}\). Interestingly, under the same conditions of large \(D\), convergence to global optimal is _not_ guaranteed when \(S()\) is Softmax.

In the subsequent sections, we will elaborate on these convergence results in detail, providing a deeper understanding of the nuances in Transformer behavior under varying configurations. To set up our analysis, we introduce \(^{V}\) as the smallest eigenvalue of \(W^{V}_{0}\), \(^{B}\) as the smallest eigenvalue of \(B_{0}\), \(^{Q}_{h},^{K}_{h},^{V}\) as the largest singular value of matrix \(W^{Q}_{h,0},W^{K}_{h,0},W^{V}\), respectively. We denote \(\|\|_{2}\) as \(_{2}\) norm and \(\|\|_{F}\) as Frobenius norm. Further, we denote \(_{}()\) and \(_{}()\) as the largest and smallest singular value of a matrix, respectively. For any vector \(v\), let \((|v|)\) denote the smallest absolute value of vector \(v\).

### Convergence to global optimal

First, we examine the role of \(W^{V}\) in the optimization of multi-head attention network structure. Our analysis demonstrates that with the hidden dimension \(HD Nn\) and proper initialization, the global optimal solution of (6) can be found using a vanilla gradient descent algorithm. The initialization requires that the matrix \(B_{0}\) has full rank. Our first result shows that, overparameterized Transformer can be trained to global optimal solution.

**Theorem 1**.: _Consider problem (4) with \(S()\) being instantiated as the Softmax kernel given in (3). Consider the following update for the variable \(M=\{W^{V}\}\): \(W^{V}_{t+1}=W^{V}_{t}-_{W^{V}}f(M_{t};X)\), where \(>0\) is the stepsize._

_Suppose \(W^{Q}_{0}\) and \(W^{K}_{0}\) are initialized such that \(^{B}>0\). Then we have:_

\[f(M_{t};X)(1-)^{t}f(M_{0};X),\] (7)

_where \(:=\|W^{O}\|^{2}(^{B})^{2}>0\); \(>0\) is defined in Appendix 1.3, and chosen such as \(<1\)._

**Remark 1**.: _The aforementioned theorem focuses on the convergence behavior when only \(W^{V}\) is being updated. We further elaborate on the initial conditions ensuring \(^{B}>0\)._

_Note that \(^{B}>0\) implies that the objective function \(f\) exhibits a landscape that is nearly convex, which is crucial for optimization. By definition, this condition implies that \(B_{0}\) has full rank, which can be fulfilled by selecting appropriate \(W^{Q}_{0}\) and \(W^{K}_{0}\), plus having large enough embedding size, satisfying \(D Nn/H\). We refer the readers to Appendix 1.3 for the derivation of this condition, which can be guaranteed by random initialization with high probability._

Furthermore, it is important to note that our work aligns with existing literature on the subject of embedding size in Transformer models. For example, in , the authors restrict their focus to the simplified case of \(N=1,H=1\). They establish the necessary condition for Softmax attention to overcome its low-rank bottleneck, which requires \(D n\). In our analysis, we derive a similar necessary condition on Transformer model size (\(D n(N/H)\)) to guarantee the global convergence when a Transformer model is trained with GD.

In Theorem 1, we have illustrated the case where only updating \(W^{V}\) already leads to global convergence. However, in practice, all parameters \(W^{V},W^{Q},W^{K}\) are updated. This case is more challenging to analyze due to the non-linearity introduced by the Softmax function. Next, we show that a similar result in Theorem 1 still holds when all the parameters are updated simultaneously.

**Theorem 2**.: _Consider problem (4), with \(S()\) being instantiated as the Softmax kernel. Consider the GD update where \(M=\{W^{Q},W^{K},W^{V}\}\): Suppose \(^{B}>0\), and the initialization \(M_{0}\) satisfy_

\[\|X\|_{F}^{5}_{h=1}^{H}((_{h}^{ Q})^{2}+(_{h}^{K})^{2})^{V}}{\|W^{O}\|_{2}( ^{B})^{2}(_{h}^{Q},_{h}^ {K},^{B})}\|(M_{0};X)-y\|_{2}.\] (8)

_Then there exists stepsize \(>0\), such that_

\[f(M_{t};X)(1-)^{t}f(M_{0};X),\] (9)

_where \(:=\|W^{O}\|^{2}(^{B})^{2}>0\), and the constants \(,\) are defined in Appendix 1.3._

**Remark 2**.: _In the stated theorem, we simplify our analysis by excluding the downstream MLP module in the typical Transformer model, since it is easy to combine the model in Equation (2) with downstream MLP layers. Further, it can be directly showed that the Transformer with MLP will lead to the **same** convergence rate of the optimization problem as updating \(W^{Q},W^{K},W^{V}\) only. To illustrate this, consider the following Transformer model:_

\[G(W^{Q},W^{K},W^{V};X_{i})=(W^{Q},W^{K},W^{V};X_{i})  W^{1}W^{2} W^{L},\] (10)

_where \(W^{l}^{n_{l-1} n_{l}}\), and \(n_{0}=d^{O}\). Based on the Transformer model defined in Equation (10), we have the following corollary._

**Corollary 1**.: _Consider problem \(_{M}\|G(M;X)-y\|^{2}\), with \(G()\) being defined in Equation (10) and \(S()\) being instantiated as the Softmax kernel. Suppose that the MLP module satisfies:_

\[n_{1} n_{2} n_{L}.\]

_Consider the following GD update (where \(M=\{W^{Q},W^{K},W^{V},W^{1},,W^{L}\}\)): Suppose \(^{B}>0\). Then, there exists a step size \(>0\) and initialization weight \(M_{0}\), such that the loss function linearly converges to \(0\)._

**Remark 3**.: _The above theorem and corollary describe the global convergence guarantee when \(W^{Q},W^{K}\) and \(W^{V}\) are updated. This is in line with the insights gained from Theorem 1. However, the conditions for initialization are more stringent, and the optimization landscape becomes inherently more complex due to the involvement of the Softmax attention through \(W^{Q}\) and \(W^{K}\)._

_To ensure the initial condition 8, we have two options: 1) Initializing \(M_{0}\) such that \(\|(M_{0};X)-y\|_{F}\) is small, which implies that the optimization starts in a region close to the global optimal solution and that the initial weight is close to the global optimal solution; 2) Balancing between \(W^{O}\) and \(W^{V}\), in the sense that \(\|W^{O}\|_{2}\) is large and \(^{V}\) is small. For a detailed account of these initialization strategies, please refer to Appendix 1.3._

_Finally, we need to point out that for Transformers with Gaussian kernel attention, we can derive similar convergence results as long as the attention kernel maintains full rank and weights are initialized appropriately. Here we do not include the theoretical statement since it is similar to the result for Softmax attention._

### Softmax vs Gaussian kernel: Softmax attention Transformers may exhibit slower convergence.

In the previous section, we explored the global convergence of training Transformer models. However, from Theorem 2, it was not clear what roles do matrices \(W^{Q}\) and \(W^{K}\) play in the entire convergence process, since Theorem 1 indicates that optimizing \(W^{V}\) alone already ensures the desired convergence. Nevertheless, it is the matrices \(W^{K}\) and \(W^{Q}\) that truly represent the power of a Transformer model, because they are used to extract token correlations.

To study how well a Transformer model can extract the token correlation, in this section, we will study the GD dynamics for Transformer models, where only \(W^{K}\) and \(W^{Q}\) are optimized (while fixing \(W^{V}\)). If optimizing these two parameters alone can still achieve zero training loss, then we claim that the input token correlation can be optimally extracted by the Transformer model.

#### 4.2.1 Notations

To begin our study, let us define that Gaussian kernel to be an \(n n\) matrix, where its \(k\)-th row and \(j\)-th column of is given by:

\[S(W_{h}^{Q},W_{h}^{K};X_{i})_{kj}=(-}( X_{ik}.W_{h}^{Q}-X_{ij}.W_{h}^{K})^{2})\] (11)

Since the training dynamics/gradients of variables \(W^{Q}\) and \(W^{K}\) have the same property in (3) and (11), we will only concentrate on optimizing \(W^{Q}\).

With some abuse of notation, define a matrix \(C\) for Softmax attention and Gaussian kernel attention, respectively. Softmax attention: \(C_{ih}:=W_{h}^{Q}(X_{i}W_{h}^{K})^{}}{} ^{n n}\).

Gaussian kernel attention: \(C_{ih}^{n n};\;(C_{ih})_{kj}=-.W _{h}^{Q}-X_{ij}.W_{h}^{K}\|^{2}}{2}\).

For both Softmax attention and Gaussian kernel attention:

\[C_{i}^{n Hn}=[C_{i1},C_{i2},,C_{iH}];\;C ^{Nn Hn}=[C_{1}^{},C_{2}^{},,C_{N}^{ }]^{}.\]

Using the above notation, the activation function \(S()\) in (3) and (11) can be related to the matrices \(C\)'s in the following manner:

Softmax attention \(:S_{ih}=(C_{ih}),\;:(S_{ih})_{kj}=(C_{ih})_{kj}\).

Additionally, note that \(C\) is a function of variables \(M\). Therefore we will sometimes use \(C(M)\) when we need to emphasize the dependency of \(C\) on \(M\).

#### 4.2.2 Main Results

Next, we will outline the conditions under which GD can still successfully find global optimal solutions for Transformers with Gaussian kernel attention (when only \(W^{Q}\) is updated), while under the same set of conditions, but with Softmax kernel attention, GD fails.

**Theorem 3**.: _Solve problem (4) with the following GD update (with \(M=\{W^{Q}\}\)): \(W_{t+1}^{Q}=W_{t}^{Q}-_{W^{Q}}f(M_{t};X)\). Suppose \(_{h}:=_{}()}{ W_{h}^{Q}})>0,\; \;h[1,2,,H]\), and the initialization condition further satisfies_

\[^{5}(_{h}^{Q}+_{h}^{K}) (\|X\|_{F}^{2}(_{h}^{Q})^{2}+(_{h}^{K})^{2})}{(|V^{}W^{O}|)^{2} (_{h},_{h}^{Q})}^{V}\|W^{O}\|_ {2}\|(M_{0};X)-y\|_{2}^{},\] (12)

\(^{}\) _is defined in Appendix_ 1.5_._

_(1) When_ \(S()\) _is a Gaussian kernel function, there exists a stepsize_ \(\) _and a positive constant_ \(\)_, such that_

\[f(M_{t};X)(1-)^{t}f(M_{0};X),\] (13)

_where_ \(,\) _are defined in Appendix_ 1.5_._

_(2) When_ \(S()\) _is a Softmax function, suppose_ \(W_{t}^{Q}\) _is bounded during the training phase, then there exists stepsize_ \(\)_, such that_

\[f(M_{t};X) f(M_{0};X)-^{}_{r=0}^{t-1} \|_{W^{Q}}f(M_{r};X)\|^{2},\] (14)

_where_ \(^{}\) _is defined in Appendix_ 1.5_._

**Remark 4**.: _First, it's important to note that the parameter size must satisfy \(Dd Nn^{2}\) for \(>0\) to hold. It is crucial to emphasize the fundamental distinction in convergence outcomes between Transformers employing Gaussian kernel attention and those utilizing Softmax attention under these conditions. With equivalent initialization conditions, training Transformers equipped with Gaussian kernel attention achieves global convergence using gradient descent (GD). Second, it is essential to emphasize that the dimension size \(Dd Nn^{2}\) is similar to the findings of works that have analyzed the convergence performance of over-parameterized neural networks Allen-Zhu et al. (2019); Du et al. (2019). The total number of samples, consisting of \(N\) samples each with \(n\) tokens, can be calculated as \(Nn\). Meanwhile, the total feature dimension is \(Dd\). The inequality implies that the width of theparameters is at least \((N)\), a relationship also illustrated in Nguyen and Mondelli (2020). The proof consists of two basic steps. The first step is to derive the closed form gradient of loss function over variable \(W^{Q}\). Intuitively, the gradient of Softmax attention is much more complicated than the Gaussian attention Transformer, which will lead to a more complicated landscape and more local solutions. The second step is to analyze the gradient of Transformers with both kernels and the same initialization Equation (12). For Gaussian attention Transformer, it can be iteratively shown during the gradient descent training: 1) The variable \(W^{Q}\) is bounded; 2) The PL condition holds (i.e, the optimization landscape remains near-convex); 3) The loss function decreases linearly. For the Softmax attention Transformer, there is no guarantee that the PL condition holds during iterative gradient descent update._

In part (2), we demonstrate that the PL condition does not hold. In particular, we identify an initial solution that satisfies all the conditions given in Theorem 3, yet fails to satisfy the PL condition. Therefore, in this case, GD leads to vanishing gradients without being able to find a global optimal solution. The details of this specific example are provided below.

**Example:** Consider Transformer with Softmax attention, and \(N=1,n=2,H=1\). Let us first write down the close form of the gradient over \(W_{1}^{Q}\):

\[;X_{1})}{ W_{1}^{Q}}=}X_{1}^{};X_{1})}{ C_{1 1}}X_{1}W_{1,0}^{K}\]

Next, we show there exists \(W^{O},W^{V},X_{1},W_{1,0}^{Q},W_{1,0}^{K}\) such that the loss function is non-zero with Equation (12) satisfied, while

\[;X_{1})}{ C_{11}}= ^{2 2}.\]

Denote \(L:=;X_{1})}{(M_{0}; X_{1})}(W^{O})^{}(X_{1}W_{0}^{V})^{} ^{2 2}\), \(;X_{1})}{ C_{11}}\) can be expressed as follows:/

\[(;X_{1})}{ C_{11}})_{11}= (L_{11}-L_{12}),\;(;X_{ 1})}{ C_{11}})_{12}=(L_{12}-L_{11}),\]

Next, we will give the value of \(W^{O},W_{0}^{V}\) to show the case where GD leads to vanishing gradient. Let \(D=d=2\), \(W^{O}=(,),X_{1}=1&0\\ 0&1\), and \(W_{0}^{V}=2a&a\\ a&2a\), where \(a\) is a constant. It is easy to show that there exists \(W_{1}^{Q}\) and \(W_{1}^{K}\) such that Equation (12) holds. Further, it is easy to verify that for this scenario, the following holds:

\[L_{11}=L_{12},L_{21}=L_{22}.\] (15)

Next, we can easily deduce that \((;X_{1})}{ C_{11}})_{11}= (;X_{1})}{ C_{11}})_{12}=0\). Similarly, we can demonstrate that \((;X_{1})}{ C_{11}})_{21}= (;X_{1})}{ C_{11}})_{22}=0\). Consequently, we have \(;X_{1})}{ W_{1}^{Q}}=\). However, if \(y_{1}\) satisfies that \(;X_{1})}{(M_{0};X_{ 1})}\), it follows \(f(M_{0};X_{1}) 0\), which means \(M_{0}\) is not global optimal solution.

## 5 Experiment: Softmax v.s. Gaussian

In this section, we present numerical results to illustrate the behaviors of Transformers models with Softmax attention and Gaussian kernel attention across various tasks.

### Dataset

We investigate two distinct tasks: Text Classification using the IMDb review dataset (Maas et al., 2011) and Pathfinder (Linsley et al., 2018). While both tasks involve processing long sequences, they exhibit different characteristics. Text Classification is a well-known NLP task that focuses on discerning relationships among token embeddings, while the Pathfinder task prioritizes capturing spatial information within the input pixels.

Figure 2: Test performance on text classification task with different attention kernels

### Model and Experiment Method

We follow the experiment setting in [Chen et al., 2021]. For both tasks, we employ a 2-layer Transformer model with the following specifications: embedding dimension \(D=64\), hidden dimension \(d=128\), and number of attention heads \(H=2\). To align the model with the classification task, we use an additional mean pooling layer as the final layer. We determine the batch size based on available memory constraints. Specifically, we set a batch size of 16 for the Text Classification task with a learning rate of \(1 10^{-4}\), and a batch size of 128 for the Pathfinder task with a learning rate of \(2 10^{-4}\). For optimization, we use Stochastic Gradient Descent (SGD) for the Text Classification task and Adam for the Pathfinder task. We conduct two types of experiments.

In the first experiment, we plot the test accuracy and test loss within the training steps with both Softmax and Gaussian kernel attention on both tasks. We repeat the training for \(10\) times and make the shadow plot on the test performance.

In our second experiment, the training process consists of two stages: In the first stage, we train the Transformer model equipped with Softmax attention (defined in Equation (3)) for 8,000 steps. In the second stage, we continue training from the pre-trained model for an additional 500 steps, with the option of using either Softmax or Gaussian kernel. To explore the optimization landscape around the trained model, we employed a technique inspired by Li et al. (2018). We select two parameter directions, specifically the \(W^{Q}\) and \(W^{K}\) matrices in the first Transformer layer. These two directions, denoted as \(d_{1},d_{2}\), are centered at the trained model \(M\), and represent the parameter space of \(W^{Q},W^{K}\), respectively. We evaluate the loss function on the set \(\{M+0.02(r-25)d_{1}+0.02(s-25)d_{2}\}\), where \(r,s[1,2,,50]\). The above set is the neighborhood of the trained model \(M\), and we chose the evaluation stepsize as \(0.02\) along the two directions \(d_{1},d_{2}\), with the total steps limit as \(100\). Within this parameter space, we plot a 3-D surface representing the landscape around the trained model.

### Results

#### 5.3.1 Test Loss & Accuracy Curve comparison

To begin with, we present some observations in our first experiment. We plot the test performance of these two tasks on Transformers with two different types of attention. From Fig 2 and Fig 3, we can conclude that in both tasks, Transformers with Gaussian kernel attention exhibit faster convergence and higher test accuracy than Softmax attention with the same model size and learning rate. Especially,

Figure 4: The loss landscapes on text classification task and Pathfinder task. For both tasks, we use the two-stage training in Section 5.2 with the same training hyperparameters, while the only difference is the attention structure in the second training stage. The two axes represent the two directions \(d_{1}\) and \(d_{2}\) as defined in Section 5.2.

Figure 3: Test performance on pathfinder task with different attention kernelstraining Transformers with Softmax attention in the Pathfinder task can lead to unstable performance as indicated in Fig 3. The test accuracy has a significantly higher variance at the same training epoch. Further, the worst test accuracy after \(20,000\) epochs is around \(0.58\) for the Softmax attention Transformer, compared with \(0.62\) for the Gaussian kernel Transformer. These observations align with the experiment results in (Chen et al., 2021) and (Tay et al., 2020), where Transformers with different attention kernels are trained with the same model size and learning rate, while Softmax attention Transformers show instability in a few tasks.

#### 5.3.2 Optimization Landscape Comparison

In Figure 4, we present a comparison of the optimization landscape between Transformers with Softmax and Gaussian kernel attention. Notably, we observe distinct differences in the training landscapes of these two attention types for both tasks. We follow the visualization method described in Section 5.2. We conduct a visualization of the optimization landscape around the trained models after a two-stage training process, with identical learning rates, network sizes, and training epochs. Keeping all other factors consistent, the disparity in the landscape provides a direct representation of the difference in the attention structure during the optimization procedure. With Softmax attention, the landscape appears more complicated compared with Gaussian kernel attention. This complexity can be interpreted as the presence of a greater number of local optima in the optimization landscape, suggesting that Transformers utilizing Softmax attention may encounter more challenges in reaching global optimal solutions. In contrast, the landscape with the Gaussian kernel is flatter. This observation aligns with our earlier findings in Figure 2 and Figure 3, where Softmax attention exhibited certain convergence issues. These observations also provide empirical evidence supporting our Theorem 3, which reflects in a slightly different perspective the complicated optimization landscape within the Softmax kernel.

## 6 Conclusion and Future Work

In conclusion, our study addresses critical gaps in our understanding of why Transformer models perform exceptionally well in a variety of machine learning tasks. Our work also provides a nuanced understanding of the advantages and disadvantages of using classical Softmax attention in Transformers. We find that while shallow Softmax attention Transformers can achieve global convergence with overparameterization, there are scenarios where this attention structure can lead to local solutions. However, those issues can be mitigated by the Gaussian kernel-based attention. In our work, we need strong initialization and large embedding size, i.e, \(HD Nn\) to obtain the global convergence, which exhibits a gap towards real case. In the future work, we will investigate how to relax the assumptions.

## 7 Acknowledgment

The work of B. Song was partially done while interning at Amazon Web Services. M. Hong holds concurrent appointments as an Amazon Scholar and as a faculty at the University of Minnesota. This paper describes their work performed at Amazon. The work of Jie Ding was supported in part by the Army Research Office Early Career Program Award under grant number W911NF2310315.