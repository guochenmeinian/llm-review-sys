# Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning

Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman

Stanford University

###### Abstract

What role do augmentations play in contrastive learning? Recent work suggests that good augmentations are _label-preserving_ with respect to a specific downstream task. We complicate this picture by showing that label-destroying augmentations can be useful in the foundation model setting, where the goal is to learn diverse, general-purpose representations for _multiple_ downstream tasks. We perform contrastive learning experiments on a range of image and audio datasets with multiple downstream tasks (e.g. synthetic datasets combining two classes, such as images and digits, and naturalistic datasets labeled with dozens of attributes). In controlled experiments where we destroy features at different rates, we find that destroying one feature a modest fraction of the time can improve learning of other features, while still enabling the dropped out feature to be learned well. Additionally, we show how this hypothesis can help explain the success of Viewmaker Networks, which generate augmentations that appear to target and destroy different features in the input examples, yet often result in better performance than standard augmentations across tasks. To support our empirical results, we theoretically analyze a simple contrastive learning setting with a linear model. In this setting, we show that label-destroying augmentations are crucial for preventing one set of features from suppressing the learning of features useful for another downstream task. Our results highlight the need for analyzing the interaction between _multiple_ downstream tasks when trying to explain the success of foundation models.

## 1 Introduction

In recent years, foundation models  have exhibited remarkable progress on a range of AI tasks [13; 31; 37; 36; 6; 11; 25; 1; 38]. A crucial characteristic of foundation models is that they can be adapted for a range of downstream tasks. For example, a foundation model trained on ImageNet should ideally not only perform well at object classification, but should also have learned general features useful for localization, segmentation, and other visual tasks. Indeed, this is borne out by recent work showing the high accuracy of foundation models on a range of downstream tasks , as well as a range of analysis work showing that models learn high-level semantic features including texture, color, pose, and style .

One popular strategy for training foundation models involves training models to match transformed versions (known as _views_ or _augmentations_) of the same input. For example, image views might include common data augmentations such as cropping or color jitter , while views for speech might include pitch modulation or spectrogram masking [27; 35]. This family of objectives includes contrastive approaches such as SimCLR and MoCo, as well as non-contrastive approaches such as BYOL and SwAV [9; 23; 20; 7].

Given the central importance of these views for defining the self-supervised task, much work has focused on the question of what views lead to high-quality representations. The prevailing consensus, exemplified by Tian et al. , holds that views should be _label-preserving_ with respectto a downstream task. In other words, because the contrastive loss will produce representations which are _invariant_ to features that vary across views, any information we wish to preserve in the representations should not be altered by such views. As Tian et al.  write: "_A good set of views are those that share the minimal information necessary to perform well at the downstream task._"

Here, we question whether this assumption--in particular, with its focus on a single task--is enough to explain why contrastive foundation models succeed on a _range_ of downstream tasks. In Section 2, we observe that the actual choice and application of **views in practice** does not align with this prevailing consensus. For example, complete invariance to several common augmentations (e.g. shifts in brightness or cropping) is undesirable since augmentations of inputs from different classes can collide. Furthermore, in many cases there are explicit ways to specify invariances (e.g. converting images to grayscale) that researchers avoid, instead specifying them indirectly via augmentations (e.g. hue shifts). These observations suggest that specifying invariances is not the sole role of these views.

Instead, we suspect that augmentations serve as a form of **feature dropout**--preventing any one feature from becoming a shortcut feature and suppressing the learning of other features. We study this idea empirically with a set of synthetic datsets constructed by overlaying a simple element (e.g. a digit, shape, letter, or speech snippet) on an image or audio recording. We find that adding such a simple feature can dramatically decrease how well the other feature is learned, but that stochastically "dropping out" the simple feature can enable both features to be learned well. Next, we use this perspective to explain the success of Viewmaker Networks, a recently proposed method that generates augmentations for contrastive learning via adversarial training. We apply viewmaker and expert views to these same synthetic datasets, as well as a naturalistic dataset of facial images annotated with 40 different attributes (e.g. "wearing lipstick" or "blond hair"). Across these settings, we find that viewmaker augmentations learn to selectively obscure various features in the input. Despite this, the viewmaker representations still learn the downstream tasks well, while expert views often struggle on one or more of the attributes. This further suggests that being label-preserving is not a necessary property of good views, as long as the label information is still _sometimes_ accessible.

Finally, we formalize the intuition that feature dropout can aid learning with a theoretical analysis of a simple linear contrastive setting. In this setting, we characterize how the noisiness of each feature directly determines how quickly features are learned, and uncover an **interaction between features** governing how fast they are learned. In particular, we show how learning one feature quickly can suppress the learning of other features, and show that adding noise to the "easiest" feature can _increase_ the rate at which other features are learned. This further indicates that _label-destroying_ augmentations may have a direct role in ensuring that contrastive models learn a broad range of features for downstream tasks.

Overall, these findings suggest the need to revisit common assumptions about the role of augmentations for contrastive learning in the foundation model setting, and move towards a better understanding of how to train generalist models that learn diverse features from unlabeled data.

## 2 Common practices are at odds with the "invariance" explanation

We begin by briefly exploring several common augmentations used in contrastive learning for natural images, and explore how they come into conflict with the common assumption described above. First, we observe that many common augmentations can affect the label of the input, depending on the downstream task. For example, many downstream image recognition tasks require color information (e.g. identifying bird species) or brightness (e.g. scene or time-of-day classification), implying that invariance to these characteristics would be undesirable. Yet hue shifts, greyscaling, and brightness shifts are common augmentations used in contrastive learning [9; 23].

Second, repeated application of some augmentations causes challenges for _all_ downstream tasks. For example, applying brightness shifts repeatedly results in any image turning completely black or completely white. Thus the class label cannot be truly invariant to this augmentation, since inputs from different classes can experience an "augmentation collision" at this black or white image (this is formalized in Appendix B).1 This argument also applies to other augmentations, including shifts in contrast2 and random masking.

Third, some augmentations are commonly used _despite_ ways of explicitly encoding invariance to them. For example, two image augmentations are _hue shifts_ and _greyscaling_. Invariance to both of these augmentations can be explicitly encoded by always converting an image to greyscale. Yet doing so is not common practice because color information is still desirable for many downstream tasks.

The contradictions between the invariance rationale for augmentations in contrastive learning and these common practices suggest the need for additional explanations for the role of augmentations.

Controlled experiments demonstrate the benefits of feature dropout in settings with multiple features

In this section, we present controlled experiments on synthetic data demonstrating how label-destroying augmentations can balance the learning of multiple features during contrastive learning. Our core toolkit is to overlay images with a set of synthetic features. As we will show, the presence of these synthetic features causes the network to learn the synthetic features very well at the expense of the image features, as measured by downstream classification accuracy. However, "dropping out" these features some fraction of the time during contrastive learning enables us to trade-off how well each feature is learned, while not resulting in complete invariance to either set of features.

### Datasets

We consider the behavior of viewmaker networks on four synthetic datasets, including three image and one audio dataset. Each dataset is constructed in such a way as to support two distinct downstream classification tasks, enabling us to examine precisely how well each downstream task is learned. The presence of two downstream tasks enables us to analyze the foundation model setting where we wish to learn features relevant for multiple downstream tasks, as opposed to one set or the other.

Image datasetsThe three image datasets are based on the canonical CIFAR-10 image-recognition dataset  (MIT-License). One task is always to predict the CIFAR-10 object label (e.g. airplane or bird). The other task is dependent on an additional feature overlaid on the image: **C+Shapes:** The CIFAR-10 image is overlaid with one of three randomly-colored shapes: a square, a triangle, or a circle. The second task is to predict what shape was overlaid (N=3 classes). **C+Digits:** The CIFAR-10 images are overlaid with four copies of a randomly-sampled digit from the MNIST dataset. The second task is to predict the digit class (N=10 classes). **C+Letters:** The CIFAR-10 images are overlaid with four copies of a randomly-colored English letter. The second task is to predict the class of the letter (N=26 classes).

Audio datasetThe audio dataset is created by overlaying the audio of a spoken digit (from the AudioMNIST dataset , MIT License) with a random background sound (collected from one of three possible classes: cafe, machinery, and traffic) [43; 42]. The tasks are to predict the digit class (N=10 classes) and to predict the sound class (N=3 classes). Inputs are presented to the network as log mel spectrograms.

### Experiments

PretrainingWe pretrain with the SimCLR algorithm for 200 epochs with a batch size of 256 and a temperature of 0.1. We use a ResNet-18 model with standard modifications for smaller inputs (including a smaller stride and no initial maxpool) as used in Tamkin et al. . We use the standard SimCLR augmentations for the image datasets , with the exception of gaussian blurring, which we found to have no impact on downstream performance (Appendix C.4). For audio, we use the SpecAug  augmentations, which randomly mask out different frequency and time bands, as well as the WaveAug  augmentations, which alter various properties of the waveform such as the pitch and speed.

Linear EvaluationWe evaluate the quality of the learned representations by training a linear softmax classifier on top of the prepool representations. We train for 100 epochs, using the same parameters as Tamkin et al. , using SGD with learning rate 0.01, momentum 0.9, weight decay 0, and batch size 128. We train separate linear classifiers using the same pretrained network for each downstream task . Augmentations are applied during training but not evaluation.

ResultsAs shown in Figure 1, we see an interaction between the two features, where dropping out the synthetic feature improves learning of the main image or audio class. Across settings, we see regions where both features are still learned well, providing a concrete example of how feature dropout can be useful when learning multiple features during contrastive learning.

## 4 Viewmaker Networks Succeed Despite Destroying Label Information

As another point of evidence that good views need not be label-preserving, we consider viewmaker networks , a generative model which produces augmentations for contrastive learning. Intuitively, viewmakers learn a stochastic augmentation policy that makes the contrastive task as hard as possible for the encoder. The stochastic augmentations are parameterized as additive perturbations bounded by an \(L_{1}\) norm, meaning the viewmaker can alter but not completely destroy the original image.

Formally, given an input \(x\), a viewmaker network \(V_{}\) is trained jointly with an encoder \(E_{}\) to optimize the minimax expression:

\[_{}_{}(E_{}(x+(x,_{1})}{||V_{}(x,_{1})||_{1}}),\ E_{}(x+ (x,_{2})}{||V_{}(x,_{2})||_{1}}))\]

Here \(\) is a multiview loss function (e.g. [9; 23]), \(x\) is a minibatch of inputs, \(||||_{1}\) is the \(L_{1}\) norm, \(\) is the _distortion budget_ controlling the strength of the views, and \(_{1},_{2} N(0,I)\) are random inputs that enable the viewmaker to learn a stochastic augmentation policy. We clamp the output of the viewmaker for images to \(\) as in Tamkin et al. .

Viewmaker networks learn to stochastically alter different parts of the input, including task-relevant features, meaning that these augmentations are not label-preserving. Nevertheless, as we will see shortly, viewmaker networks enable strong performance on multiple downstream tasks, including often better performance than expert-designed augmentations. Moreover, this **feature dropout** capability of viewmaker networks may help them learn many features well rather than just the easiest.

Figure 1: Linear probing accuracy (y-axis) after contrastive learning with varying rates of dropout of the synthetic feature (x-axis). In all cases, we see a clear tradeoff between features, where dropping out the synthetic feature improves learning of the object class.

### Experiments and Results

Experimental SettingsWe use the same experimental settings as Section 3, however without manual dropout of the synthetic features. In one set of experiments, we use the standard augmentations from Chen et al. , which we henceforth refer to as the _expert augmentations_. For the experiments with _viewmaker augmentations_, we use a budget of \(=0.05P\) for the image datasets, and \(=0.125P\) for the audio datasets, where \(P\) is the number of pixels in the input.

Additional naturalistic dataset with 40 attributesTo further validate the behavior of viewmaker on realistic multi-feature datasets, we consider the CelebA  dataset, a large database of faces annotated with 40 different features. These features cover a wide spectrum of facial attributes, such as "Has Bangs" "Wearing Lipstick" and "Smiling," and enable us to further analyze whether viewmaker networks learn a broader range of features than commonly-used augmentations.

Figure 2: **Comparison of viewmaker and expert augmentations on datasets with multiple features.** The viewmaker augmentations adapt to the particular semantics of the input data, and make targeted perturbations which remove the class-relevant information of the synthetic features (e.g. occluding the digit, shape, letter, or speech). Despite this, the encoder network is still able to learn strong representations. _Rows_ (from top): Digits, Shapes, Letters, Audio. _Columns_ (from left): Expert augmentations, viewmaker augmentations, difference between original and viewmaker augmentation, rescaled to . Center image in each grid is the original. Audio Expert views shown are Spectral views.

### Results on two-feature datasets

Qualitative evidence of feature dropoutVisually, the viewmaker augmentations seem to stochastically alter different aspects of the input, as shown in Figure 2. In addition to modifying the background of each input, the viewmaker also selectively modifies the additional synthetic features added to each domain: **C+Digits:** The viewmaker augmentations selectively add pixels to the MNIST digits, making it difficult to distinguish which number is present. **C+Shapes:** The viewmaker augmentations sometimes draw squares around the shape in the center, making it difficult to determine the shape class. **C+Letters:** The viewmaker draws letter-like markings on top of the letters, obscuring the letter identity and color. **Audio:** The viewmaker identifies the narrow band corresponding to the speech and applies perturbations to it. As can be seen in Figure 2, these label-destroying augmentations are quite common, occuring in a sizable fraction of the sampled views.

Quantitative evidence of feature dropoutWe also measure this selectivity of features quantitatively in Appendix C.2 and Figure 6. We augment images 1,200 times and observe the resulting probability assigned to the correct object class. Two clear modes appear for viewmaker, but not expert, augmentations. This corresponds to the fraction of time the viewmaker destroys the overlaid feature information (low P(correct object class)) and preserves it (high P(correct object class)).

Viewmaker succeeds despite destroying label informationAs shown in Table 1 and Table 2, viewmaker networks achieve good accuracy on both tasks, while expert augmentations frequently achieve lower performance on one or both. On image tasks, for example, while expert views achieve slightly higher performance when classifying the image only, they see a large drop in accuracy when

    & VM (CIFAR-10) & Expert (CIFAR-10) & VM (Object) & Expert (Object) \\  CIFAR-10 Only & 84.5 & **86.2** & - & - \\  C+Shape & **79.8** & 76.0 & **100.0** & **100.0** \\ C+Digit & **69.3** & 58.8 & **94.3** & 86.7 \\ C+Letter & 71.9 & **74.8** & **96.9** & 94.1 \\   

Table 1: **Transfer accuracy on different features. Viewmaker (VM) networks are able to achieve good performance across multiple downstream tasks, while expert views sometimes falter. Networks are pretrained on the datasets on the left, and transfer accuracy is reported for the different conditions on the columns. Runs are averages of three seeds (with the exception of CIFAR-10 Only, from ).**

Figure 3: **Viewmaker networks capture a broader range of features on a naturalistic dataset. Linear evaluation F1 score on CelebA for viewmaker and expert views. Attributes are sorted from lowest to highest accuracy within each augmentation type.**the synthetic feature is added. In two of these cases (Shape and Digit) the viewmaker models are able to achieve a higher accuracy on both the image and the synthetic feature, while on the third (Letters) they achieve slightly lower accuracy on the images but achieve half the error on the synthetic object. For the audio experiments the picture is similar--viewmaker avoids catastrophic drops in performance learning both features together, achieving the highest accuracy on both, while the expert views see larger drops and worse overall performance. Note that the high performance of expert views for our control tasks (CIFAR-10/Speech/Sound Only) indicates that the viewmaker views are not merely better all-around views, but that they specifically help the model learn multiple features.

These results provide additional evidence that label-preserving views are not necessary for learning good representations--and that feature dropout may improve learning of multiple tasks.

### Results on naturalistic dataset

We observe similar qualitative and quantitative results for the CelebA dataset. We train models using the same settings in Section 3.2, using a budget of 0.01, and indeed find that viewmakers capture a much broader range of features, achieving an average F1 Score of **0.509** over the 40 features, compared to **0.334** for the SimCLR augmentations. In addition, the viewmaker augmentations clearly capture a wider range of features, as can be seen in Figure 3, especially at the tail of the distribution. Furthermore, we see bimodal disruption patterns in over two-thirds of the CelebA features, as shown in Figure 9, indicating significant feature dropout across in most attributes. We also show qualitative results in Figure 8 demonstrating that the viewmaker alters attributes such as facial features, hair color, and background elements in the scene. These results further support the hypothesis that viewmaker networks exhibit feature dropout, yet capture a broader range of features than expert views.

## 5 Theoretical Analysis of Feature Interactions in Linear Contrastive Setting

In this section, we analyze a simple linear model that captures the essence of how label-destroying augmentations can improve downstream accuracy. We study a setting where the data contains many underlying features that are relevant to downstream classification tasks, and where these features are preserved to varying degrees across augmentations. We will show that a linear model trained with a contrastive objective learns these features, and that adding noise to one feature can speed learning of others during gradient descent. One difference between the linear setting we analyze and Section 4 is that here add stochastic Gaussian noise to destroy features across augmentations, as opposed to the bimodal feature dropout behavior of viewmaker networks seen in Figure 2.

Figure 4: We show how label-destroying augmentations can aid learning of other features in a linear contrastive setting: (a) The correlation of the \(k\)th feature of an augmentation pair, shown for \(d=2\). Each pair \(u_{k}^{(i)}\) and \(v_{k}^{(i)}\) have correlated projections onto the ground truth \(_{k}\) direction, representing the feature conserved across augmentations. (b) Feedforward linear network which computes the representation \(f_{}(w)\). As each feature \(_{k}\) is learned (\(_{k}_{k}\)) the representations of the two views \(f_{}(u^{(i)})\), \(f_{}(v^{(i)})\) become more similar, decreasing the contrastive loss.

### Data Model and Setting

We study a model which consists of data with \(K\) distinct features, each corresponding to some ground truth unit-vector directions \(_{1},,_{K}^{d}\). We sample each data point \(u^{K d}\) and its _augmentation_ (a.k.a. its _positive pair_ or its _view_) \(v^{K d}\) as follows. For \(k 1,,K\), the \(k\)th row of \(u\), which we denote \(u_{k}\), is drawn from the Gaussian distribution \((0,I_{d})\). The \(k\)th row of the augmentation, \(v_{k}\), is drawn from the same distribution, but is correlated with \(u_{k}\) in the \(_{k}\)-direction (and is otherwise independent in the other directions). The strength of the correlation is governed by parameter \(_{k}\) in the following sense: \(v_{k}^{T}_{k}=_{k}u_{k}^{T}_{k}+^{2}}\), where \((0,1)\). Thus the larger \(_{k}\), the stronger the correlation in that feature across the two views. Figure 4(a) visualizes the correlation of \((u_{k},v_{k})\) in an augmented pair. Formally, we can write that \((u_{k},v_{k})(0,I_{d}&_{k}_{k}_ {k}^{T}\\ _{k}_{k}_{k}^{T}&I_{d})\), for a vector \(^{k}\).

We will learn a model \(^{K d}\), which represents a collection of \(K\) feature extractors, as pictured in Figure 4(b). The model \(\), with rows \(\{_{k}\}_{k[K]}\), maps a data point \(w^{K d}\) to a representation \(f_{}(w)^{K}\) by computing a score \(w_{k}^{T}_{k}\) for each element in the representation. That is, \((f_{}(w))_{k}=w_{k}^{T}_{k}\). Our goal is that the model \(\) will be useful for a downstream classification task which depends on the ground truth features. A good representation will capture ground truth features that are correlated across augmentations, such that \(_{k}\) is aligned with \(_{k}\) or \(-_{k}\).

Training.We will study the the evolution of \(\) as we optimize a standard constrastive learning objective using gradient descent [14; 9]. At each round of gradient descent, we sample a fresh batch of \(m\) data points and their augmentations, \((U,V):=\{(u^{(i)},v^{(i)})_{i[m]}\). For each \(i,j[m]\), we compute a similarity score \(z_{ij}:= f_{}(u^{(i)}),f_{}(v^{(j)})=_{k}( _{k}^{T}u_{k}^{(i)})(_{k}^{T}v_{k}^{(j)})\) using the dot product of their \(K\)-dimensional representations. We then compute the logits \(p_{ij}:=)}{_{j^{}}(z_{ij^{}})}\) using the softmax function, and use the classwise cross entropy loss function \((;U,V):=-(p_{ii})\).

### Main Result

We will study gradient descent (GD) on the cross entropy loss, and consider how adding noise to one feature affects the learning of the other features. As suggested earlier, we can measure how well we learn the \(k\)th feature by measuring the alignment of \(_{k}\) with \(_{k}\) or \(-_{k}\). A natural way to measure this alignment is the acute angle between \(_{k}\) and \(_{k}\), given by \((^{T}_{k}|}{\|_{k} \|_{2}})\). Lemma E.1 in Appendix E proves that this quantity directly determines the test accuracy \(\) on a natural downstream linear classification task.

Formally, we say we _add noise_ to some feature \(k^{}\) of a data point \(v\), if for some \([0,1)\), we define the new noisy data point \(\) to have coordinates \(_{k^{}}= v_{k^{}}+}\), where \((0,I_{d})\), and \(_{k}=v_{k}\) for \(k k^{}\). Thus if \((u,v)\) were a pair generated with the correlation coefficients \(\{_{k}\}_{k[K]}\), then the distribution of \((u,)\) comes from the modified correlation coefficients \(\{\}_{k[K]}\) with the single modification \(_{k^{}}=_{k}\). We now present our main theorem:

**Theorem 5.1** (Noise improves feature learning).: _There exists a universal constant \(C\), such that the following holds. Let \(^{(t+1)}=^{(t)}-((U,V;)+^{(t )})\), and \(^{(t+1)}=^{(t)}-((U,;)+ ^{(t)})\), where \(\) is \(V\) with any amount of added noise in the \(k^{}\) feature. This has the effect of changing \(_{k^{}}\) to \(_{k^{}}\) for any \(_{k^{}}<_{k^{}}\). Then for any \(k k^{}\), if \(|_{k}^{T}_{k}|}^{2}}{C}\|_{k}\|\), \(\|_{k^{}}\|^{3}|_{k^{}}^{T}_{k}|\), and \(\|_{k}\|^{2}(1-_{k^{}}^{2})}{C}\), then for a small enough step size \(\), \(_{U,V}[(^{T}_{k}^{( t+1)}|}{\|_{k}^{(t+1)}\|_{2}})]> _{U,}[(^{T}_{k}^{(t+1)}|}{\|_{k}^{(t+1)}\|_{2}} )]\)._

We briefly comment on the three assumptions on \(\) in the theorem. The first assumption, \(|_{k}^{T}_{k}|}^{2}}{C}\|_{k}\|\) requires that \(_{k}\) is not too aligned with \(_{k}\) - that is, the result applies to all features \(k\) that are not already learned too well. The second two assumptions are satisfied if the \(k^{}\)th feature has been learned to some extent, and the norm of \(_{k}\) and \(_{k^{}}\) are small, which can be enforced throughout training with \(_{2}\) regularization.

The theorem guarantees that at any point in training, if we add noise to the \(k\)'th feature, the next step of GD learns all other features _better_ than if we did not add noise. To validate the implication of this result for the complete trajectory of GD, we include simulations in Appendix D. Our experiments show that introducing noise part-way through training to dominant features can significantly speed the alignment of weak features, with only a small cost to the alignment of the dominant features. We prove our result in Appendix E, including intuition and an overview of the ideas in Section E.3.

## 6 Related work

Understanding contrastive and multiview learningMany prior works have laid the foundations for current contrastive and multiview learning algorithms [4; 21; 14; 55; 2; 34; 23; 9]. Several works analyze contrastive learning to identify important factors [12; 58] or how contrastive models differ from supervised learning [57; 15; 26]. HaoChen et al.  study contrastive learning using the concept of an augmentation graph. This model assumes the fraction of non-label preserving augmentations is "extremely small"; interestingly, we show in practice this can be quite large and still yield good performance. Wang et al.  theoretically study contrastive learning under an assumption of label-preserving augmentations, though they show that such an assumption alone does not suffice to learn. Most relevant to our work, Tian et al. ; Ericsson et al.  study how the information shared between different views impacts learning of downstream tasks. We complicate this picture by analyzing the foundation model setting where a single model must learn features for multiple tasks that are not known in advance. In this setting, we find that label-destroying perturbations, thought to be harmful by Tian et al. , are useful for preventing one feature from suppressing others.

Feature suppressionOur work is closely connected to the notion of _feature suppression_, where the presence of one feature can suppress the learning of others. Several works explore this concept in contrastive learning. For example, the original SimCLR paper  noted that color jitter augmentation was necessary to prevent the network from using only the color profile of the input to solve the contrastive task. Followup work  characterizes how hyperparameters and dataset features affect feature suppression, including through a range of synthetic experiments that vary the amount of information shared between views. Other works have attempted to address feature suppression in contrastive learning, either via auxiliary losses  or by modifying representations in the latent space . Our builds upon these works by empirically and theoretically investigating feature suppression as an alternate rationale for the role of augmentations, as opposed to invariance. We also show that an existing method, viewmaker networks , can identify and potentially neutralize suppressing features in an interpretable way, resulting in better performance than expert augmentations. These insights may also generalize to other self-supervised settings, such as language modeling, where multiple features may compete .

Spurious correlations and shortcut featuresOutside the framing of feature suppression, several other works explore how classifiers can learn or make use of unwanted features. Shortcut features  describe often-simple features (e.g. the average color of an input) which are learned by networks at the expense of more salient features (e.g. the object class). This notion is connected to spurious correlations  in deep learning which have been explored extensively [40; 41; 46; 53; 56], including in the context of self-supervised learning [33; 51]. Other works have also performed theoretical analysis of how related dynamics affect learning in the supervised setting [30; 44]. In general, if such spurious correlations are known in advance, one can often design augmentations to remove such correlations and improve learning. However, our work suggests that viewmaker networks may be a useful tool in cases where such features are not known a priori--both as an interpretability tool to visualize the different features a network relies on, and as a way to reduce reliance on particular features without completely destroying the information.

## 7 Discussion and Conclusion

We explore the idea that augmentations in contrastive learning function as a sort of "feature dropout." First, we show that in datasets with multiple features, dropping out one set of features improves learning of the other features. Second, feature dropout may help explain how viewmaker networks can learn a wide range of features well, despite producing augmentations which appear to destroy different features in the input. Finally, we theoretically analyze a linear contrastive setting where we prove that label-destroying views have a positive effect on contrastive learning if the goal is to avoid learning one feature at the expense of others.

Our work has limitations: for example, while our experiments consider image and audio data, self-supervised learning may be applied to a much wider range of modalities [48; 50]. In addition, our theoretical analysis considers a linear contrastive setting, whereas current neural networks are highly nonlinear. Improving upon both of these fronts is an exciting area for future work. On the other hand, understanding augmentations as dropping out easy features suggests possible ways of improving the performance of self-supervised learning. For example, a guided version of viewmaker might enable prioritizing a subset of important features for learning, or might enable dropping out unwanted features such as watermarks, sensitive information, other image artifacts.

The challenge of learning a broad range of useful features lies at the heart of self-supervised learning. We hope our work sheds light on this challenge in contrastive learning, especially as these objectives continue to develop and are applied more broadly and at larger scale.