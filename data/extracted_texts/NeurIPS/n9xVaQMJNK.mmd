# Few-Shot Adversarial Prompt Learning on Vision-Language Models

Yiwei Zhou

School of Automation

Beijing Institute of Technology

zhouyiwei@bit.edu.cn

&Xiaobo Xia

Sydney AI Centre

University of Sydney

xiaoboxia.uni@gmail.com

&Zhiwei Lin

School of Automation

Beijing Institute of Technology

linzhiwei@bit.edu.cn

&Bo Han

Department of Computer Science

Hong Kong Baptist University

bhanml@comp.hkbu.edu.hk

&Tongliang Liu

Sydney AI Centre

University of Sydney

tongliang.liu@sydney.edu.au

Corresponding author: Tongliang Liu (tongliang.liu@sydney.edu.au) and Zhiwei Lin (linzhiwei@bit.edu.cn)

###### Abstract

The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1% training data. Code is available at: https://github.com/lionel-w2/FAP.

## 1 Introduction

The seminal works  reveal that adversarial examples , consisting of malicious perturbations imperceptible to humans, can easily mislead state-of-the-art deep neural networks (DNNs)  into making incorrect predictions. This vulnerability limits the application of DNNs in safety-critical areas, such as medicine , healthcare , and autonomous driving .

Human cognition is immune to the distribution variations induced by adversarial attacks, reflecting a fundamental difference between human and machine cognitive understanding. Humans primarilyrely on semantic information  from the context, while machines depend more on statistical distributional associations. Consequently, recent work  introduces text supervision in adversarial adaptation through foundational vision language models (VLMs) [12; 13; 14; 15; 16; 17; 18; 19], enhancing adversarial robustness with improved semantic understanding. Specifically, they adapt visual prompts by aligning adversarial visual features with static text supervision from the CLIP model . By narrowing the gap in the probability distribution between adversarial text-image logits and the ground-truth label, they achieve zero-shot adversarial robustness in downstream tasks.

However, although some progress has been made with the previous method, there are still three limitations to overcome before leveraging context to mitigate adversarial vulnerabilities. First, zero-shot adversarial robustness in downstream tasks stems from aligning image and text embeddings on large-scale generic datasets like the entire ImageNet  through adversarial adaptation, which necessitates a huge amount of time and computational resources. Second, static hand-crafted text prompts lack adversary-related hints, providing only content-related information while disregarding adversarial components. Finally, the current adaptation method only considers adversarial inputs while disregarding natural inputs. On the one hand, it fails to account for the relationship and distinctions between natural and adversarial examples, potentially leading to catastrophic forgetting of natural generalization during adversarial adaptation. Worse still, if there are distributional discrepancies in the downstream datasets, the constrained natural generalization could hinder the learning of robustness.

To address these issues, we propose a _Few-shot \(\)dversarial \(\)rompt learning (FAP)_ framework where pre-trained VLMs are adversarially adapted in a few-shot manner [21; 22] with prompt learning [23; 24; 25; 26; 27; 28]. This adapts the inputs rather than the parameters of the model. To the best of our knowledge, this is the first time to learn adversarial robustness from the perspective of few-shot prompt tuning. Due to the scarcity of data for establishing robust decision boundaries, the robust representations learned by existing adversarial visual prompt methods  are far from satisfactory. This leads us to rethink how to provide appropriate prompts for adversarial examples. Instead of using static hand-crafted text prompts, we propose to learn adversarially correlated text supervision end-to-end from adversarial examples. Moreover, we design a novel training objective that harmonizes the connection and distinction of natural and adversarial features from information across different modalities. That is, we force the multi-modal features of natural and adversarial inputs to be consistent while encouraging the differentiation between uni-modal embeddings.

Compared to existing methods, our method has several advantages. (1) It significantly reduces the dependence on abundant data, as both text supervision and learning objectives are adversarially correlated with visual embeddings, providing a better alignment to establish robust generalization from limited examples. By adapting with a 16-shot subset from ImageNet-1K, we achieve comparable zero-shot robustness in downstream tasks using only 1% training data. (2) We provide adversarially correlated text supervision learned end-to-end from adversarial examples, which notably improves the alignment between visual and textual embeddings, making superior zero-shot adversarial robustness. (3) Our novel training objective fully leverages the dual-encoder architectural advantage of CLIP. It enhances cross-modal consistency between natural and adversarial examples to avoid potential robustness generalization failures, while encourages uni-modal divergence to introduce an adversarial aware mechanism that aids in learning adversarial text supervision.

Before delving into details, we clearly summarize our contributions as follows. (1) We focus on a realistic and important research problem and discuss three major issues in previous adversarial prompt learning paradigms, potentially inspiring further improvements in this area. (2) To tackle these issues, we propose a novel adversarial few-shot prompt learning framework with learnable adversarial text supervision and an adversarial-aware prompt learning objective. This method is lightweight yet makes significant adversarial generalizations. (3) We justify our claims through a series of experiments on 11 benchmark datasets covering multiple recognition tasks. The proposed method significantly outperforms state-of-the-art adversarial prompt learning methods in adversarial few-shot learning, adversarial zero-shot transfer, and adversarial base-to-new generalization settings. Comprehensive ablation studies and discussions are also provided in Section 4.3 and Appendix D.

## 2 Preliminary

**CLIP recap.** A pre-trained CLIP model typically includes an image encoder \(\) with learned parameters \(_{}\) and a text encoder \(\) with learned parameters \(_{}\). Here we consider a \(K\)-class classification problem for an image \(\) and its corresponding label \(y\{1,,K\}\). To perform zero-shot evaluation, \(\) is first divided into \(M\) patches and converted into the patch embeddings \(()\). A class token \(c_{}\) is then appended to the patch sequence as \(()=\{c_{},e_{1}(),,e_{M}()\}\). Afterward, the image encoder \(\) processes this embedded patch sequence with ViT  blocks to produce the latent image feature representation \(^{(I)}=(();_{})\). For the text branch, we prepare hand-craft prompts \(t_{i}=\{t_{1},,t_{K}\}\) by appending the class name to a word template, such as 'a photo of a {class}'. Subsequently, \(t_{i}\) is tokenized and embedded as \((t_{i})=\{w_{1}(t_{i}),,w_{N}(t_{i}),i\}\), where \(i\) corresponds the \(i\)-th class. The text encoder \(\) then encodes these work embeddings into the latent text feature representation \(^{(t_{i})}=((t_{i});_{})\). For zero-shot classification, the probability of the image \(\) in the \(i\)-th class is

\[p(y=i)=^{(I)},^{( t_{i})})/)}{_{j=1}^{K}((^{(I)}, ^{(t_{j})})/)},\] (1)

where \((,)\) denotes the cosine similarity score and \(\) is the temperature parameter.

**CLIP-based prompt learning.** Instead of adopting a hand-crafted prompt, prompt learning attempts to train lightweight learnable prompts \(}\) with a few examples from _downstream_ data. To be concrete, \(}\) is inserted into word embeddings as \((t_{i},})=\{},w_{1}(t_{i}),,w_{N}(t_{i}),i\}\). Then, the text feature representation is \(^{(t_{i},})}=((t_{i},});_{})\). To preserve the alignment characteristics of the joint image-text feature space for zero-shot capabilities, CLIP-based prompt learning optimizes the prompt tokens by narrowing the gap in the distribution between text-image logits and the ground-truth label using cross-entropy:

\[}^{*}=_{}}_{(,y)}_{ }((^{(I)},^{(t_{i},})}),y),\] (2)

where \((^{(I)},^{(t_{i},})})\) corresponds the text-image logits. We suggest readers check Zhou et al.  for more details about CLIP-based prompt learning.

**Adversarial visual prompt.** Adversarial prompt learning optimizes prompt tokens through adversarial training, enhancing model robustness in a relatively small adaptation cost without altering the pre-trained model. Mao et al.  achieves this by adjusting the visual prompt of adversarial images in joint text-image feature space. Notably, owing to the application of text-image contrastive loss during the generation of adversarial examples, the adapted model reveals zero-shot adversarial robustness on downstream tasks. Formally, let \((,d_{})\) be the input feature space \(\) with the infinity distance metric, where \(d_{}(,^{})=\|-^{}\|_ {}\). Adversarial data \(}\) falls in to close ball \(_{}()\) of radius \(\) centered at \(\). That is, \(_{}()=\{^{} d_ {}(,^{})\}\). The learnable image prompt \(}\) is inserted to the visual patch embedding of \(}\), as \(e(},})=\{c_{},},e_{1}(}),,e_{M}(})\}\). Then, adversarial data \(}\) is generated by maximizing the text-image contrastive loss as

\[}=_{}_{}( )}_{}((}^{(I, })},),y),\] (3)

where \(}^{(I,})}=(e(},});_{})\). The learnable prompt token \(}\) is optimized given the adversarial example \(}\), hand-craft prompts \(\), and ground-truth label \(y\), by minimizing the adversarial text-image contrastive loss:

\[}^{*}=_{}}_{(,y)}_{ }((}^{(I,})},),y).\] (4)

Here, \(_{}((}^{(I,})},),y)\) is defined as a text-image contrastive adversarial training (TeCoA) loss by Mao et al.  that highlights adversarial text-image alignment.

**Drawbacks of previous methods.** Despite the promising zero-shot adversarial robustness achieved through adversarial visual prompts, certain inherent characteristics impede its widespread application.

(1) The zero-shot adversarial robustness in downstream tasks originates from the alignment of image and text embedding on a large-scale generic dataset like the entire ImageNet during prompt tuning. This necessitates an extensive amount of training data and employs prompts of considerable size (token-level prompts with a size of 200), which not only causes significant prompt-related overhead but also precludes the benefits of lightweight adaptation on the top of the pre-trained models that prompt tuning typically offers.

(2) Due to the distinct visual representation distribution between clean and adversarial examples, static hand-crafted prompts lack adversary-related hints, thereby only providing content-related information without effectively supervising the adversarial components contained in the images. However, manually adjusting hand-crafted prompts to inject additional adversarial hints is also challenging, as the imperceptibility of adversarial perturbations limits their feature description, and the intensity and distribution of these perturbations are variable throughout the training process.

(3) The current learning objective directly trains to provide prompts with adversarial examples, yet it overlooks the model capacity for natural generalization in downstream tasks. This presents a potential risk of failure, especially in the context of few-shot prompt tuning where the pre-trained model shows inadequate natural generalization on a sampled few-shot dataset.

## 3 Method

**Overview.** To address the limitations of previous methods, we propose FAP, a few-shot adversarial prompt learning framework. Our framework uses lightweight learnable prompts on the top of the pre-trained CLIP in a few-shot manner, as the case in natural prompt tuning . In more detail, we introduce learnable prompt tokens for adversarial examples, which allows the model to provide more appropriate text supervision that helps balance natural and adversarial generalization. Based on CLIP's dual-encoder architecture, we further provide a novel training objective that guides the discrimination of natural and adversarial embeddings in uni-modal feature space. This promotes uni-modal divergence to incorporate an adversarial-aware mechanism, facilitating the learning of adversarial text supervision. The overview of the proposed framework is provided in Figure 1. Below, we discuss the FAP framework step by step.

### Learnable Text Supervision for Adversarial Examples

When adapting the CLIP model, a slight change in wording could have a huge impact on performance . With the existence of adversarial examples, the situation has become worse. The distribution differences between natural and adversarial examples necessitate the design of special

Figure 1: The overview of the proposed _**F**ew-**shot_**A**_dversarial_**P**_rompt learning (FAP)_ framework. Note that only prompt tokens as well as the deep projections from image to text are tuned while the rest of the model is frozen. Our method promotes a consistent cross-modal similarity distribution between natural and adversarial examples, while encouraging differences in uni-modal representations. The adversarial-aware text supervision learned in this manner can better align adversarial features and establish robust decision boundaries with a limited number of examples. The natural and adversarial forward processes of the image encoder share parameters.

ized text supervision specifically for adversarial samples. Therefore, we introduce text prompt tokens that are end-to-end learned through adversarial examples.

Formally, our adversarial prompt learning is implemented on a few-shot subset \(\), created by sampling \(m\) examples from each of the \(K\) classes in the original dataset. Learnable prompts consist of both visual and text branches, denoted as \(=\{_{},_{}\}\). The visual prompt token \(_{}\) is incorporated into the image embedding, as observed in an adversarial visual prompt, while text prompt token \(_{}\) is inserted into word embedding, as is the case in natural prompt learning. To preserve mutual synergy between visual and text branchs, \(_{}\) is obtained from \(_{}\) through linear projection \(h\), which can be denoted as \(_{}=h(_{})\). The proposed framework can be categorized as a cross-modal prompt  with minimal modification for adversarial robustness tasks. We offer a comprehensive analysis of the prompt design in Section 4.3.

### Balancing Natural and Adversarial Generalization in Few-Shot Adversarial Prompt

For adapting the CLIP model to adversarial robustness tasks, the existing method  proposes the TeCoA loss (Eq.(4)). This method minimizes the discrepancy between the distribution of adversarial text-image similarity and one-hot ground-truth labels. While this strategy effectively aligns text representations during adversarial adaptation, it potentially compromises the model's generalization ability in specific recognition tasks under few-shot conditions.

The method's effectiveness depends on the similarity between the downstream task's distribution and the pre-trained representations. When the downstream task closely aligns with the pre-trained representation, the CLIP model shows preferable natural generalization, and adding learnable prompts for robustness adaptation is advantageous. However, a significant mismatch between the downstream distribution and pre-trained representations challenges the CLIP model's natural generalization capabilities. In such cases, expecting prompt tokens to learn both natural and robust generalization from a few adversarial examples is overly ambitious.

**Balancing natural and adversarial generalization.** Inspired by the success of TRADES  in standard adversarial training, we propose a surrogate adversarial text-image contrastive loss that decouples the adversarial text-image contrastive loss into natural and adversarial terms. By encoding image and text embeddings with their respective transformer encoder and calculating similarity across modality, we have the natural and adversarial text-image logits: \((^{(I,_{})},^{(,_{})})\) and \((}^{(I,_{})},^{(,_{ {t}})})\), where \(^{(,_{})}=\{^{(t_{1},_{})}, ,^{(_{K},_{})})\}\). The learning objective can be stated as:

\[=_{}((^{(I,_{})}, ^{(,_{})}),y)+_{ }((^{(I,_{})},^{(,_{})}),(}^{(I,_{})},^{(, _{})})),\] (5)

where \(_{}\) denotes the Kullback-Leibler (KL) divergence and \(\) is a weight parameter. In Eq. (5), the first term encourages minimizing the natural error between the natural text-image similarity and label. The second term minimizes the boundary error by narrowing the distribution gap between natural and adversarial text-image similarity to ensure cross-modal adversarial consistency. Note that a balanced two-term objective is crucial for downstream generalization, as this design alleviates the potential failure in robustness caused by discrepancies in natural generalization. We provide more analysis on the natural generalization gap in Appendix D.2.

### Uni-Modal Adversarial-Aware Mechanism

To fully leverage the structural advantages of CLIP, we go beyond enforcing consistency constraints on cross-modal text-image features and tailor adversarial robustness enhancements for uni-modal features. Specifically, we introduce an adversarial-aware mechanism for visual features, guiding the distinction between natural and adversarial examples. To the best of our knowledge, this is the first initiative to foster differentiated representations in adversarial regularization.

Given the distinct distributions of natural and adversarial examples, we argue that driving consistent outputs for natural and adversarial examples in visual models constitutes a compromise, trading off generalization for robustness. In contrast, within CLIP, we achieve robustness by maintaining adversarial consistency in the text-image joint space with the adversarial term in Eq. (5), while preserving the distributional differences of features in the uni-modal visual space to minimize the impact on generalization performance. Here, we append an extra constraint on the adversarial term with cosine similarity:

\[_{}=(^{(I,_{})},}^{(I,_{})})+1,\] (6)

where the constant \(1\) maintains the _non-negativity_ of \(_{}\). We introduce the adversarial-aware mechanism by adjusting prompt tokens to minimize similarity, thereby distinctly differentiating between natural and adversarial visual features. During the training process, the text branch learns to provide proper text supervision for different visual features, ensuring that the outputs in the text-image joint space are consistent for natural and adversarial embeddings, which have significant distributional differences in the visual space.

### Overall Learning Objective

**Objective for outer minimization.** The overall training objective can be obtained by introducing uni-modal adversarial aware mechanism \(_{}\) to Eq. (5) as:

\[_{}=_{}((^{(I, _{})},^{(,_{})}),y)+ _{}_{}((^{(I, _{})},^{(,_{})}),(}^{(I,_{})},^{(,_{})}) ).\] (7)

**Objective for inner maximization.** The goal of inner maximization is to generate the adversarial example \(}\). Here, we leverage the adversarial term in Eq. (5) as this surrogate loss and find the adversarial example \(}\) as follows:

\[}=_{}_{}( )}_{}((^{(I,_{})},^{(,_{})}),(}^{(I, {P}_{})},^{(,_{})})).\] (8)

Note that strong attacks can help robustness. Here, the general PGD attack formulation with the CE loss like Eq. (3) is also applicable. With the learning objective outlined in Eq. (7), we adapt learnable prompt \(=\{_{},_{}\}\) tokens on the few-shot dataset \(\) as:

\[^{*}=_{}_{(,y)} _{}.\] (9)

### Intuition behind Objective Design

Our learning objective highlights the differentiated processing of features under different modalities, in which we introduce an additional adversarial-aware mechanism with uni-modal image features. We discuss the intuition behind the design concept. We visualize the uni-modal embedding to demonstrate the impact of the adversarial-aware mechanism on the model's feature learning.

In Figure 1(a), we find that certain adversarial embeddings closely resemble natural examples. This suggests that the consistency of cross-modal features between natural and adversarial examples arises from the model's tendency to minimize loss by generating minimal adversarial perturbations. These exceedingly small perturbations do not effectively promote robust learning. In contrast, the adversarial-aware mechanism clearly separates the natural and adversarial embeddings in Figure 1(b), preventing the minimal perturbation shortcut and guiding the model to recognize the differences between natural and adversarial image embeddings.

For better understanding, we discuss different training objective designs and their results in Section 4.3 and describe our adversarial prompt learning and adversarial prompt testing pipeline in Appendix A. Additionally, we demonstrate the significant robustness gains our learning objective brings to other prompt designs through a case study. More details can be checked in Appendix D.4.

Figure 2: Visualization of the natural image embedding, adversarial image embedding, and text embedding after tuning with and without the adversarial-aware term. Images are sampled from the same class in the Caltech101 dataset .

Experiments

### Setups

**Baselines.** To demonstrate the expertise of the proposed method, we employ the adversarial version of multiple commonly used prompt learning designs for comparison. We categorize our baselines into two groups: (1) Methods using hand-crafted text supervision, such as zero-shot CLIP  and **AdvVP**. (2) Methods utilizing learnable text prompts, including **AdvVLP** and **AdvMaPLe**. Note that we primarily focus on learnable prompts extending the **AdvVP** framework. Details on pure text prompt effects in adversarial settings (**AdvVP**)  are discussed in Appendix D.11. Additional information about these methods and static prompt templates for each dataset are provided in Appendices C.1 and C.2, respectively.

**Datasets.** To evaluate the proposed method, we align with previous works [28; 33] and utilize 11 diverse image recognition datasets that span multiple vision tasks. Specifically, the datasets include two generic object datasets: ImageNet-1K  and Caltech101 ; a texture recognition dataset: DTD ; five fine-grained object recognition datasets: FGVCAircraft , OxfordPets , Flowers102 , Food101 , and StanfordCars ; a scene recognition dataset: SUN397 ; an action recognition dataset: UCF101 ; and a satellite image classification dataset: EuroSAT .

**Implementation details.** We conduct experiments on the ViT-B/32 CLIP architecture and report the average results over three random seeds. All models are trained for 5 epochs in cross-dataset evaluation and 10 epochs for other benchmark settings by using an SGD optimizer with a momentum of 0.9. The initial learning rate is set at 0.0035. We apply a cosine learning rate scheduler and a warm-up strategy during the first epoch. For adversarial prompt learning, we use token prompts of size 2 in both the vision and text branches across the first 9 transformer blocks. Attacks are generated under \(_{}\) threat model through a 2-step PGD attack, with a perturbation boundary \(=1/255\) and a step size \(=1/255\), following the methodologies outlined in . The adversarial robustness is evaluated using a 100-step PGD attack.

Note that due to the limited space of the main paper, we provide comprehensive evaluations, including cross-dataset evaluation (Appendix D.1), the comparison with AdvMaPLe (Appendix D.3), alternative CLIP architectures (Appendix D.5), different attack strengths (Appendix D.6), various choices of adversarial robustness evaluation methods (Appendix D.7), and different training-time attack generation (Appendix D.8).

### Main Results

**Adversarial few-shot learning.** In this scenario, we evaluate the model's ability to develop robust representations with a severely limited amount of downstream data. Specifically, we tune the model using {1, 2, 4, 8, 16} shots from each class. As shown in Figure 3, the static text prompt of baseline method struggles to align with adversarial input images under a few-shot setting. Even with an increased number of training samples, the model's performance fails to improve, indicating difficulties in adversarial learning. AdvVLP and AdvMaPLe, through end-to-end learning of adversarial text prompt tokens from adversarial examples, have acquired the capability to adjust prompts from limited samples to gain adversarial robustness. By further training with our proposed objective, our method achieves superior average natural and adversarial accuracy across 11 datasets.

**Adversarial base-to-new generalization.** We present a more challenging adversarial base-to-new generalization setting, where datasets are bifurcated into base and new subclasses. Here, models are trained with a 16-shot dataset from the base classes and are subsequently evaluated on both base and new classes. In this setting, as the number of categories in datasets is generally much smaller than the number of examples per class, models need to learn intrinsic features within each dataset and robust representations from limited examples to effectively generalize large amounts of test data.

From Table 1, we observe that our method not only surpasses all its counterparts in robust metrics, but also reveals superior natural generalization due to the joint consideration of natural and robust features in our training objective. Additionally, our method also reveals much better stability (lower standard deviation). That is, even sampled few-shot subset has a natural generalization gap, our learning objective still works well and prevents potential failure.

**Matching Benchmark Zero-Shot Results Adapted with ImageNet-1K.** In addition to comparing with the baseline AdvVP under few-shot settings, we also benchmark against zero-shot results, where robustness is evaluated through cross-dataset evaluations. Initially adapted on ImageNet-1K, our method does not require adaptation across the entire dataset nor extensive prompt designs like the AdvVP , which uses embedding-level token prompts of size 200 and pixel-level pad promoters of size 40. As shown in Table 2, our method aligns with benchmark performance using just 1.25% of ImageNet-1K examples, significantly accelerating the training process by over 97%. Moreover, enhancements from 16-shot to 32-shot training and deepening prompt layers from 9 to 12 allow our method to exceed previous adversarial prompt tuning results.

    &  &  \\  Method & Base Nat Acc & Base Adv Acc & New Nat Acc & New Adv Acc \\  AdvVP & 31.68\(\)6.57 & 14.43\(\)2.26 & 30.39\(\)6.40 & 13.36\(\)2.80 \\ AdvVLP & 58.95\(\)11.67 & 32.37\(\)6.67 & 46.92\(\)7.41 & 21.61\(\)3.86 \\ AdvMaPLe & 60.38\(\)8.03 & 30.69\(\)4.71 & 46.18\(\)6.39 & 20.25\(\)3.39 \\
**FAP** & **70.52\(\)0.82** & **38.05\(\)2.15** & **49.58\(\)3.55** & **21.86\(\)2.57** \\   

Table 1: Adversarial base-to-new Generalization performance. We report the average result of the Base Natural Accuracy (%), Base Adversarial Accuracy (%), New Natural Accuracy (%), and New Adversarial Accuracy (%) on 11 datasets. Detailed results for each dataset are provided in Appendix D.10.

Figure 3: Accuracy (%) of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9.

### More Analysis

**Trade-off between natural and adversarial robustness.** Aligning with the decoupled form of classical adversarial training , our prompt objective incorporates two terms that ensure the generalization of natural examples and the consistency of robust representations. This motivates us to investigate the trade-off between natural and adversarial robustness, and to dynamically adjust this trade-off depending on the desired level of adversarial robustness.

From Table 3, we can conclude that as \(\) increases, the proportion of the adversarial component in the total loss increases, and the natural accuracy declines continuously. Meanwhile, adversarial robustness gradually improves, reflecting the trade-off between natural and adversarial generalization. However, when \(\) becomes too large (\(>2.5\)), continuing to increase the proportion of the adversarial component does not lead to further improvements in robustness.

**Prompt depth and prompt length.** We provide architectural ablation results for prompt design concerning different prompt depth and length settings. In Table 4, we can observe that increasing both prompt depth and prompt length introduces more learnable parameters, thereby resulting in improved performance. Furthermore, we can also conclude that the performance gain obtained by increasing prompt depth is higher than that achieved by increasing prompt length, and the improvement in robustness metric is larger than in natural accuracy.

**Ablation for training objective design.** In Section 3.4, we present our proposed novel training objective tailored for adversarial prompt learning. Our loss follows a two-term design, comprising a natural term and an adversarial term. The adversarial term further considers both the consistency and diversity of natural and adversarial features. In practice, we use KL divergence to constrain cross-modal consistency and encourage uni-modal diversity with cosine similarity. In Table 5, we present other possible designs for the loss function and conduct an ablation study under the adversarial base-to-new setting. Our method provides the best robustness across all these loss function settings.

**Instability analysis for deep prompt interaction.** We report an instability of generalization performance caused by the improper deep prompt interaction, revealing that the standard cross-modal prompt interaction design, from text to image prompt token, is not plug-and-play under the setting of adversarial robustness. When natural and adversarial terms are present in a certain moderate ratio in the learning objective, the performance of the model may experience a significant decline. From Figure 4, we find that the instability intensity caused by the text-to-image design varies across different datasets, and the values of \(\) leading to this instability are also different. For instance, on some generic datasets, the performance degradation it usually brings is not significant (Figure 3(c)). However, on some fine-grained datasets, the significant performance degradation caused by this instability is unacceptable (Figure 3(b)).

    &  &  \\   & Natural Acc & PGD-100 Acc & Natural Acc & PGD-100 Acc \\ 
2 & 71.60 & 19.00 & 82.60 & 56.90 \\
4 & 75.50 & 41.50 & 85.30 & 59.20 \\
6 & 77.50 & 49.50 & 84.40 & 61.10 \\
8 & 80.10 & 52.80 & 84.00 & 60.00 \\
10 & 82.20 & **58.00** & 84.90 & 60.00 \\
12 & **84.00** & 57.30 & **85.50** & **61.80** \\   

Table 4: Natural and robust performance (%) w.r.t. different prompt depth and length settings. Results are obtained in under 16-shot adversarial prompt learning on StanfordCars.

    &  &  &  &  \\   & & & & Natural Acc (\%) & PGD-100 Acc (\%) \\  AdvVP & 16-shot (1.25\%) & 0.07 & 0.65 & 41.96 & 12.97 \\ AdvVP & Entire (100\%) & 0.24 & 49.9 & 46.58 & 25.21 \\ 
**FAP** & 16-shot (1.25\%) & 0.42 & 0.71 & 48.18 & 25.06 \\
**FAP** & 32-shot (2.49\%) & 0.43 & 1.43 & **49.93** & **25.39** \\   

Table 2: Comparison with benchmark result  which adapts models on the entire ImageNet-1K. We report the average natural and robust accuracy across downstream datasets. Running time is computed on a single NVIDIA RTX A40 GPU.

    &  &  \\   & Base Nat Acc & Base Adv Acc & New Nat Acc & New Adv Acc \\ 
1.0 & **71.95** & 36.31 & **52.47** & 22.34 \\
1.5 & 70.60 & 39.15 & 51.79 & 23.65 \\
2.0 & 68.46 & 40.36 & 46.99 & 23.73 \\
2.5 & 68.44 & **41.38** & 48.49 & **23.90** \\
3.0 & 67.15 & 40.58 & 46.15 & 22.84 \\
3.5 & 66.49 & 39.04 & 41.57 & 20.64 \\   

Table 3: Adversarial base-to-new generalization performance (%) w.r.t. different \(\) values.

To understand this, we plot the loss curve during the training process under both stable and unstable settings. As revealed in Figure 5, in unstable cases, we observe that the robust loss drops to zero early in training and remains nearly unchanged at this low level during the mid-phase, while the overall loss does not decrease as expected. This suggests the text prompt falls into a trivial local solution during optimization, equating natural and adversarial logits. This nullifies the adversarial term but overlooks natural generalization, causing consistently high natural loss. This issue typically occurs when the natural and robust terms are balanced in a moderate ratio in the training objective.

We propose a minimal refinement to prevent instability: switching the deep prompt interaction to an image-to-text scenario. Here, the text prompt is derived from the image prompt projection, limiting its adaptability. This prevents the adversarial loss from reaching zero, thus avoiding the issue.

## 5 Conclusion

In this paper, we focus on adversarial prompt tuning on vision-language models, a domain with significant potential for zero-shot downstream adversarial robustness. We precisely reveal the issues of previous methods that perform adversarial visual prompts with static text supervision. Our method distinguishes itself by introducing learnable adversarial text supervision combined with a new training objective, facilitating effective learning in a few-shot setting. The proposed method enjoys excellent algorithmic properties and matches state-of-the-art performance, notably with reduced computational demand. We believe that this work can provide some insights to the community and stimulate further research in this area.

Figure 4: Instability analysis for DTD, OxfordPets, and Caltech101. We report the model performance (%) w.r.t the ratio (\(\)) between natural and robust terms in training objectives. The results of deep prompt interaction from text to image are plotted in red line, while that from image to text are plotted in blue line.

Figure 5: Training loss curve under both stable and unstable settings. We report the total, natural, and robust loss during the whole training stage.

    &  &  &  &  &  \\    & Consistency & & & & & \\  ✗ & TeCoA & ✗ & 57.96 & 30.10 & 43.73 & 19.01 \\  ✗ & TeCoA & ✗ & 48.18 & 26.57 & 36.52 & 16.41 \\  & JS & ✗ & 74.02 & 34.38 & 56.91 & 20.75 \\  & KL & ✗ & 71.20 & 37.70 & 49.52 & 21.18 \\  & KL & MSE & **77.73** & 20.34 & **64.73** & 15.90 \\  & KL & MAE & 74.02 & 30.56 & 57.41 & 17.59 \\  ✗ & KL & Cos & 70.60 & **39.15** & 51.79 & **23.65** \\   

Table 5: Ablation study of base-to-new generalization performance (%) w.r.t. different training objective design. Here, TeCoA, JS, KL, MAE, MSE and Cos stand for Text-image Contrastive Loss, Jensen-Shannon Divergence, Kullback-Leibler Divergence, Mean Absolute Error, Mean Squared Error and Cosine Similarity, respectively.