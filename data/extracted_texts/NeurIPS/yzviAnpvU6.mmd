# ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization

Xiaoxing Wang\({}^{*}\), Xiaohan Qin\({}^{*}\), Xiaokang Yang, Junchi Yan\({}^{}\)

Dept. of CSE & School of AI & Moe Key Lab of AI, Shanghai Jiao Tong University

{figure1_wxx, galaxy-1, xkyang, yanjunchi}@sjtu.edu.cn

* Equal contribution; \({}^{}\) Corresponding author. This work was partly supported by NSFC (92370201, 62222607) and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.

###### Abstract

Gradient estimation is critical in zeroth-order optimization methods, which aims to obtain the descent direction by sampling update directions and querying function evaluations. Extensive research has been conducted including smoothing and linear interpolation. The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables. This paper resorts to the linear interpolation strategy and proposes to reduce the complexity of gradient estimation by reusing queries in the prior iterations while maintaining the sample size unchanged. Specifically, we model the gradient estimation as a quadratically constrained linear program problem and manage to derive the analytical solution. It innovatively decouples the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity. Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency. Our code is available at https://github.com/Thinklab-SJTU/ReLIZO.git.

## 1 Introduction

Zeroth-order optimization (ZO) aims to estimate the gradients by only function evaluations and solve optimization problems by the descent method. It has been successfully applied to many fields, including science , finance , and artificial intelligence , where the optimization functions are usually black-box without available gradients w.r.t. the variables. Typical ZO methods iteratively perform three major steps : 1) estimate the gradient by sampling directions to update the variable and querying the function evaluation, 2) rectify the descent direction through momentum, element-wise sign operation, stochastic variance reduced algorithms, etc., and 3) update the variables according to the descent direction. Wherein, the first step, i.e. gradient estimation, is critical since it provides the essential direction to update variables, which have been explored by many recent works .

Gradient estimation methods in ZO can be roughly categorized into two groups : linear interpolation technique  and smoothing strategy . The former proposes to sample a set of linearly independent directions to form an invertible matrix and compute the gradient through linear interpolation. Though such methods can get better descent directions, the sample size of directions should be the same as the dimension of variables, leading to an intractable complexity of building an invertible matrix and computing the matrix inversion as the dimension increases. A recent work makes the sample size in linear interpolation strategy independent of the dimension of variables yet involves an extra orthogonal requirement of the sampled vectors. The latter group proposes to estimate the gradient by a sum of directional derivatives along random directions e.g. Gaussian and uniform on a unit sphere. Such methods are simple to implement and the sample size directions are independent of the variable dimension. However, it provides a gradient estimation of a smoothed version of the objective function , which differs from the original one with a bound w.r.t. a smoothing parameter . Additionally, the prior work  shows that smoothing methods require significantly more samples than linear interpolation strategies to obtain an accurate estimation The work  leverages information theory showing that under the Lipschitz continuous gradient assumption, ZO methods require sample complexities growing polynomially with the variable dimension.

Overall, the prior gradient estimation methods require large amounts of samples at each iteration to update variables. Nevertheless, in many real scenarios, one query of function evaluation can require large amounts of resources such as tasks in AutoML and reinforcement learning, which significantly restricts the acceptable sample size at each iteration and thus slow down the convergence of ZO methods. Consequently, it has been urgently demanded to reduce the number of queries while ensuring the convergence of ZO methods.

To this end, we delve into the ZO pipeline and observe that the magnitudes of steps to update variables are limited, making it possible to reuse the queries in the prior iterations. It is hoped that such a reusing strategy can be explored to significantly reduce the number of queries of function evaluation while maintaining the sample size unchanged at each iteration. However, it is hard to be directly applied to the prior gradient estimation methods. On one hand, for smoothing strategies, it is hard to guarantee the reused samples obey the demanded distribution, which will further increase the gradient estimation bias. On the other hand, methods based on linear interpolation techniques require a large computation complexity since building an invertible matrix needs to compute the null space of reused samples. Additionally, another work  requires orthogonal sampled vectors, which can hardly be satisfied by the prior samples, making it impossible to reuse the samples.

This work refers to Taylor's expansion and proposes to estimate gradients through a linear combination of limited direction samples and their queried function evaluations. We then model the gradient estimation as a quadratically constrained linear program (QCLP) and derive the analytical solution. Our method can reduce the required sample size (decoupled from the variable dimension) for linear interpolation strategies without the orthogonal condition required. Consequently, it supports to reuse the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed from the computation in the prior iterations, significantly reducing the computation complexity during the ZO process. Theoretical analysis is performed to show that our method can be regarded as a general version of the traditional linear interpolation strategy and has a similar convergence rate. In summary, our contributions lie in:

**1) We introduce to reuse the prior queries to speed up the ZO procedure.** Unlike recent works that cannot reuse the queries due to the requirements of sampled vectors to be orthogonal in linear interpolation strategies or obey a specific distribution in smoothing techniques, we introduce to estimate the gradient with an arbitrary number of sampled vectors without orthogonal condition, making it possible to leverage the queries in the prior iterations. To the best of our knowledge, this work is the first that attempts to reuse queries to speed up the ZO procedure.

**2) We model the gradient estimation in ZO as a QCLP and derive the analytical solution by Lagrange multiplier.** Theoretical analysis also proves that our method has a similar convergence rate as conventional linear interpolation techniques. Moreover, after combining our gradient estimator based on QCLP with the reusing strategy, part of the intermediate variables that contribute to the analytical solution can also be directly indexed from the computation in the prior iterations, significantly reducing the complexity when the sample size is much lower than the variable dimension. Overall, our method is more efficient than linear interpolation-based methods with a comparable accurate estimation of the gradient.

**3) We conduct extensive experiments to show the efficacy and efficiency of our method.** We first compare our method with recent ZO methods on the CUTEst, showing that it has faster convergence speed and better solutions. Ablation studies show that the performance drop is negligible even with more than \(50\%\) reuse rate. We then conduct experiments on the black-box adversarial attack task, showing that it has lower attack loss and \(5\%\) better attack success rate compared to peer ZO solverswith similar final \(_{2}\) distortion. Our method is further applied to the Neural Architecture Search (NAS) on NAS-Bench-201, outperforming other ZO solvers with \(39\%\) less number of queries.

## 2 Related Work

**Zeroth-order optimization.** There are typically two types of gradient estimation in ZO algorithms: one-point estimation [38; 13; 54] and multi-point estimation [1; 39; 22; 19]. Since multi-point estimation is more accurate, some works [33; 52] made further research on ZO convex problems. As for the non-convex setting, the prior works [39; 22] proposed ZO-GD and its stochastic counterpart ZO-SGD. ZO algorithms have been widely employed in various scenarios, such as black-box adversarial attacks [11; 27], AutoML  and transfer learning . It is worth noting that the gradient estimation is the most critical part of ZO solvers, which is the focus of the above methods.

**Gradient estimation via smoothing strategy.** Methods based on the smoothing strategy [3; 36] estimate gradients by averaging over several directional derivatives by sampling the direction from either a Gaussian distribution  or a uniform distribution  on a unit sphere. Referring to the prior smoothing strategy based ZO algorithms [39; 22], ZO-signSGD  yields faster convergence but only guarantees to converge to a neighborhood of a solution, and ZO-AdaMM  uses adaptive momentum to enhance the optimization.

**Gradient estimation via linear interpolation.** Methods based on linear interpolation [15; 4] estimate gradient by solving linear programming. Conn et al.  derived the error bounds between the true function value and interpolating polynomials, including linear and quadratic approximation models. Berahas et al. showed that linear interpolation gives better gradients than Gaussian smoothing in derivative-free optimization. The prior work  derived bounds on the sample size and the sampling radius, showing that smoothing strategy requires significantly more samples to obtain an accurate estimation than linear interpolation. Kozak et al. approximated the gradient by finite differences computed in a set of orthogonal random directions, which is a special case of linear interpolation. This work also focuses on the scope of linear interpolation due to its more accurate estimation of gradient than smoothing techniques.

**Remarks.** Our approach has few constraints on the sample size, similar to the smoothing techniques. Meanwhile, we estimate the gradients by solving a linear program problem, similar to the linear interpolation strategies. Overall, our approach enjoys the efficiency of smoothing techniques while maintaining estimation accuracy.

## 3 The Proposed ReLIZO Method

Similar to the prior ZO methods [22; 12], we consider the problem of finding a point \(x^{*}^{d}\) such that \(f(x^{*})=_{x}f(x)\). The function \(f:^{d}\) is black-box and satisfies the following assumptions:

**Assumption 3.1**.: \(0<f(x^{0})-f(x^{*})<\), where \(x^{0}\) is the initial point.

**Assumption 3.2**.: The function \(f\) is continuously differentiable, and \( f\) is \(L\)-Lipschitz continuous for \( x^{d}\).

### Rethinking Gradient Estimation as QCLP

Consider the Taylor's expansion of function \(f\):

\[f(y)=f(x)+ f(x)^{}(y-x)+o(y-x).\] (1)

The remainder can be omitted in many scenarios. For large-scale applications it can be hard to compute the number of function evaluations required to construct quadratic models.

Given \(n\) random vectors as candidate updates to the variable \(_{x}^{n d}\), we denote the i-th row of it as \(_{x}^{i}\). The corresponding function evaluation \(\{f(x+_{x}^{i})\}_{i=1}^{n}\) can be queried and compose \(_{f}^{n}\), whose entries \(_{f}^{i}=f(x+_{x}^{i})-f(x), i=1,...,n\). According to Eq. 1, we can have the equations as follows:

\[_{x} f(x)_{f}.\] (2)In this work, we mainly focus on the case where \(n<d\), hoping to obtain approximately accurate gradients with fewer queries. To this end, we estimate the direction of gradient \( f(x)\) as \(g(x)\) by linearly combining the \(n\) sampled vectors \(\{_{x}^{i}\}_{i=1}^{n}\), yielding \(g(x)=_{x}^{}\), where \(^{n}\) represents the coefficients of the linear combination. The inner product of normalized \(g(x)\) and \( f(x)\) increases when their directions are getting closed, which can be formulated as:

\[*{arg\,max}_{g} f(x)}{\|g(x)\|}=_{x} f(x)}{_{x}_{x}^{ }}}.\] (3)

Substitute Eq. 2 into Eq. 3 and define \(A_{x}=_{x}_{x}^{}^{n n}\), We can model the estimation of gradient direction as a quadratically constrained linear program (QCLP):

\[*{arg\,max}_{}\ ^{}_{f},\ ^{}A_{x}=1.\] (4)

We employ the Lagrange multiplier method to address the QCLP problem (4). The Lagrangian function is defined as: \(L(,)=^{}_{f}-(^{}A_{x}-1)\), where \(\) is the Lagrange multiplier. Taking partial derivatives w.r.t. \(\) and setting it to zero, we have \(=_{f}-2 A_{x}=0\) and \(A_{x}=_{f}\), yielding the direction of \( A_{x}^{-1}_{f}\). Consequently, we obtain the estimated direction of the gradient:

\[g(x)=_{x}^{}_{x}^{}A_{x}^{-1}_{f}.\] (5)

Finally, we initialize the magnitude of the descent direction \(_{k}\) with learning rate \(\) and adopt the line search strategy to update \(_{k}\) to satisfy the Armijo-Goldstein condition [2; 23].

### Reusing the Queries in the Prior Iterations

Unlike recent works unable to reuse the queries due to the requirements of sampled vectors to be orthogonal in linear interpolation strategies  or to obey a specific distribution in smoothing techniques , our method has few constraints on the sampled vectors \(_{x}\), making it possible to reuse the queries in the prior iterations.

Suppose \(x_{k-1}\) is the variable and \(_{x_{k-1}}\) is the \(n\) random vectors at \((k-1)\)-th iteration. We have the queried function values at points \(\{x_{k-1}+_{x_{k-1}}^{i}\}_{i=1}^{n}\). The estimated descent direction is \(g(x_{k-1})\) with step \(_{k-1}\). After updating the variable at \((k-1)\)-th iteration, we have \(x_{k}=x_{k-1}-_{k-1}g(x_{k-1})\). Since \(f(x_{k-1})\) has also been queried, we have \(n+1\) queried points and can collect \(n+1\) random vectors for \(x_{k}\) with known queries at \(k\)-th iteration as:

\[_{x_{k}}=\{-_{k-1}g(x_{k-1})\}\{-_{k-1}g(x_{k-1 })-_{x_{k-1}}^{i}\}_{i=1}^{n}.\] (6)

Distances between \(x_{k}\) and the prior \(n+1\) queried points are \(\|x_{k}-x_{k-1}\|=\|_{k-1}g(x_{k-1})\|\) and \(\|x_{k}-(x_{k-1}+_{x_{k-1}}^{i})\|=\|_{k-1}g(x_{k-1})+_{x_{k -1}}^{i}\|, i=\{1,...,n\}\).

According to Taylor's expansion in Eq. 1, the remainder \(o(y-x)\) will be non-negligible as \(\|y-x\|\) increases. Therefore, we introduce a reusable distance bound \(b\) to filter out the samples far away from the current point \(x_{k}\), i.e., the reused vectors are obtained by:

\[_{x_{k}}^{r}=\{_{x_{k}}\|\|<b\}.\] (7)

After getting the reusable queries, we then randomly sample another \(n-|_{x_{k}}^{r}|\) vectors \(\{p_{j}\}\) to build \(_{x_{k}}^{n d}\) for gradient estimation at \(k\)-th iteration:

\[_{x_{k}}=_{x_{k}}^{r}\{p_{j}\}_{j=1}^{n-|_{x_{k}}^{r}|}.\] (8)

Overall, in the \(k\)-th iteration, only \(n-|_{x_{k}}^{r}|\) new queries should be conducted. The ablation study in Sec. 4.2 shows that as the sample size increases, even reusing 80% prior queries can also find good solutions.

**Discussion on the reusable distance bound \(b\):** ZO methods estimate the directional derivative by difference and restrict the distances between the samples and current point to a small value, which has the same order as the stepsize (initial learning rate) \( O()\) as previous works (Table 1 in ). In our method, the reusable distance bound \(b\) restricts the distances between the reusable samples and current point, which should has the same order of magnitude as the stepsize. Then we choose \(b O()\). Ablation studies in Fig. 2 show that \(b=2\) works pretty well in different optimization tasks with different sample sizes \(N\), thus we set \(b=2\) by default.

[MISSING_PAGE_EMPTY:5]

Consequently, the complexity of computing \(A_{x_{k}}\) can be reduced to \(O(n_{k}n+(n-n_{k})nd)\), which decreases as the reuse rate \(}{n}\) increases. The overall computation complexity is analyzed and compared with other ZO methods in Appendix B. The algorithm flow is shown in Alg. 1.

### Convergence Analysis

We start the analysis with a lemma that estimates the distance between the estimated gradient in our method and the projected exact gradient of the function.

**Lemma 3.1**.: _Let \(f\) satisfies Assumption 3.1 and 3.2. For every \(k\), \(_{x_{k}}^{n d}\) and \(\|_{x_{k}}^{i}\|<b\):_

\[\|_{x_{k}}g(x_{k})-_{x_{k}} f(x_{k})\|}{ 2}.\] (12)

Then we can derive the distance bound between the estimated gradient and the exact gradient of the function.

**Lemma 3.2**.: _Let the SVD of \(_{x_{k}}=U V^{}\), \(A_{x_{k}}^{-1}=(_{x_{k}}_{x_{k}}^{})^{-1}\), \(_{k}\) denotes the spectral radius of \(A_{x_{k}}^{-1}\). Based on Lemma 3.1, we have:_

\[\|g(x_{k})- f(x_{k})\|^{2} r\| f(x_{k})\|^{2}+L ^{2}b^{2}n}{4},\] (13)

_where \(r=^{d}v_{j}^{2}}{\| f(x_{k})\|^{2}}<1, v=V^{}  f(x_{k})\)._

Consequently, we can derive the following proposition:

**Proposition 3.3**.: _Suppose f is convex and \(\;[0,1)\) satisfying \(_{k}-r)\| f(x)\|^{2}}{L^{2}b^{2}n}\), we have:_

\[f(x_{k})-f(x^{*})} O(),\] (14)

which yields a sublinear convergence. Proof can be found in Appendix A. Furthermore, our experimental results demonstrate that our method can converge in many scenarios, including simulation benchmarks and real-world applications.

### Relationship with Linear Interpolation Methods

We show that our method can be regarded as a general version of the linear interpolation strategy.

First, consider the situation where \(n=d\) and the matrix \(_{x}\) is nonsingular, the estimated gradient by our method in Eq. 5 and be simplified as \(g(x)_{x}^{}A_{x}^{-1}_{f}=_{x}^{-1}_{f}\), which has a similar formulation to the gradient estimated by linear interpolation strategy .

Second, consider a more general situation where \(n<d\), the matrix \(_{x}\) can be singular, and its singular value decomposition (SVD) is denoted as \(_{x}=U V^{}\). Then \(A_{x}=_{x}_{x}^{}=U^{}U^{}\), and the estimated gradient by our method in Eq. 5 can be transformed as:

\[g(x)_{x}^{}A_{x}^{-1}_{f}=V^{}U^{}(U ^{}U^{})^{-1}_{f}=V^{}(^{ })^{-1}U^{}_{f}.\] (15)

Since \(U\) and \(V\) are orthonormal, we have \(_{x}(V^{}(^{})^{-1}U^{})_{x}= _{x}\), showing that \(V^{}(^{})^{-1}U^{}\) is the pseudo-inverse of \(_{x}\). Consequently, our method can be regarded as a general version of the linear interpolation strategy.

Moreover, in the spacial case where \(_{x}\) is orthonormal, \(^{}=\) is an identity matrix, and Eq. 15 can be simplified as \(g(x) V^{}U^{-1}_{f}=_{x}^{}_{f}\), consistent with the gradient estimation in .

## 4 Experiment

### Protocols

To demonstrate the efficacy and efficiency of our method, we conduct experiments on both simulation benchmarks and real-world scenarios. First, we test on the CUTEst , one of the most popular benchmarks for derivative-free optimization containing over 1,500 problems, many of which are parametrized to allow for variable dimensions through user-selectable parameters. Specifically, we utilize the PyCUTEst , a Python interface, to import the optimization problems from the CUTEst and solve them by multiple ZO solvers implemented ourselves on the PyTorch platform. Second, we conduct experiments on the black-box adversarial attack task, a real-world application for ZO solvers. The task attempts to search additive noises for each pixel of inputs to confuse a pretrained neural network, which is one of the most popular scenarios for ZO solvers. Third, we apply our method to the Neural Architecture Search (NAS) task, aiming to search for the best neural architecture. Since querying once in NAS task consumes large amounts of resources, we conduct experiments on NAS-Bench-201 , which can directly query the performance of 15,625 architectures on three datasets including CIFAR10, CIFAR100, and ImageNet-16-120. Finally, we conduct experiment on large-scale neural network training by fine-tuning large language models (LLMs), demonstrating the applicability of our proposed ReLIZO.

### Experiments on the Simulation Benchmark

We compare the performance of our method with baselines on four problems from the CUTEst optimization benchmark, containing an ill-conditioned or singular type problem (BOXPOWER), a nonlinear optimization test problem (SROSENBR), and a problem coming from application scenarios (COATING). The dimension of variables ranges from 50 to 1000 to evaluate the effectiveness of our method on both low-dimension and high-dimension problems. The baselines include ZO-SGD , ZO-signSGD  and ZO-AdaMM  implemented by ourselves on the PyTorch platform. Each solver updates the variables 500 times and samples 8 random directions at each iteration to update the variables. We also utilize grid search to obtain the best learning rate for each problem. The candidate learning rate \(\) ranges from {0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05}. As for our method, the total sample size at each iteration is set as 8, and the reusable distance bound \(\) is set as \(2\), where \(\) is the learning rate obtained by the grid search. The variables are initialized as the default values in CUTEst for all solvers. Experiments at each setting are repeated three times, and the results are shown in Fig. 1. We observe that our method has a faster convergence speed and achieves better solutions to different problems. Moreover, the standard deviation of our method among three replications (shadows in Fig. 1) is much lower than other methods, showing that our method can stably find good solutions.

**Ablation Study about sample size \(N\) and reusable distance bound \(b\).** To evaluate the impact of sample size at each iteration and reusable distance bound, we run our method for 500 iterations on different sample sizes ranging from {6, 8, 10, 20, 50, 100, 200} and reusable distance bounds ranging from {0, \(\), \(2\), \(5\)}, where \(\) is the learning rate at each iteration. Note that we set the sample size smaller than the dimension of variables. Results are shown in Fig. 2. The x-axis indicates the sample size, the y-axis shows the optimal value found by our solver after 500 iterations, and colors indicate different reusable distance bounds. For each setting, we conduct three replication experiments and plot the average performance and standard deviation as the nodes and lines. We also illustrate the reusing rates = \(}{}\) in Fig. 2 as the floats in the boxes beside the scatter

Figure 1: Illustration of the optimization procedure of different solvers on four problems. Each solver conducts 500 iterations to update the variables. The dimension of variable \(d\) is shown in the bracket in the title of each figure. We run each solver at each setting three times and plot the average performance and standard deviation as the line and shadow.

results, we have three observations: **1)** A larger reusable distance bound \(b\) has a larger reusable rate, and a larger sample size will also lead to a larger reusable rate. **2)** As the sample size increases, large reusable distance bound can also find good solutions, demonstrating the rationality of reusing queries. **3)** the standard deviation of performance is pretty small when \(N>6\) and \(b<5\), showing that it can stably find good solutions. Experiments and analysis on extreme conditions (e.g. small \(N\) and large \(d\)) in Appendix C.1 also demonstrate the robustness and scalability of ReLIZO.

### Experiments on Black-Box Adversarial Attacks

ZO methods are popular solutions to black-box adversarial attacks, and we conduct experiments to show the effectiveness of our ReLIZO. We focus on universal adversarial perturbation against images  and attack a well-trained DNN model on the CIFAR-10 dataset. Supported by  and , we consider the optimization problem as: \(*{arg\,min}_{}_{i=1 }^{M}f(_{i}+)+\|\|_{2}^{2}( _{i}+)[-0.5,0.5]^{d}, i\), where \(f(_{i}+)\) denotes the black-box attack loss for image \(i\), \(>0\) is a regularization hyperparameter between minimizing the attack loss and the \(_{2}\) distortion. We normalize the pixel values to \([-0.5,0.5]^{d}\), and we specify the loss function for untargeted attack as \(f(x^{})=_{j i}\{Z(x^{})_{t}-_{j t}Z(x^{ })_{j}-\}\), where \(Z(x^{})_{k}\) denotes the prediction score of class \(k\) given the input \(x^{}\), and \(>0\) governs the gap between the confidence of the predicted label and the true label \(t\). In experiments, we set \(=0\), and the attack loss \(f\) reaches the minimum value 0 as the perturbation succeeds to fool the network.

We conduct experiments on the CIFAR-10 test dataset. Similar to the settings in ZO-AdaMM , we randomly select 100 images, ensuring that these images are initially correctly classified by the model. We conduct adversarial attacks on these selected images and compare the performance of our proposed ReLIZO method with 4 existing methods: ZO-SGD , ZO-signSGD , ZO-SMD , and ZO-AdaMM . We conduct a linear search for the learning rate of each approach to maintain consistency in the \(_{2}\) distortion when employing each method for the attack loss.

   Methods & Attack success rate & Final \(_{2}\) distortion \\  ZO-SGD & 77\% & 1.34 \\ ZO-signSGD & 80\% & 1.36 \\ ZO-SMD & 85\% & 1.38 \\ ZO-AdaMM & 87\% & 1.37 \\
**ReLIZO** & **92\%** & 1.37 \\   

Table 1: Summary of attack success rate using different zeroth-order optimization methods with the final \(_{2}\) distortion remains relatively consistent. The result is based on a universal attack against 100 images under \(T=20000\) iterations.

Figure 3: Attack loss of universal attack.

Figure 2: Illustration of the best function value found by our method in 500 iterations under different settings of sample size \(N\) and reusable distance bound \(b\). We conduct three replication experiments at each setting and plot the average performance and standard deviation as nodes and lines. We also illustrate the reusing rates as the floats in the boxes beside the scatters.

tacks, and compared the loss and the attack success rate on this basis. Fig. 3 illustrates the overall loss curves, while Table 1 presents the success rates of the adversarial attacks for each method over \(T=20000\) iterations. As we can see, our ReLIZO method exhibits the fastest rate of loss reduction compared to other ZO methods. Simultaneously, it achieves the highest attack success rate (more than 5% improvement) under condition where the \(_{2}\) distortion remains relatively consistent. This result fully demonstrates the effectiveness of our method.

### Experiments on Neural Architecture Search

AutoML, including neural architecture search [32; 40; 51; 50; 49; 46; 53] and hyper-parameter optimization tasks [7; 43; 26; 48], has attracted wide attention due to its efficacy in reducing labor costs. Zero-order optimization is an effective algorithm for solving AutoML tasks , which can be modeled as bi-level optimization. Table 2 reports the performance of our method on the NAS-Bench-201, a popular benchmark in the NAS task [32; 17; 51]. The first block shows the performance of gradient-based methods which is obtained from the paper . The second block shows the performance of various ZO methods, including Zo-AdMM, ZO-SGD, ZO-signSGD and ours. Specifically, similar to DARTS, we iteratively train the operation weights by the SGD optimizer and train the architecture parameters by the ZO optimizer for 50 epochs on the CIFAR10 dataset. To query the performance of one sampled direction, we update the architecture parameters by the direction and return the loss after training the operation weights for 10 iterations. The accuracy of discovered architectures on three datasets is directly indexed from the benchmark. For each ZO optimizer, we conduct three replication experiments and report the averaged accuracy and standard deviation in Table 2. We observe that our method surpasses other ZO methods in performance. We also report the number of queries of each ZO optimizer during the whole search process and the search time in Table 3. Our method is much faster than other ZO optimizers with fewer queries. Notice that the queries in Table 3 are used to train the architecture parameters, while the cost consists of training operation weights and architecture parameters, making the reduction of cost lower than the reduction of queries.

### Experiments on Parameter Efficient Fine-tuning

Compared to gradient-based methods, ZO methods are well-suited for black-box optimization problems where gradients with respect to the variables are unavailable. Additionally, ZO methods are more memory-efficient than gradient-based methods since they do not require a backward pass,

    &  &  &  \\   & **valid** & **test** & **valid** & **test** & **valid** & **test** \\  ENAS  & 37.51\(\)3.19 & 53.89\(\)0.58 & 13.37\(\)2.35 & 13.96\(\)2.33 & 15.06\(\)1.95 & 14.84\(\)2.10 \\ RSPS  & 80.42\(\)3.58 & 84.07\(\)3.61 & 52.12\(\)5.55 & 52.31\(\)5.77 & 27.22\(\)3.24 & 26.28\(\)3.09 \\ DARTS-V1  & 39.77\(\)0.00 & 54.30\(\)0.00 & 15.03\(\)0.00 & 15.61\(\)0.00 & 16.43\(\)0.00 & 16.32\(\)0.00 \\ DARTS-V2  & 39.77\(\)0.00 & 54.30\(\)0.00 & 15.03\(\)0.00 & 15.61\(\)0.00 & 16.43\(\)0.00 & 16.32\(\)0.00 \\ GDAS  & 89.89\(\)0.08 & 93.61\(\)0.09 & 71.34\(\)0.04 & 70.70\(\)0.30 & 41.59\(\)1.33 & 41.71\(\)0.98 \\ SETN  & 84.04\(\)0.28 & 87.64\(\)0.00 & 58.86\(\)0.06 & 59.05\(\)0.24 & 33.06\(\)0.02 & 32.52\(\)0.21 \\  ZO-AdaMM  & 85.97\(\)2.28 & 89.59\(\)2.25 & 63.24\(\)4.68 & 63.28\(\)4.86 & 35.43\(\)4.07 & 35.63\(\)4.73 \\ ZO-SGD  & 88.18\(\)1.26 & 91.60\(\)1.15 & 67.59\(\)1.51 & 67.49\(\)1.05 & 39.83\(\)1.80 & 39.79\(\)1.83 \\ ZO-signSGD  & 85.94\(\)4.37 & 89.74\(\)3.64 & 61.89\(\)8.82 & 62.41\(\)8.91 & 35.23\(\)6.82 & 35.28\(\)7.51 \\
**ReLIZO (ours)** & 89.50\(\)1.53 & 92.45\(\)1.84 & 69.00\(\)2.27 & 69.03\(\)1.98 & 42.09\(\)3.55 & 42.31\(\)3.87 \\   

Table 2: Top-1 test classification accuracy (%) on NAS-Bench-201. The first block shows the performance of gradient-based methods quoted from the paper of NAS-Bench-201. The second block shows the performance of various ZO methods, which are implemented by ourselves on the PyTorch platform. The performance of the methods based on ZO optimizers is averaged over three independent trials.

 
**Methods** & ZO-AdaMM & ZO-SGD & ZO-signSGD & **ReLIZO** \\ 
**\#Queries** & 19550 & 19550 & 19550 & 11885 \\
**Cost (s)** & 63044 & 63429 & 69659 & 55005 \\  

Table 3: Number of queries and the search cost (seconds) during the NAS procedure on the search space of NAS-Bench-201.

making them applicable to network training, as demonstrated by recent works [10; 55]. Specifically, DeepZero  introduces a principled ZO framework for deep learning that is computational-graph-free and can scale to deep neural network training with performance comparable to first-order methods. The work  also applies ZO methods to large language model (LLM) fine-tuning, highlighting their memory efficiency and introducing a ZO-LLM benchmark.

To illustrate the applicability of ReLIZO in network training, we adopt it for fine-tuning an OPT-1.3b model (with 1.3 billion parameters) on the Stanford Sentiment Treebank v2 (SST2) task with a batch size 128, following the methodology of ZO-LLM . The results, shown in Table 4 indicate that ReLIZO outperforms other ZO methods across various fine-tuning schemes, including full parameter fine-tuning (FT), LoRA, Prefix-tuning, and Prompt-tuning. Notably, ReLIZO even surpasses SGD in the FT scheme while requiring significantly less memory, demonstrating its promising potential.

## 5 Conclusion, Limitations and Future Work

This work handles gradient estimation in zeroth-order optimization by modeling it as a quadratically constrained linear program problem, with analytical solution derived via the Lagrange multiplier method. Our method ReLIZO, decouples the required sample size from the variable dimension without the requirement of orthogonal condition in the recent linear interpolation works, making it possible to leverage the queries in the prior iterations. We also show that part of the intermediate variables contributing to the gradient estimation can be directly indexed, significantly reducing the computation complexity. We further perform a theoretical analysis of the convergence of ReLIZO and show the relationship with the prior linear interpolation methods. The efficacy and efficiency have been shown in extensive experiments, including simulated benchmarks, NAS, and black-box adversarial attack tasks.

**Limitations and Future work.** Compared to the gradient-based methods, ZO methods require multiple queries of function evaluation at each iteration, which restricts their applications in real-world scenarios such as neural network training. Our ReLIZO can reduce the required number of queries by the reusing strategy, potentially broadening the scope of applications of ZO, which can be explored in future work. However, in cases where the consumption of one query is negligible, the profit due to the reusing technique decreases, causing a relative increase in the cost of solving the QCLP. We leave these problems in future work.

## Impact Statements

This paper presents work whose goal is to advance the field of Machine Learning. Note that zeroth-order optimization is an effective method for black-box adversarial attacks as we have shown in Section 4.3. Its purpose is to evaluate the robustness of machine learning models, but we should also be aware of the impact this may have on AI safety. There are many potential societal consequences of our work, yet none of which we think must be specifically highlighted here.

    &  &  &  &  \\   & **Acc** & **Memory** & **Acc** & **Memory** & **Acc** & **Memory** & **Acc** & **Memory** \\  SGD & 91.1 & 44.1 GB & 93.6 & 44.1 GB & 93.1 & 42.9 GB & 92.8 & 50.6 GB \\  ZO-SGD & 90.8 & 28.7 GB & 90.1 & 19.7 GB & 91.4 & 20.7 GB & 84.4 & 18.8 GB \\ ZO-SGD-Sign & 87.2 & 31.4 GB & 91.5 & 19.7 GB & 89.5 & 20.7 GB & 72.9 & 18.8 GB \\ ZO-Adam & 84.4 & 31.4 GB & 92.3 & 19.7 GB & 91.4 & 20.7 GB & 75.7 & 18.8 GB \\
**ReLIZO (ours)** & **93.4** & 35.7 GB & **93.1** & 19.9 GB & **91.8** & 20.7 GB & **90.1** & 18.9 GB \\   

Table 4: Zero-order optimizers with different parameter efficient fine-tuning methods on OPT-1.3b model (with 1.3 billion parameters) on the Stanford Sentiment Treebank v2 (SST2) task.