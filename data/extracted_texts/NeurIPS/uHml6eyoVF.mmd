# Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks

Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks

Eszter Szekely

Current address: CCFE, Culham Science Centre, Abingdon, Oxon, OX14 3DB, UK These authors contributed equally.

Lorenzo Bardone

These authors contributed equally.

Federica Gerace

Current address: Dipartimento di Matematica, Universita' di Bologna, Bologna (BO), Italy

Sebastian Goldt

Correspondence to: {eszekely, lbardone, fgerace, sgoldt}@sissa.it International School of Advanced Studies (SISSA)

Trieste, Italy

###### Abstract

Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-\(p 4\) cumulants of \(d\)-dimensional inputs. We first discuss the fundamental statistical and computational limits of recovering the spike by analysing the number of samples \(n\) required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. Existing literature established the presence of a wide statistical-to-computational gap in this problem. We deepen this line of work by finding an exact formula for the likelihood ratio norm which proves that statistical distinguishability requires \(n d\) samples, while distinguishing the two distributions in polynomial time requires \(n d^{2}\) samples for a wide class of algorithms, i.e. those covered by the low-degree conjecture. Numerical experiments show that neural networks do indeed learn to distinguish the two distributions with quadratic sample complexity, while "lazy" methods like random features are not better than random guessing in this regime. Our results show that neural networks extract information from higher-order correlations in the spiked cumulant model efficiently, and reveal a large gap in the amount of data required by neural networks and random features to learn from higher-order cumulants.

## 1 Introduction

Discovering statistical patterns in high-dimensional data sets is the key objective of machine learning. In a classification task, the differences between inputs in different classes arise at different statistical levels of the inputs: two different classes of images will usually have different means, different covariances, and different higher-order cumulants (HOCs), which describe the non-Gaussian part of the correlations between three or more pixels. While differences in the mean and covariance allow for rudimentary classification, Refinetti _et al._ recently highlighted the importance of HOCs for the performance of neural networks: when they removed the HOCs per class of the CIFAR10 training set, the test accuracy of various deep neural networks dropped by up to 65 percentage points. The importance of higher-order cumulants (HOCs) for classification in general and the performance of neural networks in particular raises some fundamental questions: what are the fundamental limits of learning from HOCs, i.e. how many samples \(n\) ("sample complexity") are required to extract information from the HOCs of a data set? How many samples are required when using a _tractable_ algorithm? And how do neural networks and other machine learning methods like random features compare to those fundamental limits?

In this paper, we study these questions by analysing a series of binary classification tasks. In one class, inputs \(x^{d}\) are drawn from a normal distribution with zero mean and identity covariance. These inputs are therefore isotropic: the distribution of the high-dimensional points projected along a unit vector in any direction in \(^{d}\) is a standard Gaussian distribution. Furthermore, all the higher-order cumulants (of order \(p 3\)) of the inputs are identically zero. Inputs in the second class are also isotropic, except for one special direction \(u^{d}\) in which their distribution is different. This direction \(u\) is often called a "spike", and it can be encoded in different cumulants: for example, we could "spike" the covariance by drawing inputs from a Gaussian with mean zero and covariance \(+ uu^{}\); the signal-to-noise ratio \(>0\) would then control the variance of \(= u,x\). Likewise, we could spike a higher-order cumulant of the distribution and ask: what is the minimal number of samples required for a neural network trained with SGD to distinguish between the two input classes? This simple task serves as a proxy for more generic tasks: a neural network cannot be able to _extract_ information from a given cumulant if it cannot even recognise that it is different from an isotropic one.

We can obtain the fundamental limits of detecting spikes at different levels of the cumulant hierarchy by considering the hypothesis test between a null hypothesis (the isotropic multivariate normal distribution with zero mean and identity covariance) and an alternative "spiked" distribution. We can then compare the sample complexity of neural networks to the number of samples necessary to distinguish the two distributions using unlimited computational power, or efficiently using algorithms that run in polynomial time. A second natural comparison for neural networks are random features or kernel methods. Since the discovery of the neural tangent kernel , and the practical success of kernels derived from neural networks [3; 4], there has been intense interest in establishing the advantage of neural networks with _learnt_ feature maps over classical methods with _fixed_ feature maps, like kernel machines. The role of higher-order correlations for the relative advantage of these methods has not been studied yet.

In the following, we first introduce some fundamental notions around hypothesis tests and in particular the low-degree method [5; 6; 7; 8], which will be a key tool in our analysis, using the classic spiked Wishart model for sample covariance matrices [9; 10]. For spiked cumulants, the existing literature (see section 1.1 for a complete discussion) establishes the presence of a wide statistical-to-computational gap in this model. Our **main contributions** are then as follows:

* We deepen the understanding of the statistical-to-computational gap for learning from higher-order correlations on the statistical side by showing that at _unbounded computational power_, a number of samples _linear_ in the input dimension is required to reach _statistical distinguishability_. We prove this by explicitly computing the norm of the _likelihood ratio_, see theorem 2.
* On the algorithmic side, SQ bounds and previous low-degree analyses [11; 12], showed that the sample complexity of learning from HOCs is instead _quadratic_ for a wide class of polynomial-time algorithms (section 3.2). Here, we provide a different, more direct proof of such a bound.
* Using these fundamental limits on learning from HOCs as benchmarks, we show numerically that neural networks learn the mixed cumulant model efficiently, while random features do not, revealing a large separation between the two methods (sections 4.1 and 4.2).
* We finally show numerically that the distinguishability in a simple model for images  is precipitated by a cross-over in the behaviour of the higher-order cumulants of the inputs (section 4.3).

### Further related work

Detecting spikes in high-dimensional dataThere is an enormous literature on statistical-to-computational gaps in the detection and estimation of variously structured principal components of high-dimensional data sets. This problem has been studied for the Wigner and Wishart ensembles of random matrices [9; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24] and for spiked tensors [25; 26; 27; 28; 29; 30; 31; 32; 33]. For comprehensive reviews of the topic, see refs. [34; 35; 36; 37]. While the samples in these models are often non-Gaussian, depending on the prior distribution over the spike \(u\), the spike appears already at the level of the covariance of the inputs. Here, we instead study a high-dimensional data model akin to the one used by Wang & Lu  to study online independent component analysis (ICA). This data model can be interpreted as having an additional whitening step to pre-process the inputs, which is a common pre-processing step in ICA , hence inputs have identity covariance even when cumulants of order \(p 3\) are spiked. Wang & Lu  proved the existence of a scaling limit for online ICA, but did not consider the sample complexity of recovering the spike/distinguishing the distributions, which is the focus of this paper.

NGCA, Gaussian pancakes and low-degree polynomialsRelated models have been introduced under the name of _Non-Gaussian Component Analysis_ (NGCA) , and studied from the point of view of _statistical query_ (SQ) complexity in a sequence of papers [41; 42; 43; 44; 45; 46]. In particular  nicknames _Gaussian pancakes_ a class of models that includes the _spiked cumulant model_ presented here. The SQ bounds found in  and refined in the subsequent works (see Diakonikolas _et al._ and references therein) could be used, together with SQ-LDLR equivalence , to provide estimates on low-degree polynomials. Finally,  proves very general bounds on LDLR norm; our theorem 5 provides an alternative derivation of these bounds, in a setting that is closer to the experiments in section 4.

Separation between neural networks and random featuresThe discovery of the neural tangent kernel by Jacot _et al._ and the flurry of results on linearised neural networks [2; 3; 49; 50; 51; 52; 53; 54; 55] has triggered a new wave of interest in the differences between what neural networks and kernel methods can learn efficiently. While statistical separation results have been well-known for a long time , recent work focused on understanding the differences between random features and neural networks _trained by gradient descent_ both with wide hidden layers [57; 58; 59; 60; 61; 4] or even with just a few hidden neurons [62; 63]. At the heart of the data models in all of these theoretical models is a hidden, low-dimensional structure in the task, either in input space (for mixture classification) or in the form of single- or many-index target functions. The impact of higher-order input correlations in mixture classification tasks on the separation between random features and neural networks has not been directly studied yet.

ReproducibilityWe provide code for all of our experiments, including routines to generate the various synthetic data sets we discuss, on GitHub https://github.com/eszter137/data-driven-separation.

## 2 The data models

Throughout the paper, we consider binary classification tasks where high-dimensional inputs \(x^{}=(x^{}_{i})^{d}\) have labels \(y^{}= 1\). The total number of training samples is \(2n\), i.e. we have \(n\) samples per class. For the class \(y^{}=-1\), inputs \(x^{}=z^{}\), where \(z^{}_{ iid}(0,_{d})\) and \(\) denotes the normal distribution. For the class \(y^{}=1\) instead, we consider "spiked" input models as follows.

The Gaussian caseThe simplest spiked model is the spiked Wishart model from random matrix theory [9; 10], in which

\[x^{}=}g^{}u+z^{}, g^{}_{ iid} (0,1),\] (1)

where \(u=(u_{i})\) is a \(d\)-dimensional vector with norm \(\|u\|=\) whose elements are drawn element-wise i.i.d. according to some probability distribution \(_{u}\). In this model, inputs are Gaussian and indistinguishable from white noise except in the direction of the "spike" \(u\), where they have variance \(1+\), where \(>0\) is the _signal-to-noise ratio_. By construction, all higher-order cumulants are zero.

The spiked cumulant modelTo study the impact of HOCs, we draw inputs in the "spiked" class from the data model used by Wang & Lu  to study online independent component analysis (ICA). First, we sample inputs

\[^{}=}g^{}u+z^{}, g^{}_{ }p_{g},\] (2)

as in the Wishart model, but crucially sample the latent variables \(g^{}\) from some non-Gaussian distribution \(p_{g}(g^{})\), say, the Rademacher distribution; see assumption 1 for a precise statement. For any non-Gaussian \(p_{g}\), the resulting inputs \(^{}\) have a non-trivial fourth-order cumulant proportional to \(_{q}^{q}u^{ 4}\), where \(_{q}^{q}(g^{})^{4}-3(g^{ })^{2}\) is the fourth cumulant of the distribution of \(g^{}\). We fix the mean and variance of \(p_{g}\) to be zero and one, respectively, so the covariance of inputs has a covariance matrix \(=_{d}+}{{duu^{}}}\). To avoid trivial detection of the spike from the covariance, the key ingredient of the spiked cumulant model is that we whiten the inputs in that class, so that the inputs are finally given by

\[x^{}=S^{}, S=-}}{d},\] (3)

with the whitening matrix \(S\) (see appendix B.4.3). Hence inputs \(x^{}\) are isotropic Gaussians in all directions except \(u\), where they are a weighted sum of \(g^{}\) and \( z,u\). The whitening therefore changes the interpretation of \(\): rather than being a signal-to-noise ratio, as in the spiked Wishart model, here \(\) controls the quadratic interpolation between the distributions of \(g^{}\) and \(z\) in the direction of \(u\) (see eq. (55) in the appendix). This leaves us with a data set where inputs in both classes have an average covariance matrix that is the identity, which means that PCA or linear neural networks [64; 65; 66; 67] cannot detect any difference between the two classes.

## 3 How many samples do we need to learn?

Given a data set sampled from the spiked cumulant model, we can now ask: how many samples does a statistician need to reliably detect whether inputs are Gaussian or not, i.e. whether HOCs are spiked or not? This is equivalent to the hypothesis test between \(n\) i.i.d. samples of the isotropic normal distribution in \(^{d}\) as the null hypothesis \(_{n,d}\), and \(n\) i.i.d. samples of the spiked cumulant model eq. (3) as the alternative hypothesis \(_{n,d}\). In section 3.1 we will first consider the problem from a statistical point of view, assuming to have _unbounded computational power_ and no restrictions on the distinguishing algorithms. Then, in section 3.2 we will use the _low-degree method_ to understand how the picture changes when we restrict to algorithms whose running time is at most polynomial in the space dimension \(d\).

### Statistical distinguishability: LR analysis

Recall the notion of _strong asymptotic distinguishability_: two sequences of probability measures are strongly distinguishable if it is possible to design statistical tests that can classify correctly which of the two distributions a sample was drawn from with probabilities of type I and II errors that converge to 0 (see appendix B.2 for the precise definition). Using this definition of distinguishability, we will ask what is the _statistical sample complexity exponent_, i.e. the minimum \(\) such that in the high-dimensional limit \(d\), if \(n d^{}\), then \(_{n,d}\) and \(_{n,d}\) are strongly distinguishable (with no constraints on the complexity of statistical tests).

A useful quantity to consider is the _likelihood ratio_ (LR) of probability measures, which is defined as

\[L(x):=}{}(x).\] (4)

Computing the LR norm \(||L||^{2}:=_{}[L^{2}]\) is an excellent tool to probe for distinguishability: if \((_{n,d})\) and \((_{n,d})\) are strongly distinguishable, then \(\|L_{n,d}\|\) (this is the well-known _second moment method for distinguishability_, see proposition 6 in the appendix for the precise statement). In the following we will apply this method, finding a formula for the LR norm and then study its limit as a function of \(\). Here and throughout, we will denote the data matrix by \(=(x^{})^{}_{1,,n}\); in general matrices of size \(n d\) will be denoted with underlined letters; see appendix B.1 for a complete summary of our notation. We will use Hermite polynomials, denoted by \((h_{k})_{k}\), see appendix B.3for details. We assume that the spike \(u\) is drawn from a known prior \((u)\), and that the scalar latent variables \((g^{})_{=1,,n}\) are drawn i.i.d. from a distribution \(p_{g}\) with the following properties:

**Assumption 1** (Assumption on latent variables \(g^{}\)).: _We assume that the one-dimensional probability distribution of the non-Gaussian random variable \(p_{g}(g)\) is an even distribution, \(p_{g}(g=dx)=p_{g}(-g=-dx)\), with mean 0 and variance 1, and that it satisfies the following requirement on the growth of its Hermite coefficients:_

\[[h_{m}(g)]^{m}m!\] (5)

_where \(>0\) is a positive constant that does not depend on \(m\). Finally, we assume that \(p_{g}\) has tails that cannot go to 0 slower than a standard Gaussian, \([g^{2}/2]<+\)._

A detailed discussion of these assumptions can be found in appendix B.4, as well as a proof that they are satisfied by a wide class of distributions including all the compactly supported distributions with mean 0 and variance 1 (some concrete examples are \(p_{g}\)=Rademacher\((1/2)\) or \(p_{g}=(-,)\)).

Under these assumption we can compute the following formula for the LR norm.

**Theorem 2**.: _Suppose that \(u\) has \(i.i.d.\) Rademacher prior and that the non-Gaussian distribution \(p_{g}\) satisfies assumption 1. Then the norm of the total LR is given by_

\[\|L_{n,d}\|^{2}=_{j=0}^{d}}f(,-1)^{n},\] (6)

_where \(f\) is defined as the following average over two independent replicas \(g_{u},g_{v} g\) of \(g\):_

\[f(,):=*{}_{g_{u},g_{v}}[-^{2}^{2}}}e^{-^{2}+g_{v}^{2})-2(g_{u}g_{v}))}{2(1+)^{2}- 2^{2}^{2}}}+^{2}+g_{v}^{2}}{2}]\] (7)

We prove theorem 2 in appendix B.5. The theorem has key consequences in two directions:

* on the one hand, eq. (6) implies that the LR norm is bounded for \(<1\) (see lemma 13 in appendix B.5.3), which confirms that below that threshold _strong distinguishability is impossible_;
* on the other hand, eq. (6) implies that whenever there exists \(\) such that \(f(,)>1\), we find that \(\|L_{n,d}\|)^{n}}{2^{d}}\). Thus, the LR norm diverges as soon as \(n\) grows as \(n d^{}\) with any \(>1\). In appendix B.5.3 we detail as an example the case in which \(g\)Rademacher\((}{{2}})\) where the norm \(\|L_{n,d}\|\) even diverges for \(=1\) and \(d n\) for some \(>0\).

So, besides the intrinsic value of providing an exact formula for the LR of the spiked cumulant model, theorem 2 implies the presence of a phase transition at \(=1\) for the _strong statistical distinguishability_.

Figure 1: **The performance of an exhaustive-search algorithm corroborates the presence of a phase transition for \(=1\), as suggested by theorem 2.** Success rate of an exponential-time search algorithm over all the possible spikes in the \(d\)-hypercube as a function of the exponent \(\) that quantifies as \(n=d^{}\) the samples used in the log-likelihood test (8), in the \(g\)Radem\((1/2)\) case.

A complementary approach that substantiates the presence of the statistical phase transition at \(=1\) can be seen in fig. 1, where we perform a maximum log-likelihood test along \(u x^{}\) for _all_ the possible spikes \(u\) in the \(d\)-dimensional hypercube using the formula for the LR conditioned on the spike,

\[_{=1}^{n}((x^{}|u)}{p_{z}(x^{})})=_{ =1}^{n}}(- {2}(g-}x^{} u)^{2}+}{2}),\] (8)

(see eq. (61) in appendix B.5.2 for the derivation of this equation) and output the most likely \(u\). Note that due to the exponential complexity in \(d\) of the algorithm, it is unfeasible to reach large values for this parameter. However, even at small \(d\) values, the success rate for retrieving the correct spike has a very steep increase at around \(=1\), as predicted by our analysis of the LR norm in theorem 2.

### Computational distinguishability: LDLR analysis

We now compare the statistical threshold of section 3.1 with the _computational sample complexity exponent_ that quantifies the sample complexity of detecting non-Gaussianity with an efficient algorithm that runs in a time that is polynomial in the input dimension. The algorithmic sample complexity can be analysed rigorously for a wide class of algorithms using the _low-degree method_. The low-degree method arose from the study of the sum-of-squares hierarchy  and rose to prominence when Hopkins & Steurer  demonstrated that the method can capture the Kesten-Stigum threshold for community detection in the stochastic block model . In the case of hypothesis testing, the key quantity is the _low-degree likelihood ratio_ (LDLR) .

**Definition 3** (Low-degree likelihood ratio (LDLR)).: _Let \(D 0\) be an integer. The low-degree likelihood ratio of degree \(D\) is defined as_

\[L^{ D}:=^{ D}L\] (9)

_where \(^{ D}\) projects \(L\) onto the space of polynomials of degree up to \(D\), parallel to the Hilbert space structure defined by the scalar product \( f,g_{L^{2}(,)}:=_{x}[f (x)g(x)]\)._

The idea of this method is that among degree-\(D\) polynomials, \(L^{ D}\) captures optimally the difference between \(\) and \(\), and this difference can be quantified by the norm \(\|L^{ D}\|=\|L^{ D}\|_{L^{2}(_{n},_{n})}\). Hence in analogy to the _second moment method_ used in section 3.1, we can expect low-degree polynomials to be able to distinguish \(\) from \(}\) only when \(\|L^{ D(n)}_{n}\|\), where \(D(n)\) is a monotone sequence diverging with \(n\). Indeed, the following (informal) conjecture from Hopkins  states that this criterion is valid not only for polynomial tests, but for all polynomial-time algorithms:

**Conjecture 4**.: _For two sequences of measures \(_{N},_{N}\) indexed by \(N\), suppose that (i) \(_{N}\) is a product measure; (ii) \(_{N}\) is sufficiently symmetric with respect to permutations of its coordinates; and (iii) \(_{N}\) is robust with respect to perturbations with small amount of random noise. If \(\|L^{ D}_{N}\|=O(1)\) as \(N\) and for \(D( N)^{1+}\), for some \(>0\), then there is no polynomial-time algorithm that strongly distinguishes the distributions \(\) and \(\)._

Even though this conjecture is still not proved in general, its empirical confirmation on many benchmark problems has made it an important tool to probe questions of computational complexity, see theorem 8 for a simple example.

We will now compute LDLR norm estimates for the spiked cumulant model, so that the application of conjecture 4 will help to understand the _computational sample complexity_ of this model.

**Theorem 5** (LDR for spiked cumulant model).: _Suppose that \((u_{i})_{i=1,,d}\) are drawn i.i.d. from the symmetric Rademacher distribution and that the non-Gaussian distribution \(p_{g}\) satisfies assumption 1. Let \(0<<1\) and assume \(D(n)^{1+}(n)\). Take \(n,d\), with the scaling \(n d^{}\) for \(>0\). The following bounds hold:_

\[\|L^{ D(n)}_{n,d}\|^{2}((_{4}^{g}}{(1+)^{2}})^{2} })^{ D(n)/4}\] (10)

\[\|L^{ D(n)}_{n,d}\|^{2} 1+_{m=1}^{D(n)}(}{1+})^{m}m^{4m}(})^{m/4}\] (11)Taken together, eq. (10) and eq. (11) imply the presence of a critical regime for \(_{c}=2\), and describe the behaviour of \(\|L_{n,d}^{ D}\|\) for all \(_{c}\)_

\[_{n,d}\|L_{n,d}^{ D(n)}\|=1&0< <2\\ +&>2\] (12)

This theorem could be derived with different constants from lemma 26 and proposition 8 in Dudeja & Hsu . Here we also provide a different, more direct argument. We sketch the proof of theorem 5 in appendix B.6.1 and give the complete proof in appendix B.6.2. We will discuss next the implications of the results presented in section 3.1 and section 3.2

### Statistical-to-computational gaps in the spiked cumulant model

Put together, our results for the statistical and computational sample complexities of detecting non-Gaussianity in theorem 2 and theorem 5 suggest the existence of three different regimes in the spiked cumulant model as we vary the exponent \(\), with a statistical-to-computational gap: for \(0<1\), the problem is _statistically impossible_ in the sense that no algorithm is able to strongly distinguish \(\) and \(\) with so few samples, since the LR norm is bounded. For \(1<<2\), the norm of the likelihood ratio with \(g(1/2)\) diverges (even at \(=1\) for some values of \(\)), the problem could be statistically solvable (as validated by the results of exhaustive-search algorithms in fig. 1), but conjecture 4 suggests no polynomial-time algorithm is able to achieve distinguishability in this regime; this is the so-called _hard phase_. If \(>2\), the problem is solvable in polynomial time by evaluating a polynomial function (fourth-order in each sample) and thresholding; this is the _easy phase_.

The spiked cumulant model leads thus to intrinsically harder classification problems than the spiked Wishart model, where the critical regime is at \(=1\). The proof of theorem 5 reveals that this increased difficulty is a direct consequence of the whitening of the inputs in eq. (3). Without whitening, degree-2 polynomials would also give contributions to the LDLR (75) which would yield linear sample complexity. The difference in sample complexity of the spiked Wishart and spiked cumulant models mirrors the gap between the sample complexity of the best-known algorithms for matrix factorisation, which require linear sample complexity, and tensor PCA for rank-1 spiked tensors of order \(k\)[25; 28; 30], where sophisticated spectral algorithms can match the computational lower bound of \(d^{k/2}\) samples.

## 4 Learning from HOCs with neural networks and random features

The analysis of the (low-degree) likelihood ratio has given us a detailed picture of the statistical and computational complexity of extracting information from the covariance or the higher-order cumulants of data. We will now use these fundamental limits to benchmark the sample complexity of two-layer neural networks (2LNN) trained with stochastic gradient descent on a binary discrimination task, where inputs in one class are drawn from the normal distribution \((0,_{d})\), while inputs in the other class are drawn from the spiked Wishart or the spiked cumulant model. In addition, we will also benchmark random feature methods (RF) [71; 72; 73] as a finite-dimensional approximation of kernel methods [71; 72; 73].

In a nutshell, the idea behind our experiments is to first _validate_ both 2LNN and RF on the simpler spiked Wishart task and then to apply both methods to the spiked cumulant model, where inputs are generated in a way that mirrors the spiked Wishart: comparing eq. (1) with eqs. (2) and (3), we see that the only differences are the whitening, and the latent distribution \(p_{g}\). In our experiments, we choose the latent variables to be standard Gaussian for spiked Wishart, and Rademacher for the spiked cumulant - hence the latent variables have matching first and second moments. However, we will see that the spiked cumulant model exhibits a large gap in the sample complexity required for neural networks or random features to learn the problem. We relegate details on the experimental setups such as hyper-parameters to appendix A.

### Spiked Wishart model

We trained **two-layer ReLU neural networks**\(_{}(x)=v^{}(0,w^{T}x)\) with \(m=5d\) hidden neurons on the spiked Wishart classification task. We show the early-stopping test accuracy of the networks in the linear and quadratic regimes with \(n_{} d,d^{2}\) samples per class in fig. 2A and B, resp. Neural networks are able to solve this task, in the sense that their classification error _per sample_ is below 0.5, implying strong distinguishability of the whole data matrix. Indeed, some of the hidden neurons converge to the spike \(u\) of the covariance matrix, as can be seen from the maximum value of the normalised overlaps \(_{k}w_{k}^{}u/\|\|u\|}\) in fig. 2C and D, where \(w_{k}\) is the weight vector of the \(k\)th hidden neuron. In the linear regime (C), there is an increasingly clear transition from a random overlap for small data sets to a large overlap at large data sets as we increase the input dimension \(d\); in the quadratic regime (D), the neural network recovers the spike almost perfectly.

The **relatively large overlap between hidden neurons and spike at small sample complexities** (fig. 2C and D) is due to the fact that we plot the maximum overlaps over a relative large number of hidden neurons \(m=5d\); hence even at initialisation, a few neurons will have large overlaps. We verified that ensuring an initial overlap of only \(1/\) by explicit orthogonalisation did not change our results on distinguishability, see fig. 5. A possible explanation is that the dynamics of the wide network is dominated by the majority of neurons, which do not have a macroscopic overlap.

Meanwhile, we found that **the performance of random features** tends to random guessing at linear sample complexity, where we let the input dimension \(d\) with \(m/d\) and \(n_{}/d\) fixed, while at quadratic sample complexity, random features learn the task, although they perform worse than neural networks. The failure of RF in the linear regime makes sense in light of recent results that suggest that random features in this scaling regime are limited to learning a linear approximation of the target function , while the LDLR analysis appendix B.2.3 shows that the target function, i.e. the low-degree likelihood ratio, is a quadratic polynomial. However, these results are, to the best of our knowledge, restricted to the case of Gaussian isotropic inputs.

To ensure that the performance of random features does indeed tend to random guessing, we performed a **replica analysis** following Loureiro _et al._ for mixture classification tasks together with the Gaussian equivalence theorem  (black lines in fig. 2A, details in appendix C). Replica theory perfectly predicts the performance of RF we obtain in numerical experiments (red-ish dots) for various values of \(d\) at linear sample complexity. We thus find a clear separation in the sample complexity required by random features (\(n_{} d^{2}\)) and neural networks (\(n_{} d\)) to learn the spiked Wishart task. The replica analysis can be extended to the polynomial regime by a simple rescaling of the free energy  on several data models, like the vanilla teacher-student setup , the Hidden manifold model , and the vanilla Gaussian mixture classification (see fig. 8). However, we found that for the spiked Wishart model, the Gaussian equivalence theorem which we need to deal with the random feature distribution _fails_ at quadratic sample complexity. This might be due to the fact that in this case, the spike induces a dominant direction in the covariance of the random features, and this

Figure 2: **Learning the spiked Wishart task with neural networks and random features.****(A,B)** Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural networks (NN) on the spiked Wishart task, eq. (1), with linear and quadratic sample complexity (\(n_{}\ d,\ d^{2}\), respectively, where \(d\) is the input dimension). Predictions for the performance of random features obtained using replicas are shown in black. **(C,D)** Maximum normalised overlaps of the networks’ first-layer weights with the spike \(u\), eq. (1). _Parameters_: \(=5\). Neural nets and random features have \(m=5d\) hidden neurons. Full experimental details in appendix A.

direction is also key to solving the task, which can lead to a breakdown of Gaussian equivalence . A similar breakdown of the Gaussian equivalence theorem at quadratic sample complexity has been demonstrated recently in teacher-student setups by Cui _et al._ and Camilli _et al._.

### Spiked cumulant

Having thus validated the performance of 2LNN and RF on the simpler spiked Wishart model, we turn to the spiked cumulant model and to the question: can neural networks learn higher-order correlations, efficiently? The LDLR analysis predicts that a polynomial-time algorithm requires at least a quadratic number of samples to detect non-Gaussianity, and hence to solve the classification task, and we found indeed that neural networks require at least quadratic sample complexity to solve the task, fig. 3A and B. The high values of the maximum overlap between hidden neurons and cumulant spike in the linear regime (compared to \(d^{-1/2}\)) are again a consequence of choosing the maximum overlap among \(m=5d\) hidden neurons. Random features cannot solve this task even at quadratic sample complexity, since they are limited to a quadratic approximation of the target function [75; 76; 77; 88], but we know from the LDLR analysis that the target function is a fourth-order polynomial. We thus find an even larger separation in the minimal number of samples required for random features and neural networks to solve tasks that depend on directions which are encoded exclusively in the higher-order cumulants of the inputs.

### Phase transitions and neural network performance in a simple model for images

We finally show another example of a separation between the performance of random features and neural networks in the feature-learning regime on a toy model for images that was introduced recently by Ingrosso & Goldt , the non-linear Gaussian process (NLGP). The idea is to generate inputs that are (i) translation-invariant and that (ii) have sharp edges, both of which are hallmarks of natural images . We first sample a vector \(z^{d}\) from a normal distribution with zero mean and covariance \(C_{ij}=\,z_{i}z_{j}=(-|i-j|/)\) to ensure translation-invariance of the inputs, with length scale \(>0\). We then introduce edges, i.e. sharp changes in luminosity, by passing \(z\) through a saturating non-linearity like the error function, \(x_{i}=(gz_{i})/Z(g)\), where \(Z(g)\) is a normalisation factor that ensures that the pixel-wise variance \(\,x_{i}^{2}=1\) for all values of the gain \(g>0\). The classification task is to discriminate these "images" from Gaussian inputs with the same mean and covariance, as illustrated in two dimensions in fig. 4A. This task is different from the spiked cumulant model in that the cumulant of the NLGP is not low-rank, so there are many directions that carry a signal about the non-Gaussianity of the inputs.

We trained wide two-layer neural networks on this task and interpolated between the feature-learning and the "lazy" regimes using the \(\)-renormalisation trick of Chizat _et al._. As we increase \(\), the networks go from feature-learners (\(=1\)) to an effective random feature model and require an increasing amount of data to solve the task, fig. 4B. There appears to be a sharp transition from random guessing to non-trivial performance as we increase the number of training samples for all values of \(\)

Figure 3: **Learning the spiked cumulant task with neural networks and random features.****(A, B)** Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural networks (NN) on the spiked cumulant task eq. (3) with linear and quadratic sample complexity (\(n_{}\ d,\ d^{2}\), respectively, where \(d\) is the input dimension). **(C, D)** Maximum normalised overlaps of the networks’ first-layer weights with the spike \(u\), (3). _Parameters_: \(=10\). Neural nets and random features have \(m=5d\) hidden neurons, same optimisation as in fig. 2. Full experimental details in appendix A.

This transition is preceded by a transition in the behaviour of the higher-order cumulant that was reported by Ingrosso & Goldt . They showed numerically that the CP-factors of the empirical fourth-order cumulant \(T\), defined as the vectors \(^{d}\) that give the best rank-\(r\) approximation \(=_{k=1}^{r}_{k}_{k}^{ 4}\) of \(T\), localise in space if the data set from which the empirical cumulant is calculated is large enough. Quantifying the localisation of a weight vector \(w\) using the _inverse participation ratio_

\[(w)=^{d}w_{i}^{4}}{(_{i=1}^{d}w_{i}^{2} )^{2}},\] (13)

we confirm that the leading CP-factors of the fourth-order cumulant localise (purple dashed line in fig. 4C). The localisation of the CP-factors occurs with slightly less samples than the best-performing neural network requires to learn (\(=1\)). The weights of the neural networks also localise at a sample complexity that is slightly below the sample complexity for solving the task. The laziest network (\(=100\)), i.e. the one where the first-layer weights move least and which is hence closest to random features, does not learn the task even with a training set containing \(n=10^{3}d\) samples when \(d=20\), indicating again a large advantage of feature-learners over methods with fixed feature maps, such as random features.

## 5 Concluding perspectives

Neural networks crucially rely on the higher-order correlations of their inputs to extract statistical patterns that help them solve their tasks. Here, we have studied the difficulty of learning from higher-order correlations in the spiked cumulant model, where the first non-trivial information in the data set is carried by the input cumulants of order 4 and higher. Our LR analysis of the corresponding hypothesis test confirmed that data sampled from the spiked cumulant model could be statistically distinguishable (in the sense that it passes the _second moment method for distinguishability_) from isotropic Gaussian inputs at linear sample complexity, while the number of samples required to strongly distinguish the two distributions in polynomial time scales as \(n d^{2}\) for the class of algorithms covered by the low-degree conjecture , suggesting the existence of a large statistical-to-computational gap in this problem. Our experiments with neural networks show that they learn from HOCs efficiently in the sense that they match the sample complexities predicted by the analysis of the hypothesis test, which is in stark contrast to random features, which require a lot more data. In the future, a key challenge will be extend this framework to null hypotheses that go beyond isotropic Gaussian distributions. It will be intriguing to analyse the _dynamics_ of neural networks on spiked cumulant models or the non-linear Gaussian process to understand how neural networks extract information from the higher-order cumulants of realistic data sets efficiently .

Figure 4: **A phase transition in the fourth-order cumulant precedes learning from the fourth cumulant.****(A)** We train neural networks to discriminate inputs sampled from a simple non-Gaussian model for images introduced by Ingrosso & Goldt  (top) from Gaussians with the same mean and covariance (bottom). **(B)** Test error of two-layer neural networks interpolating between the fully-trained (\(=1\)) and lazy regimes (large \(\)) – see section 4.3. **(C)** The localisation of the leading CP-factor of the non-Gaussian inputs (dashed purple line) and the first-layer weights of the trained networks, as measured by the inverse participation ratio (IPR), eq. (13). Large IPR denotes a more localised vector \(w\). _Parameters_: \(g=3\), \(=1,d=20,m=100\). Full details in appendix A.

## Contributions

ES performed the numerical experiments with neural networks and random features. LB performed the (low-degree) likelihood analysis. FG performed the replica analysis of random features. SG designed research and advised ES and LB. All authors contributed to writing the paper.