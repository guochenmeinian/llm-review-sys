# Learning to Edit Visual Programs with

Self-Supervision

 R. Kenny Jones

Brown University

russell_jones@brown.edu

&Renhao Zhang

University of Massachusetts, Amherst

renhaozhang@cs.umass.edu

Aditya Ganeshan

Brown University

aditya_ganeshan@brown.edu

&Daniel Ritchie

Brown University

daniel_ritchie@brown.edu

###### Abstract

We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in _one-shot_. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the _one-shot_ model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the _one-shot_ model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.

## 1 Introduction

People seldom write code with a linear workflow. The process of authoring code often involves substantial trial-and-error: possibly correct programs are evaluated through execution to see if they raise exceptions or break input-output assumptions. When an error is identified, an edit is made, and this process is repeated. It is difficult to imagine writing any moderately complex program in a _one-shot_ paradigm, without being able to debug intermediate program versions.

The field of program synthesis studies how to automatically infer a program that meets an input specification . In this work, we consider the sub-problem of visual program induction (VPI), where the input specification is a visual target (e.g. an image) and the goal is to find a program whose execution recreates the target . This task has numerous applications across visual computing disciplines, including reverse-engineering, structure analysis, manipulation, and generative modeling.

This problem area has garnered significant interest, with many works exploring _learning_-based solutions. For domains with annotated data, supervised approaches perform well . For domains that lack program annotations, a variety of unsupervised and self-supervised learning paradigms have been proposed . Moreover, initial investigations have explored the capabilities of Large Language Models for solving simple VPI tasks .

Though these prior approaches have made impressive progress, a common limitation is that they operate within the aforementioned _one-shot_ paradigm. For instance, when using an autoregressive network these systems will condition on a visual target and iteratively predict program tokens untilcompletion. While this sequential inference procedure is sometimes wrapped in a more complex outer-search (e.g. beam-search or sequential Monte Carlo ), is allowed to reason over partial program executions , or is given access to executor-gradients that guide an inner-loop search [12; 45], all of these paradigms are distinct from how _people_ typically write programs.

In this work, we present a model that learns how to edit visual programs in a goal-directed manner. Our network consumes a complete input program, this program's executed state, and a visual target. It then proposes a local edit operation that modifies the input program to better match the target. In contrast with _one-shot_ approaches, this framing allows our network to explicitly reason over a complete program and its execution, in order to decide how this program should be modified.

We train our network without access to any ground-truth program annotations. To accomplish this, we propose an integration of our edit network with prior self-supervised bootstrapping approaches for _one-shot_ VPI models . During iterative finetuning rounds, we source paired training data for our edit network by first constructing pairs of start and end programs, and then using a domain-aware algorithm to find a set of edit operations that would bring about this transformation. This process jointly finetunes both our edit network and a _one-shot_ network, and we propose an integrated inference algorithm that leverages the strengths of both of these paradigms: the _one-shot_ model produces rough estimates that are refined with the edit network. We find that this joint self-supervised learning set-up forms a virtuous cycle: the _one-shot_ model provides a good initialization state for the edit network, and the edit network improves inner-loop inference, creating better bootstrapped training data for the _one-shot_ model.

We experimentally compare the effectiveness of integrating our edit network into this joint paradigm against using _one-shot_ models alone. Controlling for equal inference time, over multiple visual programming domains, we find that using the edit network improves reconstruction performance. Moreover, we find that the reconstruction gap between these two paradigms widens as more time is spent on test-time program search. Further, we demonstrate our method performs remarkably well even with very limited data, as learning how to edit is an inherently more local task compared with learning how to author a complete program. Finally, we run an ablation study to understand and justify our system design.

In summary, we make the following contributions: (1) A model that learns how to predict local edits that improve visual programs towards a target. (2) A self-supervised learning paradigm that jointly trains an edit network and a _one-shot_ network through bootstrapped finetuning.

We release code for our experiments at: https://github.com/rkjones4/VPI-Edit

## 2 Related Work

**Visual Program Induction** There has been growing interest in works that aim to infer structured procedural representations that explain visual data . While some research has investigated geometric heuristics to search for a well-reconstructing program [9; 42], most methods employ learned networks to guide this search. For visual programming domains that come with annotations, networks can be trained with ground-truth program supervision [1; 17; 41; 43].

However, most visual domains of interest lack such annotated data. As a result, a host of techniques have been investigated for this unsupervised setting, including: reinforcement learning [10; 34], differentiable execution architectures [20; 31; 36; 44; 45], learned proxy executors [8; 38], or bootstrapping methods [11; 15; 19]. All of these works operate within the aforementioned _one-shot_ paradigm. Of note, SIRI investigates how analytical code rewriting operations can improve VPI networks in a bootstrapped learning paradigm . Our system shares a similar motivation in that we aim to rewrite visual programs in a goal-directed fashion. However, instead of modifying programs with domain-specific fixed operations (e.g. differentiable parameter optimization), we explore a generalized alternative by introducing a network that learns how to edit programs.

Our edit network reasons over the visual execution produced by an input program to decide how the program should be edited. The idea of reasoning over program executions to improve search has been successfully demonstrated for both general program synthesis problems [6; 47] and visual program induction [10; 25]. However, different from our approach, which predicts a local edit that modifies a complete program, these approaches reason over the executions of partial programs in order to better guide auto-regressive synthesis (i.e. they largely operate in a _one-shot_ paradigm).

**Program Repair** A number of program synthesis methods have been proposed that learn how to repair or fix programs for domains where ground-truth programs are available. SED interleaves a series of synthesis, execution and debugging steps in order to improve synthesis of Karel programs from input/output examples . Related approaches have explored learning how to 'fix' programs end-to-end by manipulating latent-space encodings of programs under a fixed decoder for the RobustFill domain . While our method shares a similar motivation with these works, we demonstrate the efficacy of our approach on more complex visual programming domains, we don't assume access to ground-truth program annotations, and our edit network only predicts a single local edit operation at each step, instead of rewriting an entire program.

With the growing attention around the abilities of Large Language Models (LLMs), a number of recent works have explored how LLMs can be used to fix programs from input/output examples [5; 23; 24; 35]. Though differing in details, the typical formulation these methods take involves presenting an LLM with a previous program version, and asking it to either (i) debug exceptions or (ii) modify program behavior in light of input/output mismatches. While these initial forays show promise, LLMs have not yet been able to write complex visual programs (in part due to poor visual reasoning capabilities), and even for more general program synthesis tasks the performance gains of code-editing LLMs are not definitive .

## 3 Method

In this section, we present our approach for learning how to edit visual programs. First we formalize our task of unsupervised visual program induction. For a particular domain, we are given a domain-specific language (DSL) \(L\) and an executor \(E\) that converts programs \(z\) from \(L\) into visual outputs \(x\). Given visual inputs from a target visual dataset that lacks program annotations, \(x^{*} X^{*}\), our goal is to find find \(z^{*} L\), such that \(E(z^{*}) x^{*}\). This measure of similarity is usually checked under a domain specific reconstruction metric \(M\).

A general approach employed by prior visual program induction works is to use an autoregressive model (e.g. a Transformer) that is conditioned on a visual encoding to predict a well-reconstructing program: \(p(z|x)\). These _one-shot_ models iteratively predict the next program token until the program is complete. We present a framework that employs a similar autoregressive model, but instead of predicting a complete program from scratch, we instead predict a local edit that modifies an input program. In the rest of this section, we first present how we design our edit network (Sec. 3.1). Then we discuss our unsupervised training procedure where we jointly finetune an edit network along with a _one-shot_ network (Sec. 3.2. Finally, we describe how we combine these networks to search for visual programs (Sec. 3.3).

### Edit Network Design

Our edit network \(p(e|z,x)\) learns how to predict a local edit operation that improves an input program towards a visual target (see Figure 1). We provide our network with a triplet input state: the tokens of

Figure 1: We design a network that learns how to locally edit an input program towards a target. It first predicts what type of edit operation should be applied, then it predicts where that edit operation should be applied, and finally it autoregressively samples any parameters the edit operation requires.

an input program \(z\), this program's executed output \(E(z)\), and a visual target \(x\). From this state, our network is tasked with predicting an edit operation \(e\) that could be applied to the input program.

**Edit Operations.** There are many ways to parameterize the space of possible program edits. We choose to constrain the possible edit operations our network can produce by forcing it to select from a set of local editing operations designed for visual programs. For instance, for functional visual programming DSLs with transformation and combinator functions, we allow for seven different edit operations: modifying a transform's parameters (_MP_), modifying a transform (_MT_), adding a transform (_AT_), removing a transform (_RT_), modifying a combinator (_MC_), removing a combinator (_RC_), or adding a combinator (_AC_). We provide more details in Appendix E. Some of these edit operations do not take in parameters (removing a transform) while others require new parameters (e.g. to modify the parameters of a transform we need to know the new parameters). Each of these edit operations can be applied to a program at a specific token location, and results in a local change. Subsequently, we task our edit network with predicting three items: an edit operation type, a location for that edit operation, and any extra parameters that operation requires.

We design our system with this somewhat constrained edit operation set as it has a number of advantages. First, the application and effect of each edit operation is local; this simplifies the learning task and allows us flexibility at inference time. Moreover, ensuring that edit operations are tied to the semantics of the underlying DSL helps to promote program edits that result in syntactically valid modified programs. We compare our edit operation design against alternative formulations in our experimental results (Sec. 4.5).

**Architecture.** We implement our edit network as a Transformer decoder. This network has full attention over the conditioning information: each visual input (the executed output of the input program and the target) is encoded into a sequence of visual tokens (e.g. with a CNN) and each token of the input program is lifted with an embedding layer.

To predict the edit operation type, we take the output Transformer embedding from the first index of input program sequence. This embedding is sent through a linear layer which predicts a distribution over the possible edit operation types (yellow boxes, Fig. 1).

To predict the edit operation location, we consider the embeddings that the Transformer produces over the tokens of the input program. Each of these location codes is sent through a linear layer, which predicts a value for each operation type. For a chosen operation type, we then normalize these values into a probability distribution across the length of the input program sequence (dark-blue boxes, Fig. 1). This distribution models the likelihood of where a specific edit operation type should be applied.

Finally, we use our network to autoregressively sample any extra parameters that a chosen edit operation might require. To accomplish this, we first slightly reformat the input program by inserting a special'sentinel token'  associated with the chosen edit operation in two places: (1) at the specified edit operation location and (2) at the end location of the current program ($_AT_, Fig. 1). This'sentinel' tokens allows the network to know what operation is being applied to which position. Then, starting from the location of the second sentinel token, we can use the network to iteratively generate a sequence of parameter predictions with causal attention-masking, until an 'END' token is chosen (green boxes, Fig. 1).

**Training.** Given an input program, how do we know which edit operations are helpful? If we have access to not only a visual target, but also its corresponding program, we can find a set of edit operations that would transform the input program into this target. We follow this logic to source training data for our edit network: given a start program and an end program, we analytically identify a set of edit operations that would bring about this transformation with a _findEdits_ function. We can then convert this set of edit operations into a large set of (input, output) pairs that our network can train on. We provide further details on this algorithm in Appendix E. Once we have sourced paired data, through teacher-forcing we can train our network in a supervised fashion with a cross-entropy loss on the predicted operation type, location, and each parameter token. Though we lack known programs for the target domain of interest, we next discuss a bootstrapped finetuning procedure that provides a work-around for this issue.

### Learning Paradigm

As we operate in a paradigm where we don't have access to ground-truth programs for our target set \(X^{*}\), we take inspiration from recent self-supervised approaches that employ bootstrapped finetuning for visual program induction [12; 19]. Specifically, we develop an algorithm (Alg. 1) that integrates edit network training into the PLAD finetuning framework.

PLAD Finetuning.We begin with an overview of the PLAD method, which is depicted with the black text in Alg. 1 (see  for details). At the start of each round, the program inference network \(p(z|x)\) is run over the target dataset \(X^{*}\); the results of this inference procedure populate the entries of a best programs data-structure \(P^{}\) according to \(M\). Then an unconditional generative model \(p(z)\) is trained over the entries of \(P^{}\), and a set of 'dreamed' programs, \(P^{G}\), are sampled from this network. The weights of \(p(z|x)\) are then finetuned using paired data sourced from \(P^{}\) and \(P^{G}\). These steps are repeated for a set number of rounds, or until convergence.

Edit Model Finetuning.The blue-colored lines in Alg. 1 indicate the modifications we make to the PLAD algorithm to incorporate our edit network. Lines 8-10 explain the training logic. First we use \(p(z|x)\) to sample a set of programs \(P^{S}\) conditioned on the executed outputs of the generated programs \(P^{G}\). Treating \(P^{S}\) as the starting points and \(P^{G}\) as the end points, we can then use our _findEdits_ operation to find sets of edit operations \(ES\) that would realize these transformations. This provides us with paired data that we can use to finetune the weights of the edit network through teacher forcing, as explained in the prior section.

Synthetic Pretraining.PLAD finetuning is typically initialized with a synthetic pretraining phase (Alg. 1, line 1). During pretraining, random programs are sampled from \(L\), and \(p(z|x)\) can be trained on the paired data produced by executing these samples. Similarly, as we discuss in the results section, we find it useful to 'pretrain' the edit network on synthetic data (Alg. 1, line 2). While multiple formulations are possible here, we re-use the same logic shown on lines 8-10, except we replace the set of target programs \(P^{G}\) with random programs sampled from \(L\).

### Inference Algorithm

With the above procedure we can train our edit network, but how can we use this network to find improved visual programs? This question is not only relevant at test-time, but also impacts bootstrapped training, as we run an inner-loop search to populate the entries of \(P^{}\)(Alg. 1, line 5). As depicted on the right side of Figure 2, we design a search procedure that combines the strengths of the _one-shot_ and editing paradigms. This search procedure maintains a population of programs, which are evolved over a number of rounds. The initial population is produced by sampling \(p(z|x)\). Then for each round, we use the edit network to sample sets of edits for every program in the current population. We apply each of these sampled edits, and then re-sample the population for the next round according to a ranking based on \(M\).

Figure 2: _Left: our bootstrapping algorithm that finetunes an edit network and a _one-shot_ model towards a target dataset. Right: our inference algorithm that initializes a population with a _one-shot_ model and then mutates it towards a visual target through iterative rounds of edits and resampling._This formulation has a number of advantages. Instead of starting from a blank canvas, or with random samples, we allow \(p(z|x)\) to produce initial rough program estimates. These guesses are then refined through mutations over a series of editing rounds that are all directed at improving similarity towards the visual target. In Section 4.5 we compare this algorithm against alternative formulations. Critically, by applying this joint inference procedure during finetuning we form a virtuous cycle: improving the inference strategy leads to better \(P^{}\) entries, which results in better training data for \(p(z|x)\) and \(p(e|z,x)\), which in turn allows us to find to better \(P^{}\) entries in subsequent finetuning rounds. Finally, we note that this formulation maintains a nice symmetry between \(p(z|x)\) and \(p(e|z,x)\): in out joint finetuning algorithm \(p(e|z,x)\) trains on sequences sourced from sampling \(p(z|x)\), and in this way its training distribution of edit operations well matches the population used to initialize the inference algorithm.

## 4 Results

We evaluate our edit network with experiments over multiple domains. First we describe our experimental design (Sec. 4.1). Then we compare the ability of different methods to accurately infer visual programs in terms of reconstruction performance (Sec. 4.2). We analyze how this performance changes as a function of time spent on inference (Sec. 4.3) or the size of the training target dataset (Sec. 4.4). Finally, we discuss results of an ablation study on our method in Section 4.5.

### Experimental Design

We provide a high-level overview of our experimental design. See Appendix D for details.

**Methods.** We compare our approach (_OS+Edit_) against the alternative of using only a _one-shot_ model (_OS Only_). As described in Section 3, our approach jointly finetunes an edit network along with a _one-shot_ network, and uses both of these networks to infer visual programs (Fig. 2). To control for the added time cost incurred by our inference procedure, we adapt a sampling-based inference loop for the _OS Only_ variant, which we find results in a surprisingly strong baseline.

**Domains.** We consider three VPI domains (see App C): Layout, 2D CSG, and 3D CSG. In the Layout domain, scenes are created by placing colored 2D primitives on a canvas, and optionally modifying them by changing their size, location, or forming a symmetry group. In constructive solid geometry (CSG), complex shapes are formed by combining simple shapes with boolean set operations (union, intersection, difference). Our 2D CSG and 3D CSG domains differ in terms of their primitive types (e.g. squares vs cuboids) and the parameterizations of transformation functions: generalizing notions of scaling, translating, rotating, and symmetry grouping from \(^{2}\) to \(^{3}\).

**Network Details.** For each domain, we implement \(p(z|x)\) as a decoder-only Transformer  that conditions on a set of visual tokens and predicts up to a maximum sequence length _SL_. Similarly, we implement \(p(e|z,x)\) as a Transformer with the same architecture, except that it conditions on (i) two sets of visual tokens and (ii) an input program of length _SL_, and it is only allowed to predict edit parameters up to a length of _EL_. Our visual encoders are all standard CNNs. For Layout we use a 2D CNN that takes in an RGB 64x64 image, for 2D CSG we use a 2D CNN that takes in a binary 64x64 image, and for 3D CSG we use a 3D CNN that takes in a \(32^{3}\) voxel grid.

**Reconstruction Metric.** The reconstruction metric \(M\) guides the inference algorithm and also performs early stopping with respect to a validation set. For Layout we use _cIoU_, an intersection over union metric which only counts intersections on color matches . For 2D CSG we use an edge-based Chamfer distance (_CD_) . For 3D CSG we use intersection over union (_IoU_).

  & Layout \(cIoU\) & 2D CSG \(CD\) & 3D CSG \(IoU\) \\  _OS Only_ & 0.94 & 0.156 & 83.3 \\ _OS + Edit (Ours)_ & **0.98** & **0.111** & **85.3** \\   

Table 1: Across multiple visual programming domains we evaluate test-set reconstruction accuracy. In all cases, we find that our joint paradigm that integrates an edit network with _one-shot_ models outperforms the alternative of using only _one-shot_ models.

**Target Data.** Like prior bootstrapping methods, our finetuning algorithm specializes our networks towards a target dataset of interest, \(X^{*}\), that lacks known programs. For 2D CSG we use shapes from the dataset introduced by CSGNet , originally sourced from Trimble 3D warehouse. For 3D CSG we use shapes from the dataset introduced by PLAD , originally sourced from ShapeNet . While we use the same test-sets as prior work (3000 / 1000 for 2D CSG / 3D CSG), we find that our method is able to offer good performance with much less training data. In our base experiments, we use 1000/100 train/val shapes for 2D CSG (from 10000 / 3000 available) and and 1000/100 train/val shapes for 3D CSG (from 10000 / 1000 available). For the Layout domain, we use the manually designed scenes sourced from  (1000 train / 100 val / 144 test).

### Reconstruction Accuracy

We compare our _OS+Edit_ approach against _OS Only_ on each method's ability to infer visual programs that accurately reconstruct test-set inputs in Table 1. As demonstrated, our joint finetuning paradigm that combines an edit network with a _one-shot_ network consistently improves reconstruction performance. In these experiments, we ensure that each method gets to spend the same amount of time on inference by setting search parameters so that the average inference time per shape was equal: \(\) 5, \(\) 10, \(\) 60 seconds per shape for Layout, 2D CSG, and 3D CSG respectively. For _OS Only_, we use a sampling-based inference search where the model samples a population of complete programs for a set number of rounds. Though this approach provides a strong baseline, it was not as effective as combining our edit networks with _one-shot_ initializations. In fact, for the 2D CSG domain, our formulation achieves reconstruction scores that surpass the performance of related methods that assume access to executor-gradients. On the 2D CSG test-set, we achieve a Chamfer distance (CD) of 0.111 (lower is better), whereas UCSG-Net  gets a CD of 0.320, SIRI  gets a CD of 0.260, and ROAP  gets a CD of 0.210. Note that as the DSL, architecture, objective, and inference procedures differ across these various works, it's hard to make any absolute claims from this direct comparison. Nevertheless we would like to emphasize that our method's reconstruction performance on this task is very strong in the context of the related literature. We visualize reconstructions from this experiment in Figure 3, and find that qualitative evidence supports the quantitative trends.

### Search Time

While _one-shot_ models must author new programs from scratch without execution-feedback, our edit network has the capacity to reason over an input program, compare its execution versus the visual target, and decide how this program should be modified. As such, we hypothesize that integrating our edit network into our inference procedure will be increasingly advantageous over the _OS Only_ approach as more time is spent on test-time search. To validate this hypothesis, we explore how the reconstruction gap between these paradigms changes as a function of time spent on search (Figure 4, _left_). For 2D CSG we take a subset of the test-set (300 shapes) and run more rounds of our inference algorithm. As demonstrated, as more time is spent on test-time search (i.e. as the number of rounds increases) the reconstruction gap between _OS Only_ and _OS+Edit_ grows wider. Moreover, we note that even on the first round there is a gap between the methods, as the _one-shot_ network trained in the _OS+Edit_ paradigm had access to better \(P^{}\) entries throughout the finetuning process (i.e. the aforementioned virtuous cycle). We present qualitative results that show how the edit network evolves the population of programs towards the visual target in Figure 5.

Figure 3: Comparing reconstructions of _one-shot_ models (_top_) against our joint approach (_middle_).

### Training with limited data

While both _OS+Edit_ and _OS Only_ are unsupervised in the sense that they don't have access to any ground-truth program annotations, they do require an input set of visual data to form a target training set. We hypothesize that our edit network will be especially useful for domains with limited data (even limited unannotated data) as the program editing task is inherently more local than trying to author a complete program. Consider for instance that during finetuning, in a _one-shot_ paradigm each visual datum can only contribute a single training example, while in our paradigm an entire distribution of edit operations can be sourced by considering the many possible edit paths one could take to transform a start program into an end program. We validate this hypothesize with an experiment where we train versions of these systems while varying the size of the target training set (Fig. 4, _right_). Our joint paradigm offers very strong performance even while finetuning towards an input set of just 100 training shapes, matching the performance of _OS Only_ when it has 10x more data.

### Method Ablations

We run an ablation experiment to evaluate the design of our system on the Layout domain. We present results of this experiment in Table 2. In the rest of this section we detail all of the alternative formulations we compare against.

Edit Operations.Our default edit networks learn how to predict local edit operations from a limited set of options. We compare this paradigm with two alternatives. In the _next program_ mode, we task the edit network with predicting all of the tokens of the program that would be created by applying the target edit operation to the input program. In the _final program_ mode, we task the edit network with predicting the tokens of the final program associated with the visual target. This formulation was inspired by the success of denoising diffusion models for visual synthesis tasks , though in our setting this variant is basically an alternative _one-shot_ model with extra conditioning information but with the same target sequences. As demonstrated, neither of these approaches is as performant as our formulation where edits are predicted as local operations. Moreover, predicting an entire program is much slower compared with predicting an edit, so fewer rounds of our inference algorithm can be run with the same search time budget.

Program Corruption.We source paired training data for our edit network by constructing (start, end) program pairs and then analytically finding a set of edit operations that would complete this transformation. For an alternative, we can look towards discrete diffusion methods [30; 37; 40; 46]. In our _corruption_ variant we take inspiration from these works and design a program corruption algorithm for the Layout domain. This corruption algorithm takes an end program as input, and then samples corruption operations (i.e. inverse edit operations) that can be used as paired data for our edit network (Appendix F). As seen, this alternative formulation was not as performant as our default approach. One reason for this is that it hard to design a corruption process that converts end programs (e.g. \(P^{G}\)) into the distribution of programs that we have access to at inference time (e.g. \(P^{S}\)). Conversely, by applying our _findEdits_ operation on \(P^{G}\) and \(P^{S}\) pairs, we can source paired data for our edit network that _does_ match this distribution.

Figure 4: For 2D CSG, we compare reconstruction accuracy (Chamfer distance, lower is better, Y-axis) between using an edit network and using only a _one-shot_ network while varying time spent on inference (_left_) and training set size (_right_).

**Pretraining and Finetuning.** In our default version there are three training phases. First, \(p(z|x)\) undergoes pretraining on synthetic data. Second, \(p(e|z,x)\) undergoes pretraining on synthetic data using samples from \(p(z|x)\). Then both of these networks are jointly finetuned with respect to \(X^{*}\). In the _No FT_ variant, we don't finetune either network, in _no one-shot FT_ we don't finetune \(p(z|x)\), in _no edit FT_ we don't finetune \(p(e|z,x)\), and in _no edit PT_ we don't pretrain \(p(e|z,x)\). While the performance of our system remains remarkable strong even under these ablations, we get the best results by using all three training phases. Interestingly, for settings where \(p(z|x)\) is not specialized for \(X^{*}\), the reconstruction accuracy gap dramatically increases between the best sample in the starting population and the best sample in the final population of our inference procedure. For instance, for the _no one-shot FT_ variant, the first round cIoU score is 0.88 which gets increased to 0.972 (0.092 improvement) through the mutations proposed by the edit model, while in our default variant the first round cIoU is 0.925 (an improvement of.055).

**Inference Algorithm.** We compare our inference algorithm with two alternative versions. In _Naive OS_ we initialize the first population with \(p(z|x)\), and make edits to each population member with \(p(e|z,x)\), but we skip the population resampling step according to \(M\), and instead apply the highest likelihood edit from \(p(e|z,x)\). While the edit network is still helpful in this paradigm (0.022 improvement from the first to the last round), it performs worse compared with our default implementation. In _Rand+Edit_, we remove \(p(z|x)\) and instead fill the initial population with random program sampled from \(L\). This provides a much worse initialization (0.302 cIoU in the first round), and though our edit network successfully mutates these samples towards the target, better reconstruction performance is gained by combining our edit network with initial guesses from a _one-shot_ model.

## 5 Discussion

We have presented a system that learns how to edit visual programs in a goal-directed fashion. We develop a self-supervised bootstrapping approach that allows us to train an edit network for domains that lack ground-truth program annotations. We compare our proposed paradigm, that jointly finetunes a _one-shot_ model and an edit network, against the alternative of using only a _one-shot_ model, and find that our approach infers more accurate program reconstructions. Further, we find this performance gap is more pronounced when more time is spent on program search or when less training data is available. Finally, we justified the design of our method with an ablation experiment.

While our proposed approach advances the field of visual program induction, it does come with a few limitations. Compared with prior work, we need to train another network, this impacts the time required for both pretraining and finetuning stages. Moreover, the full benefit of using an edit network is best realized with a more complex program search, and as such we use search-time budgets that are slightly more costly compared with prior work. Though our formulation would offer improved performance for work-flows that can afford to spent more time on program search, it would be useful to consider potential speed-ups of our system . Finally we note that our current

**Method** & Final cIoU \(\) \\  _Ours_ & **0.980** \\  _Next program_ & 0.941 \\ _Final program_ & 0.920 \\  _Corruption_ & 0.964 \\  _No FT_ & 0.955 \\ _No one-shot FT_ & 0.972 \\ _No edit FT_ & 0.976 \\ _No edit PT_ & 0.953 \\  _Naive OS_ & 0.947 \\ _Rand+Edit_ & 0.906 \\  

Table 2: Ablation study comparing our method against alternative formulations.

formulation requires access to a domain-aware _findEdits_ operation that can analytically find a set of edits that realizes a transformation from a start program to an end program. While we find that our implementation generalizes across a range of visual programming domains, in future work, it would be interesting to consider to what degree this domain-aware procedure could be replaced by more general program difference algorithms . Looking ahead, we believe our framework can serve as inspiration for how to train networks that learn how to edit programs without ground-truth annotations over an even wider array of program synthesis tasks.