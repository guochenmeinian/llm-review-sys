# GarmentLab: A Unified Simulation and Benchmark

for Garment Manipulation

Haoran Lu\({}^{1,2}\)1 Ruihai Wu\({}^{1}\)1 Yitong Li\({}^{1,3}\)1

Sijie Li\({}^{1,2}\) Ziyu Zhu\({}^{1,2}\) Chuantuo Ning\({}^{1}\) Yan Shen\({}^{1}\)

Longzan Luo\({}^{1,2}\) Yuanpei Chen\({}^{1}\) Hao Dong \({}^{1}\)

\({}^{1}\)CFCS, School of CS, PKU \({}^{2}\)School of EECS, PKU \({}^{3}\)Weiyang College, THU

Equal contribution.

###### Abstract

Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present **GarmentLab**, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/

## 1 Introduction

The next-generation assistant robots should possess not only the abilities to separately manipulate a wide variety of objects, including rigid, articulated, and deformable objects, but also the capability to leverage interactions between those physical media, including flow and fluids, in order to assist humans. Among various daily tasks , garment manipulation stands out as one of the most challenging, crucial, and extensively discussed tasks in the robotics and computer vision, due to its demanding requirements for understanding dynamic properties of physical instances and interactions between them. For instance, washing clothes entails the interaction between garments and fluids, while dressing up requires collaboration between robots and humans.

Garment manipulation tasks mainly presents three challenges. Firstly, each individual garment possesses nearly infinite states and exhibits complex kinematic and dynamic properties. _Therefore, it is crucial for models to comprehend the various self-deform states of garments, which usually requireslarge amount of training data [17; 6]_**(C1)**. Secondly, garment manipulation involves interactions with various types of objects, including rigid (_e.g._, clothes hanger) and articulated (_e.g._, wardrobe) objects, as well as fluids and human body. _Consequently, enabling models to understand these interactions across diverse physical media presents great significance_**(C2)**. Finally, considering that strategies for manipulating garments are often highly complex, and visual perception of garments is more challenging due to their diverse states and patterns, _manipulating garments faces a greater sim2real gap [63; 29]_**(C3)**.

Training a powerful agent capable of overcoming these challenges necessitates a vast amount of data encompassing robot-object interactions. However, directly collecting data from the real world is impractical. Thus, researchers have long pursued benchmarks for garment manipulation [30; 4; 6; 64]. Current deformable simulations suffer from drawbacks such as missing garment meshes . Additionally, they offer a limited range of tasks, hindering further research endeavors.

Therefore, we present **GarmentLab** (Figure 1), a unified environment and benchmark for garment manipulation. **GarmentLab** has three novel components to satisfy the demands for diversity and realism: The powerful **GarmentLab Engine**, which is built upon Omniverse Isaac Sim  and supports a variety of physical simulation methods. The simulator not only supports Position-Based-Dynamic (PBD) , Finite-Element-Method (FEM) , to simulate garments, fluid and deformable objects but also makes integration with ROS  to provide an efficient teleoperation pipeline for data collection. **GarmentLab Assets** is a large-scale indoor dataset comprising 1) garments models covering 11 categories of daily garments from ClothesNet  2) various kinds of robot end-effector including gripper, suction and dexterous hands. 3) high-quality 3D assets including 20 scenes and 9000+ object models from ShapeNet . Based on realistic simulation and rich assets, we propose **GarmentLab Benchmark** containing 20 tasks divided into 5 groups to evaluate state-of-the-art vision-based and reinforcement learning based algorithm.

To tackle above challenges, our environment has three characteristics:1) **Efficient**. Garment manipulation involves nearly infinite object state and action spaces, requiring substantial data for models to understand garment structure and deformation. To meet this demand, our highly parallelized GPU-based simulator provides a significant training advantage. Larger batch sizes enhance RL-based algorithms , while faster data collection speeds reduce training time for perception-based algorithms (tackling **C1**). 2) **Rich**. The richness of our simulator can be categorized into two aspects: the diversity of **simulation content** offered by GarmentLab Assets and the depth of **physical interaction** facilitated by GarmentLab Engine. We emphasize multi-physics simulation, encompassing rigid-articulated, deformable-garment, fluid dynamics, and flow, along with their interactions. This focus is vital for training agents capable of comprehending real-world physical properties  (tackling **C2**). You can refer to videos in supplementary material for our simulation effects. 3) **Real**.

Figure 1: **GarmentLab** provides realistic simulation for diverse garments with different physical propoerties, benchmarking various novel garment manipulation tasks in both simulation and the real world.

[MISSING_PAGE_FAIL:3]

## 3 GarmentLab Environment

GarmentLab aims to integrate state-of-the-art physical simulation methods, modern graphics rendering engines, and user-friendly robotic interfaces into a unified framework (Figure 2). Below we will first introduce _GarmentLab Engine_ (Section 3.1) and _GarmentLab Asset_ (Section 3.2) to show our diversity in function and objects. As we especially focus on the exploration of multiple physical simulation methods and interaction between them, we will introduce _GarmentLab Physics_ in Section 3.3. In section 4 We will talk about our novel-proposed tasks.

### GarmentLab Engine

Built on NVIDIA's IsaacSim, GarmentLab offers a highly-paralleled data collection pipeline, realistic rendering, support for various sensors, and integration with Robot Operating System (ROS) .

**Data Pipeline.** Data pipeline mainly consists of two components: Visual Data System and RL-Training System. Visual System provides both RGB-D observations and ground-truth semantic label including 2D and 3D bounding box, normals and instance segmentation. Based on IsaacGym, RL System can establish multiple agents on GPU at the same time for efficient training.

**Rendering.** GarmentLab supports multiple camera angles, such as eye-on-hand and eye-on-base perspectives, unlike the single-camera setups of past works ,which employing naive OpenGL framework. Additionally, it utilizes GPU-enabled ray tracing for rendering, which enhances realism and challenge by creating more realistic shadows and lighting , thus reducing the sim2real gap and improving the performance of visual algorithms and mobile navigation tasks.

**ROS.** ROS is a generic and widely-used framework for building robot applications. We use ROS to align robot in realworld and the simulation, please refer to Section 6.2 for detailed. Also, although IsaacSim provides traditional Inverse-Kinematic and RMPFlow control, we also provide MoveIt framework for motion planning, which is more widely used in the real world.

**Sensor.** In addition to RGB-D observations, auxiliary observations can be accessed, such as robot joints, cloth particles and object poses. They are required in common RL framework and teacher-student network. Other Omniverse sensors (e.g., tactile, contact-report) could also be available.

Figure 2: **The Architecture of GarmentLab. (Left) Built on PhysX5, our environment supports various simulation methods. (Middle) Our environment can deliver realistic simulations of diverse robots, garments, and interactions between multiple physics media. (Right) Subsequently, we can utilize these assets to construct tasks across various categories. (Bottom) The framework supports real-world deployment.**

### GarmentLab Assets

GarmentLab Asset compiles simulation content from a variety of state-of-the-art datasets, integrating individual meshes or URDF files into complete, simulation-ready scenes with robots and sensors. We employ Universal Scene Description files to store all assets with attributes, including physics, semantics, and rendering properties. Key components along with their sources and categories are shown in Table 2. More details about each asset type are provided in Appendix A.

### GarmentLab Physics

**Simulation Method.** To ensure physically realistic simulation, we use tailored methods for different objects based on their physical characteristics. For **large garments and fluid**, we use Poston-Based Dynamics (PBD). For **small elastic garments** like gloves and socks, and everyday objects like toys and sponges, we apply Finite Element Method (FEM). **Human simulation** involves articulated skeletons with rotational joints and a surface skin mesh for high-fidelity rendering. **Robot simulation** utilizes PhysX articulation system for precise force control, P-D control, and inverse dynamics. Unlike previous works that rely on a single simulation method, GarmentLab provides platforms for exploring dynamics and kinematics of various objects and the coupling and interactions among them. **Diverse Physics Parameters.** To fully exploit the potential of various simulation methods and make garment simulation more diverse and realistic, GarmentLab provides various physics parameter configurations. For example, as cloth is modeled as a grid of particles, altering parameters such as particle size and stiffness will change garment physical behaviors. Likewise, as depicted in Figure 3, diverse physical material parameters are assigned to diverse objects. These parameters encompass, but are not limited to, surface tension and cohesion for fluids, friction for rigid objects, and modulus. It is worth noting that parameters influencing the interaction between different objects, including contact offset and reset offset, are also adjusted. For further details, please consult Appendix D.

   Asset Type & Sources & Categories \\   Garment and Cloth & ClothesNet & Hats, Ties, Masks, Gloves, Socks, Dishlcoths,... \\  Rigid and Articulated & ShapeNet, PartNet, YCB, PartNet-Mobility & Chairs, Boxes, Washing Machines, Storage Furniture,... \\  Robot & - & Franka, UR5, RidgebackFranka, ShadowHands \\  Human Model & Omniverse HumanModel & - \\  Materials & Omniverse Base Material & Fabric, Carpet, Leather, Silk, Cotton,... \\   

Table 2: **Key Components of GarmentLab Assets.**

Figure 3: **GarmentLab Physics.** GarmentLab explores the potential of different simulation methods, and provides different physical parameters, modeling the distinct properties of different materials in the real world.

GarmentLab Benchmark

**GarmentLab Benchmark** is motivated by the abilities that an intelligent manipulator agent should possess, including (1) understanding the physics of object interactions, (2) generating accurate action sequences for long-horizon, complex tasks, and (3) transferring this knowledge to the real world. To test these abilities, we classified tasks into five categories based on physical interactions. We also proposed several complex, long-horizon tasks to advance future research. The demos of tasks and real-world experiment are shown in figure 4.

**Task Categories.** To fully exploit the model's capability in understanding physical interactions and conduct comprehensive evaluations of current algorithms, we categorize 20 tasks into 5 groups. The example tasks and corresponding categories are shown in Table 3. Examples of task sequences are provided in Figure 4. You can refer to Appendix G to get more details.

**Long-Horizon Tasks.** With the advancement of robotics, there is a growing focus on completing long-horizon tasks, which integrate skills tasks such as 3D perception, manipulation and navigation. Thus we propose several long-horizon garment manipulation tasks, including _organizing clothes_, _wash clothes_, _make up tables_ and _dress up_. These tasks go beyond simply executing subtasks in sequence, as they require holistically planning how to accomplish the task based on the environment. During the execution, the algorithm needs to consider the positioning of the operation, the placement location, and carefully plan the path to avoid collisions. More analysis are shown in Section 7.

## 5 Real-World Benchmark

Real-world benchmark is crucial for not only evaluating the real-world performance of different methods, but also providing a standardized platform for researchers to reproduce and exchange methods. With the existence of real-world dataset or benchmarks for rigid , articulated  objects and furnitures , we introduce the first real-world benchmark for deformable objects and garments.

Unlike rigid or articulated objects that can be 3D-printed from CAD files, deformable objects are usually purchased without CAD files. Easily influenced by external forces, it is difficult to accurately model garments directly using traditional multi-camera calibration and surface reconstruction methods. Therefore, we use commercial scanning devices with lasers and light for mesh scanning.

Selected objects cover diverse garments (tops, trousers, socks, hats), plush toys, household items (bags, clutches), and cleaning supplies. They are primarily selected from well-known international brands for durability and accessibility. To ensure variety, objects have different shapes, sizes, transparencies, deformabilities, and textures. For instance, our dataset features various tops made from materials like assault jackets, down jackets, shirts, and vests, with a wide range of physical attributes.

Additionally, we provide semantic human annotations for object part masks and key points, supporting dexterous manipulation such as grasping specific parts and object tracking using key points. Following YCB, we present a systematic approach for defining manipulation protocols and benchmarks. These protocols specify the experimental setup for each task and provide procedural guidelines. A comprehensive description of the real-world benchmark is provided in Appendix F.

## 6 Sim2Real Framework

Transferring models from simulation to reality is crucial yet challenging. GarmentLab paves way to realistic application by integrating methods for mitigating vision (Sec. 6.1) and action (Sec. 6.2) gap.

   Task Type & Focus & Example \\   Garment-Garment & fundamental garment manipulation & fold, unfold, clothes piles retrieval... \\  Garment-Fluid & interaction between garments and fluids as well as flow, & Washing clothes, Drying clothes with a hairdryer... \\  Garment-FEMObjects & manipulating FEM Objects & Packing clothes, Dexterous grasp blush toy \\  Garment-Rigid & common interactions between clothing and rigid bodies & Hanging, Putting clothes into washing machine \\  Garment-Avatar & collaboration with human & Putting a scarf, Dress a person in T-shirt \\   

Table 3: **Task Categories of GarmentLab Benchmark.**Figure 4: **Diverse Tasks of GarmentLab Benchmark.** We introduced 20 garment and deformable manipulation tasks including complicated long-horizon tasks. The last row shows the execution of these tasks in the real world.

Figure 5: **Real-World Benchmark.** Part a demonstrates the whole pipeline of converting real-world objects into simulation assets. Part b demonstrates the performance of different categories of objects in both simulation and the real world (the first row), and the results of these objects being manipulated by the robot (the second row).

### Sim-Real Vision Alignment

GarmentLab integrates several automated and self-supervised sim2real methods, and have verified their effectiveness by predicting dense visual correspondence for manipulation  (Figure 6, Right), with quantitative manipulation success rate in Table 6.

**Keypoint Embedding Alignment.** Aligning corresponding skeleton point representations can mitigate representation gap between point cloud in simulation and the real world . By attaching markers to skeleton points and enabling robot to perform self-play, we obtain ground-truth keypoint pairs and employ InfoNCE to align corresponding point representations. Shown in Figure 6, the alignment adapts representations to the real-world distribution. Appendix E shows more details.

**Noisy Observation.** Adding noise to point cloud for training can be very effective for sim2real transfer . As shown in Figure 6, initial query results had many errors. By adding noise during training, our model became more robust, leading to smoother and more accurate representations.

**Point Cloud Alignment.** We propose aligning point clouds by optimizing an affine matrix, using chamfer distance as the loss function. As shown in Figure 6, the model initially predicts incorrect results, even for flat surfaces. However, after alignment, it successfully predicts accurate results.

### Real-World Motion Generation

For many algorithms, action trajectories generated in simulation is not align with those in the real world. We introduce two methods for generating trajectories in simulation that closely mimic real-world scenarios by leveraging prior knowledge of real-world manipulation trajectories.

**Teleoperation.** We've developed a lightweight, cost-effective teleoperation system requiring just one-click deployment. It facilitates simultaneous control of dexterous hands and grippers in both real-world and simulated settings (Figure 6). This system supports data collection for offline training, like diffusion policy.[67; 9]. Implementation details are in Appendix I

**MoveIt.** Incorporating MoveIt into our framework elevates motion planning and obstacle avoidance beyond heuristic trajectory methods, as noted in previous studies [57; 63]. Employing MoveIt for real-world robot execution also aids visual algorithms. Adapting models to MoveIt-generated trajectories during training reduces the sim-to-real gap. Detailed implementations are provided in Appendix H.

Figure 6: **Sim2Real Framework.** On the left, we highlight our MoveIt and teleoperation pipeline, a lightweight and easy-to-deploy system built using ROS. On the right, we present our three proposed visual sim-to-real algorithms, demonstrating a significant improvement in model performance after deploying these algorithms.

Experiments

### Simulation Experiment Setup

**Methods.** We selected three vision-based and two reinforcement learning (RL) algorithms for experiment, with details listed in Table 4. For vision-based algorithms, we prioritized those utilizing dense representations for garments, as they have demonstrated generalization ability and are suitable for various downstream tasks.

**Tasks.** Although we proposed many novel tasks, current algorithms cannot fully solve them. Thus, for large garments like tops, dresses, and trousers, we chose folding, hanging, and unfolding tasks. For small items like hats and gloves, we selected hanging and placing tasks to evaluate visual and RL algorithms. For dexterous and mobile tasks, existing work mostly employs RL algorithms. Hence, we evaluated the performance of both RL-state-based and RL-vision-based algorithms separately.

### Simulation Result and Analysis

Table 5 present quantitative comparisons between vision-based algorithms and RL-based algorithms. Figure 7 intuitively demonstrates the visual results of three different vision algorithms.

   Method & UniGarmentManip (UGM) & DIFT & Affordance & RL-State & RL-Vision \\   Type & 3D-Visual-Correspondence & 2D-Visual-Correspondence & 3D-Representation & RL & RL \\  BackBone & PointNet++ & Stable-Diffusion & PointNet++ & PPO & PPO \\  Input & Point Cloud & RGB Image & Point Cloud & State-Base GT & Partial Point Cloud \\   

Table 4: **Benchmark Methods of GarmentLab.**

    &  &  \\  Method & Fold & Unfold & Hang & Place & Hang \\   UGM & **61.5** / **62.1** / **59.8** & **58.3** / **60.5** / **57.2** & 61.8 / 57.5 / 59.7 & 33.2 / 35.4 & 31.8 / 29.2 \\  DIFT & 32.7 / 36.7 / 31.2 & 18.7 / 23.3 / 17.6 & 31.2 / 27.6 / 29.7 & **66.4** / **63.2** & **64.7** / **61.2** \\  Affordance & 53.2 / 51.8 / 56.9 & 32.4 / 36.7 / 31.8 & **64.1** / **60.2** / **61.3** & 63.2 / 61.5 & 62.6 / 60.4 \\  RL-State & 14.8 / 12.5 / 9.8 & 6.5 / 8.8 / 12.7 & 13.1 / 19.7 / 14.7 & 14.2 / 12.1 & 12.8 / 13.2 \\  RL-Vision & 6.7 / 8.2 / 3.2 & 5.2 / 6.2 / 8.8 & 7.6 / 5.3 / 4.1 & 13.1 / 14.8 & 11.3 / 15.2 \\   

Table 5: **Simulation Results on Traditional Tasks.** Numbers in the Large-piece column represent scores for Tops, Trousers and Skirts. Numbers in the Small-piece column represent scores for Hats and Gloves.

Figure 7: **Qualitative Results.** We visualize the qualitative results of the three vision-based algorithms: the left, middle, and right sections of this image correspond to the Affordance, Correspondence, and DIFT algorithms, respectively. Note that the DIFT exhibits query errors. For detailed analysis, please refer to Experiment Section.

**Vision-Based Algorithm.** Among the three vision-based algorithms, UGM performed best on large-piece clothing, emphasizing cross-deform and cross-object consistency in learning representations, DIFT excels with small-piece clothing due to its robustness to object rotation but lacks proficiency in understanding clothing folding. Affordance works well for tasks that do not require precise point selection, such as hanging, but struggles with folded garments.

**RL Algorithm.** Compared to vision-based algorithms, RL performs poorly on garment manipulation due to the complex dynamics of garments. Our analysis of training videos showed that RL often generates abnormal trajectories, causing clothes to get tangled with the robotic arm or be pushed away. This issue is more pronounced with RL-vision-based methods, as the higher-dimensional and partial visual observations hinder the model's ability to converge on an effective strategy. For dexterous and mobile tasks, the larger action and search spaces result in suboptimal performance. Further discussion and analysis can be found in B.

### Real-World Experiments

In our real-world experiments, we focused on testing vision-based algorithms due to the risk associated with RL actions. T-shirts for folding and hats for hanging, were selected for experimentation. Additionally, we conducted ablation study on proposed sim2real methods using UGM (Table 6). Our real-world results align with our simulation findings, indicating GarmentLab environment can enhance real-world applications. For sim2 real algorithm, without point cloud alignment and noise augmentation along with keypoint embedding alignment can improve representation smoothness and accuracy. Qualitative sim2real results are shown in Figure 6 (Right).

## 8 Conclusion

We introduce GarmentLab, a comprehensive environment and benchmark for manipulating garments and deformable objects. GarmentLab includes the GarmentLab Engine, supporting various simulation methods and ROS integration; GarmentLab Assets, a diverse dataset of robots, materials, and garments; and GarmentLab Benchmark, proposing several novel tasks. It also provides the first real-world deformable benchmark along with several sim2real methods.

## 9 Acknowledgment

This project is supported by The National Natural Science Foundation of China (No. 62376006), the National Youth Talent Support Program (8200800081) and the National Natural Science Foundation of China (No. 62136001).

   Method & UGM & DIFT & Affordance & w/o PA & w/o Noise & w/o EA \\   Tshirt-Folding & **10**/15 & 8/15 & 6/15 & 2/15 & 8/15 & 7/15 \\  Hat-Hanging & 10/15 & **14**/15 & 9/15 & 5/15 & 8/15 & 9/15 \\   

Table 6: **Real-World Experiment and Ablation Results**, w/o PA, w/o Noise, and w/o EA respectively represent UniGarmentManip without Pointcloud Alignment, Noise Augmentation, and KeyPoint Embedding Alignment.