# Graph Denoising Diffusion for Inverse Protein Folding

Kai Yi

University of New South Wales

kai.yi@unsw.edu.au

&Bingxin Zhou

Shanghai Jiao Tong University

bingxin.zhou@sjtu.edu.cn

&Yiqing Shen

Johns Hopkins University

yshen92@jhu.edu

&Pietro Lio

University of Cambridge

pl219@cam.ac.uk

&Yu Guang Wang

Shanghai Jiao Tong University

University of New South Wales

yuguang.wang@sjtu.edu.cn

equal contribution.

###### Abstract

Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we utilize amino acid replacement matrices for the diffusion forward process, encoding the biologically meaningful prior knowledge of amino acids from their spatial and sequential neighbors as well as themselves, which reduces the sampling space of the generative process. Our model achieves state-of-the-art performance over a set of popular baseline methods in sequence recovery and exhibits great potential in generating diverse protein sequences for a determined protein backbone structure. The code is available on https://github.com/ykiiiiiii/GraDe_IF.

## 1 Introduction

Inverse protein folding, or inverse folding, aims to predict feasible amino acid (AA) sequences that can fold into a specified 3D protein structure . The results from inverse folding can facilitate the design of novel proteins with desired structural and functional characteristics. These proteins can serve numerous applications, ranging from targeted drug delivery to enzyme design for both academic and industrial purposes . In this paper, we develop a diffusion model tailored for graph node denoising to obtain new AA sequences given a protein backbone.

Despite its importance, inverse folding remains challenging due to the immense sequence space to explore, coupled with the complexity of protein folding. On top of energy-based physical reasoning of a protein's folded state , recent advancements in deep learning yield significant progress in learning the mapping from protein structures to AA sequences directly. For example, discriminative modelsformulate this problem as the prediction of the most likely sequence for a given protein structure [7; 10; 17; 50]. However, they have struggled to accurately capture the one-to-many mapping from the protein structure to non-unique AA sequences.

Due to their powerful learning ability, _diffusion probabilistic models_ have gained increasing attention. They are capable of generating a diverse range of molecule outputs from a fixed set of conditions given the inherent stochastic nature. For example, Torsion Diffusion  learns the distribution of torsion angles of heavy atoms to simulate conformations for small molecules. Concurrently, SMCDiff enhances protein folding tasks by learning the stable scaffold distribution supporting a target motif with diffusion. Similarly, DiffDock adopts a generative approach to protein-ligand docking, creating a range of possible ligand binding poses for a target pocket structure.

Despite the widespread use of diffusion models, their comprehensive potential within the context of protein inverse folding remains relatively unexplored. Current methods in sequence design are primarily anchored in language models, encompassing _Masked Language Models_ (MLMs) [25; 23] and _autoregressive generative models_[17; 26; 28]. By tokenizing AAs, MLMs formulate the sequence generation tasks as masked token enrichment. These models usually operate by drawing an initial sequence with a certain number of tokens masked as a specific schedule and then learning to predict the masked tokens from the given context. Intriguingly, this procedure can be viewed as a discrete diffusion-absorbing model when trained by a parameterized objective. Autoregressive models, conversely, can be perceived as deterministic diffusion processes . It induces conditional distribution to each token, but the overall dependency along the entire AA sequence is recast via an independently-executed diffusion process.

On the contrary, diffusion probabilistic models employ an iterative prediction methodology that generates less noisy samples and demonstrates potential in capturing the diversity inherent in real data distributions. This unique characteristic further underscores the promising role diffusion models could play in advancing the field of protein sequence design. To bridge the gap, we make the first attempt at a diffusion model for inverse folding. We model the inverse problem as a denoising problem where the randomly assigned AA types in a protein (backbone) graph is recovered to the wild type. The protein graph which contains the spatial and biochemical information of all AAs is represented by equivariant graph neural networks, and diffusion process takes places on graph nodes. In real inverse folding tasks, the proposed model achieves SOTA recovery rate, improve 4.2% and 5.4% on recovery rate for single-chain proteins and short sequences, respectively,, especially for conserved region which has a biologically significance. Moreover, the predicted structure of generated sequence is identical to the structure of native sequence.

The preservation of the desired functionalities is achieved by innovatively conditioning the model on both secondary and third structures in the form of residue graphs and corresponding node features. The major contributions of this paper are three-fold. Firstly, we propose GraDe-IF, a diffusion model backed by roto-translation equivariant graph neural network for inverse folding. It stands out

Figure 1: Overview of GraDe-IF. In the diffusion process, the original amino acid is stochastically transitioned to other amino acids, leveraging BLOSUM with varied temperatures as the transition kernel. During the denoising generation phase, initial node features are randomly sampled across the 20 amino acids with a uniform distribution. This is followed by a gradual denoising process, conditional on the graph structure and protein secondary structure at different time points. We employ a roto-translation equivariant graph neural network as the denoising network.

from its counterparts for its ability to produce a wide array of diverse sequence candidates. Secondly, as a departure from conventional uniform noise in discrete diffusion models, we encode the prior knowledge of the response of AAs to evolutionary pressures by the utilization of _Blocks Substitution Matrix_ as the translation kernel. Moreover, to accelerate the sampling process, we adopt Denoising Diffusion Implicit Model (DDIM) from its original continuous form to suit the discrete circumstances and back it with thorough theoretical analysis.

## 2 Problem Formulation

### Residue Graph by Protein Backbone

A residue graph, denoted as \(=(,,)\), aims to delineate the geometric configuration of a protein. Specifically, every node stands for an AA within the protein. Correspondingly, each node is assigned a collection of meticulously curated node attributes \(\) to reflect its physiochemical and topological attributes. The local environment of a given node is defined by its spatial neighbors, as determined by the \(k\)-nearest neighbor (\(k\)NN) algorithm. Consequently, each AA node is linked to a maximum of \(k\) other nodes within the graph, specifically those with the least Euclidean distance amongst all nodes within a 30A contact region. The edge attributes, represented as \(^{93}\), illustrate the relationships between connected nodes. These relationships are determined through parameters such as inter-atomic distances, local N-C positions, and a sequential position encoding scheme. We detail the attribute construction in Appendix C.

### Inverse Folding as a Denoising Problem

The objective of inverse folding is to engineer sequences that can fold to a pre-specified desired structure. We utilize the coordinates of C\(\) atoms to represent the 3D positions of AAs in Euclidean space, thereby embodying the protein backbone. Based on the naturally existing protein structures, our model is constructed to generate a protein's native sequence based on the coordinates of its backbone atoms. Formally we represent this problem as learning the conditional distribution \(p(^{ aa}|^{ pos})\). Given a protein of length \(n\) and a sequence of spatial coordinates \(^{ pos}=\{_{1}^{ pos},,_{ i}^{ pos}, ,_{ i}^{ pos}\}\) representing each of the backbone C\(\) atoms in the structure, the target is to predict \(^{ aa}=\{_{1}^{ aa},,_{ i}^{ aa},,_{n}^{ aa}\}\), the native sequence of AAs. This density is modeled in conjunction with the other AAs along the entire chain. Our model is trained by minimizing the negative log-likelihood of the generated AA sequence relative to the native wild-type sequence. Sequences can then be designed either by sampling or by identifying sequences that maximize the conditional probability given the desired secondary and tertiary structure.

### Discrete Denoising Diffusion Probabilistic Models

Diffusion models belong to the class of generative models, where the training stage encompasses diffusion and denoising processes. The diffusion process \(q(_{t}_{0})=_{t=1}^{T}q(_{t} _{t-1})\) corrupts the original data \(_{0} q()\) into a series of latent variables \(\{_{1},,_{T}\}\), with each carrying progressively higher levels of noise. Inversely, the denoising process \(p_{}(_{0},_{1},...,_{T})=p(_{T} )_{t=1}^{T}p_{}(_{t-1}_{t})\) gradually reduces the noise within these latent variables, steering them back towards the original data distribution. The iterative denoising procedure is driven by a differentiable operator, such as a trainable neural network.

While in theory there is no strict form for \(q(_{t}_{t-1})\) to take, several conditions are required to be fulfilled by \(p_{}\) for efficient sampling: (i) The diffusion kernel \(q(_{t}|_{0})\) requires a closed form to sample noisy data at different time steps for parallel training. (ii) The kernel should possess a tractable formulation for the posterior \(q(_{t-1}_{t},_{0})\). Consequently, the posterior \(p_{}(_{t-1}|_{t})= q(_{t-1}_{t}, _{0})p_{}(_{0}|_{t})\), and \(_{0}\) can be used as the target of the trainable neural network. (iii) The marginal distribution \(q(_{T})\) should be independent of \(_{0}\). This independence allows us to employ \(q(_{T})\) as a prior distribution for inference.

The aforementioned criteria are crucial for the development of suitable noise-adding modules and training pipelines. To satisfy these prerequisites, we follow the setting in previous work . For categorical data \(_{t}\{1,...,K\}\), the transition probabilities are calculated by the matrix \([_{t}]_{ij}=q(_{t}=j_{t-1}=i)\). Employing the transition matrix and on one-hot encoded categorical feature \(_{t}\), we can define the transitional kernel in the diffusion process by:

\[q(_{t}_{t-1})=_{t-1}_{t} q(_{t})=}_{t},\] (1)

where \(}_{t}=_{1}_{t}\). The Bayes rule yields that the posterior distribution can be calculated in closed form as \(q(_{t-1}_{t},)_{t}_{t}^{ }}_{t-1}\). The generative probability can thus be determined using the transition kernel, the model output at time \(t\), and the state of the process \(_{t}\). Through iterative sampling, we eventually produce the generated output \(_{0}\).

The prior distribution \(p(_{T})\) should be independent of the observation \(_{0}\). Consequently, the construction of the transition matrix necessitates the use of a noise schedule. The most straightforward and commonly utilized method is the uniform transition, which can be parameterized as \(_{t}=_{t}+(1-_{t})_{d}_{d}^{}/d\) with \(^{}\) be the transpose of the identity matrix \(\), \(d\) refers to the number of amino acid types (_i.e._, \(d=20\)) and \(_{d}\) denotes the one vector of dimension \(d\). As \(t\) approaches infinity, \(\) undergoes a progressive decay until it reaches \(0\). Consequently, the distribution \(q(_{T})\) asymptotically approaches a uniform distribution, which is essentially independent of \(\).

## 3 Graph Denoising Diffusion for Inverse Protein Folding

In this section, we introduce a discrete graph denoising diffusion model for protein inverse folding, which utilizes a given graph \(=\{,,\}\) with node feature \(\) and edge feature \(\) as the condition. Specifically, the node feature depicts the AA position, AA type, and the spatial and biochemical properties \(=[^{},^{},^{}]\). We define a diffusion process on the AA feature \(^{}\), and denoise it conditioned on the graph structure \(\) which is encoded by _equivariant neural networks_. Moreover, we incorporate protein-specific prior knowledge, including an _AA substitution scoring matrix_ and protein _secondary structure_ during modeling. We also introduce a new acceleration algorithm for the discrete diffusion generative process based on a transition matrix.

### Diffusion Process and Generative Denoising Process

Diffusion ProcessTo capture the distribution of AA types, we independently add noise to each AA node of the protein. For any given node, the transition probabilities are defined by the matrix \(_{t}\). With the predefined transition matrix, we can define the forward diffusion kernel by

\[q(_{t}^{}_{t-1}^{})=_{ t-1}^{}_{t} q(_{t}^{}^{})=^{ }}_{t},\]

where \(}_{t}=_{1}_{t}\) is the transition probability matrix up to step \(t\).

Training Denoising NetworksThe second component of the diffusion model is the denoising neural network \(f_{}\), parameterized by \(\). This network accepts a noisy input \(_{t}=(_{t},)\), where \(_{t}\) is the concatenation of the noisy AA types and other AA properties including 20 one-hot encoded AA type and 15 geometry properties, such as SASA, normalized surface-aware node features, dihedral angles of backbone atoms, and 3D positions. It aims to predict the clean type of AA \(^{}\), which allows us to model the underlying sequence diversity in the protein structure while maintaining their inherent structural constraints. To train \(f_{}\), we optimize the cross-entropy loss \(L\) between the predicted probabilities \((^{})\) for each node's AA type.

Parameterized Generative ProcessA new AA sequence is generated through the reverse diffusion iterations on each node \(\). The generative probability distribution \(p_{}(_{t-1}|_{t})\) is estimated from the predicted probability \((^{}|_{t})\) by the neural networks. We marginalize over the network predictions to compute for generative distribution at each iteration:

\[p_{}(_{t-1}_{t})_{}^{ }}q(_{t-1}|_{t},^{})_{}( ^{}|_{t}),\] (2)

where the posterior

\[q(_{t-1}_{t},^{})=( _{t-1}_{t}Q_{t}^{}^{} _{t-1}}{^{}_{t}_{t}^{}})\] (3)

can be calculated from the transition matrix, state of node feature at step \(t\) and AA type \(^{}\). The \(^{}\) is the sample of the denoising network prediction \((^{})\).

### Prior Distribution from Protein Observations

#### 3.2.1 Markov Transition Matrices

The transition matrix serves as a guide for a discrete diffusion model, facilitating transitions between the states by providing the probability of moving from the current time step to the next. As it reflects the possibility from one AA type to another, this matrix plays a critical role in both the diffusion and generative processes. During the diffusion stage, the transition matrix is iteratively applied to the observed data, which evolves over time due to inherent noise. As diffusion time increases, the probability of the original AA type gradually decays, eventually converging towards a uniform distribution across all AA types. In the generative stage, the conditional probability \(p(_{t-1}|_{t})\) is determined by both the model's prediction and the characteristics of the transition matrix \(\), as described in Equation 2.

Given the biological specificity of AA substitutions, the transition probabilities between AAs are not uniformly distributed, making it illogical to define random directions for the generative or sampling process. As an alternative, the diffusion process could reflect evolutionary pressures by utilizing substitution scoring matrices that conserve protein functionality, structure, or stability in wild-type protein families. Formally, an _AA substitution scoring matrix_ quantifies the rates at which various AAs in proteins are substituted by other AAs over time . In this study, we employ the Blocks Substitution Matrix (BLOSUM) , which identifies conserved regions within proteins that are presumed to have greater functional relevance. Grounded in empirical observations of protein evolution, BLOSUM provides an estimate of the likelihood of substitutions between different AAs. We thus incorporate BLOSUM into both the diffusion and generative processes. Initially, the matrix is normalized into probabilities using the softmax function. Then, we use the normalized matrix \(\) with different probability temperatures to control the noise scale of the diffusion process. Consequently, the transition matrix at time \(t\) is given by \(_{t}=^{T}\). By using this matrix to refine the transition probabilities, the generative space to be sampled is reduced effectively, thereby the model's predictions converge toward a meaningful subspace. See Figure 2 for a comparison of the transition matrix over time in random and BLOSUM cases.

#### 3.2.2 Secondary Structure

Protein secondary structure refers to the local spatial arrangement of AA residues in a protein chain. The two most common types of protein secondary structure are alpha helices and beta sheets, which are stabilized by hydrogen bonds between backbone atoms. The secondary structure of a protein serves as a critical intermediary, bridging the gap between the AA sequence and the overall 3D conformation of the protein. In our study, we incorporate eight distinct types of secondary structures into AA nodes as conditions during the sampling process. This strategic approach effectively narrows down the exploration space of potential AA sequences. Specifically, we employ DSSP (Define Secondary Structure of Proteins) to predict the secondary structures of each AA and represent these structures using one-hot encoding. Our neural network takes the one-hot encoding as input and utilizes it to denoise the AA conditioned on it.

The imposition of motif conditions such as alpha helices and beta sheets on the search for AA sequences not only leads to a significant reduction in the sampling space of potential sequences, but also imparts biological implications for the generated protein sequence. By conditioning the sampling process of AA types on their corresponding secondary structure types, we guide the resulting protein

Figure 2: The middle two panels depict the transition probability of Leucine (L) from \(t=0\) to \(T\). Both the uniform and BLOSUM start as Dirichlet distributions and become uniform at time \(T\). As shown in the two side figures, while the uniform matrix evenly disperses L’s probability to other AAs over time, BLOSUM favors AAs similar to L.

sequence towards acquiring not only the appropriate 3D structure with feasible thermal stability but also the capability to perform its intended function.

### Equivariant Graph Denoising Network

Bio-molecules such as proteins and chemical compounds are structured in the 3-dimensional space, and it is vital for the model to predict the same binding complex no matter how the input proteins are positioned and oriented to encode a robust and expressive hidden representation. This property can be guaranteed by the rotation equivariance of the neural networks. A typical choice of such a network is an equivariant graph neural network . We modify its SE(3)-equivariant neural layers to update representations for both nodes and edges, which reserves SO(3) rotation equivariance and E(3) translation invariance. At the \(l\)th layer, an Equivariant Graph Convolution (EGC) inputs a set of \(n\) hidden node embeddings \(^{(l)}=\{_{1}^{(l)},,_{n}^{(l)}\}\) describing AA type and geometry properties, edge embedding \(_{ij}^{(l)}\) with respect to connected nodes \(i\) and \(j\), \(^{}=\{_{1}^{},,_{n}^{}\}\) for node coordinates and \(t\) for time step embedding of diffusion model. The target of a modified EGC layer is to update hidden representations \(^{(l+1)}\) for nodes and \(^{(l+1)}\) for edges. Concisely, \(^{(l+1)},^{(l+1)}=[^{(l)},^{},^{(l)},t]\). To achieve this, an EGC layer defines

\[_{ij}^{(l+1)} =_{e}(_{i}^{(l)},_{j}^{(l)},\| _{i}^{(l)}-_{j}^{(l)}\|^{2},_{ij}^{(l)})\] (4) \[_{i}^{(l+1)} =_{i}^{(l)}+_{j i}(_ {i}^{(l)}-_{j}^{(l)})_{x}(_{ij}^{(l+1)})\] \[_{i}^{(l+1)} =_{h}_{i}^{(l)},_{j i}_{ij }^{(l+1)},\]

where \(_{e},_{h}\) are the edge and node propagation operations, respectively. The \(_{x}\) is an additional operation that projects the vector edge embedding \(_{ij}\) to a scalar. The modified EGC layer preserves equivariance to rotations and translations on the set of 3D node coordinates \(^{}\) and performs invariance to permutations on the nodes set identical to any other GNNs.

### DDIM Sampling Process

A significant drawback of diffusion models lies in the speed of generation process, which is typically characterized by numerous incremental steps and can be quite slow. Deterministic Denoising Implicit Models (DDIM)  are frequently utilized to counter this issue in continuous variable diffusion generative models. DDIM operates on a non-Markovian forward diffusion process, consistently conditioning on the input rather than the previous step. By setting the noise variance on each step to \(0\), the reverse generative process becomes entirely deterministic, given an initial prior sample.

Similarly, since we possess the closed form of generative probability \(p_{}(_{t-1}|_{t})\) in terms of a predicted \(^{}\) and the posterior distribution \(p(_{t-1}|_{t},^{})\), we can also render the generative model deterministic by controlling the sampling temperature of \(p(^{}|_{t})\). Consequently, we can define the multi-step generative process by

\[p_{}(_{t-k}_{t})(_{}^{ }}q(_{t-k}|_{t},^{})(^{}|_{t}))^{T}\] (5)

where the temperature \(T\) controls whether it is deterministic or stochastic, and the multi-step posterior distribution is

\[q(_{t-k}_{t},^{})=( _{t-k}|_{t}Q_{t}^{} Q_{t-k}^{} ^{}_{t-k}}{^{}_{t}_{t}^{}} ).\] (6)

## 4 Experiments

We validate our GraDe-IF on recovering native protein sequences in **CATH**. The performance is mainly compared with structure-aware SOTA models. The implementations for the main algorithms (see Appendix D) at https://github.com/ykiiiii/GraDe_IF are programmed with PyTorch-Geometric (ver 2.2.0) and PyTorch (ver 1.12.1) and executed on an NVIDIA(r) Tesla V100 GPU with \(5,120\) CUDA cores and \(32\)GB HBM2 installed on an HPC cluster.

### Experimental Protocol

Training SetupWe employ **CATH v4.2.0**-based partitioning as conducted by GraphTrans and GVP. Proteins are categorized based on **CATH** topology classification, leading to a division of \(18,024\) proteins for training, \(608\) for validation, and \(1,120\) for testing. To evaluate the generative quality of different proteins, we test our model across three distinct categories: _short_, _single-chain_, and _all_ proteins. The short category includes proteins with sequence lengths shorter than 100. The single-chain category encompasses proteins composed of a single chain. In addition, the total time step of the diffusion model is configured as \(500\), adhering to a cosine schedule for noise . For the denoising network, we implement six stacked EGNN blocks, each possessing a hidden dimension of \(128\). Our model undergoes training for default of \(200\) epochs, making use of the Adam optimizer. A batch size of \(64\) and a learning rate of \(0.0005\) are applied during training. Moreover, to prevent overfitting, we incorporate a dropout rate of 0.1 into our model's architecture.

Evaluation MetricThe quality of recovered protein sequences is quantified by _perplexity_ and _recovery rate_. The former measures how well the model's predicted AA probabilities match the actual AA at each position in the sequence. A lower perplexity indicates a better fit of the model to the data. The recovery rate assesses the model's ability to recover the correct AA sequence given the protein's 3D structure. It is typically computed as the proportion of AAs in the predicted sequence that matches the original sequence. A higher recovery rate indicates a better capability of the model to predict the original sequence from the structure.

### Inverse Folding

Table 1 compares GraDe-IF's performance on recovering proteins in **CATH**, with the last two columns indicating the training dataset of each baseline method. To generate high-confidence

    &  &  &  \\   & Short & Single-chain & All & Short & Single-chain & All & 4.2 & 4.3 \\  StructGNN & 8.29 & 8.74 & 6.40 & 29.44 & 28.26 & 35.91 & ✓ & \\ GraphTrans & 8.39 & 8.83 & 6.63 & 28.14 & 28.46 & 35.82 & ✓ & \\ GCA & 7.09 & 7.49 & 6.05 & 32.62 & 31.10 & 37.64 & ✓ & \\ GVP & 7.23 & 7.84 & 5.36 & 30.60 & 28.95 & 39.47 & ✓ & \\ GVP-large & 7.68 & 6.12 & 6.17 & 32.6 & 39.4 & 39.2 & ✓ & \\ AlphaDesign & 7.32 & 7.63 & 6.30 & 34.16 & 32.66 & 41.31 & ✓ & \\ ESM-ifei & 8.18 & 6.33 & 6.44 & 31.3 & 38.5 & 38.3 & & ✓ \\ ProteinMPNN & 6.21 & 6.68 & 4.57 & 36.35 & 34.43 & 49.87 & ✓ & \\ PiFold & 6.04 & 6.31 & \(4.55\) & \(39.84\) & 38.53 & 51.66 & ✓ & \\  GraDe-IF & **5.49** & **6.21** & **4.35** & **45.27** & **42.77** & **52.21** & ✓ & \\   

Table 1: Recovery rate performance of **CATH** on zero-shot models.

Figure 3: Recovery rate on core and surface residues and different secondary structure

sequences, GradDe-IF integrates out uncertainties in the prior by approximating the probability \(p(^{ aa})_{i=1}^{N}p(^{ aa}|_{T}^{i})p(_{T }^{i})\). Notably, we observed an improvement of \(4.2\%\) and \(5.4\%\) in the recovery rate for single-chain proteins and short sequences, respectively. We also conducted evaluations on different datasets (Appendix E) and ablation conditions (Appendix F).

Upon subdividing the recovery performance based on buried and surface AAs, we find that the more conserved core residues exhibit a higher native sequence recovery rate. In contrast, the active surface AAs demonstrate a lower sequence recovery rate. Figure 3 examines AA conservation by Solvent Accessible Surface Area (SASA) (with SASA\(<0.25\) indicating internal AAs) and contact number (with the number of neighboring AAs within 8 A in 3D space) . The recovery rate of internal residues significantly exceeds that of external residues across all three protein sequence classes, with the recovery rate increasing in conjunction with the contact number. We also present the recovery rate for different secondary structures, where we achieve high recovery for the majority of secondary structures, with the exception of a minor \(5\)-turn helix structure that occurs infrequently.

### Foldability

We extend our investigation to the foldability of sequences generated at various sequence recovery rates. We fold generated protein sequences (by GraDe-IF) with AlphaFold2 and align them with the crystal structure to compare their closeness in Figure 4 (PDB ID: 3FKF). All generated sequences are nearly identical to the native one with their RMSD \( 1\) A over \(139\) AAs, which is lower than the resolution of the crystal structure at \(2.2\) A. We have also folded the native sequence by AlphaFold2, which yields an average pLDDT of \(0.91\). In comparison, the average pLDDT scores of the generated sequences are \(0.835\), underscoring the reliability of their folded structures. In conjunction with the evidence presented in Figure 3 which indicates our method's superior performance in generating more identical results within conserved regions, we confidently posit that GraDe-IF can generate biologically plausible novel sequences for given protein structures (See Appendix G).

The numerical investigation is reported in Table 2, where we pick the first \(100\) structures (ordered in alphabetical order by their PDB ID) from the test dataset and compare the performance of GraDe-IF with ProteinMPNN and PiFOLD. We follow  and define the quality of a novel sequence by the TM score between its AlphaFold2-folded structure and the native structure, with an above-\(0.5\) score indicating the design is _successful_. Overall, our GraDe-IF exhibits high pLDDT, low RMSD, and high foldability. There are several proteins that face challenges with both GraDe-IF and baseline methods for folding with a high TM score, _i.e._, 1BCT, 1BHA, and 1CYU, whose structural determination is based on NMR, an experimental technique that analyzes protein structure in a buffer solution. Due to the presence of multiple structures for a single protein in NMR studies, it is reasonable for folding tools to assign low foldability scores.

   Method & Success & TM score & avg pLDDT & avg RMSD \\  PiFOLD & \(85\) & \(0.80 0.22\) & \(0.84 0.15\) & \(1.67 0.99\) \\ ProteinMPNN & \(94\) & \(0.86 0.16\) & \(0.89 0.10\) & \(1.36 0.81\) \\ GraDe-IF & \(94\) & \(0.86 0.17\) & \(0.86 0.08\) & \(1.47 0.82\) \\   

Table 2: Numerical comparison between generated sequence structure and the native structure.

Figure 4: Folding comparison of GraDe-IF-generated sequences and the native protein (in nude).

### Diversity

Given the one-to-many mapping relationship between a protein structure and its potential sequences, an inverse folding model must be capable of generating diverse protein sequences for a fixed backbone structure. To investigate the diversity of GraDe-IF in comparison to baseline methods, we define _diversity_ as \(1-\)self-similarity, where higher diversity indicates a model's proficiency in generating distinct sequences. We employ PiFold, ProteinMPNN, and GraDe-IF to generate new proteins for the test dataset at low, medium, and high recovery levels. We vary the temperature for the former two baseline methods within the range \(\{0.5,0.1,0.0001\}\) and adjust the sample step for GraDe-IF from \(\{1,10,50\}\). The average performance from sampling \(10\) sequences is summarized in Table 3, revealing comparable results among the three models in both recovery rate and diversity. In general, increasing the probability temperature for PiFold and ProteinMPNN (or decreasing the sample step in GraDe-IF) leads to a reduction in uncertainty and more reliable predictions, resulting in higher diversities and lower recovery rates for all three models. While all three methods demonstrate the ability to recover sequences at a similar level, particularly with high probability temperatures (or low sample steps), GraDe-IF produces significantly more diverse results when the step size is minimized. Our findings demonstrate that GraDe-IF can generate sequences with a \(60\%\) difference in each sample, whereas PiFold and ProteinMPNN achieve diversity rates below \(50\%\).

We next explore the foldability of these highly diverse protein sequences designed by GraDe-IF. Figure 5 compares the generated proteins (folded by AlphaFold2) with the crystal structure (PDB ID: 3FKF). We vary the step size over a range of \(1,5,10,50\), generating \(10\) sequences per step size to calculate the average pLDDT, TM score, and diversity. For simplicity, we visualize the first structure in the figure. Decreasing the step size results in the generation of more diverse sequences. Consequently, there is a slight reduction in both pLDDT and TM scores. However, they consistently remain at a considerably high level, with both metrics approaching \(0.8\). This reduction can, in part, be attributed to the increased diversity of the sequences, as AlphaFold2 heavily relies on the MSA sequences. It is expected that more dissimilar sequences would produce a more diverse MSA. Remarkably, when step\(=1\), the sequence diversity exceeds \(0.6\), indicating that the generated sequences share an approximately \(0.3\) sequence similarity compared to the wild-type template protein sequence. This suggests the generation of protein sequences from a substantially distinct protein family when both pLDDT and TM scores continue to exhibit a high degree of confidence.

## 5 Related Work

Deep Learning models for protein sequence designSelf-supervised models have emerged as a pivotal tool in the field of computational biology, providing a robust method for training extensive

Figure 5: Folding structures of generated protein sequences with different steps.

    &  &  &  \\ 
**Method** & diversity & recovery & diversity & recovery & diversity & recovery \\  PiFold & 0.37 & 0.47 & 0.25 & 0.50 & 0.21 & 0.50 \\ ProteinMPNN & 0.51 & 0.42 & 0.27 & 0.46 & 0.26 & 0.52 \\ GraDe-IF & 0.61 & 0.33 & 0.54 & 0.47 & 0.25 & 0.53 \\   

Table 3: Numerical comparison on diversity and recovery rateprotein sequences for representation learning. These models are typically divided into two categories: structure-based generative models and sequence-based generative models. The former approaches protein design by formulating the problem of fixed-backbone protein design as a conditional sequence generation problem. They predict node labels, which represent AA types, with invariant or equivariant graph neural networks . Alternatively, the latter sequence-based generative models draw parallels between protein sequences and natural language processing. They employ attention-based methods to infer residue-wise relationships within the protein structure. These methods typically recover protein sequences autoregressively conditioned on the last inferred AA , or employing a BERT-style generative framework with masked language modeling objectives and enable the model to predict missing or masked parts of the protein sequence .

Denoising Diffusion modelsThe Diffusion Generative Model, initially introduced by Sohl-Dickstein _et al._ and further developed by Ho _et al._, has emerged as a potent instrument for a myriad of generative tasks in continuous time spaces. Its applications span diverse domains, from image synthesis  to audio generation , and it has also found utility in the creation of high-quality animations , the generation of realistic 3D objects , and drug design . Discrete adaptations of the diffusion model, on the other hand, have demonstrated efficacy in a variety of contexts, including but not limited to, text generation , image segmentation , and graph generation . Two distinct strategies have been proposed to establish a discrete variable diffusion process. The first approach involves the transformation of categorical data into a continuous space and then applying Gaussian diffusion . The alternative strategy is to define the diffusion process directly on the categorical data, an approach notably utilized in developing the D3PM model for text generation . D3PM has been further extended to graph generation, facilitating the joint generation of node features and graph structure .

## 6 Conclusion

Deep learning approaches have striven to address a multitude of critical issues in bioengineering, such as protein folding, rigid-body docking, and property prediction. However, only a few methods have successfully generated diverse sequences for fixed backbones. In this study, we offered a viable solution by developing a denoising diffusion model to generate plausible protein sequences for a predetermined backbone structure. Our method, referred to as GraDe-IF, leverages substitution matrices for both diffusion and sampling processes, thereby exploring a practical search space for defining proteins. The iterative denoising process is predicated on the protein backbone revealing both the secondary and tertiary structure. The 3D geometry is analyzed by a modified equivariant graph neural network, which applies roto-translation equivariance to protein graphs without the necessity for intensive data augmentation. Given a protein backbone, our method successfully generated a diverse set of protein sequences, demonstrating a significant recovery rate. Importantly, these newly generated sequences are generally biologically meaningful, preserving more natural designs in the protein's conserved regions and demonstrating a high likelihood of folding back into a structure highly similar to the native protein. The design of novel proteins with desired structural and functional characteristics is of paramount importance in the biotechnology and pharmaceutical industries, where such proteins can serve diverse purposes, ranging from targeted drug delivery to enzyme design for industrial applications. Additionally, understanding how varied sequences can yield identical structures propels the exploration of protein folding principles, thereby helping to decipher the rules that govern protein folding and misfolding. Furthermore, resolving the inverse folding problem allows the identification of different sequences that fold into the same structure, shedding light on the evolutionary history of proteins by enhancing our understanding of how proteins have evolved and diversified over time while preserving their functions.