# Local Superior Soups: A Catalyst for Model Merging

in Cross-Silo Federated Learning

Minghui Chen\({}^{12}\) Meirui Jiang\({}^{3}\) Xin Zhang\({}^{4}\) Qi Dou\({}^{3}\) Zehua Wang\({}^{1}\) Xiaoxiao Li\({}^{12}\)

\({}^{1}\)University of British Columbia \({}^{2}\)Vector Institute \({}^{3}\)Chinese University of Hong Kong \({}^{4}\)Meta

Correspondence to xiaoxiao.li@ece.ubc.ca

###### Abstract

Federated learning (FL) is a learning paradigm that enables collaborative training of models using decentralized data. Recently, the utilization of pre-trained weight initialization in FL has been demonstrated to effectively improve model performance. However, the evolving complexity of current pre-trained models, characterized by a substantial increase in parameters, markedly intensifies the challenges associated with communication rounds required for their adaptation to FL. To address these communication cost issues and increase the performance of pre-trained model adaptation in FL, we propose an innovative model interpolation-based local training technique called "Local Superior Soups." Our method enhances local training across different clients, encouraging the exploration of a connected low-loss basin within a few communication rounds through regularized model interpolation. This approach acts as a catalyst for the seamless adaptation of pre-trained models in in FL. We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets. Our code is available at https://github.com/ubc-tea/Local-Superior-Soups.

## 1 Introduction

Federated learning (FL)  has emerged as a promising methodology for leveraging the power of private data without the need for centralized data governance. However, data heterogeneity in FL poses significant challenges to the design of efficient training for global convergence. With the emergence of the pre-training and fine-tuning paradigm in various applications , recent studies  have attempted to address the problem of FL under data heterogeneity with pre-trained initialization. Although pre-trained federated learning can speed up convergence compared to random initialization, it still requires a significant number of communication rounds between the server and clients, often amounting to hundreds of rounds . Existing pre-trained models  often have an enormous parameter scale, and following the neural scaling law , there is a continuous trend toward increasing model parameters. Deploying models with such a large parameter size in FL introduces significant communication overhead. This greatly hampers the flexibility and scalability of model updates. Reducing FL communication overhead can be approached by reducing the scale of model parameters involved in distributed training  or reducing communication rounds . Comparing with reducing model parameters, reducing communication rounds typically leads to a more efficient reduction of network congestion , decreased energy consumption on client devices , and a lower risk of privacy breaches . _In this paper, we focus on reducing communication rounds in FL with pre-trained model as initialization._

Typically, increasing the number of local training steps can effectively reduce communication rounds. However, there is an upper limit to the extent of local training step increments. This limitation arises due to the presence of data heterogeneity, where the consistency of optimization among different clients deteriorates with the increasing number of local steps . This optimization inconsistency leads to a discrepancy between local and global models and decelerates theconvergence rate of FL. The discrepancy is often called client drift . Previously, some FL methods [25; 45] attempted to introduce proximal terms to regularize local training, with the aim of reducing local overfitting and minimizing the problem of client drift. While these methods can accelerate convergence, they restrict the progress of each local training steps towards the optimal solution, impeding the attainment of FL with more aggressive communication round reductions.

While these client drift mitigation methods can reduce local overfitting to some extent, they cannot ensure strong performance of the global aggregated models, particularly in scenarios with limited communication rounds. This situation arises when individual local clients become trapped in isolated low-loss valleys. More specifically, as illustrated in Figure 1, two models from clients 'A' and 'B', even if their optimal model distance is small, still result in a poorly performing aggregated global model. Moreover, the preceding FL methods aimed at minimizing communication rounds exclusively address scenarios involving random initialization, lacking a customized approach tailored to pre-trained models. Recent proposed centralized fine-tuning methods (_e_.\(g\)., model soups  and DiWA  - a greedy model selection version of model soups) based on model interpolation (averages of a large number of model weights) are effective approaches to seek large connected low-loss region, which are promising for applying in FL to reduce communication rounds. These methods can prevent individual clients from being trapped in isolated low-loss valleys by positioning the global model centrally within a larger low-loss region by overlapping the low-loss regions among clients, as shown in Fig. 1 (right). However, their training efficiency is exceedingly low, requiring complete retraining of numerous models, leading to _significant computational overhead_ on clients and intolerable communication costs when applied in FL, due to two aspects: First, they involve a time-consuming model selection phase within the model pool, which consists of all candidate models available for weight interpolation. Secondly, model soups entail an extensive number of model training iterations, lacking prior guidance and relying on brute-force, random, and often redundant training. Many of the trained models end up unused.

To enjoy the connected low-loss valley benefits of model soup-based methods [52; 43] without burdening local training, we propose an efficient and local model interpolation-based method, called **Local Superior Soups (_LSS_)**. To address the first issue, we propose a **sequential random model interpolation** method during training. This eliminates the need for subsequent model selection steps and ensures that the models slated for interpolation reside within the same low-loss valley during training (Sec. 3.3.1). For the second issue, we introduce two quantifiable indicators of candidate model quality, inspired by data augmentation quality quantification [32; 4]: **diversity** (Sec. 3.3.2) and **affinity** (Sec. 3.3.3). Specifically, the _diversity indicator_ quantifies the diversity among models in the model pool with their pairwise model distance, where larger distances denote higher diversity, signifying better model quality for interpolation. As illustrated in Figure 2 (left), a low-loss region, supported by models with low diversity, can be effectively covered with only a few candidate models positioned near its periphery. Thus, we propose incorporating the diversity metric as a regularization term during training to maximize the expansion of low-loss regions, thereby increasing the utilization of trained models. The _affinity indicator_ measures the affinity of each candidate model in the model pool to the initial model. Smaller distances indicate greater affinity, indicating better model quality for interpolation. This affinity is also incorporated as a regularization term during training to prevent the expansion of low-loss regions from deviating too far from the shared initialization point, thus increasing the likelihood of overlapping connected regions (as depicted on the right side of Fig. 2). These two indicators facilitate the efficient inclusion of models into the model pool, preventing wasteful training of models that may ultimately go unused.

Figure 1: Illustration on isolated (left) and connected low-loss valley with larger regions in dark red (right).

Figure 2: Illustration on diversity (left) and affinity (right) regularization.

In experiments, we found that our proposed method greatly reduces communication rounds, and we achieved the performance of models fused after multiple rounds of communication in other FL methods with only a few rounds of communication.

In summary, our contributions are as follows.

(1) We reveal the importance of regularizing local client models in the connected low-loss valleys for reducing communication rounds when initializing FL with pre-trained models. (2) We introduce an innovative and efficient model soups-based method for FL, called Local Superior Soups (_LSS_) that eliminates the need for time-consuming model selection and redundant model training in the existing soups-based approaches, while expanding connected low-loss valleys of client models for faster convergence. (3) In experimental evaluations, _LSS_ demonstrates a significant reduction in communication rounds, achieving superior performance with only a few rounds of communication, exceeding baseline FL methods significantly in four datasets and two types of distribution shifts.

## 2 Related Work

### Heterogeneous Federated Learning

FL struggles with Non-IID data, leading to various proposed algorithms. FedProx  uses proximal term to regularize local training, preventing client divergence. Scaffold  adds variance reduction to combat "clients-drift." MOON  employs mode-level contrastive learning to stabilize local training. Personalized FL  targets high local performance on Non-IID data. FedBN  applies local batch normalization to mitigate feature shift before model averaging. Recent one-shot and few-round FL methods use parallel server-side techniques like prediction ensembles , data generation [56; 18], or meta-learning  to improve aggregated model performance.

### Federated Fine-tuning and Model Interpolation

Fine-tuning leverages pre-trained models to enhance task performance . FedFTG  proposes knowledge distillation for global model fine-tuning in FL. Personalized FL employs fine-tuning to adapt global models to local ones, _e.g._, FedBABU , FTFA, and RTFA . However, this focus on local performance neglects global generalization. Inspired by linear mode connectivity [36; 12], Model Soups  combines runs with varied hyper-parameters to improving fine-tuning performance in the centralized setting. DiWA  and other Soups-based methods [52; 43; 3] extends this concept, emphasizing the importance of model diversity. Some methods induce diversity through high learning rates , cosine similarity minimization , tempered posteriors , or auxiliary dataset-trained model soups . We depict the difference of different model ensemble-based methods in our appendix 7.

## 3 Method

The structure of this Section is as follows: firstly, we provide the problem definition and corresponding notions to be used (Sec. 3.1); secondly, we reveal the dilemma for existing federated learning methods on reducing communication rounds (Sec. 3.2); finally, we propose a regularized model interpolation-based method as a solution, provide corresponding analysis (Sec. 3.3), and present the overall algorithm flow.

### Notions and Problem Definition

**Notions.** Let \(\) be the input space of data, \(\) be the label space. Consider a FL setting with \(M\) clients, \(\) local steps and \(R\) communication rounds. Let \(:=\{_{i}\}_{i=1}^{M}\) be a set of \(M\) domain, each of which is a distribution over the input space \(\). For each client, we have access to \(n\) training data points in the form of \((_{i},_{i})=\{(x_{j}^{i},y_{j}^{i})\}_{j=1}^{n}\), where \(y_{j}^{i}\) denotes the target label for input \(x_{j}^{i}\). Let \(f^{m}\) represents the parameter for the global model, \(_{i}:^{m}\) denotes the local objective function at client \(i\), and \(\) denotes a distribution on the entire set of clients. We provide a notation table in Appendix A to clarify the meanings of the corresponding notations.

**Problem definition.** We aim to address the challenge of optimizing the global performance on \(\) of aggregated models fine-tuned from different clients with data heterogeneity, while minimizing communication rounds between the clients and the server in data Non-IID setting. In terms of global performance, we perform empirical risk minimization (ERM) on the sampled data \(_{i}\) for \(i[M]\),

\[(f)=_{i=1}^{M}p_{i}_{i}(f),_{i}(f)=_{i}|}_{_{i}}_{ i}(f,)_{i=1}^{M}p_{i}=1.\] (1)

### Effect of Regularization and Local Steps on FL Convergence under Data Heterogeneity

In this section, we present a theoretical analysis to understand how communication rounds, local steps, and our introduced regularization terms affect the convergence bound in federated learning. Formally, we present the error term and posit the following assumptions for the purpose of analysis. Our analysis builds on the assumptions and convergence bound in  with formal statements in Appendix B.1. We first present a theorem providing a convergence guarantee when the proposed regularization terms are applied.

**Theorem 3.1** (Convergence Rate for Convex Local Functions with Affinity and Diversity Constraint).: _Under Convexity and Smoothness Assumption on \(\)-smooth loss function, Bounded Variance of Stochastic Gradient and Bounded Variance of Local and Global Gradient assumptions, when the client learning rate is chosen properly as, \(=\{,-}d}{^{}R^{ }},,-}}{R^{}\,3\,^{ }},}}{ R^{}\,3\,(+c)^{ }}\}\)_

_we define \(=[_{r=0}^{R-1}_{k=1}^{} (^{(r,k)})-(f^{})]\), and have_

\[}{ R}+}+ {5^{}^{}d^{}}{^{}R^ {}}+}(+c)^{}d^{}}{R^{}}\] (2)

Here, the update rule of the \(t\) iteration with the affinity and diversity term is defined as \((t+1)=(t)- g(t)-q(t,_{t},_{a})\), and the extra term satisfies \(q(t,_{t},_{a}) c\). The hyper-parameters \(_{t}\) and \(_{a}\) represent the co-efficient of tuning affinity and diversity respectively.

Besides, \(d:=\|f^{(0,0)}-f^{}\|\) refers to the distance between initialization \(f^{(0,0)}\) and the global optimum \(f^{}\), \(\) bounds variance of stochastic gradient by \([\|g_{i}(f^{(r,k)})-_{i}(f^{(r,k)})\|^{2}|f^{(r,k) }]^{2}\), and \(\) bounds variance of local and global gradient by \(_{i}_{f}\|_{i}(f^{(r,k)})-(f^{ (r,k)})\|\).

**How to reduce communication rounds under data heterogeneity?** Increasing local fine-tuning steps seems to be a straightforward technique to reduce communication costs. Nevertheless, this approach cannot reduce the an error term in the convergence rate (see the 4th term of the RHS of Eq. 2), which remains unaltered by increasing local steps. Moreover, increasing local update steps in the presence of Non-IID client data exacerbates the inconsistency in local objectives, further magnifying this error term. Here, we provide a more detailed explanation, specifically identifying the conditions under which increasing iteration steps can effectively reduce communication rounds.

**Proposition 3.2**.: _Under the data heterogeneity setting, when the total number of gradient computations across all clients (\(K=M R\)) is fixed and the local steps \(\) satisfies_

\[ }}{M^{2}}},\] (3)

_the error upper bound Eq.2 will be dominated by the second term \((1/)\)._

We provide the proof for Proposition 3.2 in Appendix B.2. Accordingly, increasing the bound in Eq. 2 and meeting the aforementioned condition for local steps allows us to reduce communication rounds. From the above in-equation, we can observe that although increasing the number of local training steps can reduce communication rounds, there is a limit to the number of steps that can be added. This limit is primarily determined by the error term introduced by local updates.

**Why connecting low-loss valley in local training with pre-trained initialization can achieve extreme communication rounds reduction?** Our analysis indicates that for substantial communication efficiency in federated learning, it is not enough to just increase local training steps. The focus should be on minimizing the error term from local updates, particularly the last term in Formula 2. This term, influenced by gradient dissimilarity (\(\)), distance to optimal weights (\(d\)), and our proposed regularization update bound \(c\), remains significant even as training steps increase.

Prior research suggests  that pre-training initialization reduces \(\) by aligning client updates, and overparameterization ensures that the optimal parameters are typically close to the initialization [22; 6; 31], decreasing \(d\). It is important to note that the regularization related term \(c\) is influenced by a combination of model diversity and affinity, and can be reduced by adjusting the parameters \(_{t}\) and \(_{a}\). In the absence of a common pre-trained initialization, ensuring model affinity within the model pool is often challenging (_i.e_., models from different clients tend to diverge significantly from the initialization values), resulting in a larger value of \(c\), which in turn affects the effectiveness of our method. Therefore, our approach is more suitable when combined with pre-trained models. Consequently, a combination of pre-training and our proposed connectivity preserving local training can effectively lower error terms from local updates, increasing the limit of local training steps and thus reducing communication rounds. More experimental support see our Appendix.

### Our Solution: _Lss_ Algorithm

In this part, we first present the shortcomings of the previous model soups method applied in FL. Secondly, we propose our three targeted improvements, _i.e_. _random model interpolation_ (Sec. 3.3.1), _diversity term_ (Sec. 3.3.2), and _affinity regularization term_ (Sec. 3.3.3). Finally, we present the complete algorithm process and detailed implementation in local client training.

**Limitation of previous model soups methods.** Previous model soups methods  can induce a trained model located in a connected low-loss valley, but their training efficiency is exceedingly low, due to two aspects: _Time-Consuming model selection phase:_ Firstly, these methods involve a time-consuming model selection phase, which consists of all candidate models available for weight interpolation [3; 52]. This phase aims to choose models that reside within the same low-loss valleys. During this selection process, significant computational resources are consumed to identify suitable models for interpolation, adding to the overall training time and complexity. _Extensive and redundant model training:_ Secondly, model soups entail an extensive number of model training iterations, lacking prior guidance and relying on brute-force, random, and often redundant training [29; 52]. Many of the trained models end up unused, further exacerbating the computational inefficiency.

#### 3.3.1 Random interpolation conserving connected low-loss region.

To address the _time-consuming model selection_ issue of the previous soups-based method, we propose a sequential random model interpolation method during training. This innovative approach streamlines the training process by eliminating the need for subsequent model selection steps within the **model pool** (_i.e_., local models to be interpolated), which traditionally consumes a considerable amount of computational resources and time. Let \(=\{f_{p_{1}},f_{p_{2}},,f_{p_{N}}\}\) be a pool of \(N\) models, where \(f_{p_{i}}\) represents the weights of the \(i\)-th model. We define the interpolated model \(f_{}\) as a weighted combination of the models in \(\). The interpolation coefficients \(=(_{1},_{2},,_{N})\) are sampled using a uniform distribution and normalization strategy. The interpolated model \(f_{}\) isthen computed as: \(f_{}=_{i=1}^{N}_{i}f_{p_{i}}\). Here, \(_{i}\) represents the weight assigned to the \(i\)-th model in the pool. The uniform distribution ensures that the coefficients \(_{i}\) are non-negative and sum to 1, providing a simple and effective way to combine the model weights from the pool \(\). Forward and backward propagation are performed using the interpolated model, updating the weights of the currently active model (_i.e_., the newly added model ) (corresponding to Algorithm 1 Line 7), while previously added model weights remain fixed.

#### 3.3.2 Diversity term.

The diversity term is proposed to address the _redundant model training_ issue of the previous soups-based methods by encouraging low-loss region expansion. In particular, the diversity indicator assesses the variability among models within the model pool by summing the distances between pairs of models. Greater distances between models indicate a higher degree of diversity, which correlates with enhanced model quality. This diversity metric is integrated into the FL local training process as a regularization term to facilitate the extensive enlargement of low-loss regions, consequently maximizing the effectiveness of trained models. The diversity term (in Algorithm 1 Line 8) measures the distance between the current training model and other models that will be averaged, and we hope that this distance to be large. The diversity loss can be defined as

\[_{}=(f,)=_{n=1}^{N} (f,f_{n}).\] (4)

Here, \(f_{n}\) belongs to local interpolated model pool \(\) and \(N\) is the number of local candidate models. The candidate models (_i.e_., model soups ingredients) are models to be interpolated in local training, and the model pool is the set of local candidate models (see Algorithm 1 Line 5). We use the \(_{2}\) norm to measure the distance between model weights.

#### 3.3.3 Affinity term.

The affinity term is proposed to control the expansion of low-loss regions and prevent local candidate model training divergence. The affinity indicator assesses the level of alignment between each candidate model within the model pool and the initial global model by calculating the cumulative distances between each candidate model and the initialization model. Smaller distances between models signify a stronger affinity, indicating higher model quality. To ensure the controlled expansion of low-loss regions and reduce the probability of overlapping connected regions, this affinity metric is integrated into the training process as a regularization term. The affinity term (in Algorithm 1 Line 8) measures the distance between the candidate model and the initial model weights, with the aim of minimizing this dissimilarity (maximize this loss term) to ensure that the distance remains relatively small. The affinity loss can be defined as

\[_{}=(f,f_{p}).\] (5)

Here, \(f_{p}\) is a pre-trained model in the first communication round (\(R=1\)). Moreover, it encourages each local model to lie in a close zone in the parameter space, which is beneficial for subsequent server aggregation, especially under data heterogeneity. We use \(l_{2}\) distance for the \(dist(,)\) metric for both Eq. 4 and Eq. 5.

#### 3.3.4 Overall pipeline.

We outline _LSS_ as follows: We begin with the initialization of the client's local model with the pretrained global model. Then we will refine the local model using affinity and diversity loss. This step is performed for a few local update steps. Finally, after updating local model, we aggregate them in the server following the common averaging operation in FedAvg . The flow of _LSS_ for local updating (Step 2 described in Sec 3.1) can be found in Algorithm 1.

In conclusion, our method aims to minimize the distance between the local fine-tuned model and the pre-trained initialized global model while maximizing the distance between the model soups ingredients (_i.e_., the models to be averaged). Our fine-tuned models find large low-loss regions on their respective local datasets while ensuring parameters close to the pre-trained initialization. It is intuitive that the parameters of our fine-tuned models can be more easily aligned with those of models fine-tuned on similar datasets, thereby improving communication efficiency.

[MISSING_PAGE_FAIL:7]

\(64\). For commonly used FL methods, due to the significant increase in local update steps that leads to worse convergence, we set their local update steps to \(8\). For SWA, SWAD, and our method, we take more local update steps, with each model being averaged trained \(8\) steps, and the default number of models to be averaged is \(4\). For the Model Soups method and DiWA, we train \(32\) models with \(8\) steps. Additional details of experiment implementations are included in the Appendix.

### Performance Comparison

**Results on label shift.** To demonstrate the effectiveness of _LSS_ on label shift scenario, we conduct comparison experiments on FMNIST and CIFAR-10 datasets. We consider fine-tuning with an extremely limited number of communication rounds (_i.e_., \(R=1\) and \(R=3\)). Table 1 reports the test accuracy with the format of mean (std) for all compared algorithms. All experiments are repeated \(3\) runs with different random seeds. In Table 1, _LSS_ achieves the best accuracy on all settings of both datasets, which validates that _LSS_ is efficient and effective in fine-tuning FL for label shift Non-IID. Notably, with just one round of communication, _LSS_ can double the accuracy of the best Non-IID FL baseline method. Surprisingly, the simple extensions of model-averaging-based domain generalization methods onto FedAvg  (the 2nd big row in Table 1) perform very well, especially when the number communication round is small. The superior performance using local weight averaging-based fine-tuning is likely because it significantly reduces the gradient variance of local and global variance (see 3.2). We further provide results on different levels of label shift in the supplementary material.

**Results on feature shift.** Table 2 evaluates on feature shift scenario using Digits-5 and DomainNet datasets. Similar to the previous experiment setting for Table 1, we repeat all the algorithms with \(3\) random seeds. Consistent with the observation in Table 2, _LSS_ is the top-performing method under all the settings for both datasets. We also observe better performance achieved by adapting model-averaging-based domain generalization methods (the 2nd big row in Table 2) in FL than the existing Non-IID FL methods (the 1st big row in Table 2), which further verifies the effectiveness of model averaging to obtain better global model while improving communication efficiency.

**Convergence plots.** We also evaluate the strength of _faster convergence_ using the proposed _LSS_ compared with FedAvg  on CIFAR-10 (label shift) and Digits-5 (feature shift). Fig. 3 depicts the testing accuracies at early and late phases regarding the number of communication rounds to reach convergence. First, by looking at the final testing accuracies on Fig. 3 (b) and (d), _LSS_ achieves better performance. Second, Fig. 3 (a) and (c) show that _LSS_ almost meets the targeted performance at the very early stage (_i.e_.around 6 to 8 rounds), whereas FedAvg requests over hundreds of communication rounds.

**Parameter-Efficient Tuning with ViT.** We also deployed the Vision Transformer (ViT)  in FL learning. On Digits-5 dataset, we evaluate the ViT model with a resolution of \(224\) and a patch size of \(16\), which was pretrained on the ImageNet-21k dataset. Due to the large number of parameters in ViT, we used a parameter-efficient fine-tuning method called LoRA  to train it for all the methods. For more details about our ViT architecture and LoRA training, please refer to the appendix. It can be observed in Fig. 4 that our method is applicable to pre-trained ViT models, demonstrating that our approach can be combined with parameter-efficient fine-tuning methods to further enhance the communication efficiency of FL.

Figure 4: Evaluation on ViT fine-tuned with LoRA (Digit5 dataset).

Figure 3: Convergence comparison of our proposed _LSS_ with FedAvg. _LSS_ achieves high accuracy much earlier (around 6 to 8 rounds) than FedAvg, which takes hundreds of communication rounds.

### Ablation Studies

We conducted ablation experiments on the main components (_i.e._, affinity, diversity term and averaged model quantity) of our proposed method and evaluated their performance on the CIFAR dataset, with the performance metric being the global model performance at communication round \(R=1\).

**Investigation on regularization losses.** In order to examine the importance of affinity loss and diversity loss, as well as the influence of their corresponding coefficients, we adjust one coefficient within a range of 0 to 4 while maintaining the other at a constant value. By comparing the performance with and without loss term, we observe that adding affinity and diversity terms can enhance the model's performance. Additionally, we observe that the two terms complement each other, and selecting appropriate coefficients can achieve significant performance improvement (_e.g._, adjusting the affinity coefficient to \(3\) as shown in Fig. 5 (a) and diversity coefficient to 3 as shown in Fig. 5 (b)).

**Investigation on the number of averaged models.** To investigate the impact of the averaged model quantity on enhancing communication efficiency and reducing gradient variance between local and global, we experiment with varied model quantities and evaluate their influence on global model performance, averaged local model performance2, and worst out-of-distribution (OOD) generalization performance on the other clients. Fig. 6 shows that increasing the number of averaged models can improve the model's OOD generalization ability and enhance the performance of the aggregated model. This similar upward trend confirms the validity of our analysis linking OOD generalization and local-global variance. We provide a more detailed analysis on connecting our proposed _LSS_ and OOD generalization in appendix C. Additionally, we can observe that increasing the number of models in our method can improve both pre-aggregation and post-aggregation model performance.

## 5 Conclusion

**Limitations and Broader Impact.** Our method reduces communication rounds but trades off training memory and performance. Future work should explore more memory-efficient deployments. While focused on vision tasks, extending to language and multimodal scenarios is promising. Balancing performance and communication in healthcare FL is promising, but excessive reduction can impair critical medical decisions. Careful trade-off consideration is essential for reliable FL applications in sensitive areas.

**Conclusion.** We propose an efficient method, Local Superior Soups (_LSS_), to reduce communication rounds in FL with pre-trained initialization, addressing the challenge of data heterogeneity. By employing sequential model interpolation, connectivity preservation, and two regularization terms (diversity and affinity), the method allows for an increase in local training steps and a reduction in communication rounds while avoiding client drift. This approach, tailored for pre-trained model adaptation in FL, offers training and inference efficiency, making it suitable for practical deployment in edge computing scenarios. As the first step towards understanding and developing model soups-based methods in pre-trained models in FL, this study conducts experiments on benchmark datasets. Our method attain superior performance with a only few rounds of communication and surpasses the performance of standard FL methods significantly across four datasets and under two distribution shift scenarios.

Figure 5: Ablation on the affinity & diversity.

Figure 6: Ablation studies on the impact of the number of averaged models on communication efficiency and performance variance. We evaluated the influence of varied model quantities on global and averaged local model performance, as well as generalization on the worst client.

Acknowledgement.M. Chen, Z. Wang and X. Li are grateful for the support of the Natural Science and Engineering Research Council of Canada (NSERC). M. Chen and X. Li are supported by the Canada CIFAR AI Chairs program, MITACS-CIFAR Catalyst Grant Program, NVIDIA Hardware Awards, the Digital Research Alliance of Canada, and Canada Foundation for Innovation (CFI). M. Jiang and Q. Dou are supported by the Research Grants Council of the Hong Kong Special Administrative Region (Project No. T45- 401/22-N).