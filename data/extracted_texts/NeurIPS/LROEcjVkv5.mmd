# Asymptotically Optimal Quantile Pure Exploration

for Infinite-Armed Bandits

 Xiao-Yue Gong

Carnegie Mellon University

exiaoyue@andrew.cmu.edu

&Mark Sellke

Harvard University

msellke@fas.harvard.edu

###### Abstract

We study pure exploration with infinitely many bandit arms generated i.i.d. from an unknown distribution. Our goal is to efficiently select a single high quality arm whose average reward is, with probability \(1-\), within \(\) of being with the top \(\)-fraction of arms. For fixed confidence, we give an algorithm with expected sample complexity \(O(})\) which matches a known lower bound up to the \((1/)\) factor. In particular the \(\)-dependence is optimal and closes a quadratic gap. For fixed budget, we show the asymptotically optimal sample complexity as \( 0\) is \((1/)(1/)^{2}/c\). The value of \(c\) depends explicitly on the problem parameters (including the unknown arm distribution) through a certain Fisher information distance. Even the strictly super-linear dependence on \((1/)\) was not known and resolves a question of .

## 1 Introduction

In many learning problems, one faces the classical exploration versus exploitation tradeoff. A central example is the (stochastic) multi-armed bandit , where an agent is presented with a set of arms each of which when played gives a stochastic reward from an unknown and arm-dependent distribution. The performance of a bandit algorithm is most commonly determined by its _regret_, i.e. the difference between its average reward and the expected reward from the best arm. Multi-armed bandits and extensions have been applied in many settings including medical trials , online advertising , cognitive radio , and information retrieval . Optimal algorithms for the multi-armed bandit, including UCB, Thompson sampling, EXP3, and various forms of mirror descent, all make a principled tradeoff between exploration and exploitation.

In this work we focus on _pure exploration_ bandit problems, a setting motivated by situations where the learning procedure consists of an _initial exploration_ phase followed by a _choice_ of policy to deploy. This is the case in hyperparameter optimization  as well as reinforcement learning from simulated environments. As there is no longer a competing need to exploit, optimal algorithms for pure exploration differ from the more common regret setting.

Pure exploration problems were introduced in  in the probably-approximately-correct (PAC) model. Here given \(K\) arms, one adaptively obtains samples until choosing one of the arms to output - the goal is to ensure that with probability \(1-\), this arm has average reward within \(\) of the best arm, with minimum possible sample complexity depending on \(\) and \(\). The early works above focused on the _fixed confidence_ setting in which one aims to minimize the expected sample complexity. Many subsequent works have also considered the _fixed budget_ problem where the sample complexity is _uniformly_ bounded.

While sharp results are known for pure exploration and other bandit problems with \(K\) arms, for many applications such as advertising there are far too many arms to explore. This motivated the study of infinite-armed bandit problems in e.g. ; the pure exploration versionwas first studied in . The main contribution of our work is to obtain near-optimal sample complexity for pure exploration problems with infinitely many arms in both the fixed confidence and fixed budget settings.

### Problem Formulation

We now precisely formulate the infinite-armed pure exploration setting. Let \(=\{a_{1},a_{2},\}\) be a countably infinite set of stochastic bandit arms indexed by \(i=1,2,\). When an arm \(i\) is sampled, it returns a \(\{0,1\}\)-valued reward with mean \(p_{i}\). The values \(p_{i}\) are drawn i.i.d. from an arbitrary reservoir distribution \(\) supported in \(\) (which is unknown to the player). We define the cumulative distribution function

\[G_{}()=^{p}[p]\]

of \(\), and its (left-continuous) inverse

\[G_{}^{-1}(p)=\{:G() p\}.\]

Finally let \(^{*}=G_{}^{-1}(1)\) denote the essential supremum of \(\), i.e. the maximum of its support.

An algorithm \(\) interacts with \(\) in the following way. At each time step \(t=1,2,,T\) the algorithm samples an arm \(i_{t}\), and observes a corresponding Bernoulli reward \(r_{t}(p_{i_{t}})\). The reward \(r_{t}\) is independent of previous actions and feedback. Eventually at some time \(T\), \(\) chooses an arm \(a_{i^{*}}\) to output. If the time-horizon \(T=N\) is fixed, we say \(\) has a _fixed budget_ constraint. If \([T] N\) is bounded only in expectation, we say \(\) has a _fixed confidence_ constraint.

For \(,,>0\), we say \(\) is \((,,)\)-PAC if

\[p_{i^{*}} G^{-1}(1-)- 1-\] (1.1)

and set

\[ G^{-1}(1-)\]

to be the target quantile value. We emphasize that while \(\) is known, \(\) may not be as it depends on the unknown \(\). The definition (1.1) stems from . As brief justification for the parameter \(\), note that in an infinite-armed setting the reservoir \(\) could give \(\)-optimal arms with arbitrarily small probability. Thus it is impossible to give a non-asymptotic classical \((,)\)-PAC in our setting without assumptions on \(\). Taking \(>0\) as above ensures that a positive fraction of arms are "good enough" and will enable such guarantees. One also cannot set \(\) to zero: for example if \(\) is supported in \([0.5-e^{-N^{4}},0.5]\), the quantile value of the output arm will be essentially uniform for any \(N\)-sample algorithm.

The purpose of this paper is to give \((,,)\)-PAC algorithms whose sample complexity \(N\) is minimal. We can now state our main results. Let us emphasize that unless explicitly stated, no assumptions on the reservoir distribution \(\) are made, nor does the algorithm have any prior knowledge about \(\). Our main result in the fixed confidence case is as follows.

**Theorem 1.1**.: _For any \((,,)\), there exists a \((,,)\)-PAC algorithm with expected sample complexity \(O(})\)._

In the fixed budget setting, our interest is especially in the high-confidence regime \( 0\), where we obtain the following. The following statement is a slightly informal combination of Theorems 3.1 and 3.2. We note that while the main statement requires \(\) to be given, this is not essential in several cases as discussed extensively in the Appendix. For example if \(\) then \(\) does not need to be given.

**Theorem 1.2** (Informal).: _For any fixed \((,)\), let \(=G^{-1}(1-)\) be given and set \(=-\). Then as \( 0\), the optimal \((,,)\)-PAC algorithm under fixed budget has sample complexity_

\[N =c_{,}^{-1} o_{ 0}(1)(1/ )(1/)^{2};\] \[c_{,} (1-2)-(1-2)^{ 2}}{2}\] (1.2) \[=^{}})^ {2}}{2}\,.\] (1.3)An equivalent statement is that given exactly \(N\) samples, the optimal failure probability \(\) to have \(p_{i^{*}} G^{-1}(1-)-\) decays as \((-(N)})\). Indeed once \(\) and \(\) are fixed, the question of minimizing the sample complexity \(N=N()\) (given a target confidence \(\)) is equivalent to minimizing the failure probability \(=(N)\) (given a sample complexity \(N\)). These viewpoints are equivalent in both settings we study, and we switch between them at times.

Interestingly the value \(\) makes no appearance in Theorem 1.2, so it is asymptotically irrelevant for the \( 0\) regime of fixed budget pure exploration. In fact the value \((1-2)-(1-2)\) appearing in the definition of \(c_{,}\) is the Fisher-information distance between \(\) and \(\) in the exponential family of Bernoulli random variables via the formula (1.3). See just below Theorem 3.1 for a brief explanation of why \(\) does not enter the asymptotic sample complexity.

**Remark 1.1**.: _In our problem formulation above we assumed rewards are Bernoulli, i.e. lie in \(\{0,1\}\). In fact as long as the quality of arm \(i\) is measured by its mean reward, this assumption loses no generality and is just a technical convenience: our results extend verbatim to \(\)-valued rewards._

_Indeed any arm with \(\)-valued rewards can be transformed into a Bernoulli arm with \(\{0,1\}\)-valued rewards and the same mean: simply turn reward \(r\) into reward \(1\) with probability \(r\), and \(0\) with probability \(1-r\). Note that this reduction (used also in Section 1.2 of ) might increase the instance-dependent sample complexity of some reservoir distributions, but our results only refer to the distribution of arm means under the reservoir which is unchanged by the reduction._

### Further Notation

We use the convention that algorithms collect \(1\) sample per unit time until terminating, so the time \(t\) equivalently denotes the number of total samples collected so far. Denote by \(n_{i,t}\) the number of samples of arm \(a_{i}\) collected by time \(t\). The \(n\)-th time \(a_{i}\) is sampled, its reward is \(r_{i,n}\). The total reward of arm \(i\) up to time \(t\) is

\[R_{i,t}=_{n=1}^{n_{i,t}}r_{i,n}.\]

The corresponding average reward is \(_{i,t}=_{i}(n_{i,t})=}{n_{i,t}}\). We use \(C\) to indicate a universal constant independent of all parameters in this paper, and \(o_{n}(1)\) and \(o_{N}(1)\) to denote quantities tending to \(0\) as \(n\) or \(N\), with other parameters implicitly held constant. However in Section C we use e.g. \(_{,}\) to indicate an asymptotic lower bound with implicit constant factor depending on the values of \(,\), which are treated as fixed. In all our uses of these notations it is \(n\) or \(N\) which is tending to infinity while other parameters are always treated as fixed.

### Related Work

As discussed above, this work belongs to the area of _pure exploration_ for multi-armed bandit problems. Unlike ordinary bandit problems where one aims to minimize the regret compared to the best arm , in pure exploration all that matters is the final arm selected by the algorithm. We survey several existing results below, with an emphasis on the high-probability regime of small \(\). See e.g. Chapter 33 of  for a more detailed survey.

Pure exploration was first studied in  in the probably-approximately-correct model. Here given \(K\) arms, one adaptively obtains samples until choosing one of the arms to output - the goal is to ensure that with probability \(1-\), this arm has average reward within \(\) of the best arm. These works showed that the optimal fixed confidence sample complexity is \((})\).

Later,  considered the _simple regret_ of pure exploration problems, namely the regret incurred at the final timestep.  studied the closely related problem of identifying the best arm, obtaining nearly tight sample complexity bounds in terms of the the sum of the squared inverse suboptimality-gaps \(H=_{i i^{*}}_{i}^{-2}\). Further upper and lower sample complexity bounds have been obtained in several works. For example  show that for fixed confidence, the sample complexity scales as \((H(1/))\) as \( 0\). The fixed budget setting, in which the number of adaptive samples is upper-bounded _almost surely_ rather than in expectation, turns out to be more difficult.  proved that the optimal fixed budget sample complexity can be \((H(K)(1/))\) as \( 0\), i.e. the fixed budget constraint may lead to an additional \((K)\)factor. However it reverts to \((H(1/))\) when the value of \(H\) is known beforehand. Many recent works have studied other aspects of pure exploration, for example by incorporating structured feedback; see .

Infinite-armed bandits have also much received previous study, e.g. . Since near-optimal arms may be arbitrarily rare, it is natural to instead compare with a **quantile** of the arm distribution. For example  aims to minimize regret relative to such a quantile.

The \((,,)\)-PAC guarantees we address in this paper were first studied in , for infinitely many arms in the fixed confidence setting. Their approach was to sample \(K\) arms and then apply a PAC algorithm for \(K\)-armed pure exploration. As discussed at the beginning of Section 2, the resulting algorithm "pays twice" for the high confidence level \(1-\) which leads to a suboptimal \(O(^{2}(1/))\) sample complexity upper bound. Top-\(k\) extensions were also studied in ; the \(^{2}(1/)\) scaling is still present in their results.

Of particular note is the work  which considers also both fixed budget and confidence settings and obtains somewhat similar looking results. However they restrict attention to a special class of reservoir distributions with supremum achieved by an atom, which must be \(\)-larger than the rest of the support. This structural assumption of a \(\)-gap intrinsically reduces fixed budget sample complexity: their result (see Theorem 4 therein) is actually better than the lower bound we show in Theorem 3.2 as there is no appearance of \((1/)\) (i.e. \((T)\) in their notation).

From their fixed budget estimate,  deduce (at the end of Section 1 therein) the same bound as Theorem 1.1 in their setting for the special case \(=\). Our Theorem 3.2 shows that for the general reservoirs we consider, passing from fixed budget to fixed confidence is inherently suboptimal: the factors of \((1/)\) would remain, but are extraneous for fixed confidence. This underscores that Theorem 1.1 is genuinely new despite the superficial similarity with , since their proof cannot work in our setting.

Finally  studied the infinite-arm pure exploration problem where \(\) is given, also focusing on the \( 0\) asymptotics. They proposed an algorithm with fixed budget sample complexity \((1/)(1/)^{2}\), and asked whether the \((1/)\) factors are necessary. Theorem 3.2 shows their bound is optimal up to constant factors in terms of \(\) and in fact obtains the tight constant. Interestingly  were motivated by complexity theoretic applications to _amplification_ and _derandomization_, where bandit arms correspond to random seeds.

We remark that the analysis in  seems to be technically incomplete. In particular in Lemma 4.5 of (the cited, journal version of) the paper, they neglect to take a union bound over sequences \((T_{1},,T_{k})\) summing to \(T\) but only estimate the probability of each fixed sequence \((T_{1},,T_{k})\). This is a serious gap since the number of such sequences is exponentially large in \(T\). However their idea to use a moving sequence of rejection thresholds was fundamentally correct. It is similar to the main phase of our Algorithm 3, for which we give a fully rigorous, supermartingale-based analysis.

## 2 The Fixed Confidence Setting

Our fixed confidence algorithms proceeds in two phases. The first phase aims to estimate the target quantile value, which we recall depends on the unknown \(\). The second phase then aims to find a single arm which is almost as good as this estimate with high probability.

Focusing on the \(\)-dependence, a challenge with infinitely many arms is that to succeed with probability \(1-\), it is necessary both to sample \((1/)\) arms to ensure a good arm is ever observed, and to sample an arm \((1/)\) times before outputting it as \(i^{*}\). This is why the approach of  requires \(O(^{2}(1/))\) samples: they obtain \(O((1/))\) samples each of \(O((1/))\) arms. However in our algorithm, the first phase samples \(O((1/))\) arms \(O(1)\) times each, while the second phase samples \(O(1)\) arms \(O((1/))\) times each. This allows us to satisfy both necessary conditions above without paying twice for the confidence level.

### The Algorithm for Fixed Confidence

We first give in Alg. 1 a simple procedure to estimate the top \(\) quantile, allowing an \(/3\) error as well as an \(/2\) error in the quantile itself. Alg. 1 obtains \(O(})\) samples from each of the first \(K=O()\) arms \(a_{1},,a_{K}\). The resulting estimator \(\) is the \(1-\) quantile of the empirical average rewards \(_{1},,_{k}\). Its main guarantee is below.

**Proposition 2.1**.: _Fix \(0,, 1\). With probability at least \(1-\), the output \(\) of Alg. 1 satisfies_

\[[G^{-1}(1-)-,G^{-1}(1-)+].\]

_Moreover, Alg. 1 has sample complexity_

\[O(}).\]

Next Alg. 2 repeatedly chooses a new arm \(a_{i}\) and obtains \(O}\) samples. It accepts if the sample mean was at least \(-\), and otherwise moves on to the next arm. If \(\) arms have been tried without success, then Alg. 2 outputs no arm, thus declaring failure. This termination condition is necessary to avoid incurring huge sample complexity when Alg. 1's estimate \(\) of \(\) is inaccurate. Since typically \(()\) of arms will be good enough to usually succeed, this also preserves the \(1-\) confidence level. We now give the following more detailed restatement of Theorem 1.1.

**Theorem 2.1**.: _Apply Algorithm 1 followed by Algorithm 2 using the resulting value \(\). This combined algorithm has expected sample complexity \(O(})\). Moreover its output \(a_{i}\)-satisfies_

\[[p_{i^{*}} G^{-1}(1-)-] 1-.\]

Proposition 2.1 and Theorem 2.1 are proved in Appendix A. We note that in analyzing Algorithm 2, typically the returned arm \(a_{i}\) has \(i K+O(1/)\). This is because each arm \(a_{i}\) in the top \(/2\) quantile has a good chance to pass the test of Algorithm 2. In particular, the expected sample complexity calculation never multiplies two \((1/)\) terms together. However, continuing for \(K+\) steps is important to ensure a \(1-\) success probability. Algorithm 2 stops after \(O((1/)/)\) arms instead of continuing forever in order to guard against erroneous estimates from Algorithm 1.

**Remark 2.1**.: _Our fixed confidence algorithm, given by combining Alg. 1 with Alg. 2 as above, requires only \(O(1)\)**batches** in expectation. Here a batched algorithm operates in a small number of batched phases. At the start of each phase, such an algorithm chooses \(b\) arms to sample exactly \(s\) times each, where \(b,s\) can both depend adaptively on the previous feedback, but cannot be changed during the current phase. Minimizing the number of required batches is often desirable, see e.g. . In particular Alg. 1 uses a single batch with \(s_{1}=}\) samples of \(b_{1}=\) arms. Then Alg. 2 can be implemented in a batched way with \(s_{2}=}\) and a sequence of batch sizes \(b_{2,i}=}{}\) for \(1 i_{2}(C(1/))\). (In the latter phase, one stops after finding an arm to accept.) While this construction uses \(O(1)\) batches **in expectation**, it could be interesting to explore pure exploration with an exactly fixed number of batches, which is more analogous to the fixed budget setting._

### Near-Optimality in the Fixed Confidence Regime

Here we explain why the guarantee of Theorem 2.1 is nearly optimal. For comparison, recall from the important work  that \((})\) samples are necessary and sufficient for \((,)\)-PAC pure exploration in the \(K\)-armed bandit problem with fixed confidence. Intuitively, one expects this problem to be related to ours via \( 1/K\). In fact the following infinite-arm analog was later shown.1 It follows that the guarantee of Theorem 1.1 is optimal up to the \((1/)\) factor.

**Proposition 2.2** (Theorem 1 and Remark 2 of ).: _There exists an absolute constant \(c>0\) such that the following holds. For any \(1/4 3/4\) and \(, 1/10\) and for any pure exploration algorithm \(\) with expected sample complexity \(N}\), there exists a reservoir distribution such that \(\) fails to be \((,,)\)-PAC._

Letting \(L=} 1/\) be the lower bound from Proposition 2.2, the expected sample complexity of our algorithm is at most \(O(L(1/)) O(L L)\). Hence Theorem 1.1 is always nearly optimal compared to the lower bound. Prior to our work there was a quadratic gap as the best upper bound for general reservoirs (Theorem 6 in ) was proportional to \(^{2}(1/)\). We note also that Theorem 5 of  showed a similar result to Proposition 2.2.

## 3 Fixed Budget

Our algorithm and lower bounds for the fixed budget setting are much more technical, with full details given in the Appendix. Here we carefully state the results and give outlines of the proofs.

### Precise Results for Fixed Budget

We first state the results in the easier case that \(\) is given. Theorems 3.1 and 3.2 below give rigorous statements of Theorem 1.2. As there, we will write \(\) for the target value \(-\) when \(\) is given, since then the value \(\) plays no role.

**Theorem 3.1**.: _For any fixed \(0<<<1\), there is a sequence \((_{N})_{N 1}\) of \(N\)-sample algorithms given explicitly by Algorithm 3 such that for any \((0,1)\) and any sequence of reservoir distributions \(_{N}\) with \(G_{_{N}}^{-1}(1-)\), with \(c_{,}\) as in (1.2):_

\[_{N}_{_{N}}[p_{i^{*}}<]) ^{2}N}{N} c_{,}.\] (3.1)

Conversely, the following lower bound applies for any quantile \((0,1)\), and holds even when \(\) is known (which only makes pure exploration easier). It implies that \(\) is asymptotically irrelevant for fixed budget sample complexity, i.e. the sample complexity of approximating the \(=0.01\)-quantile and \(=0.99\) quantile in fixed budget pure exploration depends only on the quantile values themselves as \( 0\). Some intuition for this is as follows. To succeed in pure exploration, one should have sampled the eventually outputted arm at least \(( 1/)\) times. The main obstacle to success in the fixed budget case is that any arm we obtain many samples of might gradually degrade over time. The probability of this degradation is essentially given by small probabilities coming from Chernoff-bound type events, which dominate the prior probability that the arm is in a top quantile.

**Theorem 3.2**.: _For any \(0<<1\) and \(0<<<1\) there exists a sequence of reservoir distributions \(_{N}\) with \(=G_{_{N}}^{-1}(1-)\) such that for any sequence of \(N\)-sample algorithms \(_{N}\),_

\[_{N}_{_{N}}[p_{i^{*}}<]))^ {2}N}{N} c_{,}.\] (3.2)

**Remark 3.1**.: _As discussed in the Appendix, the requirement that \(\) be known in Theorem 3.1 is technically necessary to deal with potential discontinuity of \(\) as \(\) varies, but can be removed under mild conditions. In Theorems B.1, B.2, and B.3 below we give three concrete formulations under which the guarantee (3.1) can be achieved without knowledge of \(\). Informal descriptions (any \(1\) of which suffices on its own) are:_

1. \(_{N}\) _obeys_ \(G_{_{N}}^{-1}(1-)\)_._
2. \(\) _is redefined as the average of_ \(G_{}^{-1}()\) _for_ \(\) _ranging over an interval._
3. \(_{N}=\) _is independent of_ \(N\)_, and the target value is_ \[=^{*}-_{1}\] _for fixed_ \(_{1}>\)_. (Recall_ \(^{*}\) _is the maximum value in the support of_ \(\)_.)_

**Remark 3.2**.: _In fact (see Theorem C.9 in the Appendix), Theorem 3.1 holds even if the algorithms \(_{N}\) must output \( N\) distinct arms \(i_{1}^{*},,i_{ N}^{*}\), all of which must satisfy \(p_{i_{j}^{*}}\). I.e. for suitable \((_{N})_{N 1}\),_

\[_{N}[_{j[ N]}p_{i_{j}^{*}}< ])^{2}N}{N} c_{,}.\]

### Algorithm for Fixed Budget

Here we present our Algorithm 3 for the fixed budget problem. We note that this algorithm is given as input the value of \(\). See Appendix C.1 for first-stage algorithms which estimate \(\) with sufficiently high accuracy and low sample complexity in the three scenarios of Remark 3.1. We use an explore-and-discard approach: at each time, it is currently exploring some arm \(i\), having permanently discarded arms \(1,2,,i-1\) and not yet interacted with arms \(i+1,i+2,\). It turns out to be very convenient to operate in a "B-batch-compressed" way for an increasing integer sequence \(B=(b_{1},b_{2},)\). This means that at all times, the current arm \(i\) has been sampled \(b_{k}\) times for some \(k\), and the samples between \(b_{k}\) and \(b_{k+1}\) are compressed into a single decision. The sequence \(b_{k}\) increases geometrically at a small rate, i.e. \(b_{k+1}/b_{k} 1+\) for small \(\) once \(k\) is mildly large. This batch-compression turns out to be without loss of generality up to a factor \(1+\) in the sample complexity, and helps the analysis operate on the proper geometric time-scales.

We now give a precise description as well as pseudo-code. Let \(0<_{1}_{2} 1\) be suitably small constants (i.e. choose \(_{2}\) sufficiently small, then \(_{1}\), then \(\)). We define \(b_{k}\) and other parameters as follows:

\[(a) =(1-2a)\;a,\] (3.3) \[b_{0} =_{1}^{2}(N),\] \[k_{0} =_{1+}(^{4}(N)/b_{0})\] \[b_{k} = b_{0}(1+)^{k}, k k_{0}\] \[b_{k_{0}+j} =(1+)^{j}b_{k_{0}}, j 1\] \[_{k} =--}, k k_{0}\] \[_{k_{0}+j} =(-2)-j()- ()(1-_{2})}{ N}, j 1.\]

The outer loop dictates the arm \(i\) under consideration. While exploring arm \(i\), the main phase (shown in the last for loop) consists of collecting a new batch of \(b_{k_{0}+j}-b_{k_{0}+j-1} b_{k_{0}+j-1}\) samples, and rejecting if the new empirical mean reward \(_{i,b_{k_{0}+j}}\) drops below

\[^{-1}((-2)-j()- ()(1-_{2})}{ N}).\]Let us explain the point of this formula. First ignoring the function \(\) for now, we see that the rejection threshold for \(_{i,b_{k}}\) steadily decreases with \(k\). This threshold is tuned so that when arm \(i\) has been sampled say \((N/ N)\) times, we have \(k(N)/\) which results in a threshold slightly larger than \(\). Thus an arm which survives for such a long time will be prepared for acceptance as a new-optimal arm.

This strategy can be motivated as follows. The chief worry in fixed budget exploration is that arms might slowly degrade after many samples have been invested into them, which suggests a gradually decreasing rejection threshold. We designed this threshold to drop by a constant divided by \((N)\) each time the number \(b_{k}\) of samples doubles. This ensures that for arm \(i\) to be rejected at time \(b_{k+1}\), the last \(b_{k+1}-b_{k}\) samples must have behaved atypically (compared to their past behavior) by roughly \(}}{(N)}\) standard deviations. The probability of this rare event is roughly \((-Cb_{k+1}/^{2}(N))\) for some constant \(C\) by a Chernoff bound. Therefore if all \(N\) samples are used on eventually rejected arms, the product of these rare event probabilities will be roughly \((-CN/^{2}(N))\) since each \(b_{k+1}\) counts the number of samples used on an individual arm.

The use of the non-linear function \(\) above is essential to achieve the optimal constant \(c_{,}\) in Theorem 3.1. At a high-level, \(\) balances the \(p(1-p)\)-dependence of optimal Chernoff bounds on the underlying probability \(p\) of the Binomial random variable. If the optimal constant is not desired, then Theorem 3.1 can be simplified somewhat; the nonlinear \(\) is not needed and one can double the number of samples at each step rather than multiplying by \(1+\) for small \(\). The earlier for loops in Algorithm 3 are technically important to handle small sample sizes for each given arm, before the required Chernoff bounds have kicked in asymptotically.

```
1input: parameters \(N,,\), and an infinite sequence of arms \(i=1,2,\)
2 initialize: parameters from (3.3) and \(i=0\)
3while fewer than \(N\) samples have been collecteddo
4\(i i+1\)
5 Get \(b_{0}\) samples of arm \(i\).
6if\(_{i,b_{0}}-\)then
7 Reject arm \(i\)
8 end if
9for\(k=1,2,,k_{0}\)do
10 Get \(b_{k}-b_{k-1}\) samples of arm \(i\) (total \(b_{k}\)).
11if\(_{i,b_{k}}--}\)then
12 Reject arm \(i\)
13 end if
14
15 end for
16for\(j=1,2,\)do
17 Get \(b_{k_{0}+j}-b_{k_{0}+j-1}\) samples of arm \(i\) (total \(b_{k_{0}+j}\)).
18if\((_{i,b_{k_{0}+j}})(-2)-j ()-()(1-_{2})}{ N}\)then
19 Reject arm \(i\)
20 end if
21
22 end for
23
24 end for
25 Return arm \(i\). ```

**Algorithm 3**Output arm with \(p_{i}\) using \(N\) samples with high probability

### Analysis of Algorithm 3

The analysis of Algorithm 3 goes by controlling the tail distribution for the rejection time of a given arm (this time is taken to be zero if no rejection ever happens). Intuitively, we are most worried about arms which slowly degrade, thus wasting many samples. The following lemma, proved in the Appendix by a supermartingale argument, is key to rigorize this idea.

**Lemma 3**.: _Suppose \((Y_{i})_{i 1}\) are i.i.d. random variables with non-negative integer values, and \([Y_{i}^{c}] 1\) holds for some constant \(c 0\). Then with_

\[M=_{j 0}_{1 i j}Y_{i}\]

_we have \([M A] A^{-c}\)._

We apply Lemma 3 in the following way. Let \(X_{i}\) be the number of samples used by arm \(a_{i}\) before rejection, and \(I_{i}\{0,1\}\) be the indicator of the event that \(a_{i}\) is ever rejected (even if Algorithm 3 were to continue past time \(N\) and sample arm \(i\) an infinite number of times). We set \(Y_{i}=e^{X_{i}} I_{i}\). The bulk of the analysis thus reduces to proving that \([Y_{i}^{c}] 1\) for a suitable exponent \(c\). Once this is shown, Lemma 3 ensures that \(M\) is small with high probability. Since \((M)\) is exactly the total number of samples used on eventually-rejected arms, if say \( M N(1-)\) then the last arm \(i^{*}\) to be selected must have passed enough rejection thresholds to have \(p_{i^{*}}\) with sufficiently high probability.

The early behavior of Algorithm 3 and \(b_{k}\) are specifically designed so that small values of \(Y_{i}\) contribute little in expectation. The tail behavior of \(Y_{i}\) (corresponding to rejecting arm \(i\) after a long time) is controlled by a technical analysis involving many adjacent time-scales in Subsection C.4 of the Appendix. Interestingly this tail analysis of \(Y_{i}\) never explicitly models the reward probability \(p_{i}\). This is because we are able to argue that a rejection requires the early and late time behaviors of arm \(i\) to differ _from each other_, which is unlikely since the rewards form an i.i.d. sequence.

### On the Lower Bound Proof

We consider the lower bound Theorem 3.2 to be the technical highlight of this paper. In it we face the challenge of proving a very _small_ lower bound on the failure probability of any fixed budget algorithm. To do so we construct an **online adversary with bounded probabilistic strength** to distort rewards. The setup follows; recall \(B\)-batch-compressed algorithms as defined in Subsection 3.2.

**Definition 3.1**.: _An **adaptive randomness distorting adversary**\(\) interacts with a \(B\)-batch-compressed algorithm \(\) in the following way. Suppose \(\) chooses to increase the number of samples of arm \(a_{i}\) from \(b_{k}\) to \(b_{k+1}\). Then \(\) may restrict the set of possible outcomes of these \(b_{k+1}-b_{k}\) samples. Additionally, when \(\) outputs an arm \(a_{i^{*}}\), the adversary can restrict the possible values of \(p_{i^{*}}\)._

We will refer to adversarial actions as _declarations_. Thus when \(\) chooses a batch of samples, \(\) may declare that some property holds for the observed rewards.

As defined above, an adversary \(\) can do anything. We will limit the power of \(\) to make **low-probability** declarations. To formalize this, we charge \(\) per "bit" of probabilistic distortion, and give \(\) a deterministic "budget" for doing so. We measure this budget according to the algorithm's filtration which we refer to the "jointly Bayesian" viewpoint. One should think that the reservoir distribution \(_{N}\) is known to both \(\) and \(\), but neither has any information on the true reward probabilities \(p_{i}\) beyond the observed rewards. Thus \(\) and \(\) share at any time \(t\) the posterior distribution \(^{t}\). In particular recalling (B.9), \(^{t}\) determines the distribution for the outcome of the next batch of \(b_{k+1}-b_{k}\) samples.

**Definition 3.2**.: _Suppose that at time \(t\), the declaration of \(\) has probability \(P_{t}\) to hold according to \(^{t}\). Let the sum_

\[_{t}=_{s t}(1/P_{s})\] (3.4)

_be the total cost of \(\) up to time \(t\), and \(_{N}\) the total cost of \(\). We say \(()\) holds for some \(\) if the bound \(_{N}\) almost surely._

The next key lemma shows that to obtain a lower bound for the failure probability of an algorithm, it suffices to prevent success using a low strength \(\).

**Lemma 4**.: _Suppose there is a randomness distorting adversary \(\) of strength \(\) whose declarations ensure that any algorithm \(\) outputs \(i^{*}\) satisfying \(p_{i^{*}}\) almost surely. Then the true failure probability of \(\) is_

\[^{_{N},}[p_{i^{*}}] e^{-}.\]Proof.: At each step \(t\), let the random variable \(E_{t}\) denote the minimum possible conditional probability of the event \(p_{i^{*}}\) for _any_ algorithm (in the algorithm's jointly Bayesian filtration). We claim that conditioned on \(\)'s declarations holding, for any \(\) the quantity \(M_{t} E_{t}_{s t}P_{s}\) evolves as a supermartingale in this filtration. This suffices because it implies \(E_{0}=M_{0}[M_{T}] e^{-}\). (Here \(T\) is the random number of total batches used.)

Indeed suppose we are at time \(t\) and the next batch has been declared but not sampled. (Identical arguments apply when the adversary restricts \(p_{i^{*}}\) in the last stage.) Let the \(\)-field \(_{t}\) denote all information up to this point including the declaration of the next batch. Let \(\) denote an expectation where the samples from the next batch is distributed according to \(^{t}\). Let \(}\) denote an expectation where \(\)'s declaration is conditioned to hold on the next batch. The dynamic programming principle implies that

\[[E_{t+1}_{t}] E_{t}\]

for any \(\) (with equality for the optimal \(\)). Moreover since \(\)'s declaration has \(^{t}\)-probability \(P_{t}\),

\[}[E_{t+1}_{t}][E_{t+1} _{t}]/P_{t} E_{t}/P_{t}.\]

\(P_{t}\) is \(_{t}\) measurable, so \(}[P_{t}E_{t+1}_{t}] E_{t}\). This establishes the claim and ends the proof. 

We prove Theorem3.2 by constructing a Bayesian adversary who slowly degrades the empirical performance of each arm \(a_{i}\). This adversary declares for each batch of samples that the empirical average reward \(_{i}(n_{i,t})\) of arm \(i\) will drop by \(()\), at least once the sample size \(n_{i,t} N^{}\) is large. This degradation schedule ensures that the average reward of any arm is smaller than \(\) once it has been sampled \((N^{1-})\) times. Moreover it follows from reverse Chernoff estimates that this adversary pays \(O((N)})\) cost per sample, leading to a failure probability lower bound of \((-O(N/^{2}N))\) from Lemma4. As with the upper bound, sharp constants can be tracked with more work and indeed our optimal adversary uses the same function \(\) as in Algorithm3.

ConclusionOur aim in this paper was to understand the sample complexity of pure exploration with infinitely many arms. We showed that, surprisingly, the behavior of fixed confidence and fixed budget problems is provably very different. In the former setting, there is a nearly optimal algorithm which precisely balances between sampling enough distinct arms (to estimate the quantile) and obtaining enough samples of a single arm (to be output). In the latter, the optimal algorithm must repeatedly decide whether to continue with the current arm or switch to a fresh one, via a gradually decreasing sequence of rejection thresholds.

Several interesting questions remain. One is that our fixed budget analysis is tailored to the \( 0\) setting, and does not apply if \(,,\) all tend to zero at comparable rates. Hence other behaviors could be present in such parameter regimes. Additionally, a key conceptual feature of infinite-armed bandits is the possibility that no "good" arms are among those sampled by the algorithm. By definition, this simply cannot happen in \(K\)-armed bandits. It would be interesting to identify a natural problem setting that interpolates between them. Finally high probability bounds on the fixed confidence sample complexity would interpolate between the two settings we studied.