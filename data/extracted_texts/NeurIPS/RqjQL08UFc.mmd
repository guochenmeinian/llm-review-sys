# Spectral Co-Distillation for Personalized Federated Learning

Zihan Chen\({}^{1}\), Howard H. Yang\({}^{2}\), Tony Q.S. Quek\({}^{1}\), and Kai Fong Ernest Chong\({}^{1}\)

\({}^{1}\)Singapore University of Technology and Design (SUTD)

\({}^{2}\)Zhejiang University/University of Illinois Urbana-Champaign Institute, Zhejiang University

zihan_chen@sutd.edu.sg

Corresponding author

###### Abstract

Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components, or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose _spectral distillation_, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize the local idle time in conventional PFL, we propose a wait-free local training protocol. Through extensive experiments on multiple datasets over diverse heterogeneous data settings, we demonstrate the outperformance and efficacy of our proposed spectral co-distillation method, as well as our wait-free training protocol.

## 1 Introduction

With the rapid rise in mainstream popularity of artificial intelligence (AI) models such as ChatGPT  and LoRA , there has been an increasing shift towards the development of personalized AI assistants . Hence, in a future where personalized AI services become mainstream, training AI models on personal data while preserving data privacy would become increasingly important , and maintaining the quality for such models would require collaborative training across multiple models. Personalized federated learning (PFL) emerges as a promising privacy-preserving distributed learning paradigm that is well-equipped to meet such requirements . As an extension of federated learning (FL), PFL aims to train a customized machine learning model for each client or each group of clients with similar preferences . When faced with inconsistencies in the objective functions of different clients, conventional FL fails to generalize well with just a single model, while in contrast PFL promises to generalize well across all clients, even in the presence of data heterogeneity (e.g., label distribution skew and label quantity skew) [6; 7; 8; 9; 10; 11; 12; 13; 14].

To tackle the challenges of personalization, numerous works have focused on designing new PFL systems, or enhancing the performance of personalized models from different aspects, such robustness, fairness, and model convergence[15; 16; 17; 18]. Under federated settings, personalization is achieved through capturing the (dis-)similarity of the local versus globally shared model representations. Inpractical FL/PFL applications of collaboratively training deep neural networks (DNNs), only the model parameters (e.g., model weights or gradients) are exchanged between the clients and the server [8; 19]. Existing DNN-based PFL methods capture this (dis-)similarity either by decoupling the model architecture into groups of layers/channels[20; 21; 22; 23; 24; 25], or by designing local optimization methods with regularization based directly on model weights [17; 15]. Unfortunately, the motivations for such approaches are based on empirical observations, without an overarching theory to explain model (dis-)similarity in relation to training dynamics.

In deep learning theory, the training dynamics of DNNs have been studied from the lens of Fourier analysis . A crucial insight from this analysis is that there is an implicit self-regularization effect arising from the training process itself. Given a target function \(f\) to learn, the model tends to learn the lower frequencies of the Fourier spectrum of \(f\) first before learning the respective higher frequencies. Such a bias in this training process is called _spectral bias_[27; 28]. Informally, spectral bias describes the commonly encountered phenomenon that DNNs first learn low-level features before learning high-level features.

Motivated by this insight, we can distinguish different levels of features in a model representation by looking at its Fourier spectrum. Intuitively, diverse personalized models would still share the same low-level features, and a global generic model would contain the same low-level features. Hence, despite any inconsistencies in the objective functions of different clients, there would be no conflict in learning low-level features for both the generic and personalized models. Consequently, with the expected similarity in the lower frequency components of the Fourier spectra of both the generic and personalized models, we can distill the knowledge of the lower Fourier coefficients to boost the performance of the generic model. Dually, the entire Fourier spectrum of the generic model, which includes the "averaged" high-level features across all clients, would benefit the training of the personalized models. By combining both perspectives, _we shall propose a co-distillation framework for PFL that captures (dis-)similarity in models via spectral information._

Typically, when designing PFL systems, a compute-and-wait protocol is implicitly assumed for local training [15; 23]. This means that the locally updated generic models would be sent by the clients to the server after all local computation tasks have been completed. Such a protocol would yield a period of idle waiting where clients have to wait for the next aggregated model to be broadcasted. _By circumventing this compute-and-wait protocol, we shall utilize the local idle time for training to reduce the total PFL runtime._

Overall, our contributions can be summarized as follows:

* We propose a spectral co-distillation framework for PFL. In particular, this is the first ever use of spectral distillation in PFL to capture the (dis-)similarity of the generic and personalized models. Also, this is the first ever bi-directional knowledge distillation directly between the generic and personalized models.
* We propose a wait-free local training protocol for our spectral co-distillation framework, where we utilize the idle time during global communication so as to reduce the total PFL runtime.
* Through extensive experiments on multiple datasets with heterogeneous data settings, we demonstrate the outperformance and efficacy of our proposed spectral co-distillation framework with the wait-free communication protocol design for PFL, with respect to model generalizability and the total PFL runtime.

## 2 Related work

**PFL.** In PFL, prior efforts have focused on training multiple personalized models via leveraging the similarity and relationships between the global generic model and the local personalized models, such as via model interpolation/mixture , model decoupling , and personalized optimization with customized regularizers . In DNN-based FL applications, decoupling-based approaches divide the model into a private part (kept at the local side) and a shared part (exchanged between the server and clients) [3; 25; 23]. In particular, FedPer  and FedRep  share the shallow layers and train personalized deep layers, while in contrast, LG-Fed  and CD\({}^{2}\)-pFed maintain personalized shallow layers and channels , respectively. Moreover, Fed-RoD proposes a framework to achieve state-of-the-art (SOTA) performance for generic and personalized models simultaneously, based on the "two-loss, two-predictor" design. APFL  and L2GD  consider using a mixture of local and global models to achieve personalization, in which the mixture weight controls the personalization level. Personalized local training methods have been recently explored, which include local fine-tuning in FedBABU , bi-level optimization in Ditto , feature alignment in FedPAC , and personalized model sparsification in FedMask [16; 32] and PerFedMask . More broadly, meta-learning [34; 35], gaussian processes , and hyper-network-based approaches  have been investigated in PFL. Specifically, there is another type of PFL that aims to train personalized models at the level of clusters of clients with similar preferences [38; 39; 40].

**Knowledge Distillation (KD) in FL.** KD has been widely explored in knowledge transfer scenarios, which usually is used to transfer knowledge from the pre-trained teacher model to the student model via minimizing the distance from the latent or logit outputs of the two models [41; 42]. KD-based FL frameworks have been developed with diverse setups, such as FedMD  and FedDF . On the other hand, knowledge-transfer-based PFL frameworks are investigated in [45; 46] with different model structures at the local clients, which could address the system heterogeneity and improve communication efficiency. However, such methods rely on the assumption of having access to a public labeled/unlabelled dataset, which may not be a realistic assumption in FL applications . Moreover, co-distillation methods have been investigated in communication-efficient decentralized scenarios to improve generalizability .

## 3 Proposed framework

The main goal of this work is to train a generic global model and multiple personalized models simultaneously. As summarized in Sec. 1, our proposed framework consists of three major components: spectral distillation-based personalized model training, spectral co-distillation-based generic model training, and the wait-free sequential computation-communication protocol. In this section, we first provide the preliminary and problem formulation for PFL and model spectrum in Sec. 3.1. Next, we present our proposed spectral distillation approach for PFL in Sec. 3.2, co-distillation-based generic model training in Sec. 3.3, and the wait-free local training protocol in Sec. 3.4, accordingly. Moreover, the summarized algorithm is given in Sec. 3.5.

### Preliminaries

**Problem formulation for FL and PFL.** Consider an FL system consisting of a server and \(N\) clients, in which client \(i\) has a loss function \(f_{i}:^{d}\) used for training on its local private dataset \(_{i}=\{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{n_{i}}\), where \(n_{i}=|_{i}|\) denotes the size of the local dataset of client \(i\). In conventional FL, the objective of all the participating clients in this system is to find a global model \(w^{d}\) that

Figure 1: Spectral co-distillation framework with wait-free local training for PFL, in which the generic model (GM) training and the personalized model (PM) training are carried out via spectral distillation in two different stages.

solves the following minimization problem :

\[*{minimize}_{w^{d}}\;\{F(w):=_{i=1}^{N}}{n}f_{i}(w)\},\] (1)

where \(n=_{i=1}^{N}n_{i}\) is the total number of training samples across the \(N\) clients. In a typical communication round \(t\), a subset \(_{t}\) of clients is selected to conduct local training, starting from the latest global model weights \(w_{}^{t}\). Let \(w_{}^{t}\) denote the weights of client \(i\)'s model after local training. At the end of communication round \(t\), the server would collect local models from the selected clients to update the global model via Federated Averaging (FedAvg), i.e. \(w_{}^{t+1}_{i_{t}}p_{i}^{t}w_{i}^{t}\), in which \(p_{i}^{t}=n_{i}/_{k_{t}}n_{k}\) represents the ratio of the local data samples in client \(i\) over the total number of data samples in the selected subset \(_{t}\) of clients for communication round \(t\).

There are two general types of PFL: a) training \(N\) personalized models for all \(N\) clients; and b) training \(1\) generic model and \(N\) personalized models simultaneously. In this work, we investigate the latter one, which we term as "PFL+". This means each client \(i\) has a local personalized model \(w_{,i}\) for its private dataset \(_{i}\), and all clients jointly participate in the training of the generic model \(w_{}\). After local training at client \(i\), the updated generic model is denoted by \(w_{,i}\). Thus, PFL can be formulated using a regularized loss function with regularization term \(R_{}(w_{,i},w_{,i})\). For example, \(R_{}(w_{,i},w_{,i})\) could represent the similarity/divergence between the global and local models' features, such as model weights, feature centroids, and prototypes. In our method, \(R_{}(w_{,i},w_{,i})\) represents cross-model distillation during the training of client \(i\)'s personalized model. Therefore, the objective of personalized model training in PFL+ can be formally formulated as a bi-level optimization problem :

\[: *{minimize}_{w_{,i}^{d}} \{f_{,i}(w_{,i}):=f_{i}(w_{,i})+ _{}R_{}(w_{,i},w_{,i})\} i\] (2) subject to \[w_{,i}w_{},\] (3)

where the regularization coefficient \(_{}\) is used to control the level of personalization. For client \(i\), when referring to a specific communication round \(t\), we shall denote the untrained personalized model and updated generic model by \(w_{,i}^{t-1}\) and \(w_{,i}^{t}\), respectively.

### Personalized local model training

Motivated by both theoretical and empirical insights of the spectral bias inherent in the training dynamics of DNNs, we explore the use of the Fourier spectrum of the generic model for knowledge distillation to enhance the training of personalized local models. In particular, we propose a distillation regularization term representing the divergence between the _full_ model spectra of the generic and personalized models.

First, we introduce some notation. Given vectors \(p=(p_{1},,p_{d})\), \(q=(q_{1},,q_{d})\) in \(^{d}\), define the divergence function \((p\|q):=_{i=1}^{d}p_{i} p_{i}-p_{i} q_{i}\). (By convention, \(0 0:=0\).) Note that when \(p\) and \(q\) are stochastic vectors representing parameter vectors of multinomial distributions \(P\) and \(Q\), then \((p\|q)\) is identically the Kullback-Leibler (KL) divergence from \(P\) to \(Q\). Next, let \(:^{d}^{d}\) denote discrete Fourier transform, let \(:^{d}^{d}\) be the map given by \((z_{1},,z_{d})(\|z_{1}\|,,\|z_{d}\|)\), and define the function \(s:^{d}^{d}\) by \(s:=\). For an input vector of the weights of a DNN model, the output vector after applying \(s\) shall be called the _spectrum vector_ of that model . Thus, in communication round \(t\), the spectrum vectors of the personalized model \(w_{,i}^{t-1}\) of client \(i\) and updated generic model \(w_{,i}^{t}\) are written as \(s(w_{}^{t})\) and \(s(w_{,i}^{t-1})\), respectively. We shall represent the divergence of the personalized and generic models by \((s(w_{,i}^{t-1})\|s(w_{,i}^{t}))\), the divergence of their spectrum vectors.

Concretely, we define \(R_{}(w_{,i},w_{,i}):=(s(w_{,i}^{t- 1})\|s(w_{,i}^{t}))\), and let \(f_{i}\) be the cross-entropy loss \(_{}\) for all \(i\). Then the personalized objective function \(f_{,i}\) of client \(i\) in communication round \(t\) (cf. (2)) is given by:

\[^{}(w_{,i}^{t-1}|w_{,i}^{t}):=_{ }(w_{,i}^{t-1}|_{i})+_{}( s(w_{,i}^{t-1})\|s(w_{,i}^{t})).\] (4)For simplicity, we use a common time-invariant \(_{}\) for all clients throughout training. Since we are distilling the knowledge of the spectrum vector \(s(w_{,i}^{t})\) in (4), we term our approach as _spectral distillation_.

### Generic model training

Given a PFL+ training framework, it is natural to connect the _roles of generic and personalized models_ to the _roles of the teacher and student models in distillation_, where the training of one model is guided by the knowledge distilled by the other. Co-distillation extends this idea. Intuitively, the role of each model alternates between teacher and student for knowledge distillation during training. In PFL+, since we are concurrently training both the generic and personalized models, either of them could be used for knowledge distillation. The key challenge for applying co-distillation to PFL+ is that it is not obvious what knowledge should be distilled from the personalized models to enhance the training performance of the generic model.

In the theory of deep learning, it is well-known that when training a DNN, there is a learning bias towards the lower frequencies of its Fourier spectrum . In fact, the lower-frequency components of this spectrum are robust to random weight perturbations. Hence, with diverse personalized models, we would still expect the lower-frequency components of the spectra of all models (both generic and personalized) to be similar. Consequently, we could use such lower-frequency components for knowledge distillation to enhance generic model training.

Motivated by this, we propose a truncated spectrum-based distillation loss as the regularizer for generic model training. Given \(0< 1\), let \(_{}:^{d}^{ d}\) be the projection map onto the first \( d\) entries, and define \(:=_{} s\). Then the loss function for generic model training, which depends on the truncated spectrum vectors \((w_{,i}^{t})\) and \((w_{,i}^{t-1})\), is given by:

\[^{}(w_{,i}^{t}|w_{,i}^{t-1}):= _{}(w_{,i}^{t}|_{i})+_{}((w_{,i}^{t})\|(w_{,i}^{t-1})),\] (5)

where the regularization term \(R_{}(w_{,i}^{t},w_{,i}^{t-1}):=( {s}(w_{,i}^{t})\|(w_{,i}^{t-1}))\) depends on the hyperparameter \(\), and \(_{}\) is the coefficient of this regularization term. Analogous to **(P1)**, the objective of generic model training in PFL+ could be formulated as the following bi-level optimization problem:

\[\] \[ w_{,i} for client }i,i=1,,N.\] (7)

Overall, by combining the two spectral distillation approaches together, we get a training framework for PFL+, which we shall call _spectral co-distillation_.

Figure 2: A comparison of the (a) conventional compute-and-wait protocol with the (b) proposed wait-free training protocol.

### Wait-free Local Training Protocol

In the context of federated computing, the total runtime, which includes both local computation and communication time throughout the entire training process, is a direct indicator of communication efficiency. However, current PFL frameworks adopt a compute-and-wait protocol for local training. This means that in each round, the client performs both generic and personalized model updates only after all local computation tasks have been completed, and resumes local training upon receiving the latest global model broadcasted from the server. In consequence, there is idle waiting time between model update and model broadcast; see Fig. 2(a).

To improve the communication efficiency of PFL training with respect to the total runtime, we propose a _wait-free local training protocol_, as depicted in Fig. 2(b). In our protocol, the client updates the generic model according to the conventional generic FL training and trains the personalized model during the global communication time period. Unlike existing PFL frameworks, local clients would send the updated generic model to the server before the start of the personalized model training. Thus, our protocol eliminates idle waiting time, thereby dramatically reducing total runtime. Furthermore, it could be easily incorporated into existing PFL frameworks, such as Ditto , to further improve the efficiency; see Tab. 4.

**Discussion on the proposed protocol and related work.** Our proposed wait-free local training protocol is specially designed for the PFL+ scenario, where each client trains two models locally. For simplicity, we use this protocol in our experiments, under the assumption of synchronized PFL+. For comparison in the asynchronized PFL+ setting , see Appendix. Related work that reduce the total training runtime, such as delayed gradient averaging  and wait-free decentralized FL training , are designed for conventional FL and does not deal with the PFL+ scenario. Furthermore, we also provide a discussion on how our wait-free local training protocol could be adapted to the partial client participation scheme in FL in the Appendix.

### Algorithm Summary

Our proposed spectral co-distillation framework combined with our wait-free local training protocol, is given in Algorithm 1. As an overview, we begin every communication round \(t\) with the server broadcasting the global generic model \(w_{}^{t-1}\) to each client for local computation. Each client \(i\) would send back the updated generic model \(w_{,i}^{t}\) after \(E_{}\) local computation steps for global model aggregation, then immediately start the personalized model training and continue until the global generic model \(w_{}^{t}\) is received, which marks the start of the next communication round \(t+1\).

**Remark on convergence analysis.** Note that the global loss function includes a weighted sum of the local loss functions and a regularizer. The regularizer is given in the form of the divergence function \(\), which is equivalent to KL divergence; cf. Sec. 3.2. As demonstrated in , the KL divergence usually exhibits convexity in terms of the model parameters. Consequently, since the model training undergoes (stochastic) gradient descent, it is possible to establish a convergence rate for the training of the global model (under the commonly employed assumption of smoothness of the local loss functions).

## 4 Experiments

### Experiment setup

**Datasets, DNN models, federated settings, and evaluation metrics.** We evaluated our proposed PFL+ framework with \(N\) clients on CIFAR-10/100 , and iNaturalist-2017, using model archi

   &  &  &  \\   & GM & PM & GM & PM & GM & PM \\  FedAvg & 85.35 \(\) 0.11 & (80.33 \(\) 0.38) & 80.76 \(\) 0.13 & (74.51 \(\) 0.48) & 73.51 \(\) 0.17 & (72.68 \(\) 0.39) \\ FedProx & 85.61 \(\) 0.08 & (86.28 \(\) 0.21) & 80.54 \(\) 0.14 & (76.88 \(\) 0.30) & 71.96 \(\) 0.12 & (73.77 \(\) 0.30) \\ FedDyn & 86.03 \(\) 0.13 & (85.33 \(\) 0.19) & 80.88 \(\) 0.18 & (78.93 \(\) 0.25) & 73.62 \(\) 0.14 & (74.25 \(\) 0.58) \\ FedGen & 86.17 \(\) 0.32 & (85.24 \(\) 0.47) & 79.86 \(\) 0.34 & (77.52 \(\) 0.43) & 71.36 \(\) 0.28 & (71.42 \(\) 0.63) \\ FedAvgM & 85.44 \(\) 0.05 & (82.85 \(\) 0.28) & 81.04 \(\) 0.09 & (75.71 \(\) 0.33) & 72.87 \(\) 0.06 & (72.96 \(\) 0.14) \\  pFedMe & 85.58 \(\) 0.23 & 88.17 \(\) 0.17 & 79.33 \(\) 0.14 & 84.66 \(\) 0.17 & 72.11 \(\) 0.23 & 81.18 \(\) 0.15 \\ Ditto & 85.34 \(\) 0.10 & 87.55 \(\) 0.09 & 80.70 \(\) 0.13 & 83.39 \(\) 0.12 & 73.45 \(\) 0.18 & 80.08 \(\) 0.20 \\ FedRep & (85.61 \(\) 0.19) & 87.32 \(\) 0.11 & (80.33 \(\) 0.23) & 84.10 \(\) 0.10 & (73.50 \(\) 0.24) & 79.74 \(\) 0.31 \\ FedRoD & 86.02 \(\) 0.12 & 91.67 \(\) 0.16 & **81.31 \(\) 0.15** & 85.91 \(\) 0.15 & 74.64 \(\) 0.07 & 81.37 \(\) 0.17 \\ FedBABU & (85.67 \(\) 0.24) & 91.34 \(\) 0.19 & (79.57 \(\) 0.23) & 83.22 \(\) 0.33 & (73.88 \(\)0.19) & 80.58 \(\) 0.22 \\  Ours & **86.37 \(\) 0.15** & **92.25 \(\) 0.21** & 81.27 \(\) 0.18 & **86.59 \(\) 0.17** & **75.52 \(\) 0.11** & **82.69 \(\) 0.16** \\  

Table 1: Average (3 trials) and standard deviation of the best test accuracies for generic/personalized models of various methods on CIFAR-10 with different non-IID settings. See also Remark 4.1.

   &  &  \\   & GM & PM & GM & PM \\  FedAvg & 48.37 \(\) 0.22 & (52.64 \(\) 0.48) & 38.61 \(\) 0.27 & (39.27 \(\) 0.42) \\ FedProx & 47.33 \(\) 0.15 & (53.85 \(\) 0.33) & 39.55 \(\) 0.18 & (41.33 \(\) 0.38) \\ FedDyn & 49.24 \(\) 0.27 & (57.20 \(\) 0.35) & 40.43 \(\) 0.14 & (40.92 \(\) 0.26) \\ FedAvgM & 48.55 \(\) 0.19 & (55.60 \(\) 0.26) & 39.03 \(\) 0.08 & (40.85 \(\) 0.19) \\  pFedMe & 47.29 \(\) 0.27 & 61.52 \(\) 0.25 & 38.22 \(\) 0.23 & 45.88 \(\) 0.32 \\ Ditto & 48.37 \(\) 0.25 & 60.47 \(\) 0.27 & 39.61 \(\) 0.19 & 43.12 \(\) 0.28 \\ FedRep & (46.32 \(\) 0.23) & 58.76 \(\) 0.36 & (40.11\(\) 0.35) & 45.22 \(\) 0.19 \\ FedRoD & 50.07 \(\) 0.16 & 62.51 \(\) 0.15 & 40.58 \(\) 0.22 & 45.99 \(\) 0.14 \\ FedBABU & (48.52 \(\) 0.30) & 60.33 \(\) 0.28 & (37.35 \(\) 0.29) & 44.72 \(\) 0.28 \\  Ours & **51.39 \(\) 0.22** & **63.15 \(\) 0.16** & **40.67 \(\) 0.14** & **46.82 \(\) 0.23** \\  

Table 2: Average (3 trials) and standard deviation of the best test accuracies for generic/personalized models of various methods on CIFAR-100 with different non-IID settings. See also Remark 4.1.

[MISSING_PAGE_FAIL:8]

tion cost performance of SOTA methods with/without the protocol on non-IID CIFAR-10 (\(=0.1\)), in terms of the total runtime \(_{}\) for PM to reach the target test accuracy (40%/80%). A smaller \(_{}\) indicates higher communication efficiency. For PFL methods that train generic and personalized models using the compute-and-wait local training protocol, we evaluated Ditto and FedRoD. We conduct experiments with different numbers of epochs for local PM training (3 or 5 epochs). As shown in Tab. 4, our proposed wait-free training protocol could significantly improve the efficiency of convergence time and has the potential to boost the time efficiency of PFL+ methods.

### Ablation results

**Ablation study.** In our proposed spectral co-distillation framework, we introduce the bi-directional spectrum knowledge distillation to bridge the training of generic and personalized models with the target for training good generic and personalized models simultaneously. To achieve the target, truncated and full model spectrum information are adopted in different training stages. Here, we conduct an ablation study to evaluate the effectiveness of these two components (see Tab. 5 for the effects of each component), in which we apply the distillation approaches in the two training stages separately. In the setup where both SCD-PM and SCD-GM are removed (Case I), the GM training is identical to FedAvg. In the case of removing only SCD-PM while keeping SCD-GM (Case II), each PM would be trained locally without any knowledge distilled from the GM. This is akin to the client training its model by itself, separately from the server. Naturally, the PM performance would be drastically lower. As SCD-GM is kept in Case II, where the GM is the student and the PM is the teacher, since the PM's performance is drastically lower, we would expect a drop in the GM's performance. Informally, the model would be worse off with the distillation of bad knowledge, than without distillation.

As demonstrated in Tab. 5, both the distillation methods can boost the accuracy performance of generic and personalized models, whereas the bi-directional distillation can bridge the training performance of the generic and personalized models. Specifically, we can observe that, the SCD-PM module effectively transfers the knowledge from the generic model to the personalized model and avoids over-fitting during local training.

**Generalizability on new joining clients.** In a real-world PFL system, dynamic client participation should be regarded as an important factor to consider during algorithm design, in which there would be continually new clients joining the system during training. The PFL system needs to rapidly train a good personalized model that could generalize well on the new client's local data. To evaluate the generalizability of the system, we simulate a dynamic participation system with \(80\) in-training clients and \(20\) new clients on CIFAR-10 (partitioned

Figure 3: Performance comparison for generalizability on new clients of various methods.

   &  &  \\   & GM & PM & GM & PM \\  Ours & **86.37 \(\) 0.15** & **92.25 \(\) 0.21** & **75.52 \(\) 0.11** & **82.69 \(\) 0.16** \\  Ours w/o SCD-GM & 85.35 \(\) 0.11 & 91.86 \(\) 0.17 & 73.51 \(\) 0.17 & 81.03 \(\) 0.20 \\ Ours w/o SCD-PM & 82.74 \(\) 0.39 & 79.65 \(\) 0.83 & 68.96 \(\) 0.47 & 70.51 \(\) 1.21 \\ Ours w/o Both & 85.35 \(\) 0.11 & 79.65 \(\) 0.83 & 73.51 \(\) 0.17 & 70.51 \(\) 1.21 \\  

Table 5: Ablation study results on non-IID CIFAR-10 (average and standard deviation of 3 trials). **SCD-GM** (resp. **SCD-PM**) represents the spectral distillation approaches adopted during the training of generic (resp. personalized) model.

by the Dirichlet distribution with \(=0.1\)), and deal with new clients with the global model-based fine-tuning approach. Fig. 3 gives the results of the average test accuracies of the new clients. Among all evaluated methods, our method has the best average test accuracies, illustrating the fast adaptive capability of our method.

## 5 Conclusion

In this work, we propose a spectral co-distillation framework for PFL to learn better generic and personalized models simultaneously. As far as we know, this is the first work in PFL that represents the (dis-)similarity of models via their Fourier spectra. Even without co-distillation, there have been no other works that explore spectral distillation in PFL (or even in FL). The advantage of this new approach is clear from our experiments: We achieved outperformance in both generic and personalized model training. Our framework also incorporates a simple yet effective wait-free local training protocol to reduce the overall local training time.

**Limitations.** Our proposed spectral co-distillation framework, as currently formulated, does not deal with stragglers and adversarial attacks. Their influence on performance would require further investigation. Also, our protocol assumes a synchronized network connection, which may not be practical for scenarios with large system/network heterogeneity. Moreover, it would be good to consider a more realistic local training protocol design that takes into account the issues of network/system heterogeneity; we leave the extension as future work.