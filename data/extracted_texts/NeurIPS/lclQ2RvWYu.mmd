# A Single 2D Pose with Context is Worth Hundreds

for 3D Human Pose Estimation

Qitao Zhao\({}^{1}\)  Ce Zheng\({}^{2}\)  Mengyuan Liu\({}^{3}\)  Chen Chen\({}^{2}\)

\({}^{1}\)Robotics Institute, Carnegie Mellon University

\({}^{2}\)Center for Research in Computer Vision, University of Central Florida

\({}^{3}\)Key Laboratory of Machine Perception, Peking University, Shenzhen Graduate School

Work was done while Qitao was an intern mentored by Chen Chen.

###### Abstract

The dominant paradigm in 3D human pose estimation that lifts a 2D pose sequence to 3D heavily relies on long-term temporal clues (_i.e._, using a daunting number of video frames) for improved accuracy, which incurs performance saturation, intractable computation and the non-causal problem. This can be attributed to their inherent inability to perceive spatial context as plain 2D joint coordinates carry no visual cues. To address this issue, we propose a straightforward yet powerful solution: leveraging the _readily available_ intermediate visual representations produced by off-the-shelf (pre-trained) 2D pose detectors - no finetuning on the 3D task is even needed. The key observation is that, while the pose detector learns to localize 2D joints, such representations (_e.g._, feature maps) implicitly encode the joint-centric spatial context thanks to the regional operations in backbone networks. We design a simple baseline named **Context-Aware PoseFormer** to showcase its effectiveness. _Without access to any temporal information_, the proposed method significantly outperforms its context-agnostic counterpart, PoseFormer , and other state-of-the-art methods using up to _hundreds of_ video frames regarding both speed and precision. _Project page:_ qitaozhao.github.io/ContextAware-PoseFormer

## 1 Introduction

3D human pose estimation (HPE) aims to localize human joints in 3D, which has a wide range of applications, including motion prediction , action recognition , and tracking . Recently, with the large availability of 2D human pose detectors , lifting 2D pose sequences to 3D (referred to as lifting-based methods) has been the _de facto_ paradigm in the literature. Compared to raw RGB images, 2D human poses (as an intermediate representation) have two essential advantages. On the one hand, domain gaps exist between images (input) and 3D joint locations (output), whereas this is not the case for 2D and 3D joints. Primarily, 2D joint coordinates provide highly task-relevant positional information for localizing joints in 3D. On the other, 2D coordinate representation is exceptionally lightweight in terms of memory cost. For example, in a typical COCO  setting, a 2D human pose requires only \(17 2\) floating points, whereas an RGB image requires \(256 192 3\) (or even more with higher resolution). This property enables state-of-the-art lifting-based methods to leverage extremely long-term temporal clues for advanced accuracy, _e.g._, 243 video frames for VideoPose3D , MixSTE , and P-STMO ; large as 351 frames for MHFormer .

Scaling up the input has brought consistent improvements so far, yet some concerns naturally arise. First, the performance gains seem to level off when excessively long sequences are used as input (_e.g._, improvements are marginal when the frame number increases from 243 to 351 ). Second, long input sequences bring non-trivial computational costs. For instance, temporal processing using transformers (a popular choice) is expensive, especially with large frame numbers. Third, the use offuture video frames renders real-world online applications impossible. Concretely, existing works adopt a 2D pose sequence to estimate the 3D human pose for the _central frame_ where half of the frames in the sequence are _inaccessible_ for the current time step, which is known as the non-causal problem . _While gains from long input sequences come at a cost, is there any alternative approach we can take to easily boost performance?_ We attempt to answer this question in our work.

We start by revisiting the whole pipeline of lifting-based methods, which involves two stages as shown in Fig. 1 (a). In Stage 1, a 2D pose detector estimates human joints for each image from a video clip, with a set of intermediate representations as byproducts, _e.g._, feature maps of varying resolutions. In Stage 2, the detected 2D pose sequence (output of Stage 1) is lifted to 3D, while such representations are _discarded_. This can be problematic: the (multi-scale) joint-centric spatial context encoded by these feature maps is lost. We argue: _the omission of spatial contextual information is the primary factor contributing to the time-intensive nature of existing lifting-based methods_.

_So, what makes spatial context important?_ As an inverse problem, 3D pose estimation inherently suffers from ambiguities  such as depth ambiguity and self-occlusion, yet they can be mitigated by utilizing spatial context from the monocular image. For depth ambiguity, the shading difference is an indicator of depth disparity  whereas the occluded body parts can be inferred from visible ones with human skeleton constraints . Psychological studies  provide evidence that appropriate context helps reduce ambiguities and promote visual recognition for the human perception system. Since 2D keypoints alone are unable to encode such _spatial_ context, existing lifting-based approaches, in turn, resort to long-term _temporal_ clues to alleviate ambiguities.

We offer a surprisingly straightforward solution to "fix" the established lifting framework as depicted in Fig. 1 (b): retrieving the lost intermediate visual representations learned by 2D pose detectors and engaging them in the lifting process. As previous works limit their research scope to the design of lifting models (Stage 2), the cooperation of both stages is largely under-explored. This approach yields two major benefits: First, such representations encode joint-centric spatial context that promotes reducing ambiguity - the core difficulty in 3D human pose estimation. Second, we illustrate in this work that these representations can be used out of the box, _i.e._, no finetuning on the 3D task or extra training techniques are needed.

To showcase the effectiveness of our solution, we design a simple transformer-based baseline named **Context-Aware PoseFormer**. The proposed method leverages multi-resolution feature maps produced by 2D detectors in a sparse manner. Specifically, we extract informative contextual features from these feature maps using deformable operations  where the detected 2D joints serve as reference points. This helps mitigate the noise brought by imperfect pose detectors while avoiding heavy computation. Furthermore, we design a _pose-context interaction module_ to exchange information between the extracted multi-level contextual features and the 2D joint embedding that encodes positional clues. Finally, a _spatial transformer module_ is adopted to model spatial dependencies between human joints, which follows PoseFormer . Our approach shows encouragingly strong

Figure 1: A comparison of existing lifting-based methods with ours at a framework level. (a) Existing methods (video-based): _time-intensive_ and _context-agnostic_. (b) Ours (image-based): _time-free_ and _context-aware_. We leverage intermediate visual representations from Stage 1. Notably, we do not finetune the 2D detectors for the lifting task, thus easing training and bringing no extra costs.

performance (see Fig. 2): our single-frame model outperforms 351-frame MHFormer  and other state-of-the-art lifting-based methods, highlighting the potential of leveraging contextual information in improving pose estimation accuracy.

Our contributions are summarized as follows:

1) We propose a novel framework that addresses the time-intensive issue present in existing methods by incorporating context awareness. This is achieved by leveraging readily available visual representations learned by 2D pose detectors.

2) We introduce a simple baseline that efficiently extracts informative context features from learned representations and subsequently fuses them with 2D joint embedding that provides positional clues.

3) On two benchmarks (Human3.6M  and MPI-INF-3DHP ), our single-frame model demonstrates significant performance improvements over both non-temporal and temporal methods that use up to hundreds of video frames.

4) Our work opens up opportunities for more skeleton-based methods to utilize visual representations learned by well-trained backbones from upstream tasks in an out-of-the-box manner.

## 2 Related Work

Early works [54; 17; 44; 53] estimate the 3D human pose from monocular images without explicitly using the corresponding 2D pose as an intermediate representation. With the rapid progress in 2D pose estimation [42; 8; 52], lifting 2D poses to 3D has been dominant in the literature. As our work aims at improving the lifting-based framework, we primarily introduce works of this line. We refer readers to the recent HPE survey  for a more comprehensive background and detailed information.

**Single-frame methods.** Before the surge of deep learning, Jiang  performs 3D pose estimation based on nearest neighbors with a large 3D human pose library, and this approach is recently revisited by Chen _et al._ with a closed-form algorithm to project 3D pose exemplars to 2D. Martinez _et al._ design a simple linear-layer model to illustrate that the difficulty of 3D human pose estimation mainly lies in precise 2D joint localization. Moreno-Noguer  formulates the task as 2D-to-3D distance matrix regression. To mitigate the demand for 3D annotations, Pavlakos _et al._ leverage ordinal depths of human joints as an extra training signal. Given the human skeleton topology, recent works (_e.g._, GraphSH , HCSF , GraFormer , GraphMLP ) use graph neural networks to reason spatial constraints of human joints, and lift them to 3D.

**Multi-frame methods.** As initial attempts, some works explore temporal cues for robust pose estimation in crowded scenes  or temporally consistent results . More recently, a number of works model spatial-temporal dependencies for 2D pose sequences, _e.g._, using LSTM [24; 59], CNN [45; 6] and GNN [56; 65]. They have achieved superior performance than non-temporal methods. Driven by the great success of vision transformers in image classification [13; 33], object detection [4; 75] and _etc_, transformer-based methods [74; 29; 67; 50; 69; 72] are currently the center of research interest. PoseFormer  is the first transformer-based model in the community, which builds up inter-joint correlations within each video frame and human dynamics across frames with transformer encoders, respectively. Due to its straightforward architecture, PoseFormer is highly malleable and rapidly gains a series of follow-ups [67; 69]. Our work presents a simple _single-frame_ baseline model where the _inter-joint modeling module_ is adapted from the spatial encoder of PoseFormer.

**Fusing image features with positional joint clues.** Compared to previous lifting-based works, our approach actively engages visual representations from 2D pose detectors in the lifting process, thus being _context-aware_ and achieving promising results. Notably, the fusion of image features with positional information about human joints has been explored in some works but within differing contexts or with distinct motivations. Tekin _et al._ propose to fuse image features with 2D joint heatmaps, where 2D joints are not explicitly regressed as in the lifting-based pipeline. Closer to

Figure 2: Our single-frame method outperforms both non-temporal and temporal methods that use up to 351 frames on Human3.6M.

our work, Zhao _et al._ extend their graph network (SemGCN) by incorporating image features. Specifically, image features are pooled at the detected joints and then concatenated together with joint coordinates, serving as the input to SemGCN. Though this approach is similar to ours at first glance, our work differs in that 1) our method needs no multi-stage training, 2) we consider the uncertainty associated with 2D joints in joint-context extraction, 3) we have designed dedicated modules to handle the domain gap between image features and joint coordinates, and 4) our performance surpasses that of SemGCN by a significant margin. In addition to technical details and performance comparisons, our underlying motivation for this research also differs. More discussion about relevant literature can be found in Appendix A.

## 3 Method

We propose a new framework to solve the time-intensive problem of previous lifting-based methods. This is achieved by incorporating the spatial context (encoded by intermediate feature maps from 2D pose detectors) into the 2D-to-3D lifting process (see also Fig. 1 (b)). We design a simple baseline model named **Context-Aware PoseFormer** to exemplify the gains brought by this approach (an overview is in Fig. 3). In the following, we first briefly introduce the context-agnostic counterpart of our method, PoseFormer . Then, we provide detailed descriptions of our approach.

### A Context-agnostic Counterpart: PoseFormer

PoseFormer  is one of the first transformer-based models for 3D human pose estimation, which mainly consists of two modules: the spatial encoder and the temporal encoder. PoseFormer processes an input 2D pose sequence as follows. First, the sequence \(^{F J 2}\) is projected to a latent space indicated by \(^{F J C}\), where \(F\), \(J\) and \(C\) denote the sequence length, joint number, and embedding dimension respectively. Learnable (zero-initialized) spatial positional embedding \(_{spa}^{1 J C}\) is summed with \(\) to encode joint-specific information. Second, the spatial encoder models inter-joint dependencies at each video frame (time step) with cascaded transformers. Third, the output of the spatial encoder \(^{F(J C)}\) is flattened as the input to the temporal encoder. Similarly, temporal positional embedding \(_{temp}^{F(J C)}\) is added to \(\) to encode the information related to frame indices. The temporal encoder builds up frame-level correlations with transformer layers in this stage. Finally, a 1D convolution layer is adopted to gather temporal information, and a linear layer outputs the target 3D pose \(^{1(J 3)}\) for the central video frame. PoseFormer and its follow-ups [67; 50; 69] are inherently limited to deal with ambiguities since they barely receive 2D joint coordinates as input. Due to the simplicity, scalability, and effectiveness of PoseFormer, we choose PoseFormer as the

Figure 3: An overview of Context-Aware PoseFormer. Stage 1 (left): The 2D pose detector estimates the 2D pose, with a set of feature maps as byproducts. In Stage 2 (right), we extract informative join-context features from such feature maps and subsequently fuse them with 2D pose features.

basis of our method, boosting it to be context-aware and free of temporal information. Detailed descriptions of our method are presented below.

### Context-Aware PoseFormer

Our core insight is retrieving the spatial context encoded by readily available feature maps from 2D human pose detectors. As the multi-stage design promotes 2D human pose estimation [52; 11; 63], our method also benefits from multi-resolution feature maps. An illustration of model input is on the left of Fig. 3. For a given RGB image \(\) of size \(H W 3\), an off-the-shelf 2D pose detector produces the corresponding 2D human pose \(^{J 2}\), and a set of intermediate feature maps with varying resolutions as byproducts, \(\{_{l}^{H W C_{l}}\}_{l=1}^{L}\) (where \(L\) is the total number of feature maps). Since stacked network layers and down-sampling operations increase the receptive field (and correspondingly the scale of context) of each unit on feature maps , high-resolution feature maps encode fine-grained visual cues (_e.g._, joint existence) while low-resolution ones tend to keep high-level semantics (_e.g._, human skeleton structures). During the process that 2D pose detectors are trained to localize human joints, multi-level feature maps implicitly encode information about the spatial configurations of joints.

**How to effectively utilize these feature maps** is a non-trivial question. Blindly processing them in a global way using, _e.g._, convolutions , or vision transformers , may bring unnecessary computational overhead since the background that takes up a large proportion of the image does not provide task-relevant features related to human joints. The most straightforward approach to treat feature maps in a preferably sparse manner is to sample feature vectors at the detected joints. However, in real scenarios, 2D pose detectors inevitably introduce errors. Therefore, the context feature obtained by such an approach may not well represent the spatial context associated with the joint of our interest, leading to suboptimal performance. We propose a _Deformable Context Extraction_ module to deal with this issue, which not only attends to the detected joints but also the regions around them. As the detected 2D joints are not the only source of contextual features, the errors brought by 2D pose detectors can be mitigated. Moreover, we design a _Pose-Context Feature Fusion_ module to fuse the features of two modalities: spatial contextual features and 2D joint embedding that encodes positional clues about human joints. Finally, _Spatial Inter-joint Modeling_ is adopted to build up dependencies across human parts. They are organized hierarchically (on the right, Fig. 3). The effectiveness of each element is verified in Sec. 4.3.

**Deformable Context Extraction.** This module uses deformable attention  to extract informative spatial contextual cues from feature maps. Specifically, for each detected joint, we produce a set of sampling points on multi-scale feature maps whose offsets and weights are learned based on the features of reference points (_i.e._, the detected joint of interest), denoted by \(\{_{l}\}_{l=1}^{L}\). We make learned offsets and weights "reference-position-aware" by adding the embedding of 2D joint coordinates \(\) to source features (after they are projected to a shared dimension \(C\)). Let \(l\) index a feature level and \(j\) index a human joint, and Deformable Context Extraction is formulated as:

\[_{lj}^{r_{l}} =\ (_{lj}^{n-1}+_{j})+_{lj}^{n-1}, _{lj}^{n-1},_{j}^{C},\ n=1 N_{1}\] (1) \[_{lj}^{n} =\ (_{lj}^{n})+_{lj}^{n}, _{lj}^{n}^{C},\ n=1 N_{1}\] \[(_{lj}^{n-1}+_{j}) =\ _{m=1}^{M}[_{k=1}^{K}A_{link}_{lm}_{l}( _{j}+_{link})], _{l}(_{j}+_{link})^{C}\]

where \(m\) iterates over the attention heads, \(k\) over the sampled points around the detected joint \(_{j}\), and \(K\) is the total sampling point number. For the \(l^{}\) feature map, \(_{link}\) represents the sampling offset of the \(k^{}\) sampling point in the \(m^{}\) attention head, while \(A_{link}\) denotes its corresponding attention weight. We provide visualization of sampling points learned by deformable attention in Sec. 4.3, where the learned sampling points attempt to discover ground truth 2D joints despite the noise brought by pose detectors. More details and visualization about this module are available in Appendix E.

**Pose-Context Feature Fusion.** This module aims at exchanging information between two modalities (_i.e._, pose embedding and the extracted spatial context) and between different levels of context features at the same time. Cross-modality learning gives rise to powerful vision-language models [23; 2], while the multi-scale design has proved beneficial to a large set of computer vision tasks[15; 16; 71]. Inspired by multi-modality models  that feed visual tokens and language tokens into a unified transformer encoder, we naturally adapt this design to model interactions between pose embedding and multi-level context features, where transformers learn a joint representation for both modalities [51; 7]. The implementation of this module is:

\[_{j}^{0} =\ ([_{j},_{1j}^{N_{1}},,_{Lj}^{ N_{1}}],\ dim=0), _{j}^{C},\ _{1j}^{N_{1}},,_{Lj}^{N_{1}} ^{C}\] (2) \[_{j}^{n} =\ (_{j}^{n-1})+_{j}^{n-1}, _{j}^{n-1}^{(L+1) C},\ n=1 N_{2}\] \[_{j}^{n} =\ (_{j}^{n})+_{j}^{n}, _{j}^{n}^{(L+1) C},\ n=1 N_{2}\]

where \(j\) (\(1 J\)) indicates that feature fusion is performed for each joint. Transformer layers reduce domain gaps for both modalities in shared latent space  and promote message-passing where joint-position information and multi-level contextual cues complement each other. In addition, we introduce Unified Positional Embedding \(_{uni}^{(L+1) C C}\) (\(L\) for joint-context feature vectors and 1 for 2D pose embedding) to encode modality-related and joint-specific information simultaneously.

**Spatial Inter-joint Modeling.** With the two modules above, elegant local representations are learned for each joint individually. To understand the human skeleton system and its corresponding spatial context with a global view, inter-joint dependencies are modeled based on the learned per-joint features using a spatial transformer encoder following PoseFormer . The spatial encoder in PoseFormer simply receives the linear projection of (context-agnostic) 2D joint coordinates as input, whereas our joint-level representations are enhanced with spatial contextual cues (thus being context-aware). The output of Pose-Context Feature Fusion module \(\{_{j}^{N_{2}}\}_{j=1}^{J}\) is flattened and stacked as input to the \(N_{3}\)-layer transformer encoder. Each joint token (\(J\) in total) is of \((L+1) C\) dimensions, encoding both positional and contextual information for the related joint.

**Output and loss function.** A simple linear layer is adopted to obtain the final 3D pose \(^{J 3}\). We use \(L_{2}\) loss as PoseFormer  to supervise the estimated result.

**Discussion.** Our model follows a local-to-global hierarchy. Specifically, local representations are first learned for each joint, on which inter-joint modeling is subsequently performed. This design provides our model with preferable interpretability and scalability: modules are inherently disentangled according to the dependencies they attempt to learn and can be easily extended or replaced. Plus, the local-to-global organization avoids the heavy computation brought by globally modeling all elements at once. These virtues make our method a desirable stepping stone for future research.

## 4 Experiments

Our method, Context-Aware PoseFormer, is referred to as "CA-PF-backbone name" in tables. We conduct experiments on two benchmarks (Human3.6M  and MPI-INF-3DHP ). Human3.6M  is the most popular benchmark for 3D human pose estimation, where more than 3.6 million video frames are captured indoors from 4 different camera views. MPI-INF-3DHP  includes videos collected from indoor scenes and challenging outdoor environments. Both datasets provide subjects performing various actions with multiple cameras. On Human3.6M, we report MPJPE (Mean Per Joint Position Error, where the Euclidean distance is computed between estimation and ground truth) and PA-MPJPE (the aligned MPJPE). For MPI-INF-3DHP, we report MPJPE, Percentage of Correct Keypoint (PCK) within the 150mm range, and Area Under Curve (AUC). Settings follow previous works [74; 29; 67; 50]. Due to page limitations, we place details about our implementations in Appendix B.

### Comparison on Human3.6M

**Accuracy**. We compare our single-frame method with both single-frame and multi-frame methods on Human3.6M  (Table 1). Previous methods receive CPN-detected  2D joints as input. We also follow this setting to ensure a fair comparison. Benefiting from spatial contextual clues encoded by the CPN pose detector, our approach outperforms state-of-the-art single-frame methods by a huge margin. For instance, Zeng _et al._ obtain 47.9mm MPJPE, whereas our CA-PF-CPN achieves 41.6mm (a **13.2%** error reduction). This result has already surpassed the performance of most modern multi-frame methods. To show the _flexibility_ of our method and to further investigate spatial context from various backbones, we change the backbones that generate feature maps: our CA-PF-HRNet-32  achieves 41.4mm MPJPE; with a larger backbone HRNet-48, the error reduces to 39.8mm, which is superior to _all_ previous multi-frame methods. For example, this result outperforms MixSTE  with 243 frames (39.8 v.s. 40.9mm, a 2.7% error reduction) and MHFormer  with even 351 frames (39.8 v.s. 43.0mm, a **7.4**% error reduction). Our method presents highly competitive accuracy in the absence of dense temporal modeling. Moreover, our method consistently gains improvements from advanced 2D pose detectors. We expect our approach's performance to increase further with powerful large visual foundation models, as such foundation models have been attracting increasing attention and interest.

**Computational cost**. Our method requires far less computation than state-of-the-art methods (see also Table 1). For example, with similar accuracy, MHFormer  takes 14.2 GFLOPs, whereas our CA-PF-CPN  demands only 0.6 GFLOPs (about **24\(\)** reduction). Two reasons account for the lightweight property of the proposed approach: (1) Non-temporal input removes large computation for dense temporal modeling. Modeling temporal dependencies using _e.g._, transformers  is computationally heavy especially when the frame number is increased for improved accuracy. (2) We utilize multi-scale feature maps (from pose detectors) in a sparse manner (_i.e._, with deformable attention ). We avoid applying global operations on the whole feature maps, which may introduce unnecessary computational budgets on uninformative backgrounds.

### Comparison on MPI-INF-3DHP

In Table 2, the comparison is also conducted with both single-frame (top) and multi-frame (middle) methods on the MPI-INF-3DHP dataset . We use ground truth 2D pose detection as input, following , and HRNet-32, HRNet-48  are adopted as the backbone network to generate multi-resolution feature maps. Our approach outperforms other methods, including the state-of-the-art multi-frame one, P-STMO . P-STMO uses 81 video frames as input, with additional masked joint self-supervised pre-training. In contrast, our approach has no access to temporal information and does not need an extra pre-training stage. The results verify the generalization ability of our method to different datasets, particularly in challenging outdoor environments.

    &  &  &  &  &  &  \\  & & & & Lifting Module & & \\   & GraphSH  & CVPR’21 & CPN  & 1 & - & 51.9 & - \\  & HCSF _et al._ & ICCV’21 & CPN  & 1 & - & 47.9 & 39.0 \\   & GraFormer  & CVPR’22 & CPN  & 1 & - & 51.8 & - \\   & GraphMLP  & arXiv’22 & CPN  & 1 & 0.3 & 49.2 & 38.6 \\   & Pavlo _et al._ & CVPR’19 & CPN  & 243 & 0.03 & 46.8 & 36.5 \\  & Liu _et al._ & CVPR’20 & CPN  & 243 & - & 45.1 & 35.6 \\   & Wang _et al._ & ECCV’20 & CPN  & 96 & - & 45.6 & 35.5 \\   & Zeng _et al._ & ECCV’20 & ECCV’20 & CPN  & 243 & - & 44.8 & 34.9 \\   & PoseFormer  & ICCV’21 & CPN  & 81 & 1.4 & 44.3 & 34.6 \\   & StriderTrans.  & TMM’22 & CPN  & 351 & 2.1 & 43.7 & 35.2 \\   & MHFormer  & CVPR’22 & CPN  & 351 & 14.2 & 43.0 & 34.4 \\   & MixSTE  & CVPR’22 & CPN  & 243 & 277.4 & 40.9 & **32.6** \\   & P-STMO  & ECCV’22 & CPN  & 243 & 1.5 & 43.0 & 34.4 \\   & Einflat _et al._ & WACV’23 & CPN  & 18 & 1.0 & 45.0 & 36.3 \\   & CA-PF-CPN (ours) &  &  &  &  &  &  &  \\   & CA-PF-HRNet-32 (ours) & & & HRNet-32  & 1 & 0.6 & 41.4 & 33.5 \\   & CA-PF-HRNet-48 (ours) & & & HRNet-48  & 1 & 0.6 & **39.8** & 32.7 \\   

Table 1: Comparison with both single-frame (top) and multi-frame (middle) methods on Human3.6M. MPJPE is reported in millimeters. The best results are in bold, and the second-best ones are underlined.

   Method & PCK \(\) & AUC \(\) & MPJPE \(\) \\  Xu _et al._ (_T_=1) & 80.1 & 45.8 & - \\ Li _et al._ (_T_=1) & 81.2 & 46.1 & 99.7 \\ Zeng _et al._ (_T_=1) & 82.1 & 46.2 & - \\  PoseFormer  (_T_=9) & 95.4 & 63.2 & 57.7 \\ MHFormer  (_T_=9) & 93.8 & 63.3 & 58.0 \\ MixSTE  (_T_=27) & 94.4 & 66.5 & 54.9 \\ P-STMO  (_T_=81) & 97.9 & 75.8 & 32.2 \\  CA-PF-HRNet-32 (ours) & 98.0 & 75.4 & 32.7 \\ CA-PF-HRNet-48 (ours) & **98.2** & **76.3** & **31.4** \\   

Table 2: Comparison on MPI-INF-3DHP. \(T\) indicates the number of video frames used by models.

### Ablation Study

**Gains from spatial context and ablations on model components.** In this section, we first investigate the gains _purely_ brought by spatial contextual clues from 2D pose detectors and then the effectiveness of each component in our method (please refer to Sec. 3.2 for more details). Specifically, we show how the context-agnostic counterpart of our method, (single-frame) PoseFormer  is converted to Context-Aware PoseFormer step by step (step-wise improvements are shown in Table 3).

* **Step 0, Context-Agnostic:** For PoseFormer, 2D joint coordinates detected by a pose detector are linearly embedded to dimension \(C\) as per-joint representation (_i.e._, joint embedding), and Inter-joint Modeling is subsequently performed to build up correlations across the joint embeddings. Plain 2D coordinates encode no spatial context for joints that helps reduce ambiguity, PoseFormer (and other lifting-based methods) is thus referred to as "context-agnostic". This serves as the starting point of our method.
* **Step 1, Context-Aware but simple concatenation:** The first step to make PoseFormer "context-aware" is to incorporate joint-context features into its per-joint representation. We retrieve the multi-scale feature maps produced by the 2D pose detector, simply sample feature vectors on the detected joint locations from each feature map, and project them to dimension \(C\). For each joint, the sampled context feature vectors that encode joint-centric visual cues are concatenated together with the coordinate embedding that encodes positional information. Even naive involvement (as simple as concatenation) of spatial context clues brings a huge improvement as shown in Table 3: a **15.0%** error reduction (from 51.2 to 43.5mm), which demonstrates that _leveraging the readily available visual representations learned by upstream backbones is effective and promising_.
* **Step 2, Context-Aware with Pose-Context Feature Fusion:** We promote cross-modality feature fusion by applying transformers to joint embeddings and the sampled multi-scale context features before concatenation, which brings another 1.6% error reduction.
* **Step 3, Final version of our Context-Aware PoseFormer:** The context features in step 1 are obtained by directly sampling at the detected joints, which leads to suboptimal performance since 2D pose detectors inevitably introduce noise. Thus, the sampled feature vectors can not well represent the context associated with the joint of interest. We introduce Deformable Context Extraction to mitigate this issue, which adaptively attends to the detected joint along with its surrounding regions using learnable sampling points, which further reduces the error by 3.3%.

We also offer visualization of learned sampling points (brighter colors denote larger attention weights) in Fig. 4, where they try to approach the ground truth despite the noisy detected input. Note that in training we do _not_ supervise them using ground truth 2D pose. More visualization is in Appendix E.

**What kind of spatial context are we looking for?** In this part, we go a step further to explore what context is desirable to improve 3D pose estimation accuracy. Our method benefits from multi-resolution feature maps produced by 2D detectors [52; 11; 63], which encode spatial contextual clues of various levels for human joints. For example, HRNet  outputs four feature maps in the last stage, which are 4\(\), 8\(\), 16\(\), 32\(\) downsampled compared to the input image size, respectively. We investigate which one of the four is most informative for our task via ablation experiments (in Table 4, MPJPE is reported on Human3.6M). Removing 4\(\) and 16\(\) downsampled feature

Figure 4: In the proposed Deformable Context Extraction module, sampling points attempt to approach the ground truth despite unreliable 2D joint detection.

    & Context- & Inter-joint & Pose-Context & Deform. Cont. &  &  \\  & aware & Modeling & Feature Fusion & Extraction & & \\  (0) & ✗ & ✓ & ✗ & ✗ & 446.0 & 51.2 \\ (1) & ✓ & ✓ & ✗ & ✗ & 448.0 & 43.5 (7.71) \\ (2) & ✓ & ✓ & ✓ & ✗ & 537.3 & 42.8 (8.41) \\ (3) & ✓ & ✓ & ✓ & ✓ & 609.9 & 41.4 (9.81) \\   

Table 3: Ablation study on spatial context and each component of our method. Experiments are conducted on Human3.6M with HRNet-32 as the backbone. MPJPE is reported in millimeters.

maps brings the most significant performance drop (a 3.6% and 4.1% error increase for each). In contrast, only a 1.0% error increase is observed when the 32\(\) downsampled feature map is eliminated.

Generally, high-resolution feature maps contribute more to 3D pose estimation than low-resolution ones. The reason is that low-resolution feature maps tend to keep the high-level semantics of joints, such as join type, which may not be relevant to our task - to localize human joints in 3D, where 2D-to-3D joint correspondence is pre-defined. However, we can still gain from low-resolution feature maps as they encode most wide-range spatial dependencies, promoting an understanding of the overall human skeleton structure.

**What pre-trained features are more desirable?** ImageNet  pre-trained backbones (_e.g._, ResNet ) profit a series of downstream tasks , yet this seems not applicable to 3D human pose estimation. In Table 5, we replace COCO pre-trained backbones in our method with ImageNet classification pre-trained ones, showing a remarkable performance drop. This should be attributed to the large gap between the pre-training task (image classification) and the downstream task (3D human pose estimation). Exploring more suitable pre-trained features that can transfer their effectiveness into the 3D HPE task is a promising direction. We place more ablations on backbones and pre-training datasets in Appendix D.

### Visualization

**Spatial context promotes temporal stability.** We show a frame-wise comparison on the Human3.6M  test set in Fig. 5. Our single-frame approach outperforms competitive MHFormer  using 351 frames. Moreover, our context-agnostic counterpart (Sec. 4.3) suffers from temporal instability (_i.e._, zigzags and error peaks), whereas our method presents a more stable trend. We also provide a qualitative comparison with both methods in Fig. 6. We include more visual comparison on both Human3.6M and MPI-INF-3DHP in Appendix F.

**Spatial context improves the robustness against occlusions**. We provide a comparison with PoseFormer  on in-the-wild videos in Fig. 7. We choose two sets of consecutive video frames where the 2D joint detection fails due to strong self-occlusion. Since PoseFormer only accepts 2D joints as input, its estimation is sensitive to the noise of input 2D poses. On the contrary, our method leverages spatial context from images in addition to 2D joint coordinates to localize joints in 3D. Thus, our method shows more reasonable and robust results despite noisy or even outlier 2D joints.

  
4\(\) & 8\(\) & 16\(\) & 32\(\) & MPJPE \(\) \\  ✓ & ✓ & ✓ & ✓ & 41.4 \\ ✗ & ✓ & ✓ & ✓ & 42.9\({}_{ 1.5}\) \\ ✓ & ✗ & ✓ & ✓ & 42.4\({}_{ 1.0}\) \\ ✓ & ✓ & ✗ & ✓ & 43.1\({}_{ 1.7}\) \\ ✓ & ✓ & ✓ & ✗ & 41.8\({}_{ 0.4}\) \\   

Table 4: Ablation study on feature resolutions.

   Backbone & Pre-training & MPJPE \(\) \\   & 2D Pose & 45.0 \\  & Image Class. & 51.4\({}_{ 6.4}\) \\   & 2D Pose & 41.4 \\  & Image Class. & 45.8\({}_{ 1.4}\) \\   & 2D Pose & 39.8 \\  & Image Class. & 43.9\({}_{ 1.1}\) \\   

Table 5: Ablation study on pre-training tasks.

Figure 5: Frame-wise comparison with MHFormer  and our context-agnostic counterpart (Sec. 4.3) on Human3.6M test set (S9 Discussion). \(T\) indicates the frame number used by models.

## 5 Conclusion and Discussion

This work presents a new framework that leverages readily available visual representations learned by off-the-shelf 2D pose detectors. Our approach removes the need for extensive long-term temporal modeling excessively employed by existing 3D human pose estimation methods. We further propose a straightforward baseline to showcase its effectiveness: our single-frame model significantly outperforms other single-frame and even multi-frame methods using up to 351 frames while being far more efficient. Our work opens up opportunities for future research to utilize visual representations from well-trained upstream backbones in an out-of-the-box manner.

**Limitations.** We observed in Sec. 4.4 that incorporating the spatial context of joints improves temporal stability, enhancing consistency and smoothness in the estimated results, even without access to explicit temporal clues. However, we acknowledge for all single-frame methods, including ours, mitigating jitters remains a challenge compared to multi-frame methods that leverage temporal clues. This is primarily due to the non-temporal nature of single-frame methods. A potential solution is to extend our method to model _short-term_ temporal dependencies, which should not introduce unacceptably high costs. We include some preliminary results in Appendix C. Compared to standard lifting-based methods, the memory cost of our method can be higher as we additionally process image features in the lifting stage. Leveraging the visual context of joints in a more lightweight way is our future research direction.

Figure 6: Visual comparison in hard cases, _e.g._, self-occlusion and noisy 2D detection. We include more qualitative results in Appendix F.

Figure 7: Comparison with PoseFormer  on in-the-wild videos. The 2D pose detector fails to localize 2D joints, given _confusing clothing_ (left) and severe _self-occlusion_ (right). Our method is more robust in such hard cases. False joint detection is indicated by yellow arrows, and the corresponding 3D joint estimation is indicated by orange arrows.