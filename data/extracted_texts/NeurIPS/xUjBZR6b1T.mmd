# ReVideo: Remake a Video with Motion and Content Control

Chong Mou\({}^{1,2,4}\), Mingdeng Cao\({}^{3,4}\), Xintao Wang\({}^{3*}\), Zhaoyang Zhang\({}^{3}\),

**Ying Shan\({}^{3}\), Jian Zhang\({}^{1,2,4}\)**

\({}^{1}\)School of Electronic and Computer Engineering, Peking University

\({}^{2}\)Peking University Shenzhen Graduate School-Rabbitpre AIGC Joint Research Laboratory

\({}^{3}\)ARC Lab, Tencent PCG \(\)\({}^{4}\) University of Tokyo

\({}^{4}\)Guangdong Provincial Key Laboratory of Ultra High Definition Immersive Media Technology

https://mc-e.github.io/project/ReVideo/

###### Abstract

Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to **R**emake a **Video** (**ReVideo**) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modify

Figure 1: The capability of our method to locally modify video content and motion. This ability can also be easily extended to multi-area editing. The motion control is labeled in colorful lines in videos.

ing the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, _i.e._, (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.

## 1 Introduction

Thanks to the large-scale training data and huge computing power, there have been significant advancements in diffusion-based  image and video generation. For personalization purposes, many works add control signals to the generation process, such as text-guided image [39; 40; 38] and video [18; 17; 13; 45] generation, as well as image-guided video generation [4; 54]. Based on these base models, extensive works explore how to transfer their generation capabilities to video editing. Early works based on text-to-image diffusion models implement video editing through zero-shot strategies (_e.g._, Fate-Zero , Flatten ) or one-shot tuning (_e.g._, Tune-A-Video ). However, these methods are limited by excessive manual design and a lack of video generation priors. Moreover, text prompt only provide coarse condition, limiting the editing accuracy. Compared to text, more recent methods adopt image conditions which can provide more accurate editing guidance. For instance, VideoComposer  generates style-transformed videos by providing spatial attributes (_e.g._, edge, depth) of the target video and a style reference. DreamVideo  and Make-a-protagonist  can modify a specific object in the video by providing a reference object. However, these methods still struggle with local editing and introducing new elements, such as adding new objects to a video. Recent work EVE  proposes a diffusion distillation strategy to achieve video editing while keeping unedited content unchanged. Nevertheless, the editing region and target are controlled by text, which is challenging in complex scenarios. AnyV2V  edit a video by modifying the first frame, enabling accurate customization of local content. Pika  can regenerate a specific area in the video by selecting an editing region. Although these methods improve the performance of local video editing, they only focus on visual content editing and cannot customize the motion of new content.

Motion is another crucial aspect of video, yet research on video motion editing remains limited. While some methods explore motion-guided video generation using trajectory-based motion guidance (_e.g._, DragNUWA , DragAnything , MotionCtrl , VideoSwap ) and box-based motion guidance (_e.g._, Boximator , Peekaboo ), they do not support motion editing. Additionally, other works [56; 30; 61] can transfer motion from one video to another but cannot modify it as well.

In this paper, our goal is to accurately edit content and motion in specific areas of a video. We create an easy-to-interact pipeline by setting the content editing as modifying the first frame, with trajectory lines  as the motion control signal. Other unedited content in all frames should be maintained in editing results and merged with the editing effect. However, we find that fusing unedited content with motion-customized new content is challenging, mainly for two reasons: **(1)** Training imbalance: Unedited content is dense and easier to learn, while motion trajectories are sparse and abstract, making them harder to learn. **(2)** Condition coupling: Unedited content provides both visual and inter-frame motion information, leading the model to rely on it for motion estimation, thereby ignoring the hard-to-learn trajectory lines.

To address these challenges, we design a three-stage training strategy to harmonize unedited content and motion-customized new content, enabling harmonious control of different conditions. Besides, we design a spatiotemporal adaptive fusion module to fuse these two conditions at different diffusion sampling steps and spatial locations. Furthermore, our method can compactly inject motion and content conditions into the diffusion video generation through a single control module. With these techniques, users can conveniently edit specific regions in the video by modifying the first frame and drawing trajectory lines. Notably, ReVideo is not limited to single-region editing and can customize multiple areas in parallel.

In summary, this work makes the following contributions:

* To the best of our knowledge, this is the first attempt to explore local editing of both content and motion in videos. Our method can also be easily extended to multi-area video editing.
* We propose a three-stage training strategy and a spatiotemporal adaptive fusion module to address the coupling of content and motion control in video editing, enabling compact control through a single module.
* Extensive experiments demonstrate that ReVideo performs well in several precise video editing applications, including changing content in a specific region while keeping motion constant, maintaining content while customizing new motion trajectories, and modifying both content and motion trajectories. Some examples are presented in Fig. 1.

## 2 Related Works

### Controllable Image and Video Generation

Recent advancements in diffusion models [19; 12] drive the rapid development of image and video generation. In the community of image generation, some notable works, such as Stable Diffusion , Imagen , and DALL-E2 , utilize text as the generation condition. To achieve accurate generation control, some methods, _e.g._, ControlNet  and T2I-Adapter , propose adding control modules on pre-trained diffusion models. Similarly, initial efforts in controllable video generation concentrate on the text condition, such as Video LDM , Imagen Video , VideoCrafter , and AnimateDiff . Recognizing the limitations of text prompts in capturing complex scenarios, some recent works [4; 54; 59; 13] leverage image conditions for a more direct approach. External control modules on pre-trained foundation models are also popular in controllable video generation. Such as video ControlNet [9; 60] extends the ControlNet  in image generation to video generation conditioned on a sequence of control signals, like edge maps and depth maps. In addition to spatial structure control, precise temporal motion control is also important in controllable video generation. This process is similar to the drag-based image editing [35; 32; 33]. Several recent works study this topic, such as video generation with trajectory-based motion guidance (_e.g._, DragNUWA , MotionCtrl , Motion-I2V , DragAnything ) and generation with box-based motion guidance (_e.g._, TrailBlazer , Boximator , ). These methods perform the control by training extra motion controllers on pre-trained video diffusion models.

### Diffusion-based Video Editing

Due to the lack of training data, the common approach in video editing is via training-free strategies [7; 15; 21; 25; 46; 36] or one-shot tuning [51; 3; 24]. For instance, the prior work Tune-A-Video  overfits some diffusion model parameters to a specific video. Then, it uses the overfitting parameters to produce the editing result conditioned on the target prompt. To enable a cohesive global appearance among edited frames, many methods extend the attention module of Stable Diffusion  to encompass multiple frames and conduct cross-frame attention. For instance, Pix2Video  edits the first frame and performs cross-frame attention of each frame on the first frame to preserve appearance consistency. TokenFlow  and Fairy  jointly edit a few key frames at each denoising step and propagate them throughout the video based on the nearest-neighbor field extracted from the original video. Inspired by the initial zero-shot image editing method SDEdit , the recent video foundation model SORA  achieves video editing by adding noise to the input video and then denoising it under the target description. Although these methods can preserve the general structure of original videos, the information loss and the lack of consistency constraints on the original video make them unfit for precise video editing but suitable for global editing like style transfer.

Another strategy is to train a control module to guide the generation with some characters that should persist in the editing result, such as depth [14; 27; 53], sketch , and optical flow . However, existing methods primarily focus on preserving spatial structure and are unsuitable for precise video editing. In the community of precise video editing, some works, such as InsV2V  and the recent EVE , edit the video by providing editing instructions. However, the text-based editing instruction struggles to locate a target region in some complex scenarios. AnyV2V  can edit a video by editing the first frame. Pika  is designed to regenerate a selected area in a video by text guidance.

Unlike these works, we aim to achieve accurate customization in local areas of a video. The editing target includes locally modifying content and motion and keeping the unedited content unchanged.

## 3 Method

### Preliminaries

**Stable Video Diffusion** (SVD)  is a high-quality and commonly used image-to-video generation model. To utilize the priors of high-quality video generation, we employ SVD as the base model and add control modules to achieve our editing target. Given a reference image \(_{I}\), SVD will generate a video frame sequence \(=\{^{0},^{1},...,^{N-1}\}\) of length \(N\), starting with \(_{I}\). The sampling of SVD is conducted on a latent denoising diffusion process . At each denoising step, a conditional 3D UNet \(_{}\) is used to iteratively denoise this sequence:

\[}_{0}=_{}(_{t},t,_{I}),\] (1)

where \(_{t}\) is the latent representation of \(_{t}\), \(}_{0}\) is the predication of \(_{0}\). There are two condition paths for the reference image \(_{I}\): (1) It is embedded into tokens by CLIP  image encoder and injected into the diffusion model vis cross-attention ; (2) It is encoded into a latent representation by the VAE encoder, and concatenated with the latent of each frame in channel dimension. SVD follows the EDM-preconditioning framework , which parameterizes the learnable denoiser \(_{}\) as:

\[_{}(_{t},t,_{I};)=c_{skip}() _{t}+c_{out}()F_{}(c_{in}()_{t},t, _{I};c_{noise}()),\] (2)

where \(\) is the noise level, and \(F_{}\) is the network to be trained. \(c_{skip}\), \(c_{out}\), \(c_{in}\), and \(c_{noise}\) are preconditioning hyper-parameters. \(_{}\) is trained via denoising score matching (DSM):

\[_{_{0},t,(0,^{2})}[ _{}||_{}(_{0}+,t,_{I})- _{0}||_{2}^{2}].\] (3)

### Task Formulation and Some Insights

**Task formulation**. The purpose of this paper is to locally edit a video, including visual information and motion information. In addition, the unedited content in the video should remain unchanged. Therefore, our conditional video generation involves three control signals: (1) the edited content, (2) the content of the unedited area, and (3) the motion condition in the edited area. We implement content editing by modifying the first frame of the video and then broadcasting it to subsequent video frames. Here, we denote the edited first frame as \(_{ref}^{3 W H}\). For the motion condition, we use interaction-friendly trajectory lines [57; 52] as the control signal. Specifically, the motion condition also contains N maps for a N-frame video. Each map consists of 2 channels, indicating the movement of the tracked points in the horizontal and vertical directions relative to the previous frame. The motion condition in this paper is represented as \(_{mot}^{N 2 W H}\). The unedited content \(_{cons}\) can be conveniently provided by the masked video, _i.e._, \(_{con}=\), where \(^{N 3 W H}\) and \(^{1 1 W H}\) refer to the original video and the editing region mask, respectively.

Since we adopt SVD as the pre-trained base model, its image-to-video capability can naturally serve as the import port for the edited first frame. For unedited content and customized motion trajectories, we train additional control modules to import them into the generation process.

Figure 2: Two potential structures to inject motion and content control. Structure A is a compact and efficient mode that integrates motion and content control via a single module. Structure B features independent control, structurally decoupling motion and content conditions, causing higher complexity.

**Trajectory sampling**. During training, it is essential to extract trajectories from videos to provide motion condition \(_{mot}\). At the beginning of trajectory sampling, we use a grid  to sparsify dense sampling points, obtaining \(N_{init}\) initial points. Among these points, those with larger motions are beneficial to train trajectory control. To filter out these points, we first apply motion tracking on each point to obtain their path lengths, _i.e._, \(\{l_{0},l_{1},...,l_{N_{init}-1}\}\). We use the mean of these lengths as the threshold \(l_{Th}\) to extract points whose motion length is greater than \(l_{Th}\). Then, we use the normalized lengths of these points as sampling probabilities to sample \(N\) points randomly. Because the high sparsity is not conducive for the model to learn from these trajectories, we apply a Gaussian filter  to obtain the smooth trajectory map \(_{mot}\). More details are presented in **Appendix**.

**Insights**. A naive implementation of our editing target is directly training an extra control module, like ControlNet , to inject motion and content conditions into the diffusion generation process. We present this design in structure **A** of Fig. 2. Specifically, at the input, a content encoder \(E_{c}\) and a motion encoder \(E_{m}\) embed the content condition \(_{con}\) of the unedited area and motion condition \(_{mot}\) of the editing area. These two embeddings are merged by direct summing to obtain the fused condition feature \(_{c}\). Then, a copy of the UNet encoder extracts multiscale intermediate features from \(_{c}\), which are added to the corresponding layers in the diffusion model. This process is formulated as:

\[_{c}=(_{t},t,_{ref};)+((_{t}+(_{c}),t,_{ref}; _{c})),\] (4)

where \(_{c}\) is the new diffusion features. \(\) is the function of zero-conv . \(\) and \(_{c}\) are the parameters of the SVD model and extra control module. We conduct several toy experiments based on this idea, as illustrated in Fig.3. The input video contains a woman initially moving to the left, followed by a shift to the right. The editing target is to alter the facial motion towards the right while keeping the other content unchanged. In the toy experiment 1, we fix SVD and train the control module with Eq. 3. The result shows that the content condition precisely controls the unedited area of the generated video. But the motion condition has no control effect, and the trajectory lines in the editing area (labeled with a black box) are consistent with the unedited area. A possible reason is that a single control branch has difficulty handling two control conditions simultaneously. To verify this hypothesis, we train structure B in Fig. 2 to handle these two conditions separately. The toy experiment 2 in Fig. 3 shows that the motion control is still ineffective, suggesting that the problem is more attributed to the control training rather than the network structure. To enhance the motion control training, we split the training of structure B into two stages. In the first stage, we only train the motion control module to endow it with motion control prior. In the second stage, we train the motion control and content control together. The result in toy experiment 3 shows that although the motion prior training produces good motion control capability, the control accuracy is weakened and affected by the unedited content after introducing the content control. After these toy experiments, we have the following insights:

\(\) The condition of unedited content not only contains visual information but also has rich inter-frame motion information. As a more easily learned condition, the diffusion model tends to predict the motion of the editing area through unedited content, ignoring the sparse motion trajectory control.

Figure 3: The motion control capability of two structures in Fig. 2 with different training strategies. We visualize trajectory lines in a specific area (red box) and label the editing area with a black box. Toy experiments present the coupling issue of customized motion and unedited content.

\(\) The coupling between motion-customized new content and unedited content is strong, making it difficult to overcome even using the motion prior and separate control branches.

\(\) Motion prior training is helpful in decoupling motion-customized content and unedited content.

### Coarse-to-fine Training Strategy

To rectify the ignoring of the motion control, we design a coarse-to-fine training strategy. In addition, structure B in Fig. 2 has a high computational cost, and we hope to joint control the unedited content and motion-customized new content on the concise structure A.

**Motion prior training**. As discussed above, motion trajectory is a sparse and difficult-to-learn control signal. Toy experiment 3 in Fig. 3 shows that the motion prior training can alleviate the coupling between motion-customized content and unedited content. Hence, in the first stage, we only train the motion trajectory control, allowing the control module to have good motion control prior.

**Decoupling training**. Based on the control module from the first stage, the training in the second stage aims to add content control of unedited areas. Toy experiment 3 in Fig. 3 shows that even with good motion control priors, the precision of motion control still degrades after introducing unedited content condition. Therefore, we design a training strategy to decouple motion and content control in this stage. Specifically, we set the editing part and the unedited part in a training sample \(\) to be two different videos, _i.e._, \(_{1}\) and \(_{2}\). As shown in Fig. 4, \(_{1}\) and \(_{2}\) are combined through the editing mask \(\), _i.e._, \(=_{1}+_{2}(1-)\). Since the editing region and the unedited region come from two different videos, the motion information of the editing region cannot be predicted through the unedited content. Therefore, it can decouple content control and motion control during training.

**Deblocking training**. As shown in the right part of Fig. 4, although the decoupling training achieves joint control of customized motion and unedited content with high accuracy, it breaks the consistency between the edited and unedited regions, producing block artifacts in the boundary. To rectify this issue, we design the third training stage to remove block artifacts. The training in this stage is initialized with the model from the second stage and trained on normal video data. To preserve the decoupled motion and content control prior from the second stage, we only fine-tune the key embedding \(_{k}\) and value embedding \(_{v}\) in temporal self-attention layers of the control module and SVD model. The toy experiment 4 in Fig. 3 shows that after the training of this stage, the model removes the block artifacts and retains joint control of unedited content and motion customization.

### Spatiotemporal Adaptive Fusion Module

Although the coarse-to-fine training strategy achieves decoupling of content control and motion control, we observe considerable failure cases in some complex motion trajectories. To further distinguish the control roles of unedited content and motion trajectories in the generation, we design a spatiotemporal adaptive fusion module (SAFM) as shown in Fig. 5. Specifically, SAFM predict a weight map \(\) through the editing mask \(\) to fuse motion and content control instead of direct summing. Moreover, because diffusion generation is a multi-step iterative process, the fusion of control conditions between time steps should have adaptive adjustment. Therefore, we concatenate timestep \(t\) and \(\) in the channel dimension to form a spatiotemporal condition to guide the \(\) prediction. Mathematically, the fusion of motion and content conditions is formulated as follows:

\[_{c}=E_{c}(_{con})+E_{m}(_ {mot})(1-),\ =(,t),\] (5)

Figure 4: The data construction strategy for decoupling training and editing results from this stage.

where \(\) is the function of spatiotemporal embedding. \(\) needs to be jointly trained with \(_{k}\) and \(_{v}\) in the deblocking training stage. We visualize \(\) at different time steps in the right part of Fig. 5. It can be seen that \(\) learns the spatial characteristics of the editing area. It assigns a higher weight to the motion condition in the editing area and a higher weight to the content condition in the unedited area. In addition, \(\) learns to distinguish different sampling steps \(t\) and linearly adjusts with \(t\).

## 4 Experiments

### Implementation Details

In this work, we choose Stable Video Diffusion (SVD) as the base model. Our three training stages are completed on the WebVid  dataset, which contains 10 million text-video pairs. These three stages are optimized for \(40K\), \(30K\), and \(20K\) iterations, respectively, with Adam  optimizer on 4 NVIDIA A100 GPUs. The batch size for each GPU is set as 4, with the resolution being \(512 320\). It takes about 6 days to complete all training stages. During the training process, we use CoTracker  to extract motion trajectories. In the first training stage, trajectory sampling is performed throughout the video. In the second and third training stages, a rectangular editing area is randomly selected in the video with the minimum size being \(64 64\), and trajectory sampling is performed within it. The number of trajectory lines for each training sample is randomly selected between 1 and 10.

### Comparison

Among existing methods, Pika  is the most similar to ours. Pika can perform local video editing by defining an editing area. The difference is that Pika controls the new content in the editing area by text and has no motion control. In addition, the recent work AnyV2V  proposes editing the first frame of the video to achieve entire video editing, which has similarities with our ReVideo. InsV2V , using editing instructions to edit the video, can also maintain unedited content. Therefore, in this paper, we compare our ReVideo with these three methods. The visual comparison in Fig. 6 shows that in some fine-grained editing scenarios, such as putting sunglasses on a man, AnyV2V has a loss of edited content. In addition, the unedited area of InsV2V and AnyV2V suffers from content distortion. Although Pika can generate smooth and high-fidelity results, it is difficult to accurately customize new content by text, especially in adding new objects, _e.g._, adding a dog on the soccer field. Adding new objects to the scene is also challenging for InsV2V. Due to the lack of motion

    &  &  &  \\  & PSNR \(\) & Text Alignment \(\) & Consistency \(\) & Overall \(\) & Editing Target \(\) \\  InsV2V  & 29.77 & 0.2022 & 0.9808 & \(10.2\%\) & \(5.1\%\) & 132s \\ AnyV2V  & 29.80 & 0.2143 & 0.9836 & \(2.8\%\) & \(4.0\%\) & 380s \\ Pika  & **33.07** & 0.2184 & **0.9956** & \(27.9\%\) & \(23.9\%\) & - \\ ReVideo & 32.85 & **0.2304** & 0.9864 & \(\) & \(\) & **26**s \\   

Table 1: Quantitative comparison between our ReVideo and other related works. We employ automatic metrics (_i.e._, CLIP  score, PSNR) and human evaluation to evaluate the performance.

Figure 5: The architecture of our proposed spatiotemporal adaptive fusion module (left), and the visualization of fusion weight \(\) at different timesteps (right).

control, AnyV2V and Pika usually produce static motion of the edited content, such as a car driving on the road. In comparison, our ReVideo can effectively broadcast the edited content throughout the entire video while allowing users to customize the motion in editing areas.

In addition to visual comparison, we employ automatic metrics and human evaluation to measure the performance of different methods. For this task, we build a test set containing 16 videos, with the resolution being \(720 1280\). Following previous works , automatic metrics employ CLIP score  to measure text alignment and temporal consistency. The text alignment is obtained by calculating the average CLIP cosine similarity between each frame and editing description. Temporal consistency is computed by average CLIP cosine similarity between every pair of consecutive frames. We employ PSNR  to measure the reconstruction quality of unedited content. The human evaluation considers two aspects, _i.e._, overall video quality, and whether the editing target is achieved. We allow 20 volunteers to choose the best method for each test sample on each aspect. The results in Tab. 1 show that our ReVideo performs better than InsV2V and AnyV2V in all evaluation terms.

Figure 6: The visual comparison between InsV2V , AnyV2V , Pika , and our ReVideo.

Compared with Pika, our performance is slightly lower in the evaluation of temporal consistency and the quality of unedited content. Notably, AnyV2V and Pika usually generate static motion of new content due to the lack of motion control. Static motion tends to score higher in consistency evaluation, measured by CLIP similarity of adjacent video frames. Our method has obvious advantages over Pika in text alignment and human evaluation, reflecting the significant gap between text-guided local editing and user-specified local editing. Our ReVideo can precisely specify the appearance and motion of the editing area, better meeting requirements for accurate customization.

The time complexity of different methods is also presented in Tab. 1. The experiment is conducted on an A100 GPU, with the video resolution being 768x768. Results show that our method has significantly lower time costs compared to other methods.

### Ablation Study

In our ReVideo, we design the spatiotemporal adaptive fusion module (SAFM) to help decouple the control of unedited content and motion customization in diffusion generation. It predicts a fusion weight \(\) conditioned on the editing area \(\) and time step \(\). Then, the fusion of content and motion control is achieved through Eq. 5. In this part, we conduct an ablation study on this fusion mechanism. In addition, we only fine-tune the key embedding and value embedding of the temporal self-attention layers in the SVD model and control module in the stage of deblocking training. In the ablation study, we discuss the impact of tuning parameters in deblocking training.

**The effectiveness of SAFM**. To demonstrate the effectiveness of SAFM, we replace SAFM with direct summing of motion and content control. The results in Fig. 7 show that the direct summing fusion cannot accurately control the motion in some complex motion trajectories, _e.g._, wavy lines. In comparison, using SAFM can help decouple content and motion control in the editing area, achieving more accurate trajectory guidance.

**The effectiveness of time adaptation in SAFM**. We remove the time condition in the SAFM module, _i.e._, using the same weight map \(\) to fuse content and motion control in each diffusion sampling step. The results in Fig. 7 show that not distinguishing \(\) in different sampling steps leads to unsatisfactory artifacts at the boundary of the editing area.

**Tuning parameters in deblocking training**. Although the training in stages 1 and 2 enables the control module to have good local motion control capabilities, we find that there is still an ignoring of motion control in the training of stage 3, _i.e._, deblocking training. As shown in Fig. 7, the local motion control capability is degraded after we tune the entire control module in stage 3. Therefore, we optimize a part of the parameters to maintain the prior of local motion control. Experiments show that fine-tuning spatial layers still triggers the ignoring of motion control. In comparison, fine-tuning key embedding and value embedding of the temporal layer in the control module and the base model has minimal impact on local motion control capability. The edited and unedited areas are also harmoniously fused. More ablations of tuning parameters are presented in **Appendix**.

Figure 7: Ablation study of our ReVideo.

Conclusion

In this paper, we aim to solve the problem of local video editing. The editing target includes visual content and motion trajectory modifications. To the best of our knowledge, this is the first attempt at this task. In this new task, We find a coupling problem between unedited content and motion customization. Directly training these two control conditions on the video generation model will cause the ignoring of motion control. To address this issue, we develop a three-stage training strategy to combine these two conditions coarse to fine. In addition, we design a spatiotemporal adaptive fusion module to further decouple unedited content and motion-customized content in different diffusion sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, _i.e._, (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also easily extend these applications to multi-area editing without specific training.

**Limitations**. Although our method can regenerate local areas of the video, the regeneration quality is limited by the base model. In some scenarios where the generation prior of SVD is not ideal, some unexpected artifacts may occur in the editing results.