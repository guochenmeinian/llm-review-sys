# What Representational Similarity Measures Imply about Decodable Information

Sarah E. Harvey

Center for Computational Neuroscience

Flatiron Institute, New York, NY

sharvey@flatironinstitute.org &David Lipshutz

Center for Computational Neuroscience

Flatiron Institute, New York, NY

dlipshutz@flatironinstitute.org &Alex H. Williams

Center for Neural Science, Center for Computational Neuroscience

New York University, Flatiron Institute, New York, NY

alex.h.williams@nyu.edu

###### Abstract

Neural responses encode information that is useful for a variety of downstream tasks. A common approach to understand these systems is to build regression models or "decoders" that reconstruct features of the stimulus from neural responses. Popular neural network similarity measures like centered kernel alignment (CKA), canonical correlation analysis (CCA), and Procrustes shape distance, do not explicitly leverage this perspective and instead highlight geometric invariances to orthogonal or affine transformations when comparing representations. Here, we show that many of these measures can, in fact, be equivalently motivated from a decoding perspective. Specifically, measures like CKA and CCA quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. We also show that the Procrustes shape distance upper bounds the distance between optimal linear readouts and that the converse holds for representations with low participation ratio. Overall, our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures.

## 1 Introduction

The computational neuroscience and machine learning communities have developed a multitude of methods to quantify similarity in population-level activity patterns across neural systems. Indeed, a recent review by Klabunde et al. (2020) catalogues over thirty approaches to quantifying representational similarity. Many of these measures quantify similarity in the _shape_ or _representational geometry_ of point clouds. For example, recent papers (e.g. (Srivastava et al., 2017; Li et al., 2018)) leverage the Procrustes distance and other concepts from _shape theory_--an established body of work that formalizes the notion of shape and ways to measure distance between shapes (Krause et al., 2018; Li et al., 2018). Other measures of representational similarity are not quite this explicit but still emphasize desired _geometric invariance_ properties. For example, work by Kornblith et al. (2019) popularized centered kernel alignment (CKA) by emphasizing its invariance to isotropic scaling, translation, and orthogonal transformations. These are precisely thesame invariances specified by classical shape theory [18; 10]. Earlier work by Raghu et al.  argued for a more flexible invariance to affine transformations, and they used canonical correlations analysis (CCA) for this purpose. Contemporary work in neuroscience is replete with similar geometric reasoning and quantitative frameworks .

Here we ask: What does the similarity between neural representations, potentially in terms of shape or geometry, imply about the similarity between _functions_ performed by those neural systems? Potentially, not very much. Maheswaranathan et al.  showed that recurrent neural networks with different architectures performed the same task with similar dynamical algorithms, but with different representational geometry. More recently, Lampinen et al.  showed that representational geometry can be biased by other nuisance variables, such as the order in which multiple tasks are learned. Similar limitations of representational geometry measures are discussed in [11; 8].

On the other hand, several basic observations suggest that geometry and function ought to be related. Consider, for example, the common practice of using linear models to decode task-relevant variables from neural population activity [1; 24]. The premise behind these analyses is that anything linearly decodable from system \(A\) is readily accessible to layers or brain regions immediately downstream of \(A\). Therefore, information that is linearly decodable from \(A\) may relate to functional role played by \(A\) within the overall system . Notably, the invariances of typical representational similarity measures--translations, isotropic scalings, and orthogonal transformations--closely coincide with the set of transformations that leave decoding accuracy unchanged. For example, translations and rotations of neural population activity will not impact the accuracy of a linear decoder with an intercept term and an \(L_{2}\) penalty on the weights. Thus, if we accept the premise that decoding accuracy is a proxy--even, perhaps, an imperfect proxy--for neural system function, then representational geometry may be a reasonable framework to capture something about this function .

We provide a unifying theoretical framework that connects several existing methods of measuring representational similarity in terms of linear decoding statistics. Specifically, we show that popular methods such as centered kernel alignment (CKA)  and similarity based on canonical correlations analysis (CCA)  can be interpreted as alignment scores between optimal linear decoders with particular weight regularizations. Additionally, we study how the _shape_ of neural representations is related to decoding by deriving bounds that relate the average decoding distance and the Procrustes distance. We find that the Procrustes distance is a more strict notion of geometric dissimilarity than the expected distance between optimal decoder readouts, in that a small value of the Procrustes distance implies a small expected distance between optimal decoder readouts but the converse is not necessarily true. This formalizes recent observations by Cloos et al. , who found that high Procrustes similarity implied high CKA similarity in practical settings.

## 2 Theoretical Framework for Linear Decoding

We use \(^{M N_{X}}\) and \(^{M N_{Y}}\) to denote matrices holding responses to \(M\) stimulus conditions measured across neural populations consisting of \(N_{X}\) and \(N_{Y}\) neurons, respectively. Throughout this paper we will assume that the neural responses have been preprocessed so that the columns of \(\) and \(\) have zero mean.

### Linear decoding from neural population responses

We first consider the problem of predicting (or "decoding") a target vector \(^{M}\) from neural population responses by a linear function \(\) where \(^{N_{X}}\). One can view this as a simplified neural circuit model where each element of \(}=+\) is the firing rate readout unit the \(M\) conditions. Here, we interpret \(\) as a vector of synaptic weights and the random vector \(\) represents potential noise corruptions; for example, \(_{i}(0,^{2})\) where \(^{2}>0\) specifies the scale of noise. This neural circuit interpretation of linear decoding is fairly standard within the literature [13; 17].

Within this setting, we formalize the problem of decoding \(\) from the population activity \(\) through the following class of optimization problems:

\[}{}^{}- ^{}()\] (1)where \(()\) is a function mapping \(^{M N}\) onto symmetric positive definite \(N N\) matrices. The objective is concave, and equating the gradient to zero yields a closed form solution:

\[^{*}=()^{-1}^{}\] (2)

To appreciate the relevance of eq.1, note that if \(()=_{X}+\), where \(_{X}:=^{}\) is the empirical covariance of \(\), then the solution \(^{*}\) coincides with the optimum of the familiar ridge regression problem:

\[}{} \|-\|_{2}^{2}+\|\|_{2} ^{2}\] (3)

with solution \(^{*}=(_{X}+)^{-1}^{}\), where \(>0\) specifies the strength of regularization on the coefficients. More generally, we can interpret eq.1 as maximizing the inner product between the target, \(\), and the linear readout, \(\), plus a penalty term on the scale of \(\) as quantified by the function \(^{}()}\), which is a norm on \(^{N}\).

Equation1 is a useful generalization of eq.3 because there are situations where the scale of the decoded signal matters. For instance, when the readout unit is corrupted with noise, \(}=+\) where \(_{i}(0,^{2})\), we can interpret eq.1 as maximizing the signal-to-noise ratio subject to a cost on the magnitude of the weights \(\).

In most cases, we will consider choices of \(()\) that can be written as:

\[()=a_{X}+b\] (4)

for some \(a,b 0\). Under this choice, the penalty on \(\) equals the sum of two terms:

\[^{}()=\|\|_{2}^{2}+b\|\|_{2}^{2}.\] (5)

The first term, scaled by \(a\), penalizes the activation level of the readout unit, which can be viewed as an energetic constraint. The second term, scaled by \(b\), penalizes the strength of the synaptic weights, which can be viewed as a resource constraint for building and maintaining strong synapses. We note again that ridge regression is achieved as a special case when \(a=1\) and \(b=\).

### Quantifying differences between networks through the lens of decoding

We now turn our attention to the the problem of quantifying representational similarity between \(^{M N_{X}}\) and \(^{M N_{Y}}\). As before, we specify a target vector \(^{M}\) and optimize \(\) as

Figure 1: Schematic of the proposed framework for comparing representations \(\) and \(\) (each dot represents mean neural responses to one of \(M\) conditions) in terms of a decoding target \(\). First, optimal linear decoding weights \(^{*}\) and \(^{*}\) are computed. Then the similarity between the two systems is measured in terms of the _decoding similarity_: \(^{*},^{*}\).

specified in eq. (1). We optimize \(\) accordingly:

\[}{}^{}- ^{}()\] (6)

yielding the solution \(^{*}=()^{-1}^{}\). We are interested in quantifying similarity between \(\) and \(\) by comparing the behavior of these optimal decoders. A straightforward way to quantify the alignment between the decoded signals is to compute their inner product, which we call the _decoding similarity_ (fig. 1):

\[^{*},^{*}=^{*}^{}^{*}=^{}_{X}_{Y}\] (7)

where we have leveraged eq. (2) and the analogous closed form expression for \(^{*}\) while introducing the following normalized kernel similarity matrices:

\[_{X}:=()^{-1}^{} _{Y}:=()^{-1}^{}.\] (8)

Note that we have suppressed the dependence of \(_{X}\) and \(_{Y}\) on the function \(()\) to reduce notational clutter.

Equation (7) is a reasonable quantification of similarity between \(\) and \(\) with respect to a fixed decoding task specified by \(^{M}\), and it may be a fruitful approach for hypothesis-driven comparisons of neural systems. We comment further on this possibility in our discussion. On the other hand, many neural representations support a variety of downstream behavioral tasks. For example, features extracted in early stages of visual processing (edges and textures) can be used to support many visual tasks such as object classification, segmentation, image compression, et cetera. How can eq. (7) be adapted to quantify the similarity between \(\) and \(\) across more than one pre-specified decoding task? We explore three simple ideas.

**Option 1 -- best case alignment.** An optimistic measure of representational similarity between networks is to find the decoding task \(^{M}\) that results in a maximal decoder alignment. Formally,

\[_{\|\|_{2}=1}^{*},^{*}=_{ \|\|_{2}=1}^{}_{X}_{Y}\] (9)

where the constraint that \(\|\|_{2}=1\) is needed to keep the maximal value finite.

**Option 2 -- worst case alignment.** An alternative is to find the task that minimizes alignment:

\[_{\|\|_{2}=1}^{*},^{*}=_{ \|\|_{2}=1}^{}_{X}_{Y}\] (10)

which is obviously the pessimistic counterpart to option 1 above.

**Option 3 -- average case alignment.** Instead of maximizing or minimizing over \(\), we can approach the problem by averaging over a distribution of decoding tasks. Formally, we can quantify alignment in terms of the _average decoding similarity_:

\[^{*},^{*}\] (11)

where the expectation is taken with respect to \( P_{}\).

In the next subsection, we show that the third option is closely related to several existing neural representational similarity measures. However, all three quantities are potentially of interest and they are easy to compute as we document below in propositions 1 and 2.

**Proposition 1**.: _The best case and worst case decoding alignment scores, eqs. (9) and (10), are respectively given by the largest and smallest eigenvalues of:_

\[(_{X}_{Y}+_{Y}_{X})\] (12)

**Proposition 2**.: _The average decoding similarity, eq. (11), is given by:_

\[^{*},^{*}= (_{X}_{z}_{Y}),\] (13)

_where \(_{z}:=[^{}]\) represents a kernel matrix for the targets._Proof of proposition 1.: Let \(^{M M}\) be any matrix, potentially non-symmetric. We can express \(\) as a sum of a symmetric and skew symmetric matrix, \(=+\), where:

\[=(+^{})=(-^{})\] (14)

For any \(^{M}\), we have \(^{}=^{}+^{}\). It is easy to verify that \(^{}\) equals zero. Thus, \(^{}=^{}\), and maximizing over \(\) subject to \(\|\|_{2}\) yields the largest eigenvalue of \(\) (since it is symmetric). Likewise, minimizing yields the smallest eigenvalue of \(\). Substituting \(=_{X}_{Y}\) into this argument proves the proposition. 

Proof of proposition 2.: Using eq.7, we have:

\[^{*},^{*}=[^{ }_{X}_{Y}].\]

Using the cyclic property of the trace operator to manipulate this further, we get:

\[^{}_{X}_{Y}=(_{X} ^{}_{Y}).\]

Taking the expectation with respect to \(\) and using linearity of trace yields eq.13. 

## 3 Interpretations of CKA and related measures

Intuitively, proposition2 says that the expected inner product between optimal linear readouts is equal to a weighted matrix inner product between kernel matrices \(_{X}\) and \(_{Y}\) (with respect to a positive semi-definite weighting matrix \(_{z}\), which of course depends on the distribution of decoding targets \(\)). Such kernel matrices and inner products are important for several methods of quantifying representational similarity. We can therefore leverage this result to provide new interpretations of several distance measures. The most basic idea is to consider a distribution over \(\) which satisfies \(_{z}=\). This leads us to measure distances using standard Euclidean geometry, which we state as the following corollary to proposition2. We explore a relaxation of this assumption and the limit as the number of input samples \(M\) in appendixC.

**Corollary 1**.: _For any distribution over decoding targets satisfying \(_{z}=\), the Frobenius inner product between normalized kernel matrices \(_{X}\) and \(_{Y}\) is equal to the average decoding similarity:_

\[^{*},^{*}=[ _{X}_{Y}]\] (15)

_Further, when \(_{z}=\), the (squared) average decoding distance is equal to the (squared) Euclidean distance between \(_{X}\) and \(_{Y}\):_

\[\|^{*}-^{*}\|_{2}^{2}=\|_{X}-_{Y }\|_{F}^{2}.\] (16)

This corollary can be used to unify several neural representational similarity measures, each corresponding to a different choice of the penalty function \(()\), Table1. Further, it yields an interpretation of each measure in terms of the expected overlap or difference in decoding readouts between networks.

   Similarity measure & \(a\) & \(b\) \\  Linear CKA & 0 & \(b\) \\ GULP & 1 & \(\) \\ CCA & 1 & 0 \\ ENSD & 0 & \([_{X}^{2}]\) \\   

Table 1: Existing similarity measures are equivalent to the decoding score in eq.7 or the decoding distance in eq.16 for different choices of \(a\) and \(b\) in eq.5.

Linear CKAFirst, for \(b>0\), the following choice of \(()\) yields the linear CKA score :

\[()=b^ {*},^{*}}{^{*},^{*}^{*},^{*}}}=[_{X}_{Y}]}{\|_{X}\|_{F}\|_{Y}\|_{F}}= (,)\]

where we have normalized the expected inner product to obtain a similarity score ranging between 0 and 1. Note that the centering step of CKA is taken care of by our assumption, stated at the beginning of section 2, that the columns of \(\) and \(\) are preprocessed to have mean zero. Thus, linear CKA be interpreted as a normalized average decoding similarity with quadratic weight regularization.

GULP distanceThe CKA score is closely related to Euclidean distance, so it is not surprising that we can extend our analysis to other methods that utilize this distance. For example, the sample GULP distance, proposed by , is the equal to the average decoding distance with a regularization parameter \(>0\). In particular,

\[()=_{X}+\|^{*}-^{*}\|_{2}^{2}=\|_{X}-_{Y}\|_{F}^{2}= _{}^{2}(,)\]

yields the plug-in estimate of the squared GULP distance [3, section 3]. We therefore see that the Euclidean distance between normalized kernel matrices can be interpreted as equal to the average decoding distance, or also as an approximation of the population formulation of the GULP distance established in .

Canonical correlation analysis (CCA)CCA is another method that has been used to compare neural representations that fits nicely into our framework . When the two networks have the same dimension, i.e. \(N=N_{X}=N_{Y}\), the output of CCA is a sequence of \(N\) canonical correlation coefficients \(1_{1}_{N} 0\). The average squared canonical coefficients is sometimes used as a measure of similarity,1 and Kornblith et al.  showed this to be related to the linear CKA score on whitened representations. In particular, assuming the covariance matrices \(_{X}\) and \(_{Y}\) are invertible, we have:

\[()=_{X} ^{*},^{*}}{^{*}, ^{*}^{*},^{*} }}=_{i=1}^{N}_{i}^{2}\] (17)

In practice, it is often useful to incorporate regularization into CCA because the covariance matrices \(_{X}\) and \(_{Y}\) may be ill-conditioned. As mentioned above, the regularization used in GULP distance corresponds to choosing \(()=_{X}+\). A similar approach proposed in  is to use \(()=(1-)_{X}+\) for a hyperparameter \(0 1\). This effectively allows for a continuous interpolation between a CCA-based similarity score and linear CKA.

Effective Number of Shared Dimensions (ENSD)Finally, we note a connection to recent work by Giaffar et al. , who studied the following quantity, which they call the ENSD:

\[(,)=[^{}] [^{}][^{} ^{}]}{[^{}^{}] [^{}^{}]}\] (18)

This is simply a rescaling of the inner product, \([^{}^{}]\) and therefore easily fits within our framework using the following choice for \(()\):

\[()=[_{X}^{2}]}{[ {C}_{X}]}^{*}, ^{*}=(,)\] (19)

One of the most interesting features of ENSD is it's connection to the **participation ratio**, \(\):

\[(_{X})=[_{X}])^{2}}{ [_{X}^{2}]}=(,)\] (20)

which is used within physics and computational neuroscience (e.g. ) as a continuous analogue to the rank of a matrix, and is a measure of the effective dimensionality of the data matrix \(\). In particular, one can show that \(1(_{X})(_{X})\), and that these two inequalities respectively saturate when \(_{X}\) has only one nonzero eigenvalue and when all nonzero eigenvalues are equal. It is also interesting to note that the _inverse_ participation ratio can be captured by average decoding similarity under a very simple choice for \(()\):

\[()=[_{X}] ^{*},^{*}=\| ^{*}\|^{2}=(_{X})}\] (21)

We reiterate that all of the results enumerated above depend on choosing a distribution over decoding targets which satisfies \(_{z}=\). This enforces the decoding targets \(z_{1},,z_{M}\) to be uncorrelated random variables with unit variance. The unit variance constraint is analogous to the constraint that \(\|\|_{2}=1\) that we imposed when maximizing or minimizing over \(\) in eqs. (9) and (10). Indeed, scaling the variance of the \(_{i}\)'s simply scales the magnitude of \(^{*},^{*}\). Thus, setting the variance of each sample to one (or some other constant) is reasonable. The constraint that \([z_{i}z_{j}]=0\) for all \(i j\) may be relaxed, as we discuss in appendix C.

## 4 Relating the Shape of Neural Representations to Decoding

In corollary 1 we saw that the Euclidean distance and Frobenius inner product between \(_{X}\) and \(_{Y}\) enjoys a nice interpretation in terms of (average) decoder similarity. This does not provide a completely satisfactory answer of our original question: What does the _geometry_ of a neural representation imply about its function (i.e. decoder performance)? As discussed in section 1, comparing neural representations through linear kernel matrices, \(_{X}\) and \(_{Y}\), is motivated from geometric principles--namely, their invariance to orthogonal transformations. Indeed, for any orthogonal matrix \(\), one can verify that:

\[()^{-1}=^{}()^{-1}\] (22)

whenever \(()\) is parameterized as in eq. (4). Therefore, then the transformation \(\) leaves the whitened linear kernel matrix unchanged. That is, if \(^{}=\), we have:

\[_{X}=()^{-1}^{}= {X}()^{-1}^{}^{}=_{X^{ }}.\] (23)

The **Procrustes shape distance**, \((,)\), is another popular method which is invariant to orthogonal transformations [36; 9]. This approach connects to a broader and decades-old literature on _shape analysis_[10; 18] whose applications include anatomical comparisons across biological species , and analysis of planar curves such as handwriting data . If the two neural responses are mean-centered and have the same dimensionality, then the Procrustes distance is simply the minimal Euclidean distance obtained by rotating and reflecting one set of responses onto the other. That is, when \(^{M N_{X}}\) and \(^{M N_{Y}}\) and we have \(N=N_{X}=N_{Y}\), the Procrustes distance is:

\[(,)=_{(N)}\|-\|_{F}\] (24)

When \(N_{X} N_{Y}\), a straightforward generalization of the Procrustes distance is to zero pad column-wise so that the matrix dimensions match. Geometrically, this can be interpreted as embedding the lower-dimensional point cloud into the higher-dimensional space and optimizing \(\) within this space to align the point clouds.

The fact that Procrustes distance is defined in terms of an optimal alignment transformation is appealing both intuitively and because it suggests avenues to generalize the method, such as using permutations (instead of orthogonal transformations) to align representations . Additionally, some empirical results have highlighted cases where Procrustes performs "better" than linear CKA [9; 5]. Rigorously defining and benchmarking what it means to be "better" in this context remains an open discussion , but it is nonetheless of interest to understand what Procrustes distance and associated definitions of _shape_ imply about neural decoding.

In appendix A.1 we prove the following result, which states that the Procrustes distance between appropriately normalized representations constrains the average decoding distance, or equivalently, that the average decoding distance bounds the Procrustes distance from above and below.

**Proposition 3**.: _Defining normalization constants \(=_{X}\) and \(=_{Y}\), we have an upper and lower bound on the Procrustes distance:_

\[(+)--_{}\| ^{*}-^{*}\|_{2}^{2}} (},})^{2}_{}\|^{*}- {Y}^{*}\|_{2}^{2}}\] (25)

_where \(_{}=(_{X}-_{Y})\) is the participation ratio of the matrix difference \(_{X}-_{Y}\). Equivalently, we may rewrite eq. (25) to find_

\[_{}}(},})^{4}\|^{*}- ^{*}\|_{2}^{2}(},})^{2}-(},})^{4}}{_{}},\] (26)

_where we have defined normalized transformations \(}\) and \(}\) in terms of the transformation \(()\):_

\[}:=}( )^{-1/2}}:=}()^{-1/2}.\]

_Note that \(}}^{}=_{X}\) and \(}}^{}=_{Y}\)._

We derive these bounds by leveraging the equivalence between the Procrustes distance between normalized representations \((},})\) and a distance metric on positive semi-definite kernel matrices, the _Bures distance_.

We recall that the choice of function \(()\) in the decoding problem eq. (1) (which determines optimal readout weights \(^{*}\) and \(^{*}\)) will set the normalization transformation of the representations \(}\) and \(}\). When \(()\) is chosen such that \(_{X}=_{Y}=1\),2 we can plot these bounds with \(==1\) for various values of participation ratio \(_{}\) (fig. 2).

Both upper and lower bounds are saturated (appendix A.1), and reveal an allowed region of both Procrustes and average decoding distance as a function of the participation ratio \(_{}\). Under the \(==1\) normalization, the Procrustes distance may range between 0 and \(\), and the average decoding distance may take values between \(0\) and \(_{}}}\), which is reflected in fig. 2.

The bounds in eq. (25) and eq. (26) reveal an imperfect connection between the Procrustes distance and the average decoding distance, such as the GULP distance. We recall that every choice of readout weight normalization \(()\) leads to a measure of representational distance \(\|^{*}-^{*} \|_{2}^{2}\), and that there is an associated Procrustes distance on the normalized representations \(}\) and \(}\)

Figure 2: Bounds in eq. (25) plotted (solid lines) for varying participation ratio \(_{}\) with \(==1\). A) Allowed regions (between the solid curves of like color) of Procrustes distance and expected Euclidean distance between decoded signals for different values of participation ratio \(_{}\). Bâ€“D) Allowed regions for particular \(_{}\) intervals (black solid lines) populated with calculated distances between pairs of randomly sampled positive semi-definite (PSD) matrices of size \(50 50\), subsampled to those that have \(_{}\) in the particular interval (colored points). Different colors represent different random ensembles of positive semi-definite matrix pairs, which are described in appendix B.

\((},})\). These bounds tell us that the average decoding distance \(\|^{*}-^{*}\|_{2}^{2}\) and this Procrustes distance constrain each other in a very particular way. Namely, when the Procrustes distance between normalized representations is small, then the average decoding distance must also be small.3 However, the converse is only necessarily true when the participation ratio of the difference of the kernel matrices \(_{X}-_{Y}\) is also small (e.g. fig. 2B). Indeed, as the dimensionality of \(_{X}-_{Y}\) increases, we see that the Procrustes distance between normalized representations can be large, but the average decoding distance can be relatively small, with the effect becoming more exaggerated as \(_{}\) becomes large (fig. 2D). In appendix A.1 we show that for \(==1\), the minimum possible participation ratio \(_{}\) is 2. Therefore, fig. 2 also illustrates another interesting phenomenon, namely that the upper left-hand corner of these plots is impossible to populate. That is, in the case of large expected difference in decoded signals, one can never simultaneously measure a small Procrustes distance between the normalized representations. Thus we observe quantitatively, as may be intuitive, that the Procrustes distance offers a more strict notion of geometric dissimilarity than the average decoding distance.

## 5 Discussion

Understanding meaningful ways to measure similarities between neural representations is clearly a very complex problem. The literature demonstrates a proliferation of different techniques [20; 34], but an underdeveloped understanding of how these methods relate to each other. Less still is known about how representational similarity measures interact with functional similarity or the amount and type of information that is decodable from the representation. This paper presents a theoretical framework centered around the linear decoding of information from representations, which allows us to understand some existing popular methods for measuring representational similarity as average decoding similarity or average decoding distance with different choices of weight regularization. These connections relied on averaging the decoding similarity or decoding distance over a distribution of decoding targets \(\) with \([^{}]=\). In the future it could be interesting to explore modifying these assumptions. For instance, instead of maximizing, minimizing, or taking the expectation over decoding targets, are there potentially interesting sets of fixed decoding targets that make sense when comparing networks in particular contexts? Furthermore, we focused on linear regression as a decoding method; a potentially interesting line of future work could be to extend this framework to linear classifiers (e.g. support vector machines or multi-class logistic regression). Lastly, in this paper we considered quantifying representational similarity across a finite set of \(M\) stimulus conditions. However, these finite-dimensional approaches can be framed as approximations or estimators for a population version of the problem. For example, the framework in  for the Procrustes distance, or  for the GULP distance (the plugin estimator of which is a special case of our framework as we saw above). We outline a framing of this perspective in appendix C, but a rigorous exploration of the \(M\) regime and an analysis of the behavior of estimators for similarity scores in the limited sample regime could be an important topic of future work.