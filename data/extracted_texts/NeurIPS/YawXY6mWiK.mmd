# A Full-duplex Speech Dialogue Scheme Based On Large Language Model

Peng Wang Songshuo Lu Yaohua Tang

Sijie Yan Wei Xia Yuanjun Xiong

###### Abstract

We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate in tandem, allowing the system to simultaneously speak and listen to the user. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than 3 folds compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running a LLM with only 8 billion parameters, our system exhibits a \(8\%\) higher interruption precision rate than the best available commercial LLM for voice-based dialogue.

## 1 Introduction

In a conversation between two persons, one person is either a speaker or a listener at a time. The listener listens to the speaker's speech but is free to interrupt when necessary. The speaker speaks, but either concedes the speech or continues it when the listener tries to speak. We refer to this type of interaction as _full-duplex_ dialogue. Instead, most existing chat-enabled large language models (LLMs)  view dialogue as a round-based process, where each participant will produce a full sentence before the other party responds, resulting in a _half-duplex_ dialogue. The half duplex dialogue mode is sensible when building a text-based chatbot. However, due to the bloated response latency and difficulty properly interruping the other party, it becomes infeasible for a human-like conversation experience, and thus, full-duplex dialogue capability is desired.

Despite LLMs' success in dramatically improving response quality, full-duplex dialogue is non-trivial to realize when considering LLM alone. A common chat-finetuned LLM  does not know the current time nor allow input while generating responses. It also has no awareness of any perception or motor function, which humans conveniently possess and operate in parallel to allow them to respond to external stimuli in real-time. As shown in Fig. 1, we tackle this problem by imaging an ideal agent for full-duplex dialogue. It involves one LLM and twofunctional modules. The LLM is aware of the perception module, which processes speech input, and the motor function module, which converts textual outputs generated by the LLM to speech. The two functional modules operate continuously regardless of which party in the dialogue is speaking, allowing both parties to speak, respond, and decide to interrupt or concede in real-time.

To realize full-duplex dialogue with this agent model, we instruct the LLM to view the dialogue as operating a simple finite-state machine (FSM) with two states: 1) **SPEAK** and 2) **LISTEN**, which we refer to as the _neural FSM_. At each timestep, the LLM must either: 1) process an external input token, 2) output a textual token, or 3) output a special control token to signal one of four possible state transitions, as illustrated in Fig. 1. External stimuli, such as human speech collected by the perception module, can also be streamed to the LLM and appended to the LLM's generated token sequence to potentially prompt for state transition. Any non-control tokens are immediately delivered to the motor function module to be converted to speech. With this design, the operation of the neural FSM, which governs the dialogue, becomes a natural part of the LLM's auto-regressive generation process. This allows us to perform standard instruction tuning and prompting on any pretrained LLM to enable it for full-duplex dialogue. In this work, we present the first systematic implementation of LLM-based full-duplex dialogue framework and evaluate its effectiveness in the following aspects:

1. **Simultaneous two-way interaction**. A full duplex LLM dialogue system should allow users and the machine to converse concurrently, enabling them to interrupt each other, akin to natural human dialogue, rather than a round-based dialogue.
2. **Full autonomy**. The dialogue should be content. The LLM needs to make autonomous decisions to halt, interrupt, or ask questions at proper timing by emitting control tokens of the neural FSM based on the semantic context.
3. **Rapid Response**. By processing streaming inputs from the perception module, while the other party is speaking, the system should respond to user inquiries with minimal latency.

We validate the proposed system's effectiveness in the above aspects on a curated dataset of human-machine voice interaction and design quantitative metrics regarding the above properties. Compared with state-of-the-art half-duplex dialogue systems, our approach can reduce the average response latency in dialogue by \(3\) folds. Without affecting the knowledge and reasoning capability of the LLMs, it reaches a proper response rate of \(96.7\%\) to user interruptions and a machine proper interrupt precision of \(54.7\%\) that outperforms GPT-4o and GPT-3.5-turbo-0125 significantly.

Figure 1: **Left**. Overview of the agent design that enables LLM-based full-duplex dialogue models. The agent is equipped with one LLM, one perception module, and one motor function module. The latter operates continuously and simultaneously to collect input to the LLM and produce voice-based LLM outputs. **Right**. The LLM operates a two-state neural FSM with **SPEAK** and **LISTEN** states. At each timestep, the LLM either 1) receives an external input token, 2) generates a textual token for speech, or 3) produces a control token to signal state transition in the neural FSM. This simple workflow enables full-duplex dialogue without any external moderation module.

Related work

### Full-duplex dialogue systems

Developing a speech dialogue system (SDS) capable of sustained, empathetic interaction with humans represents a pinnacle aspiration in artificial intelligence (AI). Initial systems, largely text-based, sought to mimic human conversational behavior but were constrained by rigid syntactic structures, limited vocabularies, and reliance on manual rules (Weizenbaum, 1966; Colby, 1975; Wallace, 2009; Li and Mills, 2019). The advent of Hidden Markov Models (HMMs) expanded these systems' capabilities to handle more fluid speech patterns, yet achieving the nuance and fluidity of human interaction continued to elude them (Juang and Rabiner, 1991).

Recent advancements in dialogue data and deep learning technologies have significantly propelled the evolution of speech dialogue systems (SDS). These advancements have transformed SDS from simple, command-driven interfaces into complex, AI-driven conversational agents capable of providing more natural and fluid user experiences. Prominent intelligent assistants such as Amazon Alexa2, Siri3, and Google Assistant4, along with specialized applications like Google Duplex (Matias and Leviathan, 2018) and Xiaolce (Zhou et al., 2020), exemplify this transformation. These systems not only facilitate task-oriented dialogues but also enable long-term emotional engagement, demonstrating the shift towards creating more humane and relatable systems.

Another significant technological advancement in this context is the transition from half-duplex to full-duplex communication (Chen et al., 1994), which allows for the simultaneous transmission and reception of signals, closely mimicking natural conversational flows and significantly reducing latency. Early SDS operated primarily in a half-duplex mode, where the system and the user could not speak simultaneously, resulting in stilted interactions with noticeable pauses. Initial improvements were made through streaming automatic speech recognition (ASR) and incremental dialogue processing technologies, which enhanced responsiveness and fluidity to some extent (Nakano et al., 2003). However, the inability to handle simultaneous speech input and output continued to limit the naturalness of interactions. The transition to full-duplex communication marked a significant leap forward. Full-duplex systems enable concurrent speech input and output, thereby more closely approximating human conversational behavior. Early implementations faced substantial technical hurdles, including advanced echo cancellation and managing overlapping speech without degrading user experience (Jin et al., 2021; Lin et al., 2022). Constructing effective SDS necessitates a suite of sophisticated components, collectively termed "Conversational Engine Components," including streaming ASR (Yu et al., 2020; Li et al., 2021), text-to-speech (TTS) (Tionkin et al., 2011; Trilla and Alias, 2013), intent recognition, and dialogue management, among others (Lin et al., 2022). These components are crucial for creating responsive and engaging conversational agents but introduce significant engineering challenges, particularly in maintaining low latency and handling dynamic conversational shifts.

However, achieving low-latency full-duplex systems necessitates tight coordination among dialogue strategy components, such as user query prediction (Madden et al., 2003), intent recognition (Varol et al., 2010; A et al., 2014), dialogue management (Dai et al., 2021), and even many manually crafted rules, making engineering implementation non-trivial. Despite these efforts, current full-duplex SDS still exhibit considerable error rates and often lack strong contextual awareness in prolonged dialogues.

The advent of large language models (LLMs) has notably advanced generative AI. Models like ChatGPT 5 exhibit profound capabilities in semantic understanding and logical reasoning, offering a streamlined approach to integrating various conversational components into a unified framework, potentially simplifying the construction of SDS (Heck et al., 2023). Innovations such as AudioGPT and LLaSM have further expanded these capabilities by integrating audio processing, although limitations in speech generation and full-duplex functionality persist (Huang et al., 2024; Shu et al., 2023).GPT-4o6 appears to have achieved full-duplex dialogue with users, but as a multimodal large model, its training data and implementation details remain undisclosed, rendering replication efforts extremely challenging.

### LLM as evaluators

Recent studies propose directly using LLMs as reference-free NLG evaluators (Fu et al., 2023; Wang et al., 2023). The idea involves leveraging LLMs to assess candidate outputs by evaluating their generation probability in the absence of a reference target, presupposing that LLMs have been trained to assign greater probabilities to texts of superior quality and fluency. Fu et al. (2023) propose GPTScore, a new framework that evaluated texts with generative pre-training models like GPT-3 (Brown et al., 2020), demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. Wang et al. (2023) conduct a preliminary survey of using ChatGPT as a NLG evaluator, experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. Kocmi and Federmann (2023) propose to use GPT models for evaluating machine translation tasks, provides a first glimpse into the usefulness of large language models for quality assessment of translations. Dettmers et al. (2024) provide a detailed analysis of chatbot performance based on both human and GPT-4 (Eloundou et al., 2023; Achiam et al., 2023), with evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation.

## 3 Methods

Our proposed system comprises three modules: 1) perception, 2) full-duplex capable LLM, and 3) motor function. The first is the perception module, which picks up user speeches in a dialogue. In this work, we implement it with an auto speech recognition model (Gulati et al., 2020), streaming results on \(640ms\) chunks delivered immediately to the LLM and appended to its token sequence before predicting the next token. From the generated tokens of the LLM, we can observe the states and state transitions of the neural FSM when a control token is predicted. Any textual token generated is sent to the motor function module. We implement the motor function module with a text-to-speech model (Kim et al., 2021). We discuss the definition of neural FSM in Sec. 3.1, the overall architecture in Sec. 3.2, and training of the full-duplex LLM in Sec. 3.3.

### The Neural Finite State Machine

We consider the dialogue between two parties: the _user_ and the _machine_. At any time in a full duplex dialogue, one or both parties could be speaking or waiting for others to speak. To model this process for the LLM, we instruct it to operate a finite state machine (FSM) with two states: 1) **SPEAK** and 2) **LISTEN**, which represents the LLM's role in the dialogue as it perceives. This FSM is referred to as the neural FSM. It has 4 possible state transitions:

* \(\) indicates the model would like to keep speaking when the perception module picks up user speech, we designate a control token \([]\) for this transition called.

Figure 2: In LLM based full-duplex dialogue system, the LLM operates a two state FSM, governing state transitions in the dialogue.

* **SPEAK \(\)****LISTEN** indicates the model determines it is proper to concede its speech to the user, with a control token \([]\);
* **LISTEN \(\)****LISTEN** indicates the model determines the user has not finished the speech and would like to wait for more input, with a control token \([]\);
* **LISTEN \(\)****SPEAK** indicates the model determines it would like to start speaking because either the user has finished the speech or it is proper to interrupt the user's speech. Its control token is \([]\).

The LLM could produce one of the four control tokens at any time step to signal a state transition. The perception and motor function modules have to observe the state of the neural FSM and choose the proper action. In Fig. 2 we illustrate the states and transitions of the neural FSM.

### Design of the dialogue system

The perception and motor function module observes the state and transitions of the neural FSM and chooses the proper action to complete the dialogue system. Given the neural FSM's state, their action is also conditionally independent, allowing them to operate in parallel without synchronization.

**The perception module** operates an off-the-shelf ASR model. The models run on \(640\) millisecond timesteps. At each step, it outputs one token chunk prefixed by the tokens <_usr>_ with textual content if the user speaks. When the neural FSM's state is **SPEAK**, only chunks with textual content will be sent to the LLM and appended to the LLM's generated token cache. When the neural FSM is in **LISTEN** state, a contentless chunk will also be transmitted to the LLM to represent a moment of silence.

**The motor function module** operates an off-the-shelf TTS model. It receives textual tokens from the LLM in a streaming manner, converts them to voice, and reports back to the LLM module when each token is completely voiced. When the neural FSM is in **SPEAK** state, it produces voice as soon as it receives any text token. It does not produce any voice when the neural FSM is in the **LISTEN** state.

**The LLM module** serves the roles of the dialogue manager in traditional dialogue systems and the response generator by managing the fine-tuned LLM [Touvron et al., 2023] and the neural FSM. It can be considered to operate on a virtual one-directional "tape" of tokens that expands over time. The tape starts with the LLM's system prompt. Each increment of the tape, implemented as one auto-regressive decoding step in the LLM, is triggered by three types of events listed below in decreasing order of priority

* a control token of \([]\) or \([]\) is present as the last token on the tape;
* a new output chunk from the perception module is appended to the tape;
* the last token on tape has been processed by the motor function module.

In this way, the progression of the conversation is serialized in a causal order on the tape. The operation of the neural FSM, production of response to user inquiries, and interruption of user speech are all unified into the task of predicting the next token given the content on the tape, which is inherent to LLMs.

### Adapting LLMs for full duplex dialogue

As the LLM makes fully autonomous decisions about neural FSM's state transition and responds to user inputs, it must know its role and understand the dialogue's context. Also, external input must be allowed during response generation, which is not commonly supported in existing chat-finetune LLMs [Touvron et al., 2023]. We add these capabilities to an LLM by combining two approaches: 1) instruction tuning and 2) prompt engineering.

**Instruction tuning**[Ouyang et al., 2022] originally aims at finetuning a pretrained LLM to follow user's instructions safely and helpfully. In this work, we utilize this technique to align an already chat-finetune LLM to become able to operate with the rest of the modules in the dialogue system. To devise a training dataset emulating the working environment of the LLM in the system, we instruct GPT-47 to write a set of dialogue transcripts between two parties with cases of interruption, denial, affirmation, environment noises, and topic shifting and mark them up with the control tokens of the neural FSM at proper timing. \(1500\) series of transcripts are generated for the dataset with prompt in Appendix A. More details on dataset construction are discussed in Appendix B. Based on Llama-3-8B-Instruct8, we perform supervised fine-tuning  for 20 steps on this dataset. The fine-tuning is conducted on 8 NVIDIA A100 GPUs with a batch size of 256 sequences and a learning rate of 1e-5 with the AdamW optimizer.

**Prompt engineering**. Proper system prompts can provide detailed context to the LLM at the inference stage before taking user inputs. We carefully design a system prompt (shown in Appendix H) to condition the fine-tuned LLM to operate with full awareness of the dialogue system and a proper initialization for dialogue.

## 4 Experiments and evaluation

To automatically evaluate the performance of a dialogue system, we devise a simulator that emulates real-life human-like conversations, capable of simulating both full-duplex and half-duplex dialogues. Additionally, we construct a new benchmark dataset for automated evaluation. Specifically, we develop an assessment framework that evaluates the dialogue system's performance in terms of both response latency and conversation quality.

### Benchmark Dataset

Instead of assessing the precision of generating individual signals (\([]\), \([]\), \([]\), \([]\)), we opt to directly evaluate the timing and appropriateness of dialogue interruptions, as these aspects are functionally equivalent. Take the signal \([]\) as an example: it is generated either when the user finishes speaking or when the machine attempts to interject during the user's speech. Detection of the user's speech completion can be achieved through ASR VAD or by the LLM interrupting at the end of a sentence (Right at this moment, the LLM determines that the user has finished speaking). Given that ASR VAD falls outside the scope of this article, we can conclude that the precision of generating the \([]\) signal can be evaluated by the appropriateness of the machine's interruption of the user. Conversely, if the machine refrains from interrupting the user, it will continue to signal \([]\). When the user interrupts the machine, the machine's response varies based on whether it accepts the interruption or not. If the interruption is accepted, the machine signals \([]\); otherwise, it signals \([]\). Thus, to evaluate the LLM's proficiency in controlling FSM state transitions, we incorporate interruption scenarios into our dataset in addition to conventional, uninterrupted dialogue data.

For data involving machine interruptions, we collect 2,000 data entries through two approaches. To start, we filter about 1,000 sessions of multi-turn dialogues from the shareGPT9 dataset. Filtering rules include removing entries containing code, subject-specific content like mathematical, and excessively long texts that are unsuitable for voice-based testing scenarios. Following that, GPT-4 is used to generate another 1,000 single and multi-turn oral dialogues across various domains by the prompt in Appendix B. In both approaches, the last turn must be the user's utterance. This is a round based dialogue dataset with no interruptions. In the experiment, the last turn is presented to the LLM token by token in a streaming fashion to observe if the LLM initiates an interruption.

Regarding data with user interruptions, based on the four pattern categories of interruptions described in the Appendix B, we generate 180 entries of single and multi-turn dialogues for every category using GPT-4. In each entry, the last turn must be the the user's interruption. We named the combined benchmark dataset "duplex-dialogue-3k". Sample data is shown in Appendix C.

### Automated evaluation

The simulator emulates the conversational process between a user and a machine, where user and machine are simulated by two programs (**U** and **M**). Program **M** is powered by LLM. Each time we retrieve a session of conversations between a <usr> and a <sys>> from the "duplex-dialogue-3k", where the <usr> text serves as program U's speech. This speech is "played" to program \(\) in simulation. Upon hearing the question in either half-duplex or full-duplex mode, program \(\) generates a response using the LLM. The response is then "played back" to program \(\) in simulation. In such a simulator, we can precisely measure the time delay data. To test machine interruptions of users, during the program \(\)'s turn to speak the last turn, the program \(\) can interrupt at any time. Following each interruption, a data record is generated and stored for subsequent evaluation of the appropriateness of the interruption. To test user interruptions of machine, when it comes to the last round and after \(\) played its interruption, \(\) generates a response which is being recorded.

We formulate several assessment metrics to test dialogue performance. We use a metric _proper interruption rate_ (PIR) to measure the accuracy of the interruption timing, and the metric _proper response rate_ (PRR) to indicate the proportion of reasonable responses generated by the model after an interruption. For the overall system latency, metric _first token emission delay_ (FTED) is considered: the time between the end of the user's speech and the machine's first output.

For the four types of user interruptions, noise, denial, affirmation, and shifting the topic, their corresponding metrics are \(_{}\), \(_{}\), \(_{}\) and \(_{}\), respectively. For the experiment where the machine interrupts the user, we calculate the proportions of interruptions occurring mid-sentence (\(_{}\)), at sentence completion (\(_{}\)), and instances where no interruption takes place (MIR, missed interruption rate). In the case of no interruption and interruption occurring at sentence completion, full-duplex capable LLM's answer is the same as normal LLM. Therefore, we only calculate the PIR and PRR metrics for the case of interruption occurring mid-sentence, denoted as \(_{}\) and \(_{}\). Furthermore, we can compute the precision of interrupt as \(_{}*_{}+_{}\) and approximately calculate recall as \(1-\) (the count of correct interruption is constant).

### Experiment details

In our experiments, we use the Llama-3-8B-Instruct model as the basis standard LLM model, use Llama-3-8B-Instruct-fd as the duplex LLM. The system prompt used can be found at Appendix H. For non-streaming ASR models, we use the OpenAI open-source version of the Whisper (Radford et al., 2023) model as the base model while for the streaming ASR model, we use a open-source10 U2++ Conformer (Gulati et al., 2020; Wu et al., 2021) model. For non-streaming TTS models, we use VITS (Kim et al., 2021) as the base model and the streaming TTS model uses the XTTS-v21 model from COQUI-AI. All models are deployed on one single NVIDIA A100 GPU.

### Experiment results

The FTED results are shown in Table 1, where we have four configurations of experiments. Configuration 1 is the prevailing approach in speech dialogue system, half-duplex speech dialogue scheme, which involves non-streaming ASR combined with Standard LLM (like Llama-3-8B-Instruct) and non-streaming TTS. In our experiments, we use this setup as our baseline, which has a very high FTED of \(2.28\)s, due to the speech endpoint detection latency and cumulative pipeline delays.

Above the configuration 1, to assess the latency reduction achieved by the duplex LLM's ability of recognizing the end of a user's speech, we design experiment configuration 2, referred to as semi-streaming ASR, duplex LLM and non-streaming TTS. In this configuration, the semi-streaming ASR still performs non-streaming recognition, but intermediate results will continually be sent to the LLM to check if they trigger an interruption. If not, the non-streaming ASR determines stopping through VAD (voice activity detection). As shown in Table 1, this setup of experiment achieves a latency of only \(1.49\) seconds, experiences a \(35\%\) decrease in latency. Configuration 3, streaming ASR, duplex LLM and non-streaming TTS aims to measure the impact of streaming ASR and the duplex LLM on the FTED metric, which experiences a mere \(1.15\)s delay. Ultimately, our proposed full-duplex dialogue system is the configuration 4 and achieves an astonishingly low latency of just \(0.68\) second, reduces the latency by more than 3 fold. In configuration 3, we also record the average latency for interruptions occurring mid-sentence, at sentence completion, and when the model doesn't interrupt, with the respective values being \(0\)s, \(0.83\)s and \(1.48\)s.

The results of the rationality experiments are shown in Table 2. In the case of machine interrupt user, Llama-3-8B-Instruct-fd interrupts user input \(62.8\%\) of the time, with \(38.8\%\) occurring while the user is still speaking, significantly reducing response latency. Despite the high rate of interruptions happened at middle, GPT-4-turbo evaluations reveal that \(79.1\%\) of these interruptions are deemed reasonable, along with over \(91\%\) of the responses being judged as appropriate, both within acceptable limits. Compared to GPT-4o and GPT-3.5-turbo-0125, although they have higher proportions of reasonable interruptions and responses, they interrupt within sentences much less frequently, likely due to adopting a more cautious dialogue strategy, waiting for clear stopping cues before responding. Such a cautious strategy leads to increased average interaction latency since the LLM, if not interrupting, would have to wait for the ASR VAD to signal the end of the user's speech, resulting in significantly longer waiting times as shown before (\(1.48\)s compared to \(0.83\)s). On the other hand, Llama-3-8B-Instruct goes to the opposite extreme, interrupting incomplete user input in \(75\%\) of cases, but with a reasonableness rate below \(40\%\), indicating that it lacks the ability to interrupt users at the right moments. In terms of the composite metrics, Precision and Recall, our model significantly outperforms GPT-4o and GPT-3.5-turbo-0125.

In the case of user interrupt machine, Llama-3-8B-Instruct-fd and GPT-4o show a relatively small difference in average rationality, both outperforming GPT-3.5-turbo-0125 and Llama-3-8B-Instruct. However, they have mixed results in handling third-party noise interference and affirming interjections, likely due to the challenging nature of these data categories. Overall, our model consistently achieves rationality percentages above \(90\%\) across all interruption types, indicating a more balanced performance.

We also conduct a regression experiment on the model fine-tuning to examine whether training on full-duplex data affects model's original capabilities negatively. As shown in Table 3, we evaluate the model's capabilities before and after fine-tuning using the OpenCompass12 benchmark suite across several tasks. It can be seen that there is minimal impact on the model's performance. We have not conducted a detailed discussion on the specific effects of training the model with full-duplex data

    &  \\   & Avg\(\) & 50\% & 90\% \\  Configuration 1: asr-ns+llm+tts-ns & 2.28s & 2.19s & 2.75s \\ Configuration 2: asr-sm+llm-fd+tts-ns & 1.49s & 1.60s & 1.96s \\ Configuration 3: asr-s+llm-fd+tts-ns & 1.15s & 0.78s & 2.08s \\  Configuration 4: asr-s+llm-fd+tts-s & **0.68s** & **0.41s** & 1.60s \\   

Table 1: FTED, -s for streaming, -ns for non streaming, -sm for semi streaming, -fd for full-duplex

    &  \\   & MIR & ir\({}_{}\) & ir\({}_{}\) & PIR\({}_{}\) & PRR\({}_{}\) & Precision\(\) Recall\(\) \\  GPT-4o & 52.8 & 29.7 & 17.5 & 95.6 & 98.9 & 46.6 & 47.2 \\ GPT-3.5-turbo-0125 & 74.3 & 14.4 & 11.3 & 91.1 & 95.8 & 24.7 & 25.7 \\ Llama-3-8B-Instruct & 14.6 & 10.3 & 75.1 & 36.1 & 79.3 & 37.4 & **85.4** \\ Llama-3-8B-Instruct-fd 37.2 & 24.0 & 38.8 & 79.1 & 91.3 & **54.7** & 62.8 \\    &  \\   & PRR\({}_{}\) & PRR\({}_{}\) & PRR\({}_{}\) & PRR\({}_{}\) & PRR\({}_{}\)\(\) \\  GPT-4o & 84.7 & 100.0 & 100.0 & 100.0 & 96.1 \\ GPT-3.5-turbo-0125 & 88.9 & 96.3 & 65.9 & 85.8 & 84.4 \\ Llama-3-8B-Instruct & 85.2 & 100.0 & 89.2 & 100.0 & 93.6 \\ Llama-3-8B-Instruct-fd & 95.2 & 100.0 & 91.9 & 99.5 & **96.7** \\   

Table 2: User-Machine interrupt reasonability results.

on various aspects of the generated responses, such as usefulness, relevance, and safety; this area necessitates further experimental validation. Sample records generated by the simulator for each kind of interruptions can be found at Appendix C.

## 5 Discussion & limitations

While neural FSM and full-duplex capable LLM have demonstrated strong capabilities in full-duplex dialogues, unlike large multimodal LLMs like GPT-4o that can directly take in and output audio tokens, the current systems still rely heavily on the seamless cooperation of ASR and TTS. The transmission of data between these three components introduces additional latency. However, it's worth noting that our approach shares similarities with multimodal models, which also necessitate control over input and output along the timeline for optimal dialogue experiences. Similarly, it requires the model to possess the capability of interruption.

## 6 Conclusion and future work

We present an LLM-based full-duplex dialogue system that can respond with low latency, autonomously decide to start and halt its speech based on real-time user inputs, and interrupt user speech at the proper timing. The simple models of neural FSM and serialized real-time conversation allow us to unify all tasks we expect the LLM to perform into a single task of next token prediction. Moving forward, with the advent of multimodal LLMs, we expect to further simplify the perception and motor function modules to the extent that they just need to preprocess the audio signals and play generated voice data while integrating speech-to-text and text-to-speech into the LLM itself, which could potentially lead to more natural and diverse interaction between users and dialogue systems.