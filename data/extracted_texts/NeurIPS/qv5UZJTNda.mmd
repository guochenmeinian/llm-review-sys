# Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice

Aiwen Xu

Department of Computer Science

University of California, Santa Barbara

Santa Barbara, CA 93117

aiwenxu@ucsb.edu

&Yuchen Hou

Department of Computer Science

University of California, Santa Barbara

Santa Barbara, CA 93117

yuchenhou@ucsb.edu

&Cristopher M. Niell

Department of Biology, Institute of Neuroscience

University of Oregon

Eugene, OR 97403

cniell@uoregon.edu

&Michael Beyeler

Department of Computer Science

Department of Psychological & Brain Sciences

University of California, Santa Barbara

Santa Barbara, CA 93117

mbeyeler@ucsb.edu

###### Abstract

Despite their immense success as a model of macaque visual cortex, deep convolutional neural networks (CNNs) have struggled to predict activity in visual cortex of the mouse, which is thought to be strongly dependent on the animal's behavioral state. Furthermore, most computational models focus on predicting neural responses to static images presented under head fixation, which are dramatically different from the dynamic, continuous visual stimuli that arise during movement in the real world. Consequently, it is still unknown how natural visual input and different behavioral variables may integrate over time to generate responses in primary visual cortex (V1). To address this, we introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity in freely moving mice. We show that the model achieves state-of-the-art predictions of V1 activity during free exploration and demonstrate the importance of each component in an extensive ablation study. Analyzing our model using maximally activating stimuli and saliency maps, we reveal new insights into cortical function, including the prevalence of mixed selectivity for behavioral variables in mouse V1. In summary, our model offers a comprehensive deep-learning framework for exploring the computational principles underlying V1 neurons in freely-moving animals engaged in natural behavior.

## 1 Introduction

Computational models have been crucial in providing insight into the underlying mechanisms by which neurons in the visual cortex respond to external stimuli. Deep convolutional neural networks (CNNs) have had immense success as predictive models of the primate ventral stream, in cases wherethe animal was passively viewing stimuli or simply maintaining fixation [1; 2; 3; 4; 5]. Despite their success, these CNNs are poor predictors of neural responses in mouse visual cortex , which is thought to be shallower and more parallel than that of primates [7; 8]. According to the best models in the literature [9; 10; 11; 12; 13; 14], the mouse visual system is more broadly tuned and operates on relatively low-resolution inputs to support a variety of behaviors . However, these models were limited to predicting neural responses to controlled (and potentially ethologically irrelevant) stimuli that were passively viewed by head-fixed animals.

Movement is a critical element of natural behavior. In the visual system, eye and head movements during locomotion and orienting transform the visual scene in potentially both beneficial (e.g., by providing additional visual cues) and detrimental ways (e.g., by introducing confounds due to self-movement) [16; 17; 18]. Movement-related activity is widespread in mouse cortex [19; 20] and prevalent in primary visual cortex (V1) [21; 22]. For instance, V1 neurons of freely moving mice show robust responses to head and eye position [23; 24], which may contribute a multiplicative gain to the visual response  that cannot be replicated under head fixation. V1 activity may be further modulated by variables that depend on the state of the animal and its behavioral goals [20; 22; 26; 27]. However, how these behavioral variables may integrate to modulate visual responses in V1 is unknown. Furthermore, a comprehensive predictive model of V1 activity in freely moving animals is still lacking.

To address these challenges, we make the following contributions:

* We introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity during natural vision in freely moving mice.
* We show that the model achieves state-of-the-art predictions of V1 activity during free exploration based on visual input and behavior, demonstrating the ability to accurately model neural responses in the dynamic regime of movement through the visual scene.
* We uncover new insights into cortical neural coding by analyzing our model with maximally activating stimuli and saliency maps, and demonstrate that mixed selectivity of visual and behavioral variables is prevalent in mouse V1.

## 2 Related Work

The mouse, as a model organism, offers unparalleled experimental access to the mammalian cerebral cortex . Computational models of mouse V1, including generalized linear models (GLMs) [25; 29] and customized models mimicking the mouse visual hierarchy , have been crucial in providing deeper insights into the range of computations performed by visual cortex. More recently, deep CNNs have also been used to model mouse V1 [9; 10; 11; 12; 13; 31; 32; 33; 34].

Despite their success in predicting neural activity in the macaque visual cortex , deep CNNs trained on ImageNet have had limited success in predicting mouse visual cortical activity . This is perhaps not surprising, as most ImageNet stimuli belong to static images of human-relevant semantic categories and may thus be of low ethological relevance for rodents. More importantly, these deep CNNs may not be the ideal architecture to model mouse visual cortex, which is known to be shallower and more parallel than primate visual cortex [36; 37]. In addition, mice are known to have lower visual acuity than that of primates [7; 8], and much of their visual processing may be devoted to active, movement-based behavior rather than passive analysis of the visual scene [38; 22; 39]. Although the majority of V1 neurons is believed to encode low-level visual features , their activity is often strongly modulated by behavioral variables related to eye and head position [23; 24; 25], locomotion [21; 22; 18], arousal [27; 41], and the recent history of the animal . Furthermore, mouse V1 is highly interconnected with both cortical and subcortical brain areas, which contrasts with feedforward, hierarchical models of visual processing .

A common architectural approach that has proved quite successful is to split the network into different components (first introduced by ):

* a "core" network, which typically consist of a CNN used to extract convolutional features from the visual stimulus [11; 12; 25; 42], sometimes in combination with a recurrent network ;
* a "shifter" network, which mimics gaze shifts by learning a (typically affine) transformation from head- to eye-centered coordinates, either applied to the pixel input [11; 25] or a CNN layer ;
* a "readout" network, which learns a mapping from artificial to biological neurons [11; 12; 42].

Owing to the difficulty of developing a predictive model of mouse cortex, Willeke _et al._ recently invited submissions to the Sensorium competition held at NeurIPS '22. The competition introduced a benchmark dataset of V1 neural activity recorded from head-fixed mice on a treadmill viewing static images, with simultaneous measurements of running speed, pupil size, and eye position. A baseline model was provided as well, which consisted of a 4-layer CNN core in combination with a shifter and readout network . Even though 26 teams submitted 194 different models, the overall improvement to the baseline performance was modest, raising the single trial correlation from \(.287\) to \(.325\) in the Sensorium and from \(.384\) to \(.453\) in the Sensorium+ competition. Architectural innovations (e.g., Transformers, Normalizing Flows, YOLO, and knowledge distillation), were unable to make an impact, as most improvements were gained from ensemble methods. A promising direction was taken by the winning model, which attempted to learn a latent representation of the "brain state" from the various behavioral variables, inspired by . However, the model utilized the timestamps of the test set to estimate recent neuronal activities, which the other competitors did not have access to.

Taken together, we identified three main limitations of previous work that this study aims to address:

* **Head-fixed preparations.** Most previous models operated on data from animals in head-fixed conditions with static stimuli, which do not mirror natural behavior and thus provide limited insight into visual processing in real-world environments. In contrast, the present work is applied to state-of-the-art neurophysiological recordings of V1 activity in freely moving mice. This represents a dramatic shift in the "parameter space" of visual input, from static images to dynamic, real-world visual input. One could imagine that this will make the modeling process more difficult, because the stimulus set is more complex, or easier, because it is more matched to the computational challenge the brain evolved for.
* **Limited influence of behavioral state.** Previous models often limited the influence of behavioral state to eye measurements and treadmill running speed, which were either concatenated with the visual features [14; 41], utilized in the shifter network to determine the gaze-contingent retinal input [11; 14], or used to predict a multiplicative gain factor .
* **Missing temporal dynamics.** Most previous modeling works ignored the temporal factors that might influence V1 activity and overlooked the dynamic nature of visual processing (but see ). We overcome this limitation by utilizing approximately 1-hour-long recordings of three mice freely exploring an arena, and our model is capable of handling continuous data streams of any length.

## 3 Methods

Head-mounted recording systemWe had access to data from three adult mice who were freely exploring \(48\,\) long \(\)\(37\,\) wide \(\)\(30\,\) high arena (Fig. 1A), collected with a state-of-the-art recording system  that combined high-density silicon probes with miniature head-mounted cameras (Fig. 1B). One camera was aimed outwards to capture the visual scene from the mouse's perspective ("worldcam") at \(16\,\) per frame (downsampled to \(60 80\) pixels). A second camera, aimed at the eye, was used to extract eye position (\(\), \(\)) and pupil radius (\(\)) at \(30\,\) using DeepLabCut . Pitch (\(\)) and roll (\(\)) of the mouse's head position were extracted at \(30\,\) from the inertial measurement unit (IMU). \(\), \(\), \(\), and \(\) allowed for the worldcam video to be corrected for eye movements: A 3-layer fully-connected shifter network (where each linear layer was accompanied by Tanh and BatchNorm) was trained to predict a rotation (bounded by \( 36^{}\)) and a shift (bounded by \( 16\) pixels horizontally and \( 12\) pixels vertically) based on \(\), \(\), \(\), and \(\) to convert each frame to head- to eye-centered coordinates. Locomotion speed (\(s\)) was estimated from the top-down camera feed using DeepLabCut . Electrophysiology data was acquired at \(30\,\) using a \(11\, 15\,\) multi-shank linear silicon probe (128 channels) implanted in the center of the left monocular V1, then bandpass-filtered between \(0.01\,\) and \(7.5\,\), and spike-sorted with Kilosort 2.5 . Single units were selected using Phy2 (see ) and inactive units (mean firing rate \(<3\,\)) were removed. This yielded 68, 32, and 49 active units for Mouse 1-3, respectively. To prepare the data for machine learning, all data streams were deinterlaced and resampled at \(20.83\,\) (\(48\,\) per frame; Fig. 1C). For a more detailed description of the dataset, see Appendix A and Ref. .

Model architectureWe used a 3-layer CNN (kernel size 7, \(128 64 32\) channels) to encode the visual stimulus. Each convolutional layer was followed by a BatchNorm layer, a ReLU, and a Dropout layer (0.5 rate). A fully-connected layer transformed the learned visual features into a visual feature vector, \(\) (Fig. 2, _top-right_). In a purely visual version of the model, \(\) was fed into a fully-connected layer, followed by a softplus layer, to yield a neuronal response prediction.

To encode behavioral state, we constructed an input vector from different sets of behavioral variables:

* \(\): all behavioral variables used in the Sensorium+ competition , consisting of running speed (\(s\)), pupil size (\(\)), and its temporal derivative (\(\));
* \(\): all behavioral variables used in , consisting of eye position (\(\), \(\)), head position (\(\), \(\)), pupil size (\(\)), and running speed (\(s\));
* \(\): the first-order derivatives of the variables in \(\), namely \(\), \(\), \(\), \(\), \(\), and \(s\).

To test for interactions between behavioral variables, these sets could also include the pairwise multiplication of their elements; e.g., \(_{}=\{b_{i}b_{j}\ \ (b_{i},b_{j})\}\). The input vector was then passed through a batch normalization layer and a fully connected layer (subjected to a strong L1 norm for feature selection) to produce a behavioral vector, \(\).

We then concatenated the vectors \(\), \(\), and their element-wise product \(\) (all calculated for each individual input frame), fed them through a batch normalization layer, and input them to a 1-layer gated recurrent unit (GRU) (hidden size of 512). To incorporate temporal dynamics, we constructed different versions (GRU\({}_{k}\)) of the model that had access to \(k\) previous frames. A fully-connected layer and a softplus activation function were applied to yield the neuronal response prediction.

Training and model evaluationSince the visual input depended on the movement of the mouse and the mouse could be in very different behavioral states over the length of the recording, the data was highly inhomogeneous across time. To deal with the continuous and dynamic nature of the data, we therefore split the \( 1\,\)-long recording into 10 consecutive segments. The first \(70\,\) of each segment were then reserved for training (including an 80-20 validation split) and the remaining \(30\,\) for testing.

Note that it is unlikely for data to "leak" from the train segment into the test segment. While it is possible that the mouse could have been exploring the same part of the arena at different segments of the recording, it was free to move its head, eyes, and body as it saw fit. Thus two duplicate data points could only be produced by the animal exactly duplicating the time courses of its eye, head, and body movement in the exact same location of the arena.

Figure 1: Schematic of the head-mounted recording system for freely moving mice (adapted from ). A) Three mice freely explored a \(48\,\) long \(\)\(37\,\) wide \(\)\(30\,\) high arena. B) Preparation included a silicon probe for electrophysiological recording in V1 (yellow), miniature cameras for recording the mouse’s eye position and pupil size (\(\), \(\), and \(\); magenta), and visual scene (blue), and inertial measurement unit for measuring head orientation (\(\) and \(\); green). C) Sample data from a \(9.6\,\) period during free movement showing (from top) visual scene, horizontal and vertical eye position, pupil size, head pitch and roll, locomotor speed, and a raster plot of 64 units.

Models were separately trained on the data from each mouse. Model parameters were optimized with Adam (batch size: 256, CNN learning rate: \(.0001\), full model: \(.0002\)) to minimize the Poisson loss between predicted neuronal response (\(\)) and ground truth (\(r\)) : \(_{i=1}^{N}(_{i}-r_{i}_{i})\), where \(N\) denotes the number of recorded neurons for each mouse. We used early stopping on the validation set (patience: 5 epochs), which led all models to converge in less than 50 epochs. Due to the large number of hyper-parameters, the specific network and training settings were determined using a combination of grid search and manual exploration on a validation set (see Appendix B).

To evaluate model performance, we calculated the cross-correlation (\(cc\)) between a smoothed version (\(2\,\) boxcar filter) of the predicted and ground-truth response for each recorded neuron .

All models were implemented in PyTorch and trained on an NVIDIA RTX 3090 with 24GB memory. All code, data used to train the models, and weights of the trained model can be found at https://github.com/bionicvisionlab/2023-Xu-Multimodal-Mouse-V1.

Maximally activating stimuliWe used gradient ascent  to discover the visual stimuli that most strongly activate a particular model neuron in our network. The visual input was initialized with noise sampled in \((.5,2)\). The behavioral variables were initialized to a vector of all ones, and updated in the loop with the visual stimuli. We used the Adam optimizer to repeatedly add the gradient of the target neuron's activity with respect to its inputs. We also applied L2 regularization (weight of.02) and Laplacian regularization (weight of 0.01)  on the image. This procedure was repeated 6400 times. The resulting, maximally activating visual stimuli were smoothed with a Butterworth filter (low-pass,.05 cutoff frequency ratio) to reduce the impact of high-frequency noise.

Saliency mapWe computed a saliency map  of the behavioral vector for each neuron to discover which behavioral variables contributed most strongly to each model neuron's activity. We iterated through the test dataset, recorded the gradient of each behavioral input with respect to each neuron's prediction, and then averaged the gradients per neuron to obtain the saliency map.

## 4 Results

Mouse V1 activity is best predicted with a 3-layer CNNTo determine the purely visual contribution to V1 responses, we experimented with a large number of vision architectures (see Appendix B). In the end, a vanilla 3-layer CNN (kernel size 7, \(128 64 32\) channels) yielded

Figure 2: Model architecture diagram. The vision-only network (top-right) was a CNN network, predicting the neural activity at time \(t\) given the visual input at time \(t-1\) (\(48\,\) bins). The full model combined the CNN with a behavioral encoder and a gated recurrent unit (GRU), predicting the neural activity at time \(t\) given the visual and behavioral inputs from time \(t-1\) to \(t-n\).

the best cross-correlation between predicted and ground-truth responses (Table 1), outperforming the best autoencoder architecture (kernel size: 7, encoder: \(64 128 256\) channels, decoder: \(256 128 64\) channels), ResNet-18  (a 20-layer CNN with the first input channel being replaced by 1), EfficientNet-B0  (a 65-layer CNN with the first input channel being replaced by 1), and the Sensorium baseline  (a 4-layer CNN with a readout network). The greatest improvement in cross-correlation was achieved for Mouse 1.

Behavioral variables improve most neuronal predictionsOnce we identified the 3-layer CNN as the best visual encoder, we added the different sets of behavioral variables to the network. To allow for a fair comparison with the Sensorium+ baseline , we first limited ourselves to \(=\{,,s\}\), but then gradually added more behavioral variables (\(\))  as well the derivatives of these variables (\(\)) and multiplicative pairs (\(_{}\) and \(\{\}_{}\)).

The results are shown in Table 2. All models were able to outperform the Sensorium+ baseline, and the addition of behavioral variables and their interactions further improved model performance. Note that although the full model used a GRU to combine visual and behavioral features, the input sequence length was always 1 (i.e., \(_{1}\)). That being said, it is possible that the GRU learned long-term correlations that the Sensorium+ baseline model did not have access to. Nevertheless, the biggest performance improvements were gained through the addition of behavioral variables related to head and eye position (which are present in \(\) but not in \(\)), their derivatives (\(\)), and multiplicative interactions between these variables (\(\{\}_{}\)).

We also wondered whether the prediction of only some V1 neurons would benefit from the addition of these behavioral variables. To our surprise, the cross-correlation between predicted and ground-truth responses improved for almost all recorded V1 neurons (Fig. 3).

  & &  &  &  \\ Model & & \(cc\) & MSE \(\) & \(cc\) & MSE \(\) & \(cc\) & MSE \(\) & \(cc\) & MSE \(\) \\  CNN & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Autoencoder & \(\) & \(.555.140\) & \(.0710\) & \(\) & \(.370.145\) & \(.112\) & \(\) & \(.521.144\) & \(.0974\) \\ ResNet-18  & \(\) & \(.517.159\) & \(.0782\) & \(.366.171\) & \(.107\) & \(\) & \(.511.138\) & \(.0944\) \\ EfficientNet-B0  & \(\) & \(.542.153\) & \(.0694\) & \(.393.165\) & \(.103\) & \(\) & \(.510.127\) & \(.0965\) \\ Sensorium  & \(\) & \(.519.149\) & \(.0754\) & \(.381.128\) & \(.119\) & \(\) & \(.497.136\) & \(.100\) \\ 

Table 1: Best-performing vision models, compared to the Sensorium baseline  (see Appendix B for more). Best-performing networks are indicated in bold. \(cc\): cross-correlation, mean \(\) standard deviation across neurons (\(\): the higher the better), MSE: mean-squared error (\(\): the lower the better).

Access to longer series of data in time further improves predictive performanceAfter we identified the full behavioral feature set (\(\{\}_{}\)) as the one yielding the best model performance, we extended the GRU's temporal dependence by allowing the input to vary from one frame (\(48\,\)) to a total of eight frames (\(384\,\)), and assessed the model's performance.

The results are shown in Table 3. The amount of temporal information needed by the model to reach peak predictive performance was similar across mice (\(288\,\), \(192\,\), and \(192\,\) in terms of cross-correlation, \(192\,\), \(192\,\), and \(192\,\) in terms of mean-squared error, respectively). This indicates that temporal information is important for predicting dynamic neural activity. However, the dependence on temporal information has a limit, and different neurons in V1 might possess different temporal capacities.

Well-defined visual receptive fields emergeTo assess whether the CNN+GRU\({}_{1}\) model learned meaningful visual receptive fields, we used gradient ascent (see Methods) to find the maximally activating stimulus for each neuron. Receptive fields for the 32 best-predicted neurons are shown in Fig. 4. Interestingly, most of them had well-defined excitatory and inhibitory subregions, often resembling receptive fields of orientation-selective neurons. Most excitatory and inhibitory subregions spanned approximately \(30^{}\) of visual angle (the full width of the frame, 80 pixels, roughly corresponding to \(120^{}\) of visual angle), which is roughly on the same order of magnitude compared to receptive field sizes typically observed in mouse V1, varying from \(10^{}\) to \(30^{}\).

Receptive fields were noticeably different across mice. Whereas Mouse 1 and 3 had visual receptive fields with strongly excitatory subregions, most model neurons for Mouse 2 appeared to be inhibited

Figure 3: The integration of behavioral variables improved the cross-correlation (\(cc\)) for the majority of neurons. Each dot represents a neuron. A dot above the dashed diagonal indicates a higher \(cc\) with the inclusion of behavioral variables. Histograms (small insets) illustrate the distribution of the improvement in \(cc\) across the neuronal population.

  &  &  &  \\ Model & History & \(cc\) & MSE \(\) & \(cc\) & MSE \(\) & \(cc\) & MSE \(\) \\  CNN+GRU\({}_{1}\) & \(48\,\) & \(.646.136\) & \(.0543\) & \(.508.166\) & \(.0917\) & \(.607.132\) & \(.0801\) \\ CNN+GRU\({}_{2}\) & \(96\,\) & \(.649.139\) & \(.0528\) & \(.506.174\) & \(.0898\) & \(.607.133\) & \(.0811\) \\ CNN+GRU\({}_{3}\) & \(144\,\) & \(.653.139\) & \(.0528\) & \(.528.160\) & \(.0843\) & \(.604.134\) & \(.0790\) \\ CNN+GRU\({}_{4}\) & \(192\,\) & \(.650.142\) & \(.0525\) & \(.566.169\) & \(.0799\) & \(.614.136\) & \(.0773\) \\ CNN+GRU\({}_{5}\) & \(240\,\) & \(.645.144\) & \(.0556\) & \(.519.177\) & \(.0933\) & \(.598.134\) & \(.0807\) \\ CNN+GRU\({}_{6}\) & \(288\,\) & \(.654.142\) & \(.0526\) & \(.549.175\) & \(.0823\) & \(.598.138\) & \(.0798\) \\ CNN+GRU\({}_{7}\) & \(336\,\) & \(.644.148\) & \(.0534\) & \(.533.169\) & \(.0931\) & \(.596.133\) & \(.0806\) \\ CNN+GRU\({}_{8}\) & \(384\,\) & \(.646.146\) & \(.0547\) & \(.546.179\) & \(.0840\) & \(.594.141\) & \(.0825\) \\ 

Table 3: CNN+GRU\({}_{k}\) model trained with input from \(k\) timesteps on the full feature set (\(\{\}_{}\)). Best performing networks are indicated in bold. \(cc\): cross-correlation, mean \(\) standard deviation (\(\): the higher the better), MSE: mean-squared error (\(\): the lower the better).

by visual signals (same colorbar across panels). In addition, several model neurons lacked pronounced visual receptive fields, indicating that they were more strongly driven by behavioral variables. Even though model fits were repeated with different initial values for the behavioral variables, the resulting visual receptive fields looked qualitatively the same (see Appendix C), thus demonstrating the validity of the generated receptive fields. In addition, even some of the best-predicted neurons lack a pronounced or spatially structured receptive field, implying that these neurons could be primarily driven by behavioral variables.

Analysis of behavioral saliency maps reveals different types of neuronsIntrigued by the fact that some neurons lacked pronounced visual receptive fields, we aimed to analyze the influence of behavioral state on the predicted neuronal response by performing a saliency map analysis on the behavioral inputs (see Methods). Since different behavioral variables operate on different input ranges, we first standardized the saliency map activities for each behavioral variable across the model neuron population. Saliency map activities further than 1 standard deviation from the mean were then interpreted as "driving" the neuron, allowing us to categorize each neuron as being driven by one or multiple behavioral variables (Fig. 5).

We first asked which neurons in our model were driven by which behavioral variables (Fig. 5, _top_). Consistent with , we found a large fraction of model neurons driven by eye and head position, and smaller fractions driven by locomotion speed and pupil size. Approximately 20-30% of neurons were not driven by any of these behavioral variables, rendering their responses purely visual.

However, a particular neuron could be driven by multiple behavioral variables. Repeating the above analysis, we found that most model neurons showed mixed selectivity (i.e., responding to different categories of information, such as visual and motor signals, or stimulus and reward signals), with only a minority of cells responding exclusively to a single behavioral variable, (Fig. 5, _middle_). Adding the interaction terms between behavioral variables (Fig. 5, _bottom_) did not change the fact that most model V1 neurons encoded combinations of multiple behavioral variables, often relating information about the animal's eye position to head position and locomotor speed.

## 5 Discussion

In this paper, we propose a deep recurrent neural network that achieves state-of-the-art predictions of V1 activity in freely moving mice. We discovered that our model outperforms previous models under these more naturalistic conditions, which could be attributed to the better alignment of this data with the computations performed by the mouse visual system, based on its natural visual environment and

Figure 4: The maximally activating stimuli learned in CNN+GRU\({}_{1}\), generated via gradient ascent. The 32 neurons with the highest cross-correlation (\(cc\)) from each mouse are shown, sorted by \(cc\).

behavioral characteristics. Similar to previous models, we found that a simple CNN architecture is sufficient to predict the visual response properties of cells in mouse V1.

In addition, mouse V1 is known to be strongly modulated by signals related to the movement of the animal's eyes, head, and body , which are severely restricted in head-fixed preparations. Models trained on head-fixed preparations may thus be limited in their predictive power. In contrast, our model was able to predict V1 activity on a 1-hour continuous data stream, during which the animal freely explored a real-world arena. Our analyses demonstrate the impact of the animal's behavioral state on V1 activity and reveal that most model V1 neurons exhibit mixed selectivity to multiple behavioral variables.

Accurate predictions of mouse V1 activity under natural conditionsOur brains did not evolve to view stationary stimuli on a computer screen. However, most research on neural coding in vision has been conducted under head-fixed conditions, which do not mirror natural behavior and thus provide limited insight into visual processing in real-world environments. Some visual functions mediated by the ventral stream, such as identifying faces and objects, resemble this condition, but the real visual environment is constantly shifting due to self-motion, leading to dynamic activities such as navigation or object reaching, typically mediated by the dorsal stream. To truly understand visual perception in natural environments, we need to capture the computational principles when the subjects are in motion.

In this research, we take the initial steps towards this by modeling a novel data type encompassing neural activity coupled with a visual scene captured from a freely moving animal's perspective. This represents a dramatic (but, in our opinion, crucial) shift in the "parameter space" of visual input, from static images projected on a screen to dynamic, real-world visual input.

Figure 5: Effect of behavioral variables on model neuron activity, inferred by the saliency analysis. A) Fraction of neurons that are “driven by” (i.e., their saliency map activation is further than 1 standard deviation from the mean) different behavioral variables (similar to ). A neuron that responds to (e.g.) both position and speed may be counted twice. Neurons without a strong behavioral drive are categorized as “vision only”. B) Fraction of neurons that are uniquely driven by a specific behavioral variable. Still, a large fraction of neurons are driven by multiple behavioral variables. C) Same as B), but split with interaction terms.

Surprisingly, visual responses were best predicted with a standard three-layer ("vanilla") CNN (Table 1), as compared to a multitude of more sophisticated models that included autoencoders, variational autoencoders, filter bank models, and pre-trained ResNet and EfficientNet architectures. One possible explanation might be that the neurons in our dataset were selective for other behavioral inputs that we did not have access to, and that the vanilla CNN architecture imposed the fewest assumptions about how visual input contributed to the neural activity. In addition, visual receptive fields for Mouse 2 were noticeably different from the other two mice, exhibiting pronounced inhibitory subregions (Fig. 4, _center_). This is consistent with the fact that the cortical probes of Mouse 2 were more superficial compared to the other two mice , so the recorded neurons may have both different anatomical inputs and different visual responses.

Mixed selectivity of behavioral variablesOur experiments demonstrated that the models incorporating behavioral variables and their interactions performed substantially better than the models relying exclusively on visual inputs. Moreover, our saliency map analysis showed that only around 25% of model neurons could be considered purely visual, with the majority of model neurons driven by multiple behavioral variables.

This widespread mixed selectivity is consistent with previous literature suggesting that V1 neurons may be modulated by a high-dimensional latent representation of several behavioral variables related to the animal's movement, recent experiences, and behavioral goals . It is also consistent with the idea of a basis function representation , which allows a population of neurons to conjunctively represent multiple behaviorally relevant variables. Such representations are often employed by higher-order visual areas in primate cortex to implement sensorimotor transformations . It is intriguing to find computational evidence for such a representation as early as V1 in the mouse. Future computational studies should therefore aim to study the mechanisms by which V1 neurons might construct a nonlinear combination of behavioral signals.

Limitations and future work.While our study opens a new perspective on modeling neural activity during natural conditions, there are a few limitations that need to be acknowledged. First, our data was relatively limited (around 50 neurons per animal, for 3 animals). The development of a Sensorium-style standardized dataset  for freely-moving mice would significantly benefit future research in this area, enabling more robust comparisons between different modeling approaches. Second, it would be beneficial to integrate other modalities that are known to be encoded in mouse V1 into the model. One such example is reward signals , which could provide additional information about the animal's decision-making processes and motivations during exploration.