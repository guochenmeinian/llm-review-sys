# Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective

Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective

Chengsen Wang\({}^{1}\)1 Qi Qi\({}^{1}\)1 Jingyu Wang\({}^{12}\)2

Haifeng Sun\({}^{1}\)  Zirui Zhuang\({}^{1}\)  Jinming Wu\({}^{1}\)  Jianxin Liao\({}^{1}\)

\({}^{1}\)Beijing University of Posts and Telecommunications, Beijing, China

\({}^{2}\)Pengcheng Laboratory, Shenzhen, China

{cswang, qiqi8266, wangjingyu}@bupt.edu.cn

{hfsun, zhuangzirui, wjm_18, liaojx}@bupt.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Time series forecasting has played a pivotal role across various industries, including finance, transportation, energy, healthcare, and climate. Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. However, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. To address these problems, we propose a novel framework named GLAFF. Within this framework, the timestamps are modeled individually to capture the global dependencies. Working as a plugin, GLAFF adaptively adjusts the combined weights for global and local information, enabling seamless collaboration with any time series forecasting backbone. Extensive experiments conducted on nine real-world datasets demonstrate that GLAFF significantly enhances the average performance of widely used mainstream forecasting models by 12.5%, surpassing the previous state-of-the-art method by 5.5%. Code is available at https://github.com/ForestsKing/GLAFF.

## 1 Introduction

Time series forecasting holds significant importance across various industries, including finance , transportation , energy , healthcare, and climate . With the development of deep learning techniques, neural network-based methods  have notably propelled advancements owing to their strong capability in capturing dependencies within time series. The relevant models have evolved from statistical models to RNNs, CNNs, Transformers, and LLMs. However, existing research primarily concentrates on local observations within history sliding windows, overlooking the significance of timestamps.

Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. For instance, traffic volumes on weekdays typically exhibit high peaks. Regrettably, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. DLinear  and FPT  completely overlook timestamps. Informer  and TimesNet  incorporate timestamps by summing their embeddings with position embeddings and data embeddings. These intertwinedpatterns encourage networks to extract information from more intuitive observations. iTransformer  embeds timestamp features separately into tokens employed by the attention mechanism. This embedding method across time points damaged the physical significance of timestamps. To validate this proposition, we conduct an ablation study on the aforementioned models using the Traffic dataset. The results depicted in Figure 1(a) indicate that the performance of the models exhibits no significant decline after removing timestamps. Meanwhile, our proposed GLAFF demonstrates a notable enhancement in mainstream forecasting models.

Moreover, Time series collected from the real world often be polluted . For example, a spike in electricity consumption coupled with short circuits can induce point anomalies, while a reduction in traffic volume coinciding with holidays can evoke contextual anomalies. When local information gathered from the real world contains anomalies, the absence of global information will damage the robust prediction capability of most forecasting techniques [7; 39; 43; 46]. We illustrate traffic volume for San Francisco Bay area freeways at an hourly granularity in Figure 1(b). Typically, this sequence exhibits a clear periodic pattern, alternating with five high peaks (weekdays) and two low peaks (weekends). However, due to a holiday, the week from 24 to 192 shows a deviation, resulting in three high peaks and four low peaks. As evident from the illustration in the lower right corner of Figure 1(b), the mainstream forecasting models [24; 38; 41] usually demonstrate reliable prediction capability. Nonetheless, when the observations within the history window include anomalies, as illustrated in the lower left corner of Figure 1(b), these models are significantly affected and yield notably underestimated predictions. Therefore, it is necessary to reasonably incorporate more robust global information into the existing forecasting technique.

To address the aforementioned problems, we propose a generalized framework named GLAFF (short for **G**lobal-**L**ocal **A**daptive **F**usion **F**ramework), aimed at enhancing the robust prediction capability of time series forecasting models in the real world leveraging global information. Specifically, GLAFF initially employs the Attention-based Mapper to individually model the timestamps containing global information and maps them to observations conforming to a standard distribution. Subsequently, to handle scenarios where anomalies are present within the observations of the sliding window, we utilize the Robust Denormalizer to inverse normalize the initial mappings, thereby mitigating the impact of data drift . Finally, the Adaptive Combiner dynamically adjusts the combined weights for global mapping and local prediction within the prediction window, yielding the final prediction outcome. By fusing the robustness of global information with the flexibility of local information, GLAFF demonstrates a substantial enhancement in the robust prediction capability of mainstream forecasting models. Additionally, GLAFF serves as a model-agnostic and plug-and-play framework that can seamlessly collaborate with any time series forecasting backbone.

In general, the contributions of our paper are summarised as follows:

* We propose GLAFF that leverages global information, represented by timestamps, to improve the robust prediction capability of time series forecasting models. GLAFF is a plug-and-play module that seamlessly collaborates with any time series forecasting backbone.

Figure 1: The experimental results on Traffic dataset. (a) illustrates the outcomes of the ablation study on mainstream forecasting models and their variants. (b) depicts the visualization of traffic volume (upper), successful prediction case (lower right), and failed prediction case (lower left), respectively.

* We design a Robust Denormalization module to facilitate the adaptation of GLAFF for data drift, even when the observations encompass anomalies, alongside an Adaptive Combiner module for dynamically fusing global and local information.
* We conduct comprehensive experiments on nine real-world benchmark datasets across five domains. The result demonstrates that GLAFF significantly improves the robust prediction capability of mainstream forecasting models.

## 2 Related Work

As a significant real-world challenge, time series forecasting has garnered considerable attention. Initially, ARIMA  establishes an autoregressive model and performs forecasts in a moving average manner. However, the inherent complexity of the real world often renders such statistical methodologies [2; 14; 33] challenging to adapt. With the development of deep learning techniques, neural network-based methods have become increasingly important. Recurrent neural networks [10; 13; 30] dynamically capture temporal dependencies by modeling semantic information within a sequential structure. Unfortunately, this architecture suffers from gradient vanishing/exploding and information forgetting when dealing with long sequences. To further improve prediction performance, self-attention mechanisms [19; 22; 45] and convolutional networks [21; 36; 38] have been introduced to capture long-range dependencies. Additionally, prior research  has demonstrated that a simple linear network augmented by decomposition can also achieve competitive performance. Nowadays, with fast growth and remarkable performances of large language models, there is a growing interest [3; 32; 47] in utilizing LLM to analyze time series data. Recently, the iTransformer  has emerged as the state-of-the-art method for time series forecasting tasks by embedding series from different channels into the variate tokens utilized by the attention mechanism.

Most time series forecasting techniques focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. DLinear , FPT , and other models [26; 40; 43] completely overlook timestamps. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. Informer , TimesNet , and other models [23; 37; 46] incorporate timestamps by summing their embeddings with position embeddings and data embeddings. These intertwined patterns encourage networks to extract information from more intuitive observations. iTransformer  embeds timestamp features separately into tokens employed by the attention mechanism. This embedding method across time points damaged the physical significance of timestamps.

The processing of timestamps by the previous baselines and our proposed GLAFF can be abstracted as early fusion (feature-level fusion) and late fusion (decision-level fusion). Early fusion integrates modalities into a single representation at the input level and processes the fused representation through the model. Late fusion allows each modality to run independently through its own model and fuses the outputs of each modality. Compared to early fusion, late fusion maximizes the processing effectiveness of each modality and is less susceptible to the noise of a single modality, resulting in greater robustness and reliability. This has been validated by extensive previous work [20; 27; 35].

## 3 Methodology

We propose a model-agnostic and plug-and-play framework, GLAFF, which utilizes global information, represented by timestamps, to enhance the robust prediction capability of mainstream time series forecasting models in real-world scenarios. In multivariate time series forecasting, given the history observations of \(c\) channels within \(h\) time steps \(=\{_{1},,_{h}\}^{h c}\), we aim to forecast the subsequent \(p\) time steps \(=\{_{h+1},,_{h+p}\}^{p c}\). In addition to observations, we incorporate timestamps to provide global information. For each timestamp, we extract its month, day, weekday, hour, minute, and second as timestamp features, respectively. For instance, for the timestamp _2018-06-02 12:00:00_ at moment \(t\), its feature representation is \(_{t}=^{1 6}\). The holiday markers may also be included if accessible. Unlike observations, the timestamp features within the history window \(=\{_{1},,_{h}\}^{h 6}\) and the timestamp features within the prediction window \(=\{_{h+1},,_{h+p}\}^{p 6}\) are known. In this section, we describe the detailed workflow of the entire GLAFF framework and explain how it fuses local information \(\) and global information \(,\) to predict \(\).

### Overview

GLAFF is a plug-and-play framework that seamlessly collaborates with any time series forecasting backbone. The overall architecture of the plugin is depicted in Figure 2, comprising three primary components: Attention-based Mapper, Robust Denormalizer, and Adaptive Combiner. Following local prediction \(}^{p c}\) provided by the backbone network based on history observations \(\) (maybe including underutilized history timestamps \(\) and future timestamps \(\)), GLAFF leverages global information to revise it. Initially, the Attention-based Mapper captures dependencies between timestamps through an attention mechanism, mapping timestamp features \(\) and \(\) into an initial history mapping \(}^{h c}\) and an initial future mapping \(}^{p c}\), conforming to standard distribution. Subsequently, the Robust Denormalizer inverse normalizes the initial mappings \(}\) and \(}\) to \(}^{h c}\) and \(}^{p c}\) based on quantile deviation between the initial mapping \(}\) and the actual observations \(\) within the history window, mitigating the impact of data drift. Lastly, the Adaptive Combiner dynamically adjusts the combined weights of the global mapping \(}\) and the local prediction \(}\) within the prediction window according to the disparity between the final mapping \(}\) and the actual observations \(\) within the history window, yielding the final prediction outcome \(\). By fusing the robustness of global information and the flexibility of local information, GLAFF significantly enhances the robust prediction capability of mainstream forecasting models.

### Attention-based Mapper

As depicted in the green segment of Figure 2, our proposed Attention-based Mapper employs a simplified encoder-only architecture within the Transformer  framework, comprising an embedding layer, attention blocks, and a projection layer. Analogous to typical Transformer-based encoders [37; 45], each timestamp feature is initially tokenized by an embedding layer to describe its properties, applied by self-attention for mutual interactions, and individually processed by feed-forward networks for series representations. Subsequently, a projection layer is utilized to acquire the initial mappings. Leveraging the capability of the attention mechanism for capturing long-range dependencies and parallel computation, the Attention-based Mapper can sufficiently model the global information embodied by timestamps.

Specifically, in Attention-based Mapper, the procedure for obtaining its corresponding initial mapping \(}\), which conforms to the standard distribution, based on the history timestamps \(\), is succinctly delineated as follows:

\[^{0} =()\] (1) \[^{i+1} =(^{i}),\;i=0,,l-1\] \[} =(^{l})\]

Figure 2: The overall architecture of GLAFF mainly consists of three primary components: Attention-based Mapper, Robust Denormalizer, and Adaptive Combiner.

where \(^{i}^{h d}\) denotes the intermediate feature variable output from the \(i\)-th attention block, and \(d\) represents the dimension of the intermediate feature variable. The attention blocks are stacked with \(l\) layers to capture the high-level semantic information hidden within the timestamps. To maintain simplicity in implementation, both the embedding and projection layers are comprised of a single linear layer. Following conventional protocol, the primary computation steps for the \(i\)-th attention block are outlined as:

\[^{i} =(^{i}+( ^{i},^{i},^{i}))\] (2) \[^{i+1} =(^{i}+ (^{i}))\]

where \(()\) represents the commonly adopted layer normalization and \(()\) denotes the multilayer feedforward network. The \((,,)\) indicates the Multihead Self-Attention mechanism , where \(,,\) serve as the query, key, and value respectively. Additionally, a dropout mechanism is incorporated to alleviate overfitting and enhance the generalization of the network. The process of obtaining the corresponding initial mapping \(}\) based on the future timestamps \(\), conforming to the standard distribution, mirrors the aforementioned procedure, simply substituting \(\) and \(}\) in Equation 1 with \(\) and \(}\) respectively.

### Robust Denormalizer

Due to the inherent variability of the real world, time series observations typically undergo rapid evolution over time, a phenomenon commonly referred to as data drift . This phenomenon can result in discrepancies across different time spans and hinder the generalization ability of deep learning models. Recognizing the presence of data drift, GLAFF employs a two-phase untangling modeling strategy to address the global information represented by timestamps. In the first phase, the network in Attention-based Mapper produces initial mappings, denoted as \(}\) and \(}\), which are assumed to satisfy a standard distribution for reducing the difficulty of modeling the dependencies between timestamps and observations. Subsequently, in the second phase, leveraging the distribution deviations between the initial mapping \(}\) and the actual observations \(\) within the history window, the Robust Denormalizer separately inverse normalizes the initial mappings \(}\) and \(}\) to produce the final mappings \(}\) and \(}\), mitigating the impact of data drift.

To alleviate the impact of data drift, a feasible solution [9; 17; 23; 25] has been proposed: removing dynamic factors from the original data through a normalization procedure before feeding them into the deep learning model, and subsequently reintroducing these dynamic factors via an inverse normalization procedure after output from the deep learning model. The conventional inverse normalization procedure typically considers distribution deviations in mean and standard deviation. Nonetheless, this approach is susceptible to extreme values and lacks robustness when the observations contain anomalies. Instead of relying on mean and standard deviation, we employ median and quantile ranges , respectively, to enhance the robustness of the Robust Denormalizer against anomalies. As depicted in the yellow segment of Figure 2, the procedure for Robust Denormalizer to inverse normalize initial mappings \(}\) and \(}\) into final mappings \(}\) and \(}\) can be succinctly expressed as:

\[} =}-}{} +\] (3) \[} =}-}{} +\]

where \(^{1 c}\) and \(^{1 c}\) represent the median of the initial mapping \(}\) and the actual observation \(\) for each channel, respectively. Similarly, \(^{1 c}\) and \(^{1 c}\) denote the quantile range (the distance between the \(q\) quantile and the \(1-q\) quantile) of the initial mapping \(}\) and the actual observation \(\) for each channel. Specifically, when \(q=0.75\), \(\) and \(\) correspond to the inter-quartile range (IQR3) for each channel of the initial mapping \(}\) and the actual observation \(\).

### Adaptive Combiner

Owing to the intricacies of real-world scenarios, data preferences for model bias will continuously change with online concept drifts . Therefore, we need a data-dependent strategy to change the model selection policy continuously. In other words, the combined weights of global and local information necessitate adaptive and dynamic updates. When the time series pattern exhibits clarity and stability, greater emphasis should be placed on robust global information. Conversely, increased attention should be directed towards flexible local information when the time series pattern appears ambiguous and variable. Within the framework of GLAFF, we employ an Adaptive Combiner to realize the adaptive adjustment of combined weights.

As illustrated in the red segment of Figure 2, the Adaptive Combiner initially dynamically adjusts the combined weights of the global mapping \(}\) and the local prediction \(}\) within the prediction window, based on the deviation between the final mapping \(}\) and the actual observation \(\) within the history window. Subsequently, we aggregate the dual-source information based on the combined weights to yield the final prediction \(\). Specifically, the primary computation procedure of the Adaptive Combiner is represented as:

\[ =(}-)\] (4) \[ =(}})\]

where \(^{1 c 2}\) signifies the dynamically generated combined weight by the network based on the deviation between the final mapping \(}\) and the actual observation \(\) within the history window. The \(\) denotes the concatenation operation based on the additional last dimension, and \(\) denotes the summation operation performed across the last dimension. For simplicity, the weight generation network consists solely of a Multilayer Perceptron (MLP) containing a hidden layer and a layer of \(\) for weight normalization.

Through adaptive adjustment of combined weights, our method can effectively fuses the robustness of global information and the flexibility of local information, thereby enhancing its suitability for intricate and fluctuating real-world scenarios.

## 4 Experiment

### Experimental Setup

DatasetWe conduct extensive experiments on nine datasets across five domains, including Electricity, Exchange, Traffic, Weather, and ILI, along with four ETT datasets. Detailed dataset information is provided in Appendix A.1. We follow the standard segmentation protocol [24; 37; 45], strictly dividing each dataset into training, validation, and testing sets chronologically to ensure no information leakage issues. The segmentation ratio for each dataset is set to 6:2:2. Regarding prediction settings, we also adhere to established mainstream protocols [26; 38; 41]. Specifically, we set the length of the history window to 96 for the Electricity, Exchange, Traffic, Weather, and four ETT datasets, while the prediction length varies within {96, 192, 336, 720}. For the ILI, which has fewer time points, the length of the history window is fixed at 36, and the prediction length varies within {24, 36, 48, 60}.

BackboneTo demonstrate the effectiveness of the framework, we select several mainstream forecasting models based on different architectures, including the Transformer-based Informer (2021)  and iTransformer (2024) , the Linear-based DLinear (2023) , and the Convolution-based TimesNet (2023) . Notably, iTransformer represents the previous state-of-the-art method in time series forecasting tasks. Further details regarding the backbone models are provided in Appendix A.2. As described in Section 2, these backbones encompass three different treatments for timestamps employed in prior forecasting techniques, namely summation (Informer, TimesNet), concatenation (iTransformer), and omission (DLinear).

The details of experimental setup can be found in Appendix A.3. All experiments are based on our runs, utilizing the same hardware configurations, and repeated 3 times with different random seeds.

### Main Result

Table 1 compares the prediction outcomes for mainstream baselines and GLAFF. We present a detailed version of this table in Appendix B.1. The results indicate that GLAFF significantly surpasses all four widely used mainstream baselines across all nine real-world benchmark datasets. In particular, GLAFF enhances the respective backbones by an average of 12.5%.

By fusing the robustness of global information with the flexibility of local information, GLAFF can significantly improve the robust prediction capability of mainstream forecasting models. Specifically, in the case of DLinear, a Linear-based model that entirely disregards timestamps, GLAFF enhances its prediction accuracy by 13.1%. For Transformer-based Informer and Convolution-based TimesNet utilizing simple timestamp summation, GLAFF yields performance improvements of 23.8% and 7.5%, respectively. In the case of Transformer-based iTransformer employing direct timestamp concatenation, GLAFF still produces a 5.5% improvement in accuracy. Additionally, we note a diminishing boosting effect of GLAFF as the modeling prowess of the backbone increases, indicative of the complementary nature of global and local information. Nonetheless, for the state-of-the-art iTransformer, GLAFF continues to offer substantial benefits.

It is evident that the enhancement of GLAFF varies across datasets with different characteristics. For datasets such as Traffic and Electricity, characterized by a significant number of channels and clear periodic patterns, GLAFF demonstrates superior capability in capturing the dependencies between timestamps and observations, resulting in performance enhancements of 19.5% and 15.7%, respectively. For the non-stationary datasets , such as ETTh2, ETTm2, and Exchange, the Robust Denormalizer effectively alleviates the impact of data drift, thereby augmenting prediction accuracy by 12.1%, 12.1%, and 16.4%, respectively. Regarding common datasets like ETTh1, ETTm1, Weather, and ILI, although the performance of GLAFF may not be as remarkable, it still yields improvements of 8.8%, 8.1%, 9.6%, and 9.6%, respectively.

Given the burgeoning interest in leveraging LLMs for time series, we assess the performance augmentation of GLAFF when applied to LLM-based backbones. Specifically, we employ the widely recognized FPT (2023)  as our baseline. FPT completely disregards timestamps similar to DLinear. We deploy two structures, GPT2(3) and GPT2(6), of FPT as outlined in their paper. The ILI dataset has a history window length of 36 and a prediction window length of 48, while other datasets have a history window length of 96 and a prediction window length of 192. As delineated in the findings presented in Table 2, GLAFF also offer significant benefits to the LLM-based baselines.

    &  &  &  &  &  &  &  &  &  \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & 96 & 0.333 & 0.414 & **0.217** & **0.323** & 0.196 & 0.283 & **0.147** & **0.238** & 0.175 & 0.280 & **0.154** & **0.248** & 0.153 & 0.246 & **0.120** & **0.198** \\  & 192 & 0.362 & 0.404 & **0.220** & 0.196 & 0.286 & **0.172** & **0.253** & 0.199 & 0.293 & **0.190** & 0.269 & 0.167 & 0.259 & **0.143** & **0.216** \\  & 336 & 0.352 & 0.434 & **0.230** & 0.373 & 0.208 & 0.301 & **0.197** & **0.274** & 0.211 & 0.310 & **0.185** & **0.276** & 0.138 & 0.276 & **0.168** & **0.240** \\  & 720 & 0.364 & 0.432 & **0.247** & 0.351 & 0.239 & 0.331 & **0.239** & **0.380** & 0.235 & 0.236 & **0.236** & **0.303** & 0.220 & 0.310 & **0.279** & **0.279** \\   & 96 & 0.926 & 0.736 & **0.609** & **0.560** & 0.409 & 0.440 & **0.391** & **0.418** & 0.453 & 0.481 & **0.435** & **0.464** & 0.420 & 0.454 & **0.411** & **0.441** \\  & 192 & 1.235 & 0.844 & **0.831** & **0.680** & 0.457 & 0.475 & **0.446** & **0.457** & 0.533 & 0.531 & **0.520** & **0.517** & 0.494 & 0.502 & **0.474** & **0.482** & 8.8 \\  & 336 & 1.354 & 0.835 & **0.838** & **0.698** & 0.500 & 0.500 & **0.492** & **0.492** & 0.082 & 0.621 & 0.580 & **0.596** & **0.566** & 0.538 & 0.528 & **0.534** & **0.519** \\  & 720 & 1.264 & 0.857 & **0.937** & **0.730** & 0.610 & 0.576 & **0.609** & **0.556** & 0.844 & 0.697 & **0.773** & **0.661** & 0.716 & 0.629 & **0.704** & **0.615** \\   & 96 & 0.708 & 0.549 & **0.422** & **0.443** & 0.159 & 0.278 & **0.128** & **0.205** & 0.183 & 0.298 & **0.174** & **0.276** & 0.177 & 0.287 & **0.172** & **0.271** \\  & 192 & 1.133 & 0.688 & **0.680** & **0.599** & 0.870 & 0.369 & **0.165** & **0.238** & 0.218 & 0.292 & **0.240** & **0.306** & 0.199 & 0.311 & **0.196** & **0.295** \\  & 336 & 0.997 & 0.667 & **0.477** & **0.570** & 0.207 & 0.330 & **0.206** & **0.265** & 0.240 & 0.346 & **0.219** & **0.319** & 0.220 & 0.329 & **0.231** & **0.336** & **0.290** \\  & 720 & 1.607 & 0.815 & **1.255** & **0.729** & 0.262 & 0.738 & **0.214** & **0.288** & 0.281 & 0.376 & **0.287** & 0.343 & 0.271 & 0.366 & **0.270** & **0.356** \\   & 96 & 0.953 & 0.548 & **0.503** & **0.486** & 0.339 & 0.388 & **0.309** & **0.353** & 0.449 & 0.448 & **0.381** & **0.398** & 0.383 & 0.415 & **0.349** & **0.386** \\  & 192 & 0.611 & 0.576 & **0.534** & **0.525** & 0.394 & 0.418 & **0.362** & **0.386** & 0.448 & 0.461 & **0.440** & **0.440** & **0.432** & 0.429 & 0.445 & **0.403** & **0.420** \\  & 336 & 0.888 & 0.726 & **0.707** & **0.628** & 0.450 & 0.450 & 0.414Additionally, to validate the practical applicability of GLAFF, we assess the computation costs in Appendix B.4. The results indicate that GLAFF has little effect on model training and deployment across most scenarios, particularly when considering its significant accuracy enhancement.

### Prediction Showcase

In addition to evaluation metrics, forecasting quality is crucial. To further compare GLAFF and the four mainstream forecasting models, we illustrate prediction showcases for two representative datasets in Figure 3. We provide the full prediction showcases for the nine datasets in Appendix B.5. It is evident that GLAFF can yield more realistically robust predictions. At the same time, the respective backbones are susceptible to abnormal local information.

The Traffic dataset records traffic volume in hourly granularity. Typically, this sequence exhibits a clear periodic pattern, alternating with five high peaks (weekdays) and two low peaks (weekends). However, owing to a holiday, the initial two days within the example history window do not exhibit the high peaks as usual. Due to the limited local information containing such contextual anomalies, Informer, DLinear, and iTransformer all think the prediction window should also consist of only low peaks. Although TimesNet generates predictions with high peaks, it displays an incorrect alternation between five high peaks and one low peak. By introducing sufficiently modeled global information, GLAFF has enabled the four mainstream forecasting backbones to recognize the existence of high peaks and the correct periodic patterns, thus yielding more accurate forecasts.

The Electricity dataset records electricity consumption in hourly granularity. Typically, this sequence exhibits a clear periodic pattern, alternating with five peaks (weekdays) and two flat segments (weekends). However, owing to a short circuit, the middle two days within the example history window show a spike in electricity consumption. Due to the limited local information containing such point anomalies, DLinear, TimesNet, and iTransformer all think the flat segments in the prediction window should also contain a spike. Although Informer generates predictions without spikes, it completely ignores the presence of flat segments. By introducing sufficiently modeled global information, GLAFF has enabled the four mainstream forecasting backbones to recognize the contingency of spikes and the correct periodic patterns, thus yielding more robust predictions.

    &  &  &  &  \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Electricity & 0.194\(\)0.002 & 0.278\(\)0.002 & **0.168\(\)0.005** & **0.28\(\)0.002** & 0.194\(\)0.001 & 0.279\(\)0.002 & **0.17\(\)0.002** & **0.258\(\)0.001** \\ ETH1 & 0.466\(\)0.001 & 0.483\(\)0.001 & **0.454\(\)0.003** & **0.462\(\)0.003** & 0.468\(\)0.002 & 0.438\(\)0.002 & **0.445\(\)0.002** & **0.451\(\)0.002** \\ ETH2 & 0.190\(\)0.000 & 0.304\(\)0.000 & **0.178\(\)0.017** & **0.284\(\)0.010** & 0.190\(\)0.002 & 0.303\(\)0.002 & **0.177\(\)0.005** & **0.286\(\)0.003** \\ ETH1 & 0.411\(\)0.005 & 0.431\(\)0.003 & **0.388\(\)0.009** & **0.409\(\)0.005** & 0.412\(\)0.002 & 0.431\(\)0.002 & **0.390\(\)0.012** & **0.413\(\)0.007** \\ ETH2 & 0.142\(\)0.001 & 0.257\(\)0.001 & **0.120\(\)0.001** & 0.235\(\)0.002 & 0.141\(\)0.002 & 0.256\(\)0.002 & **0.119\(\)0.002** & **0.233\(\)0.002** \\ Exchange & 0.110\(\)0.001 & 0.238\(\)0.002 & **0.090\(\)0.002** & **0.219\(\)0.002** & 0.106\(\)0.001 & 0.234\(\)0.001 & **0.088\(\)0.001** & **0.216\(\)0.001** \\ IL & 1.585\(\)0.001 & 0.900\(\)0.019 & **1.393\(\)0.024** & **0.782\(\)0.023** & 1.494\(\)0.012 & 0.854\(\)0.010 & **1.396\(\)0.026** & **0.778\(\)0.018** \\ Traffic & 0.370\(\)0.002 & 0.309\(\)0.002 & **0.296\(\)0.002** & **0.256\(\)0.001** & 0.371\(\)0.002 & 0.312\(\)0.001 & **0.301\(\)0.003** & **0.262\(\)0.002** \\ Weather & 0.241\(\)0.000 & 0.276\(\)0.000 & **0.234\(\)0.004** & **0.268\(\)0.003** & 0.243\(\)0.001 & 0.278\(\)0.001 & **0.228\(\)0.002** & **0.264\(\)0.002** \\   

Table 2: The forecasting errors for multivariate time series among GLAFF and LLM-based baselines. A lower outcome indicates a better prediction. The best results are highlighted in bold.

Figure 3: The illustration of prediction showcases among GLAFF and mainstream baselines.

### Ablation Study

We provide a comprehensive ablation study to validate the necessity of the GLAFF components. We implement our approach and its four variants on the iTransformer backbone. The results of our experiments on three representative benchmark datasets are presented in Table 3. Detailed results for the nine real-world benchmark datasets are available in Appendix B.2.

In w/o Backbone, we completely remove the backbone network within the GLAFF and map the future using only timestamps. Surprisingly, GLAFF still demonstrates favorable prediction performance without any observations. The average prediction accuracy of GLAFF even outperforms Informer and DLinear, and is also competitive with TimesNet and iTransformer. Global information proves adequate in scenarios featuring clear periodic and stable distributions.

In w/o Attention, we substitute the stacked attention blocks with MLP networks having the equivalent size. Following the replacement of attention blocks, GLAFF fails to capture the dependencies among timestamps adequately. It proves challenging to map out precise observations solely from a single timestamp. Particularly notable is the most marked decline in performance on the Traffic dataset, which has the largest number of channels, indicating the greatest modeling challenge for GLAFF.

In w/o Quantile, we replace the Robust Denormalizer with conventional inverse normalization. The experimental results illustrate that our design yields enhancements across all three datasets, particularly in the Electricity dataset. When the history window encompasses anomalies, conventional inverse normalization yields inaccurate estimates for distribution. Leveraging more robust quantiles, our Robust Denormalizer demonstrates enhanced robustness in mitigating the impacts of data drift.

In w/o Adaptive, we substitute the Adaptive Combiner with a straightforward averaging for global mapping and local prediction. We distinctly find that dynamically adjusting the combined weights can prove more efficacious in accommodating fluctuating real-world scenarios, particularly evident in the non-stationary Weather dataset. By fusing global and local information adaptively, GLAFF can seamlessly collaborate with any time series forecasting backbone.

## 5 Conclusion

In this work, our focus lies in leveraging global information, as denoted by timestamps, to enhance the robust prediction capability of time series forecasting models in the real world. We introduce a new approach named GLAFF, serving as a model-agnostic and plug-and-play framework. Within this framework, the timestamps are modeled individually to capture the global dependencies. Through adaptive adjustment of combined weights for global and local information, GLAFF facilitates seamless collaboration with any time series forecasting backbone. To substantiate the superiority of our approach, we have conducted comprehensive experiments on widely used benchmark datasets, demonstrating the substantial enhancement GLAFF provides to mainstream forecasting models. We hope that GLAFF can be used as a foundational component for time series forecasting and call on the community to give more attention to global information represented by timestamps.

    &  &  &  &  &  &  \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\    } & 96 & 0.1525 & 0.2460 & **0.1197** & **0.1979** & 0.2058 & 0.2663 & 0.1518 & 0.2450 & 0.1467 & 0.2465 & 0.1574 & 0.2502 \\  & 192 & 0.1674 & 0.2593 & **0.1434** & **0.2157** & 0.2097 & 0.2793 & 0.1684 & 0.2610 & 0.1677 & 0.2662 & 0.1740 & 0.2662 \\  & 336 & 0.1830 & 0.2762 & **0.1683** & **0.2395** & 0.2454 & 0.3014 & 0.1832 & 0.2775 & 0.1993 & 0.2954 & 0.1953 & 0.2877 \\  & 720 & 0.2199 & 0.3097 & **0.2169** & **0.2786** & 0.2984 & 0.3386 & 0.2182 & 0.3092 & 0.2593 & 0.3403 & 0.2330 & 0.3171 \\    } & 96 & 0.3084 & 0.2717 & **0.2828** & **0.2485** & 0.3348 & 0.2723 & 0.3172 & 0.2806 & 0.2909 & 0.2684 & 0.2930 & 0.2612 \\  & 192 & 0.3267 & 0.2794 & **0.2909** & **0.2528** & 0.3387 & 0.2736 & 0.3357 & 0.2884 & 0.2948 & 0.2737 & 0.2970 & 0.2610 \\  & 336 & 0.3381 & 0.2850 & **0.3095** & **0.2594** & 0.3460 & 0.2794 & 0.3482 & 0.2958 & 0.3023 & 0.2804 & 0.3082 & 0.2706 \\  & 720 & 0.3574 & 0.3015 & **0.3201** & **0.2730** & 0.3558 & 0.2906 & 0.3684 & 0.3113 & 0.3249 & 0.2984 & 0.3212 & 0.2819 \\    } & 96 & 0.1784 & 0.2229 & **0.1587** & **0.2199** & 0.2382 & 0.2695 & 0.1780 & 0.2214 & 0.1811 & 0.2270 & 0.1914 & 0.2379 \\  & 192 & 0.2308 & 0.2675 & **0.2138** & **0.2654** & 0.2882 & 0.3105 & 0.2383 & 0.2733 & 0.2364 & 0.2768 & 0.2489 & 0.2832 \\   & 336 & 0.2892 & 0.3099 & **0.2733** & **0.3058** & 0.3381 & 0.3414 & 0.2932 & 0.3146 & 0.2905 & 0.3134 & 0.3070 & 0.3251 \\   & 720 & 0.3701 & 0.3634 & **0.3520** & **0.3547** & 0.4011 & 0.3813 & 0.3752 & 0.3664 & 0.3727 & 0.3649 & 0.3829 & 0.3722 \\   & 0.2602 & 0.2827 & **0.2367** & **0.2593** & 0.3000 & 0.3004 & 0.2646 & 0.2870 & 0.2555 & 0.2876 & 0.2591 & 0.2845 \\   

Table 3: The forecasting errors for multivariate time series of ablation study among GLAFF and variants. A lower outcome indicates a better prediction. The best results are highlighted in bold.