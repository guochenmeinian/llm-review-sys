# Online Performative Gradient Descent for Learning Nash Equilibria in Decision-Dependent Games

Zihan Zhu

Duke University

&Ethan X. Fang

Duke University

&Zhuoran Yang

Yale University

Department of Statistical Science, Duke University. zihan.zhu@duke.edu

Department of Biostatistics and Bioinformatics, Duke University. xingyuan.fang@duke.edu

Department of Statistics and Data Science, Yale University. zhuoran.yang@yale.edu

###### Abstract

We study multi-agent games within the innovative framework of decision-dependent games, which establishes a feedback mechanism that population data reacts to agents' actions and further characterizes the strategic interactions among agents. We focus on finding the Nash equilibrium of decision-dependent games in the bandit feedback setting. However, since agents are strategically coupled, classical gradient-based methods are infeasible without the gradient oracle. To overcome this challenge, we model the strategic interactions by a general parametric model and propose a novel online algorithm, Online Performative Gradient Descent (OPGD), which leverages the ideas of online stochastic approximation and projected gradient descent to learn the Nash equilibrium in the context of function approximation for the unknown gradient. In particular, under mild assumptions on the function classes defined in the parametric model, we prove that the OPGD algorithm finds the Nash equilibrium efficiently for strongly monotone decision-dependent games. Synthetic numerical experiments validate our theory.

## 1 Introduction

The classical theory of learning and prediction fundamentally relies on the assumption that data follows a static distribution. This assumption, however, does not account for many dynamic real-world scenarios where decisions can influence the data involved. Recent literature on performative classification (Hardt et al., 2016; Dong et al., 2018; Miller et al., 2020) and performative prediction (Perdomo et al., 2020) offers a variety of examples where agents are strategic, and data is performative. For instance, in the ride-sharing market, both passengers and drivers engage with multiple platforms using various strategies such as "price shopping". Consequently, these platforms observe performative demands, and the pricing policy becomes strategically coupled.

In this paper, we explore the multi-agent performative prediction problem, specifically, the multi-agent decision-dependent games, as proposed by Narang et al. (2022). We aim to develop algorithms to find Nash equilibria with the first-order oracle. In this scenario, agents can only access their utility functions instead of gradients through the oracle. Finding Nash equilibria in decision-dependent games is a challenging task. Most existing works primarily focus on finding performative stable equilibria within the single-agent setting, an approach that approximates the Nash equilibrium and is relatively straightforward to compute (Mendler-Dunner et al., 2020; Wood et al., 2021; Drusvyatskiy and Xiao, 2022; Brown et al., 2022; Li and Wai, 2022).

There are two major challenges associated with this problem: (i) the distribution shift induced by performative data, and (ii) the lack of first-order information for the performative gradient. To address these two challenges, we propose a novel online gradient-based algorithm, Online Performative Gradient Descent (DPGD). In particular, our algorithm employs a general parametric framework tomodel the decision-dependent distribution, which provides an unbiased estimator for the unknown gradient, and leverages online stochastic approximation methods to estimate the parametric functions.

### Major Contributions

Our work provides new fundamental understandings of decision-dependent games. Expanding upon the linear parametric assumption in Narang et al. (2022), we propose a more comprehensive parametric framework that models decision-dependent distributions of the observed data. We also derive sufficient conditions under this parametric framework that guarantee a strongly monotone decision-dependent game, thereby ensuring a unique Nash equilibrium.

From the algorithmic perspective, we propose OPGD, the first online algorithm to find the Nash equilibrium under linear and kernel parametric models. The core problem in decision-dependent games is estimating the performative gradient. We remark that the existing algorithm only handles the linear case and cannot be extended to the non-linear parametric model (Section 3), and OPGD uses an essentially different method to learn the strategic interaction. To elaborate, under the proposed parametric framework, learning the Nash equilibrium in decision-dependent games can be formulated as a bilevel problem, where the lower level is learning the strategic model and the upper level is finding equilibriums. The OPGD algorithm leverages the ideas of online stochastic approximation for the lower problem and projected gradient descent to learn the Nash equilibrium. Moreover, we acknowledge this learning framework bridges online optimization and statistical learning with time-varying models.

We further prove that under mild assumptions, OPGD converges to the Nash equilibrium. For the linear function class, OPGD achieves a convergence rate of \((t^{-1})\), matching the optimal rate of SGD in the strongly-convex setting, where \(t\) represents the number of iterations. For the kernel function class \(\) associated with a bounded kernel \(K\), we posit that the parametric functions reside within the power space \(^{}\) and evaluate the approximation error of OPGD under the \(\)-power norm, where \(\) represents the minimal value that ensures the power space \(^{}\) possesses a bounded kernel. We present the first analysis for online stochastic approximation under the power norm (Lemma 4), in contrast to the classical RKHS norm (Tarres and Yao, 2014; Pillaud-Vivien et al., 2018; Lei et al., 2021). The difference between the RKHS \(\) and the power space \(^{}\) makes the standard techniques fail under the power norm, and we use novel proof steps to obtain the estimation error bound. We demonstrate that OPGD leverages the embedding property of the kernel \(K\) to accelerate convergence and achieves the rate of \((t^{-})\). Moreover, OPGD can handle the challenging scenario, where parametric functions are outside the RKHS. See Section 4.2 for more details.

### Related Work

Performance prediction.The multi-agent decision-dependent game in this paper is inspired by the performative prediction framework (Perdomo et al., 2020). This framework builds upon the pioneering works of strategic classification (Hardt et al., 2016; Dong et al., 2018; Miller et al., 2020), and extends the classical statistical theory of risk minimization to incorporate the performativity of data. Perdomo et al. (2020); Mendler-Dunner et al. (2020); Miller et al. (2021) introduce the concepts of performative optimality and stability, demonstrating that repeated retraining and stochastic gradient methods converge to the performatively stable point. Miller et al. (2021), in pursuit of the performatively optimal point, model the decision-dependent distribution using location families and propose a two-stage algorithm. Similarly, Izzo et al. (2021) develop algorithms to estimate the unknown gradient using finite difference methods. More recently, Narang et al. (2022); Piliouras and Yu (2022) expand the performative prediction to the multi-agent setting, deriving algorithms to find the performatively optimal point.

Learning in continuous games.Our work aligns closely with optimization in continuous games. Rosen (1965) lays the groundwork, deriving sufficient conditions for a unique Nash equilibrium in convex games. For strongly monotone games, Bravo et al. (2018); Mertikopoulos and Zhou (2019); Lin et al. (2021) achieve the convergence rate and iteration complexity of stochastic and derivative-free gradient methods. For monotone games, the convergence of such methods is established by Tatarenko and Kamgarpour (2019, 2020). Additional with bandit feedback settings, zeroth-order methods (or derivative-free methods) achieve convergence (Bravo et al., 2018; Lin et al., 2021; Drusvyatskiy et al., 2022; Narang et al., 2022), albeit with slow convergence rates (Shamir, 2013; Lin et al., 2021; Narang et al., 2022). Relaxing the convex assumption, Ratliff et al. (2016); Agarwal et al. (2019); Cotter et al. (2019) study non-convex continuous games in various settings.

Learning with kernels.Our proposed algorithm closely relies on stochastic approximation, utilizing online kernel regression for the RKHS function class. Prior research investigates the generalization capability of least squares and ridge regression in RKHS De Vito et al. (2005); Caponnetto and De Vito (2007); Smale and Zhou (2007); Rosasco et al. (2010); Mendelson and Neeman (2010). Meanwhile, extensive works study algorithms for kernel regression. For instance, Yao et al. (2007); Dieuleveut and Bach (2016); Pillaud-Vivien et al. (2018); Lin and Rosasco (2017); Lei et al. (2021) propose offline algorithms with optimal convergence rates under the RKHS norm and \(L^{2}\) norm using early stopping and stochastic gradient descent methods, while Ying and Pontil (2008); Tarres and Yao (2014); Dieuleveut and Bach (2016) design online algorithms with optimal convergence rates. The convergence of kernel regression in power norm (or Sobolev norm) is studied in Steinwart et al. (2009); Fischer and Steinwart (2020); Liu and Li (2020); Lu et al. (2022), with offline spectral filter algorithms achieving the statistical optimal rate under the power norm (Pillaud-Vivien et al., 2018; Blanchard and Mucke, 2018; Lin and Cevher, 2020; Lu et al., 2022).

**Notation.** We introduce some useful notation before proceeding. Throughout this paper, we denote the set \(1,2,,n\) by \([n]\) for any positive integer \(n\). For two positive sequences \(\{a_{n}\}_{n}\) and \(\{b_{n}\}_{n}\), we write \(a_{n}=(b_{n})\) or \(a_{n} b_{n}\) if there exists a positive constant \(C\) such that \(a_{n} C b_{n}\). For any integer \(d\), we denote the \(d\)-dimensional Euclidean space by \(^{d}\), with inner produce \(,\) and the induced norm \(\|\|=\). For a Hilbert space \(\), let \(\|\|_{}\) be the associated Hilbert norm. For a set \(\) and a probability measure \(_{}\) on \(\), let \(_{_{}}^{2}\) be the \(L^{2}\) space on \(\) induced by the measure \(_{}\), equipped with inner product \(,_{_{}}\) and \(L^{2}\) norm \(\|\|_{_{}}=}}}\). For any matrix \(A=(a_{ij})\), the Frobenius norm and the operator norm (or spectral norm) of \(A\) are \(\|A\|_{F}=(_{i,j}a_{ij}^{2})^{1/2}\) and \(\|A\|_{}=_{1}(A)\), where \(_{1}(A)\) stands for the largest singular value of \(A\). For any square matrix \(A=(a_{ij})\), denote its trace by \((A)=_{i}a_{ii}\). For any \(y^{d}\), we denote its projection onto a set \(^{d}\) by \(_{}(y)=_{x}\|x-y\|\). The set denoted by \(N_{}(x)\) represents the normal cone to a convex set \(\) at \(x\), namely, \(N_{}(x)=\{v^{d}: v,y-x 0,\; y \}\). For any metric space \(\) with metric \(d(,)\), the symbol \(()\) will denote the set of Radon probability measures \(\) on \(\) with a finite first moment \(_{z}[d(z,z_{0})]<\) for some \(z_{0}\).

## 2 Problem Formulation and Preliminaries

We briefly introduce the formulation of \(n\)-agent decision-dependent games based on Narang et al. (2022). In this setting, each agent \(i[n]\) takes the action \(x_{i}_{i}\) from an action set \(_{i}^{d_{i}}\). Define the joint action \(x(x_{1},x_{2},,x_{n})\) and the joint action set \(=_{1}_{n} ^{d}\), where \(d_{i=1}^{n}d_{i}\). For all \(i[n]\), we write \(x=(x_{i},x_{-i})\), where \(x_{-i}\) denotes the vector of all coordinates except \(x_{i}\). Let \(_{i}:\) be the utility function of agent \(i\). In the game, each agent \(i\) seeks to solve the problem

\[_{x_{i}_{i}}_{i}(x_{i},x_{-i}), _{i}(x)}_{z_{i}_{i}( x)}_{i}(x,z_{i}).\] (1)

Here \(z_{i}_{i}\) represents the data observed by agent \(i\), where the sample space \(_{i}\) is assumed to be \(_{i}=^{p}\) with \(p\) throughout this paper. Moreover, \(_{i}:(_{i})\) is the distribution map, and \(_{i}:^{d}_{i}\) denotes the loss function. During play, each agent \(i\) performs an action \(x_{i}\) and observes performative data \(z_{i}_{i}(x)\), where the performativity is modeled by the decision-dependent distribution \(_{i}(x)\). In the round \(t\), the agent \(i\) only has access to \(z_{i}^{1},,z_{i}^{t-1}\) as well as \(x^{1},,x^{t-1}\) and seeks to solve the ERM version of (1). We assume the access to the first-order oracle, namely, loss functions \(_{i}\) are known to agents but distribution maps \(_{i}\) are unknown.

**Definition 1**.: _(Nash equilibrium). In the game (1), a joint action \(x^{*}=(x_{1}^{*},x_{2}^{*},,x_{n}^{*})\) is a Nash equilibrium (Nash Jr, 1996) if all agents play the best response against other agents, namely,_

\[x_{i}^{*}=_{x_{i}_{i}}_{i}(x_{i},x_{-i}^ {*})=_{x_{i}_{i}}}_{z_{i} _{i}(x_{i},x_{-i}^{*})}_{i}(x_{i},x_{-i}^{*},z_{i}), i [n].\] (2)

In general continuous games, Nash equilibria may not exist or there might be multiple Nash equilibria (Fudenberg and Tirole, 1991). The existence and uniqueness of a Nash equilibrium in a continuous game depend on the game's structure and property. In general, finding the unique Nash equilibrium is only possible for convex and strongly monotone games (Debreu, 1952).

**Definition 2**.: _(Convex game). Game (1) is a convex game if sets \(_{i}\) are non-empty, compact, convex and utility functions \(_{i}(x_{i},x_{-i})\) are convex in \(x_{i}\) when \(x_{-i}\) are fixed._Suppose that utility functions \(_{i}\) are differentiable, we use \(_{i}_{i}(x)\) to denote the gradient of \(_{i}(x)\) with respect to \(x_{i}\) (the \(i\)-th individual gradient). We say the game (1) is \(C^{1}\)-smooth if the gradient \(_{i}_{i}(x)\) exists and is continuous for all \(i[n]\). Using this notation, we define the gradient \(H(x)\) comprised of individual gradients

\[H(x)(_{1}_{1}(x),,_{n}_{n}( x)).\]

**Definition 3**.: _(Strongly monotone game). For a constant \( 0\), a \(C^{1}\)-smooth convex game (1) is called \(\)-strongly monotone if it satisfies_

\[ H(x)-H(x^{}),x-x^{}\|x-x^{}\|^{2}, x,x^{}.\]

Note that a \(\)-strongly monotone game (\(>0\)) over a compact and convex action set \(\) admits a unique Nash equilibrium (Rosen, 1965). According to the optimal conditions in convex optimization (Boyd et al., 2004), this Nash equilibrium \(x^{*}\) is characterized by the variational inequality

\[0 H(x^{*})+N_{}(x^{*}).\] (3)

We briefly talk about the challenges and our idea of designing the algorithm. In decision-dependent games, the classical theory of risk minimization does not work. The primary obstacles to finding the Nash equilibrium in the game (1) include: (i) the distribution shift induced by performative data, and (ii) the lack of first-order information (gradient). To make it clear, standard methods, such as gradient-based algorithms, necessitate the gradient \(H(x)\). However, \(H(x)\) is unknown since distributions \(_{i}\) are unknown, and estimating \(H(x)\) is complex due to the dependency between \(_{i}(x)\) and \(x\). Mathematically, assuming \(C^{1}\)-smoothness, the chain rule directly yields the following expression for the gradient

\[_{i}_{i}(x)=}_{z_{i}_{i}(x) }_{i}_{i}(x_{i},x_{-i},z_{i})+}}_{ z_{i}_{i}(u_{i},x_{-i})}_{i}(x_{i},x_{-i},z_{i})_{u_{i}=x_{ i}},\] (4)

where \(_{i}_{i}(x,z_{i})\) denotes the gradient of \(_{i}(x,z_{i})\) with respect to \(x_{i}\). The main difficulty is estimating the second term in (4) due to the absence of closed-form expressions.

To estimate the unknown gradient \(H(x)\), we impose a parametric assumption on the observed data \(z_{i}\) and model the distribution maps \(_{i}\) using parametric functions. Note that the linear parametric assumption was first proposed in Narang et al. (2022). In this paper, we extend this assumption to a general framework and show that under the parametric assumption, the gradient \(H(x)\) has a closed-form expression, which yields an unbiased estimator for \(H(x)\).

**Assumption 1**.: _(Parametric assumption). Suppose there exists a function class \(\) and \(p\)-dimensional functions \(f_{i}:^{p}\) over the joint action set \(\) such that \(f_{i}^{p}\) and_

\[z_{i}_{i}(x) z_{i}=f_{i}(x)+_{i},  i[n],\]

_where \(_{i}^{p}\) are zero-mean noise terms with finite variance \(^{2}\), namely, \(}_{i}=0\) and \(}_{i}^{2}^{2}\)._

Under Assumption 1, assuming that \(f_{i}\) are differentiable and letting \(_{i}\) be the distribution of the noise term \(_{i}\), we derive the following expression for the utility functions \(_{i}(x)=}_{z_{i}_{i}(x)}_{i}( x,z_{i})=}_{_{i}_{i}}_{i}(x,f_{i}(x)+ _{i})\). Then the individual gradient would be \(_{i}_{i}(x)=_{i}}_{z_{i}_{i}(x)}_{i}(x,z_{i})=_{i}[}_{_{i} _{i}}_{i}(x,f_{i}(x)+_{i})]\). Consequently, the chain rule directly implies the following expression

\[_{i}_{i}(x)=}_{z_{i}_{i}(x)} _{i}_{i}(x,z_{i})+((x)}{ x_{i}} )^{}}_{z_{i}_{i}(x)}_{z_{i}} _{i}(x,z_{i}),\] (5)

where \(_{z_{i}}_{i}(x,z_{i})\) denotes the gradient of \(_{i}(x,z_{i})\) with respect to \(z_{i}\). Given a joint action \(x\), each agent \(i\) observes data \(z_{i}_{i}(x)\). Equation (5) suggests the following unbiased estimator for \(H(x)\):

\[(x)(_{i}_{i}(x))_{i [n]}=(_{i}_{i}(x,z_{i})+((x)}{  x_{i}})^{}_{z_{i}}_{i}(x,z_{i}))_{i[n]}.\] (6)

However, direct computation of \((x)\) is infeasible because \(f_{i}\) are unknown. To overcome this challenge, we approximate the unknown functions \(f_{i}\) with the function class \(^{p}\). In fact, the estimation of \(f_{i}\) can be formed as a non-parametric regression problem, namely,

\[_{i}=_{f^{p}}_{ _{i}} z_{i}-f(x)^{2}d_{i}, i[n],\] (7)

where \(_{i}\) is the joint distribution of \((x,z_{i})\) induced by \(x_{}\) and \(z_{i}_{i}(x)\). Here \(_{}\) is a user-specified sampling distribution on \(\).

## 3 The \(\) Algorithm

In this section, we consider \(\) to be the linear and kernel function classes and derive gradient-based online algorithms to find the Nash equilibrium in the game (1), namely, the Online Performance Gradient Descent (\(\)). In each iteration \(t\), assuming that \(x^{i}(x_{1}^{t},,x_{n}^{t})\) is the output of the previous iteration, \(\) performs the following update for all \(i[n]\):

1. (Estimation update). Update the estimation of \(f_{i}\) by online stochastic approximation for (7).
2. (Individual gradient update). Compute the estimator (6) and perform projected gradient steps \[x_{i}^{t+1}=_{_{i}}(x_{i}^{t}-_{t}_{ i}_{i}(x^{t})), i[n].\]

Linear Function Class.Let \(\) be the linear function class, namely, \(f_{i}(x)=A_{i}x\) for \(i[n]\), where \(A_{i}^{p d}\) are unknown matrices. Then (7) becomes the least square problem \(A_{i}=_{A^{p d}}_{(u_{i},y_{i})_{ i}}\|y_{i}-A_{i}u_{i}\|^{2}\) with random variables \(u_{i}_{},y_{i}_{i}(u_{i})\). We use the gradient of the least square objective \(\|y_{i}-A_{i}u_{i}\|^{2}\) to derive the online least square update: \(A^{} A-(Au_{i}-y_{i})u_{i}^{}\)(Dieuleveut et al., 2017; Narang et al., 2022). In each iteration \(t\), we suppose that \(A_{i}^{t-1}\) is the estimation of \(A_{i}\) from the previous iteration, \(\) samples \(u_{i}^{t}_{}\) and \(y_{i}^{t}_{i}(u_{i}^{t})\) and performs the following estimation update:

\[ A_{i}^{t}=A_{i}^{t-1}-_{t}(A_{i}^{t-1}u_{i}^{t}-y_{i}^ {t})(u_{i}^{t})^{}.\] (8)

Recalling (5), the individual gradient is \(_{i}_{i}(x)=_{z_{i}_{i}(x)}[ _{i}_{i}(x,z_{i})+A_{ii}^{}_{z_{i}}_{i}(x,z_{i})]\), where \(A_{ii}= f_{i}(x)/ x_{i}^{p d_{i}}\) denotes the submatrix of \(A_{i}\) whose columns are indexed by the agent \(i\). After step (i), \(\) draws a sample \(z_{i}^{t}_{i}(x^{t})\) and compute the estimator (6) to perform the projected gradient step:

\[ x_{i}^{t+1}=_{_{i}}(x_{i}^{t}- _{t}(_{i}_{i}(x^{t},z_{i}^{t})+(A_{ii}^{t})^{}_{z_{i} }_{i}(x^{t},z_{i}^{t}))).\] (9)

Kernel Function Class.Now we consider \(\) as the kernel function class, namely, we suppose \(f_{i}()^{p}\), where \(\) is an RKHS induced by a Mercer kernel \(K:\) and a user-specified probability measure \(_{}\). By the reproducing property of \(\), \(f_{i}\) can be represented as \(f_{i}(x)= f_{i},_{x}_{}\), where \(:\) is the feature map, i.e. \(_{x} K(,x)\) for any \(x\). Therefore, (7) becomes the kernel regression \(_{f^{p}}_{(u_{i},y_{i})_{i}}\|y_{i}-  f,_{u_{i}}_{}\|^{2}\). However, as \(\) is generally an infinite-dimensional space, the aforementioned regression problem might lead to ill-posed solutions. Consequently, we consider the regularized kernel ridge regression \(_{f^{p}}_{(u_{i},y_{i})_{i}}\|y_{i}-  f,_{u_{i}}_{}\|^{2}/2+_{t}\|f\|_{}^{2}\). In each iteration \(t\), we suppose that \(f_{i}^{t-1}\) is the estimation of \(f_{i}\) from the previous iteration, the \(\) algorithm samples \(u_{i}^{t}_{},y_{i}^{t}_{i}(u_{i}^{t})\) and takes gradient steps on the kernel ridge objective \(\|y_{i}^{t}- f,_{u_{i}^{t}}_{}\|^{2}/2+_{t} \|f\|_{}^{2}\), i.e. it takes the online kernel ridge update (Tarres and Yao, 2014; Dieuleveut and Bach, 2016):

\[ f_{i}^{t}=f_{i}^{t-1}-_{t}[(f_{i}^{t-1}(u_{i}^{t}) -y_{i}^{t})_{u_{i}^{t}}+_{t}f_{i}^{t-1}].\] (10)

We suppose that the kernel \(K\) is 2-differentiable, i.e., \(K C^{2}(,)\). Define \(_{i}:\) as the partial derivative of the feature map \(\) with respect to \(x_{i}\), namely, \(_{i}_{x}=_{i}K(x,)= K(x,)/ x_{i}\). Steinwart and Christmann (2008, Lemma 4.34) shows that \(_{i}_{x}\) exists, continuous and \(_{i}_{x}\). By the reproducing property \( f_{i}(x)/ x_{i}= f_{i},_{x}_{ }/ x_{i}= f_{i},_{i}_{x}_{}\), the individual gradient \(_{i}_{i}(x)\) has the form \(_{i}_{i}(x)=_{z_{i}_{i}(x)}[_{i} _{i}(x,z_{i})+( f_{i},_{i}_{x}_{} )^{}_{z_{i}}_{i}(x,z_{i})]\). After step (i), \(\) draws a sample \(z_{i}^{t}_{i}(x^{t})\) and performs the projected gradient step:

\[ x_{i}^{t+1}_{_{i}}(x_{i}^{t }-_{t}(_{i}_{i}(x^{t},z_{i}^{t})+( f_{i}^{t},_{i} _{x^{t}}_{})^{}_{z_{i}}_{i}(x^{t},z_{i}^{t}) )).\] (11)

We remark that the gradient steps \(_{t},_{t}\) and regularization terms \(_{t}\) should be chosen carefully to ensure convergence (see Theorem 2). Specifically, the regularization terms \(_{t}\) must shift to \(0\) gradually. If \(_{t}\) is a constant, \(f_{i}^{t}\) in (10) converges to the solution of a regularized kernel ridge regression, which is a biased estimator of \(f_{i}\). Thus (11) fails to converge because the gradient estimation has a constant bias. We present the pseudocode of \(\) for the linear setting as Algorithm 1 and for the RKHS setting as Algorithm 2 in Appendix A.

Comparison with Narang et al. (2022).We clarify the difference between \(\) and the Adaptive Gradient Method (AGM) proposed in Narang et al. (2022). To elaborate, AGM samples \(z_{i}^{t}_{i}(x^{t})\) at current the action and let agents play again with an injected noise \(u^{t}\) to obtain \(q_{i}^{t}_{i}(x^{t}+u^{t})\). The algorithm is based on the fact that \([q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]=A_{i}u^{t}\), which is not related to \(x\). Thus, \(A_{i}\) can be estimated by online least squares. We remark that \([q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]\) depends on agents' actions in the non-linear (RKHS) cases, because \([q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]=f_{i}(x^{t}+u^{t})-f_{i}(x^{t})=  f_{i},_{x^{t}+u^{t}}-_{x^{t}}_{}\). Thus, the change of action will bring additional error that makes the estimation fail to converge. In contrast, \(\) lets agents play \(u_{i}^{t}_{}\) to explore the action space and learn the strategic behavior of other agents. \(\) estimates the parametric function by solving the ERM version of (7) using online stochastic approximation (8) and (10). This learning framework can be applied to RKHS and potentially beyond that, such as overparameterized neural networks using the technique of neural tangent kernel (Allen-Zhu et al., 2019).

## 4 Theoretical Results

We provide theoretical guarantees for \(\) in both linear and RKHS settings. We first impose some mild assumptions. Similar assumptions are adopted in Mendler-Dunner et al. (2020); Izzo et al. (2021); Narang et al. (2022); Cutler et al. (2022).

**Assumption 2**.: _(\(\)-strongly monotone). The game (1) is \(\)-strongly monotone._

**Assumption 3**.: _(Smoothness). \(H(x)\) is \(L\)-Lipschitz continuous:_

\[H(x_{1})-H(x_{2}) L\|x_{1}-x_{2}\|, x_{1},x_{2} .\]

**Assumption 4**.: _(Lipschitz continuity in \(z\)). Define \(=_{1}_{2} _{n}:()\), where \(\) is the sample space \(=_{1}_{2}_{n}\). For all \(i[n],x\), there exists a constant \(>0\),_

\[}_{z(x)}^{n}_{z_ {i}}_{i}(x,z_{i})^{2}}.\]

**Assumption 5**.: _(Finite variance). There exists a constant \(>0\),_

\[}_{z_{i}_{i}(x)}_{i,z_{i}}_{ i}(x,z_{i})-}_{z_{i}_{i}(x)}_{i,z_{i}} _{i}(x,z_{i})^{2}^{2}, i[n], x ,\]

_where \(_{i,z_{i}}_{i}\) denotes the gradient of \(_{i}(x,z_{i})\) with respect to \(x_{i}\) and \(z_{i}\)._

We remark that Assumption 3 is the standard smoothness assumption for the utility functions \(_{i}(x)\)(Boyd et al., 2004; Nesterov et al., 2018). Since \(\) is a compact set within \(^{d}\), Assumption 4 holds if \(_{i}(x,z_{i})\) is Lipschitz continuous in \(z_{i}\) and the gradient \(_{z_{i}}_{i}(x,z_{i})\) is continuous in \(x\), and Assumption 5 holds if \(_{i}(x,z_{i})\) is Lipschitz in \(x\) and \(z_{i}\) (thus \(_{i,z_{i}}_{i}(x,z_{i})\) has a bounded norm). Assumption 5 implies that the variances of \(_{i}_{i}(x,z_{i})\) and \(_{z_{i}}_{i}(x,z_{i})\) are both bounded by \(^{2}\) for any \(x\) and \(z_{i}_{i}(x)\). We provide sufficient conditions for Assumption 2 in Appendix B.1.

### Convergence Rate in the Linear Setting

We introduce two assumptions necessary to derive theoretical guarantees for the linear function class.

**Assumption 6**.: _(Linear assumption). Suppose that the parametric assumption holds (Assumption 1) and \(f_{i}(x)=A_{i}x\) for \(i[n]\), where \(A_{i}^{p d}\) are unknown matrices._

**Assumption 7**.: _(Sufficiently isotropic). There exists constants \(l_{1},l_{2},R>0\) such that_

\[l_{1}I_{u_{}}uu^{},_{u _{}} u^{2} l_{2},_{u _{}}[ u^{2}uu^{}] R_ {u_{}}uu^{}.\]

Assumption 7 has been studied in the literature on online least squares regression (Dieuleveut et al., 2017; Narang et al., 2022). Essentially, this requires the distribution \(_{}\) to be sufficiently isotropic and non-singular, and it ensures the random variable \(u_{i}^{t}_{}\) in the online estimation update step (8) can explore all the "directions" of \(^{p}\). A simple example that satisfies Assumption 7 is the uniform distribution \(_{}=\), in which case \(l_{1}=l_{2}=1/3,R=3/5\).

The next theorem provides the convergence rate of \(\) under the linear setting.

**Theorem 1**.: _(Convergence in the linear setting). Suppose that Assumptions 2, 3, 4, 5, 6, and 7 hold. Set \(_{t}=2/((t+t_{0}))\), \(_{t}=2/(l_{1}(t+t_{0}))\), where \(t_{0}\) is a constant that satisfies \(t_{0} 2l_{2}R/l_{1}^{2}\). For all iterations \(t 1\), the \(x^{t}\) generated by the OPGD algorithm in Section 3 for linear function class satisfies_

\[\|x^{t}-x^{*}\|^{2} +2D_{2}(t_{0}+1))(t_{0}+2)^{2}/(t_{0}+1)^{2}}{ ^{2}(t+t_{0})}++1)^{2}\|x^{1}-x^{*}\|^{2}}{(t+t_{0})^{2}},\] (12)

_where \(D_{1}\) and \(D_{2}\) are constants that_

\[D_{1} 4^{2}(1+2(M/(t_{0}+1)+_{i[n]}\|A_{i} \|_{F}^{2})), D_{2} 2^{2}M, M^{4}_{i=1}^{n}\|A_{i}^{0}-A_{i}\|_{F}^{2}}{(t _{0}+1)^{3}}+^{2}(t_{0}+2)^{2}}{l_{1}^{2}(t_{0}+1)^{2}}.\]

We refer the reader to Appendix C.1 for complete proof. Next, we illustrate the parameters involved in Theorem 1: \(\) is the strongly monotone parameter of the game (1), \(l_{1},l_{2},R\) are intrinsic parameters describing the isotropy of the distribution \(_{}\) (Assumption 7), \(^{2}\) is the variance of the noise term \(_{i}\) defined in Assumption 1, \(\) and \(\) describe the continuity of \(_{i}\) (Assumption 4, 5), \(t_{0}\) is a sufficiently large value, \(A_{i}^{0}\) is the initial estimation of \(A_{i}\), \(x^{1}\) is the initial input. Theorem 1 is a combination of Lemma 2 and Lemma 3, where Lemma 2 is the statistical error of the online approximation step (8) and Lemma 3 is the one-step optimization error of the projected gradient step (9). Theorem 1 implies the convergence rate of OPGD in the linear setting is \((t^{-1})\), which matches the optimal rate of stochastic gradient descent in the strongly-convex setting.

### Convergence Rate in the RKHS Setting

Suppose that \(K:\) is a continuous Mercer kernel, by Mercer's theorem, it has the spectral representation \(K=_{i=1}^{}_{i}e_{i} e_{i}\), where \(\{_{i}\}_{i=1}^{}\) are eigenvalues, \(\{e_{i}\}_{i=1}^{}\) are eigenfunctions, and \(\) denotes the tensor product. Moreover, \(\{e_{i}\}_{i=1}^{}\) is an orthogonal basis of \(_{_{}}^{2}\) and \(\{_{i}^{1/2}e_{i}\}_{i=1}^{}\) is the orthogonal basis of \(\), which induces the representation \(=\{_{i=1}^{}a_{i}_{i}^{1/2}e_{i}:\{a_{i}\}_{i=1}^{ }^{2}\}\).

**Definition 4**.: _(Power space). For a constant \( 0\), the \(\)-power space of an RKHS \(\) is defined by_

\[^{}=\{_{i=1}^{}a_{i}_{i}^{/2}e_{i}:\{ a_{i}\}_{i=1}^{}^{2}\},\]

_equipped with the \(\)-power norm \(\|\|_{}\) and inner product \(,_{}\), where \(\|_{i=1}^{}a_{i}_{i}^{/2}e_{i}\|_{}( _{i=1}^{}a_{i}^{2})^{1/2}\) and \(_{i=1}^{}a_{i}_{i}^{/2}e_{i},_{i=1}^{}b_ {i}_{i}^{/2}e_{i}_{}=_{i=1}^{}a_{i}b_{i}\)._

We remark that: (i) \(^{1}=\) and \(^{}^{}\) for any \(>\), (ii) \(\|\|_{1}=\|\|_{}\) and \(\|\|_{0}=\|\|_{_{}}\), and (iii) \(^{}\) is an RKHS on \(\) with kernel \(K^{}_{i=1}^{}_{i}^{}e_{i} e_{i}\) and measure \(_{}\). We review more properties of RKHS and power spaces in Appendix Sections B.3 and B.4.

We present assumptions on the kernel function class, similar assumptions can be found in the literature on kernel regression and stochastic approximation (Caponnetto and De Vito, 2007; Steinwart et al., 2009; Dicker et al., 2017; Pillaud-Vivien et al., 2018; Fischer and Steinwart, 2020).

**Assumption 8**.: _(Source condition). Suppose Assumption 1 holds and there exists an RKHS, \(\), with a bounded differentiable Mercer kernel, \(K\), and constants \(,>0\) such that \(_{x}K(x,x)^{2}\) and \(f_{i}(^{})^{p}\) for all \(i[n]\)._

**Assumption 9**.: _(Embedding property). There exist constants \((0,1],A>0\) such that \(K^{}(x,x)=_{i=1}^{}_{i}^{}e_{i}^{2}(x) A^{2}\), for all \(x\)._

**Assumption 10**.: _(Lipschitz kernel). Suppose Assumption 9 holds and there exists \(>0\) such that \(\|_{i}_{x}^{}\|_{}\) for any \(i[n]\) and \(x\), where \(_{x}^{}:^{}\) is the feature map of the kernel \(K^{}\)._

Assumption 8 holds when \(K\) is bounded, differentiable, and each coordinate of parametric functions \(f_{i}\) lies in the power space \(^{}\). When \(<1\), Assumption 8 includes the challenging scenario, namely, \(f_{i}()^{p}\). Assumption 9 holds if there exists a power space \(^{}\) such that the kernel \(K^{}\) is bounded. Thus, Assumption 9 holds with \(=1\) for any bounded kernel \(K\). We further propose Proposition 1 as sufficient conditions for the embedding property following Mendelson and Neeman (2010). Recalling the definition of partial derivative \(_{i}^{}:^{}\) (Section 3), Assumption 10 holds if \(_{i}_{i+d}K^{}(x,x)=\|_{i}_{x}^{}\|_{ }^{2}^{2}\) for any \(x\), i.e. it holds for any Lipschitz kernel \(K^{}\).

[MISSING_PAGE_FAIL:8]

Linear function class.Let the loss function be \(_{i}(x,z_{i})=-z_{i}+x_{i}^{2}\) and set the linear parametric function as \(f_{1}(x)=x_{1}\) and \(f_{2}(x)=2x_{2}\), namely, the parametric model is \(z_{i}=A_{i}x+_{i}\) where \(A_{1}=[1\ 0]\) and \(A_{2}=[0\ 2]\). The the game (14) has the gradient \(H(x)=(2x_{1}-1,2x_{2}-2)\), therefore, the game (14) is convex, \(C^{1}\)-smooth, \(1\)-strongly monotone and the Nash equilibrium is \(x^{*}=(1/2,1)\). We set the sampling distribution as \(_{}=\), the initial point as \(x^{0}_{}\), and the initial estimation as zero. Moreover, letting \(t_{0}=10\), we set the gradient step sizes as \(_{t}=6/(t+t_{0}),_{t}=6/(t+t_{0})\).

Kernel function class.Let \(=\), \(_{}=\), and define the kernel \(Q((x_{1},x_{2}),(y_{1},y_{2}))=K(x_{1},y_{1}) K(x_{2},y_{2})\) as the product kernel of \(K(x,y)=40B_{4}(\{x-y\})\). Suppose that \(\) is the RKHS on \(\) induced by the kernel \(Q\) and the distribution \(_{}\). Set the parametric function as the product of two \(3\)-order Bernoulli polynomials, namely, \(f(x_{1},x_{2})=B_{3}(x_{1}) B_{3}(x_{2})=(x_{1}^{3}-3x_{1}^{2}/2+x_{1}/2) (x_{2}^{3}-3x_{2}^{2}/2+x_{2}/2)\). Set \(_{i}(x,z_{i})=-z_{i}+(2 x_{1})(2 x_{2})-x_{i}+x_{i}^{2}\) and let \(f_{i}(x)=(2 x_{1})(2 x_{2})\) for \(i\). Then the gradient of this game is \(H(x)=(2x_{1}-1,2x_{2}-1)\), thus, this game is convex, \(C^{1}\)-smooth, \(1\)-strongly monotone and the Nash equilibrium is \(x^{*}=(0.5,0.5)\). Following Example 1, Assumption 8, 9, 10 hold for any \(>1\) and any \(>1/4\). Set \(t_{0}=10\), \(a=7\), \(_{t}=6/(t+t_{0}),_{t}=a/(t+t_{0})^{3/4}\), and \(_{t}=1/(a(t+t_{0})^{1/4})\). Following Theorem 2, the convergence rate is \((t^{-1/2})\).

Results.We perform experiments for both parametric settings to verify the convergence rates and compare the theoretical and simulated rates, as shown in Figure 1, where both X and Y axes take the log scale. Figure 1(a) shows the converge rate of the linear setting within \(10,000\) iterations, the simulated rate matches our prediction, i.e. it is close to \((t^{-1})\). Figure 1(b) shows the convergence rate of the RKHS setting, it implies that the simulated rate is close to the theoretical rate \((t^{-1/2})\) when the iteration \(t\) is larger than \(1,000\). These results validate Theorems 1 and 2.

## 6 Conclusion and Discussion

In this paper, we study the problem of learning Nash equilibria in multi-agent decision-dependent games with access to the first-order oracle. We propose a parametric assumption to handle the distribution shift and develop a novel online algorithm OPGD  in both the linear and RKHS settings. We derive sufficient conditions to ensure the decision-dependent game is strongly monotone under the parametric assumption. We show that OPGD  converges to the Nash equilibrium at a rate of \((t^{-1})\) in the linear setting and \((t^{-})\) in the RKHS setting.