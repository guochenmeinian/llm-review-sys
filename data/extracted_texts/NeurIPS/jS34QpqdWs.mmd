# Nonstationary Sparse Spectral Permanental Process

Zicheng Sun\({}^{1}\), Yixuan Zhang\({}^{2}\), Zenan Ling\({}^{3}\), Xuhui Fan\({}^{4}\) Feng Zhou\({}^{1,5}\)

\({}^{1}\)Center for Applied Statistics and School of Statistics, Renmin University of China

\({}^{2}\)School of Statistics and Data Science, Southeast University

\({}^{3}\)School of EIC, Huazhong University of Science and Technology

\({}^{4}\)School of Computing, Macquarie University

\({}^{5}\)Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing

{sunzicheng2020, feng.zhou}@ruc.edu.cn, zhixuan@hotmail.com

Corresponding author.Equal contributions.

###### Abstract

Existing permanental processes often impose constraints on kernel types or stationarity, limiting the model's expressiveness. To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of non-stationary kernels. This technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level. Additionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach, particularly in scenarios with pronounced data nonstationarity. Additionally, ablation studies are conducted to provide insights into the impact of various hyperparameters on model performance. Code is publicly available at https://github.com/SZC20/DNSSPP.

## 1 Introduction

Many application domains involve point process data, which records the times or locations of events occurring within a region. Point process models are used to analyze these event data, aiming to uncover patterns of event occurrences. The Poisson process , as an important model in the field of point processes, plays a significant role in neuroscience , finance , criminology , epidemiology , and seismology . Traditional Poisson processes assume parameterized intensity functions, which severely restricts the flexibility of the model. To address this issue, a viable solution is to employ Bayesian nonparametric methods, by imposing a nonparametric Gaussian process (GP) prior on the intensity function, resulting in the Gaussian Cox process . This greatly enhances the flexibility of the model, while also endowing it with uncertainty quantification capabilities.

In Gaussian Cox processes, posterior inference of the intensity is challenging because the GP prior is not conjugate to the Poisson process likelihood that includes an intensity integral. Furthermore, to ensure the non-negativity of the intensity, we need to use a link function to transform the GP prior. Commonly used link functions include exponential [20; 12], sigmoid [1; 9; 6], square [18; 7; 29], ReLU , and softplus . Among these, using the square link function, i.e., the permanental process , allows for the analytical computation of the intensity integral [18; 33; 29], thus receiving widespread attention in recent years. For this reason, this work focuses on the permanental process.

Currently, the permanental process faces three issues: **(1)** The permanental process inherits the notorious cubic computational complexity of GPs, making it impractical for use with a large amountof data [18; 14]. **(2)** Existing works on permanental processes either require certain standard types of kernels, such as the squared exponential kernel, etc., to ensure that the intensity integral has an analytical solution [18; 7; 33]; or they require the kernels to be stationary [14; 29]. These constraints limit the expressive power of the model. **(3)** Furthermore, existing works have predominantly utilized simple shallow kernels, which restricts the flexibility of the model. Although some shallow kernels [34; 28] are flexible and theoretically capable of approximating any bounded kernel, they are limited in representing complex kernels due to computational constraints in practical usage.

In this study, we utilize the sparse spectral representation of nonstationary kernels to address these limitations in modeling the permanental process. The sparse spectral representation provides a low-rank approximation of the kernel, effectively reducing the computational complexity from cubic to linear level. The nonstationary sparse spectral kernel overcomes the limitation of stationary assumption. By treating the frequencies as kernel parameters for optimization, we can directly learn the nonstationary kernel from the data in a flexible manner without restricting the kernel's form. We further extend the shallow nonstationary sparse spectral kernel by hierarchically stacking multiple spectral feature mappings to construct a deep kernel, which exhibits significantly enhanced expressive power compared to shallow ones. We term the constructed model as Nonstationary Sparse Spectral Permanental Process (NSSPP) and the corresponding deep kernel variant as DNSSPP.

We conduct experiments on both synthetic and real-world datasets. The results indicate that when the data is (approximately) stationary, (D)NSSPP achieves similar performance to stationary baselines. However, when the nonstationarity in the data is pronounced, (D)NSSPP can outperform baseline models, demonstrating the superiority of (D)NSSPP. Additionally, we perform ablation studies to assess the impact of various model hyperparameters.

## 2 Related Work

In this section, we present some related works on how to reduce the computational complexity of the stationary permanental process, as well as some research on nonstationary kernels.

Efficient Permanental ProcessDue to the adoption of the GP prior in the permanental process, posterior inference involves computing the inverse of the kernel matrix. This results in a computational complexity of \((N^{3})\), where \(N\) is the number of data points. To reduce computational complexity, several works have introduced low-rank approximation methods from the GP domain into the permanental process, such as inducing points , Nystrom approximation [7; 33], and spectral representation [14; 29]. These methods essentially involve low-rank approximations of the kernel matrix, thereby reducing the computational complexity from \((N^{3})\) to \((NR^{2})\), where \(R N\) is the number of rank, such as the number of inducing points or frequencies. In this work, we utilize the spectral representation method, reducing the computational complexity from cubic to linear level. Another reason for adopting the spectral representation is that it facilitates the construction of the subsequent deep nonstationary kernel.

Nonstationary KernelStationary kernels assume that the similarity between different locations depends only on their relative distance. However, previous studies have indicated that the stationary assumption is not suitable for dynamic complex tasks, as the similarity is not consistent across the input space . To address this issue, some works proposed nonstationary kernels by transforming the input space  or using input-dependent parameters . Additionally, other works leveraged the generalized Fourier transform of kernels  to propose nonstationary spectral kernels [26; 32]. Although the aforementioned shallow nonstationary kernels are flexible and theoretically capable of approximating any bounded kernels, they are limited in representing complex kernels in practical usage. In recent years, many deep architectures have been introduced into kernels, significantly enhancing their expressive power [35; 36]. Despite the development of (deep) nonstationary kernels in GP, interestingly, to the best of our knowledge, there has been no prior work applying them in Gaussian Cox processes. This gap is what this work seeks to address.

## 3 Preliminaries

In this section, we provide some fundamental knowledge about the permanental process and sparse spectral kernel.

### Permanental Process

For a Poisson process, if \(^{D}\), the intensity function \(()=_{_{} 0}[N([, +_{}])]/_{}\) can be used to quantify the rate of events occurring at location \(\). A Cox process can be viewed as a Poisson process whose intensity function itself is a random process. The Gaussian Cox process employs the GP to model the intensity function: \(()=l f()\) where \(f\) is a GP function and \(l:^{+}\) is a link function to ensure the non-negativity of the intensity function.

The nonparametric nature of GP enables the Gaussian Cox process to be flexible in modeling events in space or time. However, the posterior inference of Gaussian Cox process is challenging. According to the likelihood function of a Poisson process:

\[p(\{_{i}\}_{i=1}^{N}|()=l f( ))=_{i=1}^{N}(_{i})(-_{}()d),\] (1)

and the Bayesian framework, the posterior of latent function \(f\) can be written as:

\[p(f|\{_{i}\}_{i=1}^{N})=^{N}( _{i})(-_{}()d) (f|0,k)}{_{i=1}^{N}(_{i})(-_{ }()d)(f|0,k)df}.\] (2)

In Eq. (2), there are two integrals. The first one \(_{}()d\) cannot be solved due to the stochastic nature of \(f\). Additionally, the integral over \(f\) in the denominator is a functional integral in the function space, which is also infeasible. This issue is the well-known doubly-intractable problem.

The link function \(l:^{+}\) has many choices, such as exponential, sigmoid, square, ReLU, and softplus. This work focuses on the utilization of the square link function, also known as the permanental process. Furthermore, if we set the kernel in the GP prior to be stationary: \(k(_{1},_{2})=k(_{1}-_{2})\), then the model is referred to as a stationary permanental process.

### Sparse Spectral Kernel

The sparse spectral representation is a common method for the low-rank approximation of kernels. This approach is based on Bochner's theorem.

**Theorem 3.1**.: _[_2_]_ _A stationary kernel function \(k(_{1},_{2})=k(_{1}-_{2}):^ {D}\) is bounded, continuous, and positive definite if and only if it can be represented as:_

\[k(_{1}-_{2})=_{^{D}}(i^{}(_{1}-_{2}))d(),\]

_where \(()\) is a bounded non-negative measure associated to the spectral density \(p()=)}{(^{D})}\)._

Defining \(()=(i^{})\), we have \(k(_{1}-_{2})=^{2}_{p()} [(_{1})(_{2})^{*}]\) where \(*\) denotes the complex conjugate and \(^{2}=(^{D})\). If we utilize the Monte Carlo to sample \(\{_{r}\}_{r=1}^{R}\) independently from \(p()\), then the kernel can be approximated by

\[k(_{1}-_{2})}{R}_{r=1}^{R} _{r}(_{1})_{r}(_{2})^{*}=^{(R)}(_{1})^{ }^{(R)}(_{2})^{*},\] (3)

where \(_{r}()=(i_{r}^{})\), \(^{(R)}()=}[_{1}(),, _{R}()]^{}\), which corresponds to the random Fourier features (RFF) proposed by . It can be proved that Eq. (3) is equivalent to a 2\(R\)-sized trigonometric representation (proof is provided in Appendix A):

\[^{(R)}()=}[(_{1}^{ }),,(_{R}^{}),( _{1}^{}),,(_{R}^ {})]^{}.\] (4)

When we specify the functional form of the kernel (spectral measure \(\)), the RFF method can learn the parameters of the kernel (spectral measure \(\)) from the data. However, when the functional form of the kernel (spectral measure \(\)) is unknown, RFF can not flexibly learn the kernel (spectral measure \(\)) from the data. Another approach is to treat the frequencies \(\{_{r}\}_{r=1}^{R}\) as kernel parameters. By optimizing these frequencies, we can directly learn the kernel from the data in a flexible manner, which corresponds to the sparse spectrum kernel method proposed by .

Our Model

The permanental process inherits the cubic computational complexity of GPs, rendering it impractical for large-scale datasets. To efficiently conduct posterior inference, many works employed various low-rank approximation methods [18; 7; 33]. These methods require certain standard types of kernels, such as the squared exponential kernel, polynomial kernel, etc., to ensure that the intensity integral has analytical solutions. This limitation severely affects the flexibility of the model. To address this issue, sparse spectral permanental processes are proposed in recent years [14; 29]. The advantage of this method lies in its ability to analytically compute the intensity integral, while not requiring restrictions on the kernel's functional form. However, the drawback is that it is only applicable to stationary kernels. Therefore, a natural question arises: can we further generalize the sparse spectral permanental processes to not only remove restrictions on the kernel's functional form but also make them applicable to nonstationary kernels?

### Nonstationary Sparse Spectral Permanental Process

Existing sparse spectral permanental processes rely on the prerequisite of Bochner's theorem, which limits the extension of the sparse spectral method to nonstationary kernels. Fortunately, for more general nonstationary kernels, the spectral representation similar to Bochner's theorem also exists.

**Theorem 4.1**.: _[_37_]_ _A nonstationary kernel function \(k(_{1},_{2}):^{D}^{D} \) is bounded, continuous, and positive definite if and only if it can be represented as:_

\[k(_{1},_{2})=_{^{D}^{D}} i(_{1}^{}_{1}-_{2}^{} _{2})d(_{1},_{2}),\]

_where \((_{1},_{2})\) is a Lebesgue-Stieltjes measure associated to some bounded positive semi-definite spectral density \(p(_{1},_{2})=_{1},_{2}) }{(^{D},^{D})}\)._

Similarly, we can use the Monte Carlo method to approximate it. However, it is worth noting that, to ensure that the spectral density \(p(_{1},_{2})\) is positive semi-definite, we must first symmetrize it, i.e., \(p(_{1},_{2})=p(_{2},_{1})\); and secondly introduce diagonal components \(p(_{1},_{1})\) and \(p(_{2},_{2})\). We can take a general density function \(g\) on the product space and then parameterize \(p(_{1},_{2})\) in the following form to ensure its positive semi-definite property [26; 32]:

\[p(_{1},_{2})=(g(_{1},_ {2})+g(_{2},_{1})+g(_{1},_{1})+g (_{2},_{2})).\] (5)

Then, we can obtain the sparse spectral feature representation of any nonstationary kernel:

\[k(_{1},_{2})=}{4} _{g(_{1},_{2})}[i(_{1}^{ }_{1}-_{2}^{}_{2})+i(_{2}^{}_{1}-_{1}^{}_{2}).\] \[+i(_{1}^{}_{1}-_ {1}^{}_{2})+i(_{2}^{} _{1}-_{2}^{}_{2})]\] \[}{4R}_{r=1}^{R}i( _{1r}^{}_{1}-_{2r}^{}_{2}) ++i(_{2r}^{}_{1}- _{2r}^{}_{2})\] (6) \[=_{1}^{(R)}(_{1})^{}_{2}^{(R)}(_{ 2})+_{2}^{(R)}(_{1})^{}_{1}^{(R)}(_{2})+_ {1}^{(R)}(_{1})^{}_{1}^{(R)}(_{2})+_{2}^{(R)} (_{1})^{}_{2}^{(R)}(_{2})\] \[=^{(R)}(_{1})^{}^{(R)}( _{2}),\]

where \(^{2}=(^{D},^{D})\), \(\{_{1r},_{2r}\}_{r=1}^{R}\) are independent samples from \(g(_{1},_{2})\), and

\[_{1}^{(R)}() =}[(_{11}^{}),,(_{1R}^{}),(_{11}^{} ),,(_{1R}^{})]^{},\] \[_{2}^{(R)}() =}[(_{21}^{}),,(_{2R}^{}),(_{21}^{} ),,(_{2R}^{})]^{},\] (7) \[^{(R)}() =_{1}^{(R)}()+_{2}^{(R)}().\]

If the GP's kernel in the permanental process (Eq. (2)) adopts the above sparse spectral nonstationary kernel, then we refer to this model as NSSPP.

### Deep Nonstationary Sparse Spectral Permanental Process

We can further enhance the expressive power of the nonstationary kernel by stacking multiple spectral feature mappings hierarchically to construct a deep kernel. The spectral feature mapping \(^{(R)}()\) has another equivalent form:

\[^{(R)}()=}[(_{11}^{ }+b_{11})+(_{21}^{}+b_{21}),, (_{1R}^{}+b_{1R})+(_{2R}^{} +b_{2R})]^{},\] (8)

where \(\{b_{1r},b_{2r}\}_{r=1}^{R}\) are uniformly sampled from \([0,2]\). The derivation of Eq. (8) is provided in Appendix B.

The spectral feature mapping in Eq. (8) can be viewed as a more complex single-layer neural network: it employs two sets of weights (\(\)) and biases (\(b\)) to linearly transform the input, followed by the application of cosine activation functions, and then adds the results to obtain a single output dimension. By stacking multiple layers on top of each other, we naturally construct a deep nonstationary kernel:

\[^{(R)}()=_{L}^{(R)}_{L-1}^{(R)} _{1}^{(R)}(),\ \ \ \ k(_{1},_{2})^{(R)}(_{1})^{} ^{(R)}(_{2}),\] (9)

where \(^{(R)}()\) is the spectral feature mapping with \(L\) layers and \(_{l}^{(R)}\) denotes the \(l\)-th layer8. This actually recovers the deep spectral kernel proposed by . If the GP's kernel in the permanental process (Eq. (2)) adopts the above deep sparse spectral nonstationary kernel, then we refer to this model as DNSSPP.

DNSSPP exhibits enhanced expressiveness compared to NSSPP due to its deep architecture. In subsequent sections, we consistently use \(^{(R)}\) as the spectral feature mapping. When it has a single layer, the model corresponds to NSSPP; when multiple layers, the model corresponds to DNSSPP.

## 5 Inference

For the posterior inference of (D)NSSPP, this work employs a fast Laplace approximation exploiting the sparse spectral representation of nonstationary kernels. Other inference methods, such as variational inference, are also feasible. Specifically, we extend the method proposed in  from stationary kernels to nonstationary kernels.

### Joint Distribution

When we approximate a kernel using Eq. (6) or Eq. (9), the GP prior can also be approximated as:

\[f()^{}^{(R)}(),\ \ \ (,).\] (10)

Additionally, we set the intensity function as \(()=f()+^{2}\). Following , we introduce an offset \(\) to mitigate the nodal line problem caused by the non-injectiveness of \(=f^{2}\). This issue results in the intensity between positive and negative values of \(f\) being forced to zero, despite the underlying intensity being positive.

Combing the likelihood in Eq. (1) and the equivalent GP prior in Eq. (10), we obtain the joint density:

\[ p(\{_{i}\}_{i=1}^{N}, |)&= p(\{_{i}\}_{i=1}^{N }|,)+ p()\\ &=_{i=1}^{N}((^{}^{(R)}(_{i})+ )^{2})-_{}()d-^{}+C,\] (11)

where \(C\) is a constant and \(\) denotes the model hyperparameters. The intensity integral term can be further expressed as:

\[_{}()d=^{} +2^{}+^{2}||,\] (12)where \(||\) is the window size, \(\) is a \(R R\) matrix, and \(\) is a \(R\)-sized vector with each entry:

\[_{i,j}=_{}_{i}^{(R)}()_{j}^{(R)}( )d,_{i}=_{}_{i}^{(R)}( )d, i,j 1,,R.\] (13)

It is worth noting that the intensity integral is potentially analytically solvable. For a single-layer spectral feature mapping (NSSPP), analytical solutions for \(\) and \(\) exist (see Appendix C). However, for multiple-layer mapping (DNSSPP), analytical solutions are not possible. In the following, we analytically solve \(\) and \(\) for NSSPP, while using numerical integration for DNSSPP.

### Laplace Approximation

We use the Laplace's method to approximate the posterior \(p(|\{_{i}\}_{i=1}^{N},)\). The Laplace's method approximates the posterior with a Gaussian distribution by performing a second-order Taylor expansion around the maximum of the log posterior. Following the common practice:

\[ p(|\{_{i}\},)( |},)=-( -})^{}^{-1}( -})- - 2,\] (14)

where \(}\) is the mode of the true posterior and \(\) is the negative inverse Hessian of the true posterior evaluated at the mode:

\[_{} p(\{_{i}\}_{i=1}^{N}, |)_{= }}=0,^{-1}=-_{}^{2} p(\{ _{i}\}_{i=1}^{N},|)_{ =}}.\] (15)

The mode \(}\) is typically not directly solvable based on Eq. (15), so we employ optimization methods to find it. Then, the precision matrix \(^{-1}\) can be computed analytically (see Appendix D).

### Hyperparameter and Complexity

The model hyperparameter \(\) consists of the kernel parameters \(,,\) (if there are multiple layers, these hyperparameters include parameters from all layers) and the bias of the intensity function \(\). We can learn these hyperparameters from the data by maximizing the marginal likelihood:

\[ p(\{_{i}\}_{i=1}^{N}|)\] \[=  p(\{_{i}\}_{i=1}^{N},|)- p(|\{_{i}\}_{i=1}^{N}, ) p(\{_{i}\}_{i=1}^{N},|)-(|},).\] (16)

Substituting Eqs. (11) and (14) into Eq. (16), we can obtain a marginal likelihood approximation. Optimizing this yields the optimal hyperparameter \(\).

Similar to common low-rank approximation methods for kernels, the computational complexity of the proposed inference method is reduced from \(O(N^{3})\) to \(O(NR^{2})\), where \(R N\), i.e., linearly with \(N\). A complete algorithm pseudocode is provided in Appendix E.

### Predictive Distribution

After obtaining the posterior \(|\{_{i}\}_{i=1}^{N}, (|},)\), we can compute the posterior of the intensity function. For any \(^{*}\), the predictive distribution of \(f(^{*})+\) is

\[f(^{*})+|\{_{i}\}_{i=1}^{N}, ((^{*}),^{2}(^{*})),\]

where

\[(^{*})=}^{}^{(R)}(^{*}) +,^{2}(^{*})=^{(R)}(^{*})^{} ^{(R)}(^{*}).\]

Then, the intensity \((^{*})=(f(^{*})+)^{2}\) can be proven to have the following mean and variance:

\[[(^{*})]=^{2}(^{*})+^{2}( ^{*}),[(^{*})]=2^{4}(^{*})+4^{2}(^{*})^{2}(^{*}).\] (17)

## 6 Experiments

In this section, we mainly analyze the superiority in performance of (D)NSSPP over baseline models on both synthetic and real-world datasets, as well as the impact of various hyperparameters. We perform all experiments using the server with two GPUs (NVIDIA TITAN V with 12GB memory), two CPUs (each with 8 cores, Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz), and 251GB memory.

### Baselines

We employ several previous works as baselines for comparison. These baselines include **VBPP** which utilizes the inducing points method , **LBPP** employing the Nystrom approximation , as well as **SSPP** and **GSSPP** utilizing the Fourier representation . We use GSSPP based on Gaussian kernel, Matern kernel with parameter 1/2 and 5/2 respectively, and we denote them as GSSPP, GSSPP-M12 and GSSPP-M52. Besides, we offer a baseline that employs stacked mappings of stationary kernels which we name it Deep Sparse Spectral Permanental Process (**DSSPP**). All of these methods utilize stationary kernels. As far as we know, there is currently no work employing nonstationary kernels in permanental processes. To compare with nonstationary baselines, we implement a nonstationary permanental process using the nonstationary spectral mixture kernel , referred to as **NSMPP**.

### Metrics

We employ two metrics to evaluate the performance of various baselines. One is the expected log-likelihood on the test data, denoted as \(}_{}\), with details provided in Appendix F. Additionally, when the ground-truth intensity is available, e.g., for the synthetic data, the root mean square error (**RMSE**) between the expected posterior intensity and the ground truth serves as another measure.

### Synthetic Data

DatasetsTo better compare the performance of (D)NSPP with baseline models on point process data in both stationary and nonstationary scenarios, we simulated a stationary and a nonstationary permanental process on the interval \(\), respectively. Specifically, we assume two kernels:

\[k_{1}(_{1},_{2})=(-\|_{1}- _{2}\|^{2}), k_{2}(_{1},_{2})=(_{1}^{}_{2}}{100}+1)^{3}(-\|_{1}-_{2}\|^{2}),\]

where \(k_{1}\) is a stationary Gaussian kernel and \(k_{2}\) is a nonstationary kernel obtained by multiplying a Gaussian kernel with a polynomial kernel. We use these two kernels to construct a stationary GP and a nonstationary GP, respectively, and randomly sample two corresponding latent functions. Based on \(()=(f()+2)^{2}\), we construct two corresponding intensity functions. Finally, we use the thinning algorithm  to simulate two sets of synthetic data. For each intensity, we simulate ten datasets and use each dataset alternately as the training set and the remaining ones as the test sets.

SetupFor NSSPP, the number of frequencies (network width) is set to 50. For DNSSPP, we experimented with three different configurations: DNSSPP-, DNSSPP-, and DNSSPP-. Each number represents the width of the corresponding network layer. Therefore, DNSSPP- implies a model with two layers, where the first layer has a width of 50 and the second layer has a width of 30, and so on. For baseline models, we need to set the number of rank for kernel low-rank approximation. We set the number of inducing points to 50 for VBPP, the number of eigenvalues to 50 for LBPP, and the number of frequencies to 50 for SSPP, GSSPP, and NSMPP. For DSSPP, to facilitate comparison with DNSSPP, we adopt the same layer configurations.

ResultsThe fitting results of intensity functions for all models are depicted in Fig. 1. To quantitatively compare the fitting performance of different methods in both stationary and nonstationary

Figure 1: (a) The fitting results of the intensity functions for all models on the stationary synthetic data; (b) those on the nonstationary synthetic data. The impact of (c) network width and depth, (d) the number of epochs and learning rate on the \(_{}\) of DNSSPP on the nonstationary data.

scenarios, we employ \(_{}\) and RMSE as metrics. The comparison results are presented in Table 1. DNSSPP with three different configurations exhibits significant advantages on both stationary and nonstationary datasets. NSSPP and NSSPP show better performance in the nonstationary scenario compared to most stationary baselines. This is reasonable, as stationary baselines face challenges in accurately modeling nonstationary data. Because stationary kernels are a subset of nonstationary kernels, nonstationary models typically perform well on stationary data as well. Compared to relatively shallow models, DNSSPP achieves better performance but also incurs a longer running time.

### Real Data

DatasetsIn this section, we use three sets of common real-world datasets to evaluate the performance of our method and the baselines. **Coal Mining Disaster**: This is a 1-dimensional dataset containing 191 incidents that occurred between March 15, 1875, and March 22, 1962. Each event represents a mining accident that killed more than 10 people. **Redwoods**: This is a 2-dimensional dataset that describes the distribution of redwoods in an area, consisting of 195 events. **Porto Taxi**: This is a large 2-dimensional dataset containing the tracks of 7,000 taxis in Porto during 2013/2014. We focus on the area with coordinates between \((41.147,-8.58)\) and \((41.18,-8.65)\), and extract 3,000 pick-up points as our dataset. For each dataset, we randomly partition the data into training set and test set of approximately equal size.

SetupFor real datasets, because we do not know the ground-truth intensity, we use \(_{}\) as the evaluation metric. Since NSSPP generally performs worse than DNSSPP, we report only the results of DNSSPP in this section. For DNSSPP, we consistently use the same three configurations as in the synthetic data. For baseline models, we choose different numbers of rank for kernel low-rank approximation for 1-dimensional and 2-dimensional datasets. Specifically, for the 1-dimensional dataset (Coal), we set the number of rank, i.e., the number of inducing points for VBPP, the number

    &  &  \\   & \(_{}\) & RMSE & Runtime(s) & \(_{}\) & RMSE & Runtime(s) \\  DNSSPP- & 156.43\(\) 37.52 & 0.061\(\) 0.020 & 9.27 & **175.70\(\) 26.89** & **0.076\(\) 0.015** & 9.40 \\ DNSSPP- & **156.55\(\) 37.44** & 0.061\(\) 0.017 & 9.00 & 173.68\(\) (26.70) & 0.094\(\) 0.012) & 9.11 \\ DNSSPP- & 156.55\(\) 37.66 & 0.068\(\) 0.017 & 10.32 & 174.47\(\) (26.57) & 0.086\(\) 0.013) & 10.07 \\ NSSPP & 153.36\(\) 36.82 & 0.070\(\) 0.009 & 8.39 & 172.87\(\) (26.86) & 0.10\(\) 0.012) & 8.96 \\ NSSMP & 153.05\(\) 36.96 & 0.065\(\) 0.018 & 3.19 & 172.17\(\) (26.24) & 0.079\(\) 0.019) & 4.63 \\  DSSPP- & 155.62\(\) 37.31 & **0.060\(\) 0.015** & 7.64 & 173.91\(\) 10.27) & 0.090\(\) 0.013) & 8.27 \\ DSSPP- & 155.51\(\) 36.98 & 0.065\(\) 0.012) & 7.56 & 172.54\(\) (26.74) & 0.103\(\) 0.015) & 7.54 \\ DSSPP- & 153.75\(\) 37.34 & 0.073\(\) 0.016) & 8.95 & 174.35\(\) (26.70) & 0.086\(\) 0.013) & 9.74 \\ SSSPP & 153.91\(\) 36.70 & 0.071\(\) 0.016) & 1.95 & 165.94\(\) 30.00) & 1.40\(\) 0.009) & 2.48 \\ GSSSPP & 154.12\(\) 37.09 & 0.073\(\) 0.014) & 7.72 & 170.13\(\) (26.43) & 0.101\(\) 0.022) & 14.19 \\ GSSSPP-M12 & 150.65\(\) 36.84 & 0.071\(\) 0.018) & 9.30 & 168.49\(\) (26.67) & 0.083\(\) 0.018) & 15.33 \\ GSSSPP-M52 & 154.46\(\) 37.24 & 0.075\(\) 0.022) & 9.63 & 169.33\(\) (26.61) & 0.107\(\) 0.026) & 14.49 \\ LDPP & 150.80\(\) 36.13 & 0.082\(\) 0.008) & 0.31 & 168.97\(\) (26.70) & 0.126\(\) (0.006) & 0.37 \\ VBPP & 155.29\(\) 36.52 & 0.072\(\) 0.021) & 1.68 & 172.95\(\) (30.00) & 0.087\(\) 0.016) & 1.83 \\   

Table 1: The performance of \(_{}\), RMSE and runtime for DNSSPP, DSSPP, NSSPP and other baselines on two synthetic datasets. For \(_{}\), the higher the better; for RMSE and runtime, the lower the better. The upper half corresponds to nonstationary models, while the lower half to stationary models. On the stationary dataset, DSSPP performs comparably to DNSSPP. On the nonstationary dataset, DSSPP does not outperform DNSSPP due to the severe nonstationarity in the data.

Figure 2: The fitting results of the intensity functions from LBPP and DNSSPP on the Redwoods and Taxi datasets. Additional results for various baselines on three datasets are provided in Appendix G.

of eigenvalues for LBPP, and the number of frequencies for SSPP, GSSPP, and NSMPP, to 10. For the 2-dimensional datasets (Redwoods and Taxi), we set the number of rank to 50. We discuss the reasonableness of the above setup in Appendix H.

ResultsFig. 2 shows the fitting results of the intensity functions from LBPP and DNSSPP on the Redwoods and Taxi datasets. The quantitative comparison results are shown in Table 2. Since real-world data is more or less nonstationary to varying degrees, nonstationary models generally perform better on real-world data. DNSSPP secures both first and second places on the Coal and Taxi datasets and achieves second place on the Redwoods dataset. For simpler datasets, such as Coal and Redwoods, which have relatively simple data patterns, the performance of DNSSPP is comparable to that of the simpler baselines. However, for the more complex Taxi dataset, DNSSPP demonstrates significant advantages over the simpler baselines. This indicates that DNSSPP has a clear advantage in handling data with more complex patterns, such as large-scale or high-dimensional datasets.

### Ablation Studies

In this section, we analyze the impact of four configuration hyperparameters on the performance of DNSSPP: the network width and depth, as well as the number of epochs and learning rate during the update of hyperparameter \(\).

#### 6.5.1 Width and Depth of Network

We investigate DNSSPP's performance with varying network sizes by adjusting width and depth on nonstationary synthetic data. In our experiments, we maintain consistent width across layers. Results are illustrated in Fig. 0(c). As the number of layers and the width of the network increase, DNSSPP's performance initially improves and then declines, reflecting the phenomenon of overfitting as the network size increases. Regarding the number of layers, DNSSPP generally exhibits underfitting when there is only a single layer, while overfitting tends to occur with four layers. In terms of network width, due to the relatively small size of the synthetic data, overfitting becomes apparent when the width is large. However, for larger datasets, a more complex network structure should be chosen.

#### 6.5.2 Epoch and Learning Rate of \(\)

During the inference process, we need to perform numerical optimization on the hyperparameter \(\), implying the need to determine the epoch and learning rate for optimization. We conduct experiments on the nonstationary synthetic data to investigate the coordination of epoch and learning rate. Results are illustrated in Fig. 0(d). When the learning rate is too low or the number of epochs is insufficient, the update of \(\) is sluggish, leading to inferior performance. However, when the learning rate is too high or the number of epochs is excessive, model performance generally declines as well. This is because our model training is a bi-level optimization: the inner level is the Laplace approximation, and the outer level is to update the hyperparameter \(\). An excessively high learning rate or too many epochs can cause \(\) to converge too quickly, making the overall model training more prone to getting stuck in local optima, thus resulting in suboptimal performance.

    &  &  &  \\   & \(_{}\) & Runtime(s) & \(_{}\) & Runtime(s) & \(_{}\) & Runtime(s) \\  DNSSPP- & 225.73(\(\) 2.96) & 10.54 & 79.56(\(\) 0.014) & 11.09 & 7171(\(\) 73) & 73.3 \\ DNSSPP- & **225.85(\(\) 2.61)** & 11.38 & 79.50(\(\) 0.020) & 10.45 & 6682(\(\) 37) & 70.1 \\ DNSSPP- & 225.79(\(\) 4.49) & 11.76 & 79.55(\(\) 0.025) & 12.72 & **7246**(\(\)**37) & 81.0 \\ NSMPP & 223.28(\(\) 3.60) & 2.88 & 77.64(\(\) 6.21) & 4.43 & 6492(\(\) 62) & 94.6 \\  SSPP & 221.42(\(\) 1.87) & 1.72 & 78.57(\(\) 2.83) & 2.45 & 6245(\(\) 45) & 58.4 \\ GSSPP & 221.08(\(\) 6.32) & 5.05 & 76.97(\(\) 5.80) & 5.94 & 6445(\(\) 97) & 110.64 \\ GSSPP-M12 & 223.11(\(\) 5.02) & 5.61 & 72.96(\(\) 11.89) & 5.90 & 6599(\(\) 76) & 104.75 \\ GSSPP-M52 & 221.89(\(\) 3.06) & 5.17 & 77.98(\(\) 6.05) & 5.23 & 6526(\(\) 113) & 112.54 \\ LBPP & 218.30(\(\) 4.12) & 0.33 & **80.40(\(\) 0.72)** & 0.34 & 6096(\(\) 25) & 3.16 \\ VBPP & 219.15(\(\) 4.54) & 1.69 & 77.06(\(\) 0.88) & 2.14 & 6156(\(\) 34) & 9.10 \\   

Table 2: The performance of \(_{}\) and runtime for DNSSPP and other baselines on three real datasets. The upper half corresponds to nonstationary models, while the lower half to stationary models.

## 7 Limitations

NSSPP not only removes restrictions on the kernel's functional form but also makes it applicable to nonstationary kernels. DNSSPP further enhances its expressive power through a deep architecture. However, the deep architecture of DNSSPP prevents analytical computation of the intensity integral, necessitating numerical integration. This reliance on numerical integration imposes a limitation on the model's computational speed.

## 8 Conclusions

In this study, we introduced NSSPP and its deep kernel variant, DNSSPP, to address limitations in modeling the permanental process. This approach overcomes the stationary assumption, allowing for flexible learning of nonstationary kernels directly from data without restricting their form. By leveraging the sparse spectral representation of nonstationary kernels, we achieved a low-rank approximation, effectively reducing computational complexity. Our experiments on synthetic and real-world datasets demonstrated that (D)NSSPP achieved competitive performance on stationary data while excelling on nonstationary datasets. This study underscores the importance of considering nonstationarity in the permanental process and demonstrates the efficacy of (D)NSSPP in this context.