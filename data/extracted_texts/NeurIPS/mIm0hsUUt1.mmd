# Efficient Testable Learning of Halfspaces

with Adversarial Label Noise

 Ilias Diakonikolas

University of Wisconsin, Madison

ilias@cs.wisc.edu

&Daniel M. Kane

University of California, San Diego

dakane@ucsd.edu

&Vasilis Kontonis

University of Texas, Austin

mailto:vasilis@cs.utexas.edu

&Sihan Liu

University of California, San Diego

i1046@ucsd.edu

&Nikos Zarifis

University of Wisconsin, Madison

zarifis@wisc.edu

###### Abstract

We give the first polynomial-time algorithm for the testable learning of halfspaces in the presence of adversarial label noise under the Gaussian distribution. In the recently introduced testable learning model, one is required to produce a tester-learner such that if the data passes the tester, then one can trust the output of the robust learner on the data. Our tester-learner runs in time \((d/)\) and outputs a halfspace with misclassification error \(O()+\), where \(\) is the 0-1 error of the best fitting halfspace. At a technical level, our algorithm employs an iterative soft localization technique enhanced with appropriate testers to ensure that the data distribution is sufficiently similar to a Gaussian. Finally, our algorithm can be readily adapted to yield an efficient and testable active learner requiring only \(d(1/)\) labeled examples.

## 1 Introduction

A (homogeneous) halfspace is a Boolean function \(h:^{d}\{ 1\}\) of the form \(h_{}()=()\), where \(^{d}\) is the corresponding weight vector and the function \(:\{ 1\}\) is defined as \((t)=1\) if \(t 0\) and \((t)=-1\) otherwise. Learning halfspaces from random labeled examples is a classical task in machine learning, with history going back to the Perceptron algorithm . In the realizable PAC model  (i.e., with consistent labels), the class of halfspaces is known to be efficiently learnable without distributional assumptions. On the other hand, in the agnostic (or adversarial label noise) model  even _weak_ learning is computationally intractable in the distribution-free setting .

These intractability results have served as a motivation for the study of agnostic learning in the distribution-specific setting, i.e., when the marginal distribution on examples is assumed to be well-behaved. In this context, a number of algorithmic results are known. The \(L_{1}\)-regression algorithm of  agnostically learns halfspaces within near-optimal 0-1 error of \(+\), where \(\) is the 0-1 error of the best-fitting halfspace. The running time of the \(L_{1}\)-regression algorithm is \(d^{(1/^{2})}\) under the assumption that the marginal distribution on examples is the standard Gaussian (and for a few other structured distributions) . While the \(L_{1}\) regression method leads to improper learners, a proper agnostic learner with qualitatively similar complexity was recently givenin . The exponential dependence on \(1/\) in the running time of these algorithms is known to be inherent, in both the Statistical Query model  and under standard cryptographic assumptions .

Interestingly, it is possible to circumvent the super-polynomial dependence on \(1/\) by relaxing the final error guarantee -- namely, by obtaining a hypothesis with 0-1 error \(f()+\), for some function \(f(t)\) that goes to \(0\) when \(t 0\). (Vanilla agnostic learning corresponds to the case that \(f(t)=t\).) A number of algorithmic works, starting with , developed efficient algorithms with relaxed error guarantees; see, e.g., . The most relevant results in the context of the current paper are the works  which gave \((d/)\) time algorithms with error \(C+\), for some universal constant \(C>1\), for learning halfspaces with adversarial label noise under the Gaussian distribution. Given the aforementioned computational hardness results, these constant-factor approximations are best possible within the class of polynomial-time algorithms.

A drawback of distribution-specific agnostic learning is that it provides no guarantees if the assumption on the marginal distribution on examples is not satisfied. Ideally, one would additionally like an efficient method to _test_ these distributional assumptions, so that: (1) if our tester accepts, then we can trust the output of the learner, and (2) it is unlikely that the tester rejects if the data satisfies the distributional assumptions. This state-of-affairs motivated the definition of a new model -- introduced in  and termed _testable learning_ -- formally defined below:

**Definition 1.1** (Testable Learning with Adversarial Label Noise ).: _Fix \(,(0,1]\) and let \(f:_{+}\). A tester-learner \(\) (approximately) testably learns a concept class \(\) with respect to the distribution \(D_{}\) on \(^{d}\) with \(N\) samples, and failure probability \(\) if the following holds. For any distribution \(D\) on \(^{d}\{ 1\}\), the tester-learner \(\) draws a set \(S\) of \(N\) i.i.d. samples from \(D\). In the end, it either rejects \(S\) or accepts \(S\) and produces a hypothesis \(h:^{d}\{ 1\}\). Moreover, the following conditions must be met:_

* _(Completeness) If_ \(D\) _truly has marginal_ \(D_{}\)_,_ \(\) _accepts with probability at least_ \(1-\)_._
* _(Soundness) The probability that_ \(\) _accepts and outputs a hypothesis_ \(h\) _for which_ \(_{(,y) D}[h() y]>f()+\)_, where_ \(:=_{g}_{(,y) D}[g( ) y]\) _is at most_ \(\)_._

_The probability in the above statements is over the randomness of the sample \(S\) and the internal randomness of the tester-learner \(\)._

The initial work  and the followup paper  focused on the setting where \(f(t)=t\) (i.e., achieving optimal error of \(+\)). These works developed general moment-matching based algorithms that yield testable learners for a range of concept classes, including halfspaces. For the class of halfspaces in particular, they gave a testable agnostic learner under the Gaussian distribution with sample complexity and runtime \(d^{O(1/^{2})}\) -- essentially matching the complexity of the problem in the standard agnostic PAC setting (without the testable requirement). Since the testable learning setting is at least as hard as the standard PAC setting, the aforementioned hardness results imply that the exponential complexity dependence in \(1/\) cannot be improved.

In this work, we continue this line of investigation. We ask whether we can obtain _fully polynomial time_ testable learning algorithms with relaxed error guarantees -- ideally matching the standard (non-testable) learning setting. Concretely, we study the following question:

_Is there a \((d/)\) time tester-learner for halfspaces with error \(f()+\)?_

_Specifically, is there a constant-factor approximation?_

As our main result, we provide an affirmative answer to this question in the strongest possible sense -- by providing an efficient constant-factor approximate tester-learner.

Labeling examples often requires hiring expert annotators, paying for query access to powerful language models, etc. On the other hand unlabeled examples are usually easy to obtain in practice. This has motivated the design of _active_ learning algorithms that, given a large dataset of unlabeled examples, carefully choose a small subset to ask for their labels. A long line of research has studied active learning of halfspaces under structured distributions either with clean labels or in the presence of noise .

In principle, testable learners could only rely on unlabeled examples during their "testing phase" in order to verify that the \(\)-marginal satisfies the required properties; and, assuming that it does so,proceed to run an active learning algorithm. However, as the testing process is often coupled with the downstream learning task -- that depends on labeled examples -- it is an interesting question to investigate the design of testable learners in the active learning setting.

**Definition 1.2** (Testable Active Learning).: _A tester-learner has sample complexity \(N\) and label complexity \(q\) in the active learning model if it draws \(N\) unlabeled samples from \(D_{}\), i.e., the \(\)-marginal of \(D\) over \(^{d}\), and queries the labels of at most \(q\) samples throughout its computation._

Main ResultOur main result is the first polynomial-time tester-learner for homogeneous halfspaces with respect to the Gaussian distribution in the presence of adversarial label noise. Formally, we establish the following theorem:

**Theorem 1.3** (Testable Active Learning Halfspaces under Gaussian Marginals).: _Let \(,(0,1)\) and \(\) be the class of homogeneous halfspaces on \(^{d}\). There exists a tester-learner with sample complexity \(N=(d,1/)(1/)\) and runtime \((d\ N)\) for \(\) with respect to \((,)\) up to 0-1 error \(O()+\), where \(\) is the 0-1 error of the best fitting function in \(\), that fails with probability at most \(\). In addition, in the active learning model, the algorithm has sample complexity \(N\) and label complexity \(q=((d\ (1/)+^{2}(1/))(1/))\)._

Before we provide an overview of our technical approach, some remarks are in order. Theorem 1.3 gives the first algorithm for testable learning of halfspaces that runs in \((d/)\) time and achieves dimension-independent error (i.e., error of the form \(f()+\), where \(f\) satisfies \(_{t 0}f(t)=0\).) Moreover, the constant-factor approximation achieved is best possible, matching the known guarantees without the testable requirement and complexity lower bounds. Prior to our work, the only known result in the testable setting, due to , achieves error \(+\) with complexity \(d^{(1/)}\). A novel (and seemingly necessary) feature of our approach is that the testing components of our algorithm depend on the labels (as opposed to the label-oblivious testers of ). As will be explained in the proceeding discussion, to prove Theorem 1.3 we develop a testable version of the well-known localization technique that may be of broader interest.

Independent WorkIn concurrent and independent work,  gave an efficient tester-learner for homogeneous halfspaces under the Gaussian distribution (and strongly log-concave distributions) achieving dimension-independent error guarantees. Specifically, for strongly log-concave distributions, their algorithm achieves error \(O(k^{1/2}^{1-1/k})\) with sample complexity and running time of \((d^{(k)},(1/)^{(k)})\). That is, they obtain error \(O(^{c})\), where \(c<1\) is a universal constant, in \(_{}(d/)\) time; and error \(()\) in quasi-polynomial \((d/)^{(d)}\) time. For the Gaussian distribution, they achieve error \(O()\) with complexity \((d,1/)\). We remark that since the learner in  corresponds to the stochastic gradient descent algorithm from , which requires labeled sample in every iteration, it is not clear whether their approach can be made label-efficient.

### Overview of Techniques

Our tester-learner is based on the well-known localization technique that has been used in the context of learning halfspaces with noise; see, e.g., . At a high-level, the idea of localization hinges on updating a given hypothesis by using "the most informative" examples, specifically examples that have very small margin with respect to the current hypothesis. Naturally, the correctness of this geometric technique leverages structural properties of the underlying distribution over examples, namely concentration, anti-concentration, and anti-anti-concentration properties (see, e.g., ). While the Gaussian distribution satisfies these properties, they are unfortunately hard to test. In this work, we show that localization can be effectively combined with appropriate efficient testing routines to provide an efficient tester-learner.

Localization and a (Weak) _Proper_ Testable LearnerAssume that we are given a halfspace defined by the unit vector \(\) with small 0-1 error, namely \(_{ D_{}}[(^{*} )()]\), for some small \(>0\), where \(^{*}\) is the unit vector defining an optimal halfspace. The localization approach improves the current hypothesis, defined by \(\), by considering the conditional distribution \(D^{}\) on the points that fall in a thin slice around \(\), i.e., the set of points \(\) satisfying \(|| O()\). The goal is to compute a new (unit) weight vector \(^{}\) that is close to an optimal halfspace, defined by \(^{*}\), with respect to \(D^{}\), i.e., \(_{^{} D^{}_{}}[( ^{*}^{})(^{} ^{})]\), for an appropriate \(>0\). We can then show that the halfspace defined by \(^{}\) will be _closer_ to the target halfspace (defined by \(^{*}\)) with respect to the _original_ distribution, i.e., we have that \(_{ D_{}}[(^{*} )(^{})] O( )\). By repeating the above step, we iteratively reduce the disagreement with \(^{*}\) until we reach our target error of \(O()\). Similarly to , instead of "hard" conditioning on a thin slice, we perform a "soft" localization step where (by rejection sampling) we transform the \(\)-marginal to a Gaussian whose covariance is \(O(^{2})\) in the direction of \(\) and identity in the orthogonal directions, i.e., \(=-(1-^{2})^{}\); see creftype 3.2.

A crucial ingredient of our approach is a _proper_ **testable, weak agnostic learner** with respect to the Gaussian distribution. More precisely, our tester-learner runs in polynomial time and either reports that the \(\)-marginal is not \((,)\) or outputs a unit vector \(\) with small constant distance to the target \(^{*}\), i.e., \(\|-^{*}\|_{2} 1/100\); see creftype 2.1. Our weak proper tester-learner first verifies that the given \(\)-marginal approximately matches constantly many low-degree moments with the standard Gaussian; and if it does, it returns the vector defined by the degree-1 Chow parameters, i.e., \(=_{(,y) D}[y]\). Our main structural result in this context shows that if \(D_{}\) approximately matches its low-degree moments with the standard Gaussian, then the Chow parameters of any homogeneous LTF with respect to \(D_{}\) are close to its Chow parameters with respect to \((,)\), i.e., for any homogeneous LTF \(f()\), we have that \(_{ D_{}}[f()] _{^{*}(,)}[f(^{*})^{*}]\); see creftype 2.3. Since the Chow vector of a homogeneous LTF with respect to the Gaussian distribution is parallel to its normal vector \(^{*}\) (see creftype 2.2), it is not hard to show that the Chow vector of the LTF with respect to \(D_{}\) will not be very far from \(^{*}\) and will satisfy the (weak) learning guarantee of \(\|-^{*}\|_{2} 1/100\). Finally, to deal with label noise, we show that if \(^{}\) has bounded second moments (a condition that we can efficiently test), we can robustly estimate \(_{ D_{}}[f()]\) with samples from \(D\) up to error \(O(})\) (see creftype 2.4), which suffices for our purpose of weak learning. The detailed description of our weak, proper tester-learner can be found in Section 2.

From Parameter Distance to Zero-One ErrorHaving a (weak) testable proper learner, we can now use it on the localized (conditional) distribution \(D^{}\) and obtain a vector \(^{}\) that is closer to \(^{*}\) in \(_{2}\) distance; see creftype 3.3. However, our goal is to obtain a vector that has small zero-one disagreement with the target halfspace \(^{*}\). Assuming that the underlying \(\)-marginal is a standard normal distribution, and that \(\|-^{*}\|_{2}=\), it holds that \(_{ D_{}}[( )(^{*})]=O()\), which implies that achieving \(_{2}\)-distance \(O()+\) suffices. We give an algorithm that can efficiently either certify that small \(_{2}\)-distance implies small zero-one disagreement with respect to the given marginal \(D_{}\) or declare that \(D_{}\) is not the standard normal.

The disagreement region of \(^{*}\) and \(\) is a union of two "wedges" (intersection of two halfspaces); see creftype 1. In order for our algorithm to work, we need to verify that these wedges do not contain too much probability mass. Similarly to our approach for the weak tester-leaner, one could try a moment-matching approach and argue that if \(D_{}\) matches its "low"-degree moments with \((,)\), then small \(_{2}\)-distance translates to small zero-one disagreement. However, we will need to use this result for vectors that are very close to the target (but still not close enough), namely \(\|-^{*}\|_{2}=\)

Figure 1: The disagreement region between a halfspace with normal vector \(\) and the target \(^{*}\) is shown in green. The unit direction \(\) corresponds to the projection of \(^{*}\) on the orthogonal complement of \(\). We assume that the \(_{2}\) distance of the two halfspaces is \(\) (and thus their angle is \(()\)). Since the slabs \(S_{i}=\{i||(i+1)\}\) have width \(\), the \(x\)-coordinate of the start of the \(i\)-th box is \((i)\).

where \(=()\); this would require matching \((1/)\) many moments (as we essentially need to approximate the wedge of Figure 1 with a polynomial) and would thus lead to an exponential runtime of \(d^{(1/)}\).

Instead of trying to approximate the disagreement region with a polynomial, we will make use of the fact that our algorithm knows \(\) (but not \(^{*}\)) and approximate the disagreement region by a union of cylindrical slabs. We consider slabs of the form \(S_{i}=\{:i||(i+1)\}\). If the target distribution is Gaussian, we know that the set \(||\) has mass \(O()\) and we can essentially ignore it. Therefore, we can cover the whole space by considering roughly \(M=O(/)\) slabs of width \(\) and split the disagreement region into the disagreement region inside each slab \(S_{i}\). We have that

\[*{}_{ D_{}}[()(^{*} )]_{i=1}^{M}*{}[| | i S_{i}]\;*{} [S_{i}]\,,\]

where \(\) is the unit direction parallel to the projection of the target \(^{*}\) onto the orthogonal complement of \(\), see Figure 1. By the anti-concentration of the Gaussian distribution we know that each slab should have mass at most \(O()\). Note that this is easy to test by sampling and computing empirical estimates of the mass of each slab. Moreover, assuming that underlying distribution is \((0,)\), we have that, conditional on \(S_{i}\) the orthogonal direction \((0,1)\) (see Figure 1) and in particular \(\) has bounded second moment. We do not know the orthogonal direction \(\) as it depends on the unknown \(^{*}\) but we can check that, conditional on the slab \(S_{i}\), the projection of \(D_{}\) onto the orthogonal complement of \(\) is (approximately) mean-zero and has bounded covariance (i.e., bounded above by \(2\)). Note that both these conditions hold when \((0,)\) and can be efficiently tested with samples in time \((d,1/)\). Under those conditions we have that that \(*{}[S_{i}]=O()\) for all \(i\). Moreover, when the conditional distribution on \(S_{i}\) (projected on the orthogonal complement of \(\)) has bounded second moment, we have that \(*{}[|| i  S_{i}] O(1/i^{2})\,.\) Combining the above, we obtain that under those assumptions the total probability of disagreement is at most \(O()\). The detailed analysis is given in Section 3.1.

Obtaining an Active Learner.Our localization-based algorithm can be readily adapted to yield an efficient active learner-tester. We first observe that the tester that verifies that the parameter distance is proportional to 0/1 error does not require any labels, as it relies on verifying moments of the \(\)-marginal of the underlying distribution. During each iteration of localization, since the learner is only trying to find a constant approximation of the defining vector of the unknown halfspace, the learner only needs roughly \(O(d)\) samples that fall in the slice we localize to. Since we can selectively query only the labels of samples that fall in the right region and the total number of iterations is at most \(O((1/))\), the labels needed to learn the defining vectors is roughly \(O(d(1/))\). In the end, the learner needs to find the best halfspace among the ones from \(O((1/))\) localization iterations. We remark this can also be done with \((1/)\) many label queries (see Section 3.6).

### Preliminaries

We use small boldface characters for vectors and capital bold characters for matrices. We use \([d]\) to denote the set \(\{1,2,,d\}\). For a vector \(^{d}\) and \(i[d]\), \(_{i}\) denotes the \(i\)-th coordinate of \(\), and \(\|\|_{2}:=^{d}_{i}^{2}}\) the \(_{2}\) norm of \(\). We use \(:=_{i=1}^{n}_{i}_{i}\) as the inner product between them. We use \(\{E\}\) to denote the indicator function of some event \(E\).

We use \(_{ D}[]\) for the expectation of the random variable \(\) according to the distribution \(D\) and \(*{}[E]\) for the probability of event \(E\). For simplicity of notation, we may omit the distribution when it is clear from the context. For \(^{d},^{d d}\), we denote by \((,)\) the \(d\)-dimensional Gaussian distribution with mean \(\) and covariance \(\). For \((,y)\) distributed according to \(D\), we denote \(D_{}\) to be the marginal distribution of \(\). Let \(f:^{d}\{ 1\}\) be a boolean function and \(D\) a distribution over \(^{d}\). The degree-\(1\) Chow parameter vector of \(f\) with respect to \(D\) is defined as \(_{ D}[f()]\). For a halfspace \(h()=()\), we say that \(\) is the defining vector of \(h\).

Moment-MatchingIn what follows, we use the phrase _"A distribution \(D\) on \(^{d}\) matches \(k\) moments with a distribution \(Q\) up to error \(\)"_. Similarly to , we formally define approximate moment-matching as follows.

**Definition 1.4** (Approximate Moment-Matching).: _Let \(k\) be a degree parameter and let \((k,d)\) be the set of \(d\)-variate monomials of degree up to \(k\). Moreover, let \(_{+}^{|(k,d)|}\) be a slack parameter (indexed by the monomials of \((k,d)\)), satisfying \(_{0}=0\). We say that two distributions \(D,Q\) match \(k\) moments up to error \(\) if \(|_{ D}[m()]-_{ Q }[m()]|_{m}\) for every monomial \(m()(k,d)\). When the error bound \(\) is the same for all monomials we overload notation and simply use \(\) instead of the parameter \(\)._

## 2 Weak Testable Proper Agnostic Learning

As our starting point, we give an algorithm that performs testable _proper_ learning of homogeneous halfspaces in the presence of adversarial label noise with respect to the Gaussian distribution. The main result of this section is given below and its proof can be found in Appendix A.

**Proposition 2.1** (Proper Testable Learner with Adversarial Label Noise).: _Let \(D\) be a distribution on labeled examples \((,y)^{d}\{ 1\}\) with \(\)-marginal \(D_{}\). Suppose that there exists a unit vector \(^{*}^{d}\) such that \(_{(,y) D}[(^{*} ) y]\). There exists an algorithm (Algorithm 2) that given \(,(0,1)\), \(N=d^{(1/^{2})}(1/)\) i.i.d. unlabeled samples from \(D_{}\), queries the labels of \(O(d/^{2})(d/)\) of them, and runs in time \((d,N)\) and does one of the following:_

* _The algorithm reports that the_ \(\)_-marginal of_ \(D\) _is not_ \((,)\)_._
* _The algorithm outputs a unit vector_ \(^{d}\)_._

_With probability at least \(1-\) the following holds: (1) if the algorithm reports anything, the report is correct, and (2) if the algorithm returns a vector \(\), it holds \(\|^{*}-\|_{2} C_{}+}\), where \(C_{}>0\) is an absolute constant._

A couple of remarks are in order. First, notice that if the algorithm outputs a vector \(\), we only have the guarantee that \(\|^{*}-\|_{2}\) is small -- instead of that the hypothesis halfspace \(h_{}()=()\) achieves small 0-1 error. Nonetheless, as we will show in the next section, conditioned on \(D\) passing some test, the error of the halfspace \(h_{}\) will be at most \(\) plus a constant multiple of \(\|^{*}-\|_{2}\) (see Lemma 3.1). Second, unlike the testable _improper_ learners in  -- which achieve error of \(+\) with similar running time and sample complexity -- our testable _proper_ learner achieves the weaker error guarantee of \(O(+})\). This suffices for our purposes for the following reason: in the context of our localization-based approach, we only need an efficient proper _weak_ learner that achieves sufficiently small _constant_ error. This holds for our proper testable learner, as long as both \(\) and \(\) are bounded above by some other sufficiently small constant.

To obtain a proper learner, we proceed to directly estimate the defining vector \(^{*}\) of the target halfspace \(h^{*}()=(^{*})\), where we assume without loss of generality that \(^{*}\) is a unit vector. The following simple fact relating the degree-1 _Chow-parameters_ of a homogeneous halfspace and its defining vector will be useful for us.

**Fact 2.2** (see, e.g., Lemma 4.3 of ).: _Let \(\) be a unit vector and \(h()=()\) be the corresponding halfspace. If \(\) is from \((,)\), then we have that \(_{(,)}[h()]=\ \)._

To apply Fact 2.2 in our context, we need to overcome two hurdles: (i) the \(\) marginal of \(D\) is not necessarily the standard Gaussian, and (ii) the labels are not always consistent with \(h^{*}()\). The second issue can be circumvented by following the approach of . In particular, if the \(\) marginal of \(D\) is indeed Gaussian, we can just treat \(D\) as a corrupted version of \((,h^{*}())\), where \((,)\) and estimate the Chow parameters robustly.

To deal with the first issue, we borrow tools from . At a high level, we certify that the low-degree moments of \(D_{}\) -- the \(\) marginal of \(D\) -- approximately match the corresponding moments of \((,)\) before estimating the Chow parameters. Importantly, this testing procedure uses only unlabeled samples. To establish the correctness of our algorithm, we show that, for any distribution \(B\) that passes the moment test, the Chow parameters of a halfspace under \(B\) will still be close to its defining vector. We state the lemma formally below. Its proof can be found in Appendix A.

**Lemma 2.3** (From Moment-Matching to Chow Distance).: _Fix \(>0\). Let \(k=C(1/)/^{2}\) and \(=}(})^{k+1}\), where \(C>0\) is a sufficiently large absolute constant. Let \(B\) be adistribution whose moments up to degree \(k\) match with those of \((,)\) up to additive error \(\). Let \(h()=()\) be a halfspace. Then we have that \(\|_{ B}[h()]- }\;\|_{2} O()\)._

With Lemma2.3 in hand, we know it suffices to estimate the Chow parameters of \(h^{*}\) with respect to \(D_{}\). This would then give us a good approximation to \(^{*}\) conditioned on \(D_{}\) indeed having its low-degree moments approximately match those of \((,)\). We use the following lemma, which estimates the Chow parameters _robustly_ under adversarial label noise.

**Lemma 2.4**.: _Let \(,(0,1)\) and \(G\) be a distribution over \(^{d}\{ 1\}\) such that \(_{ G_{}}[^{}] 2 \). Let \(^{d}\) be a unit vector such that \(_{(,) G}[( ) y].\) Then there exists an algorithm that takes \(N=O(d/^{2})(d/)\) labeled samples from \(G\), runs in time \((N)\), and outputs a vector \(\) such that \(\|_{ G_{}}[()]-\|_{2} O()\) with probability at least \(1-\)._

The proof follows from standard arguments in robust mean estimation and the details can be found in AppendixC. We remark that the number of labeled samples required by this procedure is (nearly) linear with respect to \(d\) for constant \(\), which is important for us to obtain (nearly) optimal label complexity overall.

We are now ready to describe our testable proper learner. In particular, it first certifies that the moments of the underlying distribution match with the standard Gaussian and then draws i.i.d. samples to estimate the Chow parameter. It then follows from Fact2.2, Lemmas2.3 and 2.4 that the resulting Chow vector, conditioned on that the moment test pass, is closed to the defining vector of the optimal halfspace. We given the pseudo-code of the algorithm and its analysis, which also serves as the proof of Proposion2.1, in the end of AppendixA.

## 3 Efficient Testable Learning of Halfspaces

In this section, we give our tester-learner for homogeneous halfspaces under the Gaussian distribution, thereby proving Theorem1.3. Throughout this section, we will fix an optimal halfspace \(h^{*}()=(^{*})\), i.e., a halfspace with optimal 0-1 error.

The structure of this section is as follows: In Section3.1, we present a tester which certifies that the probability of the disagreement region between two halfspaces whose defining vectors are close to each other is small under \(D_{}\). In Section3.2, we present and analyze our localization step and combine it with the tester from Section3.1 to obtain our final algorithm.

### From Parameter Distance to 0-1 Error

For two homogeneous halfspaces \(h_{}()=()\) and \(h_{}()=()\), where \(,\) are unit vectors, if \(D_{}\) is the standard Gaussian, \((,)\), we can express the probability mass of their disagreement region as follows (see, e.g., Lemma4.2 of ):

\[*{}_{ D_{}}[h_{ }() h_{}()] O(\| -\|_{2})\;.\] (1)

Hence, learning homogeneous halfspaces under Gaussian marginals can often be reduced to approximately learning the defining vector of some optimal halfspace \(h^{*}\). This is no longer the case if \(D_{}\) is an arbitrary distribution, which may well happen in our regime. We show in this section that it is still possible to "certify" whether some relationship similar to the one in Equation1 holds.

In particular, given a known vector \(\), we want to make sure that for any other vector \(\) that is close to \(\), the mass of the disagreement region between the halfspaces defined by by \(,\) respectively is small. To do so, we will decompose the space into many thin "slabs" that are stacked on top of each other in the direction of \(\). Then, we will certify the mass of disagreement restricted to each of the slab is not too large. For slabs that are close to the halfspace \(()\), we can check these slabs must not themselves be too heavy. For slabs that are far away from the halfspace, we use the observation that the points in the disagreement region must then have large components in the subspace perpendicular to \(\). Hence, as long as \(D\) has its second moment bounded, we can bound the mass of the disagreement region in these far-away slabs using standard concentration inequality. Formally, we have the following lemma, whose proof can be found in AppendixB.

**Lemma 3.1** (Wedge Bound).: _Let \(D_{}\) be a distribution over \(^{d}\). Given a unit vector \(\) and parameters \(,(0,1/2)\), there exists an algorithm (Algorithm1) that draws i.i.d. samples from \(D_{}\), runs in time \((d,1/)(1/)\), and reports either one of the following: (i) For all unit vectors \(\) such that \(\|-\|_{2}\) it holds \(_{ D_{}}[( )()] C\), for some absolute constant \(C>1\). (ii) \(D_{}\) is not the standard Gaussian \((,)\). Moreover, with probability at least \(1-\), the report is accurate._

### Algorithm and Analysis: Proof of Theorem 1.3

We employ the idea of "soft" localization used in . In particular, given a vector \(\) and a parameter \(\), we use rejection sampling to define a new distribution \(D_{,}\) that "focuses" on the region near the halfspace \(()\).

**Fact 3.2** (Rejection Sampling, Lemma 4.7 of ).: _Let \(D\) be a distribution on labeled examples \((,y)^{d}\{ 1\}\). Let \(^{d}\) be a unit vector and \((0,1)\). We define the distribution \(D_{,}\) as follows: draw a sample \((,y)\) from \(D\) and accept it with probability \(e^{-()^{2}.(^{-2}-1)/2}\). Then, \(D_{,}\) is the distribution of \((,y)\) conditional on acceptance. If the \(\)-marginal of \(D\) is \((,)\), then the \(\)-marginals of \(D_{,}\) is \((,)\), where \(=-(1-^{2})^{T}\). Moreover, the acceptance probability of a point is \(\)._

We now briefly discuss the sample and label complexities of this rejection sampling procedure. Since the acceptance probability is \(\), we need roughly \(1/\) samples from \(D\) to simulate one sample from \(D_{,}\). On the other hand, to simulate one _labeled_ sample from \(D\), one only need \(1\) label query in addition to the unlabeled samples as one can query only the points that are accepted by the procedure. This makes sure that rejection sampling is efficient in terms of its label complexity.

The main idea of localization is the following. Let \(\) be a vector such that \(\|-^{*}\|_{2}\). Suppose that we use localization to the distribution \(D_{,}\). If we can learn a halfspace with defining vector \(\) that achieves sufficiently small constant error with respect to the new distribution \(D_{,}\), we can then combine our knowledge of \(\) and \(\) to produce a new halfspace with significantly improved error guarantees under \(D\). The following lemma formalizes this geometric intuition.

**Lemma 3.3**.: _Let \(^{*},\) be two unit vectors in \(^{d}\) such that \(\|-^{*}\|_{2} 1/100\). Let \(=-(1-^{2})^{T}\) and \(\) be a unit vector such that \(\|-^{1/2}^{*}}{\|^{1/2}^{*}\|_{2}}\|_{2} 1/100\). Then it holds \(\|^{-1/2}}{\|^{-1/2} \|_{2}}-^{*}\|_{2} 5(^{2}+)\)._

We defer the proof to Appendix C. Below, we provide a few useful remarks regarding the relevant parameters.

**Remark 3.4**.: Observe that in Lemma3.3 we require that the distance of \(\) and \(^{*}\) is smaller than \(1/100\). While this constant is not the best possible, we remark that some non-trivial error is indeed necessary so that the localization step works. For example, assume that \(\) and \(^{*}\) are orthogonal, i.e., \(^{*}=0\), and that \(=-(1-^{2})^{T}\) for some \(\). Observe that \(^{1/2}\) scales vectors by a factor of \(\) in the direction of \(\) and leaves orthogonal directions unchanged. Similarly, its inverse \(^{-1/2}\) scales vectors by a factor of \(1/\) in the direction of \(\) and leaves orthogonal directions unchanged. Without loss of generality, assume that \(=_{1},^{*}=_{2}\) where \(_{1},_{2}\) are the two standard basis vectors. Then \(^{1/2}^{*}=^{*}\). Moreover, assume that \(=a_{1}+b_{2}\) (with \(a^{2}+b^{2}=1\)). We observe that \(\|-^{1/2}^{*}/\|^{1/2} ^{*}\|_{2}^{2}=\|-^{*}\|_{2}^{2}=2-2b\). However, we have that \(=^{-1/2}=(a/)_{1}+b_ {2}\). Therefore, \(\|/\|\|_{2}-^{*}\|_{2}^{2}=2-2b/ +b^{2}}\). Notice that for all \(\) it holds that \(/\|\|_{2}\) is further away from \(^{*}\) than \(\), i.e., rescaling by \(^{-1/2}\) worsens the error.

We are now ready to present an iterative testing/learning procedure that will serve as the main component of our algorithm. At a high level, we first use Algorithm2 from Proposition2.1 to learn a vector \(\) that is close to \(^{*}\) in \(_{2}\)-distance up to some sufficiently small constant. Then we localize to the learned halfspace and re-apply Algorithm2 to iteratively improve \(\). In particular, we will argue that, whenever the learned halfspace is still significantly suboptimal, the algorithm either detects that the underlying distribution is not Gaussian or keeps making improvements such that \(\) gets closer to \(^{*}\) conditioned on \(\) still being sub-optimal. The pseudocode and analysis of the algorithm can be found in AppendixC.

**Lemma 3.5**.: _Suppose \(\|^{*}-^{(t)}\|_{2} 1/100\). There is an algorithm (Algorithm3) that with probability at least \(1-\), either (i) correctly reports that the \(\)-marginal of \(D\) is not \((,)\) or (ii) computes a unit vector \(^{(t+1)}\) so that either \(\|^{*}-^{(t+1)}\|_{2}/2\) or \(\|^{*}-^{(t)}\|_{2} C\), where \(C>0\) is an absolute constant. Furthermore, the algorithm draws \(N=(d,1/)(1/)\) many unlabeled samples and uses \(O(d\ (d/))\) label queries._

After running Algorithm3 for at most a logarithmic number of iterations, we know \(\) must be close to \(^{*}\). Then, we can use Algorithm1 from Lemma3.1 to certify that the disagreement between the learned halfspace and \(h^{*}()\) is small. One technical difficulty is that we do not know precisely when to terminate the update and each update does not necessarily monotonically bring \(\) closer to \(^{*}\). As a result, the process only yields a list of vectors \(^{(1)},^{(t)}\) with the guarantee that at least one of them is sufficiently closed to \(^{*}\). A natural idea is to estimate the 0-1 errors of the halfspaces defined by the vectors in the list and simply pick the one with the smallest empirical error. Naively, we need to estimate the errors up to an additive \(\) and this may take up to \((1/^{2})\) labeled samples, which is exponentially worse than our goal in terms of the dependency on \(\). Nonetheless, we remark that comparing the errors of two halfspaces which differ by some multiplicative factors can be done much more efficiently in terms of its label complexity. In particular, it suffices for us to compare the errors of the two halfspaces _restricted_ to the area in which the predictions made by them differ. Under this conditioning, the difference between the errors then gets magnified to some constant, making comparisons much easier. Hence, we can run a tournament among the the halfspaces, which reduces to perform pairwise comparisons among the lists, to obtain our final winner hypothesis. The result is summarized in the following lemma.

**Lemma 3.6** (Tournament).: _Let \(,(0,1)\) and \(D\) a distribution over \(^{d}\{ 1\}\). Given a list of halfspaces \(\{h^{(i)}\}_{i=1}^{k}\), there is an algorithm that draws \(N=(k^{2}(k/)/)\) i.i.d. unlabeled samples from \(D_{}\), uses \((k^{2}(k/))\) label queries, runs in time \((d,N)\) and outputs a halfspace \(\) satisfying that \(_{(,y) D}[(x) y] 10\ _{i} _{(,y) D}[h^{(i)}(x) y]+\)._

We provide the pseudocode of the algorithm and its analysis in AppendixC. The proof of Theorem1.3 then follows.

Our final algorithm (Algorithm5) simply iteratively applies the localized update routine (Algorithm3) and then runs a tournament (Algorithm4) among the \(O((1/))\) candidate halfspaces obtained from the iterations. The pseudocode and its analysis, which serves as proof of our main theorem, can be found at the end of AppendixC.

## 4 Acknowledgements

ID was supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), and a DARPA Learning with Less Labels (LwLL) grant. DK was supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER). NZ was supported in part by NSF award 2023239, NSF Medium Award CCF-2107079, and a DARPA Learning with Less Labels (LwLL) grant.