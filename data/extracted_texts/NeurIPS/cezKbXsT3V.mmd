# On Separate Normalization in Self-supervised Transformers

Xiaohui Chen

Department of Computer Science

Tufts University

Medford, MA 02155

xiaohui.chen@tufts.edu

Yinkai Wang

Department of Computer Science

Tufts University

Medford, MA 02155

yinkai.wang@tufts.edu

Yuanqi Du

Department of Computer Science

Cornell University

Ithaca, NY 14850

yd392@cornell.edu

&Soha Hassoun

Department of Computer Science

Tufts University

Medford, MA 02155

soha.hassoun@tufts.edu

Li-Ping Liu

Department of Computer Science

Tufts University

Medford, MA 02155

liping.liu@tufts.edu

###### Abstract

Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the class token \([]\) and the tokens. We propose in this paper a new yet simple normalization method that separately normalizes embedding vectors respectively corresponding to normal tokens and the \([]\) token, in order to better capture their distinct characteristics and enhance downstream task performance. Our empirical study shows that the \([]\) embeddings learned with our separate normalization layer better encode the global contextual information and are distributed more uniformly in its anisotropic space. When the conventional normalization layer is replaced with a separate normalization layer, we observe an average 2.7% performance improvement in learning tasks from the image, natural language, and graph domains.

## 1 Introduction

Transformer models  have revolutionized natural language processing (NLP)  and demonstrated remarkable performances across a wide range of NLP tasks. The significance of transformer models lies in their ability to model context and capture complex linguistic patterns without being constrained by the sequential nature of data. Beyond NLP transformers have further found their successes in areas such as computer vision (CV) , speech recognition , and recommendation systems . Their flexible architecture and ability to capture dependencies have made them adaptable to diverse data modalities in these domains.

Transformer architectures have been studied extensively from various perspectives such as attention mechanisms, positional encoding (Devlin et al., 2018), and normalization techniques. Specifically, layer normalization (Ba et al., 2016) and batch normalization (Ioffe and Szegedy, 2015) are employed to enhance stability and speed up convergence during training. The literature on transformers also explores parameter initialization (Xu et al., 2019), optimization algorithms (Huang et al., 2020), regularization techniques (Steiner et al., 2021; Zhou et al., 2020), and improved architectures (Han et al., 2021). This collective research has advanced transformer architectures and their applications in NLP, CV, and other learning domains.

The study of normalization in transformer architectures is motivated by several factors (Xiong et al., 2020; Shen et al., 2020; Nguyen and Salazar, 2019). For example, Xiong et al. (2020) emphasize the importance of the warm-up of the learning rate and the position of layer normalization layers for the purpose of stable training and faster convergence. Shen et al. (2020) investigates the disadvantage of using batch normalization in transformers and proposes power normalization. While most previous works focus on how the normalization layer can be modified to stabilize the training process, it is less understood how the normalization affects the encoding abilities of these embeddings.

In self-supervised transformers, the \([]\) symbol is frequently used as a global representation for various downstream tasks (Devlin et al., 2018; He et al., 2022). Often, the normalization applied to the \([]\) symbol is shared with the rest of the tokens in the sequence, which we term it as Shared Normalization (ShareNorm). Given that the \([]\) symbol plays a special role in representation learning, a natural question is whether we should treat it separately in the normalization operation. Driven by this question, our research first scrutinizes the behavior of the current shared normalization in transformers, particularly the properties of the \([]\) embedding and its influence on downstream task performance. Subsequently, we propose a replacement of ShareNorm with Separate Normalization (SepNorm), the latter of which employs distinct normalization operations for the \([]\) symbol and the token features, as depicted in Figure 1. Through extensive analysis, we demonstrate that \([]\) embeddings learned using ShareNorm have the issue of dimensional collapse, which cannot be rectified even by enforcing uniformity (Wang and Isola, 2020). However, the straightforward substitution of SepNorm for ShareNorm substantially mitigates this issue. We empirically validate the effectiveness of SepNorm in tasks from the image, text, and graph domains, demonstrating the universal advantage of the proposed SepNorm.

## 2 Background

### Pretraining Transformers with the \([]\) symbol

Unsupervised pretraining of a transformer-based model (Vaswani et al., 2017) is widely investigated in many domains, including NLP, computer vision (CV), and graphs.

Pretraining BERT for NLP.In NLP, Devlin et al. (2018) first develop the BERT model by pretraining a transformer-based network by performing the following two tasks - masked language modeling and next sentence prediction. During pretraining, BERT takes a pair of sentences \((,)\)

Figure 1: Comparison of the shared normalization (ShareNorm, left) and the proposed separate normalization (SepNorm, right) configurations for token normalization. In the ShareNorm setup, both the \([]\) symbol and other tokens are normalized using a single-layer normalization. In the SepNorm setup, normalization is done separately: the \([]\) symbol is normalized through batch normalization, while other tokens are normalized via layer normalization.

which are represented as a special sequence

\[=[],,[],. \]

Here \([]\) is a special token that separates the two sentences. A fraction (e.g., 15%) of the tokens in \(\) and \(\) are randomly replaced by a special symbol \([]\). The first task in BERT is to predict the original tokens replaced by \([]\) with cross-entropy loss. The second task is to predict whether \(\) is the next sentence following \(\), and the decision is made by classifying the final embedding of the \([]\) symbol. After pretraining, the representation in the \([]\) is usually used for sentence-level downstream tasks such as sentiment analysis .

Pretraining MAE for CV.The Vision Transformer (ViT)  applies the transformers to computer vision tasks. In ViT, an image is usually voxelized into \(16 16\) patches, which are then flattened into a sequence of 256 tokens and fed into the ViT. He et al.  proposes a self-supervised training scheme, Masked Autoencoder (MAE), for the ViT architecture. A training image has 75% of its patches masked. The MAE feeds Tokens of unmasked patches as well as a \([]\) token into the encoder and gets the representations for these tokens. Then the decoder tries to reconstruct the original image by minimizing the mean square error (MSE). Only the encoder will be used for downstream tasks after pretraining. The \([]\) symbol is treated as the class token for linear probing and fine-tuning in the downstream tasks.

Pretraining Graphormer for molecule discovery.Graphormer  is a transformer-based model designed for graph representation learning tasks. It is used to predict the property of a graph rather than a node or edge. Specifically, Graphormer introduces a new symbol \([]\) as a node connecting to all original graph nodes. Then the vector learned for \([]\) represents the global information of the entire graph. The mechanism of \([]\) is similar to the \([]\) symbol in BERT and MAE.

In typical applications of transformers, the \([]\) symbol is not a natural data token. It summarizes other tokens to capture global information, which is especially useful in downstream tasks. For these reasons, we argue that it should be treated differently in normalization operations.

### Normalization Layers in Transformers

Given that transformers are initially proposed for NLP tasks, layer normalization (LN) Ba et al.  is typically the normalization method of choice . LN normalizes across feature dimensions and is independent of the sequence length and the batch size. For any features \(^{d}\), the LN has the following computation:

\[()=-}{ }+,=_{i=1}^{d}h_{i}, =_{i=1}^{d}h_{i}-^{2}}. \]

Here \(h_{i}\) is the \(i\)-th dimension of \(\), \(\) represents element-wise multiplication, and \(,^{d}\) are scale and bias parameters, respectively. In a transformer, all tokens, including special tokens, such as \([]\) and \([]\), are all treated equally and share the same LNs.

Batch Normalization (BN)  works by normalizing the input data to have zero mean and unit variance along the batch dimension, followed by an affine transformation to scale the result using gamma and beta parameters. BN normalizes a given vector \(\) as:

\[()=-_{B}}{_{B}}+. \]

Here \(_{B},_{B}^{2}^{d}\) are the running statistics (mean and variance) maintained by the BN. The running mean and variance are updated during training after each batch. They are usually calculated as an exponential moving average of the batch mean and variance. BN is widely adopted in CV but leads to significant performance degradation when naively used in NLP.

### Uniformity of the Learned Representations

The dimensional collapse in self-supervised representation learning is a common phenomenon where the embedding vectors only span a lower-dimensional subspace  of the entire vector space. This means that the model fails to capture data patterns with full power and instead collapses to a simpler representation. Contrastive methods (Oord et al., 2018; Chen et al., 2020b) have been one of the standard approaches to address this problem. Specifically, Wang and Isola (2020) propose the _uniformity_ metric (loss) to quantify the degree of dimensional collapse. Given a set of representation vectors \(\{_{1},,_{N}\}\) from a dataset of size \(N\), the uniformity metric \(_{}\) is computed as follows:

\[_{}=_{n=1, \\ m=n+1}^{N,N}^{-2\|_{m}}{\|_{m }\|}-_{m}}{\|_{m}\|}\|^{2}}. \]

If the distribution of the representation is perfectly uniform, then the numerical value of \(_{}\) will converge to -4 as the dimension of \(\) increases to infinity (Wang and Isola, 2020).

In self-supervised transformers, the uniformity of the representation is also taken into consideration by some works. For example, Gao et al. (2021) finetune the pretrained BERT model using the InfoNCE loss (Oord et al., 2018), and Zhang et al. (2022) jointly train the MAE loss along with uniformity loss.

## 3 Approach

### Separate Normalization

We present SepNorm, a normalization scheme that separately normalizes embeddings of the \([]\) symbol and embeddings of other tokens. In this work, we focus on the exploration of combinations of BN and LN for the two separate normalization channels.

For instance, if we apply BN to the \([]\) symbol and LN to other tokens, the learnable parameters are structured as \(g_{1}=(_{1},_{1})\) and \(g_{2}=(_{2},_{2})\). Let \(^{L d}\) represent the feature sequence, where \(L\) denotes the sequence length, and \(d\) is the feature dimension. Assume embedding \(_{0}\) in the first position corresponds to the \([]\) symbol. The normalization process is as follows:

\[^{}=(_{0};g_{1}),( _{1};g_{2}),,(_{L};g_{2}), \]

where \(^{}\) denotes the normalized features. We can also run separate normalization with one of the three other combinations:

\[^{} =(_{0};g_{1}),(_ {1};g_{2}),,(_{L};g_{2}),\] \[^{} =(_{0};g_{1}),(_ {1};g_{2}),,(_{L};g_{2}),\] \[^{} =(_{0};g_{1}),(_ {1};g_{2}),,(_{L};g_{2}).\]

Separate normalization allows the \([]\) features to be encoded distinctly from other tokens.

As a comparison, the \([]\) token's embedding and other tokens' embeddings interfere with each other in a shared normalization structure. With ShareNorm, the update directions of the LN parameters \(\{,\}\) are primarily driven by the embeddings of normal tokens. Below is the gradient calculation for these parameters,

\[}{_{i}}=_{l=1}^{L}}{}_{l,i}}}_{l,i}, }{_{i}}=_{l=1}^{L}}{}_{l,i}}, \] \[}_{l,i}=_{l,i}-_{l }}{_{l}},_{l}=_{i=1}^{d}_{l,i},_{l}= _{i=1}^{d}_{l,i}-_{l}^{2}}. \]

We see the summation in the gradient calculation is dominated by normal tokens given that the number of normal tokens is typically a large number. Given the potentially diverse characteristics (i.e., mean and scale) of feature distributions, it might be challenging for normalization parameters to accommodate both token types simultaneously. Moreover, mapping two types of token features into the same sphere may also mix the signal of \([]\) tokens with other tokens. Figure 2(a, b) demonstrates this phenomenon in the scenario where both token types utilize a ShareNorm and how using SepNorm mitigates this effect.

### Encourage the Uniformity of the \([]\) Embeddings via a Constrastive Term

We further relate SepNorm with the uniformity of embeddings. Higher uniformity values indicate that embeddings can better exploit the space to store information. Contrastive methods often employ negative instances to encourage uniformity. In particular, we incorporate SepNorm into transformers trained with U-MAE (Zhang et al., 2022), which uses a constrastive term to promote uniformity of features.

The U-MAE explicitly adds a uniformity loss term \(_{}\) to the training objective to encourage uniformity of \([]\) embeddings.

\[_{}=_{}+_{ },\ \ \ _{}=_{i}[ _{j}[_{,i}^{}\,_{,j}]] \]

Here \(_{}\) is the MAE training objective. The two indices \(i\) and \(j\) represent two sequences within the same batch. \([]\) embeddings \(_{,i}\) and \(_{,j}\), which are respectively for the two sequences, are obtained from our SepNorm during the transformer calculation. By minimizing \(_{}\), \([]\) features tend to be different from each other.

## 4 Experiments

We examine the effectiveness of the proposed SepNorm component in three domains: CV, NLP, and graphs. We then further investigate how the ShareNorm and SepNorm affect the uniformity of the \([]\) embeddings.

### Computer Vision

Datasets.We investigate the model performance on the four image datasets: STL10 (Coates et al., 2011), FGVC Aircraft (Maji et al., 2013), Street View House Numbers (SVHN) (Netzer et al., 2011), and Oxford 102 Flowers (Nilsback and Zisserman, 2008). All four datasets are for classification tasks. We follow the train/test split provided in the papers introducing the datasets. We report top-1 and top-5 accuracy for all datasets.

Vision transformers (ViT) and MAE.We choose Vision Transformer (ViT) (Dosovitskiy et al., 2020) as our feature extractor for all datasets. To pretrain the ViT, we adopt the MAE training scheme (He et al., 2022). We follow MAE and use a 75% masking ratio on input image. During the downstream tasks, we use the embeddings of the \([]\) token to predict the class labels.

Experiment setup.We follow the setup in He et al. (2022) to pretrain and evaluate the ViT. For pertaining, we train the ViT for 4000 epochs. For linear probing, we freeze the encoder's weight and train the last layer on the specific datasets for 2000 epochs. We use a batch size of 512 for pretraining and a batch size of 128 for linear probing.

Figure 2: The effect of SepNorm on feature distributions. Each subplot shows the distributions of the first 50 feature dimensions: \([]\) features are in blue, and other tokens’ features are in red. The \([]\) features of ShareNorm are more concentrated around the mean and the mean deviates more from the zero centers, while in SepNorm, the \([]\) distribution is more centered and flattened.

Experiment results.The results presented in Table 1 demonstrate the performances of our model and the baseline model. Our model consistently outperforms the baseline across multiple datasets, indicating its effectiveness in image classification tasks. In the STL-10 dataset, our approach achieves the top-1 accuracy of 93.84% and the top-5 accuracy of 99.7%, higher than the baseline's respective accuracies of 92.01% and 99.5%. Similar improvements are observed in the Aircraft, SVHN, and Flower datasets, where our model consistently outperforms the baseline in both top-1 and top-5 accuracies. These results demonstrate the effectiveness of SepNorm in enhancing image classification performance. We also visualize the embeddings of ShareNorm and SepNorm using t-SNE in Figure 3. Compared with ShareNorm, SepNorm provides embeddings that have better separation among different classes.

### Natural Language Processing

Datasets.We evaluated our approach using the STS dataset, which comprises seven semantic textual similarity (STS) tasks. These tasks, including STS 2012-2016 , STS Benchmark , and SICK-Relatedness . We also evaluate our method on multiple transfer tasks, including MR , CR , SUBJ , MPQA , SST-2 , TREC Voorhees and Tice , and MRPC . Following the evaluation settings of SimCSE , we use Spearman's correlation coefficient as the evaluation metric.

BERT and RoBERTa.We conduct our study with pretrained checkpoints of BERT (uncased)  and RoBERTa (cased) , instead of training them from scratch. Using pretrained models is common in this research field  because the findings are compatible with the common practice of finetuning pretrained models in actual learning tasks. This strategy also saves significant training time and computational resources, allowing us to extend the study to more learning tasks.

Experiment setup.We follow the experiment setup in Gao et al.  and further finetune the BERT and RoBERTa models on English Wikipedia. We evaluate the models using established STS tasks and employ standard evaluation metrics such as Spearman's correlation.

    &  &  &  &  \\  & ACC@1 & ACC@5 & ACC@1 & ACC@5 & ACC@1 & ACC@5 & ACC@1 & ACC@5 \\  MAE & 92.01 & 99.5 & 52.54 & 84.16 & 88.97 & 99.13 & 27.63 & 53.73 \\ + SepNorm & **93.84** & **99.7** & **59.02** & **86.65** & **89.18** & **99.21** & **32.51** & **60.92** \\   

Table 1: Comparison of linear probing performance of ShareNorm and SepNorm across 4 image classification datasets when the ViT\({}_{}\) is pretrained with MAE.

Figure 3: t-SNE visualization of representations learned from the STL-10 dataset.

Experiment results.The experiment results presented in Table 2 highlight the performance of our model compared to the SimCSE baseline on NLP tasks. With the SepNorm layer, BERTbase and RoBERTbase achieve overall higher average accuracy compared to ShareNorm's average accuracy. Only in the transfer learning tasks, SepNorm works slightly worse than ShareNorm in BERTbase, but the difference is marginal.

### Prediction of Molecule Properties

Datasets.We conducted experiments using the ZINC dataset , which contains approximately 250,000 molecular graphs. The task is to predict the properties of molecules from their graphs. We use a subset of 12,000 molecular graphs, as recommended by the benchmarking methodology outlined in , so that our results are comparable with other studies. Despite being smaller, the subset retained sufficient diversity and complexity for effective evaluation. We also the MolHIV dataset from the OGB  collection, which is widely used for training and evaluating graph-based models in molecular property prediction tasks.

Graphormer.We use Graphormer  as the transformer backbone to construct the predicting model. To obtain graph-level information, Graphormer adds a special node [VNode] to the graph and connects it to all normal graph nodes. The embedding of [VNode] is a summary of the entire graph and will be used in downstream classification tasks. The special node [VNode] serves the same purpose as the \([]\) token in traditional Transformer models. Graphormer has used three encodings to enhance the transformer's learning ability: centrality encoding captures node importance, spatial encoding considers spatial relations, and edge encoding incorporates edge features.

Experiment setup.We strictly follow Graphormer  in terms of the model architecture, hyperparameters, and training strategies. We replaced the ShareNorm in Graphormer with SepNorm to investigate the effectiveness of the proposed component. We evaluate the pretrained model on a broad class of graph-level prediction tasks. We report the mean absolute error for the ZINC and ZINC (subset) datasets and the area under the curve (AUC) for the MolHIV dataset.

    & & STS12 & STS13 & STS14 & STS15 & STS16 & STS-B & SICK-R & Avg. \\   \\  BERTbase & ShareNorm & 65.28 & 78.82 & 69.65 & 79.02 & 77.21 & 76.4 & **71.74** & 74.04 \\  & SepNorm & **67.01** & **82.16** & **72.48** & **81.38** & **79.11** & **77.56** & 71.36 & **75.87** \\  RoBERTbase & ShareNorm & **68.25** & 81.24 & 72.78 & 81.38 & **80.31** & 79.83 & 68.16 & 76.00 \\  & SepNorm & 66.63 & **82.40** & **74.47** & **82.39** & **80.44** & **81.14** & **69.44** & **76.70** \\   \\  BERTbase & ShareNorm & **77.72** & 81.07 & **78.97** & **85.15** & **82.00** & 82.36 & **79.74** & 81.00 \\  & SepNorm & 75.32 & **84.41** & **79.94** & 84.91 & 80.87 & **83.63** & **79.61** & **81.23** \\  RoBERTbase & ShareNorm & **77.38** & 80.87 & 78.72 & 84.02 & **82.56** & 83.08 & 78.25 & 80.70 \\  & SepNorm & 75.80 & **84.94** & **80.33** & **85.51** & 82.11 & **84.88** & **79.72** & **81.90** \\   \\  BERTbase & ShareNorm & **82.78** & 88.79 & **94.69** & **89.86** & **87.94** & **84.44** & **75.99** & **86.36** \\  & SepNorm & **82.82** & **89.08** & 94.30 & 89.70 & **87.97** & 83.88 & 75.21 & 86.14 \\  RoBERTbase & ShareNorm & 84.45 & **91.50** & 93.94 & **89.45** & 90.96 & 86.80 & **76.13** & 87.61 \\  & SepNorm & **85.11** & **91.56** & **94.30** & **89.43** & **91.66** & **90.96** & 75.58 & **88.37** \\   

Table 2: Sentence embedding performance on STS tasks and transfer tasks.

   Dataset & ZINC & ZINC (subset) & MolHIV \\  Metrics &  & AUC\(\) \\  Graphormer & 0.069 & 0.164 & 73.36\% + SepNorm & **0.052** & **0.144** & **75.64\%** \\   

Table 3: A comparison of ShareNorm and SepNorm in three tasks of graph property prediction.

Experiment results.Table 3 shows the performances of our model and the Graphormer baseline. For the ZINC datasets, Graphormer with SepNorm achieves a significantly lower mean absolute error compared to that with ShareNorm. On the MolHIV dataset, SepNorm also improves the AUC to 75.64%, compared with ShareNorm's AUC of 73.36%. These results are strong evidence that the embeddings of the [VNode] can better summarize the properties of the entire graph and thus give superior performance on downstream tasks.

### Uniformity Analysis

In this section, we investigate how, under both non-contrastive and contrastive training methods, ShareNorm and SepNorm respectively affect the uniformity of learned embeddings and further classification performances.

Experiment setup.We pretrain MAE on the STL10 dataset via four different losses:

* MAE loss \(_{}\) without any \(_{}\) on \([]\) and token embeddings. This setting is a study with MAE training only.
* MAE loss \(_{}\) with \(_{}\) on the \([]\) embeddings. We treat all \([]\) embeddings within the same batch (except itself) as negative instances.
* MAE loss \(_{}\) with \(_{}\) on the token embeddings. We treat all token embeddings within the same batch or same images (except itself) as negative instances.
* MAE loss \(_{}\) with \(_{}\) on both \([]\) and token embeddings.

We choose \(=\{0,0.01,0.1,1\}\). Note that the second loss with \(=0.1\) corresponds to the U-MAE [Zhang et al., 2022]. We also replace the normalization layer of the ViT in MAE with one of the following: [LN, BN, BN+LN, BN+BN]. The combination of different losses, different \(\)'s, and different normalization layers yields 40 specifications of the experiments.

We first report our results with MAE training only. The uniformity of learned embeddings is first measured by singular values of the decomposition of an embedding matrix: we randomly choose 10k embeddings to form the matrix. We do this separately for \([]\) embeddings and normal token embeddings. Figrue 4 shows the results, which indicate that \([]\) features learned from SepNorm exhibit better representational power and thus can better encode the global information.

Then the uniformity is measured by the score in Eqn. 4. Table 6(a) shows the numerical value of the uniformity on the STL10 and Aircraft datasets [Coates et al., 2011, Maji et al., 2013]. Compared to ShareNorm, **SepNorm significantly enhances the uniformity of \([]\) embeddings**. Interestingly, the uniformity of normal tokens' embeddings remains comparable. We also empirically verify that better uniformity on the \([]\) embeddings results in better performance on the downstream task (Figrue 6(b)). Another observation is that the uniformity of \([]\) embeddings is clearly improved when they are normalized by BN instead of LN. Our hypothesis is that BN tries to make each feature dimension useful by controlling its variance while LN may still neglect some feature dimensions.

Figure 4: **(a) Reconstruction loss of the MAE pertaining – MAE with SepNorm achieves lower MSE loss compared to ShareNorm, demonstrating a better ability to encode global contextual information. (b) & (c) Comparison of the singular values of learned (\([]\) and normal token) features with ShareNorm and different configurations of SepNorm. \([]\) embeddings learned from SepNorm have larger singular values, which suggests that vectors are better used to encode information.**

We then report results from studies with U-MAE training. Figure 5 shows the uniformity metrics obtained using different \(\)'s. When using ShareNorm, the uniformity of the \([]\) embeddings is no better than -3.088, and even the explicit uniformity loss does not help much. On the contrary, embeddings learned from the proposed SepNorm can easily achieve better uniformity scores. The study with the contrastive approach further verifies the advantage of SepNorm in terms of encouraging uniformity of \([]\) embeddings.

The results provide strong evidence that **the uniformity of the \([]\) embeddings is held down by ShareNorm even the minimization of an explicit contrastive loss cannot increase it**. We hypothesize that all features after LN will distribute in the same sphere, and \([]\) embeddings are squeezed to a small area of the sphere surface because they need to be different from embeddings of normal tokens.

Table 4 reports the downstream performance (accuracy) on STL10 across 40 different settings. We summarize our observations: (1) In the non-contrastive method MAE, with proper configuration, the performance of SepNorm is superior to that of ShareNorm. (2) In contrastive methods (\( 0\)), SepNorms' advantages are further highlighted. For example, when \(=1\), the performance of SepNorm (BN+LN) is improved by 1.6% compared to the non-contrastive method. The performance gain in SepNorm (BN+BN) is less obvious as the double BNs already impose implicit uniformity loss on both \([]\) and token embeddings.

In contrast to SepNorm, the performance of ShareNorm is less satisfactory when using contrastive methods. We believe it is very challenging to encourage the two types of embeddings to be uniformly distributed in the same sphere and keep them separable at the same time. (3) The uniformity of the token embeddings is also vital for learning a good \([]\) representation, as evident by SepNorm (BN+LN) gaining accuracy with increasing \(\) on the token embeddings. We hypothesize that by enforcing uniformity, the token embeddings are forced to contain less information about others, which encourages the \([]\) embedding to encode as much information as possible. Our empirical study also shows that, when contrastive loss  is used to encourage the uniformity of \([]\) features in self-supervised transformers, the difference between BN and LN on \([]\) features is not significant anymore.

## 5 Related Works

The training of transformer architectures with self-supervised learning has seen significant advancements in both contrastive and non-contrastive training. Among self-supervised learning methods,

Figure 6: Uniformity Analysis. **(a)** Under SepNorm, the uniformity of the \([]\) embeddings are better preserved on the STL10 and Aircraft datasets. **(b)** Uniformity is positively related to the downstream task performance – lower uniformity results in higher classification accuracy on the STL10 dataset.

non-contrastive ones do not rely on negative samples for learning. They have emerged as a powerful approach for training transformer models and demonstrated remarkable successes in various tasks. BERT  and RoBERTa  were proposed in the NLP domain. Additionally, there are some works focus on the specific task, such as speech recognition , image generation , and heterogeneous graph generation .

Contrastive methods on the contrary train networks using positive and negative samples that are constructed without manual labeling. They have also been used to train transformer-based architectures. Gao et al.  and Zhang et al.  make significant strides in natural language processing tasks, while Chen et al.  provide valuable insights into the pre-training of transformers. Meanwhile, the potential of contrastive methods in vision transformers has been demonstrated by Caron et al.  and Radford et al. . These collective efforts underscore the versatility and efficacy of contrastive methods in self-supervised learning of transformers.

Normalization layers, including layer normalization and batch normalization, are essential to transformer architectures because they help stabilize the training procedure and accelerate convergence. Xiong et al.  delve into the role of layer normalization in the transformer architecture and provide insights about how the layer improves the training stability and the performance of transformers. Similarly, Xu et al.  explores the intricacies of layer normalization and offers potential enhancements to its effectiveness. To address the limitations of traditional batch normalization in a transformer architecture, Shen et al.  introduces a new normalization layer, Powernorm, which is a variant of batch normalization. Nguyen and Salazar  focus on the normalization process in the self-attention mechanism of transformers and propose methods to optimize the normalization of self-attention. All the efforts above underscore the critical role of normalization layers in transformer models.

## 6 Conclusion

In this work, we have introduced SepNorm to separate the normalization of \([]\) embeddings from that of other tokens. Across three application domains (images, text, and graphs), SepNorm shows consistent performance improvement when it is incorporated into transformer models. Our analysis shows that SepNorm promotes uniformity of \([]\) embeddings and thus enhances the transformers' ability to encode information. As a valuable technique for improving the foundational transformer architecture, SepNorm has the potential to benefit a wide range of applications.