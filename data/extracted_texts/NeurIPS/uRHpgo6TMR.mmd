# Sampling weights of deep neural networks

Erik Lien Bolager\({}^{+}\)  Iryna Burak\({}^{+}\)  Chinmay Datar\({}^{++}\)  Qing Sun\({}^{+}\)  Felix Dietrich\({}^{+}\)

Technical University of Munich

\({}^{+}\)School of Computation, Information and Technology; \({}^{}\)Institute for Advanced Study

Corresponding author, felix.dietrich@tum.de.

###### Abstract

We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data to sample shallow and deep networks. We prove that sampled networks are universal approximators. For Barron functions, we show that the \(L^{2}\)-approximation error of sampled shallow networks decreases with the square root of the number of neurons. Our sampling scheme is invariant to rigid body transformations and scaling of the input data, which implies many popular pre-processing techniques are not required. In numerical experiments, we demonstrate that sampled networks achieve accuracy comparable to iteratively trained ones, but can be constructed orders of magnitude faster. Our test cases involve a classification benchmark from OpenML, sampling of neural operators to represent maps in function spaces, and transfer learning using well-known architectures.

## 1 Introduction

Training deep neural networks involves finding all weights and biases. Typically, iterative, gradient-based methods are employed to solve this high-dimensional optimization problem. Randomly sampling all weights and biases before the last, linear layer circumvents this optimization and results in much shorter training time. However, the drawback of this approach is that the probability distribution of the parameters must be chosen. Random projection networks  or extreme learning machines  involve weight distributions that are completely problem- and data-agnostic, e.g., a normal distribution. In this work, we introduce a data-driven sampling scheme to construct weights and biases close to gradients of the target function (cf. Figure 1). This idea provides a solution to three main challenges that have prevented randomly sampled networks to compete successfully against iterative training in the setting of supervised learning: deep networks, accuracy, and interpretability.

Figure 1: Random feature models choose weights in a data-agnostic way, compared to sampling them where it matters: at large gradients. The arrows illustrate where the network weights are placed.

**Deep neural networks.** Random feature models and extreme learning machines are typically defined for networks with a single hidden layer. Our sampling scheme accounts for the high-dimensional ambient space that is introduced after this layer, and thus deep networks can be constructed efficiently.

**Approximation accuracy.** Gradient-based, iterative approximation can find accurate solutions with a relatively small number of neurons. Randomly sampling weights using a data-agnostic distribution often requires thousands of neurons to compete. Our sampling scheme takes into account the given training data points and function values, leading to accurate and width-efficient approximations. The distribution also leads to invariance to orthogonal transformations and scaling of the input data, which makes many common pre-processing techniques redundant.

**Interpretability.** Sampling weights and biases completely randomly, i.e., without taking into account the given data, leads to networks that do not incorporate any information about the given problem. We analyze and extend a recently introduced weight construction technique  that uses the direction between pairs of data points to construct individual weights and biases. In addition, we propose a sampling distribution over these data pairs that leads to efficient use of weights; cf. Figure 1.

## 2 Related work

**Regarding random Fourier features,** Li et al.  and Liu et al.  review and unify theory and algorithms of this approach. Random features have been used to approximate input-output maps in Banach spaces  and solve partial differential equations [16; 48; 10]. Gallicchio and Scardapane  provide a review of deep random feature models, and discuss autoencoders and reservoir computing (resp. echo-state networks ). The latter are randomly sampled, recurrent networks to model dynamical systems . **Regarding construction of features,** Monte Carlo approximation of data-dependent parameter distributions is used towards faster kernel approximation [1; 59; 47]. Our work differs in that we do not start with a kernel and decompose it into random features, but we start with a practical and interpretable construction of random features and then discuss their approximation properties. This may also help to construct activation functions similar to collocation . Fiedler et al.  and Fornasier et al.  prove that for given, comparatively small networks with one hidden layer, all weights and biases can be recovered exactly by evaluating the network at specific points in the input space. The work of Spek et al.  showed a certain duality between weight spaces and data spaces, albeit in a purely theoretical setting. Recent work from Bollt  analyzes individual weights in networks by visualizing the placement of ReLU activation functions in space. **Regarding approximation errors and convergence rates of networks,** Barron spaces are very useful [2; 20], also to study regularization techniques, esp. Tikohnov and Total Variation . A lot of work [54; 19; 17; 57; 65] surrounds the approximation rate of \((m^{-1/2})\) for neural networks with one hidden layer of width \(m\), originally proved by Barron . The rate, but not the constant, is independent of the input space dimension. This implies that neural networks can mitigate the curse of dimensionality, as opposed to many approximation methods with fixed, non-trained basis functions , including random feature models with data-agnostic probability distributions. The convergence rates of over-parameterized networks with one hidden layer is considered in , with a comparison to the Monte Carlo approximation. In our work, we prove the same convergence rate for our networks. **Regarding deep networks,** E and Wojtowytsch [17; 18] discuss simple examples that are not Barron functions, i.e., cannot be represented by shallow networks. Shallow  and deep random feature networks  have also been analyzed regarding classification accuracy. **Regarding different sampling techniques,** Bayesian neural networks are prominent examples [49; 5; 26; 61]. The goal is to learn a good posterior distribution and ultimately express uncertainty around both weights and the output of the network. These methods are computationally often on par with or worse than iterative optimization. In this work, we directly relate data points and weights, while Bayesian neural networks mostly employ distributions only over the weights. Generative modeling has been proposed as a way to sample weights from existing, trained networks [56; 51]. It may be interesting to consider our sampled weights as training set in this context. In the lottery ticket hypothesis [25; 8], "winning" subnetworks are often not trained, but selected from a randomly initialized starting network, which is similar to our approach. Still, the score computation during selection requires gradient updates. Most relevant to our work is the weight construction method by Galaris et al. , who proposed to use pairs of data points to construct weights. Their primary goal was to randomly sample weights that capture low-dimensional structures. No further analysis was provided, and only a uniform distribution over the data pairs was used. We expand and analyze their setting here.

Mathematical framework

We introduce sampled networks, which are neural networks where each pair of weight and bias of all hidden layers is completely determined by two points from the input space. This duality between weights and data has been shown theoretically , here, we provide an explicit relation. The weights are constructed using the difference between the two points, and the bias is the inner product between the weight and one of the two points. After all hidden layers are constructed, we must only solve an optimization problem for the coefficients of a linear layer, mapping the output from the last hidden layer to the final output. We start to formalize this construction by introducing some notation.

Let \(^{D}\) be the input space with \(\) being the Euclidean norm with inner product \(,\). Further, let \(\) be a neural network with \(L\) hidden layers, parameters \(\{W_{l},b_{l}\}_{l=1}^{L+1}\), and activation function \(:\). For \(x\), we write \(^{(l)}(x)=(W_{l}^{(l-1)}(x)-b_{l})\) as the output of the \(l\)th layer, with \(^{(0)}(x)=x\). The two activation functions we focus on are the rectified linear unit (ReLU), \((x)=\{x,0\}\), and the hyperbolic tangent (tanh). We set \(N_{l}\) to be the number of neurons in the \(l\)th layer, with \(N_{0}=D\) and \(N_{L+1}\) as the output dimension. We write \(w_{l,i}\) for the \(i\)th row of \(W_{l}\) and \(b_{l,i}\) for the \(i\)th entry of \(b_{l}\). Building upon work of Galaris et al. , we now introduce sampled networks. The probability distribution to sample pairs of data points is arbitrary here, but we will refine it in Definition 2. We use \(\) to denote the loss of our network we would like to minimize.

**Definition 1**.: _Let \(\) be a neural network with \(L\) hidden layers. For \(l=1,,L\), let \((x_{0,i}^{(1)},x_{0,i}^{(2)})_{i=1}^{N_{l}}\) be pairs of points sampled over \(\). We say \(\) is a sampled network if the weights and biases of every layer \(l=1,2,,L\) and neurons \(i=1,2,,N_{l}\), are of the form_

\[w_{l,i}=s_{1}^{(2)}}{\|x_{l-1,i}^{(2)}-x_{l-1,i}^{(1)}}\,, b _{l,i}= w_{l,i},x_{l-1,i}^{(1)}+s_{2},\] (1)

_where \(s_{1},s_{2}\) are constants, \(x_{l-1,i}^{(j)}=^{(l-1)}(x_{0,i}^{(j)})\) for \(j=1,2\), and \(x_{l-1,i}^{(1)} x_{l-1,i}^{(2)}\). The last set of weights and biases are \(W_{L+1},b_{L+1}=(W_{L+1}^{(L)}()-b_{L+1})\)._

The constants \(s_{1},s_{2}\) are used to fix what values the activation function takes on when it is applied to the points \(x^{(1)},x^{(2)}\); cf. Figure 2. For ReLU, we set \(s_{1}=1\) and \(s_{2}=0\), so that \((x^{(1)})=0\) and \((x^{(2)})=1\). For tanh, we set \(s_{1}=2s_{2}\) and \(s_{2}=(3)/2\), which implies \((x^{(1)})=1/2\) and \((x^{(2)})=-1/2\), respectively, and \((1/2(x^{(1)}+x^{(2)}))=0\). This means that in a regression problem with ReLU, we linearly interpolate values between the two points. For classification, the tanh construction introduces a boundary if \(x^{(1)}\) belongs to a different class than \(x^{(2)}\). We will use this idea later to define a useful distribution over pairs of points (cf. Definition 2).

The space of functions that sampled networks can approximate is not immediately clear. First, we are only using points in the input space to construct both the weights and the biases, instead of letting them take on any value. Second, there is a dependence between the bias and the direction of the weight. Third, for deep networks, the sampling space changes after each layer. These apparent restrictions require investigation into which functions we can approximate. We assume that the input space in Theorem 1 and Theorem 2 extends into its ambient space \(^{D}\) as follows. Let \(^{}\) be any compact subset of \(^{D}\) with finite reach \(>0\). Informally, such a set has a boundary that does not change too quickly . We then set the input space \(\) to be the space of points including

Figure 2: Placement of the point pairs \(x^{(1)},x^{(2)}\) for activation functions ReLU (left) and tanh (right). Two data pairs are chosen in each subfigure, resulting in two activation functions on each data domain.

[MISSING_PAGE_FAIL:4]

Using this distribution also comes with the benefit that sampling and training are not affected by rigid body transformations (affine transformation with orthogonal matrix) and scaling, as long as the true function \(f\) is equivariant w.r.t. to the transformation. That is, if \(H\) is such a transformation, we say \(f\) is equivariant with respect to \(H\), if there exists a scalar and rigid body transformation \(H^{}\) such that \(H^{}(f(x))=f(H(x))\) for all \(x\), and invariant if \(H^{}\) is the identity function. We also assume that norms \(_{}\) and \(_{_{0}}\) in Equation (2) are chosen such that orthogonal matrix multiplication is an isometry.

**Theorem 3**.: _Let \(f\) be the target function and equivariant w.r.t. to a scalar and rigid body transformation \(H\). If we have two sampled networks, \(,\), with the same number of hidden layers \(L\) and neurons \(N_{1},,N_{L}\), where \(^{N_{L+1}}\) and \( H()^{N_{L+1}}\), then the following statements hold for all \(x\):_

1. _If_ \(_{0,i}^{(1)}=H(x_{0,i}^{(1)})\) _and_ \(_{0,i}^{(2)}=H(x_{0,i}^{(2)})\) _for all_ \(i=1,2,,N_{1}\)_, then_ \(^{(1)}(x)=^{(1)}(H(x))\)_._
2. _If_ \(f\) _is invariant w.r.t._ \(H\)_, then for any parameters of_ \(\)_, there exist parameters of_ \(\) _such that_ \((x)=(H(x))\)_, and vice versa._
3. _The probability measure_ \(P\) _over the parameters is invariant under_ \(H\)_._

_Sketch of proof:_ Any neuron in the sampled network can be written as \(( s_{1}},x-x^{(1)}-s_{2})\). As we divide by the square of the norm of \(w\), the scalar in \(H\) cancels. There is a difference between two points in both inputs of \(,\), which means the translation cancels. Orthogonal matrices cancel due to isometry. When \(f\) is invariant with respect to \(H\), the loss function is also unchanged and lead to the same output. Similar argument is made for \(P\), and the theorem follows (cf. Appendix A.3).

If the input is embedded in a higher-dimensional ambient space \(^{D^{}}\), with \(D<D^{}\), we sample from a subspace with dimension \(=(\{\}}) D^{}\). All the results presented in this section still hold due to orthogonal decomposition. However, the standard approach of backpropagation and initialization allows the weights to take on any value in \(^{D^{}}\), which implies a lot of redundancy when \( D^{}\). The biases are also more relevant to the input space than when initialized to zero -- potentially avoiding the issues highlighted by Holzmuller and Steinwart . For these reasons, we have named the proposed method Sampling Where It Matters (SWIM), which is summarized in Algorithm 1. For computational reasons, we consider a random subset of all possible pairs of training points when sampling weights and biases.

We end this section with a time and memory complexity analysis of Algorithm 1. In Table 1, we list runtime and memory usage for three increasingly strict assumptions. The main assumption is that the dimension of the output is less than or equal to the largest dimension of the hidden layers. This is true for the problems we consider, and the difference in runtime without this assumption is only reflected in the linear optimization part. The term \( N/M\), i.e., integer ceiling of \(N/M\), is required because only a subset of points are considered when sampling. For the full analysis, see Appendix F.

## 4 Numerical experiments

We now demonstrate the performance of Algorithm 1 on numerical examples. Our implementation is based on the numpy and scipy Python libraries, and we run all experiments on a machine with 32GB system RAM (256GB in Section 4.3 and Section 4.4) and a GeForce 4x RTX 3080 Turbo GPU with 10GB RAM. The Appendix contains detailed information on all experiments. In Section 4.1 we compare sampling to random Fourier feature models regarding the approximation of a Barron function. In Section 4.2 we compare classification accuracy of sampled networks to iterative, gradient-based optimization in a classification benchmark with real datasets. In Section 4.3 we demonstrate that more specialized architectures can be sampled, by constructing deep neural architectures as PDE solution operators. In Section 4.4 we show how to use sampling of fully-connected layers for transfer learning. For the probability distribution over the pairs in Algorithm 1, we always choose the \(L^{}\) norm for \(_{}\) and for \(l=1,2,,L\), we choose the \(L^{2}\) norm for \(_{_{l-1}}\). The code to reproduce the experiments from the paper, and an up-to-date code base, can be found at

https://gitlab.com/felix.dietrich/swimnetworks-paper,

https://gitlab.com/felix.dietrich/swimnetworks.

**Algorithm 1:** The SWIM algorithm, for activation function \(\), and norms on input, output of the hidden layers, and output space, \(\|\|_{_{0}},\|\|_{_{l}}\), and \(\|\|_{}\) respectively. Also, \(\) is a loss function, which in our case is always \(L^{2}\) loss, and \((,)\) becomes a linear optimization problem.

**Constant :**\(_{>0}\), \(_{>0}\), \(L_{>0}\), \(\{N_{l}_{>0}\}_{l=1}^{L+1}\), and \(s_{1},s_{2}\)

**Data:**\(X=\{x_{i} x_{i}^{D},i=1,2,,M\}\),

\(Y=\{y_{i} f(x_{i})=y_{i}^{N_{L+1}},i=1,2,,M\}\)

\(^{(0)}(x)=x\);

**for**\(l=1,2,,L\)**do

\(}{M} M\) ;

\(P^{(l)}^{};P^{(l)}_{i} 0 i\);

\(=\{(x^{(1)}_{i},x^{(2)}_{i})^{(l-1)}(x^{(1)}_{i}) ^{(l-1)}(x^{(2)}_{i})\}_{i=1}^{}(X X)\);

**for**\(i=1,2,,\)**do

\(^{(1)}_{i},^{(2)}_{i}^{(l-1)}(x^{(1)}_{i}), ^{(l-1)}(x^{(2)}_{i})\);

\(^{(1)}_{i},^{(2)}_{i}=f(x^{(1)}_{i}),f(x^{(2)}_{i})\);

\(P^{(l)}_{i}^{(2)}_{i}-^{(1)}_{i}\|_{ }}{\{\|^{(2)}_{i}-^{(1)}_{i}\|_{_{ l-1}},\}}\);

**end**

\(W_{l}^{N_{l},N_{l-1}},b_{l}^{N_{l}}\);

**for**\(k=1,2,,N_{l}\)**do

Sample \((x^{(1)},x^{(2)})\) from \(\), with replacement and with probability proportional to \(P^{(l)}\);

\(^{(1)},^{(2)}^{(l-1)}(x^{(1)}),^{(l-1)}( x^{(2)})\);

\(W^{(k,:)}_{l} s_{1}^{(2)}-^{(1)}}{\|^{(2)} -^{(1)}\|^{2}}\); \(b^{(k)}_{l} W^{(k,:)}_{l},^{(1)}+s_{2}\);

**end**

\(^{(l)}()(W_{l}\,^{(l-1)}()-b_{l})\);

**end**

\(W_{L+1},b_{L+1}(^{(L)}(X),Y)\);

**return**\(\{W_{l},b_{l}\}_{l=1}^{L+1}\)

  & Runtime & Memory \\  Assumption (I) & \((L M(\{ N/M,M\}+N^{2}))\) & \((M\{ N/M,M\}+LN^{2})\) \\ Assumption (II) & \((L M( N/M+N^{2}))\) & \((M N/M+LN^{2})\) \\ Assumption (III) & \((M)\) & \((M)\) \\ 

Table 1: Runtime and memory requirements for training sampled neural networks with the SWIM algorithm, where \(N=\{N_{0},N_{1},N_{2},,N_{L}\}\). Assumption (I) is that the output dimension is less than or equal to \(N\). Assumption (II) adds that \(N<M^{2}\), i.e., number of neurons and input dimension is less than the size of dataset squared. Assumption (III) requires a fixed architecture.

### Illustrative example: approximating a Barron function

We compare random Fourier features and our sampling procedure on a test function for neural networks : \(f(x)=(\|x-a\|-\|x+a\|),\) with \(x^{D}\) and the vector \(a^{D}\) defined by \(a_{j}=2j/D-1\), \(j=1,2,,D\). The Barron norm of \(f\) is equal to one for all input dimensions, and it can be represented exactly with a network with one infinitely wide hidden layer, ReLU activation, and weights uniformly distributed on a sphere of radius \(D^{1/2}\). We approximate \(f\) using networks \(\) of up to three hidden layers. The error is defined by \(e_{}^{2}=_{x}(f(x)-(x))^{2}/_{x }f(x)^{2}\). We compare this error over the domain \(=[-1,1]^{2}\), with \(10,000\) points sampled uniformly, separately for training and test sets. For random features, we use \(w N(0,1),\ b U(-,)\), as proposed in , and \(=\). For sampling, we also use \(=\) to obtain a fair comparison. We also observed similar accuracy results when repeating the experiment with the \(\) function. The number of neurons \(m\) is the same in each hidden layer and ranges from \(m=64\) up to \(m=4096\). Figure 3 shows results for \(D=5,10\) (results are similar for \(D=2,3,4\), and sampled networks can be constructed as fast as the random feature method, cf. Appendix B).

Random features here have comparable accuracy for networks with one hidden layer, but very poor performance for deeper networks. This may be explained by the much larger ambient space dimension of the data after it is processed through the first hidden layer. With our sampling method, we obtain accurate results even with more layers. The convergence rate for \(D<10\) seems to be faster than the theoretical rate.

### Classification benchmark from OpenML

We use the "OpenML-CC18 Curated Classification benchmark"  with all its 72 tasks to compare our sampling method to the Adam optimizer . With both methods, we separately perform neural architecture search, changing the number of hidden layers from \(1\) to \(5\). All layers always have \(500\) neurons. Details of the training are in Appendix C. Figure 4 shows the benchmark results. On all tasks, sampling networks is faster than training them iteratively (on average, \(30\) times faster). The classification accuracy is comparable (cf. Figure 4, second and third plot). The best number of layers for each problem is slightly higher for the Adam optimizer (cf. Figure 4, fourth plot).

Figure 4: Fitting time, accuracy, and number of layers using weight sampling, compared to training with the Adam optimizer. The best architecture is chosen separately for each method and each problem, by evaluating 10-fold cross-validation error over \(1\)-\(5\) layers with 500 neurons each.

Figure 3: Relative \(L^{2}\) approximation error of a Barron function (test set), using random features and sampling, both with sine activation. Left: input dimension \(D=5\). Right: input dimension \(D=10\).

### Deep neural operators

We sample deep neural operators and compare their speed and accuracy to iterative gradient-based training of the same architectures. As a test problem, we consider Burgers' equation, \(+u=u}{ x^{2}},x(0,2),t(0,1],\) with periodic boundary conditions and viscosity \(=0.1.\) The goal is to predict the solution at \(t=1\) from the initial condition at \(t=0\). Thus, we construct neural operators that represent the map \(:u(x,0) u(x,1)\). We generate initial conditions by sampling five Fourier coefficients of lowest frequency and restoring the function values from these coefficients. Using a classical numerical solver, we generate 15000 pairs of \((u(x,0),u(x,1))\), and split them into the train (60%), validation (20%), and test sets (20%). Figure 5 shows samples from the generated dataset.

#### 4.3.1 Fully-connected network in signal space

The first baseline for the task is a fully-connected network (FCN) trained with tanh activation to predict the discretized solution from the discretized initial condition. We trained the classical version using the Adam optimizer and the mean squared error as a loss function. We also performed early stopping based on the mean relative \(L^{2}\)-error on the validation set. **For sampling**, we use Algorithm 1 to construct a fully-connected network with tanh as the activation function.

#### 4.3.2 Fully-connected network in Fourier space

Similarly to Poli et al. , we train a fully-connected network in Fourier space. For training, we perform a Fourier transform on the initial condition and the solution, keeping only the lowest frequencies. We always split complex coefficients into real and imaginary parts, and train a standard FCN on the transformed data. The reported metrics are in signal space, i.e., after inverse Fourier transform. **For sampling**, we perform exactly the same pre-processing steps.

#### 4.3.3 POD-DeepONet

The third architecture considered here is a variation of a deep operator network (DeepONet) architecture . The original DeepONet consists of two trainable components: the trunk net, which transforms the coordinates of an evaluation point, and the branch net, which transforms the function values on some grid. The outputs of these nets are then combined into the predictions of the whole network \((u)(y)=_{k=1}^{p}b_{k}(u)t_{k}(y)+b_{0},\) where \(u\) is a discretized input function; \(y\) is an evaluation point; \([t_{1},,t_{p}]^{T}^{p}\) are the \(p\) outputs of the trunk net; \([b_{1},,b_{p}]^{T}^{p}\) are the \(p\) outputs of the branch net; and \(b_{0}\) is a bias. DeepONet sets no restrictions on the architecture of the two nets, but often fully-connected networks are used for one-dimensional input. POD-DeepONet proposed by Lu et al.  first assumes that evaluation points lie on the input grid. It performs proper orthogonal decomposition (POD) of discretized solutions in the train data and uses its components instead of the trunk net to compute the outputs \((u)(_{j})=_{k=1}^{p}b_{k}(u)_{k}(_{j})+_{0}(_ {j}).\) Here \([_{1}(_{j}),,_{p}(_{j})]\) are \(p\) precomputed POD components for a point \(_{j}\), and \(_{0}(_{j})\) is the mean of discretized solutions evaluated at \(_{j}\). Hence, only the branch net is trained in POD-DeepONet. We followed Lu et al.  and applied scaling of \(1/p\) to the network output. **For sampling**, we employ orthogonality of the components and turn POD-DeepONet into a fully-connected network. Let \(=[_{1},,_{n}]\) be the grid used to discretize the input function \(u\) and evaluate the output function \((u)\). Then the POD components of the training data are \(()=[_{1}(),,_{p}()]^{n p}.\) If \(b(u)^{p}\) is the output vector of the trunk net, the POD-DeepONet transformation can be written \((u)()= b(u)+_{0}()\). As \(^{T}=I_{p}\), we can express the output of the trunk net as \(b(u)=^{T}((u)()-_{0}()).\) Using this equation, we can transform the training data to sample a fully-connected network for \(b(u)\). We again use tanh as the activation function for sampling.

#### 4.3.4 Fourier Neural Operator

The concept of a Fourier Neural Operator (FNO) was introduced by Li et al.  to represent maps in function spaces. An FNO consists of Fourier blocks, combining a linear operator in the Fourier space and a skip connection in signal space. As a first step, FNO lifts an input signal to a higher dimensional channel space. Let \(v_{t}^{n d_{v}}\) be an input to a Fourier block having \(d_{v}\) channels and discretized with \(n\) points. Then, the output \(v_{t+1}\) of the Fourier block is computed as \(v_{t+1}=(F_{k}^{-1}(R F_{k}(v_{t}))+W v_{t})^{n  d_{v}}\). Here, \(F_{k}\) is a discrete Fourier transform keeping onlythe \(k\) lowest frequencies and \(F_{k}^{-1}\) is the corresponding inverse transform; \(\) is an activation function; - is a spectral convolution; \(\) is a \(1 1\) convolution with bias; and \(R^{k d_{v} d_{v}}\), \(W^{d_{v} d_{v}}\) are learnable parameters. FNO stacks several Fourier blocks and then projects the output signal to the target dimension. The projection and the lifting operators are parameterized with neural networks. **For sampling**, the construction of convolution kernels is not possible yet, so we cannot sample FNO directly. Instead, we use the idea of FNO to construct a neural operator with comparable accuracy. Similar to the original FNO, we normalize the input data and append grid coordinates to it before lifting. Then, we draw the weights from a uniform distribution on \([-1,1]\) to compute the \(1 1\) lifting convolution. We first apply the Fourier transform to both input and target data, and then train a fully-connected network for each channel in Fourier space. We use skip connections, as in the original FNO, by removing the input data from the lifted target function during training, and then add it before moving to the output of the block. After sampling and transforming the input data with the sampled networks, we apply the inverse Fourier transform. After the Fourier block(s), we sample a fully-connected network that maps the signal to the solution.

The results of the experiments in Figure 5 show that sampled models are comparable to the Adam-trained ones. The sampled FNO model does not directly follow the original FNO architecture, as we are able to only sample fully-connected layers. This shows the advantage of gradient-based methods: as of now, they are applicable to much broader use cases. These experiments showcase one of the main advantages of sampled networks: speed of training. We run sampling on the CPU; nevertheless, we see a significant speed-up compared to Adam training performed on GPU.

### Transfer learning

Training deep neural networks from scratch involves finding a suitable neural network architecture  and hyper-parameters [3; 66]. Transfer learning aims to improve performance on the target task by leveraging learned feature representations from the source task. This has been successful in image classification , multi-language text classification, and sentiment analysis [63; 9]. Here, we compare the performance of sampling with iterative training on an image classification transfer learning task. We choose the CIFAR-10 dataset , with 50000 training and 10000 test images. Each image has dimension \(32 32 3\) and must be classified into one of the ten classes. We consider ResNet50 , VGG19 , and Xception , all pre-trained on the ImageNet dataset . We freeze the weights of all convolutional layers and append one fully connected hidden layer and one output layer. We refer to these two layers as the classification head, which we sample with Algorithm 1 and compare the classification accuracy against iterative training with the Adam optimizer.

Figure 6 (left) shows that for a pre-trained ResNet50, the test accuracy using the sampling approach is higher than the Adam training approach for a width greater than 1024. We observe similar qualitative behavior for VGG19 and Xception (figures in Appendix E). Figure 6 (middle) shows that the sampling approach results in a higher test accuracy with all three pre-trained models. Furthermore, the deviation in test accuracy obtained with the sampling algorithm is very low, demonstrating that sampling is more robust to changing random seeds than iterative training. After fine-tuning the whole neural network with the Adam optimizer with a learning rate of \(10^{-5}\), the test accuracies of sampled networks are close to the iterative approach. Thus, sampling provide a good starting point for fine-tuning the entire model. A comparison for the three models before and after fine-tuning is contained in Appendix E. Figure 6 (right) shows that sampling is up to two orders of magnitude faster than iterative training for

Figure 5: Left: Samples of initial conditions \(u_{0}\) and corresponding solutions \(u_{1}\) for Burgersâ€™ equation. Right: Parameters of the best model for each architecture, the mean relative \(L^{2}\) error on the test set, and the training time. We average the metrics across three runs with different random seeds.

smaller widths, and around ten times faster for a width of 2048. In summary, Algorithm 1 is much faster than iterative training, yields a higher test accuracy for certain widths before fine-tuning, and is more robust with respect to changing random seeds. The sampled weights also provide a good starting point for fine-tuning of the entire model.

## 5 Broader Impact

Sampling weights through data pairs at large gradients of the target function offers improvements over random feature models. In terms of accuracy, networks with relatively large widths can even be competitive to iterative, gradient-based optimization. Constructing the weights through pairs of points also allows to sample deep architectures efficiently. Sampling networks offers a straightforward interpretation of the internal weights and biases, namely, which data pairs are important. Given the recent critical discussions around fast advancement in artificial intelligence, and calls to slow it down, publishing work that potentially speeds up the development (concretely, training speed) in this area by orders of magnitude may seem irresponsible. The solid mathematical underpinning of random feature models and, now, sampled networks, combined with much greater interpretability of the individual steps during network construction, should mitigate some of these concerns.

## 6 Conclusion

We present a data-driven sampling method for fully-connected neural networks that outperforms random feature models in terms of accuracy, and in many cases is competitive to gradient-based optimization. The time to obtain a trained network is orders of magnitude faster compared to gradient-based optimization. In addition, much fewer hyperparameters need to be optimized, as opposed to learning rate, number of training epochs, and type of optimizer.

Several open issues remain, we list the most pressing here. Many architectures like convolutional or transformer networks cannot be sampled with our method yet, and thus must still be trained with iterative methods. Implicit problems, such as the solution to PDE without any training data, are a challenge, as our distribution over the data pairs relies on known function values from a supervised learning setting. Iteratively refining a random initial guess may prove useful here. On the theory side, convergence rates for Algorithm 1 beyond the default Monte-Carlo estimate are not available yet, but are important for robust applications in engineering.

In the future, hyperparameter optimization, including neural architecture search, could benefit from the fast training time of sampled networks. We already demonstrate benefits for transfer learning here, which may be exploited for other pre-trained models and tasks. Analyzing which data pairs are sampled during training may help to understand the datasets better. We did not show that our sampling distribution results in optimal weights, so there is a possibility of even more efficient heuristics. Applications in scientific computing may benefit most from sampling networks, as accuracy and speed requirements are much higher than for many tasks in machine learning.

Figure 6: Left: Train and test accuracies with different widths for ResNet50 (averaged over 5 random seeds). Middle: Test accuracy with different models with and without fine-tuning (width = 2048). Right: Adam training and sampling times of the classification head (averaged over 5 experiments).