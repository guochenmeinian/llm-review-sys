# Beyond accuracy: Tracking more like Human via Visual Search

Dailing Zhang\({}^{1,2}\) Shiyu Hu\({}^{5}\) Xiaokun Feng\({}^{1,2}\)

**Xuchen Li\({}^{1,2}\) Meiqi Wu\({}^{3}\) Jing Zhang\({}^{2}\) Kaiqi Huang\({}^{1,2,4}\)**

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{2}\)Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)School of Computer Science and Technology, University of Chinese Academy of Sciences

\({}^{4}\)Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences

\({}^{5}\)School of Physical and Mathematical Sciences, Nanyang Technological University

zhangdailing2023@ia.ac.cn, shiyu.hu@ntu.edu.sg, fengxiaokun2022@ia.ac.cn, wumeiqi18@mails.ucas.ac.cn, {lixuchen2024, jing_zhang, kqhuang}@ia.ac.cn

###### Abstract

Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience. The recently proposed Central-Peripheral Dichotomy (CPD) theory sheds light on how humans effectively process visual information and track moving targets in complex environments. However, existing visual object tracking algorithms still fall short of matching human performance in maintaining tracking over time, particularly in complex scenarios requiring robust visual search skills. These scenarios often involve **S**patio-**T**emporal **D**iscontinuities (_i.e._, _STDChallenge_), prevalent in long-term tracking and global instance tracking. To address this issue, we conduct research from a human-like modeling perspective: (1) Inspired by the CPD, we propose a new tracker named **CPDTrack** to achieve human-like visual search ability. The central vision of CPDTrack leverages the spatio-temporal continuity of videos to introduce priors and enhance localization precision, while the peripheral vision improves global awareness and detects object movements. (2) To further evaluate and analyze _STDChallenge_, we create the _STDChallenge Benchmark_. Besides, by incorporating human subjects, we establish a human baseline, creating a high-quality environment specifically designed to assess trackers' visual search abilities in videos across _STDChallenge_. (3) Our extensive experiments demonstrate that the proposed CPDTrack not only achieves state-of-the-art (SOTA) performance in this challenge but also narrows the behavioral differences with humans. Additionally, CPDTrack exhibits strong generalizability across various challenging benchmarks. In summary, our research underscores the importance of human-like modeling and offers strategic insights for advancing intelligent visual target tracking. Code and models are available at https://github.com/ZhangDailing8/CPDTrack.

## 1 Introduction

In the real world, humans excel at locating an arbitrary moving target within complex backgrounds and can resume tracking it even after temporary loss. This enduring question in cognitive neuroscience has recently been explained to some extent by the Central-Peripheral Dichotomy (CPD) theory, which suggests that the human eyes processes all visual inputs by sorting them into central and peripheral visions . Central vision, decoded and understood by higher brain areas, focuses on interpreting details and minimizing distractions; while peripheral vision, processed in the primary visual cortex (V1), swiftly detects dynamic changes.

Meanwhile, in computer vision research, advancements in single object tracking (SOT) are increasingly closing the gap between human dynamic visual ability (DVA) and proxy tasks [3; 4; 5]. In 2013, the short-term tracking (STT) task and related benchmarks were proposed [6; 7; 8]. However, the implicit assumption of continuous motion simplifies this task to continuously locating the target in short videos of tens of seconds, which is far from human DVA. Drawing on human visual search skills, the VOT community is broadening its scope by incorporating long-term tracking (LTT) [9; 10], which includes target _absent_ (_i_.\(e\)., the moving target can disappear and reappear in tracking), and global instance tracking (GIT) , which involves _shotcut_ (_i_.\(e\)., the video sequence may include different viewpoints and scenes). We refer to these challenges collectively as **spatio-temporal discontinuity (_STDChallenge_)**, which demands enhanced visual search abilities from trackers, as shown in Fig. 1.

Unfortunately, mainstream machines have not kept pace with the aforementioned expansion of task definitions to achieve human performance. Most mainstream trackers [11; 12; 13; 14; 15; 16], influenced by the definition of STT, depend on the spatio-temporal continuity of the target. These trackers typically perform local cropping based on the previous frame's results within the motion model, a module for calculating the search region on the current frame, as shown in Fig. 2(b-1). Obviously, this modeling mechanism only simulates human central vision, making it far from replicating human visual search ability, especially in scenarios where the _STDChallenge_ increases significantly, as shown in Fig. 6(a). Several studies [17; 18] have also demonstrated that _STDChallenge_ presents a significant challenge for trackers. Thus, a natural question is: **what causes trackers to overlook the importance of the _STDChallenge_ in their modeling process**?

Some possible reasons from the benchmark perspective may answer the above question: **Limitation of datasets.** Although the definition of proxy tasks has been moving closer to human DVA (STT \(\) LTT \(\) GIT), most datasets are still influenced by the initial task characteristics (_i_.\(e\)., _single-target_, _model-free_, _causal-trackers_, _single-camera_, and _short-term_) in data collection process [19; 20; 21; 9], as in Fig. 10, resulting in the importance of the _STDChallenge_ being overlooked. Consequently, trackers can easily achieve good performance on most datasets without possessing sufficient visual search ability to handle the _STDChallenge_. **Limitation of evaluation.** Researchers contend that integrating insights from human DVA could address _STDChallenge_ and enhance the real-world robustness of machines [22; 3; 23]. However, the substantial differences in methodologies and experimental

Figure 1: Illustration of the _STDChallenge_, depicting the _absent_ of targets and _shotcut_. _STDChallenge_ is quite challenging, but CPDTrack can maintain robust tracking performance, demonstrating stronger visual search abilities compared to other trackers. (a) In the first column, most trackers fail; in the second column, they can recover; in the third column, a _STDChallenge_ occurs; in the fourth column, all trackers except for CPDTrack fail; and in the fifth column, this remains the case, meaning trackers’ limited recovery. (b) shows the status of the target within the sequence of “095” from VideoCube , and red dots means _shotcut_.

approaches between computer science and cognitive science make it difficult to determine if current technological advancements are truly bridging the gap between SOT machines and human DVA. Consequently, there is a notable absence of mechanisms to assess the true visual intelligence of trackers.

In summary, existing trackers lack consideration of human visual search ability in their modeling mechanisms, making it difficult to cope with the _STDChallenge_. This limitation is not only due to the algorithm design itself but also involves the limitations of datasets and evaluation. Therefore, this work aims to address these aspects (algorithms and benchmark) and verifies the relevant results through experimental analysis.

**A new algorithm CPDTrack (Section 3).** Our analysis of the tracker pipeline and understanding of the CPD suggests that mainstream motion models, which crop local regions from the current frame, as shown in Fig. 2, restrict the tracker's ability to locate moving targets. Drawing on the distinction between central and peripheral vision in CPD, we propose a new tracker named CPDTrack to divide the current frame accordingly. Central vision captures detailed information, leveraging the video's spatio-temporal continuity to introduce priors, while peripheral vision manages global information to enhance overall scene understanding, as shown in Fig. 3. CPDTrack also models the information query, achieving top-down control based on cognitive outcomes.

**A new _STDChallenge Benchmark_ with Visual Turing Test (Section 4).** To address the shortcomings of existing benchmarks environments, we introduce a specialized challenge environment named _STDChallenge Benchmark_ to assess the visual search abilities of both machines and humans in tracking an arbitrary moving target. This environment comprises sequences that represent the _STDChallenge_, carefully selected from both LTT and GIT benchmarks. By sampling from a variety of benchmarks, we ensure a reduction in dataset-specific biases. Simultaneously, we employ the Visual Turing Test (VTT)  on the _STDChallenge Benchmark_ to assess the disparity in intelligence between trackers and human DVA. Specifically, we examine error consistency  to compare the performance of humans and machines, and explore behavioral differences among machines with varying architectures and parameters in dynamic visual tasks. Building on these results, we conduct a thorough and detailed analysis to reveal the impact of the _STDChallenge_ on trackers.

**Comprehensive and integral experimental analyses (Section 5).** Finally, analysis results from extensive experimental settings show that, in the _STDChallenge_, CPDTrack not only achieved state-of-the-art (SOTA) performance, getting 1.7% superior N-PRE and 1.4% PRE score on _STDChallenge Benchmark_ compared to the second algorithm, as shown in Table 1, but also demonstrated remarkable alignment with human DVA, as shown in Fig. 4. Furthermore, it performed well across various tracking benchmarks, particularly excelling in difficult benchmarks .

**Contributions.** Our research explores the visual search ability of humans and trackers through detailed analyses based on algorithmic strategies and benchmark evaluations.

Figure 2: Comparison of tracking pipeline. Our CPDTrack differs from previous trackers in motion model. (a) The core components of mainstream tracking frameworks consist of a motion model, feature extraction, and a temporal module. (b-1) local tracker, which tracks targets in local areas and has difficulty recovering after failure; (b-2) global tracker, which tracks targets globally in the current frame, is susceptible to background interference and has lower efficiency; (b-3) local-global tracker, which can switch between the above two, depending on the performance of the local tracker; (b-4) our CPDTrack, which can model both central and peripheral information simultaneously.

Related Work

**Tracking Benchmark.** In 2013, the VOT competition characterized SOT using five key terms: _single-target_, _model-free_, _causal-trackers_, _single-camera_ and _short-term_. The initial three terms serve to differentiate SOT from other visual tasks [29; 30; 31; 32], whereas the last two, _single-camera_ and _short-term_, were introduced to simplify early-stage research [6; 7; 19; 21]. Since 2018, several researchers have moved beyond the _short-term_ limitation to embrace LTT [33; 9; 34], with the VOT setting a new criterion that tasks permitting complete target disappearance qualify as long-term . More recently, the introduction of GIT  has eliminated the _single-camera_ restriction, thus offering a more realistic simulation of the world. _STDChallenge Benchmark_ is dedicated to examining the distinctions between LTT and GIT compared to STT, and integrates human DVA to steer algorithmic advances.

**Visual Trackers.** Deep learning trackers are typically categorized by motion models into local and global trackers, as shown in Fig. 2. Local trackers, which are most popular method, crop a search region in current frame [11; 12; 13; 15; 35; 16; 36]. These trackers continuously improved by advancements in feature extractor and temporal analysis, have reached SOTA performance on various benchmarks [9; 19; 21]. To tackle _STDChallenge_, it is common to switch to a global detector when local trackers fail, though this transition depends on the local tracker's performance [37; 38; 39; 40; 41]. Local trackers often struggle with proactive global awareness, reducing their robustness against _STDChallenge_. Global trackers, conversely, aim to identify targets throughout the entire image, raising computational demands and easy to be confused by similar objects in the background. Due to these limitations, they are less commonly employed [42; 43]. CPDTrack is designed to model CPD by combining local and global information, thereby enabling precise and efficient global modeling and offering insights for addressing the _STDChallenge_. For a detailed analysis of these trackers, see Appendix D.

**Visual Turing Test.** Traditional evaluation methods, focused on "machine versus machine" comparisons, struggle to comprehensively assess the development of visual intelligence as they rely on predefined metrics in experimental settings. The VTT, at the intersection of turing tests and computer vision, is gaining attention for its "human versus machine" approach, which uses human visual capabilities as a baseline to fully evaluate machine effectiveness [26; 44; 45; 46; 26; 25; 24]. This paradigm has been applied in SOT that simulate human DVA: GIT demonstrates that humans can effectively resolve _STDChallenge_, and PathTracker [22; 47] has developed a circuit model to explain human decision-making processes. We are going to explore the visual search ability of humans and machines when tracking an arbitrary moving target.

## 3 Solving the _STDChallenge_

This section presents the proposed CPDTrack method in detail. We introduce CPD motion model, which divides global information into central and peripheral vision to optimize feature extraction. Additionally, we have developed a temporal clues module that serves as the information query component within CPD theory, facilitating top-down control mechanisms.

### CPD Motion Model

A crucial challenge is balancing the maintenance of local resolution and computational efficiency during the extraction of global information. To address this, we incorporate insights from cognitive neuroscience, specifically the fovea concept  and CPD, segmenting the current frame into central and peripheral. Their computation is divided into the following steps:

**Determine the position.** For the central vision, as shown in Fig. 3, we use the tracking result \(b_{t-1}=(b_{t-1}^{cx},b_{t-1}^{cy},w_{t-1},h_{t-1})\) on the previous frame to determine its position.

Central vision.To simulate the decay in visual sensitivity characteristic of the fovea, we employ gaussian distribution model--a method that has found widespread application in vision science [48; 49]. Visual sensitivity is defined as:

\[S(x)=S_{0} exp(-^{cx})^{2}}{2^{2}}),\] (1)where \(S(x)\) is sensitivity in \(x\) and \(S_{0}\) is the sensitivity in central. we set the \(=\) based on the overall size \((H,W)\) of the image and the 3-sigma rule. This approach ensures that the cumulative visual sensitivity to current frame exceeds 99.7\(\%\), effectively covering the vast majority of the visual field.

Sensitivity of central.We determine the sensitivity levels within this region using the values from \(b_{t-1}\). Integrating sensitivity as:

\[sens_{x-1}=_{b_{t-1}^{x_{1}}}^{b_{t-1}^{x_{2}}}S(x)dx,\] (2)

where \(b_{t-1}^{x_{1}}=b_{t-1}^{cx}-}{2}\) is the top-left coordinate of \(b_{t-1}\), and \(b_{t-1}^{x_{2}}=b_{t-1}^{cx}+}{2}\) is the bottom-right coordinate of \(b_{t-1}\).

Image crop resizing.As the central region is resized to a constant size, thus its width \(w_{t-1}^{e}}\) and \(w_{t-1}^{e} w_{t-1}\). Therefore, \(w_{t-1}^{e}\) is determined to be:

\[w_{t-1}^{e}=}{sens_{x-1}}=}{2(}{W})-1},\] (3)

where \(\) is a constant factor that can be adjusted, and \(\) is the cumulative density function of the standard normal distribution. Then, we get central region \(m_{t}=(b_{t-1}^{cx},b_{t-1}^{cy},w_{t-1}^{e},h_{t-1}^{e})\).

As the lower limit of the bounding box is set, typically at 10 pixels in tracking, equation (3) will not a division by zero error in practice.

Figure 3: The overall architecture of CPDTrack, referencing the latest one-stream trackers, models CPD. (a) The encoding-selection-decoding framework of CPD. **Visual selection** is the process of choosing the information to focus on, and **visual decoding** is the process of deeply understanding the selected features and information to make cognitive decisions. On the left is a mathematical model of visual selection, where we align human visual information processing with the resolution of devices and machines. (b) Architecture of the proposed CPDTrack: The original frame is treated as encoded visual data. We apply the acuity model described in (a) for visual selecting, which can be modulated by **information query** from temporal. A transformer is employed to replicate the decoding processes occurring in higher brain regions, facilitating complex cognitive tasks. The grey arrows running through both parts highlight the correspondence between the two parts.

We prove the range \(w^{e}_{t-1}\) in equation (3): First, take the derivative with respect to \(w^{e}_{t-1}(x)\), The range of values for \(x\) is \((0,W]\):

\[w^{e}_{t-1}{}^{{}^{}}(x)=)-1-xf( )}{(2(}{W})-1)^{2}},\] (4)

where \(f\) is the probability density function of the standard normal distribution. We define the numerator as \(n_{w}(x)\) and denominator as \(d_{w}(x)\)

It is clear that \(d_{w}(x) 0\) when \(x>0\); differentiate \(n_{w}(x)\):

\[n^{{}^{}}_{w}(x)=-}xf^{}().\] (5)

Clearly, \(f^{}<0\). Thus, \(n_{w}(x)\) has its minimum value at \(x=0\), \(n_{w}(x) 0\). Therefore, \(w^{e}_{t-1}(x)\)is a monotonically increasing function when \(x>0\). When \(x\) takes the maximum value, \(x=W\), \(w^{e}_{t-1}(x) 0.9987SW\).

Intuitively, the central area is determined by cropping around the target based on the \(b_{t-1}\). This differs from traditional motion models, which typically use a static context ratio for cropping. In contrast, the CPD model adapts the contextual range dynamically, offering more flexibility and precision, more visualization in B.

For the peripheral area, we adjust the entire image to match the size of the central area. This resizing facilitates the simulation of peripheral vision, aligning the scale both regions.

### Information query

In CPD , the selection process is influenced by cognitive feedback through a top-down approach, as illustrated in Fig. 3. Similarly to mode that humans can actively focus on areas of interest, CPDTrack incorporates an information query mechanism. Spatially, we determine central vision based on the results \(b_{t-1}\) of the previous frame. Semantically, we use the query to map high-level semantic information from the previous frame downwards, achieving top-down control. Specifically, each past frame is represented by a token while template is max-pooled into a query as initialization. This query, using cross-attention, extracts refined high-level semantic insights from multiple past frame tokens, then concatenated to images tokens and inputted into the ViT . Within the ViT backbone, the information query interacts with the global information of the current frame and the online template, achieving implicit cognitive feedback's top-down control. Simultaneously, this query integrates the spatio-temporal information of the current frame for use in future frame computations.

## 4 The _StdChallenge Benchmark_

_STDChallenge Benchmark_ is an integrated task closely associated with various object tracking subtasks [7; 10; 4]. We select sequences with _STDChallenge_ from established benchmarks based on specific sequence level attributes. Furthermore, inspired by cognitive psychology , we have designed a dynamic visual capability assessment framework to evaluate and compare the performances of humans and machines when tackling _STDChallenge_. This evaluation directly informs and guides machine development.

**Construct _STDChallenge Benchmark_.** Our objective is not to develop a new dataset to evaluate trackers' visual search ability. Rather, we aim to highlight the often-overlooked issue of _STDChallenge_, already acknowledged by few recent datasets but underemphasized in algorithms design. We have chosen existing benchmarks as our data sources to assess trackers' proficiency with _STDChallenge_, specifically selecting sequences from LaSOt , VOTLT2019 , and VideoCube  where such challenges are existing. Drawing inspiration from cinematography, we introduce the metric

\[STD=+n_{s}) l_{a}}{l^{2}},\] (6)

to quantify _STDChallenge_, where \(n_{a}\) means the number of _absent_, \(n_{s}\) means the number of _shotcut_, \(l_{a}\) means the length of _absent_ and \(l\) means the length of the sequence. This formula takes into account the frequency and duration of _absent_, and the frequency of _shotcut_. A higher \(STD\) value indicatesgreater _STDChallenge_. From LaSOT , VOTLT2019 , and VideoCube , we have compiled a dataset of 252 sequences that constitute the _STDChallenge Benchmark_, detailed attributes and distribution would typically be visualized in Appendix C.

**Visual Turing Test**. Our investigation begins by assessing whether individuals can effectively handle the _STDChallenge_. In line with methodologies from the field of static vision [26; 25], we organize the study using a small-N design [51; 26], enlisting five participants aged 20 to 30. During the experiment, participants watch videos and are instructed to track the target with a mouse cursor. The cursor's position is recorded at each frame by a background program. Sixteen videos (named _STDChallenge-Turing_) sampled from _STDChallenge Benchmark_ are displayed in 15 fps, each in a different order across participants. Prior to starting, participants receive detailed training to familiarize themselves with the experimental process and techniques, more details in Appendix E.

## 5 Experiments

### Visual Turing Test in _STDChallenge-Turing_

_Evaluation metrics._ Given that human typically focuses on a single point and emphasizes an object's salient features rather than its center, we have adjusted our evaluation metrics and outputs accordingly. For error consistency measurements refer to , human is successful if the point fall within the groundtruth while trackers is successful if \(IoU>0.5\) in a frame. For performance measurements, we employ the N-PRE metric , which evaluates the precision based on relative position between the center of boundingbox and groundtruth, to assess the performance of both humans and machines.

Figure 4: Quantitative indicators show that CPDTrack not only has higher accuracy in the _STDChallenge_ but also behaves more similarly to humans. (a) represents the N-PRE score on _STDChallenge-Turing_. (b) represents the distribution of N-PRE scores of humans and machines on various sequences of _STDChallenge-Turing_. (c) represents the error consistency between machines and humans, with kappa representing the average error consistency.

Figure 5: Human results do not necessarily mean correct, but humans can usually quickly re-locate the target after _STDChallenge_. In the upper row, humans can recognize environmental factors closely related to the target in the second image. In the lower row, even if the target is absent, humans are not distracted by the background in the second image. Humans are robust to occlusions in the fifth image.

_Experiments results._ In the _STDChallenge_, humans outperform most trackers, indicating that the DVA of trackers is still far from humans. Furthermore, while the performance of machines tends to vary significantly with \(STD\) changing, human consistently exhibit stable tracking ability, as shown in (b) of Fig. 4. Notably, as research progresses, the scores of SOTA trackers are gradually approaching those of humans, suggesting a narrowing gap between machine and human performance. Additionally, the low error consistency between trackers and humans suggest they employ different tracking strategies in past studies. There's a significant positive correlation between the N-PRE of trackers in _STDChallenge_ and their error consistency with humans, indicating that trackers are incorporating some implicit human-like strategies to improve performance progressively. Among them, CPDTracker, which explicitly models human strategies, achieved the highest score of \(0.167\). This method not only effectively tackles the _STDChallenge_ but also aids in guiding machines to exceed the biases of evaluation benchmarks and adapt to real-world scenarios.

_Some Reasoning._ (i) Like global trackers as shown in Fig. 2, humans process entire image information but can handle the _STDChallenge_ more effectively at higher frame rates. This demonstrates the superior efficiency of humans in video information processing. (ii) When a target is lost, humans not only quickly become aware of this but also swiftly reacquire the target. This process involves the integrated use of spatial cognition and temporal memory, demonstrating human DVA to understand global information, in contrast to mainstream trackers that rely on local information. (iii) Furthermore, our observations reveal that humans tend to focus on object's salient features over its center when tracking. This significantly leads to current evaluation metrics underestimating human capabilities, while also highlighting the clear target recognition ability of humans.

Moreover, we examined how different modules within the trackers' pipeline influence performance and error consistency. For detailed findings, refer to the Appendix F.

### Comparison with SOTA

_STDChallenge Benchmark._ The _STDChallenge Benchmark_ is a newly proposed benchmark for LTT that focuses on _STDChallenge_. Designed to refer human visual research ability, it tests capability of trackers to reacquire lost targets. CPDTrack shows improvements of \(1.7\%\)_N-PRE_ and \(1.4\%\)_PRE_ over the second-ranked tracker, as shown in Tab. 1. Given the long-tail distributions of _STDChallenge_, we charted performance fluctuations across different \(STD\). As illustrated in Fig. 6 (a), CPDTrack excels in robustness and significantly outperforms SOTA trackers, especially in sequences with increasing _STDChallenge_ (the last \(40\%\), consisting of 100 sequences). We assessed CPDTrack's ability to search targets during the _STDChallenge_. A recovery is deemed successful if IoU between bounding box and groundtruth exceeds 0.5. As illustrated in Fig. 6 (b), CPDTrack reliably maintains target tracking under _STDChallenge_. CPDTrack achieves a recovery success rate of 56.91% within the same time period, significantly outperforming the next best tracker, mixvit.

    &  &  &  \\ Motion Model & Method &  & PRE & SUC & N-PRE & PRE & SUC & N-PRE & SUC & Robust & AUC & P \\  CPD & CPDTrack & **84.2** & **73.3** & 65.9 & **82.9** & **67.1** & **70.4** & **89.5** & **75.6** & **75.3** & 66.1 & 73.0 \\   & SeqTrack  & 81.9 & 91.9 & 66.8 & 76.8 & 54.0 & 63.5 & 88.3 & 72.5 & 74.6 & **69.9** & **76.3** \\  & OSTrack  & 79.1 & 68.9 & 64.6 & 73.7 & 50.7 & 61.8 & 85.8 & 71.3 & 74.4 & 69.1 & 75.2 \\  & MixVT  & 82.5 & 71.6 & 66.7 & 76.9 & 52.2 & 63.1 & 88.5 & 72.7 & 74.7 & 69.6 & 75.9 \\  & STARR  & 80.7 & 68.2 & 64.5 & 76.3 & 49.4 & 62.1 & 86.8 & 70.4 & 74.5 & 67.1 & - \\  & KeepTrack  & 80.4 & 64.3 & 62.8 & 73.0 & 37.9 & 54.3 & 83.0 & 64.4 & 73.8 & 67.1 & 70.2 \\  & Ocean  & 57.1 & 39.9 & 40.7 & 53.9 & 19.5 & 34.2 & 74.8 & 51.2 & 73.7 & 56.0 & 56.6 \\  & SuperDMP  & 72.6 & 56.7 & 56.5 & 64.6 & 31.4 & 47.4 & 80.1 & 61.2 & 74.3 & 64.1 & - \\  & PJiMP  & 70.3 & 51.7 & 52.7 & 65.4 & 28.6 & 44.5 & 79.6 & 58.3 & 74.3 & 59.8 & 60.8 \\  & DiMP  & 65.9 & 47.0 & 48.6 & 54.6 & 18.7 & 37.1 & 77.2 & 56.0 & 74.0 & 56.9 & 56.7 \\  & SiamRPN  & 53.4 & 35.6 & 37.3 & 46.7 & 15.0 & 29.0 & 72.6 & 50.3 & 73.6 & - \\  & ATOM  & 57.8 & 39.8 & 40.8 & 43.6 & 14.0 & 26.7 & 75.2 & 53.1 & 73.8 & 51.5 & 50.5 \\  & KYS  & 60.1 & 42.6 & 44.5 & 49.3 & 17.1 & 33.7 & 80.1 & 59.4 & 73.3 & 55.4 & - \\  & SiamFC  & 33.6 & 21.2 & 20.6 & 15.8 & 3.6 & 7.4 & 52.1 & 35.6 & 72.7 & 33.6 & 33.9 \\  Local-Global & SPLT  & 60.9 & 38.2 & 40.3 & 56.5 & 15.7 & 33.7 & 72.4 & 47.6 & 73.5 & 39.9 & 42.6 \\  & DaSiamRPN  & 53.4 & 35.4 & 37.1 & 46.3 & 14.4 & 29.1 & 72.2 & 50.4 & 73.6 & 42.7 & 44.8 \\   & SiamRCNN  & 75.3 & 62.8 & 60.7 & 72.6 & 47.9 & 58.8 & 80.5 & 65.8 & 74.5 & 64.8 & - \\  & GlobalTrack  & 65.5 & 49.5 & 49.5 & 64.3 & 29.6 & 46.1 & 72.7 & 53.7 & 74.3 & 52.1 & 52.7 \\   

Table 1: State-of-the-art comparison on _STDChallenge_, VideoCube  and LaSOT . The top three results are highlight with red, blue and green fonts, respectively.

_VideoCube_. VideoCube is a detailed and challenging LTT benchmark aimed at simulating real-world complexities, including View-point changes and disappearances. It features two evaluation methods: One-Pass Evaluation (OPE) and Restarting OPE (R-OPE), the latter allows trackers to restart after failure to evaluate their robustness. CPDTrack outperforms the second-ranked machine across all performance metrics, with improvements ranging from \(1.0\%\) to \(13.1\%\). Additionally, there has been a notable enhancement in its robustness \(75.8\), as shown in Tab. 1.

_LaSOT_. LaSOT is a high-quality, large-scale benchmark for LTT featuring a test set of 280 videos with an average of 2448 frames each sequence. We attribute underperformance of CPDTrack to the limited _STDChallenge_ scenarios and varying interpretations of groundtruth within these tests as in Fig. 7.

### Ablation and Analysis

Through ablation studies, we validate the effectiveness of each module of CPDTrack in _STDChallenge_. We conducted ablation studies of CPDTrack in _STDChallenge Benchmark_ and VideoCube, comparing the impacts of "central-peripheral" vision dichotomy and the use of information query. Information query was removed in #1 as baseline to examine the performance of the CPD Motion Model itself. #2 is traditional local crop, which can be considered as only containing central vision. #3 employed only peripheral vision. #4 is the proposed CPDTrack to validate the effectiveness of cognitive feedback control in ANN. Based on #2, peripheral vision and information query was added in #5 to demonstrate the effectiveness of our designed CPD Motion Model.

**The combination of central and peripheral vision performs better than using either alone.** It can be seen that performance metrics and error consistency of baseline #1 surpass those of #2 using only central vision and #3 using only peripheral vision, with improvements range 0.7% to 9.3% in various metrics on _STDChallenge Benchmark_ and VideoCube and more like human, as in Tab. 2. However,

   &  &  &  \\  & & consistency & N-PRE & PRE & SUC & N-PRE & PRE & SUC \\ 
1 & baseline & 0.159 & 83.8 & 72.9 & 65.6 & 82.7 & 67.0 & 70.2 \\
2 & central vision & 0.137 & 81.5 & 72.2 & **66.8** & 77.7 & 57.7 & 65.9 \\
3 & peripheral vision & 0.156 & 80.7 & 69.2 & 61.7 & 80.6 & 60.0 & 65.7 \\ 
4 & baseline+query & **0.167** & **84.2** & **73.3** & 65.9 & 82.9 & **67.1** & **70.4** \\
5 & local as central & 0.165 & 83.0 & 70.6 & 64.5 & **84.3** & 62.9 & 69.0 \\  

Table 2: Ablation studies of CPDTrack on _STDChallenge_ and VideoCube. We conducted comparisons of ’central-peripheral’ vision and information query.

Figure 6: We further emphasize the visual search capabilities of CPDTrack, especially in challenging _STDChallenge_ scenarios. (a) performance of _SUC_ as the \(STD\) changes. We have listed the _SUC_ scores on the toughest 100 sequences. (b) represents robustness when facing the _STDChallenge_, where success rate refers to the percentage of frames in which the tracker successfully tracks the target.

central vision may be better at precise localization. We believe this demonstrates the complementarity of central and peripheral vision, further validating CPD theory.

**Information query is effective.** It can be seen in Tab. 2, compared to baseline#1, #4 has improved performance by 0.1%-0.4% on several metrics in _STDChallenge Benchmark_ and VideoCube, and with 0.8% improvement in error consistency. This indicates that in tracking, utilizing higher-level semantic information for top-down control is beneficial for trackers to understand sequence information. On the other hand, information query can also be considered as the retrieval of short-term memory, which suggests that combining short-term and long-term memory (online template) in tracking can better address the _STDChallenge_.

**Modeling central vision with Gaussian acuity is effective.** Compared to the traditional local crop as central vision #5, CPDTrack #4 has surpassed in performance metrics on _STDChallenge Benchmark_ and VideoCube, with improvements ranging from 1.2% to 4.2%, as in Tab. 2. The error consistency is also higher than #5. This suggests that considering the bounding box and image size, dynamically extracting context for central vision can better align with peripheral vision.

## 6 Conclusion.

Our study is inspired by the differences in dynamic visual abilities between humans and machines, notably in challenging tasks like LTT and GIT. We attribute this gap primarily to the machines' deficiency in essential visual search skills. Based on CPD from cognitive neuroscience, which categorizes visual inputs into central and peripheral vision, we understand humans' robust visual search ability under limited computational resources.

We propose CPDTrack, extensively validated for its effectiveness and closer behavioral alignment with humans in _STDChallenge Benchmark_. Ablation studies highlights that its success is due to the efficient integration of central and peripheral vision. Our findings indicate that robust visual search capabilities are crucial for gradually adapting trackers to real-world applications.