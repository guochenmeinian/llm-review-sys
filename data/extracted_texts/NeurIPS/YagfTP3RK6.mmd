# Safetywashing: Do AI Safety Benchmarks

Actually Measure Safety Progress?

Richard Ren

Equal Contribution. Center for AI Safety

Steven Basart

Equal Contribution. Center for AI Safety

Adam Khoja

Alexander Pan

University of California, Berkeley

###### Abstract

Performance on popular ML benchmarks is highly correlated with model scale, suggesting that most benchmarks tend to measure a similar underlying factor of general model capabilities. However, substantial research effort remains devoted to designing new benchmarks, many of which claim to measure novel phenomena. In the spirit of the Bitter Lesson, we leverage spectral analysis to measure an underlying capabilities component, the direction in benchmark-performance-space which explains most variation in model performance. In an extensive analysis of existing safety benchmarks, we find that variance in model performance on many safety benchmarks is largely explained by the capabilities component. In response, we argue that safety research should prioritize metrics which are not highly correlated with scale. Our work provides a lens to analyze both novel safety benchmarks and novel safety methods, which we hope will enable future work to make differential progress on safety.

## 1 Introduction

Benchmarks serve as crucial standards, providing metrics by which models and techniques are evaluated. The AI safety community has invested extensively in creating benchmarks aimed at measuring distinct safety-relevant properties . While these benchmarks have driven significant advancements, there is a critical oversight: the performance on safety benchmarks intended to measure bias, ethics, adversarial robustness, or fairness is often strongly correlated with general capabilities benchmarks such as MMLU , MATH , and GSM8K . This correlation means that simply enhancing the upstream general capabilities of models, such as by scaling parameters and increasing training data, often boosts performance across all benchmarks indiscriminately .

This oversight is problematic because safety benchmarks have seldom been scrutinized for this correlation . Consequently, this lack of scrutiny obscures the development of techniques thatspecifically and differentially improve safety. Without clear and distinct metrics and goals, efforts to advance AI safety are hindered [97; 71]. The conflation of general capability improvements with safety-specific advancements not only misleads progress assessments but also undermines the incentive to develop targeted safety solutions . To address this issue effectively, it is crucial to distinguish and prioritize safety-specific goals within the broader context of AI development.

Given this context, a pivotal question arises: how should the AI safety community allocate its efforts to differentially improve model safety? We can derive some insight from the "Bitter Lesson" , which observes that compute is becoming exponentially more available over time, and that AI research methodologies which optimize performance at a constant level of compute are subsumed by new paradigms that effectively leverage greater compute. Rather than over-indexing on the strengths and weaknesses of present-day models, this framework suggests that effective safety research should anticipate and address the flaws that will emerge or remain in future generations of models, and deemphasize issues likely to be resolved through general model scaling or the default trajectory of capabilities improvements.

Similarly, success in new safety methods should be measured not only by improvements in safety benchmark scores, but also by how much these methods make desired safety properties more correlated with scale. For example, Reinforcement Learning from Human Feedback (RLHF) [8; 58] has successfully associated toxicity reduction with model scale, an achievement that basic pretraining and instruction fine-tuning struggled to attain. By concentrating on properties and methods that specifically enhance safety independently of capabilities advancements, the safety community can make more effective use of its resources and significantly contribute to the development of safer AI systems.

## 2 Related Work

**Safety vs. capabilities.** One paradigm of measuring AI progress is a decomposition into datasets that measure "safety" vs datasets that measure "capabilities" . While the distinction between safety and capabilities is sometimes blurred, safety research tends to study empirical phenomena that are negative side effects of model deployment [96; 66; 68; 74; 61], are malicious use of models [93; 105; 44], or do not improve with scale [11; 52]. In particular, a popular debate (e.g., between McKenzie et al.  and Wei et al. ) is whether a given safety dataset is in fact tightly correlated with scale. Our work addresses this debate through a meta-analysis of safety datasets, quantifying the degree to which safety datasets are entangled with capabilities.

Figure 1: Our analysis identifies three classes of safety tasks according to the correlation between their scores and the capabilities scores. Tasks whose scores improve with scale have a positive correlation between benchmark scores and capabilities score. Tasks whose scores improve with tuning show a safer correlation on specific model classes, e.g., chat/instruct-tuned models. Finally, tasks whose scores do not improve naturally with model scale show no correlation between benchmark scores and capabilities scores.

Scaling laws and the Bitter Lesson.The Bitter Lesson  argues that the main technique to improving ML models has consistently been scale. NLP has seen the biggest embrace of this trend, with developers focused on scaling the Transformer  with more data and compute [87; 85; 14; 1; 15; 83; 5; 6]. To aid in such engineering effort, there has been an extensive body of literature on quantitatively modeling scaling laws for loss, mapping out model performance as a function of compute and data [32; 41; 55; 33] or even hyperparameter choice . Similar trends have taken hold in vision [24; 102; 26; 65] and robotics .

Scaling tends to improve not only training loss, but also downstream task performance . A common finding is that models with lower pretraining loss also have higher accuracy on downstream tasks [95; 99; 35; 25; 23; 22; 19; 42]. Importantly, most prior work examines scaling laws from a model perspective (i.e., how does performance improve with scale), whereas our work examines scaling laws from a dataset perspective (i.e., how do benchmarks saturate with scale).

Metrics and capabilities.Recent advances in observational scaling laws have provided a methodology that allows researchers to gain a deeper understanding of the underlying capabilities of machine learning models [41; 55]. Previous studies, such as those by [32; 15], have demonstrated that these scaling laws can predict model performance across various tasks. This body of work has established a foundation for using observational scaling laws as a powerful method for enhancing model training and evaluation processes by predicting performance trends based on scaling behavior. Recent research further supports the utility of these scaling laws, for extrapolating model performance, enabling a more nuanced assessment of model capabilities [75; 37].

However, while significant progress has been made in identifying and leveraging these underlying factors for general capabilities, there has been a noticeable gap in exploring how these scaling laws correlate with safety properties of deep learning systems. Although the identification of fundamental scaling relationships has been beneficial, there is a lack of research focusing on the implications of these relationships for safety datasets. Understanding how scaling impacts safety properties is crucial for developing datasets and benchmarks that can properly measure the intended effects and not by a "third variable" (i.e. capabilities). This paper aims to bridge this gap by examining the correlation between scaling laws and safety-specific characteristics, thereby providing insights that can guide the development of safer AI systems and future AI Safety datasets.

## 3 Capabilities Correlations for Evaluating Differential Progress on Safety

Estimating capabilities using benchmark scores.Inspired by prior work which applied factor analysis to matrices of model-benchmark scores , and concurrent to Ruan et al. , we apply spectral analysis of benchmark scores to identify a unified underlying _capabilities score_ for models in terms of their performance on a range of benchmarks. Given a set of \(n\) models and a suite of \(m\) capabilities benchmarks (e.g. MMLU , Winogrande , GSM8K , etc.) we construct a matrix of scores \(A^{n m}\), such that \(A_{ij}\) is the score of the \(i\)-th model on the \(j\)-th benchmark, normalized so that columns have mean \(0\) and variance \(1\).

Figure 2: Illustration of the safety task identification pipeline. We first produce a matrix of scores for a set of language models evaluated on a set of capabilities benchmarks (first step). We extract the first principal component and use it to compute a capabilities score for each model (second step). We perform analysis of base and chat/instruct-tuned models on a variety of tasks representing major areas of AI safety (third step). Finally, we identify tasks whose scores are correlated with scale, tasks whose scores improve with scale only with chat/instruct-tuning, and tasks which are uncorrelated with scale (fourth step).

**Spectral analysis of capabilities scores.**

Naive composite benchmarks usually weight their component tests equally, averaging test scores. A more principled approach can involve weighting component benchmarks according to the strength of their association with each other, with higher weight placed on benchmarks that account for greater variance in model performance across benchmarks. To achieve this, we compute a correlation matrix \(C^{m m}\) associated with \(A\), such that \(C_{ab}\) is the correlation between task \(a\) and task \(b\) performance across all models. We extract the largest eigenvalue \(\) of \(C\) and its associated unit eigenvector \(v\). The components of \(v\) act as the weights of the composite benchmark, and \(Av^{n}\) gives the capabilities scores of each model.

When \(C\) is the Pearson correlation matrix \(A^{T}A\), \(\) is the largest singular value of \(A\), and \(v\) is its associated top principal component . \(/m\) then represents the proportion of total variance in normalized model scores explained by the principal component vector. Additionally, the outer product of the capabilities scores and benchmark weights \((Av)v^{T}\) is the best rank-\(1\) approximation of \(A\). However, using Pearson correlation can be sensitive to outliers, which becomes relevant when dealing with large model sets and a heterogeneous collection of benchmarks. For that reason, our analysis takes \(C\) to be the Spearman correlation matrix , in which case \(/m\) represents the explained variance in rank scores.

**Using capabilities scores to measure capabilities correlations.** To evaluate the relationship between a new benchmark and general capabilities, which we call the _capabilities correlation_ of the benchmark, we can evaluate a set of models with known capability scores on the new benchmark and measure the correlation between capability scores and benchmark scores (we use Spearman correlation for these calculations as well). These general ability components allow for quantitative, intuitive, and principled evaluations of task relationship to general model abilities. Ultimately, however, these correlations depend on the set of models used, as well as the benchmarks chosen to produce their capabilities scores. In the Appendix, we perform a sensitivity analysis to explore the robustness of this methodology to different choices of models and benchmarks.

**Safety techniques can alter capabilities correlations.** In our analysis, we categorize models into distinct classes--base models and instruct (including chat) models--to better understand how different training paradigms impact performance on safety tasks. Base models, RLHF'ed models, light adversarial training, and future safety techniques could all be considered different model classes, with different profiles of capabilities correlations. Ideally, we should develop training regimens which produce high capabilities correlations with all relevant safety properties. By running separate analyses for each model class, we can identify the relative strengths of these techniques as models scale.

## 4 Results

We come to our central question: which tasks or datasets are correlated with capabilities? Towards answering this question, we analyze the overall capabilities scores as captured by tasks in 4.1, the tasks of Adversarial Robustness in 4.2, Bias and Toxicity in 4.3, Machine Ethics in 4.4, Malicious Use in 4.5, and Rogue AI Risk in 4.6.

To provide ease of understanding, we define a positive capabilities coefficient as yielding a safer system with scale, while a negative capabilities coefficient indicates less safe systems with scale.

Figure 3: We observe a strong correlation between training FLOPs and relative capabilities score.

### General Capabilities and Overview of Model Class Correlations

**Most variance in capabilities datasets is explained by a capabilities component.** We run analyses for base and chat models, finding that 72% and 71% of variance is captured by the capabilities component respectively. We calculate the capabilities component from the following benchmarks: LogiQA , PIQA , Hellaswag , Winogrande , COPA , MedQA , ARC Challenge , MMLU , MATH , LAMBADA , Wikitext , GSM8K , GPQA , and BBH . We also use a diverse set of model classes and derivatives to ensure robustness in results, as results can be skewed if they come from a derivative of one model (e.g. Llama-2 ); we list the 24 base models and 22 instruct/chat used for our analysis in the Appendix.

**The capabilities component is strongly correlated with scale.** We quantify the correlation of model capabilities scores with log FLOP for base (r=0.96) and chat (r=0.96) models, and plot chat models n in Figure 5. We calculate training FLOP via the approximation of 6 * params * train_tokens described in .

**Observed properties of capabilities correlations.** In our experiments, we observe three high-level categories of result:

1. Some safety benchmarks  are already highly correlated with capabilities, obeying "scaling laws" (top left).
2. Some safety benchmarks are not aligned with scale (top right) or are negatively correlated with scale (bottom left), obtaining worse safety properties as capabilities increase. At times, these problems are not solved by any type of model class.

Figure 4: Observed correlations between capabilities scores and modelsâ€™ performance. Top left: safety task positively correlated with capabilities score. Top right: safety task not correlated with capabilities score. Bottom left: safety task negatively correlated with capabilities score. Bottom right: safety task where chat models are not correlated with capabilities score while base models are negatively correlated. Parentheses include the capabilities correlation of the corresponding benchmark.

3. Some correlations are strengthened or weakened through safety techniques; for example, chat models exhibit on a higher correlation on CybersecEval2 MITRE , a task for measuring refusal to assist in malicious cyberattacks, than base models (bottom right).

In Figure 4, we examples of these scenarios. In the following sections, we continue to explore how instruction tuning affects models and highlight the need for alternative directions to be pursued across safety areas.

### Adversarial Robustness

Adversarial robustness evaluates models' ability to maintain performance when faced with adversarial examples. In the vision domain, adversarial robustness is known to have different properties from general capabilities . However, the relation between general capabilities and adversarial robustness is less clear for LLMs. Many different adversarial robustness benchmarks have been developed to assess different aspects of their robustness . We now analyze whether these benchmarks measure novel properties or are highly correlated with general capabilities.

We compute the correlation between the capability score and safety scores on the following benchmarks: AdvGLUE , AdvGLUE++ , AdvDemonstration , and HarmBench . Full results on all datasets and model classes are in the Appendix.

**Some robustness benchmarks are correlated with general capabilities.** We find that some adversarial robustness benchmarks are moderately correlated with general capabilities, while others have low or even negative correlation. For example, AdvGLUE, AdvGLUE++, and AdvDemonstration have respective capabilities correlations of \(0.68\), \(0.58\), and \(0.75\) for the instruct/chat model class. On the other hand, general capabilities are anti-correlated with robustness on HarmBench. The dynamic adversarial robustness benchmarks tested tend to have lower correlation, static adversarial benchmarks tend to have higher correlation. In other words, some robustness properties are likely to be solved as general capabilities improve, while others are not yet strongly correlated with capabilities.

**Different model classes have different scaling properties.** Just as adversarial training significantly alters the robustness properties of vision models, different classes of general-purpose AI models can yield different scaling properties for safety benchmarks. In Figure 5, we show how some LLM adversarial robustness benchmarks have higher capabilities correlations when using instruct/chat models. This demonstrates that improving the capabilities correlation of a safety benchmark is possible. Once the correlation reaches a high enough value, additional work on the benchmark is unnecessary, as it will be solved automatically as general capabilities improve.

### Bias

We investigate bias datasets aimed at quantifying language models' propagation of social stereotypes and harmful preconceptions. It is well-known that pretraining on internet data introduces bias, and one might expect that training larger models on more data would increase the amount of bias present. We test this hypothesis by measuring the capabilities coefficient of different LLM bias benchmarks.

Figure 5: For many of the benchmarks we evaluate, capabilities correlations are higher (or less negative) among Chat models. This demonstrates that evaluating correlations for multiple model classes is crucial for understanding whether a benchmark will be solved as general capabilities improve.

**Bias is often weakly correlated with capabilities, but not always.** Our findings reveal that for some bias measures, the capabilities correlation is weak as expected. For example, in Figure 6 (left) we show that BBQ Ambiguated , Anthropic Discrimination Evaluation , and CrowS-Pairs English  display this pattern across both base and instruct/chat models.

However, for other measures, improvements to general capabilities can actually reduce bias. In Figure 6 (right), we plot the capabilities score against accuracy on BBQ Disambiguated  and find that bias reduction is highly correlated with general capabilities. This observation contrasts with conventional wisdom, which suggests that scaling up models exacerbates bias due to associations in the training data .

### Machine Ethics

Machine ethics benchmarks probe models' understanding of moral concepts. There are several benchmarks that analyze machine ethics, such as ETHICS  and STEER Rationality . We report the capabilities correlation of these benchmarks in Table 1.

**High capabilities correlation.** We find that machine ethics benchmarks tend to be highly correlated with general capabilities. Many subsets of ETHICS have an extremely high capabilities coefficient for both base and instruct/chat models. These findings corroborate isolated observations of scale improving performance on machine ethics benchmarks , indicating that internet-scale pretraining imbues LLMs with an understanding of ethics and morality. However, our results also show that this correlation is not identical across all areas of machine ethics. Some topics improve much more slowly with general capabilities, suggesting a need to ensure a balanced understanding of different ethical perspectives is present in models.

### Malicious Use

Malicious use evaluations test whether models can resist being exploited for harmful ends, including spreading misinformation or enabling cybercrime. Benchmarks like HarmBench , CyberSecEval2 , and WMDP  are used to assess the susceptibility of models to malicious use. To bypass refusal training, many of these evaluations also employ adversarial prompts. We analyze the capabilities coefficients of resistance to malicious use benchmarks under current models and present results in Table 2.

  
**Ethics Evaluation** & 
 **Capabilities** \\ **Correlation** \\  \\  ETHICS (Average) & 0.80 \\ ETHICS Commonsense & 0.72 \\ ETHICS Deontology & 0.41 \\ ETHICS Justice & 0.49 \\ ETHICS Utilitarianism & 0.74 \\ ETHICS Virtue & 0.77 \\ STEER Rationality & 0.54 \\   

Table 1: Capabilities correlations for various machine ethics datasets. For brevity, we show instruct/chat models only, although correlations are also high for base models.

Figure 6: Left: We find that for several common bias benchmarks, bias is not reduced by general capabilities improvements, indicated by low capabilities correlations. Right: However, on BBQ Disambiguated capabilities score is strongly correlated with reducing bias.

**General capabilities exacerbate malicious use.** Many base models cause more harmful responses as their capabilities increase, as indicated by negative capabilities correlations. This includes many splits of HarmBench and CyberSecEval2, as well as WMDP (an unlearning dataset that penalizes high performance).

We find that instruction tuning weakens many capabilities correlations, indicating that models no longer become less safe with scale. In the MITRE task of CyberSecEval2, which measures refusal to participate in cyberattacks, the effect is even stronger, with the capabilities correlation changing from negative to positive.

These results demonstrate that instruct/chat models have improved over base models in their ability to leverage general capabilities to reduce malicious use risk. However, in most cases the correlations remain negative or weak, suggesting there is still considerable work to be done on this problem.

### Rogue AI

Rogue AI risk evaluations probe risks related to deceptive model behavior, dishonesty, and power-seeking tendencies. Previously, it was unknown whether models become more power-seeking as they scale. We report the capabilities correlations of these benchmarks in Figure 7 (left).

**Power-seeking tendencies decrease with scale, but sycophancy does not.** On the MACHIAVELLI dataset , we find that measures of power-seeking tendencies and ethical violations decrease as general capabilities improve, with moderate capabilities correlations ranging from \(0.46\) to \(0.55\). On the other hand, sycophancy  becomes worse as models become more capable, with a capabilities correlation of \(-0.73\). This highlights how different aspects of rogue AI risk are correlated with general capabilities to different extents.

Unlike power-seeking and sycophancy, we find that TruthfulQA MC1 variance is strongly correlated with general capabilities. This could be explained by training leakages or may indicate that models are able to discern fact from human falsehood as capabilities advance. Regardless, we find that TruthfulQA does not seem to measure a meaningfully different metric from capabilities benchmarks.

  
**Malicious Use** &  \\ 
**Evaluation** &  & **Chat** \\ 
**HarmBench DR** & & \\ Biochemical & -0.54 & -0.04 \\ Cybercrime & -0.50 & -0.07 \\ Harassment & -0.45 & -0.16 \\ Harmful & -0.42 & 0.24 \\ Illegal & -0.41 & 0.09 \\ Misinfo & -0.44 & -0.37 \\ 
**WMDP** & & \\ WMDP Bio & -0.91 & -0.87 \\ WMDP Chem & -0.88 & -0.86 \\ WMDP Cyber & -0.86 & -0.87 \\ 
**CybersecEval2** & & \\ Autocomplete & -0.74 & -0.77 \\ Exploit & -0.31 & -0.49 \\ Instruct & -0.43 & -0.90 \\ MITRE & -0.25 & 0.55 \\ Prompt Injection & -0.02 & -0.17 \\   

Table 2: Malicious Use Evaluations and Metrics

Figure 7: Left: Capabilities correlations on instruct/chat models on Rogue AI evaluations. Right: Capabilities correlations on instruct/chat models with accuracy on MACHIAVELLI Power.

Discussion

As our experiments show, safety and capabilities metrics can be intertwined, with some safety metrics improving naturally as a consequence of general capabilities advancements. We first summarize high-level patterns in the results, then discuss several implications of our findings.

**Patterns across safety areas.** Our capabilities correlation analysis reveals varied dependencies on model scaling across safety areas. Adversarial robustness exhibits mixed results, with some perturbation robustness tasks improving with general capabilities while jailbreak robustness remains uncorrelated with general capabilities. Bias measures show some improvement with capabilities, but many biases are unaffected, suggesting that scaling alone may not mitigate all bias issues. Machine ethics generally correlates positively with capabilities, though further refinement is needed for balanced ethical understanding. In malicious use and rogue AI risk domains, we observe negative correlations, indicating a tendency for these risks to worsen with scale unless mitigated by specific interventions. These insights guide safety research toward areas where scaling alone is insufficient for safety progress.

**The importance of low capabilities correlation.** While many datasets measure interesting aspects of safety, these aspects are often not unique and instead are highly correlated with general capabilities. We argue researcher time for developing methods should be allocated toward solving benchmarks that won't be solved with scale and general capabilities advancements. Thus, capabilities correlation can be used as a metric for identifying which problems to spend research effort making progress on. If capabilities correlation for a benchmark is low, this means it will likely require additional algorithmic effort to make progress on.

**Measuring properties that improve with scale is still valuable.** Even if an evaluation is expected to be solved with scale, it is still useful to measure it. For instance, knowing the dangerous capabilities that emerge with scale is crucial. Reporting how evaluations scale with model size can help predict future risks, particularly with dangerous capabilities. This information is highly relevant, as it can indicate which problems may worsen with scale. The goal is not to measure the usefulness of a safety dataset, but to understand how to allocate research efforts efficiently. Evaluations showing strong correlation or anti-correlation with capabilities are valuable for tracking the evolution of dangerous capabilities and ensuring precise measurement of safety metrics, even if they are eventually solved with scale. Furthermore, there are some cases in our evaluations where tasks with high capabilities correlations will be improved but not completely "solved" as general capabilities improve. However, in some cases, safety-related benchmarks may act a jangle for capabilities; jangle fallacy is the erroneous belief that two constructs are different because they have the different names, when in practice they measure the same latent factor.

**Improving capabilities correlation as a goal for safety research.** If a meaningful safety metric is strongly correlated with general capabilities, this is a good outcome, because it means the problem may be largely addressed by scaling even if present-day models struggle. As a corollary, safety research should seek to develop new methods and model classes that cause safety metrics to correlate more strongly with capabilities. However, this should not be taken too far. Past a certain threshold, further efforts to align safety metrics with general capabilities are unnecessary. Once this is achieved, research efforts can be re-allocated elsewhere. Moreover, our results show that in some cases a high capabilities correlation may not be sufficient to ensure that a safety property is fully achieved. In these cases, continued effort on developing differential safety improvements is warranted.

**New safety evaluations should report their correlation with capabilities.** This practice can ensure that evaluations initially measure a meaningful safety property that requires research effort to improve, rather than simply increased scale. This is notably done in some past papers, such as RuLES  and EQ Bench . For example, it can be useful to know if a malicious use benchmark worsens with scale. A low correlation does not necessarily imply that the safety metric is irrelevant; it may indicate flaws in the dataset, such as insufficient data to detect small changes in progress, or that the dataset measures a different aspect entirely.

**Broad application of correlation analysis.** This analysis can be applied in a broad range of scenarios when determining whether an evaluation measures a meaningfully different property. In a broad range of scenarios, it may be the case that a confounding variable that better explains performance, rather than what a benchmark claims it measures. Future investigations can also investigate correlations of safety with various types of capabilities; while previous research already shows that performance on different categories of capabilities (such as reasoning, knowledge, coding, or mathematics) seem to be tied to scale, other papers have found that reasoning and knowledge can represent different components of a PCA analysis .

**Recommendations.** Our recommendations are as follows:

1. For benchmark selection (and broadly, research problem selection), researchers should allocate more time toward creating and climbing safety benchmarks with low correlation and thus represent problems that will not be solved with general capabilities advancements using current methods.
2. For technique development, a fruitful research direction is to develop new safety methods that increase the capabilities correlation, ensuring that the safety benchmarks will improve in the future as general capabilities improve.

## 6 Conclusion

We have shown that a wide variety of safety benchmarks are tightly correlated with general model capabilities, calling their importance into question. By considering the Bitter Lesson and the continued scaling of deep learning, we argued that research in AI safety should anticipate the possibility of safety metrics being correlated with general capabilities, such that they are naturally solved with scale. To quantify this, we developed a methodology to measure the correlation of safety metrics with general capabilities via spectral analysis of accuracy on capabilities datasets. In experiments, we measured the capabilities correlation of a wide variety of safety benchmarks and datasets, finding that many prior datasets are strongly correlated with general capabilities. We make two specific recommendations: future safety benchmarks should aim for low correlation with general capabilities, while future safety methods should aim to increase correlation between relevant safety metrics and general capabilities.