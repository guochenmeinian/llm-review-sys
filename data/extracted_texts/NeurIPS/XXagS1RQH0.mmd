# Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification

Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification

 Rui Wang\({}^{1}\), Peipei Li\({}^{1}\)1, Huaibo Huang\({}^{2}\), Chunshui Cao\({}^{3}\), Ran He\({}^{2}\), Zhaofeng He\({}^{1}\)

\({}^{1}\)Beijing University of Posts and Telecommunications

\({}^{2}\)CRIPAC&MAIS, Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)WATRIX.AI

{wr_burt, lipeipei, zhaofenghe}@bupt.edu.cn

huaibo.huang@cripac.ia.ac.cn, chunshui.cao@watrix.ai, rhe@nlpr.ia.ac.cn

Corresponding author

###### Abstract

We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordinal pairwise loss to refine the CLIP feature space, where texts and images maintain both semantic alignment and ordering alignment. Extensive experiments on three ordinal classification tasks, including facial age estimation, historical color image (HCI) classification, and aesthetic assessment demonstrate its promising performance. The code is available at https://github.com/raywang335/L2RCLIP.

## 1 Introduction

Ordinal classification aims to predict labels that are related in a natural or implied order, which can be considered as a special case of ordinal regression after label discretization, i.e. discretize the continuous labels and each bin is then treated as a class. Common examples of such tasks are facial age estimation (e.g., estimating the facial age from 1 to 100), historical color image classification (e.g., assigning a time period to color photographs, ranging from the 1930s to the 1970s), and aesthetics assessment(e.g., rating image quality on a scale from "unacceptable" to "exceptional").

Compared with common classification, ordinal property of labels need to be additionally considered in ordinal classification. Many algorithms [4; 35; 12] employ the ordinal classification framework, which trains a set of classifiers or integrates probabilistic priors to directly predict rank labels. However, these methods exhibit suboptimal performance as a result of insufficiently harnessing the ordering properties. Order learning algorithms [48; 39; 26; 12] demonstrate competitive performance by effectively capturing relative ordering relationships. These approaches determine the target order of a novel instance by contrasting it with well-defined reference instances. Nonetheless, the algorithm's performance can be substantially compromised by inadequate indexing quality of the reference instances.

Furthermore, many metric learning techniques [3; 41; 13; 49] have been developed to construct an ordinal embedding space in which the distances between diverse features effectively represent the differences in their respective ranks. However, all these methods learn ranking concepts depending solely on training data, which renders them vulnerable to overfitting.

Fortunately, recent developments in large pre-trained vision-language models offer new insights for various visual tasks [32; 27; 29; 25; 56]. Compared with visual content, human language contains highly abstract concepts and rich semantic knowledge [29; 32]. Motivated by it, we attempt to borrow knowledge from language domain by two major observations. Firstly, rank templates inherently contains ordinal information, e.g. _"a sixty years old face" - "a ten years old face"_, which is also demonstrated in Figure 3(a). Secondly, inspired by [25; 56], rank features encapsulate the average information of numerous image features within a well-aligned cross-modal feature space, which can be considered as a robust prior for cross-modal metric learning.

Hence, we propose L2RCLIP to boost learning-to-rank of CLIP-based models for ordinal classification. Specifically, we first introduce a complementary prompt tuning method, termed RankFormer, to enhance the ordering relation of original rank templates. Specifically, RankFormer employs a token-wise attention layer and performs residual-style prompt blending with the original templates for prompt tuning. Moreover, inspired by pairwise metric learning , we propose cross-modal ordinal pairwise loss to ensure both semantic and ordering alignment in the CLIP feature space. Concretely, we revisit the approximate bound optimization of conventional cross-entropy loss and reformulate it in the cross-modal embedding space with ordinal language priors. OrdinalCLIP  also incorporates language priors to model ordering alignment, demonstrating impressive performance. However, our method distinguishes itself from previous works, as depicted in Figure 1. CoOp achieves semantic alignment through contrastive loss but fails to maintain ordering alignment. OrdinalCLIP addresses ordering alignment at the cost of weakened semantic alignment. Conversely, by leveraging RankFormer and cross-modal ordinal pairwise loss, our approach simultaneously considers both semantic alignment and ordering alignment.

The contributions of this paper can be summarized as follows: (1) We incorporate learning-to-rank into vision-language pre-training model for ordinal classification, in which we present RankFormer to enhance the ordering relation of vanilla language prompts. (2) We explicitly utilize the language priors and further propose cross-modal ordinal pairwise loss to refine CLIP embedding space, in which image features and text features maintain both semantic and ordering alignment. (3) Extensive experiments demonstrate the competitive performance of L2RCLIP on age estimation, aesthetics assessment and historical image dating, as well as improvements in few-shot and distribution shift experiments.

Figure 1: Comparison with CoOp(a) and OrdinalCLIP(b), where \(v\) represents the image features and \(R,r\) represents the rank templates in word embedding space and CLIP feature space, respectively. (a) CoOp aligns each rank template with its corresponding images via contrastive loss but vanilla CLIP fails to ensure ordering alignment. (b) OrdinalCLIP considers additional interpolation to explicitly maintain ordering alignment. However, the interpolation term can not preserve semantic alignment in CLIP feature space. (c) Our method enhance the ordering relation of vanilla rank templates while ensuring the semantic alignment of CLIP space.

Related Work

Ordinal ClassificationOrdinal classification attempts to solve classification problems in which _not all wrong classes are equally wrong_. Early techniques [4; 45] adopted the classification framework and train a set of classifiers to directly estimate the rank labels. These methods got degraded performance due to ignoring the ordering relation. By incorporating probabilistic priors, Geng _et al._ firstly proposed label distribution learning and assigned a Gaussian or Triangle distribution for an instance. The mean-variance loss was introduced in  for learnable label distribution and penalizes the learned variance of estimated distribution to ensure a sharp distribution. Probabilistic embedding  was developed to model the data uncertainty for ordinal regression. Liu _et al._ proposed predicting the ordinal targets that fall within a certain interval with high confidence. These methods learn better rank concepts and significantly reduce the model's overconfidence toward incorrect predictions. In contrast to them, our L2RCLIP only focuses on enhancing vision-language alignment without complicated probabilistic distribution assumption.

Furthermore, many techniques [3; 41; 13; 49; 23; 24; 22] solve the ordinal classification task from the perspective of metric learning. These methods exploit the pairwise ordering relation in embedding space, where the distance between different features reflects ordinal information. For example, Ordinal log-loss (OLL) was presented in  with an additional distance-aware weighting term for ordinal classification. RankSim  proposed a regularization loss to ensure the ordinal consistency between label and feature. Suarez _et al._ ranked the embedding distances between pairwise rank differences of features. Another way for ordinal classification is order learning [31; 48; 19], which learns ordering relation by comparison between instances. It usually show more promising results as learning relative ordering relation is much easier than learning absolute ordering relation. Lim _et al._ firstly proposed this concept and determined the ordinal information of an unseen instance by compared to some known reference instances. Lee&Kim _et al._ improved the quality of indexing to boost the performance. MWR  further proposed to learn a continuous regression score from reference instance. However, these methods often depend solely on training data to learn rank concepts, potentially leading to overfitting. To mitigate these issues, we leverage rich priors in human language to learn language-driven rank concepts.

Vision-language LearningVision-language pre-training(VLP) has significantly improved the performance on many downstream tasks by text and image matching, including segmentation [43; 8], object detection , image retrieval [36; 1], generation tasks [18; 40; 25; 5] and ordinal regression . CLIP  and ALIGN  proposed to embed images and texts into the same representation space with two separate encoders in a contrastive-based approach. The experimental results show that the impressive "zero-shot" performance on downstream tasks, which show the power of language prior. Inspired by the recent advances in NLP, prompts and adapter-based tuning becomes prevalent in improving ability of CLIP. CLIP-Adapter  adds a light-weight module on top of image and text encoder. CoOp  proposes to learn context prompts for image classification. Due to lack of ordinal property, these methods lead to degraded performance in ordinal classification. Considering the great potential of language prior, we propose to incorporate learning-to-rank into CLIP for ordinal classification.

## 3 Proposed Approach

### Problem Formulation

Ordinal classification is a unique case of image classification where labels possess ordinal properties. Mathematically, let \(x_{i}\) denote the \(i\)-th input instance with \(i=1,2,...,N\), \(y_{i}\{r_{1},r_{2},...,r_{M}\}\) with ordered ranks \(r_{M} r_{M-1} r_{1}\) denote the ground truth value and \(_{i}\) represent the predicted rank by the network, where \(N\) represents the total number of instances, \(M\) represents the number of ranks, and \(\) indicates the ordering between different ranks. Analogous to normal classification, ordinal classification aims to recover \(y_{i}\) by encoding the image to feature \(z_{i}=(x_{i})\) with encoder \(\) and then using a classifier \(f_{}()\) to compute probability \(p_{i}\). The predicted label \(_{i}\) is the result with the highest probability \(p_{y_{i}}\). The classification probability can be calculated by:\[p_{i}=^{}z_{i})}{_{j=1}^{M}exp(_{j}^{}z_{i})}.\] (1)

To exploit ordinal information in language, ordinal classification can be transformed into a vision-language alignment task. Specifically, we use a pre-trained CLIP image feature extractor \(Image()\) to extract features from input images: \(v_{i}=z_{i}=Image(x_{i})\). For text features, we first construct hard rank templates \(R=\{R_{1},R_{2},...,R_{M}\}\) for a given ordinal classification task. For each template, we convert it into fixed-length tokens and then map them into 512-dimensional word embeddings. The language feature extractor \(Text()\) encodes the embeddings as a classifier weight \(r_{i}\). The process can be formulated as: \(r_{i}=w_{i}\) = \(Text(Tokenizer(R_{i}))\). Finally, we can calculate the prediction probability \(p_{i}\) for rank \(i\) with Eq.(1).

### Proposed Method

Our objective is to integrate learning-to-rank into CLIP and learn language-driven rank concepts for ordering alignment while preserving original semantic alignment. Inspired by [16; 27], we firstly learn rank concepts in the word embedding space and subsequently refine the CLIP feature space to maintain both semantic and ordering alignment. Specifically, we introduce RankFormer to enhance the ordering relation of the original language prompts. RankFormer employs a token-wise attention layer for rank prompt tuning. To better utilize language prior, we reformulate the approximate bound optimization within cross-modal embedding space. Furthermore, we propose a cross-modal ordinal pairwise loss to ensure ordering alignment. Moreover, randomly initialized global context prompts and an asymmetrical contrastive loss are adopted to ensure semantic alignment. The overall framework is illustrated in Figure 2.

Rank-specific Prompts.As shown in Figure 3(a), rank templates in the vanilla CLIP contain some degree of ordinal information. However, a substantial proportion, nearly half of the pairs, lack clear ordinality. One intuitive approach to enhance this ordinal information is to further fine-tune these rank templates. However, implementing such a strategy introduces two primary challenges. First, the performance of different ranks for ordinal classification varies significantly driven by the imbalanced training data, and certain ranks cannot even be trained due to insufficient training data in extreme cases. Second, the contrastive loss introduces difficulties in enhancing the ordering relation during the training . To address the first challenge, OrdinalCLIP  trains only a small number of base rank templates and generates other rank templates through explicit interpolation, which confines the ordinal information of vanilla rank templates. In this paper, we train a RankFormer with token-wise attention to enhance the ordinal information of the fixed rank templates. Specifically, we denote tokenized rank templates as \(R^{}\), where \(M\) and \(D\) represent the number of templates and word embedding channels, respectively. Note that we only consider the ranking tokens with length of \(n\). Subsequently, we perform token-wise attention for prompt tuning and apply residual-style prompt blending with the original prompts. The process can be formulated as: \(R^{{}^{}}=(1-) R+ f_{FFN}(f_{MSA}(f_{LN}(R)))\), where \(\) represents the residual ratio.

Figure 2: An overview of the proposed L2RCLIP. We incorporate learning-to-rank into CLIP from two perspectives. First, RankFormer performs a token-level attention mechanism to enhance the ordering relation of vanilla rank prompts. Then, refined rank-specific prompts and randomly initialized context prompts are concatenated in the word embedding space and are sent to a text encoder to extract the corresponding text features. Moreover, we present two types of losses to refine CLIP feature space by attraction and repulsion, respectively. Attraction refers to an asymmetrical contrastive loss and a tightness term to attract paired images and text features while repulsion refers to a reweighting diversity term to ensure the ordering alignment.

Cross-modal Ordinal Pairwise Loss.Since cross-entropy loss ignored the ordinal information during training , we turn to a solution that recovers the ordering relation while maintaining semantic alignment. Inspired by the pairwise metric learning, we firstly revisit the vanilla cross-entropy loss from the perspective of approximate bound optimization. Boudi _et al._ proved that minimizing the cross-entropy loss accomplishes by approximating a lower bound pairwise cross-entropy loss \(L_{PCE}\). \(L_{PCE}\) contains a tightness term and a diversity term, as follows:

\[L_{PCE}=}_{i=1}^{N}_{j:y_{j}=y_{i} }z_{i}^{}z_{j}}_{IGHTNESS}+_{i}^{N}log_{k=1} ^{K}exp(^{N}p_{jk}z_{i}^{}z_{j}}{ N})- _{k=1}^{K}||c_{k}||}_{DIVERSITY},\] (2)

where \(z_{i}\) represents the image feature in the embedding space, \(p_{ij}\) represents the softmax probability of point \(z_{i}\) belonging to class \(j\), \(c_{k}=_{i=1}^{N}p_{ik}z_{i}\) represents the soft mean of class \(k\) and \(\) is to make sure \(L_{CE}\) is a convex function with respect to encoder \(_{w}\).

By incorporating the language priors, we turn Eq.(2) to a cross-modal pairwise cross-entropy loss. Specifically, as human language contains rich prior knowledge, text features can be considered as both hard mean \(r_{k}=}_{i=1}^{N_{k}}v_{k}\) and soft mean \(r_{k}=_{i=1}^{N}p_{ik}v_{i}\) of image features at class \(k\), where \(N_{k}\) represents the sample number of class \(k\) and \(N\) represents the total sample number. Then, we reformulate the original pairwise cross-entropy loss \(L_{PCE}\) as cross-modal pairwise cross-entropy loss \(L_{PCE}\):

\[L_{PCE}=_{i=1}^{N}v_{i}^{}r_{y_{i}}}_ {IGHTNESS}+_{i}^{N}log_{k=1}^{K}exp( {v_{i}^{}r_{k}}{})-_{k=1}^{K}||r_{k}|| }_{DIVERSITY},\] (3)

Inspired by , we use meanNN entropy estimator  to estimate the diversity term in Eq.(3). The process is formulated as:

\[L_{CPCE}^{diversity}_{i=1}^{N}_{j i}^{N}log(v _{i}^{}r_{j}+r_{i}^{}r_{j}),\] (4)

\[L_{CPCE}^{tightness}-_{i=1}^{N}v_{i}^{}r_{y_{i}},\] (5)

where \(D\) represents the feature dimensions. To recover ordering relation, we propose an additional weighting term. Intuitively, each rank template will have a high similarity score with images of a close rank and a low similarity score with images of a distant rank. As such, we opt to weight the cross-modal features with \(w_{ij}\), where \(w_{ij}\) are the distances in the label space. The final cross-modal ordinal pairwise loss is defined as follows:

\[L_{cop}(y_{i})=_{j i}^{B}w_{y_{i}j}(v_{y_{i}}+  r_{y_{i}})^{}r_{j}-v_{y_{i}}^{}r_{y_{i}},\] (6)

where \(\) controls the strength of the rank within rank templates and \(B\) represents the batchsize. To further refining the CLIP feature space, we also propose a simplified cross-modal ordinal pairwise loss \(L_{scop}\) with language-related parameters frozen(i.e. \(=0\)).

Global Context Prompts.Given that global context prompts significantly surpass manually designed discrete prompts in vision tasks [55; 27; 54], we integrate them with our complementary rank-specific prompts in RankFormer to enhance semantic alignment. Specifically, we randomly initialize \(L\) global context prompts, denoted as \(G=\{G_{1},G_{2},...,G_{L}\}\), and concatenate them with rank-specific prompts in the word embedding space of CLIP.

Asymmetrical Contrastive Loss.In vanilla CLIP , models are optimized using the standard contrastive loss, including a text-image contrastive loss \(L_{t2i}\) and an image-text contrastive loss \(L_{i2t}\). In this work, we replace the original contrastive loss with an asymmetrical contrastive loss due to 

[MISSING_PAGE_FAIL:6]

is set as the ground truth. Following , we split it into 2,476 for training, 1,136 for validation, and 1,079 for testing. Adience  contains discrete labels annotated with eight age groups. We adopt the same five-fold cross-validation protocol used in . For evaluation metric, we use the mean average error (MAE) to measure the absolute differences between the ground truth labels and the predicted ones. Classification accuracy is additionally adopted for Adience. For detailed experimental settings, please refer to the supplementary material.

Comparison with State-of-the-art Methods.In Table 1, our L2RCLIP outperforms both conventional algorithms and language-powered algorithms in all tests. For results on the Morph II, compared with conventional algorithms, L2RCLIP achieves state-of-the-art performance on MAE at 2.13, verifying the significance of leveraging rich priors in human language. Furthermore, compared to methods utilizing language priors, our L2RCLIP exhibits substantial performance improvements. CoOp significantly enhances the vanilla CLIP's performance by learning global context prompts for semantic alignment. Subsequently, OrdinalCLIP further reduces MAE by 0.07 through fixed interpolation for ordering alignment. Nonetheless, a considerable margin remains compared to our methods, which validates the effectiveness of our approach.

Table 1 also compares the results on CLAP2015. Due to the great challenge, many previous methods adopt additional boosting schemes . However, without using such schemes, L2RCLIP outperforms all conventional algorithms. Compared with language-guided models, our L2RCLIP also shows much better results on MAE, specifically a significant MAE margin of 0.15 evaluated on the test set. Table 2 shows the comparison results on Adience  using the metrics of MAE and Accuracy. Compared with Morph II and CLAP2015, Adience is used for age group estimation. The underline indicates the mean value reported in the original paper. Our method outperforms the state-of-the-art algorithms by significant gaps of 5.7% in accuracy and 0.07 in MAE.

Ordinality of Learned Rank Templates.Following , we report the ordinality score by measuring the cosine similarity between rank templates. The ordinality score is the percentage of rank template pairs that obey the ordinal property. Figure 3 reports the ordinality scores in different methods. Vanilla CLIP in Figure 3(a) contains a certain degree of ordering relations, where more than half of the rank template pairs show correct ordering relation. Moreover, CoOp in Figure 3(b) introduces additional global context prompts and improves the ordinality score by 4.56%. OrdinalCLIP in Figure 3(c) adopts an explicit interpolation strategy and further improves the ordinality by 6.02%. However, there is a noticeable "red stripe" in the upper right corner, indicating that this section of rank template pairs significantly violates ordinality. We believe that explicit interpolation effectively ensures local ordinal properties, but may fail to preserve global ordinal properties. Our methods greatly alleviate this problem and achieve a higher ordinality score. The "red blob" in the lower right corner results from insufficient samples for old individuals. Therefore, to avoid the model's overconfidence towards incorrect predictions, this part of rank templates should exhibit higher similarity while ensuring ordering alignment. By comparison, our method outperforms previous methods.

Few-shot Learning.Table 4 demonstrates the generalization ability of L2RCLIP for few-shot learning. Following , the full dataset is split into 80% for training and 20% for testing. The entire test set is used for validation, and only 1/2/4/8/16/32/64 samples in the training set from each class of labels are chosen for training. We observe that introducing rank information in CLIP

Figure 3: The similarity matrices of rank templates in different methods. The redder, the more similar the pair of rank templates. The percentages of templates pairs that obey the ordinality are: 55.36%, 59.92%, 65.94%, and **71.87%**, respectively.

actually benefits few-shot learning tasks. Compared with CoOp, both OrdinalCLIP and our L2RCLIP achieve significant improvements across all settings, which verifies the effectiveness of ordering alignment. Moreover, our L2RCLIP further reduces the average MAE by 0.24, which demonstrates the effectiveness of the proposed learning-to-rank method in CLIP.

Distribution Shift.Following , we conduct data distribution shift experiments on the MORPH II dataset for generalization. We use the same setting as the general regression setting. For the training set, we randomly choose several rank labels, e.g., 10, 20, 30, and 40. Then, in those classes, we randomly discard some portion of training data, e.g., 80 and 90. We report our experiments in Table 4. For the most severe settings, CoOp, OrdinalCLIP, and our L2RCLIP methods incur performance losses of 42.24%, 34.49%, and 30.98%, respectively, which demonstrates that our approach exhibits superior robustness when faced with data distribution shift problems. For all shift settings, our method shows better performance than OrdinalCLIP, which highlights the effectiveness of the proposed learning-to-rank method in CLIP compared with interpolation.

### Analysis

Ablation Study.In this section, we conduct an ablation study to examine the respective roles of each component in L2RCLIP, and the results are reported in Table 5. Three key observations can be drawn from Table 5. First, each proposed component demonstrates improvements over the baseline model and complements one another. Second, although RankFormer marginally enhances the ordinal information of vanilla rank templates, its performance remains unsatisfactory compared to other methods when only contrastive loss is utilized for training. This result further highlights the importance of our cross-modal pairwise loss in refining the CLIP feature space. Third, fine-tuning rank templates without RankFormer significantly impairs performance on CLAP2015, potentially due to imbalanced training for certain rank templates. This finding underscores the effectiveness of RankFormer in modeling the ordering relation derived from the complete set of rank templates.

Initialization Impact.As we aim to enhance the ordering relation vanilla rank templates, it is essential to examine the effects of initialization. We report our results in Table 6. Although our

methods fix the original rank templates, different initializations lead to similar convergence and performance with a low standard deviation value of 0.014. This observation further substantiates the robustness of our methods against diverse initializations.

Compared with interpolation-based method.To further prove the effectiveness of our proposed method, we compare our L2RCLIP with previous interpolation-based method (e.g. OrdinalCLIP). We adopt the same setting except the process of ordinality learning and we term it as L2RCLIP-I. We report the results on the aging dataset. More experiments for few-shot learning and distribution shift and the implementation details setting for L2RCLIP-I can be found in appendix.

As illustrated in Table 7, our method outperforms interpolation-based methods with a significant margin in experiments involving a large number of rank categories. This outcome is attributable to the challenge posed by direct interpolation methods in modelling complex ordering relationships. Our approach continues to surpass interpolation-based methods even in experiments with a smaller number of rank categories. Collectively, these experiments corroborate the effectiveness of the methods proposed in this study.

### Image Aesthetics Assessment

Datasets.CrowdBeauty  consists of 13,929 available Flickr photos across four categories: nature, animal, urban, and people. The aesthetic quality of each image is evaluated using five absolute rating scales: "unacceptable", "flawed", "ordinary", "professional", and "exceptional". Following previous methods [29; 20], we select 80% of the images for training and the rest for testing. Five-fold cross-validation is employed for fair comparisons. Both the mean MAE and accuracy are reported.

Results.Table 8 presents our results on the CrowdBeauty dataset. Aesthetic score prediction is challenging due to the subjectivity and ambiguity of aesthetic criteria; however, our L2RCLIP demonstrates state-of-the-art performance on most experimental settings by fully exploring learning-to-rank with language prior. Compared with OrdinalCLIP, L2RCLIP improves accuracy by 3.02% and

   Initialization & MAE(\(\)) & OS(\(\%\), \(\)) \\  _A photo of [age] years old face._ & 2.13 & 71.87 \\ _Age estimation: a person at the age of [age]._ & 2.14 & 72.07 \\ _The age of the person in the portrait is [age]._ & 2.12 & 71.32 \\ _The age of the person is [age]._ & 2.16 & 71.52 \\ _The age of the face is [age]._ & 2.14 & 71.08 \\  Mean/Std & 2.14/0.01 & 71.57/0.40 \\   

Table 6: Initialization Impact Analysis.

    & Morph & CLAP2015 &  \\  & (MAE, \(\)) & (MAE, \(\)) & (Accuracy, \(\)) & (MAE, \(\)) \\  L2RCLIP-I & 2.19 & 2.78 & 62.9 \(\) 5.5 & 0.42 \(\) 0.06 \\ L2RCLIP (Ours) & **2.13** & **2.62** & **68.2 \(\) 7.2** & **0.36 \(\) 0.05** \\   

Table 7: Results on Morph, CLAP2015 and Adience datasets.

    &  &  \\   & Nature & Animal & Urban & People & Overall & Nature & Animal & Urban & People & Overall \\  CNNPOR  & 71.86 & 69.32 & 69.09 & 69.94 & 70.05 & 0.294 & 0.322 & 0.325 & 0.321 & 0.316 \\ SORD  & 73.59 & 70.29 & 73.25 & 70.59 & 72.03 & 0.271 & 0.308 & 0.276 & 0.309 & 0.290 \\ POE  & 73.62 & 71.14 & 72.78 & 72.22 & 72.44 & 0.273 & 0.299 & 0.281 & 0.293 & 0.287 \\ GOL  & **73.8** & 72.4 & 74.2 & 69.6 & 72.7 & 0.27 & 0.28 & 0.26 & 0.31 & 0.28 \\  Vanilla CLIP  & 65.24 & 45.67 & 58.78 & 53.06 & 55.68 & 0.461 & 0.557 & 0.468 & 0.524 & 0.502 \\ CoOp  & 72.74 & 71.46 & 72.14 & 69.34 & 71.42 & 0.285 & 0.298 & 0.294 & 0.330 & 0.302 \\ OrdinalCLIP  & 73.65 & 72.85 & 73.20 & 72.50 & 73.05 & 0.273 & 0.279 & 0.277 & 0.291 & 0.280 \\ L2RCLIP(Ours) & 73.51 & **75.26** & **77.76** & **78.69** & **76.07** & **0.267** & **0.253** & **0.216** & **0.246** & **0.245** \\   

Table 8: Results on Image Aesthetics dataset.

reduces MAE by 0.35 overall, which further verifies the effectiveness of our proposed learning-to-rank method. When compared with the best approach without language prior, L2RCLIP improves overall MAE by 0.35 and overall accuracy by 3.37%. Consistent improvements are observed across all categories compared to previous methods, showcasing the effectiveness of our proposal in exploiting language ordinal information.

### Historical Image Dating

Datasets.The historical image dating dataset  serves as a benchmark for automatically predicting the decade of historical colored images. The dataset comprises five-decade categories, ranging from the 1930s to the 1970s. Following , we adopt the same train-test split and ten-fold cross-validation. Both the mean and standard deviation for MAE and accuracy metrics are reported.

Results.Table 9 showcases improvements compared to other state-of-the-art models using the full dataset. Initially, zero-shot CLIP exhibits suboptimal performance resulting from inadequate semantic and ordinal alignment. CoOp enhances the results by incorporating global context prompts to facilitate semantic alignment. Furthermore, OrdinalCLIP exploits interpolation to improve the average MAE by 0.09. Our L2RCLIP further advances the average MAE by 0.25 with a lower standard deviation. In comparison with the previous conventional model, L2RCLIP achieves a new state-of-the-art performance with an MAE of 0.43 and an accuracy of 67.22%, thereby validating the effectiveness of our proposed method.

## 5 Discussions and Conclusions

In this paper, we propose L2RCLIP to boost learning-to-rank of CLIP for ordinal classification. Specifically, we introduce a complementary prompt tuning method, termed RankFormer, to enhance the ordering relation of the original rank prompts. It performs token-level attention and residual-style prompt blending for prompt tuning. Additionally, we revisit the approximate bound optimization of cross-entropy and reformulate it in the cross-modal embedding space by incorporating the language knowledge. To additionally recover the ordinal information, we further introduce cross-modal ordinal pairwise loss to refine the ordering alignment of CLIP feature space. Extensive experiments demonstrate the effectiveness of our approach on various ordinal classification tasks, including facial age estimation, historical image dating, and image aesthetics assessment. Furthermore, L2RCLIP outperforms in the challenging few-shot learning and data distribution shift learning scenarios. Lastly, we conduct a comprehensive ablation study to verify the effectiveness of each component of our proposed method.

Broader Impacts.As a versatile approach, L2RCLIP can be applied to any ordinal classification tasks such as image aesthetics assessment or other rank assessment. However, these tasks may pose a risk of unlawful surveillance or invasion of privacy if abused. Meanwhile, as L2RCLIP is based on a large-scale vision-language model, addressing demographic biases in pre-trained vision-language models is of significant importance. Therefore, we emphasize that L2RCLIP represents a research proof of language-driven learning and is not appropriate for real-world usage without strict technical controls.

Acknowledgement.This research is sponsored by National Natural Science Foundation of China (Grant No. 62306041, 62006228), Beijing Nova Program (Grant No. Z211100002121106, 20230484488, 20230484276), and Youth Innovation Promotion Association CAS (Grant No.2022132).

    &  \\   & MAE(\(\)) & Accuracy(\(\%,\)) \\  CNNPOR  & 0.82 \(\) 0.05 & 50.12 \(\) 2.65 \\ GP-DNNOR  & 0.76 \(\) 0.05 & 46.60 \(\) 2.98 \\ POE  & 0.76 \(\) 0.04 & 54.68 \(\) 3.21 \\ MWR  & 0.58 & 57.8 \\ GOL  & 0.55 & 56.2 \\  Vanilla CLIP  & 1.01 \(\) 0.03 & 30.41 \(\) 3.32 \\ CoOp  & 0.76 \(\) 0.06 & 51.94 \(\) 2.60 \\ OrdinalCLIP  & 0.67 \(\) 0.03 & 56.44 \(\) 1.66 \\ L2RCLIP(Ours) & **0.43 \(\) 0.02** & **67.22 \(\) 1.59** \\   

Table 9: Results on HCI dataset.