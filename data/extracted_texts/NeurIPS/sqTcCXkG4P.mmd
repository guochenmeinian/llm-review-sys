# Sparsity-Preserving Differentially Private Training

of Large Embedding Models

 Badih Ghazi

Google Research

Mountain View, CA

badihghazi@gmail.com &Yangsibo Huang

Princeton University

Princeton, NJ

yangsibo@princeton.edu &Pritish Kamath

Google Research

Mountain View, CA

pritishk@google.com &Ravi Kumar

Google Research

Mountain View, CA

ravi.k53@gmail.com &Pasin Manurangsi

Google Research

Mountain View, CA

pasin@google.com &Amer Sinha

Google Research

Mountain View, CA

amersinha@google.com &Chiyuan Zhang

Google Research

Mountain View, CA

chiyuan@google.com

###### Abstract

As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen. DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much. However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency. To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during private training of large embedding models. Our algorithms achieve substantial reductions (\(10^{6}\)) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets.

## 1 Introduction

Large embedding models have emerged as a fundamental tool for various applications in recommendation systems  and natural language processing . Those models map categorical or string-valued input attributes with large vocabularies to fixed-length vector representations using embedding layers, which enable integrating non-numerical data into deep learning models. These models are widely deployed in personalized recommendation systems and achieve state-of-the-art performance in language tasks such as language modeling, sentiment analysis, and question-answering.

In the meantime, the use of large embedding models raises significant concerns about privacy violations, as it often entails the processing of sensitive individual information such as personally identifiable information, browsing habits, and dialogues . Various techniques have been proposed to enable private data analysis. Among those, differential privacy (DP)  is a widely adopted notion that could formally bound the privacy leakage of individual user information while still allowing for the analysis of population-level patterns. For training deep neural networks with DP guarantees, the most widely used algorithm is DP-SGD. It works by clipping the per-example gradient contribution, and noising the average gradient updates during each iteration of stochastic gradient descent (SGD). DP-SGD hasdemonstrated its effectiveness in protecting user privacy while maintaining model utility in a variety of applications [PTS\({}^{+}\)21, LTLH21, DFAP21, TB21, KCS\({}^{+}\)22, YNB\({}^{+}\)22, DBH\({}^{+}\)22, DGK\({}^{+}\)23].

However, applying DP-SGD to large embedding models presents unique technical challenges. These models typically contain non-numerical feature fields like user/product IDs and categories, as well as words/tokens that are transformed into dense vectors through an embedding layer. Due to the large vocabulary sizes of those features, the process requires embedding tables with a substantial number of parameters. In contrast to the number of parameters, the gradient updates are usually extremely sparse because each mini-batch of examples only activates a tiny fraction of embedding rows. This sparsity is heavily leveraged for industrial applications [GODS19, WWL\({}^{+}\)22] that efficiently handle the training of large-scale embeddings. For example, Google TPUs [AF22], a family of accelerators designed specifically for training large-scale deep networks, have dedicated hardware such as the SparseCore [JKL\({}^{+}\)23] to handle large embeddings with sparse updates. It is shown [Sna22] that this leads to significantly improved training throughput compared to training on GPUs, which did not have specialized optimization for sparse embedding lookups at the time. On the other hand, DP-SGD completely destroys the gradient sparsity as it requires adding independent Gaussian noise to _all_ the coordinates. This creates a road block for private training of large embedding models as the training efficiency would be significantly reduced compared to non-private training.

Our contributions.In this paper, we present new algorithms that target the issue of diminished gradient sparsity encountered when employing DP-SGD with large embedding models due to the addition of dense noise; this issue represents a significant practical obstacle for leveraging, during DP training, the hardware accelerators commonly used in non-private training of large embedding models. While there are previous studies of the abstract problems of sparse vectors in DP, we study the practical problem of privately training large embedding models, and through systematic evaluations, demonstrate that significant gradient size reduction is achievable in DP training of real-world large-scale embedding models (e.g., recommender systems, language models with \(>30\)M embedding parameters) with minimal loss in utility. Our contributions can be summarized as follows:

* We propose two new algorithms to preserve gradient sparsity in DP-training of large embedding models while maintaining their utility (Section3). The first algorithm, **F**iltering-**E**nabled **S**parse **T**raining (DP-FEST), selectively adds noise to pre-selected embedding rows during training. To improve the adaptivity to varying training dynamics across mini-batches and to accommodate online training, our second algorithm, **A**daptive **F**iltering-**E**nabled **S**parse **T**raining (DP-AdaFEST), adaptively computes a gradient contribution map for each mini-batch, and only injects noise into the gradients of top contributors (in a manner that complies with the DP requirements).
* We demonstrate the effectiveness of our algorithms on four benchmark datasets for online advertising and natural language understanding. Our results indicate that these algorithms can meet a wider range of application-specific demands for utility and efficiency trade-offs at a more granular level (Section4.2). Notably, they achieve a substantially sparser gradient, with a reduction in gradient size of over \(10^{6}\) compared to the dense gradient produced by vanilla DP-SGD, while maintaining comparable levels of accuracy.
* We further showcase the potential applicability and effectiveness of our algorithms to the setting of online learning with streaming data, a common setup in practice, which highlights the versatility and broad impact of our algorithms (Section4.3).

## 2 Preliminaries

### Embedding Layers and Gradient Sparsity

Models with large embedding layers are widely used in application scenarios that need to process high-dimensional sparse inputs such as words/tokens in language models [MSC\({}^{+}\)13, PSM14, DCLT19] and categorical features associated with users or items in recommendation systems [CKH\({}^{+}\)16, WFFW17, GTY\({}^{+}\)17, ZZS\({}^{+}\)18, NMS\({}^{+}\)19, WSC\({}^{+}\)21].

Let \(c\) be the number of possible values of an input feature (aka _vocabulary size_ in text models); the feature values are also called _feature buckets_ (string-valued categorical features are commonly preprocessed via hashmap bucketization). An input \(\) with the \(i\)th feature value can be represented as a one-hot vector \(_{i}^{c}\) that is \(1\) on the \(i\)th coordinate and \(0\) everywhere else. Let \(^{c d}\) be the parameters of the embedding layer (i.e., the embedding table), where the output dimension \(d\) is the embedding size_. The output of the embedding layer is computed as \(=^{}\), which is a linear map. Due to efficiency considerations, in practice, embedding layers are rarely implemented via matrix multiplication (matmul) on one-hot encoded inputs.

For a one-hot input \(=_{i}\), the embedding output is the \(i\)th row of the embedding table: \(=[i,:]\). Figure 0(a) illustrates the embedding table lookup operation. Let \(/^{d}\) be the partial derivative of the loss with respect to the embedding output. The gradient of the embedding table is \( W=/\), where \(\) is the outer product. For a one-hot input \(=_{i}\), the result of this outer product is a sparse matrix whose \(i\)th row equals \(/\). For mini-batch SGD training, the number of non-zero rows in the batch averaged gradient of the embedding table is upper bounded by the batch size, which is typically orders of magnitude smaller than \(c\). Consequently, the _gradient sparsity_ (the fraction of zero gradient coordinates) for large embedding models is very high (Figure 0(b)).

Due to this structured sparsity, in practice, both forward and backward computations of an embedding layer are implemented efficiently with gathers and scatters1, without expensive matmul. This difference from vanilla linear layers is crucial in real-world applications as the embedding tables are usually very large, with the vocabulary size \(c\) ranging from tens of thousands (language models) to millions (recommendation systems). Moreover, in some large recommendation system models, there are hundreds of different categorical features, each with a different embedding table. Therefore, maintaining the gradient sparsity is critical for any training process.

For notational simplicity, the description above focuses on single-variate features, which only activate one feature value at a time. In practice, multi-variate features activating multiple values are also used. In this case, it is common for the embedding layer to output the average or sum vectors corresponding to each activated value. We also note that in recommendation systems, each categorical feature generally has a separate embedding table; But for text models, all the words/tokens in an input document share the same embedding table.

### Differentially Private Stochastic Gradient Descent

_Differential privacy (DP)_ is a mathematical framework for ensuring the privacy of individuals in datasets. It can provide a strong guarantee of privacy by allowing data to be analyzed without revealing sensitive information about any individual in the dataset. Formally, a randomized algorithm \(\) satisfies _\((,)\)-DP_ if for any two neighboring datasets \(\) and \(^{}\) (i.e., datasets such that one can be obtained from the other by adding/removing one example), and any subset \(\) of outputs, it holds for privacy parameters \(_{>0}\) and \([0,1)\) that

\[[()]\ \ e^{}[ (^{})]+.\]

_DP Stochastic Gradient Descent_ (DP-SGD)  is a recipe for training a deep learning model with DP by modifying the mini-batch stochastic optimization process through the use of per-example gradient clipping and Gaussian noise injection. When training an ML model \(f\) parameterized by \(\) with the per-example loss function \((,)^{2}\) on dataset \(\), each optimization step \(t\) involves randomly

Figure 1: Illustration of embedding lookup (a) and gradient sparsity of the Criteo pCTR model (b). We analyze the gradient sparsity, averaged over 50 update steps, of the top five categorical features (out of a total of 26) with the highest number of buckets, as well as the sparsity of all categorical features. For more information about this dataset and model, see Section 4.1.1.

sampling a mini-batch \(_{t}\). Given \(_{t}\), \(\) starts by computing the per-example gradient for each \((x_{i},y_{i})_{t}\), where \(x_{i}\) is the feature vector and \(y_{i}\) is the corresponding label, as follows:

\[_{t}(x_{i},y_{i})_{_{t}}( f_{_{t}}(x_{i}),y_{i}).\]

It then _clips_ the gradient \(_{2}\)-norm to a maximum \(_{2}\)-norm of \(C\) as:

\[[_{t}(x_{i},y_{i})]_{C}:=_{t}(x_{i},y_{i}) (1,_{t}(x_{i},y_{i})\|_{2}}{C}).\]

Finally, it produces the private gradient \(}_{t}\) by injecting Gaussian noise into the sum of the clipped per-example gradients as:

\[}_{t}_{t}\|}(_{i}[ _{t}(x_{i},y_{i})]_{C}+(0,^{2}C^{2} )),\] (1)

where \((0,^{2}C^{2})\) is a Gaussian distribution with mean 0 and covariance \(^{2}C^{2}\), and the noise multiplier \(\) is computed from \((,)\) by inverse privacy accounting (e.g., ).

Although \(\) has been demonstrated to be an effective algorithm for training ML models with DP, (1) requires adding noise to _all_ the coordinates of the gradient, which would completely destroy any existing sparsity structure in the original gradients. This densification of sparse gradients is especially problematic for large industrial-scale embedding models, where the sparsity is heavily leveraged to improve efficiency, e.g., using dedicated APIs .

## 3 Sparsity-Preserving \(\)

In this section, we introduce two algorithms for preserving the gradient sparsity of \(\) when training large embedding models. Our main intuition is that in real-world datasets, some buckets of categorical features may be much more frequent, and therefore contain more significant or relevant information, than others. However, vanilla \(\) adds noise to the model's parameter gradients, even to those with minimal impact on the model's accuracy, which is not only wasteful but also can reduce the training efficiency by disrupting gradient sparsity (as discussed earlier in Section2.2).

### Filtering-Enabled Sparse Training (\(\))

A simple solution to this problem is _frequency filtering_, where we pre-select the top-\(k\) most informative (i.e., frequent) buckets of each categorical feature before training, and add Gaussian noise only to the gradients of these selected buckets during training; this is akin to training a smaller embedding model using a subset of the buckets. Frequency filtering significantly reduces the noise added to the gradients by restricting the noise to only the most impactful subset of features.

There are two possible ways of selecting the top-\(k\) buckets. First, if there is prior information on bucket frequency available publicly (e.g., token frequency in the pre-training set of the model for language tasks), then it can be used to select the top-\(k\) buckets. If such prior knowledge is unavailable, we can run DP top-\(k\) selection on the training dataset by adapting the algorithm of ; see Appendix B.1 for details.

### Adaptive Filtering-Enabled Sparse Training (\(\))

The limitation of frequency filtering in \(\) is its inability to adapt to varying activation patterns across different mini-batches during training. This issue becomes particularly problematic when dealing with streaming data, such as those used in recommendation systems. In such scenarios, the application of frequency filtering poses a challenge as it impedes training until a substantial amount of data is gathered to accurately estimate the frequency information.

To address this issue, we propose \(\), an adaptive method for selecting the most informative buckets using mini-batch information, as outlined in Figure2 and Algorithm1. For each mini-batch, \(\) first computes the per-example gradient and a gradient contribution map, which is a binary vector indicating which categorical feature buckets have been activated for each example in the mini-batch (Line5). These per-example gradient contributions are then aggregated (with per-example clipping applied) across the mini-batch in order to construct the batch-wise gradient contribution,and Gaussian noise with scale \(_{1}\) is added to the resulting vector to ensure DP (Line 6).3 The noisy batch-wise gradient contribution is then thresholded using a parameter \(\) to exclude non-significant gradient entries to which only a few examples in the mini-batch contribute (Line 8); This thresholding mechanism helps focus on the most relevant and informative features while reducing the impact of noisy or insignificant contributions. Finally, the algorithm updates the model parameters by adding gradient noise with scale \(_{2}\) only to the "surviving" gradient entries (Lines 9 and 10). By dynamically adapting feature selection based on bucket contributions in each mini-batch during training, our algorithm achieves adaptive feature selection and maintains the desired DP guarantees. Note that we apply the standard DP-SGD with noise multiplier \(_{2}\) to update parameters in non-embedding layers.

### Privacy Accounting

The privacy accounting of DP-AdaFEST can be done in a similar way to that of DP-SGD. The main observation is that the privacy cost of Algorithm 1 is dictated by the Gaussian noise mechanism steps in Lines 6 and 9. In particular, the privacy cost of a single iteration is equivalent to that of the composition of two Gaussian mechanisms with noise scale \(_{1}\) and \(_{2}\) respectively, which in turn is equivalent to the privacy cost of a single Gaussian mechanism of noise scale \(=(_{1}^{-2}+_{2}^{-2})^{-1/2}\). Thus, overall, the privacy cost of the entire algorithm is no more than that of DP-SGD with noise scale \(\), and identical other parameters of batch size and number of steps. Finally, we adopt the standard approach of analyzing the Poisson subsampled Gaussian mechanism [ACG\({}^{+}\)16], using numerical algorithms based on privacy loss distributions [KJH20, GLW21, GKKM22, DGK\({}^{+}\)22]. In particular, we use the open-source implementation available in Google's DP library [Goo20]. We provide more details in Appendix C for completeness.

Figure 2: Illustration of the process of DP-AdaFEST on a synthetic categorical feature which has 20 buckets. We compute the number of examples contributing to each bucket, adjust the value based on per-example total contributions (including those to other features), add Gaussian noise, and retain only those buckets with a noisy contribution exceeding the threshold for (noisy) gradient update.

An important caveat to the privacy analysis is that we follow a common practice of analyzing all the algorithms where the mini-batches are _Poisson subsampled_, wherein each example is included independently with a certain probability, even though, the actual implementation involves selecting mini-batches of fixed sizes after shuffling. See Section 4.3 of [PHK\({}^{+}\)23] for more context. But since this caveat applies to all algorithms similarly, we consider their relative comparison to be fair.

### Bias-Variance Trade-offs

Apart from the computational advantage, to understand the question of when DP-AdaFEST would lead to more accurate models as compared to DP-SGD, we look through the lens of bias-variance trade-offs in stochastic convex optimization. Consider a _convex_ objective \(()\) over \(^{D}\), for which we have a gradient oracle, that given \(\), returns a stochastic estimate \(g()\) of \(()\). We say that the gradient oracle has bias \(\) and variance \(^{2}\) if \(g()=()+()\) such that \(\|\,()\|_{2}\) and \(\,\|()-\,()\|_{2}^{2}^{2}\) holds for all \(\). When optimizing over a convex set \(^{D}\), projected gradient descent with step size \(\) is defined as iteratively performing \(_{t+1}_{}(_{t}- g(_{t}))\), where \(_{}()\) is the projection onto \(\). We recall the following guarantee on the expected excess loss obtained using standard analysis of projected stochastic gradient descent (see e.g. ).

**Lemma 3.1**.: _For an \(L\)-Lipschitz loss function, and a gradient oracle with bias \(\) and variance \(^{2}\), projected gradient descent over a set \(\) with diameter \(R\), with step size \(=+^{2})T}}\) achieves_

\[[(_{i=1}^{T}_{i}) ]-(^{*})\ \ }+^{2}}+ R.\]

Suppose the loss is \(L\)-Lipschitz. Ignoring the potential bias introduced due to clipping, DP-SGD of noise scale \(\) uses a gradient oracle with zero bias (\(=0\)) and variance \(D^{2}\). On the other hand, consider a hypothetical setting where DP-AdaFEST truncates \(\) fraction of the gradient due to masking in Line 8, resulting in the final gradient that is supported on \(h\) coordinates (\(h D\)). In this case, the bias introduced is \( L\), but the variance introduced is only \(h^{2}\). Plugging into the above excess risk bound, we would expect DP-AdaFEST to achieve a smaller excess loss when

\[(1+)^{2}+h^{2}}+ L\ <\ +D ^{2}}\,.\] (2)

This implies that a smaller truncated fraction within DP-AdaFEST could potentially yield superior utility compared to vanilla DP-SGD. Even though the discussion above is for the average iterate, and in the convex setting, our experiments corroborate this observation in training deep learning models.

## 4 Experiments

In this section we evaluate the performance of our sparsity-preserving training algorithms and compare them against vanilla DP-SGD on both the recommendation and language understanding tasks (Section 4.1). To understand adaptivity, we evaluate our algorithms on a time-series version of the data (Section 4.3); the non-time-series evaluations are in Section 4.2. We further demonstrate the applicability of our methods on language models in Section 4.4. The impact of hyper-parameters on the trade-off between utility and embedding gradient size is discussed in Section 4.5.

### Setup

#### 4.1.1 Datasets and Models

Click-through rate prediction tasks.We evaluate our algorithms on the widely-used Criteo predicted click-through rate (pCTR) dataset4, which includes over four billion ad impressions over 24 days. Each impression is represented by 13 numerical features and 26 categorical features. The objective is to predict how likely a user clicks on an ad based on these features.

The pCTR model we experiment with is a neural network that uses embedding layers for categorical features and log transformations for numerical features, followed by several fully connected layers. We use binary cross-entropy loss as the training objective and report the AUC as the evaluation metric. For further details about the training setup, please refer to Appendix D.1.

We evaluate two Criteo variants with different purposes:* Criteo-Kaggle5, a subset of examples released by Kaggle (\(\)45 million examples in total), is widely used in previous studies of click-through rate modeling. We use this to benchmark the performance of our algorithms. Note that timestamps are absent in Criteo-Kaggle. * Criteo-time-series, also commonly known as "Criteo-1TB", is the entire Criteo dataset of over 4 billion examples and has the auxiliary information indicating which day the data was collected. To simulate a real-world online training scenario, we train the model on the first 18 days of data and evaluate it on subsequent days (i.e., days 19-24); more details on the training process are in Section 4.3.

Language understanding tasks.For language understanding tasks, we employ language models from the BERT family , specifically the RoBERTa model . This model has a vocabulary size of \(50,265\) subword tokens and has been pre-trained on public web data.

We fine-tune the RoBERTa model for downstream classification tasks from the GLUE benchmark , including SST-2 , QNLI , and QQP . Following , we adopt Low-Rank Adaptation (LoRA)  to introduce trainable rank decomposition matrices into each transformer block of the language model. This approach significantly reduces the number of parameters required for downstream tasks while also improving the privacy-utility trade-off (see Appendix D.1). A notable difference in our approach compared to  is that we also train the word embedding layers in DP fine-tuning to enable bucket selection. This leads to significant accuracy improvements in the model, as shown in Table 6.

#### 4.1.2 Baseline and Algorithms

We evaluate the performance of vanilla DP-SGD, and the three sparsity-preserving algorithmic variants, DP-SGD with exponential selection , DP-FEST and DP-AdaFEST, on both the click-through rate prediction and language understanding tasks. We use a fixed batch size of \(2,048\) for the former and \(1,024\) for the latter. For all tasks, we set the privacy parameter \(\) to \(1/N\), where \(N\) is the number of training examples for the respective task.

### Evaluation on Non-Time-Series Data

We begin by considering datasets with no timestamp--Criteo-Kaggle, SST-2, QNLI, and QQP.

The decision to prioritize gradient sparsity (a key factor impacting efficiency) or utility in DP training usually depends on the task and available computational resources. However, it can be challenging for vanilla DP-SGD to cater to different needs of gradient sparsity or utility since it lacks mechanisms for trading off between these objectives. In contrast, DP-SGD with exponential selection ,

Figure 4: A comparison between DP-AdaFEST, DP-FEST, and the combined algorithm (DP-AdaFEST+) on the Criteo-Kaggle dataset for different values of \(\). The combined algorithm significantly outperforms either alone. Figure 6 demonstrates similar findings on the Criteo-time-series dataset.

Figure 3: A comparison of the best gradient size reduction achieved by DP-AdaFEST, DP-FEST, and DP-SGD with exponential selection  compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms DP-FEST.

DP-AdaFEST and DP-FEST can all offer diverse options for balancing utility and efficiency via the sparsity-controlling parameters (see Figure 8 in Appendix D.2), while our proposals DP-AdaFEST and DP-FEST offer much better privacy-utility loss.

While both DP-FEST and DP-AdaFEST offer ways to balance efficiency and utility in DP training, DP-AdaFEST stands out as the _more versatile and customizable_ algorithm. With three adjustable hyper-parameters, DP-AdaFEST offers more diverse results compared to DP-FEST's single knob, \(k\), which controls the number of preserved top buckets. The effectiveness of DP-AdaFEST is evident in Figure 3, where it achieves significantly higher gradient size reduction than DP-FEST while maintaining the same level of utility. Specifically, on the Criteo-Kaggle dataset, DP-AdaFEST reduces the gradient computation cost of vanilla DP-SGD by more than \(5 10^{5}\) times while maintaining a comparable AUC (of AUC loss less than 0.005). This reduction translates into a more efficient and cost-effective training process (Appendix D.2.1 shows wall-clock time improvements). Although the best reduction for language tasks is usually smaller since the vocabulary is already relatively condensed, the adoption of sparsity-preserving DP-SGD effectively obviates the dense gradient computation. Furthermore, in line with the bias-variance trade-off presented in Section 3.4, we note that DP-AdaFEST occasionally exhibits superior utility compared to DP-SGD when the reduction in gradient size is minimal (i.e., \(\) in Equation (2) is small). Conversely, when incorporating sparsity, DP-SGD with the exponential mechanism faces challenges in maintaining utility. In all configurations, it fails to achieve a tolerable level of utility loss.

We further explore the potential of integrating DP-AdaFEST with DP-FEST to enhance performance, which involves pre-selecting a subset of buckets using DP-FEST and subsequently training on this subset using DP-AdaFEST. Figure 4 shows that the combined algorithm (DP-AdaFEST+) outperforms either alone in utility/efficiency trade-off, with the best gradient size reduction being further improved to \(>10^{6}\). This can be attributed to the complementary strengths of the two algorithms: DP-AdaFEST offers a more flexible approach to feature selection at the batch level, while DP-FEST provides a simple yet effective means for feature selection according to the global frequency information. Besides, the combined algorithm expands the range of choices for balancing gradient sparsity and utility through the combination of hyper-parameters from both DP-FEST and DP-AdaFEST.

### Evaluation on Time-Series Data

Time-series data are notoriously challenging due to non-stationarity. In this section, we investigate if our algorithms can adapt to distribution shifts, characteristic of time-series data.

We conduct experiments using the Criteo-time-series dataset, which comprises real-world user-click data collected over 24 days (18 days for training and the rest for evaluation). To simulate the online data streaming scenario, we introduce the concept of a _streaming period_, a time interval during which the model is updated as new data is received. Experiments indicate a notable disparity between DP training (using vanilla DP-SGD) and non-DP training, with the former demonstrating increased susceptibility to distribution shifts (Table 5 in Appendix D.2).

Figure 5 presents the utility and efficiency trade-off of DP-AdaFEST and DP-FEST for time-series data. To explore the efficacy of DP-FEST, we investigate different sources of vocabulary frequency, including the information from the first day, all days, and streaming-based (i.e., running sum updated

Figure 5: A comparison between DP-AdaFEST and DP-FEST for time-series data, using the Criteo-time-series dataset with different streaming periods \(T\) and \(=1.0\). DP-AdaFEST consistently achieves a higher gradient reduction than DP-FEST at the same level of utility.

per streaming period). The experimental results indicate that using streaming-based frequency information for DP-FEST is almost as effective as using all-day frequency information and significantly better than using information only from the first day. Additionally, DP-AdaFEST consistently outperforms DP-FEST, achieving more than twice the gradient reduction at the same level of utility. These findings further demonstrate the importance of adapting to the dynamic nature of time-series data at a more fine-grained level, with DP-AdaFEST able to adapt at the batch level and DP-FEST only able to adapt at the streaming period level.

We further examine the advantages of combining DP-AdaFEST and DP-FEST to enhance the utility/efficiency trade-off on the Criteo-time-series dataset. We use a streaming period of 1 and streaming-based frequency information for DP-FEST. Figure 6 demonstrates a consistent superiority of the combined approach over individual methods, which aligns with the findings presented in Section 4.3.

### Applicability to Language Models

We further demonstrate the applicability of our method to language models by comparing it with LoRA  and reporting its performance with multilingual models.

DP-AdaFEST **is a better choice than LoRA for embedding layers.** LoRA was introduced as a method to efficiently adapt matrices of dimensions \(n d\) in language models by utilizing a rank-\(r\) approximation, where the initial use case considers the attention layers where \(n=d\). The rank-\(r\) approximation helps in reducing the memory requirements by a factor of \(m d/(m+d)*r<(m,d)/r\). However, the applicability of LoRA to embedding layers is limited for several reasons:

* **Limited improvement for unbalanced \(n\) and \(d\):** Embedding layers typically involve a large vocabulary size (\(n\) often in the millions) and a smaller embedding dimension (\(d\) typically in the hundreds). In this context, LoRA's potential benefits are limited.
* **Inability to harness APIs**: For private training of the embedding layer, DP-AdaFEST allows for efficient embedding lookup operations (row fetching) using custom APIs; In contrast, LoRA still relies on computationally costly matrix multiplication and cannot effectively utilize these APIs.
* **Limited to fine-tuning use cases**: LoRA is designed for adapting pre-trained models, while our DP-AdaFEST is versatile, functioning seamlessly in both pre-training and fine-tuning scenarios.

To more effectively demonstrate the first argument above, Table 1 compares the best embedding gradient size reductions achieved by DP-AdaFEST and LoRA against DP-SGD for SST-2 with \(=1.0\), on the RoBERTa model. We vary LoRA's rank \(r\) from \(\{4,8,16,32,64,128\}\). DP-AdaFEST consistently outperforms LoRA in gradient size reduction at similar utility levels.

Increased gradient size reduction by DP-AdaFEST for larger vocabularies.Our evaluation in Section 4.2 primarily centered around the RoBERTa model, which has a vocabulary of around \(50,000\) entries. However, it is important to note that many high-vocabulary models exist, with vocabulary sizes \(5\) to \(20\) larger than RoBERTa. For these models, DP-AdaFEST could offer even more pronounced benefits.

  
**Utility loss compared to DP-SGD** &  \\ 
0.001 & 14.59\(\) & 17.21\(\) \\
0.005 & 53.90\(\) & 64.75\(\) \\
0.01 & 53.90\(\) & 152.94\(\) \\   

Table 1: Gradient size reduction by LoRA and DP-AdaFEST for RoBERTa’s word embeddings.

  
**Utility loss compared to DP-SGD** &  \\ 
0.001 & 14.59\(\) & 17.21\(\) \\
0.005 & 53.90\(\) & 64.75\(\) \\
0.01 & 53.90\(\) & 152.94\(\) \\   

Table 2: DP-AdaFEST offers more pronounced gradient size reduction for models with larger vocabulary (i.e., higher \(|V|\)). RoBERTa results are achieved on SST-2 with \(=1.0\), and the XLM-R [\(^{+}20\)] results are achieved on the Cross-Lingual Natural Language Inference (XNLI) dataset [\(^{+}18\)] with \(=1.0\).

Figure 6: The combined approach (DP-AdaFEST+) outperforms either alone on Criteo-time-series.

### Effect of Hyper-Parameters

Finally, we study the effect of hyper-parameters on the utility and gradient size trade-off in DP-AdaFEST. Specifically, we explore the effects of the ratio of noise added to the contribution map and the noise added to the sparse gradient (i.e., \(_{1}/_{2}\)), and the thresholding value \(\).

**Effect of \(/_{2}}\).** We find that a larger ratio of \(_{1}/_{2}\) leads to a higher accuracy (Figure 7). Indeed, the contribution map can tolerate higher levels of noise, leading to a more accurate representation of the sparse gradient. For both Criteo-Kaggle and SST-2 datasets, a noise ratio of \(5\) or \(10\) delivers the best utility. On the other hand, a larger \(_{1}/_{2}\) results in a higher gradient density because the contribution map can tolerate higher noise levels, resulting in more zero-contribution buckets bypassing the threshold and increasing false negatives. Therefore, choosing an appropriate \(_{1}/_{2}\) is critical to achieving optimal model accuracy and gradient density.

**Effect of \(\).** The choice of the thresholding value, \(\), is also crucial for determining the balance between gradient sparsity and model accuracy in DP-AdaFEST. Figure 7 reveals that increasing the value of \(\) usually leads to a decrease in the gradient size, which results in a sparser gradient. We also find that when \(\) is small, increasing it does not significantly impact the model's performance. However, setting an excessively high value of \(\) (e.g., \(>500\) for a batch size of \(1024\)) can cause a sharp drop in the model's accuracy, especially when the noise ratio \(_{1}/_{2}\) is small; In such cases, important contributions may be zeroed out, resulting in a loss of information in the gradient. Again, it is critical to choose \(\) to best balance gradient sparsity and model accuracy in DP-AdaFEST.

## 5 Conclusions

In this work we present new algorithms--DP-FEST and DP-AdaFEST--for preserving gradient sparsity in DP training, particularly in applications involving large embedding models. Our algorithms achieve significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. We also provide recommendations on hyper-parameter settings.

Limitations and future work.Despite the advancements made in our proposed algorithms, there are still limitations and areas for future work. One such area is the exploration of customized hardware acceleration for sparse DP training, which has the potential to significantly improve the training efficiency of our models. By leveraging specialized hardware, we can further optimize the computational performance and speed up the training process. Additionally, combining our algorithms with other privacy-preserving techniques (e.g., Federated Learning) could lead to new insights and applications.

Figure 7: Effect of hyper-parameters on utility and embedding gradient size, including the ratio of noise added to the contribution map to the one added to the sparse gradient \(_{1}/_{2}\), and the thresholding value \(\). The joint impact of two hyper-parameters on utility and embedding gradient size can be found in Figure 9 in Appendix D.2.