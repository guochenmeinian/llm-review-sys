# Fair GLASSO: Estimating Fair Graphical Models

with Unbiased Statistical Behavior

 Madeline Navarro

Rice University

nav@rice.edu

&Samuel Rey

King Juan Carlos University

samuel.rey.escudero@urjc.es

&Andrei Buciulea

King Juan Carlos University

andrei.buciulea@urjc.es

Antonio G. Marques

King Juan Carlos University

antonio.garcia.marques@urjc.es

&Santiago Segarra

Rice University

segarra@rice.edu

###### Abstract

We propose estimating _Gaussian graphical models (GGMs)_ that are _fair with respect to sensitive nodal attributes_. Many real-world models exhibit unfair discriminatory behavior due to biases in data. Such discrimination is known to be exacerbated when data is equipped with pairwise relationships encoded in a graph. Additionally, the effect of biased data on graphical models is largely unexplored. We thus introduce fairness for graphical models in the form of two bias metrics to promote balance in statistical similarities across nodal groups with different sensitive attributes. Leveraging these metrics, we present Fair GLASSO, a regularized graphical lasso approach to obtain sparse Gaussian precision matrices with unbiased statistical dependencies across groups. We also propose an efficient proximal gradient algorithm to obtain the estimates. Theoretically, we express the tradeoff between fair and accurate estimated precision matrices. Critically, this includes demonstrating when accuracy can be preserved in the presence of a fairness regularizer. On top of this, we study the complexity of Fair GLASSO and demonstrate that our algorithm enjoys a fast convergence rate. Our empirical validation includes synthetic and real-world simulations that illustrate the value and effectiveness of our proposed optimization problem and iterative algorithm.

## 1 Introduction

Data analysis frequently requires estimating complex dyadic relationships, which can be conveniently encoded in graphical representations such as Gaussian graphical models (GGMs) [1; 2; 3]. Myriad real-world applications model structure in data by obtaining graphs from observations, in fields including neuroscience, genomics, finance, and more [4; 5; 6]. However, it is known that real-world data can encode historical biases which models ought not to consider, such as discriminatory biases against sensitive populations [7; 8]. For example, social networks often exhibit preferential relationships that may unfairly discriminate against sensitive communities [9; 10; 11]. Moreover, the use of unfair graphs for downstream tasks is known to exacerbate existing biases [12; 13; 14]. While accurate graphical representations are critical for applications and analyses, the propagation of undesirable bias in graph data necessitates learning models that balance both fairness and accuracy.

The long-standing popularity of GGMs for several applications, many of them high-stakes, warrants care in how we estimate them from potentially biased data. However, there is no formal definition of fairness for graphical models, and existing definitions for graph-based machine learning may not be applicable for obtaining fair statistical relationships. Indeed, while fairness for graph data has recently received copious attention, the study of biased graphs in statistics and graph signal processing (GSP)is only beginning [15; 16; 17]. Furthermore, previous works primarily consider fairness for downstream tasks, while few attempt to learn unbiased graphs from data [16; 17; 18]. We thus arrive at two vital questions. First, _what does it mean for a graphical model to be fair?_ We aim to compare such a notion to existing definitions of fairness on graphs. Second, _how can we obtain GGMs that are fair in the presence of biased data?_ To address these questions, we consider estimation of fair GGMs from biased observations, where nodes belong to groups corresponding to different sensitive attributes.

We propose an optimization framework to obtain fair GGMs from potentially biased data, where statistical dependencies between nodes show no preferences for particular groups. We first define fairness for graphical models by introducing two bias metrics that measure similarities in statistical behavior between pairs of groups. Our metrics are simple and convex, yet they intuitively capture biases in terms of conditional dependence. We then propose _Fair GLASSO_, a penalized maximum likelihood estimator using our bias metrics as regularizers, which aims to obtain sparse Gaussian precision matrices that optimally extract structural information from observed data while promoting fairer statistical behavior across node groups. We summarize our contributions as follows:

* _We formally define fairness for graphical models_ via two bias metrics, one balancing statistical dependencies evenly across all groups and a stronger alternative requiring each node to be balanced across all groups. We relate our definition to other notions of fairness on graphs, where ours is specific to graphs encoding conditional dependence structures, which in turn allows greater interpretability and more detailed statistical analysis.
* We present _Fair GLASSO_, a penalized maximum likelihood estimator for sparse Gaussian precision matrices that are unbiased according to any measure of graphical fairness, which we demonstrate with our proposed bias metrics. We theoretically demonstrate that our approach yields a tradeoff between fairness and accuracy, which depends on the bias in the underlying graph.
* The convexity of Fair GLASSO under our proposed metrics allows us to propose _an efficient iterative method_ based on proximal gradient descent. We show that our algorithm enjoys iterations of moderate complexity and provable convergence.
* We evaluate Fair GLASSO on both _synthetic and real-world datasets_. The former provides empirical validation of the efficiency of our algorithm and the existence of the fairness-accuracy tradeoff. The latter shows the myriad real-world applications for which we can reliably obtain graphical representations from data while also balancing statistical behavior across sensitive groups.

### Notation

For any positive integer \(p\), we let \([p]:=\{1,2,,p\}\). For a matrix \(^{p p}\) and a set of indices \([p]^{2}\), we let \(_{}\) denote a masking operation on \(\) permitting non-zero entries only at indices in \(\). If we define \(:=\{p(i-1)+i\}_{i=1}^{p}\), then \(_{}\) is a diagonal matrix containing the diagonal entries of \(\). For \(}:=[p]^{2}\) denoting the complement of the set \(\), \(_{}}=-_{}\) contains non-zero values of \(\) only in its off-diagonal entries. We also let \(()^{p^{2}}\) denote the vertical concatenation of the columns of \(\). The smallest and largest eigenvalues of a matrix \(\) are represented respectively by \(_{}()\) and \(_{}()\).

## 2 Fair Gaussian Graphical Models

GGMs succinctly model pairwise relationships in multivariate Gaussian distributions through intuitive graphical representations. We denote undirected graphs by \(=(,,)\), where \(=[p]\) is the set of \(p\) nodes and \(\) the set of edges connecting pairs of nodes. For graphs with weighted edges, \(^{p p}\) encodes the topological structure of \(\) such that \(W_{ij} 0\) if and only if \((i,j)\), that is, there is an edge in \(\) connecting nodes \(i\) and \(j\) with weight \(W_{ij}\). Let \(^{p}\) be a random vector following a zero-mean Gaussian distribution with positive definite covariance matrix \(_{0} 0\), that is, \((,_{0})\). The precision matrix \(_{0}=_{0}^{-1}\) completely describes the conditional dependence structure among the variables in \(\). In particular, for any distinct pair \(i,j[p]\), variables \(_{i}\) and \(_{j}\) are conditionally independent if and only if \([_{0}]_{ij}=0\)[1; 2]. This Markovian property yields a graphical representation, where off-diagonal entries of \(\) encode the weighted edges of a graph \(\) connecting conditionally dependent variables. In this work, we aim to obtain the structure encoded in \(_{0}\) using observations sampled from \((,_{0})\).

When considering group fairness, we associate each variable in \(\) with one of \(g\) groups that partition the variables according to a sensitive attribute [20; 21]. We represent group membership by the indicator matrix \(=[_{1},,_{g}]\{0,1\}^{p g}\), where \(Z_{ia}=1\) if and only if variable \(i\) belongs to group \(a[g]\), otherwise \(Z_{ia}=0\). Group sizes are denoted by \(p_{a}=_{i=1}^{p}Z_{ia}\) for every \(a[g]\). We also assume that groups are non-overlapping, thus \(p=_{a=1}^{g}p_{a}\). GGMs may possess biases when a group of variables behaves significantly more or less similarly to particular groups. Indeed, individuals within the same political party tend to vote similarly . In this case, entries of \(_{0}\) corresponding to pairs of voters of the same party will likely be positive and larger in magnitude.

We empirically demonstrate this phenomenon in Figure 1 for multiple real-world networks. Figures (a)a and b show two social network examples, Zachary's karate club network [22; 23] and the Dutch school network , where nodes represent individuals and edges connect them by their relationships. Figure (c)c is a political network that connects U.S. senators if their voting patterns exhibit correlated behavior . For each network, we present both their modularity with respect to group membership  and a comparison of their approximate partial correlations within and across nodal groups [25; 26], with both metrics defined in Appendix G. Figures (a)a and c show higher group-wise modularity, in line with the ubiquitous preference for within-group connections in real-world networks [9; 10]. However, observe that Figures (b)b and c exhibit a clear preference for positive correlations between nodes in the same group. Despite its presence in real-world interconnected data, this type of discriminatory behavior is typically not addressed in existing works . These examples inspire us to develop a definition of fairness for graphical models that accounts for both _correlation bias_, when behavior is highly correlated between certain groups, and _connectivity bias_, when connections are denser or sparser across certain group pairs.

### Bias Metric for Fair Graphical Models

To address biases in both connectivity and correlations, we propose a definition of group fairness for graphical models. We consider the popular notion of demographic parity (DP), the primary choice for fairness on graphs ; however, other definitions of group fairness can be similarly adapted . DP requires that outcomes be agnostic to sensitive attributes [20; 28]. In our case, we require estimation of graphical models with unbiased edge selection with respect to nodal groups. Thus, we present the following definition of dyadic DP for graphical models.

**Definition 1**: _For a graphical model with the matrix \(_{0}^{p p}\) encoding the underlying conditional dependence structure, we consider DP to be satisfied if_

\[[[_{0}]_{ij}|Z_{ia}=1,Z_{ja}=1]=[[_{0}]_{ij}|Z _{ia}=1,Z_{jb}=1]\;a,b[g].\] (1)

Note that the distribution need not be Gaussian for this definition. Intuitively, our graphical model DP requires that groups have evenly balanced connections across groups and do not behave significantly more or less similarly to certain groups. Crucially, Definition 1 accounts for both forms of bias showcased in Figure 1, _connectivity_ bias in the support of \(_{0}\) and _correlation_ bias in the signs of entries in \(_{0}\). Not only is this definition appropriately tailored to fairness for graphical models, but it

Figure 1: Three real-world networks with node groups denoted by color. Within-group edges are in blue and across-group edges in red, while edge widths correspond to edge weight magnitudes. For each network, we present (“M”) the modularity of the graphs with respect to group membership , (“W”) the ratio of positive to negative estimated partial correlations for within-group edges, and (“A”) an analogous ratio for across-group edges. Networks in (a) and (c) show high group-wise modularity, while (b) and (c) show significant preferences for positive correlations in the same group.

also provides flexibility in how we address biases. We may adjust similarities in behavior without changing the topology of the graph [14; 29], but conversely, we may alter connections if correlations cannot be changed . Finally, note that works for fairness on graphs with weighted or signed edges are rare , and to the best of our knowledge no previous work has specified fairness for graphical models that encode conditional dependencies.

We consider a graphical model unfair when there is a gap in DP, that is, when (1) does not hold. In practice, we measure biases in GGMs by approximating the DP gap. For this purpose, we propose the following bias metric inspired by ,

\[H()\ :=\ -g}_{a=1}^{g}_{b a}(_{a}^{}_{}_{a}}{p_{a}^{2}-p_{a}}- _{a}^{}_{}_{b}}{p_{a}p_{b} })^{2}.\] (2)

Each term in (2) compares the average within-group edge weight and the average across-group edge weight for every distinct group pair. Thus, \(H()\) will increase if variables belonging to two groups exhibit either significantly denser or sparser connections or significantly stronger or weaker correlations. As we aim to balance statistical behavior across groups, we consider obtaining precision matrices \(\) that balance data fidelity with small values of \(H()\). The main difference between \(H()\) and the related metric in  lies in the use of squared summands. While subtle, this modification tends to yield fairer outcomes, as group pairs are balanced overall as opposed to the metric in , which may favor balancing some pairs of groups over others.

In addition to (2), we also propose a stronger alternative metric for node-wise fairness across groups,

\[H_{}()\ :=\ _{i=1}^{p}_{a=1}^{g} (_{b a}_{}_{ a}]_{i}}{p_{a}}-_{}_{b}]_{i}}{p_{b}} )^{2},\] (3)

which is zero if and only if every variable is completely balanced across groups in terms of connections or correlation. This stronger metric is inspired by [15; 18], also modified by squaring summands as for \(H()\). As an alternative interpretation, observe that \(H_{}()\) increases when the correlation between the group of a variable \(i\) and the \(i\)-th column of \(\) increases. This node-wise penalty is stronger than \(H()\), as we require that not only pairs of groups exhibit no preference in statistical similarities but also each node must show no preference for connecting to certain groups.

For graph-based works, the predominant choice of bias metric is DP (see related works in Appendix A). Thus, we approach the nascent task of graphical model estimation with a familiar bias metric to verify our approach with established measurements. However, our formulation is suited to others such as equalized odds (EO), defined in Appendix I. While both DP and EO are popular fairness definitions, we cannot compute EO for the true precision matrix since it is conditioned on the ground truth connections. For this reason, we emphasize DP for group fairness since a measure of bias in the true precision matrix is critical to our theoretical interpretation of the fairness-accuracy tradeoff.

## 3 Fair GLASSO

### Graphical Lasso for Fair GGMs

We apply our proposed metrics in (2) and (3) to estimate GGMs from observations while mitigating both connectivity and correlation biases (see Section 2). Assume that we observe \(n\) samples from the distribution \((,_{0})\) collected in the data matrix \(^{n p}\). To estimate fair and sparse precision matrices from data, we adapt the celebrated graphical lasso method [26; 31; 32], a penalized maximum likelihood approach for recovering GGMs.

Given the sample covariance matrix \(}=^{}\), we present _Fair GLASSO_, a version of graphical lasso for fair GGMs,

\[^{*}\ =\ }{} \ (})-(+ )+_{1}\|_{}\|_{1}+_{2} R_{H}()\] \[ :=\{^{p p }: 0,\ \|\|_{2}^{2}\},\] (4)

where \(R_{H}\) denotes a bias penalty measuring the fairness of \(\) and \(_{1},_{2} 0\) tune the encouragement of sparse and fair precision matrices, respectively. For the penalty \(R_{H}\), we can choose not only our proposed metrics \(H\) and \(H_{ node}\) but also any metric for measuring bias on graphs. Similar to existing works on graph Laplacian GGMs , the addition of \(\) for \(>0\) adds practicality to our approach, permitting us to obtain positive semi-definite precision matrices. The ability to estimate rank-deficient matrices allows for disconnected graph solutions. We assume that the true precision matrix \(_{0}\) has bounded eigenvalues (see AS2 and AS3 in Section 3.2), hence the constraint \(\|\|_{2}^{2}\) on the spectral norm of \(\) for large enough \(>0\). In practice, an effective \(\) can be obtained by overshooting its value based on the minimum eigenvalue of the sample covariance \(}\). For further context of how both our proposed bias metrics \(H\) and \(H_{ node}\) and our Fair GLASSO method relate to existing works, we provide a detailed review of related works in Appendix A. We present our approach for estimating GGMs, but indeed we may consider other distributions for the problem formulation in (4), such as the Ising negative log-likelihood. As our theoretical analysis requires Gaussianity, we proceed under this assumption, but future work will see the application of fair regularization to other distributions. Moreover, our empirical results in Section 4 show satisfactory performance optimizing (4) even for real-world datasets with non-Gaussian data.

### Fair GLASSO Theoretical Analysis

We theoretically characterize the performance of Fair GLASSO. In particular, we focus on the effect of our fairness penalty in (4). Our result demonstrates the error rate of \(^{*}\) not only from a traditional statistical perspective but also in terms of the bias in the true precision matrix \(_{0}\). Indeed, as \(_{0}\) becomes more unfair, we expect that imposing unbiased estimates hinders estimation performance. Let the set \(:=\{(i,j)[p]^{2}\::\:[_{0}]_{ij} 0,\:i j\}\) contain the indices of the non-zero, off-diagonal entries of \(_{0}\). We first share the following assumptions on \(_{0}\) and \(\).

**AS1**: _(Bounded sparsity) There exists a constant_ \(s>0\) _such that the cardinality of_ \(\) _satisfies_ \(|| s\)_._
**AS2**: _(Bounded spectrum) There exists a constant_ \(>0\) _such that_ \(_{}(_{0})>0\)_._
**AS3**: _(Bounded spectrum) There exists a constant_ \(>0\) _such that_ \(_{}(_{0})<.\)__
**AS4**: _(Persistent groups) All groups have the same size, that is,_ \(p_{a}= 2\) _for every_ \(a[g]\)_._

Assumptions AS1, AS2, and AS3 follow those from the distinguished work . Note that AS4 is imposed for simplicity, but similar results hold if we merely require asymptotically similar groups sizes, where no groups vanish as \(p\). With our assumptions in place, we present our main result on the error rate of Fair GLASSO, the proof of which can be found in Appendix B.

**Theorem 1**: _Assume that AS1 to AS4 hold and that \(_{1}\) and \(_{2}=o(1)\). Moreover, let \(R_{H}=H\) from (2) and \(=0\) in (4). With probability tending to 1 as \(n,p\), there exist constants \(m_{1},m_{2}>0\) such that_

\[\|^{*}-_{0}\|_{F} m_{1}}+m_{2}_{0})}}{ }.\] (5)

_Moreover, there exists a constant \(q>0\) such that if \(_{2}\) satisfies_

\[_{2}^{2} p}{g^{2}n_{0})}},\] (6)

_then with probability tending to 1 as \(p\) we can further guarantee that_

\[\|^{*}-_{0}\|_{F} m_{1}}.\] (7)

Our error bound consists of the Frobenius norm convergence rate for graphical lasso in  and a term accounting for the bias penalty in (4). In particular, the second term in (5) portrays the influence of bias in the true precision matrix \(_{0}\). Theorem 1 not only provides an intuitive error bound for fair estimation of GGMs but also exemplifies when a tradeoff between fairness and accuracy may occur. When the true model \(_{0}\) is biased, that is, \(H(_{0})\) is large, then performance may suffer according to (5). However, if bias mitigation is mild enough, that is, if \(_{2}\) is small enough to satisfy (6), then we instead enjoy the error rate of  with no adverse effect from the bias penalty. Indeed, as the true \(_{0}\) becomes fairer, so too grows the range of values of \(_{2}\) that guarantee (7). Thus, if \(_{0}\) is unbiased, then imposing a strong bias penalty can obtain fair estimates \(^{*}\) while maintaining accuracy.

In addition to the explicit Frobenius error rate of \(^{*}\), we are also interested in when we can sufficiently describe the true model behavior. Our next result shows how well Fair GLASSO solutions approximate the true distribution, which enjoys the same rate as in Theorem 1, proven in Appendix C.

**Corollary 1**: _Let the assumptions of Theorem 1 hold. Then, with probability tending to 1 as \(n,p\), there exist constants \(m_{1}^{},m_{2}^{}>0\) such that_

\[\|^{*}_{0}-\|_{F} m_{1}^{} }+m_{2}^{}_{0})}}{}.\] (8)

_Moreover, there exists a constant \(q>0\) such that when \(_{2}\) satisfies (6), then with probability tending to 1 as \(p\) we can further guarantee that_

\[\|^{*}_{0}-\|_{F} m_{1}^{} }.\] (9)

Note that a similar rate to (9) holds up to a constant if we replace \(_{0}\) with \(}\). Thus, we may apply \(\|^{*}}-\|_{F}\) as an error metric when the true covariance matrix \(_{0}\) is unavailable.

### Algorithmic Implementation

``` Input: Sample covariance \(}\), weights \(_{1}\) and \(_{2}\), Lipschitz constant \(L\) of \(f\). Initialize \(}^{(0)}=}^{(1)},\;\;t^{(1)}=1, \;\;k=1\). whileStopping criteria not metdo  Proximal gradient descent: \(}^{(k)}=_{_{1}/L} }^{(k)}- f(}^{(k)})\). Projection step: \(}^{(k)}=_{}}^{( k)}\).  Adaptive step size update: \(t^{(k+1)}=1+)^{2}}\). Accelerated update: \(}^{(k+1)}=}^{(k)}+-1}{t^ {(k+1)}}}^{(k)}-}^{(k-1)} \). Update iteration: \(k=k+1\). end Output :Estimated precision matrix \(}=}^{(k)}\). ```

**Algorithm 1**Fair GLASSO from Gaussian observations.

If we choose \(R_{H}\) in (4) as \(H\) or \(H_{}\), the convexity of the resultant problem allows us to introduce a simple yet effective algorithm for Fair GLASSO estimates. We base our approach on an accelerated proximal gradient method known as fast iterative shrinkage algorithm (FISTA) , which is well suited to solving nonsmooth, constrained optimization problems. Moreover, our ensuing FISTA approach is still applicable under other distributions as long as the associated loss in (4) is convex and differentiable, such as the negative log-likelihood of the Ising model.

We separate the Fair GLASSO objective function \(F()=f()+h()\) into its smooth and non-smooth terms via \(f()\) and \(h()\), respectively, which are given by

\[f():=(})-( +)+_{2}R_{H}(),  h():=_{1}\|_{}}\|_{1}.\] (10)

The proposed algorithm to estimate \(^{*}\) is presented in Algorithm 1. We discuss the steps of our algorithm in Appendix D, and we provide further details in Appendix E, including specifying the gradient \(_{}f\) and the Lipschitz constant of \(f\) when \(R_{H}=H\) or \(R_{H}=H_{}\).

``` Input: Sample covariance \(}\), weights \(_{1}\) and \(_{2}\), Lipschitz constant \(L\) of \(f\). Initialize \(}^{(0)}=}^{(1)},\;\;t^{(1)}=1, \;\;k=1\). whileStopping criteria not metdo  Proximal gradient descent: \(}^{(k)}=_{_{1}/L}} ^{(k)}- f(}^{(k)})\). Projection step: \(}^{(k)}=_{}}^{( k)}\).  Adaptive step size update: \(t^{(k+1)}=1+)^{2}}\). Accelerated update: \(}^{(k+1)}=}^{(k)}+-1}{t^ {(k+1)}}}^{(k)}-}^{(k-1)} \). Update iteration: \(k=k+1\). end Output :Estimated precision matrix \(}=}^{(k)}\). ```

**Algorithm 2**Fair GLASSO with \(}^{(k)}\).

Computationally, the complexity of Algorithm 1 is limited by an eigendecomposition in the projection step and a matrix inverse in the proximal gradient descent step (see (50) and (51) in Appendix D for details). Over-the-shelf implementations of these operations render a computational complexity of \((p^{3})\). However, implementations based on fast matrix multiplication may result in an improved complexity of \((p^{2.4})\), a remarkable improvement since the optimization problem involves learning \(p^{2}\) variables. Finally, in addition to a mild computational complexity, the proposed algorithm enjoys a convergence rate of \((})\), which we formally state next and prove in Appendix F.

**Theorem 2**: _Let \(\{}^{(k)}\}_{k 1}\) be the sequence generated by Algorithm 1 for solving the optimization problem (4), where we denote the global minimum by \(^{s}\). Then, for any \(k 1\),_

\[\|}^{(k)}-^{*}\|_{F}^{2}^{(0)}-^{*}\|_{F}^{2}}{(k+1)^{2}},\] (11)

_where \(L\) is the Lipschitz constant of \(f()\) and \(\) corresponds to the spectral constraint in (4)._

Thus, Theorem 2 guarantees convergence of Algorithm 1 to the optimal solution \(^{*}\) under our constraints in (4) with either of our bias metrics in (2) or (3). Not only are the convex fairness penalties amenable to efficient algorithms with well-understood performance guarantees, we also are able to guarantee that our algorithm converges with respect to the estimation variable, which is stronger than previous works' results on convergence of the objective function .

## 4 Experiments

We illustrate the ability of Fair GLASSO to reliably estimate both synthetic and real-world graphs from data while promoting unbiased connections. Extensive experimental details including our performance metrics, the baselines with which we compare, and the real-world datasets are provided in Appendix G; these details are summarized here. We include additional experiments on the effect of varying the hyperparameters \(_{1}\) and \(_{2}\) and violating assumptions (AS1)-(AS4) of Theorem 1 on Appendix H.

We compare our method with existing approaches for both scalability and performance. In particular, we consider (i) **GL**: Traditional graphical lasso , (ii) **FGL**: Fair GLASSO with \(R_{H}=H\), (iii) **NFGL**: Fair GLASSO with \(R_{H}=H_{}\), (iv) **FST**: Network inference from spectral templates with a group-wise bias penalty [18; 36], (v) **NFST**: Network inference from spectral templates with a node-wise bias penalty [15; 18], and (vi) **RWGL**: Graphical lasso with randomly rewired edges.

We then perform GGM estimation on multiple real-world networks: (i) **Karate club**: the social network of Zachary's karate club members , (ii) **School**: A contact network of high school students , (iii) **Friendship**: The friendship network of the same high school students as in School , (iv) **Co-authorship**: An author collaboration network , and (v) **MovieLens**: A movie recommender network . Figure 1 demonstrated that interconnected data may have fair or unfair relationships; thus, our experiment not only exemplifies the viability of our approach for real-world settings but also the fairness-accuracy tradeoff depending on biases in data.

### Estimating Fair Graphs with Biased Data

Consider the realistic setting where our model is to be implemented in a fair setting, but our observations contain unfair biases . We aim to obtain accurate graphical models by reducing the biases encoded in data. We consider synthetic networks whose nodes show no preferential connections, but our observations become increasingly unfair, growing in preference for within-group correlations.

Figure 1(a) presents the error and bias from networks estimated using graphical lasso with and without bias penalties \(H\) and \(H_{}\). As expected, all methods show an increase in both error and bias as the

Figure 2: Estimation performance in terms of error and bias. (a) Bias and error for estimating a fair graph as data becomes more biased. (b) Bias and error as graph size \(p\) grows for ER graphs. (c) Bias and error for a biased real-world network  as the number of observations \(n\) grows.

data becomes more unfair, as our observations are not only straying from the true distribution but also tending toward unfair behavior. However, **FGL** and **NFGL** not only preserve a lower bias than **GL**, but we also improve estimation performance. This significant result exemplifies the situation described in Section 3.2; our proposed penalties not only yield unbiased estimates but also serve as informative priors when the underlying graph is fair. Thus, we enjoy improvement in both fairness and accuracy for this realistic setting.

### Performance as Graph Size Increases

Fair GLASSO adapts traditional GGM learning through the bias penalty, which includes (2) and (3). To observe the regularization effect of our penalties, we compare graphical lasso both with and without bias penalties for estimating synthetic networks as the graph size \(p\) grows, which also demonstrates the scalability of our method. We thus implement **GL** via a state-of-the-art approach for comparison . Figure 2b shows the relationship between error and bias in the estimated graphs. Each line corresponds to a graph estimation method, and points on the lines denote the varying dimension, ranging from \(p=50\) (highlighted via darker, filled markers) to \(p=1000\).

First, observe that **GL** achieves superior accuracy at the expense of a larger bias, while **NFGL** improves bias, albeit with greater error. In contrast, **FGL** for \(_{2}\{1,10\}\) can improve bias without sacrificing accuracy, where \(_{2}=10\) yields the most Pareto-optimal solution. This result aligns with Theorem 1, showcasing the ability of Fair GLASSO to maintain estimation performance while significantly improving the fairness of the obtained graph. Critically, even as \(p\) increases, our method enjoys relatively short running times, ranging from 0.5 seconds for 50 nodes to 30 minutes for 1000, which we show in Table 1. Our implementation of the classical algorithm in  requires 2 seconds and 170 minutes for \(p=50\) and \(p=1000\), respectively. We can then conclude that our efficient algorithm for Fair GLASSO can sufficiently handle larger graphs.

### Social Network with Synthetic Signals

We next apply Fair GLASSO for the **Karate club** network, a real-world graph with known biased connections . As this network famously exhibits group-wise modularity , we can compare different methods for estimating a real biased network. We show bias and error as the number of data samples increases from \(n=10^{2}\) (denoted by darker, filled markers) to \(n=10^{5}\) in Figure 2c. Since this graph does not have data, we generate synthetic Gaussian observations on the social network. Note that we only consider synthetic samples for this real-world dataset; the remainder are equipped with a set of real graph signals. In addition to the previously considered baselines, we also compare to **FGL-L1** and **NFGL-L1**, which correspond to Fair GLASSO using the group-wise and node-wise bias metrics in  as the penalty \(R_{H}\). These metrics may prioritize balancing some group pairs over others, as described in Section 2.1.

For all methods, increasing the number of samples improves estimation error, but bias also grows since the underlying graph is unfair. Observe that all alternatives to **GL** are able to reduce estimation bias. As expected, randomly rewiring edges from graphical lasso estimates in **RWGL** does mildly improve bias when compared to **GL**, but error also rises significantly. The methods designed for fair graph estimation achieve the greatest improvement in bias, with our proposed methods **FGL** and **NFGL** outperforming **FST** and **NFST** in both bias and error. We also observe that **FGL** and **NFGL** using our bias metrics \(H\) and \(H_{}\) with squared terms improve both fairness and accuracy over **FGL-L1** and **NFGL-L1** using the analogous metrics in [15; 18], which consider sums of absolute values of each term. Moreover, not only does **FGL** outperform other methods in both fairness and accuracy for all \(n\), but **FGL** is the only approach that simultaneously decreases bias and error. Fair GLASSO is therefore viable for estimating real-world networks with known biased connections.

We also investigate the effect of \(H\) versus \(H_{}\) for estimating group-wise modular networks. Figure 3 visualizes three graphs learned using **GL**, **FGL**, and **NFGL**. Both **FGL** and **NFGL** attempt to mitigate biased connections by reducing larger weights for existing within-group edges. However, since **FGL** aims to improve bias in expectation, Figure 3b shows an increase in negative within-group

   Nodes \(p\) & **GL** & **FGL-0** & **FGL-1** \\ 
50 & 2.06 & 0.55 & 0.64 \\
200 & 18.80 & 8.14 & 8.87 \\
1000 & 10225.74 & 1900.89 & 1893.57 \\   

Table 1: Running time in seconds of Algorithm 1 and graphical lasso via .

edges, that is, negative partial correlations between nodes in the same group. Conversely, Figure 2(c) shows a network with more balanced connections _per node_, where more nodes are connected to new edges that are both positive and negative. This result suggests that the bias metric \(H\) for group-wise balance in expectation aligns more with existing definitions of DP, while the node-wise DP gap \(H_{}\) behaves closer to an individual fairness metric .

### Fair GGMs for Real-World Data

Finally, in Table 2 we evaluate Fair GLASSO to estimate graphs from four real-world datasets consisting of two social networks, **School** and **Friendship** with _gender_ as the sensitive attribute; a collaboration network **Co-authorship** with groups representing the _type of conference_ in which each author publishes most; and a recommendation network **MovieLens**, where we consider binary sensitive attributes for each movie (node) denoting whether or not the movie was _released after 1991_. Evaluating Fair GLASSO on these datasets not only demonstrates its effectiveness on relevant real-world scenarios with biases, which are described in greater detail in Appendix G, but we can also showcase performance on non-Gaussian data, such as the discrete graph signals of the **School** and **Friendship** social networks. As each network varies in level of biases in their connections and observations, we show results for both weak and strong bias mitigation, that is, \(_{2}\{1,10^{6}\}\), for all fair graph learning methods.

For the relatively unbiased **School** and **Friendship** networks, our methods **FGL** and **NFGL** obtain superior estimation accuracy while sufficiently accounting for biases, particularly in comparison with **FST** and **NFST**. Unsurprisingly, we observe the lowest estimation error when \(_{2}=1\) is small enough such that **FGL** and **NFGL** achieve similar bias to **Ground truth**. However, observe that **FGL** and **NFGL** have low estimation error even for large \(_{2}=10^{6}\), which enjoys significant bias reduction. This verifies the results from Theorem 1 and in Figure 1(a) for real-world data; when the underlying graph is fair, our bias penalties serve as informative structural priors that improve performance.

For the **Co-authorship** network, we also observe the best accuracy using **FGL** and **NFGL** when \(_{2}\) is small enough that bias is similar to that of the true network. Critically, even when \(_{2}\) is large, we observe errors for **FGL** and **NFGL** competitive with **GL** while also achieving low bias. Moreover, for the **MovieLens** dataset, Fair GLASSO is the only method that rivals **GL** in accuracy while acquiring significantly fairer estimates. Indeed, **FGL** with \(_{2}=10^{6}\) is the only method to achieve both low error and bias simultaneously. This implies that the observations in both the **Co-authorship** and **MovieLens** datasets are biased, since high bias mitigation yielding fair estimates improves estimation performance. Thus, we demonstrate that relationships in real data can be explained by graphical models rivaling the accuracy of state-of-the-art approaches while also exhibiting fairer behavior.

Figure 3: Estimated **Karate club** network via graphical lasso with and without penalties \(H\) and \(H_{}\). Node colors denote group membership, while edge thickness denotes edge weight magnitude and edge color its sign, with blue (red) as positive (negative) correlation. (a) Estimation via **GL**. (b) Estimation via **FGL**. (c) Estimation via **NFGL**.

## 5 Conclusion

This work proposes two metrics to evaluate bias in graphical models, which we apply as regularizers for fair GGM estimation. In particular, we adapt DP to measure biases in the conditional dependence structure encoded in graphical models, where nodes may show preferences for certain groups in terms of either connections or correlations. Unlike existing works that typically only consider fairness based on the unweighted topology of a known graph, we extend the concept of graph DP for the weighted connectivity patterns represented by the precision matrix of a Gaussian distribution. Moreover, we apply our group fairness for graphical models to modify graphical lasso for estimating fair GGMs. Future work will see more general graphical models, along with other extensions both in terms of the graph setting and fairness, which we discuss further in Appendix I.

## Broader Impact

In this work, we proposed a fair adaptation of graphical lasso, an extremely prominent method for complex data analysis. The development of methods that encourage fairness is necessary to ensure ethical and trustworthy tools, particularly those applied as extensively as GGMs to several critical and sensitive applications. Biases present in real-world graphs are well known, such as biased connections due to gender in social network analysis or segregated communities of co-authors in different disciplines. We revealed that these graphical biases extend beyond preferences in connections to include within-group correlations in behavior. Indeed, while intuitive, the tendency for group members to behave similarly has not been investigated for graphical models, as bias in signed edges has not been considered. Our paper contributes to expanding available unbiased graph-based methods, leading to extensions of other graphical models and statistical tools.

Moreover, as fairness on graphs is still nascent, several graph-based tasks have yet to be considered under the lens of fairness. Indeed, models are typically encouraged to be unbiased with respect to independent entities, but recent years have seen greater attention paid to the treatment of data equipped with graphical relationships. We not only participate in this movement, but we also extend fairness on graphs by considering weighted and signed edges for graphical models encoding conditional dependencies. This paper serves as a critical step in developing fair graph-based tools, particularly as GGMs are used in several high-stakes fields, including finance and medicine.

  &  &  &  &  \\  & Error & Bias & Error & Bias & Error & Bias & Error & Bias \\ 
**Ground truth** & \(-\) & 0.2030 & \(-\) & 14.052 & \(-\) & 0.6791 & \(-\) & 0.1487 \\ 
**GL** & 0.2661 & 0.3111 & 0.1995 & 12.6102 & **0.0223** & 0.9529 & 0.6477 & 0.4068 \\
**RWGL-150** & 0.3497 & 0.3943 & 0.2308 & 12.5836 & 2.1919 & 0.9409 & 0.6509 & 0.4068 \\
**RWGL-300** & 0.3775 & 0.5110 & 0.2978 & 12.5735 & 2.2002 & 0.9184 & 0.6633 & 0.3861 \\ 
**FST** (\(_{2}=1\)) & 0.4383 & 0.8942 & 0.4188 & 0.5754 & 0.1724 & 0.4568 & 1.0606 & 0.3787 \\
**NFST** (\(_{2}=10^{6}\)) & 0.4386 & 0.8924 & 0.4068 & 0.6687 & 0.1724 & 4.0568 & 1.1149 & 0.3734 \\
**FST** (\(_{2}=10^{6}\)) & 1.7820 & 1.0767 & 1.0801 & 129.5384 & 0.1724 & 4.0568 & 1.1924 & **0.0052** \\
**NFST** (\(_{2}=10^{6}\)) & 1.6131 & 3.1971 & 1.0500 & 17.6285 & 0.1724 & 4.0568 & 1.1852 & 0.0081 \\ 
**FGL** (\(_{2}=1\)) & **0.1417** & 0.4824 & **0.1896** & 10.3317 & 0.0253 & 0.8177 & **0.0505** & 0.1657 \\
**NFGL** (\(_{2}=1\)) & **0.1417** & 0.4824 & **0.1895** & 11.8391 & 0.0253 & 0.8177 & **0.0505** & 0.1657 \\
**FGL** (\(_{2}=10^{6}\)) & 0.1449 & **0.0308** & 0.2432 & **0.1899** & 0.0248 & **0.6106** & 0.0516 & 0.0153 \\
**NFGL** (\(_{2}=10^{6}\)) & 0.2981 & 0.0827 & 0.2708 & 0.7908 & 0.0239 & 0.7104 & 0.0873 & 0.0243 \\ 

Table 2: Bias and error for estimating four real-world networks. The top row shows the bias present in the true underlying network. The best performances are in **bold**.