# Posterior Sampling for Competitive RL: Function Approximation and Partial Observation

Shuang Qiu

HKUST

masqiu@ust.hk

&Ziyu Dai

New York University

zd2365@cims.nyu.edu

&Han Zhong

Peking University

hanzhong@stu.pku.edu.cn

&Zhaoran Wang

Northwestern University

zhaoranwang@gmail.com

&Zhuoran Yang

Yale University

zhuoran.yang@yale.edu

&Tong Zhang

HKUST

tongzhang@ust.hk

Equal Contribution.

###### Abstract

This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes \(T\). To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.

## 1 Introduction

Multi-agent reinforcement learning (MARL) tackles sequential decision-making problems where multiple players simultaneously interact with the shared environment, affecting each other's behavior in a coupled manner. Under a competitive reinforcement learning (RL) setting, the goal of each player is to maximize (_resp._ minimize) her own cumulative gains (_resp._ losses) in the presence of other agents. Recent years have been tremendous practical successes of MARL in a variety of application domains, such as autonomous driving , Go , StarCraft , Dota2  and Poker . These successes are attributed to advanced MARL algorithms that can coordinate multiple players by exploiting potentially partial observations of the latent states and employ powerful function approximators (neural networks in particular), which empower us to tackle practical problems with large state spaces.

Apart from the empirical success, there is a growing body of literature on establishing theoretical guarantees for Markov games (MGs)  - a standard framework for describing the dynamics of competitive RL. In particular,  extend the works in single-agent reinforcementlearning (RL) with function approximation [29; 60; 67; 34; 5; 11; 19; 31; 21] by developing sample-efficient algorithms that are capable to solve two-player zero-sum MGs with function approximation. In addition, as opposed to the aforementioned literature on MGs assuming the state of players is fully observable, the recent work  analyze Markov games under partial observability , i.e., the complete information about underlying states is lacking. However, most of the existing works are built upon the principle of "optimism in the face of uncertainty" (OFU)  for exploration. Furthermore, from a practical perspective, achieving optimism often requires explicit construction of bonus functions, which are often designed in a model-specific fashion and computationally challenging to implement.

Another promising strand of exploration techniques is based on posterior sampling, which is shown by previous works on bandits  and RL  to perform better than OFU-based algorithms. Meanwhile, posterior sampling methods, unlike OFU-based algorithms [31; 21] that need to solve complex optimization problems to achieve optimism, can be efficiently implemented by ensemble approximations [48; 44; 17; 46] and stochastic gradient Langevin dynamics (SGLD) . Despite the superiority of posterior sampling, its theoretical understanding in MARL remains limited. The only exception is , which proposes a model-free posterior sampling algorithm for zero-sum MGs with general function approximation. However,  cannot capture some common tractable competitive RL models with a model-based nature, such as linear mixture MGs  and low witness rank MGs . Moreover, their result is restricted to the fully observable MGs without handling the partial observability of the players' states. Therefore, we raise the following question:

_Can we design provably sample-efficient posterior sampling algorithms for competitive RL with even partial observations under general function approximation?_

Concretely, the above question poses three major challenges. First, despite the success of the OFU principle in partially observable Markov games (POMGs), it remains elusive how to incorporate the partial observations into the posterior sampling framework under a MARL setting with provably efficient exploration. Second, it is also unclear whether there is a generic function approximation condition that can cover more known classes in both full and partial observable MARL and is meanwhile compatible with the posterior sampling framework. Third, with the partial observation and function approximation, it is challenging to explore how we can solve MGs under the setups of self-play, where all players can be coordinated together, and adversarial learning, where the opponents' policies are adversarial and uncontrollable by the learner. Our work takes an initial step towards tackling such challenges by concentrating on the typical competitive RL scenario, the two-player zero-sum MG, and proposing statistically efficient posterior sampling algorithms under function approximation that can solve both self-play and adversarial MGs with full and partial observations.

**Contributions.** Our contributions are four-fold: **(1)** We first propose the two generalized eluder coefficient (GEC) as the complexity measure for the competitive RL with function approximation, namely self-play GEC and adversarial GEC, that captures the exploration-exploitation tradeoff in many existing MGs, including linear MGs, linear mixture MGs, weakly revealing POMGs, decodable POMGs. The proposed measures also generalize the recently developed GEC condition  from single-agent RL to MARL, suitably adjusting the exploration policy particularly for the adversarial setting. **(2)** Incorporating the proposed self-play GEC for general function approximation, we propose a model-based posterior sampling algorithm with self-play to learn the Nash equilibrium (NE), which successfully handles the partial observability of states along with a full observable setup by carefully designed likelihood functions. **(3)** We identify POMG models aligned with the form of the adversarial GEC, which fit MG learning with adversarially-varying policies of the opponent. We further propose a model-based posterior sampling algorithm for adversarial learning with general function approximation. **(4)** We prove regret bounds for our proposed algorithms that scale sublinearly with the number of episodes \(T\), the corresponding GEC \(d_{}\), and a quantity measuring the coverage of the optimal model by the initial model sampling distribution. To the best of our knowledge, we present the first model-based posterior sampling approaches to sample-efficiently learn MGs with function approximation, handling partial observability in both self-play and adversarial settings.

**Related Works.** There is a large body of literature studying MGs, especially zero-sum MGs. In the self-play setting, many papers have focused on solving approximate NE in tabular zero-sum MGs [6; 7; 71; 51; 43], zero-sum MGs with linear function approximation [71; 16], zero-sum MGs with low-rank structures [50; 79; 47], and zero-sum MGs with general function approximation [33; 28; 73]. On the other hand, there are also several recent papers focusing on the adversarial setting [71; 61; 33; 28] aim to learn the Nash value under the setting of unrevealed opponent's policies, where the adversarial policies of the opponent are unobservable. In addition, another line of adversarial MGs concentrates on a revealed policy setting, where the opponent's full policy can be observed, leading to efficiently learning a sublinear regret comparing against the best policy in hindsight. Particularly,  and  develop efficient algorithms in tabular and function approximation settings, respectively. Our approach focuses on the unrevealed policy setting, which is considered to be a more practical setup. There are also works studying MGs from various aspects [59; 32; 45; 84; 35; 20; 52; 9; 82; 18; 72; 74], such as multi-player general-sum MGs, reward-free MGs, MGs with delayed feedback, and offline MGs, which are beyond the scope of our work. Most of the aforementioned works follow the OFU principle and differ from our posterior sampling methods. The recent work  proposes a model-free posterior sampling algorithm for two-player zero-sum MGs but is limited to the self-play setting with fully observable states. Moreover, their work requires a strong Bellman completeness assumption that is restrictive compared to only requiring realizability in our work, mainly due to the monotonicity, meaning that adding a new function to the function class may violate it. Many model-based models like linear mixture MGs  and low witness rank MGs  are not Bellman-complete, so they cannot be captured by . Without the completeness assumption, our model-based posterior sampling approaches can solve a rich class of tractable MGs, including linear mixture MGs, low witness rank MGs , and even POMGs, tackling both self-play and adversarial learning settings.

Our work is related to a line of research on posterior sampling methods in RL. For single-agent RL, most existing works such as  analyze the Bayesian regret bound. There are also some works [4; 53; 76] focusing on the frequentist (worst-case) regret bound. Our work is more closely related to the recently developed feel-good Thompson sampling technique proposed by  for the frequentist regret bound, and its extension to single-agent RL [19; 2; 3] and two-player zero-sum MGs .

Our work is also closely related to the line of research on function approximation in RL. Such a line of works proposes algorithms for efficient policy learning under diverse function approximation classes, spanning from linear Markov decision processes (MDPs) , linear mixture MDPs [5; 85] to nonlinear and general function classes, including, for instance, generalized linear MDPs , kernel and neural function classes , bounded eluder dimension [49; 68], Bellman rank , witness rank , bellman eluder dimension , bilinear , decision-estimation coefficient [24; 14], decoupling coefficient , admissible Bellman characterization , and GEC  classes.

The research on partial observability in RL  is closely related to our work. The works [36; 30] show that learning history-dependent policies generally can cause an exponential sample complexity. Thus, many recent works focus on analyzing tractable subclasses of partially observable Markov decision processes (POMDPs), which includes weakly revealing POMDPs [30; 39], observable POMDPs [26; 25], decodable POMDPs [22; 23], low-rank POMDPs, regular PSR , PO-bilinear class , latent MDP with sufficient tests , B-stable PSR , well-conditioned PSR , POMDPs with deterministic transition kernels [30; 62], and GEC . Nevertheless, in contrast to our work which focuses on the two-player competitive setting with partial observation, these papers merely consider the single-agent setting. The recent research  further generalizes weakly revealing POMDPs to its multi-agent counterpart, weakly revealing POMGs, in a general-sum multi-player setting based on the OFU principle. But when specialized to the two-player case, our work proposes a general function class that can subsume the class of weakly revealing POMGs as a special case. It would be intriguing to generalize our framework to the general-sum settings in the future.

**Notations.** We denote by \((P||Q)=_{x P}[(P(x)/Q(x))]\) the KL divergence and \(D^{2}_{}(P,Q)=1/2_{x P}(Q(x)/ P(x)}-1)^{2}\) the Hellinger distance. We denote by \(_{}\) the set of all distributions over \(\) and \(()\) the uniform distribution over \(\). We let \(x y\) be \(\{x,y\}\).

## 2 Problem Setup

We introduce the basic concept of the two-player zero-sum Markov game (MG), function approximation, and the new complexity conditions for function approximation. Concretely, we study two typical classes of MGs, i.e., fully observable MGs and partially observable MGs, as defined below.

**Fully Observable Markov Game.** We consider an episodic two-player zero-sum fully observable Markov game (FOMG2) specified by a tuple \((,,,,r,H)\), where \(\) is the state space, \(\) and \(\) are the action spaces of Players 1 and 2 respectively, \(H\) is the length of the episode. We denote by \(:=\{_{h}\}_{h=1}^{H}\) the transition kernel with \(_{h}(s^{}|s,a,b)\) specifying the probability (density) of transitioning from state \(s\) to state \(s^{}\) given Players 1 and 2's actions \(a\) and \(b\) at step \(h\). We denote the reward function as \(x=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}:\) being the reward received by players at step \(h\). We define \(=\{_{h}\}_{h=1}^{H}\) and \(=\{_{h}\}_{h=1}^{H}\) as _Markovian_ policies for Players 1 and 2, i.e., \(_{h}(a|s)\) and \(_{h}(b|s)\) are the probability of taking action \(a\) and \(b\) conditioned on the current state \(s\) at step \(h\). Without loss of generality, we assume the initial state \(s_{1}\) is fixed for each episode. We consider a realistic setting where the transition kernel \(\) is _unknown_ and thereby needs to be approximated using the collected data.

**Partially Observable Markov Game.** This paper further studies an episodic zero-sum partially observable Markov game (POMG), which is distinct from the FOMG setup in that the state \(s\) is not directly observable. In particular, a POMG is represented by a tuple \((,,,,,,, r,H)\), where \(\), \(\), \(\), \(H\), and \(\) are similarly the state and action spaces, the episode length, and the transition kernel. Here \(_{1}()\) denotes the initial state distribution. We denote by \(:=\{_{h}\}_{h=1}^{H}\) the emission kernel so that \(_{h}(o|s)\) is the probability of having a partial observation \(o\) at state \(s\) with \(\) being the observation space. Since we only have an observation \(o\) of a state, the reward function is defined as \(r:=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}(o,a,b)\) depending on actions \(a,b\) and the observation \(o\), and the policies for players are defined as \(=\{_{h}\}_{h=1}^{H}\) and \(=\{_{h}\}_{h=1}^{H}\), where \(_{h}(a_{h}|_{h-1},o_{h})\) and \(_{h}(b_{h}|_{h-1},o_{h})\) is viewed as the probability of taking actions \(a_{h}\) and \(b_{h}\) depending on all histories \((_{h-1},o_{h})\). Here we let \(_{h}:=(o_{1},a_{1},b_{1},o_{h},a_{h},b_{h})\). Then, in contrast to FOMGs, the policies in POMGs are _history-dependent_, defined on all prior observations and actions rather than the current state \(s\). We define \(_{h}^{,}(_{h}):=_{^{h}}_{1}(s_{1}) _{h^{}=1}^{h-1}[_{h^{}}(o_{h^{}}|s_{h^{}}) _{h^{}}(b_{h^{}}|_{h^{}-1},o_{h^{}})_{h^{ }}(b_{h^{}}|_{h^{}-1},o_{h^{}})_{h^{ }}(s_{h^{}+1}|s_{h^{}},a_{h^{}},b_{h^{}})] _{h}(o_{h}|s_{h})s_{1:h}\), which is the joint distribution of \(_{h}\) under the policy pair \((,)\). Removing policies in \(_{h}^{,}\), we define the function \(_{h}(_{h}):=_{^{h}}_{1}(s_{1})_{h^{ }=1}^{h-1}[_{h^{}}(o_{h^{}}|s_{h^{}})_{ h^{}}(s_{h^{}+1}|s_{h^{}},a_{h^{}},b_{h^{}})] _{h}(o_{h}|s_{h})s_{1:h}\). We assume that the parameters \(:=(_{1},,)\) are _unknown_ and thus \(_{h}\) is _unknown_ as well, which should be approximated in algorithms via online interactions.

**Online Interaction with the Environment.** In POMGs, at step \(h\) of episode \(t\) of the interaction, players take actions \(a_{h}^{t}_{h}^{t}(|_{h-1}^{t},o_{h}^{t})\) and \(b_{h}^{t}_{h}^{t}(|_{h-1}^{t},b_{h-1}^{t},o_{h}^{t})\) depending on their action and observation histories, receiving a reward \(r_{h}(o_{h}^{t},a_{h}^{t},b_{h}^{t})\) and transitions from the latent state \(s_{h}^{t}\) to \(s_{h+1}^{t}_{h}( s_{h}^{t},a_{h}^{t},b_{h}^{t})\) with an observation \(o_{h+1}^{t}_{h}(|s_{h}^{t})\) generated. When the underlying state \(s_{h}^{t}\) is observable and the policies become Markovian, we have actions \(a_{h}^{t}_{h}^{t}(s_{h}^{t})\) and \(b_{h}^{t}_{h}^{t}(s_{h}^{t})\) and the reward \(r_{h}(s_{h}^{t},a_{h}^{t},b_{h}^{t})\). Then, it reduces to the interaction process under the FOMG setting.

**Value Function, Best Response, and Nash Equilibrium.** To characterize the learning objective and the performance of the algorithms, we define the value function as the expected cumulative rewards under the policy pair \((,)\) starting from the initial step \(h=1\). For FOMGs, we define the value function as \(V^{,}:=[_{h=1}^{H}r_{h}(s_{h},a_{h},b_{h}) s_{1},, ,]\), where the expectation is taken over all the randomness induced by \(\), \(\), and \(\). For POMG, we define the value function as \(V^{,}:=[_{h=1}^{H}r_{h}(o_{h},a_{h},b_{h}),,]\), with the expectation taken for \(\), \(\), and \(\).

Our work studies the competitive setting of RL, where Player 1 (_max-player_) aims to maximize the value function \(V^{,}\) while Player 2 (_min-player_) aims to minimize it. With the defined value function, given a policy pair \((,)\), we define their _best responses_ respectively as \(()_{}V^{,}\) and \(()_{}V^{,}\). Then, we say a policy pair \((^{*},^{*})\) is a _Nash equilibrium_ (NE) if

\[V^{^{*},^{*}}=_{}_{}V^{,}=_{}_{}V^{, }.\]

Thus, it always holds that \(^{*}=(^{*})\) and \(^{*}=(^{*})\). For abbreviation, we denote \(V^{*}=V^{^{*},^{*}}\), \(V^{,*}=_{}V^{,}\), and \(V^{*,}=_{}V^{,}\), which implies \(V^{*}=V^{^{*},*}=V^{*,^{*}}\) for NE \((^{*},^{*})\). Moreover, we define the policy pair \((,)\) as an \(\)-approximate NE if it satisfies \(V^{*,}-V^{,*}\).

**Function Approximation.** Since the environment is unknown to players, the model-based RL setting requires us to learn the true model of the environment, \(f^{*}\), via (general) function approximation. We use the functions \(f\) lying in a general model function class \(\) to approximate the environment. We make a standard realizability assumption on the relationship between the model class and the true model.

**Assumption 1** (Realizability).: _For a model class \(\), the true model \(f^{*}\) satisfies \(f^{*}\)._In our work, the true model \(f^{*}\) represents the transition kernel \(\) for the FOMG and \(\) for the POMG. For any \(f\), we let \(_{f}\) and \(_{f}=(_{f},_{f},_{f})\) be the models under the approximation function \(f\) and \(V_{f}^{,}\) the value function associated with \(f\). For POMGs, we denote \(_{f,h}^{,}\) and \(_{f,h}\) as \(_{h}^{,}\) and \(_{h}\) under the model \(f\).

**MGs with Self-Play and Adversarial Learning.** Our work investigates two important MG setups for competitive RL, which are the self-play setting and the adversarial setting. In the self-play setting, the learner can control _both_ players together to execute the proposed algorithms to learn an approximate NE. Therefore, our objective is to design sample-efficient algorithms to generate a sequence of policy pairs \(\{(^{t},^{t})\}_{t=1}^{T}\) in \(T\) episodes such that the following regret can be minimized,

\[^{}(T):=_{t=1}^{T}V_{f^{*}}^{*,^{t}}-V_{f ^{*}}^{^{t},*}.\]

In the adversarial setting, we can no longer coordinate both players, and only _single_ player is controllable. Under such a circumstance, the opponent plays arbitrary and even adversarial policies. Wlog, suppose that the main player is the max-player with the policies \(\{^{t}\}_{t=1}^{T}\) generated by a carefully designed algorithm and the opponent is min-player with arbitrary policies \(\{^{t}\}_{t=1}^{T}\). The objective of the algorithm is to learn policies \(\{^{t}\}_{t=1}^{T}\) to maximize the overall cumulative rewards in the presence of an adversary. To measure the performance of algorithms, we define the following regret for the adversarial setting by comparing the learned value against the Nash value, i.e.,

\[^{}(T)=_{t=1}^{T}V_{f^{*}}^{*}-V_{f^{*}}^{ ^{t},^{t}}.\]

## 3 Model-Based Posterior Sampling for the Self-Play Setting

We propose algorithms aiming to generate a sequence of policy pairs \(\{(^{t},^{t})\}_{t=1}^{T}\) by controlling the learning process of both players such that the regret \(^{}(T)\) is small. Such a regret can be decomposed into two parts, namely \(_{t=1}^{T}[V_{f^{*}}^{*}-V_{f^{*}}^{^{t},*}]\) and \(_{t=1}^{T}[V_{f^{*}}^{*,^{t}}-V_{f^{*}}^{*}]\), which inspires our to design algorithms for learning \(\{^{t}\}_{t=1}^{T}\) and \(\{^{t}\}_{t=1}^{T}\) separately by targeting at minimizing these two parts respectively. Due to the symmetric structure of such a game learning problem, we propose the algorithm to learn \(\{^{t}\}_{t=1}^{T}\) as summarized in Algorithm 1. The algorithm for learning \(\{^{t}\}_{t=1}^{T}\) can be proposed in a symmetric way in Algorithm 3, which is deferred to Appendix A. Our proposed algorithm features an integration of the model-based posterior sampling and the exploiter-guided self-play in a multi-agent learning scenario. In Algorithm 1, Player 1 is the main player, while Player 2 is called the exploiter, who assists the learning of the main player by exploiting her weakness.

**Posterior Sampling for the Main Player.** The posterior sampling constructs a posterior distribution \(p^{t}(|Z^{t-1})\) over the function class \(\) each round based on collected data and a pre-specified prior distribution \(p^{0}()\), where \(Z^{t-1}\) denotes the random history up to the end of the \((t-1)\)-th episode. For ease of notation, hereafter, we omit \(Z^{t-1}\) in the posterior distribution. Most recent literature shows that adding an optimism term in the posterior distribution can lead to sample-efficient RL algorithms. Thereby, we define the distribution \(p^{t}()\) over the function class \(\) for the main player as in Line 3 of Algorithm 1, which is proportional to \(p^{0}(f)[ V_{f}^{*}+_{=1}^{t-1}_{h=1}^{H}L_{h}^{}(f)]\). Here, \( V_{f}^{*}\) serves as the optimism term, and \(L_{h}^{}(f)\) is the likelihood function built upon the pre-collected data. Such a construction of \(p^{t}()\) indicates that we will assign a higher probability (density) to a function \(f\), which results in higher values of the combination of the optimism term and the likelihood function. We sample a model \(^{t}\) from the distribution \(p^{t}()\) over the model class and learn the policy \(^{t}\) for the main player such that \((^{t},^{t})\) is the NE of the value function under the model \(^{t}\) in Line 4, where \(^{t}\) is a dummy policy and only used in our theoretical analysis.

**Posterior Sampling for the Exploiter.** The exploiter aims to track the best response of \(^{t}\) to assist learning a low regret. The best response of \(^{t}\) generated by the exploiter is nevertheless based on a value function under a different model than \(^{t}\). Specifically, for the exploiter, we define the posterior sampling distribution \(q^{t}()\) using an optimism term \(- V_{f}^{^{t},*}\) and the summation of likelihood functions, i.e., \(_{=1}^{t-1}_{h=1}^{H}L_{h}^{}(f)\), along with a prior distribution \(q^{0}()\), in Line 5 of Algorithm 1. The negative term \(- V_{f}^{^{t},*}\) favors a model with a low value and is thus optimistic from the exploiter's perspective but pessimistic for the main player. We then sample a model \(^{t}\) from \(q^{t}()\) and compute the best response of \(^{t}\), denoted as \(^{t}\), under the model \(^{t}\) as in Line 7.

**Data Sampling and Likelihood Function.** With the learned \(^{t}\) and \(^{t}\), we define a joint exploration policy \(^{t}\) in Line 7 of Algorithm 1, by executing which we can collect a dataset \(^{t}\). We are able to further construct the likelihood functions \(\{L_{h}^{t}(f)\}_{h=1}^{H}\) in Line 8 using \(^{t}\). Different game settings require specifying diverse exploration policies \(^{t}\) and likelihood functions \(\{L_{h}^{t}(f)\}_{h=1}^{H}\). Particularly, for the game classes mainly discussed in this work, we set \(^{t}=(^{t},^{t})\) for both FOMGs and POMGs. In FOMGs, we let \(^{t}=\{(s_{h}^{t},a_{h}^{t},b_{h}^{t},s_{h+1}^{t})\}_{h=1}^{H}\), where for each \(h[H]\), the data point \((s_{h}^{t},a_{h}^{t},b_{h}^{t},s_{h+1}^{t})\) is collected by executing \(^{t}\) to the \(h\)-th step of the game. The corresponding likelihood function is defined using the transition kernel as

\[L_{h}^{t}(f)=_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h} ^{t}).\] (1)

Furthermore, under the POMG setting, we let the dataset be \(^{t}=\{_{h}^{t}\}_{h=1}^{H}\), where the data point \(_{h}^{t}=(o_{1}^{t},a_{1}^{t},b_{1}^{t}\,,o_{h}^{t},a_{h}^{t},b_{h}^{ t})\) is collected by executing \(^{t}\) to the \(h\)-th step of the game for each \(h[H]\). We further define the associated likelihood function as

\[L_{h}^{t}(f)=_{f,h}(_{h}^{t}).\] (2)

Such a construction of the likelihood function in a log-likelihood form can result in learning a model \(f\) well approximating the true model \(f^{*}\) measured via the Hellinger distance.

### Regret Analysis for the Self-play Setting

Our regret analysis is based on a novel structural complexity condition for multi-agent RL and a quantity to measure how the well the prior distributions cover the optimal model \(f^{*}\). We first define the following condition for the self-play setting.

**Definition 1** (Self-Play GEC).: _For any sequences of functions \(f^{t},g^{t}\), suppose that a pair of policies \((^{t},^{t})\) satisfies: **(a)**\(^{t}=*{argmax}_{}_{}V_{f^{t}}^{,}\) and \(^{t}=*{argmin}_{}V_{g^{t}}^{^{t},}\), or **(b)**\(^{t}=*{argmin}_{}V_{f^{t}}^{,}\) and \(^{t}=*{argmax}_{}V_{g^{t}}^{,^{t}}\). Denoting the joint exploration policy as \(^{t}\) depending on \(f^{t}\) and \(g^{t}\), for any \(\{f,g\}\) and \((^{t},^{t})\) following **(a)** and **(b)**, the self-play GEC \(d_{}\) is defined as the minimal constant \(d\) satisfying_

\[|_{t=1}^{T}(V_{^{t}}^{^{t},^{t}}-V_{f^{*}}^{^{t}, ^{t}})|[d_{h=1}^{H}_{t=1}^{T}(_{= 1}^{t-1}_{(^{},h)}(^{t},_{h}^{})) ]^{}+2H(dHT)^{}+ HT.\]

Our definition of self-play GEC is inspired by  for the single-agent RL. Then, it shares an analogous meaning to the single-agent GEC. Here \((^{},h)\) implies running the joint exploration policy \(^{}\) to step \(h\) to collect a data point \(_{h}^{}\). The LHS of the inequality is viewed as the prediction error and the RHS is the training error defined on a loss function \(\) plus a burn-in error \(2H(dHT)^{}+ HT\) that is non-dominating when \(\) is small. The loss function \(\) and \(\) can be problem-specific. We determine \((f,_{h})\) for FOMGs with \(_{h}=(s_{h},a_{h})\) and POMGs with \(_{h}=_{h}\) respectively as

\[D_{}^{2}(_{f,h}(|_{h}),_{f^{*},h}(|_{h})),1/2(_{f,h}(_{h})/_{f^{*},h}(_{h})}-1 )^{2},\] (3)such that \(_{(,h)}[(f,_{h})]=D^{2}_{}(^{}_{ ,h},^{}_{f,h})\) for POMGs. The intuition for GEC is that if hypotheses have a small training error on a well-explored dataset, then the out-of-sample prediction error is also small, which characterizes the hardness of environment exploration.

Since the posterior sampling steps in our algorithms depend on the initial distributions \(p^{0}\) and \(q^{0}\), we define the following quantity to measure how well the prior distributions \(p^{0}\) and \(q^{0}\) cover the optimal model \(f^{*}\), which is also a multi-agent generalization of its single-agent version .

**Definition 2** (Prior around True Model).: _Given \(>0\) and any distribution \(p^{0}_{}\), we define_

\[(,p^{0})=_{>0}\{- p^{0}[ ()]\},\]

_where \(():=\{f\ :\ _{h,s,a,b}^{}( _{^{},h}(\,|\,s,a,b)\|_{f,h}(\,| \,s,a,b))\}\) for FOMGs and \(():=\{f\ :\ _{,}^{}( ^{,}_{f^{},H}\|^{,}_{f,H})\}\) for POMGs._

When the model class \(\) is a finite space, if let \(p^{0}=()\), we simply know that \((,p^{0})||\) where \(||\) is the cardinality of \(\). Furthermore, for an infinite function class \(\), the term \(||\) can be substituted by a quantity having logarithmic dependence on the covering number of the function class \(\). With the multi-agent GEC condition and the definition of \(\), we have the following regret bound for both FOMGs and POMGs.

**Proposition 1**.: _Letting \(=1/2\), \(_{1}=2)T/d_{}}\), \(_{2}=2)T/d_{}}\), \(=1/\) in Definition 1, when \(T\{4H^{2}(4HT,p^{0})/d_{},4H^{2}(4HT,q^{0})/d_ {},\)\(d_{}/H\}\), under both FOMG and POMG settings, Algorithm 1 admits the following regret bound,_

\[[_{1}^{}(T)]:=[_{t=1}^{T}(V_{f^ {}}^{}-V_{f^{}}^{^{},})] 6}HT [(4HT,p^{0})+(4HT,q^{0})]}.\]

This proposition gives the upper bound \([_{1}^{}(T)]\) following the updating rules in Algorithm 1 when the max-player is the main player. As Algorithm 3 is symmetric to Algorithm 1, we obtain the following regret bound of \([_{2}^{}(T)]\) for Algorithm 3 when the min-player is the main player.

**Proposition 2**.: _Under the same parameter settings as Proposition 1, Algorithm 3 admits the following regret bound,_

\[[_{2}^{}(T)]:=[_{t=1}^{T}(V_{f^ {}}^{,^{t}}-V_{f^{}}^{})] 6}HT [(4HT,p^{0})+(4HT,q^{0})]}.\]

Combining the results of Propositions 1 and 2, due to \(^{}(T)=_{1}^{}(T)+_{2 }^{}(T)\), we obtain the following overall regret when running Algorithms 1 and 3 together.

**Theorem 1**.: _Under the settings of Propositions 1 and 2, executing both Algorithms 1 and 3 leads to_

\[[^{}(T)] 12}HT [(4HT,p^{0})+(4HT,q^{0})]}.\]

The above results indicate that the proposed posterior sampling self-play algorithms (Algorithms 1 and 3) separately admit a sublinear dependence on GEC \(d_{}\), the number of learning episodes \(T\), as well as \((4HT,p^{0})\) and \((4HT,q^{0})\) for both FOMG and POMG settings. They lead to the same overall regret bound combining Propositions 1 and 2. In particular, when \(\) is finite with \(p^{0}=q^{0}=()\), Algorithms 1 and 3 admit regrets of \(O(}HT||})\). The quantity \(\) can be associated with the log-covering number if \(\) is infinite. Please see Appendix C for analysis.

## 4 Posterior Sampling for the Adversarial Setting

Without loss of generality, we assume that the max-player is the main agent and the min-player is the opponent. Under this setting, the goal of the main player is to maximize her cumulative rewards as much as possible, comparing against the value under the NE, i.e., \(V_{f^{}}^{}\). We develop a novel algorithm for this setting as summarized in Algorithm 2. In our algorithm, the opponent's policy is assumed to be arbitrary and is also _not revealed_ to the main player. The only information about the opponent is the current state or the partial observation of her state as well as the actions taken.

We adopt the optimistic posterior sampling approach for the main player with defining an optimism term as \( V_{f}^{}\) motivated by the above learning target, and the likelihood function \(L_{h}^{t}(f)\) with \(L_{h}^{t}(f):=_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^ {t})\) in (1) for FOMGs and \(L_{h}^{t}(f)=_{f,h}(_{h}^{t})\) in (2) for POMGsrespectively. The policy \(^{t}\) learned by the main player is from computing the NE of the value function under the current model \(f^{t}\) sampled from the posterior distribution \(p^{t}\). In addition, the joint exploration policy is set to be \(^{t}=(^{t},^{t})\) where \(^{t}\) is the potentially adversarial policy played by the opponent. Thus, we can collect the data defined as \(^{t}=\{(s^{t}_{h},a^{t}_{h},b^{t}_{h},s^{t}_{h+1})\}_{h=1}^{H}\) and \(^{t}=\{^{t}_{h}\}_{h=1}^{H}\) with \(^{t}_{h}=(o^{t}_{1},a^{t}_{1},b^{t}_{1},o^{t}_{h},a^{t}_{h},b^{t}_{h})\) for FOMGs and POMGs respectively, collected by executing \(^{t}\) to the \(h\)-th step of the game for each \(h[H]\).

**Remark 1**.: _In Algorithm 2, we define the joint exploration policy \(^{t}=(^{t},^{t})\), which is the key to the success of the algorithm design under the adversarial setting, especially for POMGs. Under the single-agent setting, the prior work  sets the exploration policy for a range of partially observable models subsumed by the PSR model as \(^{t}_{1:h-1}_{h}()\), i.e., running \(^{t}\) for steps \(1\) to \(h-1\) and then sampling the data at step \(h\) by enforcing a uniform policy. Such an exploration scheme fails to work when facing an uncontrollable opponent who does not play a uniform policy at step \(h\). Theoretically, we prove that employing policies \((^{t}_{1:h},^{t}_{1:h})\) for exploration without the uniform policy, the self-play and adversarial GEC conditions in Definitions 1 and 3 are still satisfied for a class of POMGs including weakly revealing and decodable POMGs. This eventually leads to a unified adversarial learning algorithm for both FOMGs and POMGs._

### Regret Analysis for the Adversarial Setting

Before demonstrating our regret analysis, we first define a multi-agent GEC fitting the adversarial learning scenario. Considering that the opponent's policy is uncontrollable during the learning, we let \(\{^{t}\}_{t=1}^{T}\) be arbitrary, which is clearly distinguished from self-play GEC defined in Definition 1.

**Definition 3** (Adversarial GEC).: _For any sequence of functions \(\{f^{t}\}_{t=1}^{T}\) with \(f^{t}\) and any sequence of the opponent's policies \(\{^{t}\}_{t=1}^{T}\), suppose that the main player's policies \(\{^{t}\}_{t=1}^{T}\) are generated via \(^{t}=*{argmax}_{}_{}V_{f^{t}}^{^{t}}\). Denoting the joint exploration policy as \(\{^{t}\}_{t=1}^{T}\) depending on \(\{f^{t}\}_{t=1}^{T}\), the adversarial GEC \(d_{}\) is defined as the minimal constant \(d\) satisfying_

\[_{t=1}^{T}(V_{f^{t}}^{^{t},^{t}}-V_{f^{*}}^{^{t},^{t}} )[d_{h=1}^{H}_{t=1}^{T}(_{=1}^{t-1}_{(^{},h)}(f^{t},^{}_{h}))]^{}+2H(dHT)^{ }+ HT.\]

Our regret analysis for Algorithm 2 also depends on the quantity \((,p^{0})\) that characterizes the coverage of the prior distribution \(p^{0}\) on the true model \(f^{*}\). Then, we have the following regret bound.

**Theorem 2**.: _Letting \(=\), \(=2)T/d_{}}\), \(=1/\) in Definition 3, when \(T\{4H^{2}(4HT,p^{0})/d_{},d_{}/H\}\), under both FOMG and POMG settings, Algorithm 2 admits the following regret bound,_

\[[^{}(T)] 4}HT (4HT,p^{0})}.\]

The above result indicates that we can achieve a meaningful regret bound by a posterior sampling algorithm with general function approximation, even when the opponent's policy is adversarial and her full policies \(^{t}\) are not revealed. This regret has a sublinear dependence on \(d_{}\), the number of episodes \(T\), as well as \((4HT,p^{0})\). Similarly, when \(\) is finite, Algorithm 2 admits a regret of \(O(}HT||})\). The term \(||\) can be the log-covering number of \(\) if it is infinite.

[MISSING_PAGE_FAIL:9]

where \( V^{*}_{^{t}}:=V^{^{t},^{t}}_{^{t}}-V^{*}_{f^{*}}\) and \( V^{^{t},*}_{^{t}}=V^{^{t},^{t}}_{^{t}}-V^{^ {t},*}_{f^{*}}\) are associated with the optimism terms in posterior distributions. The inequality above for Term(i) is due to Line 4 such that \(V^{^{t},^{t}}_{^{t}}=_{}V^{^{t},^{t}}_{^{t} } V^{^{t},^{t}}_{^{t}}\). By Definition 1 for self-play GEC, we obtain that \(_{t=1}^{T}(V^{^{t},^{t}}_{^{t},1}-V^{^{t}, ^{t}}_{f^{*}})\) and \(_{t=1}^{T}(V^{^{t},^{t}}_{f^{*}}-V^{^{t},^{t}}_{ ^{t},1})\) can be bounded by

\[[d_{}_{h=1}^{H}_{t=1}^{T}(_{t=1}^{t-1}_{(^{t}_{},h)}(^{t},^{t}_{h}))]^{1/2}+2H(d_{ }HT)^{}+ HT,\]

where \(^{t}\) is chosen as \(^{t}\) or \(^{t}\) respectively. By Lemma 11 and Lemma 12, we prove that for both FOMGs and POMGs, the accumulation of the losses \((^{t},^{t}_{h})\) in (3) connects to the likelihood function \(L^{t}_{h}\) defined in (1) and (2). Thus, we obtain \([]_{t=1}^{T}_{Z^{t-1}}_ {^{t} p^{t}}\{-_{1} V^{*}_{^{t}}- _{h=1}^{H}_{t=1}^{t-1}[L^{t}_{h}(^{t})-L^{t}_{h}(f^{*})]+ (^{t})}{p^{t}(^{t})}\}+2H(d_{}HT)^{}+ HT\) and \([]\) has a similar bound based on \(q^{t}\), where \(Z^{t-1}\) is the randomness history. By Lemma 10, the posterior distributions \(p^{t}\) and \(q^{t}\) following Lines 3 and 5 of Algorithm 1 can minimize the above upper bounds for \([]\) and \([]\). Therefore, we can relax \(p^{t}\) and \(q^{t}\) to be distributions defined around the true model \(f^{*}\) to enlarge above bounds. When \(T\) is sufficiently large and \(=1/2\), we have

\[[] (HT,p^{0})T/_{1}+_{1}d_{}H/4+2H( d_{}HT)^{}+ HT,\] \[[] (HT,q^{0})T/_{2}+_{2}d_{}H/4+2H( d_{}HT)^{}+ HT.\]

Choosing proper values for \(\),\(_{1}\), and \(_{2}\), we obtain the bound for \([^{}_{}(T)]\) in Theorem 1 via \(^{}_{1}(T)=\) + Term(ii). In addition, we can prove the bound of \([^{}_{}(T)]\) in a symmetric manner. Finally, combining \([^{}_{}(T)]\) and \([^{}_{}(T)]\) gives the result in Theorem 1.

**Proof Sketch of Theorem 2.** Under the adversarial setting, the policy of the opponent \(^{t}\) is not generated by the algorithm, which could be arbitrarily time-varying. We decompose \(^{}(T)=_{t=1}^{T} V^{*}_{f^{t}}+_{t=1}^{T }[V^{*}_{f^{t}}-V^{^{t},^{t}}_{f^{*}}]\) where \( V^{*}_{f^{t}}:=V^{*}_{f^{*}}-V^{*}_{f^{t}}\) relates to optimism. Since \((^{t},^{t})\) is NE of \(V^{,}_{f^{t}}\) as in Line 3 of Algorithm 2, we have \(V^{*}_{f^{t}}=_{}V^{^{t},}_{f^{t}} V^{^{t},^{t}}_{f^{t}}\), which leads to

\[^{}(T)_{t=1}^{T} V^{*}_{f^{t}}+_{t=1} ^{T}V^{^{t},^{t}}_{f^{t}}-V^{^{t},^{t}}_{f^{*}}.\]

We can bound \(_{t=1}^{T}[V^{^{t},^{t}}_{f^{t}}-V^{^{t},^{t}}_{f^{*}}]\) via adversarial GEC in Definition 3 by

\[[d_{}_{h=1}^{H}_{t=1}^{T}_{t=1}^{t-1} _{(^{t}_{},h)}(f^{t},^{t}_{h}) ^{}+2H(d_{}HT)^{}+ HT.\]

Connecting the loss \((^{t},^{t}_{h})\) to the likelihood function \(L^{t}_{h}\) defined in (1) and (2) via Lemmas 11 and 12, we obtain \([^{}(T)]_{t=1}^{T}_{Z^{t-1}} _{f^{t} p^{t}}\{_{t=1}^{T} V^{*}_{f^{t}}-_{h=1 }^{H}_{t=1}^{t-1}[L^{t}_{h}(f^{t})-L^{t}_{h}(f^{*})]+(f^{t})} {p^{t}(f^{t})}\}+2H(d_{}HT)^{}+ HT\). Lemma 10 shows \(p^{t}\) in Line 3 of Algorithm 2 can minimize this bound. Thus, relaxing \(p^{t}\) to be distribution defined around the true model \(f^{*}\), with sufficiently large \(T\) and \(=1/2\), we have

\[[^{}(T)](4HT,p^{0})T/+ d_{ }H/4+2H(d_{}HT)^{}+ HT.\]

Choosing proper values for \(\) and \(\), we eventually obtain the bound for \([^{}(T)]\) in Theorem 2.

**Discussion of Limitations.** Our work has studied several but a limited number of tractable MG classes in both FOMGs and POMGs. It is interesting to further define new MG classes by generalizing their single-agent counterparts and explore the relation between these MG classes and low self-play/adversarial GEC classes. It is also intriguing to generalize our method to general-sum settings.