# Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance

Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance

 Joshua McClellan

JHU APL

University of Maryland

joshmccl@umd.edu

&Naveed Haghani

JHU APL

naveed.haghani@jhuapl.edu

&John Winder

JHU APL

john.winder@jhuapl.edu

&Furong Huang

University of Maryland

furongh@umd.edu

&Pratap Tokekar

University of Maryland

tokekar@umd.edu

The Johns Hopkins University Applied Physics Lab

###### Abstract

Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization . These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error . In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present _Exploration-enhanced Equivariant Graph Neural Networks_ or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.

## 1 Introduction

Multi-Agent Reinforcement Learning (MARL) has found success in various applications such as robotics [3; 4; 5], complex strategy games [6; 7; 8] and power grid management [9; 10]. However, MARL algorithms can be slow to train, difficult to tune, and have poor generalization guarantees [1; 11]. This is partially because typical implementations of MARL techniques use neural networks such as Multi-Layer Perceptrons (MLP) that do not take the underlying structure into account. The models learn simple input/output relationships with no constraints or priors on the policies learned. These generic architectures lack a strong inductive bias making them inefficient in terms of the training samples required.

Figure 1: An example of how using an equivariant function approximator shrinks the total search space.

Symmetries are commonplace in the world. As humans, we exploit symmetries in our daily lives to improve reasoning and learning. It is a basic concept learned by children. Humans do not need to relearn how to eat an apple simply because it has been moved from the right to the left. In soccer, if you have learned to pass to someone on the right, it is easier to learn to pass to someone on the left. Our objective is to develop agents that are guaranteed to adhere to these basic principles, without needing to learn every single scenario from scratch.

Symmetries are particularly common in MARL. These occur in the form of _equivariance_ and _invariance_. Given a transformation matrix \(\), if \(f(x)=f(x)\) that function is said to be invariant. Similarly, \(f(.)\) is equivariant if \(f(x)=f(x)\). Rotational and reflection symmetries (the \(O(n)\) symmetry group) are particularly common in Reinforcement Learning (RL) scenarios. For example, consider the Multi-agent Particle Environment (MPE)  benchmark for MARL which has agents with simple dynamics and tasks. We note that this scenario adheres to rotational equivariance. As shown in Figure 2 rotating the agent's positions results in the opposite direction the orientation equivariant effectively shrinks the question _What is the correct representation of a complex action space when using Equivariant GNNs?_

Another specific issue we observed in EGNNs is an exploration bias in the early stages of learning due to the specific nature of the equivariant computation. This bias can lead to poor initial exploration, decreasing the probability of the agent finding the optimal policy, and potentially resulting in sub-optimal convergence. Therefore we ask the question _How can we design a GNN equivariant to rotations, but without early exploration bias?_

The main contribution of this paper is Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). This addresses the two research questions. Specifically, we show how to apply Equivariant GNNs to complex MARL tasks, to mixed discrete/continuous action spaces, and to mitigate the early exploration bias. Our contributions can be summarized as:

(1) Our approach is the first to successfully demonstrate Equivariant GNNs for MARL on standard MARL benchmarks with complex action spaces.

(2) We propose E2GN2 which has no bias in early exploration for MARL and is equivariant to rotations and reflections.

(3) We evaluate E2GN2 on common MARL benchmarks: MPE  and Starcraft Multi-agent Challenge v2  using PPO. It learns quicker, outperforming standard GNNs and MLPs by up to 2x-5x 2 in sample efficiency on terran and protoss challenges. It is worth noting that E2GN2 is an improvement on the function approximation for MARL and thus is compatible with most MARL actor-critic methods.

(4) We showcase E2GN2's ability to generalize to scenarios it wasn't trained on, due to the equivariance guarantees. This results in 5x performance over GNNs

Figure 2: An example of rotational equivariance/symmetry in MPE simple spread environment. Note as the agent (in red) positions are rotated, the optimal actions (arrows) are also rotated.

Related Works

A key theoretical foundation for our paper is , which formulated the structure for equivariant MDPs. One important takeaway is that if a reward is equivariant to a transformation/symmetry, we want the policy and dynamics to be equivariant to that symmetry. However, this work was limited to very simple dynamics with discrete actions such as cart-pole.  followed up with equivariance in multi-agent, but was again limited to simple problems. Their method is specifically formulated and tested on small discrete grid world problems with simple dynamics and discrete up, down,left actions. For example, the traffic control problem has an input of a 7x7 grid. Extending their work to continuous environments with large state spaces, large mixed discrete/continuous action spaces is not straightforward without significant modifications.

Contemporary to our research,  demonstrated E(3) equivariant networks on simple cooperative navigation problems. However, their results on more complex tasks, such as Starcraft, did not excel over the baseline. Additionally, they used SEGNN  which can result in very slow training times, making tuning difficult and cumbersome. Others took the approach  of attempting to learn symmetries via an added loss term. However, since this approach needed to learn the symmetries in parallel with learning it did not have the same guarantees as Equivariant GNNs, and did not result in significant performance gains. Another work  demonstrated rotation equivariance for complex robotic manipulation tasks. This work, while promising, was for single-agent RL and used image observations and many problems don't have access to image observations.

AI research in chemistry has taken a particular interest in adding symmetry constraints to GNNs. Works such as EGNN,  SEGNN,  E3NN,  and Equivariant transformers have demonstrated various approaches to encoding symmetries in GNNs. In this paper, we chose to focus on EGNN due to its simplicity, high performance, and quick inference time. Other works took a different approach, such as , which proposes Equivariant MLPs, solving a constrained optimization problem to encode a variety of symmetries. Unfortunately, in our experience, the inference time was slow, and we preferred a network with a graph structure such as EGNN.

## 3 Background

### Marl

Multi-agent reinforcement learning (MARL) considers problems where multiple learning agents interact in a shared environment. The goal is for the agents to learn policies that maximize long-term reward through these interactions. Typically, MARL problems are formalized as Markov games . A Markov game for \(N\) agents is defined by a set of states \(\), a set of actions \(_{1},...,_{N}\) for each agent, transition probabilities \(P:_{1}..._{N} \), and reward functions \(R_{1},...R_{N}\) mapping each state and joint action to a scalar reward.

The goal of each agent \(i\) is to learn a policy \(_{i}(a_{i}|s)\) that maximizes its expected return: \(J(_{i})=_{1},...,_{N}_{t=0}^{T}^{t}R_{i}( s_{t},a_{t}^{1},...,a_{t}^{N})\)

Where \(T\) is the time horizon, \((0,1]\) is a discount factor, and \(a_{t}^{j}_{j}(|s_{t})\). The presence of multiple learning agents makes this more complex than single-agent RL due to issues such as non-stationarity and multi-agent credit assignment.

### Equivariance

Crucial to equivariance is group theory. A group is an abstract algebraic object describing a symmetry. For example, \(O(3)\) describes the set of continuous rotation symmetries. A group action is an element of that particular group. To describe how groups o perate on data we use representations of group actions. A representation can be described as a mapping from a group element to a matrix, \(:G GL(m)\) where \((g)^{m m}\) Or instead we can more simply use: \(L_{g}:X X\) where \(g G\) where \(L_{g}\) is the matrix representation of the group element \(g G\).

A function is equivariant to a particular group or symmetry if transforming the input is equivalent to transforming the function output. More formally, \(T_{g}f(x)=f(L_{g}x)\) for \(g G\), \(L_{g}:X X\) and \(T_{g}:Y Y\). Related to equivariance is the key concept of invariance, that is a function does not change with a transformation to the input: \(f(x)=f(L_{g}x)\). Previous work  has shown that if a Markov game has symmetries in the dynamics and the reward function then the resulting optimal policy will be equivariant and the value function will be invariant. That is, \(V(L_{g})=V(s)\) and \((L_{g})=K_{g}()\) where \(L_{g}\) and \(K_{g}\), with \(g G\) are transformations to the state and action respectively.

### Equivariant Graph Neural Network

Equivariant Graph Neural Networks  (EGNN) are an extension of a standard Graph Neural Network. EGNNs are equivariant to the \(E(n)\) group, that is, rotations, translations, and reflections in Euclidean space. EGNNs have two vectors of embeddings for each graph node \(i\): the feature embeddings denoted by \(_{i}^{l}\), and coordinate embeddings denoted by \(_{i}^{l}\), where \(l\) denotes the neural network layer. The equations describing the forward pass for a single layer are below:

\[_{ij}=_{e}(_{i}^{l},_{j}^{l},\|(_{i}^{l}- _{j}^{l})\|^{2})\] (1)

\[_{i}^{l+1}=_{i}^{l}+C_{j i}(_{i}^{l}-_{j} ^{l})_{u}(_{ij})\] (2)

\[_{i}=_{j i}_{ij},_{i}^{l+1}=_{h}(_{i}^{l},_{i})\] (3)

Here \(\) represents a multi-layer perceptron, where \(_{e}:^{n}^{m}\), \(_{u}:^{m}\), and \(_{h}:^{m+p}^{p}\). The key difference between EGNN and GNN is the addition of coordinate embeddings \(_{i}\) and equation 2, which serves to update the coordinate embeddings in a manner that is equivariant to transformations from \(E(n)\). Note that \(_{i}\) will be equivariant and \(_{i}\) will be invariant to these transformations .

As noted in Section 1, application of EGNN to MARL is not straightforward. In the following section, we discuss these issues in more depth and present our solution towards addressing them.

## 4 Methods

In this section, we address both theoretically and empirically how the output of EGNN is biased by the input, leading to suboptimal exploration in RL. To mitigate this issue, we introduce Exploration-enhanced Equivariant Graph Neural Networks (E2GN2) as a method that ameliorates this bias, leading to improved exploration.

### Biased Exploration in EGNN Policies

An important component of reinforcement learning is exploration. Practitioners often use a policy parameterized by a Gaussian distribution, where the mean is determined by the policy network output, and the standard deviation is a separately learned parameter. Best practices are that the actions initially have a zero mean distribution to get a good sampling of potential state-action trajectories, i.e., \((_{i}|) N(,)\). Below we show that an EGNN will initially have a non-zero mean distribution, which can cause problems in early training.

**Theorem 1**: _Given a layer \(l\) of an EGNN with randomly initialized weights, with the equivariant component input vector \(_{i}^{l}^{n}\), equivariant output vector \(_{i}^{l+1}^{n}\) and the multi-layer perceptron \(_{u}:^{m}\), where the equivariant component is updated as: \(_{i}^{l+1}=_{i}^{l}+C_{j i}(_{i}^{l}-_{j} ^{l})_{u}(_{ij})\). Then the expected value of the output vector is approximately the expected value of the input vector:_

\[[_{i}^{l+1}][_{i}^{l}]\]

_Furthermore, given a full EGNN with L layers then the expected value of the network output is approximately the expected value of the network input_

\[[_{i}^{L}][_{i}^{0}]\]

(See appendix A for proof.)

[MISSING_PAGE_FAIL:5]

**Corollary 2.1**: _Given a policy for agent \(i\) represented by an E2GN2 and parameterized with a Gaussian distribution the policy will have the following distribution:_

\[_{i}(_{i}|) N(,)\]

Thus we see that E2GN2 should have unbiased early exploration for the equivariant component of the actions. The primary difference between EGNN and E2GN2 is the addition of \(_{u_{2}}\), which serves to offset the bias from the previous layer (or input) \(u_{i}^{l}\) To validate that this did indeed solve the exploration bias we tested E2GN2 on the same MPE simple spread environment. We observe in Figure 3 that E2GN2 did indeed have unbiased behavior when it came to early exploration, as the agents had smooth random actions, and the reward did not drastically decrease.

Analysis of E2GN2 EquivarianceHere we verify that E2GN2 still retains EGNN's guarantee of equivariance to rotations and reflections.

**Theorem 3**: _E2GN2 is equivariant to transformations from the \(O(n)\) group. In other words, it is equivariant to rotations and reflections._

(See appendix A for proof.)

Retaining our symmetries to rotations and reflections is important, since it should increase our sample efficiency and generalization. Note that E2GN2 is no longer equivariant to translations. However, in MARL translation equivariance is not necessarily a benefit. For example, consider the MPE problem with a translational equivariant policy. If we shift an agent to be 10 units right, this will add 10 to the action as well, causing it to move to the right! Essentially, this is adding an undesirable bias to the policy output: \((s+b)=(s)+b\) However, we can expect \(O(n)\) policy equivariance to improve our sample efficiency in MARL, and it is key that E2GN2 retains this guarantee.

### Adapting Architectures for Complex Action Spaces

Applying EGNN/E2GN2 to sophisticated MARL problems requires careful consideration. Many MARL environments have discrete action spaces or mixed continuous-discrete action spaces. Some components of these action spaces may be invariant and others may be equivariant. Further complicating the problem, MARL requires the neural networks to output parameters for a probability distribution (which is sampled for use in exploration). For continuous actions, agents typically use a gaussian and for discrete actions agents generally use logits. Unfortunately, it is not straightforward to map the distinct invariant and equivariant coordinate embeddings onto a single distribution in a manner that preserves equivariance.

A core benefit of the GNN structure in MARL is the scalability afforded by permutation equivariance: the ability to handle a variable number of agents without retraining. For example,  demonstrates using a GNN to train \(N\) agents, then to control \(N+1\) agents without retraining. However, this example did not operate with discrete or mixed action spaces or equivariant structures as we do here. Improperly mapping GNN outputs to logits risks losing this scalability.

To address these issues, we propose leveraging the GNN's graph structure to output different components of the action space from different nodes in an equivariant manner:

* **Discrete Actions & Invariant Components:** the invariant feature embeddings \(h_{i}\) of each agent's node are used to output discrete logits/actions and other invariant components. For action spaces with both equivariant and invariant components, this can be used as the 'action type selector' to select which type of action to apply at that type step (ie move action or targeting action).
* **Continuous Spatial Actions:** the equivariant coordinate embeddings \(u_{i}\) of each agent's node are used for continuous spatial actions such as movement.
* **Targeting Specific Entities:** for multi-agent environments with entities beyond the learning agents (e.g., enemies), logits for discrete actions pertaining to each entity (e.g., which enemy to target) are output from that entity's corresponding node. This enables the discrete action space to scale with the number of agents.

Each of these components is parameterized as a distribution (ie logits or gaussian), resulting in one to three seperate distributions (depending on the environment and which components are used). After the exploration phase samples from each of these distributions, the final action can be constructed from each of these components via concatenation or the action selection component.

This formulation enables us to retain the permutation equivariance and scalability of a GNN. An alternative to our approach is to add an MLP at the end of the GNN to convert the GNN outputs to the mixed action spaces. This will lose the scalability/locality of the neural network. For example, if you add two more agents how do you modify the final MLP to expand the action space? In our formulation, whenever an new entity \(i\) is added to the environment, we can simply add the node \(N+1\) to the graph. The action space of the GNN will now be supplemented with \(_{N+1}\) and \(_{N+1}\), allowing us to expand the action space without retraining.

By structuring the GNN output in this manner, we can handle discrete or mixed discrete-continuous action spaces while retaining the equivariance of EGNN/E2GN2 and the flexibility of GNNs to a variable number of agents and entities. This approach allows MARL agents based on equivariant GNNs to be applied to challenging environments with minimal restrictions on the action space complexity. Indeed this approach was key to enabling the success of EGNN/E2GN2 on SMACv2.

## 5 Experiments

We seek to answer the following questions with our experiments: _(1) Do rotationally equivariant policy networks and rotationally invariant value networks improve training sample efficiency? (2) Does E2GN2 improve learning and performance over EGNN? (3) Does Equivariance indeed demonstrate improved generalization performance?_

To address these questions, we use common MARL benchmarks: the multi-agent particle-world environment (MPE)  and Starcraft Multi-agent Challenge version 2 (SMACv2) . Our experiments show that equivariance does indeed lead to improved sample efficiency, with E2GN2 in particular performing especially well. We also demonstrate that generalization is guaranteed across rotational transformations.

We want to focus our experiments on the neural networks' effects on MARL performance. To isolate the impact of the network architecture, we avoid using specialized tips and tricks sometimes employed in MARL . This allows us to demonstrate that our proposed networks can improve performance

Figure 4: An example of using an Equivariant Graph Neural Network in MARL. Note that the state must be structured as a graph with each node having an equivariant \(u_{i}\) and invariant \(h_{i}\) component. As discussed in 4.3, the output of the policy uses \(u_{i}\) for equivariant (typically spatial) actions, and the \(h_{i}\) for invariant components of the actions

without relying on these additional techniques. Thus we use a common standardized open source MARL training library RLlib . We use the default multi-agent PPO algorithm, which does not use a centralized critic. We followed the basic hyperparameter tuning guidelines set forth in . That is, we use a large training batch and mini-batch size, low numbers of SGD iterations, and a small clip rate. Further hyperparameter details are found in appendix B.

We compare our results with common neural networks used for MARL benchmarks: multi-layer perceptrons (MLP), and GNNs (we use a GNN structure similar to equations 1, 3, see appendix B for details ). We also compare with the approach from  which we refer to as E3AC in our plots. Recall that E3AC also uses neural networks that guarantee equivariance: SEGNN. We integrated E3AC into RLLIB to ensure the RL training procedure remained consistent across our comparisons. Note that the majority of MARL papers on SMAC/SMACv2 use MLPs as the base network. We also compare with GNNs to show the improvement is not primarily due to the graph structure. For the main paper results, we assume full observability since we have not explicitly tackled the partial observability problem yet. In future work, we will seek to remedy this. However, to be thorough we performed experiments with partial observability, resulting in surprising success using E2GN2 (see appendix C).

### Training Environments

From the MPE environment (), we use two environments to benchmark our performance: cooperative navigation also known as spread, and predator-prey, also known as tag. The MPE tag environment consists of three RL-controlled agents seeking to catch a third evader agent. For easier comparison, we use a simple heuristic algorithm to control the evader agent. The evader simply moves in the opposite direction of the pursuers. The other MPE environment is the cooperative navigation or simple spread. In this environment, each agent seeks to minimize the distance between all obstacles. To better test equivariance, for the MPE environments we use continuous actions instead of the default discrete actions.

Next we test on SMACv2, a much more difficult environment than MPE. In SMACv2, the units are heterogenous with different capabilities (with different attack ranges, total health, and sometimes action spaces). The unit types are randomized at the beginning of the scenario. The actions include more components than simply movement (such as in MPE), agents can move and attack. The goals are more complex as well. Instead of simply navigating cooperatively as in MPE, the agents must learn attack formations and strategies. Sometimes it may be optimal to sacrifice health or allies in the purpose of the greater strategic objective. SMACv2 has three basic scenarios defined by the unit types: terrain, protoss, and zerg. We use 5 agents for each team, and the initial position configuration termed "surrounded" (see fig 7)

The SMACv2 action space poses an interesting problem for GNN structures. A key advantage of GNNs is the permutation equivariance, which leads to scalability without retraining. The default SMACv2 agents will output simply a discrete action determining movement or target for shooting. For our purposes, we modify the action space to be a mixed action space. This consists of a continuous vector for movement actions, a discrete action determining the attack target, and a boolean determining if the agent should shoot, move or no-op (to be complete, we include plots with a discrete action space in appendix C). As discussed in section 4.3 this will both prove a greater test for our algorithms and allow for the GNN to scale to larger numbers of agents.

Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time

### Training Results

We present the results from our training in this section. The results for MPE in Figure 5 are averaged across 10 seeds. As discussed previously, EGNN has poor early performance due to the early exploration bias. Despite this poor exploration, EGNN outperforms GNNs and MLPs, demonstrating the power of equivariance in MARL. Similarly, we note that in MPE tag, E2GN2 results in a strong final agent, while EGNN suffers in the initial training phases. We see that E2GN2 is able to greatly outperform EGNN's final reward, partly due to superior early exploration.

In figure 5 we also compare the wall clock time required to train each algorithm. Note that these were all trained on the same hardware and using the same training algorithm (RLLIBs PPO). E3AC remains competitive in sample efficiency on MPE spread, but it requires nearly four hours to gather one million time steps, compared to less than one hour for E2GN2. This is likely due to the SEGNN  underlying structure used by E3AC, which is slower due to the be slower for inference time. We also note that E2GN2 does manage to outperform E3AC on MPE Tag.

Next, we review the results from SMACv2 in Figure 6; these results are averaged across 10 seeds. The equivariant networks have a clear improvement in sample efficiency over the MLP and GNN. On the term environment, EGNN once again learns slower in the initial phases, but due to equivariance, it outperforms MLP/GNN. For protoss, we note that EGNN performs well but has a high variance for its performance. E3AC struggles to perform well on SMACv2, likely since it was unable to resolve the hurdle we addressed in section 4.3. Instead for SMACv2 E3AC used a simple MLP for the policy and SEGNN for the value function . The training speed and performance for E2GN2 is especially impressive since this environment is much more complex than the MPE.

### Generalization

Thus far we have demonstrated that equivariance does indeed lead to improved sample efficiency. The next question to answer is whether equivariance leads to improved generalization. We test this by using the'surrounded' initial configuration from SMACv2 shown in figure 7. By default, the agents are randomly generated in all directions (along specific axes). To test generalization, we only initialize agents on the left side of the map (i.e., surrounded left). We then test to see if the agents are able to generalize when they are tested with initial positions starting on the right (called surrounded right), and with the default surrounded configuration (termed Surrounded All or Surrounded). Theoretically, the equivariant network should see equivalent performance between the training and testing initialization, due to the guarantee of rotational equivariance for movement, and invariance for shooting actions.

The results of our tests are shown in Table 1. As expected E2GN2 has stellar generalization performance. The win rate remains the same from the training configuration (Surrounded Left) to Surrounded Right. We see an increase in win rate when testing in Surrounded All. This is likely because this Surrounded All is an easier scenario; the enemy is divided into more groups, so the agents can defeat the smaller groups one at a time. As we mentioned in the abstract, this table

Figure 6: Comparing performance of PPO on SMACv2 with various Neural Networks representing the policy and value function. Each chart represents a different race from the SMACv2 environment. We show the standard errors computed across 10 seeds.

Figure 7: SMACv2 initialization schemes used for testing generalizationindicates that for the Surrounded Right generalization test E2GN2 has \(\)5x the performance of GNN and MLP.

Next, in Table 2 we test E2GN2's ability to scale to various numbers of agents without retraining. We use the agents trained in SMACv2 from Figure 6 for these tests. We do not include the MLP agent since it cannot scale up without retraining. Note that the method described in section 4.3 was essential to enable scaling between agents. For example, if we passed the GNN output to a final MLP to map to the discrete action space this wouldn't be scalable to more agents.

The results for scalability confirm that both E2GN2 and GNN are able to maintain performance with larger numbers of agents. As the number of agents gradually increases the win rate does slowly decrease. E2GN2 does seem to have a slightly steeper loss in performance in the Zerg domain, which was also the most difficult domain for the initial training phase. We believe this is due to the higher complexity of the Zerg scenario, with certain unit types (baneli

  Environment & Network & 
 Training Initialization \\ _Surrounded Left_ \\  &  \\   & E2GN2 & \(0.57.01\) & \(.01\) & \(.01\) \\  & GNN & \(0.51.02\) & \(0.11.02\) & \(0.32.02\) \\  & MLP & \(0.33.02\) & \(0.12.02\) & \(0.24.02\) \\   & E2GN2 & \(0.59.01\) & \(.02\) & \(.02\) \\  & GNN & \(0.5.02\) & \(0.14.01\) & \(0.32.01\) \\  & MLP & \(0.42.02\) & \(0.17.02\) & \(0.27.02\) \\   & E2GN2 & \(0.35.02\) & \(.02\) & \(.02\) \\  & GNN & \(0.4.02\) & \(0.07.01\) & \(0.21.01\) \\   & MLP & \(0.21.02\) & \(0.05.01\) & \(0.12.01\) \\  

Table 1: Generalization Win Rate in SMACv2. Note that E2GN2 retains high performance, while GNN and MLP lose performance when generalizing

  Environment & Network & 
 Training Setup \\ _5 Agents_ \\  &  \\   & E2GN2 & \(0.69.02\) & \(0.65.02\) & \(0.63.02\) & \(0.62.02\) & \(0.54.04\) \\  & GNN & \(0.45.01\) & \(0.44.01\) & \(0.42.02\) & \(0.39.01\) & \(0.32.02\) \\   & E2GN2 & \(0.62.03\) & \(0.61.02\) & \(0.59.03\) & \(0.47.04\) & \(0.37.03\) \\  & GNN & \(0.39.02\) & \(0.37.01\) & \(0.36.03\) & \(0.30.02\) & \(0.21.02\) \\   & E2GN2 & \(0.36.03\) & \(0.32.03\) & \(0.31.01\) & \(0.23.01\) & \(0.18.03\) \\  & GNN & \(0.28.04\) & \(0.28.02\) & \(0.25.03\) & \(0.2.02\) & \(0.15.02\) \\  

Table 2: Generalization Win Rate: Testing RL agents ability to scale to different numbers of agents (originally trained with 5 agents)