# ODRL: A Benchmark for Off-Dynamics Reinforcement Learning

Jiafei Lyu\({}^{1}\)1 Kang Xu\({}^{2}\) Jiacheng Xu\({}^{3}\) Mengbei Yan\({}^{1}\) Jingwen Yang\({}^{2}\)

Zongzhang Zhang\({}^{3}\) Chenjia Bai\({}^{4}\)\({}^{}\) Zongqing Lu\({}^{5}\)\({}^{}\) Xiu Li\({}^{1}\)\({}^{}\)

\({}^{1}\)Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)Tencent

\({}^{3}\)National Key Laboratory for Novel Software Technology, Nanjing University

\({}^{4}\)Institute of Artificial Intelligence (TeleAI), China Telecom

\({}^{5}\)School of Computer Science, Peking University

lvjf20@mails.tsinghua.edu.cn, li.xiu@sz.tsinghua.edu.cn

###### Abstract

We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the first benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or offline, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent's adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a unified framework and introduces some extra baselines for different settings, all implemented in a single-file manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors. Our code is publicly available at https://github.com/OffDynamicsRL/off-dynamics-rl.

## 1 Introduction

Human beings are able to transfer the policies swiftly to a structurally similar task. This ability is also expected in decision-making agents, especially embodied AI . For instance, we may train the robot in a simulated environment (i.e., _source domain_) and deploy the learned policy in real-world tasks (i.e., _target domain_), where the dynamics gap may pertain between the simulation and reality. It is anticipated that the robot is able to adapt itself to real-world dynamics quickly.

How to generalize policies across different domains with dynamics discrepancies efficiently remains an open problem in reinforcement learning (RL). Such a problem setup is referred to as _off-dynamics RL_ in , but it is restricted in demanding the online source domain and target domain. We extend its scope and formally define a general off-dynamics RL setting (Definition 1), where the source domain and the target domain only differ in their transition dynamics. Existing researches realize policy adaptation under dynamics mismatch via system identification , domain randomization , learning domain classifiers , etc. However, we argue that this field lacks a standard and unified benchmark. Upon checking the latest off-dynamics RL methods , we found that they often manually construct their customized environments with dynamics shifts andconduct experiments on them. The superior performance reported in these papers does not necessarily indicate that they are indeed state-of-the-arts, due to the lack of comprehensive evaluations and comparison using a unified testbed. Eventually, the progress in off-dynamics RL can be impeded.

To mitigate this concern, we introduce ODRL, the first benchmark for off-dynamics RL. ODRL covers numerous domain categories, including locomotion, navigation, and dexterous manipulation, and a wide spectrum of varied dynamics shifts, e.g., kinematic and morphology mismatch, as depicted in Figure 1. Our benchmark provides 4 types of experimental settings in a unified framework (see Figure 2), where the source and target domain can be either online or offline. Such versatility guarantees a thorough and trustworthy assessment of off-dynamics RL algorithms under different conditions.

Furthermore, ODRL encompasses a variety of recent off-dynamics RL algorithms within diverse experimental configurations, in conjunction with some baseline methods proposed on our own. All algorithms share similar code styles and are implemented in separate single files, making it easier to recognize core algorithmic designs and performance-relevant details. It also ensures consistency and allows for a fair benchmarking comparison. Notably, only a limited budget of target domain data is permitted in ODRL to better evaluate the policy adaptation efficiency of the agent.

To summarize, we have made the following efforts: (a) we give a formal definition of the general off-dynamics RL setting; (b) we propose the first off-dynamics RL benchmark, which offers different experimental settings, diverse task categories, and dynamics shift types under a unified framework; (c) we isolate algorithm implementations into single files to facilitate a straightforward understanding of the key algorithmic designs; (d) we conduct extensive experiments to investigate the performance of existing methods under different dynamics shifts and experimental settings, and conclude some key observations and insights. It is our hope that ODRL can aid researchers in developing dynamics-aware methods and pave the way for adaptable algorithms in real-world applications.

## 2 Background

**Reinforcement Learning (RL).** We study sequential decision-making problems in a Markov Decision Process (MDP), which is specified by the tuple \(,,P,r,_{0},\), where \(,\) are state space and action space, respectively, \(P(s^{}|s,a):()\) is the transition probability, where \(\) is the probability simplex, \(r:\) is the scalar reward signal, \([0,1)\) is the discount factor, \(_{0}\) is the initial state distribution. The goal of RL is to find a policy \((|s)\) to maximize \(J()=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})]\).

**Off-dynamics RL.** In off-dynamics RL, we consider two infinite-horizon MDPs, the _source domain_\(_{}:=,,P_{},r, _{0},\) and the _target domain_\(_{}:=,,P_{},r, _{0},\). Note that the

Figure 1: **An overview of selected benchmark tasks.** ODRL includes multiple domains with various types of dynamics shifts, making it a reliable platform for evaluating policy adaptation ability.

two domains only differ in their transition probabilities, and other components like state space, action space, and reward functions are kept the same. Formally, we define our problem setting below.

**Definition 1** (Off-dynamics RL setting).: _The agent has access to sufficient data from the source domain \(_{}\) and a limited budget of data from the target domain \(_{}\), where there exist dynamics shifts between \(_{}\) and \(_{}\). The agent aims at getting better performance in the target domain \(_{}\) by leveraging data from both domains._

## 3 Related Work

Generalizing policies across varied domains is a broad and critical topic in RL, where domains can differ in transition dynamics [16; 69; 77; 12], observation spaces or action spaces [22; 6; 23; 83; 4; 79; 25], etc. As a benchmark paper, it is difficult to include all possible domain discrepancies. We set our focus on the policy adaptation across domains with dynamics mismatch, because this kind of problem often occurs in real-world applications [53; 36; 1; 85], e.g., enabling the quadruped robot that previously trained under indoor scenes to adapt its policy to outdoor varied landscapes.

Previous studies mainly handle this challenge by domain randomization [63; 49; 70; 35], system identification [10; 86; 15; 74; 17; 87; 11; 60], and meta learning approaches [50; 58; 2; 73]. These methods often rely on a manipulable simulator , or require access to the distribution of training environments. System identification can be expensive due to the need of calibrating the dynamics of the source domain, and domain randomization relies on manually-specified randomized parameters and nuanced randomization distributions. Another line of research tries to address the issue from the perspective of designing dynamics-aware algorithms without changing the parameters of the training environment. Typical methods include recognizing the dynamics change by training expressive models [24; 31; 75], selectively sharing transitions from the source domain by contrastive learning  or the proximity of paired value estimate targets across the two domains , learning domain classifiers for measuring the dynamics gap to penalize source domain rewards [16; 41; 68] or performing importance weighting [52; 51], capturing the representation mismatch between two domains for reward modification , leveraging action transformation methods that utilizing the trained dynamics models of the two domains convert transitions from the source domain [27; 14]. Furthermore, some studies close the dynamics gap between two domains by using the expert demonstration from the target domain [37; 28; 18; 61]. Despite the success, existing papers often conduct experiments within self-proposed environments, which is unhealthy for the advances of this field because it fails to truly reveal the merits of the proposed method. Meanwhile, the constructed tasks often lack sufficient diversity, and different papers set focus on distinct settings (e.g., whether the source domain is offline). These motivate us to develop a unified and standard benchmark for off-dynamics RL.

**Comparison against other benchmarks.** There are numerous benchmarks that are related to ODRL, including Gym-extensions , D4RL , DMC suite , Meta-World , RLBench , CARL , Continual World . We compare ODRL against these benchmarks below.

Among shown in Table 1, D4RL only contains single-domain offline datasets and does not focus on the off-dynamics RL. DMC suite contains a wide range of tasks, but it does not offer offline datasets and does not handle the off-dynamics RL. Meta-world is designed for the multi-task RL setting.

   Benchmark & Offline Datasets & Diverse Domains & Multi-task & Single-task Dynamics Shift \\  D4RL  & \(}}}\) & \(}}}\) & \(}}\) & \(}}\) \\ DMC suite  & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\ Meta-World  & \(}}\) & \(}}\) & \(}}\) \\ RLBench  & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\ CARL  & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\ Gym-extensions  & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\ Continual World  & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\ ODRL (this work) & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\   

Table 1: **A comparison between ODRL and other RL benchmarks.**RLBench provides demonstrations for numerous tasks but it does not involve the dynamics shift in a single task. CARL focuses on the setting where the context of the environment (e.g., reward, dynamics) can change between different episodes (i.e., it does not have a source domain or target domain, but only one domain where the dynamics or rewards can change depending on the context). CARL also does not provide offline datasets. Continual World is a benchmark for continual learning in RL. It also supports multi-task learning and can be used for transfer RL policies. ODRL, instead, focuses on the setting where the agent can leverage source domain data to facilitate the policy training in the target domain, where the task in the source domain and the target domain remain identical. Note that Gym-extension involves some tasks that are similar to some MuJoCo tasks in ODRL, but Gym-extensions is intrinsically designed for multi-task scenarios and ODRL covers more diverse task domains, dynamics shift types and also provides offline datasets for each task.

## 4 Benchmark Details

ODRL mainly contains three task categories, including locomotion, navigation, and dexterous hand manipulation. Different domains are equipped with distinct and representative dynamics shift tasks. We treat the vanilla environments within these domains as the _source domain_ and environments with dynamics shifts as the _target domain_. Both the source domain and the target domain in ODRL are allowed to be either online or offline, resulting in four varied training paradigms as illustrated in Figure 2, e.g., _Online-Offline_ setting denotes that the source domain is online while the target domain is offline. The goal is to learn policies that can achieve better performance in the target domain by leveraging transition from the _single_ source domain (with dynamics discrepancies) and _single_ target domain. ODRL offers a diverse collection of tasks and covers all possible single-task policy adaptation settings, making it a reliable and unified benchmark for off-dynamics RL.

### Benchmark Tasks

**Locomotion.** We adopt four tasks (Ant, Hopper, HalfCheetah, Walker2d) from the popular OpenAI Gym library , simulated by MuJoCo . We consider four kinds of dynamics shifts:

1. [label=()]
2. _Friction shift._ In MuJoCo, friction is represented by three components: static, dynamic, and rolling friction, where static friction means the frictional force that needs to be overcome when the robot is stationary, while dynamic friction and rolling friction stand for the frictional force between objects when they are in motion and rolling, respectively. Modifying the friction attribute can significantly change the motion characteristics of the simulated robots. We modify all friction components and consider shift levels \(\{0.1,0.5,2.0,5.0\}\), where the friction components of the target domain are set to be 0.1, 0.5, 2.0, and 5.0 times the friction components of the source domain to span across both slight shifts and serious shifts.
3. _Gravity shift._ The gravity attribute corresponds to the gravitational force acting on objects within the simulation. We only modify the strength of the gravity and keep its direction unchanged. Similarly, we consider shift levels \(\{0.1,0.5,2.0,5.0\}\) where the gravity in the target domain is established at 0.1 times, 0.5 times, 2.0 times, and 5.0 times the gravity within the source domain to cover both minor shift cases and extreme shift cases.

Figure 2: **Benchmark setting and algorithmic implementations. Our benchmark involves 4 varied experimental settings, where the source domain and the target domain can be designated as online or offline. Numerous baselines and off-dynamics RL algorithms are implemented for each setting.**

3. _Kinematic shift._ The kinematic discrepancies are realized by constraining the rotation angle ranges of certain joints in the simulated robot, i.e., some joints are broken such that it becomes infeasible to exhibit some motion characteristics in the target domain. We offer two choices of broken joints for each robot, which occur at varied parts of their body and are distinct across different robots since they possess varied structures and appearances, e.g., broken hips or broken ankles can occur in _ant_ tasks, while the thigh joint or the foot joint can be broken for the _halfcheetah_ task. For a specific type of kinematic shift, one can choose from three different shift levels (_easy_, _medium_, _hard_), where the joint's rotation angle range is restricted to distinct values.
4. _Morphology shift._ We achieve the morphology shifts by modifying the size of specific limbs or torsos of the simulated robot, without altering the state space space and action space. We also provide two types of morphological change for each robot, along with three different shift levels (_easy_, _medium_, _hard_) for each type, where the body part in the target domain is revised to different sizes, depending on the specific task. For example, the leg size in the _ant_ task can be 1/2 of its leg size in the source domain given a medium shift level.

Generally, ODRL involves 4 friction shift tasks, 4 gravity shift tasks, 6 kinematic shift tasks, and 6 morphology shift tasks for a single robot, resulting in a total of **80** tasks with dynamics mismatch in the locomotion domain. If one considers the source domain to be offline, different qualities of source domain datasets can incur a substantial number of tasks, e.g., given a fixed target domain, the source domain datasets can be medium or expert. We adopt the offline source domain datasets from D4RL , which contains 6 different types of offline datasets (random, medium, medium-replay, medium-expert, full-replay, expert), leading to **480** MuJoCo tasks for the **Offline-Online** setting in principle. Similarly, abundant tasks can be used for evaluation under other settings like **Online-Offline**.

**AntMaze.** The AntMaze domain requires navigating an 8-DoF Ant quadruped robot to the goal position within the maze. The morphologically sophisticated quadruped robot attempting to reach goals can mimic real-world navigation tasks. Following D4RL , we use a sparse 0-1 reward and a +1 reward is only assigned when the goal is reached. We employ three map sizes (_small_, _medium_, _large_), and construct 6 different map layouts for each map size (please see some examples in Figure 1), leading to an aggregate of **18** tasks. Note that the embodied Ant robot is unchanged, and only the map structures are modified, targeting examining the policy adaptation ability to varied landscapes or potential obstacles. The starting position and the _unique_ goal position in the revised target map remain consistent with the original source domain map.

**Dexterous Manipulation.** We consider four tasks (_pen_, _door_, _relocate_, _hammer_) from Adroit  that is tailored for dexterous hand manipulation. It demands controlling a 24 DoF shadow hand robot to master tasks like opening a door, hammering a nail, etc. This domain is chosen because it resembles real-world hand manipulation tasks. It is challenging since it expects fine-grained hand operations, making it an ideal testbed for measuring the policy adaptation capability of the agent when encountering high-dimensional, sparse reward tasks. We do not alter the operated objects (e.g., the hammer), but modify the robotic hand to comprise the following two kinds of dynamics shifts:

1. _Kinematic shift._ Akin to MuJoCo tasks, we simulate broken joints by limiting the rotation ranges of some hand joints, and there are numerous design choices to fulfill that due to the complexity of the hand robot. We modify rotation ranges of all finger joints in the index finger and the thumb, which should cause substantial troubles in completing tasks like twirling a pen, since it becomes much harder for the dexterous hand to grasp an object. ODRL involves three kinds of shift levels (_easy_, _medium_, _hard_) for individual tasks, where the rotation angle ranges of the index finger and thumb are set to be 1/2, 1/4, and 1/8 the rotation angle range of the paired source domain task, respectively.
2. _Morphology shift._ We shrink the sizes of the proximal, intermediate, and distal phalanges in the index, middle, ring, and little fingers to realize the morphological mismatch. Despite that the thumb is unmodified, it is still very challenging to perform fine-grained manipulations under such shifts. We also provide three shift levels (_easy_, _medium_, _hard_) for each task where the phalanges sizes are configured as 1/2, 1/4, and 1/8 the phalanges sizes of the source domain robot hand, respectively.

For each task in Adroit, we offer 3 environments with kinematic shifts and 3 environments with morphology shifts, yielding a total of **24** tasks. Combined with the aforementioned tasks, it is evident that ODRL contains a wide spectrum of dynamics shift tasks. Note that we consider these three domains because they are widely adopted in research-oriented RL papers [40; 43; 39; 20; 80; 47; 48; 78]. It is friendly for newcomers to quickly get familiar with the benchmark, develop new algorithms, and run experiments. The naming rule for benchmark tasks gives [domain]-[shift_type]-[shift_part (optional)]-[shift_level], e.g., _ant-friction-0.5_ denotes that the target domain has 0.5 times the friction components of the source domain, _hopper-kinematic-footjnt-medium_ means that the foot joint in _hopper_ robot is broken and the shift level gives medium. We defer the full list of ODRL task names, more task details, and visualization results to Appendix D.

### Offline Datasets

Since ODRL allows the target domain with dynamics discrepancies to be offline, we provide target domain offline datasets with distinct qualities (_random_, _medium_, _expert_) for locomotion and dexterous manipulation tasks, where the medium datasets are gathered by an early-stopped RL algorithm2 that has approximately one third or one half the performance of the expert policy. For the AntMaze domain, we only provide a mixing dataset for each map layout that contains both successful goal-reaching trajectories and unsuccessful ones. It is worth noting that we constrain the target domain offline dataset sizes (5000 transitions for locomotion and dexterous manipulation tasks, and 10000 samples for AntMaze tasks) owing to the fact that existing offline RL algorithms [39; 40; 21; 46; 20] can learn effective policies if a large amount of target domain data is available, potentially obviating the need of a source domain. It becomes difficult to train solely on the target domain dataset under such a low data regime. This is also reasonable in real-world scenarios, where collecting offline experiences can be expensive or time-consuming, and only limited data can be accessed, but sufficient data from another domain is available, e.g., a possibly biased simulator.

### Evaluation Protocol

In ODRL, we suggest two metrics for evaluating the performance of the learned policy, the achieved return and its normalized score in the _target domain_. We do not care about the performance of the agent in the source domain. The normalized score (NS) is calculated by: \(NS=-J_{r}}{J_{e}-J_{r}} 100\), where \(J_{}\) is the return acquired by the learned policy, \(J_{r}\) is the return by a random policy, and \(J_{e}\) is the return of an expert policy. We offer reference values of \(J_{r}\) and \(J_{e}\) for all tasks.

### Baseline Algorithms

We implement various off-dynamics RL algorithms and baselines in ODRL, categorized into 4 settings. **Online-Online:**_DARC_ that trains domain classifiers for penalizing source domain rewards, VGDF_ that performs source domain data filtering from a value estimate perspective, PAR_ that penalizes source domain data by measuring the representation mismatch between two domains; **Offline-Online:**_H2O_ that leverages the domain classifier for importance weighting, BC_VGDF_ and BC_PAR_ that incorporate the behavior cloning (BC) term for the source domain data; **Online-Offline:**_H2O and PAR_BC_ that introduces the BC term to the target domain data; **Offline-Offline:**_DARA_, which is exactly the offline version of DARC and BOSA  that employs two support-constrained objective for regularization.

Furthermore, we assemble the following baselines. **Online-Online:**_SAC_ that trains on data from both domains, SAC_IW that adopts the domain classifier for importance weighting, SAC_tune that first learns an SAC policy in the source domain and directly finetunes it in the target domain. **Offline-Online:**_BC_SAC_CQL_SAC_, MCQ_SAC that apply SAC loss on target domain samples and adopts BC loss, CQL_ loss, MCQ_ loss for source domain data, respectively, RLPD  that leverages random ensemble distillation and layer normalization for efficient online learning. **Online-Offline:**_SAC_BC_SAC_CQL_SAC_MCQ_ where SAC loss is applied upon source domain data training and BC loss, CQL loss and MCQ loss are integrated for target domain data. **Offline-Offline:**_IQL_ and TD3_BC_ that train on offline samples from both domains. Most of these methods are proposed to examine whether it is feasible to train a good policy by treating the two domains as one mixed domain (i.e., the transition is sampled from \(_{}= P_{}+(1-)P_{}, \{0,1\}\)).

The algorithmic details can be found in Appendix B. We reproduce all methods by following their official codes or respective papers within a _single_ file. This is motivated by the CORL  and CleanRL  projects, targeting at bringing together all core algorithm designs and making it "easy-to-hack" for practitioners. The aforementioned baseline algorithms share similar code styles in ODRL and are equipped with separate yaml configuration files for different tasks.

## 5 Experiments

In this section, we investigate the dynamics adaptation capability of the approaches involved in ODRL. We run experiments across multiple experimental settings and examine how these methods behave given different dataset qualities and domain gaps. For each task, we report the final mean performance along with the standard deviations _achieved in the target domain_ across 5 varied random seeds. The empirical evaluations are accompanied by some critical observations (**Obs**) and insights. Due to space limits, we defer wider experimental evaluations to Appendix E.

### Benchmark Results

Considering the extensive array of tasks in our benchmark, it is costly to run algorithms on all of them. To enable a comprehensive evaluation as much as possible, we opt for two locomotion tasks (_ant_, _walker2d_), each featuring 4 types of dynamic shifts (_friction_, _gravity_, _kinematic_, _morphology_), with each shift comprising two tasks (0.5/5.0 times friction/gravity, medium/hard shift levels for kinematic and morphology tasks3). This allows us to examine the performance of existing methods under varying dynamics discrepancies. Additionally, we consider AntMaze tasks with two distinct map sizes (_small_, _medium_), each involving two maze structures (_empty_, _centerblock_ for the small maze, and map type \(1\), \(2\) for the medium-size maze), to reveal the transfer capabilities of these methods across diverse landscapes and obstacles. Moreover, we include two dexterous manipulation tasks (_pen_, _door_) with two dynamic shifts (_broken-joint_, _shrink-finger_) and the _easy_ shift level to examine whether off-dynamics RL methods are also effective in high-dimensional and sparse reward tasks. We initially turn our attention to the **Online-Online** setting. Recall that we only have a limited budget of data from the target domain. Hence, we run all algorithms for 1M environmental steps in the source domain, and 0.1M steps in the target domain. The aggregated normalized score comparison of baselines is depicted in Figure 3. Based on the results, we summarize the following key observations.

**Obs I:**_No single off-dynamics RL algorithm can exhibit advantages across all scenarios._

**Obs 2:**_PAR achieves the best performance on locomotion tasks but fails on the Antmaze domain and Adroit domain._

**Obs 3:**_AntMaze tasks are extremely challenging and no algorithm can achieve meaningful returns, indicating that adapting policies across barriers is hard for state-based methods._

Figure 3: **Radar chart comparison of different methods.** We report the aggregated normalized score of tasks within each category given the online source domain and target domain.

**Obs 4**.: _Off-dynamics RL methods often fail on dexterous hand manipulation tasks. Only DARC and SAC_IW can achieve comparatively good performance on them._

These findings suggest that there is still a considerable distance to explore in developing a truly general dynamics-aware RL algorithm. It appears that current methods excel at addressing specific types of dynamics shifts. The lack of success with PAR in dexterous manipulation tasks can be credited to the poor reward penalty terms on the source domain data. In fact, the severe value overestimation phenomenon is often observed when running Adroit tasks with PAR. Importantly, although VGDF performs on par with PAR in locomotion tasks, it also falls short in the Adroit domain. This could be because VGDF requires training dynamics models of the target domain, which can be challenging for complex tasks like dexterous manipulation [48; 34; 45]. The fact that SAC_IW often results in inferior performance than DARC indicates that leveraging the learned domain classifiers for importance sampling weighting is not preferred in the online setting.

**Obs 5**.: _Surprisingly, simply training the SAC agent using both the source domain data and target domain samples can incur good performance on numerous kinds of dynamics shifts._

The last observation is quite intriguing, as it shows that merely considering the source and target domains as a single mixed domain and training on data from both domains without introducing any additional components can be effective in many tasks. The direct finetuning approach (i.e., SAC_tune) has advantages in certain tasks, but it frequently results in suboptimal performance when faced with morphology and gravity shifts.

### Performance Comparison Given Offline Datasets

We then proceed to examine the performance of baseline methods when either the source domain or the target domain is offline. This presents substantial challenges for the algorithm to learn transferable policies because (a) if the source domain is offline, finding dynamics-consistent transitions in the dataset may be difficult due to limited coverage; (b) if the target domain is offline, gathering high-quality experiences that are closely aligned with the target domain may become challenging, and poor samples from the source domain can negatively impact the learning process of the agent.

**Offline Source Domain.** We pick two tasks, _ant-gravity_ with shift level 5.0, and _walker2d-friction_ with shift level 0.5. We run experiments by leveraging three varied qualities of source domain datasets from D4RL (_medium-replay_, _medium_, _expert_). The agent undergoes training for 1M gradient steps and is allowed to interact with the target domain every 10 gradient steps, i.e., 0.1M environmental steps. We present the results in Figure 4 and have the corresponding observations.

**Obs 6**.: _A higher quality of the source domain dataset does not necessarily imply better performance in the target domain, even when an expert source domain dataset is provided._

**Obs 7**.: _Baseline methods that treat two domains as one mixed domain can achieve good performance on some tasks, sometimes even surpassing off-dynamics methods like BC_PAR, BC_VGDF, and H2O._

**Obs 8**.: _Methods that leverage the conservative value penalties (e.g., CQL_SAC) can outperform methods that involve the BC term (e.g., BC_PAR), given expert source domain datasets._

Figure 4: **Normalized score comparison of methods under distinct source domain datasets. We report the final average normalized score in the target domain, along with the standard deviation.**Offline Target Domain.We choose two tasks (_ant-kinematic-anklejnt_ with a hard shift level, and _walker2d-morph-leg_ with a medium shift level) and three types of target domain dataset qualities (_random_, _medium_, _expert_). We execute the implemented algorithms in the **Online-Offline** setting category for 500K gradient steps, allowing the agent to interact with the source domain at each step. We do not run for 1M steps since the offline dataset sizes of these tasks are limited (5000 transitions), and the agent may suffer from the overfitting issue if a large amount of gradient steps is employed. The results can be found in Figure 5, where we find that **Obs 7** and **Obs 8** still hold when the target domain is offline. We also conclude some new observations below.

**Obs 9**: _The agent's performance can also be inferior given expert target domain datasets (e.g., SAC_BC) and it is challenging to achieve a meaningful score using random target domain datasets._

**Obs 10**: _A larger dynamics gap from the source domain does not indicate that it becomes harder for the agent to learn effective policies, e.g., almost all methods can achieve quite good performance in the ant-kinematic-anklejnt task with a shift level hard._

### Connections Between Source Domain Performance and Target Domain Performance

We investigate if there is a positive relationship between the agent's performance in the source domain and the target domain. We select the _ant-friction-0.5_ task in the **Online-Online** setting, and _walker2d-kinematic-footjnt-medium_ task in the **Online-Offline** setting with a medium-quality target domain dataset. We conduct experiments on these tasks using the respective algorithms within each category. We chart the learning curves in both the source domain and the target domain, and display the results in Figure 6. We observe that VGDF demonstrates strong performance in the target domain for the _ant_ task, but its policy performance in the source domain is considerably weak. Conversely, DARC and SAC_IW learn relatively effective policies in the source domain, but they typically struggle to achieve good performance in the target domain. For other methods, they generally exhibit good performance in both domains. It is then interesting to conclude the following observation.

**Obs 11**: _There is no definitive correlation between the policy's performance in the source domain and the target domain (it depends on the specific algorithm)._

This observation is vital because it conveys that the performance in the source domain is not a reliable indicator for estimating the policy's performance in the target domain.

Figure 5: **Normalized score comparison of baselines given varied qualities of target domain datasets.** The final mean performance and its standard deviation in the target domain are presented.

Figure 6: **Return comparison across two domains on selected tasks.** The solid lines represent the mean return and the shaded region captures the standard deviation. Kine denotes kinematic.

Conclusion and Limitations

In this paper, we propose ODRL, the first benchmark for off-dynamics RL. ODRL allows the source or target domain to be either online or offline, and introduces a wide spectrum of dynamics shift tasks to facilitate a comprehensive and persuasive evaluation. We implement many off-dynamics RL algorithms within a single file to bring together core algorithm designs, and additionally propose some baselines for each experimental setting. Our empirical results show that no existing method can lead all types of dynamics shifts. We conclude some critical empirical observations, which can serve as valuable takeaways for readers. Our benchmark is promised to be actively maintained. We hope our benchmark can pave the way for developing general dynamics-aware RL algorithms.

**Limitations**. ODRL primarily supports sim-to-sim policy adaptation tasks. Adapting the policy to real-world scenarios may be more complicated (e.g., a combination of various dynamics shifts can occur). However, the proposed dynamics shifts in our study are common, and our benchmark should serve as a valuable testbed for developing more advanced off-dynamics RL algorithms.