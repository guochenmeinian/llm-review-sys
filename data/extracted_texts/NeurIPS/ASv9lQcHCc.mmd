# GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching

Haibin He\({}^{1}\)1, Maoyuan Ye\({}^{1}\)1, Jing Zhang\({}^{1}\), Juhua Liu\({}^{1}\), Bo Du\({}^{1}\), Dacheng Tao\({}^{2}\)

\({}^{1}\) School of Computer Science, National Engineering Research Center for Multimedia Software,

and Institute of Artificial Intelligence, Wuhan University, China

\({}^{2}\) College of Computing & Data Science at Nanyang Technological University

{haibinhe, yemaoyuan, liujuhua, dubo}@whu.edu.com

jingzhang.cv@gmail.com, dacheng.tao@gmail.com

[https://github.com/Hxyz-123/GoMatching](https://github.com/Hxyz-123/GoMatching)

Equal contribution, \(\) Corresponding author.

###### Abstract

Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching delivers new records on ICDAR15-video, DSText, BOVText, and our proposed novel test set with arbitrary-shaped text termed ArTVideo, which demonstrates GoMatching's capability to accommodate general, dense, small, arbitrary-shaped, Chinese and English text scenarios while saving considerable training budgets.

## 1 Introduction

Text spotting has received increasing attention due to its various applications, such as video retrieval  and autonomous driving . Recently, numerous image text spotting (ITS) methods  that simultaneously tackle text detection and recognition, have attained extraordinary accomplishment. In the video realm, video text spotting (VTS) involves a tracking task additionally. Although VTS methods  make significant progress, a substantial discrepancy persists when compared to ITS. We observe that the text recognition proficiency of VTS models is far inferior to ITS models. To investigate this issue, we compare the state-of-the-art (SOTA) VTS model TransDETR  and ITS model Deepsolo  for image-level text spotting performance on ICDAR15-video  and our established ArTVideo (_i.e._, Arbitrary-shaped Text in **Video**) test set (Sec.4.1) which comprises about 30% curved text.

As illustrated in Fig. 1(a), even when evaluating the image-level spotting performance on the VTS model's training set, the F1-score of TransDETR is only comparable to the zero-shot performance ofDeepsolo. The performance of the VTS model on ArTVideo is much worse. Moreover, there is a huge gap between the spotting and detection-only performance of the VTS model, which indicates that the recognition capability is the main bottleneck. We attribute this discrepancy to two key aspects: 1) the model architecture and 2) the training data. First, in terms of model architecture, ITS studies  have presented the advantages of employing advanced query formulation for text spotting in DETR frameworks . In contrast, existing Transformer-based VTS models still rely on Region of Interest (RoI) components or simply cropping detected text regions for recognition. On the other hand, some studies  have indicated that there exists optimization conflict in detection and association during the end-to-end training of MOTR . We hold that TransDETR , which further incorporates text recognition into MOTR-based architecture, may also suffer from optimization conflict. Second, regarding the training data, most text instances in current video datasets  are straight or oriented, and the bounding box labels are only quadrilateral, which constrains the data diversity and recognition performance as well. Overall, the limitations in model architecture and data probably lead to the unsatisfactory text spotting performance of the SOTA VTS model. Hence, _leveraging model and data knowledge from ITS presents considerable value for VTS_.

To achieve this, a straightforward approach is to take an off-the-shelf SOTA image text spotter and focus the training efforts on tracking across frames, akin to tracking-by-detection methods. An important question is how to efficiently incorporate a RoI-free image text spotter for VTS. In this paper, we propose a simple baseline via lon**G** and short term **Matching**, termed **GoMatching**, which leverages an off-the-shelf RoI-free image text spotter to identify text from each single frame and associates text instances across frames with a strong tracker.

Specifically, we select the state-of-the-art DeepSolo  as the image text spotter and design a **L**ong-**S**hort **T**erm Matching-based tracker termed **LST-Matcher**. Initially, to adapt the DeepSolo to video datasets while preserving its inherent knowledge, we freeze Deepsolo and introduce a rescoring mechanism. This mechanism entails training an additional lightweight text classifier called rescoring head via efficient tuning, and recalibrating confidence scores for detected instances to mitigate performance degradation caused by the image-video domain gap. The final score for each instance is determined by a fusion operation between the original score provided by the image text spotter and the calibrated score acquired from the rescoring head. The identified text instances are then sent to LST-Matcher for association. LST-Matcher can effectively harnesses both long- and short-term information, making it a highly capable tracker. As a result, our baseline significantly surpasses existing SOTA methods by a large margin with much lower training costs, as shown in Fig. 1(b).

In summary, the contribution of this paper is threefold. **1)** We identify the limitations in current VTS methods and propose a novel and simple baseline, which leverages an off-the-shelf image text spotter with a strong customized tracker. **2)** We introduce the rescoring mechanism and long-short term matching module to adapt image text spotter to video datasets and enhance the tracker's capabilities. **3)** We establish the ArTVideo test set for addressing the absence of curved texts in current video datasets and evaluating the text spotters on videos with arbitrary-shape text. Extensive

Figure 1: (a) ‘Gap between Spot. & Det.’: the gap between spotting and detection F1-score. As the spotting task involves recognizing the results of the detection process, the detection score is indeed the upper bound of spotting performance. The larger the gap, the poorer the recognition ability. Compared to the ITS model (Deepsolo ), the VTS model (TransDETR ) presents unsatisfactory image-level text spotting F1-scores, which lag far behind its detection performance, especially on ArTVideo with curved text. It indicates recognition capability is a main bottleneck in the VTS model. (b) GoMatching outperforms TransDETR by over 12 MOTA on ICDAR15-video while saving 197 training GPU hours and 10.8GB memory. Notice that since the pre-training strategies and settings vary between TransDETR and GoMatching, the comparison is focused on the fine-tuning stage.

experiments on public challenging datasets and the established ArTVideo test set demonstrate the effectiveness of our baseline and its outstanding performance with less training budgets. For example, GoMatching achieves the highest ranking on ICDAR15-video and DSText. Especially on bilingual dataset BOVText, GoMatching obtains a 45% improvement on MOTA compared to the recorded best performance . On curved text dataset ArTVideo, GoMatching also surpasses previous SOTA method  by a substantial margin.

## 2 Related Works

### Multi-Object Tracking

Multi-object tracking methods follow the tracking-by-detection (TBD) or tracking-by-query-propagation (TBQP) pipeline. TBD methods [20; 21; 22] employ detectors for localization and then use association algorithms to get object trajectories. Different from extending tracks frame-by-frame, GTR  proposes to generate entire trajectories at once in Transformer. TBQP paradigm extends query-based object detectors[14; 15] to tracking. MOTR  detects object locations and serially updates its tracking queries for detecting the same items in the following frames, achieving an end-to-end solution. However, MOTR suffers from optimization conflict between detection and association [16; 17], resulting in inferior detection performance. For the VTS task which additionally involves text recognition, a naive way of training all modules end-to-end may also lead to optimization conflict. In contrast, we explore inheriting prior knowledge of text spotting from ITS models while focusing on the tracking task.

### Image Text Spotting

Early approaches [24; 25; 26] crafted RoI-based modules to bridge text detection and recognition. However, these methods ignored one vital issue, _i.e._, the synergy problem between the two tasks. To overcome this dilemma, recent Transformer-based methods [27; 3; 28; 6] get rid of the fetters of RoI modules, and chase a better representation for the two tasks. For example, DETR-based TESTR  uses two decoders for each task in parallel. In contrast, DeepSolo  proposes a unified and explicit query form for the two tasks, without harnessing dual decoders. However, the above methods cannot perform tracking in the video.

### Video Text Spotting

Compared to ITS, existing SOTA VTS methods still rely on RoI for recognition. CoText  adopts a lightweight text spotter with Masked-RoI, then uses several encoders to fuse features derived from the spotter, and finally feeds them to a tracking head with cosine similarity matching. TransDETR  performs detection and tracking under the MOTR paradigm and then uses Rotated-RoI to extract features for the subsequent recognizer. They pursue training all modules in an end-to-end manner. In comparison, we explore how to efficiently turn a RoI-free ITS model into a VTS one. We reveal the probability of freezing off-the-shelf ITS part and focusing on tracking, thereby saving training budgets while reaching SOTA performance.

## 3 Methodology

### Overview

The architecture of GoMatching is presented in Fig. 2. It consists of a frozen image text spotter, a rescoring head, and a Long-Short Term Matching module (LST-Matcher). We adopt an outstanding off-the-shelf image text spotter (_i.e._, DeepSolo) and freeze its parameters, with the aim of introducing strong text spotting capability into VTS while significantly reducing training cost. In DeepSolo, there are \(p\) sequences of queries used for final predictions, with each storing comprehensive semantics for a text instance. To alleviate spotting performance degradation caused by the image-video domain gap, we devise a rescoring mechanism, which determines the confidence scores for text instances by considering both the scores from the image text spotter and a new trainable rescoring head. Finally, we design LST-Matcher to generate instance trajectories by leveraging long-short term information.

### Rescoring Mechanism

Owing to the domain gap between image and video datasets, employing a frozen image text spotter for direct prediction may result in relative low recall due to low text confidence, further leading to a reduction in end-to-end spotting performance. To ease this issue, we devise a rescoring mechanism via a lightweight rescoring head and a simple score fusion operation. Specifically, the rescoring head is designed to recompute the score for each query from the decoder in the image text spotter. It consists of a simple linear layer and is initialized with the parameters of the image text spotter's classification head. The score fusion operation then decides the final scores by considering both the scores from the image text spotter and the rescoring head. Let \(C^{t}_{o}=\{c^{t}_{o_{1}},...,c^{t}_{o_{p}}\}\) be a set of original scores produced by image text spotter in frame \(t\). \(C^{t}_{r}=\{c^{t}_{r_{1}},...,c^{t}_{r_{p}}\}\) is a set of recomputed scores obtained from the rescoring head. We obtain the maximum value for each query as the final score, denoted as \(C^{t}_{f}=\{c^{t}_{f_{1}}=max(c^{t}_{o_{1}},c^{t}_{r_{1}}),...,c^{t}_{f_{p}}=max (c^{t}_{o_{p}},c^{t}_{r_{p}})\}\). With final scores, the queries in frames are filtered by a threshold before being sent to LST-Matcher for association.

### Long-Short Term Matching Module

Long-short term matching module (LST-Matcher) consists of two sub-modules: the Short Term Matching module (ST-Matcher) and the Long Term Matching module (LT-Matcher), which own the same structure. ST-Matcher is steered to match simple instances between adjacent frames into trajectories, while LT-Matcher is responsible for using long term information to address the unmatched instances due to severe occlusions or strong appearance changes. Each of them contains a one-layer Transformer encoder and a one-layer Transformer decoder . We use a simple multi-layer perceptron (MLP) to map the filtered text instance queries into embeddings as the input, getting rid of using RoI features as in most existing MOT methods. In the encoder, historical embeddings are enhanced by self-attention. The decoder takes embeddings in the current frame as query and enhanced historical embeddings as key for cross-attention, and computes the association score matrix. The current instances are then linked to the existing trajectories composed of historical embeddings or generate new trajectories according to the association score matrix.

Figure 3: The inference pipeline of LST-Matcher, which is a two-stage association process: (1) ST-Matcher associates the instances with trajectories in previous frames as denoted by blue lines. (2) LT-Matcher associates the remaining unmatched instances by utilizing other trajectories in history frames as denoted by red lines.

Figure 2: **The overall architecture of GoMatching**. The frozen image text spotter provides text spotting results for frames. The rescoring mechanism considers both instance scores from the image text spotter and a trainable rescoring head to reduce performance degradation due to the domain gap. Long-short term matching module (LST-Matcher) assigns IDs to text instances based on the queries in long-short term frames. The yellow star sign ‘\(}\)’ indicates the final output of GoMatching.

To be specific, supposing a given clip including \(T\) frames and \(N_{t}\) text instances in frame \(t\) after threshold filtering. \(Q^{t}=\{q_{1}^{t},...,q_{N_{t}}^{t}\}\) is the set of text instance queries in frame \(t\). Initially, we use a two-layer MLP to map these frozen queries into embeddings \(E^{t}=\{e_{1}^{t},...,e_{N_{t}}^{t}\}\). The set of embeddings in all frames is denoted as \(E^{L}=E^{1}... E^{T}\). Let the universal set of embeddings in adjacent frames of the input batch be denoted as \(E^{S}=E^{S_{2}} E^{S_{3}}... E^{S_{T}}\) and \(E^{S_{t}}=E^{t-1} E^{t}\). Based on the predictions of image text spotter, we obtain their corresponding bounding boxes \(B^{t}=\{b_{1}^{t},...,b_{N_{t}}^{t}\}\). Let \(=\{_{1},...,_{K}\}\) be the set of ground-truth (GT) trajectories of all instances in the clip, where \(_{k}=\{_{k}^{t},...,_{k}^{t}\}\) describes a tube of instance locations \(_{k}^{t}^{4}\{\}\) through time. \(_{k}^{t}=\) means the absence of instance \(k\) in frame \(t\). Let \(_{k}^{t}\) be the matched instance index for \(_{k}^{t}\) according to the following equation:

\[_{k}^{t}=\{,&_{k}^{t}=_{i}((b_{i}^{t},_{k}^{t}))<0.5\\ _{i}((b_{i}^{t},_{k}^{t})),& .. \]

ST-Matcher calculates a short-term association score \(v_{i}^{t}(e_{_{k}^{t}}^{t},E^{S_{t}})^{N_{S_{t}}}\) for \(i\)-th instances in frame \(t\), where \(e_{_{k}^{t}}^{t}^{D}\) is a trajectory query and \(N_{S_{t}}=N_{t}+N_{t-1}\). LT-Matcher calculates a long-term trajectory-specific association score \(u_{i}^{t}(e_{k},E^{L})^{N}\) for \(i\)-th instances in frame \(t\), where \(e_{k}\{e_{_{k}^{1}}^{1},e_{_{k}^{2}}^{2},...,e_{_{k}^{T}}^{T}\},N=_{t=1}^{T}N_{t}\). Specifically, when \(v_{i}^{t}(e_{_{k}^{t}}^{t},E^{S_{t}})=0\) and \(u_{i}^{t}(e_{k},E^{L})=0\), it means no association at time \(t\). Then, ST-Matcher and LT-Matcher can predict distributions of short-term and long-term associations for all instance \(i\) in frame \(t\) which can be written as:

\[P_{s_{a}}(e_{_{k}^{t}}^{t},E^{S_{t}}) =^{t}(e_{_{k}^{t}}^{t},E^{S_{t}}))}{ _{j\{,1,...,N_{t}\}}(v_{j}^{t}(e_{_{k}^{t}}^{ t},E^{S_{t}}))}, \] \[P_{l_{a}}(e_{k},E^{L}) =^{t}(e_{k},E^{L}))}{_{j\{,1,...,N _{t}\}}(u_{j}^{t}(e_{k},E^{L}))}. \]

To ensure sufficient training of ST-Matcher and LT-Matcher, embeddings set \(E^{S}\) and \(E^{L}\) are fed into ST-Matcher and LT-Matcher during training, respectively.

During inference, we engage a memory bank to store the instance trajectories from \(H\) history frames for long term association. All filtered instances in each frame are further processed by non-maximum-suppression (NMS) before being fed into LST-Matcher for association. Unlike the training phase, where ST-Matcher and LT-Matcher are independent of each other, LST-Matcher comprises a two-stage associating procedure as described in Fig. 3. Concretely, ST-Matcher first matches the embedding \(E^{t}\) in the current frame \(t\) with the trajectories \(^{t-1}\) in the previous frame \(t-1\). Then, LT-Matcher employs other trajectories \(_{oth}^{H}\) in the memory bank to associate the unmatched ones \(E_{s\_u}^{t}\) with low association score in ST-Matcher caused by the heavy occlusion or strong appearance changes. If the association score with any trajectory calculated in ST-Matcher or LT-Matcher is higher than a threshold \(\), the instance is linked to the trajectory with the highest score. Otherwise, this instance is used to initiate a new trajectory. Finally, we combine the trajectories \(_{s}\) and \(_{l}\) predicted by ST-Matcher and LT-Matcher to obtain new trajectories \(^{N}\) for tracking in the next frame.

### Optimization

**Rescoring Loss.** To train the rescoring head, we following DETR  and use Hungarian algorithm  to find a bipartite matching \(\) between the prediction set \(\) and the ground truth set \(Y\) with minimum matching cost \(\):

\[=_{}_{i}^{N}(Y_{i},_{(i) }), \]

where \(N\) is the number of ground truth instances per frame. The cost \(\) can be defined as:

\[(Y_{i},_{(i)})=_{c}_{cls}(_{ (i)}(c_{i}))+_{b}_{1}^{N}\|b_{i}-_{(i)}\|, \]

where \(_{c}\) and \(_{b}\) serve as the hyper-parameters to balance different tasks. \(_{(i)}(c_{i})\) and \(_{(i)}\) are the probability for ground truth class \(c_{i}\) and the predicition of bounding box respectively, and represents the ground truth bounding box. \(_{cls}\) is the focal loss . Specifically, the focal loss for training the rescoring head can be formulated as:

\[_{res}=_{1}^{N}[-_{\{c_{i}\}}(1- _{(i)}(c_{i}))^{}(_{(i)}(c_{i}) )-_{\{c_{i}=\}}(1-)(_{(i)}(c_{i} ))^{}(1-_{(i)}(c_{i}))], \]

where \(\) and \(\) are the hyper-parameters of focal loss.

**Long-Short Association Loss.** In ST-Matcher, we only consider each trajectory in the universal set of adjacent frames, while in LT-Matcher we consider each trajectory in all long term frames. For each trajectory, we optimize the log-likelihood of its assignments \(_{k}\) following GTR :

\[_{s\_ass}(E^{S},_{k})=-_{t=2}^{T} P_{s_{a}}( {}_{k}^{t}|e_{_{k}^{t}}^{t},E^{S_{t}}), \]

\[_{l\_ass}(E^{L},_{k})=-_{w}_{t=1}^{T} P_{l_{ a}}(_{k}^{t}|E_{_{k}^{w}}^{w},E^{L}), \]

where \(w\{1,...,T|_{k}^{w}\}\).

In ST-Matcher and LT-Matcher, empty trajectories would be generated for these unassociated queries, and their optimization goals can be defined as:

\[_{s\_bg}(E^{S})=-_{j:_{k}^{t}=j}_{t=2}^ {T} P_{s_{a}}(^{t}=|e_{j}^{t},E^{S_{t}}), \]

\[_{l\_bg}(E^{L})=-_{w=1}^{T}_{j:_{k}^{w} =j}_{t=1}^{T} P_{l_{a}}(^{t}=|E_{j}^{w},E^{L}). \]

Finally, we can get the long-short association loss as follows:

\[_{asso}=_{s\_bg}+_{l\_bg}+_{_{k }}(_{s\_ass}+_{l\_ass}). \]

**Overall Loss.** Combined with the rescore loss \(_{res}\) in Eq. (6) and the long-short association loss \(_{asso}\) in Eq. (11), the final training loss can be defined as:

\[=_{res}_{res}+_{asso}_{asso}, \]

where the hyper-parameters \(_{res}\) and \(_{asso}\) are the weights of \(_{res}\) and \(_{asso}\), respectively.

## 4 Experiments

### Datasets and Evaluation Metrics

**ICDAR15-video** is a word-level video text reading benchmark annotated with quadrilateral bounding boxes, comprising a training set of 25 videos and a test set of 24 videos. It focuses on wild scenarios, such as driving on the road, exploring shopping streets, walking in a supermarket, _etc_.

**BOVText** is a large-scale, bilingual, and open-world benchmark for video text spotting, encompassing English and Chinese. The dataset is meticulously collected from _YouTube_ and _KuaiShou_ with different scenarios. The text box annotations are represented as quadrilaterals at the textline level.

**DSText** is a newly proposed dataset, and focuses on dense and small text reading challenges in the video. This dataset provides 50 training videos and 50 test videos. Compared with the previous datasets, DSText mainly includes the following three new challenges: dense video texts, high-proportioned small texts, and various new scenarios, _e.g._, 'Game', 'Sports', _etc_. Similar to ICDAR15-video, DSText adopts word-level annotations, which are labeled with quadrilaterals.

**ArTVideo** is a novel word-level test set established in this work to evaluate the performance of arbitrary-shaped video text, which is absent in the VTS community. It contains 20 videos with about 30% curved text instances. Straight text is annotated with quadrilaterals, while curved text is annotated with polygons. More details are provided in the Appendix A.

**Evaluation Metrics.** To evaluate performance, we adopt three evaluation metrics commonly used in ICDAR15-video competition and DSText competition, including MOTA , MOTP, and IDF1 .

### Implementation Details

In all experiments, we only use a single NVIDIA GeForce RTX 3090 (24G) GPU to train and evaluate GoMatching. As for the image text spotter in GoMatching, we apply the officially released DeepSolo . During fine-tuning GoMatching on downstream video datasets, we only update the rescoring head and LST-Matcher, while keeping DeepSolo frozen. More inference settings can be seen in the Appendix B.

**Training Setting.** The text spotting part of GoMatching is initialized with off-the-shelf DeepSolo weights and kept frozen in all experiments. We optimize other modules on video datasets. We follow EfficientDet  to adopt the scale-and-crop augmentation strategy with a resolution of 1280. The batch size \(T\) is 6. All frames in a batch are from the same video. Text instances with fusion scores higher than 0.3 are fed into the LST-Matcher during training. AdamW  is used as the optimizer. We adopt the warmup cosine annealing learning rate strategy with the initial learning rate being set to 0.00005. The loss weights \(_{res}\) and \(_{asso}\) are set to 1.0 and 0.5, respectively. For focal loss, \(\) is 0.25 and \(\) is 2.0 as in [14; 5]. The model is trained for 30k iterations on all downstream video datasets.

### Comparison with State-of-the-art Methods

**Results on ICDAR15-video.** To evaluate the effectiveness of GoMatching on oriented video text, we conduct a comparison with the state-of-the-art methods on ICDAR15-video presented in Table 0(a). As can be seen, GoMatching ranks first in all metrics on the ICDAR15-video leaderboard. By effectively combining a robust image text spotter with a strong tracker, GoMatching improves the best performance by 5.08% MOTA, 0.75% MOTP, and 3.16% IDF1, respectively. Furthermore, owing to the substantial enhancement in recognition and tracking capabilities (details can be found in Sec. 4.4 and Appendix G), GoMatching outperforms the current SOTA single-model method TransDETR by 11.08% MOTA, 3.92% MOTP, and 7.31% IDF1, respectively.

**Results on BOVText.** Except for the English word recognition scenario, GoMatching can readily adapt to other video text recognition scenarios, such as Chinese text line recognition. For BOVText, which focuses on English and Chinese textline recognition, we employ the DeepSolo trained on bilingual textline datasets and then fine-tune GoMatching on BOVText. The results are presented in Table 0(b). It is evident that GoMatching achieves a new record on the BOVText dataset and surpasses previous methods significantly. GoMatching exhibits superior performance over the previous SOTA method CoText , with improvements of 41.5% on MOTA, 6.9% on MOTP, and 14.3% on IDF1. Such exceptional performance of GoMatching on BOVText suggests its proficiency in spotting both Chinese and English text in videos. Moreover, it can be easily extended to other languages by adapting the image spotter.

**Results on DSText.** We further conduct experiments on DSText with dense and small video text scenarios. Results are presented in Table 0(c). It is worth noting that most of the previous methods on the DSText leaderboard used an ensemble of multiple models and large public datasets to enhance their performance . For example, _TencentOCR_ integrates the detection results of DBNet  and Cascade MaskRCNN  built with multiple backbone architectures, combines the Parseq  text recognizer, and further improves the end-to-end tracking with ByteTrack . _DA_ adopts Mask R-CNN  and DBNet to detect text, then uses BotSORT  to replace the tracker in VideoTextSCM  and employs the Parsec model for recognition. As a single model with a frozen image text spotter, GoMatching also shows competitive performance compared to other ensembling methods on the leaderboard. GoMatching ranks first (22.83%) on MOTA, second (80.43%) on MOTP, and third (46.09%) on IDF1. Moreover, compared to the SOTA single-model method, GoMatching achieves substantial improvements of 45.46% and 19.66% on MOTA and IDF1, respectively.

**Results on ArTVideo.** We test TransDETR and GoMatching on ArTVideo to compare the zero-shot text spotting capabilities for arbitrary-shaped text. For a fair comparison, both TransDETR and GoMatching are trained on ICDAR15-video. Unlike ICDAR15-video and DSText which only have straight text, ArTVideo has a substantial number of curved text, so we report results under four settings: tracking results on both straight and curved text, spotting results on both straight and curved text, tracking results on curved text only, and spotting results on curved text only. As shown in Table 0(d), GoMatching outperforms TransDETR under all settings. Especially when involving an additional recognition task (end-to-end spotting) or only considering curved text, the performance advantages of GoMatching are more significant. This further confirms that the previous SOTAmethods have unsatisfactory recognition capabilities and limited adaptability to complex scenarios. Furthermore, as shown in Fig. 1(b), GoMatching achieves excellent performance while significantly reducing the training budget.

Some visual results are provided in Fig. 4. It shows that GoMatching performs well on straight and curved text, and even more complex scene text. More visual results (including some failure cases) and analysis are provided in the Appendix G.

### Ablation Studies

We first conduct comprehensive ablation studies on ICDAR15-video to verify the effectiveness of each component. The experimental results are shown in Table 2. The impact of frame length on long-term association during inference is then studied, and the results are shown in Appendix C.

**Effectiveness of Utilizing Queries.** Comparing the first two rows in Table 2, we can find that using queries from the decoder of image text spotter is more beneficial for tracking than RoI features. By leveraging the unified queries from frozen DeepSolo, 0.98% and 1.05% improvements on MOTA and IDF1 are achieved. This is because queries integrate more text instance information, _i.e._, unifying multi-scale features, text semantics, and position information, which has been proven effective in DeepSolo. Although position information is essential for tracking, it is ignored in RoI features.

**Effectiveness of Rescoring Mechanism.** To verify the effectiveness of the rescoring mechanism, we test three different scoring mechanisms: the original score from DeepSolo, the score recomputed by the rescoring head, and the fusion score from the rescoring mechanism. As shown in row 2 and row 3 of Table 2, the rescoring head can alleviate the performance degradation caused by the domain gap between ICDAR15-image and ICDAR15-video, and achieve gains of 1.25% and 0.97% on MOTA and IDF1, respectively. Moreover, as shown in row 4, we can observe that combining the knowledge of rescoring head learned from the new dataset with the prior knowledge of DeepSolo can further improve MOTA and IDF1 by 0.33% and 0.32%, respectively. Appendix F contains more results.

**Effectiveness of LST-Matcher.** In this part, we conduct three experiments to prove the effectiveness of the LST-Matcher. As shown in row 4 of Table 2, we only use LT-Matcher to associate high-score text instances in the current frame with trajectories in the tracking memory bank. In row 5, we only use ST-Matcher to associate high-score text instances in the current frame with trajectories of the previous frame. In addition, as shown in row 6, we employ both LT-Matcher and ST-Matcher to test LST-Matcher. We can easily observe that compared to LT-Matcher, LST-Matcher improves MOTA

Table 1: **Comparison results with SOTA methods on four distinct datasets. \({}^{}\)\({}^{}\) denotes that the results are collected from the official competition website. \({}^{*}\)*†: we use the officially released model for evaluation. \({}^{}\)M-ME’ indicates whether multi-model ensembling is used. \({}^{}\)Y’ and \({}^{}\)N’ stand for yes and no. The best and second-best results are marked in bold and underlined, respectively.**and IDF1 by 1.72% and 1.29% respectively, while compared to ST-Matcher, the improvement on MOTA and IDF1 are 1.12% and 5.1%, respectively. In Fig. 5, we also demonstrate that using LST-Matcher can effectively mitigate the issue of ID switches caused by the strong appearance changes due to motion blur. These results validate that combining short-term and long-term information leads to more robust tracking outcomes, thereby enhancing the performance of video text spotting.

**Different training strategies.** To investigate the impacts of different training strategies on GoMatching, we establish three distinct settings on the ICDAR15-video dataset, as shown in Table 3. 1) We only fine-tune the tracker while keeping the image text spotter frozen (the first row of Table 3). 2) We first fine-tune the image text spotter on images extracted from ICDAR15-video and then further fine-tune the tracker with the image text spotter fixed (the second row of Table 3). 3) We jointly train the spotter's decoder and tracker of GoMatching while trying different learning rates for the decoder (the last three rows of Table 3). As shown in the first two rows of Table 3, fine-tuning the image spotter on the downstream video dataset results in a performance decline compared to the default setting. This is due to two data-related factors: **1)** minor variations in text content between frames in the same video lead to insufficient data diversity, causing the image text spotter to overfit more easily; and **2)** image blurring from camera motion reduces the quality of data available for training the image text spotter. When the image text spotter and tracker are trained simultaneously (the last three rows), the model's performance significantly decreases. Even with the decoder's learning rate close to zero (_i.e., 0.001_), there is still a 1.13% drop in IDF1. As the decoder's learning rate increases, the performance decline becomes more pronounced. This indicates that naive joint optimization of text spotting and tracking is challenging, likely due to conflicts between the two tasks. In future work,

  Index & Query & Scoring & LT-Matcher & ST-Matcher & MOTA (\(\)) & MOTP (\(\)) & IDF1 (\(\)) \\ 
1 & & O & ✓ & & 66.20 & 78.52 & 75.07 \\
2 & ✓ & O & ✓ & & 67.22 & 78.54 & 76.12 \\
3 & ✓ & R & ✓ & & 68.47 & 78.29 & 77.09 \\
4 & ✓ & F & ✓ & & 68.80 & 78.24 & 77.41 \\
5 & ✓ & F & ✓ & ✓ & 69.40 & **78.34** & 73.60 \\
6 & ✓ & F & ✓ & ✓ & **70.52** & 78.25 & **78.70** \\  

Table 2: Impact of difference components in the proposed GoMatching. ‘Query’ indicates that LST-Matcher employs the queries of high-score text instances for association, otherwise RoI features. Column ‘Scoring’ indicates the employed scoring mechanism, in which ‘O’ means using the original scores from DeepSolo, ‘R’ means using the scores recomputed by the rescoring head, and ‘F’ means using the fusion scores obtained from the rescoring mechanism.

Figure 4: **Visual results of video text spotting. Images from top to bottom are the results on ICDAR15-video, BOVText, DSText, and ArTVideo, respectively. Text instances belonging to the same trajectory are assigned the same color.**

it is worth trying to establish larger, more diverse video text spotting datasets and to explore more effective multi-task optimization strategies.

## 5 Conclusion

In this paper, we propose a simple yet strong baseline, termed GoMatching, for video text spotting. GoMatching harnesses the talent of an off-the-shelf query-based image text spotter and only needs to tune a lightweight tracker, effectively addressing the limitations of previous SOTA methods in recognition. Specifically, we design the rescoring mechanism and LST-Matcher to adapt the image text spotter to unseen video datasets while empowering GoMatching with excellent tracking capability. Moreover, we establish a novel test set ArTVideo for the evaluation of video text spotting models on arbitrary-shaped text, filling the gap in this area. Experiments on public benchmarks and ArTVideo demonstrate the superiority of our GoMatching in terms of both spotting accuracy and training cost.