# BOTS: Batch Bayesian Optimization of

Extended Thompson Sampling

for Severely Episode-Limited RL Settings

 Karine Karine

University of Massachusetts Amherst, USA

karine@cs.umass.edu

&Susan A. Murphy

Harvard University, USA

samurphy@g.harvard.edu

&Benjamin M. Marlin

University of Massachusetts Amherst, USA

marlin@cs.umass.edu

###### Abstract

In settings where the application of reinforcement learning (RL) requires running real-world trials, including the optimization of adaptive health interventions, the number of episodes available for learning can be severely limited due to cost or time constraints. In this setting, the bias-variance trade-off of contextual bandit methods can be significantly better than that of more complex full RL methods. However, Thompson sampling bandits are limited to selecting actions based on distributions of immediate rewards. In this paper, we extend the linear Thompson sampling bandit to select actions based on a state-action utility function consisting of the Thompson sampler's estimate of the expected immediate reward combined with an action bias term. We use batch Bayesian optimization over episodes to learn the action bias terms with the goal of maximizing the expected return of the extended Thompson sampler. The proposed approach is able to learn optimal policies for a strictly broader class of Markov decision processes (MDPs) than standard Thompson sampling. Using an adaptive intervention simulation environment that captures key aspects of behavioral dynamics, we show that the proposed method can significantly out-perform standard Thompson sampling in terms of total return, while requiring significantly fewer episodes than standard value function and policy gradient methods.

## 1 Introduction

There is an increasing interest in using reinforcement learning methods (RL) in the healthcare setting, including in mobile health Coronato et al. (2020); Yu et al. (2021); Liao et al. (2022). However, the healthcare domain presents a range of challenges for existing RL methods. In mobile health, each RL episode typically corresponds to a human subjects trial involving one or more participants that requires substantial time to carry out (weeks to months) and can incur significant cost. As a result, methods that require many episodes are usually not feasible (Williams, 1987; Mnih et al., 2013).

Within the mobile health research community specifically, adaptive intervention policy learning methods have addressed severe episode count restrictions imposed by real-world research constraints by focusing on the use of contextual bandit algorithms (Tewari and Murphy, 2017). By focusing on maximizing immediate reward, bandit algorithms have the potential to provide an improved bias-variance trade-off compared to policy gradient and state-action value function approaches (Lattimoreand Szepesvari, 2017). Linear Thompson sampling (TS) bandits are a particularly promising approach due to the application of Bayesian inference to capture model uncertainty due to data scarcity in the low episode count setting (Agrawal and Goyal, 2013).

Of course, the main drawback of bandit-like algorithms is that they select actions based on distributions of immediate rewards, thus they do not account for long term consequences of present actions (Chapelle and Li, 2011). This can lead to sub-optimal performance in real world applications where the environment corresponds to an arbitrary MDP, referred to as the "full RL" setting.

In this paper, we propose an approach to extending the linear TS bandit such that actions are selected using a state-action utility function that includes an action bias term learned across episodes using Bayesian Optimization (BO) applied to the expected return of the extended Thompson sampler. This approach retains much of the bias-variance trade-off of the classical TS bandit while having the ability to provide good performance for a strictly larger set of MDPs than the classical TS bandit.

Further, we explore the use of batch Bayesian optimization methods, including local methods, that enable multiple episodes to run in parallel while exploring the space of action bias terms in a principled manner. The use of batch BO in our target application domain is critical since real adaptive intervention studies must support multiple simultaneous participants in order to satisfy constraints on total study duration. To improve the BO exploration, we also investigate setting the TS prior parameters using a small micro-randomized trial (MRT).

We explore the above issues in the context of a recently proposed physical activity intervention simulation where the reward is in terms of step count and the actions correspond to the selection of different forms of contextualized motivational messages (Karine et al., 2023). The simulation captures key aspects of the physical activity intervention domain including a habituation process affected by treatment volume and a disengagement process affected by context inference errors.

Our results show that optimizing action bias terms using batch BO and selecting actions using the proposed utility function leads to an extended Thompson sampler that outperform standard TS and full RL methods. Using local batch BO instead of global batch BO can further improve the performance. Moreover, modeling the reward variance in addition to the action bias terms can provide further mean performance improvements in some settings. These results suggest that our proposed approach has the potential to enhance the treatment efficacy of TS-based adaptive interventions.

**Our main contributions are:** (1) We introduce a novel extended Thompson sampling bandit model and learning algorithm that can solve a broader class of MDPs than standard Thompson sampling. (2) We show that the proposed extended Thompson sampler outperforms a range of RL baseline methods in the severely episode-limited setting. (3) We provide an implementation of the proposed approach. The code is available at: github.com/reml-lab/BOTS.

## 2 Methods

In this section, we describe our extension to Thompson sampling, our approach to optimizing the additional free parameters introduced, and the JITAI simulation environment. We describe the related work in Appendix A.1.

**Extended Thompson Sampling (xTS).** Our primary goal is to reduce the myopic nature of standard TS for contextual bandits when applied in the episodic MDP setting while maintaining low variance. Our proposed extended Thompson sampling approach is based on selecting actions according to a state-action utility that extends the linear Gaussian reward model used by TS as shown below.

\[u_{ta} =r_{ta}+_{a}\] (1) \[p(r_{ta}|a,_{t}) =(r_{ta};_{ta}^{}_{t},_{Ya}^ {2})\] (2) \[p(_{ta}|_{ta},_{ta}) =(_{ta};_{ta},_{ta})\] (3)

where at each time \(t\), \(_{t}\) is the state (or context) vector, \(r_{ta}\) is the reward for taking action \(a\), \(_{ta}\) is a vector of weights for action \(a\). We refer to the additional term \(_{a}\) included in the utility as the _action bias_ for action \(a\). The action bias values are fixed within each episode of xTS, but are optimized across episodes to maximize the total return of the resulting policy \(_{xTS}\). The policy \(_{xTS}\) is similar in form to that of standard TS except that actions are selected according to the probability that they have the highest expected utility. The policy \(_{}\) maintains the same posterior distribution over immediate rewards used by standard TS. We provide the pseudo code in Algorithm 2 in Appendix A.2.3.

The basic intuition for the xTS model is that the action bias parameters enable penalizing or promoting the selection of each action \(a\) (regardless of the current state), based on action \(a\)'s average long term consequences effect on the total return. The inclusion of action bias parameters in the policy \(_{}\) provides access to a strictly larger policy space than standard TS. At the same time, the number of additional parameters introduced by xTS is low, enabling the resulting approach to retain relatively low variance when learning from a limited number of episodes.

We note that in adaptive health interventions, it can be the case that the long term consequences of specific actions taken in different state have similar effect on the total return. For example, sending a message in a messaging-based adaptive health intervention will typically have a positive short-term impact, but this action will also increase habituation. Increased habituation then typically results in lower reward for some number of future time steps. Our proposed approach has the potential to result in an improved policy in such settings relative to TS, by recognizing that the long term consequences of actions on the total return may be mismatched with the expected immediate reward that they yield.

**Bayesian Optimization for xTS.** We now turn to the question of how to select the action bias values \(_{a}\) to optimize the xTS policy. Let \(R=_{t=1}^{T}r_{t}\) represent the total reward for an episode, where \(r_{t}\) is the observed reward at time \(t\), and \(_{}[R]\) represent the expected return when following policy \(\). Letting vectors \(=\{_{a}\}_{0:A}\), \(_{0}=\{_{0a}\}_{0:A}\), \(_{0}=\{_{0a}\}_{0:A}\), and \(_{Y}^{2}=\{_{Ya}^{2}\}_{0:A}\), we propose to select \(\) to maximize the expected return of the policy \(_{}(,_{0},_{0},_{Y}^{2})\):

\[_{}=_{}\ _{_{}(,_{0}, _{0},_{Y}^{2})}[R]\] (4)

To solve this optimization problem, we propose the use of batch Bayesian optimization. The target function is the expected return of the xTS policy with respect to the \(\) parameters. We begin by defining a schedule of batch sizes \(B_{i}\) for rounds \(i\) from \(0\) to \(R\). On each round \(i\), we use a batch acquisition function to select a batch of \(B_{i}\) values of action bias parameters \(_{i1},...,_{iB_{i}}\) based on the current Gaussian process approximation to the expected return function. We run one episode of xTS using the policy \(_{}(_{ib},_{0},_{0},_{Y}^{2})\) for \(1 b B_{i}\) and obtain the corresponding returns \(R_{ib}\). All episodes in round \(i\) are run in parallel. When all episodes in round \(i\) are complete, we use the \(B_{i}\) pairs \((_{ib},R_{ib})\) to update the Gaussian process. We provide the pseudo code in Appendix Algorithm 1.

We perform experiments using several configurations of this algorithm. To start the BO procedure, we apply Sobol sampling to obtain an initial set of \(\) values. On later rounds, we typically apply the qEI acquisition function. When applying the acquisition function, we consider both unconstrained and local optimization. This choice corresponds to using global or local BO (e.g., TuRBO) (Balandat et al., 2020; Eriksson et al., 2019).

When using a global BO method to optimize xTS, this approach can represent optimal policies for any MDP where standard TS can represent an optimal policy, simply by learning to set \(_{a}=0\) for all \(a\). Moreover, our approach can also represent optimal policies for MDPs where the optimal action in each state corresponds to the action with the highest expected utility under some setting of the action bias parameters \(\). This is a strictly larger set of MDPs than can be solved using standard TS. However, it is clearly also strictly smaller than the set of all MDPs since the action bias terms are state independent, while the expected value of the next state in the optimal state-action value function \(Q^{*}(,a)\) depends on the current state.

**Setting Thompson Sampling Parameters.**

As for typical TS, xTS requires that a prior \((_{a};_{0a},_{0a})\) be specified. This prior can be set by hand using domain knowledge or hypothesized relationships between the immediate reward and the state variables. In the absence of strong domain knowledge, the prior can be set broadly to reflect this lack of prior knowledge. Finally, in the adaptive intervention domain, this prior is often set by applying Bayesian linear regression to a small number of episodes with randomized action selection referred to as a _micro-randomized trial_ (MRT). Note that these episodes can also be performed in parallel.

When learning xTS over multiple rounds, we can fix the same reward model prior for all rounds, or update the prior from round to round using Bayesian linear regression. We call these strategiesTS(Fixed) and TS(Update) respectively. Lastly, we note that the reward variance \(_{Y}^{2}\) terms also need to be chosen. These values can be chosen using domain knowledge, ascribed a hierarchical prior and marginalized over, or optimized over using the same procedure described above and in Appendix Algorithm 1, by augmenting the BO procedure to use \([,_{Y}^{2}]\) as optimization variables.

**Adaptive Intervention Trial Simulation.** To evaluate our approach, we use the Just-in-Time Adaptive Intervention (JITAI) simulation environment introduced in (Karine et al., 2023). The JITAI environment models a messaging-based physical activity intervention applied to a single individual. The JITAI environment simulates: a binary context variable (\(C\)), a real-valued habituation level (\(H\)), a real-valued disengagement risk (\(D\)), and the walking step count (\(S\)). We use step count \(S\) as the reward in RL. The context \(C\) can represent a behavioral state such as'stressed'/ 'not stressed'. The simulation includes four actions: \(a=0\) indicates that no message is sent to the participant, \(a=1\) indicates that a non-contextualized message is sent, \(a=2\) indicates that a message customized to context \(0\) is sent, and \(a=3\) indicates that a message customized to context \(1\) is sent. The maximum study length is \(50\) days, with a daily intervention. We fix the action bias for action \(0\) to \(0\). The dynamics are such that sending any message causes habituation to increase, while sending an incorrectly contextualized message causes disengagement risk to increase. The effect of habituation is to decrease the ability of a message to positively influence the step count. When the disengagement risk exceeds a set threshold, the simulation models the participant withdrawing from the trial, eliminating all rewards after that time point, which is a common problem in mobile health studies. As a result of these dynamics, actions can strongly impact future rewards.

**BOTS overview.** We summarize and provide a graphical overview of the BOTS method, as applied in the JITAI setting, including setting TS priors using the MRT, in Appendix Figure 2.

## 3 Experiments and Results

We perform extensive experiments, which are detailed in Appendix B: basic MDPs, baselines without episode limits and the severely episode-limited settings. In severely episode-limited settings, we limit the total number of participants and thus episodes to 140. For BOTS, we allocate 10 individuals in rounds 0 and 1. This leaves a total of 120 from the budget of 140 participants. We evenly partition the remaining 120 across 2, 6, 12, 24, 30, 60 or 120 rounds. We run the same configurations on the baseline methods for comparison. We note that with 50 days required per round, the configurations that use 15 or more rounds would typically not be feasible in real studies, as they would require in excess of two years to run. In all our experiments, BOTS shows better performance in a low number of rounds, as shown in Figure 1.

## 4 Conclusions

Motivated by the use of RL methods to aid in the design of just-in-time adaptive health interventions, this paper focuses on practical methods that can deal with (1) severe constraints on the number of episodes available for policy learning, (2) constraints on the total duration of policy optimization studies, and (3) long terms consequences of actions. We have proposed a two-level approach that uses extended Thompson sampling (xTS) to select actions via an expected utility that includes fixed action bias terms, while a batch Bayesian optimization method is used to learn the action bias terms

Figure 1: Results for severely episode-limited settings. Note: the x-axes show (number of rounds, batch size) combinations, not round index. In all experiments, BOTS shows a better performance in a low number of rounds.

over multiple rounds, to maximize the expected return of the xTS policy. We have also presented a practical method to set the linear reward model priors using a small micro-randomized trial. Our results show that under realistic constraints on the total episode count and the intervention study duration, the use of local batch Bayesian optimization combined with xTS outperforms the other methods considered and is a promising approach for deployment in real studies.