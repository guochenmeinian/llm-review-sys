# Parameterized Approximation Schemes for

Fair-Range Clustering

 Zhen Zhang\({}^{1,2}\), Xiaohong Chen\({}^{1,2,}\), Limei Liu\({}^{1,2}\), Jie Chen\({}^{1,2}\),

**Junyu Huang\({}^{3}\), Qilong Feng\({}^{3,2}\)1**

\({}^{1}\)National Key Laboratory Cultivation Base for Data Intelligence and Smart Society,

Hunan University of Technology and Business, Changsha 410205, China

\({}^{2}\)Xiangjiang Laboratory, Changsha 410205, China

\({}^{3}\)School of Computer Science and Engineering, Central South University,

Changsha 410083, China

zz@hutb.edu.cn, csu_cxh@163.com, seagullm@163.com,

chemjay@hnu.edu.cn, junyuhuang@csu.edu.cn, csufeng@mail.csu.edu.cn

###### Abstract

Fair-range clustering extends classical clustering formulations by associating each data point with one or more demographic labels. It imposes lower and upper bound constraints on the number of facilities opened for each label, ensuring fair representation of all demographic groups by the selected facilities. In this paper we focus on the fair-range \(k\)-median and \(k\)-means problems in Euclidean spaces. We give \((1+)\)-approximation algorithms with fixed-parameter tractable running times for both problems, parameterized by the numbers of opened facilities and demographic labels. For Euclidean metrics, these are the first parameterized approximation schemes for the problems, improving upon the previously known \(O(1)\)-approximation ratios given by Thejaswi et al. (KDD 2022).

## 1 Introduction

Clustering seeks to partition a given set of clients into disjoint, cohesive clusters. Among the many formalizations of clustering, the _\(k\)-median_ and _\(k\)-means_ problems are perhaps the most prevalent ones, owing to the concise nature of their descriptions. Given a set of clients and facilities in a metric space along with a positive integer \(k\), the \(k\)-median and \(k\)-means problems aim to open at most \(k\) facilities and connect each client to the nearest opened facility, such that the sum of the client-connection costs is minimized. In the \(k\)-median problem, the connection cost of each client is its distance to the corresponding facility, while in the \(k\)-means problem, it is the squared distance. Despite their seemingly simple definitions, the \(k\)-median and \(k\)-means problems are computationally challenging, and the development of their approximation algorithms continues to be a vibrant area of research. The current best approximation guarantees are the ratios of 2.613 (Gowda et al., 2023) for the \(k\)-median problem and 9 (Ahmadian et al., 2020) for the \(k\)-means problem.

The \(k\)-median and \(k\)-means problems are designed to maximize the similarity between clients and their corresponding facilities, allowing the opened facilities to be considered representative points for the client set. This understanding underscores the important roles that the \(k\)-median and \(k\)-means problems play in data summarization (Moens et al., 1999, Girdhar and Dudek, 2012). However, algorithms developed for these problems can often yield unfair summarization of socioeconomic data, as they prioritize minimizing the clustering costs over considering the distribution of demographic labels (e.g., gender, age, race) associated with the opened facilities (Kay et al., 2015). Driven by thisrationale, there has been considerable interest in _fair-range clustering_. Given a set of data points associated with demographic labels, fair-range clustering extends classical clustering formulations by imposing lower and upper bound constraints on the number of opened facilities associated with each label, thereby ensuring fairness across different demographic groups.

An instance \((,k,,,,,,)\) of the fair-range clustering problem is specified by positive integers \(\) and \(k\), sets \(\) of clients and \(\) of facilities in a metric space, vectors \(=(_{1},,_{})\) and \(=(_{1},,_{})\) of \(\) positive integers satisfying \(_{t}_{t}\) for each \(t\{1,,\}\), and integer \( 1\), where each \(f\) is associated with a set \((f)\{1,,\}\) of demographic labels. A feasible solution to the instance is specified by a subset \(\) of no more than \(k\) facilities satisfying \(|\{f:t(f)\}|[_{t},_{t}]\) for each \(t\{1,,\}\), and the cost of the solution is \(_{c}_{f}^{}(c,f)\), where \(\) is the distance function. The goal of the fair-range clustering problem is to identify a feasible solution with minimum cost.

The fair-range clustering problem is equivalent to the _fair-range \(k\)-median_ (FkMed) problem when \(=1\), and to the _fair-range \(k\)-means_ (FkMeans) problem when \(=2\). Despite their significance in applications requiring fair representations, the FkMed and FkMeans problems pose significantly greater computational challenges than classical clustering problems. As demonstrated by Thejaswi et al. (2021), designing polynomial-time algorithms with provable approximation guarantees for the FkMed and FkMeans problems is unlikely, as determining the existence of feasible solutions to their instances is NP-hard. For a simplified scenario where each facility is associated with a single demographic label, Thejaswi et al. (2021) showed that the FkMed and FkMeans problems can be reduced to the well known _matroid clustering_ problem, which admits constant-factor approximation algorithms (Krishnaswamy et al., 2011; Li, 2011; Swamy, 2014; Friggstad and Zhang, 2016; Krishnaswamy et al., 2018), albeit with an \(O(k)^{-1}\) multiplicative overhead in algorithmic running time. Hotegni et al. (2023) latter gave an improved reduction to the matroid clustering problem that eliminates the \(O(k)^{-1}\) overhead. They further demonstrated that solving the FkMed and FkMeans problems in this simplified scenario can be achieved even more efficiently than solving matroid clustering problems, based on smaller-size linear programs.

In practical scenarios concerning clustering problems, the number of opened facilities (i.e., \(k\)) is often considerably smaller than the input size. As such, assuming \(k\) to be small and treating it as a fixed parameter is a commonly used way for simplifying these problems, as exemplified in (Cohen-Addad et al., 2019; Goyal and Jaiswal, 2023; Chen et al., 2024; Jaiswal et al., 2024). Unfortunately, the FkMed and FkMeans problems have been demonstrated to remain challenging even with this simplification: When both \(k\) and the number of demographic labels (i.e., \(\)) are fixed parameters, Thejaswi et al. (2022) established the W-hardness of the FkMed and FkMeans problems, suggesting that exactly solving them in fixed-parameter tractable time (denoted as \((k,)\) time, meaning \(n^{O(1)}h(k,)\) for an input size of \(n\) and a positive function \(h\)) is unlikely; Cohen-Addad et al. (2019) showed that the best possible approximation ratios of \((k,)\)-time algorithms, even for the case where \(=1\), cannot be better than \(1+2e^{-1}\) for the FkMed problem and \(1+8e^{-1}\) for the FkMeans problem. This matches the \((k,)\)-time \((1+2e^{-1}+)\)-approximation algorithm for the FkMed problem and \((1+8e^{-1}+)\)-approximation algorithm for the FkMeans problem given by Thejaswi et al. (2022) for a simpler case considering only the lower bound constraint. Notably, the method for enumerating feasible constraint patterns given by Thejaswi et al. (2022) demonstrates that their algorithms can be effortlessly extended to accommodate the case involving both lower and upper bounds.

The negative result presented by Cohen-Addad et al. (2019) suggests that we cannot hope to approximate the FkMeans problem with a ratio better than \(1+8e^{-1}\) and the FkMed problem with a ratio better than \(1+2e^{-1}\) in \((k,)\) time when considering general metric spaces. However, this result does not preclude the possibility of achieving better approximations for these problems in more structured settings, such as Euclidean spaces, since the analysis in Cohen-Addad et al. (2019) is limited to general metrics. In this paper, we take the first step toward exploring the properties of Euclidean metrics for the FkMed and FkMeans problems. Our approach yields \((k,)\)-time approximation schemes, as stated in Theorem 1 in Section 3.4.

### Other Related Work

Due to the prevalence of Euclidean data in real-world applications involving clustering, significant attention has been devoted to developing algorithms that leverage the properties of Euclidean spaces.

Exploring these properties often leads to improved approximation guarantees. One such example can be found in (Cohen-Addad et al., 2022), where a \((2.406+)\)-approximation algorithm for the \(k\)-median problem and a \((5.912+)\)-approximation algorithm for the \(k\)-means problem in Euclidean spaces are proposed, improving upon the state-of-the-art approximation ratios for these problems under general metrics (Gowda et al., 2023; Ahmadian et al., 2020). Furthermore, it has been shown that the Euclidean \(k\)-median and \(k\)-means problems admit _approximation schemes2_ if \(k\) is a fixed parameter and the opened facilities can be located arbitrarily (Kumar et al., 2010; Jaiswal et al., 2014; Bhattacharya et al., 2018; Ding and Xu, 2020). These algorithms identify a subset of each client-cluster defined by an optimal solution and approximate the corresponding opened facility by the centroid of this subset. However, similar ideas are not applicable to the FkMed and FkMeans problems, as they involve finite sets of facilities and hard constraints on the labels of the opened facilities. In these cases, the centroids of the considered subsets are not guaranteed to be feasible as opened facilities.

Constraints on the number of opened facilities associated with different labels were first introduced by Hajiaghayi et al. (2010, 2012), inspired by budget considerations for the deployment of servers in content distribution networks. From then on, related clustering problems have been widely explored. When we are provided with an upper bound constraint and each facility is associated with a single label, the problems represent special cases of the matroid clustering problems and directly motivate research into the latter (Krishnaswamy et al., 2011). For the lower-bounded case, there are FPT\((k,)\)-time approximation algorithms for the \(k\)-median and \(k\)-means cost functions (given by Thejaswi et al. (2022), as previously mentioned in Section 1), and a multi-swap local-search heuristic yields an \(O()\)-approximation for the \(k\)-median cost function if each facility has a single label (Thejaswi et al., 2021; Zhang et al., 2024).

In addition to imposing constraints on the distribution of labels associated with opened facilities, fair clustering has been extensively studied under various other settings that introduce different types of constraints. For example, _group fairness_ requires each cluster to provide a fair representation of different demographic groups (Chierichetti et al., 2017; Bera et al., 2019; Bandyapadhyay et al., 2021; Dai et al., 2022; Wu et al., 2024), _proportional fairness_ ensures that no subset of clients, of a given size, can find a closed facility that provides a lower connection cost to each of its members (Chen et al., 2019; Micha and Shah, 2020), _individual fairness_ requires that the distance from each client to the nearest opened facility does not exceed a client-specified threshold (Jung et al., 2020; Mahabadi and Vakilian, 2020; Negahbani and Chakrabarty, 2021; Vakilian and Yalciner, 2022; Ahmadi et al., 2022; Bateni et al., 2024), and _social fairness_ aims to minimize the maximum clustering cost among groups of clients (Abbasi et al., 2021; Ghadiri et al., 2021; Makarychev and Vakilian, 2021; Goyal and Jaiswal, 2023; Abbasi et al., 2024).

### Preliminaries

From now on, we consider an instance \(=(,k,,,,,,)\) of the fair-range clustering problem satisfying \(\{1,2\}\), \(||=n\), and \(^{d}\), along with a constant \((0,0.5)\). Given an integer \(i 1\) and a set \(\), define \([i]=\{1,,i\}\), and let \([]^{i}\) be the Cartesian product \(}_{i}\).

Given a point \(x\) and a set \(\) of points in an Euclidean space, let \((x,)=_{p}||x-p||\) denote the distance from \(x\) to its nearest point in \(\), and let \(^{i}(x,)=_{p}||x-p||^{i}\) for each \(i 1\).

The following algebraic fact will be utilized in the analysis of the running times of our algorithms.

**Lemma 1**: _Given two real numbers \(s\) and \(t\) greater than 1, we have \(^{t}s\{s,t^{O(t)}\}\)._

The following lemma extends triangle inequality3 to squared Euclidean metrics.

**Lemma 2**: _Given three points \(x\), \(y\), and \(z\) in an Euclidean space and a real number \((0,1]\), we have \(||x-z||^{2}(1+^{-1})||x-y||^{2}+(1+)||y-z||^{2}\)._

We will also consider the weighted version of the fair-range clustering problem, which can be defined as follows.

**Definition 1** (weighted fair-range clustering): _An instance \((,k,,,,,,)\) of the fair-range clustering problem can be extended to its weighted version \((,k,,,,,,,w)\) by associating each client \(c\) with a weight \(w(c) 1\). This extension modifies the cost of a feasible solution \(\) from \(_{c}^{}(c,)\) to \(_{c}w(c)^{}(c,)\)._

## 2 An Overview of Our Algorithms

The FPT\((k,)\)-time approximation algorithms for the FkMed and FkMeans problems given by Thejaswi et al. (2022) follow the framework outlined in (Cohen-Addad et al., 2019). This framework identifies the nearest client to each facility opened in the considered optimal solution as a "leader" and introduces a set of annuli centered at each leader. Each annulus is defined such that its outer radius is \(1+\) times its inner radius. The framework then enumerates the annuli to identify those that contain the facilities corresponding to the leaders and selects the opened facilities within these annuli, as illustrated in Figure 1-\((a)\). Intuitively, the definition of the annuli and triangle inequality imply an upper bound on the distances from the selected facilities to the optimal ones. Building upon this insight, Thejaswi et al. (2022) utilized a submodular maximization method to select facilities to be opened within the annuli and demonstrated constant-factor approximation ratios.

We similarly base our algorithms on the framework proposed by Cohen-Addad et al. (2019). Our approach focuses on exploring the properties of Euclidean metrics to further refine the selection range of opened facilities. Specifically, we partition each annulus into a set of smaller cells; for each facility opened in the optimal solution, we select the center point of the cell containing it as the facility to be opened, as shown in Figure 1-\((b)\). This process involves carefully balancing the number of cells, which affects the time required to identify the desired cells, against the sizes of the cells, which influence the distance from each facility opened in the optimal solution to the center point of the cell containing it, as well as our approximation ratio. We achieve this trade-off by constructing _nets_ as defined below.

**Definition 2** (\(\)-net (Gupta et al., 2003)): _Given a density parameter \(>0\), a set \(^{d}\), and a subset \(\), we call \(\) a \(\)-net of \(\) if each \(p\) satisfies \((p,\{p\})\) and each \(p\) satisfies \((p,)\)._

We partition the annular search space into cells using a set of nets for the facilities. The trade-off between the number and sizes of the cells can be managed by adjusting the density parameters (i.e., the parameter \(\) in Definition 2). For each annulus centered around a leader and containing its corresponding facility (the one opened in the optimal solution), we estimate the demographic labels associated with this facility and identify the subset of facilities within the annulus that share these labels. A net is then constructed for this subset, using a density parameter carefully determined by the radius of the annulus. Given the Voronoi diagram defined by the net, Definition 2 suggests that the facility opened in the optimal solution is close to the center point of its corresponding Voronoi cell. By considering each member of all constructed nets as a candidate for an opened facility, we can ensure that the candidate set includes a subset closely approximating the optimal solution.

It remains to consider how to bound the running time within FPT\((k,)\) time. The algorithms given by Cohen-Addad et al. (2019) and Thejaswi et al. (2022) start with constructing a coreset, that is, a small weighted subset of the client set whose distribution closely approximates that of the full set.

Figure 1: \((a)\) The client nearest to the opened facility \(f^{*}\) is taken as the leader, around which an annular search space is constructed; \((b)\) the center point \(f\) is opened in our solution.

This facilitates the efficient identification of leaders by enumerating the coreset. In this paper, we face additional challenges in bounding the running time. For instance, when partitioning the annuli into cells, the number of cells can depend exponentially on the spatial dimension. To ensure that we can deal with the FKMed and FKMeans problems in high-dimensional Euclidean spaces within \((k,)\) time, we map the considered instance to a space of \(O( k+ n)\) dimensions using a combination of the method for constructing coresets given by Chen (2009) and Johnson-Lindenstrauss transform. Combining this data-reduction technique with our net-based approach for selecting opened facilities, we give \((k,)\)-time \((1+)\)-approximation algorithms for the FKMed and FKMeans problems.

## 3 The Algorithms

We now present our algorithms for the FKMed and FKMeans problems. In Section 3.1, we introduce our data-reduction method for decreasing the size of the considered instance. In Section 3.2, we construct annular search spaces for the facilities to be opened, using the leaders from the client set. Section 3.3 details the construction of nets for the facilities, based on which we provide approximation schemes in low-dimensional spaces. Finally, in Section 3.4, we show how to combine the data-reduction method with the algorithms designed for low-dimensional spaces to deal with high-dimensional instances of the FKMed and FKMeans problems.

### Data Reduction

In this section we map instance \(\) to a smaller weighted instance in a low-dimensional space. As mentioned in Section 2, we achieve this using the coreset-construction method given by Chen (2009) and Johnson-Lindenstrauss transform, which are detailed in the following two lemmata.

**Lemma 3** (Chen (2009)): _Given a constant \((0,0.5)\), a set \(^{d}\), an integer \(t>0\), and an integer \(\{1,2\}\), a subset \(^{}\) with a weight function \(w:^{}[1,+)\) satisfying \(|^{}| d(t^{-1}||)^{O(1)}\) and \(_{p^{}}w(p)=||\) can be constructed in \(O(||dt)\) time, such that each \(^{d}\) with \(|| t\) satisfies \(_{p^{}}w(p)^{}(p,)[1- ,1+]_{p}^{}(p,)\)._

**Lemma 4** (Johnson and Lindenstrauss (1984); Ailon and Chazelle (2009)): _Given a constant \((0,0.5)\) and a set \(^{d}\), we can construct a mapping \(:^{d}^{}\) satisfying \(=O(^{-2}||)\) and \(||(p_{1})-(p_{2})||[1,1+]||p_{1}-p_{2}||\) for each \(p_{1},p_{2}\) in \(O(d d)+(^{-1}||)^{O(1)}\) time._

The following lemma is a stronger version of Johnson-Lindenstrauss transform, which preserves distances over a broader range through terminal embedding. Specifically, it modifies the condition "for each \(p_{1},p_{2}\)" in Lemma 4 to "for each \(p_{1}\) and \(p_{2}^{d}\)".

**Lemma 5** (Narayanan and Nelson (2019)): _Given a constant \((0,0.5)\) and a set \(^{d}\), we can construct a mapping \(:^{d}^{}\) satisfying \(=O(^{-2}||)\) and \(||(p_{1})-(p_{2})||[1,1+]||p_{1}-p_{2}||\) for each \(p_{1}\) and \(p_{2}^{d}\) in \((||d^{-1})^{O(1)}\) time._

It can be assumed that each mapping \(:^{d}^{}\) constructed by Lemma 4 and Lemma 5 is injective. Such an assumption is made without loss of generality: We can create duplicates of the points in \(^{}\) that have multiple preimages under \(\). This ensures that we can always differentiate \((x)\) and \((y)\) for any two distinct points \(x\) and \(y\) in \(^{d}\), even if \((x)\) and \((y)\) have identical values across all dimensions. Distinguishing the images of the points from \(^{d}\) is essential in fair-range clustering problems because points with the same dimensional values can have different demographic labels.

Our data-reduction method, which combines Lemma 3, Lemma 4, and Lemma 5, is presented in Algorithm 1 and illustrated in Figure 2 (this figure outlines the processing flow for the clients). This algorithm first leverages Lemma 3 within the \(O( n)\)-dimensional space constructed by Lemma 4, such that the client set can be replaced with a coreset of size logarithmically dependent on \(n\) and independent of \(d\). Next, to reduce dimensions while preserving the distances between each client in the coreset and any facility, Algorithm 1 uses Lemma 5 with the coreset as input to construct an \(O( k+ n)\)-dimensional space. The following lemma provides the performance guarantees of Algorithm 1.

**Lemma 6**: _Given a constant \((0,0.5)\) and an instance \((,k,,,,,,)\) of the fair-range clustering problem with \(\{1,2\}\), \(||=n\), and \(^{d}\), Algorithm 1 constructs a mapping \(:^{d}^{}\) and an instance \((,k,},},,,,,w)\) of the weighted fair-range clustering problem in \(O(d d)+(nk^{-1})^{O(1)}\) time, which satisfy the following properties:_

1. \(_{c}}w(c)=||\)_,_
2. \(w(c) 1\) _for each_ \(c}\)_,_
3. \(|}|(k^{-1} n)^{O(1)}\)_,_
4. \(=^{-O(1)}( k+ n)\)_, and_
5. \(_{c}}w(c)^{}(c,\{(f):f\})[1-,(1+)^{2+1}]_{c}^{} (c,)\) _for each_ \(\) _with_ \(|| k\)_._

### The Annular Search Spaces

In this section we construct \(k\) annular search spaces, each corresponding to one of the \(k\) facilities to be opened. We first introduce some notations. Let \(:^{d}^{}\) be the mapping and \(}=(,k,},},,,,,w)\) be the weighted instance constructed by Algorithm 1 with \((,)\) as the input, where \(}}^{}\), \(}=\{(f):f\}\), and each \(f\) satisfies \(((f))=(f)\). Let \(}^{*}=\{f_{1}^{*},,f_{k}^{*}\}\) be an optimal solution to \(}\), and let \(opt=_{c}}w(c)^{}(c,}^{*})\) denote its cost. For each \(i[k]\), let \(_{i}=\{f}:(f)=(f_{i}^{*})\}\) denote the set of facilities that have the same set of demographic labels as \(f_{i}^{*}\), and let \(}_{i}^{*}=\{c}:*{arg\,min }_{f}^{*}}||f-c||=f_{i}^{*}\}\) be the cluster of clients defined by \(f_{i}^{*}\). Given the lower bound constraint on the number of opened facilities, it may be the case that some facilities in \(}^{*}\) correspond to empty clusters. We thus define\(}^{*}_{0}=\{f}^{*}:}^{*}_{i }=\}\) and \(}^{*}_{1}=}^{*} }^{*}_{0}\). Let \(k^{*}=|}^{*}_{1}|\). Without loss of generality, we can assume that \(}^{*}_{1}=\{f^{*}_{1},,f^{*}_{k^{*}}\}\).

Following the framework given by Cohen-Addad et al. (2019), we select opened facilities from a set of annuli centered around a group of leaders from \(}\). For each \(i[k^{*}]\), let \(c_{i}\) denote the client from \(}\) nearest to \(f^{*}_{i}\), that is, the leader corresponding to \(f^{*}_{i}\). Let \(^{}_{}=_{i[k^{*}]}^{}(c_{i},f^{*}_{i})\). For each \(i[k^{*}]\) and \(j[^{-2} n]\), let \(^{*}(i,j)=\{f_{i}:||f-c_{i}||^{}((1+ )^{j-1}^{}_{}n^{-1},(1+)^{j}^{ }_{}n^{-1})\}\) be the set of facilities from \(_{i}\) located in an annulus centered around \(c_{i}\), and let \(^{*}(i,0)=\{f_{i}:||f-c_{i}||^{} ^{}_{}n^{-1}\}\). The definitions of \(^{*}(i,j)\) and \(^{}_{}\) imply the existence of an integer \(j\{0,,^{-2} n\}\) satisfying \(f^{*}_{i}^{*}(i,j)\). Denote by \(^{*}_{i}\) such a set \(^{*}(i,j)\) containing \(f^{*}_{i}\).

Our method for constructing annular search spaces is presented in Algorithm 2. Since the collection \(\{^{*}(1,0),,^{*}(k^{*},^{-2} n )\}\) can be determined based on the values of \(\{_{1},,_{k}\}\), \(k^{*}\), \(^{}_{}\), and \(\{c_{1},,c_{k^{*}}\}\), Algorithm 2 enumerates all possible values of these parameters in step 5 to ensure that the collection can be captured. Given an integer \(i[k^{*}]\) and the sets \(^{*}(i,0),,^{*}(i,^{-2} n)\), Algorithm 2 enumerates \(^{-2} n\{0\}\) in step 12 to find the integer \(j\) with \(^{*}_{i}=^{*}(i,j)\). To avoid the case where the search spaces for the \(k\) opened facilities intersect and the set of selected facilities contains duplicate elements, Algorithm 2 employs a color-coding technique to eliminate any potential intersections. Specifically, Algorithm 2 associates each facility \(f}\) with a random integer \((f)[k]\) in step 4, and only selects facilities with \((f)=i\) when constructing the \(i\)-th search space for each \(i[k]\) in steps 14 and 17. The performance guarantees of this algorithm are presented in the following lemma.

**Lemma 7**: _The collection \(\) constructed by Algorithm 2 satisfies the following two properties:_1. _With probability no less than_ \(k^{-k}\)_, there exists a collection_ \(\{_{1},,_{k}\}\) _satisfying_ \(f_{i}^{*}_{i}_{i}^{*}\) _for each_ \(i[k^{*}]\) _and_ \(_{i}\) _for each_ \(i[k][k^{*}]\)_;_
2. _Given a collection_ \(\{_{1},,_{k}\}\)_, with_ \(_{i}\) _for each_ \(i[k]\)_, and a tuple_ \((f_{1},,f_{k})_{1}_{2} _{k}\)_, we have_ \(|\{i[k]:t(f_{i})\}|[_{t},_{t}]\) _for each_ \(t[]\)_._

### The Algorithm in Low-Dimensional Spaces

As outlined in Section 2, solutions are constructed by extracting nets from the set of facilities. The following lemma presents a method for generating nets in low-dimensional Euclidean spaces.

**Lemma 8** (Har-Peled and Mendel ): _Given a density parameter \(>0\) and a set \(^{d}\), a \(\)-net of \(\) of size at most \(\{||,^{-d}_{p_{1},p_{2}}||p_{1}-p_{2}|| ^{d}\}\) can be constructed in \(||||2^{O(d)}\) time._

Our approach for solving the low-dimensional weighted instance \(}\) is built upon Algorithm 2 and Lemma 8, and is outlined in Algorithm 3. Since Algorithm 2 yields the desired search spaces with probability \(k^{-k}\) (as given by the first property stated in Lemma 7), Algorithm 3 iteratively invokes it \(k^{k}\) times, allowing the probability of successfully constructing the desired search spaces in at least one of the iterations to be boosted to a constant. Given \(k\) sets \(_{1},,_{k}\) satisfying the first property stated in Lemma 7, Algorithm 3 constructs a net for each set of size greater than 1, and adds the members of the net to the candidate set of opened facilities. Finally, the algorithm constructs a set of feasible solutions to \(}\) based on the candidates for opened facilities, and returns the one with the minimum cost among them.

``` Input: A constant \((0,0.5)\), an instance \(}=(,k,},},,,,,w)\) of the weighted  fair-range clustering problem, and a positive integer \(n\) Output: A solution \(^{}\) to \(}\)
1\(\), \(\);
2foreach\(s[k^{k}]\)do
3 Let \(^{}\) be the collection constructed by Algorithm 2 with \((,},n)\) as the input;
4\(^{}\);
5for each \(\{_{1},,_{k}\}\) with \(_{i}\,\,i[k]\)do
6if\(|_{i}|=1\)then
7\(_{i}_{i}\);
8else
9 Let \(_{i}\) be the \(_{x,y_{i}}\|x-y\|\)-net of \(_{i}\) constructed by Lemma 8;
10
11 Let \(^{}\) be the collection constructed by transforming each tuple in \(_{1}_{2}_{k}\) into a set;
12\(^{}\);
13
14return\(^{}_{}_{c }}w(c)^{}(c,)\). ```

**Algorithm 3**The algorithm for low-dimensional weighted instances

The following lemma says that Algorithm 3 yields a \((1+O(^{}))\)-approximation solution to \(}\) with high probability.

**Lemma 9**: _The following event occurs with probability no less than \(1-e^{-1}\): Algorithm 3 yields a feasible \(^{}\) to \(}\) satisfying \(_{c}}w(c)^{}(c,^{})<(1+4 )opt\) if \(=1\) and \(_{c}}w(c)^{}(c,^{})<(1+9 )opt\) if \(=2\)._

By analyzing the time Algorithm 3 takes to construct the set of candidate solutions, as well as the size of this set, we can establish the following upper bound on the running time of Algorithm 3.

**Lemma 10**: _Algorithm 3 runs in no more than \(2^{(k^{-1})^{O(1)}+k}n^{O(1)}\) time._``` Input: A constant \((0,0.5)\) and an instance \(=(,k,,,,,,)\) of the fair-range  clustering problem satisfying \(^{d}\) Output: A solution \(^{}\) to \(\)
1 Let \(:^{d}^{}\) be the mapping and \(}=(,k,},},,,,,w)\) be the weighted instance constructed by Algorithm 1 with \((,)\) as the input;
2 Let \(^{}\) be the solution to \(}\) constructed by Algorithm 3 with \((,},||)\) as the input; return\(^{}\{^{-1}(f):f^{}\}\). ```

**Algorithm 4**The algorithm in high-dimensional spaces

### Extensions to High-Dimensional Spaces

We combine the data-reduction method given in Section 3.1 with the low-dimensional algorithm given in Section 3.3 to solve the FkMed and FkMeans problems in high-dimensional spaces, as detailed in Algorithm 4. Given a constant \((0,0.5)\) and an instance \(=(,k,,,,,,)\), the algorithm starts with constructing a mapping \(:^{d}^{}\) and a small weighted instance \(}=(,k,},},,,,,w)\), where \(}}^{}, }=\{(f):f\}\), and \(((f))=(f)\) for each \(f\). It then constructs a solution to \(}\) and returns the set of preimages of the facilities opened in this solution under \(\). The analysis of the performance guarantees of Algorithm 4 leads to the main result of this paper, as stated in Theorem 1.

**Theorem 1**: _Given an instance \((,k,,,,,,)\) of fair-range clustering with \(^{d}\) and \(\{1,2\}\) along with a real number \((0,1)\), there is a randomized \((1+)\)-approximation algorithm running in \(O(d d)+2^{(k^{-1})^{O(1)}+k}n^{O(1)}\) time, where \(n=||\)._

## 4 Conclusions

In this paper, we consider the FkMed and FkMeans problems for the case where the numbers of opened facilities and demographic labels are fixed parameters. Based on a combination of a data-reduction method and a space-partitioning approach for selecting opened facilities, we introduce \((1+)\)-approximation algorithms in Euclidean spaces, representing the first parameterized approximation schemes for the problems. Given that coreset-construction methods were known for many constrained clustering problems incorporating additional constraints on instances, such as those related to capacities (Cohen-Addad and Li, 2019), group fairness (Bandyapadhyay et al., 2021), and robustness (Huang et al., 2023), an interesting direction for future work is to extend our techniques to deal with the FkMed and FkMeans problems in similar constrained settings. Another promising avenue for exploration is to accelerate heuristic algorithms for the fair-range clustering problem, such as those given in (Thejaswi et al., 2022), using the data-reduction method proposed in this work.

## 5 Broader Impact

Our work deals with the fair-range clustering problem, providing algorithmic insights that can facilitate fair decision-making. While our algorithms have been shown to be "fair" according to specific definitions, it is essential to recognize that this fairness does not automatically warrant indiscriminate application. This underscores the need for careful consideration when implementing the algorithms proposed in this paper in real-world scenarios that prioritize fairness.