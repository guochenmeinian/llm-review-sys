# Towards Last-layer Retraining for Group Robustness with Fewer Annotations

Tyler LaBonte\({}^{1}\)  Vidya Muthukumar\({}^{2,1}\)  Abhishek Kumar\({}^{3}\)

\({}^{1}\)H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

\({}^{3}\)Google DeepMind

{tlabonte, vmuthukumar8}@gatech.edu  abhishk@google.com

###### Abstract

Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent _deep feature reweighting_ (DFR) technique  achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we introduce a lightweight method called _selective last-layer fine-tuning_ (SELF), which constructs the reweighting dataset using misclassifications or disagreements. Our empirical and theoretical results present the first evidence that model disagreement upsamples worst-group data, enabling SELF to nearly match DFR on four well-established benchmarks across vision and language tasks with no group annotations and less than \(3\%\) of the held-out class annotations. Our code is available at https://github.com/tmlabonte/last-layer-retraining.

## 1 Introduction

Classification tasks in machine learning often suffer from _spurious correlations_: patterns which are predictive of the target class in the training dataset but irrelevant to the true classification function. These spurious correlations, often in conjunction with the target class, create _minority groups_ which are underrepresented in the training dataset. For example, in the task of classifying cows and camels, the training dataset may be biased so that a desert background is spuriously correlated with the camel class, creating a minority group of camels on grass backgrounds . Beyond this simple scenario, spurious correlations have been observed in high-consequence applications including medicine , justice , and facial recognition .

Neural networks trained via the standard procedure of empirical risk minimization (ERM) , which minimizes the average training loss, tend to overfit to spurious correlations and generalize poorly on minority groups . Even worse, it is possible for ERM models to rely exclusively on the spurious feature and incur minority group performance that is no better than random guessing . Therefore, maximizing the model's _group robustness_, quantified by its worst accuracy on any group, is a desirable objective in the presence of spurious correlations .

In contrast to more generic distribution shift settings (_e.g.,_ domain generalization ), the presence of spurious correlations enables the improvement of group robustness merely by addressing model bias (without collecting additional minority group data). The recently proposed _deep feature reweighting_ (DFR)  technique efficiently corrects model bias by retraining the last layer of the neural network, a simple procedure which achieves state-of-the-art group robustness. The key hypothesis underlying DFR is that ERM models which overfit to spurious correlations still learn _core features_ that correlate with the ground-truth label on all groups, but they perform poorly because they overweight the spurious features in the last layer. Ostensibly, retraining the last layer on a group-balanced _reweighting dataset_ would then upweight the core features and improve worst-group accuracy.

DFR compares favorably to existing methods such as _group distributionally robust optimization_ (DRO) , which requires group annotations for the entire training dataset. However, DFR still necessitates a smaller reweighting dataset with group and class annotations to achieve maximal performance . This requirement limits its practical application, as the groups are often unknown ahead of time or difficult to annotate (_e.g.,_ due to financial, privacy, or fairness concerns).

### Our contributions

In this paper, we present a comprehensive examination of the performance of last-layer retraining in the absence of group and class annotations on four well-established benchmarks for group robustness across vision and language tasks.1 We first investigate the necessity of the reweighting dataset being balanced across groups, and we show that last-layer retraining can substantially improve worst-group accuracy _even when the reweighting dataset has only a small proportion of worst-group data_. Based on this observation, we propose _class-balanced last-layer retraining_ as a simple but strong baseline for group robustness without group annotations. We show that, on average over the four datasets, this method achieves \(94\%\) of DFR worst-group accuracy compared to \(76\%\) without class balancing.

The strong performance of class-balanced last-layer retraining reveals a "free lunch" with practical ramifications. While class-balanced ERM was recently proposed by  as a competitive baseline for worst-group accuracy, we show that instead of using the entire training dataset for ERM, dependence on spurious correlations can be reduced by randomly splitting the training data in two, then performing ERM training on the first split and class-balanced last-layer retraining on the second. Our experiments indicate that this technique can improve worst-group accuracy by up to \(17\%\) over class-balanced ERM on the original dataset using _no additional data or annotations (even for model selection)_ - a surprising and unexplained result given that the two splits have equally drastic group imbalance.

While retraining the last layer on a class-balanced held-out dataset can be effective, it is inferior to DFR when group imbalance is large. To close this gap, we propose _selective last-layer finetuning_ (SELF), which selects a small, more group-balanced reweighting dataset and finetunes the last layer instead of retraining. We implement SELF using points that are either _misclassified_ or _disagree_ in their predictions relative to a regularized model. Disagreement SELF does not require class annotations for the entire held-out set (only for the disagreements). We show that _disagreement SELF between the ERM and early-stopped models_ performs the best in general, surpassing class-balanced last-layer retraining by up to \(12\%\) worst-group accuracy with less than \(3\%\) of the held-out class annotations.

Overall, our work shows benefits of last-layer retraining well beyond a group-balanced held-out dataset. Our results call for _further investigation of the DFR hypothesis_: while group balance is the most important factor in DFR performance, we show that a significant gain is solely due to class balancing, and the performance discrepancy between misclassification SELF and disagreement SELF suggests that worst-group accuracy may be affected by characteristics of the reweighting dataset other than group balance. Our main results are summarized and compared to previous methods in Table 1.

## 2 Related work

This work subsumes and improves our two previous workshop papers:  broadly covers Section 4, while  represents preliminary investigation into Section 5, which we have substantially updated with new methodology, evaluation, and theoretical analysis for this version.

Sparious correlations.The performance of empirical risk minimization (ERM) in the presence of spurious correlations has been extensively studied . In vision, ERM models are widely known to rely on spurious attributes like background , texture , and secondary objects  to perform classification. In language, ERM models often utilize syntactic or statistical heuristics as a substitute for semantic understanding . This behavior can lead to bias against demographic minorities  or failure in high-consequence applications .

Robustness and group annotations.If group annotations are available in the training dataset, _group distributionally robust optimization_ (DRO)  can improve robustness by minimizing the worst-group loss, while other techniques learn invariant or diverse features . Methods which use only partial group annotations include _deep feature reweighting_ (DFR) , which retrains the last layer on a group-balanced held-out set, and _spread spurious attribute_, which performs DRO with group pseudo-labels. Recently, more lightweight methods that only adjust the model predictions using a group annotated held-out set have also been proposed . However, since the groups are often unknown ahead of time or difficult to annotate in practice, there has been significant interest in methods which do not utilize group annotations except for model selection. The bulk of these techniques utilize auxiliary models to pseudo-label the minority group or spurious feature ; notably, _just train twice_ (JTT)  upweights samples misclassified by an early-stopped model. Other techniques reweight or subsample the classes  or train with robust losses and regularization .

Unlike DFR, our methods utilize no held-out group annotations, and unlike JTT, we do not have to "train twice", only finetune the last layer. We also show that SELF performs best using _disagreements_ between the ERM and early-stopped models instead of _misclassifications_ as in JTT. Moreover, while JTT assumes the early-stopped model has low worst-group accuracy which improves during training, SELF performs well even when the early-stopped model has high worst-group accuracy which decreases during training - substantially improving performance on datasets such as CivilComments. The concurrent work of Qiu _et al._ proposed a similar method to our Section 4.2; while they use a tunable loss to upweight misclassified samples, our method shows that similar results can be achieved with no hyperparameter tuning (and therefore no group annotations for the validation set). Finally, while our results partially corroborate the findings of Idrissi _et al._ that class balancing during ERM is effective for group robustness, we observe that class-balanced last-layer retraining renders ERM class balancing optional. We compare our results with previous methods in Table 1.

Generalization via disagreement.Disagreement-based active learning for improving in-distribution generalization has been well-studied since before the deep learning era . More recent research has utilized disagreements between SGD runs to predict in-distribution gen

    &  &  \\   & Group & Class & Waterbirds & CelebA & CivilComments & MultiNLI \\  Class-balanced ERM & ✗ & ✓ & \(81.9_{ 3.4}\) & \(67.2_{ 5.6}\) & \(61.4_{ 0.7}\) & \(69.2_{ 1.6}\) \\ JTT  & ✗ & ✓ & \(85.6_{ 0.2}\) & \(75.6_{ 7.7}\) & – & \(67.5_{ 1.9}\) \\ RWY-ES  & ✗ & ✓ & \(74.5_{ 0.0}\) & \(76.8_{ 7.7}\) & \(78.9_{ 1.0}\) & \(68.0_{ 0.4}\) \\ CnC  & ✗ & ✓ & \(88.5_{ 0.3}\) & \(}\) & – & – \\  CB last-layer retraining & ✗ & ✓ & \(92.6_{ 0.8}\) & \(73.7_{ 2.8}\) & \(}\) & \(64.7_{ 1.1}\) \\ ES disagreement SELF & ✗ & ✗ & \(}\) & \(83.9_{ 0.9}\) & \(79.1_{ 2.1}\) & \(}\) \\  DFR (our impl.) & ✓ & ✓ & \(92.4_{ 0.9}\) & \(87.0_{ 1.1}\) & \(81.8_{ 1.6}\) & \(70.8_{ 0.8}\) \\ DFR  & ✓ & ✓ & \(91.1_{ 0.8}\) & \(89.4_{ 0.9}\) & \(78.8_{ 0.5}\) & \(72.6_{ 0.3}\) \\ Group DRO-ES  & ✓ & ✓ & \(90.7_{ 0.6}\) & \(90.6_{ 1.6}\) & \(80.4\) & \(73.5\) \\   

Table 1: **Comparison to other group robustness methods.** We discuss our DFR implementation in Section 3, and our proposed methods of class-balanced (CB) last-layer retraining and early-stop (ES) disagreement SELF in Sections 4 and 5, respectively. Class-balanced ERM is trained on the combined training and held-out datasets. DFR uses group annotations on the held-out dataset, while Group DRO-ES requires them for the training dataset. ES disagreement SELF uses class annotations on the training dataset (for ERM), but _requests as few as 20 labels_ from the held-out dataset. All methods except ERM and CB last-layer retraining use a small set of group annotations for model selection. We list the mean and standard deviation over three independent runs.

eralization , as well as between model classes (_e.g._, CNNs and Transformers) to predict out-of-distribution generalization . Two works that are concurrent to ours, _diversity by disagreement_ and _diversify and disambiguate_, are methods for generalization under distribution shift which maximize the disagreement between multiple predictors to learn a diverse ensemble. Compared to our work, these methods are optimized for training datasets which exhibit a _complete correlation_ (_i.e._, contain no minority group data) and they underperform in the less extreme spurious correlation setting we study.

## 3 Preliminaries

**Setting.** We consider classification tasks with input domain \(\) and target classes \(\). We assume \(\) is a set of _spurious features_ such that each sample \(x\) is associated with exactly one feature \(s\). In conjunction with the target class, the spurious features partition the dataset into groups \(=\). While the groups may be heavily imbalanced in the training distribution, we desire a model which is invariant to the spurious feature and thus has roughly uniform performance over \(\). Therefore, we evaluate _worst-group accuracy_ (WGA), _i.e._, the minimum accuracy among all groups .

We will often refer to datasets and models as _group-balanced_ or _class-balanced_, meaning that in expectation, the dataset is composed of an equal number of samples from groups in \(\) or classes in \(\), respectively. This balance can be achieved by training on a subset with equal data from each group/class, or sampling from the data so that each minibatch is balanced in expectation . To make the latter more concrete, for group balancing we first sample \(s()\), then sample \(x(|s)\) where \(\) is the training distribution; class balancing is the same with \(\) instead of \(\). We use the minibatch sampling approach for both class-balanced ERM and class-balanced last-layer retraining, and we provide a comparison with the subset method in Appendix A.

Deep feature reweighting.The recently proposed _deep feature reweighting_ (DFR) [33; 28] method achieves state-of-the-art WGA by performing ERM on the training dataset, then retraining the last layer of the neural network on a group-balanced held-out dataset, called the _reweighting dataset_. In the original implementation, half the validation set is used to construct the reweighting dataset: all data from the smallest group is included and the other groups are randomly downsampled to that size.

    &  & \)} &  \\   & Class \(y\) & Spurious \(s\) & \((y)\) & \((y)\) & \((y|s)\) & Train & Val & Test \\   & landbird & land &  & \(.730\) & \(.984\) & \(3498\) & \(467\) & \(2225\) \\  & landbird & water & & \(.038\) & \(.148\) & \(184\) & \(466\) & \(2225\) \\  & waterbird & land & & \(.012\) & \(.016\) & \(56\) & \(133\) & \(642\) \\  & waterbird & water & & \(.220\) & \(.852\) & \(1057\) & \(133\) & \(642\) \\   & non-blond & female &  & \(.440\) & \(.758\) & \(71629\) & \(8535\) & \(9767\) \\  & non-blond & male & & \(.411\) & \(.980\) & \(66874\) & \(8276\) & \(7535\) \\  & blond & female & & \(.141\) & \(.242\) & \(22880\) & \(2874\) & \(2480\) \\  & blond & male & & \(.009\) & \(.020\) & \(1387\) & \(182\) & \(180\) \\   & neutral & no identity &  & \(.551\) & \(.921\) & \(148186\) & \(25159\) & \(74780\) \\  & neutral & identity & & \(.336\) & \(.836\) & \(90337\) & \(14966\) & \(43778\) \\  & toxic & no identity & & \(.113\) & \(.047\) & \(.079\) & \(12731\) & \(2111\) & \(6455\) \\  & toxic & identity & & \(.066\) & \(.164\) & \(17784\) & \(2944\) & \(8769\) \\   & contradiction & no negation &  & \(.279\) & \(.300\) & \(57498\) & \(22814\) & \(34597\) \\  & contradiction & negation & & \(.054\) & \(.761\) & \(11158\) & \(4634\) & \(6655\) \\  & entailment & no negation & & \(.327\) & \(.352\) & \(67376\) & \(26949\) & \(40496\) \\  & entailment & negation & & \(.007\) & \(.104\) & \(1521\) & \(613\) & \(886\) \\  & neither & no negation & & \(.333\) & \(.348\) & \(66630\) & \(26655\) & \(39930\) \\  & neither & negation & & \(.010\) & \(.136\) & \(1992\) & \(797\) & \(1148\) \\   

Table 2: **Dataset composition.** We study four well-established benchmarks for group robustness across vision and language tasks. The class probabilities change dramatically when conditioned on the spurious feature. Note that Waterbirds is the only dataset that has a distribution shift and MultiNLI is the only dataset which is class-balanced _a priori_. Probabilities may not sum to \(1\) due to rounding.

Then, the feature embeddings (_i.e.,_ the outputs of the penultimate layer) of the reweighting dataset are pre-computed and used to train a logistic regression model with explicit \(_{1}\)-regularization. The results are averaged over 10 randomly subsampled reweighting datasets, and a hyperparameter search is performed over \(_{1}\) regularization strength on the other half of the validation set.

To emphasize practicality and efficiency, our implementation of DFR has some differences from the original. _(i)_ Instead of logistic regression, we train the last layer on the reweighting dataset via minibatch optimization using SGD and AdamW  for the vision and language tasks, respectively. This fits well into standard training pipelines and avoids pre-computing the feature embeddings and writing them to disk, which can be slow and memory-intensive. _(ii)_ To reduce the number of hyperparameters, we use a fixed-value \(_{2}\) regularization instead of searching over \(_{1}\) regularization strength. We observed similar performance for \(_{1}\) and \(_{2}\) regularization, which we believe is because \(_{1}\)-regularized gradients do not induce sparsity . _(iii)_ We sample uniformly at random from the groups in the held-out dataset (to get group balanced minibatches) instead of averaging over group-balanced subsets of the data . We compare the implementations in detail in Appendix A.

Datasets and models.We study four datasets which are well-established as benchmarks for group robustness across vision and language tasks, detailed in Table 2 and summarized below.

* _Waterbirds_[71; 69; 58] is an image classification dataset where the task is to predict whether a bird is a landbird or a waterbird. The spurious feature is the image background: more landbirds are present on land backgrounds than waterbirds, and vice versa.2 * _CelebA_[42; 58] is an image classification dataset where the task is to predict whether a person is blond or not. The spurious feature is gender, with \(16\) more blond women than blond men in the training set.
* _CivilComments_[7; 35] is a text classification dataset where the task is to predict whether a comment is toxic or not. The spurious feature is the presence of one of the following categories: male, female, LGBT, black, white, Christian, Muslim, or other religion.3 More toxic comments contain one of these categories than non-toxic comments, and vice versa. * more contradictions have this property than entailments or neutral pairs.

Importantly, Waterbirds is the only dataset that has a distribution shift - its validation and test datasets are group-balanced conditioned on the classes, although still class-imbalanced.4 As a result, methods which use the validation set for training can improve performance without explicit group balancing.

We utilize a ResNet-50  pretrained on ImageNet-1K  for Waterbirds and CelebA, and a BERT  model pretrained on Book Corpus  and English Wikipedia for CivilComments and MultiNLI. Following previous work, we use half the validation set for feature reweighting [33; 28] and half for model selection with group annotations [58; 41; 33; 48; 28]. We run each experiment on three random seeds and do not utilize explicit early stopping except as part of SELF (see Section 5). See Appendix D for further details on the training procedure.

## 4 Class-balanced last-layer retraining

In this section, we investigate the necessity of a group-balanced reweighting dataset for DFR and show that _class-balanced last-layer retraining_ is a simple but strong baseline for group robustness without group annotations. To enable a fair comparison with our implementation of DFR (see Section 3), class-balanced last-layer retraining follows the same training procedure, except the reweighting dataset is constructed by sampling uniformly over the classes \(\) instead of the groups \(\).

### An ablation on the proportion of worst-group data

In this section, we perform an ablation on the percentage of worst-group data in the reweighting dataset and show that class-balanced last-layer retraining can substantially improve WGA _even when the reweighting dataset has only a small proportion of worst group data._ For our ablation, we choose the worst groups based on the performance of ERM, and if two groups have similarly poor performance, we vary both. The worst groups for Waterbirds are landbirds on water backgrounds and waterbirds on land backgrounds; for CelebA blond men; for CivilComments toxic comments containing identity categories; and for MultiNLI entailments and neutral pairs containing negations.

For these experiments, we begin with the DFR reweighting dataset, _i.e.,_ a random group-balanced subset of the held-out dataset, where the size of each group is the minimum of the worst-group size and half the size of any other group.5 We define this dataset to include \(100\%\) of the worst-group data. We then reduce the percentage of worst-group data, while correspondingly increasing the percentage of data from the same class without the spurious feature. For example, the DFR reweighting dataset on CelebA has \(92\) points from each group, so reducing the worst group to \(25\%\) results in \(92\) non-blond females, \(92\) non-blond males, \(161\) blond females, and \(23\) blond males. The total data is kept constant.

Figure 1: **How much worst-group data does last-layer retraining really need?** We perform an ablation on the percentage of worst-group data used for class-balanced last-layer retraining, while keeping the total data constant. The results show that last-layer retraining can substantially improve worst-group accuracy _even when the reweighting dataset has only a small proportion of worst group data_, and that class balancing can be a major factor in its performance. The underperformance on Waterbirds below \(20\%\) is because there is too little worst-group data to observe consistent model behavior (less than ten samples). The stars \(\) denote the baseline percentage of worst-group data in the training dataset. We plot the mean over three independent runs. See Tables 12 and 13 for details.

    & Group &  \\   & annotations & Waterbirds & CelebA & CivilComments & MultiNLI \\  Class-balanced ERM & ✗ & \(81.9_{ 3.4}\) & \(67.2_{ 5.6}\) & \(61.4_{ 0.7}\) & \(}\) \\ UB last-layer retraining & ✗ & \(88.0_{ 0.8}\) & \(41.9_{ 1.4}\) & \(57.6_{ 4.2}\) & \(64.6_{ 1.0}\) \\ CB last-layer retraining & ✗ & \(}\) & \(}\) & \(}\) & \(}\) \\  DFR (our impl.) & ✓ & \(92.4_{ 0.9}\) & \(87.0_{ 1.1}\) & \(81.8_{ 1.6}\) & \(70.8_{ 0.8}\) \\ DFR  & ✓ & \(91.1_{ 0.8}\) & \(89.4_{ 0.9}\) & \(78.8_{ 0.5}\) & \(72.6_{ 0.3}\) \\   

Table 3: **Last-layer retraining on the held-out dataset.** While unbalanced (UB) last-layer retraining decreases performance, class-balanced (CB) last-layer retraining nearly matches DFR on Waterbirds and CivilComments. However, it still trails DFR on CelebA and MultiNLI; we improve these results with the SELF method described in Section 5. CB ERM is trained on the combined training and held-out datasets using class-balanced minibatches. We list the mean and standard deviation over three independent runs.

The results of this ablation are displayed in Figure 1. While a smooth increase in WGA with increasing worst-group data is expected, the extent of the early increase is surprising: over the four datasets, an average of \(67\%\) of the increase in WGA over class-unbalanced ERM is obtained by the first \(25\%\) of worst-group data. In particular, CelebA and CivilComments - the most class-imbalanced datasets we study - experience significant improvement even at low percentages of worst-group data. This phenomenon suggests that, while group balancing is still important for best results, _class balancing is a major factor in the performance of DFR_ on these two datasets.

Furthermore, class-balanced last-layer retraining can improve worst-group accuracy even when the held-out dataset has similar group imbalance as the training dataset (_i.e.,_ at the stars \(\) in Figure 1). Based on this observation, we propose that class-balanced last-layer retraining _on the entire held-out dataset_ can be a simple but strong baseline for group robustness without group annotations. Table 3 details the results; on average over the four datasets, class-balanced last-layer retraining achieves \(94\%\) of DFR performance, compared to \(76\%\) without class balancing. However, it still trails DFR by a significant amount on CelebA and MultiNLI - the most group-imbalanced datasets we study. We improve these results with our selective last-layer finetuning (SELF) method described in Section 5.

Moreover, our experiments in Figure 4 (deferred to Appendix A) indicate that class-balanced last layer retraining has similar performance regardless of whether it is initialized with class-unbalanced or class-balanced ERM features. Contrasting with Idrissi _et al._, our results suggest that class balancing in the ERM stage is optional. This result has practical relevance for expensive models pre-trained without class balancing (_e.g.,_ large language models), as the benefits of class balancing on downstream tasks can be reaped by simply retraining the last layer instead of training a new model.

### A "free lunch" in group robustness

Motivated by the promising results of Section 4.1, where we performed class-balanced last-layer retraining on a fixed held-out set, we now ask _how can we best utilize a realistic training dataset?_ In particular, practical applications often work with a predetermined data, annotation, and compute budget. Within this budget, and with no explicit held-out dataset, would one achieve better group robustness by using the entire dataset for ERM or by holding out a subset for last-layer retraining?

We investigate this question on our four benchmark datasets by randomly splitting the initial dataset into two, then performing class-balanced ERM training on the first split (\(95\%\) of the data) and class-balanced last-layer retraining on the second split (\(5\%\) of the data). Figure 1(a) illustrates the results of our experiments on the training dataset and Figure 1(b) on the combined training and held-out datasets. Since the quantity of data is higher in the case of combining the training and held-out datasets, we expect all numbers to be higher in Figure 1(b) compared to Figure 1(a) (especially on

Figure 2: **A “free lunch” in group robustness. We compare class-balanced (CB) ERM on the entire dataset to splitting the dataset and performing CB ERM on the first (\(95\%\)) split and CB last-layer retraining on the second (\(5\%\)) split. This technique improves worst-group accuracy on Waterbirds, CelebA, and CivilComments by up to \(17\%\) while using _no additional data or annotations_ for training beyond ERM. We believe it underperforms on MultiNLI because there is not enough data in the first split, _i.e.,_ ERM performance can be improved by collecting more data. We plot the mean and standard deviation over three independent runs. See Table 14 for detailed results.**

Waterbirds, which has a more group-balanced validation set). We perform no hyperparameter tuning for these experiments, and therefore we do not utilize group annotations _even for model selection_.

Figure 2 indicates that splitting the dataset and performing last-layer retraining substantially improves worst-group accuracy on Waterbirds, CelebA, and CivilComments. It decreases performance on MultiNLI, which is the only dataset where adding held-out data from the same distribution significantly increases ERM worst-group accuracy compared to DFR. Specifically, class-balanced ERM achieves \(67.4 2.4\)% WGA on the training dataset and \(69.2 1.6\)% on the combined training and held-out datasets, while our DFR implementation achieves \(70.8 0.8\)%. Therefore, we hypothesize that last-layer retraining on the second split can improve group robustness _only if there is enough data for ERM to perform near-optimally_ on the first split, _i.e.,_ if the performance of ERM on the first split is limited by dataset bias rather than sample variance.

Based on this hypothesis, our answer to the posed question is: if ERM performance is stable when holding out \(5\%\) of data, perform last-layer retraining on the held-out dataset instead of ERM on the initial dataset. We call this technique a "free lunch" because it improves worst-group accuracy with _no additional data or annotations_ beyond ERM (including for model selection). In particular, we utilize less data for ERM training and less compute due to the efficient nature of last-layer retraining. Therefore, we believe this method is especially relevant to practitioners, and it can be easily implemented with little change to data processing or model training workflows.

A critical remaining question is _why_ last-layer retraining improves group robustness; since the training and held-out datasets have equally drastic group imbalance, it is counterintuitive that reducing the quantity of data used for ERM and performing last-layer retraining would increase worst-group accuracy. In some cases, as seen on MultiNLI in Figure 2, training an ERM model with less data can be detrimental - but on the other three datasets, last-layer retraining substantially improves over ERM. We leave it to future empirical and theoretical work to better understand this phenomenon.

## 5 Selective last-layer finetuning

While class-balanced last-layer retraining can improve worst-group accuracy without group annotations, its performance is still inferior to DFR, particularly on the highly group-imbalanced CelebA and MultiNLI datasets. In these cases, training on the entire held-out set can be detrimental; instead, we show that constructing the reweighting dataset by detecting and upsampling worst-group data is more effective. We propose _selective last-layer finetuning_ (SELF), which uses an auxiliary model to select a small, more group-balanced reweighting dataset, then _finetunes_ the ERM last layer _instead of retraining entirely_ to avoid overfitting on this smaller dataset.

    &  &  \\   & Group & Class & Waterbirds & CelebA & CivilComments & MultiNLI \\  Class-balanced ERM & ✗ & ✓ & \(81.9_{ 3.4}\) & \(67.2_{ 5.6}\) & \(61.4_{ 0.7}\) & \(69.2_{ 1.6}\) \\ CB last-layer retraining & ✗ & ✓ & \(92.6_{ 0.8}\) & \(73.7_{ 2.8}\) & \(80.4_{ 0.8}\) & \(64.7_{ 1.1}\) \\  Random SELF & ✗ & ✗ & \(91.1_{ 1.9}\) & \(80.2_{ 6.4}\) & \(}\) & \(65.0_{ 4.0}\) \\ Misclassification SELF & ✗ & ✓ & \(92.6_{ 0.8}\) & \(83.0_{ 6.1}\) & \(62.7_{ 4.6}\) & \(72.2_{ 2.2}\) \\ ES misclassification SELF & ✗ & ✓ & \(92.2_{ 0.7}\) & \(80.4_{ 3.9}\) & \(65.8_{ 7.6}\) & \(}\) \\ Dropout disagreement SELF & ✗ & ✗ & \(92.3_{ 0.5}\) & \(}\) & \(69.9_{ 5.2}\) & \(68.7_{ 3.4}\) \\ ES disagreement SELF & ✗ & ✗ & \(}\) & \(83.9_{ 0.9}\) & \(79.1_{ 2.1}\) & \(70.7_{ 2.5}\) \\  DFR (our impl.) & ✓ & ✓ & \(92.4_{ 0.9}\) & \(87.0_{ 1.1}\) & \(81.8_{ 1.6}\) & \(70.8_{ 0.8}\) \\ DFR  & ✓ & ✓ & \(91.1_{ 0.8}\) & \(89.4_{ 0.9}\) & \(78.8_{ 0.5}\) & \(72.6_{ 0.3}\) \\   

Table 4: **Comparison of selective last-layer finetuning methods.** SELF nearly matches DFR and improves WGA over class-balanced (CB) last-layer retraining by up to 12%. Early-stop (**ES**) disagreement SELF performs the best overall, and disagreement methods perform especially well on CivilComments. Dropout and ES disagreement request few as 20 class annotations from the held-out dataset. All SELF methods use a small set of group annotations for model selection. We list the mean and standard deviation over three independent runs.

SELF does not use group annotations for training and can be implemented even when class annotations are unavailable in the held-out dataset, making it applicable in settings not captured by current techniques (_e.g.,_ adaptation to an unlabeled target domain ). If class annotations _are not_ available, we select points which are disagreed upon by ERM and a regularized model, then request their labels. If class annotations _are_ available, we can still use disagreement, or alternatively, we select points which are misclassified by the ERM or early-stopped model. Our experiments indicate that _disagreement SELF between the ERM and early-stopped models_ performs the best overall, increasing the worst-group accuracy of class-balanced last-layer retraining to near-DFR levels while requesting less than 3% of the held-out class annotations. Our results are detailed in Table 4.

We formalize SELF as follows. Let \(f,g:^{||}\) be models with logit outputs, \(X\) be a held-out dataset, \(c:^{2||}\) be a cost function, and \(n\) be an integer. SELF constructs the reweighting set \(D\) by greedily selecting the \(n\) points in \(X\) whose outputs incur the highest cost, _i.e.,_

\[D=*{argmax}_{S X,|S|=n}_{x S}c(f(x),g(x)).\] (1)

Then, SELF requests class annotations and performs class-balanced _finetuning_ (_i.e.,_ starting from the ERM weights) of the last layer of the ERM model on \(D\), using the same implementation as last-layer retraining (see Section 4). We study the following four variants of SELF:

* _Misclassification_: \(f\) is an ERM model and \(g\) is the labeling function, _i.e.,_, for a datapoint \(x\) with label \(y\), the output \(g(x)\) is the one-hot encoded vector for \(y\). The cost function \(c\) is cross entropy loss.
* _Early-stop misclassification_:6\(f\) is an early-stopped ERM model and \(g\) is the labeling function. The cost function \(c\) is cross entropy loss. * _Dropout disagreement_: \(f\) is an ERM model and \(g\) is the same model where inference is run with nodes randomly dropped out in the last layer . The cost function \(c\) is KL divergence with softmax.
* _Early-stop disagreement_: \(f\) is an ERM model and \(g\) is an early-stopped ERM model. The cost function \(c\) is KL divergence with softmax.

We also experiment with a pure random baseline, referred as Random SELF in Table 4, where \(D\) consists of \(n\) random points from \(X\). While Random SELF suffers high variance as expected, perhaps surprisingly its performance is still competitive on average. The reweighting dataset size \(n\) is a hyperparameter, and for disagreement SELF it corresponds to the quantity of class annotations requested. Please see Appendices B and D for hyperparameter details and ablation studies.

### Analysis of SELF performance

To quantify the extent of SELF's upsampling, we plot the percentage of the reweighting dataset consisting of worst-group data in Figure 3. In particular, we present to the best of our knowledge the first evidence that model disagreement effectively upsamples worst-group data, an observation which may be of independent interest. With that said, Figure 3 does not formally establish that better group balance has a strong correlation with WGA - rather, it suggests an _additional subtlety in the DFR hypothesis_, since group balance alone does not fully explain the performance of last-layer retraining. In addition to the balance of the reweighting dataset, it is likely that characteristics of the _specific data selected_ also contribute to SELF results. Additionally, the competitive performance of pure random SELF in Table 4 further questions the importance of group balancing in DFR.

The importance of which data are selected could explain why disagreement SELF often outperforms misclassification SELF despite having access to less information. While misclassification selects the _most difficult_ or _noisiest_ points, disagreement selects the _most uncertain_ points. For example, dropout models approximate a theoretically justified uncertainty metric  which is likely to be higher on worst-group data, and early-stopped models tend to fit simple patterns first [2; 41] including the majority group. Training on the most uncertain data is a key tenet of active learning [13; 59; 12], which rationalizes the performance of disagreement SELF and provides motivation for further investigation of _why_ last-layer retraining improves group robustness.

A remaining question is why the performance of disagreement SELF is so strong on CivilComments compared to misclassification-based methods such as JTT  and ES misclassification SELF. We show in Figure 6 (deferred to Appendix B) that, contrary to the assumptions made by JTT and other early-stop misclassification methods, the worst-group accuracy _decreases_ with training on CivilComments. This highlights a potential advantage of disagreement methods over misclassification methods, as disagreement is justified _regardless of whether the regularized or ERM model has a greater dependence on spurious features_. Moreover, while misclassification methods do indeed upsample the minority group in Figure 3, we show in Table 10 (deferred to Appendix B) that the held-out set training accuracy of misclassification SELF tends to be very low - down to \(0\%\) on MultiNLI - evidence that misclassifications are too difficult to learn without modifying the features.

Theoretical proof-of-concept.The observations of Section 5.1 raise the question of whether disagreement SELF can ever be shown to _provably upsample minority group points_. We provide a simple proof-of-concept for SELF by considering the last layer to be a linear model (which could be under- or overparameterized) with core, spurious and junk features. We show that the disagreement between the regularized model and the ERM model (measured by total variation distance between the predicted distributions) is provably higher on minority examples than majority examples, _regardless of model dependence on the spurious feature_. In particular, this shows provable benefits of disagreement SELF even in situations where the early-stopped model has higher worst-group accuracy than the convergent model, which may not be captured by related methods in the literature . Our detailed setup, assumptions, and main theoretical result (Theorem 1) are stated in detail in Appendix C.

## 6 Conclusion

In this paper, we presented a comprehensive examination of the performance of last-layer retraining in the absence of group and class annotations. We showed that class-balanced last-layer retraining is a simple but strong baseline for group robustness, and that holding out a subset of the training data to retrain the last layer can substantially improve worst-group accuracy. We then proposed selective last-layer finetuning (SELF), whose early-stop disagreement version improves performance to near-DFR levels with no group annotations and less than \(3\%\) of the held-out class annotations.

Our work has generated several open questions which could prove fruitful for further research. First, why does last-layer retraining on a held-out split of the training dataset improve group robustness (see Section 4.2)? Second, to what extent does the precise data selected matter for reweighting, _e.g.,_ which of disagreement SELF and misclassification SELF would perform better when hyperparameters are set to equalize group balance? Third, what is the optimal strategy in general for obtaining the regularized model in disagreement SELF (dropout, early-stopping, or a different technique) and why?

Figure 3: **SELF upsamples the worst group. We plot the percentage of the reweighting dataset consisting of worst-group data for each SELF method. “Baseline” represents the percentage of worst-group data in the held-out dataset, while “Balanced” is the percentage of worst-group data necessary to achieve group balance. This is to the best of our knowledge the first empirical evidence that model disagreement is an effective method for upsampling worst-group data. The worst groups in each dataset are listed in Section 4.1. We plot the mean for the best dataset size \(n\) over three independent runs. See Table 15 for detailed results.**

Acknowledgments.We thank Google Cloud for the gift of compute credits, Jacob Abernethy and Eva Dyer for the additional compute assistance, and anonymous reviewers for their helpful feedback. We also thank Harikrishna Narasimhan for reading an earlier version of the manuscript and providing thoughtful comments and Seokhyeon Jeong for catching an error in an earlier version of the code. T.L. acknowledges support from the DoD NDSEG Fellowship. V.M. acknowledges support from the NSF (through CAREER award CCF-2239151 and award IIS-2212182), an Adobe Data Science Research Award, an Amazon Research Award, and a Google Research Collabs Award.