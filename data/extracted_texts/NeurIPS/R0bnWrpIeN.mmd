# CoSy: Evaluating Textual Explanations of Neurons

Laura Kopf\({}^{1,2}\)  Philine Lou Bommer\({}^{1,3}\)  Anna Hedstrom\({}^{1,3,4}\)  Sebastian Lapuschkin\({}^{4}\)  Marina M.-C. Hohne\({}^{3,5}\)  Kirill Bykov\({}^{1,2,3}\)

\({}^{1}\)TU Berlin, Germany \({}^{2}\)BIFOLD, Germany \({}^{3}\)UMI Lab, ATB Potsdam, Germany

\({}^{4}\)Fraunhofer Heinrich-Hertz-Institute, Germany \({}^{5}\)University of Potsdam, Germany

kopf@tu-berlin.de

{pbommer,ahedstroem,mhoehne,kbykov}@atb-potsdam.de

sebastian.lapuschkin@hhi.fraunhofer.de

###### Abstract

A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While methods exist to connect neurons to human-understandable textual descriptions, evaluating the quality of these explanations is challenging due to the lack of a unified quantitative approach. We introduce CoSy (Concept Synthesis), a novel, architecture-agnostic framework for evaluating textual explanations of latent neurons. Given textual explanations, our proposed framework uses a generative model conditioned on textual input to create data points representing the explanations. By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation. We validate our framework through sanity checks and benchmark various neuron description methods for Computer Vision tasks, revealing significant differences in quality. We provide an open-source implementation on GitHub1.

## 1 Introduction

One of the key obstacles to the wider adoption of Machine Learning methods across various fields is the inherent opacity of modern Deep Neural Networks (DNNs)--in essence, we often lack an understanding of why these models make certain predictions. To address this problem, various explainability methods  have been developed to make the decision-making processes of DNNs more understandable to humans. Explainability methods have broadened their focus from interpreting the decision-making of DNNs _locally_--for instance, interpreting specific inputs using saliency maps --to understanding the _global_ behavior of models by analyzing individual model components and their functional purpose . Following the latter global explainability approach, often referred to as _mechanistic interpretability_, some methods aim to describe the specific concepts neurons have learned to detect , enabling analysis of how these high-level concepts influence network predictions.

A popular approach for explaining the functionality of latent representations of a network is to describe neurons using human-understandable textual concepts. A textual description is assigned to a neuron based on the concepts that the neuron has learned to detect or is significantly activated by. Over time, these methods have evolved from providing label-specific descriptions  to more complex compositional  and open-vocabulary explanations . However, a significant challenge remains: the lack of a universally accepted quantitative evaluation measure for open-vocabulary neuron descriptions. As a consequence, different methods devised their own evaluation criteria, making it difficult to perform general-purpose, comprehensive cross-comparisons.

In this work, we aim to bridge this gap by introducing Concept Synthesis (CoSy), the first automatic evaluation framework for textual explanations of neurons in Computer Vision (CV) models (illustrated in Figure 1). Our approach builds on recent advancements in Generative AI, which enable the generation of synthetic images that align with provided neuron explanations. We use a set of available text-to-image models to synthesize data points that are prototypical for specific target explanations. These data points allow us to evaluate how neurons differentiate between concept-related images and non-concept-related images combined in a control dataset. We summarize our contributions as below:

* We provide the first general-purpose, quantitative evaluation framework CoSy (Section 3) that enables the evaluation of individual or a set of textual explanation methods for CV models.
* In a series of sanity checks (Section 4), we analyze the choice of generative models and prompts for synthetic image generation, demonstrating framework reliability.
* We benchmark existing explanation methods (Section 5) and extract novel insights, revealing substantial variability in the quality of explanations. Generally, textual explanations for lower layers are less accurate compared to those for higher layers.

## 2 Related Works

Activation MaximizationActivation Maximization is a commonly used methodology to understand what a neuron has learned to detect . Such methods work by identifying input signals that trigger the highest activation in a neuron. This can be achieved synthetically, where an optimization process is employed to create the optimal input that maximizes the neuron's activation [18; 19; 20], or naturally, by finding such inputs within a data corpus . Activation Maximization has been employed for explaining latent representations of models [22; 23], including probabilistic models , detection of backdoor attacks  and spurious correlations . However, one of the key limitations of this methodology lies in its inability to scale, as it relies on users to manually audit maximization signals.

Automatic Neuron InterpretationA more scalable alternative approach involves linking neurons with human-understandable concepts through textual descriptions. Network Dissection  (NetDiscsect) is a pioneering method in this field, associating convolutional neurons with a concept based on the Intersection over Union (IoU) of neuron activation maps and ground truth segmentation masks.

Figure 1: A schematic illustration of the CoSy evaluation framework for Neuron 80 in ResNet18’s avgpool layer. The current challenge lies in the absence of general-purpose, quantitative evaluation measures to benchmark textual explanations of neurons. To address this, we propose CoSy, a framework consisting of three steps: first, a generative model translates textual concepts into the visual domain, creating synthetic images for each explanation using a text-to-image model. Then, inference is performed on these synthetic images alongside a control image dataset to collect neuron activations. Finally, by comparing activations from the synthetic images with those from the control dataset, we quantitatively assess the quality of the textual explanation and compare results across different explanation methods. The implementation details of this example can be found in Appendix A.2.

Building on this, Compositional Explanations of Neurons (CompExp)  enhanced explanation detail by enabling the use of compositional concepts--i.e., concepts constructed with logical operators. MILAN  further expanded this by allowing for open-vocabulary explanations, permitting the generation of descriptions beyond predefined labels. INVERT  adopted a compositional concept approach, enabling explanations for general neuron types without the need for segmentation masks. It assigns compositional labels based on a neuron's ability to distinguish concepts, using the Area Under the Receiver Operating Characteristic Curve (AUC). FALCON  and CLIP-Dissect  compute image-text similarity with a CLIP model  for the most activating images and their corresponding captions or concept sets. Each method defines its optimization criteria, lacking a unified consensus on what constitutes a good explanation. For detailed descriptions of the methods and their optimization objectives, please refer to Appendix A.1. An overview of the different techniques is illustrated in Table 1.

Prior Methods for EvaluationWhile significant effort has been made towards developing approaches and tools for evaluating _local_ explanations [28; 29; 30], there has been relatively limited focus on evaluating _global_ methods, in particular neuron description methods. Currently, to the best of our knowledge, there is no unified approach that allows for benchmarking across models and explanation methods. In their respective papers, the INVERT and CLIP-Dissect explanation methods evaluated the accuracy of their explanations by comparing the generated neuron labels with ground truth descriptions provided for neurons in the output layer of a network. However, this evaluation is limited to output neurons and fixed labels only. CLIP-Dissect additionally evaluates the quality of explanations by computing the Cosine Similarity in a sentence embedding space between the ground truth class name for each neuron and the explanation generated by the method. FALCON employs a human study conducted on Amazon Mechanical Turk to evaluate the concepts generated by the method. Participants are tasked with selecting the best explanation for each target feature from a selection of explanation methods, considering a given set of highly and lowly activating images. MILAN evaluates the performance of neuron labeling methods relative to human annotations using BERTScores . While human studies are generally beneficial, the conventional setup can be misleading and may fail to fully capture the intended evaluation criteria, introducing potential biases. Typically, annotators describe the images that most strongly activate a neuron, and these descriptions are then compared to an automatic explanation. However, this approach primarily evaluates the alignment with the most activating images rather than the accuracy of the explanation in describing the neuron's function. Moreover, these highly activating images may not accurately represent the neuron's overall behavior, as they only reflect the maximum tail of the distribution.

## 3 Method

In the following section, we introduce CoSy--a first automatic evaluation procedure for open-vocabulary textual explanations for neurons. We first define preliminary notations in Section 3.1, then describe CoSy formally in Section 3.2.

  Method & Explanation & Neuron Type & Target & Black-Box Dependency & Architecture-Agnostic \\  NetDissect  & fixed-label & conv. & IoU & — & ✓ \\ CompExp  & compositional & conv. & IoU & — & ✓ \\ MILAN  & open-vocabulary & conv. & WFMI & img2txt model & ✓ \\ FALCON  & open-vocabulary & predetermined & avg. CLIP score & CLIP & — \\ CLIP-Dissect  & open-vocabulary & scalar & SoftWPMI & CLIP & ✓ \\ INVERT  & compositional & scalar & AUC & — & ✓ \\   

Table 1: Comparison of characteristics of neuron description methods. The columns (from left to right) represent the explanation method used, its textual output type (fixed-label, compositional, or open-vocabulary), the type of neuron targeted for analysis (convolutional, scalar, or predetermined), the target metric the method optimizes (IoU, WPMI, AUC, etc.), whether the method relies on auxiliary black-box models for finding or generating explanations (img2txt model, CLIP), and whether the explanation method is architecture-agnostic, meaning it can be applied to any CV model. For a more detailed description of each method, refer to Appendix A.1.

### Preliminaries

Consider a Deep Neural Network (DNN) represented by the function \(g:,\) where \(^{h w c}\) denotes the input image domain and \(^{l}\) represents the model's output domain. We can view the model as a composition of two functions, \(F:,\) and \(L:,\) such that \(g=L F\). Here \(^{d w^{*} h^{*}}\), where \(d\) is the number of neurons in the layer, and \(w^{*},h^{*}\) represent the width and height of the feature map, respectively. The function \(F\), which we refer to as the _feature extractor_, can be chosen based on the layer of the model we aim to inspect. This could be an existing layer within the model or a concept bottleneck layer . We refer to the \(i\)-th neuron within the layer as \(f_{i}()=F_{i}():^{w^{*} h^ {*}}.\) Within the scope of this paper, we refer to _explanation method_ as an operator \(\) that maps a neuron to the textual description \(s=(f_{i}),\) where \(\) is a set of potential textual explanations. The specific set of explanations depends on the implementation of the particular method (see Appendix A.1).

### CoSy: Evaluating Open-Vocabulary Explanations

We assume that a good textual explanation for a neuron should provide a human-understandable description of an input that strongly activates the neuron. However, modern methods for explaining the functional purpose of neurons often provide open-vocabulary textual explanations, complicating the quantitative collection of natural data that represents the explanation. To address this issue, CoSy utilizes recent advancements in generative models to synthesize data points that correspond to the textual explanation. The response of a neuron to a set of synthetic images is measured and compared to the neuron's activation on a set of control natural images representing random concepts. This comparison allows for a quantitative evaluation of the alignment between the explanation and the target neuron.

The parameters of the proposed method include a control dataset \(_{0}=\{_{1}^{0},,_{n}^{0}\},n ,\) which consists of natural images representing the concepts on which the model was originally trained. Additionally, it incorporates a generative model \(p_{M}\) used for synthesizing images, along with a specified number of generated images \(m\). The control dataset typically includes a balanced selection of validation class images. Given a neuron \(f_{i}\) and explanation \(s\), CoSy evaluates the alignment between the explanation and a neuron in three consecutive steps, which are illustrated in Figure 1.

1. **Generate Synthetic Data.** The first step involves generating synthetic images for a given explanation \(s,\) which we use as a prompt to a generative model \(p_{M}\) to create a collection of synthetic images, denoted as \(_{1}=\{_{1}^{1},,_{m}^{1}\} p_{M}( s).\) This collection consists of \(m\) images, where \(m\) is adjustable as a parameter of the evaluation procedure.
2. **Collect Neuron Activations.** Given the control dataset \(_{0}\) and the set of generated synthetic images \(_{1}\), we collect activations as follows: \[_{0}=\{(f_{i}(_{1}^{0})),,(f_{i}(_{n}^ {0}))\}^{n},\] (1) \[_{1}=\{(f_{i}(_{1}^{1})),,(f_{i}( _{m}^{1}))\}^{m},\] where \(:^{w^{*} h^{*}}\) is an aggregation function for multi-dimensional neurons. Within the scope of our paper, we use Average Pooling as aggregation function \[()=h^{*}}_{k[1,w^{*}],l[1,h^{*}]} ^{w^{*} h^{*}}.\] (2)
3. **Score Explanations.** The final step of the proposed method relies on the evaluation of the difference between neuron activations on the control dataset \(_{0}\) and neuron activations given the synthetic dataset \(_{1}\). To quantify this difference, we utilize a _scoring function_\(:^{n}^{m}\) to measure the difference between the distributions of activations.

In the context of our paper, we employ the following scoring functions:

* **Area Under the Receiver Operating Characteristic (AUC)** AUC is a widely used non-parametric evaluation measure for assessing the performance of binary classification. In our method, AUC measures the neuron's ability to distinguishbetween synthetic and control data points \[_{}(_{0},_{1})=_{0}} _{b_{1}}[a<b]}{|_{0}||_{1}|}.\] (3)
* **Mean Activation Difference (MAD)** MAD is a parametric measure that quantifies the difference between the mean activation of the neuron on synthetic images and the mean activation on control data points \[_{}(_{0},_{1})=_{b _{1}}b-_{a_{0}}a}{ _{a_{0}}(a-)^{2}}},\] (4) with mean control activation \(=_{a_{0}}a\).

These two chosen metrics complement each other. AUC, being non-parametric and stable to outliers, evaluates the classifier's ability to rank synthetic images higher than control images (with scores ranging from 0 to 1, where 1 represents a perfect classifier and 0.5 is random). On the other hand, MAD allows us to parametrically measure the extent to which images corresponding to explanations maximize neuron activation.

## 4 Sanity Checks

To ensure the reliability of our proposed evaluation measure, all steps within our framework need to be subject to sanity checks . In this section, we analyze the following: (1) which generative models and prompts provide the best similarity to natural images, (2) whether the model's behavior on synthetic and natural images differs for the same class, and (3) validating that CoSy provides appropriate evaluation scores for true and random explanations, given a known ground truth class for the neuron.

### Synthetic Image Reliability

One of the key features of CoSy is its reliance on generative models to translate textual explanations of neurons into the visual domain. Thus, it is essential that the generated images reliably resemble the textual concepts. In the following section, we present an experiment where we varied several parameters of the generation procedure and evaluated the visual similarity between generated images and natural ones, focusing on concepts for which we have a collection of natural images.

For our analysis, we used only open-source and freely available text-to-image models, namely Stable Diffusion XL 1.0-base (SDXL)  and Stable Cascade (SC) . We also varied the prompts for image generation. To measure the similarity between synthetic images and natural images corresponding to the same concept, we used Cosine Similarity (_CS_) in the CLIP embedding space with the CLIP-ViT-B/32 model . We select a set of 50 random concepts from the 1,000 classes in the ImageNet validation dataset . For each [concept] we use five different prompts and employ them with SDXL and SC models, generating 50 images per class. We then measure the _CS_ between image pairs of the same class.

Figure 2 illustrates the comparison across all generative models and prompts in terms of _CS_ of generated images to natural images of the same class. The results indicate that when using Prompt 5 as input to SDXL, the synthetic images show the highest similarity to natural images. The performance is generally best with the most detailed prompt (5) and closely aligns with prompts 1, 3, and 4. Moreover, SDXL appears to be slightly more effectively realizing detailed prompts than SC. As anticipated, the poorly constructed prompt (2) results in the lowest similarity to natural images for both models. To address prompt bias and dataset dependency, we compare the object-focused ImageNet with the scene-focused Places365 (see Appendix A.3). We find that close-up prompts work well for object-centric datasets, while general prompts like "photo of" are better for scene-based datasets. If not stated otherwise, for all following experiments, Prompt 5 together with SDXL model was employed for image generation.

### Do Models Respond Differently to Synthetic and Natural Images?

Given the visual similarity between natural and synthetic images of the same class, we investigate whether CV models respond differently to these groups and if the activation differences indicate adversarial behavior. To this end, we employed four different models pre-trained on ImageNet: ResNet18, DenseNet161, GoogleNet, and ViT-B/16. For each model, we randomly selected 50 output classes and generated 50 images per class using the class descriptions. We pass both synthetic and natural images through the models, collecting the activations of the output neuron corresponding to each class.

Figure 3 (a) illustrates the distributions of the MAD between synthetic and natural images for the same class across the 50 classes. Across all models, we observe that the median activation of synthetic images is slightly higher than that of natural images of the same class. However, this difference is small, given the 0 value lies within 1 standard deviation. We also illustrate the activations of neuron \(504\) in the ResNet18 output layer for the "coffee mug" class in Figure 3 (b). The results indicate a strong overlap in the neural response to both synthetic and natural images. While synthetic images activate the neuron slightly more, this does not constitute an artifactual behavior or affect our framework, which we demonstrate in the following experiment.

Figure 3: An overview of analyses performed to study the similarity between natural and synthetic images. From left to right: (a) an overview of MAD scores between synthetic and natural image activations of the output neuron’s ground truth classes for each model studied in this work, (b) activations collected for neuron \(504\) in ResNet18 for the class “coffee mug”, showcasing the difference between the natural and synthetic distributions and (c) examples of natural versus synthetic images. In both analyses, we observe a substantial overlap in the activations of synthetic and natural images, suggesting that the models respond similarly to both types of images.

Figure 2: An overview of the impact of varying the prompt on the similarity between natural and synthetic images, using two text-to-image models. Left: average Cosine Similarity (_CS_) across all natural and synthetic images over all classes are reported. Higher _CS_ values are better, indicating greater similarity between the images. Right: an illustration of the visual differences produced by the SDXL and SC models in response to diverse prompts for the concept “submarine”, and natural images from the ImageNet validation dataset . Our results show that both SDXL and SC generate similar images, with SDXL generally being more closely aligned with natural images than SC.

### Random Baseline

A robust evaluation metric should reliably discern between random explanations resulting in low scores and non-random explanations resulting in high scores. To assess our evaluation framework regarding this requirement, we evaluated the results of the CoSy evaluation by comparing the scores of ground truth explanations with those of randomly selected explanations.

Following the experimental setup in Section 4.2, we selected a set of 50 output neurons and compared the CoSy scores of the ground truth explanations, given by the neuron label, with those of randomly selected explanations. The results, presented in Table 2, consistently demonstrate high scores for true explanations and low scores for random explanations. This experiment provides further evidence supporting the correctness of the proposed evaluation procedure. An additional experiment that excludes the target class from the control dataset is presented in Appendix A.4, along with an analysis of the robustness of the evaluation measure detailed in Appendix A.6.

## 5 Evaluating Explanation Methods

Within the scope of this section, we produce a comprehensive cross-comparison of various methods for the textual explanations of neurons. For this comparison, we employed models trained on different datasets, and we conducted our analysis on the latent layers of the models, where no ground truth is known.

### Benchmarking Explanation Methods

In this section, we evaluated three recent textual explanation methods, namely MILAN, INVERT, and CLIP-Dissect. Our analysis involves six distinct models: four pre-trained on the ImageNet dataset  (ResNet18, ResNet50, ViT-B/16 , DINO ViT-S/8 ) and two pre-trained on the Places365 dataset  (DenseNet161, ResNet50). The ImageNet dataset focuses on objects, whereas the Places365 dataset is designed for scene recognition. Consequently, we customized our prompts accordingly: Prompt 5 performs best for object recognition, while for scene recognition, we found that Prompt 3 is more effective. Therefore, Prompt 3 was utilized in the Places365 experiment.

For generating explanations with the explanation methods, we use a subset of 50,000 images from the training dataset on which the models were trained. For evaluation with CoSy, we use the corresponding validation datasets the models were pre-trained on as the control dataset. Additionally, for CLIP-Dissect, we define concept labels by combining the 20,000 most common English words with the corresponding dataset labels. For INVERT we set the compositional length of the explanation as \(L=1\), where \(L\). For more details on compute resources, refer to Appendix A.7.

Results of the evaluation can be found in Table 3. Overall, INVERT achieves the highest AUC scores across all models and datasets, except for DINO ViT-S/8 and ResNet18 applied to ImageNet, where CLIP-Dissect achieves a higher or similar score. Also across other models and datasets, CLIP-Dissect demonstrates consistently good results. Since INVERT optimizes AUC in explanation generation, it

   Model & AUC (\(\)) &  \\   & True & Random & True & Random \\  ResNet18 & 0.98\(\)0.09 & 0.47\(\)0.21 & 6.46\(\)2.07 & -0.11\(\)0.79 \\ DenseNet161 & 0.99\(\)0.08 & 0.44\(\)0.22 & 7.11\(\)1.82 & -0.19\(\)0.73 \\ GoogLeNet & 0.99\(\)0.07 & 0.48\(\)0.23 & 7.74\(\)2.14 & -0.06\(\)0.78 \\ ViT-B/16 & 0.99\(\)0.05 & 0.49\(\)0.22 & 13.12\(\)3.19 & 0.09\(\)1.10 \\   

Table 2: Comparison of true and random explanations on output neurons with known ground truth labels. This table presents the average quality scores (with standard deviations) for true explanations, derived from target class labels, and random explanations, derived from randomly selected synthetic image classes (including the target class), across four models pre-trained on ImageNet. Higher values are better. Our results consistently show high scores for true explanations and low scores for random ones.

may be biased towards AUC in our evaluation, leading to higher scores. MILAN generally performs poorly, with an average AUC below 0.65 across all tasks, indicating performance close to random guessing. MILAN tends to generate highly abstract explanations, such as "white areas", "nothing" or "similar patterns". These abstract concepts are particularly challenging for a text-to-image model to generate accurately, likely contributing significantly to the low scores of MILAN. Contrary to the AUC scores, the MAD scores suggest that CLIP-Dissect outperforms INVERT for convolutional neural networks applied to both datasets. Nonetheless, in these cases, INVERT concepts also achieve consistently high scores. Otherwise, we find similar outcomes for both metrics \(\), with MILAN achieving poor scores in all experimental settings.

### Explanation Methods Struggle to Explain Lower Layer Neurons

In addition to the general benchmarking, we aimed to study the quality of explanations for neurons in different layers of a model. Since it is well known that lower-layer neurons usually encode lower-level concepts , it is interesting to see whether explanation methods can capture the concepts these neurons detect. To investigate this, we examined the quality of explanations across layers 1 to 4 and the output layer of an ImageNet pre-trained ResNet18. In addition to three prior explanation methods, we included the FALCON method in our analysis. For more details on the implementation of FALCON see Appendix A.1.4, for additional results of the original FALCON implementation see Appendix A.8, and for qualitative examples and a discussion of lower-level concepts see Appendix A.9. For each layer, we randomly selected 50 neurons for analysis.

In Figure 4 we present the AUC and MAD results for all explanation methods across layers \(1\) to \(4\) and the output layer of ResNet18. While less pronounced for the AUC metric, in general, we find increasing scores for later layers across all methods and both metrics \(\), which suggest higher concept quality in later layers. Furthermore, we find that similar to the benchmarking experiments, MILAN achieves lower scores across metrics. Both MILAN and FALCON consistently show lower performance, with AUC scores of \(0.5\) indicating random guessing. Nonetheless, we point out that these methods typically output semantically high-level concepts. Potentially, this is related to the inherent difficulty in describing low-level abstractions in natural language given their complexity (see Appendix A.10).

   Dataset & Model & Layer & Method & AUC (\(\)) & MAD (\(\)) \\   &  &  & MILAN & 0.61\(\)0.23 & 0.69\(\)1.35 \\  & & & CLIP-Dissect & **0.93\(\)0.11** & **3.85\(\)1.88** \\  & & & INVERT & **0.93\(\)0.11** & 3.23\(\)1.72 \\   &  &  & MILAN & 0.44\(\)0.23 & -0.08\(\)0.72 \\  & & & CLIP-Dissect & 0.95\(\)0.08 & **4.98\(\)2.57** \\  & & & INVERT & **0.96\(\)0.06** & 4.62\(\)2.26 \\   &  &  & MILAN & 0.53\(\)0.19 & 0.12\(\)0.76 \\  & & & CLIP-Dissect & 0.78\(\)0.19 & 1.29\(\)1.01 \\  & & & INVERT & **0.89\(\)0.17** & **1.67\(\)0.82** \\   &  &  & MILAN & 0.59\(\)0.21 & 0.37\(\)0.91 \\  & & & CLIP-Dissect & **0.95\(\)0.08** & **4.59\(\)2.62** \\  & & & INVERT & 0.73\(\)0.27 & 2.70\(\)3.48 \\   &  &  & MILAN & 0.56\(\)0.28 & 0.44\(\)1.30 \\  & & & CLIP-Dissect & 0.82\(\)0.21 & **2.52\(\)2.33** \\   & & & INVERT & **0.85\(\)0.16** & 2.21\(\)1.95 \\    &  & MILAN & 0.65\(\)0.28 & 1.11\(\)1.67 \\   & & & CLIP-Dissect & 0.92\(\)0.11 & **3.73\(\)2.39** \\   & & & INVERT & **0.94\(\)0.08** & 3.54\(\)1.99 \\   

Table 3: Benchmarking of neuron description methods, for neurons in the second to last layer across different models. Explanations are generated for a randomly selected set of 50 neurons, with average scores for both AUC and MAD reported alongside standard deviations. Higher values indicate better performance; **bold** numbers represent the highest scores.

### What are Good Explanations?

In our approach, we propose that testing visual representations of textual explanations on neurons can provide insights into what constitutes good explanations. Building on this premise, we observe consistently high results from CLIP-Dissect and INVERT. The qualitative examples in Figure 5 demonstrate that their explanations share visually similar concepts (neurons \(155\) and \(459\)) or even identical concepts (neuron \(221\)) while both achieving high AUC and MAD scores. It is important to note that although INVERT performs slightly better in several tasks, the explanations are constrained to the input data labels. In contrast, CLIP-Dissect can generate descriptions from a broader selection of concepts, though its reliance on a black-box model reduces interpretability compared to INVERT.

There are instances, such as neuron \(260\) in Figure 5, where all explanations vary significantly. In these cases, we find that the explanation activation distributions of FALCON and MILAN often overlap with or even match the control dataset, providing the user with nearly random explanations. This observation aligns with our overall findings: both the AUC and MAD scores consistently reflect the

Figure 4: A comparison of how different explanation methods vary in their quality, as measured by (a) AUC and (b) MAD, across different layers in ResNet18. INVERT and CLIP-Dissect maintain high AUC and MAD scores across all layers, while MILAN and FALCON have lower scores. Overall, performance declines in the lower layers for all methods.

Figure 5: A qualitative example, of neuron explanations across four neurons. The first four panels include the textual explanation across INVERT, FALCON, CLIP-Dissect, and MILAN alongside three corresponding generated images. The respective AUC and MAD scores are reported below each panel. The last panel shows the activation distributions across \(50\) generated images for each method and the distribution of the control data.

low performance of FALCON and MILAN explanations in the CoSy evaluation. Also, neurons \(459\) and \(155\) demonstrate the gap between consistently higher and lower-performing explanation methods.

## 6 Conclusion

In this work, we propose the first automatic evaluation framework for textual explanations of neurons. Unlike existing ad-hoc evaluation methods, we can now quantitatively compare different neuron description methods against each other and test, whether the given explanation describes the neuron accurately, based on its activations. We can evaluate the quality of individual neuron explanations by examining how accurately they align with the generated concept data points, without requiring human involvement.

Our comprehensive sanity checks demonstrate that CoSy guarantees a reliable explanation evaluation. In several experiments, we show that neuron description methods are most applicable for the last layers, where high-level concepts are learned. In these layers, INVERT and CLIP-Dissect provide high-quality neuron concepts, whereas MILAN and FALCON explanations have lower quality and can present close to random concepts, which might lead to wrong conclusions about the network. Thus, the results highlight the importance of evaluation when using neuron description methods.

LimitationsThe use of generative models involves a distinct set of limitations. For instance, text-to-image models may not include certain concepts in their training data, which can reduce their generative performance. This limitation, however, can often be addressed by analyzing the pre-training datasets and assessing model performance. Moreover, the model's capabilities of generating highly abstract concepts like "white objects" can be limited. However, the challenges with abstract concepts also reflect the descriptive quality of the provided explanations--explanations should be inherently understandable to humans. In both cases, exploring more sophisticated, specialized, or constrained models may offer improvement.

Future WorkEvaluation of non-local explanation methods is still a largely neglected research area, where CoSy plays an important yet preliminary part. In the future, we need additional, complementary definitions of explanation quality that extend our precise definition of AUC and MAD, e.g., that involve humans to assess plausibility  or evaluate explanation quality via the success of a downstream task . Furthermore, we plan to extend the application of our evaluation framework to additional domains including NLP and healthcare. In particular, it would be interesting to analyze the quality of more recent autointerpretable explanation methods given by highly opaque, large language models (LLMs) . We believe that applying CoSy to healthcare datasets, where high-quality explanations are crucial, represents an impactful next step.