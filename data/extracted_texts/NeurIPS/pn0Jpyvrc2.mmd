# AllClear: A Comprehensive Dataset and Benchmark

for Cloud Removal in Satellite Imagery

Hangyu Zhou\({}^{1}\), Chia-Hsiang Kao\({}^{1}\)1, Cheng Perng Phoo\({}^{1}\),

**Utkarsh Mall\({}^{2}\)**, **Bharath Hariharan\({}^{1}\)**, **Kavita Bala\({}^{1}\)**

\({}^{1}\)Computer Science, Cornell University

\({}^{2}\)Computer Science, Columbia University

###### Abstract

Clouds in satellite imagery pose a significant challenge for downstream applications. A major challenge in current cloud removal research is the absence of a comprehensive benchmark and a sufficiently large and diverse training dataset. To address this problem, we introduce the largest public dataset -- _AllClear_ for cloud removal, featuring 23,742 globally distributed regions of interest (ROIs) with diverse land-use patterns, comprising 4 million images in total. Each ROI includes complete temporal captures from the year 2022, with (1) multi-spectral optical imagery from Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and land cover maps. We validate the effectiveness of our dataset by benchmarking performance, demonstrating the scaling law -- the PSNR rises from \(28.47\) to \(33.87\) with \(30\) more data, and conducting ablation studies on the temporal length and the importance of individual modalities. This dataset aims to provide comprehensive coverage of the Earth's surface and promote better cloud removal results.

## 1 Introduction

Satellite image recognition enables environmental monitoring, disaster response, urban planning , crop-yield prediction , and many more applications, but is held back significantly due to occlusion by clouds. Roughly 67% of the Earth's surface is covered by clouds at any given moment . The limited availability of cloud-free captures is especially problematic for time-sensitive events like wildfire control  and flood damage assessment . Consequently, developing effective cloud removal techniques is crucial for maximizing the utility of remote sensing data in various domains.

A major challenge holding back research into cloud removal is the lack of comprehensive datasets and benchmarks. A survey of publicly available datasets for cloud removal (Table 1) reveals several problems. First, most existing datasets are sampled from a small set of locations and thus have limited geographical diversity , impacting both the effectiveness of training and the rigor of evaluation. Second, many existing datasets filter out very cloudy images (e.g., more than 30% cloud coverage), thus preventing trained models from tackling practical situations with extensive cloud cover  (Figure 1). Third, some existing benchmarks use ground-truth cloud-free images captured at a very different time point from the time the input images are captured . This means that many changes may have occurred on the ground between thecapture of the input and the target images, introducing noise in the evaluation. Finally, existing datasets incorporate a very limited set of sensors/modalities (i.e., Sentinel-2), limiting the information available to models for faithful cloud removal.

To address these limitations and facilitate future research in cloud removal, we introduce the largest and most comprehensive dataset to date, _AllClear_. To ensure sufficient coverage of the planet's diversity, _AllClear_ includes 23,742 regions of interest (ROIs) scattered across the globe with diverse land cover patterns, resulting in four million multi-spectral images. _AllClear_ includes data from three different satellites (i.e., Sentinel-1, Sentinel-2, and Landsat-8/9) captured over a year (2022) at each ROI, allowing models to better interpolate missing information. We use this dataset to create a more rigorous sequence-to-point benchmark with more temporally aligned ground truth. Finally, besides the enormous amount of raw satellite images, we also curated a rich set of metadata for each individual image (e.g., geolocation, timestamp, land cover map, cloud masks, etc.) to support building future models for the cloud removal challenge as well as to enable stratified evaluation.

We evaluate existing state-of-the-art on AllClear and find that existing models are undertrained; training on our larger and more diverse training set significantly improves performance. We also find that models that use the full suite of available sensors as well as a longer temporal sequence of captures perform much better. Taken together, our contributions are:

* We introduce to-date the largest dataset for cloud removal, as well as a comprehensive and stratified evaluation benchmark,
* We demonstrate that our significantly larger and more diverse training set improves model performance, and
* We show empirically the importance of leveraging multiple sensors and longer time spans.

## 2 Background

### Existing Cloud Removal Datasets

Advances in cloud removal research for satellite imagery have led to the development of several datasets with unique characteristics and limitations. STGAN introduced two cloud removal datasets and established the multi-temporal task format of using three images as input (Sarukkai et al., 2020). However, the dataset discards all image crops with more than 30% cloud cover, leading to only

  
**Dataset** & **Regions** & **\# ROIs** & **\# Images** & **Satellites** \\  STGAN (Sarukkai et al., 2020) & Worldwide & 945 & 3,101 & Sentinel-2 \\ Sen2\_MTC (Huang and Wu, 2022) & Worldwide & 50 & 13,669 & Sentinel-2 \\ EarthNet2021 (Requena-Mesa et al., 2021) & Europe & 32,000 & 960,000 & Sentinel-2 \\ SEN12MS-CR (Ebel et al., 2020) & Worldwide & 169 & 366,654 & Sentinel-1/2 \\ SEN12MS-CR-TS (Ebel et al., 2022) & Worldwide & 53 & 917,580 & Sentinel-1/2 \\  AllClear & Worldwide & 23,742 & 4,354,652 & Sentinel-1/2, Landsat-8/9 \\   

Table 1: Summary of publicly available cloud removal datasets.

Figure 1: Left: Geographical distribution of _AllClear_ ROIs; middle: land cover distribution of _AllClear_ for training and testing set; right: cloud coverage distribution of the entire _AllClear_ dataset.

3K images. Following STGAN, Huang and Wu (2022) find that the annotations in STGAN can be incorrect and propose Sen2_MTC with four times more images. The Sen2_MTC dataset first samples 50 tiles globally and proceeds to divide the large tile into pieces, restricting the sampling regional diversity. STGAN and Sen2_MTC also do not describe their _data processing levels_ (e.g., level-1C Top-of-Atmosphere or level-2A Surface Reflectance imagery), making it hard to compare models trained on different datasets. Different from the STGAN and Sen2_MTC datasets, the Sen12MS-CR dataset features synthetic-aperture radar (SAR) images to augment the optics imagery. However, it has a single image pair per data point. The successor is SEN12MS-CR-TS (Ebel et al., 2022), featuring multi-temporal (multiple images per location) multi-modality paired images. For each location, 30 Sentinel-1 and Sentinel-2 images from 2018 are temporally aligned and paired as spatiotemporal patches. However, the temporal differences between the two modalities can be as large as 14 days, and the temporal difference between the input and the target can be as large as a year, resulting in noise in the evaluation. In addition, the authors construct a sequence-to-point cloud removal dataset where images with more than 50% cloud coverage are excluded. EarthNet2021 (Requena-Mesa et al., 2021) also provides sequences of carefully curated Sentinel-2 images with a spatial resolution of 20m and bands of RGB and Infrared. However, the dataset excluded spatiotemporal patches with high cloud coverage and is thus not an ideal dataset for cloud removal.

### Cloud Removal Methodology

Early work on cloud removal used a conditional GAN to map a single image to its cloudless version conditioning on the NIR channel (Enomoto et al., 2017) or SAR images (Grohnfeldt et al., 2018). These early attempts fall short of generalizing to real cloudy images (Ebel et al., 2020, Stucker et al., 2023). Singh and Komodakis (2018) and Ebel et al. (2020) improve this setup by using a cycle-consistency loss. Other approaches learn the mapping from SAR images to their corresponding multi-spectral bands (Bermudez et al., 2018, 2019, Wang et al., 2019, Fuentes Reyes et al., 2019). More recently, with the rise of transformers, multi-head attention modules have been introduced for cloud removal tasks. Yu et al. (2022) cast the cloud as image distortion and designs a distortion-aware module to restore the cloud-free images. Zou et al. (2023) utilize multi-temporal inputs along with a multi-scale attention autoencoder to exploit the global and local context for reconstruction. Ebel et al. (2023) adopt a multi-temporal inputs and attention autoencoder and estimate the aleatoric uncertainty of the prediction, which controls the quality of the reconstruction for risk-mitigation applications. Jing et al. (2023) and Zou et al. (2023) propose to utilize diffusion training objective for cloud-free image generation where the inputs only rely on the optimal images and SAR imagery is not taken into consideration. Similarly but more generally, Khanna et al. (2023) propose a generative foundation model for satellite imagery, but is not tailored for the cloud removal task.

## 3 Dataset

### Regions-of-Interest Selection

We choose our ROIs to satisfy two objectives: (a) coverage of most of the land surface and (b) a balanced sampling of land cover types. This balanced sampling in particular ensures that smaller but more popular locations like cities are as well represented as the large swathes of wilderness. To get these ROIs, we follow a two-step procedure: curating a pool of ROI candidates and then building train/benchmark subgroups balanced across land cover types, as shown in Figure 1. This ensures both the benchmark and the training sets contain a sufficient amount of data representing various land cover types.

For curating the ROI pool, unlike previous work that followed random ROI selection (Sarukkai et al., 2020, Huang and Wu, 2022, Ebel et al., 2020, 2022, Xu et al., 2023), we use grid sampling to select an ROI every 0.1\({}^{}\) latitude and every 0.1\({}^{}\)cos(\(\)) longitude, where \(\) is the latitude, from 90\({}^{}\)S to 90\({}^{}\)N. The intuition behind this approach is that the same 0.1\({}^{}\) longitude can represent 11.1 km at the equator and 4.35 km at 67\({}^{}\) latitude. This weighting provides a simple yet effective method for not over-sampling high-latitude areas. By excluding ocean areas using the GeoPandas package, we select a total of 1,087,947 ROIs.

Next, we select ROIs from the pool to achieve a more balanced dataset over land-cover use while considering the natural imbalance of land cover distribution on the earth's surface. We leverage the land cover data from the Dynamic World product (Brown et al., 2022) from Google Earth Engine,which is a 10-meter resolution Land Use / Land Cover (LULC) dataset containing class probabilities and label information for nine classes: water, tree, grass, flooded vegetation, crops, shrub and scrub, built, bare, and snow and ice. Specifically, we calculate the all-year median of the LULC in 2022 as an estimate for the land use and land cover for each ROI. We iteratively select ROIs from the candidate pool such that the average land cover for all classes (except snow and ice) is greater than 10 percent in the benchmark set and 5 percent in the train set.

Finally, for a fairer comparison with models trained on previous datasets, we take an additional measure to exclude the ROIs that are close to the SEN12MS-CR-TS dataset [Ebel et al., 2022]. Specifically, the size of tiles in the SEN12MS-TR-CS dataset is \(40 40\) km\({}^{2}\). So we exclude the ROIs in AllClear that are within a 50 km radius of the ROIs in SEN12MS-CR-TS.

### Data Preparation

_AllClear_ contains three different types of open-access satellite imagery made available by the Google Earth Engine (GEE) platform [Gorelick et al., 2017]: Sentinel-2A/B [Drusch et al., 2012], Sentinel-1A/B [Torres et al., 2012], and Landsat 8/9 [Williams et al., 2006]. For Sentinel-2, we collected all thirteen bands of Level-1C orthorectified top-of-atmosphere (TOA) reflectance product. For Sentinel-1, we acquired the S1 Ground Range Detected (GRD) product with two polarization channels (VV and VH). As for Landsat 8/9, we collected all twelve bands of Collection 2 Tier 1 calibrated TOA reflectance product. All the raw images in _AllClear_ were resampled to 10-meter resolution. We follow the default GEE preprocessing steps during all the downloading process. In addition, we include the Dynamic World Land Cover Map for all the Sentinel-2 imagery [Brown et al., 2022]. For each selected ROI, our goal is to collect all \(2.56 2.56\) km\({}^{2}\) patches in 2022 with a spatial resolution of 10 meters. We adopt the Universal Transverse Mercator (UTM) coordinate reference system (CRS), following Ebel et al. , Zhao et al. , which divides the Earth into 60 zones, each spanning 6 degrees of longitude, to ensure minimal distortion, especially along the longitude axis. Since satellite imagery do not necessarily conform to the boundaries of UTM zones, gaps (NaN values) can occur where the tile data does not cover the entire ROI. In such cases, we exclude all images containing NaN values to maintain data quality.

**Data Preprocessing.** For Sentinel-1, following Ebel et al. , we clip the values in the VV channel of S1 to \([-25;0]\) and those of the VH channels to \([-32.5,0]\). For Sentinel-2 and Landsat 8/9, we clip the raw values to \(\)[Ebel et al., 2022, Huang and Wu, 2022]. The values are then normalized to the range of \(\).

**Cloud and Shadow Mask Computation.** The cloud and shadow masks are indispensable to this dataset as they are used for guiding evaluation metric computation by masking out regions where there are clouds and shadows in the target images. To obtain the cloud mask, we use the S2 Cloud Probability dataset available on Google Earth Engine. This dataset is built by using S2cloudless [Zupanc, 2017], an automated cloud-detection algorithm for Sentinel-2 imagery based on a gradient boosting algorithm, which shows the best overall cloud detection accuracy on opaque clouds and semi-transparent clouds in the Hollstein reference dataset [Hollstein et al., 2016, Skakun et al., 2022] and the LCD PixBox dataset [Paperin et al., 2021, Spokun et al., 2022].

As for the shadow mask, ideally the cloud shadows can be estimated using the sun azimuth and cloud height but the latter information cannot be obtained. We therefore proceed with curating the shadow mask following documentation in Google Earth Engine [Jdbcode, 2023]. The shadow is estimated by computing dark pixels and projecting cloud regions. For the dark pixels, we use the Scene Classification Map (SCL) band values from Sentinel-2 to remove water pixels, as water pixels can resemble shadows. We then threshold the NIR pixel values with a threshold of 1e-4 to create a map of dark pixels. Finally, we take the intersection of the dark pixel map and the projected cloud regions to obtain the cloud shadow masks.

### Benchmarking Task Setup and Evaluation

For evaluation, we construct a sequence-to-point task using our AllClear dataset with train, validation, and test splits of 278,613, 14,215, and 55,317 samples, respectively. Each instance contains three input images (\(u_{1},u_{2},u_{3}\)), a target clear image (\(v\)), input cloud and shadow masks, target cloud and shadow masks, timestamps, and metadata such as latitude, longitude, sun elevation angle, and sun azimuth. Sentinel-2 images are considered the main sensor modality, while sensors such as Sentinel-1and Landsat-8/9 are auxiliary. Unlike previous datasets (Sarukkai et al., 2020; Requena-Mesa et al., 2021; Ebel et al., 2022), we do not threshold the cloud coverage in the input images. We also provide multiple options for cloud and shadow masks with different thresholds for users to use.

We address two temporal misalignment problems found in previous datasets: misalignment between source and target images (where the difference can be months apart) and misalignment when pairing main sensors with auxiliary sensors (where the difference can be at most two weeks) (Ebel et al., 2022). To avoid temporal misalignment issues, the target clear images are chosen from four consecutive spatial-temporal patches. In particular, the time stamps of the input and target images are either in the order \([u_{1},v,u_{2},u_{3}]\) or in the order \([u_{1},u_{2},v,u_{3}]\). This ensures that the target image does not include any novel or unseen changes that occurred after the capture of the cloudy images. For auxiliary sensors, we select the auxiliary satellite images within a two-day difference from the respective Sentinel-2 images. We fill the corresponding channels with ones if no auxiliary sensor images match are available. More details about the construction of these inputs and targets is in the supplementary.

Note that our target images may still have some clouds (since it is difficult to get a cloud-free image within each time span). To reach a balance between having diverse scenarios and limit metric inaccuracy, we set target images to have less than 10% cloud and shadow (combined) coverage and exclude the cloudy pixels when calculating the metrics. We modified various pixel-based metrics to compute only over the cloud-free areas. We adopt the following metrics common in cloud removal literature: mean absolute error (MAE) (Hodson, 2022), root mean square error (RMSE) (Hodson, 2022), peak signal-to-noise ratio (PSNR) (Hore and Ziou, 2010), spectral angle mapper (SAM) (Kruse et al., 1993), and structural similarity index measure (SSIM) (Wang et al., 2004).

## 4 Experiments

We next evaluate the usefulness of our dataset for both evaluation and training.

### Benchmarking prior methods on the AllClear test set

**Selection of SoTA model architecture.** We choose the state-of-the-art pre-trained models UnCRtainTS (Ebel et al., 2023), U-TILISE (Stucker et al., 2023), CTGAN (Huang and Wu, 2022), PMAA (Zou et al., 2023a), and DiffCR (Zou et al., 2023b) to benchmark on our AllClear dataset.

For the evaluation, all models receive three images as input. Specifically, they receive both Sentinel-2 and Sentinel-1 images concatenated along the channel dimension.

**Simple baselines** To better contextualize model performance, we follow previous works (Ebel et al., 2022, 2023) and include two simple baselines: "Least Cloudy" and "Mosaicing". The former simply uses the input image with the least cloud and shadow coverage as the output. "Mosaicing" operates in the following way: for each image coordinates in the input images if only one image is clear, we directly copy its pixel value; if more than one clear images exist, we take the average of these clear pixel values; if there is no clear image, we fill the gap with 0.5.

**Results.** The quantitative and qualitative results are shown in Table 2 and Figure 8, respectively. We first notice that simple baselines _least cloudy_ and _mosaicing_ perform well on the dataset. UnCRtainTS performs slightly better than these simple baselines in terms of SSIM and SAM. On the other hand, the U-TILISE model falls short of reaching the performance of the simple baselines. Since U-TILISE is a sequence-to-sequence model, we adopt it for sequence-to-point evaluation by choosing the image from the output sequence with the lowest MAE score as the model output. Notably, the training of U-TILISE involves adding sampled cloud masks to the cloud-free images as inputs, and it is trained to recover the original cloud-free sequence. The model is evaluated in a similar manner. The distribution disparity between the sampled cloud masks and the real clouds may contribute to the low score of U-TILISE in the real scenario. For the good performance of _least cloudy_ and _mosaicing_, we conjecture that this is because of the small temporal gap in AllClear between input and target images, so simply averaging or choosing from the input images is likely to yield good results.

Notably, models pre-trained on STGAN and Sen2_MTC datasets (specifically CTGAN, PMAA, and DiffCR) performed below simple baselines. Due to insufficient documentation of imagery specifications and pre-processing protocols in these datasets, we excluded these pre-trained models from subsequent analysis.

**Failure cases.** To understand the performance of the state-of-the-art better, we visualize the output images generated using the state-of-the-art model UnCRtainTS , which was trained on the SEN12MS-CR-TS dataset . In Figure 2, we evaluate the pre-trained model on AllClear testing cases where it receives three cloudy images as input. Overall, we observe three primary failure modes in the model's performance: (1) The model fails to draw from clear input images, particularly when the other two images are cloudy. This issue may arise because the model was trained exclusively on images with less than 50% cloud coverage, as noted by the authors . (2) The model often struggles to recover the correct color spectrum, even when the input images are mostly clear. We hypothesize that this is due to the relatively small dataset size, leading to a lack of generalization ability. (3) The model frequently fails to generalize to snow-covered land. We speculate that this is due to insufficient sampling of diverse snowy regions during training.

### Training on AllClear

We next evaluate the benefits of training on AllClear. For this purpose, we use UnCRtainTS given its good performance on prior benchmarks . To evaluate if there is any domain difference between AllClear and the previous SEN12MS-TR-CS dataset, we first run an equal-training-set-size comparison. We train UnCRtainTS on a _subset_ of AllClear that is of the same size as the training set size used in UnCRtainTS training, which is 10,167 data points. We also follow the training hyperparameters as in the original paper to avoid extra tuning. As shown in Table 3,

   Model & Training Dataset & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\  Least Cloudy & - & 28.864 & 0.836 & 6.982 & 0.078 \\ Mosaicing & - & **29.824** & 0.754 & 23.58 & 0.045 \\  UnCRtainTS  & SEN12MS-CR-TS & 29.009 & **0.898** & **5.972** & **0.039** \\ U-TILISE  & SEN12MS-CR-TS & 24.660 & 0.807 & 7.765 & 0.083 \\ CTGAN  & Sen2MTC & 27.783 & 0.840 & 8.800 & 0.041 \\  PMAA  & STGAN & 12.455 & 0.460 & 8.072 & 0.240 \\  & Sen2MTC & 24.328 & 0.768 & 8.680 & 0.078 \\  DiffCR  & STGAN & 17.998 & 0.642 & 9.512 & 0.117 \\   

Table 2: Benchmark performance of previous SoTA models evaluated on our AllClear benchmark dataset. The best performing values are in **bold** and the second best is underlined.

Figure 2: Failure case from UnCRtainTS , a previous SOTA model trained on the SEN12MS-CR-TS  cloud removal dataset.

when both models are evaluated on AllClear (i.e., the bottom two rows in Table 3), we observe that UnCRtainTS models pre-trained on both datasets have comparable results across the four metrics. This suggests that there is no noticeable domain difference between the two datasets.

**Scaling with AllClear.** We next evaluate how much we can scale UnCRtainTS using the large training set available with AllClear. Specifically, we curate a dataset of various scales using random sampling from the training dataset while evaluating on the same validation set. Table 4 shows the results. We find that more training data clearly improves accuracy significantly across all metrics, resulting in a more than 10% improvement in PSNR. Figure 3 shows that with a larger dataset the model is able to better remove clouds and better preserve the color. This suggests that cloud removal models trained on past datasets are in general _undertrained_ and AllClear's large training set is useful to help the models fit the data better.

   Fraction of Data & \(\#\) data point & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\ 
1\% & 2,786 & 27.035 & 0.898 & 5.972 & 0.039 \\
3.4\% & 10,167 & 28.474 & 0.906 & 6.373 & 0.036 \\
10\% & 27,861 & 32.997 & 0.923 & 6.038 & **0.023** \\
100\% & 278,613 & **33.868** & **0.936** & **5.232** & **0.021** \\   

Table 4: Scaling law of our model on our AllClear datasets with UnCRtainTS as backbone architecture.

Figure 3: Scaling the training dataset by tenfold gives better qualitative results.

   Evaluation Dataset & 
 Training Dataset \\ (fraction used) \\  & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\   & SEN12MS-CR-TS & **27.838** & **0.866** & **9.455** & **0.036** \\   & AllClear (\(3.4\%\)) & 26.256 & 0.847 & 10.411 & 0.041 \\   & SEN12MS-CR-TS & **29.009** & 0.898 & **5.972** & 0.039 \\   & AllClear (\(3.4\%\)) & 28.474 & **0.906** & 6.373 & **0.036** \\   

Table 3: Benchmark Performance for UnCRtainTS models retrained on AllClear.

### Stratified evaluations

We use the available land-cover type labels in _AllClear_ to conduct a stratified evaluation across land-cover types. As shown in Figure 4, we find that both PSNR and SSIM metrics are generally much worse for both water bodies and snow cover. Water bodies have transient wave patterns, and snow cover is also often transient, which may explain the difficulty of predicting these classes. Snow may also be confused with cloud.

Following past work (Ebel et al., 2022), we also perform a stratified evaluation of accuracy relative to the extent of cloud cover and shadows (Figure 5). For cloud cover, generally performance decreases with cloud percentage, which is expected. Training on a larger dataset (AllClear) substantially improves accuracy for low and medium cloud cover, but not for fully clouded regions. Note that the striped pattern is because of fully cloudy images as explained in the Appendix. Shadows are generally less of a problem, and shadow percentage seems to be uncorrelated with performance.

### Effect of various temporal spans

We next use our benchmark to see whether the common practice of using 3 input images is sufficient. We compare two models, one using 3 images and the other using all 12 images captured at that location. Both models are trained on a 10k subset of _AllClear_. The results, shown in Table 5, suggest

Figure 4: Land cover stratified evaluation of models trained with different fractions of the AllClear dataset: \(1\%\), \(3.4\%\), \(10\%\), and \(100\%\).

Figure 5: Cloud removal quality measured by PSNR (left column) and SSIM (right column) at different cloud and shadow coverage levels. The top row represents models trained on the full AllClear dataset, and the bottom row represents models trained on the SEN12MS-CR-TS dataset.

that in fact a longer timespan significantly improves accuracy. Future cloud removal techniques should therefore consider longer timespans.

### Ablation study on multi-modality preprocessing strategies

Here we explore the integration of multiple sensors into the input data. As described above, we concatenate multi-spectral Sentinel-2 images with Sentinel-1 and Landsat images to create an input with multiple channels. However, due to the differing revisit intervals of these satellites, there can be gaps in the input sequences, meaning that some Sentinel-2 images may not have corresponding Sentinel-1 or Landsat-8/9 images.

To address these gaps, we experimented with different preprocessing strategies, as shown in Table 6. We discovered that filling the gaps with different constant values significantly impacts the results. Specifically, filling with zeros yielded better performance compared to filling with ones. Also, we provided additional experiments adding an extra input dimension called the "availability mask," which is filled with zeros if there is no paired Sentinel-1 image and ones otherwise, but this approach did not improve results. Additionally, while outcomes regarding using extra Landsat images were inconsistent, filling gaps with zeros for Landsat gave the best results, albeit still lower than using only Sentinel-1 and Sentinel-2 alone. This might be due to the low-resolution of Landsat imagery; we suggest a model redesign to fully exploit Landsat images.

With the new preprocessing strategy for Sentinel-1 data gaps, we revisit the scaling law. As shown in Table 7, the scaling law holds for both preprocessing methods. Additionally, when it comes to full dataset training, the preprocessing methods do not cause significant differences in the results. Notably, the overall results improve when the Sentinel-1 gaps are filled with constant zeros for the small and medium dataset, indicating a potential inductive bias of filling with constant zeros.

   Sentinel-2 & Sentinel-1 & Landsat-8/9 & Preproc. & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\  \(\) & \(\) & & S1: FO & 28.474 & 0.906 & 6.373 & 0.036 \\  \(\) & & & - & 31.725 & 0.920 & 6.084 & 0.026 \\ \(\) & \(\) & & S1: AM & 30.506 & 0.922 & 6.258 & 0.027 \\ \(\) & \(\) & & S1: FZ & **33.107** & **0.930** & **5.719** & **0.022** \\ \(\) & \(\) & \(\) & S1: FO, LS: FO & 30.040 & 0.898 & 6.989 & 0.033 \\ \(\) & \(\) & \(\) & S1: FZ, LS: FO & 31.416 & 0.914 & 6.622 & 0.026 \\ \(\) & \(\) & \(\) & S1: FZ, LS: FZ & 32.522 & 0.923 & 6.233 & 0.024 \\   

Table 6: Multi-modality ablation studies. UnCRtainTS models are trained on a 10K subset of samples from our datasets with various setups. _S1_ and _LS_ denote Sentinel-1 and Landsat images, respectively. Preprocessing methods: _FZ_ - Fill zeros, _FO_F_ - Fill ones, _AM_ - Availability mask. _FZ/FO_ indicates filling gaps with constant zeros/ones when no nearby S1 images are available. The best-performing results are **bolded** and the second best are underlined.

   Fraction of Data & \(\#\) data point & Preproc. & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\ 
1\% & 2,786 & S1: FZ & 32.039 & 0.922 & 6.469 & 0.024 \\
3.4\% & 10,167 & S1: FZ & 33.107 & 0.930 & 5.719 & 0.022 \\
10\% & 27,861 & S1: FZ & 33.163 & 0.929 & 5.606 & 0.023 \\
100\% & 278,613 & S1: FZ & **34.148** & 0.935 & 5.338 & **0.021** \\   

Table 7: Scaling law of our model on our AllClear datasets with UnCRtainTS as backbone architecture, with gaps being zeros. Preprocessing methods: _FZ_ - Fill zeros. The best-performing results are **bolded** and the second best are underlined.

   \# Consecutive Frame as Input & PSNR (\(\)) & SSIM (\(\)) & SAM (\(\)) & MAE (\(\)) \\ 
3 & 28.474 & 0.906 & 6.373 & 0.036 \\
12 & 30.399 & 0.919 & 5.920 & 0.028 \\   

Table 5: Effect of different temporal length.

### Experiments on spatial correlation

To investigate the role of spatial correlation in model generalization, we conducted a geographical hold-out experiment. We trained and validated the model for 16 epochs using a modified version of the AllClear dataset that excluded all North American regions of interest (ROIs). The model was evaluated on two distinct test sets: one containing 2,887 North American ROIs (combined from original train, validation, and test splits) and another containing only non-North American ROIs from the original test set. The results shown in Table 8 suggest that training solely on the held-out dataset does not guarantee spatial generalization. This experiment highlights the importance of addressing spatial generalization in future research. Further investigation of the spatial correlation is presented in Appendix C.4

## 5 Limitations and Future Work

While AllClear advances the state of cloud removal in satellite imagery, we acknowledge several important limitations. First, the largest limitation of AllClear is the lack of ground truth annotations. The cloud labels are derived from existing cloud masks computed using s2cloudless algorithm (offered by Google Earth Engine), not manually annotated. Second, we are using Google Earth Engine product level-1C, which is not atmospherically corrected. The main reason is for consistency with the previous largest cloud removal dataset and the derived pre-trained models.

For future work, our findings suggest several promising directions: (1) development of hybrid approaches combining algorithmic and manual cloud annotations, (2) investigation of atmospheric correction's impact on cloud removal performance, and (3) extension to other satellite platforms and spatial resolutions.

## 6 Conclusion

This paper has introduced _AllClear_, the most extensive and diverse dataset available for cloud removal research. The larger training set significantly advances state-of-the-art performance. Our dataset also enables stratified evaluation on cloud coverage and land cover, and ablations of the sequence length and sensor type. We hope that future research can build on this benchmark to advance cloud removal, for instance by exploring the dynamics between SAR and multispectral images.