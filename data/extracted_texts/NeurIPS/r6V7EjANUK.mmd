# GSDF: 3DGS Meets SDF for Improved Neural

Rendering and Reconstruction

Mulin Yu \({}^{1}\)1  Tao Lu\({}^{1}\)1  Linning Xu\({}^{2}\)2  Lihan Jiang\({}^{4,1}\)2  Yuanbo Xiangli\({}^{3}\)2  Bo Dai\({}^{5,1}\)

\({}^{1}\)Shanghai Artificial Intelligence Laboratory, \({}^{2}\)The Chinese University of Hong Kong,

\({}^{3}\)Cornell University, \({}^{4}\) University of Science and Technology of China, \({}^{5}\) The University of Hong Kong

###### Abstract

Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often conflicts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision _during the training process_ to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry.

Figure 1: **Conceptual Illustration of GSDF. Rendering and reconstruction tasks have traditionally involved trade-offs in neural representation methods. While 3D-GS achieves high-fidelity view-dependent rendering, it often compromises on geometric accuracy. Recent approaches  use explicit regularization to align Gaussian primitives near surfaces, but this can reduce model capacity for high-fidelity visuals. Our GSDF introduces a dual-branch framework with specialized GS- and SDF-branches for rendering and geometry tasks. We propose three mutual guidances (detailed in Sec. 3.2) to enhance the quality of both tasks.**Introduction

Recent advancements in neural scene representations have showcased superior rendering capabilities [23; 24; 17], these advancements have sparked significant interest in neural surface reconstruction [39; 31; 19; 32; 10; 13]. They seek to develop unified representations, which simultaneously support high-fidelity rendering and accurate geometric reconstruction, to better support downstream applications such as robotics [16; 27], physical simulations [35; 8], and XR applications [36; 15].

Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods often suffer from a mismatched prioritization between appearance and geometry. Imposing regularization or constraints on the unified representation may boost one task, while unavoidably deteriorating the performance of the other. Recent approaches [10; 5], have explored using flat Gaussian primitives for surface modeling. Enforcing binary opacity  and jointly learned NeuS models  to regularize attributes have led to degraded rendering quality due to primitive constraints. However, works such as Adaptive Shell , Binary Occupancy Field , and Scaffold-GS  have demonstrated that incorporating geometry guidance significantly enhances rendering quality by producing well-regularized spatial structures with hybrid representations.

Building on these insights, we propose a synchronously optimized _dual-branch_ system, addressing rendering and reconstruction with hybrid representations, to bypass the conflicts and further achieve mutual enhancements, as illustrated in Fig. 1. Our method leverages mutual guidance and joint supervision to balance rendering and reconstruction without compromising their intrinsic advantages. Specifically, our system features a 3D Gaussian Splitting (3DGS) branch for rendering and a Signed Distance Field (SDF) branch for surface reconstruction. The key innovations of our approach include: (1) Utilizing rasterized depth from the GS-branch to guide ray sampling in the SDF-branch, enhancing volume rendering efficiency and avoiding local minima. (2) Applying SDF-guidance for density control in 3DGS, directing the growth of 3D Gaussians in near-surface regions and pruning elsewhere. (3) Aligning geometry properties (depth and normal) estimated from both branches. This unified system overcomes the limitations inherent in each method due to differences in rendering techniques ( rasterization vs. dense ray sampling) and scene representation (discrete primitives vs. continuous fields). Moreover, our framework is designed to accommodate future advancements in each branch.

Extensive experiments demonstrate that our dual-branch design allows: 1) The GS-branch to generate structured primitives closely aligned with the surface, reducing floaters and improving detail and edge quality in view synthesis. 2) Accelerated convergence in the SDF-branch, resulting in superior geometric accuracy and enhanced surface details. Our results confirm that an integrated blend of both reconstruction and rendering is achievable, enhancing overall robustness and performance.

## 2 Related work

Advanced Neural Rendering Techniques.Neural Radiance Fields (NeRFs)  have achieved remarkable photorealistic rendering with view-dependent effects. They use Multi-Layer Perceptrons (MLPs) to map 3D spatial locations to color and density, which are then aggregated into pixel colors through neural volumetric rendering. This approach excels in novel view synthesis but is slow due to the need for extensive point sampling along each ray and the global MLP architecture's scalability limitations. Recent research [24; 37; 4; 9] has shifted the learning burden to locally optimized spatial features, enhancing scalability. Alternative methods like rasterizing geometric primitives (e.g., point clouds) [2; 41] offer efficiency and flexibility but struggle with discontinuities and outliers. Augmenting points with neural features and incorporating volumetric rendering  improves quality but adds computational overhead. Recently, 3D Gaussian Splatting (3DGS)  revolutionized neural rendering by using anisotropic 3D Gaussians as primitives, sorted by depth and rasterized onto a 2D screen with \(\)-blending. This method achieves high-quality, detailed results at real-time frame rates. Scaffold-GS  enhanced 3DGS by introducing a hierarchical structure that aligns anchors with scene geometry, improving rendering quality and memory efficiency. However, these methods prioritize view synthesis over accurate scene geometry, often resulting in fuzzy volumetric density fields that hinder the extraction of high-quality surfaces

Neural Surface Reconstruction.The success of neural rendering has sparked significant interest in neural surface reconstruction [25; 39; 31; 19; 32; 40]. These methods typically use coordinate-based networks to encode scene geometry through occupancy fields or Signed Distance Field (SDF) values.

While MLPs with volume rendering produce smooth and complete surfaces, they often lack high-fidelity details and are slow to optimize. Recent works [19; 34; 26] have leveraged multi-resolution hashed feature grids from iNGP  to enhance representation power, achieving state-of-the-art results. Hybrid approaches combining surface and volume rendering [30; 34; 26] have also emerged to maintain rendering speed and quality. More recently, methods have explored integrating 3D Gaussian Splitting for surface learning. For example, SuGaR  aligns 3D Gaussians with potential surfaces by approximating them to 2D planar primitives with binary opacity. Similarly, 2D Gaussian Splitting  replaces 3D Gaussians with 2D Gaussians for more accurate ray-splat intersection and regularizes depth maps to enforce a more condensed Gaussian primitive distribution. Another approach  optimizes Gaussian surfels constrained by a normal prior from a pre-trained monocular estimator, enhancing surface representation. The concurrent work NeuSG  jointly optimizes 3D Gaussian Splatting with NeuS  by encouraging flat 3D Gaussians as well, with normals aligned with NeuS predictions. 3DGSR  also aligns the geometry from 3DGS with a SDF field. Despite advances in neural surface reconstruction, a fidelity gap persists due to the explicit regularization of Gaussian primitives, which hampers rendering quality compared to 3DGS. Our method optimizes primitives aligned with surfaces using geometric clues, enhancing structural integrity without losing representational power.

## 3 Method

We present a dual-branch framework with GS- and SDF-branches, jointly optimized to provide mutual guidance, as shown in Fig. 2. Our approach aligns Gaussian primitives with surfaces using geometric clues rather than constraining their shapes, enhancing both structure and representational power. These improved primitives yield more accurate and detailed scene surfaces.

In Sec.3.1, we briefly review neural rendering methods[17; 21] and neural implicit surface representations [31; 19]. Sec.3.2 details our framework and the three proposed mutual guidance techniques. Sec.3.3 outlines our training strategy and loss design.

Figure 2: **Overview of Dual-branch Guidance**. Our dual-branch framework includes a GS-branch for rendering and an SDF-branch for learning neural surfaces. This design preserves the efficiency and fidelity of Gaussian primitive for rendering [17; 21] while accurately approximating scene surfaces from an SDF field adapted from NeuS . Specifically: (1) The GS-branch renders depth maps to guide SDF-branch ray sampling, querying absolute SDF values \(|s|\) and sampling points within \(2k|s|\) (e.g., \(k=4\)). (2) Predicted SDF values guide GS-branch density control, growing Gaussians near surfaces and pruning deviated ones. (3) Mutual geometry consistency is enforced by comparing depth and normal maps from both branches, ensuring coherent alignment between Gaussians and surfaces.

### Preliminary

3D Gaussian Splatting.3D Gaussian Splatting (3DGS)  represents scenes using 3D Gaussian primitives, achieving state-of-the-art rendering quality and speed with a tile-based rasterizer. Each Gaussian is defined by its mean \(^{3}\) and covariance \(^{3 3}\) with \(G(x)=e^{-(x-)^{T}^{-1}(x-)}\), where \(x\) is a 3D position in the scene. The rasterizer efficiently sorts and \(\)-blends Gaussians onto the 2D image plane . The adaptive control of Gaussians based on gradients is also critical for improving scene representation accuracy, reducing redundancy, and refining Gaussians to better match the underlying geometry and appearance.

Scaffold-GS  improves 3DGS by enhancing scene structure fidelity and robustness to view-dependent effects. It uses a hierarchical 3D Gaussian representation, with anchor points encoding local scene information and generating local neural Gaussians. Each anchor, optimized with a feature vector, predicts the color, center, variance, and opacity of the neural Gaussians.

Neural Implicit SDFs.NeuS  integrates signed distance functions (SDFs) with NeRF's volumetric rendering, converting SDF values into volume densities to maintain geometric accuracy. SDF values are converted to opacities using a logistic function \(_{i}=((f(_{i}))-_{s}(f(_{ i+1}))}{_{s}(f(_{i}))},0).\) The color of a ray \(r\) is also calculated by accumulating weighted colors of the sample points: \(()=_{i=1}^{P}T_{i}_{i}_{i}\), and \(T_{i}=(-_{j=1}^{i-1}_{j}_{j})\), where \(_{j}\) is the interval between sampled points. Inspired by ,  introduces a multi-resolution hash grid to enhance representation power and accelerate rendering and training.

### GSDF: Dual-branch for Rendering and Reconstruction

To bridge the gap between rendering and geometric accuracy, we propose a novel approach that combines the strengths of Gaussian-based and SDF-based methods. By leveraging the high-quality rendering capabilities of 3DGS and the precise geometric representation of SDFs, our method aims to achieve superior results in both tasks.

As illustrated in Fig. 2, our dual-branch design integrates a GS-branch and an SDF-branch. We use Scaffold-GS  and NeuS  with adapted hash-encoding implementation  as the backbones, chosen for their effectiveness and simplicity. Importantly, our framework is versatile and can be easily adapted to incorporate future advanced methods for each branch.

#### 3.2.1 Gs \(\) SDF: Depth Guided Ray Sampling

To address the computational expenses of ray-sampling, techniques such as hierarchical sampling , occupancy grids [20; 4; 24], early stopping , and proposal networks [3; 26] are widely used with CUDA accelerations. When depth maps are available, either from sensors or monocular estimation, samples can be strategically placed around surface regions, which is crucial for effective optimization of the Signed Distance Field (SDF). Unlike neural implicit SDF-based methods that rely on their own predicted SDF values for ray sampling [31; 19; 28], we employ the GS-branch to provide surface proximity, avoiding the chicken-and-egg dilemma. Inspired by , where a more efficient branch provides coarse geometry guidance, we leverage depth maps from the GS-branch to refine the ray sampling range for the SDF-branch. While Gaussian primitives may be less precise, they are efficient and flexible, offering sufficient geometric clues without significant overhead.

Concretely, for a ray emitted from a camera center \(\) in the direction \(\), the depth value \(D\) from the GS-branch is:

\[D=_{i N}d_{i}_{i}_{j=1}^{i-1}(1-_{j}),\] (1)

where \(N\) is the number of 3D Gaussians encountered, \(_{i}\) is the opacity, and \(d_{i}\) is the distance from the \(i\)-th Gaussian to the camera. For SDF-branch optimization, points are sampled around \(+D\). The sampling range adapts based on predicted SDF values \(s\) at various depths:

\[s=_{s}(+D),\] (2)

where \(_{s}\) is a two-layer MLP predicting the SDF value at a given position. The sampling range is defined as \(r=[+(D-k|s|),+(D+k|s|)]\). As shown in Fig. 2, inspired from NeRF'shierarchical sampling strategy , we use coarse and fine ranges with \(k=3\) and \(k=1\), respectively, we uniformly sample \(M\) points along the ray within each range.

#### 3.2.2 SDF \(\) GS: Geometry-aware Gaussian Density Control

Many previous and concurrent methods [13; 10; 7] have attempted to learn scene surfaces from 3DGS by flattening 3D Gaussians along the normal direction and enforcing nearly binary opacity, treating them like surface primitives similar to mesh triangles. However, this approach often leads to degraded rendering quality and incomplete surfaces. Our approach, instead of putting additional regularization on the 3D Gaussian primitives, enhances the distribution of Gaussian primitives using a geometry-aware density control strategy. Building on the original gradient-based density control, we leverage the zero-level set of the SDF-branch to determine the proximity of Gaussian primitives to the surface. By querying the SDF-branch with the positions of Gaussian primitives, we are able to identify close-to-surface Gaussian primitives (i.e. with smaller absolute SDF values), allowing for more precise control over the placement and density of Gaussians.

**Growing Operator.** For each Gaussian primitive at position \(c\), we obtain its SDF value \(s=_{s}(c)\) from the SDF-branch. The growth criteria for Gaussians are then defined as:

\[_{g}=_{g}+_{g}(s),\] (3)

where \(_{g}\) is the averaged gradient of Gaussian primitives accumulated over \(K\) training iterations, as in Scaffold-GS . \((s)=(-s^{2}/(2^{2}))\) is a Gaussian function converting the SDF value to a positive factor, decreasing monotonically with distance from the zero level set. \(_{g}\) controls the influence of geometric guidance. New Gaussian primitives are added if \(_{g}\) are greater than a predefined \(_{g}\).

**Pruning Operator.** Beyond the original opacity-based criteria, we prune Gaussian primitives far from the surface, indicated by large SDF values. The pruning criteria are:

\[_{p}=_{a}-_{p}(1-(s)),\] (4)

where \(_{a}\) is the aggregated opacity over \(K\) iterations. The weight \(_{p}\) balances the contributions of transparency and SDF. Primitives with \(_{p}\) less than a predefined \(_{p}\) are pruned.

#### 3.2.3 GS \(\) SDF: Mutual Geometry Supervision

To enhance both rendering and reconstruction outcomes, our framework incorporates mutual supervision between the two branches, using depth and normal maps as key geometric features to facilitate this interconnection.

For the SDF-branch, a per-view depth map \(D_{s}\) is rendered using volumetric rendering principles, and a normal map \(N_{s}\) is derived by rendering the gradients of the SDF volumetrically. For the GS-branch, we compute a per-view depth map \(D_{g}\) following Eq. 1. The normal of each 3D Gaussian is determined by the direction of its smallest scaling factor, as in [10; 5; 6]. To render the normal map for each camera view, we accumulate the normals of the 3D Gaussians using \(\)-blending \(N_{g}=_{i N}n_{i}_{i}_{j=1}^{i-1}(1-_{j})\), where \(n_{i}\) is the estimated normal of the \(i\)-th 3D Gaussian.

This mutual geometry supervision ensures that the depth and normal maps from both branches are consistently aligned. This consistent alignment is crucial for maintaining coherence between the rendering and reconstruction processes, as it helps to prevent discrepancies that could lead to artifacts and inaccuracies when performing mutual guidance. This consistency is especially important for achieving high-quality results in complex scenes, where precise geometric alignment can significantly enhance the visual and structural integrity of the output.

### Training Strategy and Loss Design

The GS-branch is supervised by rendering losses \(_{1}\) and \(_{}\), which measure the difference between the rendered RGB images and ground truth images, and a volume regularization term \(_{vol}\) as in . The complete loss function for GS-branch is:

\[_{}=_{1}_{1}+(1-_{1})_ {}+_{vol}_{vol},\] (5)where \(_{1}\) and \(_{vol}\) are weighting coefficients. Similarly, the SDF-branch is supervised by the \(_{1}\) rendering loss, supplemented with Eikonal and curvature penalties to ensure accurate geometry reconstruction. The loss function for the SDF-branch is:

\[_{}=_{1}+_{}_{ {eik}}+_{}_{},\] (6)

where \(_{}\) represents the Eikonal loss, ensuring that the gradients of the predicted SDF field are normalized, and \(_{}\) denotes the curvature loss, promoting surface smoothness as described in [19; 28]. The coefficients \(_{}\) and \(_{}\) balance the influence of these loss terms.

The mutual geometry supervision includes depth and normal consistency losses applied to both branches. For unbounded scenes with distant backgrounds, we only apply the mutual geometric supervision to foreground regions, as these geometries are more reliable and of primary interest . The mutual loss is formulated as:

\[_{}=_{d}_{d}+_{n}_{n}=_{d}\|D_{gs}-D_{s}\|+_{n}(1- N _{s}|}{\|N_{gs}\|\|N_{s}\|}),\] (7)

where \(_{d}\) and \(_{n}\) represent the depth and normal discrepancies between the two branches, and \(_{d}\) and \(_{n}\) balance their importance. Finally, the total loss for joint learning is defined as:

\[=_{}+_{}+_{ {mutual}}.\] (8)

The hyper-parameter settings are detailed in the supplementary material.

## 4 Experiment

### Experimental Setup

**Datasets.** We evaluated results using 26 real-world and synthetic scenes from various datasets: 7 from Mip-NeRF360 , 2 from DeepBlending , 2 from Tanks&Temples , and 15 from DTU , featuring a wide range of indoor, outdoor, and object-centric scenarios.

**Implementation Details.** We implemented our dual-branch model based on 1) Scaffold-GS  and 2) an enhanced version of NeuS  with a hash-grid variant , following the practice of . The hash grid resolution spans from \(2^{5}\) to \(2^{11}\) with 16 levels, each entry having a feature dimension of 4 and a maximum of \(2^{21}\) entries per level. The coarsest 4 layers were activated initially for the DTU , and 8 layers for other datasets, with finer levels added every \(2k\) iterations. We trained the GS-branch for 15k iterations, followed by joint training of both branches for 30k iterations. The SDF-branch was warmed up for 2k iterations on the DTU and 5k on other datasets without depth-guided ray sampling. Note that our system is adaptable with various existing or future rendering and reconstruction models.

**Evaluations.** We evaluated our method against state-of-the-art rendering and reconstruction approaches. For rendering comparison, we compared with Scaffold-GS , 3D-GS , NeuS , 2D-GS , and SuGaR  using PSNR, SSIM , and LPIPS  for quantitative comparisons. We trained 3D-GS and Scaffold-GS for 45\(k\) iterations to align with our configurations. For reconstruction, we compared with NeuS , Instant-NSR  (our representative SDF-branch), SuGaR  and 2D-GS . We use Chamfer distance for quantitative evaluation. For all datasets, we used \(1/8\) of the images as test sets and the other \(7/8\) as training sets. All experiments were conducted on a single NVIDIA A100 GPU with 80G memory.

### Results Analysis

#### 4.2.1 Rendering Comparisons

Our GSDF retained high rendering quality compared to state-of-the-art 3D-GS and SDF-based methods, as shown in Table 1. Quantitative metrics against 3D-GS , 2D-GS , and Scaffold-GS  show that GSDF consistently outperformed these methods across all four datasets, showcasing improvements in all metrics. Notably, the prominent improvements in the LPIPS metric indicate that our method effectively captures high-frequency scene details and renders perceptually superior results compared to baselines.

**Enchanced Detail and Fidelity.** Notably, our method excelled in achieving _high rendering quality_ in texture-less areas, as showcased in Fig.3, which is aligned with our design motivation for geometry-aware density control. Specifically, in texture-less areas where vanilla 3D Gaussians struggled due to small accumulated gradients, our method overcame this limitation by growing anchors in surface regions which brings enhanced accuracy and scene details.

**Robustness with Noisy Gaussians.** To test the robustness, we experimented with randomly initialized Gaussian primitives on 2D-GS, Scaffold-GS, and our method. Quantitative results in Table 2 highlight the advantages of our geometric guidance, demonstrating superior performance and stability even with random input. Further visual results are provided in the supplementary material.

   Dataset &  &  &  \\ Method \& Metrics & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ 
2D-GS (rand) & 26.94 & 0.793 & 0.300 & 25.16 & 0.817 & 0.275 & 28.76 & 0.892 & 0.274 \\ Scaffold-GS (rand) & 27.84 & 0.817 & 0.259 & 26.37 & 0.838 & 0.230 & 29.12 & 0.895 & 0.260 \\
**GSDF** (rand) & **28.05** & **0.830** & **0.229** & **26.55** & **0.852** & **0.197** & **29.57** & **0.899** & **0.244** \\   

Table 2: **Rendering comparisons with random initialization against baselines over three benchmark scenes. Scaffold-GS (rand) , 2D-GS (rand)  and GSDF (rand) initialized the Gaussian primitives with random points.**

   Dataset &  &  &  \\ Method \& Metrics & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ 
2D-GS (rand) & 26.94 & 0.793 & 0.300 & 25.16 & 0.817 & 0.275 & 28.76 & 0.892 & 0.274 \\ Scaffold-GS (rand) & 27.84 & 0.817 & 0.259 & 26.37 & 0.838 & 0.230 & 29.12 & 0.895 & 0.260 \\
**GSDF** (rand) & **28.05** & **0.830** & **0.229** & **26.55** & **0.852** & **0.197** & **29.57** & **0.899** & **0.244** \\   

Table 2: **Rendering comparisons with random initialization against baselines over three benchmark scenes. Scaffold-GS (rand) , 2D-GS (rand)  and GSDF (rand) initialized the Gaussian primitives with random points.**

Figure 3: **Qualitative comparisons of GSDF against popular Gaussian-based baselines [17; 21; 13] across diverse 3D scene datasets [18; 3; 12]. As highlighted, GSDF excels in modeling delicate geometries (1st & 2nd rows) and handling texture-less and sparsely observed regions (3rd & 4th rows), which are commonly presented in larger scenes where baseline approaches struggle to address.**

#### 4.2.2 Reconstruction Comparisons

We quantitatively evaluated the reconstruction ability on the DTU dataset, where our method achieved the best Chamfer distance as shown in Table 1.

**Enhanced Geometry Accuracy and Completeness.** As illustrated in Fig.4, our method reconstructed more complete and detailed meshes compared to baseline methods. Our approach effectively bypassed local minima, preventing holes in the meshes. Notably, our method outperformed Instant-NSR, delivering smooth, continuous meshes with well-preserved high-frequency details. In contrast, meshes extracted from SuGaR  were non-manifold with broken topological relationships. 2D-GS  extracts mesh from fused depth, which leads to over-smooth geometry.

**Accelerated Convergence and Rendering.** Benefited from the GS-branch, our method optimized the SDF field significantly faster in terms of training iterations than previous methods with accurate sample placements, which often required slow and exhaustive training and rendering. For instance, NeuS  required about 8 hours of optimization on a single GPU for the DTU dataset, whereas our method achieved comparable or better results within just 2 hours on a single GPU.

Figure 4: **Reconstruction Comparison**. We visualize the reconstructed meshes from Instant-NSR  (our SDF-branch), SuGaR , 2D-GS , and Ours.

### Ablation Studies

In this section, we examine the effectiveness of each individual module. Quantitative metrics and qualitative visualizations are provided in Table 3 and Fig. 5.

**Depth-Guided Ray Sampling.** To evaluate the effectiveness of our depth-guided ray sampling detailed in Sec 3.2, we conducted an ablation on depth-guided ray sampling using the original stratified ray sampling approach . The results show that this ablated setting produced overly smoothed surfaces, failing to capture finer geometry details such as the heater and doors (Fig. 5, column 2, patches 2 & 3). The resulting less accurate SDF also impacted rendering quality, making details like the sticker on the wall appear more blurred compared to our full model.

**Geometry-aware Gaussian Density Control.** We replaced our geometry-aware Gaussian density control with the pruning and growing strategy from Scafffold-GS  to evaluate its efficacy. As shown in Fig. 5 (3rd column), this change led to missed details like the sticker on the wall (patch 4) due to small accumulated gradients on the texture-less surface. The reconstruction results also showed the absence of thin objects such as table legs and the chandelier (patch 1), highlighting the limitations of the vanilla strategy.

**Mutual Geometric Supervision.** Lastly, we ablated the proposed mutual geometric supervision by setting both \(_{d}\) and \(_{n}\) to 0 (Eq. 7). This resulted in a significant decay in surface quality and omission of details in rendering. These findings indicate that neural rendering is more tolerant of deviations than neural surface reconstruction. Without explicitly aligning Gaussians with SDF-derived geometry during optimization, the two branches can diverge, leading to sub-optimal results for both.

### Limitations and Future Works

Currently, our method is not tailored to handle challenging scenes with reflections and intense lighting variations, such as those found in indoor environments. However, we have observed that employing more structured and surface-aligned Gaussian primitives holds promise in capturing such view-dependent appearance changes with improved scene geometry. Our framework requires more memory usage because it simultaneously considers two representations. Furthermore, the performance of our SDF-branch significantly lags behind the GS-branch, leading to extended training durations compared to Scafffold-GS  only. Hence, improving the efficiency of the MLP-based

Figure 5: **Ablation results**. Visualizations of reconstructed meshes and rendered images from 1) our full method, 2) ours w/o depth-guided ray sampling, 3) ours w/o geometry-aware density control, and 4) ours w/o geometric supervision. We highlight the degradation of quality using numbered patches.

   Dataset &  &  &  \\ Method \(|\) Metrics & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  GSDF (Full) & **29.38** & **0.865** & **0.185** & **27.37** & **0.875** & **0.156** & **30.38** & **0.909** & **0.223** \\  w/o geometric supervision & 29.27 & 0.862 & 0.196 & 27.18 & 0.865 & 0.191 & 30.21 & 0.907 & 0.234 \\ w/o depth-guided sampling & 29.26 & 0.863 & 0.196 & 27.30 & 0.873 & 0.159 & 30.17 & 0.908 & 0.228 \\ w/o geometry-aware densification & 29.29 & 0.863 & 0.196 & 27.29 & 0.870 & 0.179 & 30.25 & 0.908 & 0.236 \\   

Table 3: **Quantitative Results on Ablation Studies. We separately listed the rendering metrics for each ablation described in Sec. 4.3.**SDF-branch is a crucial direction for future research. Still, training time is not the highest priority compared to inference, where primitives are stored and can be accessed efficiently.

## 5 Conclusion

In this work, we introduced a dual-branch framework that leverages the strengths of both 3D-GS and SDF, showcasing its potential to achieve both enhanced rendering and reconstruction quality. The inherent differences in two representations, rendering approaches, and supervision loss pose a challenge to the seamless integration. We therefore integrate a bidirectional mutual guidance approach during the training to circumvent these restrictions. Three types of guidance have been introduced and validated in our framework, namely: 1) depth guided sampling (GS\(\)SDF), 2) geometry-aware Gaussian density control (SDF\(\)GS); and 3) mutual geometry supervision (GS\(\)SDF). Our extensive results demonstrate the efficiency and joint performance improvement on both tasks. As the two branches maintain their original architectures, we keep their efficiency during inference, allowing room for potential enhancements by substituting each branch with more advanced models in the future. We envision our model benefiting applications demanding high-quality rendering and geometry, including embodied environments, physical simulation, and immersive VR experiences.

## 6 Acknowledgements

This work is funded in part by the National Key R&D Program of China (2022ZD0160201), and Shanghai Artificial Intelligence Laboratory.