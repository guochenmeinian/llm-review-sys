# The FineWeb Datasets: Decanting the Web for the

Finest Text Data at Scale

 Guilherme Penedo &Hynek Kydlicek &Loubna Ben allal &Anton Lozhkov Margaret Mitchell &Colin Raffel &Leandro Von Werra &Thomas Wolf &Hugging Face

###### Abstract

The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.

https://huggingface.co/datasets/HuggingFaceFW/fineweb

https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu

## 1 Introduction

Large Language Models (LLMs) have quickly become a ubiquitous technology thanks to their ability to competently perform a wide range of text-based tasks. A driving factor in the success of LLMs has been a steady increase in model sizes , which in turn necessitate ever-larger pretraining datasets. Beyond scale, other characteristics of pretraining data have proven to be important, including filtering out "low-quality" content  and removing duplicate text . Ultimately, the curation choices made when developing a pretraining dataset can have a huge impact on the downstream capabilities and performance of an LLM. As such, pretraining dataset curation strategies are often treated as closely guarded trade secrets. In fact, there are many popular "open" language models whose parameters are publicly available but whose pretraining datasets were not released and are scarcely documented . The lack of access to high-quality large-scale pretraining datasets and lack of information about their curation has led to concerns of a growing gap between proprietary and public knowledge.

In this work, we aim to minimize this gap by developing and releasing the FineWeb datasets, a collection of large-scale pretraining datasets that can be used to train performer LLMs. Specifically, we first introduce FineWeb, a 15-trillion token dataset of text sourced from 96 Common Crawl snapshots. FineWeb is sufficiently large to train a Chinchilla-optimal model  with more than 500 billion parameters. Beyond scale, FineWeb's recipe involves a principled strategy for choosing and tuning filtering heuristics that helped produce a small set of effective filters out of over fifty candidate filters from past work. In addition, we performed an in-depth exploration of how different deduplication strategies and granularities can impact performance. To validate our design choices, we ultimately demonstrate that models trained on FineWeb perform better than those trained on otherpublic web-based pre-training datasets. Inspired by recent work advocating for training LLMs on educational data [8; 9], we additionally introduce FineWeb-Edu, a subset of 1.3 trillion tokens from FineWeb that was rated as highly educational by a custom classifier. Models trained on FineWeb-Edu exhibit significantly better performance on knowledge- and reasoning-intensive benchmarks like MMLU  and ARC . Both datasets are released under the permissive ODC-By License. Apart from contributing datasets, we also release datatrove , the data processing library we developed to create FineWeb. On the whole, our work represents a significant step towards improving public knowledge and resources for curating LLM pre-training datasets.

## 2 Background

In this work, we focus on the curation of training datasets for autoregressive Transformer-based large language models (LLMs) . At their core, LLMs aim to produce a distribution over the next token of text conditioned on past tokens, where each token is typically a word or subword unit . The generality of this paradigm allows LLMs to be applied to virtually any text-based task by formulating a prefix whose continuation corresponds to performing the task (e.g. "The cat sat on the mat translated to French is..." for English-to-French translation). Such models may undergo many stages of training including pretraining on unstructured text data, fine-tuning to improve performance on a specific task , multitask fine-tuning to improve generalization to new tasks , and learning from human feedback to improve instruction-following capabilities [16; 2]. In this work, we focus solely on curating data for the pretraining stage.

While many sources have been considered for pretraining data including text from books [2; 3; 17; 4], Wikipedia [2; 3; 17; 4], and research papers [2; 4; 18], a highly common choice is to use web text, i.e. text scraped from webpages on the public internet [19; 20]. While some companies like OpenAI  and Anthropic  perform their own web scrapes, designing, implementing, and running a web scraper at scale requires significant resources and expertise. Many LLM pretraining datasets have therefore been constructed from text from the Common Crawl , a publicly available and continually updated collection of website snapshots that has been running since 2007. As of writing, Common Crawl has produced 100 web snapshots totaling preabytes of data.

Although Common Crawl has produced more than enough data to train recent LLMs, it has been shown that the performance of an LLM can heavily depend on how web text has been filtered and preprocessed before being used for pretraining . In particular, web text can contain a large amount of "unnatural" language (e.g. "boilerplate" text, gibberish, etc.). Training on unnatural language data can harm the performance of LLMs, possibly because most downstream uses of LLMs do not involve such data. On the other hand, filtering out _too_ much content can produce a dataset that is too small to perform sufficient pretraining (which typically involves only one pass or a few passes over the pretraining dataset ) for a general use model. Separately, web text can contain a large amount of duplicated content, which has also been shown to be harmful in the context of pretraining data . While deduplication may seem as straightforward as "removing duplicate text", in practice many design choices must be made (line, paragraph, or document-level deduplication? fuzzy or exact matching? etc.). The curation and relative performance of different web text-based pretraining datasets therefore heavily depends on a given dataset's filtering and deduplication pipeline.

Given that the focus of our work is to carefully design an effective Common Crawl-based pre-training dataset, we now briefly discuss the filtering and deduplication used in past public datasets.

**OSCAR** processes Common Crawl using a pipeline inspired by that of Touvron et al. , which uses a fastText-based language classifier  to filter pages based on their language and then performs deduplication at the line level using a non-cryptographic hash algorithm. **C4** uses langdetect to filter out non-English pages, then applies a series of heuristic filters (retaining only those lines that end in a terminal punctuation mark, removing short lines, discarding any page that contains a word from a "bad words" list, etc.), and finally deduplicates over three-line windows.

**CC-100** uses the cc_net pipeline, which includes fastText for language identification, performs paragraph-level hash-based deduplication, and retains only the text that is assigned a low perplexity by a n-gram language model trained on Wikipedia. **The Pile** is a composite dataset that includes "Pile-CC", a collection of text from one Common Crawl snapshot that uses pycld2  for language detection, jusText  for boilerplate removal, a classifier to filter out pages that are dissimilar form WebText (described below), and fuzzy deduplication using MinHash . **ROOTS** includes text from the pre-processed web crawl OSCAR with additional heuristic-based filtering and SimHash-based deduplication. **RedPajama** is a composite dataset that includes Common Crawl-sourced text processed using the cc_net pipeline as well as quality filtering using a classifier trained to distinguish Wikipedia level content from random Common Crawl samples. **SlimPajama** further processed RedPajama by removing short documents and performing additional fuzzy MinHash-based deduplication. **RefinedWeb** uses trafilatura  for text extraction, fastText for language identification, heuristic rules inspired by MassiveText (discussed below) to filter data, and both MinHash (fuzzy) and ExactSubstur (exact) deduplication. **RedPajama v2** has 84 Common Crawl snapshots released unfiltered and non-deduplicated but with labels from filtering techniques from cc_net, C4, MassiveText, RefinedWeb and others, as well as deduplication labels for exact (Bloom filter) and fuzzy (MinHash) deduplication. Finally, **Dolma** has a Common Crawl-based subset that uses fastText for language classification, heuristic rules from MassiveText and C4 for quality filtering, rules- and classifier-based toxicity filtering, and URL, document and paragraph-level deduplication using a Bloom filter.

Apart from public datasets, the technical reports accompanying the announcement of closed LLMs occasionally discuss pretraining datasets. **WebText** (used to train GPT-2) involves only those non-Wikipedia webpages that were linked to from Reddit posts with at least 3 karma, with text extracted using Dragnet  and Newspaper1  and an unspecified deduplication pipeline. **GPT-3's Dataset** includes content from Common Crawl that has been filtered using a classifier trained on WebText, Wikipedia, and Books, and deduplicated using MinHash. **MassiveText** (used to train Gopher) is a web-based dataset using Google's SafeSearch to remove explicit content and heuristic filters based on document's content (number of words, stop-words appearance, character repetition, etc.) as well as MinHash-based deduplication.

## 3 Building FineWeb

Our design of FineWeb is primarily empirical: we performed a series of "data ablation" experiments to test different methods at each stage of the pipeline. In this section, we chronicle our experimental results and design choices. All ablations follow our iterative dataset building process, i.e., the baseline model for each subsection includes only the processing steps from the previous subsections, unless explicitly mentioned. Our results are fully reproducible with code in our datatrove repository.

### Experimental setup

We compare pipeline design choices at each stage by training data ablation models that are identical apart from the data they were trained on (same number of parameters, architecture hyper-parameters, and trained on an equal number of randomly sampled tokens from each version of the data). We then evaluated them on the same set of downstream task benchmark datasets (discussed below). To minimize the impact of random data subset selection on evaluation scores, we trained two models for each dataset version, each using a different but equal-sized random subset of the full data and a different initialization seed, and then compared average scores.

All training was performed using the nanotron library. Data ablation models all had 1.71B parameters (including embeddings), used the Llama architecture  with a sequence length of 2048, a global batch size of ~2 million tokens, and the GPT2 tokenizer . Within a given experiment, all models were trained on the same amount of data for the same number of steps. Filtering ablations were trained on ~28 billion tokens (roughly the Chinchilla-optimal training size for this model size ), while some deduplication ablations and runs to confirm cumulative relative performance improvements after each step of filtering were conducted on 350 billion tokens. The full training hyperparameter are available in Appendix D. We make all models trained for our ablations publicly available on our dataset repository. In total, we trained over 70 models on our internal cluster, for an estimated total of 80,000 H100 GPU hours.

Evaluation was performed using the lightweight library. We aimed to select a set of benchmarks that would provide good signal at the relatively small scale of our data ablations. Specifically, we chose benchmarks where models showed minimal score variance between runs trained on different random samples of the same dataset; monotonic (or nearly monotonic) score improvement over a given training; and scores above random baseline for models of this size. These criteria ensure that the scores obtained on a subset of the data are representative of the entire dataset and that they reflect a reliable measurement of the effect of the training data on model performance. Ultimately,we selected benchmark datasets CommonSense QA , HellaSwag , OpenBook QA , PIQA , SIQA , WinoGrande , ARC , and MMLU , truncating large benchmarks to 1000 samples so that we could efficiently evaluate over the course of training. We publicly release our exact evaluation setup.

### Text extraction

Common Crawl data is available in two different formats: WARC and WET. WARC (Web ARChive format) files contain the raw data from the crawl, including the full page HTML and request metadata. WET (WARC Encapsulated Text) files provide a text-only version of crawled websites by using htmlparser. While WET files are commonly used as a starting point for dataset creation, similarly to Gao et al. , we found that WET files retained too much boilerplate and menu text. We therefore experimented with extracting the text content from the WARC files using the open source trafilatura library , which from visual inspection of the results provided good quality extraction when compared to other available libraries (less boilerplate and menu text). Custom text extraction is relatively costly, but its effects are felt on model performance: Fig. 2 shows the performance of ablation models trained on either trafilatura applied to WARC data or WET data, with minimal additional filtering (fastText language identification to filter samples with English as the highest probability label) and no deduplication. Using trafilatura-extracted text from WARC files clearly results in a more performant model and we therefore use WARC-based data in all of our following experiments.

### Base filtering

As a starting point to our filtering, we applied a basic filtering pipeline using part of the setup from RefinedWeb . Concretely, we applied URL filtering using a blocklist  to remove adult content, applied a fastText language classifier [52; 26] to keep only English text with a score \(>=0.65\), and applied quality and repetition filters from MassiveText , using the original thresholds. After applying this filtering to all of the WARC-based text extracted from the 96 snapshots available at the time of writing, we obtained roughly 36 trillion tokens of data when tokenized with the GPT-2 tokenizer. Applying these steps results in a performance uplift, as seen in Fig. 2.

### Deduplication

The web has many aggregators, mirrors, templated pages or just otherwise repeated content spread over different domains and webpages. Removing these duplicates (deduplicating) has been correlated with improvements in model performance  and a reduction in memorization of pretraining data [53; 54]. There are different ways to identify and even define duplicated data. Common approaches rely on hashing techniques or efficient data structures like suffix arrays . Methods can also be "fuzzy" by using a similarity metric or "exact" by checking for exact matches between two text chunks .

Following RefinedWeb , we experimented with MinHash, a fuzzy hash-based deduplication technique that scales efficiently to many CPU nodes and allows tuning of similarity thresholds (by controlling the number and the number of hashes per bucket) as well as the length of the subsequences considered (by controlling the n-gram size). We chose to collect each document's 5-grams, obtained using an English word tokenizer , and computed MinHashes using 112 hash functions in total, split into 14 buckets of 8 hashes each -- targeting documents that are at least 75% similar. Documents with the same 8 MinHashes in any bucket are considered duplicates of each other. We then perform a transitive clustering step where documents A, B and C will be in the same duplicate cluster if A and C are duplicates and B and C are duplicates, even if A and B do not have 8 matching MinHashes in any bucket with each other. One (randomly chosen) document is kept per duplicate cluster while the remaining duplicates are removed. We further discuss deduplication parameters in Appendix E.1.

Our first approach was to apply MinHash deduplication globally to the entire dataset (all 96 snapshots). We did this in an iterative manner: starting with the most recent snapshot (2023-50, at the time the experiment was run) and proceeding chronologically until we reached the oldest snapshot. When applied to the oldest snapshots, this process removed as much as 90% of the original base filtered data, as they were deduplicated against a large number of other snapshots. Deduplicating the entire dataset in this manner resulted in 4 trillion tokens of data. However, when training on a randomly sampled 350 billion tokens subset, our ablation models showed little improvement over a model trained on the non-deduplicated data, scoring far below RefinedWeb on our aggregate of tasks, as shown in Fig. 3.

This challenged our initial assumption that global deduplication would inevitably result in higher benchmark scores. We therefore performed an additional experiment to investigate the quality of the remaining data. We trained two models on two slices from the older 2013-48 snapshot: (a) the fully deduplicated remaining -31 billion tokens (_originally kept data_); and (b) 171 billion tokens obtained by individually deduplicating (without considering the other crawls) the -460 billion tokens that had been removed from this crawl in the iterative deduplication process (_originally removed data_). Results are presented in Fig. 4. They show that, for this older crawl _taken in isolation_, the data from it that was kept (10% of the original data) was actually _of worse quality_ than the 90% of data that was removed. We confirmed this by visual inspection: _originally kept data_ contains more ads, incoherent lists of keywords and generally badly formatted text than _originally removed data_.

We therefore tried an alternative approach: individually deduplicating each snapshot (independently from the others), using the same parameters as before. This resulted in 20 trillion tokens of data. When training on a random sample from this dataset (with data sampled from all snapshots) it matched RefinedWeb's performance, as per Fig. 5.

One of our hypotheses is that the main improvement gained from deduplication lies in the removal of large clusters of duplicates with _hundreds of thousands_ of documents  present in all crawls, while further deduplication of clusters with a small number of duplicates (less than ~100, i.e., the number of crawls) can harm performance. More specific filtering targeting the long tail of data quality might be more suited than deduplication for this subset of the data.

To attempt to improve upon individual deduplication of each snapshot, we additionally experimented with "lighter" global deduplication techniques. Ultimately, none of these techniques improved performance over independent per-snapshot deduplication. A full description of these methods and results are in Appendix E.3.

### Adding C4's filters

By this point we had reached the same performance as RefinedWeb  using our base filtering and independent MinHash. However, we noted that the C4 dataset , while smaller than FineWeb, still showed stronger performance on some of the benchmarks in our evaluation suite, in particular HellaSwag , one of the benchmarks in our aggregate group of tasks with the highest signal-to-noise ratio. Despite being one of the first large scale LLM training datasets, C4 is still frequently part of the pretraining data mixture of recent models such as LlamaA 1 . We set out to explore additional filtering steps that would allow us to match or surpass the performance of C4.

A natural starting point was to look into the processing of C4 itself.

C4 was constructed from the 2019-18 crawl by applying heuristic filters, which included dropping lines without a terminal punctuation mark, that mentioned javascript, or that had "terms-of-use"/"cookie policy" statements, and dropping documents that were too short or that contained "lorem ipsum" or a curly bracket (\(\)). We experimented with applying these filters to a baseline of the base filtered and individually deduplicated 2019-18 crawl, and, additionally, compared the results to C4 itself.

Fig. 6 shows that applying _All filters_ allows us to match _C4_'s HellaSwag performance; the _Curly bracket filter_, and the _Word lengths filter_ only give a small boost, removing 2.8% and 4.3% of tokens, respectively; the _Terminal punctuation filter_, by itself, gives the biggest individual boost, but removes _around 30%_ of all tokens; the lorem_ipsum, javascript and policy rules each remove <0.5% of training tokens, so we did not train on them individually; _All but terminal punct_ performs better than terminal_punct by itself, while removing less in total (~7%). We decided to apply all C4 filters mentioned above except the terminal punctuation filter, as it eliminates an excessively large amount of data.

Figure 5: **Individual minhash deduplication study**. Unlike Global minhash, deduplicating individually improves the average score.

Figure 6: **Comparison of C4 filters impact on HellaSwag benchmark performance**. The Terminal Punctuation filter provides the most significant performance uplift.

Figure 7: **Custom FineWeb filters study**. The combined filters outperform both the base filtered baseline as well as the best performing C4 filter (Terminal Punctation), while removing less.

### Developing additional heuristic filters

Past work has mainly developed heuristic filters through data inspection . In this work we devised a more systematic process for designing heuristic filters and tuning their thresholds. We started by collecting over **50** high-level statistics ranging from document-level metrics (e.g. number of lines, avg. line/word length, etc) to inter-document repetition metrics (inspired by MassiveText ) on both a high- and low-quality web dataset. Specifically, we used the individually and globally deduplicated versions of the 2013-48 snapshot (previously mentioned in Section 3.4) as our "high-quality" and "low-quality" datasets respectively. We then identified metrics for which the distribution of values differed significantly across the two datasets, inspected the histograms of the two distributions and empirically chose thresholds that would target sections of the histogram where the lower quality dataset frequency was higher than on the corresponding higher quality dataset section. As an example, we plot the distribution of the _fraction of lines ending with punctuation_ metric in Fig. 8. We can see that the higher quality dataset has in general higher document density for larger values of our metric, and, in particular, the lower quality dataset has a much higher density of documents for values < 0.12. We thus conclude that documents with a fraction of lines ending with punctuation < 0.12 are generally lower quality and use this value as a tentative threshold to filter documents. Following this process for all metrics yielded **16** candidate metric-threshold pairs.

We then assessed the effectiveness of these 16 newly created filters by conducting several 28B token ablation runs on the _2019-18 crawl_. Full details for these runs are in Appendix E.4. Out of all those runs, we identified **three** filters (see their ablations runs in Fig. 7) that demonstrated the most significant improvements on the aggregate benchmark score. Specifically, the chosen filters remove documents where the fraction of lines ending with punctuation is \(<=0.12\) (10.14% of tokens removed vs. 30% from the original C4 terminal punctuation filter), where the fraction of characters in duplicated lines is \(>=0.1\) (12.47% of tokens removed; the original MassiveText threshold for this ratio is \(>=0.2\)), and/or where the fraction of lines shorter than 30 characters is \(>=0.67\) (3.73% of tokens removed). When applying the three together, ~22% of tokens were removed and the aggregate score increased by about 1% in the 28B token ablations. These filters allowed us to further improve performance and, notably, surpass the C4 dataset performance while filtering out a smaller proportion of data.

### The final FineWeb dataset

Combining the decisions made in the previous sections and applying the resulting pipeline to 96 Common Crawl snapshots produces the 15T-token FineWeb dataset. Specifically, we extract text from WARC files (Section 3.2), apply base filtering (Section 3.3), perform individual per-crawl MinHash deduplication (Section 3.4), apply a selection of C4 filters (Section 3.5), and finally apply custom filters (Section 3.6). Each step provides a relative performance boost on our group of benchmark tasks, as seen in Fig. 9. For the public release of the dataset, we have also applied Personal Identifiable Information (PII) removal, by anonymizing email and public IP addresses.

In Fig. 10 we compare FineWeb with the following commonly used openly accessible web-scale datasets: RefinedWeb (500B tokens) , C4 (172B tokens) ; the Common Crawl-based part of Dolma 1.6 (3T tokens) and 1.7 (1.2T tokens) , The Pile (340B tokens) , SlimPajama (627B tokens) , the deduplicated variant of RedPajama21 (20T tokens) , English CommonCrawl section of Matrix (1.3T tokens) , English CC-100 (70B tokens) , and Colossal-OSCAR (850B tokens) . Notably, FineWeb shows strong performance and FineWeb-Edu (detailed below) outperforms all other open datasets we compared on our aggregate group of tasks, further validating

Figure 8: **Impact of Deduplication Methods on 2013-48 Crawl. Histogram showcasing higher frequency of documents with small fraction of _lines ending with a terminal mark_ for Global minhash compared to Individual one. A threshold selected for filtering is also indicated.**the design choices we made. Note that to train these models we randomly sampled 350 billion tokens from each dataset, without upsampling any individual Common Crawl snapshot.

## 4 FineWeb-Edu

An interesting approach has recently emerged for filtering LLM training datasets: using synthetic data to develop classifiers for identifying educational content. This technique was notably used in the non-public pretraining datasets of Llama 3  and Phi-3 , but its large-scale impact on web data filtering has not been publicly explored. We applied this technique to FineWeb by filtering it with an educational quality classifier developed from synthetic annotations generated by Llama-3-70B-Instruct . The resulting dataset, **FineWeb-Edu**, contains 1.3 trillion tokens. FineWeb-Edu is specifically optimized for educational content and outperforms all openly accessible web-based datasets on a number of reasoning- and knowledge-intensive benchmarks such as MMLU, ARC, and OpenBookQA by a significant margin.

To build the synthetic annotations, we use Llama-3-70B-Instruct to score 460,000 randomly sampled webpages from the FineWeb _CC-MAIN-2024-10_ snapshot for their educational quality on a scale from 0 to 5. We explored several prompt formats to automatically extract an educational score using an LLM and found that the additive scale used in previous work Yuan et al.  worked best. It allows the LLM to evaluate each criterion and build the score step-by-step, unlike the single-rating scale  which assigns a fixed score based on predefined categories. To avoid having the LLM favor highly technical pages like arXiv abstracts and submissions, we prompted it to focus on grade-school and middle-school level knowledge. The prompt used for synthetic annotations is in Appendix F.1.

To scale our filtering to the entirety of FineWeb, we trained a linear regression model on top of the _Snowflake-arctic-embed-m_ embedding model . We fine-tuned this linear regressor on 410,000 of our Llama 3 synthetic annotations for 20 epochs with a learning rate of 3e-4 (while keeping the embedding and encoder layers frozen). We selected the checkpoint with the highest F1 score on the held-out validation set containing the remaining 50,000 samples, treating Llama 3 annotations as ground-truth. After training, we rounded the model's output scores to integers from 0 to 5. We then used fixed thresholds to classify whether a given document from FineWeb was educational. We investigated the impact of using different thresholds for the filtering and ultimately chose a minimum threshold of 3 for FineWeb-Edu, which ultimately gave the best trade-off between performance on knowledge and reasoning intensive benchmarks and the performance on other benchmarks like HellaSwag . With a threshold of 3, the model achieved an F1 score of 82% on the validation set, indicating strong performance in distinguishing high-quality educational content.

Applying the classifier to the 15 trillion tokens of FineWeb required 6,000 H100 GPU hours.

To confirm the effectiveness of education filtering at a larger scale, we conducted a larger ablation training a 1.71B model on 350 billion tokens, similar to the FineWeb filtering ablations mentionedabove. As shown in Fig. 10 and Fig. 11, we observed that FineWeb-Edu surpasses FineWeb and all other open web datasets, with quite remarkable improvements on educational benchmarks such as MMLU, ARC and OpenBookQA. Specifically, MMLU score increases from 33% to 37%, a relative improvement of approximately 12%, and ARC score goes from 46% to 57%, an improvement of about 24%.

On MMLU, FineWeb-Edu can match the final performance of Matrix with almost 10x fewer tokens, demonstrating the effectiveness of classifiers trained on LLM annotations for large-scale data filtering. Additional evaluation plots can be found in Appendix F.2.

### Topic distribution

To examine how the educational classifier may skew the dataset towards certain topics, we embed  50k samples from FineWeb and 50k samples from FineWeb-Edu using a sentence-transformers  model (all-MiniLM-L6-v2) which we then project to 2D space using UMAP . Finally, we use DBSCAN  clustering to find the 100 densest topic clusters in the union of the two datasets, which we label using Llama 3.1 70B . To compare the two datasets, we plot the difference of the size (as a percentage of the entire dataset) of each cluster in FineWeb-Edu and FineWeb in Fig. 18. The educational classifier heavily favors topics such as 'Education, Learning, Teaching' or 'History, Culture, Politics', while down-sampling 'Business, Finance, Law', 'Entertainment, Film, Theater' and 'Places, Travel, Real Estate', among others.

### Domain fit

We evaluate the macro average perplexity of six checkpoints from our FineWeb and FineWeb-Edu ablation models on the domains from Paloma . We use the codebase provided in  but intentionally do not perform decontamination, to compare how well each dataset covers different domains. The results are in Fig. 12. The FineWeb model generally shows lower perplexity in broad web sources such as C4, mC4, Falcon, Dolma V1.5 or the CommonCrawl subset of RedPajama, as well as on Twitter AAE, Manosphere, Gab, reddit (100 Subreddits) or 4chan. FineWeb-Edu tends to favour sources containing Wikipedia (WikiText-103 and M2D2 Wikipedia) or that are heavy in academic content (M2D2 S2ORC, which has semantic scholar papers, and the Arxiv subset of RedPajama). FineWeb-Edu also seems to have better coverage of programming content (100 PLs) than FineWeb. We have included results per subset for some of these sources in Appendix F.4.

## 5 Bias analyses

Language models are known to reflect the biases present in their pretraining datasets [72; 73; 74; 75; 76; 77]. To provide a brief picture of dataset bias in FineWeb and FineWeb-Edu, we focus on subgroups recognised as "sensitive" or "protected" in English-speaking countries. These are a subset of subgroups that are historically subject to discrimination and are disproportionately the target of negative societal norms such as stereotyping, which is reflected in text-based language consumed for a dataset. We find that the FineWeb dataset has a relative overrepresentation of words that reflect hegemonic norms, known to be overrepresented in online text , such as'man' and 'christian'. Although biases across the _gender_, _religion_, and _age_ subgroups examined are not strong, we see the most skewed association between religion words and intimacy, such as 'christian dating' and 'jewish singles'. Fittingly, the FineWeb-Edu dataset captures associations that are less tied to intimacy compared to FineWeb and more expected from educational content of history and health, such as'man' being associated to 'king', and 'woman' associated to 'pregnancy'. Further details are provided in Appendix G.

Figure 11: **Performance Comparison on MMLU**. FineWeb-Edu achieves a 33.6% accuracy on the MMLU benchmark at only 38 billion tokens, significantly outperforming Matrix (second best on the metric), which reaches similar accuracy at 300 billion tokens.

## 6 Conclusion

In this paper, we developed the FineWeb datasets, a collection of large-scale LLM pretraining datasets that produce performant LLMs. Specifically, we release FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots, as well as FineWeb-Edu, a 1.3-trillion token dataset of educational content from FineWeb. FineWeb was created through a series of experiments that provided empirical evidence for our choice of text extraction strategy, deduplication procedure, and content filters. Both datasets are publicly released, along with the code and processing library that we used and all of the models we trained during our dataset ablation experiments.

While FineWeb and FineWeb-Edu attain state-of-the-art performance among public LLM pretraining datasets, we identify various paths to further improvement. First, both datasets are entirely comprised of web content scraped by Common Crawl. It is possible that augmenting either datasets with other datatypes (books, speech transcripts, etc.) could further improve performance. In addition, most of the experiments we ran were at a smaller scale due to computational constraints. Designing datasets at more realistic scales could provide more reliable guidance. Our evaluation setup was also by necessity limited to performance on academic benchmarks without any further instruction tuning or alignment. An evaluation setup that better reflected current usage patterns of LLMs might also be more reliable. We hope that our released datasets, code, and models help further improve public knowledge and development of performant LLM pretraining datasets.

Figure 12: **FineWeb and FineWeb-Edu fit to Paloma domains**. FineWeb has lower perplexity on broad web sources while FineWeb-Edu has better coverage of Wikipedia and programming content.