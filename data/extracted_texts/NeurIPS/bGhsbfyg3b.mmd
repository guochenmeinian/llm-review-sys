# Opponent Modeling with In-context Search

Yuheng Jing\({}^{1,2}\) Bingyun Liu\({}^{1,2}\) Kai Li\({}^{1,2,}\) Yifan Zang\({}^{1,2}\)

Haobo Fu\({}^{6}\) Qiang Fu\({}^{6}\) Junliang Xing\({}^{5}\) Jian Cheng\({}^{1,3,4,}\)

\({}^{}\) denotes corresponding authors

\({}^{1}\) Institute of Automation, Chinese Academy of Sciences

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\) School of Future Technology, University of Chinese Academy of Sciences

\({}^{4}\) AiRiA \({}^{5}\) Tsinghua University \({}^{6}\) Tencent AI Lab

{jingyuheng2022,liubingyun2021,kai.li,zangyifan2019,jian.cheng} @ia.ac.cn, {haobofu,leonfu}@tencent.com, jlxing@tsinghua.edu.cn

###### Abstract

Opponent modeling is a longstanding research topic aimed at enhancing decision-making by modeling information about opponents in multi-agent environments. However, existing approaches often face challenges such as having difficulty generalizing to unknown opponent policies and conducting unstable performance. To tackle these challenges, we propose a novel approach based on in-context learning and decision-time search named **O**pponent **M**odeling with **I**n-context **S**earch **(**OMIS**). OMIS leverages in-context learning-based pretraining to train a Transformer model for decision-making. It consists of three in-context components: an actor learning best responses to opponent policies, an opponent imitator mimicking opponent actions, and a critic estimating state values. When testing in an environment that features unknown non-stationary opponent agents, OMIS uses pretrained in-context components for decision-time search to refine the actor's policy. Theoretically, we prove that under reasonable assumptions, OMIS without search converges in opponent policy recognition and has good generalization properties; with search, OMIS provides improvement guarantees, exhibiting performance stability. Empirically, in competitive, cooperative, and mixed environments, OMIS demonstrates more effective and stable adaptation to opponents than other approaches. See our project website at https://sites.google.com/view/nips2024-omis.

## 1 Introduction

**O**pponent **M**odeling (**OM**) is a pivotal topic in artificial intelligence research, aiming to develop autonomous agents capable of modeling the behaviors, goals, beliefs, or other properties of _adversaries or teammates_ (collectively termed as _opponents_). Such modelings are used to reduce uncertainty in multi-agent environments and enhance decision-making . Despite the methodologies and insights proposed by existing OM approaches, their processes generally boil down to two stages: (1) **Pretraining**: pretrain a model with designed OM methodology on a training set of opponent policies; (2) **Testing**: deploying the pretrained model in a certain way on a testing set of opponent policies to benchmark adaptability to unknown opponents.

For these two processes, different OM approaches usually have their respective focuses: (1) **P**retraining-**F**ocused **A**pproach (**PFA**)  focuses on acquiring knowledge of responding to various opponents during pretraining and generalizing it to the testing stage; (2) **T**esting-**F**ocused **A**pproach (**TFA**)  focuses on updating the pretrained model during testing to reason and respond to unknown opponents effectively. However, existing PFAs and TFAs have their respective common and noteworthy drawbacks. For PFAs, they have _limited generalization abilities_, as thegeneralization of their pretrained models often lacks theoretical guarantees. Moreover, PFAs typically involve minimal additional operations during the testing stage, making them practically challenging to handle unknown opponents. For TFAs, they have _performance instability issues_. The _finetuning_ (_i.e._, update the pretrained model) of TFAs during testing can be tricky, as it involves several gradient updates using only a few samples to adjust the policy. Without careful manual hyperparameter tuning, TFAs always perform unstablely when facing unknown opponents.

To overcome the inherent issues of PFAs and TFAs, we propose a novel approach named **O**ponent **M**odeling with **I**n-context **S**earch (**OMIS**). The core motivation behind OMIS is _'think before you act'_: when facing an opponent with an unknown policy during testing, we first guess about his current policy based on historical context. We then conduct **D**ecision-**T**ime **S**earch (**DTS**) for a few steps using this imagined opponent policy, estimate the returns of each legal action, and choose the best one to act on. Such a process intuitively helps derive a policy more optimal than making a direct decision regarding only the current state. This approach is often reflected in real-life situations, such as the 'deep thinking' strategy employed by professional players in Go, chess, and other board games.

To enable such a DTS, we build three components: an _actor_, to respond appropriately to the current opponent during the DTS; an _opponent imitator_, who imitates the actions of the current opponent, enabling the generation of transitions in an imagined environment during the DTS; a _critic_, who estimates the value of the final search states, as we do not search until the end of the game. We argue that all three components should be _adaptive_, meaning they dynamically adjust based on changes in opponent information. Therefore, we adopt **I**n-**C**ontext **L**earning (**ICL**)-based pretraining to learn _three in-context components_, as ICL can endow them with the needed adaptability.

In summary, the methodology design of OMIS is as follows: (1) For Pretraining, we train a Transformer  model for decision-making based on ICL [20; 88; 57; 43]. We build our model with three components: an actor, who learns the best responses to various opponent policies; an opponent imitator, who imitates opponent actions; and a critic, who estimates state values; (2) For Testing, we use the pretrained three in-context components for DTS [80; 81; 13] to refine the actor's original policy. Based on predicting opponent actions and estimating state values, this DTS performs rollouts for each legal action, promptly evaluating and selecting the most advantageous action.

Theoretically, OMIS can provably alleviate the issues present in PFAs and TFAs. For _limited generalization ability_ of PFAs, OMIS's pretrained model is proven to converge on opponent policy recognition and to have _good generalization properties_: OMIS's pretrained model can accurately recognize seen opponents and recognize unseen opponents as the most familiar seen ones to some extent. For _performance instability issues_ of TFAs, OMIS's DTS avoids any gradient updates and theoretically provides improvement guarantees.

Empirically, extensive comparative experiments and ablation analyses in competitive, cooperative, and mixed environments verify the effectiveness of OMIS in adapting to unknown non-stationary opponent agents. Statistically, OMIS demonstrates better performance and lower variance during testing than other approaches, reflecting the generalizability and stability of opponent adaptation.

## 2 Related Work

**Opponent modeling.** In recent years, OM has seen the rise of various new approaches based on different methods, including those based on representation learning [29; 28; 107; 63; 40], Bayesian learning [106; 19; 24; 54], meta-learning [3; 41; 107; 94], shaping opponents' learning [22; 23; 47], and recursive reasoning [90; 101]. All approaches can be broadly categorized into PFAs and TFAs.

OM based on representation learning and meta-gradient-free meta-learning methods such as Duan et al.  typically fall into PFAs. PFAs' generalization on unknown opponents often lacks any theoretical analysis or guarantees. This also leads to PFAs not always performing well empirically. Our work utilizes ICL pretraining to provide good theoretical properties regarding generalization.

OM based on Bayesian learning and meta-gradient-based meta-learning such as Finn et al.  typically belong to TFAs. The finetuning of TFAs makes them unstable, as the opponent may continuously change policy during testing, making it challenging to adapt with a small number of samples for updating. Our work employs DTS to avoid finetuning and has improvement guarantees.

**In-context learning.** Algorithmically, ICL can be considered as taking a more agnostic approach by learning the learning algorithm itself [20; 88; 57; 43]. Recent work investigates why and how pretrained Transformers perform ICL [27; 49; 103; 1; 69]. Xie et al.  introduces a Bayesian framework explaining how ICL works. Some work [87; 2; 8] proves Transformers can implement ICL algorithms via in-context gradient descent. Lee et al.  proposes supervised pretraining to empirically and theoretically demonstrate ICL abilities in decision-making. Unlike existing decision-related work focusing on single-agent settings, our work explores the theoretical properties and empirical effects of using a Transformer pretrained based on ICL under the setting of OM.

**Decision-time search.** DTS involves searching in a simulated environment before each real action, aiming to obtain a more 'prescient' policy than no search [80; 81; 13; 50]. One of the most representative works is the AlphaGo series [74; 75; 76; 72], which achieves remarkable results in games like Go and Atari based on a DTS algorithm called _Monte Carlo Tree Search_ (MCTS) and self-play. Our work explores how to make DTS work in the context of OM. The DTS of the AlphaGo series assumes that opponent adopts the same strong policy as the agent we control. In contrast, the DTS in our work dynamically models the opponents' actions, focusing on better adapting to the current opponents.

See App. A for an overview of OM and related work on Transformers for decision-making.

## 3 Preliminaries

We formalize the multi-agent environment using an \(n\)-agent stochastic game \(,\{^{i}\}_{i=1}^{n},\)\(,\{R^{i}\}_{i=1}^{n},,T^{}\). \(\) is the state space, \(^{i}\) is the action space of agent \(i[n]\), \(=_{i=1}^{n}^{i}\) is the joint action space of all agents, \(:\) is the transition dynamics, \(R^{i}:\) is the reward function for agent \(i\), \(\) is the discount factor, and \(T\) is the horizon for each episode.

Following the tradition in OM, we mark the agent under our control, _i.e._, the _self-agent_, with the superscript \(1\), and consider the other \(n-1\) agents as _opponents_, marked with the superscript \(-1\). The joint policy of opponents is denoted as \(^{-1}(a^{-1}|s)=_{j 1}^{j}(a^{j}|s)\), where \(a^{-1}\) is the joint actions of opponents. Let the trajectory at timestep \(t\) in the current episode be \(y_{t}^{}=\{s_{0},a_{0}^{1},a_{0}^{-1},r_{0}^{1},r_{0}^{-1},, s_{t-1},a_{t-1}^{1},a_{t-1}^{-1},r_{t-1}^{1},r_{t-1}^{-1},s_{t}\}\). The _historical trajectories_\(_{t}:=(y_{T}^{(0)},,y_{T}^{},y_{t}^{})\) is always available to the self-agent. During the pretraining stage, opponent policies are sampled from a _training set of opponent policies_\(^{}:=\{^{-1,k}\}_{k=1}^{K}\). During the testing stage, opponent policies are sampled from a _testing set of opponent policies_\(^{}\), which includes an unknown number of unknown opponent policies.

In OM, the self-agent's policy can be generally denoted as \(^{1}(a^{1}|s,D)\) (abbreviated as \(\)), which dynamically adjusts based on the _opponent information data_\(D\) (referred to as **in-context data** in this paper). \(D\) can be directly composed of some part of the data from \(_{t}\), or it can be obtained by learning a representation from \(_{t}\). Building upon the pretraining, the objective of the self-agent is to maximize its expected _return_ (_i.e._, cumulative discounted reward) during testing:

\[ s_{t+1}&(|s_{t},a_ {t}^{1},a_{t}^{-1}),a_{t}^{-1}^{-1}(|s_{t}),\\ &^{-1}^{},a_{t}^{1}(|s_{t},D), \\ & D_{t},(^{})[_{t=0}^{T-1}^{t} R^{1}(s_{t},a_{t}^{1},a_{t}^{-1})].\] (1)

## 4 Methodology

In Sec. 4.1, we present how we build the in-context actor, opponent imitator, and critic for OMIS with ICL-based pretraining; in Sec. 4.2, we describe our method of using pretrained in-context components for DTS; in Sec. 4.3, we provide a theoretical analysis of both the ICL and DTS components of OMIS. We provide an overview of OMIS in Fig. 1 and the pseudocode of OMIS in App. B.

### In-Context-Learning-based Pretraining

To ensure that the actor learns high-quality knowledge of responding to various opponents, we first solve for the _Best Responses_ (BR) against different opponent policies. For each opponent policy \(^{-1,k}\) in \(^{}\) (where \(k[K]\)), we keep the opponent policy fixed as \(^{-1,k}\) and sufficiently train the PPO algorithm  to obtain the BR against \(^{-1,k}\), denoted as \(BR(^{-1,k}):=^{1,k,*}(a|s)\).

To generate training data for pretraining the three components, we continually sample opponent policies from \(^{}\) and use their corresponding BR to play against them. For each episode, we sample a \(^{-1,k}\) from \(^{}\) as opponents and use its BR \(^{1,k,*}\) as self-agent to play against it.

The procedure of generating _training data_ is as follows: for each timestep \(t\), we construct **in-context data**\(D_{t}^{k}:=(D^{},D_{t}^{})\) about \(^{-1,k}\), which is used to provide information about \(^{-1,k}\) for self-agent to recognize \(^{-1,k}\). \(D^{}=\{(_{h},_{h}^{-1,k})\}_{h=1}^{H}\) is _episode-wise in-context data_, generated by playing against \(^{-1,k}\) using any self-agent policy.1 It is used to characterize the overall behavioral pattern of \(^{-1,k}\) on an episode-wise basis. See the construction process of \(D^{}\) in App. C. \(D_{t}^{}=(s_{0},a_{0}^{-1,k},,s_{t-1},a_{t-1}^{-1,k})\) is _step-wise in-context data_, generated by the current episode involving \(^{-1,k}\) and \(^{1,k,*}\). It represents the step-wise specific behavior pattern of \(^{-1,k}\).

Furthermore, for each timestep \(t\), we collect the _Return-To-Go_ (RTG) obtained by the self-agent, denoted as \(G_{t}^{1,k,*}=_{t^{}=t}^{T}^{t^{}-t}r_{t^{}}^{1}= _{t^{}=t}^{T}^{t^{}-t}R^{1}(s_{t^{}},a_{t^{} }^{1,k,*},a_{t^{}}^{-1,k})\), where \(a^{1,k,*}^{1,k,*}\), \(a^{-1,k}\!\!^{-1,k}\), and \(V_{t}^{1,k,*}\!=\![G_{t}^{1,k,*}]\). To end with, the _training data for timestep \(t\)_ is obtained as:

\[_{t}^{k}:=(s_{t},D_{t}^{k},a_{t}^{1,k,*},a_{t}^{-1,k},G_{t}^{1,k, *}).\] (2)

After preparing the training data, we use supervised learning to pretrain an actor \(_{}(a_{t}^{1}|s_{t},D_{t}^{k})\) to learn the BR against \(^{-1,k}\), an opponent imitator \(_{}(a_{t}^{-1,k} s_{t},D_{t}^{k})\) to imitate the opponent's policy, and a critic \(V_{}(s_{t},D_{t}^{k})\) to estimate the state value of self-agent. Notably, all these components condition on \(D_{t}^{k}\) as their in-context data. For each episode, the optimization objectives are as follows:

\[_{}_{_{t}^{k},t[T],k[K]} [_{}(a_{t}^{1,k,*} s_{t},D_{t}^{k})],\] (3) \[_{}_{_{t}^{k},t[T],k[K]} [_{}(a_{t}^{-1,k} s_{t},D_{t}^{k})],\] (4) \[_{}_{_{t}^{k},t[T],k[K]} [(V_{}(s_{t},D_{t}^{k})-G_{t}^{1,k,*})^{2}].\] (5)

The left side of Fig. 1 illustrates OMIS's architecture and its pretraining procedure. Based on the understanding of ICL in decision-making, we design our architecture upon a causal Transformer .

### Decision-Time Search with In-Context Components

Following the best practices in OM, we assume the testing environment features _unknown non-stationary opponent agents_, which we denote as \(\). _Unknown_ indicates that the self-agent is unable to ascertain the _true policy_\(^{-1}\) employed by \(\). _Non-stationary_ implies that \(\) switches its policy between episodes in some way, with each switch involving randomly sampling a \(^{-1}\) from \(^{}\).

Figure 1: **Left:** The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \(^{}\). (2) Continuously sample opponent policy from \(^{}\) and collect training data by playing against it using BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \(_{}\), an opponent imitator \(_{}\), and a critic \(V_{}\). **Right:** The testing procedure of OMIS. During testing, OMIS refines \(_{}\) through DTS at each timestep. The DTS steps are as follows: (1) Do multiple \(L\)-step rollouts for each legal action, where \(_{}\) and \(_{}\) are used to simulate actions for the self-agent and opponent, respectively. \(V_{}\) is used to estimate the value of final search states. (2) Estimate a value \(\) for all legal actions, and the search policy \(_{}\) selects the legal action with the maximum \(\). (3) Use mixing technique to trade-off between \(_{}\) and \(_{}\) to choose the real action to be executed.

Following the general setup in the DTS domain, we assume the ground truth transition dynamic \(\) is available [75; 76; 12; 13; 9; 46; 38]. Based on the pretrained in-context components \(_{}\), \(V_{}\), \(_{}\), and \(\), we conduct DTS to play against \(\). At each timestep \(t\), we perform \(M\) times of rollouts with length \(L\) for each legal action \(_{t}^{1}\) of self-agent.2 This is done to estimate the value \((s_{t},_{t}^{1})\) for each \(_{t}^{1}\) under current true opponent policy \(^{-1}\) and current self-agent policy \(_{}\). The self-agent then executes the legal action with the highest \(\) value in the real environment. Our expectation is that through such a DTS, we can refine the original policy \(_{}\) to better adapt to \(\).

The specific process of the _DTS_ is as follows: for each timestep \(t\), we first construct **in-context data**\(D_{t}=(D^{},D_{t}^{})\) about \(\), and its construction method is almost identical to \(D_{t}^{k}\) mentioned in Sec. 4.1. However, since \(^{-1}\) is unknowable, we make a slight modification: \(D^{}\) is constructed by sampling consecutive segments from the most recent \(C\) trajectories in which \(\) participated.

After constructing \(D_{t}\), for any given legal action \(_{t}^{1}\), we sample the opponents' action by \(_{t}^{-1}_{}(|s_{t},D_{t})\) and transition using \(\) to obtain \(_{t+1}\) and \(_{t}^{1}\). We append \((s_{t},_{t}^{-1})\) to the end of \(D_{t}^{}\) to obtain the updated step-wise in-context data \(_{t+1}^{}\) and in-context data \(_{t+1}=(D^{},_{t+1}^{})\). Following, at the \(l\)-th step of the rollout for \(_{t}^{1}\) (\(l[L]\)), we sample self-agent action and opponent action using the following two formulas, respectively:

\[_{t+l}^{1} _{}(|_{t+l},_{t+l}),\] (6) \[_{t+l}^{-1} _{}(|_{t+l},_{t+l}).\] (7)

Transitioning with \(\) yields \(_{t+l+1}\) and \(_{t+l}^{1}\). Next, we append \((_{t+l},_{t+l}^{-1})\) to the end of \(_{t+l}^{}\) to obtain \(_{t+l+1}^{}\) and \(_{t+l+1}=(D^{},_{t+l+1}^{})\). After completing the \(L\)-th step, we use \(_{t+L+1}:=V_{}(_{t+L+1},_{t+L+1})\) to estimate the value of the final search state. When finishing \(M\) times of rollouts for \(_{t}^{1}\), we obtain an estimated value for \(_{t}^{1}\) by:

\[(s_{t},_{t}^{1}):=_{m=1}^{M}[_{t^{ }=t}^{t+L}_{}^{t^{}-t}_{t^{}}^{1}+ _{}^{L+1}_{t+L+1}].\] (8)

Here, \(_{}\) is the discount factor used in the DTS. After completing rollouts for all legal actions of the self-agent at timestep \(t\), we obtain our _search policy_ by maximizing \(\):

\[_{}(s_{t}):=_{_{t}^{1}}(s_{t},_{t }^{1}).\] (9)

In practical implementation, we observe that using \(_{}\) directly is not always effective. This is because we cannot totally precisely estimate the opponents' policy and the state value, making the results obtained from the DTS not sufficiently reliable across all states. To this phenomenon, we propose a simple yet effective **mixing technique** to balance the search policy and the original actor policy in deciding the action to be executed:

\[_{}(s_{t}):=_{}(s_{t}),\;||( s_{t},_{}(s_{t}))||>\\ a_{t}^{1}_{}(|s_{t},D_{t}),\;.\] (10)

Here, \(\) is a threshold hyperparameter. The main motivation for the mixing technique is as follows. We consider the expected return of the action selected by the DTS as the confidence of the search policy. When the confidence exceeds a certain threshold, we tend to consider that \(_{}\) has a relatively high probability of achieving better results than \(_{}\); otherwise, we prefer to use the original policy \(_{}\). See the testing procedure of OMIS on the right side of Fig. 1.

### Theoretical Analysis

Our theoretical analysis unfolds in the following two aspects. (1) We propose Lem. 4.1 and Thm. 4.2 to prove that _OMIS without DTS_ (denoted as _OMIS w/o_ S) converges to the optimal solution when facing a _true opponent policy_\(^{-1}^{}\); and it recognizes the opponent policy as the policy in \(^{}\) with a minimum certain form of KL divergence from \(^{-1}\) when facing a \(^{-1}^{}\). (2) Building upon Thm. 4.2, we further propose Thm. 4.3 to prove the policy improvement theorem of OMIS with DTS, ensuring that it leads to enhancements in performance.

To begin with, we instantiate a _Posterior Sampling in Opponent Modeling_ (PSOM) algorithm (see App. D.1) based on the _Posterior Sampling_ (PS) algorithm , where PSOM can be proven to share the same guarantees of converging to the optimal solution as PS. Based on some reasonable assumptions, we prove that OMIS w/o S is equivalent to PSOM.

**Lemma 4.1** (Equivalence of OMIS w/o S and PSOM).: _Assume that the learned \(_{}\) is consistent and the sampling of s from \(_{}^{-1}\) is independent of opponent policy, then given \(^{-1}\) and its \(D\), we have \(P(_{T}^{1}|D,^{-1};)=P(_{T}^{1}|D,^{-1}; _{})\) for all possible \(_{T}^{1}\)._

Here, '_consistent_' indicates that the network's fitting capability is guaranteed. \(_{}^{-1}(;^{-1})\) denotes the _probability distribution on all the trajectories involving \(^{-1}\) during pretraining_. \(_{T}^{1}=(s_{1},a_{1}^{1,*},,s_{T},a_{T}^{1,*})\) is _self-agent history_, where \(a^{1,*}\) is sampled from the BR to the opponent policy recognized by PSOM. The proof is provided in App. D.2.

**Theorem 4.2**.: _When \(^{-1}=^{-1,k}^{}\), if the PS algorithm converges to the optimal solution, then OMIS w/o S recognizes the policy of \(\) as \(^{-1,k}\), i.e., \(_{}\), \(_{}\), and \(V_{}\) converge to \(^{1,k,*}\), \(^{-1,k}\), and \(V^{1,k,*}\), respectively; When \(^{-1}^{}\), OMIS w/o S recognizes the policy of \(\) as the policies in \(^{}\) with the minimum \(D_{KL}(P(a^{-1}|s,^{-1})||P(a^{-1}|s,^{-1}))\)._

Based on this theorem, when OMIS w/o S faces seen opponents, it accurately recognizes the opponent's policy and converge to the BR against it; when facing unseen opponents, it recognizes the opponent's policy as the seen opponent policy with the smallest KL divergence from this unseen opponent policy and produces the BR to the recognized opponent policy. The proof is in App. D.3.

**Theorem 4.3** (Policy Improvement of OMIS's DTS).: _Given \(^{-1}\) and its \(D\), suppose OMIS recognizes \(\) as \(_{}^{-1}\) and \(V_{_{}^{-1}}^{_{-1}}\) is the value vector on \(\), where \(V(s):=V_{}(s,D),(a|s):=_{}(a|s,D)\). Let \(_{L}\) be the \(L\)-step DTS operator and \(^{}_{L}(V_{_{}^{-1}}^{})\), then \(V_{_{}^{-1}}^{^{}} V_{_{}^{-1}}^{}\) holds component-wise._

Within, OMIS's DTS can be viewed as \(_{L}\), and \(^{}\) corresponds to \(_{}\) in Eq. (9). Thm. 4.3 indicates that OMIS's DTS is guaranteed to bring improvement, laying the foundation for performance stability. Additionally, OMIS's DTS avoids gradient updates. The proof is provided in App. D.4.

**Analysis for generalization in OM.** In OM, _generalization_ can be typically defined as _performance when facing unknown opponent policies_ (e.g., _opponents like_\(\)). Existing approaches lack rigorous theoretical analysis under this definition of generalization. In Lem. 4.1, we proved that OMIS w/o S is equivalent to PSOM. In Thm. 4.2, we proved that PSOM can accurately recognize seen opponents and recognize unseen opponents as the most similar to the seen ones. Since OMIS w/o S is equivalent to PSOM, OMIS w/o S possesses the same properties. Additionally, Thm. 4.3 proved that OMIS's DTS ensures performance improvement while avoiding instability. These theoretical analyses potentially provide OMIS with benefits in terms of generalization in OM.

## 5 Experiments

### Experimental Setup

**Environments.** We consider three sparse-reward benchmarking environments for OM as shown in Fig. 2 (See App. E for detailed introductions of them):

* Predator Prey (PP) is a competitive environment with a continuous state space. In PP, the self-agent is a prey (green) whose goal is to avoid being captured by three predators (red) as much as possible. There are two obstacles (black) on the map. The challenge of PP lies in the need to model all three opponents simultaneously and handle potential cooperation among them.
* Level-Based Foraging (LBF) is a mixed environment in a grid world. In LBF, the self-agent is the blue one, aiming to eat as many apples as possible. The challenge of LBF is that cooperation with the opponent is necessary to eat apples of a higher level than the self-agent's (the apples and agents' levels are marked in the bottom-right). LBF represents a typical social dilemma.
* OverCooked (OC) is a cooperative environment using high-dimensional images as states. In OC, the self-agent is the green one, which aims at collaborating with the opponent (blue) to serve dishes as much as possible. The challenge of OC lies in the high-intensity coordination required between the two agents to complete a series of sub-tasks to serve a dish successfully.

**Baselines.** We consider the following representative PFAs, TFAs, and DTS-based OM approach:

* DRON : Encode hand-crafted features of opponents using a Mixture-of-Expert network while also predicting opponents' actions as auxiliary task (this is the most performant version in ).
* LIAM : Use the observations and actions of the self-agent to reconstruct those of the opponent through an auto-encoder, thereby embedding the opponent policy into a latent space.
* MeLIBA : Use _Variational Auto-Encoder_ (VAE) to reconstruct the opponent's future actions and condition on the embedding generated by this VAE to learn a Bayesian meta-policy.
* Meta-PG : Execute multiple meta-gradient steps to anticipate changes in opponent policies to enable fast adaptation during testing.
* Meta-MAPG : Compared to Meta-PG, it includes a new term that accounts for the impact of the self-agent's current policy on the opponent's future policy.
* MBOM : Use recursive reasoning in an environment model to learn opponents at different recursion levels and combine them by Bayesian mixing.
* OMIS w/o S: A variant of OMIS, where OMIS directly uses \(_{}\) based on \(D_{t}\) without DTS.
* SP-MCTS : Use a scripted opponent model to estimate the opponent's actions and apply MCTS for DTS. We adopt OMIS w/o S as its original self-agent policy. This is a DTS-based OM approach. Within, DRON, LIAM, and MeLIBA belong to PFAs; Meta-PG, Meta-MAPG, and MBOM belong to TFAs. See neural architecture design of all approaches in App. F. For a fair comparison, we implement MBOM and SP-MCTS using the ground truth transition \(\) as the environment model.

**Opponent policy.** We employ a diversity-driven Population Based Training algorithm MEP  to train a policy population. Policies from this MEP population are used to construct \(^{}\) and \(^{}\), ensuring that all opponent policies are performant and exhibit diversity. We measure and visualize the diversity of opponent policies within the MEP population in App. G.

We randomly select \(10\) policies from the MEP population to form \(^{}\). Then, we categorize opponent policies in the MEP population into two types: '\(seen\)' denotes policies belonging to \(^{}\), and '\(unseen\)' denotes policies not belonging to \(^{}\). We set up opponent policies with different ratios of \([seen:unseen]\) to form \(^{}\), _e.g._, \([seen:unseen]=[0:10]\) signifies that \(^{}\) is composed of 10 opponent policies that were never seen during pretraining.

During testing, all opponents are the _unknown non-stationary opponent agents_\(\) mentioned in Sec. 4.2. \(\) switches policies by sampling from \(^{}\) every \(E\) episodes.

**Specific settings.** For the pretraining stage, we train all approaches for \(4000\) steps. For the testing stage, all approaches use the final checkpoints of pretraining to play against \(\) for \(1200\) episodes. All bar charts, line charts, and tables report the _average_ and _standard deviation_ of the mean results over \(5\) random seeds. See all the hyperparameters in App. H.

### Empirical Analysis

We pose a series of questions and design experiments to answer them, aiming to analyze OMIS's opponent adaptation capability and validate each component's effectiveness.

**Question 1.**_Can OMIS effectively and stably adapt to opponents under various \(^{}\) configurations?_

We set up \(5\) different \([seen:unseen]\) ratios to form \(^{}\), and we show the average results of all approaches against \(\) corresponding to each ratio in Fig. 3. OMIS exhibits a higher average return and lower variance than other baselines across three environments, highlighting its effectiveness and stability in opponent adaptation under different \(^{}\) configurations. It can be observed that OMIS w/o S outperforms existing PFAs (_e.g._, MeLIBA) in most cases, validating that pretraining based on ICL exhibits good generalization on testing with unknown opponent policies.

The results also indicate that OMIS improves OMIS w/o S more effectively than SP-MCTS does. SP-MCTS sometimes even makes OMIS w/o S worse (_e.g._, in OC and parts of PP). This might be because (1) the opponent model of SP-MCTS, which estimates opponent actions, is non-adaptive and (2) the trade-off between exploration and exploitation in the MCTS is non-trivial to optimize.

Figure 2: The benchmarking environments.

**Question 2**.: _Can OMIS adapt well to each one of the true policies adopted by \(\)?_

In Fig. 4, we provide the average results of all approaches against each true opponent policy \(^{-1}\) employed by \(\) corresponding to ratio of \([seen:unseen]=[10:10]\). Similar to the observations in Fig. 3, OMIS exhibits higher return and lower variance than other baselines across various true opponent policies in PP, LBF, and OC. TFAs (_e.g._, Meta-PG) generally show significant performance gaps when facing different true opponent policies. This is likely due to the continuous switching of opponent policies, making finetuning during testing challenging or ineffective.

**Question 3**.: _How does OMIS work when the transition dynamics are learned?_

We include a variant named _Model-Based OMIS_ (MBOMIS) to verify whether OMIS can work effectively when the transition dynamics are unknown and learned instead. MBOMIS uses the most straightforward method to learn a transition dynamic model \(}\): given a state \(s\) and action \(a\), predicting the next state \(s^{}\) and reward \(r\) using _Mean Square Error_ (MSE) loss. \(}\) is trained using the \((s,a,r,s^{})\) tuples from the dataset used for pretraining OMIS w/o S. The testing results against unknown non-stationary opponents are shown in Fig. 5. Although MBOMIS loses some performance compared to OMIS, it still effectively improves over OMIS w/o S and generally surpasses other baselines. We also provide quantitative evaluation results of \(}\)'s estimation during testing in Tab. 1. Observations show that \(}\) generally has a small MSE value in predicting the next state and reward without normalization.

**Question 4**.: _Is OMIS robust to the frequency of which \(\) switches policies?_

We employ \(5\) different frequencies for \(\) to switch policies, _i.e._, \(E=2,5,10,20\), \(dynamic\) (abbreviated as \(dyna\)). Here, \(E=dyna\) indicates that \(\) randomly selects from \(2,5,10,20\) at the start of each switch as the number of episodes until the next switch. Fig. 7 shows the average results against \(\) with different switching frequencies. OMIS and OMIS w/o S exhibit a degree of robustness to different policy switching frequencies \(E\) of \(\) in PP, LBF, OC. Notably, as \(E\) increases, their performance generally shows a slight upward trend. This suggests that OMIS could gradually stabilize its adaptation to true opponent policies by accumulating in-context data.

**Question 5**.: _Is each part of OMIS's method design effective?_

We design various ablated variants of OMIS: (1) OMIS w/o S (See Sec. 5.1); (2) OMIS w/o mixing: a variant where the mixing technique is not used, _i.e._, using \(_{}\) instead of \(_{}\); (3) OMIS w/o

Figure 4: Average results against each true opponent policy during testing, where \([seen:unseen]=[10:10]\) and \(E=20\). Each point in X-axis denotes a policy switching of \(\), totaling \(60\) times. Y-axis denotes the average return against the corresponding switched \(^{-1}\).

Figure 7: Average results during testing when \(\) adopts different switching frequencies, where \([seen:unseen]=[10:10]\).

S, \(D^{,k}_{t}\): a variant without DTS where \(D^{,k}_{t}\) input is excluded from the model; (4) OMIS w/o S, \(D^{,k}\): a variant without DTS where \(D^{,k}\) input is excluded from the model.

Fig. 6 shows the average performance curves during pretraining for these variants against all policies in \(^{}\). In PP, LBF, and OC, OMIS w/o S consistently performs a lot worse than OMIS. This indicates that the DTS effectively improves the original policy of \(_{}\). OMIS w/o mixing exhibits a notable performance decrease compared to OMIS in LBF and OC. This suggests selective searching based on confidence is more effective than indiscriminately. It can be observed that \(D^{,k}\) and \(D^{,k}_{t}\) both play crucial roles in OMIS's adaptation to opponents, with \(D^{,k}\) making a greater contribution. This could be because capturing the overall behavioral patterns of opponents is more important than focusing on their step-wise changes.

**Question 6**.: _Can OMIS effectively characterize opponent policies through in-context data?_

For each policy in \(^{}\), we visualize OMIS's attention weights of \(D^{,k}\) over the final \(20\) timesteps in an episode in Fig. 8. Each position of tokens in \(D^{,k}\) has a weight indicated by the depth of color. In all three environments, the attention of OMIS exhibits the following characteristics: (1) Focusing on specific positions of tokens in \(D^{,k}\) for different opponent policies; (2) Maintaining a relatively consistent distribution for a given opponent policy across various timesteps within the same episode. This implies that OMIS can represent different opponent policies according to distinct patterns of different in-context data. We also provide quantitative analysis on OMIS's attention weights in App. I.

**Question 7**.: _How well does the in-context components of OMIS estimate?_

We collect true opponent actions and true RTGs as labels, using _Accuracy_ and _MSE_ as metrics to evaluate the effectiveness of \(_{}\)'s and \(V_{}\)'s estimations, respectively. In Tab. 2, we provide estimation results of the in-context components \(_{},V_{}\) during testing under different \([seen:unseen]\) ratios. It can be observed that \(_{}\) is estimated relatively accurately in OC, while \(V_{}\) is estimated relatively accurately in PP and LBF. However, \(_{}\) does not estimate very accurately in PP and LBF. This indicates that the functioning of OMIS does not necessarily depend on very precise estimates. In all three

    &  &  \\   & & \(10:0\) & \(10:5\) & \(10:10\) & \(5:10\) & \(0:10\) \\   & Avg. Next State MSE \(\) & 0.00499 \(\) 0.00002 & 0.00433 \(\) 0.00011 & 0.00401 \(\) 0.00005 & 0.00462 \(\) 0.00004 & 0.004545 \(\) 0.00002 \\  & Avg. Reward MSE \(\) & 0.13836 \(\) 0.01425 & 0.24080 \(\) 0.05558 & 0.15288 \(\) 0.01641 & 0.26276 \(\) 0.03761 & 0.22110 \(\) 0.00567 \\   & Avg. Next State MSE \(\) & 0.05759 \(\) 0.00187 & 0.05031 \(\) 0.00074 & 0.05702 \(\) 0.00160 & 0.05525 \(\) 0.00154 & 0.09272 \(\) 0.00066 \\  & Avg. Reward MSE \(\) & 0.00046 \(\) 0.00000 & 0.00004 \(\) 0.00000 & 0.00004 \(\) 0.00000 & 0.00004 \(\) 0.00000 & 0.00003 \(\) 0.00006 \\   & Avg. Next State MSE \(\) & 0.00028 \(\) 0.00001 & 0.00025 \(\) 0.00001 & 0.00028 \(\) 0.00001 & 0.00031 \(\) 0.00001 \\  & Avg. Reward MSE \(\) & 0.0000 \(\) 0.00000 & 0.00000 \(\) 0.00000 & 0.00000 \(\) 0.00000 & 0.00000 \(\) 0.00000 & 0.00000 \(\) 0.00000 \\   

Table 1: Quantitative evaluation results of the learned dynamic \(}\)’s estimation during testing. The results are calculated during testing under different \([seen:unseen]\) ratios, where \(E=20\).

Figure 5: Results of OMIS using learned dynamics against unknown non-stationary opponents. (a) Average results of testing under different \([seen:unseen]\) ratios, where \(E=20\). (b) Average results against each true opponent policy during testing, where \([seen:unseen]=[10:10]\) and \(E=20\).

Figure 6: Average performance curves during pretraining against all policies in \(^{}\).

[MISSING_PAGE_FAIL:10]