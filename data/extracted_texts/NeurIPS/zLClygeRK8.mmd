# Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning

Otmane Sakhi

Criteo AI Lab, Paris, France

o.sakhi@criteo.com

&Imad Aouali

CREST, ENSAE

Criteo AI Lab, Paris, France

i.aouali@criteo.com

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

Pierre Alquier

ESSEC Business School, Singapore

alquier@essec.edu

&Nicolas Chopin

CREST, ENSAE

nicolas.chopin@ensae.fr

###### Abstract

This work investigates the offline formulation of the contextual bandit problem, where the goal is to leverage past interactions collected under a behavior policy to evaluate, select, and learn new, potentially better-performing, policies. Motivated by critical applications, we move beyond point estimators. Instead, we adopt the principle of _pessimism_ where we construct upper bounds that assess a policy's worst-case performance, enabling us to confidently select and learn improved policies. Precisely, we introduce novel, fully empirical concentration bounds for a broad class of importance weighting risk estimators. These bounds are general enough to cover most existing estimators and pave the way for the development of new ones. In particular, our pursuit of the tightest bound within this class motivates a novel estimator (LS), that _logarithmically smooths_ large importance weights. The bound for LS is provably tighter than its competitors, and naturally results in improved policy selection and learning strategies. Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS.

## 1 Introduction

In decision-making under uncertainty, offline contextual bandit  presents a practical framework for leveraging past interactions with an environment to optimize future decisions. This comes into play when we possess logged data summarizing an agent's past interactions . These interactions, typically captured as context-action-reward tuples, hold valuable insights into the underlying dynamics of the environment. Each tuple represents a single round of interaction, where the agent observes a context (including relevant features), takes an action according to its current policy, often called _behavior policy_, and receives a reward that depends on both the observed context and the taken action. This framework is prevalent in interactive systems like online advertising, music streaming, and video recommendation. In online advertising, for instance, the user's profile is the context, the recommended product is the action, and the click-through rate (CTR) is the expected reward. By learning from past interactions, the recommender system tailors product suggestions to individual preferences, maximizing engagement and ultimately, business success.

To optimize future decisions without requiring real-time deployments, this framework presents us with three tasks: off-policy evaluation (OPE) , off-policy selection (OPS) , and off-policy learning (OPL) . OPE estimates the risk: the _negative of expected reward_ that a _target policy_ would achieve, essentially predicting its performance if deployed. OPS selects the best-performingpolicy from a finite set of options, and OPL finds the optimal policy within an infinite class of policies. In general, OPE is an intermediary step for OPS and OPL since its primary goal is policy comparison.

A significant amount of research in OPE has centered around Inverse Propensity Scoring (IPS) estimators [24; 16; 17; 18; 60; 19; 54; 38; 32; 45]. These estimators rely on importance weighting to address the discrepancy between the target and behavior policies. While unbiased under some conditions, IPS induces high variance. To mitigate this, regularization techniques have been proposed for IPS [10; 38; 54; 21; 5] trading some bias for reduced variance. However, these estimators can still deviate from the true risk, undermining their reliability for decision-making, especially in critical applications. In such scenarios, practitioners need estimates that cover the true risk with high confidence. To address this, several approaches focused on constructing either asymptotic [10; 48; 15] or finite sample [32; 21], high probability, empirical upper bounds on the risk. These bounds evaluate the performance of a policy in the worst-case scenario, adopting the principle of pessimism .

If this principle is used in OPE, it is central in OPS and OPL, where strategies are inspired by, or directly derived from, upper bounds on the risk [55; 35; 32; 49; 5; 59; 21]. Examples for OPS include Kuzborskij et al.  who employed an Efron-Stein bound for self-normalized IPS, or Gabbianelli et al.  that based their analysis on an upper bound constructed with the Implicit Exploration estimator. Focusing on OPL, Swaminathan and Joachims  exploited the empirical Bernstein bound  alongside the Clipping estimator to motivate sample variance penalization. This work was recently improved by either modifying the penalization  or analyzing the problem from the PAC-Bayesian lens . The latter direction was further explored by Sakhi et al. , Aouali et al. [5; 7], Gabbianelli et al.  resulting in tight PAC-Bayesian bounds that can be directly optimized.

Existing _pessimistic_ OPE, OPS, and OPL approaches involve analyzing the concentration properties of a _pre-defined risk estimator_, often chosen to simplify the analysis. We propose a different approach: we derive general concentration bounds applicable to a broad class of regularized IPS estimators and then identify the estimator within this class that achieves the tightest concentration bound. This leads to a tailored estimator, named Logarithmic Smoothing (LS). LS enjoys several desirable properties. It concentrates at a sub-Gaussian rate, and has a finite variance without being necessarily bounded. Its concentration upper bound allows us to evaluate the worst-case risk of any policy, enables us to derive a simple OPS strategy that directly minimizes our estimator akin to Gabbianelli et al. , and achieves state-of-the-art learning guarantees for OPL when analyzed within the PAC-Bayesian framework akin to [35; 49; 5; 7; 21].

This paper is structured as follows. Section 2 introduces the necessary background. In Section 3, we provide unified risk bounds for a broad class of regularized IPS estimators, for which LS enjoys the tightest upper bound. In Section 4, we analyze LS for OPS and OPL, and we further extend the analysis within the PAC-Bayesian framework. Extensive experiments in Section 5 highlight the favorable performance of LS, and Section 6 provides concluding remarks.

## 2 Setting and background

**Offline contextual bandit.** Let \(^{d}\) be the _context space_, which is a compact subset of \(^{d}\), and let \(=[K]\) be a finite _action set_. An agent's actions are guided by a _stochastic_ and _stationary_ policy \(\) within a policy space \(\). Given a context \(x\), \((|x)\) is a probability distribution over the action set \(\); \((a|x)\) is the probability that the agent selects action \(a\) in context \(x\). Then, an agent interacts with a contextual bandit over \(n\) rounds. In round \(i[n]\), the agent observes a context \(x_{i}\) where \(\) is a distribution with support \(\). After this, the agent selects an action \(a_{i}_{0}(|x_{i})\), where \(_{0}\) is the _behavior policy_ of the agent. Finally, the agent receives a stochastic cost \(c_{i}[-1,0]\) that depends on the observed context \(x_{i}\) and the taken action \(a_{i}\). This cost \(c_{i}\) is sampled from a cost distribution \(p(|x_{i},a_{i})\). This leads to \(n\)-sized logged data, \(_{n}=(x_{i},a_{i},c_{i})_{i[n]}\), where tuples \((x_{i},a_{i},c_{i})\) for \(i[n]\) are i.i.d. The expected cost of taking action \(a\) in context \(x\) is \(c(x,a)=_{c p(|x,a)}[c]\), and the costs are negative because they are interpreted as the negative of rewards. The performance of a policy \(\) is evaluated through its _risk_, which aggregates the expected costs \(c(x,a)\) over all possible contexts \(x\) and taken actions \(a\) by policy \(\), such as

\[R()=_{x,a(|x),c p(|x,a)}[c] =_{x,a(|x)}[c(x,a)]\,.\] (1)

The main goal is to use logged dataset \(_{n}\) to enhance future decision-making without necessitating live deployments. This often entails three tasks: OPE, OPS, and OPL. First, OPE is concerned 

[MISSING_PAGE_FAIL:3]

In Appendix F.1, we provide detailed proof, leveraging Chernoff bounds with a careful analysis of the moment-generating function. This results in the first empirical, high-order moment bound for offline contextual bandits, with several advantages. First, the bound applies to any regularization function \(h\) that satisfies the mild condition (C1), enabling the design of a tailored \(h\) that minimizes the bound. Second, it relies solely on empirical moments, without assuming the existence of theoretical moments. Third, the bound is fully empirical and tractable, facilitating efficient implementation of pessimism. Lastly, the parameter \(L\) controls the number of moments used, allowing a balance between bound tightness and computational cost. Specifically, for sufficiently small values of \(\), higher values of \(L\) yield tighter bounds, though potentially at the cost of increased computational complexity as we would need to compute higher order moments. This is formally stated as follows.

**Proposition 2** (Impact of \(L\)).: _Let \(\), \((0,1]\), \(>0\), \(L 1\), and \(h\) satisfying (C1). Then,_

\[_{i[n]}\{|}\} U_{L +1}^{,h}() U_{L}^{,h}()\,.\] (7)

From (7), the bound \(U_{L}^{,h}()\) in (6) becomes a decreasing function of \(L\) when \(_{i[n]}(1/|h_{i}|)\), suggesting that for sufficiently small \(\), the tightest bound is achieved as \(L\). This condition on \(\) also depends on the values of \(h\), highlighting the importance of the regularizer choice \(h\). In fact, once we evaluate our bounds at their optimal regularizer function \(h\), this condition on \(\) becomes unnecessary when comparing some of the optimal bounds. Specifically, we demonstrate in the following proposition that the bound with \(L=1\) can be always improved by increasing \(L\).

**Proposition 3** (Comparison of our bounds).: _Let \(\), and \(>0\), we define_

\[U_{L}^{}()=_{h}U_{L}^{,h}()\,, h_ {*,L}=*{argmin}_{h}U_{L}^{,h}()\,,\] (8)

_with the minimum taken over \(h\) satisfying (C1). Then, for any \(>0\), it holds that for any \(L>1\), \(U_{L}^{}() U_{1}^{}()\). In particular, for any \(>0\),_

\[U_{}^{}() U_{1}^{}()\,.\] (9)

Proposition 3 shows that, irrespective of the value of \(\), the bound with \(L=1\) can be always improved by bounds of increased moment order \(L\), evaluated at their optimal regularizer \(h_{*,L}\). This result encourages us to study bounds with high moment order \(L\), especially if we can derive their optimal regularizers \(h_{*,L}\). To this end, we examine two cases: \(L=1\), which results in an empirical second-moment bound, and \(L\), yielding a tight bound that does not require computing high-order moments. For each case, we identify the function \(h\) that minimizes the bound. If the minimizer for \(L=1\) is a variant of the clipping estimator , minimizing \(L\) motivates a novel logarithmic smoothing estimator. We begin by analyzing our empirical moment risk bound at \(L=1\).

### Global clipping

**Corollary 4** (Empirical second-moment risk bound with \(L=1\)).: _Let \(\), \((0,1]\), \(>0\), and \(h\) satisfying (C1). Then it holds with probability at least \(1-\) that_

\[R()_{}_{n}^{h}()+}_{n}^{h,2}()+\,.\] (10)

This is a direct consequence of (6) when \(L=1\). The bound holds for any \(h\) satisfying (C1). Thus we search for a function \(h_{*,1}\) that minimizes bound in (10). This function \(h_{*,1}\) writes

\[h_{*,1}(p,q,c)=-(p|c|/q,1/)\,.\] (11)

In particular, if we assume that costs are binary, \(c\{-1,0\}\), then \(h_{*,1}\) corresponds to clipping in (4) with parameter \(M=1/\). This is because \(-(|c|p/q,1/)=(p/q,)c\) when \(c\) is binary. This motivates the widely used clipping estimator . However, this also suggests that the standard way of clipping (as in (4)) is only optimal1 for binary costs. In general, the cost should also be clipped (as in (11)). Finally, with a suitable choice of \(=(1/)\), our bound in Corollary 4, using clipping (i.e., \(h=h_{*,1}\)), outperforms the existing empirical Bernstein bound , which was specifically derived for clipping. This confirms the strength of our general bound, as minimizing it results in a bound with tighter concentration than specialized bounds. Appendix F.4 gives the the proof to find \(h_{*,1}\) and formal comparisons with empirical Bernstein are provided in Appendix F.5. In the next section, we study our general bound when we set \(L\).

### Logarithmic smoothing

**Corollary 5** (Empirical infinite-moment bound with \(L\)).: _Let \(\), \((0,1]\), \(>0\), and \(h\) satisfying (C1). Then it holds with probability at least \(1-\) that_

\[R()_{}-_{i=1}^{n} (1- h_{i})+\,.\] (12)

Appendix F.6 provides detailed proof. Setting \(L\) in (6) results in the bound in Corollary 5, which has different properties than Corollary 4. The resulting bound has a simple expression that does not require computing high order moments. This means that we can obtain the best of both worlds, a tight concentration bound with no additional computational complexity. As the bound is increasing in \(h\), the function \(h_{*,}\) that minimizes this bound is \(h_{*,}(p,q,c)=pc/q\). This corresponds to the standard IPS in (2). This differs from the \(L=1\) bound in Corollary 4 that favored clipping. This shows the impact of the moment order \(L\) on the optimal function \(h\). For any \(\), applying the bound in Corollary 5 with the optimal \(h_{*,}\) leads to \(U_{}^{}()\), of the following expression:

\[U_{}^{}()=_{}_{n}^{}()+ \,.\] (13)

Even if we set \(h_{*,}(p,q,c)=pc/q\) (without IW regularization), \(U_{}^{}()\) can be seen as a risk upper bound of a novel regularized IPS estimator (satisfying (C1)), called Logarithmic Smoothing (LS):

\[_{n}^{}()=-_{i=1}^{n} (1- w_{}(x_{i},a_{i})c_{i})\,.\] (14)

The LS estimator in (14) is defined for any non-negative \( 0\), with its bound in (13) holding for any positive \(>0\). Notably, \(=0\) retrieves the standard IPS estimator in (2), while \(>0\) introduces a bias-variance trade-off by logarithmically smoothing the IWs (Figure 1). This estimator acts as a soft, differentiable variant of clipping with parameter \(1/\). A Taylor expansion of our estimator around \(=0\) yields

\[_{n}^{}()=_{n}()+_{=2}^{}}{}_{i=1}^{n}(w_{}(x_{i},a )c_{i})^{}\,.\] (15)

Thus, LS is a pessimistic estimator by _design_, implicitly implementing a form of _Sample All Moments Penalization_, which generalizes the _Sample Variance Penalization_. To examine the statistical properties of our estimator, we introduce

\[_{}()=[(x,a)c)^{2}}{(1-  w_{}(x,a)c)}]\,,\] (16)

which quantifies the discrepancy between \(\) and \(_{0}\). Notably, \(_{}()\) is always smaller than the second moment of the IW, effectively interpolating between a weighted first moment (\( 1\)) and the second moment (\(=0\)) of IPS. This quantity \(_{}\) characterizes the concentration properties of the LS estimator akin to the coverage ratio for IX estimator . With \(_{}\) defined, we proceed by bounding the mean squared error (MSE) of our estimator, specifically bounding its bias and variance.

**Proposition 6** (Bias-variance trade-off).: _Let \(\) and \( 0\). Let \(^{}()\) and \(^{}()\) be respectively the bias and the variance of the LS estimator. Then we have that_

\[0^{}()_{}()\,, ^{}()_{}()} {n}\,.\]

_Moreover, it holds that for any \(>0\), the variance is finite as \(^{}()|R()|/ n 1/ n\)._

We observe that both the bias and variance are controlled by \(_{}()\). Particularly, \(=0\) recovers the IPS estimator in (2), with zero bias and a variance bounded by \([w^{2}(x,a)c^{2}]/n\). When \(>0\), a bias-variance trade-off emerges. The bias is always non-negative and is capped at \(_{}()\), which diminishes to zero when \(\) is small and goes to \(|R()|\) as \(\) increases. Conversely, the variance decreases with a higher \(\). Notably, \(>0\) ensures finite variance bounded by \(1/ n\), despite the estimator being unbounded. This is different from previous estimators that relied on bounded functions to ensure finite variance. We also prove in the following that a good choice of \(=(1/)\) ensures that our LS estimator enjoys a sub-Gaussian concentration .

Figure 1: LS with different \(\)s.

**Proposition 7** (Sub-Gaussianity and comparison with Metelli et al. ).: _Let \(\), \((0,1]\) and \(>0\). Then the following inequalities holds with probability at least \(1-\):_

\[R()-_{n}^{}()\,, _{n}^{}()-R()_{ }()+\,.\]

_In particular, setting \(=_{*}=[w_{}(x,a)^{2}c^{2} ]}\) yields that_

\[|R()-_{n}^{_{*}}()|(2/)}\,, \;\;^{2}=2[w_{}(x,a)^{2}c^{2} ]/n\,.\] (16)

Thus, a particular choice of \(^{*}\) ensures that \(_{n}^{_{*}}()\) is sub-Gaussian, with a variance proxy \(^{2}\) that improves on that obtained for the Harmonic estimator of Metelli et al. . We refer the interested reader to Appendix E.2 for further discussions and proofs.

Next, we focus on the tightness of the LS upper bound in (13) as it will motivate our selection and learning strategies. Proposition 3 already showed that \(U_{}^{}()\), the bound of LS is tighter than \(U_{1}^{}()\), the bound in Corollary 4 evaluated at the Global clipping function \(h_{*,1}\). In this section, we compare the LS bound to the already tight IX bound presented by Gabbianelli et al.  and demonstrate in the following that the LS bound dominates it in all scenarios.

**Proposition 8** (Comparison with IX of Gabbianelli et al. ).: _Let \(\), \(]0,1]\) and \(>0\), the IX bound from  states that we have with probability at least \(1-\)_

\[R()_{n}^{}()+\,,_{n}^{}()= _{i=1}^{n}|x_{i})}{_{0}(a_{i}|x_{i})+/2}c_{i}.\] (17)

_Let \(U_{}^{}()\) be the upper bound of (17), we have for any \(>0\):_

\[U_{}^{}() U_{}^{}()\,.\] (18)

This result states that no matter the scenario, for any evaluated policy \(\), and any chosen \(>0\), the LS bound will be always tighter than IX. The gap between the LS and IX bounds increases when \(n\) is small, or when the evaluated policy \(\) is stochastic, as demonstrated and developed in Appendix F.8. These findings further validate the effectiveness of our approach, enabling us to identify the LS estimator, with an empirical bound that improves upon the tightest existing bounds. Consequently, we leverage the LS bound in the next section to derive our pessimistic OPS and OPL strategies.

## 4 Off-policy selection and learning

### Off-policy selection

Let \(_{}=\{_{1},...,_{m}\}\) be a finite set of policies. In OPS, the goal is to find \(_{*}^{}_{}\) that satisfies

\[_{*}^{}=*{argmin}_{_{}}R()= *{argmin}_{k[m]}R(_{k})\,.\] (19)

As we do not have access to the true risk, we use a data-driven selection strategy that guarantees the identification of policies of performance close to that of \(_{*}^{}\). Precisely, for \(>0\), we search for

\[_{n}^{}=*{argmin}_{_{}} _{n}^{}()=*{argmin}_{k[m]}_{n}^{}( _{k})\,.\] (20)

To derive our strategy in (20), we minimize the bound of LS in (13), employing pessimism . Fortunately, in our case, this boils down to minimizing \(_{n}^{}()\), since the other terms in the bound are independent of the target policy \(\). This allows us to avoid computing complex statistics [55; 32] and does not require access to the behavior policy \(_{0}\). As we show next, it also ensures low suboptimality.

**Proposition 9** (Suboptimality of our selection strategy in (20)).: _Let \(>0\) and \((0,1]\). Then, it holds with probability at least \(1-\) that_

\[0 R(_{n}^{})-R(_{*}^{})_ {}(_{*}^{})+}|/)}{ n}\,,\] (21)

_where \(_{}()\), \(_{*}^{}\) and \(_{n}^{}\) are defined in (15), (19) and (20)._The derived suboptimality bound only requires coverage of the optimal actions (support of the optimal policy \(_{*}^{}\)), and improves on IX suboptimality , matching the minimax suboptimality lower bound of pessimistic methods [34; 27; 28]. Appendix G.1 provides proof of this suboptimality bound, and we discuss how this suboptimality improves upon existing strategies in Appendix E.3. By selecting \(_{n}^{}=}|/)/n}\) for LS, we achieve a suboptimality scaling of \((1/)\),

\[0 R(_{n}^{})-R(_{*}^{})(1+ _{_{n}^{}}(_{*}^{}))}|/)/n},\] (22)

which ensures finding the optimal policy with sufficient samples. Additionally, the multiplicative constant is smaller when \(_{0}\) is close to \(_{*}^{}\), confirming the known observation that it is easier to identify the best policy if it is similar to the behavior policy \(_{0}\).

### Off-policy learning

Similar to how we extended the evaluation bound in Corollary 5 (which applies to a single fixed target policy) to OPS (where it applies to a finite set of target policies), we can further derive bounds for an infinite policy class II, enabling OPL. Several approaches have been proposed in previous work, primarily based on replacing the finite union bound over policies with more sophisticated uniform-convergence arguments. This was used by , which derived a variance-sensitive bound scaling with the covering number . Since these approaches incorporate a complexity term that depends only on the policy class, the resulting pessimistic learning strategy (which minimizes the upper bound) would be similar to the selection strategy adopted earlier, leading, for a fixed \(\), to

\[_{n}^{}=*{argmin}_{}_{n}^{ }()+()}{ n}=*{argmin}_{ }_{n}^{}().\] (23)

where \(()\) is a complexity measure . This learning strategy is straightforward because it involves a smooth estimator that can be optimized using first-order methods and does not require second-order statistics. However, analyzing this approach is more challenging because the complexity measure \(()\) varies depending on the policy class considered, is often intractable  and can only be upper bounded with problem dependent constants .

Instead of the method described above, we derive PAC-Bayesian generalization bounds [37; 11] that apply to arbitrary policy classes. This framework has been shown to provide strong performance guarantees for OPL in practical scenarios [49; 5]. The PAC-Bayesian framework analyzes the performance of policies by viewing them as randomized predictors . Specifically, let \(()=\{f_{}:[K],\}\) be a set of parameterized predictors that associate the context \(x\) with the action \(f_{}(x)[K]\). Let \(()\) be the set of all probability distributions on \(\). Each distribution \(Q()\) defines a policy \(_{Q}\) by setting the probability of action \(a\) given context \(x\) as the probability that a random predictor \(f_{} Q\) maps \(x\) to action \(a\), that is,

\[_{Q}(a|x) =_{ Q}[[f_{}(x)=a ]]\,, (x,a)\,.\] (24)

This characterization is not restrictive as any policy can be represented in this form . Deriving PAC-Bayesian generalization bounds with this policy definition requires the regularized IPS to be linear in the target policy \(\)[35; 5; 21]. Our estimator LS in (14) is non-linear in \(\). Therefore, for this PAC-Bayesian analysis, we introduce a linearized variant of LS, called LS-LIN, and defined as

\[_{n}^{}()=-_{i=1}^{n}|x_{i})}{}(1-}{_{0}(a_{i}|x_{i} )}),\] (25)

which smooths the impact of the behavior propensity \(_{0}\) instead of the IWs \(/_{0}\). We provide in the following a core result of this section, the PAC-Bayesian bound that defines our learning strategy.

**Proposition 10** (PAC-Bayes learning bound for \(_{n}^{}\)).: _Given a prior \(P()\), \((0,1]\) and \(>0\), the following holds with probability at least \(1-\):_

\[ Q(), R(_{Q})_{} {R}_{n}^{}(_{Q})+(Q||P)+ }{ n}\,,\] (26)

_where \((Q||P)\) is the Kullback-Leibler divergence from \(P\) to \(Q\)._PAC-Bayes bounds hold uniformly for all distributions \(Q()\) and replace the complexity measure \(()\) with the divergence \((Q||P)\) from a reference _prior_ distribution \(P\). Extensive research focuses on identifying the best strategies for choosing this prior \(P\). While these bounds hold for any fixed prior \(P\), in practice, it is typically set to the distribution inducing the behavior policy \(_{0}\), meaning \(P\) satisfies \(_{0}=_{P}\). This leads to an intuitive learning principle: by minimizing the upper bound, we seek policies with good empirical risk that do not deviate significantly from \(_{0}\).

Our bound can also be obtained using the truncation method from Alquier [1, Corollary 2.5]. This bound surpasses the already tight PAC-Bayesian bounds derived for Clipping , Exponential Smoothing , and Implicit Exploration , resulting in the tightest known generalization bound in OPL. Appendix G.2 gives formal proof of this bound and comparisons with existing PAC-Bayesian bounds can be found in Appendix E.4. For a fixed \(\) and a fixed prior \(P\), we derive a learning strategy that minimizes the upper bound for a subset \(()()\) of distributions, seeking

\[Q_{n}=*{argmin}_{Q()}\{_{n}^{ }(_{Q})+(Q||P)}{ n}\}\,, _{n}^{}=_{Q_{n}}\,.\] (27)

(27) is tractable and can be efficiently optimized for various policy classes [49; 5]. Below, we analyze its suboptimality compared to the best policy in the chosen class, \(_{Q^{*}}=*{argmin}_{Q()}R(_{Q})\).

**Proposition 11** (Suboptimality of the learning strategy in (27)).: _Let \(>0\), \(P()\) and \((0,1]\). Then, it holds with probability at least \(1-\) that_

\[0 R(_{n}^{})-R(_{Q^{*}})_{ }^{}(_{Q^{*}})+(Q^{*}||P)+(2/ ))}{ n},\] (28)

_where \(_{}^{}()=[(a|x)c^{2}/(_{0 }^{2}(a|x)-_{0}(a|x)c)]\) and \(_{n}^{}\) is defined in (27)._

Our suboptimality bound only requires coverage of the support of the optimal policy \(_{Q_{*}}\). This bound matches the minimax suboptimality lower bound of pessimistic learning with deterministic policies . Appendix G.3 provides a proof of Proposition 11, while Appendix E.5 discusses the suboptimality bound further and proves that it improves on the IX learning strategy of [21, Section 5]. Setting \(_{n}^{l}=2/\) guarantees us a suboptimality that scales with \((1/)\) as

\[0 R(_{n}^{})-R(_{Q^{*}})(2_{_{ n}^{}}(_{Q^{*}})+(Q^{*}||P)+(2/))/.\]

By setting the reference \(P\) to the distribution inducing \(_{0}\), we find that the learning suboptimality is reduced when the behavior policy \(_{0}\) is close to the optimal policy \(_{Q^{*}}\). This is similar to the suboptimality for our selection strategy. The suboptimality upper bound reflects a common intuition in the OPL literature: pessimistic learning algorithms converge faster when \(_{0}\) is close to \(_{Q^{*}}\).

## 5 Experiments

Our experimental setup follows the standard multiclass-to-bandit conversion used in prior studies [18; 55]. Each multi-class dataset has features and labels and we convert it to contextual bandit problems where contexts correspond to features and actions to labels. Precisely, the reward \(r\) for taking action (label) \(a\) with context (features) \(x\) is modeled as Bernoulli with probability \(p_{x}=+[a=(x)](1-2)\), where \((x)\) be the true label of features \(x\), and \(\) is a noise parameter. In particular, the true label \((x)\) represents the action with the highest average reward for context \(x\). This setup ensures an average reward of \(1-\) for the optimal action \((x)\) and \(\) for all others, constructing a logged bandit feedback dataset in the form \(\{x_{i},a_{i},c_{i}\}_{i[n]}\), where \(c_{i}=-r_{i}\) is the associated cost.

### Off-policy evaluation and selection experiments

For both evaluation and selection, we adopt the same experimental design as  to facilitate the comparison. We consider exponential target policies \((a|x)(f(a,x))\), with \(\) a temperature controlling the policy's entropy and \(f(a,x)\) the score of the item \(a\) for the context \(x\). We use this to define ideal policies as \(^{}(a|x)(\{(x)=a\})\), and also create faulty, mismatching policies for which the peak is shifted to another, wrong action for a set of faulty actions \(F[K]\). To recreate real world scenarios, we also consider policies directly learned from logged bandit feedback, of the form \(_{^{}}(a|x)(x^{t}_{a}^{ })\) and \(_{^{}}(a|x)(x^{t}_{a}^{ })\), with their parameters learned by respectively minimizing the IPS  and SN  empirical risks. More details on the definition of the different policies are given in Appendix H. Finally, 11 real multiclass classification datasets are chosen from the UCI ML Repository  (See Table 3 in Appendix H.1.1) with various number of samples, dimensions and action space sizes to conduct our experiments2.

**(OPE) Tightness of the bounds.** Evaluating the worst case performance of a policy is done through evaluating risk upper bounds [10; 32]. This means that a better evaluation will solely depend on the tightness of the bounds used. To this end, given a policy \(\), we are interested in bounds \(U()\) with a small relative radius \(|U()/R()-1|\). We compare our newly derived bounds (cIPS-L=1 for \(U_{1}^{}\) and LS for \(U_{}^{}\) both with \(=1/\)) to empirical evaluation bounds of the literature: SN-ES: the Efron Stein bound for Self Normalized IPS , cIPS-EB: Empirical Bernstein for Clipping  and the recent IX: Implicit Exploration bound . The first experiment uses the kropt dataset with \(=0.2\), collects bandit feedback with faulty behavior policy (with \(=0.25\)) to evaluate an ideal policy (\(=0.1\)), and explores how the relative radiuses of the considered bounds shrink while varying the number of datapoints. Table 1 complies the results of the experiments and suggest that the LS bound is tighter than its competitors no matter the size of the feedback collected. The second experiments uses all 11 datasets, with different behavior policies (\(_{0}\{0.2,0.25,0.3\}\)) and different noise levels (\(\{0.,0.1,0.2\}\)) to evaluate ideal policies with different temperatures (\(\{0.1,0.2,0.3,0.4,0.5\}\)), defining \( 500\) different scenarios to validate our findings. We plot in Figure 2 the cumulative distribution of the relative radius of the considered bounds. We observe that while cIPS-L=1 and IX can be comparable, the LS bound is tighter than all its competitors. We also provide detailed results in Appendix H.1.2 that further confirm the superiority of the LS bound.

**(OPS) Find the best, avoid the worst policy.** Policy selection aims at identifying the best policy among a set of finite candidates. In practice, we are interested in finding policies that improve on \(_{0}\) and avoid policies that perform worse than \(_{0}\). To replicate real world scenarios, we design an experiment where \(_{0}\) is a faulty policy (\(=0.2\)), that collects noisy (\(=0.2\)) interaction data, some of which is used to learn \(_{^{}},_{^{}}\), and that we add to our discrete set of policies \(_{k=4}=\{_{0},^{},_{^{}},_{^ {}}\}\). The goal is to measure the ability of our selection strategies to choose from \(_{k=4}\), better performing policies than \(_{0}\). We thus define three possible outcomes: a strategy can select _worse_ performing policies, _better_ performing or the _best_ policy. Our goal in these experiments is to empirically validate the pitfalls of point estimators while confirming the benefits of using the pessimism principle. To this end, we compare _pessimistic_ selection strategies to policy selection using the classical point estimators IPS  and SN . The comparison is conducted on the 11 UCI datasets with 10 different seeds resulting in 110 scenarios. We plot in Figure 2 the percentage of time each method selected the best policy, a better or a worse policy than \(_{0}\). While risk estimators can identify the best policy, they are unreliable as they can choose worse performing policies than \(_{0}\), a catastrophic outcome in critical applications. Pessimistic selection is more conservative, as it avoids poor performing policies completely and empirically confirms that tighter upper bounds result in better selection strategies: LS upper bound is less conservative and finds best policies the most (comparable to SN) while never selecting poor performing policies. Fine grained results (for each dataset) can be found in Appendix H.1.3.

### Off-policy learning experiments

We follow the successful off policy learning paradigm based on directly minimizing PAC-Bayesian risk generalization bounds [49; 5] as it comes with guarantees of improvement and avoids hyper

   Number of samples & SN-ES & cIPS-EB & IX & cIPS-L=1 (Ours) & LS (Ours) \\  \(2^{8}\) & 1.000 & 0.917 & 0.373 & 0.364 & **0.362** \\ \(2^{9}\) & 1.000 & 0.732 & 0.257 & 0.289 & **0.236** \\ \(2^{10}\) & 0.794 & 0.554 & 0.226 & 0.240 & **0.213** \\ \(2^{11}\) & 0.649 & 0.441 & 0.171 & 0.197 & **0.159** \\ \(2^{12}\) & 0.472 & 0.327 & 0.126 & 0.147 & **0.117** \\ \(2^{13}\) & 0.374 & 0.204 & 0.062 & 0.077 & **0.054** \\ \(2^{14}\) & 0.257 & 0.138 & 0.041 & 0.049 & **0.035** \\   

Table 1: Bound’s tightness (\(|U()/R()-1|\)) with varying number of samples of the kropt dataset.

parameter tuning. For comparable results, we use the same 4 datasets (described in Appendix H.2, Table 7) as in [49; 5] and adopt the **LGP**: Linear Gaussian Policies  as our class of parametrized policies. For each dataset, we use behavior policies trained on a small fraction of the data in a supervised fashion, combined with different inverse temperature parameters \(\{0.1,0.3,0.5,0.7,1.\}\) to cover cases of diffused and peaked behavior policies. These policies generate for 10 different seeds, 10 logged bandit feedback datasets resulting in 200 different scenarios to test our learning approaches. In the PAC-Bayesian OPL paradigm, we minimize the empirical upper bounds \(U()\) directly and obtain the learned policy as the bound's minimizer \(_{n}^{}\) (as in (27)). With \(_{n}^{}\) obtained, we are interested in two quantities: The guaranteed risk by the bound, which is the value of the bound \(U(_{n}^{})\) at its minimizer. This quantity reflects the worst case performance of the learned policy, a lower value implies stronger performance guarantees. We are also interested in the true risk of the minimizer of the bound \(R(_{n}^{})\) as it translates the performance of the obtained policy acting on unseen data. As this learning paradigm is based on optimizing tractable, generalization bounds, we only compare our approach to methods that provide them. Precisely, we compare our LS-LIN learning strategy in (27) to strategies based on minimizing off-policy PAC Bayesian bounds from the literature: clipped IPS (cIPS) and Control Variate clipped IPS (cvcIPS) , Exponential Smoothing (ES)  and Implicit Exploration (IX) . The results are summarized in Table 2 where we compute:

\[rI(x)=(R(_{0})-x)/(R(_{0})-R(^{*}))=(R(_{0})-x)/(R(_{0})+1)\,,\]

the improvement over \(R(_{0})\) achieved by minimizing the different bounds in terms of \(x\{U,R\}\) (guaranteed risk and true risk respectively), relative to an ideal improvement. This metric helps us normalize the results, and we report its average over 200 different scenarios, with results in bold being significantly better. Fine grained results can be found in Appendix H.2.4. We observe that the LS-LIN PAC-Bayesian bound improves substantially on its competitors in terms of the guaranteed risk, and also obtains the best performing policies (on par with the IX PAC-Bayesian bound).

## 6 Conclusion

Motivated by the _pessimism_ principle, we have derived novel, empirical risk upper bounds tailored for the regularized IPS family of estimators. Minimizing these bounds within this family unveiled Logarithmic Smoothing, a simple estimator with good concentration properties. With its tight upper bound, LS confidently evaluates a policy, and shows provably better guarantees for both selecting and learning policies than all competitors. Our upper bounds remain broadly applicable, only requiring _negative costs_. While this condition does not impact importance weighting estimators, it does not hold for doubly robust estimators. Extending our approach to derive empirical bounds for this type of estimators presents a nontrivial, yet interesting task to explore in future work. Another potential extension would be to relax the i.i.d. assumption of the contextual bandit problem to address, the general offline Reinforcement Learning setting. This direction will introduce a more challenging estimation task and requires developing new concentration bounds.

    & cIPS & cvcIPS & ES & IX & LS–LIN (Ours) \\  \(rI(U(_{n}^{}))\) & 14.48\% & 21.28\% & 7.78\% & 24.74\% & **26.31\%** \\ \(rI(R(_{n}^{}))\) & 28.13\% & 33.64\% & 29.44\% & **36.70\%** & **36.76\%** \\   

Table 2: OPL: Relative Improvement of guaranteed risk and true risk averaged over 200 scenarios.

Figure 2: Results for OPE and OPS experiments.