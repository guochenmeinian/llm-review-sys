# PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models

Jiacheng Chen &Ruizhi Deng &Yasutaka Furukawa

Simon Fraser University

###### Abstract

This paper presents _PolyDiffuse_, a novel structured reconstruction algorithm that transforms visual sensor data into polygonal shapes with Diffusion Models (DM), an emerging machinery amid exploding generative AI, while formulating reconstruction as a generation process conditioned on sensor data. The task of structured reconstruction poses two fundamental challenges to DM: 1) A structured geometry is a "set" (e.g., a set of polygons for a floorplan geometry), where a sample of \(N\) elements has \(N!\) different but equivalent representations, making the denoising highly ambiguous; and 2) A "reconstruction" task has a single solution, where an initial noise needs to be chosen carefully, while any initial noise works for a generation task. Our technical contribution is the introduction of a Guided Set Diffusion Model where 1) the forward diffusion process learns _guidance networks_ to control noise injection so that one representation of a sample remains distinct from its other permutation variants, thus resolving denoising ambiguity; and 2) the reverse denoising process reconstructs polygonal shapes, initialized and directed by the guidance networks, as a conditional generation process subject to the sensor data. We have evaluated our approach for reconstructing two types of polygonal shapes: floorplan as a set of polygons and HD map for autonomous cars as a set of polylines. Through extensive experiments on standard benchmarks, we demonstrate that PolyDiffuse significantly advances the current state of the art and enables broader practical applications. The code and data are available on our project page: https://poly-diffuse.github.io.

## 1 Introduction

Reconstruction and generation were once separate research areas with minimal overlaps. While not acknowledged by the broader research communities, state-of-the-art reconstruction and generation techniques have now exhibited increasing similarities. Focusing on structured geometry (e.g., CAD models) in this paper, Auto-regressive Transformer (AR-Transformer) is a family of state-of-the-art generative models that iteratively applies a Transformer network to generate CAD construction sequences [18; 29; 35]. On the reconstruction side, a Transformer network iteratively reconstructs and refines indoor floorplans, outdoor buildings models, and vectorized traffic maps [7; 23; 47], a process resembling the generative AR-Transformer.

In 2023, Generative AI is experiencing significant growth, primarily driven by the emergence of Diffusion Models (DM) [14; 38; 40; 41], where structured geometry generation is not an exception. DM-based approach iteratively denoises coordinates (with associated properties) to generate realistic design layouts  or floorplans . A natural question is then "Is a Diffusion Model also good at structured reconstruction?" However, the answer is not that simple.

Take the high-definition (HD) map reconstruction problem for example , which aims to reconstruct a set of polylines given sensor data in a bird's-eye view (see Figure 1). A straightforward DM formulation would assume a maximum number of polylines (with a fixed number of corners per polyline [23; 25]), take a particular permutation of the elements to construct a fixed-length vector of corner coordinates, then iteratively denoise the vector subject to sensor data as a condition. The structured reconstruction task poses two fundamental challenges to such standard DM formulations: 1) Structured geometry is often a "set" of elements (e.g., a set of polylines), where the geometry of \(N\) elements has \(N!\) different but equivalent representations, making the denoising factorially ambiguous; 2) The denoising process needs to reach a single solution (_i.e._, one of \(N!\) representations) for a reconstruction task and an initial noise needs to be set carefully, while any initial noise would work for a generation task. Figure 2 shows a toy example of three elements with \(6=3!\) permutations, where six permutation-equivalent representations of the solution lie in six different sub-spaces. However, they all follow the standard Gaussian and become indistinguishable after the diffusion process, which obfuscates the denoising learning.

Our technical contribution is a Guided Set Diffusion Model (GS-DM). The forward diffusion process learns _guidance networks_ controlling noise injection so that one representation of a sample remains separated from its permutation variants, thus resolving denoising ambiguities. Specifically, the guidance networks set an individual Gaussian as the target distribution for each element (instead of a fixed zero-mean unit-variance Gaussian for everything), and are learned before the denoising training by minimizing the permutation ambiguity as a loss function. At test time, the reverse process initializes the element-wise Gaussian noise by the guidance networks and then denoises to reconstruct polygonal shapes conditioned on the sensor data, while the guidance networks take rough initial reconstructions either from an existing method or an annotator (see Figure 1 for examples).

We have conducted extensive experiments on two polygonal structure reconstruction tasks: floorplan reconstruction from a point-cloud density image with Structured3D dataset , and HD map construction from onboard camera inputs with nuScenes dataset . PolyDiffuse augments the state-of-the-art approach of the two specific tasks and brings consistent performance improvements, especially on the geometric regularity of the reconstruction results. Our generation process takes 5 to 10 steps to produce decent results and the visual encoder only runs once. We also demonstrate a likelihood-based refinement extension of our system by exploiting the probability flow ODE , which enables broader practical applications. To our knowledge, this paper is the first to demonstrate that DM is a powerful machinery for structured reconstruction tasks, in fact, the first in a broader domain of geometry reconstruction as well. We will share all our code and data.

Figure 1: **PolyDiffuse for floorplan and HD map reconstruction. Starting from an initial proposal (_e.g._, from a human annotator or an existing method), the sensor-conditioned denoising process of our Guided Set Diffusion Model (GS-DM) generates shape reconstructions in a few sampling steps, initialized and directed by the guidance networks. The initial proposal above mimics simple human inputs that indicate rough locations and specify the number of vertices for the polygonal shapes.**

Figure 2: (a) A toy visualization for a sample with three elements, thus 6 different but equivalent points in the data space; (b) Standard DM at \(t=T\); (c) Our GS-DM at \(t=T\).

## 2 Background: Denoising Diffusion Probabilistic Models

Diffusion models or score-based generative models [14; 38; 40; 41] progressively inject noise to data in the forward (diffusion) process and generate data from noise by the reverse (denoising) process. The section provides the key equations on the denoising diffusion probabilistic models (DDPM)  that lay the foundation of our method.

DDPM considers a Markovian forward process that turns data \(_{0} q(_{0})\) into Gaussian noise:

\[q(_{t}|_{t-1}):=(_{t};\ }_{t-1},\ _{t})\] (1)

\(t=1,,T\) and \(_{t}\) is the latent at \(t\). The noise schedule \(\{_{t}\}\) is constant [14; 28] or learned by reparameterization . The above forward process is written in closed form for an arbitrary \(t\):

\[q(_{t}|_{0}) =(_{t};_{t}}_{0},(1-_{t})),\] (2) \[_{0} =1,_{t}:=_{t-1}_{t}, _{t}:=1-_{t}.\] (3)

\(_{t}\) is sampled by \(_{t}=_{t}}_{0}+_{ t})}\) for \((,)\). DDPM parameterizes the reverse process with a noise prediction (or denoising) network \(_{}(_{t},t)\) to make connections with denoising score matching and Langevin dynamics [40; 45], and the sampling step of the reverse process is derived as:

\[_{t-1}=_{t}}}[_{t}-}{_{t}}}_{}(_{t}, t)]+_{t},\ (,).\] (4)

\(_{t}^{2}\) is set to \(_{t}\) or \(_{t-1}}{1-_{t}}_{t}\). The final training objective is a reweighted variational lower bound:

\[L_{}()_{_{0},t,}[||-_{}(_{t}} _{0}+_{t}},t)||^{2}]\] (5)

## 3 Guided Set Diffusion Models (GS-DM)

Consider the task of generating a set of elements \(=\{^{1},,^{N}\}\) under a condition \(\). A straightforward application of diffusion models would be to take one permutation of the elements to form a vector, then perform diffusion and learn denoising. With an abuse of notation, we let the vector \(_{0}\) (_i.e._, the sample or denoised result in DM's notation) represent one particular permutation of \(\). The challenge is that a set of \(N\) elements has \(N!\) different but equivalent representations, and any one of the \(N!\) is a valid \(_{0}\), making the denoising factorially ambiguous 1. To alleviate the above issue, our idea learns to guide the noise injection in a per-element manner by learning two "guidance networks", such that a sample \(_{0}\) remains separated from its permutation variants throughout the diffusion process. At test time, the guidance networks produce per-element Gaussians and stepwise guidance from an initial reconstruction, from which the denoising process generates the final reconstruction iteratively conditioned on sensor data. We first explain the forward and reverse processes and then the two-stage training for the guidance and denoising networks.

**Forward process**: With the standard diffusion process, different permutation variants of a sample gradually become indistinguishable through the diffusion process. To keep a particular representation \(_{0}\) separated from its permutation variants, we propose to inject noise per element. Let \(_{t}^{i}\) denote the diffused element \(_{0}^{i}\) at timestep \(t\), we learn to change the transition distribution \(q(_{t}^{i}|_{t-1}^{i})\) by adding \(_{}(_{0},t,i)\) to the mean and multiplying \(_{}(_{0},t,i)\) to the standard deviation:

\[q(_{t}^{i}|_{t-1}^{i},_{0}):=( _{t}^{i};\ }_{t-1}^{i}+_{}(_{0},t,i),\ _{t}_{}^{2}(_{0},t,i))\] (6)

\(_{}\) and \(_{}\) take \(_{0}\) and produce outputs for \(^{}\) element at \(t\). Following the similar derivations as in DDPM , we obtain (see Appendix A.1 for derivations):

\[q(_{t}^{i}|_{0}^{i}) =(_{t}^{i};\ _{t}}_{0}^{i}+_{}( _{0},t,i),\ _{}^{2}(_{0},t,i)),\] (7) \[}_{}(_{0},0,i) =0,}_{}(_{0},t,i):=}}_{}(_{0},t-1,i)+_{}(_{0},t,i),\] (8) \[}_{}^{2}(_{0},0,i) =0,}_{}^{2}(_{0},t,i):=_{ t}}_{}^{2}(_{0},t-1,i)+(1-_{t})_{ }^{2}(_{0},t,i).\] (9)As \(q(_{t}^{i}|_{0}^{i})\) only explicitly depends on \(}_{}\) and \(}_{}\), and \(_{t}^{i}(}_{}(_{0},T,i),}_{}^{2}(_{0},T,i))\), we consider \(}_{}\) and \(}_{}\) as the guidance networks. \(_{}\) and \(_{}\) are defined implicitly by Eq. 8 and Eq. 9. To simplify the formulation, we define \(_{}(_{0},t,i):=_{t}}C(_ {0},i)\) where \(C(_{0},i)\) is a function independent of the timestep \(t\), thus implicitly defining \(_{}(_{0},t,i)=}_{}(_{0},T,i)=C(_{0},i)\).

**Reverse process**: Note that \(_{0}\) is the ground truth and only available for training. At test time, we introduce a _proposal generator_ to produce an initial reconstruction \(}_{0}\), and run \(}_{}\) and \(}_{}\) with \(}_{0}\). \(}_{0}\) can either be results from an existing method or rough annotations from a human annotator. We first draw the element-wise initial noise with \(_{T}^{i}(}_{}(}_{0},T,i),}_{}^{2}(}_{0},T,i))\), and then run denoising steps iteratively. Similar to Eq.4, the sampling step of the reverse process is derived as

\[_{t-1}^{i}=}}[_{t}^{i}- {}_{}(}_{0},t,i)-_{}(}_{0},t,i)}{1-_{t}}_{}^{i}( _{t},t,)]+}_{}(}_{0},t-1,i)+_{t}^{i}\] (10)

The denoising network \(_{}\) takes the entire \(_{t}\) (_i.e._, all elements), and \(_{}^{i}\) is the output of the ith element. Please see Appendix A.2 for derivations.

**Learning the denoising network**: Following Eq. 5 of standard DDPM, the denoising objective is

\[L_{}()_{_{0},t,}[_{i=1}^{N}||^{i}-_{}^{i}( \{_{t}}_{0}^{i}+}_{}( _{0},i,t)+_{}(_{0},i,t)^{i} \},t,)||^{2}]\] (11)

The above formulation from Eq.6 to Eq.11 resembles that of DDPM in SS2, where the key difference is the introduction of guidance networks \(}_{}\) and \(}_{}\). We then describe the training of the guidance networks, which is the key step of GS-DM.

**Learning the guidance network**: Before training the denoising network with Eq. 11, we train \(}_{}\) and \(}_{}\) to ensure that \(_{t}\) in the diffusion process keep separated from other permutation variants of \(_{0}\). We quantify the permutation invariance between \(_{0}\) and \(_{t}\) with a triplet loss \(L_{}(,,)\) widely used in metric learning  (full implementation details are in Appendix B.3):

\[L_{}()_{_{0}^{}^ {}(_{0})\{_{0}\}}L_{}( _{t},_{0},_{0}^{})+_{_{t}^{ }^{}(_{t})\{_{t}\}} L_{}(_{0},_{t},_{t}^{})\] (12)

\(^{}(_{0})\) is the set of all permutation variants of \(_{0}\). Using the terminology in metric learning, the three inputs of \(L_{}(,,)\) are the anchor, positive, and negative, respectively. Our loss only considers the closest negative permutation, which is similar to the hard negative mining in some variants of the triplet loss . We also add two regularization terms on \(}_{}\) and \(}_{}\), so that the means of all elements are not too scattered and the variances do not vanish. The final loss for training the guidance networks is:

\[L_{}()_{t,_{0},} _{1}L_{}()+_{2}_{i=1}^{N}\|1/ }_{}(_{0},t,i)\|^{2}+_{3}_{i=1}^ {N}\|}_{}(_{0},t,i)\|^{2}\] (13)

Algorithm 1 and Algorithm 2 summarize the two-stage training paradigm of GS-DM.

```
1:\(\): frozen, \(\): trainable
2:repeat
3:\(_{0} q(_{0})\)
4:\(t(\{1,,T\})\)
5:\((,)\), then compute \(_{t}\)
6: Take gradient descent step on \(_{}L_{}\) (Eq.11)
7:until converged ```

**Algorithm 2** Denoising training (stage 2)

## 4 PolyDiffuse: Polygonal shape reconstruction via GS-DM

PolyDiffuse uses the GS-DM to solve polygonal shape reconstruction tasks (see Figure 3), in particular, floorplan and HD map reconstruction. This section explains specific details and specializations.

**Feature representation**: A sample \(\) is a set of elements \(^{i}\) as defined in SS3. Each element \(^{i}\) is either a (closed) polygon or a polyline, consisting of an arbitrary (for the floorplan task) or a fixed (for the HD map task) number of vertices as a sequence: \(^{i}=v_{1}^{i},,v_{N_{i}}^{i}\). Each vertex contains a 2D coordinate. For the ground-truth data, we determine the first vertex of each element by sorting based on the Y-axis coordinate and then the X-axis coordinate, similarly to PolyGen . The polygons are always in a counter-clockwise orientation.

**Guidance network**: As derived in SS3, the forward (Eq.7) and reverse (Eq.10) processes only depend on \(}_{}\) and \(}_{}\), and each element follows \(_{T}^{i}(}_{}(_{0},T,i), }_{}^{2}(_{0},T,i))\). Therefore, we choose to directly parameterize \(}_{}(_{0},T,i)=_{ }_{}}(_{0},i)\) and \(}_{}(_{0},T,i)=_{}_{}}(_{0},i)\) as two Transformer  networks, and define for all timesteps \(t=1,,T-1\) as follows:

\[}_{}(_{0},t,i):=(1-_{t}}) \,_{}_{}}(_{0},i),}_{}(_{0},t,i):=_{t}}\, _{}_{}}(_{0},i).\] (14)

Note that Eq.14 makes the noise adaptation directly dependent on \(_{t}\) for simplicity and drops \(t\) from the input. Algorithm 1 thus learns the above two Transformers. As for the proposal generator to produce \(}_{0}\) at test time, we either employ the state-of-the-art task-specific method (_i.e._, RoomFormer  for floorplan and MapTR  for HD map) or simulate rough human annotations. Please see SS5 for experiments with different proposal generators, and Appendix B.1 for implementation details of the guidance network.

**Denoising network**: The implementation of the denoising network \(_{}(_{t},t,)\) borrows the core neural architectures from the state-of-the-art task-specific models RoomFormer  and MapTR . While these two models are both adapted from DETR  and learn direct mappings from \(\) to \(\) with encoder-decoder Transformers, we need to make a few essential modifications to turn them into valid denoising networks (full implementation details are in Appendix B.2):

* Instead of the learnable embeddings or coordinates, \(_{t}\) should be the input nodes to the Transformer decoder. Each vertex is a node, and the node feature is the concatenation of four positional encodings : two for X and Y-axis coordinates and another two for vertex and instance index.
* We encode the timestep \(t\) (or noise level) with a positional encoding , and add this feature to each block of the Transformer decoder.

Our overall formulation with GS-DM is independent of the network architectures, and more advanced networks can be easily integrated into the framework in the future to boost performance.

**Sampling acceleration**: PolyDiffuse follows Eq.10 to reconstruct polygonal shapes. Song et al. shows that DDPM can be viewed as a discretization of a stochastic differential equation (SDE) with an associated probability flow ODE, and advanced ODE solvers  can significantly accelerate the sampling process. At test time, we employ a first-order solver (_e.g._, DDIM ) and use \(10\) sampling steps for both tasks (ablation study is in SS5.3). During the generation process, PolyDiffuse only runs the image encoder once as the visual features are shared across all steps.

**Likelihood evaluation**: With the probability flow ODE , PolyDiffuse can estimate its own reconstruction quality via likelihood evaluation, thus enabling potential extensions such as search-based or human-in-the-loop refinement as a wrapper around our system. This is an exciting property not possessed by previous reconstruction approaches. We show examples in Figure 4.

Figure 3: Illustration of the forward and reverse processes of PolyDiffuse with floorplan data.

Experiments

We have implemented the system with PyTorch and used a machine with 4 NVIDIA RTX A5000 GPUs. We have borrowed the official codebase of Karras et al. to implement the diffusion model framework. The guidance networks are implemented as set-to-set Transformer decoders, and the loss weights for the guidance training are \(_{1}=1,_{2}=0.05,_{3}=0.1\). The implementation of the denoising network refers to the competing state-of-the-art, RoomFormer  and MapTR  for floorplan and HD map reconstruction, respectively. Concretely, we employ the same ResNet  image encoder, DETR-style Transformer architectures (discussed in SS4), learning rate settings, and optimizer settings as RoomFormer or MapTR, while increasing the number of training iterations (\( 2.5\) for floorplan and \( 1.5\) for HD map), as the model converges slower under a denoising formulation than a detection/regression formulation. Note that we tried the same training schedule for the competing methods but their performance dropped due to overfitting (see SS5.3 for ablation study). For each task, we use the same trained model for different types of proposals (_i.e_., existing methods or rough annotations). Complete implementation details are in Appendix B, and supplementary experiments are in Appendix D

### Floorplan reconstruction

**Dataset and metrics**: Structured3D dataset  contains 3500 indoor scenes (3000/250/250 for training/validation/test) with diverse house floorplans. The average/maximum numbers of polygons and vertex per polygon across the dataset are 6.29/17 and 5.72/38, respectively. Point clouds are converted to 256\(\)256 top-view point-density images as inputs. We use the same evaluation metrics as previous works [7; 42; 47], which consists of three levels of metrics with increasing difficulty: room, corner, and angle. The precision, recall, and F1 score are reported at each metric level.

**Competing approaches**: We compare PolyDiffuse with five approaches from the literature: Floor-SP , MonteFloor , LETR , HEAT , and RoomFormer . The first two approaches design sophisticated optimization algorithms to solve for the optimal floorplan with learned metric functions, while the latter three use end-to-end Transformer-based neural networks, thus being much faster. For our proposal generator RoomFormer, we simplify the implementation by removing the Dice loss. As the dataset is very small, we add a random rotation data augmentation and train the modified RoomFormer for \(4\) the original training iterations, which improves the overall performance.

**Quantitative & qualitative evaluation**: Table 1 presents the quantitative evaluation results. We tried two proposal generators: 1) the improved RoomFormer and 2) rough annotations. The rough annotations are prepared by turning the ground-truth data into fixed-radius small circles centered

   Evaluation Level \(\) & & & & Room & & & & & & & \\  Method & Stages & Steps & FPS\({}^{}\) & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 \\  Floor-SP  & 2 & - & - & 89. & 88. & 88. & 81. & 73. & 76. & 80. & 72. & 75. \\ MonteFloor  & 2 & 500 & - & 95.6 & 94.4 & 95.0 & 88.5 & 77.2 & 82.5 & 86.3 & 75.4 & 80.5 \\ LETR  & 1 & 1 & - & 94.5 & 90.0 & 92.2 & 79.7 & 78.2 & 78.9 & 72.5 & 71.3 & 71.9 \\ HEAT  & 2 & 3 & - & 96.9 & 94.0 & 95.4 & 81.7 & 83.2 & 82.5 & 77.6 & 79.0 & 78.3 \\ RoomFormer  & 1 & 1 & - & 97.9 & 96.7 & 97.3 & 89.1 & 85.3 & 87.2 & 83.0 & 79.5 & 81.2 \\  RoomFormer\({}^{*}\) & 1 & 1 & 29.9 & 96.3 & 96.2 & 96.2 & 89.7 & 86.7 & 88.2 & 85.4 & 82.5 & 83.9 \\ +PolyDiffuse(Ours) & 2 & 2 & 11.7 & 96.9 & 96.4 & 96.6 & 90.3 & 87.1 & 88.7 & 85.8 & 82.8 & 84.3 \\ +PolyDiffuse(Ours) & 2 & 5 & 7.1 & 98.5 & 97.9 & 98.2 & 92.5 & 89.0 & 90.7 & 90.3 & 86.9 & 88.6 \\ +PolyDiffuse(Ours) & 2 & 10 & 4.4 & **98.7** & **98.1** & **98.4** & **92.8** & **89.3** & **91.0** & **90.8** & **87.4** & **89.1** \\  Rough annotations & 1 & - & - & 17.9 & 18.2 & 18.0 & 1.3 & 1.4 & 1.3 & 0.1 & 0.1 & 0.1 \\ +PolyDiffuse(Ours) & 2 & 10 & 19.2 & 97.4 & 98.2 & 97.8 & 91.7 & 92.2 & 91.9 & 89.2 & 89.7 & 89.4 \\   

Table 1: **Quantitative evaluation on Structured3D test set .** PolyDiffuse outperforms the state-of-the-art methods by clear margins, and works well with rough annotations. Results of previous works are copied from . \({}^{*}\) indicates our modified implementation. \({}^{}\): The running time is measured on a single Nvidia RTX A5000 GPU, and we only report those run by ourselves. When using RoomFormer to produce the initial proposals, the running time counts both the RoomFormer proposal generator and the GS-DM.

at the instance centroid (see Figure 4), which simulates what a human annotator can provide easily (_i.e_., indicating instance center and the number of vertices by looking at images). When using RoomFormer as the proposal generator, PolyDiffuse outperforms all competing methods by clear margins across all the evaluation metrics. The performance gap increases with the difficulty level of the metric, indicating that PolyDiffuse is superior in high-level geometry reasoning. With the rough annotations as the initial proposal, PolyDiffuse also produces reasonable reconstructions. The recalls are especially high because of the correct number of vertices. Note that our training is not dependent on any specific proposal generator, but PolyDiffuse works well with different generators at test time. Figure 4 presents concrete qualitative examples of how PolyDiffuse improves the initial reconstruction and enforces better geometry correctness. We also provide more qualitative examples in Appendix D.3.

**Running speed analysis**: Table 1 also presents the speed-performance tradeoff of PolyDiffuse on the floorplan reconstruction task. During the denoising process, the image encoding parts of the denoising network only run once, while the transformer decoder part runs for multiple rounds. In our computation environment, the time of image encoding vs. the time of transformer decoder is 2:1. Since speed is not a crucial factor for floorplan reconstruction, we can just use more denoising steps in real applications for better reconstruction quality.

**Likelihood-based refinement**: Figure 4(right) demonstrates how we leverage the likelihood evaluation to refine reconstruction results with minimal user interaction. We manually indicate the imperfect polygons (inside dashed boxes at the left), then run our system with different numbers of vertices for these polygons while evaluating the reconstruction likelihood. The result with the best negative log-likelihood (NLL) per dimension is the refined reconstruction.

### HD map reconstruction

**Dataset and metrics**: The nuScenes dataset  provides a standard benchmark for HD map reconstruction. The dataset contains 1000 ego-centric sequences collected by autonomous vehicles with rich annotations. The data is annotated at \(2\)Hz. Each sample has 6 RGB images and LiDAR sweeps captured by onboard sensors, covering the 360\({}^{}\) horizontal FOV. In the top-down Bird-eye-view (BEV) space, the perception range is \([-15m,15m]\) for X-axis and \([-30m,30m]\) for Y-axis. The map includes three categories of elements: pedestrian crossing, divider, and road boundary. For fair comparisons, we use the same dataset split and pre-processing setups as previous works [22; 23; 25], and also represent each polyline with 20 uniformly-interpolated vertices. Similar to MapTR , we only use the RGB images as the sensor inputs in our experiments.

We follow the common evaluation protocol from the literature [23; 25], which employs the average precision (AP) with the Chamfer distance as a matching criterion. The AP is averaged over three

Figure 4: **Qualitative results for floorplan reconstruction.**_Left:_ PolyDiffuse improves the geometry of RoomFormer and turns rough annotations into decent reconstructions. _Right:_ PolyDiffuse enables search-based self-refinement by enumerating different vertex numbers of polygons and evaluating its own reconstruction likelihood (_i.e_., NLL per dimension).

distance thresholds: \(\{0.5m,1.0m,1.5m\}\), and the mean average precision (mAP) is obtained by averaging across the three classes of map elements. However, the Chamfer distance does not consider the vertex order and vertex matching between prediction and G.T., thus failing to measure the structured correctness. As directional information is critical for autonomous vehicles, we propose to augment the matching criterion with an extra order-aware angle distance. Concretely, we find the optimal vertex matching between a prediction and a G.T. with the smallest coordinate distance, then trace along the two polylines/polygons to compute the average angle distance (in degrees). A prediction is considered a true positive only when satisfying both the Chamfer and angle distance thresholds. We augment the three-level thresholds to \(\{(0.5m,5^{}),(1.0m,10^{}),(1.5m,15^{})\}\). Results are reported in both the original and augmented metrics. We provide complete implementation details and visualizations about the augmented metric in the Appendix C.

**Competing approaches**: We compare our method with VectorMapNet  and MapTR . VectorMapNet  employs an auto-regressive transformer decoder to generate the vertex of polygon/polyline one by one, conditioned on the image inputs. MapTR  is our proposal generator and is a DETR-style hierarchical detection method that directly estimates the polygon/polyline as a set of vertex sequences. MapTR and RoomFormer are technically very similar but focus on different tasks.

**Quantitative & qualitative evaluation**: Table 2 presents the quantitative evaluation results. PolyDiffuse does not significantly improve the Chamfer-distance mAP of MapTR, as it does not discover new instances. However, it shows clear advantages in the order-aware angle distance, a metric that is indicative of high-level structural regularities. Figure 5 qualitatively shows that PolyDiffuse accurately reconstructs curves with greater resemblance to the ground truth. We also experimented with the same rough annotations as in the floorplan task, and the quantitative results show that the mAP becomes lower compared to using MapTR as the proposal generator. By analyzing the results with rough annotations more carefully, we found the AP with the lowest threshold becomes much worse, while the one with the highest threshold improves. We believe this is because the guidance networks are trained with "G.T. proposals", where the rough annotations (_i.e._, fixed radius circles) have a very different curve style when serving as the proposals, thus producing inaccurate initial Gaussians and stepwise guidance. This is a potential limitation of the current method. We provide additional qualitative examples in Appendix D.3.

**Running speed analysis**: Table 2 also presents our speed-performance tradeoff on the HD map reconstruction task. In our computation environment, the time of image encoding vs. the time of transformer decoder is 4:1 when using MapTR's model architecture. As FPS is an important consideration for online applications, we need to pick the number of denoising steps carefully. Furthermore, since PolyDiffuse is not restricted to a specific task-specific method for the model

    &  &  \\  Method & Stages & Steps & FPS\({}^{}\) & AP\({}_{p}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP & AP\({}_{p}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP \\  VectorMapNet  & 1 & 20 & - & 36.1 & 47.3 & 39.3 & 40.9 & - & - & - & - \\ VectorMapNet  & 2 & 20 & - & 42.5 & 51.4 & 44.1 & 46.0 & - & - & - & - \\ VectorMapNet\({}^{+}\) & 1 & 20 & 3.9 & 40.0 & 47.6 & 39.0 & 42.2 & 33.0 & 44.5 & 27.3 & 34.9 \\ MapTR  & 1 & 1 & - & 56.2 & 59.8 & 60.1 & 58.7 & - & - & - & - \\  MapTR\({}^{+}\) & 1 & 1 & 14.3 & 55.8 & **60.9** & 61.1 & 59.3 & 46.1 & 43.4 & 41.9 & 43.8 \\ +PolyDiffuse(Ours) & 2 & 2 & 6.3 & 56.8 & 59.8 & 60.9 & 59.2 & 50.3 & 48.2 & 44.3 & 47.6 \\ +PolyDiffuse(Ours) & 2 & 5 & 4.8 & 58.1 & 59.7 & 61.2 & 59.6 & 51.8 & 49.5 & 45.4 & 48.9 \\ +PolyDiffuse(Ours) & 2 & 10 & 3.4 & **58.2** & 59.7 & **61.3** & **59.7** & **52.0** & **49.5** & **45.4** & **49.0** \\  Rough annotations & - & - & - & 18.5 & 2.2 & 0.7 & 7.1 & 8.7 & 0.0 & 0.0 & 2.9 \\ +PolyDiffuse(Ours) & 2 & 10 & 11.3 & 55.3 & 60.4 & 55.3 & 57.0 & 48.3 & 49.5 & 38.1 & 45.3 \\   

Table 2: **Quantitative evaluation results of HD map construction on nuScenes. All methods in the table use RGB inputs only and employ ResNet50 as the image backbone. \({}^{+}\): Results are obtained by running the official code with the released model checkpoint. \({}^{}\): The running time here is measured on a single Nvidia RTX A5000 GPU, and we only report for the methods run by ourselves. When using MapTR to produce the proposals, the running time counts both the MapTR proposal generator and the GS-DM.**

[MISSING_PAGE_FAIL:9]

method, PolyDiffuse, outperforms the current state of the arts on floorplan  and HD map  reconstruction while enabling broader practical use cases.

**Diffusion-based generative models**: Diffusion Models (DM) or score-based generative models [28; 38; 40; 41] have made tremendous progress in the last few years and demonstrated promising performance in content generations [9; 31; 32; 33; 43] and likelihood estimations [19; 20; 28]. In this paper, we explore the potential of DM in the context of structured reconstruction and propose a Guided Set Diffusion Model (GS-DM) by extending the DDPM  formulation. GS-DM reconstructs complex polygonal shapes with a guided denoising (reverse) process subject to sensor data and can evaluate its reconstruction likelihood by leveraging the underlying probability flow ODE . Our high-level formulation is relevant to PriorGrad , a diffusion model for acoustic data, but has essential differences - GS-DM learns the guidance networks to control the diffusion and direct the denoising in a per-element and stepwise manner, while PriorGrad pre-computes the mean and variance of the target Gaussian from the condition and directly shifts the diffusion/denoising trajectory.

## 7 Conclusion

This paper introduces PolyDiffuse, a system designed to reconstruct high-quality polygonal shapes from sensor data via a conditional generation procedure. At the heart of PolyDiffuse is a novel Guided Set Diffusion Model, that controls the noise injection in the diffusion process to avoid permutation ambiguity for denoising. PolyDiffuse achieves state-of-the-art performance on two challenging tasks: floorplan reconstruction from a point density image and HD map construction from onboard RGB images. To our knowledge, this paper is the first to demonstrate that Diffusion Models, generally regarded as a generation technique, are also powerful for reconstruction, potentially encouraging the community to further investigate the effectiveness of Diffusion Models in broader domains.

**Limitations**: PolyDiffuse suffers from two major limitations. First, it does not recover geometry instances (e.g., rooms for floorplan) missing in the initial reconstruction, while the likelihood-based refinement provides a possible solution by incorporating search or external inputs. Second, as shown in the HD mapping results with rough annotations, when the style of initial reconstructions (_e.g._, fixed radius small circles) is different from the curve style of ground truth, the guidance networks could produce bad initial Gaussians and guidance, leading to inaccurate locations of the final results. Training specific guidance and denoising networks for each type of initial reconstruction is a solution but induces more computation.

**Broader impact**: The paper benefits applications in architecture, construction, autonomous driving, and potentially human-in-the-loop annotations systems to reduce human workload. However, potential malicious or unintended applications might include military scenarios, for example, reconstructing digital models of important buildings or structures for targeted military operations.