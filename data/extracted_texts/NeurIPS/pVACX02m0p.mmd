# Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning

Anonymous authors

Paper under double-blind review

###### Abstract

In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (_e.g._, copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks.

## 1 Introduction

The rapid advancement of large language models (LLMs) has raised security and safety concerns, including issues related to copyright violations and sociotechnical harms (Huang et al., 2024; Wang et al., 2023; Li et al., 2024; Shi et al., 2024). However, retraining these models to remove undesirable data influences is often impractical due to the substantial costs and time required for such processes. This gives rise to the problem of **LLM unlearning**, which aims to effectively remove undesired data influences and/or model behaviors while preserving the utility for essential, unrelated knowledge generation, and maintaining efficiency without the need for retraining (Eldan and Russinovich, 2023; Yao et al., 2023; Liu et al., 2024; Blanco-Justicia et al., 2024).

To trace its origins, the concept of _machine unlearning_ was initially developed for data removal to comply with privacy regulations such as the "right to be forgotten" (Rosen, 2011; Hoofnagle et al., 2019), with early studies focusing on vision models (Cao and Yang, 2015; Warnecke et al., 2021; Bourtoule et al., 2021; Thudi et al., 2022; Kurmanji et al., 2024; Jia et al., 2023; Gandikota et al., 2023; Fan et al., 2024). However, it is soon adapted to LLMs to remove unwanted data, knowledge, or specific model capabilities (Eldan and Russinovich, 2023; Yao et al., 2023; Liu et al., 2024; Ji et al., 2024; Li et al., 2024; Shi et al., 2024; Maini et al., 2024; Zhang et al., 2024; Jia et al., 2024). Compared to vision model unlearning, designing effective and efficient unlearning methods for

Figure 1: (a) _Systematic overview_ of an LLM (\(\)) post-unlearning using the proposed SimNPO optimization principle, compared to the popular NPO (negative preference optimization) framework (Zhang et al., 2024) and the reference model (_i.e._, model prior to unlearning). (b) & (c) _Experiment highlights_ on the TOFU dataset with a 5% forget size (Maini et al., 2024) and on the MUSE News dataset (Shi et al., 2024). Unlearning effectiveness is measured by forget quality for TOFU and PriVLeaf for MUSE, while utility preservation is evaluated using model utility for TOFU and KnowMem on \(_{l}\) for MUSE (see Table 1 for details on task-specific metrics). In both tasks, Retrain serves as the gold standard for unlearning by fully removing the influence of the forget data.

[MISSING_PAGE_FAIL:2]

current efforts, NPO (negative preference optimization) (Zhang et al., 2024) stands out as a promising approach by framing the unlearning problem as a variant of direct preference optimization (Rafailov et al., 2024). It has demonstrated competitive performance in benchmarks like TOFU and MUSE. Thus, our work aims to conduct an in-depth exploration of NPO, identifying its current limitations, and proposing potential improvements.

**Preference optimization.** In this work, we advance LLM unlearning through the lens of preference optimization. This is motivated by aligning LLMs with human values, known as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). However, online preference optimization algorithms are often complex and challenging to optimize (Santaccoe et al., 2023; Zheng et al., 2023), driving interest in more efficient offline alternatives. Direct preference optimization (**DPO**) (Rafailov et al., 2024) introduced an offline approach that eliminates the need for a reward model, sparking the development of several reward-free offline preference objectives (Zhao et al., 2023; Azar et al., 2024; Hong et al., 2024; Ethayarajh et al., 2024; Meng et al., 2024; Yuan et al., 2024). Notable methods include RRHF (Yuan et al., 2024), SLic-HF (Zhao et al., 2023), IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), and SimPO (Meng et al., 2024). Among these methods, SimPO is a reference-free, length-normalized variant of DPO, and we will demonstrate that it is well-suited for integrating into LLM unlearning and improving NPO.

## 3 A Primer on LLM Unlearning

**Problem formulation of LLM unlearning.** Unlearning tasks can take various forms and are typically associated with a specific set of data points to be removed, known as the _forget set_ (\(_{}\)). In addition, these tasks often require a complementary set of non-forgotten data points, known as the _retain set_ (\(_{}\)), to preserve model utility by penalizing the divergence caused by unlearning. As a result, the problem of LLM unlearning can be cast as a regularized optimization problem that balances the forget and retain objectives (Liu et al., 2024; Yao et al., 2023; Zhang et al., 2024):

\[}{} _{(x,y)_{}}[_{}(y|x;)]}{}+_{(x,y)_{}}[ _{}(y|x;)]}_{},\] (1)

where \(\) represents the model parameters to be updated during unlearning, \( 0\) is a regularization parameter to penalize the 'divergence' of unlearning, and \(_{}\) and \(_{}\) represent forget and retain losses incurred when using model parameters \(\) to generate the desired response (\(y\)) given the input \(x\).

Substantial research has focused on designing and analyzing appropriate forget and retain loss functions to solve problem (1) (Liu et al., 2024; Yao et al., 2023; Zhang et al., 2024; Maini et al., 2024; Shi et al., 2024; Eldan and Russinovich, 2023; Jia et al., 2024). For instance, let \(_{}(y|x)\) represent the prediction probability of the model \(\) given the input-response pair \((x,y)\). The retain loss is typically chosen as the cross-entropy-based sequence prediction loss, \(_{}(y|x,)=-_{}(y|x)\), whose minimization encourages the model to perform well on the retain data \((x,y)_{}\). If we specify the forget loss as the _negative_ token prediction loss \(_{}(y|x,)=_{}(y|x)\), whose minimization then _discourages_ the model from learning the forget data \((x,y)_{}\). Minimizing such a forget loss is known as the _gradient ascent_ (**GA**) method (Maini et al., 2024; Thudi et al., 2022). Similarly, minimizing the regularized loss that integrates GA with the retain loss is known as the _gradient difference_ (**GradDiff**) method (Liu et al., 2022; Maini et al., 2024; Yao et al., 2023).

**Negative preference optimization (NPO).** A popular optimization framework for solving problem (1) is NPO (Zhang et al., 2024). It treats the forget data as negative examples in DPO (Rafailov et al., 2024), transforming the unbounded GA-based forget loss into a _bounded loss from below_, which helps prevent catastrophic collapse, and an _adaptive weight smoothing_ applied to the forget loss gradients, allowing for more controlled and stable unlearning. These benefits can be clearly seen from the NPO loss and its gradient as follows:

\[_{}()=_{(x,y)_{}} (-(}(y|x)}{_{}(y|x)}))}_{:= _{}(y|x),\,),\,},\] (2)

\[_{}_{}()=_{(x,y) _{}}[}(y |x)^{}}{_{}(y|x)^{}+_{}(y|x)^{}} )}_{:=w_{}(x,y),\,} _{}_{}(y|x)],\] (3)

[MISSING_PAGE_FAIL:4]

may be biased toward generating longer but lower-quality sequences (Meng et al., 2024). In such cases, an effective unlearning method should allocate less optimization effort to long-sequence forget data, while focusing more on shorter-length data that are more challenging to unlearn. See Fig. 1-(a) for an illustration. To validate this, **Fig. 2** presents the distributions of truth ratios of forget samples with different response lengths, comparing NPO with Retrain, based on the TOFU setup outlined in Table 1, using a forget set size of 5% (known as the Forget05 unlearning scenario in TOFU). Recall that a truth ratio distribution closer to that of Retrain indicates higher forget quality (FQ), with FQ\(=1\) representing optimal unlearning (_i.e._, Retrain). As shown, NPO exhibits a greater distance from Retrain when unlearning the top 50% shortest-length forget data, resulting in a lower FQ of \(0.58\). In contrast, NPO performs better unlearning for the longer 50% of the forget set, yielding a higher FQ of \(0.81\). Therefore, NPO could be ineffective at unlearning short responses. Additional analyses on the limitation (L1) will be provided in Sec. 5.

**(L2) NPO may cause ineffective gradient weight smoothing and over-unlearning.** Another issue introduced by the reference model \(_{}\) concerns the effectiveness of NPO's gradient weight smoothing, _i.e._, \(w_{}(x,y)=(2_{}(y|x^{})^{})/(_{}(y|x)^{}+_{}(y|x)^{})\) in (3). During the early optimization stage of NPO, we find \(w_{}(x,y) 1\) regardless of the varying data-specific unlearning difficulties since the initialization of the unlearned model \(\) is given by the reference model. **Fig. 3-(a,b)** support this finding by displaying the gradient smoothing weights of NPO at epoch one (Fig. 3a) and their trajectory over the course of unlearning epochs (Fig. 3b). As shown, the gradient smoothing weights of NPO show large variance, but most values are concentrated around \(w_{}(x,y) 1\) at epoch one. This suggests that NPO behaves similarly to GA in the early stage of unlearning, potentially causing over-unlearning and a large utility drop even if the weight decreases in later optimization. **Fig. 3-(c,d)** justify the above by presenting the forget quality and model utility of NPO on TOFU against unlearning epochs. As shown, NPO tends to cause a larger utility drop at early epochs compared to _SimNPO_, the improved alternative to NPO that we will introduce in Sec. 5. Additionally, NPO remains less effective in forgetting than SimNPO throughout the process.

## 5 SimNPO: Advancing NPO by Simple Preference Optimization

In the following, we address the reference model bias in NPO by using a reference-free optimization method, **SimPO** (simple preference optimization) (Meng et al., 2024). We refer to the NPO alternative derived from SimPO as **SimNPO**, simple negative preference optimization.

**Motivation of SimNPO and its forget objective.** The simplest solution to mitigating NPO's reference model bias is to directly remove \(_{}\) from the gradient in (3), setting \(_{}=0\). However, this variant would be _ineffective_, as the reference-free gradient reduces to GA, with \(w_{}(x,y)=1\). This negates NPO's advantages.

To develop a better solution for improving NPO, we address the reference model issue by revisiting the context of preference optimization and investigating whether the reference model can be excluded while still retaining the unlearning benefits provided by NPO. Our idea parallels how NPO was originally inspired by DPO (Rafailov et al., 2024). We adopt SimPO, a reference-free alternative to DPO,

Figure 3: Experimental evidence of ineffective weight smoothing and over-unlearning for NPO on TOFU with 5% forget set size: (a) NPO’s gradient weights (\(w_{}\)) at epoch 1 vs. response length \(|y|\). (b) Trajectory of \(w_{}\) for NPO over unlearning epochs, visualized using box plots to represent the distribution of gradient weights across forget samples for each epoch. (c)-(d) Forget quality and model utility of NPO across epochs.

Figure 2: Truth ratio distribution of top 50% shortest-length forget data points and the other 50% longer-length data for Retrain and NPO on TOFU with forget size 5%.

as the optimization framework for unlearning, leading to the SimNPO method. The _key difference_ between SimPO and DPO lies in their reward formulation for preference optimization. In DPO, the reward formulation is given by the comparison with the reference model, _i.e._, \((_{}(y|x)/_{}(y|x))\). This formulation was used by NPO. In contrast, SimPO takes a _reference-free but length-normalized_ reward formulation: \((/|y|)_{}(y|x)\), where \(|y|\) denotes the response length.

Taking the inspiration of SimPO, we can mitigate the reference model bias in NPO by replacing its reward formulation \((_{}(y|x)/_{}(y|x))\) in (2) with the SimPO-based reward formulation \((/|y|)(_{}(y|x))\). This modification transforms (2) into the **SimNPO loss**:

\[_{}()=_{(x,y)_{t}}[ -(-_{}(y|x)- )],\] (4)

where \( 0\) is the reward margin parameter, inherited from SimPO, which defines the margin of preference for a desired response over a dispreferred one. However, unless otherwise specified, we set \(=0\) to align with the NPO loss (2). This is also desired because \(\) introduces a constant shift to the prediction loss \(-(/|y|)_{}(y|x)\). Consequently, a larger \(\) requires greater compensation to further suppress token prediction, enforcing a stricter unlearning condition. This can accelerate the utility drop during unlearning. See Fig. 13 for an empirical justification. The SimNPO loss (4), when integrated with the regularized optimization in (1), forms the SimNPO method.

**Insights into SimNPO.** Similar to NPO, the SimNPO loss (4) is bounded from below, with a minimum value of \(0\). Approaching this minimum drives the unlearning. However, the _key distinction_ of SimNPO from NPO is its forget data-aware, length-normalized reward formulation, \((/|y|)_{}(y|x)\) in (4). This eliminates the reference model bias and results in an improved gradient smoothing scheme. Specifically, the gradient of the SimNPO loss (with \(=0\)) yields (as derived in Appendix A):

\[_{}_{}()=_{(x,y) _{t}}[}(y|x))^{/|y|} }{1+(_{}(y|x))^{/|y|}}}_{:=_{}^{*}(x,y)}_{}_{}(y|x)].\] (5)

Similar to NPO in (3), the gradient in (5) can be divided into two components: weight smoothing (\(w_{}^{}\)) and GA. However, in SimNPO, the weight smoothing is _no longer influenced by the reference model and is instead normalized by the length_\(|y|\). This introduces two key advantages (a)-(b) below, in response to NPO's limitations (L1)-(L2).

(a) SimNPO addresses the biased allocation of unlearning power by using the (data-specific) response length as a guide. For example, when \(|y|\) is large, less optimization power is allocated as long-sequence forget data could be closer to the unlearning boundary and require less intervention (Fig. 2). In the extreme case where \( 0\), the SimNPO gradient reduces to a _weighted GA_: \(_{}_{}()_{(x,y) _{t}}[1/|y|_{}_{}(y|x)]\). This is different from NPO, which becomes GA as \( 0\). **Fig. 4** empirically demonstrates the advantage of length normalization in SimNPO on TOFU, comparing the forget quality and model utility of SimNPO with other baselines and Retrain. As shown, SimNPO outperforms NPO in both forget quality and model utility, coming closest to Retrain. Even in the special case where \(=0\) (_i.e._, Weighted-GradDiff), the length normalization provides benefits over the vanilla GradDiff baseline.

(b) In addition, the reference-free, length-normalized weight smoothing prevents early-stage ineffectiveness during unlearning. It can be easily shown from (5) that \(w_{}^{}(x,y)<2/|y|\), with the distribution of weights \(w_{}^{}(x,y)\) depending on the specific forget data samples. This contrasts with NPO, where the weight distribution concentrated around \(w_{}(x,y) 1\) during the early unlearning stage, as shown in Fig. 3-(a). Furthermore, **Fig. 5** provides a detailed comparison between the gradient weights of SimNPO and NPO. As shown, SimNPO exhibits a much stronger correlation with the response length \(|y|\) during the first two unlearning epochs, prioritizing short-length forget data that are initially harder to forget. At later epochs, the gradient weights become more uniform, reflecting that SimNPO can then treat different forget data with even optimization power. This trend is different

Figure 4: Forget quality vs. model utility on TOFU with forget set size of 5%. Weighted-GradDiff (W-GradDiff) is the variant of SimNPO at \(=0\).

from NPO, which assigns more uniform gradient weights early on and only accounts for data-specific difficulty when \(w_{}(x,y)\) decreases in the later stages of unlearning.

**Further analyses via a mixture of Markov chains.** In addition to the above insights, we further validate SimNPO's advantages to overcome NPO's limitations (L1)-(L2) (Sec. 4) using a synthetic setup. For ease of controlling the unlearning difficulties of different forget data points, we consider the problem of unlearning on a mixture of Markov chains with a state space of size 10 (\(s=1,,10\)). The _retain distribution_ consists of Markov chains that transition uniformly among states \(\{1,2,3\}\). The _forget distribution_ is a mixture of two components: _Forget1_, where the chains transition uniformly among \(\{4,5,6\}\), and _Forget2_, where they move uniformly among \(\{7,8,9\}\). A small leakage probability allows the chains to transition outside their designated states occasionally, including state \(10\), which is not a designated state for any of the chains. We generate 10,000 samples for the retain distribution and 5,000 samples each for Forget1 and Forget2. A GPT-2 model is pretrained on these samples and serves as the initial model. We apply NPO and SimNPO to unlearn the forget distributions. Forget and retain performance is evaluated using the KL-divergence between predicted and true transition probabilities of the Markov chains. See Appendix B for details. We present our results in **Fig. 6** and summarize the insights below.

_In response to (L1), SimNPO is easier to unlearn short responses than NPO._ To validate this, we set the retain distribution and Forget1 with a sequence length of 20, while Forget2 is assigned a shorter sequence length of 5, representing a mix of long and short responses. **Fig. 6 (left)** shows that NPO exhibits a worse tradeoff between retain distance and forget quality on short responses (_i.e._, Forget2) compared with SimNPO. That is, to achieve the same forget quality on Forget2 as the retrained model (with forget quality \(0.44\)), NPO incurs a higher retain distance than SimNPO. As a result, NPO has an overall larger retain distance when unlearning the entire Forget distribution. In contrast, SimNPO shows more consistent performance across Forget1 and Forget2, with less variance in its tradeoff.

_In response to (L2), SimNPO unlearns already unlearned data less aggressively than NPO._ In the second case, we set the retain distribution, Forget1 and Forget2 all with a sequence length of 20. However, we exclude Forget2 during pretraining. This setup simulates a scenario where the initial model (_i.e._, the reference model in NPO) has already unlearned part of the forget dataset (_i.e._, Forget2).

**Fig. 6 (right)** shows that NPO unlearns Forget2 faster than SimNPO, even though Forget2 was already unlearned. However, NPO performs worse on Forget1 than SimNPO, likely due to overlearning Forget2, thereby reducing the overall model utility.

Figure 5: Gradient weight smoothing of NPO (\(w_{}\)) and SimNPO (\(w^{}_{}\)) vs. forget data response length \(|y|\) across different epochs (1, 2, 3, and 10) on TOFU with forget set size of 5%. Each point represents a sample. The Pearson correlation in the upper right corner indicates the relationship between gradient weight smoothing and response length. The SimNPO’s weights \(w^{}_{}\) have been rescaled (by \( 10\)) for ease of visualization.

Figure 6: Tradeoffs between forget quality (higher \(\) is better) and retain distance (lower \(\) is better) along the unlearning path of NPO and SimNPO in the synthetic experiments. Left: Forget1 and Forget2 have different sequence lengths. Right: unlearning from an initial model that has not seen Forget2. The symbols \((,)\) near the \(y\)-axis of both figures indicate the performance of the retrained model on Forget1 and Forget2, respectively.

## 6 Other Experiments

In what follows, we present more experiment results to demonstrate the effectiveness of SimNPO. See detailed experiment setups and hyperparameter selections in Appendix C.1-C.2.

**Performance on TOFU.** In **Table 2**, we present the unlearning performance of SimNPO and its various baselines on TOFU, covering both effectiveness metrics and utility metrics as shown in Table 1. Recall that 'Original' refers to the model performance prior to unlearning, serving as the _lower bound_ for unlearning effectiveness. In contrast, 'Retrain' refers to the model retrained excluding the forget set influence, serving as the _upper bound_ for unlearning effectiveness. 'FQ' stands for forget quality, and 'MU' represents model utility. These two metrics serve as the primary performance indicators for LLM unlearning on TOFU. SimNPO outperforms NPO in both FQ and MU, and is the closest approximate unlearning method to Retrain. Except for NPO, the other unlearning baselines (GA, GradDiff, and IDK) are not effective, as implied by their FQ values being smaller than \(0.01\), where FQ indicates the \(p\)-value for rejecting the indistinguishability between the unlearned model and Retrain on TOFU. In **Table 4** of **Appendix** D, we also provide examples of model responses after unlearning using SimNPO, Retrain, and NPO, along with label to degenerate. We observe that, in some cases (_e.g._, responses against Q1 and Q2 in Table A4), the NPO-unlearned model generates _repeated texts_ in response. While this repetition does not reveal the information intended for unlearning, it negatively impacts model utility and differs noticeably from Retrain's behavior. In contrast, SimNPO produces unlearning responses more closely aligned with those generated by Retrain. We conduct a follow-up study of Fig. 2 to delve deeper into the comparison between SimNPO and NPO across forget data with varying response lengths. **Fig. A4 in Appendix C.3** shows that SimNPO's improvement over NPO is most evident in forgetting short-length data, aligning with the NPO's limitation (L1) as illustrated in Sec. 4. We also find that SimNPO is more efficient than NPO in Appendix C.3..

**Additional experiments for SimNPO.** We further evaluated SimNPO on the MUSE and WMDP datasets and assessed its robustness using the relearning attacks, as described in Appendix C.3.

## 7 Conclusion

We revisited the current unlearning optimization framework, negative preference optimization (NPO), and identified its reference model bias issue, which compromises unlearning effectiveness, particularly for forget data of varying difficulty. To address this, we introduced SimNPO, a simple yet effective framework that eliminates reliance on a reference model by leveraging simple preference optimization. We provided deep insights into SimNPO's advantages through both synthetic data analysis and evaluations on existing unlearning benchmarks such as TOFU, MUSE, WMDP, and relearning attacks. In future work, we will further investigate the limitations of SimNPO and enhance it for tasks involving model capability removal. See further discussions in Appendix E-F.