# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models

ShengYun Peng\({}^{1}\) Pin-Yu Chen\({}^{2}\) Matthew Hull\({}^{1}\) Duen Horng Chau\({}^{1}\)

\({}^{1}\)Georgia Tech \({}^{2}\)IBM

{speng65,matthewhull,polo}@gatech.edu

pin-yu.chen@ibm.com

###### Abstract

Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.

## 1 Introduction

Safety alignment is the foundation to bring LLMs' behaviors in line with human preferences and restrict harmful behaviors at inference time . Though aligned LLMs have adopted one or a combination of the safety alignment methods, _e.g_., reinforcement learning from human feedback (RLHF) , instruction tuning , direct preference optimization (DPO) , and rejection sampling , LLM safety can easily be compromised by finetuning with only a few adversarially designed training examples. For instance, both GPT-3.5 Turbo and LLaMA-2  fail to refuse users' harmful queries after only finetuning with 10-shot harmful examples . This brings practical safety concern to model deployment as customization is the desirable way for specific use case. In this paper, we explore the following fundamental problems in LLM safety: **Are all open-source LLMs equally vulnerable to finetuning? Why can simple finetuning easily break LLM's safety alignment? How fast does the model start to break during finetuning?**

We discovered that all these questions can be addressed by navigating the LLM safety landscape. In deep learning literature, visualization of the model landscape has significantly improved our comprehension of generalization errors, optimization trajectories, and model ensembles .

In this paper, we introduce the notion of LLM safety landscape and quantify the risk in finetuning LLM by exploring different directions of perturbing model weights. When provided with a single model, we sample a random normalized direction to visualize its local variations. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. The shape of the landscape dictates the fine-tuning attributes: a sharp change in the safety metric indicates that the aligned model is a local minimum, making it challenging to find a point that is both safe and useful, whereas a flat local landscape offers more opportunities to discover a model that better balances safety and usefulness. Our landscape navigation provides a suite of four new insights that facilitate the understanding the LLM safety (Fig. 1):

1. **We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop.** The safety basin is evident in both 1D and 2D safety landscape of LLaMA2, LLaMA3, Vicuna, and Mistral across various random directions and different safety benchmarks. This safety landscape contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new Visage safety metric, the acronym for volumetric index for safety alignment guided by explanation, which measures the safety of an LLM's local region in model parameter spaces. (Sec. 3)
2. **Visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin.** We discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic Visage safety metric measures the risks in finetuning without assumptions on the finetuning dataset, where a higher Visage score means the model after finetuning is safer. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. (Sec. 4)
3. **LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin.** We evaluate the impact of system design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each

Figure 1: **A. “Safety basin”, a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. **B.** Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.

LLM's default system prompt as the baseline. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin. (Sec. 5)
4. **When evaluating the safety landscape using jailbreaking queries, we find that these queries are highly sensitive to perturbations in model weights.** We have collected the adversarial prompts targeting LLaMA2 and Vicuna, generated by jailbreak attacks from the literature [51; 8; 9]. Our safety landscape analysis shows that although the aligned model is vulnerable to jailbreak attacks, slighlty perturbing the model weights in the local space of the aligned model can significantly lower the attack success rate (ASR) of these jailbreaking attacks. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses. (Sec. 6)

## 2 Background and Related Works

LLM safety alignment.LLMs are language models with a large number of parameters trained on web-scale text corpora [5; 1; 43; 10; 27]. LLMs have exhibited emergent capabilities that can be broadly applied in a task-agnostic manner, such as in-context learning , chain-of-thought reasoning , and mathematical reasoning . These capabilities are largely attributed to aligning LLMs with expected human values and intentions, which involves training the model to follow instructions and being helpful, truthful, and harmless [35; 24; 40]. Specifically, harmless is achieved by safety alignment that empowers the LLM with safety guardrails so that the model can refuse harmful instructions. Common safety alignment techniques are instruction tuning , RLHF , DPO , rejection sampling , and self-alignment . However, these techniques are not designed to cover the safety risks that may arise from the subsequent custom finetuning and jailbreak attacks. Recent work has shown that both simple finetuning  and jailbreak attacks [8; 51] can circumvent safety guardrails of aligned LLMs.

LLM harmful finetuning attacks and defenses.Finetuning is widely employed to customize open-source LLMs for downstream applications [18; 11]. Typically, finetuning directly updates the parameters of pretrained models using a small dataset to enhance performance on downstream tasks. However, finetuning with a few adversarially designed training examples, or even with a benign dataset, can compromise the safety alignment of LLMs . Qi et al.  finetuned GPT-3.5 Turbo and LLaMA2-7b-chat with only 10 harmful examples, but the safety guardrails were undermined in both LLMs. Zhan et al.  removed the safety protections of GPT-4 with 95% success with only 340 examples trained with the OpenAI's finetuning API. A new line of research aims to defend against such harmful finetuning attacks at both the alignment stage and the user finetuning stage, _e.g._, Vaccine , Lisa , Antidote , Booster , and targeted Vaccine . Safety-aware LLM fine-tuning, such as Safe LoRA , can mitigate safety degradation after fine-tuning by steering the model updates toward the direction of better alignment. In this paper, we probe into the mechanism of the harmful finetuning attack via navigating the safety landscape.

Enhancing alignment with safety prompts.To communicate with LLM with precise and task-specific instructions, Bsharat et al.  presented a comprehensive principled instructions and guidelines to improve the quality of prompts for LLMs. Recently, safety researchers have also experimented with various prompts to either break or enhance LLM safety alignment. On the attack side, Jin et al.  used roleplaying to automatically and iteratively generate harmful prompts. On the defense side, Zheng et al.  leveraged prompt tuning to enhance LLM safety by directly optimizing the vanilla system prompt into safety prompt. Safety prompt is a method of safeguarding LLMs against harmful queries without changing the model weights; they are prepended to the user input or serve as a system prompt.

Jailbreaking aligned LLMs with adversarial attacks.A class of vulnerabilities known as "jailbreaks" has recently been shown to cause LLMs to violate their alignment safeguards [7; 38; 44]. Jailbreaks based on human prompt strategies rely on deception and social engineering to elicit objectionable content from LLMs, requiring creativity, manual dataset curation, and significant human effort [8; 12]. Optimization-based jailbreaks optimize the tokens input to the LLM, recognized for their effectiveness but requiring extensive computational resources and being often uninterpretable to humans [51; 29].

## 3 From LLM Safety Landscape to Visage Safety Metric

The model landscape is a crucial tool for interpreting model behaviors and understanding model characteristics. Perturbing a model along random directions reveals the local behavior of the model, while interpolating the parameters between two models illustrates the transition process from one model to the other. In this section, we introduce the notion of LLM safety landscape in both 1D (Sec. 3.1) and 2D (Sec. 3.2) scenarios. Sec. 3.3 presents the safety landscape of four popular open-source LLMs and Sec. 3.4 introduces "LLM safety basin" concept and proposes the Visage safety metric based on our landscape analysis.

### 1D Safety Landscape

Denote \(\) as the initial LLM model weights. The safety landscape is plotted by perturbing \(\) along a certain direction \(}}\) and evaluate the new model weights with a single model safety metric:

\[f()=(+}})\] (1)

where \(\) is the safety metric defined for a single model, and \(\) is a scalar parameter. For 1D-interpolation, we pick two sets of model weights \(\) and \(^{}\) and the direction is defined by the line connecting these two points, \(}}=^{}-\). For 1D-random, \(\) is the center point and we randomly sample a direction \(}\) from Gaussian distribution. We apply layer normalization to \(}\) to exclude the effect of scale invariance  so that the flatness and the sharpness across different landscape plots are comparable. Specifically, \(}\) is normalized to a unit direction and then multiplied by the Frobenius norm of each layer \(i\):

\[}}=}}{\|}\|}\|_{ i}\|\] (2)

In the rest of the paper, we will use 1D-random \(\) and 1D-interpolation \(^{}\) to represent the above two types of 1D directions.

### 2D Safety Landscape

Similar to the 1D landscape, the 2D landscape requires two directions \(}}\) and \(}}\) and the safety landscape is defined as:

\[f(,)=(+}}+ }})\] (3)

For 2D random, since both directions are randomly sampled from Gaussian distribution, the cosine similarity between \(}}\) and \(}}\) is \(\), where \(n\) is the dimension of \(\). Given current LLMs have billions of parameters, the two random directions are orthogonal and we only perform layer normalization as in Eq. 2. For 2D interpolation, we pick three sets of model weights \(\), \(^{}\), and \(^{}\) and compute the interpolated directions \(}=^{}-,}=^{ }-\). Since there is no guarantee that two interpolated directions are orthogonal, we use Gram-Schmidt algorithm to find the orthogonal basis:

\[}}=},}}=}-^{T}}}}{\|}\|^{2}}}\] (4)

To ensure the scale equivalence of two directions, we rescale \(}}\) to \((\|}}\|/\|}} \|)}}\). 2D-interpolation landscape is useful when analyzing two finetuned model weights, which are all initialized by the same aligned LLM. In the rest of the paper, we will use 2D-random \(\) and 2D-interpolation \(\)\(^{}\)\(\&\)\(^{}\) to represent the above two types of 2D directions.

### Safety Landscape of Open-source LLMs

We show the safety landscapes of four popular open-source LLMs: LLaMA2-7B-chat , LLaMA3-8B-instruct , Mistral-7B-instruct-v0.2 , and Vicuna-7B-v1.5 . For each perturbed model along the landscape direction, we evaluate on the first 80 prompts of AdvBench  "Harmful Behaviors" split (Adv 80) with ASR as the safety metric. The ASR is measured by refusal keyword detection following the original AdvBench evaluation protocol. Note that \(\) can be any harmfulness evaluation metric, _e.g._, LLM Judge  or Llama Guard . Since a recent user study  shows that GPT-4 Judge and refusal keyword detection perform closely on flagging harmful content, we use keyword detection as it is the fastest. We interpolate 20 steps on each axis for all landscapes. To ensure deterministic results, we set top-p as 0 and temperature as 1 .

**Safety landscape between pretrained and aligned LLMs.** Pretraining is the initial phase of training an LLM, aimed at developing a broad understanding of language and knowledge . Alignment, on the other hand, focuses on training LLMs to better follow instructions in prompts and align their behaviors with human preferences . Since the pretrained model is not designed with safety in its first priority, it lacks safety guardrails. In contrast, the aligned model is expected to refuse to respond to harmful user queries. We use LLaMA2 as an example and show the safety landscape of 1D-interpolation LLaMA2-7B \(\) LLaMA2-7B-chat in Fig. 1(a). Since the pretrained (LLaMA2-7B) and the aligned (LLaMA2-7B-chat) models use different chat templates, both templates are evaluated to ensure the change of safety is due to alignment. Notice that the chat template difference does not exist when comparing the aligned and the finetuned models in later sections as all of them share the same chat template and system prompt. Both lines in Fig. 1(a) show that the ASR of the aligned model is significantly lower than the pretrained model as expected. Notice that the pretrained model has a less than 100 ASR when using the aligned model chat template. This is because the pretrained model repeats the system prompt in the aligned model's chat template, which is captured by the keyword detector and treated as successful refusal. When using the aligned model's chat template, we find that the local region of the aligned model shows the same level of safety. This is surprising because early work has shown that finetuning can easily break LLM's safety alignment, which may imply the aligned model is a cusp, _i.e._, sharp corner, on the safety landscape.

**Safety landscape of an aligned LLM.** Inspired by our findings in the interpolation direction above, we are curious whether this flat safety region exists in other directions for different LLMs. Fig. 1 (top) plots the 1D and 2D random landscape of four popular open-source LLMs. For each LLM, we use the default system prompt provided by the model, with details in Appendix A. We discover that each aligned LLM serves as a robust anchor point, maintaining safety within its local region, but the safety is completely compromised outside of this local region, and the change of safety as steep as a step function. We term this new phenomenon observed universally in the LLM parameter space as "safety basin". All four investigated LLMs exhibit such phenomenon, but the differences

Figure 2: LLM safety landscape: (a) 1D-interpolation LLaMA2-7B \(\) LLaMA2-7B-chat safety landscape. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. While interpolating the model weights between the base and the chat model, we need to ensure the chat format remains consistent. Thus, we ablate on both chat formats: text completion (no template) and LLaMA2 chat template. The chat model exhibits higher safety than the base model as expected. The base model also shows an increase in safety while using the LLaMA2 chat template. (b) 1D-random LLaMA2-7B safety landscape sampled over different random directions. When provided with a single model, we sample a random normalized direction to visualize its local variations along both positive and negative directions.

are the depth and width of the basin. The finding of safety basin generalizes to different evaluation metrics and other safety datasets (Appendix B), and the model still generate fluent output when ASR is high (Appendix D). Besides, we also find that a larger model side exhibits a wider safety basin (Appendix E).

### Visage Safety Metric

The landscape visualization and analysis suggest that the average depth of the safety basin can serve as a good indicator for measuring LLM safety, reflecting both the safety of the original aligned model and the robustness of the model when its parameters are perturbed. Formally, for an \(n\)-D random safety landscape, we define Visage safety metric as the average safety margin of all models we have sampled along all random directions:

\[}=}_{(-a,a), (-b,b),}[_{max}-(,,)], <_{max}\] (5)

where \(\) and \(\) are all sampled from uniform distribution and we use \(a=b=0.5\) as we find that LLMs are completely broken after perturbing more than half of its norm. \(\) is a monotonically decreasing function in terms of safety, as a lower \(\) means a safer model. \(_{max}\) is the maximum possible value for \(\). When ASR is used as the safety metric, \(_{max}=100\).

**Stability of Visage.** Since the definition of Visage involves random directions, we run a stability test to find out how many sampled directions the model converge to a stable value. We take 1D-random LLaMA2-7B-chat as an example and show the result in Fig. 1(b). We sample 8 different directions until the average converges and find that the average of 3 different directions is close enough to the final average. Thus, we use the mean of Visage along three different directions as the evaluation metric in the rest of the paper. In Fig. 1, we also compute Visage for both 1D and 2D random landscape and the ranking of those two dimensions remain the same. Thus, we use 1D Visage for faster evaluation.

We compute the Visage score for all four LLMs using their default system prompts and chat templates. Since LLaMA3-8B-instruct does not have a default system prompt, we use the LLaMA2 system prompt. The Visage ranking is as follows: LLaMA3-8B-instruct \(>\) LLaMA2-7B-chat \(>\) Mistral-7B-instruct-v0.2 \(>\) Vicuna-7B-v1.5. We also evaluate these four models on all 520 prompts of AdvBench "Harmful Behaviors" split (Adv 520). The ASR of the models are as follows: LLaMA3-8B-instruct (0.38), LLaMA2-7B-chat (0.19), Mistral-7B-instruct-v0.2 (1.15), and Vicuna-7B-v1.5 (2.5). Although the ASRs of all four LLMs are close, the Visage reflects the safety of a model's local region, indicating the risk after finetuning, which we will explore in the next section.

## 4 Why can simple finetuning easily break LLM's safety alignment?

In this section, we navigate the LLM safety landscape and explore why safety alignment can be easily compromised by finetuning with only a few adversarially designed training examples. Sec. 4.1 details the finetuning settings. In Sec. 4.2, we discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic Visage safety metric measures the risks in finetuning without assumptions on the finetuning dataset. In Sec. 4.3, visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact in Sec. 4.4.

### Finetuning settings

We finetune on the harmful samples created by Qi et al. , which were sampled from Anthropic red-teaming dataset . Following the standard OpenAI finetuning API , each training sample is structure in a one-round conversation format.

We ensure that the system prompt used during finetuning remains consistent with the aligned model so that the differences in safety are indeed induced by finetuning. We adhere to the official finetuning recipe1 and conduct full parameter finetuning. Following the training hyperparameters in Qi et al.

, all models are finetuned for five epochs with AdamW optimizer . At inference time, we evaluate on both 80 prompts and all 520 prompts of AdvBench "Harmful Behavior" split. The finetuning is done on 4 A100 GPUs.

### Finetuning on few-shot harmful data breaks LLM's safety alignment

We finetune LLaMA2-7B-chat and Vicuna-7B-v1.5 on subsets of 10, 50, and 100 harmful examples sampled from the training dataset. As shown in Table 1, both models have close to 0% ASR before finetuning, but the ASRs increases significantly after finetuning, indicating broken safety alignment. Comparing the results of finetuning on 10, 50, 100 harmful examples, the ASRs continue to increase as expected. We also discover that different LLMs have varying rates of vulnerability to finetuning, and our Visage safety metric can successfully measure the risks in finetuning before actual finetuning. Comparing LLaMA2 with Vicuna, LLaMA2 has a higher Visage score than Vicuna, meaning that when both are finetuned on the same user data, LLaMA2 shows a lower ASR than Vicuna when evaluated on safety benchmarks. Our evaluation results on both 80 prompts and full 520 prompts verify that the safety in finetuning is reflected by our Visage safety metric. Since the Visage definition does not make assumptions on the downstream finetuning dataset, LLM-Visage serves as a task-agnostic safety metric that measures finetuning risks.

Finetuning with harmful data is dragging the model away from the safety basin but at different rates

We save the model checkpoint of each epoch during finetuning and project them onto the safety landscape to visualize the optimization trajectory. Previous work have observed that projecting on random directions fail to capture the variation in optimization trajectory because the trajectory lies in an extremely low dimensional spaces, and a random sampled direction is nearly orthogonal to this subspace. A potential solution is applying principal component analysis (PCA) on all saved model checkpoints, but the \(n\)-epoch finetuning on LLaMA2-7B-chat will lead to a feature matrix of size \(n 7B\), which makes it expensive to compute with existing PCA libraries. Therefore, we set the projection direction as the interpolated direction between the initial and the final finetuned model weights. By projecting all saved checkpoints onto this direction, we successfully capture the optimization trajectory. Fig. 1 (red dots at the bottom 2D interpolation landscape) shows the training trajectory of finetuning LLaMA2-7B-chat on 100-shot harmful data for 5 epochs. The aligned model is the initial point and each epoch is dragging the model away from the aligned model, and finally outside of the safety basin. Our safety landscape visualization enables us to understand, for the first time, how simple finetuning compromises safety alignment.

### Finetuning with harmful and safe data helps the model stay within the safety basin

Though finetuning can easily break LLMs' safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. This can be achieved by finetuning on a mixture of user data and safety data. Bianchi et al.  suggests that finetuning LLaMA1  (not aligned) on the mixture of user and safe data can improve the safety of the model. We are curious if this finetuning strategy is generalizable to other LLMs, and if it works, can we explain it with our safety landscape? Therefore, we finetune LLaMA2 and Vicuna

   Model & Visage &  AdvBench \\ Samples \\  & 
 Aligned \\  & 10-shot & 50-shot & 100-shot & mix \\  LLaMA2-7B-chat & 85.32 & 80 & 0 & 90.0 & 91.3 & 100.0 & 0 \\  & & 520 & 0.2 & 85.2 & 90.2 & 95.4 & 0.2 \\  Vicuna-7B-v1.5 & 73.26 & 80 & 5.0 & 95.0 & 97.5 & 100.0 & 1.3 \\  & & 520 & 2.5 & 89.2 & 94.0 & 96.7 & 1.2 \\   

Table 1: Finetuning on few-shot harmful data breaks LLM’s safety alignment at different rates and our Visage safety metric successfully measures the rate. LLaMA2 has a higher Visage score than Vicuna, and the ASRs on AdvBench indicate that when finetuned with the same amount of harmful data, LLaMA2 remains safer than Vicuna. Additionally, we demonstrate that finetuning with a mixture of safe and harmful data helps the model maintain its safety alignment. The “aligned” column refers to the original off-the-shelf models.

on a mixture of 100-shot harmful examples in Sec. 4.3 along with the 100-shot safe data created by Bianchi et al.  for ten epochs till convergence. In Table 1, we show that finetuning with the safe data indeed lowers the ASR.

Both initialized from the aligned model, the model that is finetuned on 100-shot harmful data from Sec. 4.3 is completely unsafe while the model that is finetuned on a mixture of 100-shot harmful and 100-shot safe data is still safe. We take LLaMA2 as an example and show 2D-interpolation LLaMA2-7B-chat \(\) LLaMA2-7B-chat 100-shot harmful & 100-shot harmful+100-shot safe in Fig. 1(bottom). In the surface plot, starting from the aligned model in the origin, the pure harmful finetuning quickly drags the model away from the safety basin, and significantly elevates the ASR. On the other hand, finetuning with a mixture of harmful and safe data keeps the model within the safety basin and thus maintaining the safety of the finetuned model.

## 5 System prompt

LLM safety landscape also highlights the system prompt's critical role in protecting a model, and how this protection transfers to its perturbed variants within the safety basin. In this section, we evaluate the impact of system prompt design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each LLM's default system prompt as the baseline. We collect different types of system prompts from both an attacker's or a defender's perspective. From an attacker's standpoint, we apply two types of prompts: (1) removing the default system prompt (Empty), and (2) using roleplaying prompt to attach a new charater to the LLM, hoping to make the LLM forget its safety responsibilities (Roleplay). From a defender's perspective, we also employ two types of prompts: (1) LLaMA2's default system prompt that explicitly includes safety concerns (LLaMA2), and (2) safety prompts that are directly optimized for a specific LLM  (Safety). The details of the system prompts used in our experiments are listed in Appendix A. For each type of the system prompt, we compute its mean Visage score from three random directions. Table 2 shows the results, illustrating how each type of system prompt affects the safety of an LLM's local region.

**LLaMA2.** The default system prompt has a Visage score of 85.32. Removing the system prompt incurs a 4.64 percentage points (pp) drop. Surprisingly, applying the roleplaying prompt does not compromise LLaMA's safety; instead, it leads to a slight increase in the Visage score. We find that roleplaying prompts are generally less effective in breaking an LLM's safety across different models. Among the six LLMs tested with the default system prompt, LLaMA2 has the highest Visage score, aligning with the observation that LLaMA2 tends to be conservative and may even refuse harmless input prompts .

**LLaMA3.** There is no default system prompt for LLaMA3, yet even without a system prompt, LLaMA3 demonstrates a high Visage score. LLaMA3 excels in following system prompt instructions while maintaining its safety, experiencing only a 2.7pp drop when using the roleplaying prompt, but showing a 9.3pp increase when the LLaMA2 system prompt is employed. This is likely due to LLaMA3's improved training procedures, which substantially reduce false refusal rates .

**Mistral.** We evaluate both Mistral-7B-instruct-v0.1 and Mistral-7B-instruct-v0.2. Fig. 3a shows the 1D-random Mistral-7B-instruct-v0.1 safety landscape under different system prompts. For both

   Model & Default & Empty & Roleplay & LLaMA2 & Safety \\  LLaMA2-7B-chat & 85.32 & 80.68 & 86.56 & 85.32 & - \\ LLaMA3-8B-instruct & - & 81.10 & 78.40 & 90.40 & - \\ Mistral-7B-instruct-v0.1 & 74.11 & 20.78 & 52.65 & 85.66 & 86.24 \\ Mistral-7B-instruct-v0.2 & 82.04 & 64.90 & 75.54 & 73.69 & 75.53 \\ Vicuna-7B-v1.3 & 82.03 & 56.13 & 77.13 & 80.18 & - \\ Vicuna-7B-v1.5 & 77.37 & 73.56 & 81.61 & 81.62 & - \\   

Table 2: LLM safety landscape highlights the system prompt’s critical role in protecting a model, and how this protection transfers to its perturbed variants in the safety basin. We measure the Visage score of different system prompt for popular open-source LLMs. Higher Visage means safer model and “-” means not applicable. For LLaMA3, there is no default system prompt in the initial release. For all other LLMs in the “safety” column, we use the optimized safety prompts specific to each LLM from Zheng et al. , with only Mistral’s safety system prompt provided.

models, removing the system prompt significantly reduces the safety score. Specifically, removing the system prompt decreases Mistral-7B-instruct-v0.1's Visage score by 53.33pp. Using the roleplaying prompt also degrades the performance for both models. Both LLaMA2 and safety system prompts effectively enhance Mistral's Visage score, but Mistral-7B-instruct-v0.1 is more sensitive to the system prompt than Mistral-7B-instruct-v0.2.

**Vicuna.** We have tested on both Vicuna-7B-v1.3 and Vicuna-7B-v1.5. Fig. 2(b) shows the 1D-random Vicuna-7B-v1.3 safety landscape under different system prompts. Vicuna-7B-v1.3 is finetuned from LLaMA1, while Vicuna-7B-v1.5 is finetuned from LLaMA2 pretrained (not aligned) model weights . We find that the performance drops for both models when removing the system prompt. As shown in Fig. 2(b), removing the system prompt reveals that the original Vicuna model is a local maximum in the safety landscape, indicating that there exist slightly perturbed model weights more resistant to harmful inputs than the fine-tuned model. This suggests that the safety alignment of Vicuna is not optimal, possibly because the fine-tuning process rarely encountered inputs with an empty system prompt. The roleplaying prompt shows mixed performance: it decreases Vicuna-7B-v1.3's safety score but increases Vicuna-7B-v1.5's safety score. Finally, using the LLaMA2 system prompt significantly improves model safety.

Overall, the system prompt does have a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

## 6 Jailbreak attacks

Previous work has shown that the safeguards of the aligned LLMs can be bypassed by adversarial attacks. We are curious whether these so-called "jailbreaks" against LLMs are still effective to slightly perturbed models within the aligned model's local region. We use the adversarial prompts from JailbreakBench , which has incorporated jailbreaking prompts targeting LLaMA2 and Vicuna generated by GCG  and PAIR  adversarial attacks. Fig. 3(a) shows the 1D-random LLaMA2-7B-chat evaluated on jailbreaking prompts. There are only 6 prompts in JailbreakBench that can successfully attack the LLaMA2 model, and our experiment shows only 4 of them are successful, thus leading to a 66.67% ASR for the aligned model. The safety landscape reveals that the jailbreaking prompts are quite sensitive to the model weights perturbation, _i.e._, there exists certain perturbed models that are significantly safer than the aligned model. This is not unique to LLaMA2, as shown by the safety landscape of Vicuna-7B-v1.5 under jailbreak attacks in Fig. 3(b). We also replace the default Vicuna system prompt with the LLaMA2 system prompt and find it improving the overall safety in the model's local region. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the

Figure 3: The system prompt has a strong impact on LLM safety landscape. From an attacker’s standpoint, we find that both removing the default system prompt and using simple roleplaying prompt jeopardizes the safety alignment, with the former exhibiting greater potency. From a defender’s perspective, we discover that LLaMA2’s original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses.

## 7 Safety _vs._ Capability Landscape

We evaluate on three datasets covering capabilities in math, history, and policy from MMLU . The shape of the LLM capability landscape is drastically different from the one in the LLM safety landscape; these landscapes do not exhibit the same trend, further confirming that the basin shape is indeed unique to the safety of LLM. We provide a detailed analysis in Appendix C.

## 8 Conclusion

We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin". Our discovery inspires us to propose the new Visage safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.

## Limitations and Future Work

We believe there are multiple directions for future research, and our work is an important first step in exploring the safety landscape of popular open-source LLMs. Given our findings that the shape of the LLM capability landscape differs significantly from that of the LLM safety landscape, a potential direction for future work is to explore how to better balance the tradeoff between capability and safety, _e.g._, finding the optimal capability performance for a given dataset while staying within the safety basin. Another direction is proposing additional sub-metrics such as basin width, depth, and smoothness. Our Visage score, defined as the average safety margin of all models sampled along random directions, can be considered as an average depth within the safety basin. The Visage score is a byproduct of our novel findings on the safety basin, and we hope our study will inspire further research into proposing more metrics, including width and smoothness.