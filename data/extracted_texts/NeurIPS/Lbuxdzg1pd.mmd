# The Secretary Problem with Predicted Additive Gap

Alexander Braun

Institute of Computer Science, University of Bonn. Email: alexander.braun@uni-bonn.de

Sherry Sarkar

Carnegie Mellon University. Email: sherryys@andrew.cmu.edu

###### Abstract

The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of \(}{{e}} 0.368\) has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question:

_What is the weakest piece of information that allows us to break the \(}{{e}}\) barrier?_

To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an _additive gap_\(c\). This gap is the difference between the highest and \(k\)-th highest weight in the sequence. Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index \(k\) and any gap \(c\), we can obtain a competitive ratio of \(0.4\) when knowing the exact gap (even if we do not know \(k\)), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction.

The full version with proofs can be found at https://arxiv.org/abs/2409.20460 [Braun and Sarkar, 2024].

## 1 Introduction

The secretary problem is a fundamental problem in online decision making: An adversary fixes non-negative, real-valued weights \(w_{1} w_{2} w_{n}\) which are revealed online in random order. The decision maker is allowed to accept (at most) one element. At the time of arrival of an element, the decision maker is required to make an immediate and irrevocable acceptance decision. The goal is to maximize the weight of the selected element. A tight guarantee3 of \(}{{e}}\) is known since the seminal work of Lindley (1961) and Dynkin (1963) (also see Ferguson (1989) or Freeman (1983)) and can be achieved with a very simple threshold policy.

In the modern era, the assumption of having no prior information on the weights is highly pessimistic. To go beyond a worst case analysis, researchers have recently considered the setting where we have some sort of learned prediction that our algorithm may use up front. This setting spawned the recent and very successful field of algorithms with predictions. Antoniadis et al. (2020) and Dutting et al. (2021) studied the secretary problem with a prediction of the largest weight in the sequence, andresolve this setting with an algorithm which yields a nice robustness-consistency trade-off. Fujii and Yoshida (2023) consider the secretary problem with an even stronger prediction: A prediction for every weight in the sequence.

However, predicting the largest element or weight can sometimes be difficult or unfavorable. For example in retail, past data may only contain information about prices and not the true values of buyers (Kleinberg and Leighton, 2003; Leme et al., 2023); or for data privacy reasons (see e.g. Asi et al. (2023)), only surrogates for largest weights are revealed in history data. This motivates to advance our understanding of the following question:

_What is the weakest piece of information we can predict that still allows us to break the \(}{{e}}\) barrier?_

Stated another way, is there a different parameter we can predict, one that does not require us to learn the best value, but is still strong enough to beat \(}{{e}}\)? This brings us to the idea of predicting the _gap_ between the highest and \(k\)-th highest weight, or in other words, predicting how valuable \(w_{k}\) is with respect to \(w_{1}\). Coming back to data privacy for example, such a parameter does successfully anonymize the largest weight in the sequence.

From a theoretical perspective, for some special cases of gaps, previous work directly implies improved algorithms. For example, if we know \(w_{1}\) and \(w_{2}\) have the same weight, using an algorithm for the two-best secretary problem (Gilbert and Mosteller, 1966) directly leads to a better guarantee. More generally, if we know the _multiplicative gap_\(}}{{w_{2}}}\), we observe that we can generalize the optimal algorithm for \(w_{1}=w_{2}\)(see e.g. Gilbert and Mosteller (1966); Buchbinder et al. (2014)). However, if we instead only know \(}}{{w_{1}}}=0\), this does not help at all. The only insight is that \(w_{n}=0\); we have no insight on the range of values taken. This essentially boils down to the classical secretary problem and the best competitive ratio again is \(}{{e}}\). So while the multiplicative gap advises about the relative values without needing to know \(w_{1}\) entirely, it is not strong enough in general to beat \(}{{e}}\). In this paper, we consider instead predicting an _additive gap_\(w_{1}-w_{k}\).

The additive gap between \(w_{1}\) and \(w_{k}\) can be viewed as interpolating between two previously studied setups: when \(w_{1}-w_{k}\) gets small, we get closer towards the \(k\)-best secretary problem, and when \(w_{1}-w_{k}\) is very large, the additive gap acts as a surrogate prediction of \(w_{1}\), the prediction setting in Antoniadis et al. (2020) and Dutting et al. (2021). As we will see, even though the additive gap is much weaker than a direct prediction for \(w_{1}\), it strikes the perfect middle ground: it is strong enough to beat \(}{{e}}\) by a constant for any possible value of the gap \(w_{1}-w_{k}\) (and even if we do not know what \(k\) is upfront). In addition, in contrast to pieces of advice studied in the literature so far, knowing an exact additive gap does not make the problem trivial to solve.

### Our Results and Techniques

Our contribution is threefold. First, in Section 3, we show the aforementioned result: knowing an exact additive gap allows us to beat the competitive ratio of \(}{{e}}\) by a constant.

**Theorem 1** (Theorem 4, simplified form).: _There exists a deterministic online algorithm which achieves an expected weight of \([] 0.4 w_{1}\) given access to a single additive gap \(c_{k}\) for \(c_{k}=w_{1}-w_{k}\) and some \(k\)._

Still, getting an exact gap might be too much to expect. Hence, in Section 4, we introduce a slight modification in the algorithm to make it robust with respect to errors in the predicted gap while simultaneously outperforming the prevalent competitive ratio of \(}{{e}}\) by a constant for accurate gaps.

**Theorem 2** (Theorem 5, simplified form).: _There exists a deterministic online algorithm which uses a predicted additive gap and is simultaneously \((}{{e}}+O(1))\)-consistent and \(O(1)\)-robust._

The previous Theorem 2 does not assume any bounds on the error of the predicted additive gap used by our algorithm. In particular, the error of the prediction might be unbounded and our algorithm is still constant competitive. However, if we know that the error is bounded, we can do much better.

**Theorem 3** (Theorem 6, simplified form).: _There exists a deterministic online algorithm which achieves an expected weight of \([] 0.4 w_{1}-2\) given access to a bound \(\) on the error of the predicted gap._

Our algorithms are inspired by the one for classical secretary, but additionally incorporate the gap: Wait for some time to get a flavor for the weights in the sequence, set a threshold based on the past observations and the gap, pick the first element exceeding the threshold.

At first glance, this might not sound promising: In cases when the gap is small, incorporating the gap in the threshold does not really affect the best-so-far term. Hence, it may seem that beating \(}{{e}}\) is still hard. However, in these cases, even though the threshold will be dominated by the best-so-far term most of the time, the gap reveals the information that the best value and all other values up to \(w_{k}\) are not too far off. That is, even accepting a weight which is at least \(w_{k}\) ensures a sufficient contribution.

Our analyses use this fact in a foundational way: Either the gap is large in which case we do not consider many elements in the sequence for acceptance at all. Or the gap is small which implies that accepting any of the \(k\) highest elements is reasonably good. For each of the cases we derive lower bounds on the weight achieved by the algorithm.

Since we do not know upfront which case the instance belongs to, we optimize our initial waiting time for the worse of the two cases. In other words, the waiting time cannot be tailored to the respective case but rather needs to be able to deal with both cases simultaneously. This introduces some sort of tension: For instances which have a large gap, we would like the waiting time to be small. By this, we could minimize the loss which we incur by waiting instead of accepting high weighted elements at the beginning of the sequence. For instances which have a small gap, the contribution of the gap to the algorithm's threshold can be negligible. This results in the need of a longer waiting time at the beginning to learn the magnitude of weights reasonably well. We solve this issue by using a waiting time which balances between these two extremes: It is (for most cases) shorter than the waiting time of \(}{{e}}\) from the classical secretary algorithm. Still, it is large enough to gain some information on the instance with reasonable probability.

As a corollary of our main theorem, we show that we can beat the competitive ratio of \(}{{e}}\) even if we only know the gap \(w_{1}-w_{k}\) but do not get to know the index \(k\). In particular, this proves that even an information like "There is a gap of \(c\) in the instance" is helpful to beat \(}{{e}}\), no matter which weights are in the sequence and which value \(c\) attains.

Complementing theoretical results, we run simulations in Section 6 which strengthen our theoretical findings. First, we show that for instances in which the classical secretary algorithm achieves a nearly tight guarantee of \(}{{e}}\), our algorithm can almost always select the highest weight. In addition, we further investigate the robustness-consistency trade-off of our algorithm. In particular, as it will turn out, underestimating is not as much of an issue as overestimating the exact gap.

### Additional Related Work

_Implications of related results on the additive gap._ In the two-best secretary problem, we can pick at most one element but win when selecting either the best or second best element. For this problem, the competitive ratio is upper bounded by approximately \(0.5736\)(Buchbinder et al., 2014; Chan et al., 2015) (the authors provide an algorithm which matches this bound, so the guarantee is tight). As our setting with \(w_{1}-w_{2}=0\) can be viewed as a special case, this yields a hardness result; the best any algorithm can perform with the exact additive gap provided upfront is approximately \(0.5736\).

_A non-exhaustive list of related work on secretary problems._ Since the introduction of the secretary problem in the 1960s, there have been a lot of extensions and generalizations of this problem with beautiful algorithmic ideas to solve them Kleinberg (2005); Babaioff et al. (2007, 2018); Feldman et al. (2018); Korula and Pal (2009); Mahdian and Yan (2011); Kesselheim et al. (2018); Rubinstein (2016). Beyond classical setups, recent work by Kaplan et al. (2020) and Correa et al. (2021) studies the secretary problem with sampled information upfront. Here, some elements are revealed upfront to the algorithm which then tries to pick the best of the remaining weights. Guarantees are achieved with respect to the best remaining element in the sequence. In addition, there are also papers bridging between the secretary problem and the prophet inequality world, e.g. Correa et al. (2020) or Correa et al. (2019) and many more (Bradac et al., 2020; Kesselheim and Molinaro, 2020; Argue et al., 2022).

_Algorithms with machine learned advice._ In the introduction, we already scratched the surface of the field on algorithms with predictions. Here, the algorithm has access to some machine learned advice upfront and may use this information to adapt decisions. Initiated by the work of Lykouris and Vassilvitskii (2021) and Purohit et al. (2018), there have been many new and interesting results in completely classical problems within the last years, including ski rental Wei and Zhang (2020), online bipartite matching Lavastida et al. (2021), load balancing Ahmadian et al. (2023), and many more (see e.g. Im et al. (2021); Zeynali et al. (2021); Almanza et al. (2021)). Since this area is developingvery fast, we refer the reader to the excellent website Algorithms-with-Predictions for references of literature.

As mentioned before, also the secretary problem itself has been studied in this framework. Antoniadis et al. (2020) consider the secretary problem when the machine learned advice predicts the weight of the largest element \(w_{1}\). Their algorithm's performance depends on the error of the prediction as well as some confidence parameter by how much the decision maker trusts the advice. In complementary work, Dutting et al. (2021) give a bigger picture for secretary problems with machine learned advice. Their approach is LP based and can capture a variety of settings. They assume that the prediction is one variable for each weight (e.g. a 0/1-variable indicating if the current element is the best overall or not). Fujii and Yoshida (2023) assume an even stronger prediction: Their algorithm is given a prediction for every weight in the sequence. In contrast, we go into the opposite direction and deal with a less informative piece of information.

Our work also fits into the body of literature studying weak prediction models, previously studied for e.g. paging Antoniadis et al. (2023), online search Angelopoulos (2021), to just mention a few. In these, several different directions for weak prediction models were considered. For example, the setting in scheduling or caching where the number of predictions is smaller than the input size Im et al. (2022); Benomar and Perchet (2024).

## 2 Preliminaries

In the secretary problem, an adversary fixes \(n\) non-negative, real-valued weights, denoted \(w_{1} w_{2} w_{n}\). For each of the elements, there is an _arrival time4_\(t_{i}}}{{}}\). Weight \(w_{i}\) is revealed at time \(t_{i}\) and we immediately and irrevocably need to decide if we want to accept or reject this element. Overall, we are allowed to accept at most one element with the objective of maximizing the selected weight. We say that an algorithm is \(\)_-competitive_ or achieves a _competitive ratio_ of \(\) if \([] w_{1}= _{i}w_{i}\), where the expectation is taken over the random arrival times of elements (and possible internal randomness of the algorithm).

In addition to the random arrival order, we assume to have access to a single prediction \(_{k}\) for one _additive gap_ together with its index \(k\). The additive gap for some index \(2 k n\) is \(c_{k} w_{1}-w_{k}\). We say that an algorithm has access to an _exact_ or _accurate_ gap if \(_{k}=c_{k}\) (as in Section 3). When the algorithm gets a predicted additive gap \(_{k}\) which might not be accurate (as in Section 4 or Section 5), we say that \(_{k}\) has _error_\(=|_{k}-c_{k}|\). We call an algorithm \(\)_-robust_ if the algorithm is \(\)-competitive regardless of error \(\) and we say the algorithm is \(\)_-consistent_ if the algorithm is \(\)-competitive when \(=0\). To fix notation, for any time \(\), we denote by \(()\) (read _best-so-far_ at time \(\)) the highest weight which did appear up to time \(\). In other words, \(()=_{i:t_{i}}w_{i}\). Also, when clear from the context, we drop the index \(k\) at the gap and only call it \(c\) or \(\) respectively.

## 3 Knowing an Exact Gap

Before diving into the cases where the predicted gap may be inaccurate in Section 4 and Section 5, we start with the setup of getting a precise prediction for the gap. That is, we are given the exact gap \(c_{k}=w_{1}-w_{k}\) for some \(2 k n\). We assume that we get to know the index \(k\) as well as the value of \(c_{k}\), but neither \(w_{1}\) nor \(w_{k}\).

Our algorithm takes as input the gap \(c\) as well as a waiting time \(\). This gives us the freedom to potentially choose \(\) independent of \(k\) if required. As a consequence, we could make the algorithm oblivious to the index \(k\) of the element to which the gap is revealed. We will use this in Corollary 1.

``` Input: Additive gap \(c\), time \(\)  Before time \(\):  Observe weights \(w_{i}\)  At time \(\):  Compute \(()=_{i:t_{i}}w_{i}\)  After time \(\):  Accept first element with \(w_{i}((),c)\) ```

**Algorithm 1** Secretary with Exact Additive Gap

Our algorithm takes as input the gap \(c\) as well as a waiting time \(\). This gives us the freedom to potentially choose \(\) independent of \(k\) if required. As a consequence, we could make the algorithm oblivious to the index \(k\) of the element to which the gap is revealed. We will use this in Corollary 1.

This algorithm beats the prevalent competitive ratio of \(}{{e}} 0.368\) by a constant.

**Theorem 4**.: _Given any additive gap \(c_{k}=w_{1}-w_{k}\), for \(=1-(}{{k+1}})^{}{{k}}}\), Algorithm 1 achieves a competitive ratio of \((0.4,}{{2}}(}{{k+1}})^{}{{k}}})\)._

Note that as \(k\) tends towards \(n\) and both become large, the competitive ratio approaches \(}{{2}}\).

We split the proof of Theorem 4 in the following two lemmas. Each of them gives a suitable bound on the performance of our algorithm for general waiting times \(\) in settings when \(w_{k}\) is small or large.

The first lemma gives a lower bound in cases when \(w_{k}\) is small in comparison to \(w_{1}\).

**Lemma 1**.: _If \(w_{k}<w_{1}\), then \([](1-)(+) w_{1}\)._

The second lemma will be used to give a bound when \(w_{k}\) is large compared to \(w_{1}\).

**Lemma 2**.: _If \(w_{k}w_{1}\), then the following two bounds hold:_

1. \([](1--(1-)^{k +1}) w_{1}\) _and_
2. \([](( {})-(1-)) w_{1}\)   _._

_As a consequence, \([]\) is also at least as large as the maximum of the two bounds._

The proofs of the lemmas as well as their combination to prove Theorem 4 can be found in the full version (Braun and Sarkar, 2024). From a high level perspective, the two lemmas give a reasonable bound depending of we either exclude a lot of elements in the algorithm (Lemma 1) or if the largest \(k\) elements ensure a sufficient contribution (Lemma 2).

As a corollary of the proof of Theorem 4, we also get a lower bound on the weight achieved by the algorithm if we are only given the gap, but not the element which obtains this gap. That is, we are given \(c_{k}\) but not the index \(k\).

**Corollary 1**.: _If the algorithm only knows \(c_{k}\), but not \(k\), setting \(=0.2\) achieves \([] 0.4 w_{1}\)._

The full version (Braun and Sarkar, 2024) contains a proof of Corollary 1 as well. It mainly relies on the fact that some lower bound we obtained in the proof of Theorem 4 holds for any choice \(\). Also, the algorithm itself only uses the gap to contribute to the threshold. The index \(k\) is only used to compute \(\). As a consequence, when choosing \(=0.2\) independent of \(k\), the algorithm is oblivious to the exact value of \(k\), but only depends on the gap \(c_{k}\). For this choice of \(\), we can show that \( 0.4\).

As a consequence, very surprisingly, even if we only get to know _some_ additive gap \(c_{k}\) and not even the index \(k\), we can outperform the prevalent bound of \(}{{e}}\). Also, observe that this is independent of the exact value that \(c_{k}\) attains and holds for any small or large gaps.

As mentioned before, Algorithm 1 is required to get the exact gap as input. In particular, once the gap we use in the algorithm is a tiny bit larger than the actual gap \(c_{k}\), we might end up selecting no element at all.

**Example 1**.: We get to know the gap to the smallest weight \(c_{n}=w_{1}-w_{n}\) and the smallest weight \(w_{n}\) in the sequence satisfies \(w_{n}=0\). Let the gap which we use in Algorithm 1 be only some tiny \(>0\) too large. In other words, we use \(c_{n}+\) as a gap in the algorithm. Still, this implies that our threshold \(((),c_{n}+)\) after the waiting time satisfies

\[((),c_{n}+) c_{n}+=w_{1}+>w_{1} w _{2} w_{n}.\]

As a consequence, we end up selecting no weight at all and have \([]=0\).

This naturally motivates the need to introduce more robust deterministic algorithms in this setting. In Section 4, we will show that a slight modification in the algorithm and its analysis allows to obtain robustness to errors in the predictions while simultaneously outperforming \(}{{e}}\) for accurate gaps.

## 4 Robustness-Consistency Trade-offs

Next, we show how to slightly modify our algorithm in order to still beat \(}{{e}}\) when getting the correct gap as input, but still be constant competitive in case the predicted gap is inaccurate. The modification leads to Algorithm 2: Initially, we run the same algorithm as before. After some time \(1-\), we will lower our threshold in order to hedge against an incorrect prediction.

``` Input: Predicted gap \(\), times \([0,1)\), \([0,1-)\)  Before time \(\):  Observe weights \(w_{i}\)  At time \(\):  Compute \(()=_{i:t_{i}}w_{i}\)  Between time \(\) and time \(1-\):  Accept first element with \(w_{i}((),)\)  After time \(1-\):  Accept first element with \(w_{i}()\) ```

**Algorithm 2** Robust-Consistent Algorithm

Note that by \([0,1-)\), we ensure that \(<1-\), i.e. the waiting time \(\) is not after time \(1-\) and hence, the algorithm is well-defined. Now, we can state the following theorem which gives guarantees on the consistency and the robustness of Algorithm 2. We will discuss afterwards how to choose \(\) and \(\) in order to outperform the classical bound of \(}{{e}}\) by a constant for accurate predictions while ensuring to be constant-robust at the same time.

**Theorem 5**.: _Given a prediction \(_{k}\) for the additive gap \(c_{k}\), define_

* \(_{1} 1--+()\) _and_ \(_{2}((1+)(1--)+( )+())\)_,_
* \(_{3}(1--(1-)^{k+1})\) _and_ \(_{4}()- (1-)\)_._

_Then, Algorithm 2 is (i) \(((_{1},_{2}),(_{3},_ {4}))\)-consistent and (ii) \((())\)-robust._

Observe that if we do not trust the prediction at all, we could set \(=}{{e}}\) and \(1-=}{{e}}\). Doing so, we do not use the prediction in our algorithm. Still, for these choices, we recover the guarantee from classical secretary of \(}{{e}}\). In other words, we can interpret \(\) as a trust parameter for the prediction which also mirrors our risk appetite. If we do not trust the prediction at all or if we are highly risk averse, we can set \(1-\). If we are willing to suffer a lot in case of an inaccurate prediction (or if we have high trust in the prediction), we will set \( 0\).

Theorem 5 yields a trade-off between robustness and consistency. In particular, for a fixed level of robustness, we can choose the optimal values for \(\) and \(1-\) for the bounds in Theorem 5 to obtain the plot in Figure 1. Observe that when not focusing on robustness (i.e. choosing robustness being equal to zero), we can achieve a consistency approximately matching the upper bound of \(0.5736\) described in Section 1.2.

Figure 1: Choosing the optimal parameters \(\) and \(1-\) for our analysis in Theorem 5: For a given level of robustness, what is the best consistency we can obtain with our analysis.

The proof of Theorem 5 can be found in the full version [Braun and Sarkar, 2024]. Concerning robustness, we can only obtain a reasonable contribution by accepting the best weight. Therefore, we derive a lower bound on the probability of accepting the highest weight via Algorithm 2. Concerning consistency, we can perform a case distinction whether \(w_{k}\) is small or large. Crucially, one is required to take the drop in the threshold after time \(1-\) into account.

For example, when using a waiting time \(=0.2\) as in Corollary 1 independent of the index \(k\) and a value of \(=0.6\), i.e. \(1-=0.4\), we get the following: Algorithm 2 is approximately \(0.383\)-consistent and \(0.183\)-robust (also see Figure 2).

In particular, we can outperform the prevalent bound of \(}{{e}}\) by a constant if the predicted gap is accurate while ensuring to be constant competitive even if our predicted gap is horribly off. Of course, when being more risk averse, one could also increase the robustness guarantee for the cost of decreasing the competitive ratio for consistent predictions.5

We highlight that these guarantees as well as Theorem 5 hold independent of any bounds on the error of the predicted gap. However, it is reasonable to assume that we have some bounds on how inaccurate our predicted gap is (for example, if our predicted gap is learned from independent random samples). We show in Section 5 that we can achieve much better competitive ratios when we know a range for the error.

## 5 Improved Guarantees for Bounded Errors

Complementing the previous sections where we had either access to the exact gap (Section 3) or no information on a possible error in the prediction (Section 4), we now assume that the error is bounded6. That is, we get to know some \(_{k}[c_{k}-;c_{k}+]\) which is ensured to be at most an \(\) off. Also, the bound \(\) on the error is revealed to us. Still, the true gap \(c_{k}\) remains unknown.

Our algorithm follows the template which we discussed before. Still, we slightly perturb \(_{k}\) to ensure that the threshold is not exceeding \(w_{1}\). This algorithm allows to state an approximate version of Theorem 4 for the same lower bounds of \(\) as in the exact gap case.

Figure 2: Trade-off between robustness and consistency as a function of the time \(1-\) for fixed choice of \(=0.2\).

**Theorem 6**.: _Given any prediction of the gap \(_{k}[c_{k}-;c_{k}+]\), where \(c_{k}=w_{1}-w_{k}\), Algorithm 3 satisfies \([] w_{1}-2\). For \(=1-()^{1/k}\), \((0.4,()^{1/k})\) and for \(=0.2\), \( 0.4\)._

As a consequence, the guarantees from the exact gap case in Section 3 carry over with an additional loss of \(2\). Also, the results when not knowing the index \(k\) carry over. In particular, this nicely complements the robustness result from Theorem 5 as follows: Once we can bound the error in a reasonable range, even not knowing the gap exactly does not cause too much of an issue. The proof of Theorem 6 can be found in the full version (Braun and Sarkar, 2024).

## 6 Simulations

In order to gain a more fine-grained understanding of the underlying habits, we run experiments7 with simulated weights and compare our algorithms among each other and to the classical secretary algorithm8.

In Section 6.1, we compare our Algorithm 1 to the classical secretary algorithm. To this end, we draw weights i.i.d. from distributions and execute our algorithm and the classical one. As it will turn out, instances which are hard in the normal secretary setting (i.e. when not knowing any additive gap) become significantly easier with additive gap; we can select the best candidate with a much higher probability. We also demonstrate that for some instances, knowing the gap has a smaller impact, though our Algorithm 1 still outperforms the classical one.

Second, in Section 6.2, we turn towards inaccurate gaps and compare Algorithm 1 developed in Section 3 to the robust and consistent variant of Algorithm 2 from Section 4. As a matter of fact, we will see that underestimating the exact gap is not as much of an issue as an overestimation. In particular, underestimating the gap implies a smooth decay in the competitive ratio while overestimating can immediately lead to a huge drop.

### The Impact of Knowing the Gap

We compare our algorithm with additive gap to the classical secretary algorithm (see e.g. (Dynkin, 1963)) with a waiting time of \(}{{e}}\).

#### 6.1.1 Experimental Setup

We run the comparison on three different classes of instances:

1. _Pareto_: We first draw some \((}{{n}},1)\). Afterwards, each weight \(w_{i}\) is determined as follows: Draw \(Y_{i}[0,]\) i.i.d. and set \(w_{i}=Y_{i}^{}\) (for more details on Pareto distributions and secretary problems, see e.g. Ferguson (1989)).
2. _Exponential_: Here, all \(w_{i}(1)\).
3. _Chi-Squared_: Draw \(w_{i}^{2}(10)\). That is, each \(w_{i}\) is drawn from a chi-squared distribution which sums over ten squared i.i.d. standard normal random variables.

For each class of instances, we average over \(5000\) iterations. In each iteration, we draw \(n=200\) weights i.i.d. from the respective distribution together with \(200\) arrival times which are drawn i.i.d. from \(\). The benchmark is the classical secretary algorithm with a waiting time of \(=}{{e}}\): Set the largest weight up to time \(\) as a threshold and accepts the first element afterwards exceeding this threshold. Algorithm 1 is executed with waiting times \(=0.2\) as well as \(=1-(}{{k+1}})^{}{{k}}}\).

#### 6.1.2 Experimental Results

When weights are sampled based on the procedure explained in (i), we observe an interesting phenomenon (see Figure 3). For the classical secretary algorithm, we achieve approximately the tight guarantee of \(}{{e}}\). Our algorithm, however, achieves a competitive ratio of approximately \(0.8\) for \(=0.2\). When having a waiting time depending on \(k\), we improve the competitive ratio for large \(k\) while suffering a worse ratio for small \(k\). This can be explained as follows. Weights which are distributed according to (i) almost always have a very large gap between the highest and second highest weight. Hence, no matter which gap we observe, it will always be sufficiently large to exclude all elements except the best one. Therefore, we only incur a loss if we do not accept anything (which happens if and only if the best element arrives before the waiting time). As a consequence, for \(=0.2\), we observe the ratio of \(0.8\) (which is the probability of the highest weight arriving after time \(\)). For the waiting times depending on \(k\), the waiting time turns out to be larger for smaller \(k\) and vice versa. The improvement in the competitive ratio for large \(k\) comes from the reduced waiting time and hence a smaller probability of facing an arrival of \(w_{1}\) during the waiting period.

Interestingly, this shows that there are instances for which the classical secretary algorithm almost obtains its tight guarantee of \(}{{e}}\) while these instances become easy when knowing an additive gap. As a side remark: One might wonder if it is always true that the index \(k\) does not play a pivotal role when using a constant waiting time \(=0.2\). In the full version [Braun and Sarkar, 2024], we show that this is not the case for exponentially distributed weights as in (ii) or Chi-Squared distributed ones as in (iii).

### Dealing with Inaccurate Gaps

In order to get a better understanding concerning inaccuracies in the gap, we run a simulation with different errors.

#### 6.2.1 Experimental Setup

Again, we average over \(5000\) iterations. In each iteration, we set \(n=200\), draw arrival times as before and weights as follows:

* _Exponential_: Here, all \(w_{i}(1)\).
* _Exponential with superstar_: Here, \(w_{i}(1)\) for \(n-1\) weights and we add a superstar element with weight \(100_{i}w_{i}\).

We compare Algorithm 1 to Algorithm 2 both with waiting time \(=0.2\). In addition, Algorithm 2 will drop the gap from the threshold after a time of \(1-=0.95\), in other words \(=0.05\).

The comparison is done for three different gaps: A small one where \(k=2\), i.e. the gap between the largest and second largest element, \(k=}{{2}}\) and \(k=n\), i.e. the gap to the smallest element. Given a multiplication factor \(\) for the error, we feed our algorithm with a predicted gap \(_{k}= c_{k}\) for \(\) going from zero to three in step size of \(0.1\). In other words, for \(=1\), we get an accurate gap, for

Figure 3: Competitive ratios for weights based on (i). On the \(x\)-axis, we have the index \(k\) from \(2\) to \(n\). The \(y\)-axis shows the competitive ratios.

\(<1\), we underestimate the gap, for \(>1\) we overestimate the gap and for \(=0\), the algorithms are equivalent to the classical secretary algorithms with waiting time \(\).

#### 6.2.2 Experimental Results

For exponentially distributed weights (see Figure 4), we can observe that underestimating the gap does not cause too many issues. In particular, when highly underestimating the gap (i.e. \(<0.5\)), both algorithms achieve a competitive ratio of approximately \(0.65\), similar to an algorithm not knowing any gap. For an accurate gap, \(=1\), larger gaps are more helpful as they block more elements from being considered. Still, \(>1\) introduces a transition. For \(>1\) and gaps between the best and a small element (e.g. \(k=100\) or \(k=200\)), overestimating the gap reduces the selection probability of _any_ weight of Algorithm 1 to zero: The predicted gap is simply too large and even exceeds \(w_{1}\). Still, Algorithm 2 is robust in a sense that we still achieve a competitive ratio of approximately \(0.15\). This constant depends on our choice of \(\). As mentioned before, there is the natural trade-off: Increasing \(\) for an improved robustness and suffer a decrease in the competitive ratio for \(=1\).

Interestingly, for the gap between the best and second best element, both algorithms are much more robust. This can be explained as the gap is small in this case anyway, so overestimating by a factor of three does not cause too much issues yet. One would require to overestimate by a much larger factor here to see a significant difference in the performance of both algorithms.

In the full version (Braun and Sarkar, 2024), we show what happens when shifting our perspective towards the more adversarial setting of exponential weights with one additional superstar as listed in (v). Here, the drop when overestimating is even more significant. Still, Algorithm 2 achieves the desired constant competitive ratio even if gaps are fairly inaccurate. However, the trade-off between consistency and robustness plays a much more important role in the choices of \(\) and \(\) here.

## 7 Conclusion and Future Directions

As we have seen, a single simple piece of information of the form "There is a gap of \(c\) in the instance" helps to improve the competitive ratio for the secretary problem. In addition, our algorithm can be made robust against inaccurate predictions without sacrificing too much in the competitive ratio.

Our results directly impose some open questions for future research. First, our guarantees seem to be not tight. Can we achieve a better competitive ratio for any gap? Or is there a matching hardness result? As a second open question, the gaps that we consider are of the form \(w_{1}-w_{k}\) for some \(k\). As a generalization, one could consider arbitrary gaps \(w_{i}-w_{j}\) for some \(1 i<j n\). Can we do something in this regime? (as sketched in the full version (Braun and Sarkar, 2024), we can for e.g. \(w_{2}-w_{3}=0\)).

Also, going beyond the single selection problem is interesting, for example by considering the multi-selection variant. For this, we give a reasonable starting point in the full version (Braun and Sarkar, 2024).