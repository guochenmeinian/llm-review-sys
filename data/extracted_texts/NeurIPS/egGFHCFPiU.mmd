# On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world. Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. In this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We empirically exhibit the effectiveness of our proposed framework in various planning domains.

## 1 Introduction

Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data  or interactions with the world [10; 9]. It is a time- and space-consuming open issue in the planning community . Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs . As demonstrated by , even in a seemingly simple common-sense domain like Blocksworld that humans usually find easy to solve, LLMs are evaluated to be quite ineffective in planning autonomously.

An interesting result shown by  is when taking the solution generated by LLMs, which is incorrect, as a seed plan to be repaired by an off-the-shelf planner, e.g., LPG , a significant improvement in search steps can be attained over the result when an empty plan provided as a seed plan for the planner. This indicates that LLMs can indeed provide some helpful information (e.g., in some sense of heuristics) for planning, even though they cannot solve planning problems solely. Inspired by the result of loosely using plans generated by LLMs as seed plans, we are curious if it is possible to "dig" more helpful information from LLMs to assist planning deeply, e.g., by inserting LLMs into planning frameworks. By doing this, we aim to answer the question: **what roles can be played exactly by LLMs in planning?** Indeed, there have been attempts to explore off-the-shelf planning techniques to help LLMs solving planning problems . Similar to , they only view planners as black-boxes without deepening the integration of LLMs in planning frameworks.

To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning . We propose a novel LLMs-based planning framework with LLMs embedded in two phases of the planning framework (namely LLMs4Plan). The first phase isto propose promising actions in "action-levels" of the planning graph using LLMs. The second phase is to propose non-mutual action sets using LLMs when backtracking the planning graph. Note that the two phases correspond to two critical steps that influence the efficiency and effectiveness in graph planning.

_For example, as shown in Figure 1(a), there could be a large number of actions in "action-level 1" when expanding "state-label 0" with Graphplan . We aim to exploit LLMs to help select a small subset of promising actions, e.g., \(\{a_{1},a_{2},a_{3}\}\) are selected in Figure 1(a). In Figure 1(b), when backtracking from "state-level \(K\)" that includes goals, there could be a large number of candidate sets of actions to be explored (e.g., "action set 1", "action set 2", "action set 3") -- actions in each candidate set are not mutually exclusive with each other (two actions are mutually exclusive if they are not allowed to be executed at the same time, e.g., actions "pick up object A" and "put down object A" are mutually exclusive). It is particularly time-consuming to search all of the valid candidate sets based on mutual constraints in each action-level \(K\), and _domain models_, and feed the prompts to LLMs to select action sets for backtracking. We embed the above two components into one of the well-known off-the-shelf graph planners, Graphplan . We study the effectiveness of different cases of adding or removing the above one or two components in Graphplan to see the significance of roles LLMs play in the graph planning framework.

Through this study, we provide new clues for how to _deeply_ embed LLMs into off-the-shelf planning frameworks, i.e., first identifying critical steps (generally time-consuming ones) in specific planning frameworks, and then designing proper prompt generation to be embedded into the frameworks. We verify that solv relying on LLMs to do planning is far from a good option, while leveraging LLMs to help deal with some critical steps in the graph planning framework is possible.

## 2 Problem Formulation

In this work we consider classical planning problems specified in the form of STRIPS . Similar ideas can be extended into more expressive planning language such as PDDL . Let \(\) be a set of atoms, each of which is composed of a predicate with zero or more parameters (e.g., _clean(room)_ is an atom indicating _room_ is _clean_). A STRIPS domain is composed of a set of action models \(\), each of which is a quadruple \( a,(a),(a),(a)\), where \(a\) is an action name with zero or more parameters, \((a)\) is a precondition list indicating the conditions under which \(a\) can be applied, \((a)\) is an adding list and \((a)\) is a deleting list indicating the effects of \(a\). Let \(\) be a set of propositions, which are instances of atoms in \(\). We define a planning problem as \(=,s_{0},g,\), where \(s_{0}\) is an initial state and \(g\) is a goal. A solution \(\) to the planning problem is a sequence of actions that transit initial state \(s_{0}\) to goal \(g\). An intuitive example of our planning problem is as shown below.

_Suppose we would like to clean a bedroom using a vacuum which is placed in a tool room. We can formulate the problem \(=,s_{0},g,\) in the form of STRIPS (note that we assume there is no parameter for each predicate and action for simplicity since there is only one tool, one bedroom and one toolroom). The set of propositions \(\) is represented by \(=\{dirty(),toolroom(),clean(),bedroom()\}\). Initial state \(s_{0}\) is represented by \(s_{0}\) = {dirty(), toolroom()}, which indicates the "bedroom" is

Figure 1: Two critical steps in graph planning

dirty, and the tool "vacuum" is in the tool room (i.e., "toolroom"). The goal \(g\) is represented by \(g=\{clean(),toolroom()\}\), which indicates the "bedroom" is clean, and the tool "vacuum" is back to the tool room. The set of action models \(\) is represented as follows:_

   Action & Preconditions & Effects \\  \(vacuum()\) & \(dirty(),bedroom()\) & \(clean(), dirty()\) \\  \(move2tr()\) & \(bedroom()\) & \(toolroom(), bedroom()\) \\  \(move2br()\) & \(toolroom()\) & \(bedroom(), toolroom()\) \\   

_Action \(vacuum()\) aims to vacuuming "bedroom", the preconditions of which are "bedroom" is dirty and the vacuum-cleaner is in "bedroom". The effects of \(vacuum()\) are adding \(clean()\) to the state where \(vacuum()\) is executed, indicating "bedroom" is clean, and deleting \(dirty()\) (i.e., \( dirty()\)) from the state, indicating "bedroom" is not dirty anymore. Action \(move2tr()\) aims to move the vacuum-cleaner to "toolroom", the precondition of which is \(bedroom()\) indicating the vacuum-cleaner is in "bedroom". The effects are adding \(toolroom()\) indicating the vacuum-cleaner is in "toolroom", and deleting \(bedroom()\), indicating the vacuum-cleaner is not in "bedroom". Similarly, action \(move2br\) aims to move the vacuum-cleaner to "bedroom", the precondition of which is \(toolroom()\), indicating the vacuum-cleaner is in "toolroom". The effects are the vacuum-cleaner is adding \(bedroom()\), indicating the vacuum-cleaner is in "bedroom", deleting \(toolroom()\), indicating the vacuum-cleaner is not in "toolroom". A solution \(\) to the problem \(\) is \(move2br(),vacuum(),move2tr()\)._

## 3 Our LLMs4Plan approach

An overview of our LLMs4Plan approach is shown in Algorithm 1. In Step 3, the pruning possibility \(_{i}\) is decreased as the exponent \(i\) increasing. In Step 4, we expand planning graph \(PG^{r}\) with one more level using LLMs to prune actions based on pruning possibility \(_{i}\) and planning problem \(\). In Steps 4, if goal \(g\) is not included by the last state-level in \(PG^{r}\), i.e., \(Satisfied(g,PG^{r})\) is false, we continue to Step 4. In Step 4, we build a set of mutual constraints \(\) based on \(PG^{r}\), i.e., \(buildConstraints(PG^{r})\). In Step 4, we sort sets of actions based on constraints \(\) using LLMs, i.e., \(sortActionsLLMs(PG^{r},\). In Step 4, we search solution \(\) based on the sorted action sets \(\) using depth-first search. In the following subsections, we will address our LLMs4Plan in detail.

```
0: Planning problem \(\), pruning possibility \(_{0}\)
0: Solution \(\)
1:\(PG^{r}=\)
2:for\(i=1\) to \(N\)do
3:\(_{i}=(_{0})^{i}\), \(k=1\)
4:while\(k<K\)do
5:\(PG^{r} expandGraphLLMs(PG^{r},,_{i})\)
6:\(k=k+1\)
7:if\(Satisfied(g,PG^{r})=false\), then continue
8:\(\) = \(buildConstraints(PG^{r})\)
9:\(=sortActionsLLMs(PG^{r},)\)
10:\(=depthFirstSearch(,PG^{r})\)
11:if\(\) Failure, then return\(\)
12:endwhile
13:endfor
14:return Failure ```

**Algorithm 1** An overview of our LLMs4Plan

### Building Planning Graphs with LLMs

A planning graph \(PG^{r}\) is the search space for a relaxed version of the planning problem, an intuitive framework of which is shown in Figure 5. It alternates layers of ground literals and actions. "Square" nodes at action-level \(i+1\) indicate actions that might be possible to be executed in state \(s_{i}\). Maintenance actions indicate dump operators that keep literals unchanged between state-levels \(i\) and \(i+1\). "Black circle" nodes at state-level \(i\) indicate literals that might possibly be true at time \(i\). Edges between state-level \(i\) and action-level \(i\) indicate literals in state-level \(i\) are preconditions of actions in action-level \(i\), while edges between action-level \(i\) and state-level \(i+1\) indicate literals in state-level \(i+1\) are adding or deleting effects of actions in action-level \(i\). The nodes in the first state-level indicate literals that are true in initial state \(s_{0}\).

The procedure of building the planning graph with LLMs (i.e., _buildGraphLLMs_) based on the given planning problem \(=,s_{0},g,\) is as follows:

1. All propositions in \(s_{0}\) and negation of propositions in \(-s_{0}\) are added into state-level 0.
2. All actions in \(\), whose preconditions are satisfied in state-level 0 and **selected by LLMs**, are added into action-level 1; a maintenance action corresponding to each proposition in state-level 0 is added into action-level 1.
3. The propositions added or deleted by actions in action-level 1 are added into state-level 1; and all propositions in state-level 0 are added into state-level 1 as well (i.e., which is done by the maintenance action).
4. We repeat steps 1-3 by increasing state-level 0 to 1 (or \(i\) to \(i+1\)) until all propositions in goal \(g\) are included by state-level \(k\).

In Step 5, we use LLMs to help select actions to build the planning graph. Note that in classical graph-based planning , all of the actions whose preconditions are satisfied will be added in the action-level. We design the prompt to consult LLMs as shown in Figure 3, where "\(\)domain\(\)", "\(\)initial state\(\)", "\(\)goal\(\)", "\(\)proposition set\(\)", and "\(\)candidate actions\(\)" are action models \(\), initial state \(s_{0}\), goal \(g\), the set of propositions \(\) and all of the candidate actions whose preconditions are satisfied in \(s_{0}\). The text in BLUE is the prompt used to guide LLMs to select actions. "\(\)example of output format\(\)" is used to guide LLMs to output actions in the desired format, e.g., "move '?fromm': 'rooma', '?to': 'roomb'".

### Building Mutual Constraints

Due to the satisfaction of action models being relaxed, actions and/or states in action-levels or state-labels may be inconsistent, i.e., there may be some actions mutually exclusive in action-levels, or some literals mutually exclusive in state-levels. As shown in Figure 4, there are three types of mutual exclusion constraints among actions. Specifically, two actions at the same action-level are mutex, if they satisfy the following conditions:

* An effect of one negates an effect of the other, which is called _inconsistent effects_.
* One deletes a precondition of the other, which is called _interference_.
* They have mutually exclusive preconditions, which is called _Competing needs_.

Otherwise they do not interfere with each other, i.e., both may appear in a solution plan. Two literals at the same state-level are mutex if one is the negation of the other, or all ways of achieving them are pairwise mutex, namely _inconsistent support_.

Figure 3: The prompt for pruning actions

Figure 2: The framework of a planning graph

An example planning graph corresponding to Example 1 is as shown in Figure 5. Action _vacuum_ is mutually exclusive with action _dumb_ for _toolroom_ at _action-level 2_ since _vacuum_'s precondition _bedroom_ is mutually exclusive with _toolroom_ at _state-level 1_.

### Sort Action Sets with LLMs and Search Solutions

After we build a set of constraints in Step 8 of Algorithm 1, we use off-the-shelf procedure presented in  to compute candidate action sets such that there are no conflicts (i.e., satisfying the constraints \(\)) among actions in each action set. After that, in Step 9, we consult LLMs to sort the action sets by designing the prompts as shown in Figure 6, which is similar to the prompt shown in Figure 3 except the command in BLUE. After we get the sorted action sets \(\), in Step 10, we conduct the dept-first search procedure as done in  by giving the priority of action sets based on the sorted action sets in \(\).

## 4 Experiment

### Experimental Setup

In the experiment, we evaluate LLMs4Plan in ten planning domains with different scenarios, including gripper, miconic, logistics, movie, blocks, satellite, zenotravel, driverlog, woodworking and openstacks. Ten problems are randomly selected for each domain. The specific scenarios and sizes are described in Appendix A.1.

To demonstrate the effectiveness of our LLMs4Plan approach, we designed five sets of comparison experiments. The methods were implemented using Python. We compared our LLMs4Plan approach with four other methods, which are listed below:

* **GP**: It is the graph-based planning algorithm mentioned above. We implement the traditional graph planning algorithm as the most important baseline for comparison. We directly provide _domain_,_pddl_ and _problem_,_pddl_ to the planner for solving.
* **GPT-3.5**: We simply construct and splice the contents of _domain_._pddl_ and _problem_._pddl_ directly. We then add the necessary command prompts to form a complete prompt into GPT3.5 and command the model to solve the problem directly.
* **GPT-4**: The process is the same as GPT-3.5.

Figure 4: Mutual exclusion of actions

Figure 5: An example of planning graph and mutual constraints indicated in RED arcs

Figure 6: The prompt for sorting action sets

* LLMs4Plan-**GPT3.5**: When we are expanding the hierarchy or backtracking, we are faced with a candidate action selection with LLMs. We guide the LLMs to select a minimal subset of actions from them and utilize this subset of actions for the next algorithmic operations, where LLMs are specifically GPT-3.5.
* LLMs4Plan-**GPT4**: Replaced the LLM model with GPT4, otherwise same as LLMs4Plan-GPT3.5.

### Experimental Metrics

In the experimental framework described, we employed three distinct metrics to assess the efficacy of various methodologies: the problem-solving success rate, the cumulative count of expansion actions and the node count for backtracking in Depth-First Search (DFS).

**Problem-solving success rate**. The solvability of a problem is a crucial metric in assessing planning problems. All approaches are required to generate a sequence of actions that is sufficient to solve the problem, and only if the problem can be transferred from the initial state to the goal state through this sequence of actions can the corresponding problem be considered to be successfully solved. Furthermore, we have established an upper bound on the depth of the problem-solving process. The optimal length of the action sequence for the test problem is known to us. Should the solution obtained surpass this predetermined depth, it signifies the inability of the method to successfully ascertain the optimal action path for this particular problem. Setting an upper bound on the depth of the problem-solving process serves the purpose of not only requiring the planner to solve problems but also demanding that it does so more efficiently. This ensures that the output action sequences are more concise and accurate, minimizing the occurrence of redundant actions.

**Total number of expansion actions**. In the GP algorithm, the expansion of actions at each layer is a fundamental process, and the number of these expansions serves as a vital metric. Under the premise of preserving effective actions, fewer expansions result in a reduced count of mutually exclusive action pairs and subsequently fewer branches in the deep search phase of backtracking, thereby enhancing efficiency. Consequently, we compute the average total number of action expansions per layer across all problems, applying different methods within various domains, as a significant metric for comparison.

**Number of nodes for backtracking DFS**. This metric serves as the cornerstone for validating our optimization efforts, as the DFS during backtracking accounts for the majority of the computational load in the GP algorithm, overshadowing the forward expansion phase. Particularly when dealing with increasing expansion depths, the exponentially growing number of DFS poses the most significant challenge for GP algorithms in tackling large-scale problems or complex solution sequences. We primarily utilize this metric to ascertain which method truly enhances the efficiency of the planning process.

Regarding the number of nodes for backtracking DFS, our analysis was confined to data from the GP and LLMs4Plan-GPT4 methods, primarily for two reasons. Firstly, the metric is relevant only in scenarios where the problem is successfully solved; failed solutions do not yield countable data. Consequently, we excluded LLMs4Plan-GPT3.5 from our statistical analysis due to its comparatively lower success rate. Secondly, these metrics are inherently calculable within the GP framework alone. Hence, directly solving problems using GPT-3.5 and GPT-4 precludes the possibility of gathering this data, as these methods operate outside the GP framework.

### Experimental Results

We present the success rates in Table 1, depict the pruning effects of action expansion for LLM on GP in Figure 7, and showcase experimental results in Table 2 comparing our approach to traditional GP algorithms in terms of the number of nodes for backtracking DFS metrics, along with relevant ablation studies.

**Ablation Experiment**: We conducted four ablation experiments to ascertain the effectiveness of forward pruning and backward sorting, detailed as follows:

1. LLMs4Plan: This method involves both forward pruning and backward sorting.
2. LLMs4Plan-**unsorted**: Here, we implement pruning without sorting.

3. LLMs4Plan**-unpruned**: In this approach, sorting is used, but not pruning.
4. **GP**: This method involves neither pruning nor sorting.

### Experimental Analysis

**Analysis of the success rate of planning**: From Table 1, several conclusions can be drawn. GPT3.5 exhibits competence primarily in resolving simple problems with short action sequence lengths, such as in the **movie** domain, while its success rates are notably low in other domains. Consequently, it struggles to enhance the capabilities of GP algorithms. Conversely, GPT4 demonstrates substantial improvements in abilities compared to GPT3.5, particularly in reasoning skills and decision-making involving long action sequences. With the enhanced reasoning and commonsense capabilities of GPT4, GP shows an enhanced success rate in certain domains. We observe instances of failure in GP, attributed to the inclusion of partially corrupt data during testing. Specifically, we introduce a proportion of corrupted data by randomly removing action preconditions and effects propositions from domain files. These instances have varying impacts across different domains, particularly affecting traditional GP algorithms reliant on the completeness of domain files. We detail the influence of random removal on success rates in Table 3. For each layer of GP expansion, the provision of action preconditions and effects propositions is essential. However, in the case of LLM-augmented GP algorithms, LLMs4Plan is capable of making rational action decisions even in the presence of missing action propositions, thereby aiding in the completion of current planning tasks. Consequently, our algorithm exhibits greater robustness in terms of success rates compared to traditional GP approaches. The specific settings for the robustness experiments are detailed more extensively in section A.2 of the appendix.

   & **GPT-3.5** & **GPT-4** & **GP** & LLMs4Plan**-**GPT3.5** & LLMs4Plan**-**GPT4** \\ 
**gripper** & 0.00 & 0.60 & 0.70 & 0.00 & **1.00** \\
**miconic** & 0.10 & 0.50 & 0.60 & 0.10 & **1.00** \\
**logistics** & 0.20 & 0.60 & 0.60 & 0.20 & **1.00** \\
**movie** & **1.00** & **1.00** & **1.00** & **1.00** & **1.00** \\
**blocks** & 0.10 & 0.70 & 0.60 & 0.30 & **1.00** \\
**satellite** & 0.00 & 0.50 & 0.90 & 0.10 & **1.00** \\
**zenotravel** & 0.20 & 0.60 & 0.90 & 0.20 & **1.00** \\
**driverlog** & 0.00 & 0.10 & 0.90 & 0.20 & **1.00** \\
**woodworking** & 0.90 & 0.90 & 0.70 & **1.00** & **1.00** \\
**openstacks** & 0.10 & 0.20 & **1.00** & 0.20 & **1.00** \\  

Table 1: Success rate results. In the table, each row corresponds to a distinct domain, while each column represents a separate approach or method. The values presented within the table indicate the success rate for each combination of domain and approach, with these rates quantified on a scale ranging from 0 to 1.

   & LLMs4Plan & LLMs4Plan**-**unsorted** & LLMs4Plan**-**unpruned** & **GP** \\ 
**gripper** & **6839** & 11294 & 4850376 & 8486698 \\
**miconic** & **11891** & 47863 & 445145 & 2018484 \\
**logistics** & **59** & 85 & 1226 & 1261250 \\
**movie** & **975163** & 1211830 & **975163** & 10869160 \\
**blocks** & **4572** & 7272 & 83129 & 1205223 \\
**satellite** & **46619** & 94811 & 67167049 & 88779785 \\
**zenotravel** & **1548** & 12166 & 839527 & 2259283 \\
**driverlog** & **574** & 1916 & 58311 & 1579486 \\
**woodworking** & **49** & 3924 & 48553 & 114502 \\
**openstacks** & **107** & 409 & 13577 & 24267 \\  

Table 2: In the table, each row corresponds to a distinct domain, while each column represents a group of ablation experiments. The values presented within the table indicate the number of nodes for backtracking DFS. Both pruning and sorting effectively enhance search efficiency, leading to a substantial reduction in the number of nodes required for searching. Generally, pruning tends to be slightly more effective than sorting.

Upon examining the generated action sequences, we observed that although GPT4 achieves a certain level of success in solving problems, the action sequences it produces tend to be longer compared to those generated by GP alone. By integrating GPT4 with graph planning, LLMs4Plan can effectively generate more optimal action sequences.

**Analysis of search efficiency**: Besides planning success rates, our method significantly improves search efficiency compared to GP algorithms. This enhancement is evident from Table 2, where, among problems with successful planning outputs, we drastically reduce the cost of search nodes, achieving an exponential level of optimization. So, **how does the LLM-augmented GP method enhance search efficiency?**

Through in-depth analysis of experimental cases, we identify two main aspects of optimization:

1. During forward expansion, LLM efficiently and effectively prunes the expansion actions, leading to varying degrees of stable reduction in the total number of expanded actions and mutually exclusive actions. Consequently, the computational load of forward expansion decreases correspondingly.
2. During the depth-first search backtracking process, LLM prioritizes searching closer to the set of planning solutions, accelerating the attainment of planning solutions and saving time by avoiding ineffective searches.

We provide an example from the 'logistics' domain to illustrate our analysis in Figure 7, where we compare the number of expansion actions before and after LLM pruning. The application of LLM for pruning demonstrates significant efficacy across all layers. In addition, we have provided further experimental results and analysis on the total number of mutually exclusive actions and expanded actions in section A.3 of the supplementary materials.

For pruning, the greatest risk is removing necessary actions, rendering the problem unsolvable. In our experiments, we observed instances where LLM prunes crucial actions in certain layers, resulting in the inability to obtain effective solutions. However, we introduced pruning probabilities to ensure algorithm completeness. Experimental results demonstrate that although the process of correcting LLM's erroneous pruning behavior through pruning probabilities may introduce additional expansion and search steps, the cumulative cost of these search steps remains significantly lower than the cost of solely using GP algorithms to solve problems. The results presented in Table 2 compare the outcomes of our method LLMs4Plan, which ensures completeness, with those of GP algorithms.

**Analysis of ablation experiments**: Table 2 reveals that both pruning and sorting contribute to enhanced search efficiency, with their combination amplifying this effect. Notably, pruning appears slightly more effective than sorting. This is likely because LLM, while pruning, also organizes the remaining actions logically. In contrast, sorting may lead to minor errors due to the multitude of actions and lengthy text. In this regard, we require further optimization of natural language processing

   & **10\%** & **20\%** & **30\%** & **40\%** & **50\%** \\ 
**gripper** & 0.40 & 0.87 & 0.73 & 0.80 & 0.67 \\
**miconic** & 0.60 & 0.60 & 0.60 & 0.80 & 0.40 \\
**logistics** & 0.20 & 0.80 & 0.80 & 0.67 & 0.60 \\
**movie** & 1.00 & 1.00 & 1.00 & 1.00 & 0.60 \\
**blocks** & 1.00 & 0.26 & 0.07 & 0.27 & 0.47 \\
**satellite** & 0.60 & 0.73 & 1.00 & 0.86 & 0.87 \\
**zenotravel** & 0.93 & 0.93 & 1.00 & 0.93 & 0.80 \\
**driverlog** & 0.80 & 1.00 & 1.00 & 1.00 & 1.00 \\
**woodworking** & 1.00 & 0.73 & 0.40 & 0.27 & 0.40 \\
**openstacks** & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\  

Table 3: This table illustrates experiments on the robustness of missing action predicates. In the table, each row corresponds to a distinct domain. Each column in the table represents the proportion of predicates we removed. A higher proportion indicates a greater amount of missing information, posing increased difficulty for the planner to solve the problem. The values in the table represent the success rates of GP in solving the problems. In the majority of domains, as the proportion of deleted predicates increases, the success rate of GP planning decreases. Overall, this indicates that GP exhibits poor robustness to missing action predicates.

techniques tailored for handling extremely long texts to enhance our framework's capability in solving more complex problems. The experiments also indicate that LLM tends not to prioritize empty actions in graph planning, favoring their later arrangement. This aligns with our analysis suggesting that prioritizing non-empty actions is more productive, as a layer without any action is essentially redundant.

**Analysis of the advantages and disadvantages of LLMs4Plan**: Upon analyzing examples where solutions failed, we observed that GPT4 is more prone to pruning errors at deeper expansion levels. This results in the discarding of effective actions, thereby unnecessarily increasing the expansion layers and hindering problem resolution. We attribute this to two primary factors. Firstly, as the expansion level deepens, both the predicate set and the candidate action set expand, leading to increasingly lengthy input prompts. This prolonged text can cause GPT-4 to gradually lose track of previous information, resulting in decision-making errors. Secondly, the nature of the predicate set in graph planning diverges from traditional planning's current state representation. This discrepancy impairs LLM's ability to accurately analyze the predicate set, leading to the erroneous elimination of effective actions. LLM lacks capacity to analyze complex predicate set combinations.

Our analysis of additional failure examples indicates that graph planning excels in efficiently handling numerous non-mutually exclusive actions in parallel, due to its ability to group these actions within the same layer. However, its limitation becomes apparent in scenarios requiring the execution of highly complex and extremely long action sequences. If a problem's optimal solution sequences are lengthy, the planning graph must be expanded considerably deeper. Despite effective pruning, this does not resolve the issue of exponential complexity growth in backtracking DFS caused by increased depth.

## 5 Conclusion and Future Work

Our comparative experiments in multiple domains demonstrated the efficacy of our LLMs4Plan in significantly enhancing the problem-solving capabilities of graph planning algorithms. Notably, LLMs4Plan boosts not just the success rate of problem resolution but also markedly enhances search efficiency and substantially reduces computational complexity. The runtime of LLMs4Plan is currently hindered by multiple LLMs calls. While our method requires multiple LLMs calls, it provides substantially improved results. There are also various ways to enhance runtime performance like using smaller LLMs like Llama  or distilling LLMs' knowledge into a smaller model . Those are interesting avenues for future research. Instead of leveraging LLMs to assist planning, it would also be possible to study acquiring action models  and more planning frameworks  with the help of LLMs.

Figure 7: An example of action pruning. Both the GP and LLMs4Plan-GPT4 methods expanded through 10 layers. The horizontal axis represents the layer number, with lower numbers indicating proximity to the initial state and higher numbers nearing the goal state. The vertical axis shows the count of expanded actions, including both domain-specific actions and numerous empty actions, characteristic of the graph planning algorithm. This implies a high pruning ratio for genuinely effective actions. LLMs prune almost every layer of expansion actions and the data in the table also contains many empty actions.