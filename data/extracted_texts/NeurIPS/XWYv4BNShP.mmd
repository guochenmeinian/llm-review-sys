# On the Size and Approximation Error of Distilled Sets

Alaa Maalouf

MIT CSAIL

&Murad Tukan

DataHeroes

&Noel Loo

MIT CSAIL

&Ramin Hasani

MIT CSAIL

&Mathias Lechner

MIT CSAIL

&Daniela Rus

MIT CSAIL

Equal contribution. Correspondence E-mail: alaam@mit.edu.

###### Abstract

Dataset Distillation is the task of synthesizing small datasets from large ones while still retaining comparable predictive accuracy to the original uncompressed dataset. Despite significant empirical progress in recent years, there is little understanding of the theoretical limitations/guarantees of dataset distillation, specifically, what excess risk is achieved by distillation compared to the original dataset, and how large are distilled datasets? In this work, we take a theoretical view on kernel ridge regression (KRR) based methods of dataset distillation such as Kernel Inducing Points. By transforming ridge regression in random Fourier features (RFF) space, we provide the first proof of the existence of small (size) distilled datasets and their corresponding excess risk for shift-invariant kernels. We prove that a small set of instances exists in the original input space such that its solution in the RFF space coincides with the solution of the original data. We further show that a KRR solution can be generated using this distilled set of instances which gives an approximation towards the KRR solution optimized on the full input data. The size of this set is linear in the dimension of the RFF space of the input set or alternatively near linear in the number of effective degrees of freedom, which is a function of the kernel, number of datapoints, and the regularization parameter \(\). The error bound of this distilled set is also a function of \(\). We verify our bounds analytically and empirically.

## 1 Introduction

Motivated by the growing data demands of modern deep learning, dataset distillation  aims to summarize large datasets into significantly smaller synthetic _distilled_ datasets, which when trained on retain high predictive accuracy, comparable to the original dataset. These distilled datasets have applications in continual learning , architecture search , and privacy preservation . Recent years have seen the development of numerous distillation algorithms, but despite this progress, the field has remained largely empirical. Specifically, there is little understanding of what makes one dataset "easier to distill" than another, or whether such small synthetic datasets even exist.

This work aims to fill this gap by providing the first bounds on the sufficient size and relative error associated with distilled datasets. Noting prior work relating neural network training to kernel ridge regression (KRR), we consider dataset distillation in the kernel ridge regression settings with shift-invariant kernels. By casting the problem into the Random Fourier Feature (RFF) space, we show that: **The size and relative error of distilled datasets is governed by the kernel's "number of effective degrees of freedom", \(d_{k}^{}\).** Specifically, in Section 4, we show that distilled sets of size \((d_{k}^{} d_{k}^{})\), exist, with \(12+2_{}\) predictive error on the training dataset, and only \(8\) error withrespect to the optimal solution computed on the full dataset, where \(\) is the kernel ridge regression regularization parameter and \(_{}\) the KRR training error on the original dataset; see Theorem 3 and Remark 7 for full details.

**These bounds hold in practice for both real and synthetic datasets**. In section 5, we validate our theorem by distilling synthetic and real datasets with varying sizes and values of \(d_{k}^{}\), showing that in all scenarios our bounds accurately predict the error associated with distillation.

## 2 Related work

**Coresets.** Coresets are weighted selections from a larger training dataset, which, when used for training, yield similar outcomes as if the whole dataset was used . The key benefit of using coresets is that they significantly speed up the training process, unlike when the full data set is used. Current coresets methods incorporate clustering techniques , bilevel optimization , sensitivity analysis , and surrogate models for approximation . New strategies are specifically designed for neural networks, where before each training epoch, coresets are chosen such that their gradients align with the gradients of the entire dataset , followed by training the model on the chosen coreset. Although coresets are usually theoretically supported, these methods fall short when the aim is to compute a coreset once for a full training procedure.

**Dataset Distillation.** To this end, dataset distillation algorithms construct synthetic datasets (not necessarily a subset from the original input) such that gradient descent training on the synthetic data-points results in high predictive accuracy on the real dataset. Cast as a bilevel optimization problem, early methods involve unrolling training computation graph  for a few gradient descent steps and randomly sampled weight initializations. More sophisticated methods aim to approximate the unrolled computation using kernel methods , surrogate objectives such gradient matching , trajectory matching  or implicit gradients . The kernel-induced points (KIP) algorithm  is a technique that employs Neural Tangent Kernel (NTK) theory to formulate the ensuing loss: \(_{KIP}=\|y_{t}-K_{TS}_{SS}^{-1}y_{S}\|_{2}^{2}\). This loss signifies the predictive loss of training infinitely wide networks on distilled datapoints \(X_{S}\) with corresponding labels \(y_{S}\), on the original training set and labels \(X_{T},y_{T}\), with \(K_{,}\) being the NTK. Dataset distillation is closely related to the use of inducing points to accelerate Gaussian Processes , for which convergence rates exist, but the existence of such inducing points is not unknown .

**From dataset distillation to kernel ridge regression.** Kernel ridge regression (KRR) extends the linear machine learning ridge regression model by using a kernel function to map input data into higher-dimensional feature spaces, allowing for more complex non-linear relationships between variables to be captured . Various methods have been proposed to improve and accelerate the training process of kernel ridge regression. Most notably, Random Fourier Features  approximates shift-invariant kernel functions by mapping the input data into a lower-dimensional feature space using a randomized cosine transformation. This has been shown to work effectively in practice due to regularizing effects , as well as providing approximation bounds to the full kernel ridge regression . Training infinite-width neural networks can be cast as kernel ridge regression with the Neural Tangent Kernel (NTK) , which allows a closed-form solution of the infinite-width neural network's predictions, enabling kernel-based dataset distillation algorithms such as .

## 3 Background

**Goal.** We provide the first provable guarantees on the existence and approximation error of a small distilled dataset in the KRR settings. We first provide notations that will be used throughout the paper.

**Notations.** Let \(\) be a Hilbert space with \(_{}\) as its norm. For a vector \(a^{n}\), we use \( a_{2}\) to denote its Euclidean norm, and \(a_{i}\) to denote its \(i\)th entry for every \(i[n]\). For any positive integer \(n\), we use the convention \([n]=\{1,2,,n\}\). Let \(A^{n m}\) be a matrix, then, for every \(i[n]\) and \(j[m]\), \(A_{i*}\) denotes the \(i\)th row of \(A\), \(A_{*j}\) denotes the \(j\)th column of \(A\), and \(A_{i,j}\) is the \(j\)th entry of the \(i\)th row of \(A\). Let \(B^{n n}\), then we denote the trace of \(B\) by \(Tr(B)\). We use \(_{m}^{m m}\) to denote the identity matrix. Finally, vectors are addressed as column vectors unless stated otherwise.

### Kernel ridge regression

Let \(^{n d}\) be a matrix and let \(y^{n}\) be a vector. Let \(k:^{d}^{d}[0,)\) be a kernel function, and let \(^{n n}\) be its corresponding kernel matrix with respect to the rows of \(\); i.e., \(_{i,j}=k(_{i*},_{j*})\) for every \(i,j[n]\). Let \(>0\) be a regularization parameter. The goal of kernel ridge regression (KRR) involving \(,y,k\), and \(\) is to find

\[^{}_{[,y,k]}*{arg\,min}_{ ^{n}}\|y-\|_{2}^{2}+ ^{T}.\] (1)

We use the notation \(f^{}_{[,y,k]}^{d}\) to denote the in-sample prediction by applying the KRR solution obtained on \(,y\) and \(\) using the kernel \(k\), i.e., for every \(x^{d}\),

\[f^{}_{[,y,k]}(x)=_{i=1}^{n}^{}_{[,y,k]_{i}}k(_{i*},x).\] (2)

To provide our theoretical guarantees on the size and approximation error for the distilled datasets, the following assumption will be used in our theorem and proofs.

**Assumption 1**.: _We inherit the same theoretical assumptions used at  for handling the KRR problem: (I) Let \(\) be the set of all functions mapping \(^{d}\) to \(\). Let \(f^{*}\) be the minimizer of \(_{i=1}^{n}|y_{i}-f(_{i*})|^{2}\), subject to the constraint that for every \(x^{d}\) and \(y\), \(y=f^{*}(x)+\), where \(()=0\) and \(()=^{2}\). Furthermore, we assume that \(y\) is bounded, i.e., \(|y| y_{0}\). (II) We assume that \(\|f^{}_{[,y,k]}\|_{} 1\). (III) For a kernel \(k\), denote with \(_{1}_{n}\) the eigenvalues of the kernel matrix \(\). We assume that the regularization parameter satisfies \(0 n_{1}\)._

**The logic behind our assumptions.** First, the idea behind Assumption (I) is that the pair \((,y)\) can be linked through some function that can be from either the same family of kernels that we support (i.e., shift-invariant) or any other kernel function. In the context of neural networks, the intuition behind Assumption (I) is that there exists a network from the desired architectures that gives a good approximation for the data. Assumption (II) aims to simplify the bounds used throughout the paper as it is a pretty standard assumption, characteristic to the analysis of random Fourier features . Finally, Assumption (III) is to prevent underfitting. Specifically speaking, the largest eigenvalue of \((+n_{n})^{-1}\) is \(}{(_{1}+n)}\). Thus, in the case of \(n>_{1}\), the in-sample prediction is dominated by the term \(n\). Throughout the following analysis, we will use the above assumptions. Hence, for the sake of clarity, we will not repeat them, unless problem-specific clarifications are required.

**Connection to Dataset distillation of neural networks.** Since the neural network kernel in the case of infinite width networks describes a Gaussian distribution , we aim at proving the existence of small sketches (distilled sets) for the input data with respect to the KRR problem with Gaussian kernel. However, the problem with this approach is that the feature space (in the Gaussian kernel corresponding mapping) is rather intangible or hard to map to, and sketch (distilled set) construction techniques require the representation of these points in the feature space.

To resolve this problem, we use a randomized approximated feature map, e.g., random Fourier features (RFF), and weighted random Fourier features (Weighted RFF). The dot product between every two mapped vectors in this approximated feature map aims to approximate their Gaussian kernel function . We now restate a result connecting ridge regression in the RFF space (or alternatively weighted RFF), and KRR in the input space.

**Theorem 2** (A result of the proof of Theorem 1 and Corollary 2 of ).: _Let \(^{n d}\) be an input matrix, \(y^{n}\) be an input label vector, \(k:^{d}^{d}[0,)\) be a shift-invariant kernel function, and \(^{n n}\), where \( i,j[n]:_{i,j}=k(_{i*},_{j*})\). Let \(>0\), and let \(d^{}_{}=Tr((+n_ {n})^{-1})\). Let \(s_{}(d^{}_{}(d^{}_{ }))\) be a positive integer. Then, there exists a pair \((,})\) such that (i) \(\) is a mapping \(:^{d}^{s_{}}\) (which is based on either the weighted RFF function or the RFF function ), (ii) \(}\) is a matrix \(}^{n s_{}}\) where for every \(i[n]\)\(}_{i*}:=(_{i*})\), and (iii) \((,})\) satisfies_

\[_{i=1}^{n}|y_{i}-f_{[},y,]}^{ }(}_{i*})|^{2} _{i=1}^{n}|y_{i}-f_{[,y,k]}^{}(_{i*} )|^{2}+4,\]

_where \(f_{[},y,]}^{}:^{s_{}} \) such that for every row vector \(z^{s_{}}\), \(f_{[},y,]}^{}(z)=z(} ^{T}}+ ns_{}_{s_{}} )^{-1}}^{T}y\). Note that, Table 1 gives bounds on \(s_{}\) when \(}\)._

Intution behind Theorem 2.Theorem 2 bounds the difference (additive approximation error) between (i) the MSE loss between the ground truth labels and the predictions obtained by applying Kernel Ridge regression (KRR) on the raw (original) data, and (ii) the MSE between the ground truth labels and the predictions obtained when applying Ridge regression on the mapped (full) training data via random Fourier features (_RFF_). Theorem 2 will be utilized to set the minimal dimension of the _RFF_ which yields the desired additive approximation, i.e., \(4\). Thus the intuition behind using this theorem is to link the dimension of the _RFF_ with the size of the distilled set. In other words, we use this error bound and sufficient size (of the minimal dimension of the _RFF_) to provide proof of the sufficient small size of the distilled set.

## 4 Main result: on the existence of small distilled sets

In what follows, we show that for any given matrix \(^{n d}\) and a label vector \(y^{n}\), there exists a matrix \(^{s_{} d}\) and a label vector \(y_{}^{r}\) such that the fitting solution in the RFF space mapping of \(\) is identical to that of the fitted solution on the RFF space mapping of \(\). With such \(\) and \(y_{}\), we proceed to provide our main result showing that one can construct a solution for KRR in the original space of \(\) which provably approximates the quality of the optimal KRR solution involving \(\) and \(y\). Thus, we obtain bounds on the minimal distilled set size required for computing a robust approximation, as well as bounds on the error for such a distilled set.

We now provide Theorem 3 followed by its proof of the existence of a small distilled set. Then we provide extensive details and intuitive explanations about the steps of the proof.

**Theorem 3** (On the existence of some distilled data).: _Let \(^{n d}\) be a matrix, \(y^{n}\) be a label vector, \(k:^{d}^{d}[0,)\) be a kernel function, \(=(0,1)\{2\}\), and let \(s_{}\) and \(}\) be defined as in Theorem 2. Assume that the rank of \(}\) is \(s_{}\), then, there exists a matrix \(^{s_{} d}\) and a label vector \(y_{}\) such that_

1. _the weighted RFF mapping_ \(}^{s_{} s_{}}\) _of_ \(\)_, satisfies that_ \((}^{T}}+ ns_{} _{s_{}})^{-1}}^{T}y=( }^{T}}+ ns_{} _{s_{}})^{-1}}^{T}y_{},\) _and_
2. _there exists an in-sample prediction_ \(f_{[,y_{},k]}^{,,y}\) _(not necessarily the optimal on_ \(\) _and_ \(y_{}\)_) satisfying_ \[_{i=1}^{n}|f_{[},y,k]}^ {}(_{i*})-f_{[,y_{},k]}^{ ,,y}(_{i*})|^{2} _{}(2\{,} \}+.\] (3) _and_ \[_{i=1}^{n}|y_{i}-f_{[,y_{},k]}^{,,y}(_{i*})|^{2} _{}\}}{n}_{i=1}^{n}|y_{i}-f_{[,y,k]}^{}( _{i*})|^{2}\] (4) \[+(4\{1+, \}+2\{,}\}).\]Proof.: Let \(\) be any matrix in \(^{s_{} d}\) such its weighted RFF mapping \(}\) is of rank equal to that of \(}\) and for every \(i[s_{}]\), \(_{j=1}^{n}k(_{i*},_{i*}) 0\).

**Proof of (i).** To ensure (i), we need to find a corresponding proper \(y_{}\). We observe that

\[(}^{T}}+ ns_{} _{s_{}})(}^{T}}+ ns_{}_{s_{}})^{-1} {}^{T}y=}^{T}y_{}\]

Let \(b=(}^{T}}+ ns_{} _{s_{}})(}^{T} {}+ ns_{}_{s_{}})^{-1} }^{T}y\), be the left-hand side term above. \(b\) is a vector of dimension \(s_{}\). Hence we need to solve \(b=}^{T}y_{}\) for \(y_{}\). Since \(\) has full rank then we have a linear system involving \(s_{}\) variables and \(s_{}\) equations. Thus, the solution to such a system exists and is \(y_{}=(}^{T})^{}b\), where \(()^{}\) denotes the pseudo-inverse of the given matrix.

**Proof of (ii).** Inspired by  and , the goal is to find a set of instances that their in-sample prediction with respect to the input data (\(\) in our context) would lead to an approximation towards the solution that one would achieve if the KRR was used only with the input data. To that end, we introduce the following Lemma.

**Lemma 4** (Restatement of Lemma 6).: _Under Assumption 1 and the definitions in Theorem 2, for every \(f\) with \(\|f\|_{} 1\), with constant probability, it holds that \(_{}\|\|_{2}\\ ^{s_{}}}_{i=1}^{n} |f(_{i*})-}_{i*}|^{2}  2\)._

Note that Lemma 4 shows that for every in-sample prediction function with respect to \(\), there exists a query \(^{s_{}}\) in the RFF space of that input data such that the distance between the prediction sample function in the input space and the in-sample prediction in the RFF space is at \(2\).

Furthermore, at  it was shown that \(\) is defined as \(=}}^{T}(} }^{T}+n_{s_{}})^{-1}[ ],\) where \([]_{i}=f(_{i*})\) for every \(i[n]\).

We thus set out to find an in-sample prediction function that is defined over \(\) such that by its infimum by Lemma 4 would be the same solution \(\) that the ridge regression on \(}\) attains with respect to the \(y\). Specifically speaking, we want to find an in-sample prediction \(f_{[,y_{},k]}^{,,y}( )\) such that

\[=}}^{T}(} }}^{T}+n_{s_{} })^{-1}_{}[],\] (5)

where (i) \(_{}[]^{n}\) such that for every \(i[n]\), \(_{}[]_{i}=f_{[,y_{},k ]}^{,,y}(_{i*})\), and (ii) \(f_{[,y_{},k]}^{,,y}( )=_{i=1}^{s_{}+1}_{i}k(_{i*}, )\) such that \(^{s_{}}\).

Hence we need to find an in-sample prediction function \(f_{[,y_{},k]}^{,,y}\) satisfying 5. Now, notice that \(^{s_{}}\), \(_{}[]^{n}\) and \(}^{T}(}} }^{T}+n_{n})^{-1}^{s_{ } n}\). Due to the fact that we aim to find \(f_{[,y_{},k]}^{,,y}\), such a task boils down to finding \(^{s_{}}\) which defines \(f_{[,y_{},k]}^{,,y}\) as in (ii). The above problem can be reduced to a system of linear equations where the number of equalities is \(s_{}\), while the number of variables is \(s_{}\).

To do so, we denote \(}}^{T}(}}}^{T}+n_{n})^{-1}\) by \(}\), and observe that we aim to solve

\[=}f_{}^{}[]=} _{i=1}^{s_{}}_{i}k(_{i*}, _{1*})\\ _{i=1}^{s_{}}_{i}k(_{i*},_{2*} )\\ \\ _{i=1}^{s_{}}_{i}k(_{i*},_{n*} ).\]We now show that every entry \(b_{j}\) (\(j[s_{}]\)) in \(\) can be rewritten as inner products between another pair of vectors in \(^{s_{}}\) instead of the inner product between two vectors in \(^{n}\). Formally, for every \(j[s_{}]\), it holds that

\[_{j}=}_{j*}_{i=1}^{s_{}} _{i}k(_{i*},_{1*})\\ _{i=1}^{s_{}}_{i}k(_{i*},_{2*} )\\ \\ _{i=1}^{s_{}}_{i}k(_{i*},_{n*} )=_{t=1}^{n}}_{j,t }k(_{1*},_{t*}),,_{t=1}^{n} }_{j,t}k(_{(s_{})*},_{t*}) _{1}\\ \\ _{s_{}}.\]

Thus, for every \(j[s_{}]\), define \(_{j*}=[_{t=1}^{n}}_{j,t}k( _{1*},_{t*}),,_{t=1}^{n}}_{j,t}k(_{s_{}*},_{t*})] ^{s_{}}\). In other words, \(A\) is a the result of a Hadamard multiplication of \(\) and a \(1\)-rank matrix \(G\) such that each of its rows is equal to \([_{t=1}^{n}k(_{1*},_{t*}), ,_{t=1}^{n}k(_{s_{}*},_{t*} )].\)

Since the rank of \(\) is full, i.e., \((A)=(}^{T}(}}}^{T}+n _{n})^{-1})=(})=s_{}\) by assumption, then it holds that

\[(A)=( G )=(D_{v}D_{u})=(),\]

where the first equality holds by definition of \(A\) and \(\) denoting the Hadamard multiplication product, the second inequality holds since by construction of \(\) and \(G=uv^{T}\) such that \(u,v^{s_{}}\) are vector with non-zero entries, and \(D_{u},D_{v}^{s_{} s_{}}\) are diagonal matrices where their diagonal are \(u\) and \(v\) respectively. The last inequality holds by property of rank function, i.e., the rank of any product of pair of square matrices \(C\) and \(D\) such that \(D\) is of full rank is equal to the rank of \(C\).

The right-hand side of (5) can reformulated as

\[}}^{T}(} }}^{T}+n_{n}) ^{-1}_{}[]=,\] (6)

where now we only need to solve \(=\). Such a linear system of equations has a solution since we have \(s_{}\) variables (the length of \(\)) and \(s_{}\) equations and the rank \(A\) is equal to \(s_{}\). For simplicity, a solution to the above equality would be \(:=()^{}\). To proceed in proving (ii) with all of the above ingredients, we utilize the following tool.

**Lemma 5** (Special case of Definition 6.1 from ).: _Let \(X\) be a set, and let \((X,\|\|_{2}^{2})\) be a \(2\)-metric space i.e., for every \(x,y,z X\), \(\|x-y\|_{2}^{2} 2(\|x-z\|_{2}^{2}+\|y-z \|_{2}^{2}).\) Then, for every \((0,1)\), and \(x,y,z X\),_

\[(1-)\|y-z\|_{2}^{2}-} \|x-z\|_{2}^{2}\|x-y\|_{2}^{2}}\|x-z\|_{2}^{2}+(1+)\|y -z\|_{2}^{2}.\] (7)

We note that Lemma 5 implies that \(x,y,z^{d}\)

\[\|x-y\|_{2}^{2}_{}\{,}\}\|x-z\|_{2}^{2}+\{1+,\}\|y-z\|_{2}^{2}.\] (8)

where for \(=2\) we get the inequality associated with the property of \(2\)-metric, and for any \((0,1)\), we obtain the inequality (7).

We thus observe that

\[_{i=1}^{n}|f_{[,y,k]}^{}( _{i*})-f_{[,y_{8},k]}^{,,y}( _{i*})|^{2}\] \[=_{i=1}^{n}|f_{[,y,k]}^{ }(_{i*})-f_{[},y,]}^{ }(}_{i*})+f_{[},y, ]}^{}(}_{i*})-f_{[,y_{8}, k]}^{,,y}(_{i*})|^{2}\] \[_{}}\}}{n}_{i=1}^{n}|f_{[,y,k]}^{} (_{i*})-f_{[},y,]}^{} (}_{i*})|^{2}+\] \[\}}{n}_{i=1}^{n}|f_{[},y,]}^{ }(}_{i*})-f_{[,y_{8},k]}^{ ,,y}(_{i*})|^{2}\] \[_{}2\{,}\}+2\{1+,\}\] \[=_{}(2\{,} \}+2\{1+,\}),\]

where the first equality holds by adding and subtracting the same term, the first inequality holds by Lemma 5, and the second inequality holds by combining the way \(f_{[,y_{8},k]}^{,,y}\) was defined and Theorem 2. Finally, to conclude the proof of Theorem 3, we derive 4

\[_{i=1}^{n}|y_{i}-f_{[,y_{8},k]}^{ ,,y}(_{i*})|^{2}=_ {i=1}^{n}|y_{i}-f_{[},y,]}^{}( }_{i*})+f_{[},y,]}^{ }(}_{i*})-f_{[,y_{8},k]}^{ ,,y}(_{i*})|^{2}\] \[_{}\}}{n}_{i=1}^{n}|y_{i}-f_{[ {},y,]}^{}(}_{i*})|^ {2}\] \[+}\}}{n} _{i=1}^{n}|f_{[},y,]}^{}( }_{i*})-f_{[,y_{8},k]}^{,,y}(_{i*})|^{2}\] \[+(4\{1+,\}+2\{,}\}),\] (9)

where the equality holds by adding and subtracting the same term, the first inequality holds by (8), and the second inequality follows as a result of the way \(f_{}^{}\) was constructed and the fact that \(\) is its infimum based on Lemma 4, and the last inequality holds by Theorem 2. 

**Intuition behind Theorem 3.** The goal of Theorem 3 is to prove the existence of a small distilled set \(\) (its size is a function of the minimal dimension of the RFF mapping required to ensure the provable additive approximation stated in Theorem 2) satisfying that: (i) The Ridge regression model trained on the mapped training data via RFF is identical to that of the Ridge regression model trained on the mapped small distilled set via RFF, (ii) more importantly there exists a KRR solution formulated for \(\) with respect to the loss of the whole big data \(\), which approximates the KRR solution on the whole data \(\) (which is the goal of KRR-based dataset distillation techniques). Thus, (iii) we derive bounds on the difference (approximation error) between (1) The MSE between the ground truth labels of the full data and their corresponding predictions obtained by the specific KRR model (we previously described) on our distilled set and (2) The MSE between the ground truth labels and the predictions obtained when applying KRR on the whole data \(\).

**The heart of our approach** lies in connecting the minimal dimension of the RFF required for provable additive approximation and the size of the distilled set. This is first done by showing that the distilled set can be any set \(S\) of instances from the input space (e.g., images) and their corresponding labels, as long as the corresponding labels must maintain a certain property. Specifically speaking, the labels of the distilled set need to be in correlation with the normal of the best hyperplane found to fit the mapped training data via RFF \(}\) via the Ridge regression model trained on \((},y)\), i.e., \((}^{T}}+ ns_{}  I_{s_{}})(}^{T}}+ ns_{} I_{s_{}})^{-1}}^{T}y= }^{T}y_{}\). From here, the idea hinges upon showing the existence of a KRR model (represented by a prediction function) that would be dependent on the prediction function that can be obtained from applying the Ridge regression problem to the mapped full training data via RFF. With such a model, the idea is to retrieve the predictions obtained when using the Ridge regression problem from the mapped training data via RFF via the use of some KRR model used on the distilled set. We thus show that through careful mathematical derivations, equation reformulation (involving ), and solving a system of equations, one is able to show the existence of a KRR solution that would allow us to use Theorem 2. Finally, to obtain our bounds, we also rely on the use of the weak triangle inequality. To that end, we now utilize the described KRR model on the distilled data together with Theorem 2 to achieve (iii).

**Remark 6**.: _Note that the assumption that \(}\) is of full rank (i.e., \(s_{}\)) can be dropped easily from Theorem 3, and as a result, we obtain that \(S\) can contain \(r\) (rank of \(}\)) instances (rows of \(\)). For additional details, please refer to Section E in the Appendix._

To simplify the bounds stated at Theorem 3, we provide the following remark.

**Remark 7**.: _By fixing \(:=2\), the bounds in Theorem 3 become_

\[_{i=1}^{n}|f_{[,y,k]}^{}(_ {i*})-f_{[,y_{8},k]}^{,,y}(_{ i*})|^{2} 8,\]

_and_

\[_{i=1}^{n}|y_{i}-f_{[,y_{8},k]}^{, ,y}(_{i*})|^{2}_{i=1} ^{n}|y_{i}-f_{[,y,k]}^{}(_{i*}) |^{2}+12.\]

_As for fixing \(:=(0,1)\), we obtain that_

\[_{i=1}^{n}|y_{i}-f_{[,y_{8},k]}^{, ,y}(_{i*})|^{2}_{i=1}^{n}|y_{i}-f_{[,y,k]}^{}(_{i*} )|^{2}+(4(1+)+ }).\]

## 5 Experimental Study

To validate our theoretical bounds, we performed distillation on three datasets: two synthetic datasets consisting of data generated from a Gaussian Random Field, and classification of two clusters, and one real dataset of MNIST binary and multi-class classification. Full experimental details for all experiments are available in the appendix.

**2d Gaussian Random Fields.** We first test our bounds by distilling data generated from the Gaussian Process prior induced by a kernel, \(k\) on 2d data. We use a squared exponential kernel with lengthscale parameter \(l=1.5\): \(k(x,x^{})=e^{-\|^{2}_{x}}{2l^{2}}}\). For \(\), we sample \(n=10^{5}\) datapoints from \((0,_{x}^{2})\), with \(_{x}[0.25,5.0]\). We then sample \(y(0,K_{XX}+_{y}^{2}I_{n})\), \(_{y}=0.01\). We fix \(=10^{-5}\) and distill down to \(s=d_{k}^{} d_{k}^{}\). The resulting values of \(d_{k}^{}\), \(s\), and compression ratios are plotted in fig. 2. We additionally plot the predicted upper bound given by Remark 7 and the actual distillation loss. Our predicted upper bound accurately bounds the actual distillation loss. To better visualize how distillation affects the resulting KRR prediction, we show the KRR predictive function \(f_{}^{}\) and the distilled predictive \(f_{}^{}\) for \(_{x}=5.0\) in fig. 0(b) and fig. 0(c).

**Two Gaussian Clusters Classification.** Our second synthetic dataset is one consisting of two Gaussian clusters centered at \((-2,0)\) and \((2,0)\), with labels \(-1\) and \(+1\), respectively. Each cluster contains 5000 datapoints so that \(n=10^{5}\). Each cluster as standard deviation \(_{x}[0.25,5.0]\). Additionally, two allow the dataset to be easily classified, we clip the \(x\) coordinates of clusters 1 andclusters 2 to not exceed/drop below \(-0.4\) and \(0.4\), for the two clusters, respectively. This results in a margin between the two classes. We visualize the dataset for \(n=300\) and \(=1.5\) in fig. 0(a). We use the same squared exponential kernel as before with \(l=1.5\), fix \(=10^{-5}\), and distill with the same protocol as before. We likewise plot \(d_{k}^{}\), \(s\), and compression ratios and distillation losses in fig. 3, again with our bound accurately containing the true distillation loss.

**Real Word Datasets Classification.** For our next test, we first consider binary classification on (i) MNIST 0 and 1 digits, (ii) SVHN 0 and 1 digit, and (iii) CIFAR-10 ship vs deer; all with labels \(-1\) and \(+1\), respectively. We use the same squared-exponential kernel with \(l=13.9\), \(l=3.0\), and \(l=8.0\), for MNIST, SVHN and CIFAR-10, respectively, which was chosen to maximize the marginal-log-likelihood, treating the problem as Gaussian Process regression. We vary \(n\), with an equal class split, and perform the same distillation protocol. Here, we additionally scale \(}\) such that \(=10^{-4}\) when \(n=5000\). Distilling yields fig. 4, fig. 5, and fig. 6, showing that our bounds can accurately predict distillation losses for real-world datasets. In fig. 7 and fig. 8 in the appendix, we test our bound for the multi-class case on MNIST 0,1,2 digits with similar settings as in the binary classification case. The results justify our bounds.

Figure 1: (a) visualizes the two clusters dataset with \(n=300\) and \(_{x}=1.5\). (b) and (c) visualizing the KRR predictive functions generated by the original dataset (b) and the distilled dataset (c) for the Gaussian Random Field experiment for \(_{x}=5.0\). The distilled dataset is able to capture all the nuances of the original dataset with a fraction of the datapoints.

Figure 3: Distillation results for synthetic data of two Gaussian clusters (n = 3)

Figure 2: Distillation results for synthetic data generated by a Gaussian Random Field (n = 3)

## 6 Conclusion

In this study, we adopt a theoretical perspective to provide bounds on the (sufficient) size and approximation error of distilled datasets. By leveraging the concept of random Fourier features (RFF), we prove the existence of small distilled datasets and we bound their corresponding excess risk when using shift-invariant kernels. Our findings indicate that the size of the guaranteed distilled data is a function of the "number of effective degrees of freedom," which relies on factors like the kernel, the number of points, and the chosen regularization parameter, \(\), which also controls the excess risk. In particular, we demonstrate the existence of a small subset of instances within the original input space, where the solution in the RFF space coincides with the solution found using the input data in the RFF space. Subsequently, we show that this distilled subset of instances can be utilized to generate a KRR solution that approximates the KRR solution obtained from the complete input data. To validate these findings, we conducted empirical examinations on both synthetic and real-world datasets supporting our claim. While this study provides a vital first step in understanding the theoretical limitations of dataset distillation, the proposed bounds are not tight, as seen by the gap between the theoretical upper bound and the empirical distillation loss in section 5. Future work could look at closing this gap, as well as better understanding the tradeoff between distillation size and relative error.