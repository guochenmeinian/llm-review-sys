# Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models

Deep Shankar Pandey \({}^{}\) Spandan Pyakurel \({}^{}\) Qi Yu

Rochester Institute of Technology

{dp7972,sp1468,qi.yu}@rit.edu

Corresponding author, \({}^{}\) equal contribution

###### Abstract

Large transformer-based foundation models have been commonly used as pre-trained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance. Parameter efficient fine-tuning (PEFT) provides promising generalization performance in adaptation while incurring minimum computational overhead. However, adaptation of these foundation models through PEFT leads to accurate but severely underconfident models, especially in few-shot learning settings. Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains. To fill out this critical gap, we develop a novel lightweight Bayesian Parameter Efficient Fine-Tuning (referred to as Bayesian-PEFT) framework for large transformer-based foundation models. The framework integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components. Our thorough theoretical analysis justifies that the Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple PEFT techniques demonstrate the outstanding prediction and calibration performance by Bayesian-PEFT.

## 1 Introduction

Transformer-based foundation models have been developed as general models with state-of-the-art generalization performance . These models leverage the rich meta-knowledge acquired during the pre-training stage to effectively adapt to complex downstream tasks , where pre-training is usually performed on massive-scale annotated datasets (_e.g.,_) through supervised learning or self-supervised learning . To achieve effective adaption, various parameter-efficient fine-tuning (PEFT) approaches have been developed  that introduce a small number of tunable parameters either within or outside of the backbone architecture to ensure good generalization capability while incurring little computational overhead because most parts of (or the entire) backbone architecture is frozen during fine-tuning . Bias-fine tuning , a representative partial tuning-based PEFT, only fine-tunes the bias parameters to downstream tasks. Adapter  and side-tune  fine-tuning techniques are instances of extra module-based PEFT that introduce extra parameterized modules and fine-tune them based on the downstream tasks. Visual Prompt-tuning  (VPT) follows the popular prompt learning paradigm by introducing a learnableprompt that is fine-tuned on the downstream task knowledge keeping the pre-trained backbone frozen.

Despite the attractive generalization performance, most foundation models adapted to downstream few-shot tasks through PEFT exhibit a somewhat surprising and undesirable behavior that may prevent them from being applied to many critical domains. Figure 1 (b) shows the predictive accuracy versus the Expected Calibration Error (ECE) of a foundation model after performing few-shot adaptation on CIFAR-100 using a series of representative PEFT methods, including VPT , Adapter , Bias Fine Tuning , and Side-tune . It is clear that the adapted model is able to provide accurate predictions even after fine-tuned on limited training samples. For example, all PEFT methods help to boost the model's accuracy to over 75% using just 5-shot fine-tuning and the accuracy reaches 80% after 10-shot fine-tuning. However, the adapted model is very poorly calibrated as shown by the large ECE scores, which are consistently over 0.3 across all the fine-tuning methods. Increasing the fine-tuning size does not show clear improvement and sometimes even hurts the calibration performance. While one may expect the poor ECE to be caused by over-confidence as we fine-tune a large foundation model using very limited training samples, the detailed ECE plots as shown in Figure 2 (a)-(c) reveal that the model is in fact severely under-confident. For example, after adapting to the 1-shot training dataset, the VPT fine-tuned model can already achieve a test accuracy close to \(50\%\) but is under-confident in almost all its predictions leading to an ECE score over \(0.45\). The under-confidence issue is observed for all representative PEFT methods across different datasets and various few-shot learning settings as evidenced by our experiments.

The under-confident few-shot adaptation behavior of foundation models closely mimics how human experts with rich domain knowledge in their own disciplines tend to make _conservative_ decisions when facing new tasks that deviate from their own expertise. Analogous to their human counterparts, the rich prior knowledge gained through the pre-training stage of foundational models overshadows the relatively limited knowledge obtained through few-shot fine-tuning, which prevents them from making more confident predictions in the downstream tasks. Unreliable uncertainty (_i.e.,_ confidence) quantification makes the predictions provided by these models less trustworthy, which may limit applying the promising "pre-train-then-finetune" paradigm to many critical domains. As shown in Figure 2 (a)-(c), the fine-tuned model seldom makes any predictions with confidence over 80%, making it difficult to leverage these predictions in any high-stakes decision-making process.

The need to balance between the rich prior knowledge gained through pre-training and the new knowledge obtained through few-shot adaptation inspires us to investigate the under-confidence issue from the Bayesian perspective. In particular, we propose to look into the predictive behavior of the few-shot adapted foundation model through the lenses of evidential learning , which is built upon Bayesian theorem and subjective logic (SL) theory . As part of the recent advances in modern Bayesian modeling, evidential learning provides a cost-effective way to perform Bayesian inference with the capability to quantify fine-grained second-order uncertainty . By leveraging the important

Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods

theoretical connection between fine-grained uncertainty and model accuracy , we unveil the underlying reason that supports the good predictive performance of a few-shot adapted foundation model and the root cause for the under-confident behavior. Drawing from this important insight as outlined above, we propose to integrate the modern PEFT techniques into a novel lightweight Bayesian framework, referred to as Bayesian-PEFT (B-PEFT), aiming to achieve highly reliable and accurate few-shot adaptations with well-calibrated and trustworthy uncertainty quantification.

The proposed Bayesian framework offers two important components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component makes novel adjustments to the base rates introduced by the SL theory to strengthen the prior belief corresponding to the knowledge gained through pre-training. Meanwhile, the adjustment does not change the relative order of the belief assigned to different classes, which ensures that the model accuracy is maintained. Our theoretical analysis shows that the proposed base rate adjustment strategy leads to more confident predictions (by increasing the gaps between the belief assigned to the ground-truth class and the rest) without compromising the model's accuracy. Figure 1(d) shows that B-PEFT significantly improves the model calibration. To further enhance the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adaptation, the second component performs Bayesian model averaging by building a diversity-inducing evidential ensemble. In addition to using different random initialization of the PEFT components, diversity is further enhanced through incorrect belief regularization that penalizes a model for assigning a high belief to a non-ground-truth class. By controlling the strength of belief regularization, different ensemble components are guided to learn from diverse features in the data space, where a light penalty allows an ensemble component to learn the common discriminative features while a heavy one will force an ensemble component to learn rare features to avoid errors on the difficult data samples. A deeper theoretical analysis of the proposed diversity-induced evidential ensemble is equivalent to Stein Variational Gradient Descent (SVGD) based ensembles . Experiments on multiple challenging few-shot learning tasks justify the superior performance of B-PEFT over state-of-the-art PEFT baselines, in terms of both prediction accuracy and uncertainty calibration performance. Our contributions can be summarized as follows:

* We identify the severe under-confidence issue of pre-trained foundation models after performing parameter-efficient fine-tuning over few-shot datasets. The fine-grained uncertainty analysis through evidential learning and SL theory reveals the root cause for their good predictive performance while being under-confident.
* We develop a novel lightweight Bayesian framework that integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components.
* We perform thorough theoretical analysis to justify why the proposed Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification.
* We carry out experiments with 4 benchmark datasets, 5 different few-shot settings, and 3 parameter efficient fine-tuning techniques that demonstrate the effectiveness of the developed model.

## 2 Related Works

Parameter Efficient Fine Tuning of Foundation Models.Transformer-based foundation models [63; 15] have been developed as an improvement over traditional convolution-based architectures

Figure 2: PEFT on the \(1\)-shot CIFAR100 dataset: all existing PEFT techniques exhibit severe under-confidence while the proposed B-PEFT reduces the ECE by almost an order of magnitude.

[29; 31] for computer vision tasks. The transformer-based models achieve strong generalization performance  after training on large datasets. Moreover, the pre-trained transformers can be fine-tuned in limited data settings leading to state-of-the-art performance [32; 27]. As a computation, memory, and parameter-efficient alternative to full fine-tuning of such large pre-trained transformers, different Parameter Efficient Fine Tuning (PEFT) approaches have been developed. PEFT techniques freeze most of the large transformer backbone, fine-tune the remaining backbone parameters and/or introduce lightweight extra modules for adapting to the downstream task. Existing approaches can broadly be categorized as extra-module-based [72; 54], partial-tuning-based [71; 9], and visual prompt tuning-based [68; 68; 28] methods. Extra-module-based methods (_e.g.,_ Adapter , side-tune) introduce small additional learnable modules and keep the pre-rained backbone frozen. Partial-tuning-based methods (_e.g.,_ Bias ) keep a large portion of the backbone frozen, and fine-tune only part of the foundation model to downstream tasks. Visual prompt-based PEFT methods (VPT) introduce a learnable prompt variable along with a learnable classification head over the fixed pre-trained backbone to be adapted to the downstream task. VPT  has shown significant improvement over other PEFT techniques, and can even outperform full fine-tuning in multiple datasets/settings .

Calibration in Deep Learning ModelsCalibration methods have been increasingly explored to achieve trustworthy deep-learning models. Post-hoc calibration methods [26; 43; 73] aim to learn a calibration map for a standard trained deep learning model such that the map can transform the poorly calibrated probabilities to calibrated probabilities. Regularization-based calibration approaches introduce explicit regularization (such as with L\({}_{2}\) regularization , entropy regularization ), or implicit regularization (such as with focal loss ) during training to ensure that the trained model is calibrated. Data augmentation methods such as Label smoothing , and mixup training  have also been explored for developing calibrated deep learning models. Recent survey  provides a discussion of the most relevant works towards developing calibrated deep learning models. Most existing calibration methods are designed to tackle the over-confidence issue, which is more commonly observed for large models trained from limited data due to overfitting. We observe that fine-tuned foundation models exhibit severe under-confidence in their predictions, where existing calibration techniques are less effective. To this end, we propose a lightweight Bayesian framework that fills this critical gap.

Few-Shot Adaptation and Relationship with Meta-Learning.In this work, we consider few-shot adaption with a focus on \(N\)-way \(K\)-shot classification [64; 22], where the model is presented with a few-shot training set with \(N\)-class, each having \(K\) examples. For instance, 1-shot Cifar100 training set consists of 1 sample from each of the 100 classes. The model is then evaluated on the test set, which is identical to the query set in the meta-testing tasks . It is worth to note that meta-learning (_e.g.,_ matching networks , MAML , VERSA , PLATIPUS ) leverages an episodic learning paradigm to achieve few-shot adaptation, where both meta-training and meta-testing are done on the task level in an episodic fashion [69; 37] with a large number of \(N\)-way \(K\)-shot training tasks. In this work, we consider more challenging few-shot adaptation tasks (_e.g.,_ 100-way 1-shot in Cifar100 and 102-way 1-shot in Flowers102) compared to the commonly used 5-way 1-shot meta-learning tasks. We leverage the power of the pre-trained foundation models, which eliminates the need of task based episodic meta-training. From a meta-learning perspective, the pre-training phase for the foundation model could be viewed as performing meta-knowledge acquisition similar to meta-training. The pre-trained model can be seen as an expert equipped with the meta-knowledge, and parameter-efficient fine-tuning performs quick adaptation to the downstream tasks, analogous to the support-set based adaptation done in meta-testing.

## 3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models

We start by introducing some fundamental concepts from evidential learning, which will serve as key building blocks in the proposed Bayesian-PEFT framework. We then detail the two Bayesian components: base rate adjustment to address under-confidence and diversity-inducing evidential ensemble to improve the reliability on both prediction accuracy and uncertainty quantification.

### Preliminaries

Evidential Deep Learning (EDL)  introduces a computationally efficient framework to transform deterministic deep learning (DL) models into uncertainty-aware models. The key idea is to introduce a higher-order conjugate prior distribution over the predicted likelihood distribution and train the DL model to output parameters of the higher-order distribution. Towards classification, EDL models[56; 11] introduce Dirichlet prior distribution for the multinomial likelihood distribution. Specifically, the output softmax layer of the DL model is replaced by a monotonic, non-negative transformation function (_e.g.,_ ReLU, SoftPlus, or \(\)) to obtain the evidence for different classes that are transformed into the Dirichlet parameters. Mathematically, for a DL model \(f_{}()\), and an input sample \(\), we have

\[e_{i}=f_{}()_{i}_{i}=e_{i }+a_{i} W\] (1)

where \(e_{i}\) is the output evidence for the \(i^{}\) class from the model \(f_{}()\) and input sample \(\), \(a_{i}\) is the base rate for the \(i^{}\) class, \(W\) is the non-informative prior weight usually set to the number of classes, \(\) is the non-negative transformation function, and \(_{i}\) parameterizes a Dirichlet distribution. Existing EDL works usually adopt a non-informative base rate of \(a_{i}= i[1,N]\). Furthermore, a multinomial distribution \(}(y|)\) over labels is parameterized as \([p_{i}]=}{S},\) where the total Dirichlet Strength \(S=_{i=1}^{N}_{i}\).

An evidential model can be trained via a Type-II Maximum Likelihood-based evidential loss \(^{}(,)\) with KL regularization that penalizes evidence assigned to non-ground-truth classes:

\[_{}(,)=^{}(, )+(|})|| (|)\] (2)

where \(}=+(-)\). Once trained, the evidential model can predict an evidence vector \(\) = \((e_{1},e_{2},...e_{N})^{}\) for a given test sample \(\). From the predicted evidence, we obtain the model's belief (\(\)) over different classes, the correct belief \(b_{}\), incorrect belief \(b_{}\), and vacuity \(u\) as

\[=}{S}, b_{}= , b_{}=(-) , u=,\] (3)

where vacuity \(u\) is a second-order uncertainty  that captures the model's lack of knowledge in its prediction; \(_{}\) and \(_{}\) quantify model accuracy and error, respectively. However, neither \(_{}\) nor \(_{}\) can be evaluated without the ground-truth label, which is not available in the testing phase. Existing theoretical work has established an important connection between \(_{}\) and dissonance , which is another second-order uncertainty  that can be quantified without the ground-truth label. More specifically, dissonance \(\) is can be evaluated as

\[}=_{n=1}^{N}b_{n}b_{j}Bal(b_{j},b_ {n})}{_{j n}b_{j}}, Bal(b_{j},b_{n})=2,b_{n})}{b_{j}+b_{n}},&b_{i}b_{j}>0\\ 0,&\] (4)

where \(Bal(,)\) is the relative mass balance function between two belief masses. The dissonance essentially captures the conflicting belief assigned to different classes .

### Strengthening the prior belief through base rate adjustment

To gain a deeper understanding of the under-confident few-shot adaptation behavior of foundation models, we perform fine-grained uncertainty analysis using the predicted evidence from an evidential model. To this end, we replace the softmax layer in a VPT fine-tuned transformer model with an exponential-based evidential head that outputs non-negative evidence for different classes. We analyze the output evidence from the evidential model that reveals some interesting insights.

Why is the model accurate?First, we observe that the relative order of the evidence assigned to different classes is accurate. This implies that the model outputs relatively greater evidence for the correct class compared to all other classes that ensure the model's strong predictive performance. To more precisely quantify the model's accuracy, we adapt the lower bound of incorrect belief theorem developed for meta-learning  to evaluate the model accuracy through its predicted dissonance:

**Theorem 1**.: _Consider an evidential model that outputs incorrect belief of \(b_{}\) and the dissonance in the beliefs is \(}\). Then, the incorrect belief of the model will be at least half of the dissonance for all predictions from the evidential model._

\[} b_{} 0 } 1\& 0 b_{} 1\] (5)

Figure 2(a) shows the test accuracy vs. dissonance curve, which is aligned with the relationship between the incorrect belief and the dissonance given in the theorem above, where a low dissonance implies a low \(b_{}\) (or a high accuracy). From all the testing samples, we observe relatively low dissonance and the highest is only slightly above 0.7 as shown in the figure. We further evaluate the Area Under the Curve of the Accuracy vs. \((1-})\) and obtain an AUC of 0.82 as shown in Figure 2(b). This implies the model is able to clearly discriminate the ground-truth label from the rest without much confusion (_i.e.,_ low dissonance) which ensures its good prediction accuracy.

Why is the model under-confident?Despite being able to assign relatively more evidence to the correct label over the rest, it is also interesting to observe that the model generally assigns very low evidence to all the labels, including the correct one. Figure 2(c) shows the evidence distribution of one representative test data sample from Cifar100. As can be seen, most classes are assigned very low evidence that is close to zero. The ground-truth class is assigned higher evidence, but it is far from sufficient to make the prediction confident. The resultant confidence is only 0.025 while the vacuity is extremely high at 0.875, implying that the model _believes_ it has very limited knowledge of the data sample despite it correctly identifying the correct label. Figure 2(d) shows the predicted vacuity over all the test samples, most of which are assigned a very high vacuity. This confirms the overly conservative behavior of the model, where _the low confidence is primarily due to the insufficient allocation of evidence_ in its predictions. On the other hand, since the model is fairly accurate, it is reasonable to believe that the model underestimates the contribution of the rich prior knowledge gained through pre-training.

Base rate adjustment to strength the prior belief.The fine-grained uncertainty analysis through the lenses of evidential learning not only explains the good predictive performance of the few-shot adapted model through PEFT but also unveils the root cause for its under-confident behavior, which is under-estimation of the contribution from the prior knowledge to the downstream task. While the classical Bayes' theorem offers a principal idea to address the issue, which is to strengthen the prior belief, there is a lack of practical way to achieve this. To this end, we propose to leverage the base rate introduced by the subjective logic theory  as an effective vehicle to adjust the prior belief gained through pre-training. According to (1), adjusting the base rate has the effect of changing the Dirichlet parameter \(\), which will change the confidence for the prediction given by \(_{i}[p_{i}]\). However, base rate adjustment needs to meet two key requirements: (1) the relative order of the Dirichlet parameters assigned to different classes should be preserved so that the predictive performance of the model remains unaffected, (2) the gap between the Dirichlet parameters for different classes is transformed such that the model becomes more confident in its predictions, making it well-calibrated. To meet these requirements, we we propose a transformation function \(_{m}\) to the model's output evidence such that the model is well calibrated without any compromise in the generalization performance:

\[=_{m}f_{}(})= +W,_{i}=a_{i}^{}= -e_{}}{e_{}}^{m}\] (6)

where \(=(_{1},_{2},..._{N})^{}\) is the adjusted base rate, and \(m 1\) controls the base rate transformation. The adjusted base rate \(\) considers evidence of all classes as a reference via \(e_{}\), and transforms the gap between different class evidences such that the model is well calibrated.

**Lemma 2**.: _The base-rate adjusted model that uses learnable base rate \(=(_{1},_{2},...,_{N})^{}\) has the same generalization performance compared to the model using fixed base rate of \(a_{i}= i[1,N]\)_

**Theorem 3**.: _For any \(m 1\), the transformation function \(_{m}\) transforms the base rate for the class with the highest evidence \(e_{}\) and class with the second highest evidence \(e_{}\) such that the gap in Dirichlet parameters between the two classes is non-decreasing._

**Remark**.: Theorem 3 ensures that the expected probability \([p_{i}]\) for the predicted class \(i\) has an increased gap with the rest of the classes, which results in an increase of the model's confidence. Therefore, if the prediction is accurate, the model's calibration performance will be improved. Meanwhile, Lemma 2 ensures that the good prediction accuracy of the model is maintained by the proposed base rate adjustment strategy. The detailed proofs are given in Appendix D.

### Building A Diversity Induced Evidential Ensemble

The second Bayesian component of the proposed B-PEFT framework aims to further improve the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adap

Figure 3: 1-shot Cifar10 results and evidence vacuity trends

tation. It performs Bayesian model averaging by building a diversity-inducing evidential ensemble. The ensemble of deep learning models (_i.e.,_ deep ensemble)  can effectively improve the generalization performance of deep learning models. Moreover, deep ensembles can capture the model uncertainty  via the agreement-disagreement between the ensemble components. Model uncertainty essentially captures the uncertainty in the model parameters, which is denoted as \(\) of the graphical model of B-PEFT as shown in Figure 4(b). The schematic diagram of B-PEFT is shown in Figure 4(a). The model uncertainty can be leveraged to evaluate the reliability of fine-grained uncertainty output by the evidential model.

The effectiveness of the ensembles has been empirically demonstrated across multiple datasets/settings  with theoretical guarantees . However, standard deep ensembling leads to limited diversity among the ensemble components as it only considers the random initialization of components. We propose a novel diversity-inducing ensembling scheme for the evidential models. Similar to the deep ensemble , we also consider randomly initialized evidential models. We additionally train each ensemble component with different strengths for incorrect evidence regularization along with evidential loss objective. The overall objective for each ensemble component is identical to (2).

However, each ensemble component is trained with different incorrect evidence (or belief) regularization strengths (_i.e.,_ different components place different priorities for the minimization of incorrect evidence over the maximization of correct evidence) which leads to diversity among the components. Since each component's priority for minimizing the incorrect evidence is different, the components focus on different attributes/features in the data that help the model avoid overfitting to an identical set of discriminative features. As a result, the proposed evidential ensembling scheme implicitly pushes the ensemble components away from each other, making it equivalent to the repulsive force in the Stein Variational Gradient Descent (SVGD) .

**Lemma 4**.: _For given incorrect evidence regularization \(_{}^{}\), and E ensemble components with regularization strengths \(_{p},p[1,P]\), the ensemble components in the evidence space are implicitly pushed away from each other by a force \(_{p}_{}^{}\) that acts identical to the repulsive force in Stein Variational Gradient Descent (SVGD) based ensembles._

Remark.The detailed proof is given in the Appendix. We present an intuitive visualization of the update of the evidential ensemble model for different strengths (\(_{1}<_{2}<...<_{P}\)) of incorrect evidence regularization for different seeds in Figure 5. Each ensemble component aims to maximize the likelihood (direction \(\)) and minimize incorrect

Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model

Figure 5: Illustration of ensemble diversity achieved through incorrect belief regularization with different strength

evidence (direction \(\)). The strengths of incorrect evidence regularization (direction \(\)) are different for each ensemble component that acts as an implicit repulsive force among the ensemble components, ensuring that they are diverse from each other. Different from the SVGD-based ensemble, in our proposed model, the particles do not need to explicitly communicate with each other making our proposed approach computationally efficient, scalable, and generalizable.

## 4 Experiments and Results

Experiment setup, datasets, and baselines.We consider \(K\)-shot adaptation (_i.e.,_ the dataset has \(K\) examples per class in the training set) with Cifar10 , Cifar100 , Food101 , and Flowers102  datasets. For instance, the \(2\)-shot Cifar100 dataset has \(2\) examples per class leading to a total of 200 labeled training samples. For all datasets and experiments, the training set is a few-shot dataset, and the evaluation is done on the standard test set available with benchmark datasets. Details of the few-shot training datasets along with additional experiment details are presented in the Appendix E. We consider large pre-trained vision transformer with ViT backbone  and consider Visual Prompt Tuning (VPT) , along with bias fine-tuning  and adapter fine-tuning  as the PEFT techniques (We use VPT as the representative PEFT where not specified due to its superior performance). We consider accuracy-preserving post-hoc calibration techniques including Temperature Scaling (TS) , Parameterized Temperature Scaling (PTS) , and Isotonic Regression (IR-MC)  as the baseline calibration techniques.

Prediction and calibration performance.We first consider standard cross-entropy (CE)-based PEFT of the supervised pre-trained ViT model on few-shot datasets. We present the accuracy and calibration results of VPT in Table 1 (a). We observe that the straightforward adaption of the models leads to accurate but under-confident models as indicated by a high ECE. The evidential models as shown in Table 1 (b) have comparable or better generalization performance across the datasets/settings. However, these models are also severely under-confident similar to CE-based models indicated by high ECE and accuracy-confidence trends (see Figure (a)a, (b)b in the Appendix). The overall performance of the calibrated evidential model using base-rate adjustment is presented in Table 1 (c). As can be seen, the accuracy remains the same as the adjusted base rate expands the gap between evidence of the class and preserves the relative order in the predicted class evidence. It effectively tackles the under-confidence issue, which leads to a significant improvement in the overall ECE performance across the datasets and settings (also see Figure (c)c in the Appendix). Table 1 (d) shows

    &  &  &  &  \\   & Accuracy \(\) & ECE \(\) & Accuracy \(\) & ECE \(\) & Accuracy \(\) & ECE \(\) & Accuracy \(\) & ECE \(\) \\   \\ 
1-Shot & \(69.578_{ 1.31}\) & \(0.437_{ 0.010}\) & \(48.637_{ 0.757}\) & \(0.393_{ 0.008}\) & \(35.702_{ 1.055}\) & \(0.263_{ 0.009}\) & \(88.161_{ 0.91}\) & \(0.61_{ 0.004}\) \\
2-Shot & \(81.771_{ 1.33}\) & \(0.400_{ 0.016}\) & \(64.501_{ 0.030}\) & \(0.494_{ 0.002}\) & \(53.954_{ 0.659}\) & \(0.39_{ 0.004}\) & \(93.462_{ 1.072}\) & \(0.55_{ 0.006}\) \\
5-Shot & \(88.707_{ 0.423}\) & \(0.255_{ 0.008}\) & \(76.758_{ 0.525}\) & \(0.517_{ 0.001}\) & \(65.856_{ 0.197}\) & \(0.424_{ 0.002}\) & \(97.363_{ 0.165}\) & \(0.472_{ 0.013}\) \\
10-Shot & \(91.061_{ 0.217}\) & \(0.212_{ 0.005}\) & \(80.720_{ 0.329}\) & \(0.501_{ 0.003}\) & \(71.566_{ 0.069}\) & \(0.444_{ 0.003}\) & \(98.244_{ 0.114}\) & \(0.439_{ 0.018}\) \\
20-Shot & \(92.678_{ 0.37}\) & \(0.166_{ 0.004}\) & \(82.608_{ 0.206}\) & \(0.487_{ 0.004}\) & \(74.914_{ 0.178}\) & \(0.460_{ 0.003}\) & \(98.431_{ 0.100}\) & \(0.425_{ 0.017}\) \\   \\ 
1-Shot & \(70.197_{ 1.013}\) & \(0.557_{ 0.011}\) & \(51.127_{ 0.045}\) & \(0.499_{ 0.004}\) & \(36.297_{ 1.407}\) & \(0.349_{ 0.014}\) & \(89.225_{ 1.03}\) & \(0.846_{ 0.004}\) \\
2-Shot & \(81.613_{ 1.716}\) & \(0.553_{ 0.01}\) & \(65.545_{ 0.349}\) & \(0.620_{ 0.004}\) & \(52.855_{ 0.551}\) & \(0.485_{ 0.005}\) & \(95.071_{ 0.413}\) & \(0.874_{ 0.006}\) \\
5-Shot & \(88.764_{ 0.896}\) & \(0.391_{ 0.015}\) & \(77.561_{ 0.716}\) & \(0.744_{ 0.006}\) & \(65.135_{ 0.27}\) & \(0.536_{ 0.005}\) & \(97.602_{ 0.199}\) & \(0.686_{ 0.02}\) \\
10-Shot & \(92.014_{ 0.353}\) & \(0.388_{ 0.006}\) & \(81.561_{ 0.291}\) & \(0.765_{ 0.002}\) & \(70.863_{ 0.261}\) & \(0.673_{ 0.003}\) & \(98.326_{ 0.233}\) & \(0.444_{ 0.008}\) \\
20-Shot & \(93.029_{ 0.239}\) & \(0.360_{ 0.015}\) & \(83.100_{ 0.184}\) & \(0.782_{ 0.001}\) & \(72.060_{ 0.309}\) & \(0.599_{ 0.003}\) & \(98.708_{ 0.014}\) & \(0.411_{ 0.013}\) \\    \\ 
1-Shot & \(70.197_{ 1.013}\) & \(0.027_{ 1.127}\) & \(0.035_{ 0.037}\) & \(0.074_{ 0.004}\) & \(36.297_{ 1.407}\) & \(0.081_{ 0.011}\) & \(89.225_{ 1.03}\) & \(0.025_{ 0.004}\) \\
2-Shot & \(81.613_{ 1.716}\) & \(0.040_{ 0.013}\) & \(65.545_{ 0.339}\) & \(0.08_{ 0.003}\) & \(52.855_{ 0.551}\) & \(0.063_{ 0.006}\) & \(95.071_{ 0.413}\) & \(0.023_{ 0.003}\) \\
5-Shot & \(88.764_{ 0.896}\) & \(0.028_{ 0.006}\) & \(77.561_{ 0.716}\) & \(0.044_{ 0.002}\) & \(65.135_{ 0.270}\) & \(0.037_{ 0.003}\) & \(97.602_{ 0.199}\) & \(0.015_{ 0.002}\) \\
10-Shot & \(92.014_{ 0.353}\

[MISSING_PAGE_FAIL:9]

7. We observe that when only a single component outputs a low vacuity for the OOD samples in Figure 7 (a-b), the variance of the ensemble is high, implying a high model uncertainty. When all the components output a high vacuity to the OOD sample in Figure 7 (c), the variance is also low, implying a low model uncertainty.

Ablation study.We carry out ablation with \(1\)-shot Cifar100 dataset to study the impact of the base rate transformation order \(m\) (Section 3.2) for different strengths of incorrect evidence regularization on the calibration performance. As we increase \(m\), the probability gap between classes improves, leading to more confident predictions. However, the model starts to become overconfident for large \(m\) values (see Figure 8).Moreover, with an increase in incorrect evidence regularization strength, the optimal \(m\) value decreases to a smaller value (_e.g.,_ optimal \(m\) = 2.0 for \(=0.1\), and optimal \(m=1.5\) for \(\) = 10). Choosing a proper \(m\) leads to the best-calibrated evidential model. We present additional results studying the impact of \(m\) in Appendix F.1.

Limited by space, we provide results comparing meta-learning methods on standard few-shot tasks in Appendix F.4, discuss impact of various components (number of classes, data size, and unfrozen parameters) in Appendix F.5, and include additional experiments and comparisons, including OOD settings, in Appendix F.6. Moreover, we carry out additional ablation experiments to study the impact of incorrect evidence regularization strength (Appendix F.2), and the impact of ensemble components (Appendix F.3). We further carry out experiments and discuss applying PEFT to foundation models pre-trained in a self-supervised fashion (Appendix G). We discuss the societal impact and limitations of the work in Appendices H and I, respectively.

## 5 Conclusion

In this work, we focus on transformer-based large vision foundation models and investigate different parameter-efficient fine-tuning techniques for effective few-shot adaptation. We observe that the existing models are severely under-confident, especially in challenging datasets and settings. Moreover, existing models lack fine-grained uncertainty quantification capabilities. We extend the models to uncertainty-aware evidential models, and resort to the evidential framework to develop a novel Bayesian parameter efficient fine-tuning (B-PEFT) framework that integrates evidence-based base rate adjustment to addresses the under-confidence and a diversity inducing evidential ensemble technique to further improve the reliability in model prediction and uncertainty quantification. The B-PEFT framework possesses theoretically sound properties to ensure its superior generalization capability and robust calibration behavior. We carry out intensive experiments across different benchmark datasets and diverse few-shot settings that demonstrate the outstanding performance of B-PEFT.

Figure 8: Impact of \(m\)

Figure 6: (a-b): Vacuity distribution of a single model and (c-d): variance distribution of ensemble models for 1/5 shots cifar10 as In-Distribution and Cifar100 as Out-of-Distribution dataset

Figure 7: Qualitative analysis of OOD samples for 1-shot adaptation of Cifar10 as ID dataset and Cifar100 as OOD dataset