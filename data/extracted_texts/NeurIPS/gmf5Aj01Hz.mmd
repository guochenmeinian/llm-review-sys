# SARAD: Spatial Association-Aware Anomaly Detection and Diagnosis for Multivariate Time Series

Zhihao Dai

Department of Computer Science

University of Warwick

Coventry, UK

zhihao.dai@warwick.ac.uk

&Ligang He

Department of Computer Science

University of Warwick

Coventry, UK

ligang.he@warwick.ac.uk

&Shuang-Hua Yang

Department of Computer Science

University of Reading

Reading, UK

shuang-hua.yang@reading.ac.uk

&Matthew Leeke

School of Computer Science

University of Birmingham

Birmingham, UK

m.leeke@bham.ac.uk

Corresponding Author

###### Abstract

Anomaly detection in time series data is fundamental to the design, deployment, and evaluation of industrial control systems. Temporal modeling has been the natural focus of anomaly detection approaches for time series data. However, the focus on temporal modeling can obscure or dilute the spatial information that can be used to capture complex interactions in multivariate time series. In this paper, we propose SARAD, an approach that leverages spatial information beyond data autoencoding errors to improve the detection and diagnosis of anomalies. SARAD trains a Transformer to learn the spatial associations, the pairwise inter-feature relationships which ubiquitously characterize such feedback-controlled systems. As new associations form and old ones dissolve, SARAD applies subseries division to capture their changes over time. Anomalies exhibit association descending patterns, a key phenomenon we exclusively observe and attribute to the disruptive nature of anomalies detaching anomalous features from others. To exploit the phenomenon and yet dismiss non-anomalous descent, SARAD performs anomaly detection via autoencoding in the association space. We present experimental results to demonstrate that SARAD achieves state-of-the-art performance, providing robust anomaly detection and a nuanced understanding of anomalous events.

## 1 Introduction

Time series anomaly detection is critical for industrial automation (Rieth et al., 2018), intrusion detection (Mathur and Tippenhauer, 2016), and healthcare sensing (Goldberger et al., 2000). Anomaly detection in these contexts is typically treated as an unsupervised learning problem, owing to the novelty of anomalies and the scarcity of labeled anomalies.

Temporal modeling is the mainstream basis of current time series anomaly detectors. By learning the dependencies between discrete time steps, temporal modeling can pinpoint the time spans of anomalies. During anomalies, unseen and peculiar temporal dependence patterns degrade data autoencoding (Wang et al., 2023) or autoregression (Zhao et al., 2020) performance, thereby enabling detection. Alternatively, irregular temporal representations can be driven to breach learned enclosinghyperspheres (Shen et al., 2020), resulting in high anomaly scores that enable detection. Despite its temporal precision in anomaly detection, temporal modeling either assumes feature independence or combines variables of diverse physical nature, the former simplifying the modeling and the latter mitigating the multicollinearity issue. Such assumptions lead to either omission or dilution of spatial information crucial to anomaly detection. Specifically, it overlooks the long-time-range spatial associations, the relationships between various features which ubiquitously characterize normal behaviors of multivariate time series. Where anomaly detection pinpoints the temporal locations of an anomaly, anomaly diagnosis identifies the spatial locations, i.e., the anomalous feature set, of an anomaly. Temporal methods also restrict diagnostic capabilities, as the lack of spatial information mismatches autoencoding-based or autoregression-based anomaly criterion, which de facto measures temporal novelty, with its objective of capturing spatial novelty.

Furthermore, time series anomalies frequently dissolve spatial associations, motivating anomaly detection in the association space. Using an vanilla Transformer (Vaswani et al., 2017), we investigate the changes in spatial associations throughout anomalies. Applied on transposed time windows (the spatial dimension comes before the temporal), an encoder-only Transformer is trained to minimize reconstruction errors on unlabeled \(N\)-variate time series and, by doing so, learns to model the multivariate series spatially via the Multi-Head Self-Attention (MHSA) illustrated in Figure 2. MHSA at each stacked \(l\)-th layer computes an intermediate association mapping \(_{h}^{l}^{N N}\) per \(h\)-th head, mapping back input \(\) to produce attention scores. The last layer's mapping \(_{h}^{L}\) thus effectively captures the contributions of \(k\)-th feature to the reconstruction of \(j\)-th feature at each location \((j,k)\), not least for its architectural proximity to the reconstructed output. Recent research (Liu et al., 2024) also highlights the important role MHSA plays in capturing the inter-feature associative relationships when applied on the multi-variate dimension. As new associations emerge and old ones dissolve over time, Figure 1 shows the association changes on a real-world benchmark. We observe that anomalies exhibit reductions for anomalous features, a phenomenon we herein coin as **Spatial

Figure 1: Spatial associations captured by Transformer on a service monitoring benchmark. 1a shows the raw time series right before, during, and after an anomaly (colored in red). Association mapping \(_{h}^{L}\) by final \(L\)-th layerâ€™s MHSA are averaged across heads to derive \(\) before (1b), during (1c), and after (1d) the anomaly. Darker cells have larger values. Anomalous features (#12 and #15) are highlighted with red bounding boxes. The reduction-only changes from before or after the anomaly to during the anomaly are shown in 1e and 1f, i.e., \((_{}-_{})\) and \((_{}-_{})\). The anomaly leads to association reductions on anomalous features, prominently column-wise on \(\).

Figure 2: MHSA.

**Association Reduction (SAR)**. The rationale is that anomalies either originate from or result in dissolution of pre-existing associations, detaching anomalous features from their non-anomalous counterparts. Additionally, we make the observation that SAR is most prominent column-wise on \(_{h}^{L}\), since each \(j\)-th column characterizes the dropouts of \(j\) from associating with other, mostly non-anomalous, features. Due to lack of explicit spatial information, temporal modeling is inadequate for exploiting SAR. More examples are given in Appendix C.

From a spatial modeling perspective, we propose SARAD to leverage spatial information and to exploit SAR for enabling robust time series anomaly detection and diagnosis. For quantifying anomalous spatial novelty in the data space, we train a Transformer on transposed time windows as an autoencoder. To capture the spatial association progression, the reduction-only changes of associations over time, the data reconstruction divides the input window by time into two halves to be processed in parallel. Consequently, the progression is the non-negative backward difference of the intermediate association mappings via MHSA. Subseries division circumvents memory storage of latest association mappings and enables time window shuffling during training, which reduces order bias, enhances generalization, and prevents catastrophic forgetting. For quantifying anomalous reduction novelty, we train a Multi-Layer Perceptron (MLP) as an autoencoder on progression in the association space. Whereas progression encompass all association reduction, autoencoding rules out those not caused by anomalies. The reconstruction errors via the data module measure data-only anomalous deviation from expected system behaviors and falter when such deviations are not prominent, e.g., at the start of an anomaly. The reconstruction errors via the progression module are sensitive to change in spatial associations, thus complementing the former. We develop a joint anomaly detection criterion that combines both. Experiments show SARAD delivers state-of-the-art detection and diagnosis performance with architectural elegance. Code is available at https://github.com/daidahao/SARAD/. We summarize our contributions as follows.

* We reveal and extract spatial association descending patterns of time series anomalies with a bespoke Transformer and subseries division. The former learns the pairwise inter-feature associations via autoencoding in the data space and the latter enables shuffled autoencoding training and memory-less progression aggregation.
* We propose progression autoencoding to quantify anomalous descent in the association space and a joint detection criterion in both data and association spaces, which complement each other.
* Experimentally, SARAD performs state-of-the-art anomaly detection and diagnosis on multivariate time series and ablation studies support our design choices.

## 2 Related Work

Influenced by the dominance of temporal modeling in time series forecasting (Wang et al., 2023; Zhang et al., 2023; Wu et al., 2021), temporal modeling is also prevalent in time series anomaly detection. Recurrent neural networks such as LSTM (Hochreiter and Schmidhuber, 1997) have innate capabilities for handling sequential data. These approaches use hidden states for past input memorization, enabling detection (Li et al., 2019; Malhotra et al., 2015) and diagnosis (Qian et al., 2021). Transformer (Vaswani et al., 2017) network is widely adopted (Fan et al., 2023; Xu et al., 2022) approach that is commonly applied to model temporal associations between different time points using its attention mechanism. Linear regression (Zeng et al., 2023) and MLP (Wang et al., 2024; Audibert et al., 2020) directly model temporal dependencies. TranAD (Tuli et al., 2022) replaces the MLP in Audibert et al. (2020) with a Transformer, making the detection criterion more robust through its adversarial training paradigm. Temporal modeling, however, is restricted by the exceptionally small receptive field in time and adversely impacted by the timestamp misalignment across features. In the context of anomaly detection, temporal modeling helps capture anomalous temporal associations (Xu et al., 2022; Yang et al., 2023), but offers limited detection capabilities in absence of spatial information. In a diagnostic context, temporal detectors mismatch anomaly criterion of temporal novelty with spatial interpretation.

Spatial associations characterize the multivariate time series commonly found in such supervisory systems for industrial control. The relationships range from strongly correlated, e.g., due to spatial proximity, to fully independent, e.g., due to mechanical disconnection. For forecasting, iTransformer (Liu et al., 2024) applies Transformer on the transposed time series to enable direct spatialmodeling. Crossformer (Zhang and Yan, 2023) screens the time series through custom Two-Stage Attention layers for more efficient spatial modeling. In terms of detection, GDN (Deng and Hooi, 2021) learns a directed graph of features for the prediction of last time points, whose errors serve as anomaly scores. GDN is partially limited by a mismatch between its singe-timestamp prediction target and the prevalent range-wise anomalies as well as unstable Top-K node selection during training. InterFusion (Li et al., 2021) learns compressed spatial and temporal dependencies, using a hierarchical Variational Auto-Encoder (Kingma and Welling, 2014) to reconstruct the series. Neither inspects temporal changes in associations throughout anomalies.

On another front, Isolation Forest (IF) models build a binary decision tree ensemble by partitioning either the data space (Liu et al., 2008) or the deep embedding space (Xu et al., 2023) formed by randomized neural networks. They are constrained by the lack of temporal and spatial (in the former case) or spatial (in the latter case) information, and their anomaly scores are not reflective of the degrees of anomalies.

We emphasize anomalous association descending patterns towards better time series detection and diagnosis. Different from previous work, we explicitly utilize the reduction in spatial associations over time during an anomaly, an insight we derived from the cyber-physical defense space. Dynamic watermarking (Satchandanan and Kumar, 2017) and similar defense techniques (Dai et al., 2023) overlay actuation with private signals to reveal attacks resulting in correlational breakdowns. While their approaches are intrusive and actively alter system behaviors, our detector remains non-intrusive, passively monitors the spatial associations, and is applicable to any supervisory system.

We refer to spatiality in this work as the multi-dimensional vector nature inherent to multivariate time series data. The terminology is also used in literature on time series related tasks (Gangopadhyay et al., 2021; Zheng et al., 2023). We note that spatiality may carry different meanings in other AI contexts, such as geographic positions or characteristics on Earth. We differentiate those meanings from our definition of spatiality, which traces its root to the spatial distribution of sensors and actuators in control systems where time series are routinely collected.

## 3 Method

The problems of anomaly detection and diagnosis are specified as follows.

Anomaly DetectionGiven a \(N\)-feature time series \(=\{x^{1},,x^{N}\}\) where \(x^{n}^{T}\) is of the same length \(T\), the objective is to predict the anomaly label \(y_{t}\{0,1\}\) at each timestamp \(t\).

Anomaly DiagnosisGiven the same time series \(\), the objective is to predict the diagnosis label \(g_{t}[N]\), the set of anomalous features at each timestamp \(t\).

### Overview

SARAD comprises two sequential modules; a Transformer for time series data reconstruction and a MLP network for spatial progression reconstruction. Table 1 decomposes the system framework of SARAD. The Transformer temporally divides by 2 and reconstructs the input time series to learn pairwise inter-feature associations and to enable order-free memory-efficient progression aggregation. The MLP reconstructs the aggregated progression to quantify anomalous association reduction while dismissing non-anomalous reduction. Towards robust anomaly detection, reconstruction errors from the two modules jointly serve as a criterion, sensitive to data-only anomalous deviation and anomalous association reduction.

### Data Reconstruction

In light of restricted capabilities of temporal detectors, here we adapt Transformer to spatially reconstruct the series data. The data module contains two components, Subseries Split & Merge and Subseries Reconstruction, shown in the first and second columns in Table 1. The former wraps around the second by temporally splitting a multivariate input series in half at its beginning and temporally merging at its end. Subseries division enables capturing of spatial progression within a single time window. Without the former, the model must store in memory the last association mappings at each step and keep to the time ordering during training, which is prone to overfitting and 

[MISSING_PAGE_FAIL:5]

representations. Notably within the MHSA with \(H\) heads as shown in Figure 2, each \((j,k)\)-th element on the association mapping \(_{h}^{N N}\) of its \(h\)-th head computes how much \(j\)-th feature's attention scores should originate from the \(k\)-th feature's key. From a broader perspective of data reconstruction, it is the quantification of the residual impact of \(k\)-th feature's originals on the \(j\)-th feature's reconstructions. We refactor MHSA implementation to enable parallel encoding of subseries.

Linear ProjectionOutput from the last encoding layer \(}_{i}^{L}\) is linearly projected and transposed to derive the reconstructed subseries \(}_{i}^{W N}\).

### Spatial Progression Reconstruction

To exploit SAR caused by anomalies, the module first extracts and aggregates the spatial progression, the non-negative backward difference of the association mappings via MHSA. In line with general anomaly detectors (Aggarwal, 2013), the module conducts autoencoding in the association space to quantify anomalous SAR and to dismiss non-anomalous SAR. Anomalous SAR occurs when, say, a compromised sensor's readings are no longer correlating with its spatially adjacent or mechanically related counterparts.

Association ProgressionWe define association progression \(_{h}^{l}^{N N}\) at the \(h\)-th attention head in the \(l\)-th layer to be the non-negative backward difference in association mappings \(\{_{1,h}^{l},_{2,h}^{l}\}\):

\[_{h}^{l}=(_{1,h}^{l}-_{2,h}^{l})\] (2)

where \(()\) passes through only non-negative values and outputs zeros otherwise.

Progression AggregationTo center the detection on association dropouts, we aggregate the column sums of progression \(_{h}^{l}\) from all attention heads in the final \(L\)-th layer to form \(}^{H N}\):

\[}=\{_{j=1}^{N}_{h,(j,k)}^{L}|h[H],k[N]\}\] (3)

We recall from the data module, each \(k\)-th column in \(_{i,h}^{l}\) quantifies the impact of \(k\)-th feature on all features' reconstruction. Taking the sum per each \(k\)-th column, we measure with \(}\) the dropout rates of \(k\) from participating all features' reconstruction. As we have observed in Section 1, SAR at the column level is indicative of time series anomalies, more so than at the row level. The last layer's progression is focused not least for its proximity to the final reconstructed output, whereafter no more information is exchanged between features. Liu et al. (2024) manifests that final layer's mappings resemble closely with the inter-feature correlations of the target, in our case, the reconstructed.

AutoencodingWith \(}\) flattened as an one-dimensional vector, a 2-layer MLP is trained to output the reconstructed and reshaped \(}}^{H N}\), synchronously with the data module training.

### Joint Training and Anomaly Detection

Training ObjectiveWe train an end-to-end model with a joint minimization objective:

\[L_{R}=||}-||_{2}^{2},\:L_{S}=||}}-}||_{2}^{2},\:L=L_{R}+_{L_{S}}L_{S}\] (4)

where \(L_{R}\) is the data reconstruction loss, \(L_{S}\) the progression reconstruction loss, and \(_{L_{S}}\) a weight hyper-parameter. Gradients are stopped from flowing into \(}\) to prevent updates to the data module and collapses in association representation. We are training two anomaly detectors simultaneously, one working in the original data space, the other in the spatial progression space.

Anomaly Detection CriterionFor an input series \(\), the anomaly score \(s\) is a scalar defined to be:

\[r=||}-||_{2}^{2},\:p=||}}-}||_{2}^{2},\:s=(r-_{r})/_{r}+(p-_{p})/_{p}\] (5)

where \(r\) is the data reconstruction error, \(p\) the progression reconstruction error, \(_{r},_{p}\) the means of \(r,p\) on the validation set, and \(_{r},_{p}\) the standard deviation of \(r,p\). The criterion takes into account the normalized errors in the data space and the progression space, each of which quantifies the anomalous magnitude in respective spaces and complment the other.

Anomaly Diagnosis CriterionFor an input series \(\) and its \(j\)-th feature, its anomaly score \(s_{j}\) is a scalar defined to be:

\[s_{j}=r_{j}=||}_{(j,)}-_{(j,)}||_{2}^{2},\] (6)

where \(r_{j}\) is feature \(j\)'s data reconstruction error. The criterion is sensitive to spatial novelty.

## 4 Experiments

SARAD is compared against state-of-the-art detectors on real-world benchmarks for detection and diagnosis, the latter only when diagnostic labels are available.

### Experimental Setup

DatasetsWe evaluate on four real-world datasets collected under industrial control and service monitoring settings. These dataset are: 1) Server Machine Dataset (**SMD**) (Su et al., 2019,b, a), 2) Pooled Server Metrics (**PSM**) dataset (Abdulaal et al., 2021,b) 3) Secure Water Treatment (**SWaT**) dataset (Mathur and Tippenhauer, 2016; iTrust, 2023), and 4) Hardware-In-the-Loop-based Augmented ICS (**HAI**) dataset (Shin et al., 2021,a). All training sets contain only unlabeled data and the test sets contain data with anomaly labels. Anomalies range from service outages to external cyber-physical attacks. We summarize the statistics of the datasets in Table 2. Descriptions of each dataset are detailed in Appendix E.

Detection MetricsReal-world benchmarks are rife with range-wise anomalies spanning consecutive time points (Wagner et al., 2023). We use the range-based metrics proposed in (Paparrizos et al., 2022). Compared against their point-based counterparts, they provide robustness to labeling delay and scoring noises as well as performant detector separability and series consistency. We compute the threshold-independent AUC-ROC and AUC-PR scores to be rid of thresholding impact and fully parameter-free Volume Under the Surface (VUS) AUC-ROC and AUC-PR scores. Full details are discussed in Appendix I.

Diagnosis MetricsConsistent with previous works (Tuli et al., 2022; Zhao et al., 2020), we use common metrics such as Hit Rate (HR) (Su et al., 2019) and Normalized Discounted Cumulative Gain (NDCG) (Jarvelin and Kekalainen, 2002) where diagnosis labels are available. At the range level, we measure the Interpretation Score (IPS) initially proposed in Li et al. (2021) and here expanded to fit the \(P\%\) parameterization. Full details are discussed in Appendix J.

BaselinesWe compare SARAD against state-of-the-art anomaly detection baselines, including Isolation Forest-based IF (Liu et al., 2008), Deep IF (DIF) (Xu et al., 2023); MLP-based USAD (Audibert et al., 2020); graph-based GDN (Deng and Hooi, 2021); LSTM-based MAD-GAN (Li et al., 2019); CNN-based DiffAD (Xiao et al., 2023); and Transformer-based TranAD (Tuli et al., 2022), ATF-UAD (Fan et al., 2023), AT (Xu et al., 2022), DCdetector (Yang et al., 2023). Noticeably, GDN employs explicit spatial modeling in its graph construction although spatial associations are not directly involved in anoamly scoring. MAD-GAN emphasizes on anomaly detection within cyber-physical systems. All baselines are trained using official implementations where available and recommended hyperparameters from respectively papers are used.

    & & Training & Test &  &  &  \\  Dataset & Features & Set & Set & Count & Ratio & min & med & max & Period \\  SMD & 38 & 708,405 & 708,420 & 327 & 4.16\% & 2 & 11 & 3,161 & 1 min \\ PSM & 25 & 132,481 & 87,841 & 71 & 27.73\% & 1 & 5 & 8,861 & 1 min \\ SWaT & 51 & 496,800 & 449,919 & 34 & 12.02\% & 101 & 447 & 35,900 & 1 sec \\ HAI & 79 & 921,603 & 402,005 & 50 & 2.23\% & 17 & 162.5 & 422 & 1 sec \\   

Table 2: Statistics of the main datasets.

### Results

Anomaly DetectionTable 3 shows the anomaly detection performance in metrics defined in Section 4.1. It demonstrates that, despite its architectural elegance, SARAD either outperforms all baselines by significant margins on the threshold-independent VUS-ROC scores (SMD: \(+15.51\)% MAD-GAN, HAI: \(+9.92\)% DiffAD) or performs on par with current best detectors (PSM: \(-1.36\)% GDN, SWaT: \(-0.36\)% DIF). IF scrutinizes the distributional shifts of anomalies with random data partitions and delivers consistent performance across datasets. DIF extends IF into randomized deep representation spaces and archives decent improvements due to more flexible partitions and temporally local information extraction via dilated convolutions. Temporal modeling methods such as DiffAD, ATF-UAD, and AT rely solely or heavily on reconstruction errors and when the errors do not correspond the the underlying anomalies their performance plummet. Adversarial training in USAD and MAD-GAN amplifies reconstruction errors of anomalies to mitigate but not eliminate such issues and thus suffer less performance drops. In contrast, our SARAD additionally accounts for the SAR frequent with anomalies and independent of data distributional shifts, thus outperforming all. SARAD also overpasses GDN, which despite its explicit spatial modeling adopts prediction errors as its sole detection criterion, limiting its performance. SARAD's top performance on SWaT and HAI underlines its ability to unravel complex spatial associations even in complex large-scale systems. Standard deviations of Table 3 are reported in Appendix K.

Anomaly DiagnosisTable 4 shows the anomaly diagnosis performance in metrics defined in Section 4.1. DiffAD uses a subset of SMD features and thus is discarded from comparisons for fairness. SARAD outperforms baselines on the point-based HR\(@150\%\) (SMD: \(+26.67\)% TranAD, SWaT: \(+5.10\)% USAD, HAI: \(+3.81\%\) GDN) and NDCG\(@150\%\) (SMD: \(+27.97\)% TranAD, SWaT: \(+4.76\)% GDN, HAI: \(+2.93\)% GDN). SARAD also outperforms on the range-based IPS\(@150\%\) on most datasets (SMD: \(43.19\)% TranAD, SWaT: \(33.43\)% TranAD). Unlike SMD which performs forensic diagnosis to label anomalous features, SWaT and HAI label only the origins of cyberattacks as diagnosis labels. Consequentially, attack origins sometimes might not behave anomalously, e.g., attacks had failed, or the full set of anomalous features were not identified, thus diminishing the performance numbers on SWaT and HAI. SARAD generally outperforms detectors underpinned by temporal modeling due to its sensitivity to spatial associative changes. SARAD also outperforms spatial detectors such as GDN whose prediction errors limit its temporal scope to a single time point. Standard deviations of Table 4 are reported in Appendix L.

VisualizationFigure 3 visualizes a real-world anomaly example via SARAD. Our detector captures the significant SAR caused by the anomalous features. The loose reconstruction of the progression raises the progression-based score \(p\) and, in turn, the joint detection score \(s\). Taking a broader view of the series in Fig. 2(h), SAR significantly raises the scores at the start of the anomaly, even when the data-based errors \(r\) are small. SARAD exploits SAR to achieve more robust anomaly detection.

    &  &  &  &  \\  Method & A\({}_{}\) & A\({}_{}\) & V\({}_{}\) & V\({}_{}\) & A\({}_{}\) & A\({}_{}\) & V\({}_{}\) & V\({}_{}\) & A\({}_{}\) & A\({}_{}\) & V\({}_{}\) & V\({}_{}\) & A\({}_{}\) & A\({}_{}\) & V\({}_{}\) & V\({}_{}\) \\  IF & 53.81 & 7.27 & 53.56 & 7.25 & 58.08 & **41.51** & 57.99 & **41.48** & 86.11 & 66.52 & 84.39 & 63.57 & 72.90 & 10.03 & 71.65 & 9.81 \\ DIF & 60.27 & 10.30 & 59.84 & 10.23 & 52.00 & 36.61 & 51.88 & 36.59 & **59.38** & **73.31** & **97.88** & 70.54 & 82.10 & 35.86 & 81.13 & 34.16 \\ TranAD & 46.86 & 5.92 & 46.54 & 5.88 & 50.20 & 35.22 & 49.47 & 35.19 & 77.18 & 76.55 & 75.60 & 25.80 & 75.06 & 25.41 \\ ATF-UAD & 43.41 & 4.98 & 43.10 & 4.97 & 46.44 & 32.30 & 46.03 & 33.18 & 55.18 & 20.66 & 54.35 & 20.58 & 70.56 & 25.47 & 69.81 & 21.06 \\ AT & 50.01 & 5.42 & 49.97 & 5.36 & 37.66 & 26.49 & 36.82 & 26.47 & 46.77 & 12.95 & 46.45 & 12.79 & 47.41 & 5.85 & 47.24 & 5.85 \\ DCdetector & 49.47 & 4.51 & 49.10 & 45.50 & 49.44 & 27.46 & 46.01 & 24.82 & 50.80 & 14.47 & 50.76 & 14.37 & N/A & N/A & N/A \\ USAD & 50.20 & 6.93 & 50.01 & 6.91 & 42.54 & 33.23 & 42.30 & 33.20 & 80.36 & 60.06 & 78.55 & 57.33 & 72.59 & 23.27 & 71.84 & 22.73 \\ GDN & 66.37 & 9.40 & 66.07 & 9.34 & **63.51** & 40.66 & **63.13** & 40.53 & 79.30 & 28.12 & 78.79 & 28.17 & 78.49 & 35.82 & 84.03 & 35.05 \\ MAD-GAN & 64.35 & 9.77 & 64.16 & 9.74 & 57.50 & 40.08 & 57.37 & 40.03 & 86.51 & 61.95 & 86.10 & 62.03 & 84.92 & 49.06 & 84.09 & 48.14 \\ DiffAD & 58.71 & 7.22 & 58.40 & 7.19 & 51.60 & 32.10 & 51.02 & 32.01 & 27.02 & 9.22 & 26.45 & 9.21 & 86.96 & 21.95 & 86.25 & 21.74 \\ 
**Ours** & **79.97** & **15.09** & **79.67** & **15.02** & 61.87 & 41.06 & 61.77 & 41.01 & 88.29 & 72.90 & 87.52 & **70.68** & **96.87** & **67.78** & **96.17** & **64.70** \\   

Table 3: Anomaly detection performance. Threshold-independent AUC-ROC and AUC-PR metrics and fully parameter-free VUS-ROC and VUS-PR metrics are reported. All values are average percentages from five random seeds. The best values are in **bold** and the second best underlined.

Complexity and Time OverheadsSARAD incurs 32 mins for training and and 0.39 ms for inference per sample on HAI, the largest dataset, falling far below the data collection time and sampling frequency. Those numbers are comparable with baselines and detailed in Appendix N.

### Ablation Studies

Spatial Progression ReconstructionTo evaluate the effectiveness of the progression module, we perform ablation studies on its submodules in Table 5. Standard deviations are reported in Appendix K. Removing the ReLU, i.e., to capture both association increases and reductions, in progression loses the focus on asscoation reduction and impairs the detection performance. Replacing the column sum operation in aggregation with the row sum which characterizes the disconnection of anomalous features from others and is shown to be less effectiveness than the column sum representing the drop out rates. Fully concatenating without sum operation dilutes the reduction patterns and significantly hurts the detection, at a cost of complexity. For the detection submodule, using the progression directly instead of the reconstruction errors registers reductions as anomalies directly and underperforms except on SMD due to its inability to rule out normal reduction patterns.

Choice of Detection CriterionTable 6 compares the detection performance using Eq. 5 (Joint), using only data-based \(r\) (DR), and using only progression-based \(p\) (SPR). While the data reconstruction is a robust criterion of anomalousness, SARAD embeds the spatial information into the joint criterion and outperforms either single criterion overall. Standard deviations are reported in Appendix K.

Additional ablation studies on the choice of diagnosis criterion are detailed in Appendix D.

    &  &  &  \\  Method & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & HS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) \\  \(P\) & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 \\  TranAD & 33.33 & 45.26 & 34.71 & 41.81 & 22.21 & 33.25 & 4.82 & 6.36 & 4.82 & 5.76 & 17.47 & 20.91 & 4.08 & 6.27 & 3.98 & 5.33 & **12.67** & **21.78** \\ ATE-UAD & 27.80 & 40.94 & 26.67 & 34.46 & 17.34 & 26.16 & 1.85 & 3.19 & 1.83 & 2.65 & 5.45 & 8.28 & 2.96 & 5.22 & 3.08 & 4.84 & 5.04 & 8.59 \\ USAD & 27.03 & 39.74 & 25.83 & 33.34 & 18.31 & 26.07 & 4.39 & 7.73 & 4.39 & 4.68 & 12.83 & 27.17 & 3.78 & 5.83 & 3.76 & 5.01 & 12.37 & 16.22 \\ GDN & 28.67 & 41.57 & 28.62 & 36.27 & 21.18 & 30.76 & 5.99 & 7.21 & 6.13 & 6.89 & 14.04 & 21.72 & 5.45 & 8.29 & 5.50 & 7.23 & 7.41 & 12.07 \\ DiffAD & N/A & N/A & N/A & N/A & N/A & N/A & 1.82 & 2.90 & 1.81 & 2.47 & 5.45 & 12.63 & 3.32 & 4.79 & 3.41 & 4.32 & 12.00 & 17.26 \\ 
**Ours** & **56.73** & **71.93** & **60.79** & **69.78** & **61.38** & **76.44** & **9.57** & **12.83** & **9.61** & **11.65** & **35.45** & **54.34** & **6.45** & **12.10** & **6.69** & **10.16** & 7.48 & 14.07 \\   

Table 4: Anomaly diagnosis performance. Point-based HR\(@P\%\), NDCG\(@P\%\), and range-based IPS\(@P\%\) are reported. All values are average percentages from five random seeds.

Figure 3: Visualization of applying SARAD for detection on SMD. 3a shows the raw time series right before and during an anomaly \(p_{i}\) (colored in red). An input time window for SARAD is bounded in the black box. 3b and 3c show the average association mapping \(^{L}\) via final \(L\)-th layerâ€™s MHSA. 3d shows the aggregated progression \(}\) according to Eq. 3. 3e is its reconstruction. 3f, 3g, 3h show the scores \(p\), \(r\), and joint \(s\) according to Eq. 5 per feature. 3i shows the anomaly scores for 3aâ€™s segment. Anomalous features (#1, #9, #10, #12, #13, #14,and #15) are highlighted with red bounding boxes.

[MISSING_PAGE_FAIL:10]

James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. 2011. Algorithms for Hyper-Parameter Optimization. In _Advances in Neural Information Processing Systems_, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (Eds.), Vol. 24. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf
* Dai et al. (2023) Zhihao Dai, Ligang He, Shuang-Hua Yang, and Matthew Leeke. 2023. Revealing Ongoing Sensor Attacks in Industrial Control System Via Setpoint Modification. In _2023 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)_. 0191-0199. https://doi.org/10.1109/DASC/PiCom/CBDCom/Cy59711.2023.10361334
* Deng and Hooi (2021) Ailin Deng and Bryan Hooi. 2021. Graph Neural Network-Based Anomaly Detection in Multivariate Time Series. _Proceedings of the AAAI Conference on Artificial Intelligence_ 35, 5 (May 2021), 4027-4035. https://doi.org/10.1609/aaai.v35i5.16523
* Fan et al. (2023) Jin Fan, Zehao Wang, Huifeng Wu, Danfeng Sun, Jia Wu, and Xin Lu. 2023. An Adversarial Time-Frequency Reconstruction Network for Unsupervised Anomaly Detection. _Neural Networks_ 168 (2023), 44-56. https://doi.org/10.1016/j.neunet.2023.09.018
* 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. 3560-3564. https://doi.org/10.1109/ICASSP39728.2021.9413914
* Garg et al. (2022) Astha Garg, Wenyu Zhang, Jules Samaran, Ramasamy Savitha, and Chuan-Sheng Foo. 2022. An Evaluation of Anomaly Detection and Diagnosis in Multivariate Time Series. _IEEE Transactions on Neural Networks and Learning Systems_ 33, 6 (2022), 2508-2517. https://doi.org/10.1109/TNNLS.2021.3105827
* Goldberger et al. (2000) Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. 2000. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. _circulation_ 101, 23 (2000), e215-e220.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long Short-Term Memory. _Neural Computation_ 9, 8 (1997), 1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735
* iTrust (2023) iTrust. 2023. Datasets. https://itrust.sutd.edu.sg/itrust-labs_datasets/.
* Jarvelin and Kekalainen (2002) Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumulated gain-based evaluation of IR techniques. _ACM Trans. Inf. Syst._ 20, 4 (oct 2002), 422-446. https://doi.org/10.1145/582415.582418
* Kang and Kang (2024) Hyeongwon Kang and Pilsung Kang. 2024. Transformer-based multivariate time series anomaly detection using inter-variable attention mechanism. _Knowledge-Based Systems_ 290 (2024), 111507. https://doi.org/10.1016/j.knosys.2024.111507
* Kim et al. (2023) Jina Kim, Hyeongwon Kang, and Pilsung Kang. 2023. Time-series anomaly detection with stacked Transformer representations and 1D convolutional network. _Engineering Applications of Artificial Intelligence_ 120 (2023), 105964. https://doi.org/10.1016/j.engappai.2023.105964
* Kingma and Ba (2015) Diederik Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In _International Conference on Learning Representations_.
* Kingma and Welling (2014) Diederik P Kingma and Max Welling. 2014. Auto-encoding variational bayes. In _International Conference on Learning Representations_.
* ICANN 2019: Text and Time Series_, Igor V. Tetko, Vera Kurkova, Pavel Karpov, and Fabian Theis (Eds.). Springer International Publishing, Cham, 703-716.
* Li et al. (2019)Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei. 2021. Multivariate Time Series Anomaly Detection and Interpretation using Hierarchical Inter-Metric and Temporal Embedding. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_ (Virtual Event, Singapore) _(KDD '21)_. Association for Computing Machinery, New York, NY, USA, 3220-3230. https://doi.org/10.1145/3447548.3467075
* Liu et al. (2008) Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation Forest. In _2008 Eighth IEEE International Conference on Data Mining_. IEEE, Pisa, Italy, 413-422. https://doi.org/10.1109/ICDM.2008.17
* Liu et al. (2024) Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. In _The Twelfth International Conference on Learning Representations_. https://openreview.net/forum?id=JePfAI8fah
* Liu et al. (2022) Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting. In _Advances in Neural Information Processing Systems_, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 9881-9893. https://proceedings.neurips.cc/paper_files/paper/2022/file/4054556fcaa934b0bf76da52cf4f92cb-Paper-Conference.pdf
* Malhotra et al. (2015) Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. 2015. Long Short Term Memory Networks for Anomaly Detection in Time Series.. In _Esann_, Vol. 2015. 89.
* Mathur and Dieppenhauer (2016) Aditya P. Mathur and Nils Ole Tippenhauer. 2016. SWaT: a water treatment testbed for research and training on ICS security. In _2016 International Workshop on Cyber-physical Systems for Smart Water Networks (CySWater)_. IEEE, Vienna, Austria, 31-36. https://doi.org/10.1109/CySWater.2016.7469060
* Nie et al. (2023) Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In _The Eleventh International Conference on Learning Representations_. https://openreview.net/forum?id=JbdcOvTOCol
* Paparrizos et al. (2022) John Paparrizos, Paul Boniol, Themis Palpanas, Ruey S Tsay, Aaron Elmore, and Michael J Franklin. 2022. Volume Under the Surface: A New Accuracy Evaluation Measure for Time-Series Anomaly Detection. _Proceedings of the VLDB Endowment_ 15, 11 (2022), 2774-2787.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In _Advances in Neural Information Processing Systems_, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
* Qian et al. (2021) Kai Qian, Jie Jiang, Yulong Ding, and Shuang-Hua Yang. 2021. DLGEA: a deep learning guided evolutionary algorithm for water contamination source identification. _Neural Comput. Appl._ 33, 18 (sep 2021), 11889-11903. https://doi.org/10.1007/s00521-021-05894-y
* Rieth et al. (2018) Cory A. Rieth, Ben D. Amsel, Randy Tran, and Maia B. Cook. 2018. Issues and Advances in Anomaly Detection Evaluation for Joint Human-Automated Systems. In _Advances in Human Factors in Robots and Unmanned Systems_, Jessie Chen (Ed.). Springer International Publishing, Cham, 52-63.
* Satchidanandan and Kumar (2017) Bharadwaj Satchidanandan and P. R. Kumar. 2017. Dynamic Watermarking: Active Defense of Networked Cyber-Physical Systems. _Proc. IEEE_ 105, 2 (2017), 219-240. https://doi.org/10.1109/JPROC.2016.2575064
* Shen et al. (2020) Lifeng Shen, Zhuocong Li, and James Kwok. 2020. Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network. In _Advances in Neural Information Processing Systems_, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 13016-13026. https://proceedings.neurips.cc/paper_files/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdfHyeok-Ki Shin, Woomyo Lee, Seungoh Choi, Jeong-Han Yun, and Byung-Gi Min. 2021a. HAI security datasets. https://github.com/icsdataset/hai.
* Shin et al. (2021) Hyeok-Ki Shin, Woomyo Lee, Jeong-Han Yun, and Byung-Gi Min. 2021b. Two ICS Security Datasets and Anomaly Detection Contest on the HIL-Based Augmented ICS Testbed. In _Cyber Security Experimentation and Test Workshop_ (Virtual, CA, USA) _(CSET '21)_. Association for Computing Machinery, New York, NY, USA, 36-40. https://doi.org/10.1145/3474718.3474719
* Su et al. (2019a) Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019a. OmniAnomaly. https://github.com/NetManAI0ps/OmniAnomaly.
* Su et al. (2019b) Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019b. Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_ (Anchorage, AK, USA) _(KDD '19)_. Association for Computing Machinery, New York, NY, USA, 2828-2837. https://doi.org/10.1145/3292500.3330672
* Tatbul et al. (2018) Nesime Tatbul, Tae Jun Lee, Stan Zdonik, Mejbah Alam, and Justin Gottschlich. 2018. Precision and Recall for Time Series. In _Advances in Neural Information Processing Systems_, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf
* Tuli et al. (2022) Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings. 2022. TranAD: deep transformer networks for anomaly detection in multivariate time series data. _Proc. VLDB Endow._ 15, 6 (feb 2022), 1201-1214. https://doi.org/10.14778/3514061.3514067
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In _Advances in Neural Information Processing Systems_, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
* Wagner et al. (2023) Dennis Wagner, Tobias Michels, Florian C.F. Schulz, Arjun Nair, Maja Rudolph, and Marius Kloft. 2023. TimeSeAD: Benchmarking Deep Multivariate Time-Series Anomaly Detection. _Transactions on Machine Learning Research_ (2023). https://openreview.net/forum?id=iMmsCI0JsSS
* Wang et al. (2023b) Chengsen Wang, Zirui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, Haifeng Sun, and Jianxin Liao. 2023b. Drift doesn't Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection. In _Advances in Neural Information Processing Systems_, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 10758-10774. https://proceedings.neurips.cc/paper_files/paper/2023/file/22f5d8e689d2a011cd8ead552ed5d59052-Paper-Conference.pdf
* Wang et al. (2023a) Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. 2023a. MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting. In _The Eleventh International Conference on Learning Representations_. https://openreview.net/forum?id=zt53IDUR1U
* Wang et al. (2024) Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, and JUN ZHOU. 2024. TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting. In _The Twelfth International Conference on Learning Representations_. https://openreview.net/forum?id=70LshfEIC2
* Wu et al. (2021) Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. In _Advances in Neural Information Processing Systems_, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 22419-22430. https://proceedings.neurips.cc/paper_files/paper/2021/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdfChunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, and Fan Zhou. 2023. Imputation-based Time-Series Anomaly Detection with Conditional Weight-Incremental Diffusion Models. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_ (Long Beach, CA, USA) _(KDD '23)_. Association for Computing Machinery, New York, NY, USA, 2742-2751. https://doi.org/10.1145/3580305.3599391
* Xu et al. (2018) Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin Qiao. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. In _Proceedings of the 2018 World Wide Web Conference_ (Lyon, France) _(WWW '18)_. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 187-196. https://doi.org/10.1145/3178876.3185996
* Xu et al. (2023) Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. 2023. Deep Isolation Forest for Anomaly Detection. _IEEE Transactions on Knowledge and Data Engineering_ 35, 12 (2023), 12591-12604. https://doi.org/10.1109/TKDE.2023.3270293
* Xu et al. (2022) Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy. In _International Conference on Learning Representations_. https://openreview.net/forum?id=LzQQ89U1qm_
* A framework for elegantly configuring complex applications. Github. https://github.com/facebookresearch/hydra
* Yang et al. (2023) Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2023. DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_ (<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>) _(KDD '23)_. Association for Computing Machinery, New York, NY, USA, 3033-3045. https://doi.org/10.1145/3580305.3599295
* Zeng et al. (2023) Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers Effective for Time Series Forecasting? 37 (Jun. 2023), 11121-11128. https://doi.org/10.1609/aaai.v37i9.26317
* Zhang et al. (2022) Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2022. TFAD: A Decomposition Time Series Anomaly Detection Architecture with Time-Frequency Analysis. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_ (Atlanta, GA, USA) _(CIKM '22)_. Association for Computing Machinery, New York, NY, USA, 2497-2507. https://doi.org/10.1145/3511808.3557470
* Zhang et al. (2023) Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Re. 2023. Effectively Modeling Time Series with Simple Discrete State Spaces. In _The Eleventh International Conference on Learning Representations_. https://openreview.net/forum?id=2EpjkjzdCAa
* Zhang and Yan (2023) Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting. In _The Eleventh International Conference on Learning Representations_. https://openreview.net/forum?id=vSVLMzj9eie
* Zhao et al. (2020) Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2020. Multivariate Time-Series Anomaly Detection via Graph Attention Network. In _2020 IEEE International Conference on Data Mining (ICDM)_. 841-850. https://doi.org/10.1109/ICDM50108.2020.00093
* Zheng et al. (2023) Yu Zheng, Huan Yee Koh, Ming Jin, Lianhua Chi, Khoa T. Phan, Shirui Pan, Yi-Ping Phoebe Chen, and Wei Xiang. 2023. Correlation-Aware Spatial-Temporal Graph Learning for Multivariate Time-Series Anomaly Detection. _IEEE Transactions on Neural Networks and Learning Systems_ (2023), 1-15. https://doi.org/10.1109/TNNLS.2023.3325667
* Zhou et al. (2022) Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting. In _Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162)_, Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu,and Sivan Sabato (Eds.). PMLR, 27268-27286. https://proceedings.mlr.press/v162/zhou22g.html
* Zhou et al. (2023) Tian Zhou, Peisong Niu, xue wang, Liang Sun, and Rong Jin. 2023. One Fits All: Power General Time Series Analysis by Pretrained LM. In _Advances in Neural Information Processing Systems_, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 43322-43355. https://proceedings.neurips.cc/paper_files/paper/2023/file/86c17de05579cde52025f9984e6e2ebb-Paper-Conference.pdfBroader Impacts

The broader impact of the work presented rests in the increasingly pervasive nature of industrial control systems, intrusion detection systems, and remote monitoring solutions in healthcare contexts, all of which commonly utilize some form of anomaly detection. Further to the time series analysis that is commonplace, the work presented in this paper demonstrates how Transformer can be used to learn the spatial associations that ubiquitously characterize these feedback-controlled systems, supplementing time series analysis to provide state-of-the-art performance. As such, the work presented has broad applicability, whilst explicitly targeting automated industrial control systems.

## Appendix B Limitations

While the model size scales linearly with the number of features, the time complexity of SARAD is quadratic with respect to the features. SARAD could incur significant training and inference overheads when the supervisory system is extensively large. We caution that the overheads of the largest dataset in our experiments fall well below data collection overhead and sampling frequency (see Appendix N). To scale, we will explore hierarchical time series anomaly detection via clustering. Another limitation of this work is the scarcity of forensically labeled datasets like SMD for anomaly diagnosis, not least due to the intensive labor and domain knowledge implied. To address that gap, we will explore publicly available audit and operational time series data for sources.

[MISSING_PAGE_FAIL:17]

Figure 6: Spatial associations captured by Transformer on SWaT (Mathur and Tippenhauer, 2016).

Figure 7: Spatial associations captured by Transformer on SWaT (Mathur and Tippenhauer, 2016).

## Appendix D Choice of Diagnosis Criterion

Concerning the rationality of data-only diagnosis criterion in Eq. 6, we consider an alternate joint diagnosis criterion in line with the detection criterion in Eq. 5.

\[r_{j}=||}_{(j,)}-_{(j,)}||_{2}^{2},\,p _{j}=||}_{(.j)}-}_{(.j )}||_{2}^{2},\,s_{j}=(r_{j}-_{r_{j}})/_{r_{j}}+(p_{j}-_{p_{j}})/ _{p_{j}}\] (7)

where \(r_{j}\) is feature \(j\)'s data reconstruction error, \(p_{j}\) its progression reconstruction error, \(_{r_{j}},_{p_{j}}\) the means of \(r_{j},p_{j}\) on the validation set, and \(_{r_{j}},_{p_{j}}\) the standard deviation of \(r_{j},p_{j}\). Table 7 compares the diagnosis performance using only \(r_{j}\) (SARAD), using only \(p_{j}\) (SPR), and using Eq. 7 (Joint). Unlike in anomaly detection, the great discrepancy between \(r_{j}\) and \(p_{j}\) more than often degrades the performance of the joint criterion. SARAD uses \(r_{j}\) only which produces suboptimal and yet reliable performance in the longer anomalous horizons. Standard deviations are reported in Appendix L.

    &  &  &  \\  Method &  & ND\(@\)\(p\%\) & IPS\(@\)\(p\%\) & HR\(@\)\(p\%\) & ND\(@\)\(p\%\) & IPS\(@\)\(p\%\) & HR\(@\)\(p\%\) & ND\(@\)\(p\%\) & IPS\(@\)\(p\%\) & IPS\(@\)\(@\)\(p\%\) & IPS\(@\)\(p\%\) \\  \(P\) & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 \\ 
**Ours** & \(r_{j}\) & **0\(\)** & **71.93** & **60.79** & **69.78** & **61.38** & **76.44** & 9.57 & 12.83 & 9.61 & 11.65 & **35.45** & **54.34** & 6.45 & 12.10 & 6.69 & 10.16 & 7.48 & 14.07 \\ 
**SPR** & \(p_{j}\) only & 42.97 & 57.33 & 46.32 & 54.85 & 52.01 & 64.82 & **14.32** & **17.18** & **14.40** & **16.18** & 20.10 & 37.37 & 5.71 & 8.50 & 5.86 & 7.58 & 9.70 & **17.04** \\ Joint & both & 48.91 & 61.49 & 53.12 & 60.59 & 56.56 & 70.

Datasets

We include four real-world datasets collected under industrial control and IT service monitoring settings for evaluations. Anomalies range from IT service outages to external cyber-physical attacks against control systems.

### Dataset Descriptions

1. **Server Machine Dataset (SMD)**Su et al. (2019, 2019) is a server metric dataset from a large-scale IT company. Engineers annotated anomalous events in the second half of the data with indicator-level attributions.
2. **Pooled Server Metrics (PSM)** dataset Abdulaal et al. (2021, 2021) captures key performance indicators of servers on an online shopping platform. Website engineers annotated anomalous events for data in the last eight weeks.
3. **Secure Water Treatment (SWaT)** dataset Mathur and Tippenhauer (2016); iTrust (2023) contains sensor readings and actuator status on a minuscule real-world water treatment system during a six-day normal operational period. A knowledgeable attacker performed 36 cyber-physical attacks during a five-day attack period and they are labelled as anomalous accordingly.
4. **HIL-based Augmented ICS (HAI)** dataset Shin et al. (2021, 2021) records measurements and control actions within a Hardware-In-the-Loop (HIL) dual power (steam-turbine and hydropower) generation testbed during its two-week operation. Both single-point primitive and multi-point combined attacks are performed on the testbed to emulate a threat actor with cyber-physical capacities. We use the 21.03 version of HAI.

All training sets contain only unlabeled data and the test sets contain data with anomaly labels. Statistics of the datasets are given in Table 2.

### Lengths of Anomalies

We further characterize the detection datasets by the lengths of the anomalous events. Figure 8 shows the empirical cumulative distribution function of the anomalous lengths. SWaT has the longest median length of 447 among the four datasets considered, followed by HAI (162), SMD (11), and lastly PSM (5). The very short lengths on SMD and PSM benefit temporal detectors which tend to embed a single or few time points (see Appendix M), whereas SARAD adopts a half time window embedding strategy. The catch is that SARAD can learn spatial relationships with temporal aggregated information per feature, while temporal detectors could not, bringing about benefits of performing anomaly detection in the spatial association space. A more scalable approach for temporal aggregation is to be explored in the future, though variable window sizes or subseries splits might be implied.

### Lengths of Diagnosis Labels

Figure 8 shows the empirical cumulative distribution function of the diagnosis label lengths. Whereas SMD forensically labels features which deviate from their normal behavioral patterns as anomalous, SWaT and HAI only label the points of attacks as anomalous as the their creators have advanced knowledge of such attacks. The latter labeling strategy results in incomplete sets of anomalous features and diminishes the diagnosis performance of all models, as evidence in Table 4 in Section 4.2.

Figure 8: Empirical distribution function of the lengths of anomalies.

Figure 9: Empirical distribution function of the lengths of diagnosis labels.

Implementation Details

We implement SARAD in Python using pyTorch library (Paszke et al., 2019) and Hydra framework (Yadan, 2019). All experiments are run on a single NVIDIA A10 (24GB) GPU. Adam optimizer (Kingma and Ba, 2015) is used and learning rate is halved every epoch for 3 epochs to prevent over-fitting. The time window size is \(2W=100\). The data reconstruction module has \(H=8\) attention heads per layer with attention length \(D=512\) and hidden length \(D_{FF}=2048\). For hyperparameter tuning, training set is temporally partitioned into 80% for training and 20% for validation. On each dataset we first perform TPE sampling (Bergstra et al., 2011) for number of encoding layers \(L\{3,5\}\) and learning rate \([10^{-4},10^{-2}]\) to derive the best data reconstruction loss \(L_{R}\) on the validation set. The progression module by default has hidden length of \(D_{P}=64\). We then perform TPE sampling to search weight \(_{L_{S}}[10^{-2},10^{2}]\) for the progression reconstruction loss \(L_{S}\) on the validation set.

## Appendix G Open-accessed Code and Data

During the review period, code is anonymized and openly available at https://github.com/daidahao/SARAD/ with specific instructions and scripts to reproduce experimental results. All data used in our experiments can be openly accessed from public repositories or requested via original authors' websites. Full links are provided in Appendix E.

## Appendix H Hyperparameter Sensitivity

We examine the hyperparameter sensitivity of SARAD's detection performance. Concretely, we consider the effects of the sliding window size (default is \(100\)), the number of training epochs (3), the attention length of Transformer encoding layers \(D\) (\(512\)), and the hidden length of the progression reconstruction module \(D_{P}\) (\(64\)). Figure 10 and 11 present the results. On the window size, a sliding window too small confines temporal modeling to small temporal receptive fields and contains anomaly detection in the association space. However, a sliding window too large incurs higher computational costs, although unlike temporal modeling the costs here are linear. On datasets with shorter anomalous lengths such as SMD and PSM, the anomalous patterns are diluted even further, resulting in performance degradation.

On the number of training epochs, fewer epochs lead to model underfitting, and yet overfitting is largely prevented with more epochs due to the aggressive learning rate halving per epoch. On the attention length \(D\), a larger Transformer is prone to overfitting with visible performance drops on SMD and PSM as \(D\) passes the default \(512\), both of whose monitored systems are smaller in scale. On the hidden length \(D_{P}\), a more complex progression anomaly detector does not adversely impact the performance, suggesting that the association space is less prone to detection overfitting than the data space.

Figure 10: Hyperparameter Sensitivity of detection performance in VUS-ROC scores.

Figure 11: Hyperparameter Sensitivity of detection performance in VUS-PR scores.

Detection Metrics

Conventional metrics such as precision, recall, F1 and the threshold-independent Area Under the Curve (AUC) scores are commonly point-based, i.e., predicted labels are scored individually by time points (Zhou et al., 2023; Zhang et al., 2022; Xu et al., 2018). Real-world benchmarks are rife with range-wise anomalies spanning consecutive time points (Wagner et al., 2023). Point-based metrics are generally ill-suited for evaluating detection performance due to the continuous-discrete conversion and the series-label misalignment (labeling anomalies precisely is hard) (Garg et al., 2022; Tatbul et al., 2018). Here, we use the range-based metrics proposed in (Paparrizos et al., 2022). Compared against their point-based counterparts, they provide robustness to labeling delay and scoring noises as well as performant detector separability and series consistency.

Given a set of anomalous ranges \(=\{p_{i}=(s_{i},e_{i})\}\) wherein each anomaly \(p_{i}\) starts at timestamp \(s_{i}\) and ends at \(e_{i}\), we enclose each range with uniform \(l/2\)-length preceding and succeeding buffers. Given the anomaly label \(y_{t}\{0,1\}\) at each timestamp \(t\), we derive a new soft label \(_{t}\) as per the minimum temporal distance of \(t\) to any anomaly \(p_{i}\):

\[_{t}=-t|/l},& p_{i}, t[s_{i}-l/2,s_{i})\\ |/l},& p_{i},t(e_{i},e_{i}+l/2]\\ y_{t},&\] (8)

where \(l\) is the buffer length, normally set to the median segment length in \(\). Within the buffers, \(_{t}\) monotonically increases from \(/2\) to 1 as the distance decreases. With the new soft label series \(}=\{_{t}\}\), we define the True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) accordingly:

\[TP=}^{T}},\,FP=(1-})^ {T}},\,TN=(1-})^{T}(1-}),\,FN=}^{T}(1-})\] (9)

We then compute the threshold-independent AUC for the Receiver Operating Characteristic (AUC-ROC), i.e., TP rate vs FP rate, and the Precision-Recall (AUC-PR) curves respectively to be rid of thresholding impact. Fully parameter-free Volume Under the Surface (VUS) scores for AUC-ROC (VUS-ROC) and AUC-PR (VUS-PR) are also computed under different buffer lengths \([0,2l]\).

Diagnosis Metrics

In line with previous works (Tuli et al., 2022; Zhao et al., 2020), we use common metrics such as Hit Rate (HR) (Su et al., 2019) and Normalized Discounted Cumulative Gain (NDCG) (Jarvelin and Kekalainen, 2002) to measure performance where diagnosis labels are available. Given a set of anomalous features \(G_{i}[N]\) at an anomalous timestamp \(t p_{i}\) as a diagnosis ground-truth and the set of top \(k\)-ranked features \(_{t}@P\%\) according to Eq. 5 where \(k=|G_{i}| P\%\), say \(k=5\) when \(|G_{i}|=3\) and \(P=150\), the HR at \(P\%\) (\(P 100\)) features is the overlap ratio between the two:

\[_{t}@P\%=_{t}@P\%|}{|G_{i}|}\] (10)

In information retrieval, DCG measures the cumulative utility of retrieved documents by their ranking order up to a certain position. NDCG normalizes the DCG by the maximum possible DCG. They are parameterized by \(P\%\) to determine the location in our evaluation and calculated as follows.

\[_{t}@P\%=_{j=1}^{k}}{_{2}(j+1)},\,_{ t}=_{j=1}^{|G_{i}|}(j+1)},\,_{t}@P\%=_{t}@P\%}{_{t}}\] (11)

where \(r_{j}\{0,1\}\) is the relevance value of the \(j\)-th element and, in this case, the membership of \(_{t}@P\%\)'s \(j\)-th feature in \(G_{i}\). NDCG has a value strictly between \(0\) and \(1\).

At the range level, we measure the **In**r**etration Score (IPS) initially proposed in Li et al. (2021) and here expanded to fit the \(P\%\) parameterization. For each anomalous range \(p_{i}\), the IPS score is:

\[_{i}@P\%=_{i}@P\%|}{|G_{i}|}\] (12)

where \(_{i}@P\%\) is the top \(k\)-ranked features according to \(_{t p_{i}}s_{j,t}\), the \(j\)-th feature's maximum anomaly score during \(p_{i}\) and \(k=|G_{i}| P\%\). It is the HR equivalence at the range level as per the highest anomalous scores per feature.

[MISSING_PAGE_EMPTY:27]

## Appendix L Standard Deviations of Diagnosis Diagnosis

Table 11 reports the standard deviations of anomaly diagnosis performance as reported in Table 4 in Section 4.2.

Table 12 reports the standard deviations of anomaly diagnosis performance as reported in Table 7 in Appendix D.

    &  &  &  \\  Method & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & IPS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) \\  \(P\) & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 \\  TranAD & 0.08 & 0.29 & 0.22 & 0.20 & 0.25 & 0.69 & 1.10 & 2.50 & 1.13 & 1.99 & 4.25 & 7.37 & 0.84 & 2.16 & 0.54 & 1.36 & 1.47 & 3.07 \\ ATF-UAD & 1.62 & 1.68 & 2.60 & 2.39 & 2.67 & 2.73 & 1.19 & 2.45 & 1.17 & 1.92 & 4.42 & 5.81 & 2.37 & 4.39 & 2.68 & 3.93 & 5.06 & 6.52 \\ USAAD & 0.37 & 0.81 & 0.90 & 1.06 & 0.50 & 0.77 & 1.73 & 1.54 & 1.75 & 1.59 & 5.77 & 3.06 & 0.52 & 0.27 & 0.72 & 0.56 & 2.04 & 1.98 \\ GDN & 1.41 & 1.27 & 1.63 & 1.57 & 1.08 & 1.20 & 1.36 & 1.33 & 1.36 & 1.32 & 2.51 & 3.15 & 2.08 & 2.59 & 2.38 & 2.70 & 2.37 & 6.06 \\ DiffAD & N/A & N/A & N/A & N/A & N/A & N/A & 0.09 & 0.16 & 0.09 & 0.12 & 3.18 & 3.11 & 0.37 & 0.52 & 0.35 & 0.44 & 0.67 & 1.45 \\ 
**Ours** & 0.46 & 0.46 & 0.56 & 0.58 & 1.61 & 0.96 & 0.98 & 1.68 & 1.03 & 1.45 & 4.20 & 2.52 & 0.64 & 0.96 & 0.74 & 0.88 & 1.71 & 1.57 \\   

Table 11: Standard deviations of anomaly diagnosis performance in Table 4. Standard deviations of point-based HR\(@P\%\), NDCG\(@P\%\), and range-based IPS\(@P\%\) are reported. All values are percentages.

    &  &  &  \\  Method & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) & HR\(@P\%\) & ND\(@P\%\) & IPS\(@P\%\) \\  \(P\) & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 & 100 & 150 \\ 
**Ours** & 0.46 & 0.46 & 0.56 & 0.58 & 1.61 & 0.96 & 0.98 & 1.68 & 1.03 & 1.45 & 4.20 & 2.52 & 0.64 & 0.96 & 0.74 & 0.88 & 1.71 & 1.57 \\  SPR & 1.38 & 1.58 & 1.28 & 1.36 & 2.05 & 2.55 & 21.26 & 24.13 & 21.31 & 23.10 & 8.55 & 10.41 & 0.95 & 1.35 & 0.99 & 1.20 & 2.94 & 4.36 \\ Joint & 1.63 & 1.27 & 1.60 & 1.39 & 1.73 & 2.46 & 0.20 & 0.92 & 0.16 & 0.62 & 2.83 & 5.36 & 0.63 & 1.20 & 0.65 & 0.94 & 1.72 & 1.94 \\   

Table 12: Standard deviations of anomaly diagnosis performance in Table 7. Standard deviations of point-based HR\(@P\%\), NDCG\(@P\%\), and range-based IPS\(@P\%\) are reported. All values are percentages.

[MISSING_PAGE_FAIL:29]

## Appendix N Model Complexity and Overheads

We study and compare the complexity and time overheads of all baselines and SARAD. Concretely, we evaluate the model complexity by the number of parameters being used and the total training time as well as the inference time per sample. All experiments on time overheads are performed on a compute node with AMD EPYC 7443 (48 cores, 96 threads) CPU, NVIDIA A10 (24GB) GPU, and 512 GB RAM.

Table 15 reports the total training time (in minutes), the inference time per sampling point, and the number of network parameters used (where applicable) of all baselines on the main datasets. The size of the training set \(T\), the number of features \(N\), and the sampling period \(\) are also reported per dataset for easy reference. SARAD, while not the fastest nor the lightest model, incurs moderate time overheads and model complexity.

We note that even SWaT, the smallest dataset in terms of actual clock time, spans approximately 6 days for collection. This is far exceeding most detectors' training time besides MAD-GAN and levigates concerns for training overheads for them. All detectors also incur an inference per sample time several magnitudes below the smallest sampling frequency of 1 second, guaranteeing real-time deployment of all detectors once trained.

    &  SMD \\ \(T=708K\) \\ \(N=38\) \\ \(=1\) min \\  } &  PSM \\ \(T=132K\) \\ \(N=25\) \\ \(=1\) min \\  } &  SWaT \\ \(T=497K\) \\ \(N=51\) min \\  } &  HAI \\ \(T=922K\) \\ \(N=51\) \\ \(=1\) s \\  } \\   &  Train. Infer. Param. \\ (mins) \\  } &  Param. \\ (mins) \\  } &  Train. Infer. Param. \\ (ms) \\  } &  Train. Infer. \\ (ms) \\  } &  Param. \\ (mins) \\  } &  Train. Infer. \\ (ms) \\  } &  Param. \\ (mins) \\  } \\  IF & **0.01** & 0.01 & N/A & **0.00** & 0.01 & N/A & **0.01** & 0.01 & N/A & **0.03** & 0.01 & N/A \\ DIF & 10.13 & 1.66 & 874K & 1.91 & 1.52 & 853K & 7.67 & 1.68 & 895K & 15.47 & 1.87 & 940K \\ TranAD & 6.80 & 0.04 & 127K & 0.96 & 0.03 & 57K & 6.06 & 0.05 & 226K & 17.15 & 0.07 & 531K \\ ATF-UAD & 14.20 & 0.06 & 414K & 1.90 & 0.05 & 408K & 12.77 & 0.06 & 421K & 16.42 & 0.06 & 436K \\ AT & 0.74 & **0.00** & 867K & 9.25 & **0.00** & 4.80M & 36.53 & **0.00** & 910K & 60.57 & **0.00** & 4.91M \\ DCdetector & 102.16 & 0.01 & 867K & 25.72 & 0.03 & 895K & 214.56 & 0.02 & 910K & & Out of Memory \\ USAD & 4.34 & 0.01 & 803K & 0.86 & 0.01 & 414K & 3.74 & 0.01 & 1.26M & 9.34 & 0.01 & 2.54M \\ GDN & 36.04 & 0.44 & **3K** & 9.64 & 0.34 & **3K** & 19.65 & 0.46 & **4K** & 65.69 & 0.68 & **6K** \\ MAD-GAN & 42.56 & 0.40 & 268K & 45.06 & 0.46 & 261K & 1416.26 & 0.40 & 274K & 165.01 & 0.47 & 289K \\ DiffAD & 11.93 & 1.49 & 38.85M & 4.94 & 7.91 & 38.85M & 15.32 & 1.69 & 38.85M & 42.87 & 2.80 & 38.85M \\ 
**Ours** & 1.47 & 0.11 & 9.57M & 2.32 & 0.12 & 15.85M & 10.87 & 0.16 & 9.59M & 31.97 & 0.39 & 15.94M \\   

Table 15: Model complexity and overheads. The total training time (in minutes), the inference time per sample (in milliseconds), and the total number of parameters (where applicable) are reported. The best values are in **bold** and the second best underlined.

Baselines

We trained all baselines using official implementations where available and recommended hyperparameters from respectively papers are used. Some baselines, such as DiffAD and AT, have dataset-specific hyperparameters and here we adopted them as well. The open-accessed URLs of the baselines used are listed as followed.

* IF (ICDM'08) (Liu et al., 2008): https://github.com/xuhongzuo/deep-iforest.
* DIF (TKDE'23) (Xu et al., 2023): https://github.com/xuhongzuo/deep-iforest.
* TranAD (VLDB'22) (Tuli et al., 2022): https://github.com/imperial-qore/ TranAD.
* ATF-UAD (NN'23) (Fan et al., 2023): https://github.com/wzhSteve/ATF-UAD.
* AT (ICLR'22) (Xu et al., 2022): https://github.com/thuml/Anomaly-Transformer.
* DCdetector (KDD'23) (Yang et al., 2023): https://github.com/DAMO-DI-ML/ KDD2023-DCdetector.
* USAD (KDD'20) (Audibert et al., 2020): https://github.com/manigalati/usad.
* GDN (AAAI'21) (Deng and Hooi, 2021): https://github.com/d-ailin/GDN.
* MAD-GAN (ICANN'19) (Li et al., 2019): https://github.com/LiDan456/MAD-GANs. Official implementation was migrated to pyTorch for uniform environmental set-up. See our codebase for details.
* DiffAD (KDD'23) (Xiao et al., 2023): https://github.com/ChunjingXiao/DiffAD.

## Appendix P Joint Detection Criterion

Figure 12 visualizes the two components of the joint detection criterion \(s\) in Eq. 5, i.e., the data reconstruction error \(r\) and the progression reconstruction error \(p\). Balanced resampling is applied here. Recall that \(s\) is the sum of normalized \(r\) and \(p\). Most anomalous samples (input series) either has high \(r\) or high \(p\), and oftentimes both. The former measures the magnitude of anomalousness in the data space, the latter in the spatial association space. The basis underpins the formalization of the joint detection criterion.

Figure 12: Joint detection criterion \(s\) of data reconstruction error \(r\) and progression reconstruction error \(p\).

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims clearly reflect the paper's contributions and scope of time series anomaly detection in the abstract and Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclose full details of our experiments in Appendixes F and O. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide open access to the data and code in Appendix G. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We disclose essential experimental settings in Section 4.1 and full details in Appendix F. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report standard deviations of the experiments in Appendices K and L. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We disclose information on compute resources for experiments in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no risks of misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite and properly credit all datasets, baselines, and libraries used in this paper (see Appendices E and O for details). Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve IRB approvals and equivalent approvals/reviews. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.