# Differentially Private Reinforcement Learning with Self-Play

Dan Qiao

Department of Computer Science \(\&\) Engineering

University of California, San Diego

San Diego, CA 92093

d2qiao@ucsd.edu

&Yu-Xiang Wang

Halciooglu Data Science Institute

University of California, San Diego

San Diego, CA 92093

yuxiangw@ucsd.edu

###### Abstract

We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.

## 1 Introduction

This paper considers the problem of multi-agent reinforcement learning (multi-agent RL), wherein several agents simultaneously make decisions in an unfamiliar environment with the goal of maximizing their individual cumulative rewards. Multi-agent RL has been deployed not only in large-scale strategy games like Go (Silver et al., 2017), Poker (Brown and Sandholm, 2019) and MOBA games (Ye et al., 2020), but also in various real-world applications such as autonomous driving (Shalev-Shwartz et al., 2016), negotiation (Bachrach et al., 2020), and trading in financial markets (Shavandi and Khedmati, 2022). In these applications, the learning agent analyzes users' private feedback in order to refine its performance, where the data from users usually contain sensitive information. Take autonomous driving as an instance, here a trajectory describes the interaction between the cars in a neighborhood during a fixed time window. At each timestamp, given the current situation of each car, the system (central agent) will send a command for each car to take (_e.g._ speed up, pull over), and finally the system gathers the feedback from each car (_e.g._ whether the driving is safe, whether the customer feels comfortable) and enhances its policy. Here, (situation, command, feedback) corresponds to (state, action, reward) in a Markov Game where the state and reward of each user are considered as sensitive information. Therefore, leakage of such information is not acceptable. Regrettably, it has been demonstrated that without the implementation of privacy safeguards, learning agents tend to maladvertently memorize details from individual training data points (Carlini et al., 2019), regardless of their relevance to the learning process (Brown et al., 2021). This susceptibility exposes multi-agent RL agents to potential privacy threats.

To handle the above privacy issue, Differential privacy (DP) (Dwork et al., 2006) has been widely considered. The output of a differentially private reinforcement learning algorithm cannot be discernedfrom its output in an alternative reality where any specific user is substituted, which effectively mitigates the privacy risks mentioned earlier. However, it is shown [Shariff and Sheffet, 2018] that standard DP will lead to linear regret even under contextual bandits. Therefore, Vietri et al.  considered a relaxed surrogate of DP: _Joint Differential Privacy_ (JDP) [Kearns et al., 2014] for RL. Briefly speaking, JDP protects the information about any specific user even given the output of all other users. Meanwhile, another variant of DP: _Local Differential Privacy_ (LDP) [Duchi et al., 2013] has also been extended to RL by Garcelon et al.  due to its stronger privacy protection. LDP requires that the raw data of each user is privatized before being sent to the agent. Although following works [Chowdhury and Zhou, 2022, Qiao and Wang, 2023] established near optimal results under these two notions of DP, all of the previous works focused on the single-agent RL setting while the solution to multi-agent RL with differential privacy is still unknown. Therefore we question:

**Question 1.1**.: _Is it possible to design a provably efficient self-play algorithm to solve Markov games while satisfying the constraints of differential privacy?_

**Our contributions.** In this paper, we answer the above question affirmatively by proposing a general algorithm for DP multi-agent RL: DP-Nash-VI (Algorithm 1). Our contributions are threefold.

* We first extend the definitions of Joint DP (Definition 2.2) and Local DP (Definition 2.3) to the multi-agent RL setting. Both notions of DP focus on protecting the sensitive information of each trajectory, which is consistent with the counterparts under single-agent RL.
* We design a new algorithm DP-Nash-VI (Algorithm 1) based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm can be combined with any Privatizer (for JDP or LDP) that possesses a corresponding regret bound (Theorem 4.1). Moreover, when there is no privacy constraint (_i.e._ the privacy budget is infinity), our regret reduces to the best known regret for non-private multi-agent RL.
* Under the constraint of \(\)-JDP, DP-Nash-VI achieves a regret of \((SABT}+H^{3}S^{2}AB/)\) (Theorem 5.2). Compared to the regret lower bound (Theorem 5.3), the main term is nearly optimal while the additional cost due to JDP has optimal dependence on \(\). Under the \(\)-LDP constraint, DP-Nash-VI achieves a regret of \((SABT}+S^{2}ABT}/)\) (Theorem 5.5), where the dependence on \(K,\) is optimal according to the lower bound (Theorem 5.6). The pair of results strictly generalizes the best known results for single-agent RL with DP [Qiao and Wang, 2023].

### Related work

We compare our results with existing works on differentially private reinforcement learning [Vietri et al., 2020, Garcelon et al., 2021, Chowdhury and Zhou, 2022, Qiao and Wang, 2023] and regret minimization under Markov Games [Liu et al., 2021] in Table 1, while more discussions about differentially private learning algorithms are deferred to Appendix A. Notably, all existing DP RL

  Algorithms for Markov Games & Regret without privacy & Regret after \(\)-JDP & Regret under \(\)-JDP \\  DP-Nash-VI (Algorithm 1) & \((SABT}+T^{2}SABT)\) & \((SABT}+S^{2}AB/)\) & \((SABT}+S^{2}AB/)\) (\(SABT}+S^{2}AB/)\) (\(SABT}+S^{2}AB/)\) \\  Nash VI (Liu et al., 2021) & \((SABT})\): & NSA & NSA \\  Lower bounds & \((SA}(A+BT)\) (Bai and Jin, 2020) & \(SA}(A+BT)+((A+BT)+((A+BT)}{^{2}(A+BT)}}))}\) & \(SA}(A+BT)+((A+BT)}{^{2}(A+ BT)})}\) \\  Algorithms for MDPs (\(1-1\)) & Regret without privacy & Regret after \(\)-JDP & Regret after \(\)-JDP \\  PCQ [Vier et al., 2020] & \((SA}AT)\) & \((SA}AT+SA}A/)^{2}\) & NSA \\  LDP-QVI [Garcelon et al., 2021] & \((SA}AT)\) & NSA & \((SA}AT+S^{2}AT/)^{2}\) \\ Private-UCBVI [Garcelon et al., 2021] & \((SA}AT)\) & \((SA}AT+S^{2}BA/)\) & \((SA}AT+S^{2}AA/)\) (\((SA}AT+S^{2}AA/)\) \\ DP-UCWVI [Garcelon et al., 2022] & \((SA}AT)\) & \((SA}AT+S^{2}BA/)\) & \((SA}AT+S^{2}AA/)\) \\  DP-UCWVI [Garcelon et al., 2021] & \((SA}AT)\) & \((SA}AT+S^{2}BA/)\) & \((SA}AT+S^{2}AA/)\) \\  

Table 1: Comparison of our results (in blue) to existing work regarding regret without privacy (_i.e._ the privacy budget is infinity), regret under \(\)-Joint DP and regret under \(\)-Local DP. In the above, \(S\) is the number of states, \(A,B\) are the number of actions for both players, \(H\) is the planning horizon and \(K\) is the number of episodes (\(T=HK\) is the number of steps). Markov decision processes (MDPs) is a special case of Markov Games where \(B=1\). \(*\): This result is the best known regret bound when there is no privacy concern. \(\): More discussions about this bound can be found in Chowdhury and Zhou . \(\): The original regret bound in Garcelon et al.  is derived under the setting of stationary MDP, and can be directly transferred to the bound here by adding \(\) to the first term. \(\): This algorithm achieved the best known results under single-agent MDPs, and our Algorithm 1 can obtain the same regret bounds under this setting.

algorithms focus on the single-agent case. In comparison, our algorithm works for the more general two-player setting and our results directly match the best known regret bounds (Qiao and Wang, 2023) when applied to the single-agent setting.

Recently, several works provide non-asymptotic theoretical guarantees for learning Markov Games. Bai and Jin (2020) developed the first provably-efficient algorithms in MGs based on optimistic value iteration, and the result is improved by Liu et al. (2021) using model-based approach. Meanwhile, model-free approaches are shown to break the curse of multiagency and improve the dependence on action space (Bai et al., 2020; Jin et al., 2021; Mao et al., 2022; Wang et al., 2023; Cui et al., 2023). However, all these algorithms base on the original data from users, and thus are vulnerable to various privacy attacks. While several works (Hossain and Lee, 2023; Hossain et al., 2023; Zhao et al., 2023; Gohari et al., 2023) study the privatization of communications between multiple agents, none of them provide regret guarantees. In comparison, we design algorithms that provably protect the sensitive information in each trajectory, while achieving near-optimal regret bounds simultaneously.

Technically speaking, we follow the idea of optimistic Nash value iteration and privatization of Bernstein-type bonuses. Optimistic Nash value iteration aims to construct both upper bounds and lower bounds for value functions, which could guide the exploration. Such idea has been applied by previous model-based approaches (Bai and Jin, 2020; Liu et al., 2021) to derive tight regret bounds. To satisfy the privacy guarantees, we are required to construct the UCB and LCB privately. In this work, we privatize the transition kernel estimate and construct a private bonus function for our purpose. Among different bonuses, we generalize the approach in Qiao and Wang (2023) and directly operate on the Bernstein-type bonus, which could enable tight regret analysis while the privatization is more technically demanding due to the variance term. To handle this, we first privatize the visitation counts such that they satisfy several nice properties, then we use these counts to construct private transition estimates and private bonuses. Lastly, we manage to prove UCB and LCB, and bound the private terms by their non-private counterparts to complete the regret analysis.

## 2 Problem Setup

We consider reinforcement learning under Markov Games (MGs) (Shapley, 1953) with Differential Privacy (DP) (Dwork et al., 2006). Below we introduce MGs and define DP under multi-agent RL.

### Markov Games and Regret

Markov Games (MGs) are the generalization of Markov Decision Processes (MDPs) to the multi-player setting, where each player aims to maximize her own reward. We consider _two-player zero-sum_ episodic MGs, denoted by a tuple \(=(,,,H,\{P_{h}\}_{h=1}^{H},\{r_{ h}\}_{h=1}^{H},s_{1})\), where \(\) is the state space with \(S=||\), \(\) and \(\) are the action space for the max-player (who aims to maximize the total reward) and the min-player (who aims to minimize the total reward) respectively with \(A=||,B=||\). Besides, \(H\) is the horizon while the non-stationary transition kernel \(P_{h}(|s,a,b)\) gives the distribution of the next state if action \((a,b)\) is taken at state \(s\) and time step \(h\). In addition, we assume that the reward function \(r_{h}(s,a,b)\) is deterministic and known1. For simplicity, we assume each episode starts from a fixed initial state \(s_{1}\). Then at each time step \(h[H]\), two players observe \(s_{h}\) and choose their actions \(a_{h}\) and \(b_{h}\) simultaneously, after which both players observe the action of their opponent and receive reward \(r_{h}(s_{h},a_{h},b_{h})\), the environment will transit to \(s_{h+1} P_{h}(|s_{h},a_{h},b_{h})\).

**Markov policy, value function.** A Markov policy \(\) of the max-player can be seen as a series of mappings \(=\{_{h}\}_{h=1}^{H}\), where each \(_{h}\) maps each state \(s\) to a probability distribution over actions \(\), _i.e._\(_{h}:()\). A Markov policy \(\) for the min-player is defined similarly. Given a pair of policies \((,)\) and time step \(h[H]\), the value function \(V_{h}^{,}()\) is defined as \(V_{h}^{,}(s)=_{,}[_{t=h}^{H}r_{t}|s_{h}=s]\) while the Q-value function \(Q_{h}^{,}(,,)\) is defined as \(Q_{h}^{,}(s,a,b)=_{,}[_{t=h}^{H}r_{t}|s_{h},a_{h},b _{h}=s,a,b]\) for all \(s,a,b\). According to the definitions, the following Bellman equation holds:

\[Q_{h}^{,}(s,a,b)=[r_{h}+P_{h}V_{h+1}^{,}](s,a,b),\;\;V_{h}^{, }(s)=[_{,}Q_{h}^{,}](s),\;\;\;(h,s,a,b).\]

**Best responses, Nash equilibrium.** For any policy \(\) of the max-player, there exists a best response policy \(^{}()\) of the min-player such that \(V_{h}^{,^{}()}(s)=_{}V_{h}^{,}(s)\) for all \((s,h)\). For simplicity, we denote \(V_{h}^{,}:=V_{h}^{,^{}()}\). Also, \(^{}()\) and \(V_{h}^{,}\) can be defined by symmetry. It is shown  that there exists a pair of policies \((^{},^{})\) that are best responses against each other, _i.e._, \(V_{h}^{^{},}(s)=V_{h}^{^{},^{}}(s)=V_{h}^{,^{}}(s),\ \ (s,h)[H]\). The pair of policies \((^{},^{})\) is called the Nash equilibrium of the Markov game, which further satisfies the following minimax property: for all \((s,h)[H],\ _{}_{}V_{h}^{,}(s)=V_{h}^{^{ },^{}}(s)=_{}_{}V_{h}^{,}(s)\). The value functions of \((^{},^{})\) are called Nash value functions and we denote \(V_{h}^{}=V_{h}^{^{},^{}},Q_{h}^{}=Q_{h}^{^{},^{}}\) for simplicity. Nash equilibrium means that no player could gain more from updating her own policy.

**Learning objective: regret.** Following previous works , we aim to minimize the regret, which is defined as below:

\[(K)=_{k=1}^{K}[V_{1}^{1,^{k}}(s_{1})-V_{1}^{^{k}, }(s_{1})],\]

where \(K\) is the number of episodes the agent interacts with the environment and \((^{k},^{k})\) are the policies executed by the agent in the \(k\)-th episode. Note that any sub-linear regret bound can be transferred to a PAC guarantee according to the standard online-to-batch conversion .

### Differential Privacy in Multi-agent RL

For RL with self-play, each trajectory corresponds to the interaction between a pair of users and the environment. The interaction generally follows the protocol below. At time step \(h\) of the \(k\)-th episode, the users send their state \(s_{h}^{k}\) to a central agent \(\), then \(\) sends back a pair of actions \((a_{h}^{k},b_{h}^{k})\) for the users to take, and finally the users send their reward \(r_{h}^{k}\) to \(\). Following previous works , here we let \(=(u_{1},,u_{K})\) denote the sequence of \(K\) unique 2 pairs of users who participate in the above RL protocol. Besides, each pair of users \(u_{k}\) is characterized by the \(\{s_{h}^{k},r_{h}^{k}\}_{h=1}^{H}\) information they would respond to all \((AB)^{H3}\) possible sequences of actions from the agent. Let \(()=\{(a_{h}^{k},b_{h}^{k})\}_{h,k=1,1}^{H,K}\) denote the whole sequence of actions suggested by the agent \(\). Then a direct adaptation of differential privacy  is defined below, which says that \(()\) and all other pairs excluding \(u_{k}\) together will not disclose much information about user \(u_{k}\).

**Definition 2.1** (Differential Privacy (DP)).: _For any \(>0\) and \(\), a mechanism \(:()^{KH}\) is \((,)\)-differentially private if for any possible user sequences \(\) and \(^{}\) that is different on one pair of users and any subset \(E\) of \(()^{KH}\),_

\[[() E] e^{}[ (^{}) E]+.\]

_If \(=0\), we say that \(\) is \(\)-differentially private (\(\)-DP)._

Unfortunately, privately recommending actions to the pair of users \(u_{k}\), while protecting their own state and reward information is shown to be impractical even for the single-player setting. Therefore, we consider a relaxed version of DP, known as _Joint Differential Privacy_ (JDP) . JDP says that for all pairs of users \(u_{k}\), the recommendation to all other pairs excluding \(u_{k}\) will not disclose the sensitive information about \(u_{k}\). Although being weaker than DP, JDP could still provide meaningful privacy protection by ensuring that even if an adversary can observe the interactions between all other users and the environment, it is statistically hard to reconstruct the interaction between \(u_{k}\) and the environment. JDP is first studied by Vietri et al.  under single-agent reinforcement learning, and we extend the definition to the two-player setting.

**Definition 2.2** (Joint Differential Privacy (JDP)).: _For any \(>0\), a mechanism \(:()^{KH}\) is \(\)-joint differentially private if for any \(k[K]\), any user sequences \(\) and \(^{}\) that is different on the \(k\)-th pair of users and any subset \(E\) of \(()^{(K-1)H}\),_

\[[_{-k}() E] e^{}[ _{-k}(^{}) E],\]_where \(_{-k}() E\) means the sequence of actions sent to all pairs of users excluding \(u_{k}\) belongs to set \(E\)._

In the example of autonomous driving, JDP ensures that even if an adversary observes the interactions between cars within all time windows except one, it is hard to know what happens during the specific time window. While providing strong privacy protection, JDP requires the central agent \(\) to have access to the real trajectories from users. However, in various scenarios the users are not even willing to directly share their data with the agent. To address such circumstances, Duchi et al. (2013) developed a stronger notion of privacy named _Local Differential Privacy_ (LDP). Now that when considering LDP, the agent can not observe the state of users, we consider the following protocol specific for LDP: at the beginning of the \(k\)-th episode, the agent \(\) first sends a policy pair \(_{k}=(_{k},_{k})\) to the pair of users \(u_{k}\), after running \(_{k}\) and getting a trajectory \(X_{k}\), \(u_{k}\) privatizes their trajectory to \(X^{}_{k}\) and sends it back to \(\). We present the definition of Local DP below, which generalizes the LDP under single-agent reinforcement learning by Garcelon et al. (2021). Briefly speaking, Local DP ensures that it is impractical for an adversary to reconstruct the whole trajectory of \(u_{k}\) even if observing their whole response.

**Definition 2.3** (Local Differential Privacy (LDP)).: _For any \(>0\), a mechanism \(}\) is \(\)-local differentially private if for any possible trajectories \(X,X^{}\) and any possible set \(E\{}(X)|X\}\),_

\[[}(X) E] e^{}[ }(X^{}) E].\]

In the example of autonomous driving, LDP ensures that the system can only observe a private version of the interactions between cars instead of the raw data.

**Remark 2.4**.: _Note that here our definitions of JDP and LDP both provide trajectory-wise privacy protection, which is consistent with previous works (Chowdhury and Zhou, 2022, Qiao and Wang, 2023). Moreover, under the special case where the min-player plays a fixed and known deterministic policy (or equivalently, \(\) only contains a single action and \(B=1\)), the Markov Game setting reduces to a single-agent Markov decision process while our JDP and LDP directly matches previous definitions for the MDP setting. Therefore, our setting strictly generalizes previous works and requires novel techniques to handle the min-player._

**Remark 2.5**.: _In the following sections we will show that LDP is consistent with sub-linear regret bounds, while it is known that we can not derive sub-linear regret bounds under the constraint of DP. We remark that there is no contradictory since here the RL protocols for DP and LDP are different. As a result, here a guarantee of LDP does not directly imply a guarantee of DP and the two notions are indeed not directly comparable._

## 3 Algorithm

In this part, we introduce DP-Nash-VI (Algorithm 1). Note that the algorithm takes Privatizer as an input. We analyze the regret of Algorithm 1 for all Privatizers satisfying the Assumption 3.1 below, which includes the cases where the Privatizer is chosen as Central (for JDP) or Local (for LDP).

We first introduce the definition of visitation counts, where \(N^{k}_{h}(s,a,b)=_{i=1}^{k-1}(s^{i}_{h},a^{i}_{h},b^{i}_{h}=s,a,b)\) denotes the visitation count of \((s,a,b)\) at time step \(h\) until the beginning of the \(k\)-th episode. Similarly, we let \(N^{k}_{h}(s,a,b,s^{})=_{i=1}^{k-1}(s^{i}_{h},a^{i}_{h},b^{ i}_{h},s^{i+1}_{h+1}=s,a,b,s^{})\) be the visitation count of \((h,s,a,b,s^{})\) before the \(k\)-th episode. In multi-agent RL without privacy constraints, such visitation counts are sufficient for estimating the transition kernel \(\{P_{h}\}_{h=1}^{H}\) and updating the exploration policy, as in previous model-based approaches (Liu et al., 2021). However, these counts base on the original trajectories from the users, which could reveal sensitive information. Therefore, with the concern of privacy, we can only incorporate these counts after a privacy-preserving step. In other words, we use a Privatizer to transfer the original counts to the private version \(^{k}_{h}(s,a,b),^{k}_{h}(s,a,b,s^{})\). We make the following Assumption 3.1 for Privatizer, which says that the private counts are close to real ones. Privatizers for JDP and LDP that satisfy Assumption 3.1 will be proposed in Section 5.

**Assumption 3.1** (Private counts).: _For any privacy budget \(>0\) and failure probability \(\), there exists some \(E_{,}>0\) such that with probability at least \(1-/3\), for all \((h,s,a,b,s^{},k)[H][K]\), the \(^{k}_{h}(s,a,b,s^{})\) and \(^{k}_{h}(s,a,b)\) from Privatizer satisfies:_(1) \(|_{h}^{k}(s,a,b,s^{})-N_{h}^{k}(s,a,b,s^{})| E_{ ,}\), \(|_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)| E_{,}\). \(_{h}^{k}(s,a,b,s^{})>0\). (2) \(_{h}^{k}(s,a,b)=_{s^{}}_{h}^ {k}(s,a,b,s^{}) N_{h}^{k}(s,a,b)\).

Given the private counts satisfying Assumption 3.1, the private estimate of transition kernel is defined as below.

\[_{h}^{k}(s^{}|s,a,b)=_{h}^{k}(s,a,b,s^{ })}{_{h}^{k}(s,a,b)},\ \ \ (h,s,a,b,s^{},k).\] (1)

**Remark 3.2**.: _Assumption 3.1 is a generalization of Assumption 3.1 of Qiao and Wang (2023) to the two-player setting. The assumption (2) guarantees that the private transition kernel \(_{h}^{k}(|s,a,b)\) is a valid probability distribution, which enables our usage of Bernstein-type bonus. Besides, \(\) is close to the empirical transition kernel based on original visitation counts according to Assumption (1)._

**Algorithmic design.** Following previous non-private approaches (Liu et al., 2021), DP-Nash-VI (Algorithm 1) maintains a pair of value functions \(\) and \(\) which are the upper bound and lower bound of the Q value of the current policy when facing best responses (with high probability). More specifically, we use private visitation counts \(_{h}^{k}\) to construct a private estimate of transition kernel \(_{h}^{k}\) (line 7) and a pair of private bonus \(_{h}^{k}\) (line 8) and \(_{h}^{k}\) (line 9). Intuitively, the first term of \(_{h}^{k}\) is derived from Bernstein's inequality while the second term is the additional bonus due to differential privacy. Next we do value iteration with bonuses to construct the UCB function \(_{h}^{k}\) (line 10) and the LCB function \(_{h}^{k}\) (line 11). The policy \(^{k}\) for the \(k\)-th episode is calculated using the CCE function (discussed below) and we run \(^{k}\) to collect a trajectory (line 14,18). Finally, the Privatizer transfers the non-private counts to private ones for the next episode (line 19). The output policy \(^{}\) is chosen as the policy \(^{k}\) with minimal gap \((_{1}^{k}-_{1}^{k})(s_{1})\) (line 21). Decomposing the output policy, the output policy \((^{},^{})\) for both players are the marginal policies of \(^{}\), _i.e._\(^{}_{h}(|s)=_{b}^{}_{h}(,b|s)\) and \(^{}_{h}(|s)=_{a}^{}_{h}(a, |s)\) for all \((h,s)[H]\).

**Coarse Correlated Equilibrium (CCE).** Intuitively speaking, CCE of a Markov Game is a potentially correlated policy where no player could benefit from unilateral unconditional deviation. As a computationally friendly relaxation of Nash Equilibrium, CCE has been applied by previous works [Xie et al., 2020, Liu et al., 2021] to design efficient algorithms. Formally, for any two functions \((,),(,): [0,H]\), \((,)\) returns a policy \(()\) such that

\[_{(a,b)}(a,b)_{a^{}}_{(a,b)}(a^{},b),\ \ _{(a,b)}(a,b)_{b^{}}_{(a,b)}(a,b^{}).\]

Since Nash Equilibrium (NE) is a special case of CCE and a NE always exists. Moreover, a CCE can be derived in polynomial time via linear programming. Note that the policies given by CCE can be correlated for the two players, therefore deploying such policy requires the cooperation of both players (line 18).

## 4 Main results

We first state the regret analysis of DP-Nash-VI (Algorithm 1) based on Assumption 3.1, which can be combined with any Privatizers. The proof of Theorem 4.1 is sketched in Appendix B with details in the Appendix. Note that \((^{k},^{k})\) denote the marginal policies of \(^{k}\) for both players.

**Theorem 4.1**.: _For any privacy budget \(>0\), failure probability \(\) and any Privatizer satisfying Assumption 3.1, with probability at least \(1-\), the regret of DP-Nash-VI (Algorithm 1) is_

\[(K)=_{k=1}^{K}[V_{1}^{,^{k}}(s_{1})-V_{1}^{ ^{k},}(s_{1})](SABT}+H^{2}S^ {2}ABE_{,}),\] (2)

_where \(K\) is the number of episodes and \(T=HK\)._

Under the special case where the privacy budget \(\) (_i.e._ there is no privacy concern), plugging \(E_{,}=0\) in Theorem 4.1 will imply a regret bound of \((SABT})\). Such result directly matches the best known result for regret minimization without privacy constraints [Liu et al., 2021] and nearly matches the lower bound of \((S(A+B)T})\)[Bai and Jin, 2020]. Furthermore, under the special case of single-agent MDP (where \(B=1\)), our result reduces to \((K)(SAT}+H^{2}S^{2}AE_{, })\). Such result matches the best known result under the same set of conditions (Theorem 4.1 of Qiao and Wang ). Therefore, Theorem 4.1 is a generalization of the best known results under MARL [Liu et al., 2021] and Differentially Private (single-agent) RL [Qiao and Wang, 2023] simultaneously.

**PAC guarantee.** Recall that we output a policy \(^{}\) whose marginal policies are \((^{},^{})\). We highlight that the output policy for each player is a single Markov policy that is convenient to store and deploy. Moreover, as a corollary of the regret bound, we give a PAC bound for the output policy.

**Theorem 4.2**.: _For any privacy budget \(>0\), failure probability \(\) and any Privatizer that satisfies Assumption 3.1, if the number of episodes satisfies that \(K(SAB}{^{2}}+\{K^{} |S^{2}ABE_{,}}{K^{}}\})\), with probability \(1-\), \((^{},^{})\) is \(\)-approximate Nash, i.e., \(V_{1}^{,^{}}(s_{1})-V_{1}^{^{},}(s_{1})\)._

The proof is deferred to Appendix C.4. Here the second term of the sample complexity bound4 ensures that the additional cost due to DP is bounded by \(O()\). The detailed PAC guarantees under the special cases where the Privatizer is either Central or Local will be provided in Section 5.

## 5 Privatizers for JDP and LDP

In this section, we propose Privatizers that provide DP guarantees (JDP or LDP) while satisfying Assumption 3.1. The proofs for this section can be found in Appendix D.

### Central Privatizer for Joint DP

Given the number of episodes \(K\), the Central Privatizer applies \(K\)-bounded Binary Mechanism (Chan et al., 2011) to privatize all the visitation counter streams \(N_{h}^{k}(s,a,b)\), \(N_{h}^{k}(s,a,b,s^{})\), thus protecting the information of all single users. Briefly speaking, Binary mechanism takes a stream of partial sums as input and outputs a surrogate stream satisfying differential privacy, while the error for each item scales only logarithmically on the length of the stream5. Here in multi-agent RL, for each \((h,s,a,b)\), the stream \(\{N_{h}^{k}(s,a,b)=_{i=1}^{k-1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s, a,b)\}_{k[K]}\) can be considered as the partial sums of \(\{(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\}\). Therefore, after observing \((s_{h}^{k},a_{h}^{k},b_{h}^{k}=s,a,b)\) at the end of episode \(k\), the Binary Mechanism will output a private version of \(_{i=1}^{k}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\). However, Binary Mechanism alone does not satisfy (2) of Assumption 3.1, and a post-processing step is required. To sum up, we let the Central Privatizer follow the workflow below:

Given the privacy budget for JDP \(>0\),

(1) For all \((h,s,a,b,s^{})\), we apply Binary Mechanism (Algorithm 2 in Chan et al. (2011)) with input parameter \(^{}=\) to privatize all the visitation counter streams \(\{N_{h}^{k}(s,a,b)\}_{k[K]}\) and \(\{N_{h}^{k}(s,a,b,s^{})\}_{k[K]}\). We denote the output of Binary Mechanism by \(_{h}^{k}\).

(2) The private counts \(_{h}^{k}\) are derived through Section 5.3 with \(E_{,}=O((HSABK/)^{2})\).

Our Central Privatizer satisfies the privacy guarantee below.

**Lemma 5.1**.: _For any possible \(,\), the Central Privatizer satisfies \(\)-JDP and Assumption 3.1 with \(E_{,}=()\)._

Combining Lemma 5.1 with Theorem 4.1 and Theorem 4.2, we have the following regret \(\&\) PAC guarantee under \(\)-JDP.

**Theorem 5.2** (Results under JDP).: _For any possible \(,\), with probability \(1-\), the regret from running DP-Nash-VI (Algorithm 1) instantiated with Central Privatizer satisfies:_

\[(K)(SABT}+H^{3}S^{2}AB/).\] (3)

_Moreover, if the number of episodes \(K\) is larger than \((SAB}{^{2}}+S^{2}AB}{ })\), with probability \(1-\), the output policy \((^{},^{})\) is \(\)-approximate Nash._

Similar to the single-agent (MDP) setting (\(B=1\)), the additional cost due to JDP is a lower order term under the most prevalent regime where the privacy budget \(\) is a constant. When applied to the single-agent case, our regret matches the best known regret \((SAT}+H^{3}S^{2}A/)\)(Qiao and Wang, 2023). Moreover, when compared to the regret lower bound below, our main term is nearly optimal while the lower order term has optimal dependence on \(\).

**Theorem 5.3**.: _For any algorithm \(\) satisfying \(\)-JDP, there exists a Markov Game such that the expected regret from running \(\) for \(K\) episodes (\(T=HK\) steps) satisfies:_

\[[(K)](S(A +B)T}+).\]

The regret lower bound results from the lower bound for the non-private learning (Bai and Jin, 2020) and an adaptation of the lower bound under JDP guarantees (Vietri et al., 2020) to the multi-player setting. Details are deferred to the appendix.

### Local Privatizer for Local DP

At the end of episode \(k\), the Local Privatizer perturbs the statistics calculated from the new trajectory before sending it to the agent. Since the set of original visitation counts \(\{_{h}^{k}(s,a,b)=(s_{h}^{k},a_{h}^{k},b_{h}^{k}=s,a,b)\}_{(h,s,a,b)}\) has \(_{1}\) sensitivity \(H\), we can achieve \(\)-LDP by directly adding Laplace noise, _i.e._, \(_{h}^{k}(s,a,b)=_{h}^{k}(s,a,b)+()\). Similarly, repeating the above perturbation to \(\{(s_{h}^{k},a_{h}^{k},b_{h}^{k},s_{h+1}^{k}=s,a,b,s^{})\}_{(h,s,a,b,s^{})}\) will lead to identical results. Therefore, the Local Privatizer with budget \(\) is as below:(1) We perturb \(^{k}_{h}(s,a,b)=(s^{k}_{h},a^{k}_{h},b^{k}_{h}=s,a,b)\) and \(^{k}_{h}(s,a,b,s^{})=(s^{k}_{h},a^{k}_{h},b^{k}_{h},s^{k}_ {h+1}=s,a,b,s^{})\) by adding independent Laplace noises: for all \((h,s,a,b,s^{},k)\),

\[^{k}_{h}(s,a,b)=^{k}_{h}(s,a,b)+( ),\ \ ^{k}_{h}(s,a,b,s^{})=^{k}_{h}(s,a,b,s^{})+ ().\] (4)

(2) Then the noisy counts are derived according to

\[^{k}_{h}(s,a,b)=_{i=1}^{k-1}^{i}_{h}(s,a,b), \ \ ^{k}_{h}(s,a,b,s^{})=_{i=1}^{k-1}^{i}_{ h}(s,a,b,s^{}),\] (5)

and the private counts \(^{k}_{h}\) are solved through Section 5.3 with \(E_{,}=O()\).

Our Local Privatizer satisfies the privacy guarantee below.

**Lemma 5.4**.: _For any possible \(,\), the Local Privatizer satisfies \(\)-LDP and Assumption 3.1 with \(E_{,}=()\)._

Combining Lemma 5.4 with Theorem 4.1 and Theorem 4.2, we have the following regret \(\&\) PAC guarantee under \(\)-LDP.

**Theorem 5.5** (Results under LDP).: _For any possible \(,\), with probability \(1-\), the regret from running DP-Nash-VI (Algorithm 1) instantiated with Local Privatizer satisfies:_

\[(K)(SABT}+S^{2}ABT }/).\] (6)

_Moreover, if the number of episodes \(K\) is larger than \((SAB}{^{2}}+S^{4}A^{2}B^{2 }}{^{2}^{2}})\), with probability \(1-\), the output policy \((^{},^{})\) is \(\)-approximate Nash._

Similar to the single-agent case, the additional cost due to LDP is a multiplicative factor to the regret bound. When applied to the single-agent case, our regret matches the best known regret \((SAT}+S^{2}AT}/)\)(Qiao and Wang, 2023). Moreover, we state the lower bound.

**Theorem 5.6**.: _For any algorithm \(\) satisfying \(\)-LDP, there exists a Markov Game such that the expected regret from running \(\) for \(K\) episodes (\(T=HK\) steps) satisfies:_

\[[(K)](S(A+B)T}+}{}).\]

The lower bound is adapted from Garcelon et al. (2021). While our regret has optimal dependence on \(\) and \(K\), the optimal dependence on \(H,S,A,B\) remains open.

### The post-processing step

Now we introduce the post-processing step. At the end of episode \(k\), given the noisy counts \(^{k}_{h}(s,a,b)\) and \(^{k}_{h}(s,a,b,s^{})\) for all \((h,s,a,b,s^{})\), the private visitation counts are constructed as following: for all \((h,s,a,b)\),

\[\{^{k}_{h}(s,a,b,s^{})\}_{s^{}}=*{argmin}_{\{x_{s^{}}\}_{s^{}}} \ _{s^{}}|x_{s^{}}-^{k}_{h}(s,a,b,s^{ })|\]

such that \(|_{s^{}}x_{s^{}}-^{k}_{h}(s,a,b) |}{4}\) and \(x_{s^{}} 0,\ \,s^{}\). \(^{k}_{h}(s,a,b)=_{s^{}}^{k} _{h}(s,a,b,s^{})\).

Lastly, we add a constant term to each count to ensure no underestimation (with high probability).

\[^{k}_{h}(s,a,b,s^{})=^{k}_{h}(s,a,b,s^{} )+}{2S},\ \ \ ^{k}_{h}(s,a,b)=^{k}_{h}(s,a,b)+}{2}.\] (8)

**Remark 5.7**.: _Solving problem (7) is equivalent to solving:_

\[\;t,\;\;|x_{s^{}}-_{h}^{k}(s,a,b,s^{} )| t,\;x_{s^{}} 0,\;\,s^{},\; \;|_{s^{}}x_{s^{}}-_{h}^{k}(s,a, b)|}{4},\]

_which is a **Linear Programming** problem with \(O(S)\) variables and \(O(S)\) linear constraints. This can be solved in polynomial time (Nemhauser and Wolsey, 1988). Note that the computation of CCE (line 14 in Algorithm 1) is also a LP problem, therefore the computational complexity of DP-Nash-VI is dominated by \(O(HSABK)\) Linear Programming problems, which is computationally friendly._

We summarize the properties of private counts \(_{h}^{k}\) below, which says that the post-processing step ensures that our private transition kernel estimate is a valid probability distribution while only enlarging the error by a constant factor.

**Lemma 5.8**.: _Suppose \(_{h}^{k}\) satisfies that with probability \(1-\), uniformly over all \((h,s,a,b,s^{},k)\),_

\[|_{h}^{k}(s,a,b,s^{})-N_{h}^{k}(s,a,b,s^{})| }{4},\;\;\;|_{h}^{k}(s,a,b)-N_{h} ^{k}(s,a,b)|}{4},\]

_then the \(_{h}^{k}\) derived above satisfies Assumption 3.1._

### Some discussions

In this part, we generalize the Privatizers in Qiao and Wang (2023) (for single-agent case) to the two-player setting, which enables our usage of Bernstein-type bonuses. Such techniques lead to a tight regret analysis and a near-optimal "non-private part" of the regret bound eventually.

Meanwhile, the additional cost due to DP has sub-optimal dependence on parameters regarding the Markov Game. The issue appears even in the single-agent case and is considered to be inherent to model-based algorithms due to the explicit estimation of private transitions (Garcelon et al., 2021). The improvement requires new algorithmic designs (_e.g._, private Q-learning) and we leave those as future works.

Lastly, the Laplace Mechanism can be replaced with other mechanisms, such as Gaussian Mechanism (Dwork et al., 2014) with approximate DP guarantee (or zCDP). The regret and PAC guarantees are readily derived by plugging in the corresponding \(E_{,}\) to Theorem 4.1 and Theorem 4.2.

## 6 Conclusion

We take the initial steps to study trajectory-wise privacy protection in multi-agent RL. We extend the definitions of Joint DP and Local DP to multi-player RL. In addition, we design a provably-efficient algorithm: DP-Nash-VI (Algorithm 1) that could satisfy either of the two DP constraints with corresponding regret guarantee. Moreover, our regret bounds strictly generalize the best known results under DP single-agent RL. There are various interesting future directions, such as improving the additional cost due to DP via model-free approaches and considering Markov Games with function approximations. We believe the techniques in this paper could serve as basic building blocks.