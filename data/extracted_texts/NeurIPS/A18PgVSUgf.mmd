# Accelerating Molecular Graph Neural Networks via Knowledge Distillation

Filip Ekstrom Kelvinius

Linkoping University

filip.ekstrom@liu.se

&Dimitar Georgiev

Imperial College London

d.georgiev21@imperial.ac.uk

&Artur Petrov Toshev

Technical University of Munich

artur.toshev@tum.de

&Johannes Gasteiger

Google Research

johannesg@google.com

These authors contributed equally to this work. Order was determined by rolling a dice.

###### Abstract

Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of various components of our framework, and investigate the potential of data augmentation to further enhance performance. All in all, we manage to close the gap in predictive accuracy between teacher and student models by as much as \(96.7\%\) and \(62.5\%\) for energy and force prediction respectively, while fully preserving the inference throughput of the more lightweight models.

## 1 Introduction

In the last couple of years, the field of molecular simulations has undergone a rapid paradigm shift with the advent of new, powerful computational tools based on machine learning (ML) . At the forefront of this transformation have been recent advances in graph neural networks (GNNs), which have brought about architectures that more effectively capture geometric and structural information critical for the accurate representation of molecules and molecular systems . Consequently, a multitude of GNNs have been developed, which now offer predictive performance approaching that of conventional gold-standard methods such as density functional theory (DFT) at a fraction of the computational cost . This has, in turn, significantly accelerated the modeling of molecular properties and the simulation of molecular systems, bolstering new research developments in many scientific disciplines, including material sciences, drug discovery and catalysis .

Nonetheless, this progress - largely coinciding with the development of bigger and more complex models, has naturally come at the expense of increased complexity . This has graduallylimited the utility of state-of-the-art GNNs in large-scale molecular simulation applications (e.g., molecular dynamics and high-throughput searches), where inference throughput (i.e., how many samples can be processed for a given time) is critical for making fast predictions about molecular systems at scale. Hence, addressing the trade-off between accuracy and computational demand remains essential for creating more affordable tools for molecular simulations and expanding the transformational impact of GNN models in the area.

Motivated by that, in this work, we investigate the potential of knowledge distillation (KD) in advancing the speed-accuracy Pareto frontier and enhancing the performance and scalability of molecular GNNs. In summary, the contributions of this paper are as follows:

* a large-scale, multi-output regression task, challenging to address with common KD methods.
* We design custom KD strategies, which we call _node-to-node (n2n)_, _edge-to-edge (e2e)_, _edge-to-node (e2n)_ and _vector-to-vector (v2v)_ knowledge distillation, which facilitate the distillation of hidden representations in directional and equivariant molecular GNNs.
* We demonstrate the effectiveness of our protocols across different teacher-student configurations and datasets, allowing us to substantially improve the performance of student models while fully preserving their throughput (see Figure 1 for an overview).
* We conduct a comprehensive empirical analysis of different components of our KD strategies, as well as explore data augmentation techniques for further improving performance.

Associated code is available online2.

## 2 Background

**Molecular simulations.** In this work, we consider molecular systems at an atomic level, i.e., \(N\) atoms represented by their atomic numbers \(=\{z_{1},...,z_{N}\}^{N}\) and positions \(=\{_{1},,_{N}\}^{N 3}\). Given a system, we want a model that can predict the energy \(E\) of the system, and the forces \(^{N 3}\) acting on each atom. Both of these properties are of high interest when simulating molecular systems. The energy of a system is essential for the prediction of its stability, whereas the forces are important for molecular dynamics simulations, where computed forces are combined with the equations of motion to simulate the evolution of the system over time.

**GNNs for molecular systems.** GNNs are a suitable framework for modeling molecular systems. Each molecular system \((,)\) can be represented as a mathematical graph \(=(,)\), where the nodes \(\) correspond to the set of atoms, and edges \(\) are created between nodes by connecting the closest neighboring atoms (typically defined by a cutoff radius and/or a maximum number of neighbors). Hence, in the context of molecular simulations, we can create GNNs that operate on

Figure 1: Using knowledge distillation, we manage to significantly boost the predictive accuracy of different student models on the OC20-2M  and COLL  datasets while fully preserving their inference throughput.

atomic graphs \(\) by propagating information between the atoms and the edges, and make predictions about the energy and forces of each system in a multi-output manner - i.e., \(,}=(,)\).

The main problem when modeling molecules and molecular properties is the number of underlying symmetries to account for, most importantly rigid transformations of the atoms. For instance, the total energy \(E\) of a system is not affected by (i.e., is _invariant_ to) rotations and translations of the system. However, the forces \(\) do change as we rotate a system - i.e., they are _equivariant_ to rotations. Therefore, to make accurate predictions about molecular systems, it is crucial to devise models that respect these symmetries and other physical constraints. There is now a plethora of diverse molecular GNNs that reflect that, e.g., SchNet , DimeNet [5; 6], PaiNN , GemNet [7; 8], NguaIP , and SCN , which have incrementally established a more holistic description of molecular systems by capturing advanced geometric features and physical symmetries. This has, however, come at the expense of computational efficiency.

**Knowledge distillation.** Knowledge distillation is a technique for compressing and accelerating ML models , which has recently demonstrated significant potential in domains such as computer vision  and natural language modeling . The main objective of KD is to create more efficient models by means of transferring knowledge (e.g., model parameters and activations) from large, computationally expensive, more accurate models, often referred to as teacher models, to simpler, more efficient models called student models . Since the seminal work of Hinton _et al._, the field has drastically expanded methodologically with the development of protocols that accommodate the distillation of "deeper" knowledge, more comprehensive transformation functions, as well as more robust distillation losses [23; 25]. Yet, these advances have mostly focused on classification, resulting in methods of limited utility in regression tasks . Moreover, most research in the area has been confined to non-graph data (e.g., images, text, tabular data). Despite recent efforts to extend KD to graph data and GNNs, these have likewise only concentrated on classification tasks involving standard GNN architectures [27; 28]. And, in particular, the application of KD to large-scale regression problems in molecular simulations, which involve state-of-the-art molecular GNN architectures containing complex, geometric node- and edge-level features, is still unexplored.

## 3 Knowledge distillation for molecular GNNs

**Preliminaries.** In the context of the aforementioned prediction task, we train molecular GNNs by enforcing a loss \(_{0}\) that combines both the energy and force prediction error as follows:

\[_{0}=_{}_{}(,E)+_{ }_{}(},),\] (1)

where \(E\) and \(\) are the ground-truth energy and forces, \(\) and \(}\) are the predictions of the model of interest, and \(_{}\) and \(_{}\) are some loss functions weighted by \(_{},_{}\).

To perform knowledge distillation, we augment this training process by defining an auxiliary KD loss term \(_{}\), which is added to \(_{0}\) (with a factor \(\)) to derive the final training loss function \(\):

\[=_{0}+_{}.\] (2)

This was originally proposed in the context of classification by leveraging the fact that the soft label predictions (i.e., the logits after softmax normalization) of a given (teacher) model carry valuable information that can complement the ground-truth labels in the training process of another (student) model . Since then, this has become the standard KD approach - commonly referred to as vanilla KD in the literature, which is often the foundation of new KD protocols. The main idea of this technique is to employ a KD loss \(_{}\) that forces the student to mimic the predictions of the teacher model. This is usually achieved by constructing a loss \(_{}=(z_{s},z_{t})\) based on the Kullback-Leibler (KL) divergence between the soft logits of the student \(z_{s}\) and the teacher \(z_{t}\).

However, such strategies - based on the distillation of the output of the teacher model only - pose two significant limitations. First, they are by design exclusively applicable to classification tasks, since there are no outputs analogous to logits in regression setups [20; 29]. This has consequently limited the utility of most KD methods in regression tasks. Second, this approach forces the student to emulate the final output of the teacher directly, which can be unattainable in regimes where the complexity gap between the two models is substantial, and thus detrimental to KD performance .

**Feature-based KD.** To circumvent these shortcomings, we focus on feature-based KD - an extension of vanilla KD concerned with the distillation of knowledge across the intermediate layers of models. In particular, we perform knowledge distillation of intermediate representations by devising a loss on selected hidden features \(H_{} U_{}\) and \(H_{} U_{}\) in the student and teacher models respectively, which takes the form:

\[_{}=_{}(_{ }(H_{}),_{}(H_{})),\] (3)

where \(_{}:U_{} U\) and \(_{}:U_{} U\) are transformations that map the hidden features to a common feature space \(U\), and \(_{}:U U^{+}\) is some loss of choice. The transformations \(_{},_{}\) can be the identity transformation, linear projections and multilayer perceptron (MLP) projection heads; whereas for the distillation loss \(_{}\), typical options include mean squared error (MSE) and mean absolute error (MAE).

The fundamental design decision when devising a KD strategy based on the distillation of internal representations is the choice of features to distill between (i.e., features \(H_{s}\) and \(H_{t}\)). One needs to ensure that paired features are similar both in expressivity and relevance to the output. Most research on feature-based distillation on graphs has so far focused on models that only have one type of scalar (node) features in classification tasks , reducing the problem to the selection of layers to pair across the student and the teacher. This is often further simplified by utilizing models that share the same architecture up to reducing the number of blocks/layers and their dimensionality.

**Features in molecular GNNs.** In contrast, molecular GNNs contain diverse features (e.g., scalars, vectors and/or equivariant higher-order tensors based on spherical harmonics) organized across nodes and edges within a complex molecular graph. These are continually evolved by model-specific operators to infer molecular properties, such as energy and forces, in a multi-output prediction fashion. Therefore, features often represent different physical, geometric and/or topological information relevant to specific parts of the output. This significantly complicates the design of an effective KD strategy, especially when the teacher and the student differ architecturally, as one needs to extract and align representations corresponding to comparable features in both models.

In this work, we set out to devise KD strategies that are representative and effective across various molecular GNNs. This is why we investigate the effectiveness of KD with respect to GNNs that have distinct architectures and performance profiles, and can be organized in teacher-student configurations at different levels of architectural disparity. In particular, we employ the following three GNN models, ordered by computational complexity (ascending):

* _SchNet_: A simple GNN model based on continuous-filter convolutional layers, which only contains scalar node features \(^{d}\). These are used to predict the energy \(\). The force is then calculated as the negative gradient of the energy with respect to the atomic positions, i.e., \(}=-\).
* used for energy prediction; as well as geometric vectorial node features \(^{3 d_{2}}\) that are equivariant to rotations and can thus be combined with the scalar features to make direct predictions of the forces (i.e., without computing gradients of the energy).
* _GemNet-OC_: A GNN model that utilizes directional message passing between scalar node features \(^{d_{h}}\) and scalar edges features \(^{d_{}}\). After each block of layers, these are processed through an output block, resulting in scalar node features \(_{}^{(i)}\) and edge features \(_{}^{(i)}\), where \(i\) is the block number. The output features from each block are aggregated into output features \(_{E}\) and \(_{F}\), which are used to compute the energy and forces respectively.

An overview of the features of the three models can be found in Table 1.

**Defining feature-based KD distillation strategies for molecular GNNs.** In the context of the three models considered in this work, we devise the following KD strategies:

   & SchNet & PaiNN & GemNet-OC \\  Scalar node features & ✓ & ✓ & ✓ \\ Scalar edge features & & & ✓ \\ Vectorial node features & & ✓ & \\ Output blocks & & & ✓ \\  

Table 1: An overview of the types of features available in the three models we use in this study.

- node-to-node (n2n):_ As all three models contain scalar node features \(H_{}\), we can distill knowledge in between these directly by defining a loss \(_{KD}\) given by

\[_{KD}=_{}(_{s}(H_{,s}), _{t}(H_{,t})),\] (4)

where \(H_{,s}\) and \(H_{,t}\) represent the node features of the student and teacher, respectively. Note this is a general approach that utilizes scalar node features only, making it applicable to standard GNNs. Here, we want to force the student to mimic the representations of the teacher for each node (i.e., atom) independently, so we use a loss that directly penalizes the distance between the features in the two models, such as MSE (similar to the original formulation of feature-based KD in Romero _et al._). Other recently proposed losses \(_{}\) for the distillation of node features in standard GNNs specifically include approaches based on contrastive learning  and adversarial training . We do not focus on such methods as much since they are better suited for (node) classification tasks (e.g., contrasting different classes of nodes), and not for molecule-level predictions.

To take advantage of other types of features relevant to molecular GNNs, we further devise three additional protocols, which we outline below.

- _edge-to-edge (e2e)_: The GemNet-OC model heavily relies on its edge features, which are a key component of the directional message passing employed in the architecture. As such, they can be a useful resource for KD. Hence, we also consider KD between edge features, which we accomplish by applying Equation (4) to the edge features \(H_{,s}\) and \(H_{,t}\) of the student and teacher, respectively.

- _edge-to-node (e2n)_: However, not all models considered in this study contain edge features to distill to. To accommodate that, we propose a KD strategy where we transfer information from GemNet-OC's edge features \(H_{,(i,j)}\) by first aggregating them as follows:

\[H_{,i}=_{j(i)}H_{,(i,j)},\] (5)

where \(i\) is some node index. The resulting features \(H_{,i}\) are scalar, node-level features, and we can, therefore, use them to transfer knowledge to the student node features \(H_{,s}\) as in Equation (4).

- _vector-to-vector (v2v):_ Similarly, the PaiNN model defines custom vectorial node features, which differ from the scalar (node and edge) features available in the other models. These are not scalar and invariant to rigid transformations of the atoms, but geometrical vectors that are equivariant with respect to rotations. As these carry important information about a given system, we also want to define a procedure to distill these. When we perform KD between two PaiNN models, we can directly distill information between these vectorial features just as in Equation (4). In contrast, when distilling knowledge into PaiNN from our GemNet-OC teacher that has no such vectorial features, we transfer knowledge between (invariant) scalar edge features and (equivariant) vectorial node features by noting that scalar edge features sit on an equivariant 3D grid since they are associated with an edge between two atoms in 3D space. Hence, we can aggregate the edge features \(\{H_{,(i,j)}\}_{j}\) corresponding to a given node \(i\) into node-level equivariant vectorial features \(H_{,i}\) by considering the unit vector \(_{ij}=_{j}-_{i}|}(_{j}-_{i})\) that defines the direction of the edge \((i,j)\), such that

\[H_{,i}^{(k)}=_{j(i)}_{ij}H_{,(i,j )}^{(k)},\] (6)

with the superscript \(k\) indicating the channel. Notice that the features \(H_{,i}^{(k)}\) fulfill the condition of equivariance with respect to rotations as each vector \(_{ij}\) is equivariant to rotations, and \(H_{,(i,j)}^{(k)}\) - a scalar not influencing its direction. Consequently, it is important to use a loss \(_{}\) that encourages vectors to align in both magnitude and direction - e.g., MSE.

**Additional KD strategies.** We further evaluate two additional KD approaches inspired by the vanilla logit-based KD used in classification, which we augment to make suitable for regression tasks:

- _Vanilla (1)_: One way of adapting vanilla KD for regression is by steering the student to mimic the final output of the teacher directly:

\[_{}=_{}_{}(_{s}, _{i})+_{}_{}(}_{s},}_{t}),\] (7)where the subscripts \({}_{ s}\) and \({}_{ t}\) refer to the predictions of the student and teacher, respectively. Note that, unlike in classification, this approach does not provide much additional information in regression tasks, except for some limited signal about the error distribution of the teacher model [20; 29].

- _Vanilla (2)_: One way to enhance the teacher signal during training is to consider the fact that many GNNs for molecular simulations make separate atom- and edge-level predictions, which are consequently aggregated into a final output. For instance, the total energy \(E\) of a system is usually defined as the sum of the predicted contributions from each atom \(=_{i}_{i}\). Hence, we can extend the aforementioned vanilla KD approach by imposing a loss on these granular predictions instead:

\[_{}=_{i=1}^{N}_{}( {}_{i, s},_{i, t}).\] (8)

These individual energy contributions are not part of the labeled data, but, when injected during training, can provide more fine-grained information than the aggregated prediction.

## 4 Experimental results

To evaluate our proposed methods, we perform comprehensive benchmarking experiments on the OC20-2M  dataset (structure to energy and forces (S2EF) task) - a large and diverse catalyst dataset; and COLL  - a challenging molecular dynamics dataset. We use the model implementations provided in the Open Catalyst Project (OCP) codebase3 (see Appendix A for detailed information about training procedure and model hyperparameters).

**Benchmarking baseline models.** We start by first evaluating the baseline performance of the models we employ in this study. As previously mentioned, we select SchNet, PaiNN and GemNet-OC for our experiments as they cover most of the accuracy-complexity spectrum, with the last representing the state-of-the-art on OC20 S2EF and COLL at the time of experimentation. To demonstrate this, we benchmark the predictive accuracy and inference throughput of the models on the two aforementioned datasets. In conjunction with the default PaiNN and GemNet-OC models, we also experiment with more lightweight versions of the two architectures - referred to as PaiNN-small and GemNet-OC-small respectively, where we reduce the number of hidden layers and their dimensionality. We train all models to convergence ourselves, except for the GemNet-OC model on OC20-2M, where we utilize the pre-trained model available within the OCP repository (July 2022).

We present our benchmarking results on OC20 S2EF in Table 2, which summarizes the performance of the five models with respect to the following four metrics: energy and force MAE (i.e., the mean absolute error between ground truth and predicted energies and forces); force cos (i.e., the cosine similarity between ground truth and predicted forces); and energy and forces within threshold (EFwT) - i.e., the percentage of systems whose predicted energies and forces are within a specified threshold from the ground truth . Since force cos and EFwT are correlated with energy and force MAE, we focus on the latter throughout the paper but present all four for completeness.

   &  &  \\  &  &  \\   & Samples / & Energy MAE & Force MAE & Force cos & EFwT \\ Model & GPU sec. \(\) & \(\) & \(/\) & \(\) & \% \(\) \\  SchNet & \(1100\) & \(1308\) & \(65.1\) & \(0.204\) & \(0\) \\ PaiNN-small & \(680\) & \(489\) & \(47.1\) & \(0.345\) & \(0.085\) \\ PaiNN & \(264\) & \(440\) & \(45.3\) & \(0.376\) & \(0.14\) \\ GemNet-OC-small & \(158\) & \(344\) & \(31.3\) & \(0.524\) & \(0.51\) \\ GemNet-OC & \(107\) & \(286\) & \(25.7\) & \(0.598\) & \(1.06\) \\  

Table 2: Evaluation of the performance of our five baseline models on the OC20 S2EF task. All models are trained on the OC20-2M dataset. Values represent the average across the four available validation sets. Results for individual validation datasets are provided in Appendix B.

Our results highlight the substantial trade-off between predictive accuracy and computational cost across the GNN architectures, and, therefore, the need for methods that can alleviate this limitation. We observe the same trend on the COLL dataset (see Appendix C).

**Similarity analysis of baseline models.** To make our analysis exhaustive, we set out to design experiments involving teacher and student architectures of a variable degree of architectural disparity. As a proxy of that, we derive similarity scores based on central kernel alignment (CKA) . In particular, we calculate the pairwise CKA similarity between the node features of our trained SchNet, PaiNN and GemNet-OC models. The results of this analysis are summarized in Figure 2. Focusing on intra-model similarities first (plots on the diagonal), we observe that, while representations from different layers within PaiNN and SchNet have a generally high degree of similarity, GemNet-OC exhibits the opposite behavior, with features extracted at each layer being significantly different from those captured across the rest of the architecture. This is consistent with the architectures of these three models, with features in PaiNN and SchNet being iteratively updated by adding displacement features computed at each layer, while those in GemNet-OC representing separate output features. When examining inter-model similarity instead, we notice that, generally speaking, node features in SchNet and PaiNN are similar, whereas those between SchNet and GemNet-OC, and PaiNN and GemNet-OC, diverge significantly as we move deeper into GemNet-OC.

**Knowledge distillation results.** Based on the aforementioned analyses, we define the following teacher-student pairs, covering the whole spectrum of architectural disparity: PaiNN to PaiNN-small (_same_ architecture); PaiNN to SchNet (_similar_ architectures); GemNet-OC to PaiNN (_different_ architectures). We additionally explore KD from GemNet-OC to GemNet-OC-small (_same_ architecture) on OC20. We train student models by utilizing an offline KD strategy , where we distill knowledge from the more competent, pre-trained teacher model to the simpler, more lightweight student model during the training of the latter. We augment the training of each student model with our KD protocols and evaluate the effect on predictive accuracy against the models trained without KD. If not mentioned otherwise, we utilize the following setup: we use MSE as a distillation loss \(_{}\); a learned linear layer as a transformation function \(_{s}\) applied to the features of the student; and the identity transformation as \(_{t}\). When distilling knowledge from/into GemNet-OC models, we use the aggregated node- and edge-level output features, which is reminiscent of the review-based setup proposed in . For PaiNN and SchNet, we use the final node features.

The results of our experiments are summarized in Tables 3 and 4, presenting a comparative analysis of the predictive performance of different student models trained with and without the implementation of knowledge distillation on the OC20-2M and COLL datasets, respectively. Focusing on energy predictions first, we observe that, by utilizing KD, we achieve significant improvements in performance in virtually all teacher-student configurations. In particular, we manage to close the gap in performance between student and teacher models by \( 60\%\) or more in six out of the seven configurations, reaching results as high as \(96.7\%\) (distilling PaiNN to PaiNN-small on the COLL dataset). Putting our results into context, we remark that our PaiNN model trained with _n2n_ KD from GemNet-OC, for instance, provides more accurate energy predictions than more advanced models such as GemNet-dT  which is substantially slower. The only setup where results are not as definite is when training SchNet on OC20-2M with KD from PaiNN, where we close \(10.8\%\). However, it is noteworthy to highlight that this still corresponds to a significant absolute improvement (i.e., notice the big initial difference in performance between the two baseline models on this dataset).

We similarly observe an improvement in the accuracy of student models in force predictions in all teacher-student configurations. Although we observe force improvements (typically \(\)\(5\)-\(25\%\)) that are generally not as pronounced as those achieved in energy prediction, we note that we reach

Figure 2: Similarity analysis between the node features of SchNet, PaiNN and GemNet-OC using CKA (averaged over \(n=987\) nodes).

[MISSING_PAGE_FAIL:8]

student architecture. We notice similar behavior across other teacher-student configurations and KD strategies (see Figures 6 and 7 in Appendix G), allowing us to monitor and quantify the effect of KD on student models as we explore different KD settings and design choices.

**Hyperparameter studies.** We additionally conduct a thorough hyperparameter study to evaluate the effect of different design choices within our KD framework. We summarize our results below.

- _Effect of distillation loss_: Apart from our default MSE-based distillation loss \(_{}\), we also experimented with more advanced losses such as Local Structure Preservation (LSP)  and Global Structure Preservation (GSP) , as well as directly optimizing CKA. We observed the best results with our default MSE loss, with other options substantially hurting accuracy (see Appendix E.1).

- _Effect of transformation function_: We also investigated a number of different transformation functions and the effect they have on performance. The transformations we utilized include: the identity transformation (when appropriate); learned linear transformations, and MLP projection heads. Our results showed that our default linear mapping is a sufficiently flexible choice as it gives the best results(see Appendix E.2).

- _Effect of feature selection_: We additionally explored the effect of feature selection on KD performance. In particular, we analyzed the change in the predictive accuracy of PaiNN as a student model as we distill features from earlier layers in the teacher (GemNet-OC), or distill knowledge into earlier layers in the student. Our results suggest that using features closer to the output is the best-performing strategy (see Appendix E.3). We also performed CKA-based similarity analyses to monitor how model similarity changes as we vary the features we used for KD (see Figure 8 in Appendix G).

**Data augmentation.** As for most other applications, data labeling for molecular data is costly as it requires running computationally expensive quantum mechanical calculations to obtain ground truth energies and forces. Motivated by this, we explore two data augmentation techniques, which we use to generate new data points that we label with the teacher and use for KD. We briefly describe these below (see Appendix D for more details).

   & &  \\   & &  & Force MAE & Force cos & EFwT \\  & Model & \(\) & \(/\) & \(\) & \% \(\) \\    & _S_: PaiNN-small & \(104.0\) & \(80.9\) & \(0.984\) & \(5.4\) \\  & _T_: PaiNN & \(85.8\) & \(64.1\) & \(0.988\) & \(10.1\) \\   & _Vanilla (1)_ & \(106.1(-11.5\%)\) & \(82.0(-6.5\%)\) & \(0.984(2.3\%)\) & \(4.46(-20.2\%)\) \\   & _Vanilla (2)_ & \(\) (\(\)) & \(80.9(0\%)\) & \(0.983(-2.3\%)\) & \(4.3(-23.7\%)\) \\   & _n2n_ & \(92.5(63.2\%)\) & \(77.8(18.5\%)\) & \(0.984(18.2\%)\) & \(\) (\(\)) \\   & _v2v_ & \(90.4(74.7\%)\) & \(\) (\(\)) & \(\) (\(\)) & \(5.8(8.4\%)\) \\   & _S_: SchNet & \(146.5\) & \(121.2\) & \(0.970\) & \(2.75\) \\  & _T_: PaiNN & \(85.8\) & \(64.1\) & \(0.988\) & \(10.1\) \\   & _Vanilla (1)_ & \(146.1(0.7\%)\) & \(120.8(0.7\%)\) & \(0.970(1.1\%)\) & \(2.54(-2.9\%)\) \\   & _Vanilla (2)_ & \(\) (\(\)) & \(120.9(0.5\%)\) & \(0.970(1.1\%)\) & \(\) (\(\)) \\   & _n2n_ & \(141.6(8.1\%)\) & \(\) (\(\)) & \(\) (\(\)) & \(2.63(-1.6\%)\) \\   & _S_: PaiNN & \(85.8\) & \(64.1\) & \(0.988\) & \(10.1\) \\  & _T_: GemNet-OC & \(44.8\) & \(38.2\) & \(0.994\) & \(20.2\) \\   & _Vanilla (1)_ & \(86.2(-1.1\%)\) & \(63.9(0.6\%)\) & \(0.988(1.5\%)\) & \(10.1(0.1\%)\) \\   & _Vanilla (2)_ & \(61.4(59.5\%)\) & \(62.9(4.6\%)\) & \(0.988(5.2\%)\) & \(13.0(29.2\%)\) \\   & _n2n_ & \(\) (\(\)) & \(\) (\(\)) & \(\) (\(\)) & \(\) (\(\)) \\   & _e2n_ & \(77.3(20.8\%)\) & \(63.3(3.0\%)\) & \(0.988(7.9\%)\) & \(11.0(9.2\%)\) \\   & _v2v_ & \(81.2(11.2\%)\) & \(63.3(3.1\%)\) & \(0.988(3.4\%)\) & \(10.5(4.6\%)\) \\  

Table 4: Evaluation results on the COLL test set. Numbers in brackets represent the proportion of the gap between the student (_S_) and the teacher (_T_) that has been closed by the respective KD strategy (in %). Best results are given in **bold**.

- _Random rattling:_ Adding noise to existing structures (also known as "rattling") is a form of data augmentation that has been used in the context of pretraining of molecular GNNs [42; 43], and as a regularization strategy . Inspired by this, we utilized "rattling" in the context of KD, where we added random noise to the atomic positions of systems and used the teacher to derive energy and force labels for these rattled samples. We then combined these rattled structures with the original dataset during the training of student models. However, this approach did not provide significant improvements. Additionally, we tried using gradient ascent to find perturbations that maximize the discrepancy between the teacher and student predictions, similar to , but this did not show improvements over random noise, and also increased training time.

- _Synthetic Data:_ Samples in OC20 S2EF originate from the same relaxation trajectory and are therefore correlated. To tackle this, we generated our own distilled dataset coined \(d\), which consists of one million samples generated by sampling new systems (generated with the OC Datasets codebase 4), running relaxations with our pre-trained GemNet-OC model, and then subsampling approximately \(10\%\) of the frames. We explored different ways of incorporating this new \(d\) dataset, all of which were based on joint training with the OC20 S2EF 2M data (similar to what we did with the rattled systems). To study different combinations of the ground truth DFT samples and the \(d\) samples during training, we defined two hyperparameters determining: (a) how many of the samples per batch originate from each of the datasets; and (b) how to weight the loss contributions based on the origin of data. Unfortunately, and contrary to similar approaches, e.g., in speech recognition , the results we observed did not significantly improve on the baseline models.

## 5 Conclusion

In this paper, we investigated the utility of knowledge distillation in the context of GNNs for molecules. To this end, we proposed four distinct feature-based KD strategies, which we validated across different teacher-student configurations and datasets. We showed that our KD protocols can significantly enhance the performance of different molecular GNNs without any modifications to their architecture, allowing us to run faster molecular simulations without substantially impairing predictive accuracy. With this work, we aim to elucidate the potential of KD in the domain of molecular GNNs and stimulate future research in the area. Interesting future directions include: the combination of KD strategies (e.g., \(n2n\) and \(v2v\)); extending the framework to other types of features (e.g., tensorial features ), molecular tasks and datasets; better understanding the connection between KD performance and model expressivity (e.g., can model similarity inform KD design); and performing a more comprehensive stability analysis . One caveat of our approach is that even though inference times are not affected, training times are, albeit not necessarily if a pre-trained teacher model is available (see Appendix H). Finally, it is important to recognize that such technologies, while innovative, could be used for potentially harmful purposes, such as the simulation or discovery of toxic systems, or the development of harmful technologies.

Figure 3: Similarity analysis between Gemnet-OC and PaiNN without KD (left) and with KD (right). The feature pair used during KD is indicated with \(\). Similarity analyses for other KD strategies and teacher-student configurations are presented in Appendix G.