# Structured Semidefinite Programming

for Recovering Structured Preconditioners

 Arun Jambulapati

Simons Institute

jmblpati@berkeley.edu

&Christopher Musco

New York University

cmusco@nyu.edu

&Jerry Li

Microsoft Research

jerrl@microsoft.com

&Kirankumar Shiragur

Broad Institute of MIT and Harvard

shiragur@stanford.edu

&Aaron Sidford

Stanford University

sidford@stanford.edu

&Kevin Tian

University of Texas at Austin

kjtian@cs.utexas.edu

Work completed at Stanford and the University of Washington.Work completed at Stanford.Work completed at Stanford and Microsoft Research.

###### Abstract

We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including:

* **Diagonal preconditioning.** We give an algorithm which, given positive definite \(^{d d}\) with \(()\) nonzero entries, computes an \(\)-optimal diagonal preconditioner in time \((()(^{ },^{-1}))\), where \(^{}\) is the optimal condition number of the rescaled matrix.
* **Structured linear systems.** We give an algorithm which, given \(^{d d}\) that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in \(\) in \((d^{2})\) time.

Our diagonal preconditioning results improve state-of-the-art runtimes of \((d^{3.5})\) attained by general-purpose semidefinite programming, and our solvers improve state-of-the-art runtimes of \((d^{})\) where \(>2.3\) is the current matrix multiplication constant. We attain our results via new algorithms for a class of semidefinite programs (SDPs) we call _matrix-dictionary approximation SDPs_, which we leverage to solve an associated problem we call _matrix-dictionary recovery_.

## 1 Introduction

Preconditioning is a fundamental primitive in the theory and practice of numerical linear algebra, optimization, and data science. Broadly, its goal is to improve conditioning properties (e.g., the range of eigenvalues) of a matrix \(\) by finding another matrix \(\) which approximates the inverse of \(\) and is more efficient to construct and apply than computing \(^{-1}\). This strategy underpins a variety of popular recently-developed tools, such as adaptive gradient methods for machine learning (e.g., Adagrad and Adam ), and near-linear time solvers for combinatorially-structuredmatrices (e.g., graph Laplacians ). Despite widespread practical adoption of such techniques, there is a surprising lack of provably efficient algorithms for preconditioning.

Our work introduces a new tool, _matrix-dictionary recovery_, and leverages it to obtain the first near-linear time algorithms for several structured preconditioning problems in well-studied applications. Informally, the problem we study is as follows (see Section 4 for the formal definition).

\[&}}\{_{i}\}}\\ &=_{i}w_{i}_{i}}}\{_{i}\}.\] (1)

We develop general-purpose solvers for the problem (1). We further apply these solvers to obtain state-of-the-art algorithms for fundamental tasks such as preconditioning linear systems and regression, and approximately recovering structured matrices, including the following results.

* **Diagonal preconditioning.** We consider the classical numerical linear algebra problem of _diagonal preconditioning_. Given \(_{ 0}^{d}\), the goal is to find a diagonal \(_{ 0}^{d}\) minimizing the condition number of \(^{}^{}\). Theorem 1 obtains the first near-linear time algorithms for this problem when the optimal condition number of the rescaled matrix is small.
* **Semi-random regression.** We consider a related problem, motivated by semi-random noise models, which takes full-rank \(^{n d}\) with \(n d\) and seeks \(_{ 0}^{n}\) minimizing the condition number of \(^{}\). Theorem 2 gives the first near-linear time algorithm for this problem, and applications of it reduce risk bounds for statistical linear regression.
* **Structured linear systems.** We robustify Laplacian system solvers, e.g., , to obtain near-linear time solvers for systems in dense matrices well-approximated spectrally by Laplacians in Theorem 3. We also give new near-linear time solvers for several families of structured matrices, e.g., dense inverse Laplacians and M-matrices,4 in Theorems 4 and 5. 
For the preconditioning problems considered in Theorems 1, 2, and 3, we give the first runtimes faster than a generic SDP solver, for which, state-of-the-art runtimes  are highly superlinear (\((d^{3.5})\) for diagonal preconditioning and \((d^{2})\) for approximating Laplacians, where \(d\) is the matrix dimension and \(>2.3\) is the current matrix multiplication constant ). For the corresponding linear system solving problems in each case, as well as in Theorems 4 and 5, the prior state-of-the-art was to treat the linear system as generic and ran in \((d^{})\) time.

**Organization.** We begin by overviewing the main applications of our matrix-dictionary recovery framework in Sections 2 and 3, which respectively cover our results on diagonal preconditioning and structured linear algebra. These sections are self-contained (with some definitions of the matrix families we study in Section 3 deferred to the supplementary material), and can be read independently. In Section 4, we contextualize and formalize the general matrix-dictionary recovery problem (1) we introduce and study. We also provide our main meta-algorithm and its guarantees, and an overview of how the results of Sections 2 and 3 follow from applications of it. We finally compare our framework to related algorithms and give a more thorough runtime comparison in Section 5.

**Notation.** In Section 2 (focusing on diagonal preconditioning and semi-random regression) only, we refer to the matrices to be preconditioned as \(\) or \(\). This is for consistency with the numerical linear algebra literature, where \(\) represents a positive definite kernel matrix, and \(\) denotes the data in a regression problem \(_{x}\|x-b\|_{2}\). In the rest of the paper, our notation will be consistent with (1). The \(d d\) symmetric matrices are \(^{d}\), the positive semidefinite (PSD) and definite (PD) cones are \(_{ 0}^{d}\) and \(_{ 0}^{d}\); the remainder of our notation is standard and deferred to Section 2 of the supplement.

## 2 Diagonal preconditioning

When solving linear systems via iterative methods, one of the most popular preconditioning strategies is to use a diagonal matrix. This is appealing because diagonal matrices can be applied and inverted quickly. Determining the best diagonal preconditioner is a classical numerical linear algebra problem studied since the 1950s , and has gained recent popularity due to its use in adaptive gradient methods . In Section 4, we discuss how diagonal preconditioningis an instance of (1) in the matrix-dictionary \(_{i}=e_{i}e_{i}^{}\), where \(e_{i}\) is the \(i^{}\) basis vector. Leveraging this viewpoint, we design algorithms for two natural instantiations of diagonal preconditioning.

**Outer scaling.** One formulation of the optimal diagonal preconditioning problem, which we refer to as _outer scaling_, asks to optimally reduce the condition number of positive definite \(^{d d}\) with a diagonal matrix \(\), i.e., return diagonal \(=(w)\) for \(w_{>0}^{d}\) such that5

\[(^{}^{}) _{o}^{}():=_{\,}(^{}^{}).\]

Given \(\), a solution to \(x=b\) can be obtained by solving the better-conditioned \(^{}^{}y=^{ {1}{2}}b\) and returning \(x=^{}y\). The optimal \(\) can be obtained via a semidefinite program (SDP) , but the computational cost of general-purpose SDP solvers outweighs benefits for solving linear systems. Outer scaling is poorly understood algorithmically; prior to our work, even attaining a constant-factor approximation to \(_{o}^{}()\) without a generic SDP solver was unknown.

This state of affairs has resulted in the widespread use of heuristics for constructing \(\), such as _Jacobi preconditioning_ and _matrix scaling_. The former strategy, where the preconditioner is taken as the inverse diagonal to \(\), was notably highlighted by Adagrad , which used Jacobi preconditioning to improve computational costs. However, both heuristics have clear drawbacks from theoretical or practical perspectives.

Prior to our work the best approximation guarantee known for Jacobi preconditioning was a result of van der Sluis , which shows the Jacobi preconditioner is an \(m\)-factor approximation to the optimal preconditioning problem where \(m d\) is the maximum number of non-zeros in any row of \(\): in dense matrices this is linear in the problem dimension and can be much larger than \(_{o}^{}()\). We review and slightly strengthen this result in Appendix C of the supplement. We also prove a new _dimension-independent_ baseline result of independent interest: the Jacobi preconditioner obtains condition number no worse than \((_{o}^{}())^{2}\). Unfortunately, we exhibit a family of matrices showing this bound is tight, dashing hopes they solve outer scaling near-optimally. On the other hand, while sometimes effective as a heuristic , matrix scaling algorithms target a different objective (normalizing row and column sums) and do not yield provable guarantees on \((^{}^{})\).

**Inner scaling.** Another formulation of diagonal preconditioning, which we refer to as _inner scaling_, takes as input a full-rank \(^{n d}\) and asks to find an \(n n\) positive diagonal \(\) with

\[(^{})_{i}^{}( ):=_{\,}( ^{}).\]

As a comparison, when outer scaling is applied to the kernel matrix \(=^{}\), \(^{}^{}\) can be seen as rescaling the columns of \(\). On the other hand, in inner scaling we instead rescale rows of \(\). Inner scaling has natural applications to improving risk bounds in a robust statistical variant of linear regression, which we comment upon shortly. Nonetheless, as in the outer scaling case, no algorithms faster than general SDP solvers are known to obtain even a constant-factor approximation to \(_{i}^{}()\). Further, despite clear problem similarities, it is unclear how to best extend heuristics (e.g., Jacobi preconditioning and matrix scaling) for outer scaling to inner scaling.

**Our results.** We give the first nontrivial approximation algorithms (beyond generic SDP solvers) for both the outer and inner scaling problems, yielding diagonal preconditioners attaining constant-factor approximations to \(_{o}^{}\) and \(_{i}^{}\) in near-linear time.6\(_{}()\) denotes the time required to multiply a vector by \(\); this is at most the sparsity of \(\), but can be substantially faster for structured \(\).

**Theorem 1** (Outer scaling).: _Let \(>0\) be a fixed constant.7 There is an algorithm, which given full-rank \(_{ 0}^{d}\) computes \(w_{ 0}^{d}\) such that \((^{}^{})(1+ )_{o}^{}()\) with probability \( 1-\) in time \(O(_{}()(_{o}^{}())^{1.5}(^{}()}{}))\)._

**Theorem 2** (Inner scaling).: _Let \(>0\) be a fixed constant. There is an algorithm, which given full-rank \(^{n d}\) for \(n d\) computes \(w_{ 0}^{n}\) such that \((^{})(1+)_{i}^{ }()\) with probability \( 1-\) in time \(O(_{}()(_{i}^{}())^{1.5}(^{}()}{}))\)._Our methods pay a small polynomial overhead in the quantities \(^{*}_{o}\) and \(^{*}_{i}\), but notably suffer no dependence on the _original conditioning_ of the matrices. Typically, the interesting use case for diagonal preconditioning is when \(^{*}_{o}()\) or \(^{*}_{i}()\) is small but \(()\) or \((^{})\) is large, a regime where our runtimes are near-linear and substantially faster than directly applying iterative methods.

It is worth noting that in light of our new results on Jacobi preconditioning, the end-to-end runtime of Theorem 1 for solving linear systems (rather than optimal preconditioning) can be improved: accelerated gradient methods on a preconditioned system with condition number \((^{*}_{o})^{2}\) have runtimes scaling as \(^{*}_{o}\). That said, when repeatedly solving multiple systems in the same matrix, Theorem 1 may offer an advantage over Jacobi preconditioning. Our framework also gives a potential route to achieve the optimal end-to-end runtime scaling as \(_{o}}\), detailed in Appendix D of the supplement.

Beyond that which is obtainable by black-box using general SDP solvers, we are not aware of any other claimed runtime in the literature for solving the inner and outer scaling problems considered in Theorems 1 and 2. Directly using state-of-the-art SDP solvers [JKL\({}^{+}\)20, HJS\({}^{+}\)22] incurs substantial overhead \((n^{}+nd^{2.5})\) or \((n^{}+d^{4.5}+n^{2})\), where \(<2.372\) is the current matrix multiplication constant . For outer scaling, where \(n=d\), this implies an \((d^{3.5})\) runtime; for other applications, e.g., preconditioning \(d d\) perturbed Laplacians where \(n=d^{2}\), the runtime is \((d^{2})\). Applying state-of-the-art approximate SDP solvers (rather than our custom ones, i.e., Theorems 6 and 7) appears to yield runtimes \((() d^{2.5})\), as described in Appendix E.2 of . This is in contrast with our Theorems 1, 2 which achieve \((()(^{})^{1.5})\). Hence, we improve existing tools by \((d)\) factors in the main regime of interest where the optimal rescaled condition number \(^{}\) is small. Concurrent to our work, [QGH\({}^{+}\)22] gave algorithms for constructing optimal diagonal preconditioners using interior point methods for SDPs, which run in at least the superlinear times discussed previously.

**Statistical aspects of preconditioning.** Unlike an outer scaling, a good inner scaling does not speed up a least squares regression problem \(_{x}\|x-b\|_{2}\). Instead, it allows for a faster solution to the reweighted problem \(_{x}\|^{}(x-b)\|_{2}\). This has a number of implications from a statistical perspective. We explore an interesting connection between inner scaling preconditioning and _semi-random_ noise models for least-squares regression, situated in the literature in Section 5.

As a motivating example of our noise model, consider the case when there is a hidden parameter vector \(x_{}^{d}\) that we want to recover, and we have a "good" set of consistent observations \(_{g}x_{}=b_{g}\), in the sense that \((_{g}^{}_{g})\) is small. Here, we can think of \(_{g}\) as being drawn from a well-conditioned distribution. Now, suppose an adversary gives us a superset of these observations \((,b)\) such that \(x_{}=b\), and \(_{g}\) are an (unknown) subset of rows of \(\), but \((^{})(_{g}^{} _{g})\). This can occur when rows are sampled from heterogeneous sources. Perhaps counterintuitively, by giving additional consistent data, the adversary can arbitrarily hinder the cost of iterative methods. This failure can be interpreted as being due to overfitting to generative assumptions (e.g., sampling rows from a well-conditioned covariance, instead of a mixture): standard iterative methods assume too much structure, where ideally they would use as little as information-theoretically possible.

Our inner scaling methods can be viewed as "robustifying" linear system solving to such semi-random noise models (by finding \(\) yielding a rescaled condition number comparable or better than the indicator of the rows of \(_{g}\), which are not known a priori). In Section 6 of the supplement, we demonstrate applications of inner scaling in reducing the mean-squared error risk in statistical regression settings encompassing our semi-random noise model, where the observations \(b\) are corrupted by (homoskedastic or heteroskedastic) noise. In all settings, our preconditioning algorithms yield computational gains, improved risk bounds, or both, by factors of roughly \((^{})/^{}_{i}()\).

## 3 Robust linear algebra for structured matrices

Over the past decade, the theoretical computer science and numerical linear algebra communities have dedicated substantial effort to developing faster solvers for regression problems in various families of combinatorially-structured matrices. Perhaps the most prominent example is , who gave a near-linear time solver for linear systems in graph Laplacian matrices.8 A long line of exciting work has obtained improved solvers for these systems , which have been used to improve the runtimes for a wide variety of graph-structured problems, including maximum flow , sampling random spanning trees , graph clustering , and more . Additionally, efficient linear system solvers have been developed for solving systems in other types of structured matrices, e.g., block diagonally dominant systems , M-matrices , and directed Laplacians .

**Perturbations of structured matrices.** Despite the importance of these matrices with combinatorial structure, previously-developed solvers are in some ways quite brittle. For example, there are simple matrix families closely related to Laplacians for which the best-known runtimes for solving linear systems are achieved by ignoring problem structure, and using generic matrix multiplication techniques as a black box. Perhaps the simplest example is solving systems in _perturbed Laplacians_, i.e., matrices which admit constant-factor approximations by a Laplacian matrix, but which are not Laplacians themselves. This situation can arise when a Laplacian is used to approximate a physical phenomenon . We show that the framework we develop for (1) yields, as a consequence, robustifications and recovery routines building upon previously-developed solvers for structured linear systems. As a first example, we give the following perturbed Laplacian solver.

**Theorem 3** (Perturbed Laplacian solver).: _Let \(^{n n}\) be such that there exists an (unknown) Laplacian \(\) with \(^{*}\), and that \(\) corresponds to a graph with edge weights between \(w_{}\) and \(w_{}\), with \(}{w_{}} U\). For any \((0,1)\) and \(>0\), there is an algorithm recovering a Laplacian \(^{}\) with \(^{}(1+)^{*}\) with probability \( 1-\) in time \(O(n^{2}(^{*})^{2}(U}{ }}{}))\). Consequently, there is an algorithm for solving linear systems in \(\) to \(\)-relative accuracy with probability \( 1-\), in time \(O(n^{2}(^{*})^{2}(U}{ }}{}))\)._9__

Theorem 3 can be viewed as solving a preconditioner construction problem, where we know there exists a Laplacian matrix \(\) which spectrally resembles \(\), and wish to efficiently recover a Laplacian with similar guarantees. Our matrix-dictionary recovery framework (1) captures the setting of Theorem 3 by leveraging an appropriate matrix-dictionary of edge Laplacians, discussed in Section 4. The conceptual message of Theorem 3 is that near-linear time solvers for Laplacians robustly extend through our preconditioning framework to efficiently solve matrices approximated by Laplacians. Beyond this specific application, our framework could be used to solve perturbed generalizations of future families of structured matrices.

**Recovery of structured matrices.** In addition to directly spectrally approximating and solving in matrices which are well-approximated by preconditioners with diagonal or combinatorial structure, our framework also yields solvers for new families of matrices. We show that our preconditioning techniques can be used in conjunction with properties of graph-structured matrices to provide solvers and spectral approximations for _inverse M-matrices_ and _Laplacian pseudoinverses_. Recovering Laplacians from their pseudoinverses and solving linear systems in the Laplacian pseudoinverse arise when trying to fit a graph to data or recover a graph from effective resistances, a natural distance measure (see  for motivation and discussion of related problems). More broadly, the problem of solving linear systems in inverse symmetric M-matrices is prevalent and corresponds to statistical inference problems involving distributions that are multivariate totally positive of order 2 (\(_{2}\)) . Our main results are the following.

**Theorem 4** (M-matrix recovery and inverse M-matrix solver).: _Let \(\) be the inverse of an unknown invertible symmetric M-matrix, let \(\) upper bound its condition number, and let \(U\) be the multiplicative range of \(\).10 For any \((0,1)\) and \(>0\), there is an algorithm recovering a \((1+)\)-spectral approximation to \(^{-1}\) in time \(O(n^{2}(}{}))\). Consequently, there is an algorithm for solving linear systems in \(\) to \(\)-relative accuracy with probability \( 1-\), in time \(O(n^{2}())\)._

**Theorem 5** (Laplacian recovery and Laplacian pseudoinverse solver).: _Let \(\) be the pseudoinverse of unknown Laplacian \(\), and that \(\) corresponds to a graph with edge weights between \(w_{}\) and \(w_{}\), with \(}{w_{}} U\). For any \((0,1)\) and \(>0\), there is an algorithm recovering a Laplacian \(^{}\) with \(^{}^{}(1+)^{}\) in time \(O(n^{2}(}{}))\). Consequently, there is an algorithm for solving linear systems in \(\) to \(\)-relative accuracy with probability \( 1-\), in time \(O(n^{2}())\)._Theorems 4 and 5 are perhaps a surprising demonstration of the utility of our techniques: just because a matrix family is well-approximated by structured preconditioners, it is not a priori clear that their inverses also are. However, we show that by applying recursive preconditioning tools in conjunction with our recovery methods, we can obtain analogous results for these inverse families. These results add to the extensive list of combinatorially-structured matrix families admitting efficient linear algebra primitives. We view our approach as a proof-of-concept of further implications in designing near-linear time system solvers for structured families via algorithms for (1).

Similarly to our results in Section 2, our results on solving matrix-dictionary recovery for graph-structured matrices (Theorems 3, 4, and 5) are the first we are aware of with runtimes improving upon black-box generic algorithms. In particular, for key matrices in each of these cases (e.g., constant-factor spectral approximations of Laplacians, inverse M-matrices, and Laplacian pseudoinverses) we obtain \((n^{2})\) time algorithms for solving linear systems in these matrices to inverse polynomial accuracy. This runtime is near-linear when the input is dense and in each case when the input is dense the state-of-the-art prior methods were to run general linear system solvers using \(O(n^{})\) time.

## 4 Matrix-dictionary recovery: a general preconditioning framework

Our general strategy for matrix-dictionary recovery, i.e., recovering preconditioners in the sense of (1), is via applications of a new custom approximate solver we develop for a family of structured SDPs. SDPs are fundamental optimization problems that have been the source of extensive study for decades , with numerous applications across operations research and theoretical computer science , statistical modeling , and machine learning . Though there have been recent advances in solving general SDPs (e.g.,  and references therein), the current state-of-the-art solvers have superlinear runtimes, prohibitive in large-scale applications. Consequently, there has been extensive research on designing faster approximate SDP solvers under different assumptions .

We now provide context for our solver for structured "matrix-dictionary approximation" SDPs, state our algorithm and its guarantees, and summarize how it is used to obtain Theorems 1, 2, 3, 4, and 5.

**Positive SDPs.** One prominent class of structured SDPs are what we refer to as _positive SDPs_, namely SDPs in which the cost and constraint matrices are all positive semidefinite (PSD), a type of structure present in many important applications , including those in this paper. Positive SDPs generalize positive linear programming, itself a well-studied problem over the past several decades . It was recently shown that a prominent special case of positive SDPs known as _packing SDPs_ can be solved in nearly-linear time , a fact that has had numerous applications in robust learning and estimation  as well as in combinatorial optimization . However, extending known packing SDP solvers to broader classes of positive SDPs, e.g. mixed packing-covering SDPs has been elusive , and is a key open problem in the algorithmic theory of structured optimization.11 The mixed packing-covering SDP problem is parameterized by "packing" and "covering" matrices \(\{_{i}\}_{i[n]},,\{_{i}\}_{i[n]},_{-}^{d}\), and asks to find the smallest \(>0\) such that there exists \(w_{ 0}^{n}\) with \(_{i[n]}w_{i}_{i}\) (packing into \(\)) and \(_{i[n]}w_{i}_{i}\) (covering \(\)). Redefining \(_{i}^{-}_{i} ^{-}\), \(_{i}^{-}_{i}^{- }\) for all \(i[n]\), (a slight strengthening of) this problem is equivalent to finding \(w_{ 0}^{n}\) such that

\[_{i[n]}w_{i}_{i}_{i[n]}w_{i} _{i},\] (2)

or refuting its existence. This was studied by , and an important open problem in structured convex programming is designing a "width-independent" solver for testing feasibility of (2) up to a \(1+\) factor (i.e. testing whether (2) is approximately feasible with an iteration count polynomial in \(^{-1}\) and polylogarithmic in other parameters). Such solvers have remained elusive beyond pure packing SDPs , even for basic extensions such as pure covering.

**Matrix-dictionary approximation SDPs.** In Theorem 6, we develop our main meta-algorithm, an efficient solver for specializations of (2) where the packing and covering matrices \(\{_{i}\}_{i[n]},\{_{i}\}_{i[n]}\)are multiples of each other. As we will see, this family of structured SDPs, which we call _matrix-dictionary approximation SDPs_, is highly effective for capturing the forms of approximation required by (1). All our aforementioned preconditioning results follow via careful applications of our matrix-dictionary approximation SDP solver in Theorem 6 (and a generalization of it in Theorem 7).

Specifically, we develop efficient algorithms for the following main meta-problem we study. Given a set of matrices (a "matrix-dictionary") \(\{_{i}\}_{i[n]}_{ 0}^{d}\), a constraint matrix \(_{>0}^{d}\), a tolerance parameter \((0,1)\), and \(^{} 1\) such that there exists \(w^{}_{ 0}^{n}\) with

\[_{i[n]}w_{i}^{}_{i}^{ },\] (3)

the goal of matrix-dictionary approximation is to return weights \(w_{ 0}^{n}\) such that

\[_{i[n]}w_{i}_{i}(1+)^ {}.\] (4)

When \(=\), the problem in (3), (4) is a special case of (2) where each \(_{i}=_{i}=^{}_{i}\); we call this the isotropic case. We further handle general \(\), and demonstrate that our formulation captures many interesting applications. We refer to the problem in (3), (4) as _matrix-dictionary recovery_.

Our results: matrix-dictionary recovery.Our results concerning (3) and (4) assume that the matrix-dictionary \(\{_{i}\}_{i[n]}\) is "simple" in two respects. First, we assume that we have explicit factorizations

\[_{i}=_{i}_{i}^{},\ _{i} ^{d m}.\] (5)

Our applications in Sections 2 and 3 satisfy this assumption with \(m=1\). Second, denoting \((w):=_{i[n]}w_{i}_{i}\), we assume we can approximately solve systems in \((w)+\) for any \(w_{ 0}^{n}\) and \( 0\). Concretely, for any \(>0\), we assume there is a linear operator \(}_{w,,}\) which we can compute and apply in \(_{}^{}\) time,12 and that \(}_{w,,}((w)+ )^{-1}\) in that:

\[\|}_{w,,}v-((w)+ )^{-1}v\|_{2}\ \ v^{d}.\] (6)

In this case, we say "we can solve in \(\) to \(\)-relative accuracy in \(_{}^{}\) time." If \(\) is a single matrix \(\), we say "we can solve in \(\) to \(\)-relative accuracy in \(_{}^{}\) time." Notably, for the matrix-dictionary in our applications, e.g., diagonal \(1\)-sparse matrices or edge Laplacians, such access to \(\{_{i}\}_{i[n]}\) exists so we obtain end-to-end efficient algorithms. Ideally (for near-linear time algorithms), \(_{}^{}\) is roughly the total sparsity of \(\{_{i}\}_{i[n]}\), which holds in all our applications.

Under these assumptions, we give the following novel meta-solvers for matrix-dictionary recovery.13

**Theorem 6** (Matrix dictionary recovery, isotropic case).: _Given matrices \(\{_{i}\}_{i[n]}\) with explicit factorizations (5), such that (3) is feasible for \(=\) and some \(^{} 1\), we can return weights \(w_{ 0}^{n}\) satisfying (4) with probability \( 1-\) in time_

\[O(_{}(\{_{i}\}_{i[n]})(^{ })^{1.5}(}{}}{ })).\]

Here \(_{}(\{_{i}\}_{i[n]})\) denotes the computational complexity of multiplying an arbitrary vector by _all_ matrices in \(\{_{i}\}_{i[n]}\). Notably, in the isotropic case \(=\), Theorem 6 does not require solvers in the sense of (6). We next state our solver which handles the case of general \(\), under access to (6).

**Theorem 7** (Matrix dictionary recovery, general case).: _Given matrices \(\{_{i}\}_{i[n]}\) with explicit factorizations (5), such that (3) is feasible for some \(^{} 1\) and we can solve in \(\) to \(\) relative accuracy in \(^{}_{}\) time, and \(\) satisfying_

\[(),\] (7)

_we can return weights \(w^{n}_{ 0}\) satisfying (4) with probability \( 1-\) in time_

\[O(_{}(^{})^{2} (}{}}{}) ),_{}:=_{}(\{ _{i}\}_{i[n]}\{\})+^{}_ {}.\]

The first condition in (7) is no more general than assuming we have a "warm start" reweighting \(w_{0}^{n}_{ 0}\) (not necessarily \(\)) satisfying \(_{i[n]}[w_{0}]_{i}_{i}\), by exploiting scale invariance of the problem and setting \(_{i}[w_{0}]_{i}_{i}\). The second bound in (7) is equivalent to \(()\) up to constant factors, since given a bound \(\), we can use the power method to shift the scale of \(\) so it is spectrally larger than \(\). The operation requires just a logarithmic number of matrix vector multiplications with \(\), which does not impact the runtime in Theorem 7.

**Proof sketches of Theorems 6 and 7.** We defer full proofs of Theorems 6 and 7 to Section 3 of the supplement, but overview our techniques here. Our main workhorse is the following Algorithm 1, which solves a decision variant of the isotropic matrix-dictionary recovery problem (i.e., \(=\)), leveraging any subroutine \(_{}\) for solving pure packing instances of (2) from the literature.14

We define our approximation notions (used in Lines 6 and 10 of Algorithm 1) in Section 2 of the supplement. In Section 3.1 of the supplement, we analyze correctness of Algorithm 1 using regret bounds for the matrix multiplicative weights framework , which Lines 4-12 are an instance of, and demonstrate tolerance to the stated approximations. Our proof shows that Algorithm 1 meets its output guarantees, which directly implies a solver for the matrix-dictionary recovery problem (3), (4) in the isotropic case \(=\), provided we can efficiently implement the algorithm's steps.15

By carefully combining polynomial approximations to the exponential, Johnson-Lindenstrauss sketches, and the power method, we obtain the needed approximations in Lines 6 and 10 of Algorithm 1, which combined with our correctness proof yields Theorem 6. Our proof of Theorem 7 in Section 3.2 of the supplement builds upon Theorem 6 and recursive preconditioning, based on the observation that if we could efficiently apply \(^{-}\), setting \(_{i}^{-}_{i}^{- }\) reduces to the isotropic case. We show that at a \(}\) overhead, we can use (6) to efficiently simulate \((+)^{-}\) for \(\) values which are recursively halved, allowing use of Theorem 6 for each recursive call. This general type of adaptive regularization strategy, which we term a homotopy method, is reminiscent of techniques used by other recent works in the literature on numerical linear algebra and structured continuous optimization, such as .

**Preconditioning applications.** Formal proofs of our applications in Section 2 are given in Section 5 of the supplement. Theorem 2 follows immediately from Theorem 6 with the dictionary \(_{i}=a_{i}a_{i}^{}\), where \(\{a_{i}\}_{i[n]}\) are rows of \(\), which satisfies (5) with \(m=1\). Specifically, note for \(w^{n}_{ 0}\),

\[(w)=_{i[n]}w_{i}_{i}=_{i[n]}w_{i}a_{i}a_{i} ^{}=^{}.\]

A result analogous to Theorem 1, but depending quadratically on \(^{}_{0}()\), follows from applying Theorem 7 with \(n=d\), \(_{i}=e_{i}e_{i}^{}\), \(=^{}_{0}()\), and \(=\) (i.e., using the dictionary of \(1\)-sparse diagonal matrices to approximate \(\), which satisfies (5) with \(m=1\) and (6) with \(^{}_{}=d\)). Compared to inner scaling, this application exchanges the role of the dictionary and the constraint matrix. Theorem 1 goes beyond this black-box use via a homotopy method for simulating access to matrix square roots (to reduce to the isotropic case), yielding an improved \((^{}_{0}())^{1.5}\) dependence.

Our applications in Section 3 are deferred to Section 4 of the supplement. Robust linear system solvers for perturbed variants of structured matrix families follow directly from Theorem 7, taking the matrix dictionary to be a suitable basis for the relevant non-perturbed structured family, which naturally satisfy (6). As an example, to prove Theorem 3, we use Theorem 7 with the dictionary of all \(b_{e}b_{e}^{}\) for edges \(e\) of a complete graph, where \(b_{e}\) is the associated incidence vector; the access (6) is then afforded by known Laplacian solvers. Finally, Theorems 4 and 5 follow by combining Theorems 6, 7 with homotopy techniques, alongside structural facts about M-matrices and Laplacians.

**Further work.** A natural open question is if, e.g., for outer scaling, the \(_{o}^{}()\) dependence in Theorem 1 can be reduced further, ideally to \(^{}()}\). This would match the most efficient solvers in \(\) under diagonal rescaling, _if the best known outer scaling was known in advance_. Towards this goal, we prove in Appendix D of the supplement that if a width-independent variant of Theorem 6 is developed, it can achieve such improved runtimes for Theorem 1 (with an analogous improvement for Theorem 2). We also give generalizations of this improvement to finding rescalings which minimize natural average notions of conditioning, under existence of such a conjectured solver.

## 5 Additional related work

**Matrix-dictionary recovery.** Our algorithm for Theorem 6 is based on matrix multiplicative weights , a popular meta-algorithm for approximately solving SDPs, with carefully chosen gain matrices formed by using packing SDP solvers as a black box. In this sense, it is an efficient reduction from structured SDP instances of the form (3), (4) to pure packing instances.

Similar ideas were previously used in  (repurposed in ) for solving graph-structured matrix-dictionary recovery problems. Our Theorems 6 and 7 improve upon these results both in generality (prior works only handled \(=\), and \(^{}=1+\) for sufficiently small \(\)) and efficiency (our reduction calls a packing solver \( d\) times for constant \(,^{}\), while  used \(^{2}d\)calls). Perhaps the most direct analog of Theorem 6 is Theorem 3.1 of , which builds upon the proof of Lemma 3.5 of  (but lifts the sparsity constraint). The primary qualitative difference with Theorem 6 is that Theorem 3.1 of  only handles the case where the optimal rescaling is in \([1,1.1]\), whereas we handle general \(^{}\). This restriction is important in the proof technique of , as their approach relies on bounding the change in potential functions based on the matrix exponential of dictionary linear combinations (e.g., the Taylor expansions in their Lemma B.1), which scales poorly with large \(^{}\). Moreover, our method is a natural application of the MMW framework, and is arguably simpler. This simplicity is useful in diagonal scaling applications, as it allows us to obtain a tighter characterization of our \(^{}\) dependence, the primary quantity of interest. Finally, to our knowledge Theorem 7 (which handles general constraint matrices \(\), crucial for our applications in Theorems 3, 4, and 5) has no analog in prior work, which focused on the isotropic case.

**Semi-random models.** The semi-random noise model we introduce in Section 2 for linear system solving, presented in more detail and formality in Section 6 of the supplement, follows a line of noise models originating in . A semi-random model consists of an (unknown) planted instance which a classical algorithm performs well against, augmented by additional information given by a "monotone" or "helpful" adversary masking the planted instance. Conceptually, when an algorithm fails given this "helpful" information, it may have overfit to its generative assumptions. This model has been studied in various statistical settings . Of particular relevance to our work, which studies robustness to semi-random noise in the context of fast algorithms (as opposed to the distinction between polynomial-time algorithms and computational intractability) is , which developed an algorithm for semi-random matrix completion.

#### Acknowledgments

AS was supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship. KS was supported by a Stanford Data Science Scholarship and a Dantzig-Lieberman Operations Research Fellowship. KT was supported by a Google Ph.D. Fellowship, a Simons-Berkeley VMware Research Fellowship, a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and a PayPal research award.

We would like to thank Huishuai Zhang for his contributions to an earlier version of this project, Moses Charikar and Yin Tat Lee for helpful conversations, and anonymous reviewers for feedback on earlier variations of this paper.