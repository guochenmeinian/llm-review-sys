# Unveiling Transformer Perception by

Exploring Input Manifolds

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a general method for the exploration of equivalence classes in the input space of Transformer models. The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. We illustrate how this method can be used as a powerful tool for investigating how a Transformer sees the input space, facilitating local and task-agnostic explainability in Computer Vision and Natural Language Processing tasks.

## 1 Introduction

In this paper, we propose a method for exploring the input space of Transformer models by identifying equivalence classes with respect to their predictions. We define an _equivalence class_ of a Transformer model as the set of vectors in the embedding space whose outcomes under the Transformer process are the same. The study of the input manifold on which the inverse image of models lies provides insights for both explainability and sensitivity analyses. Existing methods aiming at the exploration of the input space of Deep Neural Networks and Transformers either rely on perturbations of input data using heuristic or gradient-based criteria [16; 22; 17; 14], or they analyze specific properties of the embedding space .

Our approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. In the XAI scenario, our framework can facilitate local and task-agnostic explainability methods applicable to Computer Vision (CV) and Natural Language Processing (NLP) tasks, among others.

In Section 2, we summarise the preliminaries of the mathematical foundations of our approach. In Section 3, we present our method for the exploration of equivalence classes in the input of the Transformer models. In Section 4, we perform a preliminary investigation of some applicability options of our method on textual and visual data. In Section 5, we discuss the relevant literature about embedding space exploration and feature importance. Finally, in Section 6, we give our concluding remarks1.

[FOOTNOPreliminaries

In this Section, we provide the theoretical foundation of the proposed approach, namely the Geometric Deep Learning framework based on Riemannian Geometry .

A neural network is considered as a sequence of maps, the layers of the network, between manifolds, and the latter are the spaces where the input and the outputs of the layers belong to.

**Definition 1** (Neural Network).: _A neural network is a sequence of \(^{1}\) maps \(_{i}\) between manifolds of the form:_

\[M_{0}}M_{1}}M_{2}}}M_{n-1}}M _{n}\] (1)

_We call \(M_{0}\) the input manifold and \(M_{n}\) the output manifold. All the other manifolds of the sequence are called representation manifolds. The maps \(_{i}\) are the layers of the neural network. We denote with \(_{(i)}=_{n}_{i}:M_{i} M_{n}\) the mapping from the \(i\)-th representation layer to the output layer._

As an example, consider a shallow network with just one layer, the composition of a linear operator \(A+b\) with a sigmoid function \(\), where \(A^{m n}\) and \(b^{m}\): then, the input manifold \(M_{0}\) and the output manifold \(M_{1}\) shall be \(^{n}\) and \(^{m}\), respectively, and the map \(_{1}()=(A+b)\). We generalize this observation into the following definition.

**Definition 2** (Smooth layer).: _A map \(_{i}:M_{i-1} M_{i}\) is called a smooth layer if it is the restriction to \(M_{i-1}\) of a function \(^{(i)}(x):^{d_{i-1}}^{d_{i}}\) of the form_

\[^{(i)}_{}(x)=F^{(i)}_{}(_{}A^{(i) }_{}x_{}+b^{(i)}_{})\] (2)

_for \(i=1,,n\), \(x^{d_{i}}\), \(b^{(i)}^{d_{i}}\) and \(A^{(i)}^{d_{i} d_{i-1}}\), with \(F^{(i)}:^{d_{i}}^{d_{i}}\) a diffeomorphism._

**Remark 1**.: _Transformers implicitly apply for this framework, since their modules are smooth functions, such as fully connected layers, GeLU and sigmoid activations._

Our aim is to transport the geometric information on the data lying in the output manifold to the input manifold: this allows us to obtain insight on how the network "sees" the input space, how it manipulates it for reaching its final conclusion. For fulfilling this objective, we need several tools from differential geometry. The first key ingredient is the notion of singular Riemannian metric, which has the intuitive meaning of a degenerate scalar product which changes point to point.

**Definition 3** (Singular Riemannian metric).: _Let \(M=^{n}\) or an open subset of \(^{n}\). A singular Riemannian metric \(g\) over \(M\) is a map \(g:M Bil(^{n}^{n})\) that associates to each point \(p\) a positive semidefinite symmetric bilinear form \(g_{p}:^{n}^{n}\) in a smooth way._

Without loss of generality, we can assume the following hypotheses on the sequence (1): _i)_ The manifolds \(M_{i}\) are open and path-connected sets of dimension \( M_{i}=d_{i}\). _ii)_ The maps \(_{i}\) are \(^{1}\) submersions. _iii)_\(_{i}(M_{i-1})=M_{i}\) for every \(i=1,,n\). _iv)_ The manifold \(M_{n}\) is equipped with the structure of Riemannian manifold, with metric \(g^{(n)}\). Definition 3 naturally leads to the definition of the pseudolength and of energy of a curve.

**Definition 4** (Pseudolength and energy of a curve).: _Let \(:[a,b]^{n}\) a curve defined on the interval \([a,b]\) and \(\|v\|_{p}=(v,v)}\) the pseudo-norm induced by the pseudo-metric \(g_{p}\) at point \(p\). Then the pseudolength of \(\) and its energy are defined as_

\[Pl()=_{a}^{b}\|(s)\|_{(s)}ds=_{a}^{b}((s),(s))}ds, E()=_{a}^{b}\| (s)\|_{(s)}^{2}ds\] (3)

The notion of pseudolength leads naturally to define the distance between two points.

**Definition 5** (Pseudodistance).: _Let \(x,y M=^{n}\). The pseudodistance between \(x\) and \(y\) is then_

\[Pd(x,y)=\{Pl(): M,\,^{1}([0,1 ]),\,(0)=x,\,(1)=y\}.\] (4)One can observe that endowing the space \(^{n}\) with a singular Riemannian metric leads to have non trivial curves whose length is zero. A straightforward consequence is that there are distinct points whose pseudodistance is therefore zero: a natural equivalence relation arises, _i.e._\(x y Pd(x,y)=0\), obtaining thus a metric space \((^{n}/,Pd)\).

The second crucial tool is the notion of pullback of a function. Let \(f\) be a function from \(^{p}\) to \(^{q}\), and fix the coordinate systems \(x=(x_{1},,x_{p})\) and \(y=(y_{1},,y_{q})\) on \(^{p}\) and on \(^{q}\), respectively. Moreover, we endow \(^{q}\) with the standard Euclidean metric \(g\), whose associated matrix is the identity. The space \(^{p}\) can be equipped with the pullback metric \(f^{*}g\) whose representation matrix reads as

\[(f^{*}g)_{ij}=_{h,k=1}^{q}(}{ x_{i}} )g_{hk}(}{ x_{j}}).\] (5)

The sequence (1) shows that a neural network can be considered simply as a function, a composition of maps: hence, taking \(f=_{n}_{n-1}_{1}\) and supposing that \(M_{0}=^{p},M_{n}=^{q}\), the generalization of (5) applied to (1) provides with the pullback of a generic neural network.

Hereafter, we consider in (1) the case \(M_{n}=^{q}\), equipped with the trivial metric \(g^{(n)}=I_{q}\), _i.e._, the identity. Each manifold \(M_{i}\) of the sequence (1) is equipped with a Riemannian singular metric, denoted with \(g^{(i)}\), obtained via the pullback of \(_{(i)}\). The pseudolength of a curve \(\) on the \(i\)-th manifold, namely \(Pl_{i}()\), is computed via the relative metric \(g^{(i)}\) via (3).

### General results

We depict hereafter the theoretical bases of our approach. We denote with \(_{i}\) the submap \(_{i}_{n}:M_{i} M_{n}\), and with \(_{0}\) the map describing the action of the complete network. The starting point is to consider the pair \((M_{i},Pd_{i})\): this is a pseudometric space, which can be turned into a full-fledged metric space \(M_{i}/_{i}\) by the metric identification \(x_{i}y Pd_{i}(x,y)=0\). The first result states that the length of a curve on the \(i\)-th manifold is preserved among the mapping on the subsequent manifolds.

**Proposition 1**.: _Let \(: M_{i}\) be a piecewise \(^{1}\) curve. Let \(k\{i,i+1,,n\}\) and consider the curve \(_{k}=_{k}_{i}\) on \(M_{k}\). Then \(Pl_{i}()=Pl_{k}(_{k})\)._

In particular this is true when \(k=n\), _i.e._, the length of a curve is preserved in the last manifold. This result leads naturally to claim that if two points are in the same class of equivalence, then they are mapped into the same point under the action of the neural network.

**Proposition 2**.: _If two points \(p,q M_{i}\) are in the same class of equivalence, then \(_{i}(p)=_{i}(q)\)._

The next step is to prove that the sets \(M_{i}/_{i}\) are actually smooth manifolds: to this aim, we introduce another equivalence relation: \(x_{_{i}}y\) if and only if there exists a piecewise \(: M_{i}\) such that \((0)=x,(1)=y\) and \(_{i}(s)=_{i}(x)\  s\). The introduction of this equivalence relation allows us to easily state the following proposition.

**Proposition 3**.: _Let \(x,y M_{i}\), then \(x_{i}y\) if and only if \(x_{_{i}}y\)._

The following corollary contains the natural consequences of the previous result; the second point of the claim below is the counterpart of Proposition 2.

**Corollary 1**.: _Under the hypothesis of Proposition 3, one has that \(M_{i}/\!\!_{i}=M_{i}/\!\!_{_{i+1}}\). Moreover, if two points \(p,q M_{i}\) are connected by a \(^{1}\) curve \(: M_{i}\) satisfying \(_{i}(p)=_{i}(s)\) for every \(s\), then they lie in the same class of equivalence._

Making use of the Godement's criterion, we are now able to prove that the set \(M_{i}/_{i}\) is a smooth manifold, together with its dimension.

**Proposition 4**.: \(}{_{i}}\) _is a smooth manifold of dimension \(dim((M_{0}))\)._

This last achievement provides practical insights about the projection \(_{i}\) on the quotient space, that consists the building block of the algorithms used for recovering and exploring the equivalence classes of a neural network.

**Proposition 5**.: \(_{i}:M_{i} M_{i}/_{i}\) _is a smooth fiber bundle, with \(Ker(d_{i})=M_{i}\), which is therefore an integrable distribution. \(M_{i}\) is the vertical bundle of \(M_{i}\). Every class of equivalence \([p]\) is a path-connected submanifold of \(M_{i}\) and coincide with the fiber of the bundle over the point \(p M_{i}\)._

## 3 Methodology

The results depicted in Section 2.1 provide powerful tools for investigating how a neural network sees the input space starting from a point \(x\). In particular we point out the following remarks: i) If two points \(x,y\) belonging to the input manifold \(M_{0}\) are are such that \(x_{0}y\), then \((x)=(y)\); ii) given a point \(p M_{n}\), the counterimage \(^{-1}(p)\) is a smooth manifold, whose connected components are classes of equivalences in \(M_{0}\) with respect to \(_{0}\). A necessary condition for two points \(x,y M_{0}\) to be in the same class of equivalence is that \((x)=(y)\); iii) any class of equivalence \([x]\), \(x M_{0}\), is a maximal integral submanifold of \(M_{0}\). The above observations directly provide with a strategy to build up the equivalence class of an input point \(x M_{0}\). Proposition 5 tells us that \(M_{0}\) is an integrable distribution, with dimension equal to the dimension of the kernel of \(g^{(0)}\): we can hence find \(dim(Ker(g^{(0)}))\) vector fields which are a base for the tangent space of \(M_{0}\). This means that we can compute the eigenvalue decomposition of \(g^{(0)}_{x}\) and consider the \(L\) linearly independent eigenvectors, namely \(\{v_{l}\}_{l=1,,L}\), associated to the null eigenvalue: these eigenvectors depend _smoothly_ on the point, a fact that is not trivial when the matrix associated to the metric depends on several parameters . We can build then all the null curves by randomly selecting one eigenvector \(\{v_{l}\}\) and then reconstruct the curve along the direction \(\) from the starting point \(x\). From a practical point of view, one is led to solve the Cauchy problem, a first order differential equation, with \(=\) and initial condition \((0)=x\).

### Input Space Exploration

This whole procedure is coded in the Singular Metric Equivalence Class (SiMEC) and the Singular Metric Exploration (SiMExp) algorithms, whose general schemes are depicted in Algorithms 1 and 2. SiMEC reconstructs the class of equivalence of the input via the exploration of the input space by randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp, in order to move from a class of equivalence to another we consider the eigenvectors relative to the nonzero eigenvalues. This requires the slight difference in lines 5 to 7 between Algorithm 1 and Algorithm 2.

```
1: Set the network \(\); choose the maximum number of iterations. Choose the input \(p_{0}\).
2:for\(k=0,1,,K-1\)do
3: Compute \(g^{0}_{(p_{k})}\)
4: Compute the pullback metric \(g^{0}_{p_{k}}\)
5: Diagonalize \(g^{0}_{p_{k}}\) and find the eigenvectors \(\{v_{l}\}_{l}\) associated to the zero eigenvalue
6: Randomly select \(\{v_{l}\}_{l}\)
7:\(=1/g^{0}_{p_{k}})}\)
8:\(p_{k+1} p_{k}+\)
9:endfor
10: Optionally: store \(\{p_{k}\}_{k=0,,K}\) for optimizing future computations
11: Project \(p_{k}\) to the nearest feasible region ```

**Algorithm 1** The Singular Metric Equivalence Class (SiMEC) algorithm.

There are some remarks to point out. From a numerical point of view, the diagonalization of the pullback may lead to have even negative eigenvalues: hence one may use the notion of energy of a curve, related to the pseudolength. The update rule for the new point (line 8) amounts to solve the differential problem via the Euler method: for a reliable solution, we suggest to choose a small step-length \(\). On the other hand, if the value of \(\) is too small more iterations are needed to moveaway from the starting point sensibly. Therefore there is a trade-off between the reliability of the solution and the exploration pace. The proof of the well-posedness theorem for Cauchy problems, cf. [18, Theorem 2.1], yields some insights, suggesting to set \(\) equal to the inverse of the Lipschitz constant of the map \(\) - which in practice we can estimate with the inverse of the square root of the largest eigenvalue \(_{M}\) of the pullback metric \(g^{0}_{p_{k}}\). This is our default choice for Algorithm 1. We also note that Algorithm 1 is more sensitive to the choice of the parameter \(\) compared to Algorithm 2. To build points in the same equivalence class Algorithm 1 needs to follow a null curve closely with as little approximations as possible, namely with a small \(\). In contrast Algorithm 2, whose goal is to change the equivalence class from one iteration to the next, does not have the same problem and larger \(\) are allowed. Out default choice is therefore to set \(=2_{M}^{-1/2}\) for Algorithm 2. As for the computational complexity of the two algorithms, the most demanding step is the computation of the eigenvalues and eigenvectors, which is \(O(n^{3})\), with \(n\) the dimension of the square matrix \(g^{0}_{p_{k}}\). Since all the other operations are either \(O(n)\) or \(O(n^{2})\), we conclude that the complexity of both Algorithms 1 and 2 is \(O(n^{3})\).

### Interpretability

Algorithms 1 and 2 allow for the exploration of the equivalence classes in the input space of a Transformer model. However, the points explored by these algorithms may not be directly interpretable by a human perspective. For instance, an image or a piece of text may need to be decoded to be "readable" by a human observer. Furthermore, we present an interpretation of the eigenvalues of the pullback metric which allows us to define a feature importance metric. We present two interpretability methods for Transformers based on input space exploration. Both methods are then demonstrated on a Vision Transformer (ViT) trained for digit classification , and two BERT models, one trained for hate speech classification and the other trained for MLM [7; 19].

```
1:Inputs:
2: Transformer model \(T\) with: Tokenizer \(t_{T}\), Embedding layer \(e_{T}\), Intermediate layers \(l_{T}\)
3:Input data \(x\)
4:Retrieve segments \(x^{t}=t_{T}(x)\)
5:Compute embeddings \(x^{e}=e_{T}(x^{t})\)
6:Compute intermediate representations \(g^{n}_{l_{T}(x^{e})}\)
7:Calculate the pullback metric \(g^{0}_{x^{e}}\)
8:Diagonalize \(g^{0}_{x^{e}}\) to extract eigenvalues
9:Identify the maximum eigenvalue for each embedding, indicating its importance
10:Output: Heatmap of embedding importance based on the eigenvalues ```

**Algorithm 4** Exploration of Embedding Space in Transformers

Feature importance.Consider a Transformer model \(T\) whose architecture includes a tokenizer \(t_{T}\) (or patcher for images) that segments the input so that each segment can be converted into a continuous representation by an embedding layer \(e_{T}\). This results in a matrix of dimensions \(n_{s} h\), where \(n_{s}\) represents the number of segments, and \(h\) denotes the hidden size of the model's embeddings. The eigenvalues of the pullback metric can be used to deduce the importance of each embedding and, by extension, the significance of the segments they represent, with respect to the final prediction. The process for determining the importance of textual tokens or image patches is outlined in Algorithm 3. The appearance of the resulting heatmaps varies according to the type of input used. An example of experiments with ViT on the MNIST dataset  is shown in Figure 1 that depicts heatmaps for two MNIST instances. Figure 2, on the left, illustrates two experiment using Algorithm 3 on both a BERT model for hate speech detection and a BERT model for MLM.

Interpretation of input space exploration.Using SiMEC and SiMExp to explore the embedding space reveals how Transformer models perceive equivalence among different data points. Specifically, these methodologies facilitate the sequential acquisition of embedding matrices \(p_{0} p_{K}\) at each iteration, as detailed in Algorithms 1 and 2. Algorithm 4 implements a practical application of the SiMEC/SiMExp approach with Transformer models. A key feature of this method is its ability to selectively update specific tokens (for text inputs) or patches (for image inputs) during each iteration. This selective updating allows us to explore targeted modifications that prompt the model to either categorize different inputs as the same class or recognize them as distinct. Unlike traditional approaches where modifications are predetermined, this method lets the model itself guide us to understand which data points belong to specific equivalence classes. To interpret embeddings resulted from the exploration process, they must be mapped back into a human-understandable form, such as text or images. The interpretation of an embedding vector depends on the operations performed by the Transformer's embedding module \(e_{T}\). If \(e_{T}\) consists only of invertible operations, it is feasible to construct a layer that performs the inverse operation relative to \(e_{T}\). The output can then be visualized and directly interpreted by humans, allowing for a comparison with the original input to discern how differences in embeddings reflect differences in their representations (e.g., text, images). If the operations in \(e_{T}\) are non-invertible, a trained decoder is required to reconstruct an interpretable output from each embedding matrix \(p_{0} p_{K}\). When using a BERT model, it is feasible to utilize layers that are specialized for the masked language modeling (MLM) task to map input embeddings back to tokens. This approach is effective whether the BERT model in question is specifically designed for MLM or for sentence classification. In the case of sentence classification models, it is necessary to select a corresponding MLM BERT model that shares the same internal architecture, including the number of layers and embedding size.

Algorithm 5 depicts the process of interpreting Algorithm 4 outputs for both ViT and BERT experiments. After initializing the decoder according to the model type, the embeddings \(p_{0} p_{K}\) need to be constrained to a feasible region. This region is defined by the distribution of embeddings derived from the original input instances. Next, the embeddings are decoded, and the selected segments for exploration are extracted. These segments are then used to replace the corresponding parts of the original input instance. Figure 3 depicts an example outcome of Algorithm 5 applied on a ViT exploration experiment. Given that the interpretation process includes both a capping step and a decoding step (lines 10 and 11 of Algorithm 5), it's important to note that there isn't a direct 1:1 correspondence between each iteration's update and the interpretation outcomes. Our primary focus is on exploring the input embedding space, rather than the input image or input sentence spaces. For further investigation, we provide a detailed discussion on considering interpretation outputs as alternative prompts in Section 4.

Figure 1: Example output from Algorithm 3 applied to digit classification. These two instances are predicted as 3 (left) and 4 (right). The brightness of the color indicates the eigenvalue’s magnitude. The brighter the color, the more sensitive the patch. This indicates that changes in the values of these sensitive patches are likely to have a greater impact on the prediction probabilities. Each patch in the heatmap corresponds to a \(2 2\) square pixel.

Figure 2: Example outputs from Algorithm 3. The darker the color, the higher the token’s eigenvalue. Left: The sentence analysed is classified as “offensive” by the BERT for hate speech detection, with significant contributions from tokens [CLS], politicians, corrupt, and ##it (part of the word _deceeiful_). Right: Example instance processed by a BERT model for masked language modeling. [MASK] is predicted as “ham”, with the most influential tokens being pizza and cheese.

## 4 Experiments

Experiments are conducted on textual and visual data. We aim to perform a preliminary investigation of 3 features of our approach: (i) how the class probability changes on the decoded output of SiMEC/SiMExp, (ii) what is the trade-off between the quantity and the quality of the output, and (iii) how our method can be used to extract feature importance-based explanations.

In the textual case, we experiment with hate speech classification datasets: we use HateXplain2, which provides a ground truth for feature importance, plus a sample of 100 hate speech sentences generated by prompting ChatGPT3, which serve purposes (i) and (ii). In the visual case, we perform experiments on MNIST  dataset.

Using interpretation outputs as alternative promptsAn interesting investigation is to determine if our interpretation algorithm (Algorithm 5) can generate alternative prompts that stay in the same equivalence class as the original input data or move to a different one, based on SiMEC and SiMExp explorations. We test how the probability assigned to the original equivalence class by the Transformer model changes as the SiMEC and SiMExp algorithms explore the input embedding manifold.

For BERT experiments we generate prompts to inspect the probability distribution over the vocabulary for tokens updated by Algorithms 1 and 2. We decode the updated \(p_{0} p_{K}\) using Algorithm 5, focusing on tokens updated through the iterations. For each of these decoded tokens, we extract the top-5 scores to obtain 5 alternative tokens to replace the original ones, creating 5 alternate prompts. We then extract the prediction \(i^{*}=*{arg\,max}_{i}y_{i}\) for the original sentence, which represents the output whose equivalence class we aim to explore. Finally, we classify the new prompts, obtaining the corresponding predictions \(Y=^{(0)}^{(K)}\), where each \(^{(k)}^{N}\), \(N\) being the number of prediction classes. We visualize the prediction trend for the \(i^{*}\)th value in every \(^{(0)}^{(K)}\) categorizing the images into two subsets: those that lead to a change in prediction \(Y_{c}=\{^{(k)} Y*{arg\,max}_{i}y_{i}^{(k)} i ^{*}\}\) and those that don't \(Y_{s}=\{_{i} Y*{arg\,max}_{i}y_{i}^{(k)}=i^{*}\}\).

Figure 3: Example of SiMEC and SiMExp output interpretation for ViT digit classification. Left: Original MNIST image of an “8”. Center: Interpretation of a \(p_{1000}\) from a SiMEC experiment, where \(p_{1000}\) is predicted as “8”. Right: Interpretation of a \(p_{1000}\) from a SiMExp experiment, where \(p_{1000}\) is predicted as “4”. All patches are subject to SiMEC and SiMExp updates.

Sentence classification experiments4 involved 1000 iterations from both SiMEC and SiMExp, applied to a subset of 8 sentences from the ChatGPT hate speech dataset. The plot on the left side of Figure 4 illustrates that, as the original embeddings are increasingly modified, SiMExp tends to produce alternatives with lower prediction values for \(i^{*}\) compared to SiMEC. Thus, even if predictions change in SiMEC experiments, the equivalence class prediction value remains approximately constant and higher than in SiMExp. Considering the plot on the right side of Figure 4, SiMExp identifies prompts that lower the prediction value for \(i^{*}\). ViT and MLM experiments are detailed in the Supplementary Materials.

Input space explorationWe measure the time required to explore the input space of a ViT with the SiMEC algorithm and compare it with a perturbation-based method. The perturbation-based method mimics a trial-and-error approach as it takes an input image and, at each iteration, perturbs it by a semi-random vector \(_{t+1}=a_{t}_{t}+\), where \(a_{t}=1\) if \(y_{t}=y_{t-1}\), \(a_{t}=-1\) otherwise, \(\) is an orthogonal random vector from a standard normal distribution and \(\) is the step length. With the perturbation, we obtain a new image, then check whether the model yields the same label for the new image. The perturbation vector is re-initialized at random from a normal distribution \(20\%\) of the times to allow for exploration. We construct this method to have a direct comparison with ours in the absence of a consolidated literature about the task.

We train a ViT model having 4 layers and 4 heads per layer on the MNIST dataset5. The SiMEC algorithm is run for 1000 iterations, so that it can generate 1000 examples starting from a single image. In a sample of 100 images, the average time is approximately 339 seconds.6 In the same time, the perturbation-based algorithm can produce up to 36000 images. However, we notice that the perturbation-based algorithm ends up producing monochrome (pixel color has zero variance) or totally noisy images, which provide little information about the behavior of the model. Excluding only the images with low color variance (\(<0.01\)), we are left, on average, with \(19\) images (standard deviation \(13.9\)). SiMEC, in contrast, doesn't present this behavior, as all 1000 images have high enough intensity variance and are thus useful for explainability purposes.

As BERT has many more parameters with respect to our ViT model, processing textual data takes longer. Specifically, in a sample of 16 sentences, the average time needed to run 1000 iterations on a sentence is 7089 seconds, taking into account both MLM and classification experiments.

Feature importance-based explanationsWe compare our method against Attention Rollout (AR)  and the Relevancy method proposed by Chefer et al. . In the textual case, we provide a quantitative evaluation using the HateXplain dataset, which contains 20147 sentences (of which 1924 in the test set) annotated with _normal_, _offensive_ and _hate speech_ labels as well as the positions of words that support the label decision. We then measure the cosine similarity between the importance assigned by each method to each word in a sentence and the ground truth. Notice that, since the

Figure 4: Analysis involving results SiMEC and SiMExp applied to BERT for hate speech detection. Left: Prediction values for \(i^{*}\) for each \( Y_{c}\). Right: Prediction values for \( Y_{s}\).

dataset contains multiple annotations, the ground truth \(y\) for each word \(w\) is obtained as the average of the binary labels assigned by each annotator, and therefore \(y(w)[0;1]\). We also normalize all scores in \([0;1]\) so to have them on the same scale. The average similarity achieved by our method is **0.707** (standard deviation \(\) = 0.302), against **0.7** (\(\) = 0.315) for Relevancy and **0.583** (\(\) = 0.318) for AR. This proves our method to be more effective in finding the most sensitive tokens for classification. We provide an example on image classification in the Supplementary Materials.

## 5 Related work

Our work relates to embedding space exploration literature, and has at least one collateral applications in the XAI domain, namely producing feature importance-based explanations.

Embedding space exploration.Works dealing with embedding space exploration mostly focus on the study of specific properties of the embedding space of Transformers, especially in NLP. For instance, Cai et al.  challenge the idea that the embedding space is inherently anisotropic  discovering local isotropy, and find low-dimensional manifold structures in the embedding space of GPT and BERT. Bis et al.  argue that the anisotropy of the embedding space derives from embeddings shifting in common directions during training. In the field of CV, Vilas et al.  map internal representations of a ViT onto the output class manifold, enabling the early identification of class-related patches and the computation of saliency maps on the input image for each layer and head. Applying Singular Value Decomposition to the Jacobian matrix of a ViT, Salman et al.  treat the input space as the union of two subspaces: one in which image embedding doesn't change, and another one for which it changes. Except for the last one, all the aforementioned approaches rely on data samples. By studying the inverse image of the model, instead, we can do away with data samples.

Feature importance-based explanations.Feature importance is a measure of the contribution of each data feature to a model prediction. In the context of Computer Vision and Natural Language Processing, it amounts to giving a weight to pixels (or patches of pixels) in an image and tokens in a piece of text, respectively. In recent years, much research has focused on Transformers in both CV and NLP. Most approaches are based on the attention mechanism of the Transformer architecture. Abnar and Zuidema  quantify the overall attention of the output on the input by computing a linear combination of layer attentions (Attention Rollout) or applying a maximum flow algorithm (Attention Flow). To overcome the limitations  of attention-based methods, Hao et al.  use the concept of _attribution_, which is obtained by multiplying attention matrices by the integrated gradient of the model with respect to them. Chefer et al.  propose the Relevancy metric to generalize attribution to bi-modal and encoder-decoder architectures. Other methods are perturbation-based, where perturbations of input data are used to record any change in the output and draw a saliency map on the input. In order to overcome the main issue with such methods, i.e. the generation of outlier inputs, Englebert et al.  apply perturbations after the position encoding of the patches. In contrast with these methods, ours does not need arbitrary perturbations of inputs, and considers all parameters of the model, not only the attention query and key matrices.

## 6 Conclusions

Our exploration of the Transformer architecture through a theoretical framework grounded in Riemannian Geometry led to the application of our two algorithms, SiMEC and SiMExp, for examining equivalence classes in the Transformers' input space. We demonstrated how the results of these exploration methods can be interpreted in a human-readable form and conducted preliminary investigations into their potential applications. Notably, our methods show promise for ranking feature importance and generating alternative prompts within the same or different equivalence classes.

Future research directions include expanding our experimental results and delving deeper into the potential of our framework for controlled input generation within an equivalence class. This application holds significant promise for enhancing the explainability of Transformer models' decisions and for addressing issues related to bias and hallucinations.