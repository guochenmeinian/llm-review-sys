# A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention

A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention

 Hugo Cui

Statistical Physics of Computation laboratory

EPFL, Lausanne, Switzerland

&Freya Behrens

Statistical Physics of Computation laboratory

EPFL, Lausanne, Switzerland

&Florent Krzakala

Information, Learning and Physics laboratory

EPFL, Lausanne, Switzerland

&Lenka Zdeborova

Statistical Physics of Computation laboratory

EPFL, Lausanne, Switzerland

###### Abstract

Many empirical studies have provided evidence for the emergence of algorithmic mechanisms (abilities) in the learning of language models, that lead to qualitative improvements of the model capabilities. Yet, a theoretical characterization of how such mechanisms emerge remains elusive. In this paper, we take a step in this direction by providing a tight theoretical analysis of the emergence of semantic attention in a solvable model of dot-product attention. More precisely, we consider a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples we provide a tight closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional attention mechanism (with tokens attending to each other based on their respective positions) or a semantic attention mechanism (with tokens attending to each other based on their meaning), and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to a linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.

## 1 Introduction

Recent years have seen an upheaval in our ability to learn and implement complex tasks from textual data. Instrumental in these advances is the use of self-attention layers , which provide an efficient method of extracting information from sentences - both the information encoded in the ordering (i.e. _positions_) of the words, and that encoded in the meaning (i.e. _semantics_) of the words. In theory, attention layers can learn to leverage both types of information, by having tokens attend to each other based on their respective positions (a mechanism called _positional attention_ in ) and/or respective meanings (henceforth referred to as _semantic attention_). In this paper, we aim at a theoretical understanding of the emergence of these different mechanisms in attention layers, and the transitions therebetween.

Many empirical studies have provided evidence for the emergence of specific algorithmic mechanisms (abilities) in the learning of language models that lead to qualitative improvements of the model capabilities [3; 4; 5]. By reverse-engineering trained models into human-interpretable components [6; 7; 8] a growing body of work on mechanistic interpretability aims to empirically understand which precise algorithmic mechanisms a neural network learns. Such investigations have demonstrated that attention layers are able to implement a wide range of different algorithms, even for the same task, using both positional and semantic attributes of the inputs. We offer a particularly simple illustration of this idea in Appendix A, where we show that for a sequence modelling task involving counting different algorithmic mechanisms co-exist, each corresponding to a distinct local minimum of the empirical loss. Out of these possibilities, the precise implementation that a model learns during training is jointly affected by its architecture , the training procedure itself  and the available data . It remains an open question how to theoretically characterise the conditions under which a given behaviour emerges in the model leading to said qualitative improvements.

From a theoretical perspective, even the nature of this type of algorithmic emergence is unclear. It is not known whether it is simply a fast but smooth change in performance or whether the emergence is due to a sharp boundary between fundamentally different regimes of learning . In our work, we take inspiration from physics, where a similar theoretical question about the nature of phase transitions was posed a century ago for models of interacting particles, such as the famous Ising model describing ferromagnetism . In the limit of infinitely many particles, it was shown that it is possible to theoretically deduce sharp discontinuities in some properties of interest (e.g. the magnetization of a magnet), delineating qualitatively very different regimes (e.g. magnetized or not). While mathematically, a large size limit needs to be considered to confirm the existence of sharp phase transitions, this asymptotic theory usually closely matches simulations, even for relatively moderate finite sizes.

In the theory of feed-forward fully connected neural networks, phase transitions in the network's generalisation ability as more training samples are available were studied as early as in , and their existence was proven mathematically rigorously in . In those works, the limit of many particles corresponds to taking the number of training samples and the dimensionality of the data to infinity at a fixed ratio. These theories rely on the property that macroscopic quantities of interest, such as the test error, become concentrated and deterministic in the high-dimensional limit. A dimension-free set of equations is then derived which predicts these deterministic quantities. Since then, a plethora of works along these lines both in statistical physics of phase transitions and in the theory of feed-forward neural networks have continued to study these phenomena, see e.g. .

In this work, for the first time, we bring this type of study to the analysis of neural networks with attention layers. While several previous theoretical studies of the attention mechanism considered some type of high-dimensional limit , none of them identified a phase transition between different types of mechanisms that are implemented by the attention layer. Simultaneously, the finite-dimensional real-world models that are the focus of works in mechanistic interpretability do not lend themselves to a tractable definition of a high-dimensional limit, which is necessary to identify a phase transition theoretically, as explained above. We aim to fill this gap by introducing and analysing a tractable model that permits such a sharp high-dimensional characterisation for attention layers (see Fig. 1).

Figure 1: **A phase transition in a toy model of attention.**_(A)_ We investigate a tied low-rank attention model in a teacher-student setting. The teacher mixes the \(L\) individual tokens of dimension \(d\) according to a semantic (as a function of the token’s content \(\)) and a positional (as a function of the token’s position) attention matrix. The student can only use positional encodings \(\) to fit the positional properties of the teacher. _(B)_ Schematic view of the loss landscape of the teacher, which contains both a positional and a semantic minimum. _(C)_ We find that in the asymptotic high-dimensional limit and as a function of the sample complexity and the composition of the teacher, the global minimum switches, constituting a phase transition between positional and semantic learning.

In particular our contributions are:

* We describe a model with a single self-attention layer with tied, low-rank query and key matrices. On Gaussian input data and realizable labels, we show that _this model exhibits a phase transition in terms of sample complexity_ between a semantic and a positional mechanism.
* As the main technical result, we analyse this model in the asymptotic limit where the embedding dimension \(d\) of the tokens and the number \(n\) of training samples grow proportionally. We provide a _tight closed-form characterization_ of the test error and training loss achieved at the minima of the non-convex empirical loss. Using this high-dimensional characterization, we locate the positional-semantic phase transition, thus providing the first theoretical result about the _emergence of sharp phase transitions_ in a model of dot-product attention.
* We contrast the performance of the dot-product attention layer with that of a linear model, which can only implement positional mechanisms, and show how the former outperforms the latter once it learns the semantic mechanism, _highlighting the advantage of the attention architecture_ for this task, when there is a sufficient amount of training data.

In Section 2, we discuss further related work. Section 3 defines the general version of a solvable model of tied low-rank attention, and Section 4 provides a tight characterization of the global minimum of its empirical loss. Section 5 analyses a concrete instance of dot-product attention and demonstrates that in this case the global minimum corresponds to either a semantic or positional mechanism, depending on the training data and task, with a phase transition between them. We conclude with a discussion of the limitations of our analysis in Section 6.

## 2 Related work

Theory of attentionAttention models have been the object of sizeable theoretical scrutiny in recent years, with a growing body of work investigating various aspects such as their expressivity [25; 27; 28], inductive bias [29; 30; 31], training dynamics [2; 32; 33; 34], and in-context learning [26; 35; 36; 37; 38].  and  analyze models with frozen non-trainable queries and keys, under the lens of signal propagation in such frozen models, or of their expressivity.  similarly studies the learning of the value matrix and positional encodings only, fixing keys and queries to identity, and shows how a transformer with a single attention head can learn spatial structure with a purely positional attention mechanism. The works of [29; 37] analyze the learning of a single layer of attention, with trainable queries and keys, assuming linear or ReLu activations - instead of the standard softmax activation.  provide convergence bounds for non-linear transformer models with a single attention layer, with trainable queries and keys. Because these studies are not tight, they do not allow to capture sharp changes in the behaviour of attention mechanisms such as phase transitions, and cannot for the same reason provide a theoretical description of the sudden emergence of new algorithmic mechanisms. A first tight description was provided in , in the context of learning a high-dimensional graphical model with a single layer of factored attention, leveraging its formal equivalence to a linear and convex learning problem. On the other hand, this model does not exhibit any emergent phenomenon.  analyze how induction head mechanisms can be learnt from gradient descent on the population loss, i.e. when an infinite amount of data is available. The present manuscript conducts a tight analysis of the non-convex learning of a non-linear attention model with trainable tied queries and keys from a finite train set, thereby allowing the description of sharp phase transitions in sample complexity in the behaviour and performance of the model.

Positional encodingsTo combine the positional and semantic information in textual or general sequential data, a variety of models and input encodings have been explored. Many approaches are based on autoregressive models, e.g. recurrent architectures , where the positional information is provided implicitly by the order in which the input is processed. While some transformers can leverage implicit positional information through causal masks in training [41; 42; 43], in principle a dot product attention layer requires an explicit encoding of positional information as it views the input sequence in parallel, as a bag of words . Several works experimentally explore different types of positional encodings with the goal of improving the downstream task performance [44; 45]. In this work, we provide a tractable model to quantify the generalization error of a single layer of attention in the presence of positional encodings.

Theory of phase transitions in neural networksIn supervised learning with feed-forward fully connected neural networks, phase transitions in sample complexity were identified in settings where the data consists of random Gaussian samples, and the labels are generated by a target neural networks with random weights. For a single-layer perceptron and a variety of teacher weights distributions and activation functions, a discontinuity of the optimal test error as the number of samples increases was established in [18; 17; 19]. For a two-layer neural network, [21; 20] evidenced a specialization threshold in the sample complexity below which linear regression matches the optimal test accuracy, and above which a strictly better accuracy can be reached. To our awareness, phase transitions in neural networks with attention layers have not been studied theoretically yet.

## 3 Tied low-rank attention model

Input data modelWe consider a model of embedded sentences with uncorrelated (1-gram) words. More precisely, a sentence \(^{L d}\), where \(L\) is the sentence length and \(d\) represents the embedding dimension, consists of \(L\) tokens \(\{_{}\}_{1 L}\) independently drawn from a Gaussian distribution \(_{}(0,_{})\) with covariance \(_{}^{d d}\). In the following, we denote the probability distribution of \(\) as \(p_{x}\). Note that while this sentence model does not involve in itself statistical correlations between tokens, the task (target function) will entail interactions between different tokens. While more general data models involving inter-token correlations can also be readily analyzed such analyses come at the price of much more intricate analytical formulae. We thus choose for clarity to restrict the discussion to this simple instance, which already displays rich phenomenology, as will be explored in Section 5. We defer a discussion and an analytical treatment of the general case to Appendix C.

Target functionThe target function (teacher) is assumed to be of the form

\[y(x)=[}_{}],\] (1)

for a function \(:^{L t}^{L L}\). The term \([}{{}}_{}] ^{L L}\) in (1) should be interpreted as the target attention matrix, which mixes the tokens of the input \(\). This attention matrix is parametrized by the target weights \(_{}^{d r_{}}\).

Tied attentionWe consider the learning of the target (1) by a single attention layer

\[f_{}(x)=[}(+)]( +).\] (2)

In (2), \(^{L d}\) is a _fixed_ matrix, corresponding to positional encodings, and \(^{d r_{}}\) is a trainable weight matrix. We denote subsequently \(_{}^{d}\) the \(-\)th row of \(\). Like the target (1), the parametric function (2) takes the form of a data-dependent attention matrix \([}{{}}(+)] ^{L L}\) mixing the tokens of the input \(\). Note that, compared to the usual attention mechanism  employed in practice, (2) corresponds to setting the value weights to identity, and - since (2) is parametrized by a single matrix \(\)- tying the key and query weights. While the assumption of tied weights is not strictly necessary, it makes for simpler and more interpretable analytical characterizations, and is thus considered in this work for clarity. We provide in Appendix C a full analysis of the united architecture for completeness.

Empirical risk minimizationWe study the learning of the attention layer (2), when a training set \(=\{^{},y(^{})\}_{=1}^{n}\) with \(n\) independently sampled sentences \(\{^{}\}_{=1}^{n}\), and the associated labels \(\{y(^{})\}_{=1}^{n}\), is available. The target (1) can be learnt by carrying out an empirical risk minimization:

\[}=*{argmin}_{^{d r}}[ _{=1}^{n}\|y(^{})-f_{}(^{}) \|^{2}+\|\|^{2}].\] (3)

The performance of the resulting trained model \(f_{}}\) is measured at test time by the mean squared error (MSE)

\[_{g}_{ p_{x}}\|y()-f _{}}()\|^{2}.\] (4)Closed-form characterization of the training loss

High-dimensional limitWe analyze the learning problem (3) in the limit where the embedding dimension \(d\) and the number of training samples \(n\) jointly tend to infinity, while their ratio \(=}{{d}}\) (henceforth referred to as the sample complexity) stays of order \(_{d}(1)\). We further assume the sentence length \(L\), the ranks \(r_{s},r_{t}\) of the weights \(,_{}\), and the norm of the positional embeddings \(\|\|\), to be \(_{d}(1)\). This limit has been considered in a stream of previous works (e.g. [46; 47; 48]) and allows to derive closed-form characterization of the ERM problem (3), which we present in the next section. It also exhibits a particularly rich learning phenomenology which we further explore in Section (5). Finally, let us comment briefly on the assumption that \(r_{s}=_{d}(1)\), which in words implies that the weight matrix \(\) is _low-rank_. While primarily motivated by technical limitations here, it is worth noting that low-rank weights are also considered in machine learning practice, in the context of model compression  or fine-tuning .

The **main technical result** of the present work is a closed-formed characterization of the test MSE (4) and training loss (3) achieved in the high-dimensional limit when training the model (2) via the empirical risk minimization of (3).

**Assumption 4.1**.: _The covariances \(\{_{}\}_{=1}^{L}\) admit a common set of eigenvectors \(\{_{i}\}_{i=1}^{d}\). We further note \(\{_{i}^{}\}_{i=1}^{d}\) the eigenvalues of \(_{}\). The eigenvalues \(\{_{i}^{}\}_{i=1}^{d}\) and the projection of the positional embedding \(\{_{}\}_{=1}^{L}\) and the teacher columns \(\{_{j}^{}\}_{j=1}^{r_{t}}\) on the eigenvectors \(\{_{i}^{}_{}\}_{i,}\), \(\{_{i}^{}_{j}^{}\}_{i,j}\) are assumed to admit a well-defined joint distribution \(\) as \(d\) - namely, for \(=(_{1},...,_{L})^{L}\),\(=(_{1},...,_{r_{t}})^{r_{t}}\) and \(=(_{1},...,_{L})^{L}\):_

\[_{i=1}^{d}_{=1}^{L}(_{i}^{}- _{})(_{i}^{}_{}-_{ })_{j=1}^{r_{t}}(_{i}^{}_{j}^{} -_{j})(,,).\] (5)

In words, Assumption 4.1 guarantees that all parameters of the problem admit well-defined limits in the considered asymptotic limit, with the further assumption that the covariances \(\{_{}\}_{=1}^{L}\) of the different tokens can be jointly diagonalized. We are now in a position to state the main technical result of the present work.

**Result 4.2**.: _Under Assumption 4.1, in the limit \(n,d\), \(\|\|,}{{d}},L,r_{s},r_{t}=_{d}(1)\), the summary statistics_

\[_{}_{}^{}_{} {Q}_{}}{d}^{r_{t} r_{t}}, q_{}}^{}_{} }}{d}^{r_{s} r_{s}},\] \[m_{}}^{}_{}}{d} ^{r_{s}}, _{}}^{}_{} {Q}_{}}{d}^{r_{s} r_{t}}\] (6)

_concentrate in probability, and are solutions of the set of finite-dimensional self-consistent equations_

\[q_{}=\!d(,,)_{}\!( _{r_{s}}\!\!+\!\!_{=1}^{L}\!_{}_{ })^{\!-\!1}\!(\!_{=1}^{L}\!_{} _{}\!\!+\!(_{=1}^{L}_{}_{ }+_{}_{})^{\! 2}\!)\!( _{r_{s}}\!\!+\!\!_{=1}^{L}\!_{}_{ })^{\!-\!1}\\ V_{}= d(,,)_{}(_{r_{s}}+ _{=1}^{L}_{}_{})^{\!-\!1}\\ m_{}= d(,,)_{}(_{r_{s}}+ _{=1}^{L}_{}_{})^{\!-\!1}( _{=1}^{L}_{}_{}+_{}_{})\\ _{}= d(,,)_{}(_{r_ {s}}+_{=1}^{L}_{}_{})^{\!-\!1} (_{=1}^{L}_{}_{}+_{} _{})^{}.\] (7)

\[_{}=_{,U}V_{}^{-1}((,U)_{}-q_{}^{}_{}-m_{})^{\! 2}V_{}^{-1}\\ _{}=_{}_{}^{}q_{}^{-1}\!\!-\! _{,U}V_{}^{-1}\!\!((,U)_{}-q_{}^{}_{}-m_{})_{}^{ }q_{}^{-}\\ _{}=_{,U}V_{}^{-1}((,U)_{}-q_{}^{}_{}-m_{})\\ _{}=_{,U}V_{}^{-1}((,U)_{}-q_{}^{}_{}-m_{})(u_{}-_{}^{ }q_{}^{-}{{2}}}_{})^{}(_{}- _{}^{}q_{}^{-1}_{})^{-1}\] (8)

[MISSING_PAGE_EMPTY:6]

## 5 Positional-to-semantic phase transition

Rank one dot-product attentionIn the following, we turn to a special case of tied low-rank attention (2) - namely a dot-product attention layer, which is the example from Fig. 1:

\[[}(+)]= ((+)^{}(+)^{} ).\] (13)

As in (2), we allow for positional encodings \(\) in the dot-product attention parametrization (13). We further consider a specific case of target attention matrix (1) of the form

\[[}_{}]=(1-) (_{}_{}^{} ^{})+ A.\] (14)

with \(A^{L L}\) a fixed matrix. In (14), the parameter \(\) tunes the relative strength of the dot-product term and the fixed matrix term, and interpolates between a fully positional and a fully semantic task:

* For \(=0\), the target reduces to the first dot-product term, and is purely semantic, in that the \(i,j-\)th element of the score matrix \((^{1}/\!\!\!\!/_{}_{}^{}^{})\) only depends on the tokens \(_{i},_{j}\) and not explicitely on their respective placements \(i,j\) inside the sentence. To learn satisfyingly the target, the learning model thus has to learn a _semantic_ attention matrix.
* For \(=1\), the target reduces to the second fixed term \(A\) in (14). The attention matrix \(A\) associated thereto is purely positional, in the sense that \(A_{ij}\) is a function of \(i,j\) but not of \(_{i},_{j}\). To complete the learning task, a _positional_ mechanism then needs to be learnt.

The parameter \(\) thus allows to tune the amount of semantic/positional content in the target (14), and thus the extent to which the task requires the model to implement semantic attention (small \(\)s) or rather positional attention (large \(\)s). In the following, for definiteness, we further assume \(r_{s}=r_{t}=1\) and set \(_{}\) to be a fixed random Gaussian vector drawn from \((0,_{d})\), and choose the positional encodings \(_{1}=-_{2}=_{d}\). Finally, for simplicity, we consider sentences with two tokens \(L=2\) and isotropic token covariances \(_{1}=_{2}=^{2}_{d}\).

Semantic and positional mechanismsThe summary statistics \(_{},m_{}\) describing the global minimizer of the empirical loss minimization (3) of the dot-product attention (13) on the target(14) are captured alongside the corresponding test error (4) and training loss (3), by Result 4.2. The solution of the system of equations (7) is not unique, and different stable fixed points describe different corresponding critical points of the non-convex empirical loss landscape (3). In practice, we notably find two solutions of (7), corresponding to two minima associated with different mechanisms implemented by the dot-product attention (13) when approximating the target (14):

-_Positional solution_ One solution of (7) correspond to vanishing overlap \(=0\) between the trained weights \(}\) and the semantic target weights \(_{}\), and non-zero \(m>0\) between the trained weights \(}\) and the positional embedding \(_{1}=-_{2}\). Consequently, the argument of the dot-product attention \(}(+)\) has a sizeable token-independent -thus positional- contribution \(}\), alongside a token-dependent semantic part \(}\). Because of the positional terms, the resulting learnt attention attention matrix \((}{{d}}(+)}}^{}(+)^{})\) implements a partly positional mechanism.

-_Semantic solution_ Another solution of the system of equations (7) is associated with a vanishing overlap \(m=0\) between the learnt weights \(}\) and the positional embeddings, and a finite overlap \(>0\) with the target weights \(_{}\). Therefore the resulting learnt attention matrix \((}{{d}}(+)}}^{}(+)^{})(}{{d}}}}^{}^{})\) is largely semantic.

While the system of self-consistent equations (7) may admit other solutions, we did not find solutions with lower training loss than the two aforedescribed fixed points. Which of these solution corresponds to the global minimum - and thus the solution of the optimization (3)- depends on the sample complexity \(\) and the positional/semantic parameter \(\) (14), as we describe in the following subsection.

Positional-to-semantic phase transitionFor a fixed parameter \(\) in (14), an analysis of equations (7), further detailed in Appendix C, reveals that for a sizeable range of \(\), in the probed setups, there exists a threshold \(_{c}\) for the sample complexity so that

* For \(<_{c}\), the global minimum of (3) corresponds to a positional mechanism, and is described by the positional solution of (7) of Result 4.2 with \(=0,m>0\).
* For \(>_{c}\), the global minimum of (3) corresponds to a semantic mechanism, and is described by the semantic solution of (7) of Result 4.2 with \(>0,m=0\).

The dot-product attention thus displays _a phase transition in sample complexity from a positional mechanism to a semantic mechanism_, implementing the simpler positional mechanism when having access to small amounts of data, and only learning the semantic content of the target (14) when presented sufficient data. The critical sample complexity \(_{c}\) generically grows with the positionality \(\) of the target function (14), as the semantic content - i.e. the first term of (14)- is less apparent for larger \(\), and thus requires larger amounts of data to be identified and approximated by the dot-product attention (13). An example for \(=0.3\) is given in Fig. 2. In Fig. 3 (center) the difference

Figure 3: **Phase transition between semantic and positional training loss.** Setting and experiments were performed identical to Fig. 2. **left** Scaling \(d\) and \(n\) jointly for \(=1.5\) concentrates for \(\) and \(m\), in different locations for the positional and semantic local minima each. We show 30 runs for each \(d\). **(center)** The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at \(_{1}\) or at \(_{}\). The green dashed line represents the theoretical prediction for the threshold \(_{c}()\) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. **(right)** The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at \(_{}\), and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction –following from Result 4.2 and Result 15– for the threshold sample complexity \(_{l}()\) above which the dot-product attention (2) outperforms the baseline (15).

in training loss \(_{t}\) between the positional and semantic solutions of (7) is represented, alongside the difference in training loss at convergence experimentally reached by gradient descent. For small (resp. large) sample complexity \(<_{c}\) (resp. \(>_{c}\)), the training loss of the positional (resp. semantic) minimum is lower, and thus corresponds to the global minimum.

Experimentally, the positional minimum can be reached for \(<_{c}\) via gradient descent by initializing the weights \(\) of the attention (13) close to the positional embedding \(_{1}\). By the same means, the semantic minimum can be reached from an initialization at the teacher weights \(_{}\) (14). Henceforth, we refer with a slight abuse to the minimum experimentally reached from a positional (resp. semantic) initialization as the positional (resp. semantic) minimum, even when it is not global. Note that importantly the semantic initialization is informed in nature, in that it necessitates the knowledge of the target parameters \(_{}\). Note that even though the minima we characterize analytically are fixed points of gradient descent, a precise analysis of the _dynamics_ of gradient descent from an agnostic (random) initialization, and ascertaining whether the optimizer reaches the global minimum, is an interesting question but falls out of the scope of the present manuscript - which is an analysis of the loss landscape. We however conduct numerical experiments from a random initialization of \(\) in Appendix E.4, and show that the dynamics may reach either of the local minima, or get stuck in a different one.

In Fig. 2, we compare our analytical characterizations for different metrics at the global mimimum - the summary statistics \(,m\) (middle), and the test MSE (right)-, with the corresponding experimental estimates, obtained by optimizing (3) with the Pytorch implementation of gradient descent, from a positional (resp. semantic) initialization for \(<_{c}\) (resp. \(>_{c}\)), displaying overall good agreement. In Fig. 3 (left) and Appendix E.1 we further verify that in the scaling limit of our analysis, namely \(n,d\) for \(=O(1)\), the agreement improves with growing \(n,d\).

The dot-product attention (13) thus implements a semantic mechanism when learning from sufficient amounts of data. The learning of the semantic mechanism by the dot-product attention at sample complexities \(>_{c}\) corresponds to a noticeable drop in the generalization MSE as can be observed in Fig. 2, right. But just how essential is the learning of semantic mechanism in the ability of the dot-product attention to generalize well? We explore this question in the following subsection, by comparing the dot-product attention (13) to a purely positional attention model.

Purely positional baselineIn this subsection, for the same target (14), we contrast the dot-product attention model (13), analyzed in the previous subsections, to the baseline given by a linear layer

\[f_{W}()=W,\] (15)

with a trainable weight matrix \(W^{L L}\). As for the dot-product attention (13), we consider the case where the weights \(\) are learnt by minimizing the empirical risk

\[=*{argmin}_{W^{L L}}_{=1}^{n}  y(^{})-f_{W}(^{})^{2}.\] (16)

The model (15) is a natural counterpart to the dot-product architecture (13). In (15), the attention matrix is parametrized by a single fully-trainable matrix \(W\), instead of being parametrized as a dot-product attention as in (13). A seminal difference in the two parametrizations is that while the elements of \(*{softmax}(}{{d}}^{}^{ })\) can depend on the input tokens \(\), and therefore express semantic information, the elements \(W_{ij}\) of \(W\) can only depend on the positions \(i,j\). The model (15) can thus only implement _positional mechanisms_, while the dot-product attention (13) can implement both linear and semantic mechanisms, as discussed above. Finally, observe that the model (15) is closely related to the one analyzed by  in another asymptotic limit. The following result characterizes the test error achieved by the purely positional model (15):

**Result 5.1**.: _In the same asymptotic limit as Result (4.2), the learnt weights \(\) trained by minimizing the empirical risk (16) coincide with the minimizer of the population risk, and thus admit the compact expression_

\[=_{}[}_{ }]=_{h}[h]\] (17)

_where the average bears over a finite-dimensional matrix \(h^{L t}\) with independent rows \(h_{}\) with statistics \(h_{}(0,_{})\), where \(_{}\) was defined in (6) in Result (4.2). We remind that \([}{{}}_{}]\)corresponds to the target score matrix (1). Finally, the test MSE \(}{{dL}}_{}\|y()-f_{}()\|^{2}\) achieved by the trained dense linear model \(f_{}\) (15) admits the asymptotic characterization_

\[_{g}^{}= _{}^ {}+_{h}[ h]_{}[h]^{}-_{h} _{}[h]^{}.\] (18)

The MSE achieved by the baseline (15) when learning the target (14) is plotted in Fig. 2 (right) as the orange solid line, alongside the MSE achieved by the dot-product attention (13) discussed in previous subsections. Remarkably, in the setup of Fig. 2, in the positional regime \(<_{c}\) when the dot-product attention relies on a positional mechanism \(=0,m>0\) to approximate the target, the dot-product attention (13) is outperformed by the purely positional attention (15) \(_{g}>_{g}^{}\). In contrast, in the semantic regime \(>_{c}\) where the dot-product attention learns the semantic mechanism, there exists a sample complexity \(_{l}_{c}\) above which \(_{g}<_{g}^{}\), i.e. the dot-product attention (13) outperforms the dense linear baseline (15). This threshold value \(_{l}\) is plotted for various positionality strengths \(\) in Fig. 3, alongside the positional-to-semantic threshold \(_{c}\). Interestingly, we observe \(_{l}_{c}\) in all probed settings, temptingly suggesting the natural interpretation that the dot-product attention needs to learn the semantic mechanism first (at \(=_{c}\)) in order to then be able to outperform the best positional approximation \(f_{}\) (at \(=_{l}\)). This highlights the importance of the semantic mechanism, enabled by the dot-product parametrization (13), in learning targets with semantic content such as (14).

## 6 Limitations

Compared to the original transformer  we consider a _simplified model_: the query and key matrices in our model are sharing weights and are of a low rank, as a value matrix we use the identity, and we employ only one head and a single layer. Further, our data model is limited to Gaussian data with sentences of 1-grams. Concerning the analysis, our characterization holds only in the _high-dimensional limit_, but we show that even in the finite case experimental values lie close to the theoretical prediction. Since the analysis _only concerns the minima of the models loss landscape_ the implications for the dynamics of learning algorithms, e.g. gradient decent, are limited. This shows in our numerical experiments where we need to initialize GD close to the minima in order to arrive at them. It is yet unclear if there are scaling limits of learning algorithms which would reliably find the lower or a specific one of the minima from a random initialization.

## Conclusion

We explored the interplay between positional and semantic attention, through the prism of tied low-rank self-attention in high dimensions. In a theoretically controlled setting, we characterized the global optimum of the empirical loss, when learning a target attention layer. This global optimum was found to correspond to either a positional or a semantic mechanism, with a phase transition between the two mechanisms occurring as the sample complexity increases. We believe the present asymptotic analysis of the inner workings of attention mechanisms opens up exciting research directions. Considering alternative attention architectures (including a readout network after the attention layer, or considering cross-attention) or training procedures (such as masked language modelling, or training with causal masks), are some possible extensions which will hopefully pave the way towards a satisfactory theoretical comprehension of attention mechanisms. Finally, elucidating under which conditions either minimum can be reached by a given optimizer from a random initialization constitutes an important future research avenue.