# MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation

Muhammad Osama Khan

New York University

osama.khan@nyu.edu

Junbang Liang

Amazon

junbanl@amazon.com

Chun-Kai Wang

Amazon

ckwang@amazon.com

Shan Yang

Amazon

ssyang@amazon.com

Yu Lou

Amazon

ylou@amazon.com

###### Abstract

Pre-training has been an important ingredient in developing strong monocular depth estimation models in recent years. For instance, self-supervised learning (SSL) is particularly effective by alleviating the need for large datasets with dense ground-truth depth maps. However, despite these improvements, our study reveals that the later layers of the SOTA SSL method are actually suboptimal. By examining the layer-wise representations, we demonstrate significant changes in these later layers during fine-tuning, indicating the ineffectiveness of their pre-trained features for depth estimation. To address these limitations, we propose MeSa, a unified framework that leverages the complementary strengths of masked, geometric, and supervised pre-training. Hence, MeSa benefits from not only general-purpose representations learnt via masked pre-training but also specialized depth-specific features acquired via geometric and supervised pre-training. Our CKA layer-wise analysis confirms that our pre-training strategy indeed produces improved representations for the later layers, overcoming the drawbacks of the SOTA SSL method. Furthermore, via experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these enhanced representations translate to performance improvements in both the in-distribution and out-of-distribution settings. We also investigate the influence of the pre-training dataset and demonstrate the efficacy of pre-training on LSUN, which yields significantly better pre-trained representations. Overall, our approach surpasses the masked pre-training SSL method by a substantial margin of 17.1% on the RMSE. Moreover, even without utilizing any recently proposed techniques, MeSa also outperforms the most recent methods and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.

## 1 Introduction

Monocular depth estimation is an important computer vision problem, with applications ranging from self-driving cars to augmented reality and robotics. Initially, supervised learning methods [10; 11; 26] were developed to tackle this problem, utilizing annotated depth data for training the models. However, collecting diverse real-world datasets with precise ground-truth depth is extremely challenging. Hence, self-supervised methods were developed that learn depth from stereo image pairs [12; 14] ormonocular videos  without relying on depth annotations. Particularly, self-supervised monocular depth estimation from videos is especially appealing for real-world applications, as it only requires a single camera for data collection. These self-supervised methods typically employ view synthesis as the main supervision signal and are trained via the photometric loss .

In recent years, pre-training has emerged as an important factor in developing strong depth estimation models. For instance, Ranftl et al.  noticed that randomly initialized models tend to be about 35% worse than the same models pre-trained on ImageNet. Building on this, Xie et al.  recently obtained SOTA results by directly fine-tuning a SimMIM  pre-trained network for depth estimation. SimMIM , a masked pre-training [21; 43] method, learns self-supervised representations via the following pretext task: given a partially masked input image, the network reconstructs the masked portions of the image. Interestingly, this relatively simple pre-training task obtains SOTA performance on depth estimation without utilizing any geometric priors.

Despite these pre-training algorithms yielding significant improvements, we demonstrate that the last layers of the SOTA SSL method  are actually suboptimal. To investigate this, we compare the similarity of the pre-trained representation and fine-tuned representation for each layer (Figure 2). Intuitively, a higher similarity means that the pre-trained representation is well-suited for the downstream task and hence requires minimal updates during the fine-tuning process and vice versa . Our analysis reveals significant changes in these later layers during fine-tuning, indicating the ineffectiveness of the pre-trained features for depth estimation.

Based on the intuition that different pre-training strategies capture distinct types of features, we propose a unified pre-training framework, MeSa, that addresses the aforementioned limitations. MeSa leverages the complementary strengths of three types of learning strategies: masked, geometric, and supervised pre-training. Via this synergy, our framework benefits from not only general-purpose representations learnt via masked pre-training but also specialized depth-specific features acquired via geometric and supervised pre-training.

Our layer-wise analysis confirms that our pre-training strategy indeed produces improved representations for the later layers, thereby successfully overcoming the drawbacks of the SOTA SSL method. Furthermore, via experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these enhanced representations translate to performance improvements in both the in-distribution and out-of-distribution settings. Additionally, we investigate the impact of the pre-training dataset and demonstrate the superiority of pre-training on the LSUN dataset, resulting in significantly improved pre-trained representations. Moreover, via benefiting from the 3D projective geometry during pre-training, our method helps eliminate the artifacts observed in the SOTA SSL model that lacks geometric priors.

To sum up, our main contributions include:

* A qualitative analysis of pre-training effectiveness based on layer-wise feature similarities that uncovers the insight that the later layers of the SOTA SSL method are not optimally pre-trained.
* A novel pre-training pipeline, MeSa, that effectively pre-trains the entire network by leveraging the complementary strengths of masked, geometric, and supervised pre-training, thereby benefiting from both general-purpose as well as specialized depth-specific features.
* Our pre-trained model surpasses the SOTA masked pre-training method by a substantial margin of 17.1% on the RMSE. Moreover, even without utilizing any recently proposed techniques, it also outperforms the most recent methods and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.

## 2 Related Work

### Monocular Depth Estimation

Depth estimation was initially treated as a supervised learning problem [10; 11; 26], requiring ground-truth depth to train the networks. Since collecting ground-truth depth is a challenging problem in itself, some methods explored using synthetic data . However, it is also not trivial to generate large varied synthetic datasets containing diverse scenes. A promising alternative is to train depth estimation networks using self-supervised learning, where view synthesis forms the main supervision signal and depth is synthesized as an intermediate step. Zhou et al.  developed one of the first monocular self-supervised algorithms, where they trained joint depth estimation and pose estimation networks using monocular videos. Since then, several methods [17; 45; 7; 14; 28] have been proposed to improve various components. For instance, Monodepth2  proposed a simple model to elegantly handle occlusions as well as reduce visual artifacts.

SC-DepthV1  proposed a geometry consistency loss in order to learn scale-consistent depth maps across the video. SC-DepthV2  built on top of this and proposed an auto-rectify network to alleviate training instabilities due to rotation of handheld cameras, thereby enabling better depth estimation on indoor scenes. Following this, SC-DepthV3  used an external pre-trained depth estimation network to generate pseudo-depth, which was used to improve the depth estimation performance in dynamic scenes. Remarkably, Xie et al.  recently achieved SOTA performance by directly fine-tuning a masked pre-trained SimMIM  model for depth estimation without utilizing any geometric constraints used in previous methods. In this paper, we analyze the layer-wise representations of this SOTA model and leverage the 3D projective geometry in order to overcome its drawbacks, thereby designing a strong self-supervised learning algorithm for monocular depth estimation.

### Self-supervised Learning

Self-supervised learning (SSL) is a powerful approach for learning representations from unlabeled data without requiring human annotation. SSL learns by leveraging various pretext tasks such as relative patch prediction , rotation prediction , and colorization . A popular class of SSL methods is contrastive learning [40; 20; 6; 5; 16; 41] which learns transformation invariant representations via maximizing the similarity of positive pairs and minimizing the similarity of negative pairs. Masked image modeling (MIM) has recently gained traction in vision [21; 43] following the success of masked pre-training in NLP [8; 27]. Although these MIM approaches have obtained SOTA results in several tasks, a comprehensive understanding of the pre-trained representations is still lacking, making the development of new SSL methods challenging. Recently, a few methods [42; 32; 33] have attempted to understand these pre-trained presentations. Our work

Figure 1: Our proposed framework, MeSa, effectively leverages both general-purpose as well as depth-specific features via utilizing the complementary strengths of three pre-training strategies â€“ 1) Masked, 2) Geometric, and 3) Supervised pre-training. The pre-trained models are fine-tuned in a supervised manner on the downstream dataset.

follows in this direction and uses the insights from our analysis to design a better self-supervised learning algorithm that effectively pre-trains the entire network for depth estimation.

## 3 Method

As illustrated in Figure 1, our framework consists of three pre-training strategies - 1) Masked, 2) Geometric, and 3) Supervised pre-training. By synergistically integrating these three strategies, MeSa facilitates effective representation learning that benefits from both general-purpose representations (via masked pre-training) as well as depth-specific features (via geometric and supervised pre-training). Hence, MeSa effectively pre-trains the entire network including the later layers, thereby addressing the limitations of the SOTA SSL method. In the following subsections, we briefly introduce masked, geometric, and supervised pre-training in Sections 3.1, 3.2, and 3.3 respectively. This is then followed by the fine-tuning and implementation details in Section 3.4. Lastly, Section 3.5 outlines the techniques used for the layer-wise analysis to qualitatively understand the representations learnt via the three pre-training strategies.

### Masked Pre-training

Masked pre-training learns representations by masking a region of the input image and reconstructing the masked region based on the partially masked input. In this work, we utilize the SimMIM framework proposed by Xie et al.  for masked pre-training. However, other pre-training methods could be used as well since our framework is agnostic to the exact choice of pre-training method. We use a simple masking strategy and randomly mask out some patches of the input image. The decoder consists of a single linear layer to predict the pixels of the masked regions. Finally, the network is trained using a plain \(_{1}\) loss applied to the masked regions:

\[L=_{M}|}\|_{M}-g(f(}))_{M}\|_{1}\] (1)

where \(f\) is the encoder, \(g\) is the decoder, \(\) is the raw image, and \(}\) is the partially masked image input to the encoder. The subscript \(M\) denotes the masked subset of pixels whereas \(||\) denotes the number of pixels. After training, the decoder \(g\) is usually discarded and the encoder \(f\) is used for downstream tasks.

### Geometric Pre-training

Geometric pre-training learns representations via utilizing the 3D projective geometry. In this work, we use the SC-DepthV2  framework for geometric pre-training, where depth and pose networks are trained jointly on monocular videos. Given consecutive image pairs \(I_{a}\), \(I_{b}\) from a video, we first generate the predicted depths \(D_{a}\), \(D_{b}\) of the two views and the relative camera pose \(P_{ab}\) between them:

\[D_{a}=g^{}(f(I_{a})), D_{b}=g^{}(f(I_{b})), P_{ab}=h(I_{ a},I_{b})\] (2)

where \(f\) and \(g^{}\) are the depth estimation encoder and decoder respectively whereas \(h\) is the pose estimation network.

View synthesis forms the main supervision signal (i.e., given one view, the task is to predict a view of the same scene from a different viewpoint). Given the predicted depth \(D_{a}\), relative camera pose \(P_{ab}\), and the source view \(I_{b}\), differentiable bilinear interpolation  is used to generate a prediction of the reference view \(I_{a^{}}\). Hence, depth is actually synthesized as an intermediate step and photometric loss between the generated \(I_{a^{}}\) and actual \(I_{a}\) images is used to train the network:

\[L_{P}=|}_{p}( \|I_{a}(p)-I_{a}^{}(p)\|_{1}+(1-)_{aa^{}}(p)}{2})\] (3)

where \(\) is the set of valid points projected from \(I_{a}\) to \(I_{b}\) and \(_{aa^{}}\) is the similarity between \(I_{a}\) and \(I_{a^{}}\) computed via the SSIM function .

Moreover, the geometry consistency loss is used to ensure that the two depth maps (\(D_{a}\) and \(D_{b}\)) are consistent in terms of 3D geometry:

\[L_{G}=|}_{p}D_{ }(p)\] (4)where \(D_{ diff}\) is the depth inconsistency map between \(D_{a}\) and \(D_{b}\). For further details, please refer to SC-Depth .

### Supervised Pre-training

Supervised pre-training learns representations via using off-the-shelf supervised pre-trained networks for depth estimation. In this work, we use the SC-DepthV3  framework for supervised pre-training, which uses the LeReS  pre-trained network to generate pseudo-depth. In contrast to standard supervised pre-training, the pre-trained network here is only used for pseudo-depth estimation , which allows to improve performance on dynamic objects via the confident depth ranking loss (\(L_{CDR}\)) and on object boundaries via the egde-aware relative normal loss (\(L_{ERN}\)). Hence, the overall loss for joint training via geometric and supervised pre-training is given by:

\[L= L_{P}^{M}+ L_{G}+ L_{N}+ L_{ CDR}+ L_{ ERN}\] (5)

where \(L_{P}^{M}\) is the weighted photometric loss, \(L_{G}\) is the geometry consistency loss, \(L_{N}\) is the normal matching loss, \(L_{CDR}\) is the confident depth ranking loss, and \(L_{ERN}\) is the edge-aware relative normal loss.

### Implementation Details

We pre-train via the three learning strategies sequentially in order to avoid training instability associated with concurrent training . Moreover, this also allows us to significantly reduce the pre-training time since we can leverage existing pre-trained models, thereby eliminating the need for training the entire pipeline from scratch whenever a new pre-training method is added.

In the masked pre-training phase, we employ a Swin-v2-L network as the encoder \(f\) and a single linear layer as the decoder \(g\). For the subsequent geometric and supervised pre-training stages, we keep the pre-trained encoder \(f\) and replace the decoder with a DispNet \(g^{}\). Additionally, we utilize a ResNet18  backbone in the pose network \(h\) to compute the relative camera pose between the concatenated input images.

We use the LSUN  dataset for masked pre-training whereas we utilize the training split of the NYUv2  dataset for geometric pre-training. For supervised pre-training, we leverage the pre-trained network provided by LeReS  to generate the pseudo-depth. We follow the training details outlined in the official methods (SimMIM  and SC-Depth ), with the exception of using an image size of 480\(\)480 instead of 256\(\)320 for geometric and supervised pre-training on NYUv2.

To evaluate all the pre-trained methods, we follow the SOTA SSL method  and fine-tune them on the training split of the NYUv2 dataset . We fine-tune for 75 epochs on 8 A100 GPUs, using a polynomial learning rate schedule with a 0.9 factor and a min lr of \(10^{-5}\) and a max lr of \(10^{-4}\).

Figure 2: Layer-wise analysis of the SOTA masked pre-trained model. **Left**: We discover a distinctive U-shaped pattern (top row) in representations of the pre-trained model as we delve deeper into the network. **Right**: We observe that the later layers (5-8) undergo significant changes during fine-tuning, as depicted by the lower similarities between pre-trained and fine-tuned features along the diagonal line. These results indicate that the pre-trained representations of the later layers are not effectively utilized for the downstream depth estimation task.

### Layer-wise Analysis

Following previous work [42; 30], we use CKA (centered kernel alignment)  for our analysis, which is a metric that allows us to compare the representations of various layers within a network. CKA similarity values range from 0 to 1, with increasing values meaning that the two representations are more similar. We compare the layer-wise representations of nine layers within the Swin-v2-L architecture. For convenience, we refer to them as layers 0-8, with the exact layers detailed in the supplementary material.

We perform two types of analyses to evaluate the efficacy of the pre-trained layer-wise representations. Firstly, we compare the representations of layers 1-8 in the network to layer 0 (Figure 2 left) in order to study the changes in representations as we delve deeper into the network. Secondly, for each layer, we compare the similarity of its pre-trained representation to its fine-tuned representation (Figure 2 right) in order to understand how each layer evolves during the fine-tuning process. Intuitively, a higher similarity means that the pre-trained representation is well-suited for the downstream task and hence requires minimal updates during the fine-tuning process and vice versa .

## 4 Experiments

Firstly, we analyze the layer-wise representations of the SOTA SSL model in order to illustrate the suboptimal pre-trained representations of the later layers (Section 4.1). To address this shortcoming, we propose a novel pre-training pipeline, MeSa, that not only achieves improved quantitative performance in both in-distribution (Section 4.2) and out-of-distribution (Section 4.3) settings but also learns better layer-wise representations (Section 4.4). Lastly, we investigate the impact of different pre-training datasets on the layer-wise features (Section 4.5) and conclude with a comprehensive comparison against the SOTA depth estimation models (Section 4.6).

### Layer-wise Analysis of the SOTA SSL Model

Although the SOTA SSL model yields excellent performance, it is unclear which parts of the pre-trained model contribute to this improvement. In order to understand this, we analyze the layer-wise representations of this masked pre-trained model using the techniques discussed in Section 3.5.

Based on the analysis in Figure 2 (left), we discover a U-shaped pattern in the representation space, where the representations initially start getting farther apart but then become more similar to the initial layers towards the end of the network. The top row of Figure 2 (left) compares the representation of layer 0 against the representations of layers 0-8. The similarity of layer 0's representation with itself is 1 as expected. Thereafter, for layers 1-4, the representations begin to diverge from layer 0's representation, as indicated by the decreasing similarities (darker colors). However, interestingly, past layer 4, the similarity increases (lighter colors) suggesting that the representations are becoming more similar to the layer 0 representation. This U-shaped pattern is likely a result of the pre-training objective, which is to reconstruct masked portions of the image. Hence, the later layers end up having similar representations to the first few layers. A similar pattern has also been observed in [32; 33] in the context of speech processing.

Whereas this U-shaped pattern is useful for solving the self-supervised pretext task, it is not obvious if it aids in learning effective representations for downstream tasks. To investigate this, we compare the similarity of the pre-trained and fine-tuned representations of each layer in Figure 2 (right). From

   Pre-training Strategy & RMSE \(\) & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(\) & \( 10\) \\  MP  & 0.287 & 0.949 & 0.994 & 0.999 & 0.083 & 0.035 \\ MP + GP & 0.269 & 0.951 & 0.993 & 0.998 & 0.076 & 0.033 \\ MP + GP + SP (MeSa) & **0.265** & **0.954** & **0.995** & **0.999** & **0.074** & **0.032** \\  Relative Improvement (\%) & 7.64 & 0.54 & 0.06 & 0 & 10.2 & 8.05 \\   

Table 1: Geometric and supervised pre-training have complementary benefits to masked pre-training and result in significant improvements. MP: masked pre-training, GP: geometric pre-training, SP: supervised pre-training.

the diagonal line (top-left to bottom-right), we observe that the similarities are quite small for the later layers (5-8). Lower similarities imply that these layers undergo significant changes during fine-tuning, indicating that they are not effective for the downstream depth estimation task .

### MeSa

To address the aforementioned drawbacks, we propose a novel pre-training pipeline, MeSa, that utilizes the complementary benefits of masked, geometric, and supervised pre-training.

Table 1 shows the depth estimation results of the three pre-training strategies on the NYUv2 dataset. The results demonstrate significant performance improvements achieved by all three learning strategies, highlighting their complementary strengths. Overall, our approach surpasses the SOTA SSL model, which relies solely on masked pre-training, by 10.2% on absolute relative error (0.083\(\)0.074) and 7.64% on RMSE (0.287\(\)0.265). Figure 3 presents qualitative results of the three pre-training strategies on the NYUv2 dataset, demonstrating significant improvements through the integration of geometric and supervised pre-training. When relying solely on masked pre-training, as in the SOTA SSL method, artifacts appear at the top and bottom of the predicted depth maps due to the absence of ground-truth depth (during fine-tuning) in these regions. By utilizing geometric pre-training, our method effectively eliminates these artifacts since the photometric loss utilizes 3D projective geometry and learns accurate depth across the entire image without relying on ground-truth. Moreover, geometric and supervised pre-training also help enhance the sharpness of the predicted depth maps.

### Out-of-distribution Performance

In this section, we evaluate the performance of the three pre-training strategies in the out-of-distribution (OOD) setting. To this end, we evaluate the models trained on the NYUv2 dataset directly on the IBims-1 dataset without any additional fine-tuning.

Table 2 presents the OOD results on the IBims-1 dataset. In addition to the overall depth estimation accuracy (measured via AbsRel), it is also important to improve the accuracy of depth boundaries

   Pre-training Strategy & \(_{}^{}\) & \(_{}^{}\) & \(_{}^{}\) & \(_{}^{}\) & AbsRel \(\) \\  MP  & 2.40 & 30.05 & 3.26 & 8.16 & 0.089 \\ MP + GP & 2.25 & 24.69 & 2.46 & **6.29** & **0.082** \\ MP + GP + SP (MeSa) & **2.16** & **19.31** & **2.17** & 6.44 & 0.083 \\  Relative Improvement (\%) & 10.3 & 35.8 & 33.4 & 22.9 & 8.4 \\   

Table 2: In addition to improving the overall depth estimation accuracy, geometric and supervised pre-training also enhance the accuracy of depth boundaries and planar regions in the OOD setting. MP: masked pre-training, GP: geometric pre-training, SP: supervised pre-training.

Figure 3: Qualitative comparisons of the three pre-training strategies on NYUv2. Left\(\)right: Image, MP, MP+GP, MP+GP+SP (MeSa). **Top**: predicted depth maps, **bottom**: error maps (blue\(\)lower error; red\(\)higher error).

(measured via \(^{}_{}\) and \(^{}_{}\)) as well as planar regions (measured via \(^{}_{}\) and \(^{}_{}\)), both of which are critical for many real-world applications. Our results highlight that we surpass the SOTA masked pre-training method on all five metrics in the OOD setting. Notably, we achieve substantial improvements of 35.8% (30.05\(\)19.31) and 33.4% (3.26\(\)2.17) on depth boundary completeness (\(^{}_{}\)) and planarity (\(^{}_{}\)), respectively. For a detailed description of all metrics, please refer to IBims-1 .

Figure 4 visualizes the OOD performance of the three pre-training strategies. From the results, we observe a significant improvement in the sharpness of depth maps when leveraging geometric and supervised pre-training. Moreover, the localization of depth boundaries is also greatly enhanced.

### Layer-wise Analysis of MeSa

In this section, we analyze layer-wise representations of the MeSa pre-trained network to ascertain that our pre-training strategy indeed produces improved representations for the later layers, thereby successfully overcoming the drawbacks of the SOTA SSL method.

Similar to Section 4.1, for each layer, we compare the similarity of its pre-trained representation to its fine-tuned representation (Figure 5). As mentioned earlier, when using masked pre-training (MP) alone, the later layers (5-8) are not particularly beneficial for depth estimation since they undergo significant changes during fine-tuning, illustrated by the low similarity values. On the other hand, incorporating geometric pre-training (GP) alongside MP (i.e., MP+GP) allows for more effective

Figure 4: Qualitative comparisons of the OOD performance of the three pre-training strategies on IBims-1. Left\(\)right: Image, MP, MP+GP, MP+GP+SP (MeSa). **Top**: predicted depth maps, **middle**: error maps (blue\(\)lower error; red\(\)higher error), **bottom**: edge maps.

Figure 5: Layer-wise analysis of the three pre-training strategies, comparing the representations of the pre-trained model to the fine-tuned model. MeSa effectively pre-trains the entire network, including the last few layers. Left\(\)right: MP, MP+GP, MP+GP+SP (MeSa).

[MISSING_PAGE_FAIL:9]

### Future Work

In this work, we adopt a sequential pre-training regimen in order to mitigate issues related to training instability  and to capitalize on the benefits of leveraging existing pre-trained models. Nevertheless, it would be interesting to explore how concurrent pre-training impacts the learnt representations. Moreover, in addition to masked image modeling (MIM), it would be insightful to study the effect of alternative SSL approaches on shaping the learnt feature space. Lastly, it is intriguing to investigate the representations learnt by recent depth estimation methods such as soft token and mask augmentation , metric bins module  and pre-trained text-to-image diffusion models . An exploration of how these methods might be effectively integrated with the MeSa framework to benefit from their complementary strengths is a promising avenue for future work.

## 5 Conclusion

We propose MeSa, a novel pre-training pipeline, to overcome the limitations of the SOTA SSL method, which fails to optimally pre-train the later layers. Via unifying the complementary strengths of masked, geometric, and supervised pre-training, MeSa learns effective layer-wise representations across the entire network. Moreover, MeSa leads to a substantial improvement of 17.1% on the RMSE compared to the SOTA SSL method and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.