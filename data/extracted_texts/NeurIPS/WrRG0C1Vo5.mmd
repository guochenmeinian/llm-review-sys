# Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training

Hitesh Sapkota Dingrong Wang Zhiqiang Tao Qi Yu

Rochester Institute of Technology

{hxs1943, dw7445, zhiqiang.tao, qi.yu}@rit.edu

Corresponding author

###### Abstract

The recently developed sparse network training methods, such as Lottery Ticket Hypothesis (LTH) and its variants, have shown impressive learning capacity by finding sparse sub-networks from a dense one. While these methods could largely sparsify deep networks, they generally focus more on realizing comparable accuracy to dense counterparts yet neglect network calibration. However, how to achieve calibrated network predictions lies at the core of improving model reliability, especially when it comes to addressing the overconfident issue and out-of-distribution cases. In this study, we propose a novel Distributionally Robust Optimization (DRO) framework to achieve an ensemble of lottery tickets towards calibrated network sparsification. Specifically, the proposed DRO ensemble aims to learn multiple diverse and complementary sparse sub-networks (tickets) with the guidance of uncertainty sets, which encourage tickets to gradually capture different data distributions from easy to hard and naturally complement each other. We theoretically justify the strong calibration performance by showing how the proposed robust training process guarantees to lower the confidence of incorrect predictions. Extensive experimental results on several benchmarks show that our proposed lottery ticket ensemble leads to a clear calibration improvement without sacrificing accuracy and burdening inference costs. Furthermore, experiments on OOD datasets demonstrate the robustness of our approach in the open-set environment.

## 1 Introduction

While there is remarkable progress in developing deep neural networks with densely connected layers, most of these dense networks have poor calibration performance , limiting their applicability in safety-critical domains like self-driving cars  and medical diagnosis . The poor calibration is mainly due to the fact that there exists a good number of wrongly classified data samples (_i.e.,_ low accuracy) with high confidence resulting from the memorization effect introduced by an over-parameterized architecture . Recent sparse network training methods, such as Lottery Ticket Hypothesis (LTH)  and its variants [3; 2; 38; 18; 16; 35] generally assume that there exists a sparse sub-network (_i.e.,_ lottery ticket) in a randomly initialized dense network, which could be trained in isolation and also match the performance of its dense counterpart network in terms of accuracy. While these methods may, to some extent, alleviate the overconfident issue, most of them require pre-training of a dense network followed by multi-step iterative pruning, making the overall training process highly costly, especially for large dense networks. Even for techniques that do not rely on pre-training and iterative pruning (_e.g.,_ Edge Popup or EP ), their learning goal focuses on pushing the accuracy up to the original dense networks and hence may still exhibit a severely over-fitting behavior, leading to a poor calibration performance as demonstrated in Figure 1 (b).

Inspired by the recent success of using ensembles to estimate uncertainties [13; 34], a potential solution to realize well-calibrated predictions would be training multiple sparse sub-networks andbuilding an ensemble from them. As such, by leveraging accurate uncertainty quantification, the ensemble is expected to achieve better calibration. However, existing ensemble models of sparse networks rely on pre-training and iterative fine-tuning for learning each sub-network , leading to a significant overhead for building the entire ensemble. Furthermore, an ensemble of independently trained sparse sub-networks does not necessarily improve the calibration performance. Since these networks are trained in a similar fashion from the same training data distribution, they could be strongly correlated such that the ensemble model will potentially inherit the overfitting behavior of each sub-network as shown in Figure 1(c). Therefore, the calibration capacity of sparse sub-network ensemble can be compromised as shown empirically in Figure 1 (d).

To further enhance the calibration of the ensemble, it is critical to ensure sufficient diversity among sparse sub-networks so that they are able to complement each other. One natural way to achieve diversity is to allow each sparse sub-network (ticket) to primarily focus on a specific part of training data distribution. This inspires us to leverage the AdaBoost  framework that sequentially finds tickets by manipulating training data distribution based on errors. By this means, the AdaBoost facilitates the training for a sequence of complementary sparse sub-networks. However, the empirical analysis (see Table 1) reveals that in the AdaBoost ensemble, most sub-networks (except for the first one) severely under-fit data leading to poor generalization ability. This is mainly because of the overfitting behavior of the first sub-network, which assigns very low training losses to the majority of data samples, making the subsequent sub-networks concentrate on very rare difficult samples that are likely to be outliers or noises. Hence, directly learning from these difficult samples without having global knowledge of the entire training distribution will result in the failure of subsequent training tickets and also hurt the overall calibration.

To this end, we need a more robust learning process for proper training of complementary sparse sub-networks, each of which can be learned in an efficient way to ensure the cost-effective construction of the entire ensemble. We propose a Distributionally Robust Optimization (DRO) framework to schedule learning an ensemble of lottery tickets (sparse sub-networks) with complimentary calibration behaviors that contribute to an overall well-calibrated ensemble as shown in Figure 1 (e-h). Our technique directly searches sparse sub-networks in a randomly initialized dense network without pre-training or iterative pruning. Unlike the AdaBoost ensemble, the proposed ensemble ticket method starts from the original training distribution and eventually allows learning each sub-network from different parts of the training distribution to enrich diversity. This is also fundamentally different from existing sparse ensemble models , which attempt to obtain diverse sub-networks in a heuristic way by relying on different learning rates. As a result, these models offer no guaranteed complementary behavior among sparse sub-networks to cover a different part of training data, which is essential to alleviate the overfitting behavior of the learned sparse sub-networks. In contrast, we realize a principled scheduling process by changing the uncertainty set of DRO, where a small set pushes sub-networks learning with easy data samples and a large set focuses on the difficult ones (see Figure 2). By this means, the ticket ensemble governed by our DRO framework could work complementary and lead to much better calibration ability as demonstrated in Figure 1(h). On the one hand, we hypothesize that the ticket found with easy data samples will tend to be learned and

Figure 1: Calibration performance by expected calibration error (ECE) on Cifar100 dataset with ResNet101 architecture with density \(=\)15%. EP refers to the Edge Popup algorithm .

overfitted easily, resulting in overconfident predictions (Figure 1(e)). On the other hand, the ticket focused on more difficult data samples will be less likely to overfit and may become conservative and give under-confident predictions. Thus, it is natural to form an ensemble of such lottery tickets to complement each other in making calibrated predictions. As demonstrated in Figure 1 (h), owing to the diversity in the sparse sub-networks (e-g), the DRO ensemble exhibits better calibration ability. It is also worth noting that under the DRO framework, our sparse sub-networks already improve the calibration ability as shown in Figure 1 (f-g), which is further confirmed by our theoretical results.

Experiments conducted on three benchmark datasets demonstrate the effectiveness of our proposed technique compared to sparse counterparts and dense networks. Furthermore, we show through the experimentation that because of the better calibration, our model is being able to perform well on the distributionally shifted datasets  (CIFAR10-C and CIFAR100-C). The experiments also demonstrate that our proposed DRO ensemble framework can better detect open-set samples on varying confidence thresholds. The contribution of this work can be summarized as follows:

* a new sparse ensemble framework that combines multiple sparse sub-networks to achieve better calibration performance without dense network training and iterative pruning.
* a distributionally robust optimization framework that schedules the learning of an ensemble complementary sub-networks (tickets),
* theoretical justification of the strong calibration performance by showing how the proposed robust training process guarantees to lower the confidence of incorrect predictions in Theorem 2,
* extensive empirical evidence on the effectiveness of the proposed lottery ticket ensemble in terms of competitive classification accuracy and improved open-set detection performance.

## 2 Related Work

**Sparse networks training.** Sparse network training has received increasing attention in recent years. Representative techniques include lottery ticket hypothesis (LTH)  and its variants [5; 32]. To avoid training a dense network, supermasks have been used to find the winning ticket in the dense network without training network weights . Edge-Popup (EP) extends this idea by leveraging training scores associated with the neural network weights and only weights with top scores are used for predictions. There are two key limitations to most existing LTH techniques. First, most of them require pre-training of a dense network followed by multi-step iterative pruning making the overall training process expensive. Second, their learning objective remains as improving the accuracy up to the original dense networks and may still suffer from over-fitting (as shown in Figure 1).

**Sparse network ensemble.** There are recent advancements in building ensembles from sparse networks. A pruning and regrowing strategy has been developed in a model, called CigL , where dropout serves as an implicit ensemble to improve the calibration performance. CigL requires weight updates and performs pruning and growing for multiple rounds, leading to a high training cost. Additionally, dropping many weights may lead to a performance decrease, which prevents building highly sparse networks. This idea has been further extended by using different learning rates to generate different typologies of the network structure for each sparse network [18; 35]. While diversity among sparse networks can be achieved, there is no guarantee that this can improve the calibration performance of the final ensemble. In fact, different networks may still learn from the training data in a similar way. Hence, the learned networks may exhibit similar overfitting behavior with a high correlation, making it difficult to generate a well-calibrated ensemble. In contrast, the proposed DRO ensemble schedules different sparse networks to learn from complementary parts of the training distribution, leading to improved calibration with theoretical guarantees.

**Model calibration.** Various attempts have been proposed to make the deep models more reliable either through calibration [9; 24; 32] or uncertainty quantification [7; 30]. Post-calibration techniques have been commonly used, including temperature scaling [24; 9], using regularization to penalize overconfident predictions . Recent studies show that post-hoc calibration falls short of providing reliable predictions . Most existing techniques require additional post-processing steps and an additional validation dataset. In our setting, we aim to improve the calibration ability of sparse networks without introducing additional post-calibration steps or validation dataset.

## 3 Methodology

Let \(_{}=\{,\}=\{(_{1},y_{1}),..,(_{N},y_{N})\}\) be a set of training samples where each \(_{n}^{D}\) is a D-dimensional feature vector and \(y_{n}[1,C]\) be associated label with \(C\) total classes. Let \(M\) be the total number of base learners used in the given ensemble technique. Further, consider \(\) to be the density ratio in the given network, which denotes the percentage of weights we keep during the training process. The major notations are summarized in the Appendix.

### Preliminaries

**Edge-Popup (EP)**. EP finds a lottery ticket (sparse sub-network) from a randomly initialized dense network based on the score values learned from training data. Specifically, to find the sub-network with density \(\), the algorithm optimizes the scores associated with each weight in the dense network. During the forward pass, the top-\(\) weights in each layer are selected based on their scores. During the backward pass, scores associated with all weights are updated, which allows potentially useful weights that are ignored in previous forward passes to be re-considered.

**Expected calibration error.** Expected Calibration Error (ECE) measures the correspondence between predicted probability and empirical accuracy . Specifically, mis-calibration is computed based on the difference in expectation between confidence and accuracy: \(_{}[|(=y|=p)-p|]\). In practice, we approximate the expectation by partitioning confidences into \(T\) bins (equally spaced) and take the weighted average on the absolute difference between each bins' accuracy and confidence. Let \(B_{t}\) denote the \(t\)-th beam and we have \(=_{t=1}^{T}|}{N}|acc(B_{t})-conf(B_{t})|\).

### Distributionally Robust Ensemble (DRE)

As motivated in the introduction, to further enhance the calibration of a deep ensemble, it is instrumental to introduce sufficient diversity among the component sparse sub-networks so that they can complement each other when forming the ensemble. One way to achieve diversity is to allow each sparse sub-network to primarily focus on a specific part of the training data distribution. Figure 2 provides an illustration of this idea, where the training data can be imagined to follow a multivariate Gaussian distribution with the red dot representing its mean. In this case, the first sub-network will learn the most common patterns by focusing on the training data close to the mean. The subsequent sub-networks will then learn relatively rare patterns by focusing on other parts of the training data (_e.g.,_ two or three standard deviations from the mean).

**AdaBoost ensemble.** The above idea inspires us to leverage the AdaBoost framework  to manipulate the training distribution that allows us to train a sequence of complementary sparse sub-networks. In particular, we train the first sparse sub-network from the original training distribution, where each data sample has an equal probability to be sampled. In this way, the first sparse sub-network can learn the common patterns from the most representative training samples. Starting from the second sub-network, the training distribution is changed according to the losses suffered from the previous sub-network during the last round of training. This allows the later sub-networks to focus on the difficult data samples by following the spirit of AdaBoost.

However, our empirical results reveal that in the AdaBoost ensemble, most sub-networks (except for the first one) severely underfit the training data, leading to a rather poor generalization capability. This is caused by the overfitting behavior of the first sparse sub-network, which assigns very small training losses to a majority of data samples. As a result, the subsequent sub-networks can only focus on a limited number of training samples that correspond to relatively rare patterns (or even outliers and noises) in the training data. Directly learning from these difficult data samples without a general knowledge of the entire training distribution will result in the failure of training the sub-networks.

Figure 2: Robust ensemble where \(\) defines the size of an uncertainty set with \(_{1}_{2}_{3}\).

**Distributionally robust ensemble (DRE).** To tackle the challenge as outlined above, we need a more robust learning process to ensure proper training of complementary sparse sub-networks. Different from the AdaBoost ensemble, the training of all sub-networks starts from the original training distribution in the DRO framework. Meanwhile, it also allows each sub-network to eventually focus on learning from different parts of the training distribution to ensure the desired diverse and complementary behavior. Let \(l(_{n},)\) denote the loss associated with the \(n^{th}\) data sample with \(\) being the parameters in the sparse sub-network. Then, the total loss is given by

\[^{}()=_{^{}}_{n=1}^{N}z_{n}l(_{n},)\] (1)

The uncertainty set defined to assign weights \(\) is given as

\[^{}:=\{^{N}:^{ }=1, 0,D_{f}(\|}{N}) \}\] (2)

where \(D_{f}(\|)\) is \(f\)-divergence between two distributions \(\) and \(\) and \(\) controls the size of the uncertainty set and \( 1^{N}\) is \(N\)-dimensional unit vector. Depending on the \(\) value, the above robust framework instantiates different sub-networks. For example, by making \(\), we have \(^{}=\{^{N}:^{ }=1, 0,D_{f}(\|}{N}) \}\). In this case, we train a sub-network by only using the most difficult sample in the training set. On the other extreme with \( 0\), we have \(^{}=\{^{N}:^{ }=1, 0,D_{f}(\|}{N})  0\}\), which assigns equal weights to all data samples. So, the sub-network learns from the original training distribution.

To fully leverage the key properties of the robust loss function as described above, we propose to perform distributionally robust ensembling learning to generate a diverse set of sparse sub-networks with well-controlled overfitting behavior that can collectively achieve superior calibration performance. The training process starts with a relatively small \(\) value to ensure that the initially generated sub-networks can adequately capture the general patterns from the most representative data samples in the original training distribution. The training proceeds by gradually increasing the \(\) value, which allows the subsequent sub-networks to focus on relatively rare and more difficult data samples. As a result, the later generated sub-networks tend to produce less confident predictions that complement the sub-networks generated in the earlier phase of the training process. This diverse and complementary behavior among different sparse sub-networks is clearly illustrated in Figure 1 (e)-(g). During the ensemble phase, we combine the predictions of different sub-networks in the logit space by taking the mean and then performing the softmax. In this way, the sparse sub-networks with high \(\) values help to lower the overall confidence score, especially those wrongly predicted data samples. Furthermore, the sub-networks with lower \(\) values help to bring up the confidence score of correctly predicted data samples. Thus, the overall confidence score will be well compensated, resulting in a better calibrated ensemble.

### Theoretical Analysis

In this section, we theoretically justify why the proposed DRE framework improves the calibration performance by extending the recently developed theoretical framework on multi-view learning . In particular, we will show how it can effectively lower the model's false confidence on its wrong predictions resulting from spurious correlations. To this end, we first define the problem setup that includes some key concepts used in our theoretical analysis. We then formally show that DRO helps to tackle the spurious correlations by learning from less frequent features that characterize difficult data samples in a training dataset. This important property further guarantees better calibration performance of DRO as we show in the main theorem. It is worth to note that our theoretical analysis is primarily from the spurious correlation perspective. This is only one of the potential sources that can lead to over-confidence, resulting in poor-calibration in neural networks. The goal is offer deeper insights on why the proposed approach is able to improve the calibration performance resulting from spurious correlations by effectively lowering the model's false confidence on its wrong predictions.

**Problem setup.** Assume that each data sample \(_{n}^{D}\) is divided into \(P\) total patches, where each patch is a \(d\)-dimensional vector. For the sake of simplicity, let us assume each class \(c[1,C]\) has two characterizing (major) features \(_{c}=\{_{c,i}\}_{t=1}^{L}\) with \(L=2\). For example, the features for Cars could be Headlights and Tires. Let \(_{N}^{S}\) and \(_{N}^{M}\) denote the set of _single-view_ and _multi-view_ datasamples, respectively, which are formally defined as

\[\{_{n},y_{n}\}_{N}^{S}_{c,1}_{c,2}\\ \{_{n},y_{n}\}_{N}^{M}_{c,1} _{c,2}\] (3)

The noise features (also called minor features) refer to those that do not characterize (or differentiate) a given class \(c\) (_e.g.,_ being part of the background). In important applications like computer vision, images supporting such a "multi-view" structure is very common . For example, for most car images, we can observe all main features, such as Wheels, Tires, and Headlights so they belong to \(_{N}^{M}\). Meanwhile, there may also be images, where multiple features are missing. For example, if the car image is taken from the front, the tire and wheel features may not be captured. In most real-world datasets, such single-view data samples are usually much limited as compared to their multi-view counterparts. The Appendix provides concrete examples of both single and multi-view images. Let us consider \((,y)_{N}^{S}\) with the major feature \(_{c,l}\) where \(y=c\). Then each patch \(^{p}^{d}\) can be expressed as

\[^{p}=a^{p}_{c,l}+_{^{} _{c}}^{p,^{}}^{}+^{p}\] (4)

where \(=\{_{c,1},_{c,2}\}_{c=1}^{C}\) is collection of all features, \(a^{p}>0\) is the weight allocated to feature \(_{c,l}\), \(^{p,^{}}[0,]\) is the weight allocated to the noisy feature \(^{}\) that is not present in feature set \(_{c}\)_i.e.,_\(^{}_{c}\), and \(^{p}(0,(^{p})^{2})\) is a random Gaussian noise. In (4), a patch \(^{p}\) in a single-view sample \(\) also contains set of minor (noise) features presented from other classes _i.e.,_\(^{}_{c}\) in addition to the main feature \(_{c,l}\). Since \(_{c,l}\) characterizes class \(c\), we have \(a^{p}>^{p,^{}};^{} _{c}\). However, since the single-view data samples are usually sparse in the training data, it may prevent the model from accumulating a large \(a^{p}\) for \(_{c,l}\) as shown Lemma 1 below. In contrast, some noise \(^{}\) may be selected as the dominant feature (due to spurious correlations) to minimize the errors of specific training samples, leading to potential overfitting of the model.

We further assume that the network contains \(H\) convolutional layers, which outputs \(F(;)=(F_{1}(),...F_{C}())^{C}\). The logistic output for the \(c^{th}\) class can be represented as

\[F_{c}()=_{h[H]}_{p[P]}[_{c,h}, ^{p}]\] (5)

where \(_{c,h}\) denote the \(h^{th}\) convolution layer (feature map) associated with class \(c\). Under the above data and network setting, we propose the following lemma.

**Lemma 1**.: _Let \(_{c,l}\) be the main feature vector present in the single-view data \(_{N}^{S}\). Assume that number of single-view data samples containing feature \(_{c,l}\) is limited as compared with the rest, i.e., \(N_{_{c,l}} N_{_{c,l}}\). Then, at any iteration \(t>0\), we have_

\[_{c,h}^{t+1},_{c,l}=_{c,h}^{t}, _{c,l}+_{}_{n=1}^{N}z_{ n}[_{y_{j}=c}(V_{c,h,l}(_{n})+)(1-_{c}(F( _{n})))]\] (6)

_where \(\) is a dataset specific constant, \(\) is the learning rate, \(_{c}\) is the softmax output for class \(c\), and \(V_{c,h,l}(_{j})=_{p_{_{c,l}}( _{j})}(_{c,h},_{j}^{p} a^{p})\) with \(_{_{c,l}}(_{j})\) being the collection of patches containing feature \(_{c,l}\) in \(_{j}\). The set \(\) is an uncertainty set that assigns a weight to each data sample based on it loss. In particular, the uncertainty set under DRO is given as in (2) and we further define the uncertainty set under ERM: \(^{}:=^{N}:z_{n}= ; n[1,N]}\). Learning via the robust loss in (1) leads to a stronger correlation between the network weights \(_{c,h}\) and the single-view data feature \(_{c,l}\):_

\[\{_{c,h}^{t},_{c,l}\}_{Robust}>\{_{c, h}^{t},_{c,l}\}_{ERM}; t>0\] (7)

**Remark.** The robust loss \(^{}\) forces the model to learn from the single-view samples (according to the loss) by assigning a higher weight. As a result, the network weights will be adjusted to increase the correlation with the single-view data features \(_{c,l}\) due to Lemma 1. In contrast, for standard ERM, weight is uniformly assigned to all samples. Due to the sparse single-view data features (which also makes them more difficult to learn from, leading to a larger loss), the model does not grow sufficient correlation with \(_{c,l}\). In this case, the ERM model instead learns to memorize some noisy feature \(^{}\) introduced through certain spurious correlations. For a testing data sample, the ERM model may confidently assign it to an incorrect class \(k\) according to the noise feature \(^{}\). In the theorem below, we show how the robust training process can effectively lower the confidence of incorrect predictions, leading to an improved calibration performance.

**Theorem 2**.: _Given a new testing sample \(_{S}^{N}\) containing \(_{c,l}\) as the main feature and a dominant noise feature \(^{}\) that is learned due to memorization, we have_

\[\{_{k}()\}_{Robust}<\{_{k}()\} _{ERM}\] (8)

_where \(^{}\) is assumed to be a main feature characterizing class \(k\)._

**Remark.** For ERM, due to the impact of the dominate noisy feature \(^{}\), it assigns a large probability to class \(k\) since \(^{}\) is one of its major features, leading to high confidence for an incorrect prediction. In contrast, the robust learning process allows the model to learn a stronger correlation with the main feature \(_{c,l}\) as shown in Lemma 1. Thus, the model is less impacted by the noise feature \(^{}\), resulting in reduced confidence in predicting the wrong class \(k\). Such a key property guarantees an improved calibration performance, which is clearly verified by our empirical evaluation. It is also worth noting that Theorem 2 does not necessarily lead to better classification accuracy. This is because (8) only ensures that that the false confidence is lower than an ERM model, but there is no guarantee that \(\{_{k}()\}_{Robust}<\{_{c}()\} _{Robust}\). It should be noted that our DRE framework ensures diverse sparse sub-network focusing on different single-view data samples from different classes. As such, an ensemble of those diverse sparse subnetworks provides maximum coverage of all features (even the weaker one) and therefore can ultimately improve the calibration performance. The detailed proofs are provided in the Appendix.

## 4 Experiments

We perform extensive experimentation to evaluate the distributionally robust ensemble of sparse sub-networks. Specifically, we test the ability of our proposed technique in terms of calibration and classification accuracy. For this, we consider three settings: (a) general classification, (b) out-of-distribution setting where we have in-domain data but with different distributions, and (c) open-set detection, where we have unknown samples from new domains.

### Experimental Settings

**Dataset description.** For the general classification setting, we consider three real-world datasets: Cifar10, Cifar100 , and TinyImageNet . For the out-of-distribution setting, we consider the corrupted version of the Cifar10 and Cifar100 datasets which are named Cifar10-C and Cifar100-C . It should be noted that in this setting, we train all models in clean dataset and perform testing in the corrupted datasets. For open-set detection, we use the SVHN dataset  as the open-set dataset and Cifar10 and Cifar100 as the close-set data. A more detailed description of each dataset is presented in the Appendix.

**Evaluation metrics.** To assess the model performance in the first two settings, we report the classification accuracy (\(\)) along with the Expected Calibration Error (\(\)). In the case of open-set detection, we report open-set detection for different confidence thresholds.

**Implementation details.** In all experiments, we use a family of ResNet architectures with two density levels: \(9\%\) and \(15\%\). To construct an ensemble, we learn 3 sparse sub-networks each with a density of \(3\%\) for the total of \(9\%\) density and that of \(5\%\) density for the total of density \(15\%\). All experiments are conducted with the 200 total epochs with an initial learning rate of 0.1 and a cosine scheduler function to decay the learning rate over time. The last-epoch model is taken for all analyses. For the training loss, we use the EP-loss in our DRO ensemble that optimizes the scores for each weight and finally selects the sub-network from the initialized dense network for the final prediction. The selection is performed based on the optimized scores. More detailed information about the training process and hyperparameter settings can be found in the Appendix.

### Performance Comparison

In our comparison study, we include baselines that are relevant to our technique and therefore we primarily focus on the LTH-based techniques. Specifically, we include the initial lottery ticket hypothesis (LTH)  that iteratively performs pruning from a dense network until the randomly initialized sub-network with a given density is reached. Once the sub-network is found, the model trains the sub-network using the training dataset. Similarly, we also include L1 pruning . We also include three approaches CigL , Sup-ticket , DST Ensemble  which are based on the pruning and regrowing sparse network training strategies. From Venkatesh et al.  we consider MixUp strategy as a comparison baseline as it does not require multi-step forward passes. A dense network is also included as a reference (denoted as _Dense_1). Furthermore, we report the performance obtained using the EP algorithm  on a single model with a given density. Finally, we also include the deep ensemble technique (_i.e.,_ Sparse Network Ensemble (SNE), where each base model is randomly initialized and independently trained. The approaches that require pre-training of a dense network are categorized under the _Dense Pre-training_ category. Those performing sparse network training but actually updating the network parameters are grouped as _Sparse Training_. It should be noted that sparse training techniques still require iterative pruning and regrowing. Finally, techniques that attempt to search the best initialized sparse sub-network through mask update (_e.g.,_ EP) are grouped as _Mask Training_.

    &  &  &  \\   & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\   & & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    & _Dense_1 & 94.82 & 5.87 & 95.12 & 5.99 & 76.40 & 16.89 & 77.97 & 16.73 \\   Dense Pre-training & _L1 Pruning _ & 93.45 & 5.31 & 93.67 & 6.14 & 75.11 & 15.89 & 75.12 & 16.24 \\  & _L1H _ & 92.65 & 3.68 & 92.87 & 6.02 & 74.09 & 15.45 & 74.41 & 16.12 \\  & _DLTH _ & 93.27 & 5.87 & 95.12 & 7.09 & 77.29 & 16.64 & 77.86 & 17.26 \\  & _Mixup _ & 92.86 & 3.68 & 93.06 & 6.01 & 74.15 & 5.41 & 74.28 & 16.05 \\  Sparse Training & _CigL _ & 92.39 & 5.06 & 93.41 & 4.60 & 76.40 & 9.30 & 76.46 & 9.91 \\  & _DST Ensemble _ & 88.87 & 2.02 & 84.93 & 0.8 & 63.57 & 7.23 & 63.22 & 6.18 \\  & Sup-ticket  & 94.52 & 3.30 & 95.04 & 3.10 & 78.28 & 10.20 & 78.60 & 10.50 \\  Mask Training & _AdaBoost_ & 93.12 & 5.13 & 94.15 & 5.46 & 75.15 & 22.96 & 75.89 & 24.54 \\  & _EP _ & 94.20 & 3.97 & 94.35 & 4.03 & 75.05 & 14.62 & 75.68 & 14.41 \\  & _SNE_ & 94.70 & 2.51 & 94.48 & 3.51 & 75.69 & 9.02 & 75.22 & 10.89 \\  & **DRE (Ours)** & 94.60 & **0.7** & 94.28 & **0.7** & 74.68 & **1.20** & 74.37 & **2.09** \\   

Table 1: Accuracy and ECE performance with \(9\%\) density for Cifar10 and Cifar100.

    &  &  &  \\   & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\   & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    & Dense1 & 79.65 & 19.63 & 79.65 & 19.63 & 54.75 & 35.32 & 54.75 & 35.32 \\   Dense Pre-training & _L1 Pruning _ & 77.34 & 17.95 & 76.39 & 17.89 & 52.06 & 31.45 & 51.67 & 30.98 \\  & _L1H _ & 75.85 & 17.88 & 76.15 & 17.62 & 50.79 & 31.23 & 51.35 & 30.56 \\  & _DLTH _ & 79.67 & 21.74 & 80.12 & 20.31 & 54.82 & 37.55 & 55.12 & 35.74 \\  & _Mixup _ & 76.35 & 17.74 & 76.88 & 17.55 & 51.36 & 31.12 & 51.92 & 30.35 \\  Sparse Training & _CigL _ & 70.80 & 21.04 & 69.84 & 21.42 & 49.42 & 25.86 & 51.49 & 24.13 \\  & Sup-ticket _ & 72.89 & 17.80 & 73.01 & 18.82 & 48.80 & 24.99 & 48.81 & 25.62 \\  Mask Training & _AdaBoost_ & 75.94 & 22.96 & 74.55 & 21.46 & 51.36 & 38.45 & 51.25 & 38.34 \\  & _EP _ & 77.58 & 17.82 & 77.73 & 17.46 & 52.18 & 30.60 & 52.14 & 29.48 \\  & _SNE_ & 78.93 & 15.73 & 78.61 & 15.56 & 54.74 & 24.22 & 54.00 & 20.54 \\  & **DRE (Ours)** & 78.57 & **10.92** & 78.00 & **10.19** & 54.11 & **14.28** & 53.21 & **8.13** \\   

Table 3: Accuracy and ECE performance on out-of-distribution datasets.

    &  &  &  \\   & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\   & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    & Dense1 & 79.65 & 19.63 & 79.65 & 19.63 & 54.75 & 35.32 & 54.75 & 35.32 \\   Dense Pre-training & _L1 Pruning _ & 77.34 & 17.95 & 76.39 & 17.89 & 52.06 & 31.45 & 51.67 & 30.98 \\  & _L1H _ & 75.85 & 17.88 & 76.15 & 17.62 & 50.79 & 31.23 & 51.35 & 30.56 \\  & _DLTH _ & 79.67 & 21.74 & 80.12 & 20.31 & 54.82 & 37.55 & 55.12 & 35.74 \\  & _Mixup _ & 76.35 & 17.74 & 76.88 & 17.55 & 51.36 & 31.12 & 51.92 & 30.35 \\  Sparse Training & _CigL _ & 70.80 & 21.04 & 69.84 & 21.42 & 49.42 & 25.86 & 51.49 & 24.13 \\  & Sup-ticket _ & 72.89 & 17.80 & 73.01 & 18.82 & 48.80 & 24.99 & 48.81 & 25.62 \\  Mask Training & _AdaBoost_ & 75.94 & 22.96 & 74.55 & 21.46 & 51.36 & 38.45 & 5

**General classification setting.** In this setting, we consider clean Cifar10, Cifar100, and TinyImageNet datasets. Tables 1, 2, and 6 (in the Appendix) show the accuracy and calibration error for different models with density \(9\%\) and \(15\%\). It should be noted that for the TinyImageNet dataset, we could not run the Sparse Training techniques due to the computation issue (_i.e.,_ memory overflow). This may be because sparse training techniques require maintaining additional parameters for the pruning and regrowing strategy. In the Appendix, we have made a comparison of the proposed DRE with those baselines on a lower architecture size. There are three key observations we can infer from the experimental results. First, sparse networks are able to maintain or improve the generalization performance (in terms of accuracy) with better calibration, which can be seen by comparing dense network performance with the edge-popup algorithm. Second, the ensemble in general helps to further lower the calibration error (lower the better). For example, in all datasets, standard ensemble (SNE) consistently improves the EP model. Finally, the proposed DRE significantly improves the calibration performance by diversifying base learners and allow each sparse sub-network to focus on different parts of the training data. The strong calibration performance provides clear empirical evidence to justify our theoretical results.

**Out-of-distribution classification setting.** In this setting, we assess the effectiveness of the proposed techniques on out-of-distribution samples. Specifically,  provide the Cifar10-C and Cifar100-C validation datasets which are different than that of the original clean datasets. They apply different corruptions (such as blurring noise, and compression) to shift the distribution of the datasets. We assess those corrupted datasets using the models trained using the clean dataset. Table 3 shows the performance using different architectures. In this setting, we have not included DST Ensemble, because: (a) its accuracy is far below the SOTA performance, and (b) same training mechanism as that of the Sup-ticket, whose performance is reported. As shown, the proposed DRE provides much better calibration performance even with the out of distribution datasets.

**Open-set detection setting.** In this setting, we demonstrate the ability of our proposed DRO ensemble in detecting open-set samples. For this, we use the SVHN dataset as an open-set dataset. Specifically, if we have a better calibration, we would be able to better differentiate the open-set samples based on the confidence threshold. For this, we randomly consider \(20\%\) of the total testing in-distribution dataset as the open-set samples from the SVHN dataset. The reason for only choosing a subset of the dataset is to imitate the practical scenario where we have very few open-set samples compared to the close-set samples. We treat the open-set samples as the positive and in-distribution (close-set) ones as the negative. Since this is a binary detection problem, we compute the F-score  at various thresholds, which considers both precision and recall. Figure 3 shows the performance for the proposed technique along with comparative baselines. As shown, our proposed DRE (refereed as DRO Ensemble) always stays on the top for various confidence thresholds which demonstrates that strong calibration performance can benefit DRE for open-set detection as compared to other baselines.

### Ablation Study

In this section, we investigate the impact of the backbone architecture along with the size of the ensemble. Additional ablation studies are presented in Appendix D.9.

**Performance analysis of different backbones.** Table 4 (a) reports the performance of Cifar10 from both DRE and EP using different backbone architectures. In case of WideResNet28-10, the calibration error is low without sacrificing the accuracy. It also demonstrates that the superior performance of DRE is not limited to a specific backbone. In case of ViT, DRE still achieves a much lower calibration error than EP. However, using ViT as a backbone, the accuracy from both EP and DRE is lower and ECE is higher than other backbones. Existing studies show that without pretraining, the lack of useful

Figure 3: Open-set detection performance on different confidence thresholds.

inductive biases for ViT can cause performance drop . Since no pretraining is conducted in both EP and DRE, it causes a lower accuracy (and a higher ECE).

**Impact of number of sparse-sub-networks.** In this analysis, we study the impact of number of sparse sub-networks. It should be noted that our work is not limited only for \(M=3\). We can instead increase the \(M\) value. For example, Table 4 (b) shows the performance for an ensemble model with \(M=5\), where each sub-network is trained with \(=3\%\), leading to a total \(=15\%\). We also show the performance with \(M=3\), where each sub-network is trained with \(=5\%\). As can be seen, if there is a sufficient learning capacity for each sub-network, the ECE score can further improve with the increase of \(M\).

### Qualitative Analysis

In this section, we provide illustrative examples to further justify why the proposed DRE is better calibrated as compared to existing baselines. Figure 4 (a)-(d) show the confidence values for the wrongly classified samples by different sparse sub-networks in the DRE ensemble. As can be seen, each sparse sub-network provides confidence values in different ranges, where the sub-network in (a) is learned from most representative samples and the one in (c) is from the most difficult ones. As these sub-networks are complementary with each other, the DRE has a much better confidence distribution for incorrect samples than the baselines. In contrast, as shown in Figure 8 of Appendix D.12, all the baselines, including the dense network, EP sparse networks, and the SNE, tend to allocate much higher confidence values to wrongly classified examples, leading to poor calibration. In case of correctly classified samples, the proposed DRE generates confident predictions and thereby not compromising the calibration performance, which is further discussed in Appendix D.12.

## 5 Conclusion

In this paper, we proposed a novel DRO framework, called DRE, that achieves an ensemble of lottery tickets towards calibrated network sparsification. Specifically, with the guidance of uncertainty sets under the DRO framework, the proposed DRE aims to learn multiple diverse and complementary sparse sub-networks (tickets) where uncertainty sets encourage tickets to gradually capture different data distributions from easy to hard and naturally complement each other. We have theoretically justified the strong calibration performance by demonstrating how the proposed robust training process guarantees to lower the confidence of incorrect predictions. The extensive evaluation shows that the proposed DRE leads to significant calibration improvement without sacrificing the accuracy and burdening inference cost. Furthermore, experiments on OOD and open-set datasets show its effectiveness in terms of generalization and novelty detection capability, respectively.

Table 4: ACC and ECE with different backbones and number of subnetworks

Figure 4: Confidence scores of incorrectly classified samples in CIFAR100 with ResNet101.