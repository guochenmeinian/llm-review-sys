# The Elephant in the Room: Towards A Reliable Time-Series Anomaly Detection Benchmark

Qinghua Liu and John Paparrizos

Department of Computer Science and Engineering

The Ohio State University

{liu.11085,paparrizos.1}@osu.edu

###### Abstract

Time-series anomaly detection is a fundamental task across scientific fields and industries. However, the field has long faced the "elephant in the room:" critical issues including flawed datasets, biased evaluation measures, and inconsistent benchmarking practices that have remained largely ignored and unaddressed. We introduce the TSB-AD to systematically tackle these issues in the following three aspects: (i) **Dataset Integrity**: with 1070 high-quality time series from a diverse collection of 40 datasets (doubling the size of the largest collection and four times the number of existing curated datasets), we provide the first large-scale, heterogeneous, meticulously curated dataset that combines the effort of human perception and model interpretation; (ii) **Measure Reliability**: by revealing issues and biases in evaluation measures, we identify the most reliable and accurate measure, namely, VUS-PR for anomaly detection in time series to address concerns from the community; and (iii) **Comprehensive Benchmarking**: with a broad spectrum of 40 detection algorithms, from statistical methods to the latest foundation models, we perform a comprehensive evaluation that includes a thorough hyperparameter tuning and a unified setup for a fair and reproducible comparison. Our findings challenge the conventional wisdom regarding the superiority of advanced neural network architectures, revealing that simpler architectures and statistical methods often yield better performance. The promising performance of neural networks on multivariate cases and foundation models on point anomalies highlights the need for further advancements in these methods. We open-source the benchmark at https://github.com/TheDatumOrg/TSB-AD to promote further research.

## 1 Introduction

The explosion of Internet of Things (IoT) applications has significantly increased the volume of sequential measurements . Analytical tasks such as querying , forecasting , classification , and clustering  over these ordered sequences of observations, commonly referred to as _time series_, are necessary virtually in every scientific discipline and their corresponding industries . Among these tasks, time-series anomaly detection is widely applied across various sectors , ranging from manufacturing quality assurance and data center monitoring to preventing financial fraud. Recently, there has been a surge in interest in this area, primarily driven by advancements in neural network architectures  and the availability of diverse datasets . However, the research state of this field has long been plagued by the use of flawed benchmark datasets , biased evaluation measures , and inconsistent benchmark practices.

The discussion regarding the quality of time-series anomaly detection datasets was initiated by Wu & Keogh , who identified common flaws, including triviality, anomaly density, mislabeling, and run-to-failure bias. To address these issues, they introduced a manually curated dataset featuring univariate time series with only a single anomaly, which was often artificially introduced. Therefore,this dataset is not necessarily representative of realistic settings (i.e., virtually all previously published real-world datasets contain more than one anomaly) and disregards other problematic, potentially anomalous regions, leading again to different types of mislabeling problems, as discussed in Section 3.1. Other flaws, such as trivial anomalies, may not necessarily justify the exclusion of a dataset because the real challenge lies in the failure of sophisticated methods to perform effectively, even in these simplified scenarios. Designing a comprehensive benchmark has long been a discussion. However, it seems that "everyone wants to do the model work instead of the data work" , resulting in limited new efforts to produce a large-scale, high-quality dataset.

Moreover, the consistent use of flawed evaluation measures continues to create the illusion of progress. For instance, the widely used point-adjustment technique, with the good intention of calibrating the anomaly prediction, favors noisy inputs, and even a random anomaly score has a decent chance of outperforming SOTA methods [48; 92]. Despite the flaw, the measure remains prevalent in recent research of deep-learning-based methods [107; 97; 105; 114], raising concerns about whether the advances are due to improved methods or merely more 'noisy' scoring. With the recently proposed measures [99; 32; 42; 69], it remains unclear which measures to adopt.

Inconsistent benchmarking practices compound these issues. Different communities evaluate their methods on different datasets, which presents a significant challenge when conducting a meta-analysis of their empirical performance. Furthermore, comprehensive benchmark studies may also introduce certain biases. For instance, one of the most cited studies  presents an analysis where not all methods were executed on the same datasets (i.e., methods failing to produce results within a specified amount of time or, due to some error, were essentially penalized). In addition, comparing methods with optimized hyperparameters on a single synthetic dataset could lead to misleading conclusions. Moreover, the lack of a unified setup for data preprocessing introduces unfairness in comparison.

From the above, it becomes a necessity to create a robust and reliable benchmark that merges collective wisdom from dozens of published datasets and previous benchmark studies while fixing flaws to facilitate unbiased and consistent benchmark practice. We start with a review of related work (Section 2), then we present our contributions:

* We discuss common flaws in existing datasets and evaluation measures (Section 3).
* We introduce TSB-AD, which comprises 40 datasets--doubling the size of the largest collection, four times the number of existing curated datasets, as well as 40 detection algorithms (Section 4).
* We perform a thorough evaluation on the TSB-AD benchmark to ensure fairness and reliability, and we discuss the insights gained from our research (Section 5).

Finally, we conclude with the implications of our work (Section 6).

## 2 Related Work

### Time-Series Anomaly Detection

**[Type of Time Series]** A time series is defined as an ordered sequence of real-valued observations. Consider the signal from \(N\) sensors over time \(T\), represented as \(X=\{x_{1},...,x_{T}\}\), where \(x_{t}^{N}\). A time series is termed _univariate_ if \(N=1\) and _multivariate_ if \(N>1\).

    &  &  &  \\   & \# Datasets & \# Curated TS & Uni & Multi & Stat & NN & FM & HP & \# Measures \\  Wu \& Keogh  & 1 & 250 & ✓ & \(\) & - & - & - & - & - \\ Lai _et al._ & 5 & 0 & ✓ & ✓ & 7 & 2 & 0 & \(\) & 3 \\ Schmid _et al._ & 15 & 0 & ✓ & ✓ & 49 & 22 & 0 & \(^{*}\) & 3 \\ Paparinos _et al._ & 18 & 0 & ✓ & \(\) & 9 & 3 & 0 & \(\) & 9 \\ Wagner _et al._ & 2 & 21 & \(\) & ✓ & 0 & 28 & 0 & ✓ & 3 \\ Zhang _et al._ & 15 & 0 & ✓ & ✓ & 11 & 6 & 0 & ✓ & 4 \\ 
**TSB-AD (ours)** & 40 & 1070 & ✓ & ✓ & 25 & 10 & 5 & ✓ & 10 \\   

* Hyperparameter tuning is conducted exclusively on a synthetic dataset and applied across the entire evaluation process.

Table 1: Comparison among TSB-AD and other existing time-series anomaly detection benchmarks. TSB-AD features the most extensive collection and manually curated datasets and provides the broadest coverage of algorithm categories comprising statistical methods (Stat), neural network-based methods (NN), and foundation model-based methods (FM). It also provides a holistic view of evaluation measures with robust hyperparameter tuning across all datasets (HP).

**[Type of Anomalies]** Anomalies in time series can occur in the form of a single value or collectively in the form of sub-sequences. Point and contextual anomalies, termed _point-based_, are individual data points that deviate significantly from the majority or expected pattern within a specific context, respectively. Collective anomalies, known as _sequence-based_, consist of sequences of points that deviate from a typical, previously observed pattern.

**[Taxonomy of Detection Algorithms]** The approaches to this task can be categorized based on the level of prior knowledge available: (i) unsupervised, which does not require any labeled data; (ii) semi-supervised, requiring labels only for normal instances; and (iii) supervised, which requires labeled normal and anomalous instances. Due to the limited availability of labeled anomalies, unsupervised or semi-supervised anomaly detection methods are more common. Based on the nature of the processing, the methods can be divided into three categories: distance-based, density-based, and prediction-based methods. Please refer to Appendix B.2 for a more detailed description.

### Comparision with Existing Benchmarks

Several benchmark studies have been conducted on time-series anomaly detection, but as previously discussed, they often exhibit certain flaws and biases. We select representative works and compare them with TSB-AD in Table 1. We highlight differences in three key aspects. First, regarding datasets, TSB-AD represents the most extensive collection of time-series anomaly detection datasets to date, nearly doubling the size of the previous largest collection . Beyond the sheer volume, we have meticulously curated the datasets using a principled approach that integrates human perception with algorithmic assistance, details of which are elaborated in Section 4.1. The number of curated time series in TSB-AD is more than four times that of the previous manually curated dataset , we further include multivariate time series, and address biases in the UCR  dataset. Second, regarding algorithms, TSB-AD is the first to introduce foundation models into anomaly detection benchmarks and encompass representative and top-performing methods identified in earlier studies. Third, in terms of evaluation, our objective is to establish a reliable and frequently updated testbed for fair model performance comparison. We conduct an in-depth investigation into the reliability of evaluation measures and the hyperparameter tuning of various algorithms, providing recommendations based on our findings--aspects that previous studies have neglected [74; 93; 112].

## 3 Common Flaws Creates Illusion of Progress

### Flaws in Datasets

We categorize common flaws in existing benchmark datasets, as illustrated in Figure 1.

**[Mislabeling Issues]** Concerns arise over the potential mislabeling of data, which may not be entirely attributable to dataset creators since they had access to additional, non-disclosed data. Our analysis primarily stems from observations of inconsistent labeling, where similar patterns are differently classified--some as anomalies and others not. Figure 1 (a) demonstrates a case where the second spike, similar to a previously labeled anomaly, remains unlabeled, indicating a false negative. Conversely, the anomaly labeled in the lower diagram lacks distinctive features, suggesting a false positive.

**[Bias in Datasets]** The datasets exhibit biases such as the run-to-failure bias, where anomalies predominantly occur towards the end of the time series, as exemplified by the Yahoo dataset . This bias can skew results in favor of algorithms that predict the final data points as anomalies. Furthermore, some datasets, such as the UCR dataset , operate under the assumption that the ideal number of anomalies per dataset is one, leading to the marking of only the most prominent anomaly. However, this is often not reflective of real-world conditions where anomalies of either the

Figure 1: Categorization of common flaws existing in current datasets. Anomalies are marked in red.

same or different types will occur multiple times. As depicted in Figure 1 (b), the potential anomalies highlighted within a circle are overlooked.

**[Feasibility of Datasets for Anomaly Detection]** Often, datasets designed for classification tasks are inappropriately repurposed for anomaly detection by simply reclassifying the minority class as 'anomalous.' However, it is beyond the intended scope of unsupervised anomaly detectors to identify classes with minimal occurrences, thereby constraining their applicability for benchmarking anomaly detection algorithms. Additionally, an unrealistic ratio of anomalies contravenes the fundamental principle that anomalies should be infrequent occurrences as depicted in Figure 1 (c).

### Flaws in Evaluation Measures

The detection of anomalies can be viewed as a binary classification problem where data points are classified into normal or abnormal observations. Thus, traditional classification evaluation measures are applicable to anomaly detection. However, the direct application of these measures to time-series anomaly detection causes issues. First, anomaly detection often deals with imbalanced datasets, which can compromise the reliability of certain measures. Second, traditional measures may not account for the sequential nature of time series. For instance, the standard F1 score treats each time step independently, disregarding the temporal dependencies between time steps. Third, while recent advancements have sought to modify these measures to better suit the time-series context, some of these adaptations can still introduce biases, potentially giving misleading indications of progress.

**[Unraveling ROC Curve in Anomaly Detection]** AUC-ROC  evaluates the performance of the model by measuring the area under a curve plotting the true positive rate (TPR) against the false positive rate (FPR), as illustrated in Figure 2 (a). However, anomaly detection typically features a significantly larger count of true negatives compared to false positives, which often yields low FPRs across various thresholds. Consequently, only a small portion of the ROC curve holds relevance under such circumstances. One potential approach to address this issue is to focus solely on specific segments of the curve . In addition, AUC-PR  has been advocated as a more informative alternative for imbalanced datasets .

Furthermore, previous benchmark studies [93; 74] have assumed that an AUC-ROC value exceeding 0.8 by at least one detection algorithm indicates high-quality labeling. However, as illustrated in Figure 2 (a), despite an AUC-ROC of 0.97, the presence of two false negatives directly challenges this criterion. This scenario not only questions the previous assumption but also underscores the potential for the AUC-ROC measure to overestimate performance in anomaly detection. We argue that relying on a single measure is insufficient for accurately assessing label quality. To address this, we introduce a principled method that combines human perception and algorithmic tests to assess the label quality. A detailed description will be provided in Section 4.1.

**[Shortcomings of Point-based Evaluation Measures]** The two measures discussed above are point-based evaluation measures in which each point is considered independently, and the detection of each point contributes equally to the AUC. As illustrated in Figure 2 (b), a slight lag in anomaly score results in a significant difference in point-based evaluation measures. However, such lag is often unavoidable due to inconsistencies in labeling practices across datasets and potential delays introduced by anomaly detectors. Consequently, a lack of robustness to lag introduces bias into overall evaluation results. In the context of time series, we argue that two similar anomaly scores with a slight lag should yield approximately the same accuracy measures. For example, a high anomaly score near the boundary of an anomaly should be rewarded similarly to a high anomaly score within the center of

Figure 2: Overview of flaws in evaluation measures.

a range-based anomaly. Range-based evaluation measures, such as VUS-PR , have been proposed to accommodate the sequential nature of time series rather than focusing solely on individual points. Further details on other range-based evaluation measures are provided in Section 4.3.

**[Bias Towards Random Score]** Point Adjustment (PA) assumes that detecting any point within an anomalous segment is considered as if all points within that segment were detected. However, as depicted in Figure 2 (c), this measure tends to favor noisy predictions, whereby even random scores have a decent chance of predicting at least one point in a sequence of ground truth anomalies, performing comparably to state-of-the-art anomaly detectors . Moreover, randomly generated predictions under point adjustment can even outperform SOTA methods, with its point-adjusted F score approaching one as the average length of the anomalies increases. Despite its tendency to substantially overestimate detection performance, this technique remains prevalent in numerous current studies . It is imperative that future evaluations employ unbiased measures to ensure accurate method assessments. Please refer to Section 5.2.1 for further discussion of the reliable evaluation measures.

## 4 TSB-AD: A Reliable Time-Series Anomaly Detection Benchmark

### Dataset Overview

#### 4.1.1 Dataset Construction Pipeline

As illustrated in Figure 3, the dataset construction process encompasses three primary steps: (i) dataset collection, which collects datasets introduced over recent decades for anomaly detection in time series; (ii) flaw identification to exclude problematic time series that exhibit common flaws as described in Section 3.1; and (iii) label quality assessment to ensure high-quality labeling, details of are provided in Section 4.1.2. Each step incorporates the consensus of four human annotators.

**[Step 1]** The process begins with an extensive collection of 13 univariate and 20 multivariate public time-series anomaly detection datasets which will be further detailed in the Appendix B.1. To enhance the diversity and size of dataset collection, we implement a transformation strategy that converts multivariate time series into univariate formats by treating each channel as an independent series. This strategy is based on the following observations: in some multivariate datasets, only a limited number of channels (often just one) provide valuable information for anomaly detection, while other channels contain categorical, binary, or random values. In addition, our correlation analysis, which evaluates the relationship between the anomaly score of each channel and the ground truth anomaly labels, demonstrates that certain channels exhibit a stronger correlation with the ground truth than others. These observations helped us transform the informative channels of multivariate time series into univariate time series datasets while ensuring the ignored channels do not contribute to the detection of anomalies. During this step, we evaluate multiple anomaly detectors across each channel using distinct evaluation measures. For each time series, we record the highest measure across all detectors as the evaluation result for that data. Subsequently, we select the top 40% of time series for each measure based on these evaluation results (it is important to note that the dataset pruning process is iterative; any time series with suboptimal labeling that passes initial stages can be addressed and removed in subsequent iterations). We then identify the intersection of these selected sets, resulting in an additional 13 univariate time series datasets. By this step, we have obtained a total collection of 46 datasets of univariate and multivariate time series.

**[Step 2]** Given the lack of consensus in a formal definition of what constitutes a time-series anomaly and the lack of context for producing the labels, we rely on provided anomaly labels to assess

Figure 3: Illustration of dataset construction pipeline and summary statistics of TSB-AD, which comprise 1070 curated time series, with 870 univariate and 200 multivariate.

their suitability for anomaly detection tasks from the perspective of a machine learning practitioner. Moreover, we note that relying solely on measures to assess label quality is not sufficient, as discussed in Section 3.2. Therefore, we further perform manual inspections to identify and remove time series exhibiting common flaws as described in Section 3.1.

**[Step 3]** The task of assessing a dataset's suitability for anomaly detection and the rationality of its labeled anomalies often surpasses human annotators' capabilities. Hence, we utilize anomaly detection algorithms to aid in verifying label quality, as detailed in the following sections.

Upon completing these steps, we have obtained a high-quality set of anomaly-labeled time series, encompassing both univariate (TSB-AD-U) and multivariate (TSB-AD-M) datasets as depicted in Figure 3. The datasets are divided into two partitions: the Eval set, designated for evaluation, and the Tuning set, used for optimizing hyperparameters. However, the number of time series in some univariate datasets, such as UCR  and YAHOO , is much larger than others. This disparity can lead to a scenario where methods that perform well on that one dataset dominate the entire benchmark. To address this issue, we employ strategic sampling techniques to ensure a more balanced distribution for TSB-AD-U-Eval. Detailed information about the sampling process can be found in Appendix B.1. Please refer to Table 2 for summary statistics of TSB-AD datasets.

#### 4.1.2 Label Quality Assessment

As depicted in Figure 4, the objective of the algorithm test is to differentiate among the following scenarios: (i) good label quality, where at least one anomaly detector successfully identifies the anomalies; (ii) bias within the dataset, which can be addressed by segmenting highly confident regions or extending the label; (iii) good label quality but the anomaly is inherently difficult to detect; (iv) the lack of sufficient in-context data to indicate an anomaly, often resulting from the improper application of classification datasets to anomaly detection. Our goal is to establish a benchmark that enables pure anomaly detectors to identify anomalies solely based on the time series data, without relying on external knowledge. Please refer to Appendix B.1.2 for details on algorithm tests.

### Time-Series Anomaly Detection Algorithms

In TSB-AD, we have compiled a comprehensive collection of 40 time-series anomaly detection algorithms, comprising statistical, neural network-based, and the latest foundation model-based methods. These methods are selected as representatives of the top-performing models identified by previous benchmark studies [74; 93; 112] and recent research publications. Please refer to the Appendix B.2 for details on the description of algorithms and implementation.

**[Statistical Method]** This category encompasses methods that utilize statistical assumptions to detect anomalies manifesting as deviations from the expected data distribution.

Figure 4: Illustration of the label quality assessment. The flowchart on the left depicts the algorithm testing procedure, starting with an input of labeled time series and the corresponding anomaly scores generated by multiple detectors. The four diagrams on the right exhibit various cases of label quality, including (a) good, (b) poor, (c) hard anomaly, and (d) those that require segmentation.

**[Neural Network-based Method]** Methods in this category often adopt a semi-supervised approach, learning to model normal patterns from a history of training data that are anomaly-free and subsequently pinpointing anomalies in new test data.

**[Foundation Model-based Method]** In recent years, there has been a paradigm shift driven by the emergence of foundation models . These models exhibit impressive few-shot or even zero-shot generalization capabilities across a broad spectrum of downstream tasks, often surpassing task-specific models. Works in this area generally fall into two categories: the adaptation of LLMs for time-series anomaly detection tasks, and the utilization of foundation models pre-trained on large-scale time-series data for various time-series applications. In the former category, OFA  finetunes the pre-trained GPT backbone on time series data. In the latter category, MOMENT  is a family of time-series foundation models for general-purpose time-series analysis, pre-trained through a masked time-series modeling approach. We assess both the zero-shot (ZS) and fine-tuned (FT) detection capabilities of MOMENT. Additionally, models like Lag-Llama , Chronos , and TimesFM , originally designed for time-series forecasting, are repurposed in our benchmarks to perform anomaly detection. This is achieved by comparing forecast values derived from a sliding context against the actual values. To ensure fairness in comparison, we employ the mean squared error between the predictions and the actual values as the anomaly score.

### Evaluation Measures

Aligning with previous benchmark evaluations [93; 74; 37], we treat the threshold setting on the anomaly score as an orthogonal problem to our primary focus of model performance evaluation. Our approach either utilizes measures that summarize performance across all possible thresholds or iterates over these thresholds to identify the optimal setting. Considering time-series anomaly detection as both a binary classification and semantic segmentation task , we incorporate both _point-wise_ measures, which assess the accuracy of detection of individual anomalies, and _range-wise_ measures, which offer a robust evaluation of model performance in the context of time series.

**[Point-wise Measures]** For point-wise anomaly detection, we employ widely used measures such as **AUC-ROC**, **AUC-PR**, and **Standard-F1**. For completeness, we additionally incorporate the imperfect yet widely employed **PA-F1** measure, which applies point adjustment to the prediction. The **Event-based-F1** addresses biases from point-adjustment techniques by treating each anomaly segment as an individual event, contributing only once to either a true positive or a false negative.

**[Range-wise Measures]** By considering the sequential nature of time series data, **R-based-F1** expand upon traditional measures by incorporating factors such as existence reward, overlap reward, and cardinality factor. Moreover, **Affiliation-F1** introduces a novel approach by focusing on the proximity between predicted and actual anomaly sequences, measuring the temporal distance between their occurrences. Traditional measures like AUC-ROC and AUC-PR, which assign equal importance to each detection point, often overlook the nuances of labeling consistency and the impact of time lags on anomaly scores. The Volume Under the Surface (VUS) measures, **VUS-ROC** and **VUS-PR**, aim to overcome these issues by incorporating a tolerance buffer around outlier boundaries and adopting continuous values over binary labels, thus enhancing the relevance of anomaly scoring. In addition, **PATE** applies proximity-based weighting around anomaly intervals to calculate the weighted version of the area under the Precision and Recall curve.

## 5 Benchmark Evaluation and Analysis

### Experimental Setup

**[Tuning/Evaluation Dataset Spilitting]** For both TSB-AD-U and TSB-AD-M, we allocate 15% of the data from each dataset to construct a hyperparameter tuning set. This selection ensures that the tuning set includes representative time series from each dataset, which helps to mitigate bias associated with tuning based solely on one synthetic dataset . Subsequently, the evaluation and comparison of model performance are conducted on the remaining time series.

**[Hyperparameter Tuning]** To ensure fairness by comparing algorithms under their optimal configurations, we design a search space for each algorithm based on recommendations from its original publication or open-source implementation. For instance, for the LOF method , we explore a range of models varying the number of neighbors among {10, 20, 30, 40, 50} and the distancemeasures {minkowski, manhattan, euclidean}, resulting in 15 different models. The top-performing model on the tuning set is then selected as the proxy of this detection algorithm for further evaluation. Similarly, for neural network-based methods, we explore hyperparameters such as learning rate and the number of hidden layers [90; 65; 97; 38]. In this way, we obtain over 450 variants from 40 detection algorithms. For details on the candidate hyperparameter settings, please see Appendix C.

### Experimental Results and Discussion

To ensure a fair and reliable benchmark evaluation, we begin with the investigation of evaluation measures. Subsequently, we compare various methods through both global and fine-grained analyses. Finally, we compare the insights derived from our study with those from prior benchmark studies to assess progress and consistency in findings. Key results are presented in this section, with supplementary details, including additional comparisons and runtime analyses in Appendix D.

#### 5.2.1 Investigation of Evaluation Measures

With 10 evaluation measures available in TSB-AD (Section 4.3), our objective is to identify measures that robustly measure the performance of anomaly detectors without favoring certain anomaly scores or patterns. We explore these measures from the following two perspectives.

**[Case Study]** To provide a detailed analysis of how each evaluation measure applies its criteria across varying prediction scenarios, we compare different evaluation measures across multiple synthetic examples in Figure 5. Beginning with S1, where predictions occur before the ground truth anomaly, to Random (C), which uses a continuous uniform random score ranging from 0 to 1, and Random (D), a synthetic example with a binary random anomaly score.

We categorize the issues related to evaluation measures into three main categories: (i) bias, referring to measures that favor certain cases or provide inconsistent evaluations under similar conditions; (ii) indiscrimination, where measures fail to meaningfully distinguish between different predictions; and (iii) lack of adaptability, where measures do not account for the specific characteristics of time series data. As shown in Figure 5, with respect to bias, AUC-ROC and VUS-ROC yield high scores for random cases, sometimes exceeding those of S1-S3. PATE and PA-F1 are affected by Random (D) and Random (C), respectively. Specifically, PATE demonstrates sensitivity to lag and inconsistent penalization for early detection, as seen when comparing S1 to S5 and S7 to S9. For discrimination, Affiliation-F provides almost no differentiation across S1 to S10, yielding consistently high scores across scenarios. Regarding lack of adaptability, measures highlighted in yellow fail to account for the time series nature, leading to significant score variations with slight shifts in prediction due to lag.

Figure 5: Comparison of evaluation measures for synthetic data examples (left) under different scenarios. S8 corresponds to the oracle situation where prediction is exactly the labeled anomaly. We use different color codings to represent different problematic cases.

Figure 6: Illustration of reliability of evaluation measures regarding (a) lags and (b) biases.

**[Quantitative Analysis]** Beyond the case study, we conduct a quantitative analysis to evaluate the sensitivity of evaluation measures to lags--misalignments between predicted anomalies and ground truth-- which are inevitable due to inconsistent labeling practices across datasets and potential lags introduced by anomaly detectors. To enhance clarity, we exclude measures with significant bias. Specifically, we introduce lags into the labels and compute the standard deviation of the evaluation results associated with different anomaly scores. As shown in Figure 6 (a), VUS-PR demonstrates substantial robustness compared with other measures. Moreover, as illustrated in Figure 6 (b), when comparing the Random (C) with the anomaly score generated by 32 anomaly detectors, the random score achieves a ranking of 26 under the PA-F1 measure. This finding confirms that PA-F1 exhibits significant bias toward noisy input, rendering it unsuitable for reliable evaluations.

Based on the criteria outlined above, VUS-PR emerges as the most robust (less sensitive to lags), accurate (unbiased and effective across different scenarios), and fair (consistent under similar cases) evaluation measure. In contrast, PATE, while extending the principles of VUS, introduces new challenges that complicate method evaluation and significantly increase computational demands. In the following section, we use VUS-PR as the measure for fair and accurate evaluations. For comprehensive performance analysis, results using additional measures are provided in Appendix D.

#### 5.2.2 Benchmark Accuracy Evaluation

Utilizing the most reliable evaluation measures and our curated dataset, we aim to reassess representative anomaly detectors and reveal the current state of research progress through a rigorous benchmarking study. For fair comparison, we apply z-normalization to the time series as a preliminary data preprocessing step, unless an alternative normalization is used in the original implementation. Figure 7 illustrates the evaluation results on TSB-AD-U/M. For clarity, only the top 12 methods are shown here, with a more detailed comparison available in Appendix D. In the boxplot, methods are ordered from left to right according to their rankings based on the average VUS-PR score across all the time series in TSB-AD. While the average VUS-PR value is useful for a global assessment, it should be accompanied by rigorous statistical analysis and CD diagrams to determine whether an improvement in average VUS-PR also reflects an improvement in the average rank per time series. Detailed statistical analysis is provided in Figure 11.

Among the top 12 methods in TSB-AD-U, more than half are statistical approaches, with Sub-PCA dominating the rankings. Only two neural-network-based methods (USAD, CNN) and one foundation-model-based method (MOMENT) are represented, where the fine-tuned version of MOMENT outperforms the zero-shot version. Given that the pretraining datasets of MOMENT include anomaly detection datasets, we provide further analysis of potential data contamination in Appendix D.3. In TSB-AD-M, neural-network-based methods show increased promise, with CNN and OmniAnomaly ranking second and third, respectively. However, statistical methods continue to be highly effective in multivariate cases. Several top-performing methods, including PCA, CNN, and USAD, consistently rank highly across both TSB-AD-U and TSB-AD-M.

#### 5.2.3 Analysis on Anomaly Types

We provide a fine-grained analysis of model performance across various anomaly types on TSB-AD-U. Model comparisons are conducted using the Friedman test , followed by a posthoc

Figure 7: Accuracy evaluation on univariate (a) TSB-AD-U and multivariate (b) TSB-AD-M. In the boxplot, the mean value is marked by a dashed line and the median by a solid line.

Nemenyi test , a widely used statistical approach for comparing multiple algorithms across datasets. Algorithm groups exhibiting no significant performance differences are interconnected horizontally in the Critical Diagram (CD). As depicted in Figure 8, we start by comparing different methods on time series data characterized by point-based anomalies and sequence-based anomalies. Foundation-model-based approaches demonstrate strong potential in detecting point-based anomalies, with TimesFM and Chronos ranking first and third, respectively. However, for sequence-based anomalies, statistical methods continue to dominate, with Sub-KNN and POLY outperforming other methods. Overall, neural networks appear more effective in detecting point anomalies. Subsequently, we analyze the performance difference in scenarios with a single anomaly versus multiple anomalies. No neural-network-based approach ranks as a top candidate in single-anomaly scenarios; however, in more complex cases with multiple anomalies, MOMENT begins to show effectiveness.

#### 5.2.4 Discussion

We share the following research insights drawn upon the experimental results on TSB-AD. (i) Statistical-based methods generally demonstrate robust performance, while neural network-based methods do not exhibit the superiority often attributed to them. However, neural networks and foundation models still strive to excel in detecting point anomalies and in handling multivariate cases. (ii) Simpler architectures such as CNNs and LSTMs generally outperform more complex designs, such as advanced transformer architectures. This finding is consistent with recent research . (iii) Foundation models excel at detecting point-based anomalies but struggle with sequence anomalies mostly due to their predictive mechanism, which estimates only one new value per step using a limited look-back window. When faced with long sequence anomalies, the constrained temporal context often leads to reduced performance and noisy scores. The use of flawed point-adjustment techniques that favor these noisy scores further exacerbates this issue, creating an illusion of progress. (iv) The performance of time-series foundation models shows great promise: they not only achieve good performance after fine-tuning but also demonstrate superior zero-shot capabilities when compared to most existing statistical and neural network-based methods. However, a primary concern with foundation models is the risk of data contamination due to the large scale of the pretraining data. Therefore, caution is needed in their deployment. (v) The effort to integrate LLMs into time-series anomaly detection  has yielded unsatisfactory results, indicating a significant research gap in this area. (vi) Among the top-performing methods, Sub-PCA and KShapeAD demonstrate exceptional performance, despite having been overlooked as basic baselines for many years and remaining undiscovered in previous extensive evaluation studies . The strong performance of CNN and OmniAnomaly in multivariate cases--contradicting previous benchmarks , where KMeansAD was found to be superior--suggests that complex scenarios in multivariate time series require greater modeling capacity, often beyond that of statistical methods.

Finally, it is important to note that no benchmark is perfect, we mainly rely on a limited number of experienced time-series users for manual inspection. We plan to keep expanding the methods and leaderboards and address issues of datasets to ensure a reliable and continuously updated benchmark.

## 6 Conclusion

In this paper, we introduce TSB-AD to address the biases in current benchmarking practices, as well as the issues stemming from flawed datasets and evaluation measures. We provide the first large-scale, manually curated dataset spanning 40 datasets, along with a collection of 40 anomaly detection algorithms and an investigation of 10 evaluation measures. We believe that TSB-AD can serve as a reliable testbed with high-quality datasets (TSB-AD-U/M) and an accurate evaluation measure (VUS-PR), and we advocate for continued efforts in refining dataset creation practices.

Figure 8: Illustration of model performance on different types of anomaly on TSB-AD-U.