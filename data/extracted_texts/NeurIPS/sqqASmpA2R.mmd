# Stable and low-precision training for large-scale vision-language models

Mitchell Wortsman\({}^{*}\)\({}^{1}\) Tim Dettmers\({}^{*}\)\({}^{1}\) Luke Zettlemoyer\({}^{12}\) Ari Morcos\({}^{1}\)\({}^{2}\) Ali Farhadi\({}^{1}\)\({}^{1}\) Ludwig Schmidt\({}^{1}\)\({}^{134}\)

###### Abstract

We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge--the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become underestimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.

## 1 Introduction

Large models trained on large datasets have recently led to multiple breakthroughs in machine learning such as GPT-3  and PaLM . While many components are necessary for successful large-scale training, two critical elements are training speed and stability. To enable further progress, we must ensure that 1) training is fast--the model should be able to see a lot of data even if it is large, and 2) training is stable--large models should not suffer from loss spikes which degrade performance. We study these two directions in the context of contrastive language-image pre-training (CLIP) . We examine CLIP-style models because of their importance in computer vision: CLIP-style models reach state-of-the-art performance on a wide range of image classification tasks  and underlie image generation methods such as DALLE-2  and Stable Diffusion . Our contributions towards fast training and stable training are as follows.

**Towards fast training**, we introduce **SwitchBack**, a linear layer for quantized training with int8 precision which matches the performance of the bfloat16  baseline within 0.1 percentage points for CLIP ViT-Huge--a larger model than considered in the original CLIP paper. Linear layers account for the majority of the compute in standard transformer models, usually more than 90%, comprising the key, query, value, and out projection of the attention blocks as well as the multilayer perceptron. We perform all linear layers in low-precision (int8) while retaining other layers, such as layer norms, in higher precision. With this setup, we observe end-to-end speedups between 13 and 25% for CLIP ViT-Huge training: 25% compared to a standard linear layer implemented using the PyTorch  autograd python module and 13% compared to the standard PyTorch layer which include background CUDA/C++ optimizations which are difficult to replicate for custom layers.

SwitchBack starts from the observation that quantization noise grows with the inner dimension in a matrix multiplication. For CLIP training, the weight gradient computation involves a large inner dimension because CLIP training requires a large batch size . Hence SwitchBack uses 16 bit precision matrix multiplication for the weight gradient computation while using int8 multiplications for the forward pass and layer input gradient computations. This approach leads to large accuracy improvements compared to LLM.int8()  (Figure 1). We will provide open-source Triton  kernels for Switchback to enable future work on efficient quantization schemes.

Besides int8 training, we also study large-scale 8-bit float (fp8)  training. We do not have access to hardware that supports fp8 data types, which is currently more rare than int8, so we use an accurate simulation of fp8 computation. SwitchBack also outperforms straightforward 8-bit float (fp8) baselines because tensor-wise quantized baselines diverge at \(>\)420M scale (Figure 1). However, we demonstrate that these methods can achieve high accuracy if the network is trained while keeping feature magnitudes small, which we accomplish via layer-scale  initialized with zeros.

**Towards stable training**, we find that loss spikes occur in CLIP training when the AdamW  second moment estimator becomes out-of-date in the patch embedding  layer. In particular, the learning signal changes so that the moving averages of squared gradients underestimates their true magnitude. Indeed, in the absence of stability interventions, we show that loss spikes can be predicted by examining this ratio of the squared gradients to their moving average. We therefore recommend an AdamW-AdaFactor  hybrid, which we refer to as **StableAdamW** as it removes instabilities at the scales we consider and outperforms gradient clipping. Concretely, StableAdamW is AdamW with the update clipping technique introduced in AdaFactor. Update clipping tracks the average ratio of the gradient square to the second moment estimator and lowers the learning rate when the ratio is large.

The remainder of this paper is organized as follows: Section 2 focuses on low-precision training while Section 3 stabilizes training by reducing loss spikes.

## 2 8-bit training

This section develops and compares methods for eight-bit training of language-vision transformer models. First, Section 2.1 discusses preliminaries and related work. Next, Section 2.2 introduces and tests SwitchBack, a linear layer for int8 and float8 training. Finally, Section 2.3 develops alternatives to SwitchBack which can be used for float8.

### Preliminaries and related work

Neural networks today typically use 16-bit operations for training  in either the float16 or bfloat16 format . Floating point formats use a subset of bits to represent the exponent while the remainder specifies the fraction (often referred to as the mantissa). The float16 format uses 5 bits for the exponent while bfloat16 uses 8 and therefore covers a larger range--float16 has a range of \((5.96 10^{-8},65504)\) while bfloat16 has a range of \((10^{-38},3 10^{38})\). Most floating point formats also have denormalized numbers which allow for a "soft underflow" which gets exponentially closer to 0.0f for each additional

Figure 1: We introduce SwitchBack, a linear layer for low-precision training. (**Left**) SwitchBack for int8 training matches the zero-shot ImageNet  accuracy of standard bfloat16 training within 0.1 percentage point for CLIP ViT-Huge [46; 20] and outperforms LLM.int8() . (**Right**) For float8 (fp8) training , a baseline which uses tensor-wise quantization diverges for large models while SwitchBack matches the baseline. In these large-model, small-data experiments, our focus is on comparing methods and not final model accuracy, so we use short runs which makes it feasible to run many experiments.

bit in the mantissa. To prevent underflows float16 mixed precision training  has been developed which works as follows. The loss of a mini-batch is multiplied by a loss scalar to scale the loss and following backpropagation gradients into the representable range of fp16. This loss scaling is undone by rescaling the weight gradients before the optimizer updates fp32 main weights with the fp16 gradients. In PyTorch , the loss scalar is initialized to 65536. Everytime an Inf/NaN is encountered, the update is skipped and the loss scalar is halved. If no Inf/NaN are encountered for 2k iterations, the scalar is doubled.

When the loss scalar becomes too low in float16 training the loss slowly diverges. This was observed by Cherti et al.  when training ViT-Huge CLIP models and remedied by switching to bfloat16. Another instance of float16 creating issues at scale was the training of OPT  and BLOOM models . Indeed, many obstacles faced during the OPT project could have been alleviated by using bfloat16 . Similarly, all float16 training runs for BLOOM ended in divergence, only after using bfloat16 was the training stable. However, fast bfloat16 support is only available on TPUs, or GPUs developed with or after the NVIDIA Ampere series (2021 or later).

While 16 bit training is the standard today, hardware support for 8 bit operations are becoming more common. Hopper GPUs support float8 (fp8)  and Ampere GPUs support int8. However, it is currently (2023) very difficult to attain Hopper GPUs. Moreover, while int8 and int4 are used for inference [17; 65; 16], and there is earlier work exploring 8 bit training for convnets [61; 78; 10], these formats are not commonly used for training transformer models at scale. The CLIP ViT-Huge models we train have 1B parameters including the image and text towers which is 40x larger than a standard ResNet-50 (23M) , and quantization is more challenging for large tensors . Additional related work on quantization of large scale models (larger than BERT-large) and low-precision training and be found in Appendix A.

### SwitchBack

#### 2.2.1 Method

**Overview.** A linear layer consists of three matrix multiplications--one in the forward pass to compute outputs and two in the backwards pass to compute gradients for the input and weights. Our SwitchBack layer uses 8 bit precision for the first two matrix multiplies but switches back to higher precision for the weight gradient.

We compute the weight gradient in higher precision because this matrix multiplication involves dot products between vectors which have a length of batch size times sequence length. As CLIP training requires large batch sizes [46; 44], this inner dimension of batch size times sequence length is much larger than for the other matrix multiplies. As we show in Appendix D, variance due to quantization increases with the inner dimension of the matrix multiply. This modification is what differentiates SwitchBack from LLM.int8(), allowing SwitchBack to match the bfloat16 baseline (Figure 1).

**Notation.** A standard linear layer is comprised of inputs \(X^{b n}\), weights \(W^{m n}\), and outputs \(Y^{b m}\). In the forward pass, outputs are computed as \(Y=X^{}\). In the backwards pass the layer receives gradients of the loss with respect to \(Y\), which we denote \(\). Then, gradients to inputs \(\) are computed via \(=W\) while gradients to the weights \(\) are computed via \(=^{}X\). For linear layers in a transformer , \(b\) is batch size times sequence length, while \(n\) and \(m\) are small multiples of the embedding dimension.

**Quantization.** For the matrix multiplies in 8 bit precision we use quantization. There are a multiple quantization techniques to choose from and we will release code for all these alternatives. However, we find the best trade-off of simplicity and performance is from using i) row-wise quantization  for the inputs and gradients and ii) tensor-wise quantization for the weights. Additional information on quantization methods is provided by Dettmers et al.  but we summarize below. Using int8 as an example, which can represent integers from \(-127\) to 127, we now define row-wise and tensor wise quantization. For a matrix \(X\) with rows \(x_{1},...,x_{b}\), row-wise quantization \(Q_{}\) and tensor-wise quantization \(Q_{}\) are given respectively by

\[Q_{}(x_{1}\\ \\ x_{n})=((x_{1})} x_{1}\\ \\ (x_{b})} x_{b}),\;\;Q_{ }(X)=( (X)} X)\] (1)

where \(\) is the maximum of the absolute value.

Importantly, when applying \(Q_{}\) we also save the row-wise absolute maximums so that we can use them later for dequantization. We refer to this as the quantization state, or _state_, for short, so \(_{}(X)=[(x_{1}),...,(x_{b})]^{}^{b 1}\). Equivalently, for tensor-wise quantization we only need to store the tensor-wise absolute maximum so \(_{}(X)=(X)\).

Since only the matrix multiply occurs in int8 precision we need to dequantize the outputs back to the original floating point precision. The forward pass with quantization and dequantization becomes

\[_{}(W)}{127^{2}}_{}(X)*}(X)Q_{}(W )^{}}_{}\] (2)

where \(*\) denotes elementwise-multiplication, which in this case is broadcasted so that row \(i\) of the matrix \(Q_{}(X)Q_{}(W)^{}\) is multiplied by element \(i\) of \(_{}(X)\).

As mentioned previously, we use row-wise quantization for the inputs and gradients and tensor-wise quantization for the weights. We find that using row-wise quantization for both matrices increases complexity at a negligible or no performance increase. As such, we use this simpler approach.

The last detail in our algorithm is hardware specific. NVIDIA GPUs, which we use in this work, do not implement the int8/float8 operation \(AB\) for matrices \(A\) and \(B\) and only \(AB^{T}\) is implemented. As such, it is necessary to transpose the weight matrix in the backward pass. To reduce the overhead of transposition and quantization we fuse both operations, meaning we load the required data once from slow DRAM into fast SRAM/shared memory and then perform both operation in this cached memory - this is critical for achieving speedups. We call this operation tensor-wise_quantize_transpose, which is a fused tensor-wise quantize and transpose operation. Putting the pieces together, the result is Algorithm 1.

**Variants.** While Algorithm 1 is the most straightforward version of SwitchBack, we also present two alternative versions--SwitchBackM and SwitchBackQ--and will release triton  implementations for all three. Appendix C contains pseudocode. SwitchBackM (Algorithm 3) is a memory efficient version of SwitchBack which only saves 8 bit tensors for the backwards pass--we recommend its use when memory is limited. The small downside of SwitchBackM is that it requires an additional dequantize operation during the backwards pass which increases the runtime overhead. For CLIP ViT-Huge we observed only a negligible accuracy differences between SwitchBack and SwitchBackM. In addition, we present SwitchBackQ (Algorithm 4) which uses row-wise and column-wise quantization for the weights instead of tensor-wise. While we did not observe this to improve accuracy at the scales we consider, it's possible that it will perform better than SwitchBack at larger scale.

**float8.** While the explanation so far has used int8 as an example, the code for SwitchBack and float8 (fp8) is nearly identical. The only modification is that operations such as \((127x/(x))\) are replaced by \((x/(x))\) where we simulate float8cast by rounding to the exact values of the floats data type. This simulation improves on the simulation of  which only clips the input tensors into the representable range of the floats data type, but not the exact values of the floats data type. This simulation theoretically matches float8 training, but we are unable to perform real floats training because we lack the hardware that supports float8 arithmetic. As such, we perform arithmetic in 16-bit with exact float8 values. For our int8 experiments we conduct the multiplications in int8 using A100 GPUs--we perform real int8 training without any simulation.

#### 2.2.2 Experimental setup

To evaluate SwitchBack we train CLIP  visual transformer  models on LAION-2B . Typically CLIP training, especially at ViT-Huge scale, is prohibitively expensive. Our goal is not high final accuracy but rather to contrast different methods for low-precision training. To enable running multiple experiments, we therefore only train for a small number of samples seen--380 million images--and use patch-dropout 0.5 . We note that the experiment is still very expensive, corresponding to roughly 300 epochs of ImageNet training in terms of samples seen, or approximately 2.9e20 FLOPs per training run. After training on LAION-2B we evaluate the models zero-shot on ImageNet  using the 80 prompt templates from CLIP .

We use batch size 16384 (per-gpu batch size of 256) and train for a total of 20k iterations. The first 5k iterations are linear warmup while the remaining 15k are cosine decay. Training and evaluation are conducted with the OpenCLIP library  with learning rate 2e-3, weight decay 0.2, and batch-size 16384 using the optimizer described in Section 3.5.

#### 2.2.3 Results

We test two main questions: (1) can we replicate 16-bit performance with SwitchBack and (2) can we get speedups. To test (1) we train CLIP models with SwitchBack across multiple scales with both int8 and float8 precision (Figure 1). To test (2) we profile operations in an individual linear layer and also measure end-to-end training speed. Loss curves for the training runs in Figure 1 are shown in Appendix Figure 9.

**Accuracy.** We find that SwitchBack can match standard 16-bit training performance and outperform baselines for both a) int8 precision and b) float8 precision.

For our int8 experiments (Figure 1, right), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) the matrix multiplication kernels from LLM.int8() , which is equivalent to SwitchBackQ (Algorithm 4) if the weight gradient multiplication was also performed in int8 using row- and column-wise quantization, and iii) SwitchBack. SwitchBack has a negligible accuracy drop of 0.1 percentage points compared to the bfloat16 baseline for CLIP ViT-Huge. In contrast, there is a drop of 5.9 percentage points when training with LLM.int8(). Section D details our hypothesis for why LLM.int8() fails to replicate 16-bit performance for CLIP training.

For our simulated float8 training experiments (Figure 1, right), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) a baseline which uses tensor-wise quantization for all matrices, that is the weights, inputs, and gradients, and iii) SwitchBack. SwitchBack has a negligible accuracy drop of 0.1 percentage points from the bfloat16 baseline for CLIP ViT-Huge. In contrast, training diverges for the baseline that uses tensor-wise quantization for all matrices.

**Speed.** By writing custom triton kernels  we achieve end-to-end speedups from 13-25% for CLIP ViT-Huge training. 25% compared to a standard linear layer implemented using the PyTorch  autograd python module and 13% compared to the standard PyTorch layer which include optimizations that are difficult to replicate for custom layers. Moreover, we find that the overhead due to quantization operations decreases with scale and is \(\)10% for CLIP ViT-Huge. A detailed analysis of our speed-ups is in Appendix B.

### Float8 training by reducing feature magnitude

We find that SwitchBack is necessary for high accuracy int8 training. However, this section develops other interventions which enable float8 training without SwitchBack. We show that high accuracy can be achieved via float8 training with tensor-wise quantization for the inputs, weights, and gradients, solong as the network is initialized and trained in a way which discourages large feature magnitudes. We accomplish via layer-scale  initialized to zero.

We use the bitsandbytes library  to simulate float8 training using the fp8 types from Micikevicius et al. . We use tensor-wise quantization for the inputs, weights, and gradients, so that all operations occur in simulated float8. In our simulation, we represent each value only with the exact values representable by float8, but we perform computations in float16 precision. We believe that tensor-wise quantization approximates the removal of quantize operations entirely. This is because, as we show in Appendix C.2 (Figure 11), the maximum of these tensors tends to evolve smoothly. Consequently, using a moving average for a maximum which is divided directly in the matmul is similar to tensor-wise quantization.

Layer-scale, introduced by Touvron et al. , scales each self-attention and MLP block output hidden state by a learnable vector of shape embed_dim. A pre-norm transformer block with layer-scale tensors \(_{1}\) and \(_{2}\) is defined as

\[x_{k}^{}=x_{k}+_{1}*(_{1}(x _{k})), x_{k+1}=x_{k}^{}+_{2}*(_{2 }(x_{k}^{})),\] (3)

where \(*\) is broadcasted elementwise multiplication.

Typically, layers are initialized so that they approximately preserve the variance of their inputs, and inputs have approximately unit variance [26; 27]. However, when combined with residual connections this can lead to higher norms in deeper networks.

Consequently, researchers have proposed initialization and scaling schemes which remedy this issue [1; 71; 4; 19]. Layer-scale with initialization 0 is an example of one such scheme--at initialization the transformer is an identity function. While \(_{1},_{2}\) are typically initialized as vectors of \(10^{-4}\) or \(10^{-6}\), we use 0 for simplicity.

Figure 2 (right) demonstrates that the layer-scale intervention is successful at controlling the average magnitude output. Without the intervention, the average feature magnitude \([(x_{k})]\) becomes high for later blocks. Previous work  has shown that large feature magnitudes result in issues for low precision training.

Results for simulated fp8 training are shown in Figure 2 (left) for ViT-Large. We find that all fp8 runs diverge except for when we use layer-scale initialized to zero. Concretely, Figure 2 compares i) the baseline which uses bfloat16 training, ii) using fp8 with tensor-wise quantization and no further modifications, which slowly diverges, iii) adding gradient clipping to ii), which also diverges, iv) adding KQ layernorm  to ii), which also diverges, and v) using _zero-init layerscale_, which trains without diverging. While there is a difference still between fp8 and bfloat16 training, this is primarily because of layerscale. Moreover, we believe that with hyperparameter tuning layerscale would match standard training in terms of accuracy.

Figure 2: **(Left)** Training CLIP ViT-Large models with simulated fp8 precision using tensor-wise quantization for the inputs, weights, and gradients. All methods we try diverge except for using _zero-init layerscale_, which multiplies the output of each self-attention or mlp block with a learnable vector initialized to zero. **(Right)** Examining feature magnitudes (i.e., the average absolute value of the output for transformer block \(k\)) for CLIP ViT-Huge at the beginning (init) and end of training. This suggest why zero-init layer scale enables float8 training—zero-init layer scale prevents high feature magnitudes which may cause issues for low precision training . Without the intervention, the average feature magnitude becomes large for later blocks.

## 3 Stability

We now switch focus from accelerating learning by reducing precision to addressing instabilities which can arise during training. Section 3.1 reviews preliminaries and related work while Section 3.2 details the experimental setup. Next, Section 3.3 examines trends for training instability, finding loss spikes to increase with model scale but decrease with lower AdamW \(_{2}\). Then, Section 3.4 finds that loss spikes arise in our setting due to an out-of-date AdamW second moment estimator leading Section 3.5 to adopt and tests a fix developed in the context of AdaFactor . Appendix Section G connects this section on stability with the previous section on low-precision training.

### Preliminaries and related work

Loss spikes can emerge when scaling up models [8; 25; 14; 68; 70; 54; 73]. These instabilities may slow learning, or even destabilize training completely. Various solutions have been proposed, including freezing the embedding layer , adding additional layer normalization [14; 25], or reparametrizing the weights .

In our work we investigate instabilities which arise during CLIP training. Unlike the instabilities observed in [14; 68] which lead to a slow divergence, we study fast loss spikes. Our results indicate that these spikes arise when the second moment estimator is out of date for early layers.

While our analysis and methods build directly on Shazeer and Stern  (AdaFactor), there are important differences. In contrast with Shazeer and Stern , who only observe instabilities without warmup, we observe instabilities despite a long warmup period. Moreover, in contrast with Shazeer and Stern  we find that an out-of-date second moment estimator is primarily an issue for the (patch) embedding layer, and measure how well loss spikes are predicted by this event. Finally, we note that researchers have moved away from AdaFactor in its original formulation for large-scale training [47; 11; 69], finding AdaFactor to under-perform AdamW . We believe this is due to the factored second moment or absence of first moment. This is why our focus is AdamW  which is the de facto standard optimizer for transformers.

After the initial version of this paper we became aware of Cohen et al.  which offers a general and principled treatment of fast loss spikes, and which we recommend to readers. Moreover, we direct the readers attention to the concurrent work of .

### Experimental setup

As in Section 2, we train ViT CLIP models on LAION  using OpenCLIP  and evaluate them zero-shot on ImageNet. Since we are not interested in final performance and instead interested in studying instability--even for very large models--we use a short run which allows us to conduct multiple experiments. Concretly, we use patch-dropout 0.5  and 20k iterations. The first 5k iterations are linear warmup while the remainder are cosine decay . We follow the CLIP paper  in that i) we do not use gradient clipping unless otherwise mentioned, though we do clip the logit_scale parameter, and ii) we add a layer-norm after the patch embedding and before the main transformer. Unless otherwise mentioned, experiments use batch size 16384 (per-gpu batch size of 256), learning rate 2e-3 and weight decay 0.2. We initially tried adding a layer-norm before the patch embedding as in , but removed this as we found it to hurt performance at CLIP ViT-Huge scale.

Figure 3: Loss spikes increase with **model size** for fixed learning rate and batch size. Reducing AdamW \(_{2}\) from its default in PyTorch of 0.999 mitigates loss spikes. Reducing \(_{2}\) too much slows training.

### Loss spikes increase with model size, batch size, and learning rate

We begin our studying of loss spikes by observing how their presence varies when changing model size, batch size, and learning rate. The following sections build on these observations--in particular the finding that lowering the AdamW \(_{2}\) hyperparameter removes spikes entirely.

We find that loss spikes increase when increasing model size, batch size, or learning rate. The first result is shown in Figure 3 while the second two analagous results are in Appendix Figures 12 and 13. Importantly, these figures show that loss spikes can be avoided by reducing the \(_{2}\) hyperparameter for in AdamW. On the other hand, if \(_{2}\) is reduced too much then learning is slowed which results in worse performance .

### On \(_{2}\) and an out-of-date second moment estimator

Based on the observation in the previous section that lowering \(_{2}\) reduces spikes, this section traces the cause of loss spikes to an out-of-date second moment estimator in the patch embedding layer.

**Overview.** Adaptive optimizers such as AdaGrad , Adam , or AdaFactor  scale the update differently for each individual parameter. This is often conceptualized a per-parameter learning rate. For instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).

This adaptivity can be a very useful tool for accelerating training, but can also cause issues when the learning signal changes. Concretely, exponential moving averages can become out of date causing updates to be scaled by a value that is too large. This issue is discussed in Section 5 of Shazeer and Stern , and we summarize below.

As in Algorithm 2, let \(u_{t}=\{u_{t,j}\}_{j=1}^{n}\) denote the exponential moving average (EMA) of squared gradients \(g_{t}^{2}=\{g_{t,j}^{2}\}_{j=1}^{n}\) for neural network parameters \(^{n}\). Ignoring the bias correction term1, at each iteration \(t\), \(u_{t}\) is updated as \(_{2}u_{t-1}+(1-_{2})g_{t}^{2}\) where \(_{2}\) is referred to as the _decay_ for the EMA. Then, the update is scaled by \(1/(}+)\), where \(\) is a small value added numerical stability. Often the ratio \(v_{t}/(}+)\) is thought of as signal-to-noise ratio of the gradient over time.

However, this method can break down when the learning signal changes and \(u_{t}\) ceases to be a good estimator for the running average of \(g_{t}^{2}\). Consider the case where the gradient magnitudes have been historically very small for some parameters so \(1/(}+)\) is large for those parameters. If, then, at iteration \(t\) those parameters suddenly receive a larger gradient signal the update can be catastrophically big. We refer to the scenario as the **stuck-in-the-past** scenario.

Overall, if \(_{2}\) is too small then convergence may be slowed . If \(_{2}\) is too large then \(u_{t}\) can become out-of-date and no longer a good estimator for \(g_{t}^{2}\), resulting in per-parameter scaling that is too large.

Figure 4: The learning signal can change so that the AdamW second moment estimator \(u_{t}\) is out-of-date and underestimates the squared gradients \(g_{t}^{2}\). This can be detected if the aggregate quantity \(_{t}=[g_{t}^{2}/u_{t}]}\) is far from 1. This figure observes a predictive relationship between the event of an RMS spike and a loss spike— we observe a spike in \(_{t}\) 1-8 iterations before a loss spike. For lower \(_{2}\), \(_{t}\) does not deviate far from 1. This result looks at \(_{t}\) for the patch embedding layer only. This predictive relationship is further examined in Figures 15 to 20 of Appendix E.

**Measurement.** We now discuss measurement of the aforementioned **stuck-in-the-past** scenario and search for a predictive relationship between this event and a loss spike. We follow Shazeer and Stern  and measure the following root-mean-square quantity, \(_{t}=[g_{t}^{2}/u_{t}]}\). If \(u_{t}\) is a good estimator for \(g_{t}^{2}\) then the aggregate quantity \(_{t}\) will be around 1. The **stuck-in-the-past** scenario described above corresponds to an \(_{t} 1\).

As illustrated in Figure 3 we observe instability for high \(_{2}\) in our experiments even though we have 5k iterations of warm-up. While Shazeer and Stern  first recognize the out-of-date second moment estimator issue, in their experimental setting they only observe instability without warm-up.

We now aim to establish a predictive relationship between the **stuck-in-the-past** scenario and loss spikes. We present initial results in Figure 4, where we examine \(_{t}\) for the the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), \(_{t}\) stays around 1 for lower \(_{2}\).

As this is just one example, we further elaborate on the predictive relationship between an RMS spike in the embedding layer in Section E through Figures 15, 16, 17, 18, 19, and 20. For analysis purposes, we define a heuristic to characterize loss and RMS spikes in visual.conv1.weight. We then show that 28 out of 30 detected loss spikes follow an RMS spike by 1-8 iterations, while the probability that a loss spike follows an RMS spike by chance is only 1%. Moreover, we find that the same predictive relationship does not exist for the RMS in other transformer layers.

### StableAdamW: AdamW with update clipping from AdaFactor

This Section develops and tests StableAdamW (Algorithm 2), an AdamW-Adafactor hybrid.

To stabilize training, the AdaFactor optimizer divides the learning rate for iteration \(t\) by \(1/(_{t},1)\).2 They refer to this as _update clipping_. The effect is to slow training when \(u_{t}\) is no longer a good estimator for \(g_{t}^{2}\).

As discussed in Section 3.4, our stability issues can be traced to an out-of-date \(u_{t}\) which is what led Shazeer and Stern  to update clipping, even though their stability issues are also solved with warm-up. Therefore, we port update clipping to the standard AdamW optimizer with \(d=1\) and refer to the resulting AdamW-Adafactor hybrid as StableAdamW (Algorithm 2). A modification we make is to compute and divide learning rate by \((_{t},1)\) independently for each tensor, which is for implementation convenience. This means that the expectation will be computed independently for each layer to produce a different \(_{t}\).

We now test how StableAdamW compares with other stability interventions such as gradient clipping3 or lowering \(_{2}\). These results, presented in Figure 5 find that StableAdamW (i.e., AdamW + update clipping) outperforms these aforementioned interventions for CLIP ViT-Huge. While gradient clipping and update clipping both remove instability, update clipping performs better in terms of zero-shot ImageNet accuracy. With update or gradient clipping, higher \(_{2}\) such as 0.99 tends to

Figure 5: Adding update clipping to AdamW mitigates loss spikes and outperforms other interventions such as gradient clipping with norm 1. Code for the AdamW-AdaFactor hybrid we recommend of AdamW + update clipping is in Algorithm 2. The left plot shows loss curves for \(_{2}=0.99\) while the right displays accuracy ablating over \(_{2}\).

perform better. Appendix F provides further commentary and implementation considerations for StableAdamW.

## 4 Limitations, broader impacts, and conclusion

We believe the main limitation of our work is that it is non-exhaustive. For instance, we only simulate float8 training and our experiments focus solely on CLIP-style training. In terms of broader impact, our work may enable additional CLIP models, whose broader impact is examined extensively by Section 7 of Radford et al. . Finally, we believe that our findings on accelerating and stabilizing large multi-modal model training will be broadly useful to the community.

#### Acknowledgements

For insightful discussions we thank Romain Beaumont, Yair Carmon, Mehdi Cherti, Brian Cheung, Alex Fang, Gabriel Ilharco, Jenia Jitsev, LAION, Sarah Pratt, Christoph Schuhmann, Ross Whightman, and Sho Yaida. We thank Emad Mostaque and stability.ai for compute resources.

This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.