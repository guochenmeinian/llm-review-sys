# DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries

Joshua Engels

ThirdAI

josh.adam.engels@gmail.com &Benjamin Coleman

ThirdAI

benjamin.ray.coleman@gmail.com &Vihan Lakshman

ThirdAI

vihan@thirdai.com &Anshumali Shrivastava

ThirdAI, Rice University

anshu@thirdai.com, anshumali@rice.edu

###### Abstract

We study the problem of _vector set search_ with _vector set queries_. This task is analogous to traditional near-neighbor search, with the exception that both the query and each element in the collection are _sets_ of vectors. We identify this problem as a core subroutine for semantic search applications and find that existing solutions are unacceptably slow. Towards this end, we present a new approximate search algorithm, DESSERT (**D**ESSERT **E**ffeciently **S**earches **S**ets of **E**mbeddings via **R**etrieval **T**ables). DESSERT is a general tool with strong theoretical guarantees and excellent empirical performance. When we integrate DESSERT into ColBERT, a state-of-the-art semantic search model, we find a 2-5x speedup on the MS MARCO and LoTTE retrieval benchmarks with minimal loss in recall, underscoring the effectiveness and practical applicability of our proposal.

## 1 Introduction

Similarity search is a fundamental driver of performance for many high-profile machine learning applications. Examples include web search , product recommendation , image search , de-duplication of web indexes  and friend recommendation for social media networks . In this paper, we study a variation on the traditional vector search problem where the dataset \(D\) consists of a collection of _vector sets_\(D=\{S_{1},...S_{N}\}\) and the query \(Q\) is also a vector set. We call this problem _vector set search_ with _vector set queries_ because both the collection elements and the query are sets of vectors. Unlike traditional vector search, this problem currently lacks a satisfactory solution.

Furthermore, efficiently solving the vector set search problem has immediate practical implications. Most notably, the popular ColBERT model, a state-of-the-art neural architecture for semantic search over documents , achieves breakthrough performance on retrieval tasks by representing each query and document as a set of BERT token embeddings. ColBERT's current implementation of vector set search over these document sets, while superior to brute force, is prohibitively slow for real-time inference applications like e-commerce that enforce strict search latencies under 20-30 milliseconds . Thus, a more efficient algorithm for searching over sets of vectors would have significant implications in making state-of-the-art semantic search methods feasible to deploy in large-scale production settings, particularly on cost-effective CPU hardware.

Given ColBERT's success in using vector sets to represent documents more accurately, and the prevailing focus on traditional single-vector near-neighbor search in the literature , we believe that the potential for searching over sets of representations remains largely untapped. An efficient algorithmic solution to this problem could enable new applications in domainswhere multi-vector representations are more suitable. To that end, we propose DESSERT, a novel randomized algorithm for efficient set vector search with vector set queries. We also provide a general theoretical framework for analyzing DESSERT and evaluate its performance on standard passage ranking benchmarks, achieving a 2-5x speedup over an optimized ColBERT implementation on several passage retrieval tasks.

### Problem Statement

More formally, we consider the following problem statement.

**Definition 1.1**.: Given a collection of \(N\) vector sets \(D=\{S_{1},...S_{N}\}\), a query set \(Q\), a failure probability \( 0\), and a set-to-set relevance score function \(F(Q,S)\), the _Vector Set Search Problem_ is the task of returning \(S^{}\) with probability at least \(1-\):

\[S^{}=*{argmax}_{i\{1,...N\}}F(Q,S_{i})\]

Here, each set \(S_{i}=\{x_{1},...x_{m_{i}}\}\) contains \(m_{i}\) vectors with each \(x_{j}^{d}\), and similarly \(Q=\{q_{1},...q_{m_{q}}\}\) contains \(m_{q}\) vectors with each \(q_{j}^{d}\).

We further restrict our consideration to structured forms of \(F(Q,S)\), where the relevance score consists of two "set aggregation" or "variadic" functions. The inner aggregation \(\) operates on the pairwise similarities between a single vector from the query set and each vector from the target set. Because there are \(|S|\) elements in \(S\) over which to perform the aggregation, \(\) takes \(|S|\) arguments. The outer aggregation \(A\) operates over the \(|Q|\) scores obtained by applying \(A\) to each query vector \(q Q\). Thus, we have that

\[F(Q,S) =A(\{Inner_{q,S}:q Q\})\] \[Inner_{q,S} =(\{(q,x):x S\})\]

Here, \(\) is a vector similarity function. Because the inner aggregation is often a maximum or other non-linearity, we use \(()\) to denote it, and similarly since the outer aggregation is often a linear function we denote it with \(A()\). These structured forms for \(F=A\) are a good measure of set similarity when they are monotonically non-decreasing with respect to the similarity between any pair of vectors from \(Q\) and \(S\).

### Why is near-neighbor search insufficient?

It may at first seem that we could solve the Vector Set Search Problem by placing all of the individual vectors into a near-neighbor index, along with metadata indicating the set to which they belonged. One could then then identify high-scoring sets by finding near neighbors to each \(q Q\) and returning their corresponding sets.

There are two problems with this approach. The first problem is that a single high-similarity interaction between \(q Q\) and \(x S\) does not imply that \(F(Q,S)\) will be large. For a concrete example, suppose that we are dealing with sets of word embeddings and that \(Q\) is a phrase where one of the items is "keyword." With a standard near-neighbor index, \(Q\) will match (with 100% similarity) any set \(S\) that also contains "keyword," regardless of whether the other words in \(S\) bear any relevance to the other words in \(Q\). The second problem is that the search must be conducted over all individual vectors, leading to a search problem that is potentially very large. For example, if our sets are documents consisting of roughly a thousand words and we wish to search over a million documents, we now have to solve a billion-scale similarity search problem.

Contributions:In this work, we formulate and carefully study the set of vector search problem with the goal of developing a more scalable algorithm capable of tackling large-scale semantic retrieval problems involving sets of embeddings. Specifically, our research contributions can be summarized as follows:

1. We develop the first non-trivial algorithm, DESSERT, for the vector set search problem that scales to large collections (\(n>10^{6}\)) of sets with \(m>3\) items.
2. We formalize the vector set search problem in a rigorous theoretical framework, and we provide strong guarantees for a common (and difficult) instantiation of the problem.

3. We provide an open-source C++ implementation of our proposed algorithm that has been deployed in a real-world production setting1. Our implementation scales to hundreds of millions of vectors and is 3-5x faster than existing approximate set of vector search techniques. We also describe the implementation details and tricks we discovered to achieve these speedups and provide empirical latency and recall results on passage retrieval tasks. 
## 2 Related Work

Near-Neighbor Search:Near-neighbor search has received heightened interest in recent years with the advent of vector-based representation learning. In particular, considerable research has gone into developing more efficient _approximate_ near-neighbor (ANN) search methods that trade off an exact solution for sublinear query times. A number of ANN algorithms have been proposed, including those based on locality-sensitive hashing [1; 41], quantization and space partition methods [19; 12; 14], and graph-based methods [28; 18]. Among these classes of techniques, our proposed DESSERT framework aligns most closely with the locality-sensitive hashing paradigm. However, nearly all of the well-known and effective ANN methods focus on searching over individual vectors; our work studies the search problem for sets of entities. This modification changes the nature of the problem considerably, particularly with regards to the choice of similarity metrics between entities.

Vector Set Search:The general problem of vector set search has been relatively understudied in the literature. A recent work on database lineage tracking  addresses this precise problem, but with severe limitations. The proposed approximate algorithm designs a concatenation scheme for the vectors in a given set, and then performs approximate search over these concatenated vectors. The biggest drawback to this method is scalability, as the size of the concatenated vectors scales quadratically with the size of the vector set. This leads to increased query latency as well as substantial memory overhead; in fact, we are unable to apply the method to the datasets in this paper without terabytes of RAM. In this work, we demonstrate that DESSERT can scale to thousands of items per set with a linear increase (and a slight logarithmic overhead) in query time, which, to our knowledge, has not been previously demonstrated in the literature.

Document Retrieval:In the problem of document retrieval, we receive queries and must return the relevant documents from a preindexed corpus. Early document retrieval methods treated each documents as bags of words and had at their core an inverted index . More recent methods embed each document into a single representative vector, embed the query into the same space, and performed ANN search on those vectors. These semantic methods achieve far greater accuracies than their lexical predecessors, but require similarity search instead of inverted index lookups [15; 33; 26].

ColBERT and PLAID:ColBERT  is a recent state of the art algorithm for document retrieval that takes a subtly different approach. Instead of generating a single vector per document, ColBERT generates a _set_ of vectors for each document, approximately one vector per word. To rank a query, ColBERT also embeds the query into a set of vectors, filters the indexed sets, and then performs a brute force _sum of max similarities_ operation between the query set and each of the document sets. ColBERT's passage ranking system is an instantiation of our framework, where \((q,x)\) is the cosine similarity between vectors, \(\) is the max operation, and \(A\) is the sum operation.

In a similar spirit to our work, PLAID  is a recently optimized form of ColBERT that includes more efficient filtering techniques and faster quantization based set similarity kernels. However, we note that these techniques are heuristics that do not come with theoretical guarantees and do not immediately generalize to other notions of vector similarity, which is a key property of the theoretical framework behind DESSERT.

## 3 Algorithm

At a high level, a DESSERT index \(\) compresses the collection of target sets into a form that makes set to set similarity operations efficient to calculate. This is done by replacing each set \(S_{i}\) with a sketch \([i]\) that contains the LSH values of each \(x_{j} S_{i}\). At query time, we compare the corresponding LSH values of the query set \(Q\) with the hashes in each \([i]\) to approximate the pairwise similarity matrix between \(Q\) and \(S\) (Figure 1). This matrix is used as the input for the aggregation functions \(A\) and \(\) to rank the target sets and return an estimate of \(S^{*}\).

We assume the existence of a locality-sensitive hashing (LSH) family \((^{d})\) such that for all LSH functions \(h\), \(p(h(x)=h(y))=(x,y)\). LSH functions with this property exist for cosine similarity (signed random projections) , Euclidean similarity (\(p\)-stable projections) , and Jaccard similarity (minhash or simhash) . LSH is a well-developed theoretical framework with a wide variety of results and extensions [4; 3; 20; 40]. See Appendix C for a deeper overview.

Algorithm 1 describes how to construct a DESSERT index \(\). We first take \(L\) LSH functions \(f_{t}\) for \(t[1,L]\), \(f_{t}\). We next loop over each \(S_{i}\) to construct \([i]\). For a given \(S_{i}\), we arbitrarily assign an identifier \(j\) to each vector \(x S_{i}\), \(j[1,|S_{i}|]\). We next partition the set \([1,m_{i}]\) using each hash function \(h_{t}\), such that for a partition \(p_{t}\), indices \(j_{1}\) and \(j_{2}\) are in the same set in the partition iff \(h(S_{j_{1}})=h(S_{j_{2}})\). We represent the results of these partitions in a universal hash table indexed by hash function id and hash function value, such that \([i]_{t,h}=\{j\,|\,x_{j} S_{i} f_{t}(x_{j})=h\}\).

Algorithm 2 describes how to query a DESSERT index \(\). At a high level, we query each sketch \(_{i}\) to get an estimate of \(F(Q,S_{i})\), \(score_{i}\), and then take the argmax over the _estimates_ to get an estimate of \(_{i\{1,,N\}}F(Q,S_{i})\). To get these estimates, we first compute the hashes \(h_{t,q}\) for each query \(q\) and LSH function \(f_{t}\). Then, to get an estimate \(score_{i}\) for a set \(S_{i}\), we loop over the hashes \(h_{t,q}\) for each query vector \(q\) and count how often each index \(j\) appears in \([i]_{t,h_{t,q}}\). After we finish this step, we have a count for each \(j\) that represents how many times \(h_{t}(q)=h_{t}(x_{j})\). Equivalently, since \(p(h(x)=h(y))=(x,y)\), if we divide by \(L\) we have an estimate for \((x_{j},q)\). We then apply \(\) to these estimates and save the result in a variable \(_{q}\) to build up the inputs to \(A\), and then apply \(A\) to get our final estimate for \(F(Q,S_{i})\), which we store in \(score_{i}\).

## 4 Theory

In this section, we analyze DESSERT's query runtime and provide probabilistic bounds on the correctness of its search results. We begin by finding the hyperparameter values and conditions that are necessary for DESSERT to return the top-ranked set with high probability. Then, we use these results to prove bounds on the query time. In the interest of space, we defer proofs to the Appendix.

Notation:For the sake of simplicity of presentation, we suppose that all target sets have the same number of elements \(m\), i.e. \(|S_{i}|=m\). If this is not the case, one may replace \(m_{i}\) with \(m_{}\) in our analysis. We will use the boldface vector \((q,S_{i})^{|S_{i}|}\) to refer to the set of pairwise similarity calculations \(\{(q,x_{1}),,(q,x_{m_{i}})\}\) between a query vector and the elements of \(S_{i}\), and we will drop the subscript \((q,S_{i})\) when the context is clear. See Table 1 for a complete notation reference.

Figure 1: The DESSERT indexing and querying algorithms. During indexing (left), we represent each target set as a set of hash values (\(L\) hashes for each element). To query the index (right), we approximate the similarity between each target and query element by averaging the number of hash collisions. These similarities are used to approximate the set relevance score for each target set.

### Inner Aggregation

We begin by introducing a condition on the \(\) component of the relevance score that allows us to prove useful statements about the retrieval process.

**Definition 4.1**.: A function \(():^{m}\) is \((,)\)-maximal on \(U^{m}\) if for \(0< 1\), \( x U\):

\[()\]

The function \(()=\) is a trivial example of an \((,)\)-maximal function on \(^{m}\), with \(==1\). However, we can show that other functions also satisfy this definition:

**Lemma 4.1.1**.: _If \((x):\) is \((,)\)-maximal on an interval \(I\), then the following function \((x):^{m}\) is \((,)\)-maximal on \(U=I^{m}\):_

\[()=_{i=1}^{m}(x_{i})\]

   Notation & Definition & Intuition (Document Search) \\  \(D\) & Set of target vector sets & Collection of documents \\ \(N\) & Cardinality \(|D|\) & Number of documents \\ \(\) & DESERT index of \(D\) & Search index data structure \\ \(S_{i}\) & Target vector set \(i\) & \(i\)th document \\ \(Q\) & Query vector set & Multi-word query (e.g., a question) \\ \(S^{*}\) & See Definition 1.1 & The most relevant document to \(Q\) \\ \(x_{j} S_{i}\) & \(j\)th vector in target set \(S_{i}\) & Embedding from document \(i\) \\ \(q_{j} Q\) & \(j\)th vector in query set \(Q\) & Embedding from a query \\ \(d\) & \(s_{j},x_{j}^{d}\) & Embedding dimension \\ \(m_{i}\), \(m\) & Cardinality \(|S_{i}|\), \(m_{i}=m\) & Number of embeddings in \(i\)th document \\ \(F(Q,S_{i})\) & \(Q\) and \(S_{i}\) relevance score & Measures query-document similarity \\ \(score_{i}\) & Estimate of \(F(Q,S_{i})\) & Approximation of relevance score \\ \([i]\) & Sketch of \(i\)th target set & Estimates relevance score for \(S_{i}\) and any \(Q\) \\ \((a,b)\) & \(a\) and \(b\) vector similarity & Embedding similarity \\ \(A\), \(\) & See Section 1.1 & Components of relevance score \\ \(L\) & Number of hashes & Larger \(L\) increases accuracy and latency \\ \(f_{i}\) & \(i\)th LSH function & Often maps nearby points to the same value \\ \((q,S_{i}),\) & \((q,x_{j})\) for \(x_{j} S_{i}\) & Query embedding similarities with \(S_{i}\) \\   

Table 1: Notation table with examples from the document search application.

Note that in \(\), the \((,)\)-maximal condition is equivalent to lower and upper bounds by linear functions \( x\) and \( x\) respectively, so many natural functions satisfy Lemma 4.1.1. We are particularly interested in the case \(I=\), and note that possible such \(\) include \((x)=x\) with \(==1\), the exponential function \((x)=e^{x}-1\) with \(=1\), \(=e-1\), and the debiased sigmoid function \((x)=}-\) with \( 0.23\), \(=0.25\). Our analysis of DESSERT holds when the \(\) component of the relevance score is an \((,)\) maximal function.

In line \(12\) of Algorithm 2, we estimate \(()\) by applying \(\) to a vector of normalized counts \(}\). In Lemma 4.1.2, we bound the probability that a low-similarity set (one for which \(()\) is low) scores well enough to outrank a high-similarity set. In Lemma 4.1.3, we bound the probability that a high-similarity set scores poorly enough to be outranked by other sets. Note that the failure rate in both lemmas decays exponentially with the number of hash tables \(L\).

**Lemma 4.1.2**.: _Assume \(\) is \((,)\)-maximal. Let \(0<s_{}<1\) be the maximum similarity between a query vector and the vectors in the target set and let \(}\) be the set of estimated similarity scores. Given a threshold \( s_{}<<\), we write \(=- s_{}\), and we have_

\[[(}) s_{}+] m^{L}\]

_for \(=((-)}{(1-s_{})})^{}()}{-})(s_{},1)\). Furthermore, this expression for \(\) is increasing in \(s_{}\) and decreasing in \(\), and \(\) has one sided limits \(_{ s_{}}=1\) and \(_{}=s_{}\)._

**Lemma 4.1.3**.: _With the same assumptions as Lemma 4.1.2 and given \(>0\), we have:_

\[Pr[(}) s_{}-] 2e^{-2L^{2}/ ^{2}}\]

### Outer Aggregation

Our goal in this section is to use the bounds established previously to prove that our algorithm correctly ranks sets according to \(F(Q,S)\). To do this, we must find conditions under which the algorithm successfully identifies \(S^{}\) based on the approximate \(F(Q,S)\) scores.

Recall that \(F(Q,S)\) consists of two aggregations: the inner aggregation \(\) (analyzed in Section 4.1) and the outer aggregation \(A\). We consider normalized _linear_ functions for \(A\), where we are given a set of weights \(0 w 1\) and we rank the target set according to a weighted linear combination of \(\) scores.

\[F(Q,S)=_{j=1}^{m}w_{j}(}(q_{j},S))\]

With this instantiation of the vector set search problem, we will proceed in Theorem 4.2 to identify a choice of the number of hash tables \(L\) that allows us to provide a probabilistic guarantee that the algorithm's query operation succeeds. We will then use this parameter selection to bound the runtime of the query operation in Theorem 4.3.

**Theorem 4.2**.: _Let \(S^{}\) be the set with the maximum \(F(Q,S)\) and let \(S_{i}\) be any other set. Let \(B^{}\) and \(B_{i}\) be the following sums (which are lower and upper bounds for \(F(Q,S^{})\) and \(F(Q,S_{i})\), respectively)_

\[B^{}=}_{j=1}^{m_{q}}w_{j}s_{}(q_{j},S^{})  B_{i}=}_{j=1}^{m_{q}}w_{j}s_{}(q_{j},S_{i})\]

_Here, \(s_{}(q,S)\) is the maximum similarity between a query vector \(q\) and any element of the target set \(S\). Let \(B^{}\) be the maximum value of \(B_{i}\) over any set \(S_{i} S\). Let \(\) be the following value (proportional to the difference between the lower and upper bounds)_

\[=(B^{}-B^{})/3\]

_If \(>0\), a DESSERT structure with the following value2 of \(L\) solves the search problem from Definition 1.1 with probability \(1-\)._

\[L=O((m}{}))\]

### Runtime Analysis

**Theorem 4.3**.: _Suppose that each hash function call runs in time \(O(d)\) and that \(|[i]_{t,h}|<T\  i,t,h\) for some positive threshold \(T\), which we treat as a data-dependent constant in our analysis. Then, using the assumptions and value of \(L\) from Theorem 4.2, Algorithm 2 solves the Vector Set Search Problem in query time_

\[O(m_{q}(Nm_{q}m/)d+m_{q}N(Nm_{q}m/))\]

This bound is an improvement over a brute force search of \(O(m_{q}mNd)\) when \(m\) or \(d\) is large. The above theorem relies upon the choice of \(L\) that we derived in Theorem 4.2.

## 5 Implementation Details

Filtering:We find that for large \(N\) it is useful to have an initial lossy filtering step that can cheaply reduce the total number of sets we consider with a low false-negative rate. We use an inverted index on the documents for this filtering step.

To build the inverted index, we first perform \(k\)-means clustering on a representative sample of individual item vectors at the start of indexing. The inverted index we will build is a map from centroid ids to document ids. As we add each set \(S_{i}\) to \(\) in Algorithm 1, we also add it into the inverted index: we find the closest centroid to each vector \(x S_{i}\), and then we add the document id \(i\) to all of the buckets in the inverted index corresponding to those centroids.

This method is similar to PLAID, the recent optimized ColBERT implementation , but our query process is much simpler. During querying, we query the inverted index buckets corresponding to the closest _filter_probe_ centroids to each query vector. We aggregate the buckets to get a count for each document id, and then only rank the _filter_k_ documents with DESSERT that have the highest count.

Space Optimized Sketches:DESSERT has two features that constrain the underlying hash table implementation: (1) every document is represented by a hash table, so the tables must be low memory, and (2) each query performs many table lookups, so the lookup operation must be fast. If (1) is not met, then we cannot fit the index into memory. If (2) is not met, then the similarity approximation for the inner aggregation step will be far too slow. Initially, we tried a naive implementation of the table, backed by a std::vector, std::map, or std::unordered_map. In each case, the resulting structure did not meet our criteria, so we developed TinyTable, a compact hash table that optimizes memory usage while preserving fast access times. TinyTables sacrifice \(O(1)\) update-access (which DESSERT does not require) for a considerable improvement to (1) and (2).

A TinyTable replaces the universal hash table in Algorithm 1, so it must provide a way to map pairs of (hash value, hash table id) to lists of vector ids. At a high level, a TinyTable is composed of \(L\) inverted indices from LSH values to vector ids. Bucket \(b\) of table \(i\) consists of vectors \(x_{j}\) such that \(h_{i}(x_{j})=b\). During a query, we simply need to go to the \(L\) buckets that correspond to the query vector's \(L\) lsh values to find the ids of \(S_{i}\)'s colliding vectors. This design solves (1), the fast lookup requirement, because we can immediately go to the relevant bucket once we have a query's hash value. However, there is a large overhead in storing a resizable vector in every bucket. Even an empty bucket will use \(3*8=24\) bytes. This adds up: let \(r\) be the hash range of the LSH functions (the number of buckets in the inverted index for each of the \(L\) tables). If \(N=1M\), \(L=64\), and \(r=128\), we will use \(N L r 24\ =196\) gigabytes even when _all of the buckets are empty_.

Thus, a TinyTable has more optimizations that make it space efficient. Each of the \(L\) hash table repetitions in a TinyTable are conceptually split into two parts: a list of offsets and a list of vector ids. The vector ids are the concatenated contents of the buckets of the table with no space in between (thus, they are always some permutation of \(0\) through \(m\) - 1). The offset list describes where one bucket ends and the next begins: the \(i\)th entry in the offset list is the (inclusive) index of the start of the \(i\)th hash bucket within the vector id list, and the \(i+1\)th entry is the (exclusive) end of the ith hash bucket (if a bucket is empty, \(indices[i]=indices[i+1]\)). To save more bytes, we can further concatenate the \(L\) offset lists together and the \(L\) vector id lists together, since their lengths are always \(r\) and \(m_{i}\) respectively. Finally, we note that if \(m 256\), we can store all of the the offsets and ids can be safely be stored as single byte integers. Using the same hypothetical numbers as before, a _filled_ TinyTable with \(m=100\) will take up just \(N(24+L(m+r+1))\ =14.7\)GB.

The Concatenation Trick:In our theory, we assumed LSH functions such that \(p(h(x)=h(y))=(x,y)\). However, for practical problems such functions lead to overfull buckets; for example, GLOVE has an average vector cosine similarity of around \(0.3\), which would mean each bucket in the LSH table would contain a third of the set. The standard trick to get around this problem is to _concatenate_\(C\) hashes for each of the \(L\) tables together such that \(p((x)=h(y))=(x,y)^{C}\). Rewriting, we have that

\[(x,y)=()\] (1)

During a query, we count the number of collisions across the \(L\) tables and divide by \(L\) to get \((h(x)=h(y))\) on line \(11\) of Algorithm 2. We now additionally pass \(count/L\) into Equation 1 to get an accurate similarity estimate to pass into \(\) on line \(12\). Furthermore, evaluating Equation 1 for every collision probability estimate is slow in practice. There are only \(L+1\) possible values for the \(count/L\), so we precompute the mapping in a lookup table.

## 6 Experiments

**Datasets:** We tested DESSERT on both synthetic data and real-world problems. We first examined a series of synthetic datasets to measure DESSERT's speedup over a reasonable CPU brute force algorithm (using the PyTorch library  for matrix multiplications). For this experiment, we leave out the prefiltering optimization described in Section 5 to better show how DESSERT performs on its own. Following the authors of , our synthetic dataset consists of random groups of Glove  vectors; we vary the set size \(m\) and keep the total number of sets \(N=1000\).

We next experimented with the MS MARCO passage ranking dataset (Creative Commons License) , \(N 8.8M\). The task for MS MARCO is to retrieve passages from the corpus relevant to a query. We used ColBERT to map the words from each passage and query to sets of embedding vectors suitable for DESSERT . Following , we use the development set for our experiments, which contains \(6980\) queries.

Finally, we computed the full resource-accuracy tradeoff for ten of the LoTTE out-of-domain benchmark datasets, introduced by ColBERTv2 . We excluded the pooled dataset, which is simply the individual datasets merged.

**Experiment Setup:** We ran our experiments on an Intel(R) Xeon(R) CPU E5-2680 v3 machine with 252 GB of RAM. We restricted all experiments to 4 cores (8 threads). We ran each experiment with the chosen hyperparameters and reported overall average recall and average query latency. For all experiments we used the average of max similarities scoring function.

### Synthetic Data

The goal of our synthetic data experiment was to examine DESSERT's speedup over brute force vector set scoring. Thus, we generated synthetic data where both DESSERT and the brute force implementation achieved perfect recall so we could compare the two methods solely on query time.

The two optimized brute force implementations we tried both used PyTorch, and differed only in whether they computed the score between the query set and each document set individually ("Individual") or between the query set and all document sets at once using PyTorch's highly performant reduce and reshape operations ("Combined").

In each synthetic experiment, we inserted \(1000\) documents of size \(m\) for \(m[2,4,8,16,...,1024]\) into DESSERT and the

Figure 2: Query time for DESSERT vs. brute force on \(1000\) random sets of \(m\) glove vectors with the \(y\)-axis as a log scale. Lower is better.

brute force index. The queries in each experiment were simply the documents with added noise. The DESSERT hyperparameters we chose were \(L=8\) and \(C=_{2}(m)+1\). The results of our experiment, which show the relative speedup of using DESSERT at different values of \(m\), are in Figure 2. We observe that DESSERT achieves a consistent 10-50x speedup over the optimized Pytorch brute force method and that the speedup increases with larger \(m\) (we could not run experiments with even larger \(m\) because the PyTorch runs did not finish within the time allotted).

### Passage Retrieval

Passage retrieval refers to the task of identifying and returning the most relevant passages from a large corpus of documents in response to a search query. In these experiments, we compared DESSERT to PLAID, ColBERT's heavily-optimized state-of-the-art late interaction search algorithm, on the MS MARCO and LoTTE passage retrieval tasks.

We found that the best ColBERT hyperparameters were the same as reported in the PLAID paper, and we successfully replicated their results. Although PLAID offers a way to trade off time for accuracy, this tradeoff only increases accuracy at the sake of time, and even then only by a fraction of a percent. Thus, our results represent points on the recall vs time Pareto frontier that PLAID cannot reach.

MS MARCO ResultsFor MS MARCO, we performed a grid search over DESSERT parameters \(C=\{4,5,6,7\}\), \(L=\{16,32,64\}\), \(filter\_probe=\{1,2,4,8\}\), and \(filter\_k=\{1000,2048,4096,8192,16384\}\). We reran the best configurations to obtain the results in Table 2. We report two types of results: methods tuned to return \(k=10\) results and methods tuned to return \(k=100\) results. For each, we report DESSERT results from a low latency and a high latency part of the Pareto frontier. For \(k=1000\) we use the standard \(R@1000\) metric, the average recall of the top \(1\) passage in the first \(1000\) returned passages. This metric is meaningful because retrieval pipelines frequently rerank candidates after an initial retrieval stage. For \(k=10\) we use the standard \(MRR@10\) metric, the average mean reciprocal rank of the top \(1\) passage in the first \(10\) returned passages. Overall, DESSERT is 2-5x faster than PLAID with only a few percent loss in recall.

LoTTE ResultsFor LoTTE, we performed a grid search over \(C=\{4,6,8\}\), \(L=\{32,64,128\}\), \(filter\_probe=\{1,2,4\}\), and \(filter\_k=\{1000,2048,4096,8192\}\). In Figure 3, we plot the full Pareto tradeoff for DESSERT on the \(10\) LoTTE datasets (each of the \(5\) categories has a "forum" and "search" split) over these hyperparameters, as well as the single lowest-latency point achievable by PLAID. For all test datasets, DESSERT provides a Pareto frontier that allows a tradeoff between recall and latency. For both Lifestyle test splits, both Technology test splits, and the Recreation and Science test-search splits, DESSERT achieves a 2-5x speedup with minimal loss in accuracy. On Technology, DESSERT even exceeds the accuracy of PLAID at half of PLAID's latency.

## 7 Discussion

We observe a substantial speedup when we integrate DESSERT into ColBERT, even when compared against the highly-optimized PLAID implementation. While the use of our algorithm incurs a slight recall penalty - as is the case with most algorithms that use randomization to achieve acceleration - Table 2 and Figure 3 shows that we are Pareto-optimal when compared with baseline approaches.

We are not aware of any algorithm other than DESSERT that is capable of latencies in this range for set-to-set similarity search. While systems such as PLAID are tunable, we were unable to get them to operate in this range. For this reason, DESSERT is likely the only set-to-set similarity search algorithm that can be run in real-time production environments with strict latency constraints.

 Method & Latency (ms) & \(MRR@10\) \\  DESSERT & 9.5 & 35.7 \(\) 1.14 \\ DESSERT & 15.5 & 37.2 \(\) 1.14 \\ PLAID & 45.1 & 39.2 \(\) 1.15 \\   
 Method & Latency (ms) & \(R@1000\) \\  DESSERT & 22.7 & 95.1 \(\) 0.49 \\ DESSERT & 32.3 & 96.0 \(\) 0.45 \\ PLAID & 100 & 97.5 \(\) 0.36 \\  

Table 2: MS MARCO passage retrieval, with methods optimized for k=10 (left) and k=1000 (right). Intervals denote 95% confidence intervals for average latency and recall.

We also ran a single-vector search baseline using ScaNN, the leading approximate kNN index . ScaNN yielded 0.77 Recall@1000, substantially below the state of the art. This result reinforces our discussion in Section 1.2 on why single-vector search is insufficient.

Broader Impacts and Limitations:Ranking and retrieval are important steps in language modeling applications, some of which have recently come under increased scrutiny. However, our algorithm is unlikely to have negative broader effects, as it mainly enables faster, more cost-effective search over larger vector collections and does not contribute to the problematic capabilities of the aforementioned language models. Due to computational limitations, we conduct our experiments on a relatively small set of benchmarks; a larger-scale evaluation would strengthen our argument. Finally, we assume sufficiently high relevance scores and large gaps in our theoretical analysis to identify the correct results. These hardness assumptions are standard for LSH.

## 8 Conclusion

In this paper, we consider the problem of vector set search with vector set queries, a task understudied in the existing literature. We present a formal definition of the problem and provide a motivating application in semantic search, where a more efficient algorithm would provide considerable immediate impact in accelerating late interaction search methods. To address the large latencies inherent in existing vector search methods, we propose a novel randomized algorithm called DESSERT that achieves significant speedups over baseline techniques. We also analyze DESSERT theoretically and, under natural assumptions, prove rigorous guarantees on the algorithm's failure probability and runtime. Finally, we provide an open-source and highly performant C++ implementation of our proposed DESSERT algorithm that achieves 2-5x speedup over ColBERT-PLAID on the MS MARCO and LoTTE retrieval benchmarks. We also note that a general-purpose algorithmic framework for vector set search with vector set queries could have impact in a number of other applications, such as image similarity search , market basket analysis , and graph neural networks , where it might be more natural to model entities via sets of vectors as opposed to restricting representations to a single embedding. We believe that DESSERT could provide a viable algorithmic engine for enabling such applications and we hope to study these potential use cases in the future.

Figure 3: Full Pareto frontier of DESSERT on the LoTTE datasets. The PLAID baseline shows the lowest-latency result attainable by PLAID (with a FAISS-IVF base index and centroid pre-filtering).

Acknowledgments

This work was completed while the authors were working at ThirdAI. We do not have any external funding sources to acknowledge.