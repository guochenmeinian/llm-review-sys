# Decomposing Novel into Known: Part Concept Learning For 3D Novel Class Discovery

Tingyu Weng Jun Xiao Haiyong Jiang

School of AI

University of Chinese Academy of Sciences

wengtingyu18@mails.ucas.ac.cn,{xiaojun,haiyong.jiang}@ucas.ac.cn

 Haiyong is the corresponding author.

###### Abstract

In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined _DNIK_, to mitigate the above problems. _DNIK_ learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (\(+11.7\%\), \(+14.1\%\), and \(+16.3\%\) improvements on average for three tasks, respectively). Code and data will be released.

## 1 Introduction

3D recognition is fundamental for 3D tasks and plays a critical role in many applications such as robotics and autonomous vehicles. Recently, deep learning-based 3D recognition  has achieved remarkable progress in this task. Unfortunately, most methods are limited to annotated classes in the training set (i.e., known classes), and cannot recognize novel classes like human beings, which limits the robustness of 3D recognition in the real world. Therefore, it is crucial and valuable to investigate the _3D novel class discovery task (NCD)_ so that 3D recognition can discover novel classes by leveraging the knowledge of known classes.

Though 2D NCD has been extensively investigated, a straightforward extension of 2D NCD methods  to 3D NCD usually performs poorly due to the _feature bias problem_, _i.e._, the learned features are heavily biased towards known classes and have difficulty in generalizing to novel classes. The main factors lie in two aspects. First, learned feature representations are usually dominated by known classes while being oblivious to novel classes. Second, 3D recognition usually focuses on shortcut features  only capture a special global structure or local regions specific to known classes  as shown in Fig. 1.

In this work, we tackle the above problems based on the observation that humans can subconsciously decompose shapes into familiar part concepts, which along with their spatial relations provide effective cues for recognizing different novel shapes . Therefore part concepts can bridge the gaps between known shapes and those novel ones with a shared part space, thus alleviating thefeature bias problem in 2D NCD extensions. On the other hand, the compositions and spatial relations between parts make it easier to recognize novel shapes. For example, as shown in Fig. 1(b), a novel class, e.g., a chair, contains familiar concepts like plane, leg, and backrest observed in known classes (e.g., tables and sofas), therefore we can easily recognize the chair according to its part concepts and part spatial relations.

Along with this idea, we present a novel part concept-based framework coined _DNIK_ (**d**ecompose **n**ovel **i**nto **k**nown parts) for 3D NCD as shown in Fig. 2. Specifically, we first construct a learnable part concept bank that encodes rich geometric patterns from known classes. Then _DNIK_ can project translation-invariant part-level features of an input shape into the spanning space of part concepts so that shapes from different categories can be represented as part-concept composition features sharing the common embedding space. Therefore learned features by known class recognition can be generalized to novel classes. We further leverage part-wise spatial relations of input shapes with a part relation encoding module (PRE) for discriminative part relation features. These two kinds of features are then combined for 3D shape recognition. To avoid the collapse of the part concept bank and encourage diverse part concepts across classes, three constraints on part concepts are also considered. Moreover, we construct three 3D NCD tasks for the evaluation of different scenarios and also extend our method to the generalized class discovery task (GCD) .

Our contributions can be summarized as follows: **(i)** the analysis of biased features and the necessity of part concepts in 3D NCD; **(ii)** a part concept-based framework leveraging both part concept features and part-wise relations for 3D NCD; **(iii)** the design of three necessary constraints on part concepts to facilitate effective part concept learning; **(iv)** various 3D NCD and GCD tasks for the evaluation of different scenarios. Extensive experiments show that our method significantly outperforms baseline methods on all 3D NCD and GCD tasks and can efficiently recognize novel shapes.

## 2 Related Work

**Novel Class Discovery.** As there is no existing work on 3D NCD, we mainly review related works on 2D NCD. Related works can be classified into two categories. The first category of methods [9; 14; 15] usually first train a classification network on the known dataset and then cluster the novel data according to the prediction or output features of the pre-trained network. However, learned features in this way are highly biased toward known classes, leading to performance bottlenecks in novel classes. By contrast, the other line of works [7; 10] explores both known and novel samples for NCD during the clustering phase. Therefore feature representation can be shared between known and novel classes and allow for better generalization to novel classes. AutoNovel  utilizes both known and novel samples for self-supervised representation learning and joint known class classification and NCD. Following works further bridge the gap between known and novel classes by mixing up known and novel samples , and introducing consistency between similar objects  and predictions from two different branches . On the other hand, UNO  unifies the learning of both known and novel classes using a single cross-entropy loss based on optimal transports between pseudo-label assignments. Following this paradigm, the similarity relation of samples between known and novel classes is further explored by a spacing loss in the embedding space , compositional experts , and symmetric KL divergence . Recently NCD has been extended to the more practical general

Figure 1: (a) The Grad-CAM  of a known shape (table). 3D recognition methods focus on shortcut features representing a special global structure or local region. (b) Novel shape can be represented as a part concept composition along with part-wise spatial relations.

category discovery task (GCD) , which assumes that unlabeled datasets contain both known and novel classes.  and  tackle this problem by using semi-supervised K-means and purely parameterized models respectively. In this work, we address 3D NCD and GCD by introducing part concepts and part-wise relations as the bridge between known classes and novel ones.

**Part-based 3D Recognition.** Shape parts are the basic building blocks for 3D shapes [22; 4] and play an important role in both human perception and various 3D recognition tasks. For example, Zhao et al.  represent repeatable shape parts as prototypes to improve few-shot 3D object detection. Chowdhury et al.  learn part information as a robust prior for incremental learning. Wei et al.  align part-based features to reduce the geometry shifts across different domains. Zhao et al.  propose a part codebook-based self-attention to leverage the generalized geometric information for better voxel-based feature extractors. Inspired by these works, we learn a part concept bank to map features of known and novel classes into a shared space.

## 3 Method

This work aims at 3D NCD that learns to group 3D shapes in the unlabeled set into several classes by leveraging the knowledge of known classes in a labeled dataset. Basically, 3D NCD assumes the availability of two datasets: an unlabeled dataset \(D^{u}=\{X_{i}^{u}\}_{i=1}^{N^{u}}\) with 3D shapes sampled from novel classes \(C^{u}\) and a labeled dataset \(D^{l}=\{(X_{i}^{l},y_{i}^{l})\}_{i=1}^{N^{l}}\), where \(X_{i}^{l}\) denotes a 3D shape and \(y_{i}^{l} C^{l}\) is the corresponding categories label. The classes in the two datasets are disjoint, therefore we have \(C^{l} C^{u}=\). However, shapes from \(D^{l}\) and \(D^{u}\) usually endow with some similarities. For example, both a chair and a desk have legs and plane surfaces. Following the literature [7; 10; 48], we assume the number of novel classes \(C^{u}\) is known a prior.

### Analysis on the Extension of 2D NCD Methods

As there is no existing work on 3D NCD, we instead extend a mainstream 2D NCD method UNO  to the 3D domain as the baseline. UNO learns a shared feature extractor \(f\) and two linear classifiers \(^{l}\) and \(^{u}\) with \(C^{l}\) and \(C^{u}\) output neurons. The classifiers \(^{l}\) and \(^{u}\) are optimized with a cross-entropy loss using labels of known classes and pseudo labels of novel classes generated by a clustering method (_i.e._, Sinkhorn-Knopp ). However, naive extensions of 2D NCD to the 3D domain work badly for two reasons. First, feature representation is usually dominated by known classes, while oblivious to novel classes. Second, shape-level features extracted from 3D networks confront serious

Figure 2: **The overall architecture**. A point cloud and its augmentation are fed into a backbone network to encode point-wise features \(Z\) and grouped into part set \(Q\) respectively. As illustrated in the pink box, we propose a **local geometric aggregation (LGA)** module to learn position-invariant part feature \(Z_{l}\) and calculate the concept activation map \(S\) use the **Poincaré distance** between \(Z_{l}\) and learnable **part concept bank**\(\). Then we can construct part composite features \(Z_{p}\) according to the Eq. 2. We also propose a **position relation encoder (PRE)** module to extract the part position feature \(Z_{r}\) as illustrated in the orange box. Finally, \(Z_{p}\) and \(Z_{r}\) are concatenated and fed into an MLP for recognition.

shortcuts as illustrated in Fig. 1. Thus learned features are heavily biased toward the known classes and are hard to serve novel classes.

We conduct a toy experiment to verify our analysis. We chose \(table,sofa,stool\) as known classes and \(chair,bench,bathtub,plant\) as novel classes from ModelNet . The first two novel classes have similar shape parts with the known classes, e.g., legs, while the last two do not. To reveal the wane and wax between known and novel classes during optimization, we investigated performance curves of UNO  in Fig. 3 Left. In the first 60 epochs, training 3D recognition with known class supervision can help novel class discovery. Therefore there exist some sharing feature representations between two different sets of classes and UNO can achieve better accuracy on novel classes than unsupervised clustering (_i.e._, w/o the cross-entropy loss). However, with the training proceeding, the performance in novel classes starts to decrease and can even be worse than unsupervised clustering, indicating that learned features are gradually biased to known classes and lose generalizability .

Fig. 3 Right shows the average cosine similarity between the shape-level or part-level concepts of known classes and the shape or part features of novel classes learned from the trained UNO method . Shape features are direct outputs of the feature extractor while part features denote pooling features of overlapped parts split from shapes. We then define the shape-level concepts by averaging the shape features of each known class and part-level concepts by clustering the part features of known classes using K-means. For shape-level concepts, the cosine similarities of chairs and benches are low and comparable to those of bathtubs and plants, which indicates that shape features cannot be shared between known and novel classes. However, part-level concepts can provide higher cosine similarities and reflect similar properties between known and novel classes better.

### Part Concepts for Cross-category Generalization

In this work, we approach 3D NCD with a learnable part concept bank shared across different classes. The overall framework is shown in Fig 2. The basic observation is that geometric patterns and shape parts, _i.e._, part concepts, are more likely to occur in different categories of shapes as analyzed in Sec. 3.1. We first learn part concepts from shapes of known classes in order to deliver more generalizable feature representation and reduce biases when facing novel classes. Then we leverage the relative positions of different parts to further enhance the recognization of novel classes.

**Learning Part-level Features.** As we intend to learn a set of part concepts representing generalizable shape parts, we first encode part-level features inspired by the paradigm of PointMAE . Specifically, we extract point-wise features \(Z^{L D}\) for each point in the input point cloud \(X^{L 3}\) with an off-the-shelf 3D backbone, e.g., PointNet , where \(L\) denotes the number of 3D points and \(D\) is the dimension of point cloud features. Then we use farthest point sampling (FPS) to sample \(N^{q}\) points as the centroid seeds and group \(K\) neighboring points as overlapped parts \(Q=\{Q_{i}\}_{i=1}^{N^{q}}\). Part set \(Q_{i}=\{z_{i1},,z_{iK}\}^{K D}\) includes features at its centroid \(z_{i1}\) followed by point features of its \(K-1\) nearest neighbors \(\{z_{i2},,z_{iK}\}\). Intuitively, we can obtain part-level features by pooling point features within each part \(Q_{i}\), but part features obtained in this way are dependent on the part location and may hinder the learning of shared part concepts at different positions. To cope with this problem, we propose a local geometric aggregation (LGA) module following [42; 26] to decouple the spatial location and aggregate part-level features \(Z_{l}^{N^{q} D}\) by

\[Z_{l_{i}}=((-z_{i1}}{})), i\{1, ,N^{q}\},\] (1)

Figure 3: **Left:** The training curves of UNO  based methods. **Right:** Cosine similarities between features of novel classes and shape or part-level concepts.

where \(\) is the scalar standard deviation across channels, and \(\) denotes a multi-layer perception (MLP) with channels \((D D)\) to encode the local geometry features.

**Part Concept learning.** After obtaining part-level features \(Z_{l}\), we use them to learn shared part concepts among categories. First, we construct a learnable part concept bank denoted with \(=\{P_{m}\}_{m=1}^{M}\), where part concept \(P_{m}^{1 D}\) encodes features of different shape parts and can represent _a plane, corners, legs_ as illustrated on the top of Fig. 2. In our implementation, part concepts are initialized with a Gaussian distribution \(N(,)\). We project part-level features \(Z_{l}\) into the spanning space of part concepts \(P\) according to their respective similarities. Then part-level features \(Z_{l}\) are approximated by part composition features \(Z_{p}^{N_{q} D}\) as follows:

\[Z_{p_{i}}=_{j}S_{ij}P_{j},\] (2)

where \(S^{N^{q} M}\) measures the similarity between part features \(Z_{l}\) and the part concept bank \(P\). The concept activation map \(S\) is defined as:

\[S=h(d(Z_{l},P^{T})),Z_{l} W_{z} Z_{l},P W_{p} P,\] (3)

where \(W_{z},W_{p}^{D D_{p}}\) are learnable weight matrixes for feature dimension reduction, \(h()\) is a softmax function to produce normalized probabilities along each row, and \(d(,)\) is a distance function that compares part features and part concepts. To ensure that the geometric knowledge of each part feature can be learned by a particular part concept, distance \(d(,)\) should produce significant similarity differences between part features and different part concepts. A common choice of \(d(,)\) is to use dot product similarity  or cos similarity  defined in the Euclidean space. However, as shown in Fig. 4(a), we found that the Euclidean distance functions lead to uniform activations of part features on different part concepts. Different from the Euclidean space where the distance grows linearly, the distance between part features and concepts grows exponentially in the hyperbolic space [17; 30; 28], which can produce sharp concept activation maps (see Fig. 4(a)). Therefore, we project features into the hyperbolic space using Poincare sphere model following  and the distance function between \(Z_{l}\) and \(P\) in Poincare manifolds is given by:

\[d(Z_{l},P)=(-P\|_{2}}{(1-\|Z_{l}\|_{2})(1-\|P\|_{2}) }+1).\] (4)

Though learned part concepts can cover most shape parts in novel classes, there still exist some novel part concepts that are not contained in known classes. In this case, the concept activation map between a part and part concepts \(P\) is likely to be uniform and results in ambiguous composite features. In our method, we further concatenate the local part features \(Z_{l}\) with the part composite features \(Z_{p}\) to confront this problem.

**Part Relation Encoder.** As part composite features \(Z_{p}\) only consider position-independent compositions of part concepts and neglect mutual position relations between different parts that usually deliver shape structures as shown in Fig. 1(b). To this end, we propose a novel Position Relation Module that adopts a shallow Transformer Encoder  with two self-attention layers, denoted as TransEnc, to learn mutual relations between part concepts as relation feature \(Z_{r}\). Since the position information has been decoupled from \(Z_{l}\), we regain position-aware part features \(Z_{g}=\{Z_{g_{i}},...,Z_{g_{N_{q}}}\}^{N^{q} D}\) from \(Q\) using max-pooling and encode \(Z_{r}^{N_{q} D}\) as follows:

\[Z_{g_{i}}=(Q_{i}),\] (5) \[Z_{r}=(Z_{g}).\]

Afterward, we concatenate the position relation feature \(Z_{r}\) with the point composite features \(Z_{p}\) and feed the result into an MLP with \(2D D\) and a max-pooling layer to produce category-aware features \(Z_{c}\). We can classify the input shape \(X\) by feeding \(Z_{c}\) to a linear layer predicting the class-wise probabilities.

### Constraining Part Concepts

Though part concepts are proposed to represent different parts, they can easily collapse to the same values. The main reason is that the similarity between randomly initialized part concepts and part features is similar during the initial training stage. Consequently, part composite features based on the concept activation map would gradually be identical and part concepts updated with similarly back-propagated gradients from them converge to similar features. To ensure the representation ability and generalization of the part concept bank, we propose three constraints to train part concepts.

**Self-distillation Loss.** To guarantee that each part's geometric knowledge can be learned by specific part concepts, the concept activation distribution of each part should be sharp. To achieve this, we propose a self-distillation loss inspired by . We enforce each part concept activation \(S\) of the shape parts \(Q\) to be consistent with a sharper one of the corresponding part concept activation \(\) from the augmented shape parts \(\) (see Fig 4 (b)) as follows:

\[L_{sd}=-((p(S_{i},_{s}))_{i}+(p(_{i},_{s})) S_{i}),\] (6)

where \(p(S_{i},_{s})=/_{s})}{_{j=1}^{N^{q}}(s_{i}^{ }/_{s})}\) is a sharp concept activation distribution of \(Q_{i}\) controlled by temperature parameter \(_{s}\) (set 0.1 empirically) and \(()\) means the gradients is not back-propagated. Loss \(L_{sd}\) enhances the sharpness and consistency of the concept activation under different data augmentations for both the labeled and unlabeled datasets.

**Supervised Contrastive Loss.** During training, we found that even a few part concepts can support the recognition of known classes as shown in the red boxes in Fig. 4(c). In this case, the part concept bank lacks the incentive to learn more part concepts from known shapes, which reduces the information it can share with novel classes. Thus, for the labeled dataset \(D^{l}\), we employ a supervised contrastive loss \(L_{sc}\) to increase the inter-class discrepancy of their concept activation maps. As shown in the green boxes in Fig. 4(c), through \(L_{sc}\), the part concept bank can learn additional parts (_i.e._, leg and pedestal) from known shapes to increase their inter-class discrepancy. Given a mini-batch of \(B\) shapes \(\{X_{b}\}_{b=1}^{B}\) and their augmentation \(\{_{b}\}_{b=1}^{B}\), we first sum their activation maps \(S_{B}^{2B M N^{q}}\) into shape concept activation \(T_{B}^{2B M}\) along the part dimension, where \(T_{i}\) summarizes all part concepts in a shape. Then the supervised contrastive loss is defined on \(2B\) shapes as follows:

\[L_{sc}=-_{T_{j}^{+}}(T_{i},T_{j}^{+})/_{c})}{_{b=1}^{|B|}(-(T_{i},T_{b})/ _{c})},\] (7)

where \(=\{\{T_{j}^{+} T_{B}:y_{j}=y_{i}\}_{i}\}\) is the set that contains the shape concept activation \(_{i}\) of the augmented sample and other samples belonging to the same class in the mini-batch, \((.,.)\) calculates the cosine similarity, and temperature \(_{c}\) is set 0.1 empirically.

**Diversity Loss.** To capture different patterns in 3D shapes, part concepts should be diverse. We can encourage this by minimizing the similarity between different part concepts with the following loss:

\[L_{cd}=_{m=1}^{M}_{P^{-}\{P_{m}\}_{m=1}^{M}/P_{m}}(0,(P_{ m},P^{-})-),\] (8)

Figure 4: (a) The part concept activation is likely to be uniform in the Euclidean space but sharp in the Poincaré space. (b) Self-distillation loss \(L_{sd}\) encourages part activations to align with the sharper distributions of their corresponding data augmentation. (c) Supervised contrastive loss \(L_{sc}\) forces the part concept activations of different known classes to be separated from each other so that more part concepts can be learned.

where \(\) is the cosine similarity threshold between a code and the other ones. In our experiments, we set \(=0.1\).

### The Training Losses

Following the literature [48; 10; 7], we construct two linear classifiers \(^{l}\) and \(^{u}\). We employ a standard cross-entropy loss \(L_{ce}\) to optimize the known class classifier \(^{l}\) and a self-classification loss \(L_{self}\) for training novel class classifier \(^{u}\). Loss \(L_{self}\) minimizes the cross entropy between novel shapes and their augmentation with the assumption that the distribution of each novel class is uniform and is formulated following . In summary, the overall training objective loss function in our model is:

\[L=L_{ce}+L_{self}+_{1} L_{sd}+_{2} L_{sc}+_{3}  L_{cd},\] (9)

where loss \(L_{sd}\) in Eq.(6), \(L_{sc}\) in Eq.(7) and \(L_{cd}\) in Eq.(8) enforce basic assumptions of part concepts. Weight term \(_{s}\) balances different losses and is set to \((0.1,0.1,0.1)\). During inference, part composite features of a test sample \(Z_{c}\) are passed through both classifiers to obtain the corresponding logits, then the two logits are concatenated and fed into a softmax layer to obtain the class-wise probability.

## 4 Experiments

### Evaluation Datasets for 3D NCD

We construct three NCD tasks to evaluate the performance of 3D NCD based on four different kinds of 3D datasets covering CAD shapes (_i.e._, ModelNet40  and ShapeNetCore ), and indoor objects constructed using 3D scans and multi-view images (_i.e._, ScanObjectNN  and Co3D ).

**Random Split 3D NCD** focuses on evaluating the impact of a different number of known and novel classes on the 3D NCD following . The task is denoted with _DATASET-K-U_, where \(K\) is the number of known classes and the other \(U\) classes as novel ones.

**Similarity Split 3D NCD.** The semantic similarity between known and novel classes can greatly affect the performance of NCD. A highly semantically similar label set can provide more useful information for discovering novel classes. Based on the transfer leakage metric in , we split novel classes as three sets (i.e., _High, Medium, and Low_) according to the descending similarity w.r.t known classes.

**Cross Domain 3D NCD** assesses the performance of 3D NCD on cross-domain shape recognition and reflects the real-world scenarios where novel shapes may come from different domains. Therefore, we select known and novel classes with semantic similarity from different datasets respectively. The corresponding tasks are denoted with (known class dataset \(\) novel class dataset).

### Comparison with SOTA methods

To our best knowledge, there is no previous attempt at the 3D NCD problem. So we extend different kinds of SOTA 2D NCD methods to 3D NCD, including _DTC_, _AutoNovel_, _NCL_, _UNO_, and _IIC_. In addition, training a novel class classifier with pseudo labels generated by K-means can achieve good results and we also compare with it, denoting as _Kmeans+_. We implement the above baselines by substituting the image-based backbone with a 3D one (_i.e._, PointNet) for 3D NCD.

**Results on the Random Split 3D NCD Task** are shown in Tab. 1. For the known class recognition, all compared methods perform well, but the performance on the novel classes diverges a lot. When the number of known classes is large, the network can inherit more knowledge for the novel class recognition. The NCL and IIC can achieve better results on most datasets since they force the separation between known and novel classes while learning similarity relationships within the novel classes, so the learned features are more discriminative for the novel classes. Our method can gain consistent improvements on novel classes of all tasks (\(+11.7\%\) on average) while attaining the best know-class accuracy for most tasks (\(5/6\)). This demonstrates that part concepts can effectively bridge the gaps between known and novel shapes thus mitigating the feature bias. When the number of known classes is small, the performance of compared methods drops significantly. By contrast, our method decreases much less and still achieves the best results (\(+12.5\%\) increases on average). Weconjecture that the part concept bank can learn more local features for sharing than the other methods using global features.

**Results on the Similarity Split 3D NCD Task** are presented in Tab. 2. _NCL, UNO, IIC, and Kmeans+_ can achieve better results on novel classes with high similarities. However, the performance of these methods drops significantly (\(-13.5\%\) on average) with the decrease in semantic similarity. The method that achieves the best results on _High_-similarity novel classes cannot obtain the best on _Medium_ and _Low_ ones simultaneously. This indicates that the learned knowledge of these methods is biased towards known classes with highly similar geometry while lacking generalizability to other novel ones. By contrast, our method achieves large improvements for all similarities (\(+14.3\%\), \(+11.8\%\), \(+17.5\%\) on average) while reducing the gap between different similarity tasks. This is because part concepts can show up in different shapes, e.g., the back of a sofa and a chair in Fig. 1 and align all shapes into the part space, allowing for generalization even for low-similarity novel classes.

**Results on the Cross-Domain 3D NCD Task** are reported in Tab. 3. We can observe that compared methods lack robustness on cross-domain shapes and perform poorly. Our method performs well among different datasets and outperforms existing methods by a large margin (\(+16.3\%\) on average). We believe the good performance is owing to the fact that part concepts are robust to data domains and can be generalized to different kinds of data.

### Ablation Study

In this part, we conduct all ablations on the ModelNet similarity split 3D NCD. Readers can find more experiments in the supplementary.

**Model components.** In Tab. 4-2, adding the part concept bank without the LGA only achieves a very small improvement compared to the baseline, which indicates part concepts without positional invariance are hard to fit novel classes. In 3 and 4, LGA can ensure better generalization over novel classes, and the part concept bank can align the part features from known and novel classes into the same space. Therefore they can collaborate to boost the performance for a large margin (\(5.8\%\) and \(3.9\%\)). In 5, using the Poincare metric can yield much sharper concept activations and make different part concepts more discriminative, therefore raising the performance (\(+3.7\%\) and \(+8.5\%\)). In 6, self-distillation loss \(L_{sd}\) encourages each part concept to learn specific patterns to avoid collapses and contributes to an average \(6.25\%\) gain in accuracy. In 7 and 8, loss \(L_{sc}\) and \(L_{cd}\) force the network to

    & ModelNet 10-30 & Co3D 30-20 & Co3D 20-30 & ShapeNet 40-15 & ShapeNet 20-35 \\   & Novel & Known & Novel & Known & Novel & Known & Novel & Known & Novel & Known \\  DTC  & 49.7 & 79.6 & 31.1 & 92.5 & 31.1 & 77.0 & 21.9 & 84.6 & 40.8 & 80.8 & 31.0 & 89.9 \\ AutoNovel  & 46.6 & 88.1 & 46.7 & 93.8 & 52.2 & 80.6 & 42.0 & 86.1 & 36.1 & 82.2 & 35.2 & 91.4 \\ NCL  & 61.3 & **89.1** & 51.2 & 93.7 & 52.4 & 80.2 & 39.5 & 89.6 & 44.4 & 83.4 & 42.2 & 87.7 \\ UNO  & 59.3 & 86.7 & 45.4 & 93.2 & 48.6 & 80.1 & 41.6 & 87.3 & 45.5 & 82.2 & 28.6 & 90.7 \\ IIC  & 62.7 & 87.6 & 49.7 & 94.2 & 45.9 & 80.0 & 37.4 & 86.7 & 44.3 & 81.5 & 29.1 & 90.6 \\ Kmeans+ & 59.9 & 83.0 & 41.6 & 92.2 & 52.1 & 76.6 & 43.8 & 84.8 & 48.1 & 80.9 & 36.6 & 90.9 \\  Ours & **72.0** & 88.9 & **63.0** & **96.0** & **61.0** & **85.0** & **58.5** & **90.1** & **62.6** & **84.7** & **53.2** & **92.4** \\ Improvement & **+9.3** & **-0.2** & **+11.8** & **+1.3** & **+8.6** & **+4.4** & **+14.7** & **+0.5** & **+14.5** & **+1.3** & **+11.0** & **+1.0** \\   

Table 1: The quantitative results of the random split 3D NCD task. The results are averaged among five random trials. The best results are in **bold**, and the second-best ones are underlined.

    & Co3D & ShapeNet \\   & High & Medium & Low & Known & High & Medium & Low & Known & High & Medium & Low & Known \\  DTC  & 28.3 & 39.6 & 32.5 & **95.7** & 41.3 & 35.2 & 26.7 & 89.2 & 37.6 & 36.6 & 36.6 & 87.5 \\ AutoNovel  & 39.2 & 42.2 & 44.6 & 94.8 & 57.1 & 54.7 & 34.7 & 87.5 & 39.2 & 38.3 & 39.2 & 87.1 \\ NCL  & 56.6 & 56.5 & 45.1 & 95.2 & 57.4 & 53.1 & 39.7 & 89.8 & 45.3 & 36.9 & 38.7 & 87.1 \\ UNO  & 49.8 & 47.4 & 45.5 & 95.1 & 49.4 & 48.1 & 31.3 & 87.9 & 35.4 & 43.9 & 31.2 & 85.4 \\ IIC  & 59.6 & 56.0 & 38.9 & 93.9 & 55.8 & 48.8 & 43.1 & 84.5 & 47.1 & 38.4 & 28.7 & 85.6 \\ Kmeans+ & 58.5 & 54.2 & 39.6 & 95.0 & 54.3 & 49.5 & 37.3 & 84.6 & 44.9 & 45.9 & 33.8 & 86.1 \\  Ours & **73.2** & **68.3** & **66.4** & 95.2 & **69.3** & **65.2** & **56.9** & **93.3** & **64.7** & **54.6** & **57.1** & **89.2** \\ Improvement & **+13.6** & **+11.8** & **+20.9** & **0.5** & **+11.9** & **+10.5** & **+13.8** & **+3.5** & **+17.6** & **+8.7** & **+17.9** & **+17.7** \\   

Table 2: The accuracy results on the similarity split 3D NCD task. _High, Medium, Low_ denote the novel classes have _High, Medium, Low_ semantic similarity with known classes.

learn more diverse part concepts, which is more helpful for high-similarity novel classes (\(3.4\%\) vs \(1.2\%\)), since their concept activation maps are more similar (_e.g._, chair and stool). By comparing 8 and 8, we can see the three proposed constraints (\(L_{sd}\), \(L_{sc}\), and \(L_{cd}\)) are essential to the final good performance and strengthen the results by \(+8.8\%\) and \(+7.9\%\). In 8, the PRE can further incorporate discriminative position relation features to enhance the performance (\(+1.6\%\) on average).

**Hyperparameters.** In Fig. 5(a), we conduct experiments on different numbers of part concepts, _i.e._, \(M\). Considering part concepts should increase with known classes, we set \(M=N^{c} C^{l}\), When \(N^{c} 3\), the results are poor, which indicates that few part concepts cannot adequately encode all part patterns in known classes. However, the performance also drops when \(N^{c}>6\). This may be because too many concepts are harder to be optimized.

The number of points contained in each overlapped part determines the part size, therefore affecting the generalization and discrimination abilities. The discrimination ability of parts decreases when the size is reduced and the generalization ability improves. Specifically, small parts are hard to contain meaningful local geometric patterns, while large parts may contain special structures that are biased towards known classes as shown in Fig. 5(b). Our approach achieves the best results when \(K=64\).

Since the part set of a shape is split by FPS, few parts lead to large variations in the part sets of similar shapes, resulting in inconsistent part composite features. In Fig. 5(c), the result will decrease continuously when \(N^{q}<128\). Our method achieves the best performance when \(N^{q}=256\).

**Visualizations of the Feature Embedding** with t-SNE  are shown in Fig. 6. For the NCL  and IIC , the features of different novel classes are mixed up in the embedding space, while ours can push away embedding regions of different novel classes to a greater extent.

**Part Concept Visualization.** In order to reveal what part concepts can learn, we show concept activation maps between some part concepts and part features in Fig. 7. We can see that part concepts can activate similar semantic parts of both known and novel shapes, such as legs, arms, bottles, backrests, planes, and screens. This confirms that part concepts can effectively generalize knowledge learned from known classes to novel classes and represent novel classes as the composition of part concepts. For example, a chair can be represented as {legs, arms, a backrest, and a plane}. We notice that part concepts can be robustly discovered and activated even if geometric structures are slightly different, e.g., arms of chairs, sofas, and benches.

### Extension for Generalized Class Discovery

_DNIK_ can also be extended for generalized class discovery (GCD) , for which the unlabeled dataset \(D^{u}\) contains 3D shapes from both known and novel classes. In order to adapt _DNIK_ for this task, we concatenate the known class classifier \(^{l}\) and the novel class classifier \(^{u}\) into a unified

    & \(\)\(\)CaS\(\) & \(\)\(\)Scan. &  \\   & Novel & Novel & Novel & Novel & & & 48.8 & 37.9 \\  DTC  & 26.6 & 29.6 & 25.2 & & & & 52.6 & 44.8 \\ AutoNoset  & 26.8 & 36.7 & 41.6 & & & & 58.4 & 48.7 \\ NCL  & 35.2 & 39.0 & 37.4 & & & & 62.1 & 57.2 \\ UNO  & 27.7 & 43.9 & 35.0 & & & & 67.9 & 63.9 \\ IIC  & 30.8 & 39.8 & 35.4 & & & & 70.1 & 64.8 \\ Kmeans* & 29.0 & 41.8 & 26.0 & & & & 71.3 & 65.1 \\  Ours & **64.0 +28.8** & **53.4 +9.5** & **52.5 +10.5** & & & & & **73.2** & **66.4** \\   

Table 3: The results on cross-domain task. M. and Scan. are ModelNet and ScanObjectNN.

    & & & & & & & & & \\    } &   } &   } &   } &   } &   } &   } &   } \\  & & & & & & & & 48.8 & 37.9 \\  & & & & & & & & & 49.9 & 40.3 \\  & ✓ & ✓ & & & & & & & 52.6 & 44.8 \\  & ✓ & ✓ & & & & & & & 58.4 & 48.7 \\  & ✓ & ✓ & ✓ & & & & & & 62.1 & 57.2 \\  & ✓ & ✓ & ✓ & ✓ & ✓ & & & & 67.9 & 63.9 \\  & ✓ & ✓ & ✓ & ✓ & ✓ & & & & 70.1 & 64.8 \\  & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & 71.3 & 65.1 \\   

Table 4: Ablation experiments. PCB means part concept bank. LGA, Poincaré, and PRE are in Fig. 2.

classifier. All samples from both the labeled dataset and unlabeled dataset go through the unified classifier to obtain the class-wise probability. Both \(L_{ce}\) and \(L_{self}\) are now used to optimize the unified classifier rather than the novel class classifier of the NCD setting.

We extend two 2D GCD methods to 3D GCD, including _GCD_, _SimGCD_,and also extend AutoNovel  and UNO  for GCD tasks (denoted as AutoNovel+ and UNO+) by concatenating logits of known classes and novel classes following SimGCD . Results are shown in Tab. 5 and Tab. 6. It can be seen that all compared methods show decreased results compared to the NCD tasks in Tab. 1 and Tab. 2. This indicates that samples of known classes in the unlabeled dataset \(D^{u}\) introduce noise into the training and confuse the classifier. GCD  can achieve better novel class accuracy (+5.6%) than other compared methods because it uses semi-supervised Kmeans to cluster overall samples, which can produce more reliable pseudo labels. Our method can still achieve superior performance (+12.1%) on novel classes of all tasks while attaining comparable know-class accuracy for most tasks. This indicates that the idea of leveraging part compositions for novel class discovery can perform well in both the NCD setting and the GCD setting.

## 5 Conclusion

In this work, we present a novel part concept-based framework _DNIK_ for 3D NCD. DNIK leverages part concepts and part-wise relations that are widely shared by both known classes and novel ones. Therefore part concepts and part-wise relations learned from known classes can reinforce the recognition of novel shapes. Comprehensive experiments demonstrate the method overwhelms all baselines consistently and significantly on all metrics. We hope our work can inspire more research on find-grained concept learning and 3D NCD.

**Limitations.** Part concepts in the work are not capable of learning multi-scale geometric parts, thus restricting the representation ability and resulting in sensitive scale parameters in Fig. 5 (b).

    &  &  \\   &  &  \\   &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  \\  AutoNovel+  & 32.6 & 33.6 & 94.3 & 50.8 & 32.1 & **89.6** & UNO  & 41.7 & 36.5 & 88.5 & 52.1 & 44.9 & 88.3 \\ UNO+  & 41.7 & 36.5 & 88.5 & 52.1 & 44.9 & 88.3 & **50.0** & CD  & 58.5 & 54.7 & 95.7 & 56.2 & 46.0 & 89.2 \\ SANGD+  & 42.7 & 43.5 & 94.0 & 54.3 & 42.1 & 82.4 & 52.4 & 43.5 & 94.0 & 54.3 & 42.1 & 82.4 \\  Ours & **69.1** & 88.9 & **60.4** & **92.1** & **56.7** & **83.2** & **54.1** & **61.7** & UNO  & **70.6** & **62.8** & **58.9** & **62.5** & **58.9** & **58.0** \\ Improvement & **+18.5** & **-6.1** & **+19.2** & **+2.6** & **+10.3** & **+4.3** & **+4.3** & **+4.3** & **+4.3** & **+4.3** & **+4.3** & **+4.3** \\   

Table 5: The quantitative results of the random split 3D Table 6: The accuracy results on the similarity split 3D GCD task.

Figure 7: Visualization of different part concepts and their corresponding activations on different shapes of known and novel classes. Red indicates higher activations and similarities.