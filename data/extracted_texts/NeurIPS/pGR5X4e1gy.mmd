# Learning on Large Graphs

using Intersecting Communities

 Ben Finkelshtein

University of Oxford

&Ismail Ilkan Ceylan

University of Oxford

&Michael Bronstein

University of Oxford / ATTHYRA

&Ron Levie

Technion -- Israel Institute of Technology

###### Abstract

Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node's representation in an input graph by aggregating messages from the node's neighbors, which necessitates a memory complexity of the order of the _number of graph edges_. This complexity might quickly become prohibitive for large graphs provided they are not very sparse. In this paper, we propose a novel approach to alleviate this problem by approximating the input graph as an intersecting community graph (ICG) - a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph _does not depend on the graph size_. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the _number of nodes_ (rather than edges). This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.

## 1 Introduction

The vast majority of graph neural networks (GNNs) learn representations of graphs based on the message passing paradigm , where every node representation is synchronously updated based on an aggregate of messages flowing from its neighbors. This mode of operation hence assumes that the set of all graph edges are loaded in the memory, which leads to a memory complexity bottleneck. Specifically, considering a graph with \(N\) nodes and \(E\) edges, message-passing leads to a memory complexity that is _linear in the number of edges_, which quickly becomes prohibitive for large graphs especially when \(E N\). This limitation motivated a body of work with the ultimate goal of making the graph machine learning pipeline amenable to large graphs .

In this work, we take a drastically different approach to alleviate this problem and design a novel graph machine learning pipeline for learning over large graphs with memory complexity _linear in the number of nodes_. At the heart of our approach lies a graph approximation result which informally states the following: every undirected graph with node features can be represented by a linear combination of intersecting communities (i.e., cliques) such that the number of communities required to achieve a certain approximation error is _independent of the graph size_ for dense graphs. Intuitively, this results allows us to utilize an "approximating graph", which we call _intersecting community graph_ (ICG), for learning over large graphs. Based on this insight -- radically departing from the standard message-passing paradigm -- we propose a new class of neural networks acting on the ICG and on the set of nodes from the original input graph, which leads to an algorithm with memory and runtime complexity that is _linear in the number nodes_.

Our analysis breaks the traditional dichotomy of approaches that are appropriate either for small and dense graphs or for large and sparse graphs. To our knowledge, we present the first rigorously motivated approach allowing to efficiently handle graphs that are both large and dense. The main advantage lies in being able to process a very large non-sparse graph without excessive memory and runtime requirements. The computation of ICG requires linear time in the number of edges, but this is an offline pre-computation that needs to be performed _once_. After constructing the ICG, learning to solve the task on the graph -- the most computation demanding aspect -- is very efficient, since the architecture search and hyper-parameter optimization is linear in the number of nodes.

**Context.** The focus of our study is _graph-signals_: undirected graphs with node features, where the underlying graph is given and fixed. This is arguably the most common learning setup for large graphs, including tasks such as semi-supervised (transductive) node classification. As we allow varying features, our approach is also suitable for spatiotemporal tasks on graphs, where a single graph is given as a fixed domain on which a dynamic process manifests as time-varying node features. Overall, our approach paves the way for carrying out many important learning tasks on very large and relatively dense graphs such as social networks that typically have \(10^{8} 10^{9}\) nodes and average node degree of \(10^{2} 10^{3}\) and do not fit into GPU memory.

**Challenges and techniques.** There are two fundamental technical challenges to achieve the desired linear complexity, which we discuss and elaborate next.

_How to construct the ICG efficiently with formal approximation guarantees?_

Our answer to this question rests on an adaptation of the Weak Regularity Lemma  which proves the existence of an approximating ICG in _cut metric_ - a well-known graph similarity measure1. Differently from the Weak Regularity Lemma, however, we need to _construct_ an approximating ICG. Directly minimizing the cut metric is numerically unstable and hard to solve2. We address this in Theorem 3.1, by proving that optimizing the error in Frobenius norm, a much easier optimization target, guarantees a small cut metric error. Hence, we can approximate uniformly "granular" adjacency matrices by low-rank ICGs in cut metric. Uniformity here means that the number of communities \(K\) of the ICG required for the error tolerance \(\) is \(K=^{-2}\), independently of on any property of the graph, not even its size \(N\).3 This result allows us to design an efficient and numerically stable algorithm. Figure 1 shows an adjacency matrix with an inherent intersecting community structure, and our approximated ICG, which captures the statistics of edge densities, ignoring their granularity.

_How to effectively use_ ICG _for efficient graph learning?_ We present a signal processing framework and deep neural networks based on the ICG representation (ICG-NN). As opposed to message-passing networks that require \((E)\) memory and time complexity, ICG-NNs operate in only \((N)\) complexity. They involve node-wise operations to allow processing the small-scale granular behavior of the node features, and community operations that account for the large-scale structure of the graph. Notably, since communities have large supports, ICG-NN can propagate information between far-apart nodes in a single layer.

**Contributions.** Our contributions can be summarized as follows:

* We present a novel graph machine learning approach for large and non-sparse graphs which enables an \((N)\) learning algorithm on ICG with one-off \((E)\) pre-processing cost for constructing ICG.
* Technically, we prove a constructive version of Weak Regularity Lemma in Theorem 3.1 to efficiently obtain ICGs, which could be of independent theoretical interest.
* We introduce ICG neural networks as an instance of a signal processing framework, which applies neural networks on nodes and on ICG representations.
* Empirically, we validate our theoretical findings on semi-supervised node classification and on spatio-temporal graph tasks, illustrating the scalability of our approach, which is further supported by competitive performance against strong baseline models.

Figure 1: **Top**: adjacency matrix of a simple graph. **Bottom**: approximating 5 community ICG.

A primer on graph-signals and the cut-metric

**Graph signals.** We are interested in undirected (weighted or simple), featured graphs \(G=(,,)\), where \(\) denotes the set of nodes, \(\) denotes the set of edges, and \(\) denotes the node feature matrix, also called the signal. We will write \(N\) to represent the _number of nodes_ and \(E\) to represent the _number of edges_. For ease of notation, we will assume that the nodes are indexed by \([N]=\{1,,N\}\). Using the graph signal processing convention, we represent the graphs in terms of a _graph-signal_\(G=(,)\), where \(=(a_{i,j})_{i,j=1}^{N}^{N N}\) is the adjacency matrix, and \(=(s_{i,j})_{i[N],j[d]}^{N D}\) is the \(D\)-channel signal. We assume that the graph is undirected, meaning that \(\) is symmetric. The graph may be unweighted (with \(a_{i,j}\{0,1\}\)) or weighted (with \(a_{i,j}\)). We additionally denote by \(_{n}^{}\) the \(n\)-th row of \(\), and denote \(=(_{n})_{n=1}^{N}\). By abuse of notation, we also treat a signal \(\) as the function \(:[N]^{D}\) with \((n)=_{n}\). We suppose that all signals are standardized to have values in \(\). In general, any matrix (or vector) and its entries are denoted by the same letter, and vectors are seen as columns. The \(i\)-th row of the matrix \(\) is denoted by \(_{i,:}\), and the \(j\)-th column by \(_{:,j}\). The Frobenius norm of a square matrix \(^{N N}\) is defined by \(\|\|_{}=}_{n,m=1}^{N}| b_{n,m}|^{2}}\), and for a signal \(^{N D}\) by \(\|\|_{}=_{n=1}^{N}_{j=1}^ {D}|s_{n,j}|^{2}}\). The Frobenius norm of a matrix-signal, with weights \(a,b>0\), is defined to be \(\|(,)\|_{}:=\|_{ }^{2}+b\|\|_{}^{2}}\). We define the _degree_ of a weighted graph as \(():=N^{2}\|\|_{}^{2}\). In the special case of an unweighted graph we get \(()=E\). The pseudoinverse of a full rank matrix \(^{N K}\), where \(N K\), is \(^{}=(^{})^{-1}^{}\). **Cut-metric.** The _cut metric_ is a graph similarity, based on the _cut norm_. We show here the definitions for graphs of the same size; the extension for arbitrary pairs of graphs is based on graphons (see Appendix B.2). The definition of matrix cut norm is well-known, and we normalize it by a parameter \(E\) that indicates the characteristic number of edges of the graphs of interest, so cut norm remains within a standard range. The definition of cut norm of signals is taken from .

**Definition 2.1**.: _The matrix cut norm of \(^{N N}\), normalized by \(E\), is defined to be_

\[\|\|_{}=\|\|_{;N,E}:=_{,[N]}_{i}_{j }m_{i,j}.\] (1)

_The signal cut norm of \(^{N D}\) is defined to be_

\[\|\|_{}=\|\|_{;N}:=_{j=1}^{D}_{w[N]}_{i w}z_{i,j}.\] (2)

_The matrix-signal cut norm of \((,)\), with weights \(>0\) and \(>0\) s.t. \(+=1\), is defined to be_

\[\|(,)\|_{}=\|(,)\|_{ ;N,E}:=\|\|_{;N,E}+\| \|_{;N}.\] (3)

Given two graphs \(,^{}\), their distance in _cut metric_ is the _cut norm_ of their difference, namely

\[\|-^{}\|_{}=_{, [N]}_{i}_{j}(a_{ i,j}-a_{i,j}^{}).\] (4)

The right-hand-side of (4) is interpreted as the difference between the edge densities of \(\) and \(^{}\) on the block on which these densities are the most dissimilar. Hence, cut-metric has a probabilistic interpretation. Note that for simple graphs, \(-^{}\) is a granular function: it has many jumps between the values \(-1\), \(0\) and \(1\). The fact that the absolute value in (4) is outside the integral, as opposed to being inside like in \(_{1}\) norm, means that cut metric has an averaging effect, which mitigates the granularity of \(-^{}\). The _graph-signal cut metric_ is similarly defined to be \(\|(,)-(^{},^{})\|_{}\).

**Cut metric in graph machine learning.** The cut metric has the following interpretation: two (deterministic) graphs are close to each other in cut metric iff they look like random graphs sampled from the same stochastic block model. The interpretation has a precise formulation in view of the Weak Regularity Lemma  discussed below. This makes the cut metric a natural notion of similarity for graph machine learning, where real-life graphs are noisy, and can describe the same underlying phenomenon even if they have different sizes and topologies. Moreover,  showed that GNNs with normalized sum aggregation cannot separate graph-signals that are close to each other in cut metric. In fact, cut metric can separate any non-isomorphic graphons , so it has sufficient discriminative power to serve as a graph similarity measure in graph machine learning problems. As opposed to past works that used the cut norm to derive theory for existing GNNs, e.g., [46; 40; 33], we use cut norm to derive new methods for a class of problems of interest. Namely, we introduce a new theorem about cut norm - the constructive weak regularity lemma (Theorem 3.1) - and use it to build new algorithms on large non-sparse graphs.

## 3 Approximation of graphs by intersecting communities

### Intersecting community graphs

Given a graph \(G\) with \(N\) nodes, for any subset of nodes \([N]\), denote by \(_{}\) its indicator (i.e., \(_{}(i)=1\) if \(i\) and zero otherwise). We define an _intersecting community graph-signal (_ICG_) with \(K\) classes (\(K\)-ICG) as a low rank graph-signal \((,)\) with adjacency matrix and signals given respectively by

\[=_{j=1}^{K}r_{j}_{_{j}}_{_{j}}^{},=_{j=1}^{K}_{_{j}}_{j}^{}\] (5)

where \(r_{j}\), \(_{j}^{D}\), and \(_{j}[N]\). In order to allow efficient optimization algorithms, we relax the \(\{0,1\}\)-valued hard affiliation functions \(_{}\) to functions that can assign soft affiliation values in \(\). Letting \(\) denote the set of all indicator functions of the form \(_{}\), for subsets \([N]\), we can formally define soft affiliation models as follows.

**Definition 3.1**.: _A set \(\) of functions \(:[N]\) that contains \(\) is called a soft affiliation model._

**Definition 3.2**.: _Let \(d\), and let \(\) be a soft affiliation model. We define \([]^{N N}^{N D}\) to be the set of all elements of the form \((r^{},^{})\), with \(\), \(r\) and \(^{D}\). We call \([]\) the soft rank-1 intersecting community graph (ICG) model corresponding to \(\). Given \(K\), the subset \([]_{K}\) of \(^{N N}^{N D}\) of all linear combinations of \(K\) elements of \([]\) is called the soft rank-\(K\) ICG model corresponding to \(\)._

ICG models can be written in matrix form as follows. Given \(K,D\), any intersecting community graph-signal \((,)^{N N}^{N D}\) in \([]_{K}\) can be represented by a triplet of _community affiliation matrix_\(^{N K}\), _community magnitude vector_\(^{K}\), and _community feature matrix_\(}^{K D}\). In the matrix form, \((,)[]_{K}\) if and only if it has the form

\[=()^{} {P}=,\]

where \(()\) is the diagonal matrix in \(^{K K}\) with \(\) as its diagonal elements.

### The Semi-Constructive Graph-Signal Weak Regularity Lemma

The following theorem is a semi-constructive version of the Weak Regularity Lemma for intersecting communities. It is semi-constructive in the sense that the approximating graphon is not just assumed to exist, but is given as the result of an optimization problem with an "easy to work with" loss, namely, the Frobenius norm error. The theorem also extends the standard weak regularity lemma by allowing soft affiliations to communities instead of hard affiliations. For comparison to previously proposed constructive regularity lemmas see Section 7.

**Theorem 3.1**.: _Let \((,)\) be a \(D\)-channel graph-signal of \(N\) nodes, where \(()=E^{}\). Let \(K\), \(>0\), and \(\) be a soft affiliation model. Consider the matrix-signal cut norm with weights \(, 0\) not both zero, and the matrix-signal Frobenius norm with weights \(\|(,)\|_{}:=}{E}\| \|_{}^{2}+\|\|_{}^{2}}\). Let \(m\) be sampled uniformly from \([K]\), and let \(R 1\) such that \(K/R\). Then, in probability \(1-\) (with respect to the choice of \(m\)), for every \((^{*},^{*})[]_{m}\),_

\[\|(,)-(^{*},^{*})\|_{F}(1+ )_{(,)[]_{m}}\|(,)-(,)\|_{}\]

\[\|(,)-(^{*},^{*})\|_{ ;N,E^{}}}}+}.\] (6)The proof of Theorem 3.1 is given in Appendix B, where the theorem is extended to graphon-signals. Theorem 3.1 means that we need not consider a complicated algorithm for estimating cut distance. Instead, we can optimize the Frobenius norm, which is much more direct (see Section 4). The term \(\) describes the stability of the approximation, where a perturbation in \((^{*},^{*})\) that corresponds to a small relative change in the optimal Frobenius error, leads to a small additive error in cut metric.

### Approximation capabilities of ICGs

**ICGs vs. Message Passing.** Until the rest of the paper we suppose for simplicity that the degree of the graph \(E^{}\) is equal to the number of edges \(E\) up to a constant scaling factor \(0< 1\), namely \(E^{}= E\). Note that this is always true for unweighted graphs, where \(=1\). The bound in Theorem 3.1 is closer to be uniform with respect to \(N\) the denser the graph is (the closer \(E\) is to \(N^{2}\)). Denote the average node degree by \(=E/N\). To get a meaningful bound in (6), we must choose \(K>N^{2}/E\). On the other hand, for signal processing on the ICG to be more efficient than message passing, we require \(K<\). Combining these two bounds, we see that ICG signal processing is guaranteed to be more efficient than message passing for any graph in the _semi-dense regime_: \(^{2}>N\) (or equivalently \(E>N^{3/2}\)).

**Essential tightness of Theorem 3.1.** In (2, Theorem 1.2) it was shown that the bound (6) is essentially tight in the following sense4. There exists a universal constant \(c\) (greater than \(1/2312\)) such that for any \(N\) there exists an unweighted graph \(^{N N}\) with \(()=E\), such that any \(^{N N}\) that approximates \(\) with error \(\|-\|_{;N,E} 16\) must have rank greater than \(c}{E}\). In words, there are graphs of \(E\) edges that we cannot approximate in cut metric by any ICG with \(K}{E}\). Hence, the requirement \(K N^{2}/E^{}\) for a small cut metric error in (6) for _any_ graph is tight.

**ICGs approximations of sparse graphs.** In practice, many natural sparse graphs are amenable to low rank intersecting community approximations. For example, a simplistic model for how social networks are formed states that people connect according to shared characteristics (i.e. intersecting communities), like hobbies, occupation, age, etc . Moreover, since ICGs can have negative community magnitudes \(r_{k}\), one can also construct heterophilic components (bipartite substructures) of graphs with ICGs5. Hence, a social network can be naturally described using intersecting communities, even with \(K<N^{2}/E\). For such graphs, ICG approximations are more accurate than their theoretical bound (6). In addition, Figure 5 in Appendix F.5 shows that in practice the Frobenius local minimizer, which does not give a small Frobenius error, guarantees small cut metric error even if \(K<N^{2}/E\). This means that Frobenius minimization is a practical approach for fitting ICGs also for sparse natural graphs. Moreover, note that in the experiments in Tables 2 and 1 we take \(K<N^{2}/E\) and still get competitive performance with SOTA message passing methods.

## 4 Fitting ICGs to graphs

Let us fix the soft affiliation model to be all vectors \(^{N}\). In this subsection, we propose algorithms for fitting ICGs to graphs based on Theorem 3.1.

### Fitting an ICG to a graph with gradient descent

In view of Theorem 3.1, to fit a soft rank-\(K\) ICG to a graph in cut metric, we learn a triplet \(^{N K}\), \(^{K}\) and \(^{K D}\) that minimize the Frobenius loss:

\[L(,,)=\|-() ^{}\|_{}^{2}+\|-\|_{ }^{2},\] (7)

where \(\) is a scalar that balances the two loss terms. To implement \(^{N K}\) in practice, we define \(=()\) for a learned matrix \(^{N K}\).

Suppose that \(\) is sparse with \(K^{2},K N\). The loss (7) involves sparse matrices and low rank matrices. Typical time complexity of sparse matrix operations is \((E)\), and for rank-\(K\) matrices it is \((NK^{2})\). Hence, we would like to derive an optimization procedure that takes \((K^{2}N+E)\) operations. However, the first term of (7) involves a subtraction of the low rank matrix \(()^{}\) from the sparse matrix \(\), which gives a matrix that is neither sparse nor low rank. Hence, a naive optimization procedure would take \((N^{2})\) operations. The next proposition shows that we can write the loss (7) in a form that leads to a \((K^{2}N+KE)\) time and \((KN+E)\) space complexities.

**Proposition 4.1**.: _Let \(=(a_{i,j})_{i,j=1}^{N}\) be an adjacency matrix of a weighted graph with \(E\) edges. The graph part of the Frobenius loss can be written as_

\[\|-()^{} \|_{}^{2} =}((^{}) ()(^{})() )+\|\|_{}^{2}\] \[-}_{i=1}^{N}_{j(i)}_{i,:}()(^{})_{:,j}a_{i,j}.\]

_Computing the right-hand-side and its gradients with respect to \(\) and \(\) has a time complexity of \((K^{2}N+KE)\), and a space complexity of \((KN+E)\)._

The proof is in Appendix C. We optimize \(\), \(\), and \(\) using gradient descent (GD) on (7) implemented via Proposition 4.1. After convergence, we further refine \(\) by setting \(=^{}\).

### Initialization

In Appendix D we propose a good initialization for the GD minimization of (7). Suppose that \(K\) is divisible by \(3\). Let \(_{K/3}^{N K/3}\) be the matrix consisting of the leading eigenvectors of \(\) as columns, and \(_{K/3}^{K/3 K/3}\) the diagonal matrix of the leading eigenvalues. For each eigenvector \(\) with eigenvalue \(\), consider the three soft indicators

\[_{1}=_{+}}{\|_{+}\|_{ }},_{2}=_{-}}{\|_{-}\| _{}},_{3}=_{+}+_{-}}{\|_{+}+_{-}\|_{}}\] (8)

with community affiliation magnitudes \(r_{1}=2\|_{+}\|_{}^{2}\), \(r_{2}=2\|_{-}\|_{}^{2}\) and \(r_{3}=-\|_{+}+_{-}\|_{}^{2}\) respectively. Here, \(_{}\) is the positive or negative part of the vector. One can now show that the ICG \(\) based on the \(K\) soft affiliations corresponding to the leading \(K/3\) eigenvectors approximates \(\) in cut metric with error \((K^{-1/2})\). Note that computing the leading eigenvectors is efficient with any variant of the power iteration for sparse matrices, like any variant of Lanczos algorithm, which takes \((E)\) operations per iteration, and requires just a few iteration due to its fast super-exponential convergence . We moreover initialize \(\) optimally by \(=^{}\).

### Subgraph SGD for fitting ICGs to large graphs

In many situations, we need to process large graphs for which \(E\) is too high to read to the GPU memory, but \(NK\) is not. These are the situation where processing graph-signals using ICGs is beneficial. However, to obtain the ICG we first need to optimize the loss (7) using a message-passing type algorithm that requires reading \(E\) edges to memory. To allow such processing, we next show that one can read the \(E\) edges to shared RAM (or storage), and at each SGD step read to the GPU memory only a random subgraph with \(M\) nodes.

At each interation, we sample \(M N\) random nodes \(:=(n_{m})_{m=1}^{M}\) uniformly and independently from \([N]\) (with repetition). We construct the sub-graph \(^{()}^{M M}\) with entries \(a_{i,j}^{()}=a_{n_{i},n_{j}}\), and the sub-signal \(^{()}^{M K}\) with entries \(_{i}^{()}=_{n_{i}}\). We similarly define the sub-community affiliation matrix \(^{()}^{M K}\) with entries \(q_{i,j}^{()}=q_{n_{i},j}\). We consider the loss

\[L^{()}(^{()},,)=\|^{()}-^{ ()}()^{()^{}}\|_{}^{2}+\|^{()}-^{()} ()\|_{}^{2}\] (9)

which depends on all entries of \(\) and \(\) and on the \(\) entries of \(\). Each SGD updates all of the entries of \(\) and \(\), and the \(\) entries of \(\) by incrementing them with the respective gradients of \(L^{()}(^{},,)\). Hence, \(_{}L^{()}(^{()},,)\) may only be nonzero for entries \(_{n}\) with \(n\). Proposition E.1 in Appendix E shows that the gradients of \(L^{()}\) approximate the gradients of \(L\). More concretely, \(_{^{()}}L^{()}_{^{() }}L\), \(_{}L^{()}_{}L\), and \(_{}L^{()}_{}L\). Note that the stochastic gradients with respect to \(^{()}\) approximate a scaled version of the full gradients.

## 5 Learning with ICG

Given a graph-signal \((,)\) and its approximating ICG \((,)\), the corresponding soft affiliations \(=(_{k})_{k=1}^{K}\) represent intersecting communities that can describe the adjacency structure \(\) and can account for the variability of \(\). In a deep network architecture, one computes many latent signals, and these signals typically correspond in some sense to the structure of the data \((,)\). It is hence reasonable to put a special emphasis on latent signals that can be described as linear combinations of \((_{k})_{k=1}^{K}\). Next, we develop a signal processing framework for such signals.

**Graph signal processing with ICG.** We develop a signal processing approach with \((NK)\) complexity for each elemental operation. Let \(^{N K}\) be a fixed community matrix. We call \(^{N D}\) the _node space_ and \(^{K D}\) the _community space_. We process signals via the following operations:

* _Synthesis_ is the mapping \(\) from the community space to the node space, in \((NKD)\).
* _Analysis_ is the mapping \(^{}\) from the node space to the community space, in \((NKD)\).
* _Community processing_ refers to any operation that manipulates the community feature vector \(\) (e.g., an MLP in which a linear operator requires \(KD KD\) parameters or a Transformer) in \((K^{2}D^{2})\) operations.
* _Node processing_ is any function \(\) that operates in the node space on nodes independently via \(():=((_{n}))_{n=1}^{N}\), where \(:^{D}^{D^{}}\) (e.g., an MLP) takes \((DD^{})\) operations. Node processing has time complexity of \((NDD^{})\).

Analysis and synthesis satisfy the reconstruction formula \(^{}=\). Moreover, \(^{}\) is the projection of \(\) upon the space of linear combinations of communities \(\{^{K D}\}\). Note that when \(\) is not low rank, since \(\) is almost surely full rank when initialized randomly, the optimal configuration of \(\) would avoid having repetitions of communities, as this would unnecessarily reduce its rank. Therefore, \(\) is typically full rank, and in this generic case \(^{}=(^{})^{-1}^{}\).

**Deep ICG Neural Networks.** In the following, we propose deep architectures based on the elements of signal processing with an ICG, which take \((D(NK+K^{2}D+ND))\) operations at each layer. In comparison, simplified message passing layers (e.g., GCN and GIN), where the message is computed using just the feature of the transmitting node, have a complexity of \((ED+ND^{2})\). General message passing layers, which define messages via a function applied on the concatenated pair of node features of each edge, take \((ED^{2})\) operations for MLP message functions. Therefore, ICG neural networks are more efficient than MPNNs if \(K D\), where \(\) is the average node degree.

We denote by \(D^{()}\) the dimension of the node features at layer \(\) and define the initial node representations \(^{(0)}=\) and the initial community features \(^{(0)}=^{}\). The node features at layers \(0 L-1\) are defined by

\[^{(+1)}=(^{()}_{1}^{()}+ (^{()})_{2}^{()}),\ \ \ \ \ ^{(+1)}=^{}^{()},\]

where \(\) is a neural network (i.e. multilayer perceptron or MultiHeadAttention), \(_{1}^{()},_{2}^{()}^{D^{()} D^{( +1)}}\) are learnable matrices acting on signals in the node space, and \(\) is a non-linearity. We call this method ICG neural network (ICG-NN). The final representations \(^{(L)}\) can be used for predicting node-level properties.

We propose another deep ICG method, where \(^{()}^{K D^{()}}\) are taken directly as trainable parameters. Namely,

\[^{(+1)}=(^{()}_{s}^{()}+^ {()}^{()}).\]

We call this method ICG\({}_{}\)-NN for the _unconstrained_ community features. The motivation is that \(^{()}\) exhausts the space of all signals that are linear combinations of the communities. If the number of communities is not high, then this is a low dimensional space that does not lead to overfitting in typical machine learning problems. Therefore, there is no need to reduce the complexity of \(^{()}\) by constraining it to be the output of a neural network \(\) on latent representations. The term \(^{()}\) can hence be interpreted as a type of positional encoding that captures the community structure.

**ICG-NNs for spatio-temporal graphs.** For a fixed graphs with varying node features, the ICG is fitted to the graph once6. Given that there are \(T\) training signals, each learning step with ICG-NN takes\((TN(K+D))\) rather than \((TED^{2})\) for MPNNs. Note that \(T\) does not scale the preprocessing time. Hence, the time dimension amplifies the difference in efficiency between ICG-NNs and MPNNs.

## 6 Experiments

We empirically validate our methods with the following experiments: 7

* **Runtime analysis** (Section 6.1): We report the forward pass runtimes of ICG\({}_{}\)-NN and GCN , empirically validating the theoretical advantage of the former. We further extend this analysis in Appendices F.7 and F.8.
* **Node classification** (Appendix F.1): We evaluate our method on real-world node classification datasets [43; 45; 36], observing that the model performance is competitive with standard approaches.
* **Node classification using Subgraph SGD** (Section 6.2 and Appendix F.3): We evaluate our subgraph SGD method (Section 4.3) to identify the effect of sampling on the model performance on the tolokers and Flickr datasets [43; 65]. We find the model's performance to be robust on tolokers and state-of-the-art on Flickr.
* **Spatio-temporal tasks** (Section 6.3): We evaluate ICG\({}_{}\)-NN on real-world spatio-temporal tasks  and obtain competitive performance to domain-specific baselines.
* **Comparison to graph coarsening methods** (Appendix F.2): We provide an empirical comparison between ICG-NNs and a variety of graph coarsening methods on the Reddit  and Flickr  datasets, where ICG-NNs achieve state-of-the-art performance.
* **Additional experiments:** We perform an ablation study over the number of communities (Appendix F.4) and the choice of initialization in Section 4.2 (Appendix F.6). We moreover experimentally demonstrate a positive correlation between the Frobenius error and cut norm error as hinted by Theorem 3.1 (Appendix F.5), and perform a memory allocation analysis (Appendix F.9).

### How does the runtime compare to standard GNNs?

**Setup.** We compare the forward pass runtimes of our signal processing pipeline (ICG\({}_{}\)-NN) and GCN  on _Erdos-Renyi_\((n,p(n)=0.5)\) graphs with up to \(7k\) nodes. Node features are independently drawn from \(U\) and the initial feature dimension is \(128\). Both models use a hidden dimension of \(128\), \(3\) layers and an output dimension of \(5\).

**Results.** Figure 2 reveals a strong square root relationship between the runtime of ICG\({}_{}\)-NN and the runtime of GCN. This aligns with our expectations, as the time complexity of GCN is \((E)\), while the time complexity of ICG-NNs is \((N)\), highlighting the computational advantage of using ICGs. We complement this analysis with experiments using sparse _Erdos-Renyi_ graphs in Appendix F.7.

### Node classification using Subgraph SGD

**Setup.** We evaluate ICG\({}_{}\)-NN on the non-sparse graphs tolokers , following the 10 data splits of . We report the mean ROC AUC and standard deviation as a function of the ratio of nodes that are removed from the graph. We compare to the baseline of MLP on the full graph.

**Results.** Figure 3 shows a slight degradation of 2.8% when a small number of nodes is removed from the graph. However, the key insight is that when more than 10% of the graph is removed, the performance stops degrading. These results further support our Proposition E.1 in Appendix E about the error between subgraph gradients and full-graph gradients of the Frobenius loss, and establish ICG-NNs as a viable option for learning on large graphs.

Figure 3: ROC AUC of ICG\({}_{}\)-NN and an MLP as a function of the \(\%\) nodes removed from the graph.

Figure 2: Runtime of K-ICG\({}_{}\)-NN (for K=100) as a function of GCN forward pass duration on graphs \(G(n,p(n)=0.5)\).

### Spatio-temporal graphs

**Setup & baselines.** We evaluate ICG-NN and ICG\({}_{}\)-NN on real-world traffic networks, METR-LA and PEMS-BAY  following the methodology described by . We segment the datasets into windows of 12 time steps and train the models to predict the subsequent 12 observations. For all datasets, these windows are divided sequentially into 70% for training, 10% for validation, and 20% for testing. We report the mean absolute error (MAE) and standard deviation averaged over the forecastings. The baselines DCRNN , GraphWaveNet , AGCRN , T&S-IMP, TTS-IMP, T&S-AMP, and TTS-AMP , are adopted from . We incorporate a GRU to embed the data before inputting it into our ICG-NN models, we symmetrize the graph to enable our method to operate on it, and we disregard the features in the ICG approximation (setting \(=0\) in (7)). Additionally, we use the Adam optimizer and detail all hyperparameters in Appendix I.

**Results.** Table 1 shows that ICG-NNs achieve competitive performance when compared to methods that are specially tailored for spatio-temporal data such as DCRNN, GraphWaveNet and AGCRN. Despite the small graph size (207 and 325 nodes) and the low ratio of edges (graph densities of \(3.54 10^{-2}\) and \(2.24 10^{-2}\)), ICG-NNs perform well, corroborating the discussion in Section 3.3.

## 7 Related work

We provide the main related work in this Section. In Appendix G we give an additional review.

**Intersecting communities and stochastic block models.** We express graphs as intersections of cliques, or communities, similarly to classical works in statistics and computer science . Our approach can be interpreted as fitting a stochastic-block-model (SBM) to a graph. As opposed to standard SBM approaches, our method is interpreted as data fitting with norm minimization rather than statistical inference. Similarly to the intersecting community approach of BigCLAM , our algorithm takes \((E)\) operations. Unlike BigCLAM, however, we can approximate any graph, as guaranteed by the regularity lemma. This is possible since ICGs are allowed to have negative coefficients, while BigCLAM only uses positive coefficients due to its probabilistic interpretation. To the best of our knowledge, we are the first to propose a SBM fitting algorithm based on the weak regularity lemma. For a survey on SBMs we refer the reader to .

**The Weak Regularity Lemma.** The Regularity Lemma is a central result in graph theory with many variants and many proof techniques. One version is called the _Weak Regularity Lemma_ (for graphs , graphons , or graph-signals and graphon-signals ), and has the following interpretation: every graph can be approximated in the cut metric by an ICG, where the error rate \(\) is uniformly \((K^{-1/2})\)8. While [20, Theorem 2] proposes an algorithm for finding the approximating ICG, the algorithm takes \(N2^{(K)}\) time to find this minimizer9. This time complexity is too high to be practical in real-world problems. Alternatively, since the cut metric is defined via a maximization process, finding a minimizing ICG by directly optimizing cut metric via a GD approach involves a min-max problem, which appears numerically problematic in practice. Moreover, computing cut norm is NP-hard . Instead, we consider a way to bypass the need to explicitly compute the cut norm, finding a \(K\) community ICG with error \((K^{-1/2})\) in the cut metric by minimizing the Frobenius norm. Here, each gradient descent step takes \(O(EK)\) operations, and \(EK\) is typically much smaller than \(N2^{K}\). As opposed to the algorithm in , our theorem is only semi-constructive, as GD is not guaranteed to find the global minimum of the Frobenius error. Still, our theorem motivates a _practical_ strategy for estimating ICGs, while the algorithm formulated in  is only of theoretical significance. We note that for the Szemeredi (non-weak) regularity lemma , it was shown by Alon

    & METR-LA & PEMS-BAY \\  & 0.44 & 0.34 \\  DCRNN & 3.22 \(\) 0.01 & 1.64 \(\) 0.00 \\ GraphWaveNet & 3.05 \(\) 0.03 & 1.56 \(\) 0.01 \\ AGCRN & 3.16 \(\) 0.01 & 1.61 \(\) 0.00 \\ T&S-IMP & 3.35 \(\) 0.01 & 1.70 \(\) 0.01 \\ TTS-IMP & 3.34 \(\) 0.01 & 1.72 \(\) 0.00 \\ T&S-AMP & 3.22 \(\) 0.02 & 1.65 \(\) 0.00 \\ TTS-AMP & 3.24 \(\) 0.01 & 1.66 \(\) 0.00 \\  ICG-NN & 3.42 \(\) 0.03 & 1.76 \(\) 0.00 \\ ICG\({}_{}\)-NN & 3.12 \(\) 0.01 & 1.56 \(\) 0.00 \\   

Table 1: Results on dense temporal graphs. Top three models are colored by First, Second, Third.

et al.  that a regular partition with error \(<\) into non-intersecting communities (the analogue to ICG in the Szemeredi regularity lemma) can be found in polynomial time with respect to \(N\), provided that \(N\) is very large (\(N 2^{-20}\)). For more details on the Weak Regularity Lemma, see Appendix A.

**Subgraph methods.** Learning on large graphs requires sophisticated subgraph sampling techniques for deployment on contemporary processors . After the preprocessing step on the ICG, our approach allows processing very large networks more accurately, avoiding subsampling schemes that can alter the properties of the graph in unintended ways.

**GNNs with local pooling.** Local graph pooling methods, e.g., , construct a sequence of coarsened versions of the given graph, each time collapsing small sets of neighboring nodes into a single "super-node." At each local pooling layer, the signal is projected from the finer graph to the coarser graph. This is related to our ICG-NN approach, where the signal is projected upon the communities. As opposed to local graph pooling methods, our communities intersect and have large-scale supports. Moreover, our method does not lose the granular/high-frequency content of the signal when projecting upon the communities, as we also have node-wise operations at the fine level. In pooling approaches, the graph is partitioned into disjoint, or slightly overlapping communities, each community collapses to a node, and a standard message passing scheme is applied on this coarse graph. In ICG-NNs, each operation on the community features has a global receptive field in the graph. Moreover, ICG-NNs are not of message-passing type: the (flattened) community feature vector \(\) is symmetryless and is operated upon by a general MLP in the ICG-NN, while MPNNs operate by the same function on all edges.

In terms of computational efficiency, GNNs with local pooling do not asymptotically improve runtime, as the first layer operates on the full graph, while our method operates solely on the efficient data structure.

**Variational graph autoencoders.** Our ICG construction is related to statistical network analysis with latent position graph models . Indeed, the edge weight between any pair of nodes \(n,m\) in the ICG is the diagonal Bilinear product between their affiliation vectors, namely, \(_{j}r_{j}_{_{j}}(n)_{_{j}}(m)\), which is similar to the inner produce decoder in variational graph autoencoders . However, as opposed to variational graph autoencoders, we have the coefficients \(r_{j}\), that can be negative and hence allow a more expressive decoding.

## 8 Summary

We introduced an new approach for learning on large non-sparse graphs, using ICGs. We proved a new constructive variant of the weak regularity lemma, which shows that minimizing the Frobenius error between the ICG and the graph leads to a uniformly small error in cut metric. We moreover showed how to optimize the Frobenius error efficiently. We then developed a signal processing setting, operating on the ICG and on the node reprsentations in \((N)\) complexity. The overall pipeline involves precomputing the ICG approximation of the graph in \((E)\) operations per iteration, and then solving the task on the graph in \((N)\) operations per iteration. Both fitting an ICG to a large graph, and training a standard subgraph GNNs, require an online subsampling method, reading from slow memory during optimization. However, fitting the ICG is only done once, and does not require an extensive hyperparameter optimization. Then, learning to solve the task on the graph with ICG-NNs is efficient, and can be done directly on the GPU memory. Since the second learning phase is the most demanding part, involving an extensive hyperparameter optimization and architecture tuning, ICG-NN offer a potentially significant advantage over standard subgraph GNNs. This gap between ICG-NNs and MPNNs is further amplified for time series on graphs.

The main limitation of our method is that the ICG-NN is fitted to a specific ICG, and cannot be naively transferred between different ICGs approximating different graphs. Another limitation is the fact that the current ICG construction is limited to undirected graphs, while many graphs, especially spatiotemporal, are directed. One potential avenue for future work is thus to extend the ICG construction to directed graphs. Additional future work will study the expressive power of ICGs.

**Theorem A.1** (Weak Regularity Lemma ).: _For every \(>0\) and every graph \(G=(,)\), there is an equipartition \(=\{_{1},,_{k}\}\) of \(\) into \(k 2^{c/^{2}}\) classes such that \(_{G}()\). Here, \(c\) is a universal constant that does not depend on \(G\) and \(\)._

The weak regularity lemma states that any large graph \(G\) can be represented by the weighted graph \(G^{}\) with node set \(^{}=\{_{1},,_{k}\}\), where the edge weight between the nodes \(_{i}\) and \(_{j}\) is \((_{i},_{j})}{|_{i}|,|_ {j}|}\), which depicts a smaller, coarse-grained version of the large graph. The "large-scale" structure of \(G\) is given by \(G^{}\), and the number of edges between any two subsets of nodes \(_{i}_{i}\) and \(_{j}_{j}\) is close to the "expected value" \(e_{(_{i},_{j})}\). Hence, the deterministic graph \(G\) "behaves" as if it was randomly sampled from the "stochastic block model" \(G^{}\).

It can be shown that irregularity coincides with error in cut metric between the graph and its coarsening SBM. Namely, to see how the cut-norm is related to irregularity (10), consider a graph \(W_{G}\) induced by the graph \(G=\{,\}\). Let \(=\{_{1},,_{k}\}\) be an equipartition of \(\). Consider the graph \(G^{}\) defined by the adjacency matrix \(A^{}=(a_{i,j}^{})_{i,j=1}^{||}\) with

\[a_{i,j}^{}=(_{q_{i}},_{q_{j}})}{| _{q_{i}}|\,,|_{q_{j}}|},\]

where \(_{q_{i}}\) is the class that contains node \(i\). Now, it can be verified that

\[\|W_{G}-W_{G^{}}\|_{}=_{G}( ).\]

Hence, the weak regularity lemma can be formulated with cut norm instead of irregularity.

## Appendix B Graphon-Signal Intersecting Communities and the Constructive Regularity Lemma

In this section we prove a constructive weak regularity lemma for graphon-signals, where Theorem 3.1 is a special case. We start by defining graphon-signals in Subsection B.1. We define graphon-signal cut norm and metric in Subsection B.2, and graphon-signal intersecting communities in B.3. In Subsections B.4 and B.5 we formulate and prove the constructive graphon-signal weak regularity lemma. Finally, in Subsection B.6 we prove the constructive graphon-signal weak regularity lemma as a special case.

For \(m,J\), the \(L_{2}\) norm of a measurable function \(f=(f_{i})_{i=1}^{j}:^{m}^{J}\), where \(f_{i}:^{m}\), is defined to be \(\|f\|_{2}=^{J}_{^{m}}|f_{j}() |^{2}d}\).

### Graphon-signals

A graphon  can be seen as a weighted graph with a "continuous" node set \(\). The space of graphons \(\) is defined to be the set of all measurable symmetric function \(W:^{2}\), \(W(x,y)=W(y,x)\). Each _edge weight_\(W(x,y)\) of a graphon \(W\) can be seen as the probability of having an edge between nodes \(x\) and \(y\). Graphs can be seen as special graphons. Let \(_{m}=\{I_{1},,I_{m}\}\) be an _interval equipartition_: a partition of \(\) into disjoint intervals of equal length.

A graph with an adjacency matrix \(=(a_{i,j})_{i,j=1}^{N}\)_induces_ the graphon \(W_{}\), defined by \(W_{}(x,y)=a_{ xm, ym}\)10. Note that \(W_{}\) is piecewise constant on the partition \(_{m}\). We hence identify graphs with their induced graphons. A graphon can also be seen as a generative model of graphs. Given a graphon \(W\), a corresponding random graph is generated by sampling i.i.d. nodes \(\{u_{n}\}\) from the graphon domain \(\), and connecting each pair \(u_{n},u_{m}\) in probability \(W(u_{n},u_{m})\) to obtain the edges of the graph.

The space of grahon-signals is the space of pairs of measurable functions \((W,s)\) of the form \(W:^{2}\) and \(s:^{D}\), where \(D\) is the number of signal channels. Note that datasets with signals in value ranges other than \(\) can always be transformed (by translating and scaling in the value axis of each channel) to be \(^{D}\)-valued. Given a discrete signal \(:[N]^{}\), we define the induced signal \(s_{}\) over \(\) by \(s_{}(x)=s_{ xm}\). We define the _Frobenius distance_ between two graphon-signals \((W,s)\) and \((W^{},s^{})\) simply as their \(L_{2}\) distance, namely,

\[\|(W,s)-(W^{},s^{})\|_{}:=\|_{2}^{2}+b_{j=1}^{D}\|s_{j}-s_{j}^{}\|_ {2}^{2}}\] (11)

for some \(a,b>0\).

### Cut-distance

The _cut metric_ is a natural notion of graph similarity, based on the _cut norm_. The graphon-signal cut norm was defined in , extending the standard definition to a graphon with node features.

**Definition B.1**.: _The graphon cut norm of \(W L^{2}^{2}\) is defined to be_

\[\|W\|_{}=_{S,T}|_{S}_{T}W(x,y) dxdy|.\] (12)

_The signal cut norm of \(s=(s_{1},,s_{D})(L^{2})^{D}\) is defined to be_

\[\|s\|_{}=_{j=1}^{D}_{U}| _{U}s_{j}(x)dx|.\] (13)

_The graphon-signal cut norm of \((W,s) L^{2}^{2}(L^{2})^{D}\), with weights \(>0\) and \(>0\), is defined to be_

\[\|(W,s)\|_{}=\|W\|_{}+\|s \|_{}.\] (14)

Given two graphons \(W,W^{}\), their distance in _cut metric_ is the _cut norm_ of their difference, namely

\[\|W-W^{}\|_{}=_{S,T}|_{S} _{T}W(x,y)-W^{}(x,y)dxdy|.\] (15)

The right-hand-side of (15) is interpreted as the similarity between the adjacencies \(W\) and \(W^{}\) on the block on which they are the most dissimilar. The _graphon-signal cut metric_ is similarly defined to be \(\|(W,s)-(W^{},s^{})\|_{}\). The cut metric between two graph-signals is defined to be the cut metric between their induced graphon-signals.

In Subsection B.6 we show that the graph-signal cut norm of Definition 2.1 are a special case of graphon-signal cut norm for induced graph-signals.

### Intersecting community graphons

Here, we define ICGs for graphons. Denote by \(\) the set of all indicator function of measurable subset of \(\)

\[=\{_{u} u\}.\]

**Definition B.2**.: _A set \(\) of bounded measurable functions \(q:\) that contains \(\) is called a soft affiliation model._

**Definition B.3**.: _Let \(D\). Given a soft affiliation model \(\), the subset \([]\) of \(L^{2}^{2}(L^{2})^{D}\) of all elements of the form \((rq(x)q(y),bq(z))\), with \(q\), \(r\) and \(b^{D}\), is called the soft rank-1 intersecting community graphon (ICG) model corresponding to \(\). Given \(K\), the subset \([]_{K}\) of \(L^{2}^{2}(L^{2})^{D}\) of all linear combinations of \(K\) elements of \([]\) is called the soft rank-\(K\) ICG model corresponding to \(\). Namely, \((C,p)[]_{K}\) if and only if it has the form_

\[C(x,y)=_{k=1}^{K}r_{k}q_{k}(x)q_{k}(y) p(z)=_{k=1 }^{K}b_{k}q_{k}(z)\]

_for \((q_{k})_{k=1}^{K}^{K}\) called the community affiliation functions, \((r_{k})_{k=1}^{K}^{K}\) called the community affiliation magnitudes, and \((b_{k})_{k=1}^{K}^{K D}\) called the community features._

### A constructive graphon-signal weak regularity lemma

The following theorem is the semi-constructive version of the weak regularity lemma for intersecting communities of graphons. Recall that \(\) and \(\) denote the weights of the graphon-signal cut norm (14).

**Theorem B.1**.: _Let \(D\). Let \((W,s)\) be a \(D\)-channel graphon-signal, \(K\), \(>0\), and let \(\) be a soft indicators model. Let \(m\) be sampled uniformly from \([K]\), and let \(R 1\) such that \(K/R\). Consider the graphon-signal Frobenius norm with weights \(^{2}+_{j=1}^{D}\|s_{j}\|_ {}^{2}}\). Then, in probability \(1-\) (with respect to the choice of \(m\)), for every \((C^{*},p^{*})[]_{m}\),_

\[\|(W,s)-(C^{*},p^{*})\|_{F}(1+ )_{(C,p)[]_{m}}\|(W,s)-(C,p)\|_{}\] \[\|(W,s)-(C^{*},p^{*})\|_{}\] \[(\|W\|_{}^{2}+\|s\|_{}}+}^{2}+^{2}\|s\|_{}} )+}.\]

### Proof of the soft constructive weak regularity lemma

In this subsection we prove the constructive weak graphon-signal regularity lemma.

#### b.5.1 Constructive regularity lemma in Hilbert spaces

The classical proof of the weak regularity lemma for graphons is given in . The lemma is a corollary of a regularity lemma in Hilbert spaces. Our goal is to extend this lemma to be constructive. For comparison, let us first write the classical result, from [38, Lemma 4], even though we do not use this result directly.

**Lemma B.2** ().: _Let \(_{1},_{2},\) be arbitrary nonempty subsets (not necessarily subspaces) of a real Hilbert space \(\). Then, for every \(>0\) and \(g\) there is \(m 1/^{2}\) and \((f_{i}_{i})_{i=1}^{m}\) and \((_{i})_{i=1}^{m}\), such that for every \(w_{m+1}\)_

\[| w,g-(_{i=1}^{m}_{i}f_{i})| \,\|w\|\|g\|.\] (16)

Next, we show how to modify the result to be written explicitly with an approximate minimizer of a Hilbert space norm minimization problem. The proof follows the step of  with some modifications required for explicitly constructing an approximate optimizer.

**Lemma B.3**.: _Let \(\{_{j}\}_{j}\) be a sequence of nonempty subsets of a real Hilbert space \(\) and let \( 0\). Let \(K>0\), let \(R 1\) such that \(K/R\), and let \(g\). Let \(m\) be randomly uniformly sampled from \([K]\). Then, in probability \(1-\) (with respect to the choice of \(m\)), any vector of the form_

\[g^{*}=_{j=1}^{m}_{j}f_{j}=(_{j})_{j=1}^{m} ^{m}=(f_{j})_{j=1}^{m}_ {1}_{m}\]_that gives a close-to-best Hilbert space approximation of \(g\) in the sense that_

\[\|g-g^{*}\|(1+)_{,}\|g -_{i=1}^{m}_{i}f_{i}\|\!,\] (17)

_where the infimum is over \(^{m}\) and \(_{1}_{m}\), also satisfies_

\[ w_{m+1},| w,g-g^{*} |\|w\|\|g\|+}.\]

Proof.: Let \(K>0\). Let \(R 1\) such that \(K/R\). For every \(k\), let

\[_{k}=(1+)_{,}\|g-_{i=1}^{k }_{i}f_{i}\|^{2}\]

where the infimum is over \(=\{_{1},,_{k}\}^{k}\) and \(=\{f_{1},,f_{k}\}_{1}_{k}\). Note that \(\|g\|^{2}}{1+}}{1+} 0\). Therefore, there is a subset of at least \((1-)K+1\) indices \(m\) in \([K]\) such that \(_{m}_{m+1}+\|g\|^{2}\). Otherwise, there are \(\) indices \(m\) in \([K]\) such that \(_{m+1}<_{m}-\|g\|^{2}\), which means that

\[_{K}<_{1}-\|g\|^{2}(1+ )\|g\|^{2}-(1+)\|g\|^{2}=0,\]

which is a contradiction to the fact that \(_{K} 0\).

Hence, there is a set \([K]\) of \((1-)K\) indices such that for every \(m\), every

\[g^{*}=_{j=1}^{m}_{j}f_{j}\] (18)

that satisfies

\[\|g-g^{*}\|^{2}_{m}\]

also satisfies

\[\|g-g^{*}\|^{2}_{m+1}+\|g\| ^{2}.\] (19)

Let \(w_{m+1}\). By the definition of \(_{m+1}\), we have for every \(t\),

\[\|g-(g^{*}+tw)\|^{2}}{1+}\|^{2}}{1+}-\|g\|^{2}.\]

This can be written as

\[ t,\|w\|^{2}t^{2}-2 w,g-g^{*}  t+\|g\|^{2}+(1-)\| g-g^{*}\|^{2} 0.\] (20)

The discriminant of this quadratic polynomial is

\[4 w,g-g^{*}^{2}-4\|w\|^{2}(\|g\|^{2}+(1-)\|g-g^{*}\|^{2})\]

and it must be non-positive to satisfy the inequality (20), namely

\[4 w,g-g^{*}^{2}  4\|w\|^{2}(\|g\|^{2}+(1- )\|g-g^{*}\|^{2})\] \[ 4\|w\|^{2}(\|g\|^{2}+(1- )_{m})\] \[ 4\|w\|^{2}(\|g\|^{2}+(1- )(1+)\|g\|^{2})\]

which proves

\[ w,g-g^{*}\|w\|\|g\|+}.\]

This is also true for \(-w\), which concludes the proof.

#### b.5.2 Proof of the Soft Constructive Weak Regularity Lemma for Graphon-Signals

For two functions \(f,g:\), we denote by \(f g:^{2}\) the function

\[f g(x,y)=f(x)g(y).\]

We recall here Theorem B.1 for the convenience of the reader.

**Theorem B.1**.: _Let \((W,s)\) be a graphon-signal, \(K\), \(>0\), and let \(\) be a soft indicators model. Let \(m\) be sampled uniformly from \([K]\), and let \(R 1\) such that \(K/R\). Consider the graphon-signal Frobenius norm with weights \(^{2}+_{j=1}^{D}\|s_{j}\|_{ }^{2}}\). Then, in probability \(1-\) (with respect to the choice of \(m\)), for every \((C^{*},p^{*})[]_{m}\),_

_if_ \[\|(W,s)-(C^{*},p^{*})\|_{F}(1+)_{(C,p)[ ]_{m}}\|(W,s)-(C,p)\|_{}\] _then_ \[\|(W,s)-(C^{*},p^{*})\|_{}\] \[(\|W\|_{}^{2}+ \|s\|_{}}+}^{2}+ ^{2}\|s\|_{}})+}.\]

The proof follows the techniques of a part of the proof of the weak graphon regularity lemma from , while tracking the approximate minimizer in our formulation of Lemma B.3. This requires a probabilistic setting, and extending to soft indicators models. We note that the weak regularity lemma in  is formulated for non-intersecting blocks, but the intersecting community version is an intermediate step in its proof.

Proof.: Let us use Lemma B.3, with \(=L^{2}^{2}(L^{2})^{D}\) with the weighted inner product

\[(W,s),(W^{},s^{})=_{^{2}}W(x,y)W^{ }(x,y)dxdy+_{j=1}^{D}_{}s_{j}(x)s^{}_{j}(x)dx,\]

and corresponding norm \(^{2}+_{j=1}^{D}\|s_{j}\|_ {}^{2}}\) and corresponding weighted inner product, and \(_{j}=[]\). Note that the Hilbert space norm is the Frobenius norm in this case. In the setting of the lemma, we take \(g=(W,s)\), and \(g^{*}[]_{m}\). By the lemma, in the event of probability \(1-1/R\), any approximate Frobenius minimizer \((C^{*},p^{*})\), namely, that satisfies \(\|(W,s)-(C^{*},p^{*})\|_{}(1+)_{(C,p)[ ]_{m}}\|(W,s)-(C,p)\|_{}\), also satisfies

\[(T,y),(W,s)-(C^{*},p^{*})\|(T,y)\|_{}\|(W,s)\|_{ }+}\]

for every \((T,y)[]\). Hence, for every choice of measurable subsets \(,\), we have

\[|_{}_{}(W(x,y)-C^{*}(x,y))dxdy|\] \[=|_{}_{}(W(x,y)-C^{*}(x,y))dxdy.-._{}_{ }(W(x,y)-C^{*}(x,y))dxdy.\] \[-._{}_{}(W(x,y)-C^{*}(x,y))dxdy|\] \[|(1_{ } 1_{},0),(W,s)-(C^{*},p^{*}) |+|(1_{} 1_{},0),(W,s)-(C^{*},p^{*}) |\] \[+|(1_{}  1_{},0),(W,s)-(C^{*},p^{*})|\] \[\|(W,s)\|_{}\|(1_{} 1_{},0)\|_{}+\|(1_{} 1_{ },0)\|_{}+\|(1_{} 1_{},0)\|_{ }+}\] \[}^{2}+ \|s\|_{}}+}\]Hence, taking the supremum over \(,\), we also have

\[\|W-C^{*}\|_{}\|W\|_{ }^{2}+\|s\|_{}}+}.\]

Similarly, for every measurable \(\) and every standard basis element \(=(_{j,i})_{i=1}^{D}\) for any \(j[D]\),

\[|_{}(s_{j}(x)-p_{j}^{*}(x))dx|\] \[=|(0,_{}),(W,s)-(C^{*},p^{*})|\] \[\|(W,s)\|_{}\|(0,_{ })\|_{}+}\] \[}^{2}+ \|s\|_{}}+},\]

so, taking the supremum over \(\) independently for every \(j[D]\), and averaging over \(j[D]\), we get

\[\|s-p^{*}\|_{}}^{ 2}+^{2}\|s\|_{}}+}.\]

Overall, we get

\[\|(W,s)-(C^{*},p^{*})\|_{}\|W\|_{ }^{2}+\|s\|_{}}+}^{2}+^{2}\|s\|_{}}+}.\]

\(\)

### Proof of the constructive weak regularity lemma for sparse graph-signals

We now show that Theorem 3.1 is a special case of Theorem B.1, restricted to graphon-signals induced by graph-signals, up to choice of the graphon-signal weights \(\) and \(\). Namely, choose \(=^{}N^{2}/E\) and \(=^{}\) with \(^{}+^{}=1\) as the weights of the graphon-signal cut norm of Theorem B.1, with arbitrary \(^{},^{} 0\) satisfying \(^{}+^{}=1\). It is easy to see the following relation between the graphon-signal and graph-signal cut norms

\[\|(W_{},s_{})-(W_{^{*}},s_{^{*}})\|_{}=\|(,)-(^{*},^{*})\|_{,N,E},\]

where \(\|(,)-(^{*},^{*})\|_{,N,E}\) is based on the weights \(^{}\) and \(^{}\). Now, since

\(()/N^{2}=\|\|_{}^{2}=}\), and by \(\|\|_{} 1\), the bound in Theorem B.1 becomes

\[\|(,)-(^{*},^{*})\|_{,N,E} {2}}{E^{2}}}+^{} ^{}}{E}}+^{} }{E}}+^{ 2}}+}\]

\[ C}+}\]

where, since \(^{}+^{}=1\), and by convexity of the square root,

\[C=}+} {^{}}+}+^{}}=.\]

The only thing left to do is to change the notations \(^{}\) and \(^{}\).

## Appendix C Fitting ICGs to graphs efficiently

Here, we prove Proposition 4.1. For convenience, we recall the proposition.

**Proposition 4.1**.: _Let \(=(a_{i,j})_{i,j=1}^{N}\) be an adjacency matrix of a weighted graph with \(E\) edges. The graph part of the Frobenius loss can be written as_

\[\|-()^{}\| _{}^{2} =}((^{}) ()(^{})() )+\|\|_{}^{2}\] \[-}_{i=1}^{N}_{j(i)}_{i, :}()(^{})_{:,j}a_{i,j}.\]

_Computing the right-hand-side and its gradients with respect to \(\) and \(\) has a time complexity of \((K^{2}N+KE)\), and a space complexity of \((KN+E)\)._

Proof.: The loss can be expressed as

\[\|-()^{} \|_{}^{2}=}_{i,j=1}^{N}(a_{i,j}-_ {i,:}()(^{})_{:,j})^{2}\] \[=}_{i=1}^{N}(_{j(i)} (a_{i,j}-_{i,:}()(^{} )_{:,j})^{2}+_{j(i)}(-_{i,:} ()(^{})_{:,j})^{2})\] \[=}_{i=1}^{N}(_{j(i)} (a_{i,j}-_{i,:}()(^{} )_{:,j})^{2}+.\] \[+._{j=1}^{N}(_{i,:}()(^{})_{:,j})^{2}-_{j(i)}( _{i,:}()(^{})_{:,j}) ^{2})\]

We expand the quadratic term \((a_{i,j}-_{i,:}()(^{} )_{:,j})^{2}\), and get

\[\|-()^{} \|_{}^{2}\] \[=}_{i,j=1}^{N}(_{i,:}()(^{})_{:,j})^{2}+}_{i=1}^{N} _{j(i)}(a_{i,j}^{2}-2_{i,:}( )(^{})_{:,j}a_{i,j})\] \[=\|()^{}\|_{ }^{2}+}_{i,j=1}^{N}a_{i,j}^{2}-} _{i=1}^{N}_{j(i)}_{i,:}( )(^{})_{:,j}a_{i,j}\] \[=}(( )^{}()^{} ())+}_{i,j=1}^{N}a_{i,j}^{2}\] \[-}_{i=1}^{N}_{j(i)}_{i, :}()(^{})_{:,j}a_{i,j}\]

The last equality follows the identity \(,^{N K}:\ (^{})= (^{})\), with \(=()^{}()\) and \(=^{}\).

The middle term in the last line is constant and can thus be omitted in the optimization process. The leftmost term in the last line is calculated by performing matrix multiplication from right to left, or by first computing \(^{}\) and then the rest of the product. Thus, the time complexity is\((K^{2}N)\) and the largest matrix held in memory is of size \((KN)\). The rightmost term is calculated by message-passing, which has time complexity of \((KE)\). Thus, Computing the right-hand-side and its gradients with respect to \(\) and \(\) has a time complexity of \((K^{2}N+KE)\) and a space complexity of \((KN+E)\). 

## Appendix D Initializing the optimization with eigenvectors

Next, we propose a good initialization for the GD minimization of (7). For that, we consider a corollary of the constructive soft weak regularity for grapohns, with the signal weight in the cut norm set to \(=0\), using all \(L^{2}\) functions as the soft affiliation model, and taking relative Frobenius error with \(=0\). In this case, the best rank-\(K\) approximation theorem (Eckart-Young-Mirsky Theorem [51, Theorem 5.9]), the minimizer of the Frobenius error is the projection of \(W\) upon its leading eigenvectors. This leads to the following corollary.

**Corollary D.1**.: _Let \(^{N N}\) be a matrix with \(()=N^{2}\|\|_{}^{2}=E^{}\). Let \(K\), let \(m\) be sampled uniformly from \([K]\), and let \(R 1\) such that \(K/R\). Let \(_{1},,_{m}\) be the leading eigenvectors of \(\), with eigenvalues \(_{1},,_{m}\) of highest magnitudes \(|_{1}||_{2}||_{m}|\), and let \(^{}=_{k=1}^{m}_{k}_{k}_{k}^{}\). Then, in probability \(1-\) (with respect to the choice of \(m\)),_

\[\|-^{}\|_{;N,E^{}}}}}.\]

The initialization is based on Corollary D.1 as follows. We consider the leading \(K/3\) eigenvectors, assuming \(K\) is divisible by \(3\), for reasons that will become clear shortly. Hence, for \(=_{K/3}_{K/3}_{K/3}^{}\), where \(_{K/3}^{N K/3}\) is the matrix consisting of the leading eigenvectors of \(\) as columns, and \(_{K/3}^{K/3 K/3}\) the diagonal matrix of the leading eigenvalues, we have

\[\|-\|_{;N,E}<}} {}.\] (21)

To obtain soft affiliations with values in \(\), we now initialize \(\) as described in (8).

## Appendix E Learning ICG with subgraph SGD

In this section we prove that the gradients of the subgraph SGD approximate the gradients of the full GD method.

**Proposition E.1**.: _Let \(0<p<1\). Under the above setting, if we restrict all entries of \(\), \(\), \(\), \(\) and \(\) to be in \(\), then in probability at least \(1-p\), for every \(k[K]\), \(d[D]\) and \(m[M]\)_

\[|_{q_{n_{m},k}}L-_{q_{n_{m},k}}L^{( )}| )+2(N)+2(K)+2 (2)}{M}},\] \[|_{r_{k}}L-_{r_{k}}L^{()}|  4)+2(N)+2(K)+2(2)}{M}},\] \[|_{f_{k,d}}L-_{f_{k,d}}L^{()}| )+2(K)+2( D)+2(2)}{M}}.\]

Proposition E.1 means that the gradient with respect to \(\) and \(\) computed at each SGD step approximate the full GD gradients. The gradients with respect to \(\) approximate a scaled version of the full gradients, and only on the sampled nodes, where the unsampled nodes are not updated in the SGD step. This means that when optimizing the ICG with SGD, we need to scale the gradients with resect to \(\) of the loss by \(\). To use the same entry-wise learning rate in SGD as in GD one must multiply the loss (9) by \(M/N\). Hence, SGD needs in expectation \(N/M\) times the number of steps that GD requires. This means that in SGD we trade the memory complexity (reducing it by a factor \(M/N\)) by time complexity (increasing it by a factor \(N/M\)). Note that slower run-time, while not desirable, can be tolerated, while higher memory requirement is often prohibitive due to hardware limitations.

From here until the end of this section we prove Proposition E.1. For that, we compute the gradients of the full loss \(L\) and the sampled loss \(L^{()}\).

To avoid tensor notations, we treat by abuse of notation the gradient of a function as a linear operator. Recall that the differential \(_{}=_{}(^{})\) of the function \(:^{R}^{U}\) at the point \(^{}^{R}\) is the unique linear operator \(^{R}^{U}\) that describes the linearization of \(\) about \(^{}\) via

\[(^{}+^{})=(^{})+ _{}^{}+o()\]

where \(\). Given an inner product in \(^{R}\), the gradient \(_{}=_{}(^{})^{U  R}\) of the function \(:^{R}^{U}\) at the point \(^{}^{R}\) is defined to be the vector (guaranteed to uniquely exist by the Riesz representation theorem) that satisfies

\[_{}^{}=^{ },_{}\]

for every \(^{}^{R}\). Here, the inner product if defined row-wise by

\[^{},_{}:=( ^{},_{}[u,:] )_{u=1}^{U},\]

where \([u,:]\) is the \(u\)-th row of \(_{}\). In our analysis, by abuse of notation, we identify \(_{}\) with \(_{}\) for a fixed given inner product.

Define the inner product between matrices

\[,_{2}=_{i,j}b_{i,j}d_{i,j}.\]

Denote

\[L(,)=\|-\|_{}^{2}+\| -\|_{}^{2}.\]

First, the gradient of \(\|-\|_{}^{2}\) with respect to \(\) is

\[_{}\|-\|_{}^{2}=-} (-),\]

which is identified by abuse of notation by the linear functional \(^{N^{2}}\)

\[_{}\|-\|_{}^{2}( {D})=,-}(-)_{2}.\]

Similarly,

\[_{}\|-\|_{}^{2}=-( -):^{ND}.\]

Given any parameter \(^{R}\) on which the graph \(=()\) and the signal \(=()\) depend, we have

\[_{}L(,)=}(-)_{} +(-)_{},\] (22)

where \(_{}:^{R}^{N^{2}}\) and \(_{}^{R}^{ND}\) are linear operators, and the products in (22) are operator composition, namely, in coordinates it is elementwise multiplication (not matrix multiplication).

Let us now compute the gradients of \(=()^{}\) and \(=\) with respect to \(\), \(\) and \(\) in coordinates. We have

\[_{r_{k}}c_{i,j}=q_{i,k}q_{j,k},\]

and

\[_{q_{m,k}}c_{i,j}=r_{k}_{i-m}q_{j,k}+r_{k}_{j-m}q_{i,k},\]

where \(_{l}\) is 1 if \(l=0\) and zero otherwise. Moreover,

\[_{f_{k,l}}p_{i,d}=q_{i,k}_{l-d},\]

and

\[_{q_{m,k}}p_{i,d}=f_{k,d}_{i-m}.\]Hence,

\[_{r_{k}}L=}_{i,j=1}^{N}(c_{i,j}-a_{i,j})q_{i,k}q_{j,k},\]

\[_{q_{m,k}}L=}_{j=1}^{N}(c_{m,j}-a_{m,j})r_{k}q_{j,k}+ }_{i=1}^{N}(c_{i,m}-a_{i,m})r_{k}q_{i,k}+ _{d=1}^{D}(p_{m,d}-s_{m,d})f_{k,d},\]

and

\[_{f_{k,l}}L=_{i=1}^{N}(p_{i,l}-s_{i,l})q_{i,k}.\]

Similarly,

\[_{r_{k}}L^{()}=}_{i,j=1}^{M}(c_{n_{i},n_{j}}-a_{ n_{i},n_{j}})q_{n_{i},k}q_{n_{j},k},\]

\[_{q_{m,k}}L^{()}= }_{j=1}^{M}(c_{n_{m},n_{j}}-a_{n_{m},n_{j}})c_{ k}q_{n_{j},k}\] \[+}_{i=1}^{M}(c_{n_{i},n_{m}}-a_{n_{i},n_{m}})c_ {k}q_{n_{i},k}+_{d=1}^{D}(p_{n_{m},d}-s_{n_{m},d})f_{k,d},\]

and

\[_{f_{k,l}}L^{()}=_{i=1}^{M}(p_{n_{i},l}-s_ {n_{i},l})q_{n_{i},k}.\]

We next derive the convergence analysis, based on Hoeffding's Inequality and two Monte-Carlo approximation lemmas.

**Theorem E.2** (Hoeffding's Inequality).: _Let \(Y_{1},,Y_{M}\) be independent random variables such that \(a Y_{m} b\) almost surely. Then, for every \(k>0\),_

\[_{m=1}^{M}(Y_{m}-[Y_{m}])  k 2-M}{(b-a)^{2}}.\]

The following is a standard Monte Carlo approximation error bound based on Hoeffding's inequality.

**Lemma E.3**.: _Let \(\{i_{m}\}_{m=1}^{M}\) be uniform i.i.d in \([N]\). Let \(^{N}\) be a vector with entries \(v_{n}\) in the set \([-1,1]\). Then, for every \(0<p<1\), in probability at least \(1-p\)_

\[|_{m=1}^{M}v_{i_{m}}-_{n=1}^{N}v_{n}| }.\]

Proof.: This is a direct result of Hoeffding's Inequality on the i.i.d. variables \(\{v_{i_{m}}\}_{m=1}^{M}\). 

The next lemma derives a Monte Carlo approximation error bound based on Hoeffding's inequality for 2D arrays, in case one samples only 1D independent sample points, and the 2D sample points are all pairs of the 1D points (which are hence no longer independent).

**Lemma E.4**.: _Let \(\{i_{m}\}_{m=1}^{M}\) be uniform i.i.d in \([N]\). Let \(^{N N}\) be symmetric with \(a_{i,j}[-1,1]\). Then, for every \(0<p<1\), in probability more than \(1-p\)_

\[|}_{j=1}^{N}_{n=1}^{N}a_{j,n}-}_ {m=1}^{M}_{l=1}^{M}a_{i_{m},i_{l}}| 2}.\]Proof.: Let \(0<p<1\). For each fixed \(n[N]\), consider the independent random variables \(Y_{m}^{n}=a_{i_{m},n}\), with

\[(Y_{m}^{n})=_{j=1}^{N}a_{j,n},\]

and \(-1 Y_{m} 1\). By Hoeffding's Inequality, for \(k=}\), we have

\[|_{j=1}^{N}a_{j,n}-_{m=1}^{M}a_{i_{m},n} | k\]

in an event \(_{n}\) of probability more than \(1-p/N\). Intersecting the events \(\{_{n}\}_{n=1}^{N}\), we get

\[ n[N]:|_{j=1}^{N}a_{j,n}-_{ m=1}^{M}a_{i_{m},n}| k\]

in the event \(=_{n}_{n}\) probability at least \(1-p\). Hence, by the triangle inequality, we also have in the event \(\)

\[|_{l=1}^{M}_{j=1}^{N}a_{j,i_{l}}-}_ {l=1}^{M}_{m=1}^{M}a_{i_{m},i_{l}}| k,\]

and

\[|}_{n=1}^{N}_{j=1}^{N}a_{j,n}-_{n=1 }^{N}_{m=1}^{M}a_{i_{m},n}| k.\]

Hence, by the symmetry of \(\) and by the triangle inequality,

\[|}_{n=1}^{N}_{j=1}^{N}a_{j,n}-}_ {l=1}^{M}_{m=1}^{M}a_{i_{m},i_{l}}| 2k.\]

Now, since all entries of \(\), \(\), \(\), \(\), \(\), \(\) and \(\) are in \(\), we may use Lemmas E.3 and E.4 to derive approximation errors for the gradients of \(L\). Specifically, for any \(0<p_{1}<1\), for every \(k[K]\) there is an event \(_{k}\) of probability at least \(1-p_{1}\) such that

\[|_{r_{k}}L-_{r_{k}}L^{()}| 4)+2(N)+2(2)}{M}}.\]

Moreover, for every \(k[K]\) and \(j[D]\), and every \(0<p_{2}<1\) there is an event \(_{k,j}\) of probability at least \(1-p_{2}\) such that

\[|_{f_{k,l}}L-_{f_{k,l}}L^{()}| {D})+2(2)}{M}}.\]

For the approximation analysis of \(_{q_{m,l}}L\), note that the index \(n_{m}\) is random, so we derive a uniform convergence analysis for all possible values of \(n_{m}\). For that, for every \(n[N]\) and \(k[K]\), define the vector

\[}L^{()}}= }_{j=1}^{M}(c_{n,n_{j}}-a_{n,n_{j}})c_{k}q_{n_{j },k}\] \[+}_{i=1}^{M}(c_{n_{i},n}-a_{n_{i},n})c_{k}q_{n_{ i},k}+_{d=1}^{D}(p_{n,d}-s_{n,d})f_{k,d},\]

Note that \(}L^{()}}\) is not a gradient of \(L^{()}\) (since if \(n\) is not a sample from \(\{n_{m}\}\) the gradient must be zero), but is denoted with \(\) for its structural similarity to \(_{q_{m,k}}L^{()}\). Let \(0<p_{3}<1\). ByLemma E.3, for every \(k[K]\) there is an event \(_{k}\) of probability at least \(1-p_{3}\) such that for every \(n[N]\)

\[|_{q_{n,k}}L-}L^{()}} |)+2(N)+2(2)}{M}}.\]

This means that in the event \(_{k}\), for every \(m[M]\) we have

\[|_{q_{m_{m,k}}}L-_{q_{m_{m,k}}}L^{()}| )+2(N)+2(2)}{M}}.\]

Lastly, given \(0<p<1\), choosing \(p_{1}=p_{3}=p/3K\) and \(p_{2}=p/3KD\) and intersecting all events for all corrdinates gives and event \(\) of probability at least \(1-p\) such that

\[|_{r_{k}}L-_{r_{k}}L^{()}|  4)+2(N)+2(K)+2(2)}{M}},\] \[|_{f_{k,l}}L-_{f_{k,l}}L^{()}| )+2(K)+2( D)+2(2)}{M}},\]

and

\[|_{q_{m_{m},k}}L-_{q_{m_{m},k}}L^{()} |)+2(N)+2(K)+2(2)}{ M}}.\]

## Appendix F Additional experiments

### Node classification

**Setup.** We evaluate ICG-NN and ICG\({}_{u}\)-NN on the non-sparse graphs tookers , squirrel  and twitch-gamers . We follow the 10 data splits of Platonov et al.  for tolokers, the 10 data splits of Pei et al. , Li et al.  for squirrel and the 5 data splits of Lim et al. , Li et al.  for twitch-gamers. We report the mean ROC AUC and standard deviation for tolokers and the accuracy and standard deviation for squirrel and twitch-gamers. We also evaluate the relative Frobenius error between the graph and the ICG as a measure of the quality of the ICG approximation.

The baselines MLP, GCN , GAT , H\({}_{2}\)GCN , GPR-GNN , LINKX , GloGNN  are taken from Platonov et al.  for tolokers and from Li et al.  for squirrel, twitch-gamers and penn94. We use the Adam optimizer and report all hyperparameters in Appendix I.

**Results.** All results are reported in Table 2. Observe that ICG-NNs achieve state-of-the-art results across the board, despite the low ratio of edge (graph densities of \(2.41 10^{-4}\) to \(8.02 10^{-3}\)). ICG-NNs surpass the performance of more complex GNN models such as GT, LINKX and GloGNN, which solidify ICG-NNs as a strong method and a possible contender to message-passing neural networks.

### Comparison with graph coarsening methods over large graphs

In this section we highlight the main differences and similarities between graph coarsening methods and ICG-NNs, followed by an empirical comparison over large graphs.

#### f.2.1 Conceptual comparison

Graph coarsening methods , graph condensation methods  and graph summarization methods  replace the original graph by one which has fewer nodes, with different features and a different topological structure, while sometimes preserving certain properties, such as the degree distribution .

**Theoretical guarantees.** Graph coarsening methods do not typically stem from theoretical guarantees, whereas ICG-NNs provably approximate the graph.

**Computational complexity.** Graph coarsening methods usually account for the graph's structure by apply message-passing on the coarsened graph. Condensation methods require \((EM)\) operationsto construct a smaller graph [26; 67; 56], where \(E\) is the number of edges of the original graph and \(M\) is the number of nodes of the condensed graph. Conversely, ICGs estimate the ICG with \((K^{2}N+KE)\) operations, where \(K\) is typically smaller than \(M\).

**Node processing.** Graph coarsening methods process representations on an iteratively coarsened graph, while ICG-NN also process the fine node information at every layer.

**Graphs larger than memory.** ICG-NNs offer a subgraph sampling approach when the original graph cannot fit in memory. In contrast, the aforementioned methods lack a strategy for managing smaller data structures when computing the compressed graph.

#### f.2.2 Empirical comparison

**Setup.** We evaluate ICG-NN and ICG\({}_{}\)-NN on the large graph datasets Flickr  and Reddit . We follow the splits of  and report the accuracy and standard deviation.

The standard graph coarsening baselines Coarsening , Random , Herding , K-Center , One-Step  and the graph condensation baselines DC-Graph , GCOND , SFGC  and GC-SNTK  are taken from [67; 56]. We use the Adam optimizaer and report all hyperparameters in Appendix I.

**Results.** ICG-NNs present state-of-the-art performance in Table 3 when compared to a vareity of both graph coarsening methods and graph condensation methods, further solidifying its effectiveness.

    & tolokers & squirrel & twitch-gamers \\ \# nodes & 11758 & 5201 & 168114 \\ \# edges & 519000 & 217073 & 6797557 \\ avg. degree & 88.28 & 41.71 & 40.43 \\ relative Prob. & 0.69 & 0.39 & 0.8 \\  MLP & 72.95 \(\) 1.06 & 28.77 \(\) 1.56 & 60.92 \(\) 0.07 \\ GCN & 83.64 \(\) 0.67 & 53.43 \(\) 2.01 & 62.18 \(\) 0.26 \\ GAT & 83.70 \(\) 0.47 & 40.72 \(\) 1.55 & 59.89 \(\) 4.12 \\ GT & 83.23 \(\) 0.64 & - & - \\ H\({}_{2}\)GCN & 73.35 \(\) 1.01 & 35.70 \(\) 1.00 & OOM \\ GPR-GNN & 72.94 \(\) 0.97 & 34.63 \(\) 1.22 & 61.89 \(\) 0.29 \\ LINKX & - & 61.81 \(\) 1.80 & 66.06 \(\) 0.19 \\ GloGNN & 73.39 \(\) 1.17 & 57.88 \(\) 1.76 & 66.34 \(\) 0.29 \\  ICG-NN & 83.73 \(\) 0.78 & 58.48 \(\) 1.77 & 65.27 \(\) 0.82 \\ ICG\({}_{}\)-NN & 83.51 \(\) 0.52 & 64.02 \(\) 1.67 & 66.08 \(\) 0.74 \\   

Table 2: Results on large graphs. Top three models are colored by First, Second, Third.

    & Reddit & Flickr \\ \# nodes & 232965 & 89250 \\ \# edges & 11606919 & 899756 \\ avg. degree & 49.82 & 10.08 \\  Coarsening & 47.4 \(\) 0.9 & 44.6 \(\) 0.1 \\ Random & 66.3 \(\) 1.9 & 44.6 \(\) 0.2 \\ Herding & 71.0 \(\) 1.6 & 44.4 \(\) 0.6 \\ K-Center & 58.5 \(\) 2.1 & 44.1 \(\) 0.4 \\ One-Step & - & 45.4 \(\) 0.3 \\  DC-Graph & 90.5 \(\) 1.2 & 45.8 \(\) 0.1 \\ GCOND & 90.1 \(\) 0.5 & 47.1 \(\) 0.1 \\ SFGC & 89.9 \(\) 0.4 & 47.1 \(\) 0.1 \\ GC-SNTK & - & 46.6 \(\) 0.2 \\  ICG-NN & 89.6 \(\) 1.2 & 50.4 \(\) 0.1 \\ ICG\({}_{}\)-NN & 93.6 \(\) 1.2 & 52.7 \(\) 0.1 \\   

Table 3: Comparison with graph coarsening methods over large graphs. Top three models are colored by First, Second, Third.

### Node classification on large graphs using Subgraph SGD

**Setup.** We evaluate ICG-NN and ICG\({}_{}\)-NN on the large graph Flickr . We follow the splits of  and report the accuracy and standard deviation. We set the ratio of condensation \(r=\) to \(1\%\), where \(N\) and \(M\) are the number of nodes in the graph and the number of sampled nodes.

The graph coarsening baselines Coarsening , Random , Herding , K-Center , One-Step  and the graph condensation baselines DC-Graph , GCOND , SFGC  and GC-SNTK  are taken from . We use the Adam optimizaer and report all hyperparameters in Appendix I.

**Results.** Table 4 shows that ICG\({}_{}\)-NN with subgraph ICGm with 1% sampling rate, achieves competitive performance with coarsening and condensation methods that operate on the full graph in memory.

### Ablation study over the number of communities

**Setup.** We evaluate ICG-NN and ICG\({}_{}\)-NN on non-sparse graphs tobakers  and squirrel . We follow the 10 data splits of Platonov et al.  for tobakers and Pei et al. , Li et al.  for squirrel. We report the the mean ROC AUC and standard deviation for tobakers and the accuracy and standard deviation for squirrel.

**Results.** Figure 4 shows several key trends across both datasets. First, as anticipated, when a very small number of communities is used, the ICG fails to approximate the graph accurately, resulting in degraded performance. Second, the optimal performance is observed when using a relatively small number of communities, specifically 25 for tobakers and 75 for squirrel. Lastly, using a large number of communities does not appear to negatively impact performance.

### The cut-norm and Frobenius norm

While the cut norm is the theoretical target of the ICG approximation, it is expensive to estimate directly. However, Theorem 3.1 implies that optimizing the Frobenius norm, which is computationally inexpensive, leads to a small cut norm.

In this experiment, we first aim to demonstrate the positive correlation between the Frobenius error and the cut norm error. Moreover, we show that even though the Frobenius error cannot be made close to zero in general, it is still correlated with cut norm. We show that when the relative Frobenius

   Model & Accuracy \\  Coarsening & 44.6 \(\) 0.1 \\ Random & 44.6 \(\) 0.2 \\ Herding & 44.4 \(\) 0.6 \\ K-Center & 44.1 \(\) 0.4 \\ One-Step & 45.4 \(\) 0.3 \\  DC-Graph & 45.8 \(\) 0.1 \\ GCOND & 47.1 \(\) 0.1 \\ SFGC & 47.1 \(\) 0.1 \\ GC-SNTK & 46.5 \(\) 0.2 \\  ICG-NN & 50.4 \(\) 0.1 \\ ICG\({}_{}\)-NN & 50.8 \(\) 0.1 \\   

Table 4: Results on Flickr using 1% node sampling. Top three models are colored by First, Second, Third.

Figure 4: Test ROC AUC of tobakers (**left**) and test accuracy of squirrel (**right**) as a function of the number of communities.

error is reduced below a specified threshold (not close to zero in general), the ICG approximation allows ICG-NNs to achieve state-of-the-art results.

**Setup.** We evaluate our ICG approximation on non-sparse graphs tookers  and squirrel . We vary the number of communities used by the approximation and report the relative cut-norm (estimated by ) as a function of the relative Frobenius norm. We also present the resulting relative Frobenius error for our node classification experiments in Table 2.

**Results.** As expected, the cut-norm and Frobenius norm are positively correlated, both decreasing as more communities are used, as shown in Figure 5. We also observe in Table 2 that ICG-NNs perform best on the squirrel dataset and worst on twitch-gamers, aligning with their relative Frobenius error. The lower the relative Frobenius error, the more accurate the ICG approximation, leading to better performance. Table 2 and Figure 5 also revealed a useful rule of thumb: when the relative Frobenius error is below 0.8, it correlates with a very small cut norm error. This, in turn, leads to an accurate ICG approximation, enabling ICG-NNs to achieve state-of-the-art results.

### Initialization

We repeated the node classification experiments presented in Table 2, using random initialization when optimizing the ICG, and compared the results to those obtained with eigenvector initialization.

**Setup.** We evaluate ICG-NN and ICG\({}_{}\)-NN on the non-sparse graphs tookers , squirrel  and twitch-gamers . We follow the 10 data splits of Platonov et al.  for tolokers, the 10 data splits of Pei et al. , Li et al.  for squirrel and the 5 data splits of Lim et al. , Li et al.  for twitch-gamers. We report the mean ROC AUC and standard deviation for tolokers and the accuracy and standard deviation for squirrel and twitch-gamers.

**Results.** Table 5 indicates that eigenvector initialization generally outperforms random initialization across all datasets. While the improvements are minimal in some cases, such as with the tolokers and squirrel datasets, they are more pronounced in the twitch-gamers dataset.

Additionally, eigenvector initialization offers a practical advantage in terms of training efficiency. On average, achieving the same loss value requires 5% more time when using random initialization

Figure 5: Cut-norm as a function of Frobenius norm on the tolokers (**left**) and squirrel (**right**) datasets. The number of communities used is indicated next to each point.

    & tolokers & squirrel & twitch-gamers \\ \# nodes & 11758 & 5201 & 168114 \\ \# edges & 519000 & 217073 & 6797557 \\ avg. degree & 88.28 & 41.71 & 40.43 \\  random init. & 83.30 \(\) 0.92 & 62.03 \(\) 0.98 & 64.73 \(\) 0.44 \\ eigenvector init. & 83.31 \(\) 0.64 & 62.10 \(\) 1.67 & 66.10 \(\) 0.42 \\   

Table 5: Time until convergence in seconds on large graphs.

compared to eigenvector initialization. This efficiency gain further supports the utility of the proposed initialization method.

### Run-time analysis

In this subsection, we compare the forward pass run times of our ICG approximation process, signal processing pipeline (ICG\({}_{}\)-NN) and the GCN  architecture. We sample graphs of sizes up to 7,000 from a dense and a sparse _Erd\(\)s-Renyi_ distribution \((n,p(n)=0.5)\) and \((n,p(n)=)\), correspondingly. Node features are independently drawn from \(U\) and the initial feature dimension is \(128\). ICG\({}_{}\)-NN and GCN use a hidden dimension of \(128\), \(3\) layers and an output dimension of \(5\).

**Results.** Figure 6 demonstrates a strong linear relationship between the runtime of our ICG approximation and that of the message-passing neural network GCN for both sparse and dense graphs. Unlike GCN, our ICG approximation is versatile for multiple tasks and requires minimal hyperparameter tuning, which reduces its overall time complexity. Additionally, Appendix F.7 reveals a strong square root relationship between the runtime of ICG\({}_{}\)-NN and the runtime of GCN for both sparse and dense graphs. This aligns with our expectations, as the time complexity of GCN is \((E)\), while the time complexity of ICG-NNs is \((N)\), highlighting the computational advantage of using ICGs over message-passing neural networks.

### Time until convergence

**Setup.** We measured the average time until convergence in seconds for the GCN and ICG\({}_{}\)-NN architectures on the non-sparse graphs tolokers , squirrel  and twitch-gamers . We follow

Figure 6: Empirical runtimes: K-ICG approximation process as a function of GCN forward pass duration on the dense (**left**) and sparse (**right**) _Erd\(\)s-Renyi_ distribution \((n,p(n)=0.5)\) and \((n,p(n)=)\) for K=10, 100.

Figure 7: Empirical runtimes: K-ICG\({}_{}\)-NN as a function of GCN forward pass duration on the dense (**left**) and sparse (**right**) _Erd\(\)s-Renyi_ distribution \((n,p(n)=0.5)\) and \((n,p(n)=)\) for K=10, 100.

the 10 data splits of Platonov et al.  for tolokers, the 10 data splits of Pei et al. , Li et al.  for squirrel and the 5 data splits of Lim et al. , Li et al.  for twitch-gamers. We ended the training after 50 epochs in which the validation metric did not improve. We set the hyper-parameters with best validation results.

**Results.** Table 6 shows that ICG\({}_{}\)-NN consistently converges faster than GCN across all three benchmarks. Specifically, converges approximately 15 times faster on the tolokers dataset, 17 times faster on the squirrel dataset, and 25 times faster on the twitch-gamers dataset compared to GCN, indicating a significant improvement in efficiency.

### Memory allocation analysis

In this subsection, we compare the memory allocated during the ICG approximation process and during a forward pass of the GCN  architecture. We sample graphs of sizes up to 2,000 from a dense _Erdos-Renyi_ distribution \((n,p(n)=0.5)\). Node features are independently drawn from \(U\) and the initial feature dimension is \(128\). ICG\({}_{}\)-NN and GCN use a hidden dimension of \(128\), \(3\) layers and an output dimension of \(5\).

**Results.** Appendix F.9 demonstrates a strong linear relationship between the the memory allocated during the ICG approximation process and during a forward pass of the message-passing neural network GCN. This aligns with our expectations: the memory allocation for the GCN architecture is \((ED)\), and of ICG-NNs \((EK+NKD)\), where \(D\) is the feature dimension and \(K\) is the number of communities used for the ICG approximation.

    & tolokers & squirrel & twitch-gamers \\  GCN & 510 & 3790 & 1220 \\ ICG\({}_{}\)-NN & 33 & 225 & 49 \\   

Table 6: Time until convergence in seconds on large graphs.

Figure 8: Memory allocated for K-ICG approximation (K=10,100) as a function of the memory allocated for GCN on graphs \(G(n,p(n)=0.5)\).

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_EMPTY:33]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We prove our proposed constructive version of the Weak Graph Regularity Lemma. We use the construction to then develop a new pipeline for learning on large non-sparse graphs efficiently, following what is written in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We discuss the model's limitations in the conclusions section. Primarily, our approach is constrained to the single, relatively large, undirected graph setting. Consequently, this restricts the number of datasets we can experiment on and, by extension, limits the scope of our experimental analysis. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The appendix contains the complete proofs for all of our theoretical results, along with the full set of assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: The main paper details the models, datasets, data splits, and optimizer used. Additionally, the appendix includes the complete hyperparameter grids tested for each experiment and any other relevant implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a URL to our reproducible repository, which includes detailed operation instructions. Additionally, the appendix contains the complete hyperparameter grids tested for each experiment and any other relevant implementation details. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. ** The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We follow the standard train and test splits and optimizer. The train/test splits and optimizer are detailed in Section 6 and are present in our repository. The appendix contains the complete hyperparameter grids tested for each experiment and any other relevant implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We include standard deviation error bars in our figures and tables. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We use a single NVidia L40 GPU (with a standard memory and time of execution) in all of our experiments. This is reported in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper proposes a general purpose computational pipeline for efficient learning on large non-sparse graphs, focusing mainly on theoretical/mathematical results, e.g., a new constructive version of the Weak Graph Regularity Lemma,. We believe there are no societal consequences of our work that require specific highlighting here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the owners of each dataset and state the version and splits that are used in Section 6. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.