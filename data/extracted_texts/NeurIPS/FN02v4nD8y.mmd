# Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark

Brianna M. Karpowicz\({}^{1,2}\) Joel Ye\({}^{3}\)1

Chaofei Fan\({}^{4}\) Pablo Tostado-Marcos\({}^{5}\)

Fabio Rizzoglio\({}^{6}\) Clay Washington\({}^{1,2}\) Thiago Scodeler\({}^{7}\) Diogo de Lucena\({}^{7}\)

Samuel R. Nason-Tomaszewski\({}^{1,2}\) Matthew J. Mender\({}^{8}\) Xuan Ma\({}^{6}\)

Ezequiel Matias Arneodo\({}^{5,9}\) Leigh R. Hochberg\({}^{10-12}\) Cynthia A. Chestek\({}^{8}\)

Jaimie M. Henderson\({}^{4}\) Timothy Q. Gentner\({}^{5}\) Vikash Gilja\({}^{5}\) Lee E. Miller\({}^{6}\)

Adam G. Rouse\({}^{13}\) Robert A. Gaunt\({}^{14}\) Jennifer L. Collinger\({}^{3,14}\) Chethan Pandarinath\({}^{1,2}\)

\({}^{1}\)Emory University \({}^{2}\)Georgia Tech \({}^{3}\)Carnegie Mellon University \({}^{4}\)Stanford University

\({}^{5}\) University of California San Diego \({}^{6}\) Northwestern University \({}^{7}\) Agency Enterprise Studios

\({}^{8}\) University of Michigan \({}^{9}\) Instituto de Fisica La Plata \({}^{10}\) Harvard Medical School

\({}^{11}\) Department of Veterans Affairs \({}^{12}\) Brown University

\({}^{13}\) University of Kansas Medical Center \({}^{14}\) University of Pittsburgh

Equal contributions. Correspondence to \({}^{}\)chethan@gatech.edu

###### Abstract

Intracortical brain-computer interfaces (iBCIs) can restore movement and communication abilities to individuals with paralysis by decoding their intended behavior from neural activity recorded with an implanted device. While this activity yields high-performance decoding over short timescales, neural data are often nonstationary, which can lead to decoder failure if not accounted for. To maintain performance, users must frequently recalibrate decoders, which requires the arduous collection of new neural and behavioral data. Aiming to reduce this burden, several approaches have been developed that either limit recalibration data requirements (few-shot approaches) or eliminate explicit recalibration entirely (zero-shot approaches). However, progress is limited by a lack of standardized datasets and comparison metrics, causing methods to be compared in an ad hoc manner. Here we introduce the FALCON benchmark suite (Few-shot Algorithms for COnsistent Neural decoding) to standardize evaluation of iBCI robustness. FALCON curates five datasets of neural and behavioral data that span movement and communication tasks to focus on behaviors of interest to modern-day iBCIs. Each dataset includes calibration data, optional few-shot recalibration data, and private evaluation data. We implement a flexible evaluation platform which only requires user-submitted code to return behavioral predictions on unseen data. We also seed the benchmark by applying baseline methods spanning several classes of possible approaches. FALCON aims to provide rigorous selection criteria for robust iBCI decoders, easing their translation to real-world devices. [https://snel-repo.github.io/falcon/](https://snel-repo.github.io/falcon/)

## 1 Introduction

Brain-computer interfaces (BCIs) provide a path to restore movement and communication in individuals with paralysis by decoding neuronal population activity to uncover the user's intention. BCIs have recently achieved many promising demonstrations, including high degree of freedom robot arm control , computer use and communication , and speech decoding . A specific class of BCIs known as intracortical BCIs (iBCIs) have enabled many of these impressive technological feats. However, many of these demonstrations have required decoders to be recalibrated daily oreven more frequently, interrupting device use and burdening the user. Real-world iBCI deployment will require maintaining high performance over long time periods with minimal recalibration. The challenge here stems from nonstationarities in the neural data that are caused by many factors acting at multiple timescales, such as shifts in the position of the electrode relative to surrounding tissue, changes in tissue properties in response to electrode implantation, electrode malfunction, or neural plasticity [14; 15]. These nonstationarities result in a changing relationship between neural data and behavior, necessitating frequent decoder recalibration to maintain high performance.

Fortunately, despite these nonstationarities, there are many potential ways to leverage structure in neural or behavioral data to help reduce the burden of recalibration . For example, while the spiking activity recorded on an individual iBCI electrode can change over timescales of hours, neural population activity contains low dimensional structure (manifolds) that shows a consistent relationship with behavior over months to years [17; 18; 19]. Realignment methods that exploit these conserved manifolds [20; 21; 22; 23; 24; 25; 26]can restore decoding without explicit calibration periods. Alternatively, rather than focusing on structure intrinsic to the neural data, another set of approaches attempts to achieve robustness by continually recalibrating decoders using the retrospective analysis of data collected during the subject's normal use of the iBCI [27; 28; 29]. A third strategy has centered on supervised deep network training using many sessions to yield decoders that are robust to session-to-session variability [30; 31], which may be extended further to potentially yield universal iBCI decoders that generalize to new subjects or tasks [32; 33; 34]. These diverse and potentially complementary efforts converge around a single problem statement: the real-world iBCI decoding challenge is to maintain high performance on distinct, but related, data distributions, with minimal data from the new setting.

While these diverse approaches have thus far used their own ad-hoc evaluation, standardization could enable rigorous comparison to assess real-world potential and highlight advances upon which future efforts can build. We propose the FALCON benchmark, **F**ew-Shot **A**lgorithms for **C**onsistent **Neural Decoding, as a common evaluation for stable, long-term decoding performance. FALCON releases 5 multi-session datasets that span movement and communication tasks relevant to iBCIs: human and monkey reach and grasp behavior (H1, M1), monkey finger movement (H2), human handwriting (H2), and birdsong (B1). These datasets are divided into held-in and held-out sessions. To evaluate how well few-shot decoders advance iBCI robustness given real-world data constraints, only a small amount of supervised data is released from held-out sessions. Approaches for the more challenging settings of only using neural data on new sessions (unsupervised), or no data from new sessions (zero-shot), can also be evaluated using the same data splits. This report describes the design of the benchmark, its datasets, and the performance of baseline models. By introducing FALCON, we aim to establish standardized evaluation practices for robust iBCI decoding approaches that can provide researchers with metrics to select methods for in-device use.

### Related work

**Benchmarks of BCI Decoding.** FALCON evaluates iBCI decoding, or the prediction of intention from neural activity. To date, benchmarks of decoding have been uncommon compared to other fields using machine learning. The early BCI competition series  and more recent additions of the International BCI Competition  and MOABB  evaluated decoding in offline (i.e., pre-recorded) noninvasive neural datasets in multiple subjects and highlighted several of the challenges faced in iBCI datasets (multi-session transfer, removal of repeated data structure). More recently, the Brain2Text decoding benchmark  evaluates speech decoding in human iBCIs. However, absent a strong benchmarking culture, models across intracortical and noninvasive neural recording modalities [32; 33; 34; 397; 398; 40; 41; 42] are still often evaluated on different public or private datasets. This lack of standardization makes comparison across works difficult due to subjectivity in preprocessing, metric choice, and evaluation design.

**Benchmarks on Neural Data.** Benchmarks for neural data analysis are related but differently motivated than decoding benchmarks. BrainScore  evaluates the ability of models to predict brain data when trained on non-neural data tasks. The Sensorium  and Algonauts  challenges evaluate encoding models that predict brain activity of mouse visual cortex and human fMRI, respectively, given visual stimuli. The Neural Latents Benchmark (NLB)  evaluates latent variable models on spiking activity from different brain areas of monkeys. While the NLB has a decoding metric, this metric is computed with ridge regression on inferred latent variables and is not treated as a primary endpoint. FALCON directly evaluates decoding, allows more flexibility in decoder architecture, and more closely aligns with the goal of evaluating the quality of iBCI decoders.

## 2 Benchmark evaluation pipeline and metrics

### Evaluation strategy and pipeline

FALCON evaluates behavioral decoding from iBCI neural activity in five datasets. Each dataset comprises multiple sessions of data divided into two contiguous splits: held-in and held-out (**Fig. 1a**). As in standard decoder calibration, held-in sessions provide sufficient data to train a high-performing decoder; held-out sessions are prepared for evaluating few-shot decoder performance and therefore include insufficient data to prepare decoders from scratch ( **Fig. 1a,b**). All datasets provide multi-unit threshold crossings (detected voltage deflections caused by nearby neuron action potentials) recorded from intracortical electrodes and behavioral data (specified per task). An evaluation split of the same length is withheld from both held-in and held-out. All remaining data is released for held-in sessions while a small fraction of data is released for held-out sessions. Note that the held-in split provides a standard-data regime iBCI decoding benchmark, but FALCON focuses on few-shot decoding performance in the held-out split.

Submitted decoders are executables that implement an iBCI prediction interface. The evaluation server (EvalAI ) requires causal, open-loop predictions to be made on streaming neural data, timestep-by-timestep. The communication datasets make predictions on coarser timescales (per sentence for H2, per song motif for B1). These formats mimic current iBCI use for their respective tasks. Lack of trial structure in movement tasks is an important training time consideration; decoders trained on trialized data can degrade significantly when evaluated continuously (Section A.5.1). We note that an important limitation of FALCON is that evaluation may be susceptible to promoting models that exploit trial structure implicit in the datasets, even if this does not benefit iBCI control .

Figure 1: **FALCON Evaluation Design.****(a)** Top: BCI decoders are prepared by collecting calibration data where a user attempts to perform a cued behavior. This process yields paired examples of behavioral outputs and associated neural data. Bottom: Current practice requires new calibration data to train new decoders. High decoding performance may require substantial data, motivating methods for few-shot decoding. FALCON provides full “held-in” sessions and evaluates few-shot decoding on “held-out” sessions. **(b)** Each session in a FALCON dataset contains multiunit threshold crossings and behavioral data (which are discrete sentences in H2 and continuous covariates otherwise). Evaluation data is withheld from all sessions. All remaining data is released publicly for held-in sessions. Only a small fraction of data is released for held-out sessions. **(c)** FALCON’s design enables comparison of different approaches for consistent decoding. Zero-shot methods use no data from held-out sessions, few-shot methods use the calibration splits from held-out sessions, and test-time adaptive methods can implement behavior-free, unsupervised decoder updates during evaluation.

### Supported approaches and benchmark scope

FALCON allows methods with varying data assumptions to be evaluated in a common setting (**Fig. 0(c)**). Decreased data use provides greater reduction of user burden, but can be more challenging.

**Zero-shot** methods directly predict behavior on new sessions with fixed model parameters. This typically requires using deep networks that train on many sessions (e.g. months) of data. Such methods have enabled high performance cursor control on new days for months into the future [30; 31]. Recent efforts exploring subject generalization [49; 50; 34] and neural data foundation models [32; 33] may alleviate the burden of large scale data collection on individual users. However, as current multi-session zero-shot methods impose large data collection burdens on the user and may still degrade after long-term use, there is a practical need to explore adaptive methods as well.

**Few-shot supervised** methods assume the collection of limited calibration data for every session of use. For people with paralysis, a high-performance iBCI that requires a short calibration procedure before use may still confer a large advantage over other assistive technologies. Current supervised deep networks typically adapt to short calibration blocks through fine-tuning of a pretrained model [32; 33; 34]. Few-shot supervision is the least strict setting that can be evaluated with FALCON that still reduces user burden, for which the highest performance is expected.

**Few-shot unsupervised** methods remove the need for behavioral data on new days, skipping explicit calibration periods by allowing recalibration procedures to be performed using only neural data from normal iBCI use. Due to their lack of reliance on behavioral data, unsupervised approaches are not subject to problems that may arise from behavioral labels, which may be difficult to obtain during iBCI use when guessing a user's intent post-hoc can be unreliable. Unsupervised methods typically assume that the neural activity has an underlying manifold which maintains a stable relationship to behavior over long periods of time [20; 21; 22; 23; 24; 25]. However, the specific context of iBCI use, such as strategy or posture, may lead to a change in the manifold-to-behavior mapping and violate this assumption. FALCON's datasets are drawn from consistent behavior across days, though due to behavioral complexity, not all behavioral conditions will be sampled in the few-shot calibration data.

**Test-time adaptation** generally leverages behavioral priors to provide model labels on unlabeled data. Currently proposed test-time adaptation methods avoid the collection of any calibration data on test days. Instead, these methods use neural data and inferred behavioral labels collected during normal iBCI use to perform "semi-supervised" decoder recalibration. However, these methods have only been demonstrated for two-dimensional cursor use and language communication [27; 28; 29], suggesting open challenges for broad behavioral domains, such as in FALCON's movement datasets.

### Metrics

Each dataset uses a standard decoding metric. The movement tasks (M1, M2, H1) require predictions of multi-dimensional motor covariates, such as muscle activity. For these tasks, accuracy is reported using the coefficient of determination (\(R^{2}\)), computed as a variance-weighted average across the \(R^{2}\) of individual motor covariates. \(R^{2}\) is useful for interpreting low-dimensional predictions as a constant mean prediction achieves an \(R^{2}\) of 0 and max \(R^{2}\) is 1. The handwriting task (H2) requires prediction of English characters from a corpus of common sentences; we use word error rate (WER) as a metric, computed as the edit distance between the predicted and expected sequence divided by the length of the intended sequence. Birdsong decoding (B1) reports performance as mean squared error (MSE) on the predicted spectrogram; MSE is preferable for evaluating spectrogram predictions as the predictions are much higher-dimensional than in movement tasks. Metrics are computed per session, and across-session mean and standard deviation are reported on EvalAI. Mean and standard deviation are computed separately for held-in and held-out splits.

## 3 Datasets

FALCON aims to provide a comprehensive evaluation of few-shot decoding across contemporary iBCI applications. FALCON datasets span two primary groups of tasks: movement and communication (**Fig. 2**). FALCON's movement datasets have either kinematic or muscle outputs, and the communication datasets have either text or vocal outputs. Because most human iBCI study participants have limited independent movement, human behavioral data are those that the researcher asked the participant to attempt or imagine, while animal behavioral data are recorded from physicalactions. All datasets contain electrophysiological voltage recordings collected from intracortical microelectrodes. We extract threshold crossings from the recorded voltages to yield spiking activity, as is standard practice for iBCIs . Detailed descriptions of each dataset and their locations can be found in Section A.3.

While the ultimate goal of some iBCI research is applications in humans, we provide animal datasets because animal models are essential to develop iBCI applications and for basic scientific discovery . Using both animal and human data also improves the likelihood of finding models with broad effectiveness, as levels of instability are likely to vary across subjects and species .

**Mi: Monkey reach and grasp.** The M1 dataset consists of recordings using Floating Microelectrode Arrays (Microprobes), implanted in the precentral gyrus while two monkeys (M1-A and M1-B) reached to, grasped, and manipulated an object in a specific location (4 possible objects, 8 possible locations) . Intramuscular electromyography (EMG) was recorded from 16 muscles in the right hand and upper extremity. The large number of object/location combinations leads to a wide variety of muscle activations. Unlike higher-level behavioral variables (such as robotic arm endpoint velocities), EMG is a directly measurable output of the motor nervous system, and thus provides a signal that should have a close correspondence to neural activity on a moment-by-moment basis. EMG is also directly relevant to iBCIs that combine with functional electrical stimulation to control paralyzed limbs . Monkey EMG is interesting to iBCI research as human iBCI users with paralysis have limited muscle control and likely lack the ability to produce EMG decoding targets; recent works have proposed cross-species transfer to exploit monkey EMG data for iBCI applications .

**M2: Monkey finger movements.** The M2 dataset consists of Utah array recordings from the precentral gyrus while a monkey made finger movements to control a virtual hand to acquire cued target positions . Finger actuation ranged from full extension to full flexion with cued movements focusing on the index finger and/or the middle-ring-small (MRS) finger group. The goal of including M2 in the FALCON benchmark is to develop methods that accurately predict individuated finger movements over time. Finger control is a critical aspect of dexterous hand function and is a key target for iBCI control that aims to restore upper limb and hand function to individuals. Recent work has shown that the encoding of finger behaviors in motor cortex may be compositional ; yet, the implications of this finding on iBCI control and decoding stability are unclear.

**Hi: Human robotic effector.** The H1 dataset contains Utah array recordings from the hand and arm motor cortex of a human iBCI participant, collected in a long-term clinical study on iBCIs for sensorimotor control. The participant was cued to attempt to reach and grasp with their right

Figure 2: **FALCON datasets span iBCI use cases.** For each column, _top_: task schematic; _middle_: neural activity for all channels over time; _bottom_: example behavioral outputs. Each panel includes a vertical timeline denoting held-in (gray) and held-out (teal) sessions in the dataset. Ticks mark individual sessions, colored vertical bars indicate time elapsed within or between splits. **Tasks:** Movement datasets (mechanical arm) include: monkey reach-to-grasp (M1, 2 monkeys (A/B), 64/96 channels neural data, 16 channels muscle activity), monkey finger movements (M2, 96 channels neural data, 2-dimensional finger movements), and human robotic effector (H1, 172 channels neural data, 7-dimensional hand and arm velocity outputs). Communication tasks (speaking head) include human handwriting (H2, 192 channels neural data) and songbird vocalization (81, 85 channels neural data).

hand. This data was used to calibrate an iBCI for control of a robotic arm in a 7 degree-of-freedom task [1; 63; 64]. These data are open loop, meaning that the participant attempted cued movements but was not directly controlling the output and could not correct errors in real-time. H1 contains a breadth of combinations of robotic arm command variables (3D limb kinematics, 1D rotation, 3D grasp shape) that are often decoding targets for iBCIs. High-dimensional control is particularly burdensome to calibrate, as the large number of possible endpoints demands calibration procedures that are often several minutes long . Developing methods to improve the efficiency of calibration to novel sessions would advance the practical viability of using iBCIs for high-dimensional control.

**h2: Human handwriting.** The H2 dataset contains neural activity recorded using Utah arrays placed in the "hand knob" area of the dorsal motor cortex of a human iBCI participant, collected as part of the BrainGate2 Clinical Trial. The participant was asked to copy a sentence by attempting to write each letter individually . The H2 dataset falls in the domain of brain-to-text BCIs, which aim to restore communication capabilities. Decoders for this task need to accurately predict the intended character as well as determine when that character was intended to be written, as the task is fully self-paced. This task is therefore not amenable to traditional linear decoders and will require more sophisticated approaches, most canonically RNN decoders with a Connectionist Temporal Classification loss [5; 10; 11; 29]. Additionally, due to the goal of predicting words or sentences, communication iBCIs often use large language models to further refine predictions or build stable decoders [10; 29].

**b1: Songbird vocalization.** The B1 dataset features neural recordings from a zebra finch songbird using Neuropixels 1.0 probes  implanted in the motor brain region robust nucleus of the arcopallium (RA). Alongside neural activity, this dataset includes simultaneous free-behavior audio recordings during awake-singing. Songbird neuroanatomy and vocal behavior have direct parallels to human speech , thereby offering a valuable model for exploring neurally-driven speech synthesis applications. The B1 dataset presents a unique challenge for stable decoding approaches. Neuropixels probes may exhibit nonstationarities that vary significantly in type and timescale compared to traditional microelectrode arrays. Given vocal ground-truth, decoders designed for B1 aim to synthesize high-fidelity continuous amplitude waveforms or spectrogram representations of vocal output. This strategy may better preserve the prosodic elements in reconstructed vocalizations, a significant challenge inherent to current brain-to-text approaches. By developing stable birdsong decoding strategies, we aim to establish baseline methods that can be adapted to human brain-to-speech iBCIs.

## 4 Results

We seed FALCON with representative approaches to provide an initial characterization of the stability challenge. Implementation details on all baselines are provided in Section A.4. For all datasets, we provide standard decoders applied to held-out sessions in two ways: (1) trained in a many-shot manner using redacted data ("oracle" decoders) approximately upper-bound performance and (2) applied zero-shot ("static" decoders) to lower-bound performance. For motor datasets (M1, M2, H1), we fit a Wiener Filter (WF; ridge regression with history) on inferred neural firing rates derived from an exponential spike smoothing kernel (see Section A.4.1). A single-session WF is a simple but effective baseline for offline decoding from high quality spiking activity and is a representative default method for closed loop control. We also train a single-session recurrent neural network (RNN) and a multi-session Neural Data Transformer (NDT2 Multi ) to establish the performance of higher capacity nonlinear models. For the human handwriting dataset H2, we provide an RNN trained on multiple sessions to predict English letters from neural activity, and a second RNN that additionally uses language models (LMs) as priors to correct RNN outputs and improve accuracy. On B1, we apply the EnSongdec decoder  which predicts song embeddings from spiking data using a feedforward network and synthesizes them into continuous birdsong using a pretrained EnCodec model .

We also sample state-of-the-art methods for robust decoding to demonstrate how existing approaches perform on FALCON datasets. For movement datasets, we provide two deep-network-based unsupervised few-shot alignment approaches, Nonlinear Manifold Alignment with Dynamics (NoMAD)  and Cycle-consistent Generative Adversarial Network (CycleGAN) . Similar to neural latent variable models , NoMAD and CycleGAN use an RNN and an MLP, respectively, to infer neural firing rates through a Poisson firing rate model. These methods then apply distributional alignment to match inferred neural firing distributions on held-out sessions to those of held-in sessions. Different single-session models are trained to provide the different held-in scores. Additionally, we train NDT2Multi models that only use calibration data. The h2 stability baseline is a test-time adaptive method that uses the LM-corrected outputs as pseudo-labels to iteratively recalibrate the RNN (Continual Online Recalibration with Pseudo-labels; CORP ). As vocalization decoding has seen limited development of specific decoder stabilization approaches, we pose B1 as an open question and solicit potential solutions from FALCON submissions.

### FALCON datasets exhibit unstable decoding performance across sessions.

We first show that FALCON datasets exhibit qualitative nonstationarities, reflecting the challenges faced in iBCI use. **Figure 3a** shows neural spiking activity from all sessions in the M1 dataset. It is clear that neural firing exhibits different properties across sessions. We also visualize this data in 3 dimensions using principal components analysis (PCA) (**Fig. 3b**). Under a common projection, we plot average time courses for different reach directions. Directions are clearly separable in both sessions (supporting decoding), but the required decoding map changes between sessions.

Next, we quantify that each session's neural activity can provide good decoding of behavior. We train oracle decoders for each session, which use all non-evaluation data. Specifically, held-in oracle decoders use the data from the calibration split, and held-out oracle decoders use both the calibration split and the redacted data. All oracle decoders are evaluated on respective session evaluation splits. For movement datasets, oracle decoders consist of Wiener Filters with cross-validated history (WFs) or single-session RNN models (**Fig. 3c-e**, blue/red). For h2, the oracle decoder is an RNN trained to predict letters from neural data, trained jointly on all held-in calibration splits and incrementally with each session's held-out calibration and redacted splits (**Fig. 3f**, blue). For B1, we apply an EnSongdec model , which uses neural data to predict song embeddings before reconstructing song spectrograms (**3g**, blue). For all datasets, variability in oracle decoding performance is nontrivial but small, implying that performance drops from transferring decoders to new sessions are not due to degraded neural data or a lack of correspondence between neural and behavioral data.

Finally, we quantify decoding instabilities in each dataset with zero-shot static decoders. The specific static decoder was chosen from the held-in session oracle decoders as the highest performing on

Figure 3: **Static decoders exhibit decoding instabilities on FALCON datasets. (a)** Raster plot showing 1 minute of data for each M1-A session, separated by red vertical lines. **(b)** Neural trajectories from PCA fit on M1-A Day 0 smoothed spiking activity and applied to Day 0 and Day 23 smoothed spiking activity. Colored by reach direction. Thick lines show the average of all reaches in a given direction and thin lines indicate single reaches. **(c-e)**\(R^{2}\) of oracle decoders (RNN red, WF blue) and static decoders (RNN gray, WF black) for held-in and held-out splits for M1-A, h2, and H1. Higher values indicate more accurate performance. Downward triangles indicate points with negative \(R^{2}\) otherwise not visible on these axes. Selected static decoders are annotated with a gray or black circle. **(f)** Word error rate (WER) of oracle (blue) and static (black) decoders for H2. Lower values indicate more accurate performance. Rather than training one model per held-in session, the held-in decoder is trained using all held-in sessions (performance denoted by horizontal line). Held-out dataset performance reported as mean \(\) standard deviation across 5 random seeds. (g) Mean squared error (MSE) of oracle (blue) and static (black) EnSongdec models for B1 held-in and held-out splits. Lower values indicate more accurate performance. Static decoder chosen from held-in datasets indicated with black circle.

the evaluation split of the other held-in sessions, to approximate good generalization to held-out sessions. We apply the decoder unmodified to the held-out datasets, simulating an iBCI decoder's naive (zero-shot) performance on a new session without recalibration. All datasets showed marked decoding instability (**Fig. 3c-g**, black/gray), with drops in WF performance up to 0.28 \(R^{2}\) (M1), 0.27 \(R^{2}\) (M2), and 0.14 \(R^{2}\) (H1). RNN decoders exhibit more instability on FALCON movement datasets, with drops up to 1.94 \(R^{2}\) (M1), 0.82 \(R^{2}\) (M2) and 0.78 \(R^{2}\) (H1). Communication datasets demonstrate similar trends - error increases as much as 0.40 WER (H2) and 9.03e-4 MSE (B1).

### FALCON baselines demonstrate the difficulty of improving M1 decoder stability.

We next compare current few-shot approaches applied to M1-A in detail. On the held-in datasets, NDT2 yields the highest performance, followed by NoMAD + WF, and CycleGAN + WF (**Fig. 4a**). From held-in to held-out sessions, NDT2 dropped by at most 0.30 \(R^{2}\), NoMAD by at most 0.21 \(R^{2}\), and CycleGAN by at most 0.24 \(R^{2}\). Compared to the static WF (drop \(\) 0.28 \(R^{2}\)), the baseline approaches show at most a marginal improvement, indicating that stability challenges still affect all approaches applied to M1. Model ranking and relative performance are largely preserved across sessions, implying that averaging \(R^{2}\) across sessions summarizes performance without obscuring gains on specific sessions.

For Day 30, we also present decoded predictions for three key muscles - the biceps (BCPs), flexor carpi radialis (FCR), and extensor digitorum (EDC) - for each baseline approach. In **Fig. 4b**, each column is an individual reach for one of the location-object pairs available in the evaluation split on Day 30. Comparing the predicted EMG traces to the measured EMG traces provides context for interpreting the \(R^{2}\) numbers and understanding which features of the EMG (the baseline, the high frequency features, the magnitude) are predicted well by each method. For example, NDT2 captures more high frequency changes in the muscle activity than other methods, potentially due to its nonlinear decoding. In **Fig. 4c**, we show \(R^{2}\) values for example muscles individually. Per-muscle performance preserves the method ranking shown in **Fig. 4a**, providing further confidence that the variance-weighted \(R^{2}\) over output dimensions is sound.

### FALCON baseline performance drops from held-in to held-out datasets.

Baseline results on all datasets are shown in **Table 1**. FALCON quantifies notable performance gaps across methods. For example, within oracle decoders, which are by definition trained using the same data, increased model complexity can substantially improve decoding (M2: 0.27 vs 0.77 \(R^{2}\) WF/NDT2 Multi; H2: 0.11 vs 0.02 WER RNN Multi/+ LM). Moreover, it is unsurprising that using

Figure 4: **Baseline model predictions on M1-A dataset.****(a)** Performance (\(R^{2}\)) of each baseline model on individual held-in and held-out M1 datasets. Box indicates the dataset that will be elaborated on in later panels. **(b)** Example decoded EMG traces for each baseline approach. Three of the sixteen total muscles shown: biceps (BCPs), flexor carpi radialis (FCR), and extensor digitorum (EDC). Each column is an example trial for one object (sphere or button) and location (angle) pair. Gray traces are the measured EMG for that muscle and experimental condition, and colored traces are the EMG predicted by each decoder-stabilization method on Day 30. **(c)**\(R^{2}\) values computed for three individual muscles. Together with the remaining muscles, these values comprise the variance-weighted \(R^{2}\) presented in panel (a).

more data will provide large performance gains (ZS to FSU to FSS in movement datasets, ZS to TTA in H2). FALCON encourages the submission of novel approaches in each class of data use.

Given FALCON's flexibility to accommodate many classes of approaches, a method's held-in score may be used to contextualize its own held-out score. Oracle decoders establish the approximate variability in performance between held-in and held-out splits, which appears relatively small (e.g., max difference = 0.04 \(R^{2}\) for NDT2 Multi on H1, 0.04 WER for RNN Multi on H2). Yet, all methods show sizable gaps between held-in and held-out scores, far exceeding the expected variability. In absolute terms, all decoders perform well on held-in M (\(R^{2}\) = 0.46-0.78), but performance drops by 0.12-0.18 \(R^{2}\) on the held-out split (and the RNN has an extreme failure). M2 and H1, which show lower overall decoding performance, maintain that held-out scores are only a fraction of the potential performance indicated by held-in scores. This is also true for H2, where CORP provides a great advance over zero-shot methods but yields error on the held-out datasets that is nearly 4x higher than that of oracle decoders on average (0.03 vs 0.11 WER). These results indicate that room for improvement remains in the few-shot challenge on FALCON datasets.

## 5 Discussion

FALCON extends previous efforts to benchmark models of neural data by presenting a standardized evaluation procedure for algorithms that improve decoder robustness in iBCI applications. We release datasets from 3 movement and 2 communication tasks, spanning monkeys, songbirds, and human participants. FALCON is designed to be inclusive of many classes of approaches; we demonstrate standardized comparison of 5 different approaches for movement datasets, 3 different approaches for H2, and 1 approach for B1, each with varying complexity and data-use strategies. These initial models far under-sample the wide design space of methods; we believe further submissions to FALCON will help clarify the value of different training data and priors. We hope that FALCON will encourage new approaches to be developed and adopted for real-world iBCI devices.

We expect that FALCON will enable machine learning researchers to apply cutting-edge approaches to a neuroengineering problem. To this end, we impose minimal restrictions on training strategies: we allow zero-shot, few-shot, or test-time adaptation and provide generous compute for model inference.

 \) / Held-In \(R^{2}\) \(\))**} \\    & Class & H1-A & M2 & H1 \\ Wiener Filter (WF) & OR & \(0.53_{ 0.04}/0.54\) & \(0.26_{ 0.03}/0.27\) & \(0.21_{ 0.04}/0.24\) \\ RNN & OR & \(0.75_{ 0.05}/0.75\) & \(0.56_{ 0.04}/0.59\) & \(0.44_{ 0.13}/0.51\) \\ NDT2 Multi & OR & \(0.78_{ 0.04}/0.77\) & \(0.58_{ 0.04}/0.62\) & \(0.63_{ 0.08}/0.68\) \\  & & & & \\ WF & ZS & \(0.34_{ 0.06}/0.46\) & \(0.06_{ 0.04}/0.15\) & \(0.16_{ 0.03}/0.20\) \\ RNN & ZS & \(-.60_{ 0.45}/0.52\) & \(-0.07_{ 0.23}/0.20\) & \(0.09_{ 0.18}/0.31\) \\ CycleGAN + WF  & FSU & \(0.43_{ 0.04}/0.61\) & \(0.22_{ 0.06}/0.32\) & \(0.12_{ 0.06}/0.15\) \\ NoMAD + WF  & FSU & \(0.49_{ 0.03}/0.64\) & \(0.20_{ 0.10}/0.35\) & \(0.13_{ 0.10}/0.21\) \\ NDT2 Multi  & FSS & \(0.59_{ 0.07}/0.77\) & \(0.43_{ 0.08}/0.63\) & \(0.52_{ 0.04}/0.62\) \\  \\   & Class & H2 (WER) & & Class & B1 (MSE \( 10^{-4}\)) \\ RNN Multi & OR & \(0.15_{ 0.01}/0.11\) & EnSongdec  & OR & \(7.47_{ 0.99}/5.61\) \\ RNN Multi + LM & OR & \(0.03_{ 0.00}/0.02\) & & & \\ RNN Multi & ZS & \(0.53_{ 0.02}/0.11\) & EnSongdec & ZS & \(21.8_{ 3.91}/5.18\) \\ RNN Multi + LM & ZS & \(0.37_{ 0.01}/0.02\) & & & \\ CORP  & TTA & \(0.11_{ 0.01}/0.02\) & & & \\ 

Table 1: **FALCON baselines.** Metric means and standard deviations over sessions, computed for held-in data and held-out data separately. Standard deviations only shown for the held-out split, for clarity. _Metrics_: \(R^{2}\) for movement tasks, word error rate (WER) for H2, mean squared error (MSE) for B1. _OR_: oracle models trained with unreleased data on held-out split. _ZS_: Zero-shot/static. _FSU_: Few-shot unsupervised. _FSS_: Few-shot supervised. _TTA_: Test-time adaptive. _Multi_: denotes training with multiple held-in datasets; otherwise models use a single held-in dataset.

While the provided baselines train with only the calibration data, FALCON is compatible with foundation models and can be used to assess their efficacy for improving iBCI robustness.

Extensions and limitationsFALCON's datasets, except for B1, contain constrained behavior with trial structure, derived from repeated cues to start and stop stereotyped behavior. Model memorization of trial structure can impede closed loop control and has been a major hurdle for adopting deep networks across iBCI settings . Corroborating this narrative, NDT2 models trained on trialized data degraded in FALCON's continuous evaluation (Section A.5.1). To penalize sensitivity to trial structure, FALCON does not provide trial labels in movement decoding tasks. However, FALCON datasets are still inherently structured. Providing datasets with more naturalistic behavior is technically challenging, particularly in humans without intact motor abilities for whom intended behavior must be communicated post-hoc. Nonetheless, future extensions may endeavor to evaluate more free and diverse behaviors, bringing evaluation closer to real-world iBCI use. To more easily aggregate a large number of behaviors, advances in cross-subject or cross-task generalization  motivate analogous few-shot benchmarks where users are given restricted data for a new subject or behavior.

An important consideration in interpreting FALCON is that it evaluates open loop prediction, not closed loop iBCI control. Closed loop control introduces shifts in neural data due to sensory feedback  and consequent user compensation. Users can correct for certain classes of decoder error, implying that worse decoder predictions may not yield poor control . The popular robotics paradigm of evaluating control in simulation  is challenging for iBCI given the complexity of simulating these considerations. Understanding how to design evaluation that avoids this open-to-closed loop performance gap remains an open problem for the field, and it is important to note that consistent decoding in FALCON may not necessarily yield consistent real-world control. Nonetheless, FALCON solidifies a current community focus on reducing data requirements. Thus, approaches reaching performance saturation in the FALCON benchmark would significantly advance the field.

FALCON datasets all provide multiple sessions of data for individual subjects. While multi-session data is a substantial advance over single dataset benchmarks (e.g. ), methods can have variable performance when applied to different subjects . The relatively unique nature of the tasks in FALCON and the cost of intracortical experiments are currently prohibitive to providing data from the high number of subjects needed to support claims of subject generalization. Evaluation of subject generalization will be an important priority for real-world application when these datasets become more common, and FALCON can be easily adapted to support these datasets.

Finally, FALCON baselines exclusively use spiking activity for decoding. While spiking activity is the default input for many iBCIs, the experimental procedure for determining spiking thresholds often involves researcher discretion. Generally, thresholds are set as a multiple of the RMS of voltages recorded during a baseline period, but the precise multiple and protocol for baseline collection varies from dataset to dataset. To encourage research into stability methods that might avoid human variability or thresholding overall, we have additionally released the raw 30kHz broadband activity for M2 and B1.

Ethical considerationsAnimal datasets were collected with approval by Institutional Animal Care and Use Committees. Human datasets were collected with Institutional Review Board approval, as part of clinical trials conducted under FDA Investigational Device Exemptions. Informed consent was obtained prior to any experimental procedures. Approvals and experimental procedures can be found in the primary references for each dataset.

FALCON focuses on algorithms that solve a problem specific to iBCIs. Such devices are intended to restore function to individuals with disabilities or impairments resulting from brain injury or disease. However, their widespread adoption raises ethical considerations with respect to the impact of these devices on human identity, privacy, and equity, which are the subject of ongoing study .

FALCON also makes use of previously collected animal datasets. Animal models are critical to neuroscientific research that aids in improving our understanding of the brain and develops medical devices for the treatment or assistance of neurological disorders. We hope that by releasing standardized animal datasets, the FALCON benchmarking effort will contribute to the minimization of redundant data collection by allowing researchers to make better use of existing data.