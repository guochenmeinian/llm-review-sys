# A Cross-Domain Benchmark for Active Learning

Thorben Werner

University of Hildesheim

Universitatsplatz 1 31141 Hildesheim

werner@ismll.de &Johannes Burchert

University of Hildesheim

Universitatsplatz 1, 31141 Hildesheim

burchert@ismll.de &Maximilian Stubbemann

University of Hildesheim

Universitatsplatz 1, 31141 Hildesheim

stubbemann@ismll.de &Lars Schmidt-Thieme

University of Hildesheim

Universitatsplatz 1, 31141 Hildesheim

schmidt-thieme@ismll.uni-hildesheim.de

Institute of Computer Science - Information Systems and Machine Learning Lab (ISMLL)

###### Abstract

Active Learning (AL) deals with identifying the most informative samples for labeling to reduce data annotation costs for supervised learning tasks. AL research suffers from the fact that lifts from literature generalize poorly and that only a small number of repetitions of experiments are conducted. To overcome these obstacles, we propose _CDALBench_, the first active learning benchmark which includes tasks in computer vision, natural language processing and tabular learning. Furthermore, by providing an efficient, greedy oracle, _CDALBench_ can be evaluated with 50 runs for each experiment. We show, that both the cross-domain character and a large amount of repetitions are crucial for sophisticated evaluation of AL research. Concretely, we show that the superiority of specific methods varies over the different domains, making it important to evaluate Active Learning with a cross-domain benchmark. Additionally, we show that having a large amount of runs is crucial. With only conducting three runs as often done in the literature, the superiority of specific methods can strongly vary with the specific runs. This effect is so strong, that, depending on the seed, even a well-established method's performance can be significantly better and significantly worse than random for the same dataset.

## 1 Introduction

Deep neural networks (NN) have produced state-of-the-art results on many important supervised learning tasks. Since Deep NNs usually require large amounts of labeled training data, Active Learning (AL) deals with selecting the most informative samples out of a large pool of unlabeled data, so that only these samples need to be labeled. It has been shown that a small labeled set of this nature can be used to train well-performing models. In the last decade, many different algorithms for AL have been proposed and almost every method has reported lifts over every single one of itspredecessors. 2 However, real insights into the current state of AL are hard to draw from these works, due to the following reasons: 1. These works do not use a standardized evaluation setting with fixed datasets and baseline approaches. 2. Due to computational constraints, a lot of works perform only a small amount of experimental runs, hence it is questionable wether the superiority of a specific approach can be concluded from the conducted experiments. 3. The works are only evaluated in a specific domain, such as computer vision or language processing. However, AL is a general principle of supervised learning, and thus methods should be evaluated in multiple domains to assess their capabilities.

While multiple benchmark suites have been proposed to solve problem 1, to the best of our knowledge, all of them are either limited in the domains they consider or do not contain enough runs to generate conclusive results. Hence, the current SOTA in AL is still not well-understood and principled shortcomings of different algorithms and wether they are domain-independent are currently not identified.

Here we step in with _CDALBench_, an AL benchmark which covers multiple application domains and reports a large amount of runs per experiment, so that the significance of performance differences can be estimated. Specifically, _CDALBench_ consists of datasets from computer vision, natural language processing and the tabular domain. We provide our datasets both in normal format as well as "embedded" by a fixed embedding model, enabling evaluation of AL methods in this semi-supervised setting. Furthermore, we propose two novel synthetic datasets to highlight general challenges for AL methods. The applied evaluation protocol in _CDALBench_ uses 50 runs for each experiment. By having such a large amount of runs, we can evaluate the significance of performance gaps and identify the best performing approaches for each dataset as well as whole domains. Furthermore, we show that the small amount of runs other works do, in fact, produce misleading results. To be more specific, we show that if only 3 restarts are employed for each experiment, the performance of specific methods strongly varies. As we will see, even the ranking of the different methods averaged over many datasets fluctuates with the specific set of runs. This effect is so strong, that, depending on the seed, even a well-established method's performance can be significantly better and significantly worse than random for the same dataset.

To enable the computation of an oracle performance for a protocol with large amounts of restarts, we propose a _greedy oracle algorithm_ which uses only a small amount of search steps to estimate the optimal solution. Our oracle relies on directly testing a small sample of points in every iteration whether they induce an improvement in test accuracy and selects the optimal point from that small sample. While being more time-efficient than established oracle functions, it possibly underestimates the real upper bound performance. However, as our experiments will show, it is still outperforming all current AL methods by at least 5% and thus is suitable as an upper bound.

 Paper & Sampling & \#Data & \#Alg & Img & Txt & Tab & Synth & Semi & Oracle & Repetitions \\  Beck et al.  & batch & 4 & 7 & ✓ & - & - & - & - & - & - \\ Hu et al.  & batch & 5 & 13 & ✓ & ✓ & - & - & - & - & 3 \\ Zhou et al.  & batch & 3 & 2 & ✓ & ✓ & - & - & - & ✓ & 5 \\ Zhan et al.  & single+batch & 35 & 18 & - & - & ✓ & ✓ & - & ✓ & 10-100 \\ Munjal et al.  & batch & 2 & 8 & ✓ & - & - & - & - & - & 3 \\ Li et al.  & batch & 5 & 13 & ✓ & - & - & - & ✓ & - & - \\ Rauch et al.  & batch & 11 & 5 & - & ✓ & - & - & - & - & 5 \\ Zhang et al.  & batch & 6 & 7 & ✓ & - & - & - & - & - & 2-4 \\ Bahir et al.  & batch & 69 & 16 & - & - & ✓ & - & - & - & 2-4 \\ Ji et al.  & batch & 3 & 8 & ✓ & - & - & - & - & - & - \\ Lueth et al.  & batch & 4 & 5 & ✓ & - & - & - & ✓ & - & 3 \\
**Ours** & single+batch & 9(14) & 11 & ✓ & ✓ & ✓ & ✓ & ✓ & 50 \\ 

Table 1: Comparison of our benchmark with the existing literature. Oracle curves serve as an approximation of the best possible AL algorithm. Our benchmark contains 9 datasets (14 including the encoded versions). “Semi” indicates whether the paper is employing any form of self- or semi-supervised learning. A “-” for repetitions means that we could not determine how often each experiment was repeated in the respective framework. _CDALBench_ is the only benchmark which evaluates a high number of runs and considers all 5 domains.

Our experimental evaluation shows that there exists no clear SOTA method for AL. The superiority of methods is strongly dataset- and domain-dependent with the outstanding observation, that the image domain works fundamentally different than the tabular and text domain. Here, the best performing approach for text and tabular data, namely _margin sampling_, is significantly outperformed by _least confident sampling_, which does not belong to the top performing approaches in any other domain. Thus, using the performance of AL approaches on the image domain as a proxy of AL in general, as it is often done , is questionable. To further analyze performance of common methods, we propose _Honeypot_ and _Diverging Sine_, two synthetic datasets, designed to be challenging for naive decision-boundary- and clustering-based approaches respectively. Hence, they provide insights in principled shortcomings of AL methods.

In summary, _CDALBench_ is an experimental framework which includes an efficient oracle approximation, multiple application domains, enough repetitions to draw valid conclusions and two synthetic tasks to highlight shortcomings of AL methods. By being the first benchmark to providing these points in one code-base, we believe that _CDALBench_ is a major step forward of assessing the overall state of AL research, independent of specific application domains. _CDALBench_ is publicly available under https://github.com/wernerth94/A-Cross-Domain-Benchmark-for-Active-Learning/.

Our contributions include the following:

1. We show that the small number of repetitions that previous works have employed is not sufficient for meaningful conclusions. Sometimes even making it impossible to assess if a performance is above or below random.
2. We propose an efficient and performant oracle which is constructed iteratively in a greedy fashion, overcoming major computational hurdles.
3. We propose _CDALBench_, the first general benchmark providing tasks in the domains of image, text and tabular learning. It further contains synthetic and pre-encoded data to allow for a sophisticated evaluation of AL methods. Our experiments show, that there is no clear SOTA method for AL across different domains.
4. We propose _Honeypot_ and _Diverging Sin_, two synthetic datasets designed to hinder AL by naive decision-boundary- or clustering-based approaches respectively. Thus, they provide an important tool to identify shortcomings of existing AL methods.

## 2 Problem Description

Given two spaces \(,\), \(n=l+u\) data points with \(l\) labeled examples \(=\{(x_{1},y_{1}),,(x_{l},y_{l})\}\), \(u\) unlabeled examples \(=\{x_{l+1},,x_{n}\}\), a model \(:\), a budget \( b u\) and an annotator \(A:\) that can label \(x\). We call \(x\), \(y\) predictors and labels respectively where \((x,y)\) are drawn from an unknown distribution \(\). Find an AL method \(:^{(i)},^{(i)} x^{(i)}^{(i)}\) that iteratively selects the next unlabeled point \(x^{(i)}\) for labeling

\[^{(i+1)} ^{(i)}\{(x^{(i)},A(x^{(i)}))\}\] \[^{(i+1)} ^{(i)}\{x^{(i)}\}\]

with \(^{(0)}=(,s)\) and \(^{(0)}=(^{(0)}_{i},A(^{(0)}_{i})) \,i[1,,s]\), where \((,s)\) selects \(s\) points per class for the initial labeled set \(^{(0)}\).

So that the average expected loss \(:\) of a machine learning algorithm fitting \(^{(i)}\) on the respective labeled set \(^{(i)}\) is minimal:

\[_{i=0}^{B}_{(x,y)}(y,^{( i)})\]

## 3 Related Work

While multiple benchmark suites have been proposed for AL, none of them provide experiments for more than two domains. The authors of  and  even focus exclusively on the image domain. Especially the tabular domain is underrepresented in preceding benchmarks, as only  provides experiments for it. The interplay between AL and semi-supervised learning is similarly under-researched, as only two works exist [18; 20], both of them only using images. An oracle algorithm has been proposed by two works [34; 31]. Both of these algorithms rely on search and are computationally very expensive, while our proposed method efficiently can be constructed sequentially. The two closest related works to this benchmark are  and , who also place a much higher emphasis on the problem of evaluating AL methods under high variance than their predecessors (indicated in Tab. 1 by a dashed line). The authors of  posed a total of 11 "recommendations" for reliable evaluation of AL methods. We largely adapt the proposed recommendations and extend their work to multiple domains and query sizes. For a complete list of the recommendations and our implementation of them, please refer to App. A. This work also pays attention to the so-called "pitfalls" of AL evaluation proposed in . For a complete list of the pitfalls and our considerations regarding them, please refer to App. B. To the best of our knowledge, we are the first to extend reliable SOTA (based on [13; 20]) experimentation to a total of 5 data domains and a high number of repetitions per experiment.

## 4 Few Repetitions are not Sufficient for Meaningful Results

To evaluate how many repetitions are necessary to obtain conclusive results in an AL experiment, we computed 100 runs of our top-performing AL method on one dataset. Our best method is margin sampling and we chose the Splice dataset for its average size and complexity.

This allows us firstly, to obtain a very strong estimation of the "true" average performance of margin sampling on this dataset and secondly, to draw subsets from this pool of 100 runs. Setting the size of our draws to \(\) and sampling uniformly, we can approximate a cross-validation process with \(\) repetitions. Each of these draws can be interpreted as a **reported result in AL literature** where the authors employed \(\) repetitions. Figure 1 shows the "true" mean performance of margin sampling (green) in relation to random sampling (black) and the oracle performance (red). We display 5 random draws of size \(\) in blue. We can observe that even for a relatively high number of repetitions the variance between the samples is extremely high, resulting in some performance curves being worse than random and some being significantly better. When setting \(=50\) we observe all samples to converge close to the true mean performance. In addition to this motivating example, we carried out our main evaluation (Tab. 3) multiple times by sampling 3 from our available runs uniformly at random and comparing the results. We found significant differences in the performance of AL methods on individual datasets, as well as permutations in the final ranking. This partly explains the ongoing difficulties in reproducing results for AL experiments and benchmarks. The details can be found in App. C. For this benchmark we employ 50 repetitions of every experiment.

### Seeding vs. Repetitions

Considering the high computational cost of 50 repetitions, another approach to ensure consistency between experiments would be to reduce the amount of variance in the experiment by keeping as

Figure 1: Random draws from a pool of 100 runs for margin sampling on the Splice dataset with different numbers of repetitions (\(=\{3,5,50\}\)). Green curves are the mean performance of all 100 runs, while the samples are blue. Even with 3 or 5 repetitions, we can observe that single draws for margin sampling display below-random performance (black), while the true mean should be above random.

many subsystems (weight initialization, data splits, etc.) as possible fixed with specialized seeding. We describe a novel seeding strategy in Appendix D that is capable of tightly controlling the amount variance in the experiment. However, previous works have noted that an actively sampled, labeled set does not generalize well between model architectures or even different initializations of the same model (), providing a bad approximation of the quality of an AL method (i.e. measured performances for an AL method might not even transfer to a different model initialization). Hence, we opt for letting the subsystems vary in controlled way (For details, please refer to App. D) and combine that with a high number of repetitions to obtain a good average of the generalization performance of each AL method.

## 5 CDALBench: A Cross-Domain Active Learning Benchmark

A detailed description of the preprocessing of each dataset can be found in Appendix E.

**Tabular:** AL research conducted on tabular data is sparse (only  from the considered baseline papers). We, therefore, introduce a set of tabular datasets that we selected according to the following criteria: (i) They should be solvable by medium-sized models in under 1000 samples, (ii) the gap between most AL methods and random sampling should be significant (potential for AL is present) and (iii) the gap between the AL methods and our oracle should also be significant (research on these datasets can produce further lifts). We use **Splice**, **DNA** and **USPS** from LibSVMTools .

**Image:** We use **FashionMNIST** and **Cifar10**, since both are widely used in AL literature.

**Text:** We use **News Category** and **TopV2**. Text datasets have seen less attention in AL research, but most of the papers that evaluate on text (, ) use at least one of these datasets. We use both, as they complement each other in size and complexity.

We would like to point out that these datasets are selected for speed of computation (both in terms of the required classifier and the necessary budget to solve the dataset). We are solely focused on comparing different AL methods in this paper and do not aim to develop novel classification models on these datasets. Our assumption is that a well-performing method in our benchmark will also generalize well to larger datasets and classifiers, because we included multiple different data domains, classifier types and sizes in our experiments.

Adapting the semi-supervised setting from , we offer all our datasets un-encoded (normal) as well as pre-encoded (semi-supervised) by a fixed embedding model that was trained by unsupervised contrastive learning. The text datasets are an exception to this, as they are only offered in their encoded form. Pre-encoded datasets enable us to test small query sizes on more complex datasets like Cifar10 and FashionMnist. They also serve the purpose of investigating the interplay between semi-supervised learning techniques and AL, as well as alleviating the cold-start problem described in  as they require a way smaller seed set. The classification model for every encoded dataset is a single linear layer with softmax activation. The embedding model was trained with the SimCLR  algorithm adopting the protocol from . To ensure that enough information from the data is encoded by our embedding model, the quality of embeddings during pretext training was measured after each epoch. To this end, we attached a linear classification head to the encoder, fine-tuned it to the data and evaluated this classifier for test accuracy. The checkpoint of each encoder model will be provided together with the framework.

Every dataset has a fixed size for the seed set \(^{(0)}\) of 1 sample per class, with the only exceptions being un-encoded FashionMnist and Cifar10 with 100 examples per class to alleviate the cold-start problem in these complex domains.

  & Model & B & 1 & 5 & 20 & 50 & 100 & 500 & 1K \\  Semi DNA & Linear & 40 & o & o & & & & \\ Semi Spice & Linear & 100 & o & o & o & & & \\ TopV2 & BILSTM & 200 & o & o & o & & & \\ Spice & MLP & 400 & o & o & o & o & o & \\ DNA & MLP & 300 & o & o & o & o & o & \\ USPS & MLP & 400 & o & o & o & o & o & \\ Semi Cifar10 & Linear & 450 & o & o & o & o & o & \\ Semi FMnist & Linear & 500 & o & o & o & o & o & \\ Semi USPS & Linear & 600 & o & o & o & o & o & \\ News & BiLSTM & 3K & & o & o & o & o & \\ FMnist & ResNet18 & 10K & & & & & o & o \\ Cifar10 & ResNet18 & 10K & & & & & o & o \\ 

Table 2: Employed model, chosen budget and available query sizes for each dataset

### Query Sizes

We selected query sizes for each dataset to accommodate the widest range possible that results in a reasonable runtime for low query sizes and allows for at least 4 round of data acquisition for high query sizes. The available query sizes per dataset can be found in Table 2.

### Realism vs. Variance

We would like to point out that some design choices for this framework prohibit direct transfer of our results to practical applications. This is a conscious choice, as we think that this is a necessary trade-off between realism and experiment variance. We would like to highlight the following design decisions:

(i) Creating test and validation splits from the full dataset rather than only the labeled seed set (following ). Fully fledged test and validation splits are unobtainable in practice, but they provide not only a better approximation of the methods generalization performance, but also a better foundation for hyperparameter tuning, which is bound to reduce variance in the experiment.

(ii) Choosing smaller classifiers instead of SOTA models. Since we are not interested in archiving a new SOTA in any classification problem, we instead opt to use smaller classifiers for the following reasons: Smaller classifiers generally exhibit more stable training behavior, on average require fewer sampled datapoints to reach their full-dataset-performance and have faster training times. For every dataset, the chosen architecture's hyperparameters are optimized to archive maximum full-dataset performance. Generally, we use MLPs for tabular, RestNet18 for image and BiLSTMs for text datasets. Every encoded dataset is classified by a single linear layer with softmax activation. The used model for each dataset can be found in Tab. 2. For a detailed description and employed hyperparameters please refer to Appendix E.

### Evaluation Protocol

Following , the quality of an AL method is evaluated by an "anytime protocol" that incorporates classification performance at every iteration, as opposed to evaluating final performance after the budget is exhausted. We employ the normalized area under the accuracy curve (AUC):

\[(_{},,B):=_{i=1}^{B} (_{},^{(i)})\] (1)

Since AUC is still influenced by the budget, we define a set of rules to set this hyperparameter upfront, so that we are not favoring a subset of methods by handcrafting a budget. In this work, we choose the budget per dataset to be the first point at which one of 2 stopping conditions apply: (i) an method (except oracle) manages to reach 99% of the full-dataset-performance (using the smallest query size) or (ii) the best method (except oracle) did not improve the classifier's accuracy by at least 2% in the last 20% of iterations. The first rule follows , while the second rule prevents excessive budgets for cases with diminishing returns in the budget. The resulting budgets can be found in Tab. 2.

As described in Sec. 4, we repeat each experiment 50 times. Each repetition retains the train/test split (often given by the dataset itself), but creates a new validation split that is sampled from the entire dataset (not just the seed set \(^{(0)}\)).

Apart from plotting standard performance curves and reporting their AUC values per dataset in App. F, we primarily rely on ranks to aggregate the performance of an AL method across datasets. For each dataset and query size, the AUC values of all AL methods are sorted and assigned a rank based on position, with the best rank being 1. These ranks can safely be averages across datasets as they are no longer subjected to scaling differences of each dataset. Additionally, we employ Critical Difference (CD) diagrams (like Fig. 2) for statistical testing. CD diagrams  use the Wilcoxon signed-rank test, which is a variant of the paired T-test, to find significant differences of ranks between AL methods. For a detailed description of how every CD diagram is created, please refer to App. G.

A Greedy Oracle Algorithm

Using additional resources, like excessive training time, or direct access to a labeled test set, an oracle method for AL finds the oracle set \(_{b}\) for a given dataset, model, and training procedure that induces the highest AUC score for a given budget. However, due to the combinatorial nature of the problem, this is computationally infeasible for realistic datasets. Hence, previous works have proposed approximations to this oracle sequence.  used simulated annealing to search for a subset with maximal test accuracy and used the best solution after a fixed time budget. Even though their reported performance curves display a significant lift over all other AL methods, we found the computational cost of reproducing this oracle for all our datasets to be prohibitive (The authors reported the search to take several days per dataset on 8 V100 GPUs). In this paper, we propose a greedy oracle algorithm that constructs an approximation of the optimal set in an iterative fashion. Our oracle algorithm uniformly samples at iteration \(i\) a subset \(_{S}\) of size \(\) of the not already labeled data points \(^{(i)}\). Then it recovers the label \(y\) for each of the sampled \(u_{S}\) and selects the point \(u\) for which the classifier \(^{(i)}\) trained on \(^{(i)}\{u\}\) has maximal performance. Due to the algorithms greedy nature (considering only the next point to pick), our oracle frequently encounters situations where every point in \(u\) would incur a negative lift (worsening the test performance). This can happen, for example, if the oracle picked a labeled set that enables the classifier to correctly classify a big portion of easy samples in the test set, but now fails to find the next **single** unlabeled point that would enable the classifier to succeed on one of the hard samples. This leads to a situation, where no point can immediately incur an increase in test performance and therefore the selected data point can be considered random. To circumvent this problem, we use our best-performing AL method (margin sampling ) as a fallback option for the oracle. Whenever the oracle does not find an unlabeled point that results in an increase in performance, it defaults to margin sampling from the entire unlabeled pool \(^{(i)}\) in that iteration. The resulting greedy algorithm constructs an approximation of the optimal labeled set that consistently outperforms all other algorithms by a significant margin, while requiring relatively low computational cost (\((B)\)). We fix \(=20\) in this work, as this gives us an average lift of 5% over the best performing AL method per dataset (which is significant for AL settings) and we expect diminishing returns for larger \(\). The pseudocode for our oracle can be found in App. H. Even though our proposed algorithm is more efficient than other approaches, the computational costs for high budget datasets like Cifar10 and FashionMnist meant that we could not compute the oracle for all 10000 datapoints. To still provide an oracle for these two datasets, we select two points per iteration instead of one and stop the oracle computation at a budget of 2000. The rest of the curve is forecast with a 2-stage linear regression that asymptotically approaches the upper bound performance of the dataset. A detailed description can be found in App. I.

## 7 Experiments

### Implementation Details

At each iteration \(i\) the AL method picks an unlabeled datapoint based on a fixed set of information \(\{^{(i)},^{(i)},B,|^{(i)}|-|^{(1 )}|,^{(i)},^{(1)},^{(i)},_{}\}\), where \(_{}\) is the optimizer used to fit \(^{(i)}\). This set grants full access to the labeled and unlabeled set, as well as all parameters of the classifier and the optimizer. Additionally, we provide meta-information, like the size of the seed set through \(|^{(i)}|-|^{(1)}|\), the remaining budget though the addition of \(B\) and the classifiers potential through \(^{(1)}\) and \(^{(i)}\). We allow AL methods to derive information from this set, e.g. predictions of the classifier \(^{(i)}(x);\;\;x^{(i)}^{(i)}\), clustering, or even training additional models. However, the method may not incorporate external information e.g. other datasets, queries to recover additional labels, additional training steps for \(\), or the test/validation set.

For our study we selected AL methods with good performances reported by multiple different sources that can work with the set of information stated above. For a list of all AL methods, please refer to Table 3, with detailed descriptions being found in Appendix J.

The model \(^{(i)}\) can be trained in two ways. Either the parameters of the model are reset to a fixed initial setting \(^{(0)}\) after each AL iteration and the classifier is trained from scratch with the updated labeled set \(^{(i)}\), or the previous state \(^{(i-1)}\) is retained and the classifier is fine-tuned on \(^{(i)}\) for a reduced number of epochs. In this work, we use the fine-tuning method for un-encoded datasets to save computational time, while we use the from-scratch training for encoded datasets since they have very small classifiers and this approach generally produces better results. Our fine-tuning scheme always trains for at least one epoch and employs an aggressive early stopping with a patience of 2 afterwards.

### Results on Real-world Data

In Table 3 we provide the rank of each AL method per dataset. Please note, that we are averaging not only over runs, but also over query sizes per dataset, impacting AL methods that do not adapt well to a wide range of query sizes. For the results per query size, please refer to App. K. As mentioned in contribution 3, our results on real-world data show significant differences in the performance of methods between data domains: Not only do some methods overperform on some domains (like least confidence (LC) sampling on images), but the Top-3 of methods (except oracle) does not contain the same three methods for **any** two domains. Most interestingly, the image domain, which received most of the attention in benchmarking so far could even be considered an outlier, as this is the only domain where the Top-1 method changes. This highlights the dire need for diverse data domains in AL benchmarking.

Results for the semi-supervised domain appear mostly in line with the other 3 domains, but a closer analysis of performances split into encoded images and encoded tabular reveals the need for further research. For details, please refer to App. L.

Finally, we would like to emphasize that the total average rank of our top 3 algorithms (column "Normal" in Tab. 3) are 4.8, 4.9 and 5.1. No single algorithm was able to perform well in every domain, either being outperformed by a specialist algorithm in each domain, or experiencing a severe drop in performance in a poorly matched domain.

## 8 Honeypot and Diverging Sine

AL approaches can be categorized into two types: uncertainty and geometric approaches. Typical members of the first category are variants of uncertainty sampling like entropy, margin and LC sampling  as well as BALD . Typical members of the second category are clustering approaches

  & Splice & DNA & USPS & Cifar10 & FMhist & TopV2 & News & Normal & Semi \\  Oracle & 1.0\(\)0.01 & 1.0\(\)0.01 & 1.0\(\)0.01 & 1.0\(\)0.01 & 1.0\(\)0.01 & 1.0\(\)0.01 & 1.0\(\)0.0 & 1.0 & 2.1 \\ Margin & 6.8\(\)0.02 & 4.5\(\)0.01 & 2.7\(\)0.01 & 7.2\(\)0.01 & 4.9\(\)0.03 & 3.1\(\)0.01 & 4.5\(\)0.0 & 4.8 & 4.7 \\ Galaxy & 9.5\(\)0.02 & 9.4\(\)0.02 & 2.4\(\)0.01 & 2.7\(\)0.01 & 5.6\(\)0.01 & 2.6\(\)0.01 & 2.6\(\)0.01 & 2.0\(\)0.0 & 4.9 & 5.7 \\ Badge & 6.2\(\)0.01 & 6.5\(\)0.01 & 3.8\(\)0.01 & 6.2\(\)0.01 & 5.1\(\)0.04 & 4.2\(\)0.01 & 3.6\(\)0.0 & 5.1 & 6.0 \\ LeastConfident & 9.6\(\)0.02 & 11.1\(\)0.02 & 9.1\(\)0.02 & 2.6\(\)0.01 & 4.4\(\)0.00 & 8.9\(\)0.02 & 5.2\(\)0.01 & 7.3 & 7.1 \\ DSA & 7.8\(\)0.02 & 7.7\(\)0.01 & 8.5\(\)0.01 & 6.4\(\)0.01 & 5.6\(\)0.00 & 7.0\(\)0.02 & 8.1\(\)0.01 & 7.3 & 7.3 \\ CoreGCN & 7.2\(\)0.01 & 5.2\(\)0.01 & 11.4\(\)0.01 & 8.6\(\)0.01 & 7.1\(\)0.01 & 5.0\(\)0.01 & 7.4\(\)0.01 & 7.4 & 8.9 \\ BALD & 4.1\(\)0.01 & 4.8\(\)0.01 & 6.4\(\)0.01 & 13.0\(\)0.01 & 8.4\(\)0.00 & 8.6\(\)0.02 & 7.7\(\)0.0 & 7.6 & 8.3 \\ Entropy & 6.8\(\)0.02 & 4.0\(\)0.01 & 8.6\(\)0.01 & 5.4\(\)0.01 & 10.8\(\)0.02 & 11.1\(\)0.01 & 7.9 & 7.3 \\ LSA & 6.2\(\)0.01 & 7.2\(\)0.01 & 6.3\(\)0.01 & 8.6\(\)0.01 & 11.6\(\)0.01 & 8.5\(\)0.01 & 7.7\(\)0.0 & 8.0 & 8.1 \\ Random & 9.4\(\)0.01 & 9.7\(\)0.01 & 6.3\(\)0.01 & 9.4\(\)0.01 & 12.1\(\)0.00 & 8.9\(\)0.01 & 8.3\(\)0.0 & 9.2 & 7.6 \\ Coreset & 7.3\(\)0.01 & 9.5\(\)0.01 & 11.5\(\)0.01 & 7.7\(\)0.01 & 7.8\(\)0.00 & 9.5\(\)0.02 & 11.5\(\)0.01 & 9.3 & 7.9 \\ TypiClust & 9.1\(\)0.01 & 10.6\(\)0.01 & 13.0\(\)0.02 & 8.9\(\)0.01 & 12.0\(\)0.01 & 13.0\(\)0.02 & 13.0\(\)0.01 & 11.4 & 9.9 \\ 

Table 3: Performances for AL methods on real-world datasets, aggregated for un-encoded (normal) and encoded (semi-supervised) datasets. Performance is shown as average ranks over repetitions (1.0 is the best rank). Methods are sorted by aggregated performance on un-encoded (normal) datasets.

Figure 2: Ranks of each AL method aggregated by domain. Horizontal bars indicate a **non-**significant rank difference. The significance is tested via a paired-t-test with \(=0.05\).

like Coreset , BADGE  and TypiClust . Both types of methods have principled shortcomings in terms of their utilized information that makes them unsuitable for certain data distributions. To test for these specific shortcomings, we created two synthetic datasets, namely "Honeypot" and "Diverging Sine", that are hard to solve for methods focused on the classifier's decision boundary or data clustering respectively. To avoid methods memorizing these datasets, they are generated from scratch for each experiment.

Honeypot creates two easy to distinguish clusters and one "honeypot" that represents a noisy region of the dataset with potentially miss-labeled, miss-measured or generally adverse samples. The honeypot is located on the likely decision boundary of a classifier that is trained on the beneficial samples to maximize its adverse impact on purely uncertainty-based AL methods. Diverging Sine samples datapoints for each class from two diverging sinusoidal functions that are originating from the same y-intercept. This creates a challenging region on the left hand side, where a lot of datapoints need to be sampled, and an easy region on the right hand side, where very few datapoints are sufficient. The repeating nature of a sine function encourages diversity-based AL methods to equally sample the entire length, drastically oversampling the right hand side of the dataset.

Both datasets have a budget of \(B=60\) and are tested with query sizes 1 and 5.

We provide the rank of all AL Methods on Honeypot and Diverging Sine in Fig. 3. Results for the Honeypot dataset reveal expected shortcomings of uncertainty sampling methods like margin, entropy and LC sampling as well as BALD. In addition, BADGE is underperforming for this dataset compared to real-world data. Both margin sampling and BADGE (the two best methods) being vulnerable to adverse samples or simply measurement noise, highlights the need for further research into robust AL methods.

Results for Diverging Sine also confirm expected behavior, as clustering methods (Coreset, TypiClust) fall behind uncertainty methods (entropy, margin, LC sampling), with the exception of BADGE. The fact that BADGE is able to perform well on Diverging Sine highlights the importance of embeddings for the clustering methods, as the gradient embedding from BADGE seems to be able to encode uncertainty information, guiding the selection into the left hand regions of the dataset. We provide a small ablation study on the importance of the embeddings by testing a version of Coreset and TypiClust on this dataset that does not use the embeddings produced by the classification model, but rather clusters the data directly. "Coreset Raw" and "TypiClust Raw" both perform worse than their embedding-based counterpart.

## 9 Comparison to other Benchmarks

Comparing our results to the findings of other works based in accuracy scores would be meaningless, as every work employs different models, hyperparameters and training loops. We instead opt to compare only the ranking of algorithms to the literature.

Our results generally are reflected in domain-specific benchmarks -  also find least confidence sampling and BADGE to be the best algorithms for images (they don't test Galaxy),  also find

Figure 3: Synthetic “Honeypot” and “Diverging Sine” datasets. The optimal decision boundary is not part of the dataset and serves only as a visual guide.

BADGE to be the best algorithm for text (they don't test margin sampling or Galaxy) and  also find margin sampling to be the best algorithm for tabular data.

In this work, we provide the first AL benchmark that spans all major data domains and is easily reproducible while matching and extending previous published results from single-domain benchmarks.

## 10 Using this Benchmark

We strongly advocate to test newly proposed AL methods not only on a wide variety of real data domains, but also to pay close attention to the Honeypot and Diverging Sine datasets to reveal principled shortcomings of the method in question. Both tasks can be easily carried out by implementing the new AL method into our code base. For Limitations and Future Work, please refer to App. O.

AcknowledgementFunded by the Lower Saxony Ministry of Science and Culture under grant number ZN3492 within the Lower Saxony "Vorab" of the Volkswagen Foundation and supported by the Center for Digital Innovations (ZDIN).