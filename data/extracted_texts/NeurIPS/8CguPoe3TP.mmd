# Bayesian Nonparametrics Meets Data-Driven

Distributionally Robust Optimization

 Nicola Bariletto

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

nicola.bariletto@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

minhnhat@utexas.edu

###### Abstract

Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet process) theory and a recent decision-theoretic model of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet process representations. We also show that the smoothness of the criterion naturally leads to standard gradient-based numerical optimization. Finally, we provide insights into the workings of our method by applying it to a variety of tasks based on simulated and real datasets.

## 1 Introduction

In machine learning and statistics applications, several quantities of interest solve the optimization problem

\[_{}_{p_{*}}(),\]

where \(_{p}():=_{ p}[h(,)]\) is the expected risk associated to decision \(\), under cost function \(h(,)\) (measurable in the argument \(\)) and given that the distribution of the \((,())\)-valued data \(\) is \(p\).1 For instance, if we are dealing with a supervised learning task where \(=(x,y)^{m-1}\), \(h(,)\) is usually a loss function \((f_{}(x),y)\) quantifying the cost incurred in predicting \(y\) with \(f_{}(x)\) - here the decision variable is \(\), which parametrizes the function \(f_{}:^{m-1}\). For the rest of the paper, we assume \(^{m}\), \(^{d}\) and \(h:[0,K]\) for some \(K<\).2

In most cases of interest the true data-generating process \(p_{*}\) is unknown, and only a sample \(^{n}=(_{1},,_{n})\) from it is available. The most popular solution is to approximate \(p_{*}\) by the empirical distribution \(p_{^{n}}\), and optimize \(_{p_{^{n}}}() n^{-1}_{i=1}^{n}h( ,_{i})\). However, especially for small sample sizes \(n\) and complex data-generating mechanisms, this can result in poor out-of-sample performance, leading to the need for robust alternatives. A flourishing literature on Distributionally RobustOptimization (DRO) has provided several methods in that direction [though not always with data-driven applications as the primary focus; see 35, for a recent exhaustive review of the field]. A prominent approach is the min-max DRO (mM-DRO) one, whereby a worst-case criterion over an ambiguity3 set of plausible distributions is minimized [19; 2; 3; 10; 46; 11]. Recent notable results involve the study of mM-DRO problems where the ambiguity set is defined as a Wasserstein ball of probability measures centered at the empirical distribution [29; 24].

Contribution.Differently from the mM-DRO paradigm, we propose a distributionally robust procedure based on the minimization of the following criterion:

\[V_{^{n}}():=_{_{}}(_{ p}())Q_{^{n}}(p),\] (1)

where \(:[0,K]_{+}\) is a _continuous_, _convex_ and _strictly increasing_ function, and \(Q_{^{n}}\) is a Dirichlet process posterior conditional on \(^{n}\).

As we show below, our proposal brings together insights from two well-established strands of literature - decision theory under ambiguity and Bayesian nonparametric statistics, - contributing in a novel way to the field of data-driven distributionally robust optimization. As we establish throughout the article, among the key advantages of the criterion are: (i) its favorable statistical properties in terms of probabilistic finite-sample and asymptotic performance guarantees; (ii) the availability of tractable approximations that are easy to optimize using standard gradient-based methods; and (iii) its ability to both improve and stabilize the out-of-sample performance of standard learning methods.

The rest of the paper is organized as follows. In Section 2, we motivate the formulation in Equation (1) by providing a concise overview of decision theory under ambiguity and its connections to Bayesian statistics and regularization. In Section 3, we study the statistical properties of procedures based on \(V_{^{n}}\). In Section 4, we propose and study tractable approximations for \(V_{^{n}}\) based on the theory of DP representations. In Section 5, we highlight the robustness properties of our method by applying it to a variety of learning tasks based on real and simulated data. Section 6 concludes the article. Proofs of theoretical results and further background are provided in Appendices A and B, respectively, while in Appendix C we discuss how the smoothness of the proposed criterion yields straightforward gradient-based optimization, and present more details on the numerical experiments. Code to replicate our experiments can be found at the following link: https://github.com/nbariletto/BNP_for_DRO.

## 2 Decision Theory and Bayesian Statistics

Following a long-standing tradition in Bayesian statistics and decision theory , the distributional uncertainty on the data-generating process \(p_{*}\) can be dealt with by defining a prior \(Q\) over it. We point out that this Bayesian approach contrasts with the classical one, where a prior would typically be placed directly on the parameter \(\), whose data-driven optimal value would be determined as a function (e.g., the mean or mode) of the resulting posterior distribution. In this new framework, instead, the parameter \(\) is treated as a variable to be optimized, while the prior is assigned to the entire data-generating process. This perspective allows us to incorporate several valuable Bayesian concepts, as we will clarify throughout the paper, while preserving the flexibility of the original optimization-based learning framework. Notably, \(\) does not need to be interpreted as the parameter of a full generative model, as would be required in a classical Bayesian setting. Instead, it can represent a vector of parameters associated with a generic, possibly complex loss function, such as those employed in modern deep learning architectures.

Specifically, our Bayesian approach is equivalent to modeling the observed data \(^{n}=(_{1},,_{n})\) as exchangeable with de Finetti measure \(Q\):

\[_{i} p }}{{}} p, i=1,,n,\] \[p  Q.\]

Due to the stochasticity of \(p\), \(_{p}()\) is itself a random variable, and a sensible procedure is to maximize its posterior expectation. Let \(Q_{^{n}}\) be the posterior law of \(p\) conditional on the sample \(^{n}\)Then, one solves the following problem:

\[_{}\,_{p Q_{^{n}}}[_{p}( )]=_{}\ _{_{}}_{}h(,)p()Q_{^{n}} (p)=_{}\ _{ p(|^{n})}[h(,)],\]

where \(_{}\) denotes the space of probability measures on \(()\) endowed with the Borel \(\)-algebra \((_{})\) generated by the topology of weak convergence, while \(p(|^{n}):=_{_{}}p()Q_{^{n}}(p)\) denotes the posterior predictive distribution. In sum, within this general Bayesian framework, the data-driven problem reduces to minimizing \(h(,)\) averaged w.r.t. the posterior predictive distribution, i.e., \(_{}_{p(|^{n})}()\).

### The Dirichlet Process

A natural choice is to model the prior \(Q\) as a Dirichlet process (DP), and \(Q_{^{n}}\) is then a DP posterior. First proposed by , the DP is the cornerstone nonparametric prior over spaces of probability measures. Its specification involves a _concentration parameter_\(>0\) and a _centering probability measure_\(p_{0}\). Intuitively, the DP is characterized by the following finite-dimensional distributions: \(p(,p_{0})\) implies \((p(A_{1}),,p(A_{k}))( p_{0}(A_{1}),,  p_{0}(A_{k}))\) for any finite measurable partition \(\{A_{1},,A_{k}\}\) of \(\).4 A key property of the DP is its almost sure discreteness, which allows to write \(p}}{{=}}_{j 1}p_{j}_{_{j}}\) (where probability weights and atom locations are independent). Moreover, the DP is conjugate with respect to exchangeable sampling. In our case, this means

\[p(,p_{0}) Q_{^{n}}= +n,p_{0}+p_{^{n}}.\]

That is, conditional on the sample \(^{n}\), \(p\) is again a DP with larger concentration parameter \(+n\) and centered at the predictive distribution \(p(|^{n}):=p_{0}+p_{^{n}}\). The latter is a compromise between the prior guess \(p_{0}\) and the empirical distribution \(p_{^{n}}\), and the balance between the two is determined by the relative size of \(\) and \(n\). The predictive distribution is also related to the celebrated Blackwell-MacQueen Polya un scheme (or Chinese restaurant process) to draw an exchangeable sequence \((_{i})_{i 1}\) distributed according to \(p(,p_{0})\): Draw \(_{1} p_{0}\) and, for all \(i>1\) and \(<i\), set \(_{i}=_{}\) with probability \((+j-1)^{-1}\), else (i.e., with probability \((+j-1)^{-1}\)) draw \(_{i} p_{0}\).

Given the large support of \(Q_{^{n}}\), which consists of all probability measures whose support is included in that of \(p(|^{n})\), the DP is a reasonable and tractable option to mitigate misspecification concerns. Then, leveraging the mentioned expression for the DP predictive distribution, the problem specializes to

\[_{}_{p_{^{n }}}()+_{p_{0}}()}\] (2)

[see also 27, 45]. In practice, adopting the above Bayesian approach amounts to introducing a regularization term depending on the prior centering distribution \(p_{0}\). Compared to the simple empirical risk \(_{p_{^{n}}}()\), this type of criterion displays lower variance (because \(_{p_{0}}()\) is non-random) at the cost of some additional, asymptotically-vanishing bias w.r.t. the theoretical criterion \(_{p_{}}()\). We also note that such bias can be attenuated in finite samples as long as the prior guess \(p_{0}\) and the true data-generating process \(p_{}\) are close enough in terms of the difference \(|_{p_{0}}()-_{p_{}}()|\).5

Connections to Regularization in Linear Regression.One of the most ubiquitous data-driven learning tasks is linear regression . It is well-known that, in this setting, coefficient estimation (e.g., via maximum likelihood or least squares) can be framed as a minimization problem of the sample average of the squared loss function. It turns out that, applying the Bayesian regularized approach (2), an interesting equivalence with standard regularization techniques such as Ridge  and LASSO  emerges.

**Proposition 2.1**.: _Let \(h(,(y,x))=(y-^{}x)^{2}\). Then, denoting \(_{,n}:=/n\), the following equivalences hold:__1. If \(p_{0}=(0,I)\), then \(\) solving (2) implies that it solves_

\[_{}\{_{i=1}^{n}h(,_{i})+_ {,n}\|\|_{2}^{2}\};\]

_2. If \(V=(|_{1}|^{-1},,|_{d-1}|^{-1})\) and \(p_{0}=(0,V)\), then \(\) solving (2) implies that it solves_

\[_{}\{_{i=1}^{n}h(,_{i})+ _{,n}\|\|_{1}\}.\]

Proposition 2.1 is insightful because it highlights a novel Bayesian interpretation of Ridge and LASSO linear regression. In fact, it is well known that both methods are equivalent to maximum-a-posteriori estimation of regression coefficients when the latter are assigned either a normal or a Laplace prior. In our setting, instead of a parametric prior on the regression coefficients, we place a nonparametric one on the joint distribution of the response and the covariates. The degree of regularization, then, is naturally guided by the prior confidence parameter \(\) and the sample size \(n\). We also note that sparsity is only one of the possible data-generating features one might want to enforce in regularized estimation,6 and the nonparametric Bayesian approach offers greater flexibility, compared to Ridge and LASSO, to incorporate such patterns by specifying the prior expectation \(p_{0}\) of the joint response-covariate distribution.

### Ambiguity Aversion

As we just showed, adopting a traditional Bayesian framework, uncertainty about the model \(p\) is resolved by using the posterior \(Q_{^{n}}\) to directly average \(p\) out. This procedure, however, does not take into account the (partly) subjective nature of the beliefs encoded in \(Q_{^{n}}\), and the aversion to this that a statistical decision maker (DM) might have. In fact, the result of the procedure is that the DM ends up minimizing the expected risk, where the average is taken according to the predictive distribution. In practice, then, the latter is put on the same footing as an objectively known probability distribution, such as the true model.

This issue has been thoroughly studied and addressed in the economic decision theory literature [18; 6]. In that context, the economic DM faces an analogous expected utility maximization problem \(_{}_{ p}[u(,)]\) (e.g., to allocate her capital to a portfolio of investments subject to random economic shocks \(\)). However, she does not possess enough objective information to pick one single model of the world \(p\), but deems a larger set of models plausible. One possibility, then, is that the DM forms a second-order belief (e.g., a prior \(Q\)) over such set, and resolves uncertainty by directly averaging expected utility profiles \(_{ p}[u(,)]\) w.r.t. \(p Q\).

Just like in our data-driven problem, however, direct averaging does not account for ambiguity aversion.  proposed and axiomatized a tractable "Smooth Ambiguity Aversion" (SmAA) model, whereby second-order averaging is preceded by a deterministic transformation \(\) inducing uncertainty aversion via its curvature: The DM optimizes \(_{_{}}(_{ p}[u(,)])Q(p)\), and criterion (1) simply specializes the SmAA model to the data-driven case.7 When optimization takes the form of minimization, ambiguity aversion is driven by the degree of convexity of \(\).8 In particular, convexity encodes the DM's tendency to pick decisions that yield less variable expected loss levels across ambiguous probability models. To see this intuitively, examine the simple case when only two models, \(p_{1}\) and \(p_{2}\), are supported by \(Q=_{p_{1}}+_{p_{2}}\). Consider two decisions \(_{1}\) and \(_{2}\) that, under \(p_{1}\) and \(p_{2}\), yield the expected risks marked on the horizontal axis of Figure 1. While \(_{p}(_{1})Q(p)=_{p}(_{2})Q (p)=^{*}\), the convexity of \(\) implies\((_{p}(_{1}))Q(p)<(_{p}(_ {2}))Q(p)\). That is, although \(_{1}\) and \(_{2}\) yield the same loss in \(Q\)-expectation, the ambiguity-averse criterion favors \(_{1}\) because it ensures less variability across uncertain distributions \(p_{1}\) and \(p_{2}\).

Interestingly,  showed that the SmAA model belongs to a general class of ambiguity-averse preferences, which admit a common utility function representation. For SmAA preferences with \((t)=(^{-1}t)-\) (with \(>0\) and under additional technical assumptions), this representation implies the equivalence of problem (1) with

\[_{}_{P:P Q_{^{n}}}_{p P }[_{p}()]-(P\|Q_{^{n}})},\]

where \((\|)\) is the Kullback-Leibler divergence and \(\) denotes absolute continuity. The above result further clarifies the mechanism through which distributional robustness is induced: Intuitively, instead of directly averaging over \(p Q_{^{n}}\), one computes a worst-case scenario w.r.t. the mixing measure, penalizing distributions that are further away from the posterior - the latter acts as a reference probability measure. Moreover, in the limiting case \( 0\), the mM-DRO setup is recovered, with ambiguity set \(=\{p_{}: P Q_{^{n}},p=_{ _{}}qP(q)\}\). In the other limiting case \(\) (with the convention \(0=0\)), the ambiguity neutral Bayesian criterion (2) is instead recovered.

## 3 Statistical Properties

In this section, we analyze the statistical properties of the criterion \(V_{^{n}}()\), as a function of the sample size \(n\). A first issue of interest, addressed in Proposition 3.1, is to study its asymptotic point-wise behavior (cf. , Corollary 4.17).

**Proposition 3.1**.: _Let \(^{n}\) be iid according to \(p_{*}\) and \( h(,)\) continuous for all \(\). Then, for all \(\),_

\[_{n}V_{^{n}}()=(_{p_{*}}())\]

_almost surely._

This ensures that, as more data are collected, the proposed criterion approaches, with probability 1, the true theoretical risk (up to the strictly increasing transformation \(\)).

While point-wise convergence to the target ground truth is a first desirable property for any sensible criterion, it is not enough to characterize the behavior of the optimization's out-of-sample performance, nor the closeness of the optimal criterion value and the criterion optimizer(s) to their theoretical counterparts. In the following subsections, we study these properties both in the finite-sample regime and in the asymptotic limit \(n\).

Figure 1: Graphical display of smooth ambiguity aversion at work. Although \(_{1}\) and \(_{2}\) yield the same loss \(^{*}\) in \(Q\)-expectation, the ambiguity averse criterion favors the less variable decision \(_{1}\). Graphically, this is because the orange line connecting \((_{p_{1}}(_{1}))\) to \((_{p_{2}}(_{1}))\) lies (point-wise) below the line connecting \((_{p_{1}}(_{2}))\) to \((_{p_{2}}(_{2}))\).

Finite-Sample Guarantees.Denote

\[_{n}*{arg\,min}_{}V_{^{n}}(), _{*}*{arg\,min}_{}_{p_{*}} (),\]

and we assume the above sets of minimizers to be non-empty throughout the article. In finite-sample analysis, a first question of interest is whether probabilistic performance guarantees hold for the robust criterion optimizer \(_{n}\). In our setting, one can naturally measure performance by the narrowness of the gap between \(_{p_{*}}(_{n})\) and \(_{p_{*}}(_{*})\). As we clarify later, Lemma 3.2 is a first step towards establishing this type of guarantees.

**Lemma 3.2**.: _Let \(\) be twice continuously differentiable on \((0,K)\), with \(M_{}:=_{t(0,K)}^{}(t)<+\) and \(_{}^{*}:=_{t(0,K)}_{}(t)<+\), where \(_{}(t):=^{}(t)/^{}(t) 0\). Then_

\[_{}|V_{^{n}}()-(_{p_{*}}( ))| M_{}_{}| _{p_{^{n}}}()-_{p_{*}}()|+K+}{2}_{}^{*}.\]

Lemma 3.2 links the \(\) distance of the criterion \(V_{^{n}}\) from the theoretical risk to three key objects:

1. The classical \(\) distance between the empirical and theoretical risk, \(_{}|_{p_{^{n}}}()-_{p_{ *}}()|\);
2. The \(\) distance between the theoretical risk and the risk computed w.r.t. the base probability measure \(p_{0}\), \(_{}|_{p_{0}}()-_{p_{*}}()|\). In fact, while in the formulation of Lemma 3.2 we bound such distance by \(K\) (see the second addendum) to eliminate the dependence on the unknown but fixed \(p_{*}\), one could equivalently replace \(K\) by \(_{}|_{p_{0}}()-_{p_{*}}()|\). This clarifies that, if \(p_{0}\) is a good guess for \(p_{*}\), i.e., if the above \(\) distance is small, adopting a Bayesian prior centered at \(p_{0}\) can improve finite sample bounds;
3. The _Arrow-Pratt coefficient_\(_{}(t)\) of absolute ambiguity aversion. In the economic theory literature on decision-making under risk, this is a well-known concept measuring the degree of risk aversion of decision makers, with point-wise larger values of \(_{}\) corresponding to more risk aversion. See  for a discussion on the straightforward adaptation of this measure to the ambiguity (rather than risk) aversion setup we work in.

Most importantly, Lemma 3.2 allows us to prove the following Theorem, which yields the performance guarantees we are after.

**Theorem 3.3**.: _For all \(>0\)_

\[[(_{p_{*}}(_{n}))-( _{p_{*}}(_{*}))]\] \[_{}_{ p_{^{n}}}()-_{p_{*}}()\ (}-K- {K^{2}}{2}_{}^{*}).\]

Theorem 3.3 allows to obtain finite-sample probabilistic guarantees on the excess risk \((_{p_{*}}(_{n}))-(_{p_{*}}(_{*}))\) via bounds on \(_{}_{p_{^{n}}}()- _{p_{*}}()\). The latter is a well-studied quantity, and the sought bounds follow from standard results relying on conditions on the complexity of the function class \(:=\{h(,):\}\), as measured by its Vapnik-Chervonenkis dimension, metric entropy, etc. We refer the reader to  for a systematic treatment of the topic and specific useful results.

Asymptotic Guarantees.So far, we have studied the finite-sample behavior of the out-of-sample performance of \(_{n}\). Another closely related type of results deals with the asymptotic limit of such performance, as well as with the convergence of optimum criterion values and optimizing parameters to their ground-truth counterparts. In this Subsection, attention is turned to theoretical results of this kind.

Finite-sample guarantees on \(_{}_{p_{^{n}}}()- _{p_{*}}()\) are usually of the form

\[_{}_{p_{^{n}}}( )-_{p_{*}}() 1-_{n},\]with \(_{n=1}^{}_{n}<\). This implies (via a straightforward application of the first Borel-Cantelli Lemma) the almost sure vanishing of \(_{}_{p_{^{n}}}()-_ {p_{*}}()\). Thus, we include this as an assumption of the next Theorem. Moreover, we introduce a functional dependence of \(\) on \(n\), and denote \(_{n}\) accordingly.

**Theorem 3.4**.: _Retain the assumptions of Lemma 3.2 and \(_{n}_{}_{p_{^{n}}}( )-_{p_{*}}()=0\) almost surely. Moreover, assume that \(_{n}\) satisfies (1) \(_{n}_{_{n}}^{*}=0\), (2) \(_{n 1}M_{_{n}}<\), and (3) \(_{n}_{t[0,K]}|_{n}(t)-t|=0\). Then the next two almost sure limits hold:_

\[_{n}_{p_{*}}(_{n})=_{p_{*}}(_ {*}),_{n}V_{^{n}}(_{n})=_{p_{*}}( _{*}).\]

Theorem 3.4 is crucial because it ensures that, asymptotically, the excess risk vanishes and the finite-sample optimal value converges to the optimal value under the data generating process.9

_Remark 3.5_.: From a design point of view, the type of \(n\)-dependent parametrization of \(\) required in Theorem 3.4 is sensible, as it is equivalent to adopting vanishing levels of ambiguity aversion (uniformly vanishing Arrow-Pratt coefficient) as the sample size grows - that is, as one obtains a more and more precise estimate of the true distribution \(p_{*}\). Moreover, this assumption is in the spirit of the condition imposed on the radius of the Wasserstein ambiguity ball in , which is required to vanish as the sample size grows. Finally, it is easy to show that \(_{n}(t)=_{n}(_{n}^{-1}t)-_{n}\), for positive \(_{n},\) satisfies the conditions of Theorem 3.4, and from now on we silently assume this form for \(_{n}\).

Finally, we leverage the above results to ensure the convergence of the sequence of optimizers \((_{n})_{n 1}\) to a theoretical optimizer.

**Theorem 3.6**.: _Let \( h(,)\) be continuous for all \(\) and \(_{n}_{p_{*}}(_{n})=_{p_{*}}(_{*})\) almost surely (e.g., as ensured in Theorem 3.4). Then, almost surely, \(_{n}_{n}=\) implies \(_{p_{*}}()=_{p_{*}}(_{*})\)._

## 4 Monte Carlo Approximation

In what follows, we fix a sample \(^{n}\) and propose simulation strategies to estimate \(V_{^{n}}()\). The latter, in fact, is analytically intractable due to the infinite dimensionality of \(Q_{^{n}}\). To that end, we exploit a key representation of DPs first established by : If \(p(,q)\), then \(p}}}{{=}}_{j=1}^{}p_{j}_{j}\), where \(_{j}}}}{{}}q\) and the sequence of weights \((p_{j})_{j 1}\) is constructed via a stick-breaking procedure based on iid \((1,)\) samples (see Appendix B). Thus, for large enough integers \(T\) and \(N\), we propose the following Stick-Breaking Monte Carlo (SBMC) approximation for \(V_{^{n}}()\):

\[_{^{n}}(,T,N):=_{i=1}^{N}_ {j=0}^{T}p_{ij}h(,_{ij}),\] (3)

where, \(T\) denotes the number of stick-breaking steps performed before truncating each Monte Carlo sample from the DP posterior \(Q_{^{n}}\), while \(N\) denotes the number of such samples. Algorithm 1 in Appendix B details the procedure, which essentially approximates the posterior DP via truncation and takes expectations accordingly.

_Remark 4.1_.: We propose to truncate the stick-breaking procedure at some fixed step \(T\). Another strategy would involve truncating it at a random step \(T_{i}():=t:_{j=1}^{t}p_{ij} }\) for some small \(>0\). This allows to directly control the approximation error at each Monte Carlo sample , though it leads to simulated measures with supports of different cardinalities. For the sake of theory, we opt for the fixed-step/random-error approximation, though the random-step/fixed-error one is equally viable in practice.

_Remark 4.2_.: On top of being a theory-based approximation for \(V_{^{n}}()\), the criterion (3) can be interpreted as implementing a form of _robust Bayesian bootstrap_. Instead of directly averaging the risk \(h(,)\) with respect to the empirical distribution, we first obtain \(N\) bootstrap samples of size \(T\) from the predictive (which is a compromise between the empirical and the prior centering distributions), we weight observations according to the stick-breaking procedure, and finally take a grand average of the \(\)-transformed weighted sums. This connection with the Bayesian bootstrap suggests the following alternative Multinomial-Dirichlet Monte Carlo (MDMC) version of \(V_{^{n}}()\):

\[_{i=1}^{N}_{j=1}^{T}w_{ij}h(,_{ij}) ,\]

where \((w_{i1},,w_{iT})}}{{}}T;,,\) and the atoms are iid according to the predictive (see Algorithm 2 in Appendix B).1 For practical computation, we recommend using the MDMC approximation, as it tends to yield more balanced weights, compared to SBMC, even for low values of \(T\).

With the following results, we ensure finite-sample and asymptotic guarantees on the closeness of optimization procedures based on the SBMC approximation versus the target \(V_{^{n}}\).

**Lemma 4.3**.: _Assume \(\) is a bounded subset of \(^{d}\) and, for all \(\), \( h(,)\) is \(c()\)-Lipschitz continuous. Then, for all \(T,N 1\) and \(>0\),_

\[_{}_{^{n}}(,T,N)-V_{^{ n}}() M_{}K ^{T}+\]

_with probability at least_

\[1-2(C_{T}()}{} )^{d}\{-}{4(K)(6( K)+)}\}+\{-\} \]

_for some constant \(C_{T}>0\)._

Heuristically, the bound in Lemma 4.3 is obtained by decomposing the left-hand side of the inequality into a first term depending on the truncation error induced by the threshold \(T\), and a second term reflecting the Monte Carlo error related to \(N\). Moreover, analogously to Lemma 3.2, Lemma 4.3 easily implies finite-sample bounds on the excess "robust risk" \(V_{^{n}}(_{n}(T,N))-V_{^{n}}(_{n})\), where \(_{n}(T,N)_{}_{^{n}}( ,T,N)\). Another consequence is the following asymptotic convergence Theorem, whose proof is analogous to that of Theorem 3.4.

**Theorem 4.4**.: _Under the same assumptions of Lemma 4.3, and if \(_{T 1}C_{T}<\) (see Appendix A for details on \(C_{T}\)),_

\[_{T,N}_{}_{^{n}}(,T,N)-V_{^{n}}()=0\]

_almost surely. Also, almost surely_

\[_{T,N}_{^{n}}(_{n}(T,N),T,N)=V_{^{n}}(_{n}),_{T,N}V_{^{n}}(_ {n}(T,N))=V_{^{n}}(_{n}).\]

In words, Theorem 4.4 ensures that, as the truncation and MC approximation errors vanish, the optimal approximate criterion value converges to the optimal exact one, and that the exact criterion value at any approximate optimizer converges to the exact optimal value. Also note that Theorems 3.4 and 4.4, when combined, provide guarantees on the convergence of \(_{^{n}}(_{n}(T,N),T,N)\) (the empirical criterion one has optimized in practice) to \(_{p_{*}}(_{*})\) (the theoretical optimal target) as the sample size increases and the DP approximation improves.

Finally, as a byproduct of Theorem 4.4, convergence of any approximate robust optimizer to an exact one is established as follows.11

**Theorem 4.5**.: _Let \( h(,)\) be continuous for all \(\). Moreover, assume_

\[_{T,N}V_{^{n}}(_{n}(T,N))=V_{^{n}}( _{n})\]

_almost surely (e.g., as ensured above). Then, almost surely, \(_{T,N}_{n}(T,N)=_{n}\) implies \(V_{^{n}}(_{n})=V_{^{n}}(_{n})\)._Experiments

We applied our robust optimization procedure to a host of simulated and real datasets, and we report results in this Section. Before proceeding, we notice that, given the finite approximations proposed in Section 4 and under mild regularity assumptions on the loss function \(h\), the proposed robust criterion is amenable to standard gradient-based optimization procedures (see Appendix C for further details and an insightful interpretation of the gradient of our criterion as yielding _robustly weighted stochastic gradient descent steps_).

Simulation Studies.We tested our method on three different learning tasks featuring a high degree of distributional uncertainty in the data generating process, and compared performance to the corresponding ambiguity neutral (i.e., simply regularized) and unregularized procedures. First, we performed a high-dimensional sparse linear regression simulation experiment. We simulated 200 independent samples of size \(n=100\) from a linear model with \(d=90\) features (moderately correlated with each other), only the first \(s=5\) of which have unitary positive marginal effect on the scalar response \(y\). Second, we performed a simulation experiment on univariate Gaussian mean estimation in the presence of outliers. We simulated 200 independent samples of 13 observations, 10 of which come from a 0-mean Gaussian distribution and 3 from an outlier distribution, and tested the ability of the three methods to recover the true mean (i.e., 0). Third, we performed a simulation experiment on high-dimensional sparse logistic regression for binary classification. We set up a data-generating mechanism similar to the linear regression experiment, where a small subset of features linearly influence the log odds-ratio.

Appendix C collects further details on the above experiments as well as plots summarizing the results (see Figures 2, 3, and 4). All three experiments reveal the ability of our robust method to improve out-of-sample performance and estimation accuracy in two ways, i.e., (i) by yielding good results on average, and especially (ii) by reducing performance variability. The latter is a key robustness property that our method is designed to achieve.

Real Data Applications.We tested our method on three diverse real-world datasets. In the first study, we applied our method to predict diabetes development based on a host of features, as collected in the popular and public Pima Indian Diabetes dataset. Because the outcome is binary, we used logistic regression as implemented (i) with our robust method, (ii) with \(L_{1}\) regularization, and (iii) in its plain, unregularized version. We selected hyperparameters via cross-validation and tested the out-of-sample performance of the three methods applied to disjoint batches of training observations to assess the methods' performance variability. As we report in Appendix C, our robust method outperforms both alternatives on average and does significantly better in reducing variability.

We performed two further studies on linear regression applied to two popular UCI Machine Learning Repository datasets: The Wine Quality dataset  and the Liver Disorders dataset . Similarly to the first study, we compared the performance of our method to OLS (unregularized) estimation and \(L_{1}\)-penalized (LASSO) regression. After cross-validation for parameter selection, we train the models multiple times on separate batches of data and compute out-of-sample performance on a large held-out set of observations. As the results reported in Appendix C show, also in these settings our robust DP-based method performs better than the alternatives both on average and especially in terms of lower variability. Taken together, the experimental results described in this Section corroborate empirically the robustness properties of the proposed criterion, as examined theoretically throughout the paper.

## 6 Discussion

The paper tackled the problem of optimizing a data-driven criterion in the presence of distributional uncertainty about the data-generating mechanism. To mitigate the underperformance of classical methods, we introduced a novel distributionally robust criterion, drawing insights from Bayesian nonparametrics and a decision-theoretic model of smooth ambiguity aversion. We established connections with standard regularization techniques, including Ridge and LASSO regression, and theoretical analysis revealed favorable finite-sample and asymptotic guarantees on the performance of the robust procedure. For practical implementation, we presented and examined tractable approximations of the criterion, which are amenable to gradient-based optimization. Finally, we applied our method to a variety of simulated and real datasets, offering insights into its practical robustness properties. Naturally, our work presents some limitations that give rise to interesting directions for future research. In particular, we note the need for a deeper examination of the model workings in terms of (i) its parameter configuration and (ii) its broader application to general learning tasks (e.g., when the loss function is adapted to accommodate deep learning architectures). Moreover, our method, as many others in the distributional robustness literature, is only suited to process homogeneously generated (e.g., iid or exchangeable) data, leaving room to explore extensions to more complex dependence structures. Finally, we highlight that our study offers prospects for investigating connections among such varied yet interconnected strands of literature as optimization, decision theory, and Bayesian statistics.