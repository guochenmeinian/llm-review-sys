# Dual Control Variate for Faster Black-box Variational Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of _jointly_ reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.

## 1 Introduction

Black-box variational inference (BBVI) [12; 22; 16; 2] has become a popular alternative to Markov Chain Monte Carlo (MCMC) methods. The idea is to posit a variational family and optimize it to be close to the posterior, using only "black-box" access to the target model (evaluations of the density or gradient). This is done by estimating a stochastic gradient of the KL-divergence and deploying it in stochastic optimization. A key advantage of this procedure is that it allows the use of data subsampling in each iteration, which can greatly speed-up optimization with large datasets.

The optimization of BBVI is often described as a doubly-stochastic optimization problem [30; 27] in that BBVI's gradient estimation involves two sources of randomness: Monte Carlo sampling from the variational posterior and data subsampling from the full dataset. Because of the doubly-stochastic nature, one common challenge for BBVI is the variance of the gradient estimates: If this is very high, it forces very small stepsizes, leading to slow optimization convergence [20; 3].

Numerous works have been devoted to reducing the "Monte Carlo" noise that results from drawing samples from the current variational distribution [19; 25; 8; 9; 4]. These methods can typically be seen as creating an approximation of the objective for which the Monte Carlo noise can be integrated exactly, and using this to define a zero-mean random variable, i.e. a control variate, that is negatively correlated with the original gradient estimator. These methods can be used with subsampling by creating approximations for each datum. However, they are only able to reduce Monte Carlo noise for each datum--they do not reduce subsampling noise. This is critical, as subsampling noise is often the dominant source of gradient variance (Sec. 3).

At the same time, for (non-BBVI) optimization problems with _only_ subsampling noise, the optimization community has developed incremental gradient methods that "recycle" previous gradient evaluations [26; 28; 13; 6; 7], leading to faster convergence. These methods do not address Monte Carlo noise. In fact, due to the way these methods rely on efficiently maintaining running averages, they cannot typically be applied to doubly-stochastic problems at all.

In this paper, we present a method that _jointly_ controls Monte Carlo and subsampling noise in BBVI. The idea is to create approximations of the target for each datum where the Monte Carlo noisecan be integrated exactly. Then, we maintain running averages of the _approximate_ gradients, with noise integrated, overcoming the issue of applying incremental gradient ideas to doubly-stochastic problems. The resulting method not only addresses both forms of noise but _interactions_ between them as well. We demonstrate through a series of experiments with diagonal Gaussian variational inference on a range of probabilistic models that the method leads to lower variance and significantly faster convergence than existing methods.

## 2 Background: Black-box variational inference

Given a probabilistic model \(p(x,z)=_{n=1}^{N}p(x_{n} z)p(z)\) and observed data \(\{x_{1},,x_{N}\}\), variational inference's goal is to find a tractable distribution \(q_{w}(z)\) with parameters \(w\) to approximate the (often intractable) posterior \(p(z x)\) over the latent variable \(z^{d}\). BBVI achieves this by finding the set of parameters \(w\) that minimize the KL-divergence from \(q_{w}(z)\) to \(p(z x)\), which is equivalent to minimizing the negative Evidence Lower Bound (ELBO)

\[f(w)=-*{}_{}*{}_{q_ {w}()}[N p(x_{})+ p( )]-(w),\] (1)

where \((w)\) denotes the entropy of \(q_{w}\).

Since the inner expectation with respect to \(\) is typically intractable, BBVI methods rely on stochastic optimization with unbiased gradient estimates. These gradient estimates are typically obtained using the score function method  or the reparameterization trick [15; 23; 30]. The latter is often the method of choice, as it usually seems to yield estimators with lower variance. The idea is to define a fixed base distribution \(s()\) and a deterministic transformation \(_{w}()\) such that for \( s\), \(_{w}()\) is equal in distribution to \(q_{w}\). Then, the objective from Equation (1) can be re-written as

\[f(w)=*{}_{}*{}_{ }f(w;,), f(w;, )=-N p(x_{n}_{w}())- p(_{w} ())-(w),\] (2)

and its gradient can be estimated "naively" by drawing a random \(n\) and \(\), and evaluating

\[g_{}(w;n,)= f(w;n,).\] (3)

BBVI has two advantages. First, since it only evaluates \( p\) (and its gradient) at various points, it can be applied to a diverse range of models, including those with complex and non-conjugate likelihoods. Second, by subsampling data it can be applied to large datasets that might be impractical for traditional methods like MCMC [12; 16].

## 3 Sources of gradient variance in BBVI

Let \(_{n,}[ f(w;,)]\) denote the variance1 of the naive estimator from Eq. 3. The two sources for this variance correspond to data subsampling (\(n\)) and Monte Carlo noise (\(\)). It is natural to ask how much variance each of these sources contributes. We study this by (numerically) integrating out each of these random variables individually and comparing the variances of the resulting estimators.

Let \(f(w;n)=*{}_{}f(w;n,)\) be the objective for a single datum \(n\) with Monte Carlo noise integrated out. This can be thought of as an estimator for datum \(n\) with a "perfect" control variate. Similarly, let \(f(w;)=*{}_{n}f(w;n,)\) be the objective for a fixed \(\) evaluated on the full dataset. In Fig. 1 we generate a single optimization trace using our gradient estimator (described below). Then, for each iteration, we estimate the variance of \( f(w;,)\), \( f(w;)\), and \( f(w;)\)2 using a large number of samples. In Table 1 we show the variance at the final iterate on a variety of datasets. (For large datasets, it is too expensive to compute the variance this way at each iteration.)

Our empirical findings suggest that, despite the exact mix of the two sources being task dependent, subsampling noise is usually larger than MC noise. They also show the limits of reducing a single source of noise: No control variate applied to each datum could do better than \( f(w;n)\), while no incremental-gradient-type method could do better than \( f(w;)\).

## 4 Dual Control Variate

We now introduce the _dual control variate_, a new approach for controlling the variance of gradient estimators for BBVI. Control variates  can reduce the variance of a gradient estimator by adding a zero-mean random variable that is negatively correlated with the gradient estimator. Considering that the objective of BBVI is a function of both \(n\) and \(\), an ideal control variate should also be a function of these variables. We take two steps to construct such a control variate.

1. Inspired by existing control variates for BBVI [19; 9], we create an approximation \((w;n,)\) of the true objective \(f(w;n,)\), designed so that the expectation \(_{}\,(w;n,)\) can easily be computed for any datum \(n\). A common strategy for this is a Taylor-appproximation--to replace \(f\) with a low-order polynomial. Then, if the base distribution \(s()\) is simple, the expectation \(_{}[(w;n,)]\) is often available in closed-form.
2. Inspired by SAGA , we maintain a table \(W=\{w^{1},,w^{N}\}\) that stores the variational parameters at the last iteration each of the data points \(x_{1},,x_{N}\) were accessed. We also maintain a running average of gradient estimates evaluated at the stored parameters, denoted by \(M\). Unlike SAGA, however, this running average is for the gradients of the _approximation_\(\), with the Monte Carlo noise \(\) integrated out, i.e. \(M=_{n}\,_{}\,(w^{n};n,)\).

Intuitively, as optimization nears the solution, the weights \(w\) tend to change slowly. This means that the entries \(w^{n}\) in \(W\) will tend to become close to the current iterate \(w\). Thus, if \(\) is a good approximation of the true objective, we can expect \( f(w;n,)\) to be close to \((w^{n};n,)\), meaning the two will be strongly correlated. However, thanks to the running average \(M\), the full expectation of \((w^{n};n,)\) is available in closed-form. This leads to our proposed gradient estimator

\[g_{}(w;n,)= f(w;n,)+\,\,\,\,\,(w^{m};m, )-(w^{n};n,)}_{}(w;n,)$}}.\] (4)

   Task & \(_{n,}[ f(w;n,)]\) & \(_{n}[ f(w;n)]\) & \(_{}[ f(w;)]\) \\  Sonar & \(4.04 10^{4}\) & \(2.02 10^{4}\) & \(1.16 10^{4}\) \\ Australian & \(9.16 10^{4}\) & \(8.61 10^{4}\) & \(2.07 10^{3}\) \\ MNIST & \(4.21 10^{8}\) & \(3.21 10^{8}\) & \(1.75 10^{4}\) \\ PPCA & \(1.69 10^{10}\) & \(1.68 10^{10}\) & \(3.73 10^{7}\) \\ Tennis & \(9.96 10^{7}\) & \(9.59 10^{7}\) & \(8.56 10^{4}\) \\   

Table 1: BBVI gradient variance decomposition across various tasks, computed at the optimization endpoint. Using a batch size of 100, step size of \(1\) for MNIST, PPCA, and Tennis, and a batch size of 5, step size of \(5\) for Sonar and Australian. We generally observe subsampling noise \(_{n}[ f(w;n)]\) surpassing MC noise \(_{}[ f(w;)]\).

Figure 1: **Gradient Variance Decomposition in Bayesian Logistic Regression using Mean-field BBVI.** The orange line denotes variance from data subsampling (\(n\)), and the green line denotes Monte Carlo (MC) noise variance (\(\)). For Sonar, both noise sources exhibit similar scales with a batch size of 5. However, for Australian, subsampling noise dominates. Regardless, our proposed gradient estimator \(g_{}\) (red line, Eq. (4)) mitigates subsampling noise and controls MC noise, aligning closely with or below the green line (i.e. the variance without data subsampling) in both datasets.

[MISSING_PAGE_FAIL:4]

In doubly-stochastic problems with objectives of the form \(f(w;n,)\), data \(n\) is subsampled as well as \(\). While the above control variate has most commonly been used without subsampling, it can also be used with subsampling, by developing an approximation \((w;n,)\) to \(f(w;n,)\) for each datum \(n\). This leads to the control variate \(_{}\,(w;n,)-(w ;n,)\) and gradient estimator

\[g_{}(w;n,)= f(w;n,)+ \,(w;n,)-(w;n,)}_{}(w;n,)$}}.\] (8)

It is important to note that this control variate is unable to reduce variance coming from data subsampling. Even if \((w;n,)\) were a _perfect_ approximation there would still be gradient variance due to \(n\) being sampled randomly. This can be shown by noting that the variance of this estimator is given by (see Appendix. B.1 for a full derivation using the law of total variance)

\[[g_{}]=\,}{} \,}{}[ f(w;n,)-( w;,)]+[ f(w;n)][ f(w; )].\] (9)

While the first term of the expression above can be made arbitrarily small in the ideal case of a perfect approximation \( f\), the second term is irreducible, regardless of the quality of the approximation used. Therefore, this approach cannot reduce subsampling variance. As shown in Fig. 2 and Table 1, subsampling variance is typically substantial, and often several orders of magnitude larger than Monte-Carlo variance. When this is true, this control variate, which is only able to reduce variance coming from Monte Carlo sampling, will have minimal effect on the overall gradient variance.

### Data subsampling and incremental gradient methods

We now consider a stochastic optimization problem with objective \(f(w)=_{}\,f(w;)\), where \(\) is uniformly distributed on \(\{1,,N\}\), representing data indices, but no other stochasticity (i.e. no Monte Carlo sampling). While one could compute \(f\) or its gradient exactly, this is expensive when \(N\) is large. A popular alternative involves drawing a random \(\) and using the estimator \( f(w;)\) with a stochastic optimization method, such as stochastic gradient descent. Alternatively, for such problems, _incremental gradient_ methods [26; 28; 13; 7; 10] often lead to faster convergence.

While details vary by algorithm, the basic idea of incremental gradient methods is to "recycle" previous gradient evaluations to reduce randomness. For example, SAGA  stores the parameters \(w^{n}\) of the most recent iteration where \(f(w;n)\) was evaluated and takes a step as

\[w w-( f(w;n)+\,}{ }\, f(w^{};)- f(w^{n};n)),\] (10)

where \(\) is a step size and the expectation over \(m\) is tracked efficiently using a running average, meaning the cost per iteration is independent of \(N\). The update rule above can be interpreted as using a control variate to reduce the variance of the naive estimator \( f(w;n)\) as

\[g(w;n)= f(w;n)+\, f(w^{}; )- f(w^{n};n)}_{}.\] (11)

When \(w^{m} w\), the first and last terms in Eq. (11) will approximately cancel, leading to a gradient estimator with significantly lower variance.

We now consider a doubly-stochastic objective \(f(w;n,)\). In principle, one might consider computing the estimator from Eq. (11) for each value of \(\), i.e. using the gradient estimator

\[g_{}(w;n,)= f_{n}(w;n,)+\, f(w^{};,)- f(w^{n};n, )}_{}(w;n,)$}}.\] (12)

This has two issues. First, the resulting method does not address Monte Carlo noise due to sampling \(\). This can be shown by noting that the variance of this estimator is given by (see Appendix B.2)

\[[g_{}]=\,}{} \,[ f(w;,)- f(w^{}; ,)]+\,[ f(w;)][  f(w;)].\] (13)

Since the second term in the variance expression above is irreducible, the variance cannot be expected to go to zero, no matter how close all the stored vectors \(w^{n}\) are to the current parameters. Intuitively, this approach cannot do better than simply evaluating the objective on the full dataset for a random \(\)The second issue is more critical: \(g_{}\)_cannot be implemented efficiently_. The value of \( f(w^{n};n,)\) is dependent on \(\), which is resampled at each iteration. Therefore, it is not possible to efficiently maintain \(_{}\, f(w^{};,)\) needed by Eq. (12) as a running average. In general, this can only be computed by looping over the full dataset in each iteration. While possible, this destroys the computational advantage of subsampling. For some models with special structure [32; 34] it is possible to efficiently maintain the needed running gradient. However, this can only be done in special cases with model-specific derivations, breaking the universality of BBVI.

It may seem odd that \(g_{}\) has these computational issues, while \(g_{}\)--an estimator intended to reduce variance even further--does not. The fundamental reason is that the dual estimator only stores (approximate) gradients after integrating over the Monte Carlo variable \(\), so the needed running average is independent of \(\).

### Ensembles of control variate

It is possible to combine multiple control variates. For example,  combined control variates that reduced Monte Carlo noise  with one that reduced subsampling noise  (for a special case where \(g_{}\) is tractable). While this approach can be better than either control variate alone, it still does not reduce _joint_ variance. To see this, consider a gradient estimator that uses a convex combination of the two above control variates. For any \((0,1)\) write

\[g_{}(w;n,)= f(w;n,)+}(w;n,)+(1-)c_{}(w;n,)}_{c_{ }(w;n,)}.\] (14)

It can be shown (Appendix B.3) that if both \(c_{}\) and \(c_{}\) are "perfect", that is, if \((w;n,)=f(w;n,)\) and \(w^{n}=w\) for all \(n\), then

\[[g_{}]=^{2}\,_{}[ f (w;)]+(1-)^{2}\,_{}[ f(w;)].\] (15)

Even in this idealized scenario, such an estimator cannot reduce variance to zero, because each of the individual control variates leaves one source of noise uncontrolled. The dual control variate overcomes this because it models interactions between \(\) and \(n\).

## 6 Related work

Recent work proposed to approximate the optimal batch-dependent control variate for BBVI using a recognition network . Similar to our work, they take into account the usage of subsampling when designing their variance reduction techniques for BBVI. However, like \(g_{}\), their control variate reduces the _conditional_ variance of MC noise (conditioned on \(n\)) but is unable to reduce subsampling noise (like \(g_{}\)).

It is also worth discussing a special incremental gradient method called SMISO , designed for doubly-stochastic problems. Intuitively, SMISO uses exponential averaging to approximately marginalize out \(\), and then runs MISO/Finito [7; 18] (an incremental gradient method similar to SAGA) to control the subsampling noise. While the method is similar to running SGD with an incremental control variate, it is not obvious how to separate the control variate from the algorithm, meaning we cannot use the SMISO idea as a control variate to get a gradient estimator that can be used with other optimizers like Adam, we include a detailed discussion on this issue in Appendix. A. Nevertheless, we still include SMISO as one of our baselines.

## 7 Experiments

This section empirically demonstrates the effectiveness of the dual control variate for BBVI. We focus on mean-field Gaussian BBVI, where the variational posterior follows a multivariate Gaussian with diagonal covariance \(q_{w}(z)=(,(^{2}))\), with parameters \(w=(,())\).

The gradient estimators \(g_{}(w;n,)\) and \(g_{}(w;n,)\) require an approximation function with expectation over \(\) available in closed form. Inspired by previous work , we get an approximation for \(f(w;n,)\) using a second order Taylor expansion for the negative total likelihood \(N p(x_{n} z)+ p(z)\) around \(z_{0}=_{w}(0)\)3, which yields

\[(w;n,)=k_{n}(z_{0})+(_{w}()-z_{0} )^{} k_{n}(z_{0})+(_{w}()-z_{0} )^{}^{2}k_{n}(z_{0})(_{w}()-z_{0})^{}+ (w),\] (16)

where we assume the entropy can be computed in closed-form. For a mean-field Gaussian variational distribution, the expected gradient of the approximation Eq. (16) can only be computed efficiently (via Hessian-vector products) with respect to the mean parameter \(\) but not for the scale parameter \(\), which means \(g_{}(w;n,)\) and \(g_{}(w;n,)\) can only be used as the gradient estimator for \(\). Fortunately, controlling only the gradient variance on \(\) often means controlling most of the variance, as, with mean-field Gaussians, the total gradient variance is often dominated by variance from \(\).

### Experiment setup

We evaluate our methods by performing BBVI on a range of tasks: binary Bayesian logistic regression on two datasets, Sonar (number of samples \(N=208\), dimensionality \(D=60\)) and Australian (\(N=690\), \(D=14\)); multi-class Bayesian logistic regression on MNIST  (\(N=60000\), \(D=7840\)); probabilistic principal component analysis  (PPCA, \(N=60000\), \(D=12544\)); and Bradley-Terry model  for tennis player ranking (Tennis, \(N=169405,D=5525\)). We give full model descriptions in Sec. 7.3.

**Baselines.** We compare \(g_{}\) (Eq. (4)) with \(g_{}\) (Eq. (3)) and \(g_{}\) (Eq. (8)). For Sonar and Australian (small datasets) we include \(g_{}\) (Eq. (12)) as well, which requires a full pass through the full dataset at each iteration. For larger-scale problems, \(g_{}\) becomes intractable, so we use SMISO instead.

**Optimization details.** We optimize using Adam  for the larger-scale MNIST, PPCA, and Tennis datasets and SGD without momentum for the small-scale Sonar and Australian dataset for transparency. The optimizer for SMISO is pre-determined by its algorithmic structure and cannot be changed. For all estimators, we perform a step-size search (see Appendix C) to ensure a fair comparison and use a single shared \(\) for all samples in the batch.

**Mini-batching.** In practice, for efficient implementation on GPUs, we draw a mini-batch \(\) of data at each iteration (reshuffling for each epoch). For \(\), \(\), and SMISO, we update multiple entities in the parameter table per iteration and adjust the running mean accordingly. For the Sonar and Australian datasets, due to their small sizes, we use \(||=5\). For other datasets we use \(||=100\).

**Evaluation metrics.** We track the ELBO on the full dataset, explicitly computing \(_{}\) (summing over the full dataset) and approximating \(_{}\) with \(5000\) Monte Carlo samples. We present ELBO vs. iterations plots for a single example learning rate as well as ELBO values for the best learning rate chosen retrospectively for each iteration. In addition, we present the final ELBO after training vs. step size at different iterations. For the Sonar and Australian datasets, given the small size, we include a detailed trace of gradient variance on \(\) across different estimators. This enables empirical validation of the lower bounds derived in Eq. (9) and Eq. (13).

Figure 2: **Dual control variate helps reduce gradient variance. The naïve gradient estimator (Eq. (3)) is the baseline, while the \(\) estimator (Eq. (8)) controls the Monte Carlo noise, the \(\) estimator (Eq. (12)) controls for subsampling noise, and the proposed \(\) estimator (Eq. (4)) controls for both. The variance of \(\) and \(\), as is shown in Eq. (9) and Eq. (13) are lower-bounded by the dotted lines, while \(\) is capable of reducing the variance to significantly lower values, leading to better and faster convergence (Fig. 3).**

**Initialization.** The variational parameters are randomly initialized using a standard Gaussian and all results reported are averages over multiple independent trials: We run 10 trials for Sonar and Australian, and 5 for the larger scale problems due to resource constraint.

### Results

The experiment results for Sonar and Australian are presented in Fig. 2 and Fig. 3. Both the \(\) and \(\) estimators have lower variance than the \(\) estimator, but the improvement varies by the dataset. The excellent performance of the (impractical) \(\) estimator on Australian shows the importance of reducing subsampling noise. Overall, the \(\) estimator has the lowest variance, which enables larger learning rates and thus faster optimization.

Similar results can be observed on MNIST, PPCA, and Tennis in Fig. 4 (for these datasets \(\) is intractable, so we include SMISO as a baseline instead). Again, \(\) yields faster and better convergence than \(\) and \(\). Whereas SMISO, which does not adopt momentum nor adaptive step size, suffers from slow convergence speed in that it has to utilize a small step size to prevent diverging during optimization. We provide comparisons of different estimators using SGD in Appendix. E.

### Model descriptions

**Binary/Multi-class Bayesian logistic regression.** A standard logistic regression model with standard Gaussian prior.

**Probabilistic principal component analysis (PPCA).** Given a centered dataset \(_{1},,_{N}^{D}\), PPCA  seeks to extract its principal axes \(^{D K}\) by assuming \(}_{n}(,^{}+( ^{2}))\). In our experiments, we employ a standard Gaussian prior on \(\) and use BBVI to approximate the posterior over \(\). We then test PPCA on the standardized training set of MNIST with \(K=16\) and \(=\).

**Bradley Terry model (Tennis).** This is a model used to rank players from pair-wise matches. Each player is represented by a score \(_{i}\), and each score is assigned a standard Gaussian prior. The result of a match between two players is modeled by the inverse logit of their score difference \(_{n}(^{-1}(_{i}-_{j}))\) where \(_{n}=1\) denotes a win by player \(n\). We subsample over matches and perform inference over the score of each player. We evaluate the model on men's tennis matches log starting from 1960, which contains the results of \(169405\) matches among \(5525\) players.

Figure 3: **With reduced variance (Fig. 2), the dual estimator provides better convergence at a larger step size.** On Sonar, Monte Carlo noise and subsampling noise are of similar scale, therefore jointly controlling them shows better performance than methods that only control one source of noise. On Australian, where the subsampling noise dominates, \(\) shows similar performance compared with \(\), which controls subsampling noise but _cannot_ be efficiently computed (requires pass over the full dataset at each iteration).

### Efficiency analysis

We now study the computational cost of different estimators. In terms of the number of "oracle" evaluations (i.e. evaluations of \(f(w;n,)\) or its gradient), the naive estimator is the most efficient, requiring a single oracle evaluation per iteration. The \(\) estimator requires one gradient and also one Hessian-vector product, while the \(\) estimator needs one gradient and two Hessian-vector products, one for the control variate and one for updating the running mean \(M\).

Additionally, Table 2 shows measured runtimes based on a JAX implementation on an Nvidia 2080ti GPU. All numbers are for a single optimization step, averaged over 200 steps. Overall, each iteration with the \(\) estimator is between 1.5 to 2.5 times slower than \(\), and around 1.2 times slower than \(\). Lastly, given that \(\) achieves a given performance in an order of magnitude fewer iterations (Figs. 3 and 4), it is the fastest in terms of wall-clock time. The exact wall-clock time v.s. ELBO results are presented in Appendix. F.

    &  &  &  \\   & & & MNIST & PPCA & Tennis \\  naive & \(_{n,}[ f(w;,)]\) & 1 & 10.4ms & 12.8ms & 10.2ms \\ \(\) & \(_{n}[ f(w;)]\) & 2 & 12.8ms & 18.5ms & 14.6ms \\ \(\) & \(_{}[ f(w;)]\) & N+2 & 328ms & 897ms & 588ms \\ \(\) & 0 & 3 & 17.6ms & 31.2ms & 29.6ms \\ \(\) & \(_{}[ f(w;)]\) & N & 201ms & 740ms & 203ms \\ \(c_{cv}\) & 0 & 2N & 360ms & 1606ms & 246ms \\   

Table 2: Variance, oracle complexity, and wall-clock time for different estimators. Notice that \(\) is more expensive than \(\). We hypothesize this is because \(\) uses separate \(w^{n}\) for different data points, which is less efficient for parallelism.

Figure 4: **On larger scale problems, the dual estimator leads to improved convergence.** In large-scale problems, \(\) shows little or no improvement upon naive while \(\) converges faster. We suspect that most of the improvement in the dual estimator comes from reducing subsampling variance. SMISO shows slow convergence. We suspect that is because it is an “SGD-type” algorithm while all others use Adam. Note that the step size for SMISO is rescaled for visualization. The loss shows periodic structure in Tennis, this happens because gradients have correlated noise that cancels out at the end of each epoch.