# Provable convergence guarantees for black-box variational inference

Justin Domke

University of Massachusetts Amherst

domke@cs.umass.edu

&Guillaume Garrigos

Universite Paris Cite and Sorbonne Universite, CNRS

Laboratoire de Probabilites, Statistique et Modelisation

F-75013 Paris, France

garrigos@lpsm.paris

Robert Gower

Center for Computational Mathematics

Flatiron Institute, New York

rgower@flatironinstitute.org

###### Abstract

Black-box variational inference is widely used in situations where there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs--namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides rigorous guarantees that methods similar to those used in practice converge on realistic inference problems.

## 1 Introduction

Variational inference tries to approximate a complex target distribution with a distribution from a simpler family . If \(p(z,x)\) is a target distribution with latent variables \(z^{d}\) and observed data \(x^{n}\), and \(q_{w}\) is a variational family with parameters \(w\), the goal is to minimize the negative evidence lower bound (ELBO)

\[f(w)\ :=\ }_{x q_{w}} p( ,x)}_{l(w)}+}_{x q_{w}} q _{w}()}_{h(w)}. \]

For subsequent discussion, we have decomposed the objective into the _free energy_\(l\) and the _negative entropy_\(h\). Minimizing \(f\) is equivalent to minimizing the KL-divergence between \(q_{w}\) to \(p(|x)\), because \(}(q_{w}\|p(|x))=f(w)+ p(x)\). Recent research has focused on "black box" variational inference, where the target distribution \(p\) is sufficiently complex that one can only access it through evaluating probabilities (or gradients) at chosen points . Crucially, one can still get stochastic _estimates_ of the variational objective \(f\) and of its gradient, and use these to optimize.

Still, variational inference can sometimes be unreliable , and some basic questions remain unanswered. Most notably: does stochastic optimization of \(f\) converge to a minimum of the objective? There has been various progress towards answering this question. One line of research seeks to determine if the variational objective \(f\) has favorable structural properties, such as smoothness or (strong) convexity  (Sec. 2.1). Another line seeks to control the "noise" (variance or expected squared norm) of different gradient estimators  (Sec. 2.2). However, fewfull convergence guarantees are known. That is, there are few known cases where applying a given stochastic optimization algorithm to a given target distribution is known to converge at a given rate.

We identify two fundamental barriers preventing from analysing this VI problem as a standard stochastic optimization problem. First, the gradient noise depends on the parameters \(w\) in a non-standard way (Sec. 2.3). This adds great technical complexity and renders many traditional stochastic optimization proofs inapplicable. Second, stochastic optimization theory typically requires that the (exact) objective function \(f\) is Lipschitz continuous or Lipschitz smooth. But in our VI setting, under some fairly benign assumptions, the ELBO is neither Lipschitz continuous nor Lipschitz smooth.

We obtain non-asymptotic convergence guarantees for this problem, under simple assumptions.

**Central contributions (Informal).** Suppose that the target model \( p(,x)\) is concave and Lipschitz-smooth, and that \(q_{w}\) is in a Gaussian variational family parameterized by the mean and a factor of the covariance matrix (Eq. 2). Consider minimizing the negative ELBO \(f\) using either one of the two following algorithms:

* a proximal stochastic gradient method, with the proximal step applied to \(h\) and the gradient step applied to \(l\), estimating \( l(w)\) with a standard reparameterization gradient estimator (Eq. 5), and using a triangular covariance factor;
* a projected stochastic gradient method, with the gradient applied to \(f=l+h\), estimating \( f(w)\) using either of two common gradient estimators (Eq. 7 or Eq. 9), with the projection done over symmetric and non-degenerate (Eq. 3) covariance factors

Then, both algorithms converge with a \(1/\) complexity rate (Cor. 12), or \(1/T\) if we further assume that \( p(,x)\) is _strongly_ concave (Cor. 13).

We also give a new bound on the noise of the "sticking the landing" gradient estimator, which leads to faster convergence when the target distribution \(p\) is closer to Gaussian, up to _exponentially_ fast convergence when it is exactly Gaussian (Cor. 14).

This is achieved through a series of steps, that we summarize below.

1. We analyze the _structural properties_ of the problem. Existing results show that with a Gaussian variational family, if \(- p(,x)\) is (strongly) convex or Lipschitz smooth, then so is the free energy \(l\). This is for instance known to be the case for some generalized linear models, and we give a new proof of convexity and smoothness for some hierarchical models including hierarchical logistic regression (see Appendix 7.3). The remaining component of the ELBO, the neg-entropy \(h\), is convex when restricted to an appropriate set. It is not smooth, but it was recently proved to be smooth over a certain non-degeneracy set.
2. We study the noise of three common _gradient estimators_. They do not satisfy usual noise bounds, but we show that they all satisfy a new quadratic bound (Definition 5). For the sticking-the-landing estimator, our bound formalizes the longstanding intuition that it should have lower noise when the variational approximation is strong (Thm. 4).
3. We identify and solve the key _optimization_ challenges posed by the above issues via new convergence results for the proximal and projected stochastic gradient methods, when applied to objectives that are smooth (but not _uniformly_ smooth) and with gradient estimators satisfying our quadratic bound.

### Related work

Recently, Xu and Campbell  analyzed projected-SGD (stochastic gradient descent) for Gaussian VI using the gradient estimator we will later call \(_{}\) (7), with a particular rescaling. They show that, _asymptotically_ in the number of observed data, their method converges locally with a rate of \(1/\), under mild assumptions. Our results are less general in requiring convexity, but are non-asymptotic, apply with other gradient estimators, and give a faster \(1/T\) rate with strong convexity.

Lambert et al.  introduce a VI-like SGD algorithm, derived from a discretisation of a Wasserstein gradient flow, and show it converges at a \(1/T\) rate for the 2-Wasserstein distance when the log posterior is smooth and strongly concave. This line was continued by Diao et al. , who propose a proximal-SGD method based on the decomposition \(f=l+h\). They obtain a \(1/\) rate when the log posterior is smooth and convex, and a \(1/T\) rate when it is smooth and strongly convex. Unlike typical black-box VI algorithms used in practice, these algorithms require computing the Hessian of the posterior. We analyze more straighforward applications of SGD to the VI problem using standard gradient estimators. Under the same assumptions, our algorithms have the same rates for KL-divergence, which imply the same rates for 2-Wasserstein distance by of Pinsker's inequality and that the total-variation norm upper-bounds Wasserstein distance (Pinsker, 2016, Remark 8.2).

In concurrent work, Kim et al. , consider a proximal-SGD method similar to our approach in Sec. 6. They obtain a \(1/T\) convergence rate, similar to what Cor. 12 gives when the log posterior is strongly concave. They also consider alternative parametrizations of the scale parameters that render the ELBO globally smooth, and obtain a nonconvex result: Under a relaxed version of the Polyak-Lojasiewicz inequality, they can guarantee a \(1/T^{4}\) rate.

## 2 Properties of Variational Inference (VI) problems

Traditionally, the ELBO was optimized using message passing methods, which essentially assume that \(p\) and \(q\) are simple enough that block-coordinate updates are possible [2; 3; 4]. However, in the last decade a series of papers developed algorithms based on a "black-box" model where \(p\) is assumed to be complex enough that one can only evaluate \( p\) (or its gradient) at selected points \(z\). The key observation is that even if \(p\) is quite complex, it is still possible to compute _stochastic estimates_ of the gradient of the ELBO, which can be deployed in a stochastic optimization algorithm [5; 6; 8; 9; 10].

This paper seeks rigorous convergence guarantees for this problem. We study the setting where the variational family is the set of (dense) multivariate Gaussian distributions, parameterized in terms of \(w=(m,C)\), where \(m\) is the mean and \(C\) is a factor of the covariance, i.e.

\[q_{w}(z)=(z|m,CC^{}). \]

We optimize over \(w\), where \(=\{(m,C):C 0\}\) and will further require \(C\) to be either symmetric or triangular. This does not really change the problem, since for a given covariance matrix \(\) there always exists a symmetric (or lower triangular) positive definite factor \(C\) such that \(CC^{}=\).

Now, is it possible to solve this optimization problem? Without further assumptions, it is unlikely any guarantee is possible, because it is easy to encode NP-hard problems into this VI framework [27; 28]. We discuss below the assumptions we will make to be able to solve the problem.

### Structural properties and assumptions

The properties of the free energy \(l\) depend on the properties of the target distribution \(p\). It is necessary to assume that \(p\) is somehow "nice" to ensure the problem can be solved. Titsias and Lazaro-Gredilla [17, Proposition 1] showed that if \(- p(,x)\) is convex, then \(l\) is convex too. Challis and Barber [16, Sec 3.2] showed that if the likelihood \(p(|z)\) is convex and the prior \(p(z)\) is Gaussian, then \(l\) is strongly-convex. Domke [13; Theorem 9] showed that if \(- p(,x)\) is \(\)-strongly convex, then \(l\) is \(\)-strongly convex as well, and that the constant is sharp. Similarly, Domke [13; Theorem 1] showed that if \( p(,x)\) is \(M\)-smooth, then \(l\) is also \(M\)-smooth, and that the constant is sharp.

In this paper we make two assumptions about the target distribution \(p\): the negative log-probability \(- p(,x)\) must be _convex_ (or strongly convex), and _Lipschitz smooth_. Section 7.3 (Appendix) gives some example models where these assumptions are satisfied. For example, if the model is Bayesian linear regression, or logistic regression with a Gaussian prior, then \(- p(,x)\) is smooth and strongly convex. In addition, if the target is a hierarchical logistic regression model, then \(- p(,x)\) is smooth and convex.

Assumptions on \(p\) also impact what a minimizer \(w^{*}=(m^{*},C^{*})\) of \(f(w)\) can look like. Intuitively, if the target \( p(,x)\) is \(\)-strongly concave, then we would expect the target distribution to be "peaky". This means that the optimal distribution would be close to a delta function centered at the MAP solution: \(m^{*}\) would be close to some maximum of \( p(,x)\) noted \(\), and the covariance factor \(C^{*}\) would not be too large. This intuition can be formalized: in this context we have \(\|C^{*}\|^{2}+\|m^{*}-\|_{2}^{2} d/\)(13, Theorem 10). Similarly, if \( p(,x)\) is \(M\)-Lipschitz smooth, then we expect that the target is not too concentrated, so we might expect that the optimal covariance cannot not be too small. Formally, it can be shown that the singular values of the covariance factor \(C^{*}\) are greater than \(1/\)[13, Theorem 7].

The properties of the neg-entropy \(h\) are inherited from the choice of the variational family and do not depend on \(p\). Since we consider a Gaussian variational family, \(h\) is known in closed-form. To avoid some technical issues, we will restrict \(h\) so that the covariance factor is positive definite, so \(h(w)\) is equal to \(- C\) up to a constant if \(C\) is positive definite (see Appendix 7.1), and to \(+\) otherwise. So \(h\) inherits the properties of the negative log determinant, meaning it is a proper closed convex function whenever \(C\) is symmetric or triangular (see Appendix 7.2). From an optimization perspective, it is natural to use a gradient-based algorithm. But unfortunately \(h\) is not Lipschitz smooth on its domain, because its gradient can change arbitrarily quickly when the singular values of \(C\) become small. However, it can be shown [13, Lemma 12] that \(h\) is \(M\)-smooth over the following set of _non-singular_ parameters (which contains the solution \(w^{*}\), see the previous paragraph) given by

\[_{M}=\{w=(m,C):C 0_{}(C)}\}. \]

Instead of computing gradients, an optimizer might want to use the proximal operator of \(h\), which can also be computed in closed form (see Sec. 4).

### Gradient estimators

This paper considers gradient estimators based on the "path" method or "reparameterization". These assume some base distribution \(s\) is known, together with some deterministic transformation \(T_{w}\), such that the distribution of \(T_{w}()\) is equal to \(q_{w}\) when \( s\). Then, we can write

\[_{w}}_{ q_{w}}()=}_{ s}_{w}(T_{w}()). \]

In the case of multivariate Gaussians, the most common choice is to use \(s=(0,I)\) and \(T_{w}(u)=Cu+m\), which we will consider in the rest of the paper.

We will consider three different gradient estimators based on the path-type strategy (Eq. 4). We will provide new noise bounds for each of them (all the proofs are in Appendix 8). Our analysis is mostly based on the following general result [20, Thm. 3].

**Theorem 1**.: _Let \(T_{w}(u)=Cu+m\) for \(w=(m,C)\). Let \(:^{d}\) be \(M\)-smooth, suppose that \(\) is stationary at \(\), and define \(=(,0)\). Then_

\[}_{(0,I)}\|_{w}(T_{w }())\|_{2}^{2}(d+1)M^{2}\|m-\|_{2}^{2}+ (d+3)M^{2}\|C\|_{F}^{2}(d+3)M^{2}\|w-\|_{2}^ {2}.\]

_Furthermore, the first inequality cannot be improved._

Intuitively, this result says that the noise of a gradient estimator is lower when the scale parameter \(C\) is small and the mean parameter \(m\) is close to a stationary point.

The first estimator we consider is \( l(w)\) only. It is given by taking \(=- p\) into Eq. 4, i.e.

\[_{}()\ :=\ -_{w} p(T_{w}(),x), (0,I). \]

The next result gives a bound on the noise of this estimator, which is a direct consequence of Theorem 1. The second line uses Young's inequality to bound the noise in terms of (i) the distance of \(w\) from \(w^{*}\) and (ii) a constant determined by the distance of \(w^{*}\) from some _fixed_ parameters \(\).

**Theorem 2**.: _Suppose that \( p(,x)\) is \(M\)-smooth and has a maximum (or stationary point) at \(\), and define \(=(,0)\). Then, for every \(w\) and every solution \(w^{*}\) of the VI problem,_

\[}\|_{}()\|_{2}^{2}  (d+3)M^{2}\|w-\|_{2}^{2}\] \[ 2(d+3)M^{2}\|w-w^{*}\|_{2}^{2}+2(d+3)M^{2}\|w^{*}-\|_{2}^{2}.\]

Second, we consider an estimator of the gradient of the full objective \(l+h\). It is obtained by simply taking the above estimator and adding the true (known) gradient of the neg-entropy, i.e.

\[_{}()\ :=\ _{}()+  h(w),(0,I). \]The noise of \(_{}\) can be bounded since it only differs from \(_{}\) by the deterministic quantity \( h(w)\), and the fact that--provided \(w_{L}\)--the singular values of \(w\) cannot be too small and so \( h(w)\) cannot be too large. This is formalized in the following theorem, where again, the second line relaxes the result into a term based on the distance of \(w\) from \(w^{*}\) plus constants. (Another type of noise bound  for \(_{}\) was obtained by Kim et al.  for diagonal Gaussians and a variety of parameterizations of the covariance.)

**Theorem 3**.: _Suppose that \( p(,x)\) is \(M\)-smooth, that it is maximal at \(\), and define \(=(,0)\). Then, for every \(L>0\), for every \(w_{L}\) and every solution \(w^{*}\) of the VI problem,_

\[\|_{}()\|_{ 2}^{2}  2(d+3)M^{2}\|w-\|_{2}^{2}+2dL\] \[ 4(d+3)M^{2}\|w-w^{*}\|^{2}+4(d+3)M^{2}\|w^{*}-\|^{2}+2dL.\]

While \(_{}\) used the exact gradient of \(h\), it may be beneficial to use a stochastic estimator \(h\) instead. To derive our third estimator, write the gradient of the ELBO as1

\[ l(w)+ h(w)=_{w}}_{x q_{w}}[- p (,x)+ q_{v}()]_{v=w},\]

where the parameters \(v\) serve as a way to "hold \(w\) constant under differentiation". This leads to our third estimator, called the "sticking the landing" (STL) gradient estimator

\[_{}()\ :=\ _{}()+ [_{w} q_{v}(T_{w}())]_{v=w}, (0,I). \]

Intuitively, we expect that \(_{}\) will tend to have lower noise than \(_{}\) when the posterior is well-approximated by the variational distribution. The reason is that, as observed by Roeder et al. , if \(q_{w}(z)\) were a _perfect_ approximation of \(p(z|x)\), then the two terms in Eq. 9 would exactly cancel (for every \(u\)) and so the estimator would have zero variance. Below we formalize this intuition in what we believe is the first noise bound on \(_{}\).

**Theorem 4**.: _Suppose that \( p(,x)\) is \(M\)-smooth. Consider the residual \(r(z):= p(z,x)- q_{w^{*}}(z)\) for any solution \(w^{*}\) of the VI problem, assume that it has a stationary point \(\), and define \(=(,0)\). Then \(r\) is \(K\)-smooth for some \(K[0,2M]\), and for all \(w_{M}\),_

\[\|_{}\|_{2}^{2}  8(d+3)M^{2}\|w-w^{*}\|_{2}^{2}+2(d+3)K^{2}\|w-\| _{2}^{2}\] \[ 4(d+3)(K^{2}+2M^{2})\|w-w^{*}\|_{2}^{2}+4(d+3)K^{2}\|w^{*}-\|_{2}^{2}.\]

_Moreover, if \(p(|x)\) is Gaussian then \(K=0\)._

When the target is Gaussian then \(K=0\) in Eq. 10, meaning that when \(w=w^{*}\), the STL estimator has no variance. This is a distinguishing feature of \(_{}\), as opposed to \(_{}\) or \(_{}\).

### Challenges for optimization

Three major issues bar applying existing analysis of stochastic optimization methods to our VI setting: 1) non-smooth composite objective 2) lack of uniform smoothness and 3) lack of uniformly bounded noise of the gradient estimator.

The first issue is due to the non-smoothness of neg-entropy function \(h\). This means that under the benign assumption that the target \( p\) is smooth, the full objective \(l+h\)_cannot_ be smooth, since a nonsmooth function plus a smooth function is always nonsmooth. This renders stochastic optimization proofs (e.g. those for the "ABC" conditions ) that do not use projections or proximal operators inapplicable.

One way to tackle this first issue would be to use a non-smooth proof technique, but these rely on having a uniform gradient noise bound (our third issue). Alternatively, one can overcome the non-smoothness of the neg-entropy function by either using a proximal operator, or projecting onto a set where \(h\) is smooth, namely \(_{M}\). This is the strategy we pursue.

The second issue is that existing analyses for proximal/projected stochastic methods either rely on a uniform noise bound ([31, Cor. 3.6]) or uniform smoothness , both of which are not known to be true for VI. By uniform smoothness, we refer to the assumption that \( p(T_{w}(u),x)\) is \(M\)-smooth for every \(u\), with \(M\) being independent of \(u\). Instead, we can only guarantee that \( p(T_{w}(u),x)\) is smooth in expectation, i.e. that \(l(w)\) is smooth. Several works [9, Cond. 1][37, Thm. 1][38, Sec. 4][39, Assumption A1][40, Thm. 1][41, Assumption 3.2] assumed that the full objective \(l+h\) is smooth, but we are not aware of cases where this holds in practice for VI and in our parameterization (Eq. 2) this _cannot_ be true if \( p\) is smooth.

The third issue is the lack of a uniform noise bound. Most non-smooth convergence guarantees within stochastic optimization  assume that the noise of the gradient estimator is uniformly bounded by a constant. But this does not appear to be true even under favorable assumptions--e.g. it is untrue if the target distribution is a standard Gaussian. The best that one can hope is that the gradient noise can be bounded by a quadratic that depends on the current parameters \(w\), and--depending on the estimator--even this may only be true when the parameters are in the set \(_{M}\). Thus, our main optimization contribution is to provide theoretical guarantees for stochastic algorithms under such a specific noise regime, which we make precise in the next definition.

**Definition 5**.: Let \(\) be a differentiable function. We say that a random vector \(\) is a **quadratically bounded estimator** for \(\) at \(w\) with parameters \((a,b,w^{*})\), if it is unbiased \([]=(w)\) and if the expected squared norm is bounded by a quadratic function of the distance of parameters \(w\) to \(w^{*}\), i.e. \(\|\|_{2}^{2} a\|w-w^{*}\|_{2}^{ 2}+b\).

The noise bounds derived in Section 2.2 for the estimators \(_{}\), \(_{}\), and \(_{}\) imply that they are _uniformly quadratically bounded estimators_. See Appendix 8.2 for a table with the corresponding constants \(a\) and \(b\).

## 3 Stochastic Optimization with quadratically bounded estimators

In this section we give new convergence guarantees for the Prox-SGD algorithm and the Proj-SGD algorithm with quadratically bounded gradient estimators. Because these may be of independent

**Description** & **Definition** \\ Estimator for \( l\) & \(_{}=-_{w} p(T_{w}(u),x)\) \\ Estimator for \( l+ h\) & \(_{}=-_{w} p(T_{w}(u),x)+ h(w)\) \\ Estimator for \( l+ h\) & \(_{}=-_{w} p(T_{w}(u),x)+_{w} q_{v}(T_{ w}(u))|_{v=w}\) \\ Constraint set & \(_{M}=\{(m,C):C 0,\;_{}(C) 1/\}\) \\ Optimum & \(w^{*}*{argmin}_{w}l(w)+h(w)\) \\ Stationary point of \(l\) & \(=(,0)\) for any \(\) that is a stationary point of \( p(,x)\) \\
**Condition on \(- p(z,x)\)** & **Consequence** \\ none & \(h(w)\) is convex when \(C\) is symmetric or triangular \\  & \(h(w)\) is \(M\)-smooth over \(_{M}\) \\ convex & \(l(w)\) is convex \\ \(\)-strongly convex & \(l(w)\) is \(\)-strongly convex \\  & \(\|w^{*}-\|_{2}^{2}\) \\ \(M\)-smooth & \(l(w)\) is \(M\)-smooth \\  & \(w^{*}_{M}\) \\  & \(_{}\), \(_{}\), and \(_{}\) are quadratically bounded \\ 

Table 1: Table of known black-box VI properties relevant to optimization. In some cases there could be multiple valid \(w^{*}\) or \(\) in which case these results hold for all simultaneously.

interest, they are presented generically, without any reference to the VI setting. We specialize these results to VI in Section 4. For both algorithms, we present results assuming the problem to be strongly convex or just convex. All proofs for this section can be found in the Appendix (Sec. 9).

### Stochastic Proximal Gradient Descent

Here we want to minimize a function which is the sum of two terms \(l+h\), were both \(l\) and \(h\) are proper closed convex functions, and \(l\) is smooth. For this we will use stochastic proximal gradient.

**Definition 6** (Prox-SGD).: Let \(w^{0}\) be a fixed initial parameters and let \(_{1},_{2},\) be a sequence of step sizes. The stochastic proximal gradient (Prox-SGD) method is given by

\[w^{t+1}=_{_{t}h}(w^{t}-_{t}g^{t}),\]

where \(g^{t}\) is a gradient estimator for \( l(w^{t})\), and the proximal operator is defined as

\[_{ h}(w)\ :=\ *{argmin}_{v}h(v)+\|w-v\|_{2}^{2}.\]

**Theorem 7**.: _Let \(l\) be a \(\)-strongly convex and \(M\)-smooth function, and let \(=*{argmin}(l)\). Let \(h\) be a proper closed convex function, and let \(w^{*}=*{argmin}(l+h)\). Let \((w^{t})_{t}\) be generated by the Prox-SGD algorithm, with a constant stepsize \((0,\{,\}]\). Suppose that \(g^{t}\) is a quadratically bounded estimator (Def. 5) for \( l\) with parameters \((a,b,w^{*})\). Then,_

\[\|w^{T+1}-w^{*}\|_{2}^{2}(1-)^{T}\|w^{0 }-w^{*}\|_{2}^{2}+(b+M^{2}\|w^{*}- \|_{2}^{2}). \]

_Alternatively, if we use the decaying stepsize \(_{t}=\{,}\}\), then_

\[\|w^{T}-w^{*}\|^{2}  ^{2}}{T^{2}}\|w^{0}-w^{*}\|^{2}+ T}(b+M^{2}\|w^{*}-\|_{2}^{2}). \]

_In both cases, \(T=(^{-1})\) iterations are sufficient to guarantee that \(\|w^{T}-w^{*}\|^{2}\)._

The above theorem gives an anytime \(1/T\) rate of convergence when the stepsizes are chosen based on the strong convexity and gradient noise constants \(\) and \(a\). Note that we do not need to know precisely those constants: We show in Appendix 9.3 that using any constant step size proportional to \(T/ T\) leads to a \( T/T\) rate.

**Theorem 8**.: _Let \(l\) be a proper convex and \(M\)-smooth function. Let \(h\) be a proper closed convex function, and let \(w^{*}*{argmin}(l+h)\). Let \((w^{t})_{t}\) be generated by the Prox-SGD algorithm, with a constant stepsize \((0,]\). Suppose that \(g^{t}\) is a quadratically bounded estimator (Def. 5) for \( l\) with parameters \((a,b,w^{*})\). Then,_

\[[f(^{T})- f]\ \ (a-w^{*}\|^{2}}{(1-^{T})}+b),\]

_where \(}{{=}}}\) and \(^{T}}{{=}}^{T}^{t+ 1}w^{t}}{_{t=1}^{T}^{t+1}}\). In particular, if \(=}\), then_

\[[f(^{T})]- f\ \ }(2a\|w^{0 }-w^{*}\|^{2}+b) T\{}{a},2\}.\]

_Thus, \(T=(^{-2})\) iterations are sufficient to guarantee that \([f(^{T})- f]\)._

### Stochastic Projected Gradient Descent

Here we want to minimize a function \(f\) over a set of constraints \(\), where \(\) is a nonempty closed convex set and \(f\) is a proper closed convex function which is differentiable on \(\). To solve this problem, we will consider the stochastic projected gradient algorithm.

**Definition 9** (Proj-SGD).: Let \(w^{0}\) be some fixed initial parameter, and let \(_{1},_{2},\) be a sequence of step sizes. The projected stochastic gradient (Proj-SGD) method is given by

\[w^{t+1}=_{}(w^{t}-_{t}g^{t}),\]

where \(g^{t}\) is a gradient estimator for \( f(w^{t})\), and the projection operator is defined as

\[_{}(w)=*{argmin}_{v} \|v-w\|_{2}^{2}.\]

Note that we do not require \(f\) to be _smooth_, meaning that this setting is not a particular case of the one considered in Section 3.1. In fact, the arguments we use in the proofs for Proj-SGD are more closely related to stochastic _subgradient methods_ than stochastic gradient methods.

**Theorem 10**.: _Let \(\) be a nonempty closed convex set. Let \(f\) be a \(\)-strongly convex function, differentiable on \(\). Let \(w^{*}=*{argmin}_{}(f)\). Let \((w^{t})_{t}\) be generated by the Proj-SGD algorithm, with a constant stepsize \((0,\{,\}]\). Suppose that \(g^{t}\) is a quadratically bounded estimator (Def. 5) for \( f\) with parameters \((a,b,w^{*})\). Then,_

\[\|w^{T}-w^{*}\|^{2}(1- )^{T}\|w^{0}-w^{*}\|^{2}+. \]

_Alternatively, if we use the decaying stepsize \(_{t}=\{,}\}\), then_

\[[\|w^{T}-w^{*}\|^{2}]  T^{2}}\|w^{0}-w^{*}\|^{2}+T}. \]

_In both cases, \(T=(^{-1})\) iterations are sufficient to guarantee that \(\|w^{T}-w^{*}\|^{2}\)._

Note that Eq. 13 is a sum of two terms: one that decays exponentially in \(T\) and one that decreases only when the stepsize \(\) is sufficiently small. This has an important consequence: if one uses the gradient estimator \(*{g_{STL}}\) and the target distribution is exactly a Gaussian, then \(b=0\) (Appendix 8.2), meaning that the algorithm will converge at an _exponential_ rate. This is similar to many results in the stochastic optimization literature showing faster rates hold when interpolation holds .

**Theorem 11**.: _Let \(\) be a nonempty closed convex set. Let \(f\) be a convex function, differentiable on \(\). Let \(w^{*}*{argmin}_{}(f)\). Let \((w^{t})_{t}\) be generated by the Proj-SGD algorithm, with a constant stepsize \((0,+)\). Suppose that \(g^{t}\) is a quadratically bounded estimator (Def. 5) for \( f\) at \(w^{t}\) with constant parameters \((a,b,w^{*})\). Then,_

\[[f(^{T})-_{}f]\;\;(a-w^{*}\|^{2}}{1-^{T}}+b).\]

_where \(}}{{=}}}\) and \(^{T}}}{{=}}^{T-1} ^{t+1}w^{t}}{_{t=0}^{T-1}^{t+1}}\). Finally if \(=}{}\) and \(T 2\) then_

\[[f(^{T})]- f\;\;}{ }\|w^{0}-w^{*}\|^{2}+}. \]

_Thus, \(T=(^{-2})\) iterations are sufficient to guarantee that \([f(^{T})- f]\)._

## 4 Solving VI with provable guarantees

We now specialize the optimization results of the previous section to our VI setting. We aim to minimize the negative ELBO, \(f:^{d} V\{+\}\), which decomposes as \(f=l+h\) where

\[l(w)=-*{}_{ q_{w}} p(,x)  h(w)=*{}_{ q_{w}} q_{w}( )C 0,+, \]

and where \(V\) is the vector space of matrices in which the covariance factors \(C\) belong. Under the assumption that the log-target \( p(,x)\) is \(M\)-smooth and concave, we propose two strategies to minimize this objective. All the proofs for this section can be found in Appendix 10.

1. Apply the Prox-SGD algorithm to the sum \(l+h\), using a proximal step with respect to \(h\), and a stochastic gradient step with respect to \(l\). The gradient of \(l\) will be estimated with \(_{}\) (5), which is globally quadratically bounded. The prox of \(h\) admits a closed form formula if we choose that \(V\) is the space of lower triangular matrices.
2. Apply the Proj-SGD algorithm to minimize \(f\) over \(_{M}\). The gradient of \(f\) will be estimated using either \(_{}\) (7) or \(_{}\) (9), both of which are quadratically bounded on \(_{M}\). The projection onto \(_{M}\) admits a closed form formula if choose that \(V\) is the space of symmetric matrices.

**Corollary 12** (Prox-Sgd for Vi).: _Consider the VI problem where \(q_{w}\) is a multivariate Gaussian distribution (Eq. 2) with parameters \(w=(m,C)^{d}^{d}\), and assume that this problem admits a solution \(w^{*}\). Suppose that \( p(,x)\) is \(M\)-smooth and concave (resp. \(\)-strongly concave). Generate a sequence \(w^{t}\) by using the Prox-SGD algorithm (Def. 6) applied to \(l\) and \(h\) (Eq. 16), using \(_{}\) (5) as an estimator of \( l\). Let the stepsizes \(_{t}\) be constant and equal to \(1/(}T})\) (resp. be decaying as in Theorem 7 with \(a_{}=2(d+3)M^{2}\)). Then, for a certain average \(^{T}\) of the iterates, we have for \(T 2\) that_

**Corollary 13** (Proj-Sgd for Vi).: _Consider the VI problem where \(q_{w}\) is a multivariate Gaussian distribution (Eq. 2) with parameters \(w=(m,C)^{d}^{d}\), and assume that this problem admits a solution \(w^{*}\). Suppose that \( p(,x)\) is \(M\)-smooth and concave (resp. \(\)-strongly concave). Generate a sequence \(w^{t}\) by using the Proj-SGD algorithm (Def. 9) applied to the function \(f=l+h\) (Eq. 16) and the constraint \(_{M}\) (Eq. 3), using \(_{}\) (7) or \(_{}\) (9) as an estimator of \( f\). Let the stepsizes \(_{t}\) be constant and equal to \(\) (resp. be decaying as in Theorem 10) with \(a=a_{}=4(d+3)M^{2}\) or \(a=a_{}=24(d+3)M^{2}\). Then, for a certain average \(^{T}\) of the iterates, we have for \(T 2\) that_

\[[f(^{T})- f]\ =\ (1/)\ [\|w^{T}-w^{*}\|_{2}^{2}]\ =\ (1/T)\]

**Corollary 14** (Proj-Sgd for Vi - Gaussian target).: _Consider the setting of Corollary 13, in the scenario that \( p(,x)\) is \(\)-strongly concave, that we use the \(_{}\) estimator, and that we take a constant stepsize \(_{t}(0,\{}},\}]\). Assume further that \(p(|x)\) is Gaussian. Then,_

\[[\|w^{T}-w^{*}\|_{2}^{2}]\ \ (1-)^{T}\|w^{0}-w^{*}\|_{2}^{2}.\]

Let us now discuss the practical implementation of these two algorithms (more details and an explicit implementation of the methods are given in Appendix 10.2). These require computing either the proximal operator of the neg-entropy \(h\), or the projection onto the set of non-degenerate covariance factors \(_{M}\). These can be computed  for every \(w=(m,C)\) as:

* \(_{ h}(w)=(m,C+ C)\), where \( C\) is diagonal with \( C_{ii}=(^{2}+4}-C_{ii})\),
* \(_{}(w)=(m,UU^{})\), where \(C\) has the SVD \(C=UDU^{}\) and \(_{ii}=\{D_{ii},1/\}\).

Our theory for the Prox-SGD algorithm formally assumes that the covariance factor \(C\) lives in the vector space of lower-triangular matrices. This can be implemented by letting \(C^{d d}\) and "clamping" the upper-triangular entries to zero throughout computation. Then the gradient \(_{}\) (5) can be computed using automatic differentiation, and the proximal operator can be computed as in the above equation. Or, one may choose \(w=(m,c)\) where \(c^{d(d+1)/2}\) is the lower-triangular entries of the matrix so \(C=(c)\). Gradients with respect to \(w=(m,c)\) can again be estimated using automatic differentiation (including the \(\) operator), and the proximal operator can be computed by forming \(C\), using the above formula, and then extracting the lower-triangular entries.

Similarly, our theory for the Proj-SGD algorithm formally assumes the covariance factor \(C\) lives in the vector space of symmetric matrices. This can be implemented by letting \(C^{d d}\), computing the gradient estimator \(_{}\) (7) or \(_{}\) (9) over the reals using automatic differentiation, and "symmetrizing" \(C\) throughout computation. That is, set \(CC+C^{}\) after applying each gradient update, right before projection. Or, one may choose \(w=(m,c)\) where \(c^{d(d+1)/2}\) is the lower-triangular entries of the matrix, so \(C=(c)\), where \(\) is a function similar to \(\) except creating symmetric matrices. The gradient estimator \(_{}\) (7) or \(_{}\) (9) can be computedusing automatic differentiation (including the \(\) operator). In this case, to project one would first form \(C\), then use the above formula, and then extract the lower-triangular matrices.

In terms of cost, computing \( q_{w}(z)\) or \(T_{w}(u)\) needs \((d^{2})\) operations. Computing the prox of \(h\) requires \(d\) operations, but in contrast projecting onto \(_{M}\) requires diagonalizing a symmetric matrix, which takes \((d^{3})\) operations. We note that this is not necessarily an issue, because 1) eigenvalue decomposition has excellent computational constants in moderate dimensions; 2) computing the target \( p(z,x)\) may itself require \((d^{3})\) or more operations; 3) it is common to average a "minibatch" of gradient estimates, meaning each eigenvalue decomposition computation is amortized over many ELBO evaluations. Still, in high dimensions, when \( p(z,x)\) is inexpensive, and small minibatches are used, computing such decomposition in each iteration could become a computational bottleneck. In this scenario, the Proj-SGD algorithm is clearly cheaper, with its \((d)\) cost for the proximal step.

## 5 Discussion

While this paper has focused on convergence with dense Gaussian variational distributions, the results in Sec. 3 apply more generally. It is conceivable that the necessary smoothness, convexity, and noise bounds could be established for other variational families, in which case these optimization results would provide "plug in" guarantees. We suspect this might be fairly easy to do with, e.g., elliptical distributions or location-scale families, using similar techniques as done with Gaussians. But it may be difficult to do so for broader variational families.

If \(- p\) is strongly convex and smooth, another proof strategy is possible: Smoothness guarantees that \(w^{*}_{M}\) and strong convexity guarantees that \(w^{*}\{w:\|w-\|_{2}^{2} d/\}\). So one could do projected SGD, projecting onto the intersection of these two sets. The negative ELBO would be strongly convex and smooth over that set, and since \(\|w-\|_{2}\) is bounded, gradient noise is upper-bounded by a constant, meaning classical projected SGD convergence rates that assume smoothness, strong convexity, and uniform gradient noise apply. However, projecting onto the intersection of those sets poses a difficulty and this uniform noise bound may be loose in practice.

Several variants of the gradient estimators we consider have been proposed to reduce noise, often based on control variates or sampling from a different base distribution . It would be interesting to establish gradient noise bounds for these estimators.

Various VI variants have been proposed that use _natural_ gradient descent. It would also be interesting to seek convergence guarantees for these algorithms.

VI can be done with "score" or "reinforce" gradient estimators , which use the identity

\[_{w}*{}_{ q_{w}}()= *{}_{ q_{w}}()_{w}  q_{w}()\]

in place of how we used Eq. 4 for the "path" estimators we considered. While path estimators often seem to have lower variance, this is not always true: Take an unnormalized log-posterior \((z)\) where \(z\) is scalar. Now, define \(^{}(z)=(z)+(z/)\), where \(\) is very small. Then, \(\) and \(^{}\) represent almost the same posterior, and score estimators for them will have almost the same variance. Yet, the derivative of the added term is \((z/)/\), so a path estimator for \(^{}\) will have much higher variance than one for \(\). This underlines why smoothness is essential for our guarantees--it excludes posteriors like \(^{}\). Future work further unravel when score estimators perform better than path estimators.

One attractive feature of VI is that data subsampling can often be used when computing gradients, decreasing the cost of each iteration. While we have not explicitly addressed this, our proofs can be easily generalized, at least for target distributions of the form \(p(z,x)=p(z)_{n=1}p(x_{n}|z)\). The only issue is to bound the noise of the subsampled estimator. While our gradient variance guarantees use Theorem 1 from , a more general result [20, Theorem 6] considers data subsampling. Very roughly speaking, with uniform data subsampling of 1 datum of a time, the gradient noise bounds in Thms. 2 and 3 would increase by a factor between 1 (no increase) and the number of data, depending on how correlated the data are--less correlation leads to a larger increase. (More precisely, the second term in Thm. 3 would not increase.) These increases would manifest as larger constants \(a\) and \(b\) in the quadratic bounds, after which exactly the same results hold. However, it is not obvious if the bound for \(}_{}\) in Thm. 4 can be generalized in this way.