# Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation

Myong Chol Jung

Monash University

david.jung@monash.edu

&He Zhao

CSIRO's Data61

he.zhao@ieee.org

&Joanna Dipnall

Monash University

jo.dipnall@monash.edu

&Lan Du

Monash University

lan.du@monash.edu

Corresponding author

###### Abstract

Uncertainty estimation is an important research area to make deep neural networks (DNNs) more trustworthy. While extensive research on uncertainty estimation has been conducted with unimodal data, uncertainty estimation for multimodal data remains a challenge. Neural processes (NPs) have been demonstrated to be an effective uncertainty estimation method for unimodal data by providing the reliability of Gaussian processes with efficient and powerful DNNs. While NPs hold significant potential for multimodal uncertainty estimation, the adaptation of NPs for multimodal data has not been carefully studied. To bridge this gap, we propose Multimodal Neural Processes (MNPs) by generalising NPs for multimodal uncertainty estimation. Based on the framework of NPs, MNPs consist of several novel and principled mechanisms tailored to the characteristics of multimodal data. In extensive empirical evaluation, our method achieves state-of-the-art multimodal uncertainty estimation performance, showing its appealing robustness against noisy samples and reliability in out-of-distribution detection with faster computation time compared to the current state-of-the-art multimodal uncertainty estimation method.

## 1 Introduction

Uncertainty estimation of deep neural networks (DNNs) is an essential research area in the development of reliable and well-calibrated models for safety-critical domains [49; 45]. Despite the remarkable success achieved by DNNs, a common issue with these model is their tendency to make overconfident predictions for both in-distribution (ID) and out-of-distribution (OOD) samples [16; 43; 35]. Extensive research has been conducted to mitigate this problem, but most of these efforts are limited to unimodal data, neglecting the consideration of multimodal data [57; 11; 50; 63; 38; 34].

In many practical scenarios, safety-critical domains typically involve the processing of multimodal data. For example, medical diagnosis classification that uses X-ray, radiology text reports and the patient's medical record history data can be considered as a form of multimodal learning that requires trustworthy predictions . Despite the prevalence of such multimodal data, the problem of uncertainty estimation for multimodal data has not been comprehensively studied. Furthermore, it has been demonstrated that using existing unimodal uncertainty estimation techniques directly for multimodal data is ineffective, emphasising the need for a meticulous investigation of multimodal uncertainty estimation techniques [24; 17].

Several studies investigated the problem of multimodal uncertainty estimation with common tasks including 1) assessing calibration performance, 2) evaluating robustness to noisy inputs, and 3) detecting OOD samples. Trusted Multi-view Classification (TMC)  is a single-forward deterministic classifier that uses the Dempster's combination rule  to combine predictions obtained from different modalities. The current state-of-the-art (SOTA) in this field is Multi-view Gaussian Process (MGP) , which is a non-parametric Gaussian process (GP) classifier that utilises the product-of-experts to combine predictive distributions derived from multiple modalities. Although MGP has shown reliability of GPs in this context, it should be noted that the computational cost of a GP increases cubically with the number of samples .

Neural processes (NPs) offer an alternative approach that utilises the representation power of DNNs to imitate the non-parametric behaviour of GPs while maintaining a lower computational cost . It has been shown that NPs can provide promising uncertainty estimation for both regression  and classification tasks  involving unimodal data. Despite the promising potential of NPs for multimodal data, to the best of our knowledge, no research has yet investigated the feasibility of using NPs for multimodal uncertainty estimation.

In this work, we propose a new multimodal uncertainty estimation framework called Multimodal Neural Processes (MNPs) by generalising NPs for multimodal uncertainty estimation. MNPs have three key components: the dynamic context memory (DCM) that efficiently stores and updates informative training samples, the multimodal Bayesian aggregation (MBA) method which enables a principled combination of multimodal latent representations, and the adaptive radial basis function (RBF) attention mechanism that facilitates well-calibrated predictions. Our contributions are:

1. We introduce a novel multimodal uncertainty estimation method by generalising NPs that comprise the DCM, the MBA, and the adaptive RBF attention.
2. We conduct rigorous experiments on seven real-world datasets and achieve the new SOTA performance in classification accuracy, calibration, robustness to noise, and OOD detection.
3. We show that MNPs achieve faster computation time (up to 5 folds) compared to the current SOTA multimodal uncertainty estimation method.

## 2 Background

Multimodal ClassificationThe aim of this research is to investigate uncertainty estimation for multimodal classification. Specifically, we consider a multimodal training dataset \(D_{train}=\{\{x_{i}^{m}\}_{m=1}^{M},y_{i}\}_{i=1}^{N_{train}}\), where \(M\) is the number of input modalities and \(N_{train}\) is the number of training samples. We assume that each \(i^{th}\) sample has \(M\) modalities of input \(x_{i}^{m}^{d^{m}}\) with the input dimension of \(d^{m}\) and a single one-hot encoded label \(y_{i}\) with the number of classes \(K\). In this study, we consider the input space to be a feature space. The objective is to estimate the labels of test samples \(\{y_{i}\}_{i=1}^{N_{test}}\) given \(D_{test}=\{\{x_{i}^{m}\}_{m=1}^{M}\}_{i=1}^{N_{test}}\), where \(N_{test}\) is the number of test samples.

Neural ProcessesNPs are stochastic processes using DNNs to capture the ground truth stochastic processes that generate the given data . NPs learn the distribution of functions and provide uncertainty of target samples, preserving the property of GPs. At the same time, NPs exploit function approximation of DNNs in a more efficient manner than GPs. For NPs, both training and test datasets have a context set \(C=\{C_{X},C_{Y}\}=\{x_{i}^{C},y_{i}^{C}\}_{i=1}^{N_{C}}\) and a target set \(T=(T_{X},T_{Y})=\{x_{i}^{T},y_{i}^{T}\}_{i=1}^{N_{T}}\) with \(N_{C}\) being the number of context samples and \(N_{T}\) the number of target samples2, and the learning objective of NPs is to maximise the likelihood of \(p(T_{Y}|C,T_{X})\).

Conditional Neural Processes (CNPs), the original work of NPs , maximise \(p(T_{Y}|r(C),T_{X})\) with \(r(C)=}_{i=1}^{N_{C}}_{}(cat[x_{i}^{C};y_{ i}^{C}])^{d_{c}}\) where \(_{}\) is an encoder parameterised by \(\), \(cat[;]\) is the concatenation of two vectors along the feature dimension, and \(d_{e}\) is the feature dimension. The mean vector \(r(C)\) is the permutation invariant representation that summarises the context set which is passed to a decoder with \(T_{X}\) to estimate \(T_{Y}\).

The original CNPs use the unweighted mean operation to obtain \(r(C)\) by treating all the context points equally, which has been shown to be underfitting . To improve over this, Attentive NeuralProcesses (ANPs)  leveraged the scaled dot-product cross-attention  to create target-specific context representations that allocate higher weight on the closer context points:

\[r_{*}(C,T_{X})=(T_{X}C_{X}^{T}/)}_{A(T_{X},C_{ X})^{N_{T} N_{C}}}_{}(cat[C_{X};C_{Y}])}_{ ^{N_{C} d_{e}}}\] (1)

where \(A(T_{X},C_{X})\) is the attention weight with \(T_{X}\) as the query, \(C_{X}\) is the key, the encoded context set is the value, and \(d\) is the input dimension of \(T_{X}\) and \(C_{X}\). The resulted target-specific context representation has been shown to enhance the expressiveness of the task representation .

The training procedure of NPs often randomly splits a training dataset to the context and the target sets (i.e., \(N_{C}+N_{T}=N_{train}\)). In the inference stage, a context set is provided for the test/target samples (e.g., a support set in few-shot learning or unmasked patches in image completion). While the context set for the test dataset is given for these tasks, NPs also have been applied to other tasks where the context set is not available (e.g., semi-supervised learning  and uncertainty estimation for image classification ). In those tasks, existing studies have used a _context memory_ during inference, which is composed of training samples that are updated while training . It is assumed that the context memory effectively represents the training dataset with a smaller number of samples. We build upon these previous studies by leveraging the concept of context memory in a more effective manner, which is suitable for our task.

Note that the existing NPs such as ETP  can be applied to multimodal classification by concatenating multimodal features into a single unimodal feature. However, in Section 5.1, we show that this approach results in limited robustness to noisy samples.

## 3 Multimodal Neural Processes

The aim of this study is to generalise NPs to enable multimodal uncertainty estimation. In order to achieve this goal, there are significant challenges that need to be addressed first. Firstly, it is essential to have an efficient and effective context memory for a classification task as described in Section 2. Secondly, a systematic approach of aggregating multimodal information is required to provide unified predictions. Finally, the model should provide well-calibrated predictions without producing overconfident estimates as described in Section 1.

Our MNPs, shown in Figure 1, address these challenges with three key components respectively: 1) the dynamic context memory (DCM) (Section 3.1), 2) the multimodal Bayesian aggregation (MBA) (Section 3.2), and 3) the adaptive radial basis function (RBF) attention (Section 3.3). From Section 3.1 to 3.3, we elaborate detailed motivations and proposed solutions of each challenge, and

Figure 1: Model diagram of MNPs: DCM refers to Dynamic Context Memory, MBA refers to Multimodal Bayesian Aggregation, and RBF refers to radial basis function.

Section 3.4 outlines the procedures for making predictions. Throughout this section, we refer the multimodal target set \(T^{M}=(\{T^{n}_{X}\}_{m=1}^{M},T_{Y})=\{\{x^{m}_{i}\}_{m=1}^{M},y_{i}\}_{i=1}^{N_ {T}}\) to the samples from training and test datasets and the multimodal context memory \(C^{M}=\{C^{m}_{X},C^{m}_{Y}\}_{m=1}^{M}\) to the context set.

### Dynamic Context Memory

MotivationIn NPs, a context set must be provided for a target set, which however is not always possible for non-meta-learning tasks during inference. One simple approach to adapt NPs to these tasks is to randomly select context points from the training dataset, but its performance is suboptimal as randomly sampling a few samples may not adequately represent the entire training distribution. Wang et al.  proposed an alternative solution by introducing a first-in-first-out (FIFO) memory that stores the context points with the predefined memory size. Although the FIFO performs slightly better than the random sampling in practice, the performance is still limited because updating the context memory is independent of the model's predictions. Refer to Appendix C.1 for comparisons.

Proposed SolutionTo overcome this limitation of the existing context memory, we propose a simple and effective updating mechanism for the context memory in the training process, which we call Dynamic Context Memory (DCM). We partition context memory \(\{C^{m}_{X},C^{m}_{Y}\}\) of \(m^{th}\) modality into \(K\) subsets (\(K\) as the number of classes) as \(\{C^{m}_{X},C^{m}_{Y}\}=\{C^{m}_{X,k},C^{m}_{Y,k}\}_{k=1}^{K}\) to introduce a class-balance context memory, with each subset devoted to one class. Accordingly, \(N^{m}=N^{m}_{K} K\) where \(N^{m}\) is the number of context elements per modality (i.e., the size of \(\{C^{m}_{X},C^{m}_{Y}\}\)) and \(N^{m}_{K}\) is the number of class-specific context samples per modality. This setting resembles the classification setting in , where class-specific context representations are obtained for each class.

DCM is initialised by taking \(N^{m}_{K}\) random training samples from each class of each modality and is updated every mini-batch during training by replacing the least "informative" element in the memory with a possibly more "informative" sample. We regard the element in DCM that receives the smallest attention weight (i.e., \(A(T_{X},C_{X})\) in Equation (1)) during training as the least informative one and the target point that is most difficult to classify (i.e., high classification error) as a more informative sample that should be added to DCM to help the model learn the decision boundary. Formally, this can be written as:

\[C^{m}_{X,k}[i^{*},:]=T^{m}_{X}[j^{*},:], k\{1, K\}, m\{1,  M\}\] (2)

\[i^{*}=_{K}\}}{}}_ {t=1}^{N_{T}}_{X},C^{m}_{X,k})}_{^{N_{T} N^ {m}_{K}}}[t,i]\,,\,j^{*}=\}}{} _{k=1}^{K}(T_{Y}[j,k]-^{m}_{Y}[j,k])^{2}\] (3)

where \([i,:]\) indicates the \(i^{th}\) row vector of a matrix, \([i,j]\) indicates the \(i^{th}\) row and the \(j^{th}\) column element of a matrix, and \(^{m}_{Y}\) is the predicted probability of the \(m^{th}\) modality input. \(i^{*}\) selects the context memory element that receives the least average attention weight in a mini-batch during training, and \(j^{*}\) selects the target sample with the highest classification error. To measure the error between the predictive probability and the ground truth of a target sample, one can use mean squared error (MSE) (Equation (3)) or cross-entropy loss (CE). We empirically found that the former gives better performance in our experiments. Refer to Appendix C.1 for ablation studies comparing the two and the impact of \(N^{m}\) on its performance.

The proposed updating mechanism is very efficient as the predictive probability and the attention weights in Equation (3) are available with no additional computational cost. Also, in comparison to random sampling which requires iterative sampling during inference, the proposed approach is faster in terms of inference wall-clock time since the updated DCM can be used without any additional computation (refer to Appendix C.1 for the comparison).

### Multimodal Bayesian Aggregation

MotivationWith DCM, we obtain the encoded context representations for the \(m^{th}\) modality input as follows:

\[r^{m}=^{m}_{}(cat[C^{m}_{X};C^{m}_{Y}])^{N^{m}  d_{e}}, s^{m}=^{m}_{}(cat[C^{m}_{X};C^{m}_{Y}]) ^{N^{m} d_{e}}\] (4)where \(\) and \(\) are the encoders' parameters for the \(m^{th}\) modality. Next, given the target set \(T_{X}^{m}\), we compute the target-specific context representations with the attention mechanism:

\[r_{*}^{m}=A(T_{X}^{m},C_{X}^{m})r^{m}^{N_{T} d_{e}}, s_ {*}^{m}=A(T_{X}^{m},C_{X}^{m})s^{m}^{N_{T} d_{e}}\] (5)

where the attention weight \(A(T_{X}^{m},C_{X}^{m})^{N_{T} N^{m}}\) can be computed by any scoring function without loss of generality. Note that a single \(i^{th}\) target sample consists of multiple representations from \(M\) modalities \(\{r_{*,i}^{m}=r_{*}^{m}[i,:]^{d_{e}}\}_{m=1}^{M}\). It is important to aggregate these multiple modalities into one latent variable/representation \(z_{i}\) for making a unified prediction of the label.

Proposed SolutionInstead of using a deterministic aggregation scheme, we propose Multimodal Bayesian Aggregation (MBA), inspired by . Specifically, we view \(r_{*,i}^{m}\) as a sample from a Gaussian distribution with mean \(z_{i}\): \(p(r_{*,i}^{m}|z_{i})=(r_{*,i}^{m}|z_{i},(s_{*,i}^{m }))\) where \(s_{*,i}^{m}=s_{*}^{m}[i,:]^{d_{e}}\) is the \(i^{th}\) sample in \(s_{*}^{m}\). Additionally, we impose an informative prior on \(z_{i}\): \(p(z_{i})=_{m=1}^{M}(u^{m},(q^{m}))\) with the mean context representations of \(u^{m}^{d_{e}}\) and \(q^{m}^{d_{e}}\), which assign uniform weight across the context set as follows:

\[u^{m}=}_{i=1}^{N^{m}}_{}^{m}(cat[C_{X} ^{m}[i,:];C_{Y}^{m}[i,:]]),\,q^{m}=}_{i=1}^{N^{m}} _{}^{m}(cat[C_{X}^{m}[i,:];C_{Y}^{m}[i,:]])\] (6)

where \(\) and \(\) are the encoders' parameters for the \(m^{th}\) modality. Note that the encoders in Equation (4) and (6) are different because they approximate different distribution parameters.

**Lemma 3.1** (Gaussian posterior distribution with factorised prior distribution).: _If we have \(p(x_{i}|)=(x_{i}|,_{i})\) and \(p()=_{i=1}^{n}(_{0,i},_{0,i})\) for \(n\) i.i.d. observations of \(D\) dimensional vectors, then the mean and covariance of posterior distribution \(p(|x)=(|_{n},_{n})\) are:_

\[_{n}=[_{i=1}^{n}(_{i}^{-1}+_{0,i}^{-1}) ]^{-1},_{n}=_{n}[_{i=1}^{n}(_{i}^{-1} x_{i}+_{0,i}^{-1}_{0,i})]\] (7)

As both \(p(z_{i})\) and \(p(r_{*,i}^{m}|z_{i})\) are Gaussian distributions, the posterior of \(z_{i}\) is also a Gaussian distribution: \(p(z_{i}|r_{*,i})=(z_{i}|_{z_{i}},(_{z_{i}} ^{2}))\), whose mean and variance are obtained by using Lemma 3.1 as follows:

\[_{z_{i}}^{2}=[_{m=1}^{M}((s_{*,i}^{m})^{}+(q^{m})^{ })]^{},_{z_{i}}=_{z_{i}}^{2}[_ {m=1}^{M}(r_{*,i}^{m}(s_{*,i}^{m})^{}+u^{m}(q^{m})^{ })]\] (8)

where \(\) and \(\) are element-wise inverse and element-wise product respectively (see Appendix A for proof).

We highlight that if the variance \(s_{*,i}^{m}\) of a modality formed by the target-specific context representation is high in \(_{z_{i}}^{2}\), \(q^{m}\) formed by the mean context representations dominates the summation of the two terms. Also, if both \(s_{*,i}^{m}\) and \(q^{m}\) are high for a modality, the modality's contribution to the summation over modalities is low. By doing so, we minimise performance degradation caused by uncertain modalities (see Section 5.1 for its robustness to noisy samples). Refer to Appendix C.2 for ablation studies on different multimodal aggregation methods.

### Adaptive RBF Attention

MotivationThe attention weight \(A(T_{X}^{m},C_{X}^{m})\) in Equation (3) and (5) can be obtained by any attention mechanism. The dot-product attention is one of the simple and efficient attention mechanisms, which has been used in various NPs such as [28; 53; 9; 47]. However, we have observed that it is not suitable for our multimodal uncertainty estimation problem, as the dot-product attention assigns excessive attention to context points even when the target distribution is far from the context distribution. The right figure in Figure 1(a) shows the attention weight in Equation (1) with \(A(x_{OOD}^{m},C_{X}^{m})\) where \(x_{OOD}^{m}\) is a single OOD target sample (grey sample) far from the context set (red and blue samples). This excessive attention weight results in overconfident predictive probability for OOD samples (see the left figure of Figure 1(a)), which makes the predictive uncertainty of a classifier hard to distinguish the ID and OOD samples.

Proposed SolutionTo address the overconfident issue of the dot-product attention, we propose an attention mechanism based on RBF. RBF is a stationary kernel function that depends on the relative distance between two points (i.e., \(x-x^{}\)) rather than the absolute locations , which we define as \(^{m}(x,x^{})=(-||} {(l^{m})^{2}}||^{2})\) where \(||||^{2}\) is the squared Euclidean norm, and \(l^{m}\) is the lengthscale parameter that controls the smoothness of the distance in the \(m^{th}\) modality input space. The RBF kernel is one of the widely used kernels in GPs , and its adaptation with DNNs has shown well-calibrated and promising OOD detection . Formally, we define the attention weight using the RBF kernel as:

\[A(T_{X}^{m},C_{X}^{m})=(G(T_{X}^{m},C_{X}^{m}))\] (9)

where the elements of \(G(T_{X}^{m},C_{X}^{m})^{N_{T} N^{m}}\) are \([G]_{ij}=^{m}(T_{X}^{m}[i,:],C_{X}^{m}[j,:])\), and Sparsemax  is an alternative activation function to Softmax. It is defined as \((h):=}{}||p-h||^{2}\) where \(^{K-1}\) is the \(K-1\) dimensional simplex \(^{K-1}:=\{p^{K}|^{}p=1,p\}\). Here we use Sparsemax instead of the standard Softmax because Sparsemax allows zero-probability outputs. This property is desirable because the Softmax's output is always positive even when \(^{m}(x,x^{})=0\) (i.e., \(||x-x^{}||^{2}\)) leading to higher classification and calibration errors (see Appendix C.3 for ablation studies).

The lengthscale \(l^{m}\) is an important parameter that determines whether two points are far away from each other (i.e., \(^{m}(x,x^{}) 0\)) or close to each other (i.e., \(0<^{m}(x,x^{}) 1\)). However, in practice, \(l^{m}\) has been either considered as a non-optimisable hyperparameter or an optimisable parameter that requires a complex initialisation . To address this issue, we propose an adaptive learning approach of \(l^{m}\) to form a tight bound to the context distribution by leveraging the supervised contrastive learning . Specifically, we let anchor index \(i T_{ind}\{1,,N_{T}\}\) with

Figure 2: Predictive probability of upper circle samples (blue) is shown in the left column with (a) the dot-product attention, (b) the RBF attention without \(_{RBF}\), (c) and the RBF attention with \(_{RBF}\). Upper circle samples (blue) are class 1, lower circle samples (red) are class 2, and grey samples are OOD samples. The attention weight \(A(x_{OOD}^{m},C_{X}^{m})\) of an OOD sample across 100 context points is shown in the right column. The summarised steps of training MNPs are shown in (d). Refer to Appendix B for the experimental settings.

negative indices \(N(i)=T_{ind}\{i\}\) and positive indices \(P(i)=\{p N(i):T_{Y}[p,:]=T_{Y}[i,:]\}\). Given an anchor sample of the target set, the negative samples refer to all samples except for the anchor sample, while the positive samples refer to other samples that share the same label as the anchor sample. We define the multimodal supervised contrastive loss as:

\[^{M}_{CL}=_{m=1}^{M}^{m}_{CL}=_{m=1}^{M}_{i=1}^{N_{T}}-_{p P(i)} (T^{m}_{X}[i,:],T^{m}_{X}[p,:])/)}{_{n  N(i)}(^{m}(T^{m}_{X}[i,:],T^{m}_{X}[n,:])/)}\] (10)

where \(|P(i)|\) is the cardinality of \(P(i)\), and \(\) is the temperature scale. This loss encourages higher RBF output of two target samples from the same class and lower RBF output of two target samples from different classes by adjusting the lengthscale. In addition to \(^{M}_{CL}\), a \(l_{2}\)-loss is added to form the tighter bound by penalising large lengthscale. Overall, the loss term for our adaptive RBF attention is \(_{RBF}=^{M}_{CL}+*_{m=1}^{M}||l^{ m}||\) with the balancing coefficient \(\).

We show the difference between the RBF attention without \(_{RBF}\) (see Figure 2b) and the adaptive RBF attention with \(_{RBF}\) (see Figure 2c). It can be seen that the decision boundary modelled by the predictive probability of the adaptive RBF attention is aligned with the data distribution significantly better than the non-adaptive one. For ablation studies with real-world datasets, see Appendix C.4.

### Conditional Predictions

We follow the standard procedures of Gaussian process classification to obtain predictions [65; 24; 42; 19] where we first compute the predictive latent distribution \(p(f(T^{M}_{X})|C^{M},T^{M}_{X})\) as a Gaussian distribution by marginalising \(Z=\{z_{i}\}_{i=1}^{N_{T}}\):

\[p(f(T^{M}_{X})|C^{M},T^{M}_{X})= p(f(T^{M}_{X})|Z)p(Z|R^{*}(C^{M},T^{M}_{X }))\,dZ\] (11)

where \(T^{M}_{X}=\{T^{m}_{X}\}_{m=1}^{M}\), \(R^{*}(C^{M},T^{M}_{X})=\{r_{*,i}\}_{i=1}^{N_{T}}\), and \(p(f(T^{M}_{X})|Z)\) is parameterised by a decoder. Then, we obtain the predictive probability \(_{Y}\) by:

\[_{Y}=(p(f(T^{M}_{X})))p(f(T^{M}_{X})|C^{M},T^{M} _{X})\,df(T^{M}_{X})\] (12)

Similarly, we can obtain the unimodal predictive latent distribution \(p(f(T^{m}_{X})|\{C^{m}_{X},C^{m}_{Y}\},T^{m}_{X})\) and the unimodal predictive probability \(^{m}_{Y}\) for the \(m^{th}\) modality as:

\[^{m}_{Y}=(p(f(T^{m}_{X})))p(f(T^{m}_{X})|\{C^{m} _{X},C^{m}_{Y}\},T^{m}_{X})\,df(T^{m}_{X})\] (13)

We minimise the negative log likelihood of the aggregated prediction and the unimodal predictions by:

\[_{T_{Y}}=-_{T_{Y}}[_{Y}]- {1}{M}_{m=1}^{M}^{m}_{T_{Y}}=-_{T_{Y}}[ _{Y}]-_{m=1}^{M}_{T_{Y}}[ ^{m}_{Y}]\] (14)

Since Equations (11)-(13) are analytically intractable, we approximate the integrals by the Monte Carlo method . The overall loss for MNPs is \(=_{T_{Y}}+*_{RBF}\) where \(\) is the balancing term. Refer to Figure 2d for the summarised steps of training MNPs.

## 4 Related Work

Neural ProcessesCNP and the latent variant of CNP that incorporates a latent variable capturing global uncertainty were the first NPs introduced in the literature [13; 14]. To address the under-fitting issue caused by the mean context representation in these NPs, Kim et al.  proposed to leverage the attention mechanism for target-specific context representations. This approach has been shown to be effective in subsequent works [53; 29; 9; 47; 67; 22]. Many other variants, such as SNP [51; 67], CNAP , and MPNPs  were proposed for common downstream tasks like 1D regression, 2D image completion [28; 14; 15; 10; 61; 26; 20], and image classification [13; 62; 25]. However, none of these studies have investigated the generalisation of NPs to multimodal data. This study is the first to consider NPs for multimodal classification and its uncertainty estimation.

Multimodal LearningThe history of multimodal learning that aims to leverage multiple sources of input can be traced back to the early work of Canonical Correlation Analysis (CCA) . CCA learns the correlation between two variables which was further improved by using feed-forward networks by Deep CCA (DCCA) . With advances in various architectures of DNNs, many studies on multimodal fusion and alignment were proposed [4; 12; 55; 6]. In particular, transformer-based models for vision-language tasks [36; 52; 1] have obtained great attention. Nonetheless, most of these methods were not originally intended for uncertainty estimation, and it has been demonstrated that many of them exhibit inadequate calibration [16; 43].

Multimodal Uncertainty EstimationMultimodal uncertainty estimation is an emerging research area. Its objective is to design robust and calibrated multimodal models. Ma et al.  proposed the Mixture of Normal-Inverse Gamma (MoNIG) algorithm that quantifies predictive uncertainty for multimodal regression. However, this work is limited to regression, whereas our work applies to multimodal classification. Han et al.  developed TMC based on the Dempster's combination rule to combine multi-view logits. In spite of its simplicity, empirical experiments showed its limited calibration and capability in OOD detection . Jung et al.  proposed MGP that combines predictive posterior distributions of multiple GPs by the product of experts. While MGP achieved the current SOTA performance, its non-parametric framework makes it computationally expensive. Our proposed method overcomes this limitation by generalising efficient NPs to imitate GPs.

## 5 Experiments

Apart from measuring the test accuracy, we assessed our method's performance in uncertainty estimation by evaluating its calibration error, robustness to noisy samples, and capability to detect OOD samples. These are crucial aspects that a classifier not equipped to estimate uncertainty may struggle with. We evaluated MNPs on seven real-world datasets to compare the performance of MNPs against four unimodal baselines and three multimodal baselines.

To compare our method against unimodal baselines, we leveraged the early fusion (EF) method  that concatenates multimodal input features to single one. The unimodal baselines are (1) **MC Dropout (MCD)** with dropout rate of 0.2, (2) **Deep Ensemble (DE)** with five ensemble models, (3) **SNGP**'s GP layer , and (4) **ETP** with memory size of 200 and a linear projector. ETP was chosen as a NP baseline because of its original context memory which requires minimal change of the model to be used for multimodal classification.

The multimodal baselines consist of (1) **Deep Ensemble (DE)** with the late fusion (LF)  where a classifier is trained for each modality input, and the final prediction is obtained by averaging predictions from all the modalities, (2) **TMC**, and (3) **MGP**. For TMC and MGP, we followed the settings proposed by the original authors. In each experiment, the same feature extractors were used for all baselines. We report mean and standard deviation of results from five random seeds. The bold values are the best results for each dataset, and the underlined values are the second-best ones. See Appendix B for more detailed settings.

### Robustness to Noisy Samples

Experimental SettingsIn this experiment, we evaluated the classification performance of MNPs and the robustness to noisy samples with six multimodal datasets [24; 17]. Following  and , we normalised the datasets and used a train-test split of 0.8:0.2. To test the robustness to noise, we added zero-mean Gaussian noise with different magnitudes of standard deviation during inference (10 evenly spaced values on a log-scale from \(10^{-2}\) to \(10^{1}\)) to half of the modalities. For each noise level, all possible combinations of selecting half of the modalities (i.e., \(\)) were evaluated and averaged. We report accuracy and expected calibration error (ECE) for each experiment. Please refer to Appendix B for details of the datasets and metrics.

ResultsWe provide the test results without noisy samples in Table 1, 2, and 5 and the results with noisy samples in Table 3. In terms of accuracy, MNPs outperform all the baselines in 5 out of 6 datasets. At the same time, MNPs provide the most calibrated predictions in 4 out of 6 datasets, preserving the non-parametric GPs' reliability that a parametric model struggles to achieve. This shows that MNPs bring together the best of non-parametric models and parametric models. Also,MNPs provide the most robust predictions to noisy samples in 5 out of 6 datasets achieved by the MBA mechanism. Both unimodal and multimodal baselines except MGP show limited robustness with a large performance degradation.

In addition to its superior performance, MNPs, as shown in Table 5, are also faster than the SOTA multimodal uncertainty estimator MGP in terms of wall-clock time per epoch (up to \( 5\) faster) measured in the identical environment including batch size, GPU, and code libraries (see Appendix B for computational complexity of the two models). This highly efficient framework was made possible

    &  \\  Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\  MCD & 99.25\(\)0.00 & 92.33\(\)1.09 & 91.32\(\)0.62 & 92.95\(\)0.29 & 71.75\(\)0.25 & 71.68\(\)0.36 \\ DE (EF) & 99.20\(\)0.11 & 93.16\(\)0.70 & 91.76\(\)0.33 & 92.99\(\)0.09 & 72.70\(\)0.39 & 71.67\(\)0.23 \\ SNGP & 98.85\(\)0.22 & 89.50\(\)0.75 & 87.06\(\)1.23 & 91.24\(\)0.46 & 64.68\(\)4.03 & 67.65\(\)1.03 \\ ETP & 98.75\(\)0.25 & 92.33\(\)1.99 & 91.76\(\)0.62 & 92.08\(\)0.33 & 72.58\(\)1.35 & 67.43\(\)0.95 \\ DE (LF) & 99.25\(\)0.00 & 92.33\(\)0.70 & 87.21\(\)0.66 & 92.97\(\)0.13 & 67.05\(\)0.38 & 69.98\(\)0.36 \\ TMC & 98.10\(\)0.14 & 91.17\(\)0.46 & 91.18\(\)1.72 & 91.63\(\)0.28 & 67.68\(\)0.27 & 65.17\(\)0.87 \\ MGP & 98.60\(\)0.14 & 92.33\(\)0.70 & 92.06\(\)0.96 & 93.00\(\)0.33 & 70.00\(\)0.53 & **72.30\(\)0.19** \\ MNPs (Ours) & **99.50\(\)0.00** & **93.50\(\)1.71** & **95.00\(\)0.62** & **93.46\(\)0.32** & **77.90\(\)0.71** & 71.97\(\)0.43 \\   

Table 1: Test accuracy (\(\)).

    &  \\  Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\  MCD & 0.009\(\)0.000 & 0.069\(\)0.017 & 0.299\(\)0.005 & 0.017\(\)0.003 & 0.181\(\)0.003 & 0.388\(\)0.004 \\ DE (EF) & 0.007\(\)0.000 & 0.054\(\)0.010 & 0.269\(\)0.004 & 0.036\(\)0.001 & 0.089\(\)0.003 & 0.095\(\)0.003 \\ SNGP & 0.023\(\)0.004 & 0.200\(\)0.010 & 0.852\(\)0.012 & 0.442\(\)0.004 & 0.111\(\)0.063 & 0.227\(\)0.010 \\ ETP & 0.020\(\)0.002 & 0.051\(\)0.009 & 0.287\(\)0.007 & 0.096\(\)0.002 & 0.045\(\)0.008 & 0.100\(\)0.010 \\ DE (LF) & 0.292\(\)0.001 & 0.270\(\)0.009 & 0.567\(\)0.006 & 0.023\(\)0.002 & 0.319\(\)0.005 & 0.270\(\)0.003 \\ TMC & 0.013\(\)0.002 & 0.141\(\)0.002 & 0.072\(\)0.011 & 0.068\(\)0.002 & 0.180\(\)0.004 & 0.594\(\)0.008 \\ MGP & 0.006\(\)0.004 & **0.038\(\)0.007** & 0.079\(\)0.007 & **0.009\(\)0.003** & 0.062\(\)0.006 & 0.036\(\)0.003 \\ MNPs (Ours) & **0.005\(\)0.001** & 0.049\(\)0.008 & **0.040\(\)0.005** & 0.017\(\)0.003 & **0.038\(\)0.009** & **0.028\(\)0.006** \\   

Table 2: Test ECE (\(\)).

    &  \\  Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\  MCD & 82.15\(\)0.17 & 76.08\(\)0.61 & 64.65\(\)0.77 & 73.45\(\)0.11 & 48.97\(\)0.33 & 42.63\(\)0.08 \\ DE (EF) & 82.16\(\)0.18 & 76.94\(\)0.82 & 65.53\(\)0.20 & 73.99\(\)0.19 & 49.45\(\)0.35 & 41.92\(\)0.06 \\ SNGP & 72.46\(\)0.41 & 61.27\(\)1.24 & 56.52\(\)0.69 & 56.57\(\)0.17 & 38.19\(\)1.86 & 37.49\( DCM that stores a small number of informative context points. It also highlights the advantage of using efficient DNNs to imitate GPs, which a non-parametric model like MGP struggles to achieve.

### OOD Detection

Experimental SettingsFollowing the experimental settings of , we trained the models with three different corruption types of CIFAR10-C  as a multimodal dataset and evaluated the OOD detection performance using two different test datasets. The first test dataset comprised half CIFAR10-C and half SVHN  samples, while the second test dataset comprised half CIFAR10-C and half CIFAR100  samples. We used the Inception v3  pretrained with ImageNet as the backbone of all the baselines. The area under the receiver operating characteristic (AUC) is used as a metric to classify the predictive uncertainty into ID (class 0) and OOD (class 1).

ResultsTable 4 shows test accuracy and ECE with CIFAR10-C and OOD AUC against SVHN and CIFAR100. MNPs outperform all the baselines in terms of ECE and OOD AUC. A large difference in OOD AUC is observed which shows that the proposed adaptive RBF attention identifies OOD samples well. Also, we highlight MNPs outperform the current SOTA MGP in every metric. A marginal difference in test accuracy between DE (LF) and MNPs is observed, but MNPs achieve much lower ECE (approximately \(8.6\) folds) with higher OOD AUC than DE (LF).

## 6 Conclusion

In this study, we introduced a new multimodal uncertainty estimation method by generalising NPs for multimodal uncertainty estimation, namely Multimodal Neural Processes. Our approach leverages a simple and effective dynamic context memory, a Bayesian method of aggregating multimodal representations, and an adaptive RBF attention mechanism in a holistic and principled manner. We evaluated the proposed method on the seven real-world datasets and compared its performance against seven unimodal and multimodal baselines. The results demonstrate that our method outperforms all the baselines and achieves the SOTA performance in multimodal uncertainty estimation. A limitation of this work is that despite the effectiveness of the updating mechanism of DCM in practive, it is not theoretically guaranteed to obtain the optimal context memory. Nonetheless, our method effectively achieves both accuracy and reliability in an efficient manner. We leave developing a better updating mechanism for our future work. The broader impacts of this work are discussed in Appendix D.