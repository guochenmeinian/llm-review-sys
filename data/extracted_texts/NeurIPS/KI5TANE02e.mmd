# Score-based generative models are provably robust:

an uncertainty quantification perspective

 Nikiforos Mimikos-Stamatopoulos

Department of Mathematics

Universite Cote d'Azur

nmimikos@unice.fr

&Benjamin J. Zhang

Division of Applied Mathematics

Brown University

benjamin_zhang@brown.edu

&Markos A. Katsoulakis

Department of Mathematics and Statistics

University of Massachusetts Amherst

markos@umass.edu

###### Abstract

Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation (WUP) theorem, a _model-form UQ_ bound that describes how the \(L^{2}\) error from learning the score function propagates to a Wasserstein-1 (\(_{1}\)) ball around the true data distribution under the evolution of the Fokker-Planck equation. We show how errors due to (a) finite sample approximation, (b) early stopping, (c) score-matching objective choice, (d) score function parametrization expressiveness, and (e) reference distribution choice, impact the quality of the generative model in terms of a \(_{1}\) bound of computable quantities. The WUP theorem relies on Bernstein estimates for Hamilton-Jacobi-Bellman partial differential equations (PDE) and the regularizing properties of diffusion processes. Specifically, _PDE regularity theory_ shows that _stochasticity_ is the key mechanism ensuring SGM algorithms are provably robust. The WUP theorem applies to integral probability metrics beyond \(_{1}\), such as the total variation distance and the maximum mean discrepancy. Sample complexity and generalization bounds in \(_{1}\) follow directly from the WUP theorem. Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution. Additionally, our results clarify the _trade-offs_ among multiple error sources in SGMs.

## 1 Introduction

Score-based generative models (SGMs)  are highly effective , producing high quality samples, with more stable and less computationally intensive training methods than generative adversarial nets and normalizing flows . The models are empirically robust to approximations and errors in learning the score function. While SGM generalization properties have been studied for idealized conditions , analyses of their robustness in practical settings remains underexplored. This paper analyzes SGMs through the _regularity theory of nonlinear partial differential equations_ (PDEs) , specifically Hamilton-Jacobi-Bellman (HJB) equations . Our main result is the _Wasserstein uncertainty propagation_ (WUP) theorem (Theorem 3.1), a versatile model-form uncertainty quantification (UQ) bound which we use to theoretically explain the robustness of SGMs to approximation errors inpractical implementation. Generalization bounds for integral probability metrics (IPMs), such as the Wasserstein-1 (\(_{1}\)) and the total variation (TV) distance follow directly from the WUP theorem.

In the context of SGMs, the WUP theorem shows how an \(L^{2}\) neighborhood around the true score function propagates to an IPM neighborhood around the true data distribution. By relating how various approximations in SGMs contributes to \(L^{2}\) error with respect to the true score function, we establish how well the resulting SGM generalizes. Theorem 3.2 shows how the error in the learned score-function with respect to the explicit score-matching objective, or uniform-in-time \(L^{2}\) error, and the choice of initial condition define the radii of \(_{1}\) and TV neighborhoods, respectively. Additionally, Theorem 3.3 addresses the case where the score function is learned from finite data using the denoising score-matching (DSM) objective and incorporates early stopping.

Our bounds capture _trade-offs_ among the errors. The ability of SGMs to generalize depends on choices such as the early stopping time and how overtraining to the DSM objective and neglecting early stopping lead to SGMs that _memorize_ and overfit to the data. Notably, our approach relies on minimal assumptions on the data distribution, being agnostic to the manifold hypothesis. This contrasts with existing convergence results which assume the hypothesis  or not [5; 7]_a priori_. Furthermore, unlike previous work, we obtain our generalization bounds with respect to the \(_{1}\) distance directly, _without_ appealing to the Girsanov theorem, the Kullback-Leibler divergence, the \(^{2}\) divergence, or Pinsker's inequality [5; 11; 7]. This suggests our bounds may be _sharper_ than those which bound stronger norms and divergences. A notable feature of our DSM generalization bound is that the error bound is a computable function of the DSM objective, contrasting with prior results typically assume the learned score is close to the truth with respect to the ESM objective .

The WUP theorem also enables _robust uncertainty quantification_ (UQ) for score-based generative models, a rare capability in generative models. Robust UQ recognizes that learning complex models involves multiple sources of uncertainty due to modeling choices and imperfect data. The _distributionally robust_ perspective  quantifies a uncertainty set based on divergences and probability metrics [13; 14; 15; 16] to measure the impact of model uncertainty around a baseline model. The drawback is that these sets are typically difficult to find computationally. The WUP theorem is an example of a robust UQ bound for IPMs that is computable.

### Contributions

* We introduce the use of _regularity theory of nonlinear PDEs_ for analyzing generative flows [9; 10]. Our key idea is using the _Kolmogorov backward equation_ to study the evolution of IPMs under a generative flow, yielding generalization bounds. WUP is one particular application of this idea, and while we use it to study SGMs, this analysis is not limited to only SGMs. We show that the regularizing properties of the underlying Fokker-Planck equation imply SGM's robustness to errors with few assumptions on the data distribution. An intuitive explanation of our main technical contribution is provided in Section 4.
* The WUP theorem (Theorem 3.1), which we state and prove, describes how an \(L^{2}\) ball centered around the true score function maps to a neighborhood of IPMs (such as \(_{1}\), TV, and MMD ). WUP is a _model-form uncertainty quantification bound_, which maps uncertainties introduced by modeling choices when practically implementing SGM. The resulting generalization bounds WUP produces demonstrate that with properly chosen model parameters, SGM is robust to errors due to score-matching, finite sample approximation, early stopping time, and choice of the reference distribution. (Theorems 3.2, 3.3, and C.1).
* When applied to SGM, WUP produces generalization bounds under minimal hypotheses on the data distribution and score function, and explains the impact of implementation errors has on generalization. Our approach is _agnostic_ to the manifold hypothesis, applying whether or not the data distribution has a density. Moreover, it is adaptable to additional assumptions about the target distribution to yield improved generalization bounds.

### Related work

Convergence and generalization of SGMs have been well-studied. Many approaches [5; 7; 18] assume the target distribution is absolutely continuous with respect to a Gaussian, and obtain generalization bounds for TV, \(^{2}\), and \(_{1}\) by bounding the KL divergence, a stronger divergence, via the Girsanov theorem [11; 6; 18], Pinsker's inequality , functional inequalities , or other means [11; 19].

While  shows that kernel estimators for the score function are minimax optimal distribution estimators, they make no connections to deep learning-based SGMs, and they assume the target distribution is sub-Gaussian and is in a Sobolev space. Meanwhile,  has comprehensive sample complexity results that considers neural net approximations in the finite sample regime. Their results, however, are specialized to distributions in a Besov space, and, similar to other work [5; 11; 7; 19], rely on bounding the KL divergence, which breakdown under the manifold hypothesis.

Our results are similar to  which derives \(_{1}\) bounds directly, proving SGM convergence under the manifold hypothesis. However, the analysis assumes a particular discretization of the SGM. In , an uncertainty propagation bound for the Wasserstein-2 distance is proven using a similar approach to this work. Their work, however, relies on strong, uncheckable assumptions on the score function and target measure, does not address the errors we study here, and is not extendable to IPMs.

## 2 Background and notation

Let dimension \(d\), and terminal time \(T>0\). Denote \(^{d}^{d}\) to be the unit torus and \(R^{d}^{d}\) to be the torus of radius \(R>0\). Let \(()\) be the space of probability distributions on \(\), where \(\) is \(^{d}\) or \(R^{d}\). Let \(()\) be the target data distribution, known only through a finite number of samples \(\{z_{j}\}_{j=1}^{N}\). Let \(f:[0,T]^{d}\) be a vector field and \(:\) be a positive function. Score-based generative modeling [1; 2] is based on considering two Stochastic Differential Equations (SDEs for short) whose flow maps are inverses of each other. Define \(Y(s)\), \(X(t)\) to be the forward and reverse diffusion processes over time interval \(s,t[0,T]\) that evolve according to

\[Y(s) =-f(T-s,Y(s))s+(T-s)W(s),\ \ Y(0)\] (1) \[X(t) =[f(t,X(t))+(t)^{2}^{}(T-t,X(t)) ]t+(t)W(t),\ \ X(0) m_{0},\] (2)

where \(Y(s)^{}(s,)\), and \(W(t)\) is a standard Brownian motion in \(^{d}\). Here, \(^{}(s,)\) is the density of \(Y(s)\) at time \(s\) where the initial distribution was \(\). From , it is known that if \(m_{0}=^{}(T,)\), then \(X(t)^{}(T-t,)\). SGMs generated new samples from \(\) simulating trajectories of the reverse process (2) using an approximate score function \(_{}=^{}\). Typically \(f\) and \(\) are chosen such that \(^{}(T,)\) is well-approximated by a normal distribution, which is then used as the initial distribution.

The score function is learned via samples \(\{z_{j}\}_{j=1}^{N}\) from \(\) by minimizing one of several _score-matching_ objectives. Let \(_{}\) be some function where the parameters \(\) are learned via one of three score-matching objectives. For a path \(:[0,T]()\), we define the functionals

\[(,)=_{0}^{T}_{}|_{}- |^{2}(s)s\ \ _{L}(,)=_{0}^{T}_{}(|_{ }|^{2}+2_{})(s)s,\] (3)

where the subscript \(L\) highlights the linear dependence of \(_{L}\) on the underlying measure \(\). The _explicit score matching objective_ (ESM) is \(J_{}(^{},)=(^{},)\). As evaluation of the true score function \(^{}\) is typically inaccessible, the _implicit_ (ISM) \(J_{}(^{},)=_{L}(^{},)\) or the _denoising_ (DSM) _score-matching_ objectives [24; 1] are computed in practice

\[J_{}(^{},)=_{0}^{T}_{}_{} |_{}-^{x^{}}|^{2} ^{x^{}}(s)(x^{})s.\]

Here \(^{x^{}}(s)\) denotes the probability transition kernel from \(x^{}\) to \(x\) of (1) at time \(s\). The DSM objective is most frequently used in practice as it does not require computing derivatives of \(_{}\). It does, however, require the evaluation of \(^{x^{}}(s)\) in closed form, which is typically only accessible for linear SDEs.

**Remark 2.1** (Choice of domain \(\)).: _While our approach will also produce bounds when \(=^{d}\), we primarily focus on the torus \(R^{d}\), which is equivalent to a bounded domain with periodic boundary conditions. This choice ensures that the long-time behavior of the noising process (1) converges to the uniform distribution on \(R^{d}\). Therefore, we set \(f=\) and \((t)=\) instead of using the Ornstein-Uhlenbeck process. We make this choice for mathematical clarity, however our results generally apply, with minor modifications, to the entire space \(^{d}\) and for other noising processes._

### Equivalence of score-matching objectives in the finite sample regime

Finite sample approximations of ESM, ISM, and DSM are not generally equivalent. However, [24; 25], show that DSM becomes equivalent to ESM and ISM in the finite sample regime when \(^{}(s)\) is replaced with its _kernel_ density estimate \(^{N}(s)\), where \(^{N}(0)=^{N}=_{i=1}^{N}_{z_{i}}\). The kernel estimate, however, does not have a well-defined score at \(s=0\), so the DSM objective is often integrated only for \(s[,T]\), an example of early-stopping in SGM . In continuous time, this has the effect of score-matching for the mollified distribution \(^{N,}=^{N}_{}\), where \(_{}\) is the transition probability kernel for \(s=\)1. Then it can be shown that

\[(^{N,},)=J_{}(^{N,}, )=J_{}(^{N,},)=J_{}(^{N, },)+4\|}\|_{2}^{2}.\] (4)

## 3 An uncertainty quantification approach to generalization in SGMs

We describe our UQ approach to generalization in SGMs and overview our main results. Discussion of the proof methods is deferred to Sections 4 and 5. Our primary goal is to study how practical and approximation errors made in implementing SGMs translate into errors in the generative distribution relative to the true distribution.

### Source of errors in score-based generative modeling

We attribute errors of SGM to the following six sources:

* **Data distribution is only accessible via samples \(e_{1}\)**: The target distribution is only known through a finite set of samples. In practice, the regularity of the score function and data distribution, such as Lipschitzianity or whether the distribution has a density, is unknowable.
* **Choice of score-matching objective \(e_{2}\)**: In practice, score-matching objectives are approximated via samples. The DSM objective is most frequently used as it avoids computing derivatives of the score function. While DSM and ISM are equivalent given the exact density evolution \((x,s)\), they differ when approximated with finite samples. Previous analysis typically assumes that the learned score function is close to the true score function within some \(L^{2}\) distance, i.e., a ball determined by \(J_{}\). In contrast to previous work , we show in Theorem B.5 how training through DSM translates into a bound for ESM. Moreover we prove Proposition B.8 which states that if there exists a neural net that well-approximates the true score then that the target density is necessarily regular.
* **Expressiveness of the score function \(e_{3}\).** The expressivity of neural net approximations to the score function will depend on the particular expressivity of the parametrization.
* **Choice of reference distribution \(e_{4}\).** With access to the exact score function, the denoising process (2) produces trajectories that sample from \(\) only if the initial condition starts at \(^{}(,T)\). In practice, however, the initial distribution is usually a Gaussian approximation of \(^{}\) or, in the case of the OU noising process, its corresponding stationary distribution.
* **Early stopping of the denoising process \(e_{5}\).** In practice, the score-matching objective is integrated from \(s[,T]\), for small \(\), instead of \(s=0\). This prevents the SGM from _memorizing_ the training data , and is equivalent to running the denoising process for \(t[0,T-]\). Early stopping is _crucial_ when the data distribution is supported on a low-dimensional manifold, where it has no density with respect to Lebesgue measure. Previous analyses of SGM often adjust \(\) to optimize generalization bounds .
* **Discretization error \(e_{6}\).** The denoising SDE must be solved via a numerical scheme. Previous work  have considered the impact of discretization error on the generalization abilities of SGM. While our analysis does not consider the discretization error, our approach can be extended to study discretization error through the use of modified equations .

### Model-form uncertainty quantification

Let \(\) be an _integral probability metric_ (IPM) between measures \(_{1},_{2}()\)

\[(_{1},_{2})=_{}(x)d(_{2}-_ {1}),\] (5)for some function space \(\). We assume \(=\{:,\,\|\|_{} 1\}\) or \(\{:,\,\|\|_{} 1\}\), corresponding to the Wasserstein-1 (\(_{1}\)) and total variation distances, respectively. Recall that if \(_{1},_{2}\) have densities, then their TV distance is equivalent to the \(L^{1}\) distance between their densities.

Let \(\) be the target data distribution and take the stationary distribution of the noising process on the torus \(m_{g}(0)=)}\) as the initial condition. Given a learned score function \(_{}\), let \(m_{g}(T)\) be the generative distribution produced by SGM. We study how IPMs between \(\) and \(m_{g}(T)\) relate to errors from the first five sources of errors discussed above, i.e., we seek bounds of the form

\[(m_{g}(T),)(e_{1},e_{2},e_{3},e_{4},e_{5}).\] (6)

Our key insight is that for score-based generative modeling, we can derive bounds of the form (6) via a _model-form uncertainty quantification_ bound for the generative Fokker-Planck equation. The Wasserstein Uncertainty Propagation theorem formalizes this insight.

### Wasserstein uncertainty propagation theorem

The WUP theorem is a general statement about how \(L^{2}\) neighborhoods in the space of drift functions map to neighborhoods in \(()\) defined by IPMs.

**Theorem 3.1** (Wasserstein Uncertainty Propagation).: _Let \(=R^{d}\) or \(^{d}\). Let \(b^{1},b^{2}:[0,T]^{d}\) be given with \(\| b^{1}\|_{}<\), and \(m_{1},m_{2}()\). If \(m^{i}\) for \(i=1,2\) are given by_

\[_{t}m^{i}- m^{i}-(m^{i}b^{i})=0,\ \ m^{i}(0)=m_{i}\] (7)

_then, up to a universal constant \(C>0\), we have the following:_

* _If_ \(_{0 t T}\|(b^{2}-b^{1})(t)\|_{L^{2}(m^{2}(t))} _{0 t T}(_{}|(b^{2}-b^{1})(t,x)|^ {2}m^{2}(t,x)dx)^{1/2}_{1},\) _then_ \[\|m^{2}(T)-m^{1}(T)\|_{L^{1}()} C(\|_{}}+ 1)(}_{1}(m_{1},m_{2})+_{1} ),\] (8) _and_ \[\|m^{2}(T)-m^{1}(T)\|_{L^{1}()} C(\|_{}}+ 1)(\|m_{1}-m_{2}\|_{L^{1}()}+_{1}).\] (9)
* _For_ \(=R^{d}\)_, if_ \(\|b^{2}-b^{1}\|_{L^{2}(m^{2})}(_{0}^{T}_{}|(b ^{2}-b^{1})(t,x)|^{2}m^{2}(t,x)dxdt)^{1/2}_{2},\) _then_ \[_{1}(m^{2}(T),m^{1}(T)) CR^{}(1+ \|_{}})(_{1}(m_{1},m_{2})+_{2}).\] (10)

Equation (8) is a particularly notable result as we bound the TV distance between \(m^{1}(T)\) and \(m^{2}(T)\) in terms of a weaker \(_{1}\) metric between \(m_{1}\) and \(m_{2}\). This is due to the regularizing effects of diffusion processes, which is showcased in the proof. See Section 4 and Section A.1 for full details.

### Robustness of errors under ESM

We use WUP to produce generalization bounds when the only errors are due to the choice of the initial condition and the approximation of the score function with respect to the ESM objective.

**Theorem 3.2** (ESM bounds).: _Let \(()\) where \(=R^{d}\) for some \(R>0\). Moreover let \(e_{nn}>0\). Assume that the learned score function \(_{}\) is such that \((^{},) e_{nn}\). Then for \(_{}=_{}(T-t,x)\) the generated distribution \(m_{g}(T)\) where_

\[_{t}m_{g}- m_{g}-(m_{g}_{})=0(0,T],\ \ m_{g}(0)=^{d})},\] (11)

_satisfies_

\[_{1}(,m_{g}(T)) CR^{}(1+_ {}\|_{}})}{R^{2}}} _{1},^{d})}}_{(e_{4})}+}}_{(e_{3})}.\] (12)_If the stronger estimate \(_{0 t T}\|_{}-(^{})\|_{L^{2}(^{ }(t))}^{2} e_{nn}\), holds, then_

\[\|m_{g}(T)-\|_{L^{1}()} C(_{}\|_{ }}+1)(e^{-}}}{}_{ 1},(R^{d})}+} ).\] (13)

_Here, the constants \(C,\) depend only on the dimension \(d\)._

Applying WUP for \(b^{1}\) and \(b^{2}\) to be the true and learned score function, respectively, with Proposition D.3 on the convergence of the noising process to the stationary measure immediately yields the estimates above. Notice that the TV estimate (13) is comparable to Theorem 2 of , which also assumes a uniform-in-time \(L^{2}\)-accurate score function. Again, notice that we are able to bound the TV distance between \(m_{g}(T)\) and \(\) using the weaker \(_{1}\) distance between \(\) and \((R^{d})}\).

### Robustness of errors under DSM

In practice, the score is learned through samples using the DSM objective with an early stopping parameter . SGM is effective at producing samples from distributions supported on (or near) low dimensional manifolds. Our \(_{1}\) generalization bound describes how early stopping aids in generalization.

**Theorem 3.3**.: _(Pointwise DSM generalization) Let \(_{}=_{}(T-t,x)\) and \(m_{g}:[0,T] R^{d}\) be given by (11). Assume the learned score function is such that \(J_{DSM}(^{N,},)=(^{N,},)<e_{nn}\). Let \(0<<\) be such that \(^{N,}\), \(<^{}\). Then up to a dimensional constant \(C=C(d)>0\),_

\[_{1}(,m_{g}(T))}_{$)}}+R^{3/2}(1+_{}\|_{}}) }}_{1}(,(R^{d})})}_{$)}}+_{nn}},\] (14)

_where_

\[_{nn}}}}_{$)}}+}+T\| _{}\|_{C^{2}([0,T])}^{2})}_{1}(^{N},)}_{$)}}}_{$)}}.\] (15)

The pointwise DSM generalization bound applies to every finite training sample of size \(N\). A crucial part of this theorem relates the error in the practical DSM objective function to the ESM error needed to apply the WUP theorem. We connect the DSM objective early stopping to the ESM error with the mollified distribution \(^{}\) in Lemma B.5. This result is _agnostic_ to the manifold hypothesis for \(\), as long as \(>0\). Such bounds will blow up if the KL divergence were used instead. In fact, previous results that use the KL divergence to bound the TV distance  will produce vacuous bounds for the \(_{1}\) distance under the manifold hypothesis as their \(_{1}\) generalization bounds are derived by bounding the KL divergence.

**Remark 3.4** (Density lower bound).: _Similar to , our DSM generalization bound relies on a density lower bound assumption. We note however, that our DSM generalization bound holds for any random sample \(\{z_{i}\}_{i=1}^{N}\). This density lower bound assumption can be removed via Jensen's inequality if we consider the expected \(_{1}\) distance between \(\) and \(m_{g}(T)\) over random empirical measures. See Theorem C.1 and its proof in Section C._

**Remark 3.5** (Trade-offs and memorization).: _Theorem 3.3 captures trade-offs when training SGMs and memorization effects. To minimize the error from early stopping, we can let \( 0\). However, empirically, without early stopping, SGMs overfit to the kernel approximation and memorize the training data . The bound is vacuous when \(=0\) regardless of whether the distribution lies on a low-dimensional manifold. As \( 0\), training the DSM to be small implies that the score function must approximate a rough function with large Lipschitz constant, which will increase the bound. This shows that overtraining the DSM objective may not necessarily yield a better generative model. Moreover, suppose that \(^{N}=\), then as \( 0\) and \(e_{nn} 0\), we have that \(_{1}(,m_{g}(T)) 0\), indicating the model memorizes the training data. Our results corroborate those of ._Regularity theory of Hamilton-Jacobi-Bellman PDEs enables uncertainty quantification in SGMs

A recent result in  established connections between generative flows with the theory of PDEs, more specifically the theory of mean field games. We continue investigating these connections and showcase how one may study generative modeling algorithms by obtaining stability estimates for the Fokker-Planck equation. We provide a proof sketch for our Wasserstein Uncertainty Quantification theorem (Theorem 3.1). Our strategy involves (1) constructing test functions for the IPMs using the Kolmogorov backward equation, (2) choosing the suitable function space for the terminal data depending on the desired IPM, and (3) obtaining gradient estimates of the test functions via Bernstein estimates. The theorem relies on the gradient of the test function remaining bounded for \(t[0,T)\), which is guaranteed by the regularizing properties of diffusion processes. See A.1 for full details about the proof.

### Kolmogorov backward equation determines suitable test functions

From (7), we aim to compute bounds for \((m^{1}(T),m^{2}(T))=_{(x)}(m^{1}(T)-m^ {2}(T))dx\). The measure \(=m^{1}-m^{2}\) satisfies the PDE

\[_{t}--( b^{1}+m^{2}(b^{1}-b^{2 }))=0(0,T),\ \ (0)=m_{2}-m_{1}.\] (16)

Let \(:[0,T]\) be a test function in space and time. We integrate in space and time against the PDE (16) and integrate by parts to move the derivatives on to \(\) which yields

\[_{}(T,x)(T,x)-(0,x)(0,x)dx +_{0}^{T}_{}(-_{t}- +b^{1})dxdt\] (17) \[+_{0}^{T}_{}m^{2}(b^{1}-b^{2})dxdt=0\]

Notice that if we choose the test function \(\) to satisfy the _Kolmogorov backward equation_ (KBE)

\[-_{t}-+b^{1}=0[0,T) ,\ \ (T,x)=(x)\] (18)

with terminal condition \(\), then from (17), we have

\[_{}(T,x)(x)dx=_{}(0,x)(0,x)dx+_ {0}^{T}_{}m^{2}(t)(t,x)(b^{2}-b^{1})(t)dxdt.\] (19)

The equality above is valid for any terminal condition \(\). Depending on the choice of function space \(\) for \(\), we obtain bounds on different IPMs. Taking the supremum over \(\) we have

\[(m^{1}(T),m^{2}(T))_{}|_{} (0,x)(0,x)dx|+_{}|_{0}^{T} \!\!\!_{}m^{2}(b^{2}-b^{1})dxdt|.\] (20)

Recall that \(\) is related to \(\) via the KBE (18) To obtain bounds for \(\), we need to bound the two terms in (20), which depend on the choice of function space \(\) and assumptions on the drift terms.

### IPM bounds depend on choice of terminal function space and gradient estimates

We split our proof sketch into two parts; the first part focuses on deriving \(L^{1}\) estimates, while the second derives \(_{1}\) estimates.

\(L^{1}\) estimates.We first note that because of the _regularizing_ properties of (18) we can obtain bounds on \(_{}(0,x)(0,x)dx\) with weaker norms on \(\). Notice that

\[_{}(0,x)(0,x)dx\|(0)\|_{^{}} \|(0)\|_{},\] (21)

where \(\) denotes a generic space of functions, and \(^{}\) is its dual. In what follows, we show that regularizing effects of (18) takes functions in \(\) to functions with more regularity \(\) such that \(\) is compactly embedded in \(\). This in turn implies that \(\|\|_{^{}}\) is weaker than \(\) and so we are able to bound the stronger norm \(\) by the weaker norm.

* To prove (8), we \(=\|\|_{L^{1}()}\), which corresponds with \(=\{:,\|\|_{} 1\}\). Observe that we can take \(=\{:,\|\|_{} 1\}\), in which case we obtain \[_{}(0,x)(0,x)dx=\|(0,x)\|_{} _{}(m_{1}-m_{2})}dx\] (22) \[_{1}(m_{1},m_{2})\|(0,x)\|_{}.\] Notice that this bound crucially depends on showing that \((0,x)\) is Lipschitz.
* To prove (9), we can simply take \(=\), and obtain \[_{}(0,x)(0,x)\|(0,x)\|_{L^{1}()}\|( 0,x)\|_{}.\] (23)

For (8) and (9), Cauchy-Schwarz on the spatial integral shows the second term can be bounded as

\[_{0}^{T}_{}m^{2}(b^{2}-b^{1})dxdt _{0}^{T}\|(b^{1}-b^{2})(t)\|_{L^{2}(m^{2}(t))}\| (t,x)\|_{L^{2}(m^{2}(t))}dt\] (24) \[_{0 t T}\|(b^{1}-b^{2})(t)\|_{L^{2}(m^{2}(t))} _{0}^{T}\|(t,)\|_{}dt.\]

Notice here that it is crucial to produce estimates for \(\).

Wasserstein-1 (\(_{1}\)) estimates.To prove (10), we choose \(==\{:,\|\|_{ } 1\}\), the space of \(1\)-Lipschitz functions. We obtain (22) again, except the terminal data \(\) also has Lipschitz constant equal to 1. Applying Cauchy-Schwarz in space and time implies the second term is bounded

\[_{0}^{T}_{}m^{2}(b^{2}-b^{1})dxdt (_{0}^{T}_{}||^{2}m^{2}dxdt) ^{}\|b^{1}-b^{2}\|_{L^{2}(m^{2})}\] (25) \[ T_{0 t T}\|(t,)\|_{}\|b^{1 }-b^{2}\|_{L^{2}(m^{2})}.\]

We again see that we require estimates for \(\), and we need them to be bounded.

### Bernstein estimates from HJB theory provide gradient estimates

To obtain estimates for \(\), we could differentiate (18) to derive a PDE for \(\). The resulting PDE, however, will have \(_{t}()\) grow linearly with \(\), and so if we apply (reverse) Gronwall's inequality, the resulting estimates for \(\) will grow exponentially in time. To avoid this exponential time dependence, we first perform a Hopf-Cole transform \(u=-2\) on (18) to derive the HJB equation for \(u\)

\[-_{t}u- u+| u|^{2}+b u=0,\ \ u(T,x)=-2((x)).\] (26)

Here we provide an example of classical Bernstein estimates (Proposition D.1 and Corollary D.2) to obtain bounds for \(\) without using Gronwall's inequality . The main idea is to derive a PDE (60) for the function \(z=| u|^{2}\) by taking the gradient of (26) and then taking the inner product with \( u\). Then by showing that the function \(w(t,x)=z-Cu\) for sufficiently large \(C\) attains its maximum at \((T,x_{0})\), we can show that

\[z(t,x) w(T,x_{0})+Cu(t,x)|2|^{2}+C\|2 \|_{}+C\|u\|_{}.\] (27)

Using the maximum principle for (18), we find \(\|u\|_{}=\|2\|_{}\), yielding \(z(t,x) C\|\|_{C^{1}}\). Assuming that \(\) is Lipschitz continuous implies boundedness of \(z\) and therefore \(\) for all time. A similar result holds when \( L^{}\) only (see (59)). Detailed proofs and related bounds are provided in Proposition D.1. Applications of the bounds to derive the WUP is provided in Section A.1.

**Remark 4.1** (The regularizing role of stochasticity).: _In our analysis, the stochasticity in SGMs provides two types of regularizing effects. The first is early stopping, which adds a small amount of Gaussian noise of the data distribution. This, in effect, is equivalent to running the noising process _for a short amount of time and immediately mollifies the initial empirical distribution so that it has a smooth density. Second, the Laplacian is the key mechanism that regularizes the test function in (18) , which then allows us to bound, for example, the stronger \(\|\|_{L^{1}}\) norm by the weaker \(_{1}\)-norm. Recall that in PDE theory, the stochasticity of a generative flow manifests as the Laplacian operator in the Fokker-Planck and Kolmogorov backward equations . Without the Laplacian the regularizing the test functions in (18) would not be possible in general and, in fact, we would not have access to long time behavior results._

## 5 Proof sketches -- Score-based generative models are robust to errors

We now provide sketches of the proofs for the main generalization bounds in Theorems 3.2 and 3.3. The full proofs are provided in Sections A.2 and A.3, respectively.

### Theorem 3.2: ESM generalization bound

Theorem 3.2 is a generalization bound with respect to error from the score function approximation with respect to the ESM objective (\(e_{3}\)) and the choice of reference measure (\(e_{4}\)). Assuming we have an (uniform-in-time) \(L^{2}\)-close approximation of the score function, first apply the WUP Theorem 3.1 with \(m_{1}=(R^{4})}\) and \(m_{2}=^{}(T,)\). The distance between \(m_{1}\) and \(m_{2}\) can be expressed in terms of \(\) by studying the long time behavior of periodic solutions to the heat equation. Proposition D.3 shows the heat equation is a contraction under \(_{1}\). Applying it to \(m^{1}\) and \(m^{2}\) yields the desired result.

While Theorem 3.2 has no explicit assumptions on \(\), the assumption that the approximate score function \(_{}\) is close in \(L^{2}\) to the true score implicitly implies regularity of \(\). Specifically, if \(_{}\) is Lipschitz (which is true for neural networks in practice) and satisfies \((,)<e_{nn}\), then \(\) must have finite entropy. This implies that \(\) necessarily has a density. See Proposition B.8 for the formal statement and proof.

### Theorem 3.3: Pointwise DSM generalization bound

In practice, the score function is learned via a Monte Carlo approximation of the DSM objective (2). To avoid overfitting to the kernel estimator and memorizing the dataset , the time integral is taken only over \(s[,T]\) where \(\) is the early stopping parameter. This is equivalent to training with the ESM objective with the true score replaced with the kernel approximation at time \(\). To derive generalization bounds of SGMs trained via DSM, we establish the relationships between (1) the mollified distribution \(^{}=()\) and the true distribution \(\), and (2) the DSM objective \(J_{DSM}(^{N,},)=(^{N,},)\) and the ESM objective with respect to the score function of the mollified distribution \((^{^{}},)\). Formally, this strategy involves bounding the following two terms

\[_{1}(,m_{g}(T))}_{}_{1}(,^{})}_{(e_{5})}+_{1}(^{},m_{g}(T))}_{}\] (28)

For the early stopping error, we use the regularizing properties of the heat equation to show that \(_{1}(,^{}) C\), where \(C\) only depends on the dimension \(d\). This implies that, measured in \(_{1}\), early stopping only incurs a nominal \(C\) error _even if the \(\) does not admit a density_. This result would not possible if we were to study generalization error in terms of KL or TV directly.

A bound for the second term is obtained by comparing the ESM objective value between the learned score function and the true score function of the early stopped distribution, \((^{^{*}},)\) to the DSM objective \((^{N,},)\). We present Theorem B.5 and its corollary, which under the assumption that \(^{N,}\) and \(^{^{*}}\) have a lower bound \(\), state that if \((^{N,},_{})<e_{nn}\), then \((^{^{*}},_{})<e_{nn}^{}\) with

\[e_{nn}^{}=e_{nn}+C1+}+ {1}{}+T\|_{}\|_{C^{2}([0,T])}^{2} _{1}(^{N},).\] (29)

The main idea behind Theorem B.5 is to note that the difference between the ESM and DSM objective functions can be written as a difference in ISM objective functions plus the entropy difference between \(^{N,}\) and \(^{^{*}}\). Details for bounding this term is provided in Proposition B.2 and Lemma B.3.

To arrive at the final result, apply the WUP theorem to derive generalization bounds for \(_{1}(^{},m_{g}(T))\) under the assumption that \((^{^{}},)<e_{nn}^{}\), along with (29). Finally, combine this result with the error due to the early stopping \(_{1}(,^{})\). Full details of this proof is provided in Section A.3.

Discussion: PDE regularity theory and UQ for generative modeling

Our main contribution is the study of generalization in score-based generative models from the perspective of uncertainty quantification. The regularity theory of nonlinear PDEs is the key technical tool that produces our results. We emphasize that the tools we use here can be used generative models beyond SGMs, and that we have not pushed our analysis to the limits of our tools in SGMs. Moreover, we also emphasize some downstream UQ applications for SGMs that may be of future interest.

### The significance of the _regularizing_ properties of SGMs

A surprising result of our work is deriving bounds for the stronger \(L^{1}\) distance in terms of the weaker \(_{1}\) distance (see (22) and Theorem 3.2). The key insight (Equation (21)) is that the evolution of observables defined by the KBE (18) _regularizes_ the test function. We have not fully exploited the regularizing effects of (18) in this paper, as we only focused on \(L^{1}\) and \(_{1}\) estimates.

**Improved bounds in Sobolev spaces \(H^{s}\)**. To illustrate other extensions and choices of \(\) in (21), observe that in the trivial case when \(b^{1}=b^{2}=0\), (19) simplifies to \((T,x)(x)dx=(0,x)((T))(x)dx,\) where \(\) is the heat kernel. If \(\|\|_{} 1,\) then \((T) C^{}\) and we have estimates of the form \(_{1}(m^{1}(T),m^{2}(T)) C(s,T,d)\|m_{1}-m_{2}\|_{H^{-s}}\) for all \(s.\) When \(b^{1},b^{2}\) are not identically zero we still expect such estimates, though they will depend on the regularity of \(b^{1}\). To highlight the importance of regularizing effects, note that by , if \((^{d}),\) then for the empirical measure \(^{N},\) we expect \(_{1}(,^{N})}}\). However if \(s>,\|-^{N}\|_{H^{-s}}}.\) This suggests that improved regularity may influence overcoming the curse of dimensionality.

### A connection to likelihood-free inference

Computing expectations with respect to posterior distributions is a key task in Bayesian inference. Generative modeling, in particular, has a key role in future developments of _likelihood-free_ inference [32; 33; 34; 35]. For generative models to be trustworthy for inference, they must to be shown to be robust. The WUP theorem provides error bounds for approximating expectations with respect to some true unknown distribution, and may be significant for SGMs in likelihood-free inference.

For example, suppose we wish to estimate \(_{}h\) for some distribution \(\) and observable \(h\), and an SGM \(m_{g}(T)\) is constructed to approximate the expectation. Bounds of the form (6), such as the WUP theorem, translate into guarantees on the expectations:

\[_{}h-_{m_{g}(T)}h(m_{g}(T),)(e_{1},e_{2},e_{3},e_{4},e_{5}).\] (30)

In the context of SGM, the inequalities are _a posteriori_ bounds, meaning they can be computed after learning the model. Additional regularity on \(h\) may yield improved guarantees.

### Enabling distributionally robust optimization (DRO)

Robust UQ methods are based on the perspective that learning any complex model will typically involve multiple sources of uncertainty due to modeling choices, model reduction or learning from imperfect data. These uncertainties are not just in parameters but are inherently present in the mathematical model itself and will propagate to any predictions. There is substantial related work in recent years using a distributional robustness perspective, , using divergences or probability metrics and their variational representations to quantify the impact of model uncertainty around a baseline model that may be either learned (e.g. a generative model) or could be just an empirical distribution from an unknown true distribution. The approach can generally be described as quantifying an uncertainty set around the baseline model that the worst-case distribution belongs to via some neighborhood defined in terms of a probability divergence [13; 14], a Wasserstein distance  or maximum mean discrepancy . There are, however, drawbacks to each of these approaches: a divergence ball contains only distributions with the same support as the baseline, while the uncertainty set may be hard to determine practically. The WUP Theorem is a related robust UQ notion where the uncertainty ball is in an IPM, e.g. 1-Wasserstein, MMD, or TV. However, it allows us to bypass the robust UQ for stochastic processes relying on restrictive, path-space probability divergence-based approaches or Girsanov's Theorem, [36; 37]. Furthermore, the WUP Theorem bounds use PDE theory to provide a computable uncertainty set, as we also demonstrate in the case of DSM, see Theorem 3.3.