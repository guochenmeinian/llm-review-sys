# Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization

Nathan Grinsztajn

InstaDeep

n.grinsztajn@instadeep.com

&Daniel Furelos-Blanco

Imperial College London1

Shikha Surana

InstaDeep

&Clement Bonnet

InstaDeep

&Thomas D. Barrett

InstaDeep

###### Abstract

Applying reinforcement learning (RL) to combinatorial optimization problems is attractive as it removes the need for expert knowledge or pre-solved instances. However, it is unrealistic to expect an agent to solve these (often NP-)hard problems in a single shot at inference due to their inherent complexity. Thus, leading approaches often implement additional search strategies, from stochastic sampling and beam-search to explicit fine-tuning. In this paper, we argue for the benefits of learning a population of complementary policies, which can be simultaneously rolled out at inference. To this end, we introduce Poppy, a simple training procedure for populations. Instead of relying on a predefined or hand-crafted notion of diversity, Poppy induces an unsupervised specialization targeted solely at maximizing the performance of the population. We show that Poppy produces a set of complementary policies, and obtains state-of-the-art RL results on four popular NP-hard problems: traveling salesman, capacitated vehicle routing, 0-1 knapsack, and job-shop scheduling.

## 1 Introduction

In recent years, machine learning (ML) approaches have overtaken algorithms that use handcrafted features and strategies across a variety of challenging tasks (Mnih et al., 2015; van den Oord et al., 2016; Silver et al., 2018; Brown et al., 2020). In particular, solving combinatorial optimization (CO) problems - where the maxima or minima of an objective function acting on a finite set of discrete variables is sought - has attracted significant interest (Bengio et al., 2021) due to both their (often NP) hard nature and numerous practical applications across domains varying from logistics (Sbihi and Eglese, 2007) to fundamental science (Wagner, 2020).

As the search space of feasible solutions typically grows exponentially with the problem size, exact solvers can be challenging to scale; hence, CO problems are often also tackled with handcrafted heuristics using expert knowledge. Whilst a diversity of ML-based heuristics have been proposed, reinforcement learning (RL; Sutton and Barto, 2018) is a promising paradigm as it does not require pre-solved examples of these hard problems. Indeed, algorithmic improvements to RL-based CO solvers, coupled with low inference cost, and the fact that they are by design targeted at specific problem distributions, have progressively narrowed the gap with traditional solvers.

RL methods frame CO as sequential decision-making problems, and can be divided into two families (Mazyakina et al., 2021). First, _improvement methods_ start from a feasible solution and iterativelyimprove it through small modifications (actions). However, such incremental search cannot quickly access very different solutions, and requires handcrafted procedures to define a sensible action space. Second, _construction methods_ incrementally build a solution by selecting one element at a time. In practice, it is often unrealistic for a learned heuristic to solve NP-hard problems in a single shot, therefore these methods are typically combined with search strategies, such as stochastic sampling or beam search. However, just as improvement methods are biased by the initial starting solution, construction methods are biased by the single underlying policy. Thus, a balance must be struck between the exploitation of the learned policy (which may be ill-suited for a given problem instance) and the exploration of different solutions (where the extreme case of a purely random policy will likely be highly inefficient).

In this work, we propose Poppy, a construction method that uses a _population_ of agents with suitably diverse policies to improve the exploration of the solution space of hard CO problems. Whereas a single agent aims to perform well across the entire problem distribution, and thus has to make compromises, a population can learn a set of heuristics such that only one of these has to be performant on any given problem instance. However, realizing this intuition presents several challenges: (i) naively training a population of agents is expensive and challenging to scale, (ii) the trained population should have complementary policies that propose different solutions, and (iii) the training approach should not impose any handcrafted notion of diversity within the set of policies given the absence of clear behavioral markers aligned with performance for typical CO problems.

Challenge (i) can be addressed by sharing a large fraction of the computations across the population, specializing only lightweight policy heads to realize the diversity of agents. Moreover, this can be done on top of a pre-trained model, which we clone to produce the population. Challenges (ii) and (iii) are jointly achieved by introducing an RL objective aimed at specializing agents on distinct subsets of the problem distribution. Concretely, we derive a policy gradient method for the population-level objective, which corresponds to training only the agent which performs best on each problem. This is intuitively justified as the performance of the population on a given problem is not improved by training an agent on an instance where another agent already has better performance. Strikingly, we find that judicious application of this conceptually simple objective gives rise to a population where the diversity of policies is obtained without explicit supervision (and hence is applicable across a range of problems without modification) and essential for strong performance.

Our contributions are summarized as follows:

1. We motivate the use of populations for CO problems as an efficient way to explore environments that are not reliably solved by single-shot inference.
2. We derive a new training objective and present a practical training procedure that encourages performance-driven diversity (i.e. effective diversity without the use of explicit behavioral markers or other external supervision).
3. We evaluate Poppy on four CO problems: traveling salesman (TSP), capacitated vehicle routing (CVRP), 0-1 knapsack (KP), and job-shop scheduling (JSSP). In these four problems, Poppy consistently outperforms all other RL-based approaches.

## 2 Related Work

ML for Combinatorial OptimizationThe first attempt to solve TSP with neural networks is due to Hopfield and Tank (1985), which only scaled up to 30 cities. Recent developments of bespoke neural architectures (Vinyals et al., 2015; Vaswani et al., 2017) and performant hardware have made ML approaches increasingly efficient. Indeed, several architectures have been used to address CO problems, such as graph neural networks (Dai et al., 2017), recurrent neural networks (Nazari et al., 2018), and attention mechanisms (Doudon et al., 2018). Kool et al. (2019) proposed an encoder-decoder architecture that we employ for TSP, CVRP and KP. The costly encoder is run once per problem instance, and the resulting embeddings are fed to a small decoder iteratively rolled out to get the whole trajectory, which enables efficient inference. This approach was furthered by Kwon et al. (2020) and Kim et al. (2022), who leveraged the underlying symmetries of typical CO problems (e.g. of starting positions and rotations) to realize improved training and inference performance using instance augmentations. Kim et al. (2021) also draw on Kool et al. and use a hierarchical strategy where a seeder proposes solution candidates, which are refined bit-by-bit by a reviser. Closer to our work, Xin et al. (2021) train multiple policies using a shared encoder and separate decoders.

Whilst this work (MDAM) shares our architecture and goal of training a population, our approach to enforcing diversity differs substantially. MDAM explicitly trades off performance with diversity by jointly optimizing policies and their KL divergence. Moreover, as computing the KL divergence for the whole trajectory is intractable, MDAM is restricted to only using it to drive diversity at the first timestep. In contrast, Poppy drives diversity by maximizing population-level performance (i.e. without any explicit diversity metric), uses the whole trajectory and scales better with the population size (we have used up to 32 agents instead of only 5).

Additionally, ML approaches usually rely on mechanisms to generate multiple candidate solutions (Mazyakina et al., 2021). One such mechanism consists in using improvement methods on an initial solution: de O. da Costa et al. (2020) use policy gradients to learn a policy that selects local operators (2-opt) given a current solution in TSP, while Lu et al. (2020) and Wu et al. (2021) extend this method to CVRP. This idea has been extended to enable searching a learned latent space of solutions (Hottung et al., 2021). However, these approaches have two limitations: they are environment-specific, and the search procedure is inherently biased by the initial solution.

An alternative exploration mechanism is to generate a diverse set of trajectories by stochastically sampling a learned policy, potentially with additional beam search (Joshi et al., 2019), Monte Carlo tree search (Fu et al., 2021), dynamic programming (Kool et al., 2021), active search (Hottung et al., 2022), or simulation-guided search (Choo et al., 2022). However, intuitively, the generated solutions tend to remain close to the underlying deterministic policy, implying that the benefits of additional sampled candidates diminish quickly.

Population-Based RLPopulations have already been used in RL to learn diverse behaviors. In a different context, Gupta et al. (2018), Eysenbach et al. (2019), Hartikainen et al. (2020) and Pong et al. (2020) use a single policy conditioned on a set of goals as an implicit population for unsupervised skill discovery. Closer to our approach, another line of work revolves around explicitly storing a set of distinct policy parameters. Hong et al. (2018), Doan et al. (2020), Jung et al. (2020) and Parker-Holder et al. (2020) use a population to achieve a better coverage of the policy space. However, they enforce explicit attraction-repulsion mechanisms, which is a major difference with respect to our approach where diversity is a pure byproduct of performance optimization.

Our method is also related to approaches combining RL with evolutionary algorithms (EA; Khadka and Tumer, 2018; Khadka et al., 2019; Pourchot and Sigaud, 2019), which benefit from the sample-efficient RL policy updates while enjoying evolutionary population-level exploration. However, the population is a means to learn a unique strong policy, whereas Poppy learns a set of complementary strategies. More closely related, Quality-Diversity (QD; Pugh et al., 2016; Cully and Demiris, 2018) is a popular EA framework that maintains a portfolio of diverse policies. Pierrot et al. (2022) has recently combined RL with a QD algorithm, Map-Elites (Mouret and Clune, 2015); unlike Poppy, QD methods rely on handcrafted behavioral markers, which is not easily amenable to the CO context.

One of the drawbacks of population-based RL is its expensive cost. However, recent approaches have shown that modern hardware, as well as targeted frameworks, enable efficient vectorized population training (Flajolet et al., 2022), opening the door to a wider range of applications.

## 3 Methods

### Background and Motivation

RL FormulationA CO problem instance \(\) sampled from some distribution \(\) consists of a discrete set of \(N\) variables (e.g. city locations in TSP). We model a CO problem as a Markov decision process (MDP) defined by a state space \(\), an action space \(\), a transition function \(T\), and a reward function \(R\). A state is a trajectory through the problem instance \(_{t}=(x_{1},,x_{t})\) where \(x_{i}\), and thus consists of an ordered list of variables (not necessarily of length \(N\)). An action, \(a\), consists of choosing the next variable to add; thus, given state \(_{t}=(x_{1},,x_{t})\) and action \(a\), the next state is \(_{t+1}=T(_{t},a)=(x_{1},,x_{t},a)\). Let \(^{*}\) be the set of _solutions_, i.e. states that comply with the problem's constraints (e.g., a sequence of cities such that each city is visited once and ends with the starting city in TSP). The reward function \(R:^{*}\) maps solutions into scalars. We assume the reward is maximized by the optimal solution (e.g. \(R\) returns the negative tour length in TSP).

A _policy_\(_{}\) parameterized by \(\) can be used to generate solutions for any instance \(\) by iteratively sampling the next action \(a\) according to the probability distribution \(_{}(,_{t})\). We learn \(_{}\) using REINFORCE [Williams, 1992]. This method aims at maximizing the RL objective \(J()_{}_{_{ },}R()\) by adjusting \(\) such that good trajectories are more likely to be sampled in the future. Formally, the policy parameters \(\) are updated by gradient ascent using \(_{}J()=_{}_{ _{},}(R()-b_{})_{}(p_{}())\), where \(p_{}()=_{t}_{}(a_{t+1},_{t})\) and \(b_{}\) is a baseline. The gradient of the objective, \(_{}J\), can be estimated empirically using Monte Carlo simulations.

Motivating ExampleWe argue for the benefits of training a population using the example in Figure 1. In this environment, there are three actions: **Left**, **Right**, and **Up**. **Up** leads to a medium reward, while **Left/Right** lead to low/high or high/low rewards (the configuration is determined with equal probability at the start of each episode). Crucially, the left and right paths are intricate, so the agent cannot easily infer from its observation which one leads to a higher reward. Then, the best strategy for a computationally limited agent is to always go **Up**, as the guaranteed medium reward (2 scoops) is higher than the expected reward of guessing left or right (1.5 scoops). In contrast, two agents in a population can go in opposite directions and always find the maximum reward. There are two striking observations: (i) the agents do not need to perform optimally for the population performance to be optimal (one agent gets the maximum reward), and (ii) the performance of each individual agent is worse than in the single-agent case.

The discussed phenomenon can occur when (i) some optimal actions are too difficult to infer from observations and (ii) choices are irreversible (i.e. it is not possible to recover from a sub-optimal decision). This problem setting can be seen as a toy model for the challenge of solving an NP-hard CO problem in a sequential decision-making setting. In the case of TSP, for example, the number of possible unique tours that could follow from each action is exponentially large and, for any reasonably finite agent capacity, essentially provides the same obfuscation over the final returns. In this situation, as shown above, maximizing the performance of a population will require agents to specialize and likely yield better results than in the single-agent case.

### Poppy

We present the components of Poppy: an RL objective encouraging agent specialization, and an efficient training procedure taking advantage of a pre-trained policy.

Population-Based ObjectiveAt inference, reinforcement learning methods usually sample several candidates to find better solutions. This process, though, is not anticipated during training, which optimizes the 1-shot performance with the usual RL objective \(J()=_{}_{_{}, }R()\) previously presented in Section 3.1. Intuitively, given \(K\) trials, we would like to find the best set of policies \(\{_{1},,_{K}\}\) to rollout once on a given problem. This gives the following population objective:

Figure 1: In this environment, the upward path always leads to a medium reward, while the left and right paths are intricate such that either one may lead to a low reward or high reward with equal probability. **Left**: An agent trained to maximize its expected reward converges to taking the safe upward road since acting optimally is too computationally intensive, as it requires solving the maze. **Right**: A 2-agent population can always take the left and right paths and thus get the largest reward.

\[J_{}(_{1},,_{K})_{} _{_{1}_{_{1}},,_{K}_{_{K}}} [R(_{1}),,R(_{K})],\]

where each trajectory \(_{i}\) is sampled according to the policy \(_{_{i}}\). Maximizing \(J_{}\) leads to finding the best set of \(K\) agents which can be rolled out in parallel for any problem.

**Theorem 1** (Policy gradient for populations).: _The gradient of the population objective is:_

\[ J_{}(_{1},_{2},,_{K})=_{ }_{_{1}_{_{1}},,_{K} _{_{K}}}R(_{i^{*}})-R(_{i^{*}*}) p _{_{i^{*}}}(_{i^{*}}),\]

_where: \(i^{*}=*{arg\,max}_{i\{1,,K\}}R(_{i})\) (index of the agent that got the highest reward) and \(i^{**}=*{arg\,max}_{i i^{*}}R(_{i})\) (index of the agent that got the second highest reward)._

The proof is provided in Appendix B.1. Remarkably, it corresponds to rolling out every agent and only training the one that got the highest reward on any problem. This formulation applies across various problems and directly optimizes for population-level performance without explicit supervision or handcrafted behavioral markers.

The theorem trivially holds when instead of a population, one single agent is sampled \(K\) times, which falls back to the specific case where \(_{1}=_{2}==_{K}\). In this case, the gradient becomes:

\[ J_{}()=_{}_{ _{1}_{},,_{K}_{}}R(_{i^{*}}) -R(_{i^{*}*}) p_{}(_{i^{*}}),\]

_Remark_.: The occurrence of \(R(_{i^{**}})\) in the new gradient formulation can be surprising. However, Theorem 1 can be intuitively understood as training each agent on its true contribution to the population's performance: if for any problem instance the best agent \(i^{*}\) was removed, the population performance would fallback to the level of the second-best agent \(i^{**}\), hence its contribution is indeed \(R(_{i^{*}})-R(_{i^{**}})\).

Optimizing the presented objective does not provide any strict diversity guarantee. However, note that diversity maximizes our objective in the highly probable case that, within the bounds of finite capacity and training, a single agent does not perform optimally on all subsets of the training distribution. Therefore, intuitively and later shown empirically, diversity emerges over training in the pursuit of maximizing the objective.

```
1:Input: problem distribution \(\), number of agents \(K\), batch size \(B\), number of training steps \(H\), pre-trained parameters \(\).
2:\(_{1},_{2},,_{K}()\) {Clone the pre-trained agent \(K\) times.}
3:for step 1 to \(H\)do
4:\(_{i}()\  i 1,,B\)
5:\(_{i}^{k}(_{i},_{k})\  i 1,,B,  k 1,,K\)
6:\(k_{i}^{*}*{arg\,max}_{k K}R(_{i}^{k})\  i 1,,B\) {Select the best agent for each problem \(_{i}\).}
7:\( L(_{1},,_{K})_{i B} (_{i}^{k_{i}^{*}})\) {Backpropagate through these only.}
8:\((_{1},,_{K})(_{1},,_{K})-  L(_{1},,_{K})\) ```

**Algorithm 1** Poppy training

Training ProcedureThe training procedure consists of two phases:

1. We train (or reuse) a single agent using an architecture suitable for solving the CO problem at hand. We later outline the architectures used for the different problems.
2. The agent trained in Phase 1 is cloned \(K\) times to form a \(K\)-agent population. The population is trained as described in Algorithm 1: only the best agent is trained on any problem. Agents implicitly specialize in different types of problem instances during this phase.

Phase 1 enables training the model without the computational overhead of a population. Moreover, we informally note that applying the Poppy objective directly to a population of untrained agents can be unstable. Randomly initialized agents are often ill-distributed, hence a single (or few) agent(s) dominate the performance across all instances. In this case, only the initially dominating agents receive a training signal, further widening the performance gap. Whilst directly training a population of untrained agents for population-level performance may be achievable with suitable modifications, we instead opt for the described pre-training approach as it is efficient and stable. Throughout this article, we use the reinforcement learning baseline of POMO (Kwon et al., 2020), which was proved to be efficient when training with multiple starting points. We investigate in Appendix C the effect of using the analytical baseline from Theorem 1 instead, which we show leads to improved performance.

ArchitectureTo reduce the memory footprint of the population, some of the model parameters can be shared. For example, the architecture for TSP, CVRP, and KP uses the attention model by Kool et al. (2019), which decomposes the policy model into two parts: (i) a large encoder \(h_{}\) that takes an instance \(\) as input and outputs embeddings \(\) for each of the variables in \(\), and (ii) a smaller decoder \(q_{}\) that takes the embeddings \(\) and a trajectory \(_{t}\) as input and outputs the probabilities of each possible action. Figure 2 illustrates the training phases of such a model. For JSSP, we implement a similar encoder-decoder architecture taken from Jumanji (Bonnet et al., 2023) with the encoder being shared across the population. For all problems, we build a population of size \(K\) by sharing the encoder \(h_{}\) with all agents, and implementing agent-specific decoders \(q_{_{i}}\) for \(i\{1,,K\}\). This is motivated by (i) the encoder learning general representations that may be useful to all agents, and (ii) reducing the parameter overhead of training a population by keeping the total number of parameters close to the single-agent case. Please see Appendix A.1 for a discussion on model sizes.

## 4 Experiments

We evaluate Poppy on four CO problems: TSP, CVRP, KP and JSSP. We use the JAX implementations from Jumanji (Bonnet et al., 2023) to leverage hardware accelerators (e.g. TPU). To emphasize the generality of our method, we use the hyperparameters from Kwon et al. (2020) for each problem when possible. We run Poppy for various population sizes to demonstrate its time-performance tradeoffs.

TrainingOne training step corresponds to computing policy gradients over a batch of 64 (TSP, CVRP, KP) or 128 (JSSP) randomly generated instances for each agent in the population. Training time varies with problem complexity and training phase. For instance, in TSP with 100 cities, Phase 1 takes 4.5M steps (5 days), whereas Phase 2 takes 400k training steps and lasts 1-4 days depending on the population size. We took advantage of our JAX-based implementation by running all our experiments on a v3-8 TPU.

### Routing Problems (TSP, CVRP)

TasksWe consider two routing tasks: TSP and CVRP. Given a set of \(n\) cities, TSP consists in visiting every city once and coming back to the starting city while minimizing the total traveled distance. CVRP is a variant of TSP where a vehicle with limited capacity departs from a depot node and needs to fulfill the demands of the visited nodes. The vehicle's capacity is restored when the depot is visited.

Figure 2: Phases of the training process with a model using static instance embeddings. **Left (Phase 1**): the encoder and the decoder are trained from scratch. **Right (Phase 2)**: the decoder is cloned \(K\) times, and the whole model is trained using the Poppy training objective (i.e. the gradient is only propagated through the decoder that yields the highest reward).

SetupWe use an encoder-decoder architecture based on that by Kool et al. (2019) and Kwon et al. (2020) (see Appendix E for details). During Phase 1, an architecture with a single decoder is trained; then, the decoder is cloned for each agent in the population and trained according to Algorithm 1. Following Kwon et al. (2020), we generate multiple solutions for each instance \(\) by considering a set of \(P[1,N]\)_starting points_, where \(N\) is the number of instance variables, and use the same reinforce baseline at training. We refer the reader to Appendix A.2 for details.

The training instances are of size \(n=100\) in both tasks. The testing instances for \(n=100\) are due to Kool et al. (2019) and Kwon et al. (2020) for TSP and CVRP, respectively, whereas those for \(n\{125,150\}\) are due to Hottung et al. (2022).

BaselinesWe use the exact TSP-solver Concorde Applegate et al. (2006) and the heuristic solver HGS (Vidal et al., 2012; Vidal, 2022) to compute the optimality gaps for TSP and CVRP, respectively. The performance of LKH3 (Helsgaun, 2017) is also reported for TSP. We highlight that the performance of HGS is not necessarily optimal.

In both TSP and CVRP, we evaluate the performance of the RL methods MDAM (Xin et al., 2021) with and without beam search, LIH (Wu et al., 2021), and POMO (Kwon et al., 2020) in three settings: (i) with greedy rollouts, (ii) using \(r\) stochastic rollouts, and (iii) using an ensemble of \(r\) decoders trained in parallel. The value of \(r\) corresponds to the largest tested population (i.e., 16 in TSP and 32 in CVRP). Setting (ii) is run to match Poppy's runtime with its largest population, while Setting (iii) performs Phase 2 without the population objective. We also report the performance of the RL methods Att-GCRN+MCTS (Fu et al., 2021) and 2-Opt-DL (de O. da Costa et al., 2020) in TSP, and NeuRewriter (Chen and Tian, 2019) and NLNS (Hottung and Tierney, 2020) in CVRP.

ResultsTables 1 and 2 display the results for TSP and CVRP, respectively. The columns show the average tour length, the optimality gap, and the total runtime for each test set. The baseline performances from Fu et al. (2021), Xin et al. (2021), Hottung et al. (2022) and Zhang et al. (2020) were obtained with different hardware (Nvidia GTX 1080 Ti, RTX 2080 Ti, and Tesla V100 GPUs, respectively) and framework (PyTorch); thus, for fairness, we mark these times with \(*\). As a comparison guideline, we informally note that these GPU inference times should be approximately divided by 2 to get the converted TPU time.

In both TSP and CVRP, Concorde and HGS remain the best algorithms as they are highly specialized solvers. In relation to RL methods, Poppy reaches the best performance across every performance metric in just a few minutes and, remarkably, performance improves as populations grow.

In TSP, Poppy outperforms Att-GCRN+MCTS despite the latter being known for scaling to larger instances than \(n=100\), hence showing it trades off performance for scale. We also emphasize that specialization is crucial to achieving state-of-the-art performance: Poppy 16 outperforms POMO 16 (ensemble), which also trains 16 agents in parallel but without the population objective (i.e. without specializing to serve as an ablation of the objective).

In CVRP, we observe Poppy 32 has the same runtime as POMO with 32 stochastic rollouts while dividing by 2 the optimal gap for \(n=100\).2 Interestingly, this ratio increases on the generalization instance sets with \(n\{125,150\}\), suggesting that Poppy is more robust to distributional shift.

AnalysisFigure 3 illustrates the resulting behavior from using Poppy on TSP100. We display on the left the training curves of Poppy for three population sizes: 4, 8, 16. Starting from POMO, the population performances quickly improve. Strikingly, Poppy 4 outperforms POMO with 100 stochastic samples, despite using 25 times fewer rollouts. The rightmost plot shows that whilst the population-level performance improves with population size, the average performance of a random agent from the population on a random instance gets worse. We hypothesize that this is due to agent specialization: when each agent has a narrower target sub-distribution, it learns an even more specialized policy, which is even better (resp. worse) for problem instances in (resp. out) of the target sub-distribution. This is in contrast with a simple policy ensemble, for which the average agent performance would remain the same regardless of the population size. Additional analyses are made in Appendix E.1, where we show that every agent contributes to the population performance. Overall, this illustrates that Poppy agents have learned complementary policies: though they individually perform worse than the single agent baseline POMO, together they obtain better performance than an ensemble, with every agent contributing to the whole population performance.

### Packing Problems (KP, JSSP)

TasksTo showcase the versatility of Poppy, we evaluate our method on two packing problems: KP and JSSP. In KP, the goal is to maximize the total value of packed items. JSSP can be seen as a 2D packing problem in which the goal is to minimize the schedule makespan under a set of constraints.

SetupFor KP, we employ the same architecture and training as for TSP and CVRP. For JSSP, we use an attention-based actor-critic architecture proposed by Bonnet et al. (2023) and use REINFORCE with a critic as a baseline to train our agents. Furthermore, akin to the other problems, we first train the agent until convergence and then duplicate the heads to make our population (see Appendix E for more details). In both problems, the evaluation budget consists of 1,600 samples per instance distributed over the population.

BaselinesWe evaluate our method on KP against several baselines including the optimal solution based on dynamic programming, a greedy heuristic, and POMO (Kwon et al., 2020). For JSSP, we compare Poppy against the optimal solution obtained by Google OR-Tools (Perron and Furnon, 2019), L2D (Zhang et al., 2020) with greedy rollouts and stochastic sampling (consisting of 8,000 samples

  &  &  \\  &  &  &  \\ Method & Obj. & Gap & Time & Obj. & Gap & Time & Obj. & Gap & Time \\  HGS & 15.56 & \(0.000\%\) & 3D & 17.37 & \(0.000\%\) & 12H & 19.05 & \(0.000\%\) & 16H \\ LKH3 & 15.65 & \(0.53\%\) & 6D & 17.50 & \(0.75\%\) & 19H & 19.22 & \(0.89\%\) & 20H \\  MDAM (greedy) & 16.40 & \(5.38\%\) & 45S\({}^{*}\) & - & - & - & - & - & - \\ MDAM (beam search) & 15.99 & \(2.74\%\) & 53M\({}^{*}\) & - & - & - & - & - & - \\ LIH & 16.03 & \(3.00\%\) & 5H\({}^{*}\) & - & - & - & - & - & - \\ NeuRewriter & 16.10 & \(3.45\%\) & 66M\({}^{*}\) & - & - & - & - & - & - \\ NLNS & 15.99 & \(2.74\%\) & 62M\({}^{*}\) & 18.07 & 4.00\% & 9M\({}^{*}\) & 19.96 & \(4.76\%\) & 12M\({}^{*}\) \\ POMO & 15.87 & \(2.00\%\) & 10S & 17.82 & \(2.55\%\) & 2S & 19.75 & \(3.65\%\) & 3S \\ POMO (32 samples) & 15.77 & \(1.31\%\) & 5M & 17.69 & 1.82\% & 1M & 19.61 & \(2.94\%\) & 1M \\ POMO 32 (ensemble) & 15.78 & \(1.36\%\) & 5M & 17.70 & \(1.87\%\) & 1M & 19.57 & \(2.73\%\) & 1M \\
**Poppy 4** & 15.80 & \(1.54\%\) & 1M & 17.72 & \(2.01\%\) & 10S & 19.61 & \(2.95\%\) & 20S \\
**Poppy 8** & 15.77 & \(1.31\%\) & 2M & 17.68 & \(1.78\%\) & 20S & 19.58 & \(2.75\%\) & 40S \\
**Poppy 32** & **15.73** & **1.06\%** & 5M & **17.63** & **1.47\%** & 1M & **19.50** & **2.33\%** & 1M \\ 

Table 2: CVRP results.

  &  &  \\  &  &  &  \\ Method & Obj. & Gap & Time & Obj. & Gap & Time & Obj. & Gap & Time \\  HGS & 15.56 & \(0.000\%\) & 3D & 17.37 & \(0.000\%\) & 12H & 19.05 & \(0.000\%\) & 16H \\ LKH3 & 15.65 & \(0.53\%\) & 6D & 17.50 & \(0.75\%\) & 19H & 19.22 & \(0.89\%\) & 20H \\  MDAM (greedy) & 16.40 & \(5.38\%\) & 45S\({}^{*}\) & - & - & - & - & - \\ MDAM (beam search) & 15.99 & \(2.74\%\) & 53M\({}^{*}\) & - & - & - & - & - \\ LIH & 16.03 & \(3.00\%\) & 5H\({}^{*}\) & - & - & - & - & - \\ NeuRewriter & 16.10 & \(3.45\%\) & 66M\({}^{*}\) & - & - & - & - & - \\ NLNS & 15.99 & \(2.74\%\) & 62M\({}^{*}\) & 18.07 & 4.00\% & 9M\({}^{*}\) & 19.96 & \(4.76\%\) & 12M\({}^{*}\) \\ POMO & 15.87 & \(2.00\%\) & 10S & 17.82 & \(2.55\%\) & 2S & 19.75 & \(3.65\%\) & 3S \\ POMO (32 samples) & 15.77 & \(1.31\%\) & 5M & 17.69 & 1.82\% & 1M & 19.61 & \(2.94\%\) & 1M \\ POMO 32 (ensemble) & 15.78 & \(1.36\%\) & 5M & 17.70 & \(1.87\%\) & 1M & 19.57 & \(2.73\%\) & 1M \\
**Poppy 4** & 15.80 & \(1.54\%\) & 1M & 17.72 & \(2.01\%\) & 10S & 19.61 & \(2.95\%\) & 20S \\
**Poppy 8** & 15.77 & \(1.31\%\) & 2M & 17.68 & \(1.78\%\) & 20S & 19.58 & \(2.75\%\) & 40S \\
**Poppy 32** & **15.73** & **1.06\%** & 5M & **17.63** & **1.47\%** & 1M & **19.50** & **2.33\%** & 1M \\ 

Table 1: TSP results.

per problem instance) and our own implementation of MDAM using the same network architecture and budget as Poppy.

ResultsTable 3 shows the results for KP (3a) and JSSP (3b). The columns indicate the final performance on the test set, which is the average values of items packed for KP and the average schedule duration for JSSP, the optimality gap and the total runtime of these evaluations. The L2D results were obtained on an Intel Core i9-10940X CPU and a single Nvidia GeForce 2080Ti GPU, and thus, their runtimes are marked with \(*\).

The results from both packing problems demonstrate that Poppy outperforms other RL baselines. For KP, we observe that Poppy leads to improved performance with a population of 16 agents, dividing the optimality gap with respect to POMO sampling and ensemble by a factor of 12 and 42 for the exact same runtime. We emphasize that although these gaps seem small, these differences are still significant: Poppy 16 is strictly better than POMO in 34.30% of the KP instances, and better in 99.95%. For JSSP, we show that Poppy significantly outperforms L2D in both greedy and sampling settings, and closes the optimality gap by 14.1% and 1.8%, respectively. It should be noted that Poppy outperforms L2D sampling despite requiring 5 times fewer samples. Additionally, to further demonstrate the advantages of training a population of agents, we compare Poppy with a single attention-based model (method named _Single_) evaluated with 16 stochastic samples. The results show that given the same evaluation budget, Poppy outperforms the single agent and closes the optimality gap by a further 1%.

## 5 Conclusions

Poppy is a population-based RL method for CO problems. It uses an RL objective that incurs agent specialization with the purpose of maximizing population-level performance. Crucially, Poppy does

Figure 3: Analysis of Poppy on TSP100. **Left:** Starting from POMO clones, training curves of Poppy for three different population sizes (4, 8, 16). **Right:** With the population objective, the average performance gets worse as the population size increases, but the population-level performance improves.

Table 3: Packing problems results.

not rely on handcrafted notions of diversity to enforce specialization. We show that Poppy achieves state-of-the-art performance on four popular NP-hard problems: TSP, CVRP, KP and JSSP.

This work opens the door to several directions for further investigation. Firstly, we have experimented on populations of at most 32 agents; therefore, it is unclear what the consequences of training larger populations are. Whilst even larger populations could reasonably be expected to provide stronger performance, achieving this may not be straightforward. Aside from the increased computational burden, we also hypothesize that the population performance could eventually collapse once no additional specialization niches can be found, leaving agents with null contributions behind. Exploring strategies to scale and prevent such collapses is an interesting direction for future work.

Secondly, our work has built on the current state-of-the-art RL for CO approaches in a single- or few-shot inference setting to demonstrate the remarkable efficacy of a population-based approach. However, there are other paradigms that we could consider. For example, active-search methods allow an increased number of solving attempts per problem and, in principle, such methods for inference-time adaption of the policy could be combined with an initially diverse population to further boost performance. Indeed, we investigate the performance of Poppy with a larger time budget in Appendix D and find that Poppy combined with a simple sampling procedure and no fine-tuning matches, or even surpasses, the state-of-the-art active search approach of Hottung et al. (2022).

Finally, we recall that the motivation behind Poppy was dealing with problems where predicting optimal actions from observations is too difficult to be solved reliably by a single agent. We believe that such settings are not strictly limited to canonical CO problems, and that population-based approaches offer a promising direction for many challenging RL applications. Specifically, Poppy is amenable in settings where (i) there is a distribution of problem instances and (ii) these instances can be attempted multiple times at inference. These features encompass various fields such as code and image generation, theorem proving, and protein design. In this direction, we hope that approaches that alleviate the need for handcrafted behavioral markers whilst still realizing performant diversity akin to Poppy, could broaden the range of applications of population-based RL.