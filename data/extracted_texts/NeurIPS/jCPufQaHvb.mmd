# _Con4m_: Context-aware Consistency Learning Framework for Segmented Time Series Classification

Junru Chen

Zhejiang University

jrchen_cali@zju.edu.cn

&Tianyu Cao

Zhejiang University

ty.cao@zju.edu.cn

&Jing Xu

State Grid Power Supply Co. Ltd.

ltxu1111@gmail.com &Jiahe Li

Zhejiang University

jiaheli@zju.edu.cn &Zhilong Chen

Zhejiang University

zhilongchen@zju.edu.cn

&Tao Xiao

State Grid Power Supply Co. Ltd.

xtxjtu@163.com &Yang Yang

Zhejiang University

yangya@zju.edu.cn

Corresponding author.

###### Abstract

Time Series Classification (TSC) encompasses two settings: classifying entire sequences or classifying segmented subsequences. The raw time series for segmented TSC usually contain **M**ultiple classes with **V**arying **D**uration of each class (_MVD_). Therefore, the characteristics of _MVD_ pose unique challenges for segmented TSC, yet have been largely overlooked by existing works. Specifically, there exists a natural temporal dependency between consecutive instances (segments) to be classified within _MVD_. However, mainstream TSC models rely on the assumption of independent and identically distributed (_i.i.d._), focusing on independently modeling each segment. Additionally, annotators with varying expertise may provide inconsistent boundary labels, leading to unstable performance of noise-free TSC models. To address these challenges, we first formally demonstrate that valuable contextual information enhances the discriminative power of classification instances. Leveraging the contextual priors of _MVD_ at both the data and label levels, we propose a novel consistency learning framework _Con4m_, which effectively utilizes contextual information more conducive to discriminating consecutive segments in segmented TSC tasks, while harmonizing inconsistent boundary labels for training. Extensive experiments across multiple datasets validate the effectiveness of _Con4m_ in handling segmented TSC tasks on _MVD_. The source code is available at https://github.com/MrNobodyCali/Con4m.

## 1 Introduction

Time Series Classification (TSC) is one of the most challenging problems in the field of machine learning. TSC aims to assign labels to a series of temporally ordered data points. These points either form a complete sequence or are subsequences (segments) resulting from the segmentation of a long time series. Existing works  largely focus on the assumption of independent and identically distributed (_i.i.d._), in which case each sequence or segment is regarded as an independent instance to be classified, not differentiating between these two settings. In fact, for many practical applications, the raw time series before segmentation for segmented TSC tasks contain **M**ultiple classes with

**V**arying **D**uration of each class (_MVD_). For example, in the healthcare domain, the brain signals of epileptic patients often record over several days, encompassing multiple seizure onsets, each with varying durations and intervals. In the field of activity recognition, sensors continuously record users' behavior data, including walking, riding, and running, among other activities, each with varying durations. Therefore, the characteristics of the raw _MVD_ lead to the uniqueness of segmented TSC tasks. Given this, our research concentrates on effectively modeling segmented TSC tasks based on _MVD_, presenting distinctive challenges.

**(1) Leveraging contextual information.** In contrast to TSC tasks for complete sequences, in which classified sequences are relatively independent, there exist natural temporal dependencies between consecutive classified segments for segmented TSC. We take the seizure detection task as an example, in which given the brain signals of epileptic patients, the model should identify whether a segment includes seizure waves or not. As illustrated in Figure 1(a), given the sustained nature of seizure onsets, the model predictions for consecutive segments should exhibit coherence, with seizure and normal predictions appearing in continuous and concentrated patterns. However, mainstream TSC models  focus on the _i.i.d._ assumption and model the internal context within each segment to be classified, largely overlooking the dependencies between consecutive segments.

In the domain of video analysis, works in temporal action segmentation (TAS)  have modeled the temporal dependency between different video frames and made frame-wise predictions. However, unlike the I3D features  used as input in these works, time series lack a unified pretrained model for feature extraction, and the dependency between segments is more variable and ambiguous. Furthermore, TAS works focus on modeling the dependency of instances from a data perspective, without explicitly leveraging contextual label information. Therefore, how to leverage contextual information in segmented TSC tasks to make more reasonable classifications is crucial and challenging.

**(2) Inconsistent boundary labels.** In domains with precious labels, different annotators collaboratively contribute annotations. The raw annotations of _MVD_ typically include the start and end times for each class. However, due to inherent ambiguity and a lack of unified quantification standards, for _MVD_, the boundaries between states are not clearly defined, or the transitional state itself represent a mixed state. Consequently, behind inconsistent labels, there is no artificially defined true label. Therefore, our work aims to harmonize this inconsistency as much as possible to reduce the instability

Figure 1: (a) Reasonable model predictions exhibit coherence across consecutive segments rather than repeated interruptions. (b) In the healthcare domain, different physicians have varying annotations regarding the start and end times of seizure waves. (c) Based on the proximity to the boundary, we divide each class sequence into 5 levels, from which an equal number of segments are sampled. A one-layer MLP is trained on the segments from each level respectively for the same number of epochs. (d) We visualize the predicted probability of the trained MLP for each level. We observe that as the segments approach the boundaries, the model finds it increasingly challenging to make correct classifications, resulting in more extreme wrong predictions. This strongly underscores the significance of handling boundary segments.

of model training and enhance its performance. Returning to Figure 1(b), in the seizure detection task, owing to the natural fuzzy transition from seizure onset to completely normal, different physicians have varying experiences regarding when seizure waves terminate.

Furthermore, inconsistent boundary labeling causes that boundary segments with similar patterns may have opposite labels, leading to unstable model training. To validate the detrimental impact, as shown in Figure 1(c), we divide each class sequence in the seizure detection task into 5 levels, where higher levels indicate proximity to the boundary. We then sample an equal number of balanced binary segments for each level. Subsequently, a one-layer MLP is trained for the same number of epochs on the segments from each level respectively. Figure 1(d) visualizes the results after training. We observe that as the level increases (closer to the boundary), the model's accuracy steadily decreases, and erroneous predictions become more extreme. The results highlight the significant detrimental impact of inconsistent boundary labels on noise-free model performance.

Noisy label learning (NLL)  aims to learn robust models from data containing corrupted labels. While the inconsistent labels in _MVD_ are not intentionally corrupted but rather stem from implicit discrepancies due to experiential differences, NLL remains the most relevant approach to address such discrepancies. To the best of our knowledge, Scale-teaching  and SREA  are the only NLL works specifically designed for time series and are thus the most relevant to our work. However, they also face the issue of overlooking contextual dependencies across consecutive time segments, posing the challenge of handling inconsistent boundary labels using context during training.

To overcome the challenges above, we propose _Con4m_-a label **C**onsistency learning framework, which leverages effective **C**ontextual information, achieving **C**oherent predictions and **C**ontinuous representations for **s**egmented TSC tasks, while harmonizing inconsistent boundary labels for training. Specifically, we first formally demonstrate that valuable contextual information enhances the discriminative power of classification instances. Based on the insights, by incorporating prior knowledge of data locality and label coherence, we guide and constrain the model to focus on contextual information more conducive to discriminating consecutive segments in segmented TSC tasks. Meanwhile, leveraging model predictions that thoroughly encompass contextual information, _Con4m_ progressively changes the training labels in an adaptive manner to harmonize inconsistent labels across consecutive segments. This leads to a more robust model. Our contributions are summarized as follows:

**(1)** We are the first to propose a practical consistency learning framework _Con4m_ for the segmented TSC based on the raw _MVD_. **(2)** By comprehensively integrating prior knowledge from the data and label perspectives, we guide the model to focus on effective contextual information. Based on context-aware predictions, a progressive harmonization approach for handling inconsistent training labels is designed to yield a more robust model. **(3)** Extensive experiments on three public and one private _MVD_ datasets demonstrate the superior performance of _Con4m_. The _Con4m_'s ability to harmonize inconsistent labels is further verified by the label substitution experiment and case study.

## 2 Theoretical Analysis

In this section, we aim to formally demonstrate the benefit of contextual information for classification tasks, and to establish the existence of an upper bound for this benefit. Consequently, by introducing prior knowledge, we can guide the model to focus on valuable contextual information more conducive to improving the benefit for segmented TSC tasks.

Assuming that the random variables of the instances to be classified and the corresponding labels are denoted as \(_{t}\) and \(_{t}\). \(_{t}\) represents the contextual instance set introduced for \(_{t}\). \(x_{A_{k}}\) denotes the random variable for the contextual instance set. Mutual information measures the correlation between two random variables. In a classification task, a higher correlation between instances and labels indicates that the instances are more easily distinguishable by the labels. This benefits the classification task, making it more readily addressable. Therefore, from an information-theoretic perspective, we elucidate the benefit of contextual information through the following theorem.

**Theorem 2.1**.: _The more the introduced contextual instance set enhance the discriminative power of the target instance, the greater the benefit for the classification task._

Proof.: Firstly, we establish that the introduction of contextual information does not compromise classification tasks, _i.e._, it does not diminish the correlation between instances and labels.

\[(_{t};_{t},_{A_{k}})=(_{t };_{A_{k}}|_{t})+(_{t};_{t}) (_{t};_{t}).\] (1)The inequality holds due to the non-negativity of conditional mutual information.

According to (1), the increase in \((_{i};_{_{t}}|_{t})\) determines the extent to which the introduction of contextual information can be beneficial for classification tasks. Expanding \((_{t};_{_{t}}|_{t})\), we have:

\[(_{t};_{_{t}}|_{t}) =_{_{t}}p(_{t})_{_{ _{t}}}_{_{t}}p(_{t},_{_{t}}|_{t})_{t},_{_{t}}|_{t})}{p( _{t}|_{t})p(_{_{t}}|_{t})}\] \[=_{_{t}}p(_{t})_{_{ _{t}}}_{_{t}}p(_{t}|_{t},_{ _{t}})p(_{_{t}}|_{t})_{t}| _{t},_{_{t}})}{p(_{t}|_{t})}\] \[=_{_{t}}p(_{t})_{_{ _{t}}}p(_{_{t}}|_{t})D_{}(p(_{ t}|_{t},_{_{t}})\|p(_{t}|_{t})).\]

Given a fixed instance \(_{t}\) and the inherent distribution \(p(_{t}|_{t})\) of the data, the KL divergence is a convex function for \(_{_{t}}\) that attains its minimum at \(p(_{t}|_{t},_{_{t}})=p(_{t}| _{t})\). As \(p(_{t}|_{t},_{_{t}})\) approaches the boundary of the probability space, where the predictive probability of one class approaches \(1\) and the rest approach \(0\), the value of KL divergence increases. A stronger discriminative power regarding \(_{t}\) implies less uncertainty regarding \(_{t}\), which is equivalent to approaching the boundary of the probability space.

Due to the convexity of the KL divergence and the boundedness of \(p(_{t}|_{t},_{_{t}})\), there exists a contextual instance set in the data that maximizes \(D_{}(p(_{t}|_{t},_{_{t}})\|p( _{t}|_{t}))\). We denote the instance set as \(_{t}^{*}\) and the maximum value of KL divergence as \(D_{t}^{*}\). Besides, we note that \(_{_{_{t}}}p(_{_{t}}|_{t})=1\). Hence, we can obtain the upper bound for the information gain \((_{t};_{_{t}}|_{t})_{ _{t}}p(_{t})_{_{_{t}}}p(_{ _{t}}|_{t})D_{t}^{*}_{_{t}}p(_{t} )D_{t}^{*}\). The convexity of the KL divergence also implies monotonicity, indicating that as \(A_{t}\) approaches \(A_{t}^{*}\), the KL divergence increases, leading to a greater information gain for the classification task. 

According to Theorem 2.1, valuable contextual information enhances the discriminative power of the instances. While the optimal instance set \(A_{t}^{*}\) is challenging to directly obtain or optimize, focusing the model on contextual instances more likely to be included in \(A_{t}^{*}\) is beneficial for enhancing the performance of the classification task. Furthermore, \(_{_{t}}\) not only contains information at the data level but also encompasses information at the label level (which can be replaced with \(_{_{t}}\)). Therefore, we can guide the model to focus on contextual information more conducive to segmented TSC tasks by simultaneously introducing prior knowledge from both the data and label perspectives.

## 3 The _Con4m_ Method

In this section, we introduce the details of _Con4m_. Based on the insights of Theorem 2.1, we introduce contextual prior knowledge of data locality (Sec. 3.1) and label coherence (Sec. 3.2) to guide the model to focus on contextual information more conducive to discriminating consecutive segments in segmented TSC tasks. In Sec. 3.3, inspired by the idea of noisy label learning, we propose a label harmonization framework to achieve a more robust model. Before delving into the details of _Con4m_, we provide the formal definition of the segmented TSC task in our work.

**Definition 3.1**.: _Given a time interval comprising of \(T\) consecutive time points and labels, denoted as \((X,Y)=\{(X_{1},Y_{1}),(X_{2},Y_{2}),,(X_{T},Y_{T})\}\), a \(w\)-length sliding window with stride length \(r\) is employed for segmentation. \((X,Y)\) is partitioned into \(L\) time segments, represented as \((x,y)=\{(x_{i},y_{i})\ =(\{X_{(i-1) r+1},,X_{(i-1) r+w}\},(\{Y_{(i-1)  r+1},,Y_{(i-1) r+w}\}))|i\ =1,,L\}\). The model is tasked with predicting segmented labels \(y_{i}\) for each time segment \(x_{i}\)._

### Continuous Contextual Representation Encoder

Local continuity is an inherent attribute of _MVD_, meaning each class should be locally continuous and only change at its actual boundary. Smoothing with a Gaussian kernel [18; 16; 66] promotes the continuity of representations of time segments in a local temporal window. This not only helps the model make similar predictions of consecutive segments within the same class but also aligns with the gradual nature of class transitions. Furthermore, for graph neural networks based on the homophily assumption, aggregating neighbor information belonging to the same class can improve the discriminative power of the target instance . Therefore, we introduce the Gaussian prior to guide the model to focus on contextual instances \(_{t}\) proximate to the target instance.

Vanilla self-attention  with point-wise attention computations often fail to obtain continuous representations after aggregation. Therefore, we use the Gaussian kernel \((x,y|)\) as prior weights to aggregate neighbors to obtain smoother representations. Since the neighbors of boundary segments may belong to different classes, we allow each segment to learn its own scale parameter \(\). Formally, as Figure 2(a) shows, the two-branch **Con-Attention** in the \(l\)-th layer is:

\[Q,K,V_{s},V_{g},= c^{l-1}W_{Q}^{l},c^{l-1}W_{K}^{l},c^{l-1}W_{V_{s}}^{l},c^{l-1}W_{V_{g }}^{l},c^{l-1}W_{}^{l},\] \[S^{l}=(}{}), G ^{l}=([_{i}}( -}{2_{i}^{2}})]_{i,j\{1,,L\}}),\] \[z_{s}^{l}=S^{l}V_{s}, z_{g}^{l}=G^{l}V_{g}, z^{l}= (z_{s}^{l},z_{g}^{l}),\]

where \(L\) is the number of consecutive segments, \(d\) is the dimension of hidden representations, \(c^{l-1}^{L d}\) is the output representations of the \(l-1\)-th layer, and \(W_{*}^{l}^{d d}\) are all learnable matrices. \(()\) refers to row normalization by index \(i\). To distinguish between two computational branches, we use \(g/G\) to represent the branch based on Gaussian prior, and \(s/S\) to represent the branch based on self-attention. \(S^{l}\) and \(G^{l}\) are the aggregation weights. We use the conventional attention mechanism  to adaptively fuse \(z_{s}^{l}\) and \(z_{g}^{l}\). Finally, as illustrated in Figure 2(a), by stacking the multi-head version of Con-Attention layers, we construct Con-Transformer, which serves as the backbone of the continuous encoder of _Con4m_ to obtain final representations \(c\). We employ learnable absolute positional encoding (APE)  for the input representations.

Figure 2: Overview of _Con4m_. (a) Overview of continuous contextual representation encoder in _Con4m_. The leftmost part shows the details of Con-Attention. The right part of the figure shows the architecture of Con-Transformer and the whole encoder of _Con4m_. (b) Overview of context-aware coherent class prediction and consistent label training framework in _Con4m_. The right part describes the neighbor class consistency discrimination task and the prediction behavior constraint. The leftmost part presents the training and inference details for label harmonization.

### Context-aware Coherent Class Prediction

In the segmented TSC task of _MVD_, consecutive time segments not only provide contextual information at the data level but also possess their own class information. As depicted in Figure 1(a), considering the persistence of each class and the gradual nature of class transitions, the model's predictions should exhibit more coherence and concentration, rather than being interspersed. Therefore, we integrate and constrain the model's predictions from both the individual and holistic perspectives to achieve more coherent predictions.

Neighbor Class Consistency Discrimination.In graphs, label propagation algorithms [30; 33] are often utilized to refine and smooth the predictions of neighbor instances, thereby enhancing their discrimination. Drawing inspiration from this, by weightedly aggregating predictions from similar time segments, the model can focus on contexts \(_{t}\) more likely to belong to the same class as the target segment. Although there is no explicit graph structure between time segments, we can train a discriminator to determine whether two segments belong to the same class. The model then aggregates the contextual class predictions based on the discriminator's outputs, thus making more robust predictions. As the right part of Figure 2(b) shows, we formalize this process as follows:

\[=([_{2}(c_{i}\|c_{j}) ]_{i,j\{1,,L\}}),=(_{1}(c)),=_{:,:,1},\]

where \(^{L L 2}\) is the probability of whether two segments in the same time interval belong to the same class and \((\|)\) denotes tensor concatenation. \(\) represents the model's independent prediction for a segment, while \(\) denotes the context-aware prediction that incorporates the results from neighboring segments. We then define the two training losses as \(_{1}=(,y)\) and \(_{2}=(,)\), where \(=[_{_{1}=_{j}}]_{i,j\{1, ,L\}}\). Given that \(_{1}\) and \(_{2}\) are of the same magnitude, we equally sum them as the final loss.

Prediction Behavior Constraint.Unlike graphs, there exists a holistic temporal relationship between consecutive time segments. Therefore, we should further constrain the overall predictive behavior along the time axis. For _MVD_, as Figure 1(a) shows, within a suitably chosen time interval, consecutive segments almost span at most two classes. Therefore, we ensure the monotonicity of predictions across consecutive segments through hard constraints, thereby utilizing contextual label information \(_{_{t}}\) to integrate and refine predictions across these segments.

As shown in the middle part of Figure 2(b), for each class in the predictions, there are only four prediction behaviors for consecutive segments, namely _high confidence_, _low confidence_, _confidence decreasing_, and _confidence increasing_. To constrain the behavior, we use function fitting to integrate \(\). Considering the wide applicability, we opt for the hyperbolic tangent function (_i.e._, Tanh) as our basis. Formally, we introduce four tunable parameters to exactly fit the monotonicity as:

\[=(x|a,k,b,h)=a(k(x+b))+h,\]

where parameter \(a\) constrains the range of the function's values, \(k\) controls the slope of the transition of the function, \(b\) and \(h\) adjust the symmetry center of the function, and \(x\) is the given free vector in the x-coordinate. We use the MSE loss to fit the contextual predictions \(\) as \(_{3}=\|(x|a,k,b,h)-\|^{2}\). It deserves to be emphasized that \(\) in the process has no gradient and therefore does not affect the parameters of the encoder. Please see Appendix B for more fitting details.

After function fitting, we obtain independent predictions \(\) for each segment and constrained predictions \(\) that leverage contextual label information. For the inference stage, we use the average of them as the final coherent predictions, _i.e._, \(=(+)/2\).

### Label Consistency Training Framework

Due to inherent ambiguity, the annotation of _MVD_ often lacks quantitative criteria, resulting in experiential differences across individuals. Such discrepancies are detrimental to models and we propose a training framework to enable _Con4m_ to adaptively harmonize inconsistent labels.

Learning from easy to hard.We are based on the fact that although people may have differences in the fuzzy transitions between classes, they tend to reach an agreement on the most significant core part of each class. In other words, the empirical differences become more apparent when approaching the transitions. Therefore, we adopt curriculum learning techniques to help the model learn instancesfrom the easy (core) to the hard (transition) part. Formally (see the diagram in Figure 1(b)), for a continuous \(K\)-length class, we divide it into \(N_{l}=5\) equally sized levels as follows:

\[((N_{l}-1)},(N_{l}+1)} );;[1,}) ((2N_{l}-1)},K].\] (2)

Then we sample the same number of time intervals from each level. The higher the level, the more apparent the inconsistency. Therefore, as the left part of Figure 2(b) shows, during the training stage, _Con4m_ learns the time intervals in order from low to high levels, with a lag gap of \(E_{g}=5\) epochs.

Harmonizing inconsistent labels.Inspired by the idea of noisy label learning, we gradually change the raw labels to harmonize the inconsistency. The model preferentially changes the labels of the core segments that are easier to reach a consensus, which can avoid overfitting of uncertain labels. Moreover, the model will consider both the independent and constrained predictions to robustly change inconsistent labels. Specifically, given the initial label \(y_{0}\), we update the labels \(y_{e}= p_{e}\) for the \(e\)-th epoch, where \(p_{e}\) is obtained as follows:

\[_{e}^{5}=_{e}[_{e-m}]_{m\{0, ,4\}},_{e}^{5}=_{e}[_{e-m}]_ {m\{0,,4\}},\] \[p_{e}=(1-)\,y_{0}+((1-) _{e}^{5}+_{e}^{5}),\]

where \(_{e}=([((e-m)/2)]_{m\{0,,4\}})\) is the exponentially averaged weight vector to aggregate the predictions of the latest \(5\) epochs to achieve a more robust label update. \(_{e-m}\) and \(_{e-m}\) are the independent and constrained predictions in the \(e-m\)-th epoch respectively and \(\) denotes the dot product. The dynamic weighting factor, \(\), is used to adjust the degree of label update. As the left part of Figure 2(b) shows, \(\) linearly increases from \(0\) to \(1\) with \(E_{}\) epochs, gradually weakening the influence of the original labels. Besides, in the initial training stage, the model tends to improve independent predictions. As the accuracy of independent predictions increases, the model assigns a greater weight to the constrained predictions. See the hyperparameter analysis for \(E_{}\) in Appendix C.

## 4 Experiment

### Experimental Setup

Datasets.In this work, we use three public [31; 7; 37] and one private _MVD_ data to measure the performance of models. Specifically, the Tufts fNIRS to Mental Workload  data (**fNIRS**) contains brain activity recordings from adult humans performing controlled cognitive workload tasks. The **HHAR** (Heterogeneity Human Activity Recognition) dataset  captures sensor data from multiple smart devices to explore the impact of device heterogeneity on human activity recognition. The SleepEDF  data (**Sleep**) contains PolySomnoGraphic sleep records for subjects over a whole night. The private **SEEG** data records brain signals indicative of suspected pathological tissue within the brain of epileptic patients. More detailed descriptions can be found in Table 1 and Appendix D.

Label disturbance.We introduce a novel disturbance method to the raw labels \(Y\) of the public datasets to simulate scenarios where labels are inconsistent. Specifically, we first look for the boundary points between different classes in a complete long _MVD_ data. Then, we randomly determine with a \(0.5\) probability whether each boundary point should move forward or backward. Finally, we randomly select a new boundary point position from \(r\%\) of the length of the class in the direction of the boundary movement. In this way, we can interfere with the boundaries and simulate label inconsistency. Meanwhile, a larger value of \(r\%\) indicates a higher degree of label inconsistency. For SEEG dataset, inconsistent labels already exist in the raw data and we do not disturb it.

Baselines.We compare _Con4m_ with state-of-art models from various domains, including two noisy label learning (NLL) models for time series classification (TSC): SREA  and Scale-teaching 

    & **Sample** & **\# of** & **\# of** & **Subjects** & **Groups** & **Cross** & **Total** & **Interval** & **Window** & **Slide** & **Total** \\  & **Frequency** & **Features** & **Classes** & & **Validation** & **Intervals** & **Length** & **Length** & **Length** & **Segments** \\  fNIRS & 5.2Hz & 8 & 2 & 68 & 4 & 12 & 4,080 & 38.46s & 4.81s & 0.96s & 146,880 \\ HHAR & 50Hz & 6 & 6 & 9 & 3 & 6 & 5,400 & 60s & 4s & 2s & 156,600 \\ Sleep & 100Hz & 2 & 5 & 154 & 3 & 6 & 6,000 & 40s & 2.5s & 1.25s & 186,000 \\ SEEG & 250Hz & 1 & 2 & 8 & 4 & 3 & 8,000 & 16s & 1s & 0.5s & 248,000 \\   

Table 1: Overview of _MVD_ datasets used in this work.

(Scale-T), three image classification models with noisy labels: SIGUA , UNICON  and Sel-CL , three supervised TSC models: MiniRocket , TimesNet  and PatchTST , and three temporal action segmentation (TAS) models: MS-TCN2 , ASFormer  and DiffAct . See more detailed descriptions of the baselines in Appendix E.

**Implementation details.** We use cross-validation  to evaluate the model's generalization ability by partitioning the subjects in the data into non-overlapping subsets for training and testing. As shown in Table 1, for INIRS and SEEG, we divide the subjects into \(4\) groups and follow the \(2\) training-\(1\) validation-\(1\) testing (\(2\)-\(1\)-\(1\)) setting to conduct experiments. We divide the HHAR and Sleep datasets into \(3\) groups and follow the \(1\)-\(1\)-\(1\) experimental setting. Notice that SEEG data is derived from real clinical datasets and annotated by multiple experts, resulting in naturally inconsistent labels. We employ a voting mechanism which brings annotators together to collectively decide the boundaries to minimize discrepancies in test labels. Considering the high cost of this approach, we do not apply it to the training and validation sets. Therefore, we leave the test group aside and only change the training and validation groups to conduct cross-validation. Finally, we only report the mean values of cross-validation results in the main context. See more details and the full results in Appendix G.

### Label Disturbance Experiment

The average results over all cross-validation experiments are presented in Table 2. Overall, _Con4m_ outperforms almost all baselines across all datasets and all disturbance ratios.

**Results of different methods.** For INIRS, TAS models achieve competitive performance compared to _Con4m_, demonstrating the advantage in modeling contextual data dependency among segments. For HHAR, Sleep and SEEG data with more ambiguous boundaries, the performance of TAS models deteriorates significantly, and TSC and NLL models slightly outperform TAS models. Benefiting from multi-scale modeling, Scale-T exhibits significantly better performance on the Sleep and SEEG data compared to SREA. Nevertheless, _Con4m_ that fully consider contextual information demonstrate a notable performance improvement (HHAR-0%: 3.24%; Sleep-0%: \(7.15\%\); SEEG: \(6.45\%\)) in more complex and ambiguous data.

**Results of different \(r\%\).** NLL methods demonstrate close performance degradation as \(r\%\) increases from \(0\%\) to \(20\%\) compared with _Con4m_. However, with a higher ratio from \(20\%\) to \(40\%\), SIGUA, UNICON, Sel-CL, SREA, and Scale-T show averaged \(3.01\%\), \(5.23\%\), \(1.92\%\), \(3.34\%\), and \(3.22\%\) decrease across fNIRS and Sleep data, while _Con4m_ shows \(2.37\%\) degradation. For TSC models, non-deep learning-based MiniRocket shows a more robust performance compared to other TSC models. The performance of PatchTST on fNIRS data exhibits significant instability, possibly due to its tendency to overfit inconsistent labels too quickly. DiffAct in TAS models shows the most sensitive performance to boundary perturbations from \(0\%\) to \(40\%\) across three public data (\(13.23\%\) decrease). The stable performance of _Con4m_ indicates that our proposed training framework can effectively harmonize inconsistent labels.

    &  &  &  & SEEG \\   & \(0\%\) & \(20\%\) & \(40\%\) & \(0\%\) & \(20\%\) & \(40\%\) & \(0\%\) & \(20\%\) & \(40\%\) & raw \\   & MS-TCN2  & 71.48 & 70.99 & 69.40 & 69.79 & 66.72 & 62.29 & 60.07 & 59.03 & 56.17 & 61.88 \\  & ASFormer  & **71.69** & 70.75 & 69.18 & 62.52 & 60.92 & 60.77 & 59.09 & 55.52 & 53.89 & 56.71 \\  & DiffAct  & 71.15 & 69.72 & 65.45 & 56.76 & 53.86 & 50.63 & 49.12 & _43.32_ & _38.86_ & 60.62 \\   & MiniRocket  & 61.28 & 60.41 & 57.87 & 70.34 & 63.32 & 59.25 & 62.00 & 61.75 & 58.38 & 62.39 \\  & TimesNet  & 67.47 & 65.39 & 63.45 & 72.07 & 70.19 & 66.76 & 59.50 & 57.72 & 55.73 & _50.99_ \\  & PatchTST  & _51.79_ & _55.38_ & _52.67_ & _52.00_ & _45.46_ & _45.69_ & 58.40 & 56.16 & 53.05 & 58.45 \\   & SIGUA  & 67.37 & 65.24 & 63.47 & 68.94 & 68.47 & 67.60 & 54.28 & 53.07 & 51.32 & 53.19 \\  & UNICON  & 61.15 & 60.45 & 57.35 & 62.26 & 61.63 & 58.34 & 62.26 & 61.63 & 58.34 & 60.53 \\  & Sel-CL  & 63.86 & 62.45 & 61.75 & 73.00 & 72.28 & 72.81 & 63.48 & 63.45 & 61.72 & 60.50 \\   & SREA  & 70.10 & 69.65 & 69.40 & 68.64 & 66.02 & 65.67 & _48.81_ & 48.80 & 45.72 & 55.21 \\  & Scale-T  & 70.40 & 68.06 & 66.51 & 77.77 & 76.71 & **75.97** & 63.21 & 63.40 & 60.77 & 67.64 \\   & _Con4m_ & 71.28 & **71.27** & **70.04** & **80.29** & **78.59** & 75.52 & **68.02** & **66.31** & **64.31** & **72.00** \\   

Table 2: Comparison with baseline methods in the testing \(F_{1}\) score (%) on three datasets. The **best results** are in bold and we underline the second best results. The _worst results_ are denoted in italics.

**Results of symmetric disturbance.** We also corrupt the labels with symmetric disturbance based on segmented labels \(y\) rather than raw _MVD_ labels, which is commonly employed in the NLL works [62; 43; 32] of the image classification domain. As shown in Figure 3(a), compared to our novel boundary disturbance, _Con4m_ exhibits stronger robustness to symmetric disturbance. Even with the \(20\%\) disturbance ratio, _Con4m_ treats it as a form of data augmentation, resulting in improved performance. This indicates that overcoming more challenging boundary disturbance aligns better with the nature of time series data.

### Label Substitution Experiment

Since ambiguous boundaries are inherent to SEEG data and the majority voting procedure is costly, we limit this procedure to only one high-quality testing group in the label disturbance experiment. Besides, on the SEEG data, _Con4m_ modifies approximately \(10\%\) of the training labels, which is a significant proportion. Therefore, it is necessary to further evaluate the effectiveness of our label harmonization process on SEEG data. Specifically, we train the TSC baselines based on the harmonized labels generated by _Con4m_ and observe to what extent the performance of TSC models is improved. As shown in Figure 3(b), PatchTST and TimesNet, employing deep learning architectures, are more susceptible to label inconsistency, so they obtain more significant performance improvement (\(4.11\%\) and \(7.53\%\)). Unlike modified PatchTST that considers the contextual data information across consecutive segments, TimesNet only focuses on the independent segments, thus having a more dramatic improvement. In contrast, MiniRocket achieves only a \(1.68\%\) increase, indicating that MiniRocket is more robust with a non-deep learning-based simple random feature mapping.

### Ablation Experiment

We introduce two types of model variations. **(1) Preserve only one module.** We preserve only the Con-Transformer (Con-T), Coherent Prediction (Coh-P), or Curriculum Learning (Cur-L) module separately. **(2) Remove only one component.** In addition to removing the above three modules, we also remove the function fitting component (-Fit) and \(\) (\(E_{}=0\)) to verify the necessity of prediction behavior constraint and progressively updating labels.

As shown in Table 3, when keeping one module, +Coh-P achieves the best performance with an averaged \(2.78\%\) decrease in \(F_{1}\) score, indicating that introducing the contextual label information are most effective for _MVD_. The utility of each module varies across datasets. For example, for Sleep data, the Con-T contributes more to performance improvement compared to the Cur-L module, while the opposite phenomenon is observed for SEEG data. As for removing one component, even when we only remove the Tanh function fitting, the \(F_{1}\) score significantly decreases \(1.72\%\) on average. On

    &  &  \\   & & + Con-T & + Coh-P & + Cur-L & - & Con-T & - & - Coh-P & - Cur-L & - Fit & - & \(\) &  \\ 
**Dataset** & \(r_{1}^{}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) & Acc. & \(F_{1}\) \\  Sleep & 20 & 65.97 & 65.05 & 65.76 & 65.10 & 65.31 & 64.76 & 56.53 & 65.84 & 65.07 & 65.85 & 65.43 & 66.06 & 65.28 & 62.02 & 59.97 & **66.61** & **66.31** \\  Sleep & - & 63.94 & 62.67 & 64.42 & 62.76 & 63.69 & 62.23 & 64.44 & 63.05 & 64.23 & 63.03 & 64.89 & 63.07 & 64.69 & 63.32 & 61.93 & 57.98 & **65.34** & **64.31** \\  SEEG & - & 71.68 & 67.85 & 71.69 & 69.04 & 71.32 & 67.22 & 73.85 & 70.59 & 72.41 & 68.26 & 74.17 & 71.18 & 73.47 & 70.63 & 70.70 & _66.04_ & **74.60** & **72.00** \\   

Table 3: Comparison with model ablations in the \(F_{1}\) score (%) in inconsistent scenarios. The **best results** are in bold and we underline the second best results. The _worst results_ are denoted in italics.

Figure 3: Comparison results of symmetric disturbance and label substitution experiments.

the Sleep-\(20\%\) and SEEG data, the drop caused by -Fit is more significant than that caused by some other modules. Moreover, the model variation -\(\) achieves the worst results (\(9.23\%\) decrease in \(F_{1}\)). The results imply that during early training stages, the model tends to learn the consistent parts of the raw labels. Premature use of unreliable predicted labels as subsequent training supervision signals leads to model poisoning and error accumulation.

### Case Study

We present a case study to provide a specific example that illustrates how _Con4m_ works for _MVD_ in Figure 4. We show comparative visualization results for the predictions in a continuous time interval in the SEEG testing set. In SEEG data, we assign the label of normal segments as \(0\) and that of seizures as \(1\). As the figure shows, _Con4m_ demonstrates a more coherent narrative by constraining the prediction behavior and aligning with the contextual data information. In contrast, Scale-T, Sel-CL and MiniRocket exhibit noticeably interrupted and inconsistent predictions. MS-TCN2 fails to identify normal segments. More impressively, _Con4m_ accurately identifies the consistent boundary within the time interval spanning across two classes. We also utilize the C-score proposed by the ClaSP model  to assess the segmentation capability of models. The C-score reflects the ability of models to recognize segmentation boundaries by measuring the trade-off between precision (correctly identified change points) and recall (finding all true change points), ensuring the model captures meaningful transitions without over-segmenting or missing important splits. We compute the scores for each model across three sets of experiments on SEEG data and take the average. Notably, _Con4m_ outperforms the other models significantly, while MS-TCN2, specifically designed for segmentation tasks, also achieves impressive scores. This verifies that the label consistency framework can harmonize the boundaries more effectively. Refer to Appendix H for more cases.

## 5 Conclusion and Discussion

In this work, we focus on the raw time series _MVD_ for segmented time series classification (TSC) tasks, demonstrating unique challenges that are overlooked by existing mainstream TSC models. We first formally demonstrate that valuable contextual information enhances the discriminative power of classification instances. Based on the insights, we introduce contextual prior knowledge of data locality and label coherence to guide the model to focus on contextual information more conducive to discriminating consecutive segments in segmented TSC tasks. Leveraging effective contextual information, a label consistency learning framework _Con4m_ is proposed to progressively harmonize inconsistent labels during training. Extensive experiments validate the superior performance achieved by _Con4m_ and highlight the effectiveness of the proposed consistent label training framework. Our work still has some limitations. We have solely focused on analyzing and designing end-to-end supervised models. Further exploration of large-scale models would be challenging yet intriguing. _Con4m_ is a combination of segmentation and classification, both of which are fully supervised. Exploring its application in unsupervised segmentation tasks is worthwhile. When faced with more diverse label behaviors, the function fitting module needs to engage in more selection and design of basis functions. Nevertheless, our work brings new insights to the TSC domain, re-emphasizing the importance of the inherent temporal dependence of time series.

Figure 4: Case study for a continuous time interval in SEEG testing set. The C-score, introduced by the ClaSP model , assessing the ability of models to recognize segmentation boundaries by measuring the trade-off between precision (correctly identified change points) and recall (finding all true change points).