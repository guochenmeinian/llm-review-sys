# Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms

Dimitri Meunier

Gatsby Computational Neuroscience Unit

University College London

dimitri.meunier.21@ucl.ac.uk

Equal Contribution.

Zikai Shen

Department of Statistical Science

University College London

zikai.shen.22@ucl.ac.uk

Mattes Mollenhauer

Mernatrix Momentum

mattes.mollenhauer@merantix-momentum.com

Arthur Gretton

Gatsby Computational Neuroscience Unit

University College London

arthur.gretton@gmail.com

&Zhu Li

Department of Mathematics

Imperial College London

zli12@ic.ac.uk

###### Abstract

We study theoretical properties of a broad class of regularized algorithms with vector-valued output. These _spectral algorithms_ include kernel ridge regression, kernel principal component regression and various implementations of gradient descent. Our contributions are twofold. First, we rigorously confirm the so-called _saturation effect_ for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level. Second, we present an upper bound on the finite sample risk for general vector-valued spectral algorithms, applicable to both well-specified and misspecified scenarios (where the true regression function lies outside of the hypothesis space), and show that this bound is minimax optimal in various regimes. All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications.

## 1 Introduction

We investigate a fundamental topic in modern machine learning--the behavior and efficiency of learning algorithms for regression in high-dimensional and potentially infinite-dimensional output spaces \(\). Given two random variables \(X\) and \(Y\), we seek to empirically minimize the squared expected risk

\[(F):=[\|Y-F(X)\|_{}^{2}]\] (1)

over functions \(F\) in a _reproducing kernel Hilbert space_ consisting of vector-valued functions from a topological space \(\) to a Hilbert space \(\). The study of this setting as an ill-posed statistical inverse problem is well established: see e.g. 46, 6, 53, 3, 5, 17. In this work, we study the setting when \(\) is high- or infinite-dimensional, since it has been less well covered by the literature, yet has many applications in multitask regression [7; 2] and infinite-dimensional learning problems, includingthe conditional mean embedding [20; 21; 41], structured prediction [11; 12], causal inference , regression with instrumental and proximal variables [42; 35], the estimation of linear operators and dynamical systems [47; 37; 26; 38; 25], and functional regression . Interestingly, the aforementioned infinite-dimensional applications typically use the classical _ridge regression_ algorithm. Our goal is to motivate the use of alternative learning algorithms in these settings, while providing strong theoretical guarantees.

Classically, the ill-posed problem (1) is solved via regularization strategies, which are often implemented in terms of so-called _spectral filter functions_ in the context of inverse problems in Hilbert spaces . When applied to the learning problem given by (1), these filter functions correspond to learning algorithms including ridge regression, a variety of different implementations of _gradient descent_, _principal component regression_, and other related methods (we refer the reader to 19 and 2 for overviews of the real-valued and vector-valued output variable case, respectively). Algorithms based on spectral filter functions when \(=\) are studied extensively, see e.g. [5; 34]. To the best of our knowledge, the detailed behavior of this general class of algorithms has remained unknown when \(\) is a general Hilbert space, with the exception of a few results for special cases in the setting of ridge regression [6; 31].

**Overview of our contributions.** In this manuscript, we aim to theoretically understand vector-valued spectral learning algorithms. The contribution of our work is twofold: (i) we rigorously confirm the _saturation effect_ of ridge regression for general Hilbert spaces \(\) (see paragraph below) in the context of lower bounds on rates for the learning problem (1) and (ii) we cover a gap in the existing literature by providing _upper rates for general spectral algorithms_ in high- and infinite-dimensional spaces. Our results explicitly allow the _misspecified learning case_ in which the true regression function is not contained in the hypothesis space. We base our analysis on the concept of _vector-valued interpolation spaces_ introduced by [30; 31]. The interpolation space norms measure the smoothness of the true regression function, replacing typical source conditions found in the literature which only cover the well-specified case. _To the best of our knowledge, these are the first bounds covering this general setting for vector-valued spectral algorithms._

**Saturation effect of ridge regression.** The widely-used ridge regression algorithm is known to exhibit the so-called saturation effect: it fails to exploit additional smoothness in the target function beyond a certain threshold. This effect has been thoroughly investigated in the context of Tikhonov regularization in inverse problems [16; Chapter 5], but is generally reflected only in upper rates in the learning literature, see e.g. [34; 5]. Interestingly, existing lower bounds [6; 5; 31] are usually formulated in a more general setting and do not reflect this saturation effect, leaving a gap between upper and lower rates. We leverage the bias-variance decomposition paradigm to lower bound the learning risk of kernel ridge regression with vector-valued output, in order to close this gap.

**Learning rates of vector-valued spectral algorithms.** Motivated by the fact that the saturation effect is technically unavoidable with vector-valued ridge regression, we proceed to study the generalization error of popular alternative learning algorithms. In particular, we provide upper rates in the vector-valued setting consistent with the known behavior of spectral algorithms in the real-valued learning setting, based on their so-called _qualification property_[5; 34]. In particular, we confirm that a saturation effect can be bypassed in high and infinite dimensions by algorithms such as principal component regression and gradient descent, allowing for a better sample complexity for high-smoothness problems. Furthermore, we study the misspecified setting and show that upper rates for spectral algorithms match the state-of-the-art upper rates for misspecified vector-valued ridge regression recently obtained by . Those rates are optimal for a wide variety of settings. Moreover, we argue that applications of vector-valued spectral algorithms are easy to implement by making use of an extended _representer theorem_ based on , allowing for the numerical evaluation based on empirical data--even in the infinite-dimensional case.

**Related Work.** The saturation effect of regularization techniques in deterministic inverse problems is well-known. For example, [40; 36; 22] study the saturation effect for Tikhonov regularization and general spectral algorithms. In the kernel statistical learning framework, the general phenomenon of saturation is discussed by e.g. [3; 19]. Recent work by  investigates saturation effect in the learning context by providing a lower bound on the learning rate. To the best of our knowledge, however, all studies in the statistical learning context focus on the case when \(Y\) is real-valued. General upper bounds of kernel ridge regression with real-valued or finite-dimensional \(Y\) have been extensively studied in the literature (see e.g., [6; 50; 8; 17]), where minimax optimal learning rates are derived. Recent work [30; 31] studies the infinite-dimensional output space setting with Tikhonov regularization and obtains analogous minimax optimal learning rates.  later study a setting where both the input and output space is the infinite dimensional Sobolev RKHS and establish the minimax optimal rate. For kernel learning with spectral algorithms, existing work (see e.g., [3; 5; 32; 34; 54; 28]) focuses on real-valued output space setting and obtains optimal upper learning rates depending on the qualification number of the spectral algorithms, where only [54; 28] consider the misspecified learning scenario where the target regression function does not lie in the hypothesis space. For vector-valued output spaces,  considers learning with vector-valued random features. However, general investigations of spectral algorithms for vector-valued output spaces are absent in the literature.

**Structure of this paper.** This paper is structured as follows. In Section 2, we introduce mathematical preliminaries related to reproducing kernel Hilbert spaces, vector-valued regression as well as the concept of vector-valued interpolation spaces. Section 3 contains a brief review the so-called saturation effect and a corresponding novel lower bound for vector-valued kernel ridge regression. In Section 4, we investigate general spectral learning algorithms in the context of vector-valued interpolations spaces and provide our main result: upper learning rates for this setting.

## 2 Background and Preliminaries

Throughout the paper, we consider a random variable \(X\) (the covariate) defined on a second countable locally compact Hausdorff space2\(\) endowed with its Borel \(\)-field \(_{}\), and the random variable \(Y\) (the output) defined on a potentially infinite dimensional separable real Hilbert space \((,,_{})\) endowed with its Borel \(\)-field \(_{}\). We let \((,,)\) be the underlying probability space with expectation operator \(\). Let \(P\) be the push-forward of \(\) under \((X,Y)\) and \(\) and \(\) be the marginal distributions on \(\) and \(\), respectively; i.e., \(X\) and \(Y\). We use the Markov kernel \(p:_{}_{+}\) to express the distribution of \(Y\) conditioned on \(X\) as

\[[Y A|X=x]=_{A}p(x,dy),\]

for all \(x\) and events \(A_{}\), see e.g. . We introduce some notation related to linear operators on Hilbert spaces and vector-valued integration; formal definitions can be found in Appendix A for completeness, or we refer the reader to [52; 14]. The spaces of Bochner square-integrable functions with respect to \(\) and taking values in \(\) are written as \(L_{2}(,_{},;)\), abbreviated as \(L_{2}(;)\). We obtain the classical Lebesgue spaces as \(L_{2}():=L_{2}(;)\). Throughout the paper, we write \([F]\) or more explicitly \([F]_{}\) for the \(\)-equivalence class of (potentially pointwise defined) measurable functions from \(\) to \(\), which we naturally interpret as elements in \(L_{2}(;)\) whenever they are square-integrable. Let \(H\) be a separable real Hilbert space with inner product \(,_{H}\). We write \((H,H^{})\) as the Banach space of bounded linear operators from \(H\) to another Hilbert space \(H^{}\), equipped with the operator norm \(\|\|_{H H^{}}\). When \(H=H^{}\), we simply write \((H)\) instead. We write \(S_{2}(H,H^{})\) as the Hilbert space of Hilbert-Schmidt operators from \(H\) to \(H^{}\) and \(S_{1}(H,H^{})\) as the Banach space of trace class operators (see Appendix A for a complete definition). For two Hilbert spaces \(H,H^{}\), we say that \(H\) is (continuously) embedded in \(H^{}\) and denote it as \(H H^{}\) if \(H\) can be interpreted as a vector subspace of \(H^{}\) and the inclusion operator \(i:H H^{}\) performing the change of norms with \(ix=x\) for \(x H\) is continuous; and we say that \(H\) is isometrically isomorphic to \(H^{}\) and denote it as \(H H^{}\) if there is a linear isomorphism between \(H\) and \(H^{}\) which is an isometry.

**Tensor Product of Hilbert Spaces:** Denote \(H H^{}\) the tensor product of Hilbert spaces \(H\), \(H^{}\). The element \(x x^{} H H^{}\) is treated as the linear rank-one operator \(x x^{}:H^{} H\) defined by \(y^{} y^{},x^{}_{H^{}}x\) for \(y^{} H^{}\). Based on this identification, the tensor product space \(H H^{}\) is isometrically isomorphic to the space of Hilbert-Schmidt operators from \(H^{}\) to \(H\), i.e., \(H H^{} S_{2}(H^{},H)\). We will hereafter not make the distinction between these two spaces, and treat them as being identical.

**Remark 1** (1, Theorem 12.6.1).: _Consider the Bochner space \(L_{2}(;H)\) where \(H\) is a separable Hilbert space. One can show that \(L_{2}(;H)\) is isometrically identified with the tensor product space \(H L_{2}()\), and we denote as \(\) the isometric isomorphism between the two spaces. See Appendix A for more details on tensor product spaces and the explicit definition of \(\)._

**Scalar-valued Reproducing Kernel Hilbert Space (RKHS).** We let \(k:\) be a symmetric and positive definite kernel function and \(\) be a vector space of functions from \(\) to \(\), endowed with a Hilbert space structure via an inner product \(,_{}\). We say that \(k\) is a reproducing kernel of \(\) if and only if for all \(x\) we have \(k(,x)\) and for all \(x\) and \(f\), we have \(f(x)= f,k(x,)_{}\). A space \(\) which possesses a reproducing kernel is called a reproducing kernel Hilbert space (RKHS; see e.g. 4). We denote the canonical feature map of \(\) as \((x)=k(,x)\).

We require some technical assumptions on the previously defined RKHS and kernel, which we assume to be satisfied throughout the text:

1. \(\) is separable: this is satisfied if \(k\) is continuous, given that \(\) is separable3; 2. \(k(,x)\) is measurable for \(\)-almost all \(x\);
3. \(k(x,x)^{2}\) for \(\)-almost all \(x\).

The above assumptions are not restrictive in practice, as well-known kernels such as the Gaussian, Laplace, and Matern kernels satisfy them on \(^{d}\). We now introduce some facts about the interplay between \(\) and \(L_{2}(),\) which has been extensively studied by ,  and . We first define the (not necessarily injective) embedding \(I_{}: L_{2}()\), mapping a function \(f\) to its \(\)-equivalence class \([f]\). The embedding is a well-defined compact operator as long as its Hilbert-Schmidt norm is finite. In fact, this requirement is satisfied since its Hilbert-Schmidt norm can be computed as [51, Lemma 2.2 & 2.3]\(\|I_{}\|_{S_{2}(,L_{2}())}=\|k\|_{L_{2}()}\). The adjoint operator \(S_{}:=I_{}^{}:L_{2}()\) is an integral operator with respect to the kernel \(k\), i.e. for \(f L_{2}()\) and \(x\) we have [49, Theorem 4.27]

\[(S_{}f)(x)=_{}k(x,x^{})f(x ^{})(x^{}).\]

Next, we define the self-adjoint, positive semi-definite and trace class integral operators

\[L_{X}:=I_{}S_{}:L_{2}() L_{2}() C_{X}:=S_{}I_{ }:.\]

**Vector-valued Reproducing Kernel Hilbert Space (vRKHS).** Let \(K:()\) be an operator valued positive-semidefinite (psd) kernel. Fix \(K\), \(x\), and \(h\), then \((K_{x}h)():=K(,x)h\) defines a function from \(\) to \(\). The completion of

\[_{}:=\{K_{x}h x,h\}\]

with inner product on \(_{}\) defined on the elementary elements as \((K_{x}h,K_{x^{}}h^{})_{}:= h,K (x,x^{})h^{}_{}\), defines a vRKHS denoted as \(\). For a more complete overview of the vector-valued reproducing kernel Hilbert space, we refer the reader to  and [31, Section 2]. In the following, we will denote \(\) as the vRKHS induced by the kernel \(K:()\) with

\[K(x,x^{}):=k(x,x^{})_{}, x,x^{ }.\] (2)

We emphasize that this family of kernels is the de-facto standard for high- and infinite-dimensional applications  due to the crucial _representement theorem_ which gives a closed form solution for the ridge regression problem based on the data. We generalize this representer theorem to cover the general spectral algorithm case in Proposition 1.

**Remark 2** (General multiplicative kernel).: _Without loss of generality, we provide our results for the vRKHS \(\) induced by the operator-valued kernel given by \(K(x,x^{})=k(x,x^{})_{}\). However, with suitably adjusted constants in the assumptions, our results transfer directly to the more general vRKHS \(}\) induced by the more general operator-valued kernel_

\[(x,x^{}):=k(x,x^{})T\]

_where \(T:\) is any positive-semidefinite self-adjoint operator. The precise characterization of the adjusted constants is given by [31, Section 4.1]._

An important property of \(\) is that it is isometrically isomorphic to the space of Hilbert-Schmidt operators between \(\) and \(\)[31, Corollary 1]. Similarly to the scalar case we can map every element in \(\) into its \(\)-equivalence class in \(L_{2}(;)\) and we use the shorthand notation \([F]=[F]_{}\) (see Definition 6 in Appendix A for more details).

**Theorem 1** (vRKHS isomorphism).: _For every function \(F\) there exists a unique operator \(C S_{2}(,)\) such that \(F()=C()\) with \(\|C\|_{S_{2}(,)}=\|F\|_{}\) and vice versa. Hence \( S_{2}(,)\) and we denote the isometric isomorphism between \(S_{2}(,)\) and \(\) as \(\). It follows that \(\) can be written as \(=\{F: F=C(),\,C S _{2}(,)\}\)._

### Vector-valued Regression

We briefly recall the basic setup of regularized least-squares regression with Hilbert space-valued random variables. The squared expected risk for vector-valued regression is

\[(F):=[\|Y-F(X)\|_{}^{2}]=_{ }\|y-F(x)\|_{}^{2}p(x,dy)(dx),\] (3)

for measurable functions \(F:\). The analytical minimizer of the risk over measurable functions is the _regression function_ or the _conditional mean function_\(F_{} L_{2}(;)\) given by

\[F_{}(x):=[Y X=x]=_{}y\,p(x,dy), x .\]

Throughout the paper, we assume that \([\|Y\|_{}^{2}]<+\), i.e., the random variable \(Y\) is square-integrable. Note that this implies \(F_{} L_{2}(;)\). Our focus in this work is to approximate \(F_{}\) with kernel-based regularized least-squares algorithms, where we pay special attention to the case when \(\) is of high or infinite dimension. We pick \(\) as a hypothesis space of functions in which to estimate \(F_{}\). Note that by Theorem 1, minimizing the functional \(\) on \(\) is equivalent to minimizing the following functional on \(S_{2}(,)\),

\[}(C):=Y-C(X)_{}^ {2}.\] (4)

It is shown in [38, Proposition 3.5 and Section 3.4] that the optimality condition can be written as

\[C_{YX}=C_{}C_{X}, C_{} S_{2}(,),\] (5)

where \(C_{YX}:=[Y(X)]\) is the cross-covariance operator. As discussed in full detail by , the problem (5) can be formulated as a potentially ill-posed inverse problem on the space of Hilbert-Schmidt operators. As such, a regularization is required; we introduce regularized solutions of this problem in Section 4 through the classical concept of spectral filter functions.

### Vector-valued Interpolation Space and Source Condition

We now introduce the background required in order to characterize the smoothness of the target function \(F_{}\), both in the well-specified setting (\(F_{}\)) and in the misspecified setting (\(F_{}\)). We review the results of  and  in constructing scalar-valued interpolation spaces, and  in defining vector-valued interpolation spaces.

**Real-valued Interpolation Space:** By the spectral theorem for self-adjoint compact operators, there exists an at most countable index set \(I\), a non-increasing sequence \((_{i})_{i I}>0\), and a family \((e_{i})_{i I}\), such that \(e_{i}_{i I}\)4 is an orthonormal basis (ONB) of \(I_{}} L_{2}()\) and \((_{i}^{1/2}e_{i})_{i I}\) is an ONB of \(( I_{})^{}\), and we have

\[L_{X}=_{i I}_{i}\{,[e_{i}]\}_{L_{2}()}[e_{i}], C_{X}= _{i I}_{i}\{,_{i}^{}e_{i}\}_{}_{i} ^{}e_{i}\] (6)

For \( 0\), the \(\)-interpolation space  is defined by

\[[]^{}:=\{_{i I}a_{i}_{i}^{/2}\,[e_{i}]: (a_{i})_{i I}_{2}(I)\} L_{2}(),\]

equipped with the inner product

\[\{_{i I}a_{i}(_{i}^{/2}[e_{i}]),_{i I}b_{i}(_{ i}^{/2}[e_{i}])\}_{[]^{}}=_{i I}a_{i}b_{i},\]for \((a_{i})_{i I},(b_{i})_{i I}_{2}(I)\). The \(\)-interpolation space defines a Hilbert space. Moreover, \((_{i}^{/2}[e_{i}])_{i I}\) forms an ONB of \([]^{}\) and consequently \([]^{}\) is a separable Hilbert space. In the following, we use the abbreviation \(\|\|_{}:=\|\|_{[]^{}}\).

**Vector-valued Interpolation Space:** Introduced in , vector-valued interpolation spaces generalize the notion of scalar-valued interpolation spaces to vRKHS with a kernel of the form (2).

**Definition 1** (Vector-valued interpolation space).: _Let \(k\) be a real-valued kernel with associated RKHS \(\) and let \([]^{}\) be the real-valued interpolation space associated to \(\) with some \( 0\). The vector-valued interpolation space \([]^{}\) is defined as (refer to Remark 1 for the definition of \(\))_

\[[]^{}:=(S_{2}([]^{},) )=\{F F=(C),\ C S_{2}([]^{},)\}.\]

_The space \([]^{}\) is a Hilbert space equipped with the inner product_

\[ F,G_{}:= C,L_{S_{2}([ ]^{},)}(F,G[]^{}),\]

_where \(C=^{-1}(F),\,L=^{-1}(G)\). For \(=0\), we retrieve \(\|F\|_{0}=\|F\|_{L_{2}(;)}=\|C\|_{S_{2}(L_{2}(),)}\)._

**Remark 3** (Interpolation space inclusions).: _Note that we have \(F_{} L_{2}(;)\) since \(Y L_{2}(;)\) by assumption. Furthermore, for \(0<<\), [17, Eq. (7)] imply the inclusions_

\[[]^{}[]^{}[ ]^{0} L_{2}(;).\]

_Under assumptions 1 to 3 and with \(\) being a second-countable locally compact Hausdorff space, \([]^{0}=L_{2}(;)\) if and only if \(\) is dense in the space of continuous functions vanishing at infinity, equipped with the uniform norm [31, Remark 4]._

**Remark 4** (Well-specified versus misspecified setting).: _We say that we are in the well-specified setting if \(F_{}[]^{1}\). In this case, there exists \(\) such that \(F_{}=\,\,-\)almost surely and \(\|F_{}\|_{1}=\|\|_{}\), i.e. \(F_{}\) admits a representer in \(\) (see Remark 5 in Appendix A). When \(F_{}[]^{}\) for \(<1\), \(F_{}\) may not admit such a representation and we are in the misspecified setting, as \([]^{1}[]^{}\)._

Definition 1 and Remarks 3 and 4 motivate the use of following assumption on the smoothness of the target function: there exists \(>0\) and a constant \(B 0\) such that \(F_{}[]^{}\) and

\[\|F_{}\|_{} B.\] (SRC)

We let \(C_{}:=^{-1}(F_{}) S_{2}([]^{},)\). (SRC) directly generalizes the notion of a so-called Holder-type source condition in the learning literature  and allows to characterize the misspecified learning scenario.

### Further Assumptions

In addition to (SRC), we require standard assumptions to obtain the precise learning rates for kernel learning algorithms. We list them below. For constants \(D_{2}>0\) and \(p(0,1]\) and for all \(i I\),

\[_{i} D_{2}i^{-1/p}.\] (EVD)

For constants \(D_{1},D_{2}>0\) and \(p(0,1)\) and for all \(i I\),

\[D_{1}i^{-}_{i} D_{2}i^{-1/p}.\] (EVD+)

(EVD) and (EVD+) are standard assumptions on the _eigenvalue decay_ of the integral operator: they describe the interplay between the marginal distribution \(\) and the RKHS \(\) (see more details in 6, 17). (EVD+) is needed in order to establish lower bounds on the excess risk. Note that we have excluded the value \(p=1\) from (EVD+); indeed, \(p=1\) is incompatible with the assumption of a bounded kernel, a fact missed by previous works and of independent interest (see Appendix, Remark 7).

For \([p,1]\), the inclusion \(I_{}^{,}:[]^{} L_{}()\) is continuous, and \( A>0\) such that

\[\|I_{}^{,}\|_{[]^{} L_{}( )} A.\] (EMB)

Property (EMB) is referred to as the _embedding property_ in . It can be shown that it holds if and only if there exists a constant \(A 0\) with \(_{i I}_{i}^{}e_{i}^{2}(x) A^{2}\) for \(\)-almost all \(x\)[17, Theorem 9]. Since we assume \(k\) to be bounded, the embedding property always hold true when \(=1\)Furthermore, (EMB) implies a polynomial eigenvalue decay of order \(1/\), which is why we take \( p\). (EMB) is not needed when we deal with the well-specified setting, but is crucial to bound the excess risk in the misspecified setting.

Finally, we assume that there are constants \(,R>0\) such that

\[_{}\|y-F_{*}(x)\|_{}^{q}p(x,dy)q!^{2}R^{q-2},\] (MOM)

is satisfied for \(\)-almost all \(x\) and all \(q 2\). The (MOM) condition on the Markov kernel \(p(x,dy)\) is a _Bernstein moment condition_ used to control the noise of the observations (see 6, 17 for more details). If \(Y\) is almost surely bounded, for example \(\|Y\|_{} Y_{}\) almost surely, then (MOM) is satisfied with \(=R=2Y_{}\). It is possible to prove that the Bernstein condition is equivalent to sub-exponentiality, see [38, Remark 4.9].

## 3 Saturation Effect of Kernel Ridge Regression

The most established way of learning \(F_{*}\) is by kernel ridge regression (KRR), which can be formulated as the following optimization problem: given a dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{n}\) independently and identically sampled from the joint distribution of \(X\) and \(Y\),

\[_{}:=*{arg\,min}_{}_{i=1}^{n}\|y_{i}-F(x_{i})\|_{}^{2}+\|F\|_ {}^{2},\] (7)

where \(>0\) is the regularization parameter. The generalization error of vector-valued KRR is expressed as \(_{}-F_{*}\), and controlled in different norms: see  for an extensive study. We recall here a simplified special case of the key results obtained in this work. In the next Theorem, \(,\) are inequality up to positive multiplicative constants that are independent of \(n\).

**Theorem 2** (Upper and lower bounds for KRR in the well-specified regime).: _Let \(_{}\) be the KRR estimator from (7). Furthermore, let the conditions (EVD+), (SRC) and (MOM) be satisfied for some \(0<p 1\) and \( 1\). Then, with high probability we have_

\[\|[_{_{n}}]-F_{*}\|_{L_{2}(;)}^{2} n^{-}_{n}=(n^{-}),\]

_and furthermore for all learning methods (i.e., measurable maps) of the form \(D_{D}\),_

\[\|[_{D}]-F_{*}\|_{L_{2}(;)}^{2}  n^{-}.\]

Theorem 2 shows the minimax optimal learning rate for vector-valued KRR for \(\). However, when \(>2\), the obtained upper bound saturates at \(n^{-}\), creating a gap with the lower bound. This phenomenon is referred to as the saturation effect of Tikhonov regularization, and has been well investigated in deterministic inverse problems . In the case where \(\) is real-valued,  prove that the saturation effect cannot be avoided with Tikhonov regularization. Below, we give a similar but generalized bound on lower rates for the case that \(\) is a Hilbert space. For this result only, we assume that \(\) is a compact subset of \(^{d}\). We give the proof in Appendix B.

**Theorem 3** (Saturation of KRR).: _Let \(\) be a compact subset of \(^{d}\). Let \(=(n)\) be an arbitrary choice of regularization parameter satisfying \((n) 0\) as \(n+\) and let \(_{}\) be the KRR estimator from (7). We assume that the noise is non-zero and bounded below, i.e. there exists \(>0\), such that_

\[_{}\|y-F_{*}(x)\|_{}^{2}p(x,dy) ^{2},\]

_is satisfied for \(\)-almost all \(x\). We assume in addition and for this result only that \(k\) is Holder continuous (see Definition 11 in the appendix), i.e., \(k C^{}()\) for \((0,1]\). Suppose that Assumptions (EVD+) and (SRC) hold with \(p(0,1)\) and \( 2\). For \( 0\), for sufficiently large \(n>0\), where the hidden index bound depends on \(\), with probability greater than \(1-e^{-}\), there exists some constant \(c_{}>0\) such that_

\[[\|[_{}]-F_{*}\|_{L_{2}(; )}^{2}|x_{1},,x_{n}| c_{}n^{-}.\]The assumption that \(k\) is Holder continuous is crucial in lower bounding the variance with a covering number argument. Kernels satisfying this assumption include Gaussian kernels, Laplace kernels and Matern kernels. Theorem 3 clearly demonstrates that the learning rate from vector-valued KRR cannot reach the information theoretic lower rate given in Theorem 2.

As discussed above,  propose a similar lower bound in the real-valued case, and we now highlight two fundamental differences with  in the proof. First, while both works adopt the same bias-variance decomposition, we need to lower bound the bias and the variance term with infinite-dimensional output in our setting. Second, we adopt a different and simpler approach in proving the lower bound, since there are a number of issues with the proof of , both in the treatment of the bias and of the variance. For a detailed comparison with the earlier work, and an explanation of the differences in our approach, please refer to Remark 6 in the Appendix.

## 4 Consistency and optimal rates for general spectral algorithms

**Regularized population solution**: Our goal is to regularize (5) in such a way that we get a unique and well-defined solution that provides a good approximation to \(F_{*}\). We first recall the concept of a filter function (i.e., a function on an interval which is applied on self-adjoint operators to each individual eigenvalue via the spectral calculus, see 16), that will allow to define a regularization strategy. One may think of the following definition as a class of functions approximating the inversion map \(x 1/x\) while still being defined for \(x=0\) in a reasonable way. We use the definition given by , but equivalent definitions can be found throughout the literature.

**Definition 2** (Filter function).: _Let \(^{+}\). A family of functions \(g_{}:[0,)[0,)\) indexed by \(\) is called a filter with qualification \( 0\) if it satisfies the following two conditions:_

1. _There exists a positive constant_ \(E\) _such that, for all_ \(\)__ \[_{}_{x[0,^{2}]}^{1-}x^{}g_{ }(x) E\] (8)
2. _There exists a positive constant_ \(_{}<\) _such that_ \[_{[0,]}_{}_{x[0,^{2}]}|r_{ }(x)|x^{}^{-}_{}, r_{}(x):=1-g_{}(x)x.\] (9)

Below, we give some standard examples which are discussed by e.g.  in the context of kernel regression with scalar output variables, and in  for the vector-valued case. A variety of additional algorithms can be expressed in terms of a filter function.

1. _Ridge regression._ From the Tikhonov filter function \(g_{}(x)=(x+)^{-1}\), we obtain the known ridge regression algorithm. In this case, we have \(E==_{}=1\).

2. _Gradient Descent._ From the Landweber iteration filter function given by

\[g_{k}(x):=_{i=0}^{k-1}(1- x)^{i}k:=1/ ,k\]

we obtain the gradient descent scheme with constant step size \(>0\), which corresponds to the population gradient iteration given by \(F_{k+1}:=F_{k}-(F_{k})\) for \(k\). In this case, we have \(E=1\) and arbitrary qualification with \(_{}=1\) whenever \(0< 1\) and \(_{}=^{}\) otherwise. Gradient schemes with more complex update rules can be expressed in terms of filter functions as well .

3. _Kernel principal component regression._ The truncation filter function \(g_{}(x)=x^{-1}[x]\) yields kernel principal component regression, corresponding to a hard thresholding of eigenvalues at a truncation level \(\). In this case we have \(E=_{}=1\) for arbitrary qualification \(\).

**Population solution**: Given a filter function \(g_{}\), we call \(g_{}(C_{X})\)5 the regularized inverse of \(C_{X}\). We may think of the regularized inverse as approximating the _pseudoinverse_ of \(C_{X}\) (see e.g. ) when \( 0\). We define the regularized population solution to (4) as

\[C_{}:=C_{YX}g_{}(C_{X}) S_{2}(,),  F_{}():=C_{}().\] (10)The solution arising from standard regularization strategies leads to well-known statistical methodologies. We refer to  for the background on filter functions in classical regularization theory.

**Empirical solution**: Given the dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{n}\), the empirical analogue of (10) is

\[_{}:=_{YX}g_{}(_{X}),_{ }():=_{}(),\] (11)

where \(_{YX}\), \(_{X}\) are empirical covariance operators define as

\[_{X}:=_{i=1}^{n}(x_{i})(x_{i})_{YX}:=_{i=1}^{n}y_{i}(x_{i}).\]

Note that (11) is the regularized solution of the empirical inverse problem

\[_{YX}=_{X}, S_{2}(,),\]

which arises as the optimality condition for minimizers on \(\) of the empirical analogue of (3), given by \(_{n}(F):=_{i=1}^{n}\|y_{i}-F(x_{i})\|_{3}^{2}\); see Proposition 2 in the Appendix for a proof. For the vector-valued kernel given in (2), it is well-known that \(_{}\) can be computed in closed-form for the ridge regression estimator--even in infinite dimensions . For general filter functions, an extended representer theorem is given by  in the context of finite-dimensional multitask learning: this approach works in infinite dimensions as well. We give the closed form solution based on  below (we include the proof in Appendix D.1).

**Proposition 1** (Representer theorem for general spectral filter).: _Let \(()_{ij}=k(x_{i},x_{j})\), \(1 i,j n\) denote the Gram matrix associated to the scalar-valued kernel \(k\). We have_

\[_{}(x)=_{i=1}^{n}y_{i}_{i}(x),(x)= {n}g_{}(}{n})_{x}^{n },(_{x})_{i}=k(x,x_{i}), 1 i n.\] (12)

**Example 1** (Conditional integration).: _Consider now a random variable \(Z\) taking values in a topological space \(\) on which we define a second RKHS \(^{}^{}\) with kernel \(:\) and canonical feature map \(:^{},z(z,)\). The conditional mean embedding  is defined as_

\[F_{*}(x):=[(Z) X=x], x.\]

_We immediately see the link with vector-valued regression with \(Y=(Z)\) and \(=^{}\). The conditional mean embedding allows us to compute the conditional expectation of any element of \(^{}\). Indeed, using the reproducing property, for \(f^{}\), we have for all \(x\),_

\[[f(Z) X=x]= f,[(Z) X=x]_{ ^{}}.\]

_Given a dataset \(\{(x_{i},z_{i})\}_{i=1}^{n}\)6 and an estimate of the conditional mean embedding \(F_{*}\) with a spectral algorithm \(_{}\) as in Eq. (11), and substituting the formula in Eq. (12), we obtain \([f(Z) X=x] f,_{}(x)_{ ^{}}=_{i=1}^{n} f,(z_{i})_{ ^{}}_{i}(x)=_{z}^{}(x),\) where \((_{z})_{i}=f(z_{i})\), \(1 i n\)._

**Learning rates:** We now give our main result, the learning rates for the difference between \([_{}]\) and \(F_{*}\) in the interpolation norm, where \(F_{}\) and \(_{}\) are given by (10) and (11) based on a general spectral filter satisfying Definition 2. The proof is deferred to Section C in the Appendix.

**Theorem 4** (Upper learning rates).: _Let \(_{}\) be an estimator based on a general spectral filter with qualification \( 0\). Furthermore, let the conditions (EVD), (EMB), (MOM) be satisfied with \(0<p 1\). With \(0 1\), if (SRC) is satisfied with \(< 2\), we have_

1. _in the case_ \(+p\)_, let_ \(_{n}=((n/^{}(n))^{-})\) _for some_ \(>1\)_, for all_ \(>(6)\) _and sufficiently large_ \(n 1\)_, there is a constant_ \(J>0\) _independent of_ \(n\) _and_ \(\) _such that_ \[\|[_{_{n}}]-F_{*}\|_{}^{2}^{2}J( n})^{-}\] _is satisfied with_ \(P^{n}\)_-probability not less than_ \(1-6e^{-}\)_._2. _in the case_ \(+p>\)_, let_ \(_{n}=(n^{-})\)_, for all_ \(>(6)\) _and sufficiently large_ \(n 1\)_, there is a constant_ \(J>0\) _independent of_ \(n\) _and_ \(\) _such that_ \[\|[_{_{n}}]-F_{}\|_{}^{2} ^{2}Jn^{-}\] _is satisfied with_ \(P^{n}\)_-probability not less than_ \(1-6e^{-}\)_._

Theorem 4 provides the upper rate for vector-valued spectral algorithms. In particular, in combination with the lower bound in Theorem 2, we see that vector-valued spectral algorithms with qualification \(\) achieve an optimal learning rate when the smoothness \(\) of the regression function is in the range \((-p,2]\). For algorithms with infinite \(\) such as gradient descent and principal component regression, we confirm that they can exploit smoothness of the target function just as in the real-valued setting [3; 5; 30], while not suffering from saturation. For Tikhonov regularization, where \(=1\), the rates recover the state-of-the-art results from . Finally, we point out that obtaining minimax optimal learning rates for \(<-p\) still remains challenging even in the real-valued output scenario. Note however that for a large variety of RKHS, \(\) is arbitrarily close to \(p\) and we obtain optimal rates for the whole range \((0,2]\): we refer to [31; 54] for a detailed discussion.

We provide a proof sketch for Theorem 4. The key technical challenge in extending the results of  to spectral filter functions lies in the analysis of the estimation error. The estimation error in \(-\)norm is bounded as \(\|[_{}-C_{}]\|_{S_{2}([]^{},)} 3^{-}\| (_{}-C_{})_{X,}^{} \|_{S_{2}(,)}\) (see Eq. (37) in Appendix C.3). We rely on the fact that \(_{}=_{X}g_{}(_{X})+r_{}( {C}_{X})\) (see Definition 2), to obtain the decomposition \(_{}-C_{}=(_{YX}-C_{}_{X}) g_{}(_{X})-C_{}r_{}(_{X})\), which yields two terms to be controlled,

\[\|(_{}-C_{})_{X,}^{}\|_{S_{2}(,)}_{YX}-C_{}_{X})g_{}(_{X})_{X, }^{}\|_{S_{2}(,)}}_{(I)}+ r_{}(_{X})_{X,}^{ }\|_{S_{2}(,)}}_{(II)}\]

To control term (I), we use the definition of the filter function \(g_{}\) (Definition 2) to obtain that \(\|_{X,}g_{}(_{X})\|_{ } 1\). Thus it suffices to control the term \(\|(_{YX}-C_{}_{X})C_{X,}^{-}\|_{S_{2}(,)}=\|_{i=1}^{n }(x_{i},y_{i})\|_{S_{2}(,)}\), where \((x,y)=(y-C_{}(x)) C_{X,}^{- }(x)\). We proceed by bounding \([|(X,X)|_{S_{2}(,)}^{m}]\) for \(m 1\), and then use Bernstein's inequality to derive the upper bound on \(\|(_{YX}-C_{}_{X})C_{X,}^{-}\|_{S_{2}(,)}\). To control term (II), Lemma 9 in C.1 shows that

\[(II)\|_{X,}^{}r_{}(_{X})g_ {}(C_{X})C_{X}^{}\|_{}.\]

The term on the right side is bounded in prior work on scalar-valued spectral method, and we refer the reader to [54, Theorem 16]. The results of Theorem 4 are then obtained by choosing regularization parameter \(=(n)\) to optimally trade off approximation and estimation errors.

## 5 Conclusion

In this work, we have rigorously explored the theoretical properties of vector-valued spectral learning algorithms, focusing on their performance in infinite-dimensional output spaces. We first proved the saturation effect observed in vector-valued kernel ridge regression, highlighting its limitations in exploiting additional smoothness in regression functions. We then presented upper bounds on the finite sample risk for a general class of spectral learning algorithms, demonstrating their minimax optimality across various scenarios, including misspecified learning settings.

Our results open avenues for further research, particularly in developing more efficient implementations for practical use in high-dimensional machine learning problems such as causal inference and functional data analysis.

**Acknowledgement:** Dimitri Meunier, Arthur Gretton and Zhu Li were supported by the Gatsby Charitable Foundation.