# Large Language Models of Code Fail at

Completing Code with Potential Bugs

Tuan Dinh\({}^{1}\)\({}^{}\) Jinman Zhao\({}^{2}\)1 Samson Tan\({}^{2}\) Renato Negrinho\({}^{2}\)

Leonard Lausen\({}^{2}\) Sheng Zha\({}^{2}\) George Karypis\({}^{2}\)

\({}^{1}\)University of Wisconsin-Madison \({}^{2}\)Amazon Web Services

tuan.dinh@wisc.edu

{jinmaz,samson,renatoni,lausen,zhasheng,gkarypis}@amazon.com

Equal contribution.Work done while interning at Amazon Web Services.

\({}^{1}\)University of Wisconsin-Madison \({}^{2}\)Amazon Web Services

tuan.dinh@wisc.edu

{jinmaz,samson,renatoni,lausen,zhasheng,gkarypis}@amazon.com

###### Abstract

Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the _buggy-code completion_ problem, inspired by the realistic scenario of real-time code suggestion where the code context contains _potential bugs_ - anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance.2

## 1 Introduction

Suggesting code for a given context is a frequently used feature in modern integrated development environments (IDEs) , bringing productivity gains to the code-writing process. This task is widely studied as code completion  in the literature, with techniques and models ranging from probabilistic or sequence modeling , incorporating code structure as prior knowledge , to adopting deep neural networks  and pre-training techniques  to learn representations for code. Recently, large Transformer-based language models of code (Code-LLMs)  have become a promising paradigm for code completion, attaining state-of-the-art (SotA) performance in various code learning tasks including code completion and generation.

However, existing works studying Code-LLMs often assume the absence of bugs, despite the frequent occurrences and the cost of bugs in software development: on average, 70 bugs are created per 1000 code lines ; and fixing bugs costs 50% of development time . Consider a practical coding scenario. A developer wants to use the code suggestion feature in an IDE when writing code. With a high probability, their real-time code context, as an input to the code predictor, contains typos or less refined, potentially buggy implementations. Since bugginess is a property of a complete program,it is not well-defined how to detect bugs and repair the buggy code in this short and incomplete code context, making the application of existing bug-detection or code-repair tools sub-optimal or infeasible. It is also worth mentioning that the gap between the _in vitro_ and _in vivo_ performances of code completion models  remains large. Therefore, a natural question arises: _Can existing Code-LLMs provide good code suggestions given the unrefined nature of draft code?_

To answer this question, we introduce and study the problem of completing code with potential bugs in the code context, dubbed _buggy-code completion (bCC)_. In this study, we focus on using Code-LLMs to generate functional implementations from a code context, where the code context consists of a problem specification and a piece of partial code.4 A _potential bug_ in a piece of partial code is a code span that can become a bug provided some completion, _i.e.,_ it fails the completion and at the same time can be changed to make the completion work. Note that a potential bug is not a bug per se without a completion. Shown in Figure 1 is an illustration of our task with a problem description at the top and partial code in the middle. On the left are a reference code context and a correct completion from the chosen Code-LLM. On the right, the highlighted potential bug (-=) makes the reference completion incorrect. The Code-LLM reacts to this potential bug by generating a different completion (bottom right). However, the completed code is still functionally incorrect.

To conduct a quantitative study of bCC, we construct two datasets. First, _buggy-HumanEval_ dataset contains interview-style coding problems from HumanEval dataset , with buggy/reference partial code pairs being generated by introducing semantic-altering operator changes to reference solutions. This dataset provides a well-controlled setting to assess models' behavior upon potential bugs. Second, _buggy-FixEval_ dataset, based on FixEval , contains user submissions to coding-context problems. The buggy/reference pairs are constructed from rejected and accepted submissions by the same user to a given problem. This dataset helps assess models' performance over a realistic distribution of potential bugs. Our benchmarks are well associated with the existing benchmarks for Code-LLMs.

Via our empirical studies, we find that the presence of potential bugs drastically degrades the code-completion performance of high-performing Code-LLMs, with test-case pass rates dropping to below 5% across both datasets for all tested model variants. For instance, on buggy-HumanEval, the test-case pass rate of CodeGen-2B-mono completions drops from \(54.9\%\) (reference partial code) to \(3.1\%\) (partial code contains potential bugs), which is worse than the score when no partial code is provided (\(9.3\%\)). Our results demonstrate that Code-LLMs are highly susceptible to potential bugs.

Furthermore, we attempt several post-hoc methods to augment Code-LLMs to better deal with potential bugs, namely removal-then-completion, completion-then-rewriting, and rewriting-then-completion. The latter two augment the Code-LLMs with an external code repairer as the rewriter component. Our evaluation shows that the attempted methods improve the buggy-code completion performance of all tested Code-LLMs. However, the performance gap remains large between these

Figure 1: **Illustrations for code completion (left) and buggy-code completion (right). (top) problem statement for function implementation, (middle) partial code with (right) or without (left) potential bugs (highlighted), (bottom) code completions from CodeGen-2B-mono . The completed code is functionally correct on the left but incorrect on the right, failing test case below_zero() == False. The example is based on HumanEval/3 from buggy-HumanEval.**

methods and the completion with reference partial code. We provide further case studies and analyses, _e.g.,_ effects of potential bugs' locations or the successful cases of naive completion for a better understanding of the behavior of the tested models in buggy-code completion.

Study scope and contributions.This work aims to explore and understand the behaviors of Code-LLMs under the buggy-code completion setting. We (i) define the novel buggy-code completion task, (ii) introduce two representative benchmark datasets, (iii) demonstrate the inability of Code-LLMs to handle potential bugs, and (iv) evaluate several baseline methods for improving Code-LLMs on buggy-code completion.

## 2 Buggy-Code Completion

In this work, we consider the code completion setting with inputs consisting of (1) a specification \(h\) that specifies the desired functionality of the program and (2) a code context \(s\), which is some unfinished code to be completed. Here, \(h\) is given as a docstring or a problem statement in English, and \(s\) is given as a few lines of code that are the beginning part of a program, which we refer to as _partial code_ or _(code) prefix_. In the conventional setting of code completion, the objective is to suggest a completion \(c\) such that \(t:=s::c\) is a program that satisfies \(h\), where "\(::\)" denotes code concatenation. Our _buggy_-code completion setting extends the conventional one with a challenging and realistic consideration: \(s\) may contain _potential bugs_.

Consider a real-world scenario where a programmer works on unfinished code and utilizes the code auto-completion feature for suggestions. Note that the coding process often sees coding mistakes or introduces inconsistent code usages, which are not necessarily "incorrect" per se. However, there is a fair chance that such mistakes or inconsistencies cause unintended and undesirable behaviors in the final program. In other words, they become bugs. Based on this intuition, we define potential bugs as the following:

**Definition 2.1** (_potential bug_).: Consider a specification \(h\) and a reference code prefix \(s\) for which some completion \(c\) exists such that \(t:=s::c\) satisfies \(h\). A _potential bug_ is manifested as a small edit \(e\) over \(s\) such that \(t^{}:=s^{}::c\) does not satisfy \(h\), where \(s^{}\) is the result of applying \(e\) on \(s\).

We note that potential bugs are not bugs per se and are only defined with respect to some reference code prefix \(s\) and completion \(c\). The code prefix \(s^{}\) containing potential bugs is referred to as "potentially buggy" or simply "buggy" prefix (with respect to \(s\)) throughout the paper. In most cases, we omit the "with respect to" part for brevity. The resulting term "buggy prefix" refers to the definition here and by no means assumes that the prefix itself is buggy. Our study focuses on potential bugs associated with semantic bugs, _i.e.,_ edits that do not introduce syntax errors, as semantic bugs are generally more challenging and interesting than syntax ones.

_Remark 2.2_.: Intuitively, we assume that the reference prefix \(s\) highly correlates to preferred (practical) implementations of the task and that any deviation from \(s\) is likely less preferred. We use the operational definition of potential bugs in 2.1 for its simplicity and verifiability, as well as for that it allows the initial exploration of the previously under-investigated scenario of bCC. We note that by this definition, a reference prefix \(s\) itself can, in some cases, albeit less likely, be "buggy" with respect to some other references. However, such a case is less of a concern in this study, as we are mostly interested in finding a completed functional program out of \(s\) or \(s^{}\).

Figure 2: **Performance (pass@1 (\(\))) degradation in the presence of potential bugs. CG: CodeGen. Potential bugs severely harm the completion of all CodeLMs (red bars) compared to non-buggy settings (teal bars), making it even worse than completion without partial code (blue bars).**

**Definition 2.3** (_buggy-code completion, bCC_).: Given a specification \(h\) and a code prefix \(s^{}\) containing potential bugs, _buggy-code completion_ is the task of generating a complete program \(t\) that satisfies \(h\).

We deliberately loosen the constraint that \(t\) should contain \(s^{}\) as a prefix, as by definition, and as well as one can see from the example in Figure 1, fixating on the buggy prefix can make suggesting a satisfying solution difficult, if not impossible. As we explore in Section 4.3, allowing models to suggest fixes to the buggy code prefix significantly increases the possibility that a generated program passes the tests. However, it is often still possible to continue a buggy prefix to a valid program solution (see an example in Figure 12).

An alternative view of the setting is that the buggy code prefix provides a noisy and potentially flawed precondition for generating a solution to the coding problem. It presents a good-faith effort from a user to instruct the tool about their intended code solutions. A good-behaving model should take this as a hint and, at worst, discard it so that the generation performance is not worse than when no code prefix is given. However, this is not the case for our evaluated models (Section 4.2).

_Remark 2.4_.: Our bCC formulation is _not_ a simple combination of code repair and code completion. As bugginess is a property of completed programs, repairing the partial code is an ill-defined problem, thus making repairing-then-completion also ill-defined. Therefore, our bCC task is better viewed as an extension of code completion with the additional challenge that generating semantically correct continuations from the given partial code without deviation may be difficult or even infeasible. This challenge requires models to be aware of the existence of potential bugs for better suggestions.

## 3 Benchmarks

This section introduces our new datasets with proposed baseline methods and evaluation metrics.

### Datasets for bCC

Based on our task definition, each instance in a bCC benchmark should contain a problem description specifying requirements, a piece of buggy code prefix to be completed, and a set of test cases for assessing the correctness of the finished code. Optionally, an instance also has a corresponding reference code prefix and a valid solution completed from the reference code. To our knowledge, no existing dataset meets all the desiderata. Commonly used large-scale datasets for code completion, _e.g._, Py150  and Java Corpus  do not come with test cases. Recent small-scale manually curated datasets for code generation, _e.g._, HumanEval , MBPP , APPs , come with test cases and reference solutions but no buggy or failed solutions. There is also no predefined way to extract partial code from the reference solutions. This is also the case for program-repair datasets: popular datasets either lack test cases, _e.g._, [23; 24; 25; 26] and/or are of small sizes, _e.g._, [27; 28; 29].

We introduce two datasets for evaluating bCC in Python, both with all the aforementioned desired components. We ensure that 1) all partial code snippets contain potential bugs that fulfill our definition, _i.e._, their respective completed code is incorrect, certified by failing test cases; and 2) all potential bugs are semantic in nature, _i.e._, they cause no syntax error in their respective completed code.

#### 3.1.1 Buggy-HumanEval

We first introduce buggy-HumanEval to offer a controlled setting for evaluating bCC in the presence of a single semantic bug. Buggy-HumanEval contains 1896 bCC instances constructed from a subset of HumanEval problems . The HumanEval dataset is a popular dataset of manually written introductory coding problems designed for evaluating the code generation ability of Code-LLMs.

Potential bugs are introduced as semantic-altering operator changes to reference solutions. We search for applicable binary operators in reference solutions and change them into their semantic opposites, _e.g._, + into -. To ensure that the edit introduces a bug, we execute the altered program and only keep the ones failing some tests. We then specify a line after the altered operator to split the solution into a code prefix and suffix. We keep the buggy prefix as part of the input for bCC. We split the unaltered solution at the same line to get a corresponding reference prefix. On average, the problems selected for buggy-HumanEval have longer solutions than the unselected ones in HumanEval, with 8.2 lines vs. 2.9 lines of code, respectively. Appendix A.1 provides details of our dataset.

#### 3.1.2 Buggy-FixEval

To evaluate bCC with more realistic bugs, we introduce buggy-FixEval with bCC instances constructed from CodeNet  and FixEval . FixEval is a program repair benchmark based on user submissions to competitive programming websites. Each data sample in FixEval can be viewed as a pair of submitted programs from the same user, with the accepted submission being regarded as reference or fixed and the preceding rejected submission being regarded as buggy.

To create buggy-FixEval, we match and pair each problem with its problem statement found in CodeNet and omit the problems with no match. For each submission pair, we identified potential bugs as the differences between the rejected and accepted submissions. We ensure that the rejected submission contains no syntax error and fails at least one test case. We split the solutions into halves and regard the prefix from the rejected solution as containing potential bugs and the prefix from the accepted solution as a reference. To guarantee the differences between the buggy prefix and the reference prefix related to potential bugs, we impose a limit on the character-level edit distance between them, ignoring comments and white spaces. The lower limit ensures that the differences are not comments or white spaces. The upper limit (\(20\), chosen by manual inspection) reduces the chance that the difference is related to a reformatting or a re-implementation. We then manually inspected all the remaining pairs and excluded undesirable cases such as that the reference prefix itself is already a correct solution or that the two prefixes are semantically equivalent. Finally, we execute the concatenation of the buggy prefix and the reference completion and ensure that it fails at least one test case. More details about creating buggy-FixEval can be found in Appendix A.2.

### Baseline Methods for bCC

Beyond the naive generation from code context, we study several methods for augmenting Code-LLMs to utilize the partial code better while mitigating the effect of potential bugs. Assuming a bug detection component is equipped, we focus on simple and modular-design approaches that do not require additional training and are flexible for further incorporating newly developed Code-LLMs.

Removing partial code, then completing (removal-then-completion).We bypass the negative effect of buggy partial code by removing the entire content code fragment from the model input. In particular, for buggy-HumanEval, the input to Code-LLMs after removal consists of the problem statement and the function header (and examples). For buggy-FixEval, we keep only the statement and the code fragment used for reading input data. Intuitively, as removal-then-completion guarantees that the input to Code-LLMs contains no bug, we expect this method to perform better than the naive completion. However, the drawback of removal-then-completion is its sacrifice of all potentially useful information brought by the partial code.

Completing first, then rewriting the program (completion-then-rewriting).This approach attempts to fix the buggy code using pre-trained code-repair models [31; 32; 25], _e.g.,_ neural translating a buggy program into a valid program . As code-repair models, or code fixers, are usually trained to fix the complete programs, we first complete the code by naive completion, then apply code fixers on the potentially buggy programs. We use RealiT , a SotA program repair model as the code fixer. RealiT is designed for misused variables, wrong literal, wrong binary, and unary operators, which is especially suitable for the bugs introduced in the buggy-HumanEval.

Rewriting the partial code, then completing (rewriting-then-completion).This approach attempts to resolve potential bugs in the partial code before completing. To do so, we first locate code lines containing the potential bug, then rewrite these lines. To detect potential bugs, we propose a likelihood-based measure to identify the line most likely to contain a potential bug. In particular, we score each code line with the following procedure. First, for each token, we define its buggy score to be the difference in likelihoods between the token with the highest likelihood (_i.e._, the argmax token) and the observed token. The buggy score for each line is then calculated by taking either the maximum or average of non-zero buggy scores of its tokens. The line with the highest score is most likely to contain the potential bugs. We use the InCoder-6B  as the infilling language model for rewriting code. We provide a detailed explanation and example illustration of likelihood-based measures in Appendix B to understand our rewriting-then-completion method better.

### Evaluation Metrics

We measure the functionality of a completed program by executing it against the provided test cases. Following the recent code-generation works (_e.g._, 10, 34, 12, 33), we measure the pass@k (\(\)) for each bCC instance as pass@\(k 1-/\) where \(n\) completions are sampled from a model, and \(c\) of them pass all the tests. We choose \(n=100\) and \(k=1,10,100\). This metric estimates the probability that any of \(k\) samples from the model passes the tests. As multiple bCC instances may be derived from the same programming problem, we first average the pass@\(k\) within each problem, then average across all problems (macro-average) to avoid the dominance of a few problems.

While passing all test cases does not 100% guarantee the correctness of a program, this metric provides a practical and efficient proxy for functional correctness. Note that we do not use match-based metrics, _e.g._, exact match or CodeBLEU (35) because they do not properly reflect the functionality of generated code (22; 10; 18), and no reference completion is available for buggy prefixes.

## 4 Experiments

We design two experiment sets to investigate (1) how well the existing Code-LLMs adapt to bCC (Sec. 4.2) and (2) whether we can have a simple fix with Code-LLMs for bCC (Sec. 4.3). We provide ablation studies on potential bugs (Sec. 4.5) and on combining buggy-based completion with reference prefix (Sec.C.4). Sec. 4.6 presents case studies about interesting behaviors of Code-LLMs under bCC, with additional results in Appendix C.

### Experiment Settings

Code-LLMs.We evaluate the two popular and open-sourced Code-LLMs for code completion. CodeGen is a family of LLMs trained on both natural and programming language corpora with high performance in code generation on HumanEval (10). We use released model checkpoints: CodeGen-350M-mono, CodeGen-2B-mono, and CodeGen-16B-mono. InCoder models are trained with a causal masking objective, allowing them to fill blocks of code conditioned on arbitrary left and right contexts. We use the released model checkpoints InCoder-1B and InCoder-6B, each with 1B and 6B parameters. We select CodeGen and InCoder since they are publicly available with high performance of code generation.

Generating completions._Input format._ For buggy-HumanEval, a model is expected to complete an unfinished function. Following the HumanEval benchmark (10), we set the models' input as the partial code leading up to the completion location, with the problem description embedded as the docstring of the unfinished function. For buggy-FixEval, the completion task is to complete an unfinished program with inputs being the problem description as a file-level docstring (quoted within triple quotation marks), followed by the partial code leading up to the completion location. _Generation and sampling details._ Following the best-performing settings reported in the corresponding works (12; 33), we use temperature sampling with temperature = 0.6 for CodeGen and top-\(p\) sampling with \(p\) = 0.95 and

    &  &  &  \\   & & CODEGEN- & INCOER- & CODEGEN- & INCOER- & INCOER- \\   & & 350M & 2B & 1B & 6B & 350M & 2B & 1B & 6B \\  reference & completion & 43.0 & 54.9 & 41.1 & 50.6 & 27.6 & 37.8 & 24.1 & 32.3 \\   & completion & 0.7 & 3.1 & 0.5 & 1.0 & 2.4 & 4.3 & 1.2 & 1.8 \\   & removal-then-completion & 4.7 & 9.3 & 3.9 & 6.9 & **3.3** & **8.6** & **2.9** & **6.4** \\   & rewriting-then-completion & 14.1 & **24.9** & 9.1 & 16.4 & 2.4 & 7.2 & 2.6 & 5.1 \\   & completion-then-rewriting & **22.5** & 23.6 & **22.7** & **25.2** & 2.3 & 4.7 & 1.7 & 3.0 \\   

Table 1: Pass@1 (\(\)) of completion methods on buggy-HumanEval and buggy-FixEval datasets. For all Code-LLMs, all three proposed methods improve the completion performance of the naive completion. On average, completion-then-rewriting and rewriting-then-completion achieve the best scores on buggy-HumanEval and buggy-FixEval, respectively. Nevertheless, there are still substantial gaps between these methods and completion with reference prefixes. _*Best methods in each column are in **bold**. Table 7 in Appendix C provides results with a larger model (CodeGen-16B-mono).

temperature = 0.2 for InCoder. Based on the reference solutions and computing efficiency, we set the maximum length limit for outputs from 200 to 600 tokens, varying with the problem sets. We observed similar performance trends between the tested models across different length settings. We post-process the output string following the same procedure used in their code releases.

Code-repair and code-rewriting models.For removal-then-completion, we use the latest model of RealiT  as the code-fixer, which is trained and tested over artificial and realistic bugs. For the code-rewriting model, we use the InCoder-6B model  due to its ability to code infilling and apply the similar settings used for infilling reported in the model's paper .

### How Well Do Existing Code-LLMs Perform on Buggy-Code Context?

We evaluate the latest Code-LLMs for buggy-code completion on buggy-HumanEval and buggy-FixEval, shown in Figure 2. In particular, the performance is measured in terms of pass@1, and we use four models: CodeGen-350M-mono, CodeGen-2B-mono, InCoder-1B, and InCoder-6B. First, comparing the scores between reference and buggy partial code for each model, we see that the presence of potential bugs is detrimental to the completion models, with pass@1 drops from 41.1-54.9% to 0.5-3.1% over buggy-HumanEval, and from 24.1-37.8% to 1.2-4.3% over buggy-FixEval. Moreover, the pass@1's upon buggy partial code is universally dominated by those without partial code: 3.9-9.3% over buggy-HumanEval, 2.9-8.6% over buggy-FixEval. Table 7 (Appendix C) shows the similar findings with a very large model (CodeGen-16B-mono). These results indicate that (i) the tested Code-LLMs drastically fail at bCC instantiated by our datasets, and (ii) the presence of potential bugs destroys the benefit brought by the partial code.

Why do Code-LLMs fail at bCC?We manually inspect samples from the best-performing CodeGen-2B-mono model and find the two most common failure modes among failed samples. First, the model fails to react to the potential bugs (_i.e.,_ common completions remain the same), as shown in Figure 8. This mode happens in 90% of instances and 93% of problems with at least one failed instance. We conjecture that the model is not sensitive to and thus ignores minor code changes and/or chooses to default to common patterns in the training data. Secondly, the model fails to bypass the potential bugs, likely because such patterns are rare in high-quality code. In other words, the model might have recognized the potential bugs and significantly changed the output distribution but still failed. Figure 7 illustrates an example of this case. We provide further details in Appendix D.2.

### How Effective Are Baseline Completion Methods Against Potential Bugs?

We evaluate the completion methods introduced in Section 3.2 using the same bCC setting, shown in Table 1. We include results of completing reference partial code to see how potential bugs affect the Code-LLMs. Figure 6 in Appendix C provides full results and interpretations for \(k=1,10,100\).

The effect of proposed completion methods.All three proposed methods outperform the naive buggy-code completion baseline. On buggy-HumanEval, we observe the general trend of completion-then-rewriting outperforming rewriting-then-completion, which outperforms removal-then-completion. Note that these scores of removal-then-completion are generally lower than reported scores of the similar method [12; 33] probably because buggy-HumanEval is derived from a relatively more challenging subset of HumanEval (see Section 3.1.1). On buggy-FixEval, we observe removal-then-completion to outperform rewriting-then-completion, which outperforms completion-then-rewriting. The performance gap increases as the size of the completion model rises. Similar comparison trends are observed for pass@k for \(k=10,100\). Nevertheless, performance gaps remain significant between the best method and the completion from the reference code for all settings.

The effect of Code-LLMs' capacity.For each type of code completion model, the larger version performs better than the smaller version using the same method. For instance, CodeGen-2B-mono outperforms CodeGen-350M-mono for all settings in two datasets. Compared to CodeGen, InCoder models, in general, obtain better pass@1 but worse pass@10 and pass@100. This suggests that CodeGen models generate more diverse completions, while InCoder models generate more precise completions. Furthermore, InCoder is more sensitive to buggy partial code than CodeGen models, evidenced by the lower scores from naive bCC.

Synthetic bugs versus real bugs.Among the two bCC datasets, we observe that the overall performance of mitigation methods is better on buggy-HumanEval than buggy-FixEval. This indicates the difficulty of realistic potential bugs in buggy-FixEval: There may be multiple bugs; bugs may be potentially mixed with non-bug-fixing changes; and bugs are more nuanced than single operator changes. Furthermore, while achieving the best performance in most cases, completion-then-rewriting only shows marginal differences from other methods when using larger models on buggy-FixEval.

Take-away:_Our baseline methods improve the completion for all evaluated Code-LLMs. However, the remaining performance gaps to the completion with reference partial code are still large._

### What If Partial Code Does Not Have Potential Bugs?

As shown in Table 2, mitigation methods for bCC may harm completion from reference code context (removal-then-completion is the same for both settings, thus not listed.) This suggests that a general code completion should consider both cases when potential bugs may and may not exist.

With our baselines, we can use the thresholding approach for detecting potential bugs. For instance, in rewriting-then-completion, a token is only considered a potential bug if its likelihood gap to the argmax token is beyond a threshold (between 0 and 1). Table 3 compares pass@1 scores of rewriting-then-completion varying thresholds on buggy-FixEval. We can see that the threshold of 0.9 can help achieve a relatively good balance for the two cases. A similar approach can be applied to completion-then-rewriting, as RealiT provides the probability of a token being a bug.

_Remark 4.1_ (_Imbalance in real distributions_).: We note that distributions of natural code bugs are usually imbalanced as practical bugs occur infrequently . However, potential bugs may occur more frequently as our target setting is a work-in-progress code scenario  rather than high-quality code of popular open-source projects in previous studies. Nevertheless, for comparing methods, we evaluate more balanced data to avoid the dominant effect of performance on reference code.

### Analysis of Effect from Bug and Split Locations

To understand how bug location and context size affect completion, we aggregate results by the location of potential bugs and the location of partial code splits within buggy-HumanEval. The locations are normalized as (potential bug line \(\#\))/(\(\#\) lines) and (split line \(\#\))/(\(\#\) lines), where potential bug line \(\#\), split line \(\#\), and \(\#\) lines are the number of lines starting from the function header to the line containing the potential bug, to the end of the partial code, and to the end of the canonical solution.

Figure 3 presents heatmaps of pass@1 scores (averaged over the bCC instances falling into each cell) evaluated on the CodeGen-2B-mono with naive completion on reference HumanEval, and naive

   Dataset &  &  \\   & 2B-mono &  &  &  \\  & reference & buggy & reference & buggy & reference & buggy \\  naive completion & \(54.9\) & \(3.1\) & \(43\) & \(0.7\) & \(37.8\) & \(4.3\) & \(27.6\) & \(2.4\) \\  rewriting-then-completion & \(49.6\) & \(24.9\) & \(35.9\) & \(14.1\) & \(37.0\) & \(7.2\) & \(26.4\) & \(2.4\) \\ completion-then-rewriting & \(27.7\) & \(23.6\) & \(22.2\) & \(22.5\) & \(19.4\) & \(4.7\) & \(3.7\) & \(2.3\) \\   

Table 2: Comparing pass@1 (\(\)) of completion methods on reference and buggy partial code. While the proposed mitigation methods achieve better performances than naive completion with buggy prefixes, they may harm the completion when no bug exists (reference).

  
**buggy-FixEval** &  \\  partial-code setting & \(0\) & \(0.3\) & \(0.9\) & \(1\) \\  reference & \(8.6\) & \(14.6\) & \(37.0\) & \(37.8\) \\ buggy & \(8.6\) & \(7.1\) & \(7.2\) & \(4.3\) \\   

Table 3: Balancing rewriting-then-completion for the buggy and reference settings with bug-detection thresholding. Completion performances can be adjusted via varying bug-detection thresholds from 0 (fix all partial codes) to 1 (keep all partial codes). Results are with CodeGen-2B-mono.

completion, completion-then-rewriting, and rewriting-then-completion (max) on buggy-HumanEval. First, the model performs better given longer partial code in the reference case (Figure 2(a)). While the naive completion performs overall poorly with potential bugs (Figure 2(b)), it performs relatively well when potential bugs appear on or near the last line of the partial code (along the diagonal). More interestingly, rewriting-then-completion achieves higher scores when potential bugs appear later (Figure 2(d)), while completion-then-rewriting performs better with longer code prefixes (Figure 2(c)). We suspect that a longer prefix makes completion models less likely to deviate from the reference completion. Hence, as inputs to the subsequent rewriting model, the completed code better resembles input types for which the rewriting model is trained, making the repairing more likely to succeed.

### Case Studies

Are potential bugs always harmful?As discussed in Section 2, potential bugs do not guarantee the completed code to be buggy. While we observe performance degradation of Code-LLMs under the presence of potential bugs, we find several interesting cases where the models manage to generate correct code. For instance, Figure 12 in Appendix shows that for the potential bug == (highlighted) modified from!=, the completion model deviates its original algorithmic flow with continue command and completes with correct functionality, albeit different from the canonical solution. This is an example that some bCC cases are recoverable and that Code-LLMs can adapt to them.

When do Code-LLMs succeed at bCC?For the successful cases of naive completion at bCC, we observe that either the model (i) ignores the incorrect state and generates the correct completion or (ii) takes into account the potential bug to generate a completion adapting to it. Figure 9 in Appendix shows an example when Code-LLMs ignore the if-else statement to bypass the potential bug. Further case studies and more elaborate discussions are in Appendix D.

## 5 Related Work

Code completion.Code completion provides code suggestions based on a given context [39; 40; 38]. The scope of completion ranges from the next token or line , method and class names , to the entire of the function  or program. Early works [5; 43; 4] viewed code as sequences of tokens and applied statistical language models to the problem, along with other attempts at building probabilistic models of code [44; 41; 45]. Later works adopted deep neural networks [8; 46] and pre-training techniques [9; 47] for better code modeling and completion. Our work considers the code completion setting at the function and program levels and focuses on using large language models for completion. Beyond sequence modeling, recent works considered integrating code prior knowledge via abstract syntax trees [6; 8; 48], code token types , graph structures , hierarchical context , or generating the sketch , or even extending the task's information beyond the given input files [51; 52]. We focus on using only the task description and partial code as input prompts to the model, allowing the use of more Code-LLMs and publicly available datasets.

Figure 3: **Average pass@1 scores by bug and code split locations using CodeGen-2B-mono on buggy-HumanEval. _Left to right_: (a) naïve completion on reference code, (b), (c), (d) naive completion, completion-then-rewriting, and rewriting-then-completion on buggy code. Locations are normalized by the length of reference solutions. For each split location in (a), the scores may vary across potential bug locations as the buggy-instance distribution is non-uniform across bug locations.**

Automatic program repair.The research on automatic program [53; 54; 55; 56; 57; 58; 59; 25] relieves developers from the enormous effort of finding and fixing programming bugs. Recently, Code-LLMs have been adapted for program repair by translating buggy programs into their reference counterparts [60; 31; 61]. Among those, we use RealiT  as our repair model in the completion-then-rewriting method since they obtain the SotA results and utilize similar simple mutations during training. Despite the similarity, code repair commonly targets fixing bugs from _complete_ programs while we study potential bugs from partial code. To enrich the amount of data for program repair, methods have been proposed to synthesize artificial bugs through code mutants [62; 63; 64; 65; 25] or learning to create bugs . Similarly, we employ code mutants to create artificial bugs.

Relation to adversarial examples.Adversarial examples are instances where small perturbations to the input lead the model to change into wrong predictions. They have been extensively studied in computer vision [67; 68] and natural language processing . Recent works suggested similar situations for code-learning models, where small, semantic-_preserving_ code transformations led to performance degradation [70; 71]. Buggy-code completion can be seen as a dual problem to adversarial examples, where we expect the model to adjust its predictions up on small semantic-_altering_ changes in the input. In our case, sensitivity is not a problem, but _in_sensitivity is.

Benchmarks for buggy-code completion.Multiple benchmarks have been studied for code completion and program repair. For _code completion_, CodeXGLUE , CodeNet , and HumanEval  are widely used. CodeXGLUE contains corpora of Java and Python programs for completion but only supports match-based evaluation. CodeNet collects programming problems from online judge sites, with both solutions and test cases. HumanEval can be considered a Python-function completion dataset, with the context being the problem statement and function header. We derive our datasets from HumanEval and CodeNet datasets. For _neural program repair_, many datasets require the match-based evaluation [24; 26] or focus on the compiler errors [23; 25], which are different from our setting. While IntroClass , QuixBugs , Defects4J , or Refactory  provide the test suites for evaluation, their test does not reflect the real-world bugs or lacks context support for the use with Code-LLMs . FixEval  is recently proposed as a new context-aware program repair dataset to mitigate these limitations, with many problems derived from the real submitted programs. However, as FixEval does not provide the problem statement and focuses solely on the program repair, we derive a new benchmark using FixEval and its source of problems - CodeNet .

## 6 Discussion and Conclusion

Limitations.Our baseline methods developed for bCC may degrade the completion performance on reference code context, as shown in Section 4.4, suggesting the need for balancing the buggy and reference settings in solutions to bCC. Furthermore, while buggy-FixEval is associated with real-world coding contest programs, it is unclear how closely buggy-FixEval aligns to the general software development setting where obtaining a test suite and proper evaluation are more challenging.

Impact and applications.As our work focuses on the less refined and more error-prone work-in-progress code, the code context should be viewed as a hint of user intent rather than a high-quality "gold" implementation. It thus naturally follows that a pair programmer or a smart tool should suggest a change to the draft code rather than blindly continue it if they believe a certain part of the existing draft is not intended. From a user experience perspective, an IDE can display code change suggestions to a user's existing code if such parts are identified. Similar functionality already exists for other types of code change suggestions, _e.g.,_ spelling correction and missing imports.

Conclusion.We introduce and define the buggy-code completion problem, inspired by the practical coding scenario where one completes a coding program given the problem statement and a partial code with potential bugs. We construct two new datasets, buggy-HumanEval and buggy-FixEval, as task benchmarks and find that the presence of potential bugs significantly degrades the completion performance of all evaluated large language models of code. Our further investigation of completion methods for Code-LLMs in dealing with potential bugs shows that completing with potential bugs remains challenging despite augmenting models with external program-repair models. We provide extensive ablation and case studies for further understanding and analysis of buggy-code completion setting. We hope our novel and systematic study paves the way for future works in understanding and improving the usability of Code-LLMs under practical software-development settings.

[MISSING_PAGE_FAIL:11]

* Austin et al.  Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.
* Haque et al.  Md Mahim Anjum Haque, Wasi Uddin Ahmad, Ismini Lourentzou, and Chris Brown. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. _arXiv preprint arXiv:2206.07796_, 2022.
* Raychev et al.  Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees. _ACM SIGPLAN Notices_, pages 731-747, 2016.
* Allamanis and Sutton  Miltiadis Allamanis and Charles Sutton. Mining source code repositories at massive scale using language modeling. In _2013 10th Working Conference on Mining Software Repositories (MSR)_, pages 207-216. IEEE, 2013.
* Hendrycks et al.  Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Gupta et al.  Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c language errors by deep learning. In _Thirty-First AAAI conference on artificial intelligence_, 2017.
* Tufano et al.  Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. An empirical study on learning bug-fixing patches in the wild via neural machine translation. _ACM Transactions on Software Engineering and Methodology (TOSEM)_, 28(4):1-29, 2019.
* Yasunaga and Liang  Michihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In _International Conference on Machine Learning (ICML)_, 2021.
* Huq et al.  Faria Huq, Masum Hasan, Md Mahim Anjum Haque, Sazan Mahbub, Anindya Iqbal, and Toufique Ahmed. Review4repair: Code review aided automatic program repairing. _Information and Software Technology_, 143:106765, 2022.
* de Goues et al.  Claire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu, Stephanie Forrest, and Westley Weimer. The manybugs and introclass benchmarks for automated repair of c programs. _IEEE Transactions on Software Engineering_, 41(12):1236-1256, 2015.
* Lin et al.  Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multilingual program repair benchmark set based on the quixey challenge. In _Proceedings Companion of the 2017 ACM SIGPLAN international conference on systems, programming, languages, and applications: software for humanity_, pages 55-56, 2017.
* Hu et al.  Yang Hu, Umair Z Ahmed, Sergey Mechtaev, Ben Leong, and Abhik Roychoudhury. Refactoring based program repair applied to programming assignments. In _2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)_, pages 388-398. IEEE, 2019.
* Puri et al.  Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. Codenet: A large-scale AI for code dataset for learning a diversity of coding tasks. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Wang et al.  Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. _arXiv preprint arXiv:2109.00859_, 2021.
* Richter and Wehrheim  Cedric Richter and Heike Wehrheim. Can we learn from developer mistakes? learning to localize and repair real bugs from real bug fixes. _arXiv preprint arXiv:2207.00301_, 2022.

*  Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. _arXiv preprint arXiv:2204.05999_, 2022.
*  Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022.
*  Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. _arXiv preprint arXiv:2009.10297_, 2020.
*  Jingxuan He, Luca Beurer-Kellner, and Martin Vechev. On distribution shift in learning-based bug detectors. In _International Conference on Machine Learning_, pages 8559-8580. PMLR, 2022.
*  Rafael-Michael Karampatsis and Charles Sutton. How often do single-statement bugs occur? the manystubs4j dataset. In _Proceedings of the 17th International Conference on Mining Software Repositories_, MSR '20, page 573-577, New York, NY, USA, 2020. Association for Computing Machinery.
*  Xuechen Li, Chris J. Maddison, and Daniel Tarlow. Learning to extend program graphs to work-in-progress code, 2021.
*  Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from examples to improve code completion systems. In _Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering_, pages 213-222, 2009.
*  Sebastian Proksch, Johannes Lerch, and Mira Mezini. Intelligent code completion with bayesian networks. _ACM Transactions on Software Engineering and Methodology (TOSEM)_, 25(1):1-31, 2015.
*  Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class names. In _Proceedings of the 2015 10th joint meeting on foundations of software engineering_, pages 38-49, 2015.
*  Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Attandilian. Productivity assessment of neural code completion. In _Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming_, pages 21-29, 2022.
*  Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In _Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering_, pages 269-280, 2014.
*  Miltiadis Allamanis and Charles Sutton. Mining idioms from source code. In _Proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering_, pages 472-483, 2014.
*  Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: probabilistic model for code. In _International Conference on Machine Learning_, pages 2933-2942. PMLR, 2016.
*  Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In _International conference on machine learning_, pages 245-256. PMLR, 2020.
*  Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intellicode compose: Code generation using transformer. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 1433-1443, 2020.
*  Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees to transformers. in 2021 ieee/acm 43rd international conference on software engineering (icse), 2021.

*  Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. Pymt5: multi-mode translation of natural language and python code with transformers. _arXiv preprint arXiv:2010.03150_, 2020.
*  Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt, and Miltiadis Allamanis. Learning to complete code with sketches. In _International Conference on Learning Representations_, 2021.
*  Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. ReACC: A retrieval-augmented code completion framework. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6227-6240, 2022.
*  Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis. Better context makes better code language models: A case study on function call argument completion. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023.
*  Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. _arXiv preprint arXiv:1711.00740_, 2017.
*  Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural program repair by jointly learning to localize and repair. _arXiv preprint arXiv:1904.01720_, 2019.
*  Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational models of source code. In _International conference on learning representations_, 2019.
*  Dobrik Georgiev, Marc Brockschmidt, and Miltiadis Allamanis. Heat: Hyperedge attention networks. _arXiv preprint arXiv:2201.12113_, 2022.
*  Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noel Pouchet, Denys Poshyvanyk, and Martin Monperrus. Sequencer: Sequence-to-sequence learning for end-to-end program repair. _IEEE Transactions on Software Engineering_, 47(9):1943-1959, 2019.
*  Yi Li, Shaohua Wang, and Tien N Nguyen. Dflix: Context-based code transformation learning for automated program repair. In _Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering_, pages 602-614, 2020.
*  Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic feedback. In _International Conference on Machine Learning_, pages 10799-10808. PMLR, 2020.
*  Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. _arXiv preprint arXiv:2103.06333_, 2021.
*  Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Pre-trained contextual embedding of source code. _ArXiv_, abs/2001.00059, 2019.
*  Jibesh Patra and Michael Pradel. Semantic bug seeding: a learning-based approach for creating realistic bugs. In _Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 906-918, 2021.
*  Cedric Richter and Heike Wehrheim. Learning realistic mutations: Bug creation for neural bug detectors. In _2022 IEEE Conference on Software Testing, Verification and Validation (ICST)_, pages 162-173. IEEE, 2022.
*  Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. Learning how to mutate source code from bug-fixes. In _2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)_, pages 301-312. IEEE, 2019.

*  Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. Coconut: combining context-aware neural translation models using ensemble for program repair. In _Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis_, pages 101-114, 2020.
*  Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. Self-supervised bug detection and repair. _Advances in Neural Information Processing Systems_, 34:27865-27876, 2021.
*  Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. _Ieee Access_, 6:14410-14430, 2018.
*  Samuel Henrique Silva and Peyman Najafirad. Opportunities and challenges in deep learning adversarial robustness: A survey. _arXiv preprint arXiv:2007.00753_, 2020.
*  Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial attacks on deep-learning models in natural language processing: A survey. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 11(3):1-41, 2020.
*  Jordan Henkel, Goutham Ramakrishnan, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas Reps. Semantic robustness of models of source code. In _2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)_, pages 526-537. IEEE, 2022.
*  Goutham Ramakrishnan and Aws Albarghouthi. Backdoors in neural models of source code. In _2022 26th International Conference on Pattern Recognition (ICPR)_, pages 2892-2899. IEEE, 2022.
*  Rene Just, Darioush Jalali, and Michael D Ernst. Defects4j: A database of existing faults to enable controlled testing studies for java programs. In _Proceedings of the 2014 International Symposium on Software Testing and Analysis_, pages 437-440, 2014.

[MISSING_PAGE_FAIL:16]

### Buggy-FixEval

FixEval8 is a dataset derived from CodeNet9 for studying program repair. It consists of programming problems from online programming contest websites10, each with test cases, user-submitted solutions and judging verdicts to the submissions (_e.g., accepted_ - passing all the tests; _wrong answer_ - failing at least one test; or _time limit exceeded_ - the program did not terminate within the specified time limit). The test cases in FixEval are more extensive than those in HumanEval, as the programming contest websites use them to automatically judge a submission's correctness. Each problem comes with an alphabetical label ("A" to "F") indicating the order in which the problem appears in its contest, with "A" usually being the easiest. Different from HumanEval, a solution or submission here is supposed to be a _complete_ program, reading input from and writing output to the standard input and output instead of just a function implementation.

To derive a dataset for buggy-code completion, we pair each FixEval problem with its matching problem statements from CodeNet and discard problems without a match. Among the remaining problems, we focus on the relatively easy "A"-label problems because the post-"A" problems have been reported to be very challenging to solve even for recent language models of code: pass@1 \(<\) 12% with CodeT5 under a program-repair setting [19, Section 5.3].

We start by selecting rejected-accepted solution pairs. For each streak of Python submissions from a user to a problem, if the streak ends with an _accepted_ submission, we take the last submission with the verdict being none of _accepted_, _compilation error_ or _runtime error_ as its rejected counterpart.

Then, we remove all comments and empty lines for each accepted-rejected pair and check the following. (1) Neither submission contains exit() or system calls, as they cause trouble for execution. (2) The first halves of the two submissions contain at least one non-white-space difference, including case difference, space/indent difference, or variable names for the assignment operator. (3) The character-level edit distance between the first halves of the two submissions is smaller than 20. Regarding the threshold of 20, we sampled 100 pairs of submissions with various edit distances. We qualitatively verified that under this threshold, the differences between the two submissions are, in the majority of cases, focused semantic changes (or likely bug fixes) and not refactoring or re-implementations.

Figure 4: **Example of a buggy-FixEval instance**. The example is based on p02546.

The above process resulted in 492 pairs of prefixes from rejected-accepted solutions from 94 "A"-level problems. We manually inspected all the pairs to ensure the pairs satisfy the desiderata described in Section 3.1. Namely, we identify the following cases.

1. The prefix from the accepted solution is already a complete program that passes all the test cases. This can happen when the accepted solution is very short, for example, fewer than three lines, and/or followed by comments or unnecessary code. This counts for 26 pairs.

Figure 5: **Example of buggy-HumanEval instance. The example is based on HumanEval/0. The buggy prompt (Figure c) is directly derived from the original prompt (Figure a) and the canonical solution (Figure b). We reuse the test suite (Figure d) for our evaluation.**

2. The prefix from the rejected solution and the prefix from the accepted solution are semantically equivalent up to variable name normalization. This counts for 33 pairs.
3. The prefix from the rejected solution contains no control flow (_e.g.,_ if, for, while) and no output clause. In this case, a completion can ignore the prefix. This counts for 141 pairs.
4. The prefix from the rejected solution is bound to output wrong answers upon a certain input. For example, it contains a print() statement within a wrong condition. This counts for 25 pairs.

A list of identified cases can be found in our code repository. Case 1 makes the completion trivial from reference prefixes. Case 2 makes the comparison between the buggy completion and reference completion less meaningful. Case 3 makes the buggy completion less challenging. Thus, we exclude instances identified as cases 1, 2, and 3. We keep the case 4 instances because they fit the definition of potential bugs and are especially interesting in the sense that the "bugginess" already manifested in the code prefix.

After the above step, we are left with 292 pairs. We then use the first half of the rejected submission as a piece of buggy partial code and the first half of the accepted submission as the corresponding piece of reference code for buggy-code completion. A "buggy" prompt is formed by prepending the problem statement as a file-level docstring, or a multi-line string literal, to the buggy partial code. See an example in Figure 4.

## Appendix B Details of rewriting-then-completion Method

With rewriting-then-completion, we attempt to eliminate potential bugs via a two-step procedure. We first locate the most likely line to contain a potential bug using a likelihood-based measure and then rewrite the line using an infilling code language model.

### Line Selection

Our underlying idea is to treat potential bugs as outliers in the generation flow of Code-LLMs. We observe that most reference code has lower perplexity than the corresponding buggy code.

Likelihood-based measures.We calculate the score of each line as follows: With a model that can give a token distribution for each token location, we define the token score as the difference between the likelihoods of the most probable token (i.e., the argmax token) and the actual observed token. We get the score of a line by taking either the maximum or the average of all the non-zero token scores within it. The line with the largest score is finally selected to be rewritten.

To help better understand likelihood-based measures, consider the problem and the code prefix with potential bugs shown in Figure 8. The partial code has four lines in the function body. Given a language model \(G\), for each token \(x\), we calculate the token-level score as \(p_{2}-p_{1}\):

* \(p_{1}\): probability of generating \(x\) from the code up to \(x\)
* \(p_{2}\): probability of generating \(x^{*}\) from the code up to \(x\), where \(x^{*}\) gives the highest probability according to \(G\).

For example, consider line 3 and token ==. We would have \(p_{1}=0.01\) (probability of \(G\) generating ==), and \(p_{2}=0.95\) (1= is the most probable token according to \(G\)). The score of == is then 0.94. We obtain the line's score by taking the maximum scores at all token locations in the line. This way of aggregating token scores to line scores is referred to as "Likelihood (Max.)" Below is the variation used to report results in the main text.

Note that we take two measures to reduce the uncertainties of the likelihood estimation: (i) instead of the likelihood score of the target token itself, we use the maximal margin between the target token and the argmax token (similar to the popular approaches used in uncertainty quantification), and (ii) we aggregate the score gap along the line and set a high threshold to be more conservative.

We find that accuracies of localizing the line of potential bugs (with the same setting described above) are approximately 82% and 53% respectively on buggy-HumanEval and buggy-FixEval. For buggy-FixEval, we compare the detected line with the line of the first semantic difference between buggy and reference prefixes.

Heuristic oracle for comparison.To see how well our likelihood-based measures work, we compare them against a heuristic oracle for predicting the line of potential bugs. In particular, we compare the buggy code prefix against the corresponding reference code prefix and select the first line with non-trivial differences. The differences are not about space, indent, or comment. Since this requires access to the corresponding reference code prefix, it cannot be used as part of a solution for bCC. We thus refer to it as an "oracle" and only use it here as a method of reference to gauge the performance of other line-selection methods. We do not use this oracle in any methods reported in the main text.

### Rewriting

After identifying the line most likely to contain a potential bug, we rewrite it by masking it and generating a replacement line using a code-infilling model such as InCoder.

### Results

Shown in Table 6 is our comparison between different likelihood-based measures. We observe that the Heuristic Oracle substantially outperforms the likelihood-based methods in buggy-HumanEval, but the reverse is true for buggy-FixEval. We provide a likely explanation for this phenomenon in the next paragraph. We also observe that Likelihood (Max.) outperforms Likelihood (Avg.) on average on buggy-HumanEval, but the reverse is true for buggy-FixEval. A possible explanation is that since the potential bugs in buggy-HumanEval take the form of a single token change, they are more amendable to being detected by focusing on the token with the largest score (maximum aggregation). In such a scenario, taking the average across all tokens in the line might dilute this signal. On the other hand, potential bugs are likely more subtle in a natural setting (represented by buggy-FixEval), and the potential bug may be spread out across multiple tokens. The fact that the average aggregation considers this may explain why it outperforms the maximum aggregation on buggy-FixEval.

We note that a large gap in performance between buggy-HumanEval and buggy-FixEval for the Heuristic Oracle method. This may be justifiable based on the information known to the Heuristic Oracle. In buggy-HumanEval, we introduce the bugs synthetically via a single semantically altering operator change. Hence, the oracle method has guaranteed knowledge of the line where the bug was introduced. This is not the case in buggy-FixEval since the examples were constructed directly from human submissions without synthetic perturbations. Therefore, the line corresponding to the first difference between these two submissions may not correspond to the bug location, or multiple (instead of one) locations can change between these two submissions. Therefore, this method is closer to a heuristic than a true oracle for buggy-FixEval. This is reflected in Heuristic Oracle's substantially weaker performance on FixEval. The fact that likelihood-based methods outperform the Heuristic Oracle on buggy-FixEval supports this explanation.

    &  &  \\   & & CG-350M & CG-2B & InCoder-1B & InCoder-6B \\   & Heuristic Oracle & **22.1** & **29.6** & **21.4** & **28.5** \\  & Likelihood (Max.) & 14.1 & 24.9 & \(9.1\) & \(16.4\) \\  & Likelihood (Avg.) & \(13.2\) & \(23.0\) & 9.8 & 16.9 \\   & Heuristic Oracle & \(1.7\) & \(3.3\) & \(1.2\) & \(2.2\) \\  & Likelihood (Max.) & \(2.4\) & \(7.2\) & **2.6** & \(5.1\) \\   & Likelihood (Avg.) & **2.7** & **7.9** & \(2.4\) & **5.4** \\   

Table 6: Pass@1 (\(\)) of each selection and completion method on the buggy-HumanEval and buggy-FixEval datasets. The best results for each dataset and completion model are in **bold**. The better likelihood aggregation method is underlined.

## Appendix C Full Definition Statement and Additional Results

### Full Statement for Definition 2.1 in Section 2

**Definition C.1** (potential bug in a code prefix).: Assume a problem given by specification \(h\) with the test \(f_{h}\) for evaluating its functional correctness and reference partial code \(s\). Let \(_{s}^{h}\) be the set of all valid programs, or _solutions_ that satisfy \(h\) and have the prefix \(s\), _i.e.,_\(_{s}^{h}=\{t:=s::c|f_{h}(t)=1\}\). Let \(_{s}^{h}\) be the set of all valid completed code, _i.e.,_\(_{s}^{h}=\{c|s::c_{s}^{h}\}\). A potential bug of \(s\) is an edit \(g_{e}\) on a token of \(s\) that causes at least one existing solution to fail, _i.e.,_ for \(s^{}=g_{e}(s)\), there exists \(c_{s}^{h}\) s.t. \(f_{h}(s^{}::c)=0\). The prefix \(s^{}\) is called _buggy_ with respect to reference prefix \(s\).

### Full Results with \(k=1,10,100\) and Interpretation for Section 4.3

Figure 6 presents the full comparison among pass@k for \(k=1,10,100\) for all methods. Intuitively, higher \(k\) reveals more on the diversity of generated solutions. The relative performances between different Code-LLMs are consistent across different \(k\)'s. Regarding the relationship between completion methods,

* The naive completion and completion-then-rewriting methods achieve relatively small increases as \(k\) goes up, as the partial code constrains the input to the completion step.
* The increase in performance of removal-then-completion is relatively higher than naive completion when \(k\) increases, as its input is not constrained by the buggy partial code prefixes.
* The performance of rewriting-then-completion achieves relatively big increases among the methods as \(k\) goes up. This is probably because rewriting-then-completion balances the completion-then-rewriting and removal-then-completion approaches. Thus, the pre-completion rewriting step increases the performance along with the diversity of code prefixes provided in the completion step.

### Performance on CodeGen-16B-mono

We provide results for CodeGen-16B-mono in table 7. As we can see, CodeGen-16B-mono also exhibits a similar phenomenon to our observations in previous Table 1 for smaller models, _i.e.,_ potential bugs make Code-LLMs more challenging to generate correct programs and proposed mitigation methods still suffer from a large gap to the reference-prefix completion.

Figure 6: Comparison of CodeLMs per each completion method in terms of pass@k. CG: CodeGen, Comp: Completion. We see the trend consistent with CodeGen-2B-mono \(>\) InCoder-6B \(>\) CodeGen-350M-mono \(>\) InCoder-1B across methods and \(k\)’s.

### Ablation Study: Concatenating Buggy-based Completion with Reference Prefix

In this ablation study, we use the buggy prefix to complete, then replace the buggy prefix with the reference prefix, shown in the bottom row of Table 8 (reference prefix + buggy-based completion). The results indicate that Code-LLMs fail to react to the change in the buggy prefix, as discussed in Section 4.2.

## Appendix D Detailed Case Studies

For the case studies, We use the synthetic buggy-HumanEval dataset for our study. We use CodeGen-2B-mono as the completion model throughout this section as it achieved the best bCC performance overall. We surface interesting bCC examples by comparing pass rates for reference and buggy prefixes. For example, the Code-LLM may easily complete some reference prefixes (_i.e._, high pass rate) but potentially fails with buggy prefixes (zero pass rate). Other examples may have a non-zero pass rate, meaning that the Code-LLM can adapt to the potential bug in the prefix and yield a correct solution. One observation that was observed uniformly about this study is that prefixes with potential bugs typically lead to lower pass rates, even when those were non-zero. This suggests that for these prefixes, the Code-LLM has to make more effort to find a correct solution.

### Are Potential Bugs Always Harmful?

As discussed in Section 2, potential bugs do not guarantee that the completed code will be buggy. These prefixes were sourced from completed examples that had a bug that could be ascribed to the prefix, but that does not mean by itself that the prefix will be impossible to complete correctly. While we observe the performance degradation of Code-LLMs under the presence of potential bugs, we find several interesting cases where these models generate correct code nonetheless.

Figure 12 shows that for the potential bug (highlighted operator) == modified from!= in the reference code, the completion model updates its original algorithmic flow with continue command and completes with correct code. While the new completion code is different from the canonical solution of the reference code, it is functionally correct. Thus, this implies that codellms may adapt in some recoverable cases.

### Why Do Code-LLMs Fail at bCC?

We investigate the cases where our models succeed with the reference partial code but fail with the buggy partial code.

  
**Prefix** & **Method** & **buggy-HumanEval** & **buggy-FixEval** \\ reference & completion & 68.9 & 49.4 \\   & completion & 4.4 & 8.0 \\  buggy & removal-then-completion & 20.2 & **17.6** \\  & rewriting-then-completion & **24.3** & 9.7 \\  & completion-then-rewriting & 24.1 & 7.7 \\   

Table 7: Pass@1 (\(\)) of completion methods with CODEGEN-16B-Mono. *_Best method in each column is in **bold**_.

    &  & **buggy-FixEval** \\  CODEGEN- & 2B & 350M & 2B & 350M \\  reference prefix + reference-based completion & \(54.9\) & \(43\) & \(37.8\) & \(27.6\) \\ buggy prefix + buggy-based completion & \(3.1\) & \(0.7\) & \(4.3\) & \(2.4\) \\ reference prefix + buggy-based completion & \(40.8\) & \(31.6\) & \(4.6\) & \(2.5\) \\   

Table 8: Concatenating buggy-code-based completion with reference prefix.

Our first finding is that the model often fails to react to the changes in code, _i.e.,_ it produces the same completion for both buggy and reference code prefixes. We identify this by comparing if the most popular completions (out of all the sampled completions for a given instance) are the same for the reference and buggy code prefixes. We see that this happens in 90% of the instances and 93% of the problems (with at least one failed instance). Figure 7 shows an example. This suggests that the model is not sensitive to and thus ignores minor code changes and/or defaults to common patterns in the training data.

In cases where the most popular completions differ for the reference and the buggy code prefixes, we found that the potential bug often makes the code prefix significantly more challenging to complete correctly. In other words, the model may have recognized the potential bug and thus have significantly changed the output distribution but still failed. Figure 8 shows an example. Here, the balance check condition is reversed due to potential bugs. The model could ignore the given code or even define a new function to overwrite the buggy one. However, this drastically deviates from the commonly observed code patterns and thus may create a significant hurdle for the completion model if it is not introduced or adapted to such cases.

### When Do Code-LLMs Succeed at bCC?

Figure 9 shows an example when Code-LLMs ignore the if-else statement to bypass the potential bugs. In successful cases, we have observed that either the model ignores the incorrect state and generates the correct completion or considers the potential bug in the prefix to create a completion adapted to it.

Figure 7: An example case of buggy-code completion where the model reacts to the code change but still fails. The bCC instance is based on HumanEval/3. The model completions shown here are representative completions for the given prefixes.

### How Are the Proposed Completion Methods Better at bCC?

The completion-then-rewriting method can detect and fix several types of suspicious operators. Figure 10 shows an example from the string_xor problem. Here, we see that the direct completion failed to react to the potential bug and continued as if the == operator was not changed to!=, which gives an incorrect solution. However, the code-repair model was able to identify and fix the!= operator back to ==, producing a correct solution. Similarly, Figure 11 shows a success case from rewriting-then-completion, where it restores the introduced operator change. As buggy-HumanEval consists of bCC instances constructed from single operator changes, these methods can be particularly effective over naive completion in circumventing the introduced operator change.

Figure 8: An example case of bCC where the model fails to react to the code change. The bCC instance is based on HumanEval/0. The model completions shown here are the dominant completions for the given prefixes.

* Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True """

Prefix with potential bugs

``` fromtypingimportList defhas_close_elements(numbers: List[float],threshold:float) ->bool: foridx,elem in enumerate(numbers): foridx2,elem2 in enumerate(numbers): ifidx==idx2: distance=abs(elem- elem2) ```

Model completion

``` continue else: distance=abs(elem- elem2) ifdistance<threshold: returnTrue returnFalse ```

Figure 9: An example case of bCC where the model bypasses the potential bug and succeeds. The bCC instance is based on HumanEval/0. The model completion for the reference prefix is the most common. The model completion for the buggy prefix is one of the two successful completions.

* """InputaretwostringsaandbconsistingonlyofisandOs. PerformbinaryXORontheseinputsandreturnresultalsoasatstring. >>>string_xor('010','110') '100' """

Prefixwithpotentialbugs

``` fromtypingimportList defstring_xor(a:str,b:str)-> str: defxor(i,j): ifi=j: return'0' ```

Modelcompletion

``` else: return'1' return'1' ```

Problemspecification

``` """Checkifingivenlistofnumbers,areanytwonumbersclosertoeachother than giventhreshold. >>>has_close_elements([1.0,2.0,3.0],0.5) False >>>has_close_elements([1.0,2.8,3.0,4.0,5.0,2.0],0.3) True """

Prefixwithpotentialbugs

``` fromtypingimportList defhas_close_elements(numbers: List[float],threshold:float) ->bool: foridx,elemin enumerate(numbers): foridx2,elem2in enumerate(numbers): ifidx==idx2: ```

Outputfromcompletion-then-rewriting

``` fromtypingimportList defstring_xor(a:str,b:str)-> str: defxor(i,j): ifi==j: return'0' else: return'1' return'1' return'1' ```

return''.join([xor(i,j)fori,jinzip(a,b)]) ```

Figure 11: An example case of bCC, where the rewriting-then-completion method successfully fixes the potential bug. The bCC instance is based on HumanEval/0.

Figure 10: An example case of bCC where the completion-then-rewriting method successfully fixes the potential bug. The bCC instance is based on HumanEval/11. The model completion for the buggy prefix is representative.

Check 1f in given list of numbers, are any two numbers closer to each other than given threshold.

Figure 12: **A success case of buggy-code completion**. The model manages to generate a functionally correct completion by surpassing the potential bug (highlighted) by a continue command. The completions are from CodeGen-2B-mono on HumanEval/0 from buggy-HumanEval.