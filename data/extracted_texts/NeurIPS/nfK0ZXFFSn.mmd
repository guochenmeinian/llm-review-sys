# HaloScope: Harnessing Unlabeled LLM Generations

for Hallucination Detection

 Xuefeng Du\({}^{1}\)   Chaowei Xiao\({}^{2}\)   Yixuan Li\({}^{1}\)

\({}^{1}\)Department of Computer Sciences, University of Wisconsin-Madison

\({}^{2}\)Information School, University of Wisconsin-Madison

{xfdu,sharonli}@cs.wisc.edu, cxiao34@wisc.edu

###### Abstract

The surge in applications of large language models (LLMs) has prompted concerns about the generation of misleading or fabricated information, known as hallucinations. Therefore, detecting hallucinations has become critical to maintaining trust in LLM-generated content. A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data. To address the challenge, we introduce HaloScope, a novel learning framework that leverages the unlabeled LLM generations in the wild for hallucination detection. Such unlabeled data arises freely upon deploying LLMs in the open world, and consists of both truthful and hallucinated information. To harness the unlabeled data, we present an automated membership estimation score for distinguishing between truthful and untruthful generations within unlabeled mixture data, thereby enabling the training of a binary truthfulness classifier on top. Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin. Code is available at https://github.com/deeplearning-wisc/haloscope.

## 1 Introduction

In today's rapidly evolving landscape of machine learning, large language models (LLMs) have emerged as transformative forces shaping various applications . Despite the immense capabilities, they bring forth challenges to the model's reliability upon deployment in the open world. For example, the model can generate information that is seemingly informative but untruthful during interaction with humans, placing critical decision-making at risk . Therefore, a reliable LLM should not only accurately generate texts that are coherent with the prompts but also possess the ability to identify hallucinations. This gives rise to the importance of hallucination detection problem, which determines whether a generation is truthful or not .

A primary challenge in learning a truthfulness classifier is the scarcity of labeled datasets containing truthful and hallucinated generations. In practice, generating a reliable ground truth dataset for hallucination detection requires human annotators to assess the authenticity of a large number of generated samples. However, collecting such labeled data can be labor-intensive, especially considering the vast landscape of generative models and the diverse range of content they produce. Moreover, maintaining the quality and consistency of labeled data amidst the evolving capabilities and outputs of generative models requires ongoing annotation efforts and stringent quality control measures. These formidable obstacles underscore the need for exploring unlabeled data for hallucination detection.

Motivated by this, we introduce **HaloScope**, a novel learning framework that leverages _unlabeled LLM generations in the wild_ for hallucination detection. The unlabeled data is easy-to-access and can emerge organically as a result of interactions with users in chat-based applications. Imagine, for example, a language model such as GPT  deployed in the wild can produce vast quantities of text continuously in response to user prompts. This data can be freely collectible, yet often contains a mixture of truthful and potentially hallucinated content. Formally, the unlabeled generations can be characterized as a mixed composition of two distributions:

\[_{}=(1-)_{}+_{ },\]

where \(_{}\) and \(_{}\) denote the marginal distribution of truthful and hallucinated data, and \(\) is the mixing ratio. Harnessing the unlabeled data is non-trivial due to the lack of clear membership (truthful or hallucinated) for samples in mixture data.

Central to our framework is the design of an automated membership estimation score for distinguishing between truthful and untruthful generations within unlabeled data, thereby enabling the training of a binary truthfulness classifier on top. Our key idea is to utilize the language model's latent representations, which can capture information related to truthfulness. Specifically, HaloScope identifies a subspace in the activation space associated with hallucinated statements, and considers a point to be potentially hallucinated if its representation aligns strongly with the components of the subspace (see Figure 2). This idea can be operationalized by performing factorization on LLM embeddings, where the top singular vectors form the latent subspace for membership estimation. Specifically, the membership estimation score measures the norm of the embedding projected onto the top singular vectors, which exhibits different magnitudes for the two types of data. Our estimation score offers a straightforward mathematical interpretation and is easily implementable in practical applications.

Extensive experimental results on contemporary LLMs confirm that HaloScope can effectively improve hallucination detection performance across diverse datasets spanning open-book and closed-book conversational QA tasks (Section 4). Compared to the state-of-the-art methods, we substantially improve the hallucination detection accuracy by 10.69% (AUROC) on a challenging TruthfulQA benchmark , which favorably matches the supervised upper bound (78.64 % vs. 81.04%). Furthermore, we delve deeper into understanding the key components of our methodology (Section 4.4), and extend our inquiry to showcase HaloScope versatility in addressing real-world scenarios with practical challenges (Section 4.3). To summarize our key contributions:

* Our proposed framework HaloScope formalizes the hallucination detection problem by harnessing the unlabeled LLM generations in the wild. This formulation offers strong practicality and flexibility for real-world applications.
* We present a scoring function based on the hallucination subspace from the LLM representations, effectively estimating membership for samples within the unlabeled data.
* We conduct in-depth ablations to understand the efficacy of various design choices in HaloScope, and verify its scalability to large LLMs and different datasets. These results provide a systematic and comprehensive understanding of leveraging the unlabeled data for hallucination detection, shedding light on future research.

Figure 1: Illustration of our proposed framework HaloScope for hallucination detection, leveraging unlabeled LLM generations in the wild. HaloScope first identifies the latent subspace to estimate the membership (truthful vs. hallucinated) for samples in unlabeled data \(\) and then learns a binary truthfulness classifier.

## 2 Problem Setup

Formally, we describe the LLM generation and the problem of hallucination detection.

**Definition 2.1** (**LLM generation**).: _We consider an \(L\)-layer causal LLM, which takes a sequence of \(n\) tokens \(_{}=\{x_{1},...,x_{n}\}\), and generates an output \(=\{x_{n+1},...,x_{n+m}\}\) in an autoregressive manner. Each output token \(x_{i},i[n+1,...,n+m]\) is sampled from a distribution over the model vocabulary \(\), conditioned on the prefix \(\{x_{1},...,x_{i-1}\}\):_

\[x_{i}=*{argmax}_{x}P(x|\{x_{1},...,x_{i-1}\}),\] (1)

_and the probability \(P\) is calculated as:_

\[P(x|\{x_{1},...,x_{i-1}\})=*{softmax}(_{o}_{ L}(x)+_{o}),\] (2)

_where \(_{L}(x)^{d}\) denotes the representation at the \(L\)-th layer of LLM for token \(x\), and \(_{o},_{o}\) are the weight and bias parameters at the final output layer._

**Definition 2.2** (**Hallucination detection**).: _We denote \(_{}\) as the joint distribution over the truthful input and generation pairs, which is referred to as truthful distribution. For any given generated text \(\) and its corresponding input prompt \(_{}\) where \((_{},)\), the goal of hallucination detection is to learn a binary predictor \(G:\{0,1\}\) such that_

\[G(_{},)=\{1,&\ (_{},)_{}\\ 0,&.\] (3)

## 3 Proposed Framework: HaloScope

### Unlabeled LLM Generations in the Wild

Our key idea is to leverage unlabeled LLM generations in the wild, which emerge organically as a result of interactions with users in chat-based applications. Imagine, for example, a language model such as GPT deployed in the wild can produce vast quantities of text continuously in response to user prompts. This data can be freely collectible, yet often contains a mixture of truthful and potentially hallucinated content. Formally, the unlabeled generations can be characterized by the Huber contamination model  as follows:

**Definition 3.1** (**Unlabeled data distribution**).: _We define the unlabeled LLM input and generation pairs to be the following mixture of distributions_

\[_{}=(1-)_{}+_{ },\] (4)

_where \((0,1]\). Note that the case \(=0\) is idealistic since no false information occurs. In practice, \(\) can be a moderately small value when most of the generations remain truthful._

**Definition 3.2** (**Empirical dataset**).: _An empirical set \(=\{(_{}^{1},}_{1}),...,( _{}^{N},}_{N})\}\) is sampled independently and identically distributed (i.i.d.) from its mixture distribution \(_{}\), where \(N\) is the number of samples. \(}_{i}\) denotes the response generated with respect to some input prompt \(_{}^{i}\) with the tilde symbolizing the uncertain nature of the generation._

Despite the wide availability of unlabeled generations, harnessing such data is non-trivial due to the lack of clear membership (truthful or hallucinated) for samples in mixture data \(\). In a nutshell, our framework aims to devise an automated function that estimates the membership for samples within the unlabeled data, thereby enabling the training of a binary classifier on top (as shown in Figure 1). In what follows, we describe these two steps in Section 3.2 and Section 3.3 respectively.

### Estimating Membership via Latent Subspace

The first step of our framework involves estimating the membership (truthful vs untruthful) for data instances within a mixture dataset \(\). The ability to effectively assign membership for these two types of data relies heavily on whether the language model's representations can capture informationrelated to truthfulness. Our idea is that if we could identify a latent subspace associated with hallucinated statements, then we might be able to separate them from the rest. We describe the procedure formally below.

**Embedding factorization.** To realize the idea, we extract embeddings from the language model for samples in the unlabeled mixture \(\). Specifically, let \(^{N d}\) denote the matrix of embeddings extracted from the language model for samples in \(\), where each row represents the embedding vector \(_{i}^{}\) of a data sample \((_{}^{i},}_{i})\). To identify the subspace, we perform singular value decomposition:

\[_{i} :=_{i}-\] (5) \[ =^{},\]

where \(^{d}\) is the average embedding across all \(N\) samples, which is used to center the embedding matrix. The columns of \(\) and \(\) are the left and right singular vectors, and form an orthonormal basis. In principle, the factorization can be performed on any layer of the LLM representations, which will be analyzed in Section 4.4. Such a factorization is useful, because it enables discovering the most important spanning direction of the subspace for the set of points in \(\).

Membership estimation via latent subspace.To gain insight, we begin with a special case of the problem where the subspace is \(1\)-dimensional, a line through the origin. Finding the best-fitting line through the origin with respect to a set of points \(\{_{i}|1 i N\}\) means minimizing the sum of the squared distances of the points to the line. Here, distance is measured perpendicular to the line. Geometrically, finding the first singular vector \(_{1}\) is also equivalent to maximizing the total distance from the projected embedding (onto the direction of \(_{1}\)) to the origin (sum over all points in \(\)):

\[_{1}=_{\|}\|_{2}=1}_ {i=1}^{N}_{i},^{2},\] (6)

where \(,\) is a dot product operator. As illustrated in Figure 2, hallucinated data samples may exhibit anomalous behavior compared to truthful generation, and locate farther away from the center. This reflects the practical scenarios when a small to moderate amount of generations are hallucinated while the majority remain truthful. To assign the membership, we define the estimation score as \(_{i}=_{i},_{1}^{2}\), which measures the norm of \(_{i}\) projected onto the top singular vector. This allows us to estimate the membership based on the relative magnitude of the score (see the score distribution on practical datasets in Appendix B).

Our membership estimation score offers a clear mathematical interpretation and is easily implementable in practical applications. Furthermore, the definition of score can be generalized to leverage a subspace of \(k\) orthogonal singular vectors:

\[_{i}=_{j=1}^{k}_{j}_{i}, _{j}^{2},\] (7)

where \(_{j}\) is the \(j^{}\) column of \(\), and \(_{j}\) is the corresponding singular value. \(k\) is the number of spanning directions in the subspace. The intuition is that hallucinated samples can be captured by a small subspace, allowing them to be distinguished from the truthful samples. We show in Section 4.4 that leveraging subspace with multiple components can capture the truthfulness encoded in LLM activations more effectively than a single direction.

### Truthfulness Classifier

Based on the procedure in Section 3.2, we denote \(=\{}_{i}:_{i}>T\}\) as the (potentially noisy) set of hallucinated samples and \(=\{}_{i}:_{i} T\}\) as the candidate truthful set. We

Figure 2: Visualization of the representations for truthful (in orange) and hallucinated samples (in purple), and their projection onto the top singular vector \(_{1}\) (in gray dashed line).

then train a truthfulness classifier \(_{}\) that optimizes for the separability between the two sets. In particular, our training objective can be viewed as minimizing the following risk, so that sample \(}\) from \(\) is predicted as positive and vice versa.

\[ R_{,}(_{ })&=R_{}^{+}(_{})+R_{ }^{-}(_{})\\ &=_{}}\ \{_{}( }) 0\}+_{}}\ \{_{}( })>0\}.\] (8)

To make the \(0/1\) loss tractable, we replace it with the binary sigmoid loss, a smooth approximation of the \(0/1\) loss. During test time, we leverage the trained classifier for hallucination detection with the truthfulness scoring function of \(S(^{})=_{}(^{})} }{1+e^{_{}(^{})}}\), where \(^{}\) is the test data. Based on the scoring function, the hallucination detector is \(G_{}(^{})=\{S(^{})\}\), where \(1\) indicates the positive class (truthful) and \(0\) indicates otherwise.

## 4 Experiments

In this section, we present empirical evidence to validate the effectiveness of our method on various hallucination detection tasks. We describe the setup in Section 4.1, followed by the results and comprehensive analysis in Section 4.2-Section 4.4.

### Setup

**Datasets and models.** We consider four generative question-answering (QA) tasks for evaluation, including two open-book conversational QA datasets CoQA  and TruthfulQA  (generation track), closed-book QA dataset TriviaQA , and reading comprehension dataset TydiQA-GP (English) . Specifically, we have 817 and 3,696 QA pairs for TruthfulQA and TydiQA-GP datasets, respectively, and follow  to utilize the development split of CoQA with 7,983 QA pairs, and the deduplicated validation split of the TriviaQA (_rc.nocontext subset_) with 9,960 QA pairs. We reserve 25% of the available QA pairs for testing and 100 QA pairs for validation, and the remaining questions are used to simulate the unlabeled generations in the wild. By default, the generations are based on greedy sampling, which predicts the most probable token. Additional sampling strategies are studied in Appendix E.

We evaluate our method using two families of models: LLaMA-2-chat-7B & 13B  and OPT-6.7B & 13B , which are popularly adopted public foundation models with accessible internal representations. Following the convention, we use the pre-trained weights and conduct zero-shot inference in all cases. More dataset and inference details are provided in Appendix A.

**Baselines.** We compare our approach with a comprehensive collection of baselines, categorized as follows: (1) _uncertainty-based_ hallucination detection approaches-Perplexity , Length-Normalized Entropy (LN-entropy)  and Semantic Entropy ; (2) _consistency-based_ methods-Lexical Similarity , SelfCKGPT  and EigenScore ; (3) _prompting-based_ strategies-Verbalize  and Self-evaluation ; and (4) _knowledge discovery-based_ method Contrast-Consistent Search (CCS) . To ensure a fair comparison, we assess all baselines on identical test data, employing the default experimental configurations as outlined in their respective papers. We discuss the implementation details for baselines in Appendix A.

**Evaluation.** Consistent with previous studies [32; 23], we evaluate the effectiveness of all methods by the area under the receiver operator characteristic curve (AUROC), which measures the performance of a binary classifier under varying thresholds. The generation is deemed truthful when the similarity score between the generation and the ground truth exceeds a given threshold of 0.5. We follow Lin _et al._ and use the BLUERT  to measure the similarity, a learned metric built upon BERT  and is augmented with diverse lexical and semantic-level supervision signals. Additionally, we show the results are robust under a different similarity measure ROUGE  following Kuhn _et al._ in Appendix D, which is based on substring matching.

**Implementation details.** Following , we generate the most likely answer by beam search with 5 beams for evaluation, and use multinomial sampling to generate 10 samples per question with a temperature of 0.5 for baselines that require multiple generations. Following literature [6; 2],we prepend the question to the generated answer and use the last-token embedding to identify the subspace and train the truthfulness classifier. The truthfulness classifier \(_{}\) is a two-layer MLP with ReLU non-linearity and an intermediate dimension of 1,024. We train \(_{}\) for 50 epochs with SGD optimizer, an initial learning rate of 0.05, cosine learning rate decay, batch size of 512, and weight decay of 3e-4. The layer index for representation extraction, the number of singular vectors \(k\), and the filtering threshold \(T\) are determined using the separate validation set.

### Main Results

As shown in Table 1, we compare our method HaloScope with competitive hallucination detection methods, where HaloScope outperforms the state-of-the-art method by a large margin in both LLaMA-2-7b-chat and OPT-6.7b models. We observe that HaloScope outperforms uncertainty-based and consistency-based baselines, exhibiting 16.47% and 26.71% improvement over Semantic Entropy and EigenScore on the challenging TruthfulQA task. From a computation perspective, uncertainty-based and consistency-based approaches typically require sampling multiple generations per question during testing time, incurring an aggregate time complexity \(O(Km^{2})\) where \(K\) is the number of repeated sampling, and \(m\) is the number of generated tokens. In contrast, HaloScope does not require sampling multiple generations and thus is significantly more efficient in inference, with a standard complexity \(O(m^{2})\) for transformer-based sequence generation. We also notice that prompting language models to assess the factuality of their generations is not effective because of the overconfidence issue discussed in prior work . Lastly, we compare HaloScope with CCS , which trains a binary truthfulness classifier to satisfy logical consistency properties, such that a statement and its negation have opposite truth values. Different from our framework, CCS does not leverage LLM generations but instead human-written answers, and does not involve a membership estimation process. For a fair comparison, we implemented an improved version CCS*, which trains the binary classifier using the LLM generations (the same as those in HaloScope). The result shows that HaloScope significantly outperforms CCS\({}^{*}\), suggesting the advantage of our membership estimation score. Moreover, we find that CCS\({}^{*}\) performs better than CCS in most cases. This highlights the importance of harnessing LLM generations for hallucination detection, which better captures the distribution of model-generated content than human-written data.

   Model & Method & Single & TruthfulQA & TriviaQA & CoQA & TYudiQA-GP \\    } & Perplexity  & ✓ & 56.77 & 72.13 & 69.45 & 78.45 \\  & LN-Entropy  & ✗ & 61.51 & 70.91 & 72.96 & 76.27 \\  & Semantic Entropy  & ✗ & 62.17 & 73.21 & 63.21 & 73.89 \\  & Lexical Similarity  & ✗ & 55.69 & 75.96 & 74.70 & 44.41 \\  & EigenScore  & ✗ & 51.93 & 73.98 & 71.74 & 46.36 \\  & SelfCKGPT  & ✗ & 52.95 & 73.22 & 73.38 & 48.79 \\  & Verbalize  & ✓ & 53.04 & 52.45 & 48.45 & 47.97 \\  & Self-evaluation  & ✓ & 51.81 & 55.68 & 46.03 & 55.36 \\  & CCS  & ✓ & 61.27 & 60.73 & 50.22 & 75.49 \\  & CCS\({}^{*}\) & ✓ & 67.95 & 63.61 & 51.32 & 80.38 \\  & HaloScope (Ours) & ✓ & **78.64** & **77.40** & **76.42** & **94.04** \\    } & Perplexity  & ✓ & 59.13 & 69.51 & 70.21 & 63.97 \\  & LN-Entropy  & ✗ & 54.42 & 71.42 & 71.23 & 52.03 \\  & Semantic Entropy  & ✗ & 52.04 & 70.08 & 69.82 & 56.29 \\  & Lexical Similarity  & ✗ & 49.74 & 71.07 & 66.56 & 60.32 \\  & EigenScore  & ✗ & 41.83 & 70.07 & 60.24 & 56.43 \\  & SelfCKGPT  & ✗ & 50.17 & 71.49 & 64.26 & 75.28 \\  & Verbalize  & ✓ & 50.45 & 50.72 & 55.21 & 57.43 \\  & Self-evaluation  & ✓ & 51.00 & 53.92 & 47.29 & 52.05 \\  & CCS  & ✓ & 60.27 & 51.11 & 53.09 & 65.73 \\  & CCS\({}^{*}\) & ✓ & 63.91 & 53.89 & 57.95 & 64.62 \\  & HaloScope (Ours) & ✓ & **73.17** & **72.36** & **77.64** & **80.98** \\   

Table 1: **Main results. Comparison with competitive hallucination detection methods on different datasets. All values are percentages (AUROC). “Single sampling” indicates whether the approach requires multiple generations during inference. Bold numbers are superior results.**

### Robustness to Practical Challenge

HaloScope is a practical framework that may face real-world challenges. In this section, we explore how well HaloScope deals with different data distributions, and its scalability to larger LLMs.

Does HaloScope generalize across varying data distributions?We explore whether HaloScope can effectively generalize to different data distributions. This investigation involves directly applying the extracted subspace from one dataset (referred to as the source (s)) and computing the membership assignment score on different datasets (referred to as the target (t)) for truthfulness classifier training. The results depicted in Figure 3 (a) showcase the robust transferability of our approach HaloScope across diverse datasets. Notably, HaloScope achieves a hallucination detection AUROC of 76.26% on TruthfulQA when the subspace is extracted from the TriviaQA dataset, demonstrating performance close to that obtained directly from TruthfulQA (78.64%). This strong transferability underscores the potential of our method to facilitate real-world LLM applications, particularly in scenarios where user prompts may undergo domain shifts. In such contexts, HaloScope remains highly effective in detecting hallucinations, offering flexibility and adaptability.

HaloScope scales effectively to larger LLMs.To illustrate effectiveness with larger LLMs, we evaluate our approach on the LLaMA-2-13b-chat and OPT-13b models. The results of our method HaloScope, presented in Table 2, not only surpass two competitive baselines but also exhibit improvement over results obtained with smaller LLMs. For instance, HaloScope achieves an AUROC of 82.41% on the TruthfulQA dataset for the OPT-13b model, compared to 73.17% for the OPT-6.7b model, representing a direct 9.24% improvement.

### Ablation Study

In this section, we conduct a series of in-depth analyses to understand the various design choices for our algorithm HaloScope. Additional ablation studies are discussed in Appendix C-G.

How do different layers impact HaloScope's performance?In Figure 3 (c), we delve into hallucination detection using representations extracted from different layers within the LLM. The AUROC values of truthful/hallucinated classification are evaluated based on the LLaMA-2-7b-chat model. All other configurations are kept the same as our main experimental setting. We observe a notable trend that the hallucination detection performance initially increases from the top to middle layers (e.g., 8-14th layers), followed by a subsequent decline. This trend suggests a gradual capture of contextual information by LLMs in the first few layers, followed by a tendency towards overconfidence in the final layers due to the autoregressive training objective aimed at vocabulary mapping. This observation echoes prior findings that indicate representations at intermediate layers [6; 2] are the most effective for downstream tasks.

   Method & TruthfulQA & TydiQA-GP & TruthfulQA & TydiQA-GP \\   &  &  \\   & 57.81 & 72.66 & 58.64 & 55.50 \\    & 54.88 & 52.42 & 59.66 & 76.10 \\   HaloScope (Ours) & **80.37** & **95.68** & **82.41** & **81.58** \\   

Table 2: Hallucination detection results on larger LLMs.

Figure 3: (a) Generalization across four datasets, where “(s)” denotes the source dataset and “(t)” denotes the target dataset. (b) Effect of the number of subspace components \(k\) (Section 3.2). (c) Impact of different layers. All numbers are AUROC based on LLaMA-2-7b-chat. Ablation in (b) & (c) are based on TruthfulQA.

Where to extract embeddings from multi-head attention?Moving forward, we investigate the multi-head attention (MHA) architecture's effect on representing hallucination. Specifically, the MHA can be conceptually expressed as:

\[_{i+1}=_{i}+_{i}_{i}( _{i}),\] (9)

where \(_{i}\) denotes the output of the \(i\)-th transformer block, \(_{i}(_{i})\) denotes the output of the self-attention module in the \(i\)-th block, and \(_{i}\) is the weight of the feedforward layer. Consequently, we evaluate the hallucination detection performance utilizing representations from three _different locations within the MHA architecture_, as delineated in Table 3.

We observe that the LLaMA model tends to encode the hallucination information mostly in the output of the transformer block while the most effective location for OPT models is the output of the feedforward layer, and we implement our hallucination detection algorithm based on this observation for our main results in Section 4.2.

Ablation on different design choices of membership score.We systematically explore different design choices for the scoring function (Equation 7) aimed at distinguishing between truthful and untruthful generations within unlabeled data. Specifically, we investigate the following aspects: **(1)** The impact of the number of subspace components \(k\); **(2)** The significance of the weight coefficient associated with the singular value \(\) in the scoring function; and **(3)** A comparison between score calculation based on the best individual LLM layer versus summing up layer-wise scores. Figure 3 (b) depicts the hallucination detection performance with varying \(k\) values (ranging from 1 to 10). Overall, we observe superior performance with a moderate value of \(k\). These findings align with our assumption that hallucinated samples may be represented by a small subspace, suggesting that only a few key directions in the activation space are capable of distinguishing hallucinated samples from truthful ones. Additionally, we present results obtained from LLaMA and OPT models when employing a non-weighted scoring function (\(_{j}=1\) in Equation 7) in Table 4. We observe that the scoring function weighted by the singular value outperforms the non-weighted version, highlighting the importance of prioritizing top singular vectors over others. Lastly, summing up layer-wise scores results in significantly worse detection performance, which can be explained by the low separability between truthful and hallucinated data in the top and bottom layers of LLMs.

What if directly using the membership score for detection?Figure 4 showcases the performance of directly detecting hallucination using the score defined in Equation 7, which involves projecting the representation of a test sample to the extracted subspace and bypasses the training of the binary classifier as detailed in Section 3.3. On all four datasets, HaloScope demonstrates superior performance compared to this direct projection approach on LLaMA, highlighting the efficacy of leveraging unlabeled data for training and the enhanced generalizability of the truthfulness classifier.

   Score design & TruthfulQA & TydiQA-GP & TruthfulQA & TydiQA-GP \\   &  &  \\   & 77.24 & 90.26 & 71.72 & 80.18 \\    & 65.82 & 87.62 & 62.98 & 70.03 \\    & **78.64** & **94.04** & **73.17** & **80.98** \\   

Table 4: Hallucination detection results on different membership estimation scores.

Figure 4: Comparison with using direction projection for hallucination detection. Value is AUROC.

   Embedding location & TruthfulQA & TydiQA-GP & TruthfulQA & TydiQA-GP \\  \)} &  &  \\   & **78.64** & **94.04** & 68.95 & 75.72 \\   \(()\) & 75.63 & 92.85 & 69.84 & 73.47 \\   \(()\) & 76.06 & 93.33 & **73.17** & **80.98** \\   

Table 3: Hallucination detection results on different representation locations of multi-head attention.

Comparison with training on labeled data.We further benchmark our approach against a supervised oracle, where each generation in \(\) is explicitly labeled as truthful or hallucinated similar to , and a binary classifier is trained based on representations of the labeled data. This enables us to assess the performance gap between HaloScope and the upper bound. In Figure 5, we compare our approach with the supervised oracle on the same test set and report the AUROC metric. The results, based on the LLAMA-2-7b-chat model, demonstrate that our method achieves hallucination detection accuracy on TruthfulQA (AUROC: 78.64%) that favorably matches the performance of the supervised oracle (AUROC: 81.04%). This is encouraging considering that our method does not rely on any manual annotation.

Qualitative results.We provide qualitative examples of the model's truthfulness score (as introduced in Section 3.3) for different generated texts given the same input prompts during inference (Figure 6). The model is LLAMA-2-chat-13b and we utilize questions in TruthfulQA that can cause misconception even for humans, for example, _Q: Who first started Tesla Motors?_ We find that HaloScope can accurately produce the scores that align with the truthfulness of the answers (the higher the score, the more truthful the answer is).

## 5 Related Work

Hallucination detection has gained interest recently for ensuring LLMs' safety and reliability . The majority of work performs hallucination detection by devising uncertainty scoring functions, including those based on the logits  that assumed hallucinations would be generated by flat token log probabilities, and methods that are based on the output texts, which either measured the consistency of multiple generated texts  or prompted LLMs to evaluate the confidence on their generations . Additionally, there is growing interest in exploring the LLM activations to determine whether an LLM generation is true or false . For example, Chen et al.  performed eigendecomposition with activations but the decomposition was done on the covariance matrix that required multiple generation steps to measure the consistency. Zou et al.  explored probing meaningful direction from neural activations. Our approach is different in three aspects: 1) HaloScope estimates the membership for unlabeled data by identifying the _hallucination subspace_ rather than a single direction in , which can capture the truthfulness encoded in LLM activations more effectively (evidenced in Figure 3); 2) HaloScope trains a truthfulness classifier based on membership estimation results, where the explicit training procedure brings more benefits for generalizable hallucination detection compared to direct projection in  (Section 4.4); and 3) our paper conducts comprehensive and in-depth evaluation on common benchmarks, thus offering more practical insights than . Another branch of works, such as Li, Duan and Azaria et al. , employed labeled data for extracting truthful directions, which differs from our scope on harnessing unlabeled LLM generations. Note that our studied problem is different from the research on hallucination mitigation , which aims to enhance the truthfulness of LLMs' decoding process.  utilized unlabeled data for out-of-distribution detection, where the approach and problem formulation are different from ours.

Figure 5: Comparison with ideal performance when training on labeled data.

Figure 6: Examples from TruthfulQA that show the effectiveness of our approach. Specifically, we compare the truthfulness scores \(S(^{})\) (Section 3.3) of HaloScope with different answers to the prompt. The green check mark and red cross indicate the ground truth of being truthful vs. hallucinated.

Conclusion

In this paper, we propose a novel algorithmic framework HaloScope for hallucination detection, which exploits the unlabeled LLM generations arising in the wild. HaloScope first estimates the membership (truthful vs. hallucinated) for samples in the unlabeled mixture data based on an embedding factorization, and then trains a binary truthfulness classifier on top. The empirical result shows that HaloScope establishes superior performance on a comprehensive set of question-answering datasets and different families of LLMs. Our in-depth quantitative and qualitative ablations provide further insights on the efficacy of HaloScope. We hope our work will inspire future research on hallucination detection with unlabeled LLM generations, where a promising future work can be investigating how to train the hallucination classifier in order to generalize well with a distribution shift between the unlabeled data and the test data.

## 7 Acknowledgement

We thank Froilan Choi and Shawn Im for their valuable suggestions on the draft. The authors would also like to thank NeurIPS anonymous reviewers for their helpful feedback. Du is supported by the Jane Street Graduate Research Fellowship. Li gratefully acknowledges the support from the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Philanthropic Fund from SFF, and faculty research awards/gifts from Google and Meta.