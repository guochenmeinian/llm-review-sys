# An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations

Haoran Yang

University of Technology Sydney

haoran.yang-2@student.uts.edu.au

&Xiangyu Zhao

City University of Hong Kong

xianzhao@cityu.edu.hk

&Yicong Li

University of Technology Sydney

yicong.li@student.uts.edu.au

&Hongxu Chen

University of Queensland

hongxu.chen@uq.edu.au

&Guandong Xu

University of Technology Sydney

guandong.xu@uts.edu.au

Corresponding author.

###### Abstract

Graph contrastive learning (GCL) has emerged as an effective technology for various graph learning tasks. It has been successfully applied in real-world recommender systems, where the contrastive loss and downstream recommendation objectives are combined to form the overall objective function. However, this approach deviates from the original GCL paradigm, which pre-trains graph embeddings without involving downstream training objectives. In this paper, we propose a novel framework called CPTPP, which enhances GCL-based recommender systems by leveraging prompt tuning. This framework allows us to fully exploit the advantages of the original GCL protocol. Specifically, we first summarize user profiles in graph recommender systems to automatically generate personalized user prompts. These prompts are then combined with pre-trained user embeddings for prompt tuning in downstream tasks. This helps bridge the gap between pre-training and downstream tasks. Our extensive experiments on three benchmark datasets confirm the effectiveness of CPTPP compared to state-of-the-art baselines. Additionally, a visualization experiment illustrates that user embeddings generated by CPTPP have a more uniform distribution, indicating improved modeling capability for user preferences. The implementation code is available online2 for reproducibility.

## 1 Introduction

Graph contrastive learning (GCL) has gained significant attention in the research community as a prominent self-supervised learning paradigm. Several recent studies have showcased the effectiveness of GCL in various general graph representation tasks [21; 16; 28; 37; 30], including node classification and link prediction. Moreover, GCL has also demonstrated its applicability in real-world domains, such as recommender systems [27; 32; 14]. By introducing additional self-supervision signals, GCL provides recommender systems with a means to address the challenge of improving performance.

Most recommendation methods based on GCL typically combine contrastive loss with recommendation objectives to optimize the model in an end-to-end manner. However, this training protocol doesnot align with the purpose of GCL, which is primarily designed for pre-training graph representations without involving downstream task objectives [21; 16]. In this approach, GCL pre-trains embeddings that are then fine-tuned on specific tasks using downstream models. Incorporating both GCL and recommendation objectives into the overall training objective can disrupt the embedding pre-training process and requires careful control of the weight placed on contrastive loss. Additionally, previous studies on GCL-based recommendation methods [27; 14] have shown that the weights of contrastive loss in the overall objective are significantly smaller compared to the weight on the recommendation objective. This is done to ensure desired performance on recommendation tasks. Therefore, based on these observations, simply combining contrastive loss with downstream recommendation objectives may not be effective for recommendation tasks.

The disparity between the pre-training objective and downstream tasks hinders the effective extraction of useful information from pre-trained embeddings by downstream models [12; 26]. Consequently, researchers often opt for combining GCL with recommendation objectives. However, it is important to note that GCL pre-training targets primarily assess the agreement of mutual information among graph elements, such as nodes, edges, and sub-graphs. This differs from conventional graph learning tasks like node classification and link prediction. Consequently, the pre-training targets of GCL also significantly diverge from downstream recommendation objectives that involve interaction (link) prediction between users and items. Consequently, the reduction of such dissimilarities is essential to enhance the performance of GCL-based recommendation approaches.

In this paper, we present the CPTPP framework as an extension of recent advancements in prompt tuning for enhancing recommendation performance [23; 33] utilizing user embeddings pre-trained by GCL. The technique of prompt tuning has emerged as a prominent method for fine-tuning pre-trained models. By constructing appropriate prompts for downstream learning modules, this approach effectively reformulates downstream tasks, thereby reducing disparities [26; 12; 15; 18]. By incorporating prompt-tuning, we can modify existing GCL-based recommendation methods to align with the original GCL protocol involving pre-training and fine-tuning. Previous endeavors have also explored the integration of prompt learning into conventional recommendation models [26; 3]. Despite their advantages, applying the prompt mechanism directly to GCL-based recommendation methods is still difficult and not straightforward, _i.e., how can we generate personalized user prompts using only the user-item interaction graph without side information_ (e.g., _age and occupation_)? To address this issue, we summarise three methods to produce different user profiles, including _historical interaction records_, _adjacency matrix factorization_, and _high-order user relations_, based on the user-item interaction graph for the personalized user prompt generation, which is applicable in situations devoid of side information. Comprehensive experiments conducted on three publicly available datasets illustrate the effectiveness of the proposed method with different types of prompts.

In summary, the contributions of this work are three-fold: (1) We propose a reformulation of existing GCL-based recommendation methods by incorporating the prompt tuning mechanism. This allows us to fully leverage the advantages of GCL during the pre-training phase, rather than relying on the combination of contrastive loss with downstream objectives. (2) We introduce three user profiles derived from the user-item interaction graph as inputs for the prompt generator. By using these profiles, we are able to generate personalized prompts that enhance the quality of user embeddings in graph-based recommendation systems. (3) We conduct extensive experiments on three publicly available benchmark datasets to validate the effectiveness of our model. Through these experiments, we analyze the important components and hyper-parameters of our approach and also investigate the impact of different personalized prompts generated by our method.

## 2 Methodology

In this section, the proposed method, graph **C**ontrastive **P**re-**T**raining with **P**rom**P**t-tuning for recommendation (**CPTPP**), will be introduced to reveal the intuitions and the technical details.

### Framework Overview

There are three modules in the proposed CPTPP method, as shown in Figure 1: (1) graph contrastive learning module, leveraging the advantages of GCL to the pre-train user and item embeddings, (2) personalized prompts generation module, applying prompt mechanism to generate personalizedprompts for users, and (3) recommendation module, fusing the generated personalized prompts and pre-trained user embeddings to conduct prompt-tuning for the downstream recommendation task.

### Graph Contrastive Learning Module

In order to achieve optimal performance in downstream tasks, the selection of a suitable pre-training strategy is crucial for generating high-quality inputs for downstream modules. GCL has been demonstrated as a powerful technique for graph pre-training [21; 16; 30; 37] and has emerged as an effective tool for leveraging self-supervised signals to enhance graph-based recommendation models [27; 14; 24; 32]. In the case of graph-based recommender systems, GCL represents a viable option for pre-training embeddings. Furthermore, our work specifically focuses on reforming and improving existing GCL-based recommendation methods. Therefore, it is imperative that we formulate a GCL module within our proposed method. Current GCL-based recommendation methods [14; 24; 27; 32] have explored various graph augmentation techniques on user-item interaction graphs in order to generate augmented graphs for GCL, enabling the extraction of informative semantics from the graph structures. Alternatively, some studies [31; 14] have designed context embeddings tailored for GCL in recommended settings. Although different approaches exist for constructing contrasting samples, they all share a common backbone for the GCL training protocol.

Here, we give a formal description of the GCL training protocol. Let \(u_{i}\) denote the target graph element (_e.g.,_ user node), \(u_{i}^{+}\) represent the positive sample generated from \(u_{i}\) (_e.g.,_ the neighbor node of the target node), and \(^{-}=\{u_{i,0}^{-},u_{i-1}^{-},,u_{i,n}^{-}\}\) be the set of contrasting samples of \(u_{i}\) (_e.g.,_ non-neighbour nodes of the target node). Considering the settings of the recommendation task, we use \(G\) to represent the overall user-item graph, and all the target, positive sample, and contrasting samples are within graph \(G\). To acquire embeddings of these graph elements, we adopt \(f(*)\) as the graph encoder to process them, and the target embedding is denoted by \(_{i}=f(u_{i};G)\), \(^{+}=f(u_{i}^{+};G)\) is the embedding of the positive sample, and \(\{_{i,0}^{-},_{i,1}^{-},,_{i,n}^{-}\}\) are the list of embeddings of the negative contrasting samples. Following the settings of InfoNCE , the self-supervised learning objective can be formulated as follows:

\[_{contra}=-_{i},_{i}^{+})/ )}{_{t=0}^{|^{-}|}(sim(_{i},_{i,t}^ {-})/)},\] (1)

Figure 1: The overview of the proposed method CPTPP.

where \(\) is the temperature hyper-parameter and sim(\(\), \(\)) is the similarity metric. In existing research works [24; 27; 32; 31; 14], researchers usually combine the aforementioned contrastive learning loss function with the recommendation objectives to formulate an overall objective function to train the model in an end-to-end manner:

\[_{overall}=_{rec}+_{contra},\] (2)

where \(\) is a hyper-parameter that controls the weight of the contrastive learning objective. However, as mentioned in Section 1, the proposed CPTPP adopts a _pre-train, prompt, fine-tune_ manner to train the model and treats GCL as a pre-training task instead of combining the contrastive loss with recommendation objectives. To leverage recent research progress in GCL, we can adopt various GCL learning methods tailored for the recommendation task here, like NCL , SGL , and SimGCL , to obtain high-quality user and item embeddings. Then, the pre-trained user and item embeddings will be processed by the prompt mechanism in the following.

### Prompts Generation Module

Following the pre-training phase, our method, named CPTPP, incorporates a personalized prompts generation module to utilize the pre-trained user and item embeddings effectively. The primary objective of this module is to address the limitations present in existing prompt and recommendation research. Prior studies [18; 15; 26] have highlighted the triviality and resource-intensive nature of hard prompt design, making it impractical for real-world scenarios. Additionally, we observe that most current approaches rely on side information (_e.g.,_ user descriptions) to generate prompts, and there lacks a specific paradigm for prompting in graph-based recommendation scenarios. To overcome these limitations, we propose the integration of a prompt generator  that generates personalized prompts tailored specifically for graph-based recommendation contexts.

#### 2.3.1 Personalized Prompt Generator

The main scope of the generated prompts lies in narrowing the gap between the pre-training targets and the downstream objectives to utilize the pre-trained models or embeddings better. Some research works designed hard prompts tailored for recommendation tasks converting recommendation tasks into NLP tasks , which unifies multiple recommendation tasks in a single framework. For example, a convention recommendation task can be converted to a sentence, _'User 123 will purchase item [id_token]'_. Then, NLP techniques will be applied to predict the token. However, PPR  argued that such an NLP-style hard prompt designing method has two major limitations: (i) It is difficult to apply NLP techniques to predict the designated tokens since these tokens could be a user ID, item ID, or ratings, which lack meaningful semantics. (ii) The designed hard prompts are universal and cannot be customized for various users or items.

To address the challenges, we adopt a method to construct personalized prompts from user profiles in a soft prompt automatic generation manner [12; 15; 26]. Let \(_{i}^{u}^{d 1}\) denote the profile of user \(i\). We, then, concatenate all the users' profiles to form the user profile matrix \(^{u}=[_{1}^{u},_{2}^{u},,_{n}^{ u}]^{d n}\). This matrix will be fed into a two-layer perceptron \(f()\) to acquire personalized prompts for each user \(^{u}=[_{1}^{u},_{2}^{u},,_{n}^{ u}]^{p n}\) as follows:

\[^{u}=f(^{u})=_{2}(_{1} ^{u}+b_{1})+b_{2},\] (3)

where \(p\) is the prompt size, \(_{1}^{h d}\) and \(_{2}^{p h}\) are trainable weights, \(b_{1}^{h n}\) and \(b_{2}^{d n}\) are biases, and \(()\) is the activation function. \(d\) and \(h\) represent the dimensions of the pre-trained embeddings and hidden dimensions, respectively. The generated prompts will be concatenated with the pre-trained user embeddings in a pre-fixed manner  and tuned by the downstream objectives in the recommendation module to fulfill the process of prompt-tuning. Specifically, let \(_{pre\_train}^{d n}\) denote pre-trained user embeddings. Then, we have the inputs from the user side for the recommendation module:

\[_{concat}=^{u}\\ _{pre\_train}^{T}^{n(p+d)}.\] (4)

#### 2.3.2 Personalized Inputs for Prompt Generation

The quality of the generated prompts depends on the personalized inputs for the generator. Current research on prompt learning for recommendation mainly focuses on utilizing existing user features(_e.g.,_ age, gender, and occupation) and historical interaction records as the inputs to generate personalized prompts [26; 11; 25]. However, these methods are designed for conventional and sequential recommendations, which are not entirely aligned with graph recommendations. It is necessary to summarise and explore how to generate personalized prompts from the perspective of the graph recommendation system. In this section, we summarise three types of inputs for the generator to generate personalized prompts for the graph-based recommendation: historical interaction records, adjacency matrix factorization, and high-order user relations.

_Historical Interaction Records_. It is a common and widely-used method to illustrate users' features or preferences via aggregating his/her historical interaction records, which is feasible in various scenarios in recommendation systems. Let \(_{k}^{u}=\{i_{k,1},i_{k,2},,i_{k,m}\}\) denote the item set which are purchased by user \(k\). We use \(_{k,j}^{d}\) to represent the embedding of the \(j\)-th item in user \(k\)'s purchase history. Then, the profile of user \(k\) can be acquired by aggregating embeddings of those items purchased by the user \(k\):

\[_{k}^{u}=Aggregation(_{k,1},_{k,2},, _{k,m}),\] (5)

where \(Aggregation(*)\) is the aggregation function to read out the user's profile.

The concatenation of all the user profiles can form the matrix \(^{u}\) to be processed by the personalized prompt generator. Let \(^{n q}\) denote the adjacency matrix for the recommendation system, which contains \(n\) users and \(q\) items. If we have the pre-trained item embeddings \(_{pre\_train}=[_{1},_{1},,_{q}] ^{d q}\), then, we have:

\[^{u}=(_{pre\_train}^{T})^{T}.\] (6)

_Adjacency Matrix Factorization_. The adjacency matrix is an effective tool to demonstrate the user-item relations in the recommendation system. However, the adjacency matrix usually suffers from sparsity problems and thus cannot be smoothly applied in many real-world recommendation scenarios. To address this problem, researchers proposed several matrix factorizations (MF) methods [19; 7] to decompose the adjacency matrix to obtain two matrices, \(\) and \(\), denoting the latent embeddings for users and items, which are much denser than the adjacency matrix \(\) itself. The process of MF can be formulated as follows:

\[*{arg\,min}_{,}_{i=1}^{n} _{j=1}^{q}(_{i,j}-}_{i,j}),\] (7)

where \(}_{i,j}=_{k}_{i,k}_{k,j}^{T}= _{i}_{j}^{T}\). After the MF process, we have the latent matrix of users, \(^{n d}\), serving as the user profile matrix \(^{u}\) after transposed \(^{u}=^{T}\) and can be fed into a personalized prompt generator to produce personalized prompts \(^{u}\) for each user. Specifically, we set the size of latent embeddings as \(d\), the same as the size of pre-trained embeddings.

_High-Order User Relations_. Learning informative embeddings from a 1-hop user-item interaction graph is challenging when there is no side information. To address this limitation, we propose to leverage high-order user relations to enrich the learned embeddings via constructing 2-ego graphs for each user node to find the links between the other users and itself . Then, we fuse the target user's purchase history and high-order neighbor embeddings to represent the target user profile.

We first construct the high-order connectivity matrix to achieve the goal. Let \(^{*}=}}^{(n+q) (n+q)}\) denote the high-order connectivity matrix, where \(}=&\\ ^{T}&^{(n+q)(n+q)}\), recording all the users and items to which a user or item node is connected. Then, we build the matrix \(=[_{pre\_train},_{pre\_train}]^{T} ^{(n+q) d}\) to store pre-trained embeddings. Next, we can acquire matrix \(^{n d}\), which are the users' personalized profiles about high-order user relations, via:

\[\\ =^{*}^{(n+q)  d},\] (8)

where \(^{q d}\), denoting the high-order item relations and being omited after \(\) is extracted. Then, a matrix transpose operation is required to obtain the user profile matrix \(^{u}=^{T}\).

### Recommendation Module

After the pre-training and the personalized prompts generation phase, a recommendation module is required so that we can verify whether the prompt-tuning module rectifies the pre-trained embeddings by GCL and makes them be adapted to the downstream recommendation tasks better. In this module, we take the inner product of user and item embeddings as the predicted score for the recommendation. Bayesian Personalized Ranking (BPR)  is adopted as the training objective to tune the pre-trained embeddings based on the predicted scores. The motivation for formulating such a simple recommendation module is to avoid the performance gain brought by the delicate designs of those advanced recommendation models, which could affect the observations on our proposed method.

```
0: User embedding table \(_{E}\); Item embedding table \(_{E}\); User-item interaction graph adjacency matrix \(\); Graph contrastive learning model \(f(*)\); User profile \(^{u}\); Prompt generator \(g()\); Multi-layer perceptron MLP \(()\); Pre-train epoch \(i\); Prompt-tune epoch \(j\). Output: User and item embedding tables \(_{E}^{*}\) and \(_{E}^{*}\).
1Pre-train phase:
2Initialize \(_{E}\), \(_{E}\); \(_{E}^{{}^{}},_{E}^{{}^{}}_ {E},_{E}\);
3\(count=0\);
4while\(count<i\)do// Update user and item embedding tables. \(_{E}^{{}^{}},_{E}^{{}^{}}\) = \(f(_{E}^{{}^{}},_{E}^{{}^{}};)\);
5\(count=count+1\);
6
7 end while
8Prompt-tune phase:
9\(_{E}^{*}_{E}^{{}^{}}\); \(_{E}^{*}_{E}^{{}^{}}\);
10\(count=0\);
11while\(count<j\)do// Personalized prompt generation. \(^{u}=g(^{u})\); // Concatenate & fusion. \(_{E}^{*}=([^{u};_{E}^{*}]^{T}) ^{n d}\);  Optimise \(=_{i}_{rec}^{i}+||||_{2}^ {2}\);  Update \(_{E}^{*}\), \(_{E}^{*}\); \(count=count+1\);
12
13 end while return\(_{E}^{*}\), \(_{E}^{*}\) ```

**Algorithm 1**CPTPP algorithm

#### 2.4.1 Prompts and Pre-Trained Embeddings Fusion

We concatenate the generated personalized prompts and the pre-trained user embeddings in the previous step and have \(_{concat}^{n(p+d)}\), whose dimensionality is different from the pre-trained embeddings \(_{pre\_train}^{n d}\). Hence, we need first to fuse the personalized prompts and the pre-trained user embeddings for the recommendation objective training. Specifically, we adopt a multi-layer perceptron (MLP) \(g()\) as the mapping function that is \(g:^{n(p+d)}^{n d}\). Then, we can have dimensionality-reduced user representations \(^{*}=g(_{concat})^{n d}\), enhanced by the personalized prompts. After that, we can apply the inner product to predict how likely the user \(i\) would interact with the item \(j\) by \(_{i,j}=_{i}^{*}_{j}\), where \(_{i}^{*}\) is the \(i\)-th row of \(^{*}\).

#### 2.4.2 Training Objective for Recommendation Task

For simplicity and fair comparison, we adopt BPR  loss as the training objective for the recommendation task. For each user \(i\), we have:

\[_{rec}^{i}=_{j^{+}_{i}^{u}}_{j^{-} _{i}^{u}}-(_{i,j^{+}}-_{i,j^{-}}).\] (9)However, it is unaffordable to consider all the unobserved interactions of the user \(i\). Therefore we sample several negative items \(_{i}^{u}\), where \(|_{i}^{u}|<<|_{i}^{u}|\), in practice.

Moreover, we introduce \(L2\)-norm into the training objective to regularize the parameters in the model to address the overfitting problem and improve generalization ability. Therefore, the overall objective function can be formulated as:

\[=_{i}_{rec}^{i}+||||_{2}^ {2}.\] (10)

### Summary

After the training process ends, the model can be used to conduct inference. For the inference phase, we do not conduct pre-train and prompt-tune again. What we need to do is to extract target user and item embeddings from the trained embedding tables. Then, we calculate their inner product to predict the probability that the user will interact with the item in the future.

The complete training procedure of CPTPP is illustrated by Algorithm 1. We first initialize the user and item embedding tables (line 2). Then, we apply a GCL model to conducting embedding pre-training (line 4 - 7). Next, we step into the prompt-tuning phase and assign the pre-trained embeddings to \(_{E}^{*}\) and \(_{E}^{*}\) (line 9). Following, we input the user profile to the prompt generator to produce the personalized prompts (line 12) and combine them with \(_{E}^{*}\) (line 13). Finally, we use \(_{E}^{*}\) and \(_{E}^{*}\) to calculate the loss and update them accordingly (line 14 \(\) 15). The update procedure will repeat until the termination condition is achieved (line 11 \(\) 17).

## 3 Experiment

To verify the effectiveness of the proposed method, CPTPP, in this paper, we conduct extensive experiments and demonstrate the results with insightful analysis in this section.

### Experimental Setup

This section introduces the experimental settings, including datasets and baselines we used, performance metrics, and hyper-parameter settings for CPTPP. More details about how to get access to the datasets and implementation details are listed in **Appendix A**.

**Datasets** To verify the performance of CPTPP in the recommendation task, we select three popular datasets: **Douban**, **MovieLens-1M**, and **Gowalla**. The detailed statistics about the three datasets are listed in Table 1. For each dataset, we randomly select 80% of historical user-item interactions as the training set, and the rest 20% records will serve as the testing set. Following the settings widely adopted by the research community [22; 6], we treat each user-item interaction record as a positive instance and conduct negative sampling to couple it with a negative instance, which is an unobserved user-item interaction in the dataset.

**Baselines** We select several baselines for comparison experiments: **BPR-MF**, **BUIR**, **SelfCF**, **NCL**, and **SimGCL**. For CPTPP, we have three variations, which are CPTPP-H, CPTPP-M, and CPTPP-R, respectively. -_H_ takes historical interaction records for personalized prompt generation. -_M_ indicates that we take adjacency matrix factorization for personalized prompts generation. Furthermore, -_R_ takes high-order user relations for the personalized prompt generation.

**Metrics** To evaluate the quality of top-\(K\) recommendation, we adopt three popular metrics, which are _Hit Ratio@\(K\)_, _Precision@\(K\)_, and _NDCG@\(K\)_, respectively. In our settings, the value of \(K\) is set to 5 and 20. Following the evaluation protocol in [14; 32], we take the full ranking strategy .

   Dataset & \#Users & \#Items & \#Interactions & Density \\  Douban & 2,848 & 39,586 & 894,887 & 0.794\% \\  ML-1M & 6,040 & 3,900 & 1,000,209 & 4.246\% \\  Gowalla & 29,858 & 40,981 & 1,027,370 & 0.084\% \\   

Table 1: Dataset Statistics

#### Hyper-parameter Settings

To ensure reproducibility, we disclose the comprehensive hyper-parameter settings for implementing our proposed CPTPP in the source codes and **Appendix A.3**.

### Experiment Results

We conduct experiments and provide analysis in this section. More supplementary experiment results, including comparison, hyper-parameter, and ablation study, are revealed in **Appendix B**.

#### 3.2.1 Overall Comparison Studies

Table 2 shows the comparison results among all the baselines and different versions of the proposed methods. (i) We can first observe that the traditional method BPR-MF is outperformed by all the other methods as they utilize contrastive learning to introduce extra unsupervised training signals. (ii) Among all the baselines, GCL-based recommendation methods, including NCL and SimGCL, significantly and consistently outperform those self-supervised recommendation methods without graph learning module equipped, BUIR and SelfCF. It is because those GCL-based methods adopt graph neural networks, leveraging the sophisticated structure semantics in user-item interaction graphs to enrich learned user embeddings and item embeddings. (iii) But we notice that SimGCL only outperforms NCL on dataset Gowalla, which has a much larger scale than the others, probably because SimGCL adopts a simplified GCL method that relieves the model overfitting problem on a large-scale dataset. It is the potential reason NCL outperforms SimGCL on smaller datasets, as the simplified GCL method may not provide sufficient self-supervised training signals. (iv) Though the proposed CPTPP solely adopts BPR loss, which is significantly different from the pre-training procedure, for the recommendation task training, we utilize the prompt learning mechanism to better adapt the embeddings pre-trained by the GCL method to the downstream task, expecting to improve the recommendation performance. According to the experiment results, all versions of our proposed method achieve competitive results. Such results reflect prompt-tuning's effectiveness in narrowing the gap between the pre-train objective and the downstream tasks.

To further evaluate the performance of the GCL-based recommendation methods, we visualize the produced user embeddings produced by t-SNE and Gaussian kernel density estimation (KDE). We can see that CPTPP has a more uniform distribution of the produced user embeddings, illustrated by the uniformity of the color maps, especially on dataset ML-1M and Gowalla. As suggested by Z. Lin _et al._, the more uniform the embedding distribution is, the more powerful the capability to model the diverse preferences of users the produced embeddings will have, which reflects the superiority of CPTPP compared to the baselines. The visualizations and analysis are listed in **Appendix B.1**.

    &  &  \\   & & BPR-MF & BUIR & SelfCF & NCL & SimGCL & CPTPP-H & CPTPP-M & CPTPP-R \\   & Hit Ratio@5 & 0.0134 & 0.0156 & 0.0161 & 0.0161 & 0.0161 & 0.0164 & **0.0165*** & 0.0164 \\  & Hit Ratio@20 & 0.0446 & 0.0492 & 0.0502 & 0.0507 & 0.0489 & 0.0521 & **0.0528*** & 0.0523 \\  & Precision@5 & 0.1812 & 0.2113 & 0.2185 & 0.2187 & 0.2182 & 0.2221 & **0.2235*** & 0.2224 \\  & Precision@20 & 0.1512 & 0.1667 & 0.1699 & 0.1717 & 0.1657 & 0.1766 & **0.1790*** & 0.1772 \\  & NDCG@5 & 0.1904 & 0.2209 & 0.2264 & 0.2313 & 0.2370 & 0.2359 & **0.2378*** & 0.2355 \\  & NDCG@20 & 0.1749 & 0.2019 & 0.2058 & 0.1958 & 0.2020 & 0.2065 & **0.2098*** & 0.2070 \\    & Hit Ratio@5 & 0.0469 & 0.0617 & 0.0624 & 0.0655 & 0.0631 & **0.0676*** & 0.0674 & 0.0672 \\  & Hit Ratio@20 & 0.1454 & 0.1519 & 0.1643 & 0.1796 & 0.1698 & 0.1851 & **0.1861*** & 0.1845 \\  & Precision@5 & 0.1800 & 0.2368 & 0.2396 & 0.2513 & 0.2420 & **0.2592*** & 0.2585 & 0.2577 \\  & Precision@20 & 0.1395 & 0.1457 & 0.1576 & 0.1723 & 0.1629 & 0.1776 & **0.1785*** & 0.1770 \\  & NDCG@5 & 0.1968 & 0.2722 & 0.2689 & 0.2818 & 0.2767 & **0.2919*** & 0.2895 & 0.2878 \\  & NDCG@20 & 0.2103 & 0.2367 & 0.2508 & 0.2683 & 0.2670 & 0.2781 & **0.2782*** & 0.2756 \\    & Hit Ratio@5 & 0.0429 & 0.0479 & 0.0497 & 0.0488 & 0.0513 & 0.0518 & 0.0512 & **0.0519*** \\  & Hit Ratio@20 & 0.1039 & 0.0993 & 0.1042 & 0.1040 & 0.1065 & 0.1115 & 0.1103 & **0.1120*** \\   & Precision@5 & 0.0624 & 0.0698 & 0.0723 & 0.0711 & 0.0746 & 0.0754 & 0.0745 & **0.0755*** \\   & Precision@20 & 0.0378 & 0.0361 & 0.0379 & 0.0378 & 0.0387 & 0.0406 & 0.0401 & **0.0407*** \\   & NDCG@5 & 0.0770 & 0.0911 & 0.0939 & 0.0894 & 0.0963 & **0.0963** & 0.0953 & 0.0961 \\   & NDCG@20 & 0.0939 & 0.0990 & 0.1036 & 0.1005 & 0.1126 & **0.1092** & 0.1083 & **0.1092** \\   

*\({}^{*}\)\({}^{*}\) indicates that CPTPP outperforms the best baseline significantly (i.e., two-sided t-test with \(p<0.05\)).

Table 2: The experiment results of comparison studies. The figures in boldface indicate the best performance achieved by one of the three versions of CPTPP, and the figures underlined indicate the best performance among all the baselines.

#### 3.2.2 Hyper-Parameter Studies

To investigate the properties of our proposed CPTPP method, we conduct hyper-parameter studies on an important term, the dimension size of the personalized prompt. By fixing all the other hyper-parameters, we comprehensively examine the performance of three versions of the proposed CPTPP on all the datasets with different prompt sizes. Specifically, the size of the personalized prompt is selected from \(\{8,16,32,64,128,256\}\). We choose two metrics, _Precision@5_ and _NDCG@5_, to demonstrate CPTPP's performance variations with regard to different prompt sizes. All the experiment results are shown in Figure 3 and Figure 5 in **Appendix B.2**. (i) The first finding we can observe is that, in most cases, CPTPP has the best performance when the prompt size is not larger than the dimensionality of user embeddings, _i.e._, _64_. A potential reason is that the prompt is usually less informative than the pre-trained embeddings, so a sizeable prompt dimension would introduce too much noise to disturb and conceal the structural semantics contained in the pre-trained user embeddings. (ii) We also notice a significant performance improvement when prompt size is 256 in several cases, such as CPTPP-M on dataset ML-1M and CPTPP-R on dataset Gowalla. Such outlier performance could be caused by random factors during the overall training process. However, they still fail to significantly outperform the CPTPP model, which has a much smaller prompt size. Therefore, a small prompt size for prompt-tuning is a better option in practice as they achieve a relatively better recommendation quality and higher efficiency.

#### 3.2.3 Ablation Studies

As we summarize three strategies to generate personalized prompts for users, we conduct the ablation study to explore the differences among these methods. Two ablation studies are conducted in this section to illustrate the performance of three variations of the CPTPP method. The first ablation study is about the overall evaluation of recommendation quality, whose analysis is listed below. The second one is about the embedding visualizations and the related analysis, illustrated in **Appendix B.3**.

We notice that (i) CPTPP-M achieves the best performance on dataset Douban. Nevertheless, the performance of CPTPP-M degrades on dataset ML-1M and is the worst case on dataset Gowalla. Considering the number of users reflected in Table 1, we find that the performance of CPTPP-M drops as the dataset's number of users increases. So, CPTPP-M has good performance if the number of users in the dataset is relatively small. It may be because matrix factorization, as a naive method, cannot

Figure 3: The performance, demonstrated by _Precision@5_ and _NDCG@5_, of all variations of CPTPP.

Figure 2: The visualizations of the user embeddings generated by the proposed method.

fully reveal user preferences in a complex user-item interaction graph with too many user nodes. (ii) CPTPP-R utilizes high-order relationships among users to enrich the generated personalized prompts for users. In such settings, the item information would also be aggregated due to the message-passing mechanism in GNNs. Therefore, it achieves the best performance on the dataset Govalla, having the most users and the most complex user-user relation among all the datasets. (iii) CPTPP-H has moderate performance. CPTPP-H adopts historical interaction records, formed by trainable item embeddings, to generate personalized prompts. Those trainable elements endow CPTPP-H with a more robust capability to represent user preferences than matrix factorization. It is also reasonable that CPTPP-R outperforms CPTPP-H as CPTPP-H lacks consideration of high-order user relations.

## 4 Related Work

### GCL in Recommendation Systems

The GCL-based recommendation system is now a trending topic in the research community. Researchers leverage the advantages of GCL to improve current graph-based recommendation methods further, achieving satisfying performance. The research scope first lies in utilizing user-item interaction records to accommodate GCL techniques. For example, HMG-CR  innovatively proposes a concept called hyper-meta path. Then GCL is applied to adaptively learn the behavioural patterns of users via contrasting different paths. NCL  improves graph-based recommendation models via neighborhood-enriched contrastive learning, including semantic neighbors and structural neighbors (\(r\)-_ego_ graphs). To better understand the role of GCL in the recommendation systems, SGL  conducts a comprehensive theoretical analysis, giving insights and achieving promising performance. Based on the findings of SGL, SimGCL  is proposed to simplify GCL in recommendation via discarding complex augmentations, reducing the volume of GCL-based recommendation models, and performing competitive results in the experiments. However, current GCL-based recommendation methods combine the graph contrastive loss with the recommendation objectives to formulate an overall objective to train the models, suffering from various limitations summarised in the previous sections. Our method innovatively introduces the prompt mechanism to build a _pre-train then prompt-tune_ paradigm for GCL-based recommender systems to address the limitations we discussed.

### Prompt-Tuning

Prompt-tuning is a novel and trending paradigm for pre-train models in the natural language process (NLP). The core idea of prompt-tuning is to re-formulate the downstream tasks, narrowing the huge gap between them and the pre-train objective [1; 18]. There are two types of methods to achieve prompt-tuning . The first is manually designing or searching for proper discrete prompts (hard prompts) [2; 8; 18]. However, such a fashion is trivial and resource-consuming as the search space is extremely large and expert knowledge is required  in some application scenarios. To address this limitation, another line of methods is proposed, focusing on generating continuous vector embeddings as the soft prompts [4; 15]. The application of prompt-tuning in recommendation systems has been explored. For example, P5  reforms the recommendation tasks to the NLP tasks and follows the hard-prompt fashion to perform recommendations. PPR , instead, takes a soft-prompt and pre-fix strategy  to generate personalized prompts for users in the recommendation systems automatically. Nevertheless, graph learning and its applications are now out of the scope of prompt-tuning research. Besides, most current prompt learning methods require side information to produce high-quality prompts, resulting in a limited comfort zone for its applications. Our CPTPP first adopts the prompt mechanism to the GCL-based recommendation area.

## 5 Conclusion

In this paper, we propose a CPTPP method to adopt a prompt-tuning technique to reform and improve current GCL-based recommendation methods. To better accommodate prompt learning to graph recommendation scenarios, we summarise several graph-oriented user profiles to generate personalized user prompts to conduct prompt-tuning for downstream recommendation tasks. Comprehensive experiments have shown the effectiveness, superiority, and properties of the proposed CPTPP method. The future research directions about prompt-tuning in GCL-based recommendation may be two-fold: how to (i) generate personalized prompts and (ii) integrate prompt-tuning strategy into GCL protocols.