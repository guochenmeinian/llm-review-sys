# Stability and Generalization of Asynchronous SGD:

Sharper Bounds Beyond Lipschitz and Smoothness

 Xiaoge Deng   Tao Sun1 Shengwei Li   Dongsheng Li1  Xicheng Lu

College of Computer Science and Technology

National University of Defense Technology, China

dengxg@nudt.edu.cn, suntao.saltfish@outlook.com, lucasleesw9@gmail.com

dsli@nudt.edu.cn, xclu@nudt.edu.cn

###### Abstract

Asynchronous stochastic gradient descent (ASGD) has evolved into an indispensable optimization algorithm for training modern large-scale distributed machine learning tasks. Therefore, it is imperative to explore the generalization performance of the ASGD algorithm. However, the existing results are either pessimistic and vacuous or restricted by strict assumptions that fail to reveal the intrinsic impact of asynchronous training on generalization. In this study, we establish sharper stability and generalization bounds for ASGD under much weaker assumptions. Firstly, this paper studies the on-average model stability of ASGD and provides a non-vacuous upper bound on the generalization error, without relying on the Lipschitz assumption. Furthermore, we investigate the excess generalization error of the ASGD algorithm, revealing the effects of asynchronous delay, model initialization, number of training samples and iterations on generalization performance. Secondly, for the first time, this study explores the generalization performance of ASGD in the non-smooth case. We replace smoothness with the much weaker Holder continuous assumption and achieve similar generalization results as in the smooth case. Finally, we validate our theoretical findings by training numerous machine learning models, including convex problems and non-convex tasks in computer vision and natural language processing.

## 1 Introduction

The last decade has witnessed explosive growth in the scale of models and datasets in the machine learning (ML) community . In light of this tendency, asynchronous distributed optimization has become crucial to ensure efficient training of large-scale ML models . Specifically, the _asynchronous stochastic gradient descent_ (ASGD) algorithm eliminates the synchronization barrier between the distributed training workers, enabling each worker to independently perform idle-free asynchronous gradient updates, thereby accelerating model training. Despite this asynchronous updating introduces delays that result in model inconsistency, the convergence of ASGD is still guaranteed under some mild assumptions .

An intriguing observation is that ML models learned by stochastic gradient descent (SGD)  not only achieve zero training error but also demonstrate good generalization performance on unknown test datasets . Generalizability is a classical topic in the statistical ML fields, and associated analytical techniques include VC dimension , Rademacher complexity , PAC-Bayesian , uniform convergence , information-based, and compression-based bounds . In this paper, we are going to study generalizability in the sense of algorithmic stability . This stability-based analytical framework allows bypassing the model dimensionality so that we can focus onexploring the generalization properties of optimization algorithms. Hardt et al.  investigated the generalization error of SGD on the basis of algorithmic uniform stability. Assuming that the loss function is convex, \(L\)-Lipschitz and \(\)-smooth, and running SGD for \(K\) iterations with a learning rate \(_{k}<2/\), they obtained an upper bound on the generalization error of \((L^{2}_{k=1}^{K}_{k}/n)\), where \(n\) represents the total number of training samples. In a recent work , they proposed the on-average model stability and established a tighter generalization bound of \((1/n)\) for low-noise settings, without requiring the \(L\)-Lipschitz assumption.

Research on the generalization of asynchronous stochastic gradient descent algorithms mainly concentrates on parsing the effect of asynchronous delay \(\) on algorithm stability and generalization. Leveraging the algorithmic uniform stability tool, Regatti et al.  presented an upper generalization error bound of \((L^{2}K^{}/n)\) in the non-convex case, assuming \(L\)-Lipschitz, \(\)-smooth functions, and a decreasing learning rate. However, empirical experiments  show that this bound is too loose to reflect the effect of asynchronous delay on algorithmic stability accurately. Deng et al.  directed their attention towards convex quadratic functions and established an upper bound on the generalization error as \(}((K-)/n)\) by utilizing the algorithmic average stability . This bound suggests that the introduced asynchronous delays can enhance algorithm stability, consequently improving its generalization performance under appropriate learning rates. Unfortunately, the analytical technique proposed in  is confined to quadratic optimizations.

In this study, we delve deeper into the generalization performance of the ASGD algorithm. In particular, we utilize the on-average model stability tool to conduct a fine-grained analysis of the stability and generalization for ASGD under much weaker assumptions. Our contributions are summarized as follows.

* Without relying on the Lipschitz assumption, this study establishes the on-average model stability of ASGD and provides an upper bound on the generalization error of \((1/+1/)\). In contrast to existing work [13; 33], our results are non-vacuous and applicable to the general convex case.
* For the first time, we study the excess generalization error and provide an upper bound of \((1/+\|_{1}-^{*}\|^{2}/n)\) for ASGD. Our findings demonstrate that appropriately increasing the asynchronous delay, selecting a good initial model, and increasing the number of training samples can improve the generalization performance.
* Under the much weaker \((,)\)-Holder continuous gradient assumption, we establish an excess generalization error bound of \((1/}+\|_{1}-^{*}\|^{ {}{1+}}/^{1+})\), which reveals similar properties to the smooth case. To the best of our knowledge, this is the first study of the stability and generalization of ASGD in the non-smooth case.
* We conduct comprehensive experiments using the ASGD algorithm, covering convex optimization problems and non-convex computer vision and natural language processing tasks. Empirical evidence confirms that appropriately increasing the asynchronous delay improves the algorithm stability and reduces the generalization error, which is consistent with our theoretical findings.

## 2 Related Work

**Asynchronous training,** with origins dating back at least to [6; 45], has emerged as an essential distributed method for training modern large-scale ML tasks. It effectively addresses the synchronization bottleneck among multiple workers and mitigates the straggler problem inherent in distributed systems . This study focuses on the stochastic gradient descent algorithm with asynchronous updates [1; 30]. Lian et al.  proved that ASGD has an asymptotic sublinear convergence rate in non-convex smooth optimization, which is consistent with SGD. Arjevani et al.  provided tight upper and lower complexity bounds for ASGD in convex quadratic optimization. These theoretical results were subsequently extended to general quasi-convex and non-convex settings . It is noteworthy that the aforementioned theoretical analyses are based on bounded or fixed delay assumptions, whereas recent studies [11; 28] explored the performance of ASGD under arbitrary delays.

A crucial aspect of asynchronous research revolves around the interaction between learning rates and delays. For one thing, most existing theoretical analyses require the learning rate to be inversely proportional to the asynchronous delay to guarantee the convergence of the ASGD algorithm [2; 26; 40]. For another, numerous studies opt for adaptive adjustments of the learning rate based on varying asynchronous delays to improve the convergence rate of ASGD [34; 38; 49; 53]. The dependence of learning rate on asynchronous delay also influences the stability and generalization studies of ASGD presented in this paper.

**Algorithm stability** originated from perturbation analysis , which measures the difference in the algorithm's output from changing a single input training sample. Generalization error refers to the performance disparity of the output model between training and testing datasets. Hence, algorithm stability is naturally connected to generalizability [8; 16; 36]. For the mainstream SGD algorithm, extensive stability-based studies have been conducted for convex, non-convex, smooth and non-smooth cases [5; 17; 22; 23; 32; 54]. Recently, algorithm stability analysis has been extended to distributed training scenarios . Considerable research has explored the generalization performance of distributed decentralized SGD from the stability perspective [14; 43; 55].

However, the current generalization studies for ASGD remain inadequate. Building upon the algorithmic uniform stability, Regatti et al.  presented a pessimistic generalization error bound \((K^{}/n)\) of ASGD in the smooth non-convex case, where \(\) represents the maximum delay. In a recent development, Deng et al.  established a tighter upper generalization error bound of \(((K-)/n)\) using average stability, and Sun et al.  investigated a high-probability PAC-Bayesian generalization error bound \((1/)\) for ASGD. However, the theoretical analyses presented in [13; 41] only hold in quadratic optimization problems, limiting their applications. To the best of the authors' knowledge, existing generalization analyses of ASGD are either pessimistic and vacuous or constrained by strict assumptions. Therefore, the objective of this study is to establish sharper stability and generalization bounds for ASGD under much milder assumptions.

## 3 Preliminaries

**Notations**. Lowercase and bold letters represent scalars and \(d\)-dimensional column vectors, respectively. The \(_{2}\)-norm of a vector \(\) is denoted by \(\|\|\). Calligraphic capital letters represent mathematical sets. We write \(a=(b)\) if there exists a constant \(0<c<+\) such that \(a c b\), and \(}()\) hides logarithmic factors. Moreover, we denote \(a b\) if \(a=(b)\) and \(b=(a)\).

Let \(^{d}\) and \(\) denote the input and output spaces, respectively. In this study, we focus on the general supervised learning problem in ML. This task involves training a model on a data set \(=\{_{1},,_{n}\}\), where each data point \(_{i}=(_{i},y_{i})=\) is independently and identically distributed (i.i.d.) sampled from an unknown distribution \(\). We evaluate the performance of model \(\) on training sample \(\) with a loss function \(f(;)\). The training process can be formalized as learning a model parameter \(^{d}\) to minimize the empirical risk, denoted as

\[_{}\;F_{}()=_{i=1} ^{n}f(;_{i}). \]

SGD is the workhorse for solving the empirical risk minimization (ERM) problem (1), which iteratively updates the model parameter by \(_{k+1}=_{k}-_{k} f(_{k};_{ i_{k}})\).

ASGD is a powerful variant of SGD for distributed learning, which fully exploits the computational power of distributed clusters to accelerate the training process. In the distributed parameter server architecture , the distributed workers are responsible for computing gradients, while the model updates occur on the parameter server side. Upon receiving the gradient from a worker, the server immediately utilizes it to update the model without waiting for gradient information from other workers. The ASGD procedure is described in Algorithm 1 (located in Appendix A.1). It is noteworthy that although ASGD avoids synchronization overhead, it introduces delays in model updating. To be specific, while worker \(m\) is computing and uploading the gradient, the model parameter on the server side may has already been updated by another worker \(m^{}\). In essence, the model used for gradient computation on the worker is inconsistent with the model updated by the server. This characteristic renders ASGD a delayed gradient update, expressed as

\[_{k+1}=_{k}-_{k} f(_{k-_{k}}; _{i_{k}}), \]

where \(_{k},_{k},_{k}\), and \(_{i_{k}}\) denote the model parameter, learning rate, asynchronous delay, and training sample at the \(k\)-th iteration, respectively. It is worth noting that the index \(i_{k}\) is chosen uniformly at random from the set \(\{1,,n\}\).

For the model \(\) learned through ASGD by minimizing the empirical risk (1) on the training data set \(\), people are more concerned with its performance on the unknown distribution \(\), i.e., the following popular risk

\[F()=_{}[f(;)]. \]

The empirical risk (1) and the popular risk (3) of a model are not the same, and the difference between them is referred to generalization error. More formally, denote the model learned by algorithm \(A\) on data set \(\) as \(A()\), and its _generalization error_ is defined as

\[_{}:=_{,A}[F(A())-F_{ }(A())]. \]

The expectation here is taken over the randomness of the algorithm and the training data. This study is dedicated to bounding \(_{}\) by algorithmic stability. Let

\[^{}=\{_{1}^{},..,_{n}^{}\}, ^{(i)}=\{_{1},..,_{i-1},_{i}^{ },_{i+1},..,_{n}\}. \]

\(^{}\) is also a data set i.i.d. sampled from the unknown distribution \(\), but is independent of the data set \(=\{_{1},,_{n}\}\). \(^{(i)}\) is a perturbed data set formed by replacing the \(i\)-th sample in \(\) with \(_{i}^{}\). Based on these notations, Lei and Ying  defined the following _on-average model stability_.

**Definition 1** (On-average model stability).: A randomized algorithm \(A\) is on-average model \(_{}\)-stable if

\[_{,^{},A}_{i=1}^{ n}\|A()-A(^{(i)})\|^{2}_{ }.\]

Leveraging the smoothness (Definition 2) assumption, the connection between this algorithmic stability and the generalization error \(_{}\) is established in the following lemma [Theorem 2, ].

**Lemma 1**.: _Let \(\!>\!0\). Assume that the function \(\!\!f(;)\) is non-negative and \(\)-smooth for any \(\). Then, if algorithm \(A\) is on-average model \(_{}\)-stable, the generalization error satisfies_

\[_{,A}[F(A())-F_{}(A())]_{,A}[F_{}(A ())]+_{}.\]

While the smooth function assumption is common in optimization and generalization analyses , it does impose constraints on the applicability . For instance, the hinge loss, which is widely used in the ML fields, does not satisfy the smooth property. In this paper, therefore, we also investigate the stability of ASGD under the much weaker Holder continuous gradient assumption (Definition 3), so as to establish broader and fine-grained generalization results. With the Holder continuous condition, stability and generalization can be connected similarly to Lemma 1.

**Lemma 2** (Theorem 2, ).: _Let \(>0\). For any \(\), the function \( f(;)\) is non-negative, convex, and the gradient \( f(;)\) is \((,)\)-Holder continuous. If algorithm \(A\) is on-average model \(_{}\)-stable, then the generalization error satisfies_

\[_{,A}[F(A())-F_{}(A())]^{2}}{2}_{,A}[F ^{}(A())]+_{}.\]

_Here, \(\), and \(c_{,}\) is a constant dependent on \(,\)._

Furthermore, since the generalization performance of a model is primarily reflected in the popular risk (3), this study also examines the _excess generalization error_, denoted as \(_{gen}}\), where \(^{*}_{}F()\) and

\[_{gen}}:=_{,A}[F(A( ))-F(^{*})]. \]

**Definition 2** (Smoothness).: The function \( f(;)\) is \(\)-smooth (\(>0\)) if for any \(\) and \(,^{d}\),

\[\| f(;)- f(;)\| \|-\|.\]

**Definition 3** (Holder continuous).: Let \(\), \(>0\). The function \( f(;)\) is \((,)\)-Holder continuous if for any \(\) and \(,^{d}\),

\[\| f(;)- f(;)\| \|-\|^{}.\]

It is noteworthy that the \((,)\)-Holder continuous gradient is equivalent to a \(\)-smooth function when \(=1\). Whereas \(=0\) implies that the function gradient is bounded, i.e., there exists a constant \(L>0\) such that \(\| f(;)\| L\). Although many analyses of ASGD are grounded on the bounded gradient condition , this assumption is somewhat unrealistic . Notably, our analysis of the algorithm stability and the generalization error does not rely on the bounded gradient assumption.

Stability and Generalization Bounds

This section explores the stability and generalization of the ASGD algorithm in the context of smooth loss functions, and the proof is given in Appendix B. Firstly, we present the assumption required for this study.

**Assumption 1**.: The parameter space \(^{d}\) is a bounded convex set. Then, for any \(,\), there exists a constant \(r>0\) such that \(\|-\| r\).

Assumption 1 is standard in analyzing SGD and its variants, as it is easy to hold with the projection operator [5; 17; 23; 30; 43]. More specifically, we consider the following projected ASGD updates

\[_{k+1}=_{}_{k}-_{k} f(_{k-_{k}};_{i_{k}}). \]

Since the projection operator \(_{}\) is non-expansive, it has no impact on the stability and generalization analysis of the ASGD algorithm.

**Remark 1**.: Let \(_{k}\) and \(_{k}^{(i)}\) denote the models produced by ASGD (7) after \(k\) iterations on the datasets \(\) and \(^{(i)}\) (defined in (5)), respectively. According to Assumption 1, it follows that \(\|_{k}-_{k}^{(i)}\| r\). Notably, this result is intuitively understandable as the datasets \(\), \(^{(i)}\) differ only by a single sample, and the initialization is the same (\(_{1}=_{1}^{(i)}\)). In contrast to a recent work , where the authors assumed a normal distribution with bounded mean and variance for the difference between models \(_{k}\) and \(_{k}^{(i)}\), our study does not necessitate such a strong assumption.

### Algorithmic Stability of ASGD

The stability-based analysis of SGD hinges significantly on the non-expansiveness of the gradient update operator [17; 23]. Namely, if function \(f\) is convex and smooth, then \(,\), \(\)

\[\|- f(;)-(- f( ;))\|\|-\|.\]

However, this well-posed property is no longer applicable in the context of asynchronous gradient updates. To address this issue, we present the following critical lemma to bound the delayed gradient update operator.

**Lemma 3**.: _Let the loss function be convex, \(\)-smooth, and Assumption 1 holds. Denote \(_{k}\) and \(_{k}^{(i)}\) as the models produced by ASGD (7) with learning rates \(_{k} 2/\) for \(k\) iterations on the datasets \(\) and \(^{(i)}\), respectively. Then_

\[_{k}-_{k} f(_{k-_{k}};_{i _{k}})-_{k}^{(i)}-_{k} f(_{k-_{k}}^{ (i)};_{i_{k}})^{2}_{k}-_{k}^{(i)}^{2}+2_{k}^{2}r^{2}_{j=1}^{_{k}}_{k-j}.\]

By leveraging the properties established in Lemma 3, we can demonstrate an approximately non-expansive recursive property for \(\|_{k+1}-_{k+1}^{(i)}\|^{2}\) and subsequently establish the on-average model stability (Definition 1) of the ASGD algorithm as follows.

**Theorem 1** (Stability).: _Suppose the loss function is non-negative, convex, and \(\)-smooth. Let Assumption 1 holds. If we run ASGD (7) with a non-increasing learning rate \(_{k} 1/2\) for \(k\) iterations, then the on-average model stability satisfies (\(e\) is the natural constant)_

\[_{}=(1+k/n)}{n}_{1}\| _{1}-^{*}\|^{2}+4 r^{2}+2F(^{*}) _{l=1}^{k}_{l}^{2}+2^{2}r^{2}_{l=1}^{ k}_{l}_{j=1}^{_{l}}_{l-j}.\]

In line with the findings of study , increasing the number of training iterations impairs the stability of ASGD. Compared to SGD , we introduce an additional term \((_{l=1}^{k}_{l}_{j=1}^{_{l}}_{l-j})\) to characterize the effect of asynchronous delay on the stability of ASGD. Also similar to the data-dependent stability study , Theorem 1 indicates that model initialization affects the algorithmic stability, i.e., selecting a better model initiation point \(_{1}\) can effectively improve the stability.

### Generalization Error Bounds

Together with Lemma 1 and Theorem 1, we can now present the generalization error (4) of the ASGD algorithm under smooth conditions.

**Theorem 2** (Generalization error).: _Let Assumption 1 holds, and assume that the loss function is non-negative, convex, and \(\)-smooth. Running ASGD (7) with a non-increasing learning rate \(_{k} 1/2\) for \(K\) iterations, then the generalization error is given by_

\[_{}=_{,A}[F_{ }(_{K})]+_{k=1}^{K}_{k}_{j=1}^{_{k}}_{k -j}+_{1}\|_{1}-^{*}\|^{2}+ 1+F(^{*})_{k=1}^{K}_{k}^{2}.\]

This finding suggests that both the model initialization and optimization processes have an impact on the generalization performance. In practical applications, one can reduce the generalization error by selecting a good initial model \(_{1}\) to start the training task. Additionally, it is crucial to finish the optimization process promptly since too many training iterations can detrimentally affect the generalization performance.

Furthermore, Theorem 2 reveals a close relationship between the generalizability of ASGD and the learning rate. As discussed in Section 2, asynchronous training typically utilizes delay-inverse correlated learning rates to ensure algorithmic performance. In the low-noise case, namely, \(F(^{*})=0\), Stich and Karimireddy  demonstrated that \(F_{}(_{K})=(1/)\) for ASGD under the conditions of smooth and general quasi-convex loss functions, with a learning rate of \(_{k}=c()^{-1}\). Employing this learning rate strategy, the following corollary can be derived.

**Corollary 1**.: _Let \(F(^{*})=0\), \(K n\), and the conditions specified in Theorem 2 hold. If we set the learning rate \(_{k}=c()^{-1}\) with a constant \(c>0\) and \(=_{k=1}^{K}_{k}/K\), then the generalization error satisfies_

\[_{,A}[F(_{K})-F_{}(_{ K})]=}+}.\]

At this point, although the asynchronous training also introduces an additional generalization error term of \((1/)\), increasing the delay can instead mitigate this detriment. Unlike previous ASGD generalization research [14; 33], this study does not rely on the Lipschitz assumption. In contrast to the vacuous upper bound of \((K^{}/n)\) in , we provide a sharper result and demonstrate that increasing the asynchronous delay reduces the generalization error. While Deng et al.  present a similar result \(((K-)/n)\) with respect to the maximum delay \(\) in the convex quadratic optimization, our bound holds in general convex settings. Furthermore, our results are associated with the average delay \(\) rather than the pessimistic maximum delay \(\) in [13; 14; 33].

### Excess Generalization Error

According to definitions (4) and (6), the excess generalization error \(_{}\) can be decomposed as

\[_{}=_{}+_{,A}[ F_{}(A())-F_{}(^{*})], \]

where the second term is known as the optimization error. The analysis of optimization error for ASGD usually requires the following bounded gradient assumption [26; 28; 33].

**Assumption 2**.: The gradient \( f(;)\) is bounded. That is, for any \(,\), there exists a constant \(L>0\) such that \(\| f(;)\| L\).

**Remark 2**.: Assumption 2, also known as the Lipschitz condition, is used in the optimization analysis of ASGD to bound the model deviations induced by asynchronous delays, i.e., \(\|_{k}-_{k-_{k}}\| L_{j=1}^{_{k}}_{k-j}\).

For the excess generalization error of ASGD, we shift our focus to the average model \(}_{K}:=_{k=1}^{K}_{k}_{k}/_{k=1}^{K }_{k}\). It is noteworthy that since the parameter space \(\) is a convex set, \(}_{K}\) and is frequently considered as the output of the ASGD algorithm. We first present the optimization error with respect to this average model in the following lemma, followed by the excess generalization error theorem of ASGD.

**Lemma 4**.: _Assuming that the loss function is non-negative, convex, and \(\)-smooth. Let Assumptions 1 and 2 hold, if we run ASGD (7) with a non-increasing learning rate \(_{k} 1/2\), then the optimization error satisfies_

\[_{,A}[F_{}(}_{K})-F_{ }(^{*})]=_{1}- ^{*}\|^{2}}{_{k=1}^{K}_{k}}+1+F(^{*}) ^{K}_{k}^{2}}{_{k=1}^{K}_{k}}+^{K }_{k}_{j=1}^{_{k}}_{k-j}}{_{k=1}^{K}_{k}}.\]

**Theorem 3** (Excess generalization error).: _Let Assumptions 1, 2 hold, and assume that the loss function is non-negative, convex, and \(\)-smooth. Running ASGD (7) with the non-increasing learning rate \(_{k} 1/2\) for \(K\) iterations, then the excess generalization error is_

\[_{}= 1+^{K}_{k}^{2}}{ _{k=1}^{K}_{k}}F(^{*})+_{1} \|_{1}-^{*}\|^{2}+1+F(^{*})_{k =1}^{K}_{k}^{2}\] \[+_{1}-^{*}\|^{2}}{_{k=1}^{K} _{k}}+_{k=1}^{K}_{k}_{k}+_{j=1}^{_{k}} _{k-j}+_{l=1}^{k}_{l}_{j=1}^{_{l}}_{l-j} /_{k=1}^{K}_{k}.\]

Compared to the generalization error in Theorem 2, the excess generalization error is no longer explicitly dependent on the optimization error \(F_{}(_{K})\) and is more closely coupled to the learning rate. Considering the low-noise case \(F(^{*})=0\), which is common in modern deep learning, the following corollary can be further derived.

**Corollary 2**.: _Let \(F(^{*})=0\), \(K\!\!n\) and the conditions in Theorem 3 hold. Set the learning rate as \(_{k}=c()^{-1}\) with a constant \(c>0\) and \(=_{k=1}^{K}_{k}/K\). Then if \( K^{}\), the excess generalization error satisfies_

\[_{,A}[F(}_{K})-F( ^{*})]=}+_{1}-^{*}\|^{2}}{n}.\]

To the best of our knowledge, this is the first excess generalization error result for the ASGD algorithm. Compared to Corollary 1, this generalization bound with appropriate delays is sharper and no longer relies on the optimization error result in .

## 5 Generalization in Non-smooth Case

This section investigates the stability and generalization of the ASGD algorithm in the context of non-smooth cases. The analysis follows a similar technical roadmap as in Section 4. Firstly, we derive the stability of ASGD by leveraging the approximately non-expansive property of the delayed gradient update operators. Then, the generalization error is given in conjunction with Lemma 2. Subsequently, we analyze the optimization process of ASGD and present the excess generalization error for the non-smooth settings.

However, without the smooth condition, the non-expansive property of asynchronous gradient updates is further compromised, and the optimization process also introduces additional errors. Under the much weaker Holder continuous gradient assumption, We establish similar stability and generalizability results for ASGD as in the smooth case, which has not been explored in existing research. Please refer to Appendix C for the proof details of this section.

**Lemma 5**.: _Let Assumption 1 holds, and assume that the loss function is non-negative, convex, and has a \((,)\)-Holder continuous gradient. Then, the delayed gradient update operator satisfies_

\[_{k}-_{k} f(_{k-_{k}}; _{i_{k}})-(_{k}^{(i)}-_{k} f(_{k-_ {k}}^{(i)};_{i_{k}}))^{2}=\!\|_{k}-_{k} ^{(i)}\|^{2}+(_{k}_{j=1}^{_{k}}_{k-j}+_{k}^{ }).\]

Compared to Lemma 3, an additional term \((_{k}^{})\) is introduced here to compensate for the absence of smoothness. Fortunately, since the coefficient of \(\|_{k}-_{k}^{(i)}\|^{2}\) is not larger than \(1\), the delayed gradient update of ASGD remains approximately non-expansive at an appropriate learning rate. Leveraging this property, we are able to give the on-average model stability of ASGD in the non-smooth case.

**Theorem 4** (Stability).: _Suppose the loss function is non-negative, convex, and has a \((,)\)-Holder continuous gradient. Let Assumption 1 holds. Then, the on-average model stability of ASGD satisfies_

\[_{}=_{l=1}^{k} _{l}^{2}_{,A}F_{}^{}(_{l-_{l}})+_{l=1}^{k}_{l}_{j=1}^{ _{l}}_{l-j}+_{l=1}^{k}_{l}^{}.\]Theorem 4 shows that the algorithmic stability of ASGD not only depends on the learning rate, but is also closely related to the optimization process. Similar to , we replace the gradient bound (Lipschitz constant) in the uniform stability  with the loss function value, which leads to sharper stability and generalizability results when combined with the subsequent optimization analysis. Substituting this algorithm stability into Lemma 2 yields the generalization error of the ASGD algorithm under the Holder continuous condition (omitted in Appendix C.3).

**Remark 3**.: Although Assumption 1 and the smooth (or Holder continuous) condition implies Lipschitz continuity, our point is to replace the upper gradient bound with function value, thereby establishing sharper stability and generalization bounds that do not depend on the Lipschitz constant.

Subsequently, we present the optimization error of ASGD in the non-smooth case, and the excess generalization error is followed by the decomposition (8).

**Lemma 6**.: _Assuming that the loss function is non-negative, convex, and has a \((,)\)-Holder continuous gradient. Let Assumptions 1 and 2 hold, then the optimization error of ASGD (7) with a non-increasing learning rate satisfies_

\[_{,A}[F_{}(}_ {K})- F_{}(^{*})]=_ {1}-^{*}\|^{2}+_{k=1}^{K}_{k}_{j=1}^{_{k}}_{k-j} ^{}}{_{k=1}^{K}_{k}}\\ +_{k=1}^{K}_{k}^{2}^{ {1+}}}{_{k=1}^{K}_{k}}_{1}\|_{1}-^{*}\|^{2}+1+F(^{*})_{k=1}^{K}_{k}^{2}+_{ k=1}^{K}_{k}^{}^{}.\]

**Theorem 5** (Excess generalization error).: _Let Assumptions 1, 2 hold, and assume that the loss function is non-negative, convex, and has a \((,)\)-Holder continuous gradient. Running ASGD (7) with the learning rate \(_{k}=c()^{-1}\) for \(K\!\!n\) iterations, then if \(F(^{*})=0\) and the average delay satisfies \( K^{^{}}\) with \(^{}=\{,\}\), the excess generalization error is_

Notably, the generalization performance decreases in the non-smooth case, but the underlying properties remain consistent with the smooth setting (Corollary 2). That is, the generalization performance can be improved by choosing a good initial model, increasing the number of training samples, and appropriately adjusting the asynchronous delays. Additionally, when there is no asynchronous delay in the training system, the first term in Theorem 5 vanishes, yielding an excess generalization error bound of \((1/^{1+})\). This outcome is consistent with the findings from the study of the SGD algorithm in , but without requiring more computation \(K n^{}\).

## 6 Experimental Validation

In this section, we extensively evaluated various machine learning tasks under the distributed parameter server architecture to investigate the practical stability and generalization performance of ASGD. Our experiments included convex optimization problems as well as non-convex computer vision (CV) and natural language processing (NLP) tasks. We simulated a distributed system with \(M\!=\!16\) workers and performed asynchronous training in a more general stochastic gradient descent format as follows

\[_{k+1}=_{k}-_{k}_{m_{k}}_{ k-_{k}}^{m}. \]

Here, \(_{k}\) is a non-empty subset of \(\{1,,M\}\) containing the workers that participated in asynchronous training at the \(k\)-th iteration, and \(_{k-_{k}}^{m}\) represents the delayed gradient computed by worker \(m\) on model \(_{k-_{k}}\). Our experiments also focus on parsing the impact of asynchronous delays on algorithmic stability and generalization. Following our theoretical findings, we set the learning rate to \(0.1/\) for different delays, where \(\) denotes the average delay.

For the convex optimization problem, we employed a single-layer linear network with the mean squared error for a classification task on the RCV1 data set from the LIBSVM database . This data set contains \(20,242\) training data with \(47,236\) features per sample. In the field of computer vision, we chose the popular ResNet18 model for image classification on the CIFAR10 and CIFAR100datasets. ResNet , a convolutional neural network with residual modules and shortcut connections, has demonstrated remarkable performance across various CV tasks. CIFAR10 and CIFAR100  are widely used image datasets, both containing \(60,000\) color images of \(32 32\) pixels. For natural language processing tasks, we conducted experiments using BERT on the SST-2 task within the GLUE platform . BERT  is a pre-trained language model based on the Transformer architecture, known for its impressive performance in handling various NLP tasks. The SST-2  task in the GLUE evaluation benchmark comprises a total of \(67,350\) training samples for single-sentence categorization.

Due to computational resource limitations, this experiment cannot sequentially replace a single sample to train \(n\) models and calculate the on-average model stability (Definition 1). Instead, we construct a perturbed data set \(^{(i)}\) by randomly removing a sample from the data set \(\), and then train on the two datasets separately to record the model difference \(\|A()-A(^{(i)})\|^{2}\). Repeating the process multiple times, we take the average value to approximate the algorithmic stability. As for the generalization error (4), it is directly approximated by the absolute difference between the training error and the testing error of the model.

Figure 1 and Figure 2 (located in Appendix D) illustrate the generalizability and stability of the ASGD algorithm in training the three types of machine learning tasks. The experimental results show that continuous training impairs the stability and generalization of ASGD, which is consistent with the theorems presented in Sections 4 and 5. Conversely, when training with a learning rate that is inversely correlated with the asynchronous delay, an appropriate increase in the delay improves the algorithm stability and thus reduces the generalization error. This observation is in consistent with the theoretical bound in Corollary 1, which utilizes the specific learning rate \(_{k}=c/\).

## 7 Concluding Remarks

This study establishes sharper and broader stability and generalization bounds for ASGD under much weaker assumptions. We provide upper bounds for the on-average model stability and generalization error of ASGD without relying on the Lipschitz continuous condition. Moreover, for the first time, we study the stability and generalizability of ASGD in the non-smooth setting. Our generalization results are non-vacuous and applicable to the general convex case. Furthermore, we validate our theoretical findings with experiments on various machine learning tasks.

We also conducted experiments using delay-independent learning rates (Figures 3 and 4 in Appendix D). Interestingly, these results also suggest that asynchronous training is beneficial for generalization. This empirical finding challenges the pessimism of our generalization error result under constant learning rates (omitted in Appendix B.4), and motivates further exploration of the generalizability of ASGD. There are several directions for future research. The study of non-convex problems can focus on showing that asynchronous updates are approximately non-expansive even without convexity, then leading to non-vacuous stability and generalization results. Another avenue for research involves investigating tighter high probability bounds that attenuate the dominant role of the learning rate on generalization, thereby elucidating the experimental phenomena in Appendix D.

Figure 1: The generalization errors of three categories of machine learning models trained using ASGD with learning rate \(_{k}=0.1/\). The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in generalization performance is observed as the number of training iterations increases, and the generalization performance can be improved by appropriately increasing the asynchronous delay.