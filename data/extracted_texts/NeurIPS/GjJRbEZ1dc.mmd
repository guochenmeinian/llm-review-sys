# Multitask Learning with No Regret:

from Improved Confidence Bounds to Active Learning

Pier Giuseppe Sessa

ETH Zurich

piergiuseppe.sessa@inf.ethz.ch

equal contributionPierre Laforgue

Universita degli Studi di Milano

pierre.laforgue@unimi.it

Nicolo Cesa-Bianchi

Universita degli Studi di Milano

Politecnico di Milano

nicolo.cesa-bianchi@unimi.it

Andreas Krause

ETH Zurich

krausea@ethz.ch

###### Abstract

Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel confidence intervals for multitask regression in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second key application of our results, we introduce a novel multitask active learning setup where several tasks must be simultaneously optimized, but only one of them can be queried for feedback by the learner at each round. For this problem, we design a no-regret algorithm that uses our confidence intervals to decide which task should be queried. Finally, we empirically validate our bounds and algorithms on synthetic and real-world (drug discovery) data.

## 1 Introduction

In many real-world applications, one often faces multiple related tasks to be solved sequentially or simultaneously. The goal of multitask learning (MTL)  is to leverage the similarities across the tasks to obtain more accurate and robust models. Indeed, by jointly learning multiple tasks, MTL can exploit their statistical dependencies, yielding better generalization and faster learning than treating each task independently. MTL has gained significant attention in recent years, as it has been shown to be effective in a wide range of applications, including natural language processing, computer vision, federated learning, and drug discovery, see e.g., .

A very natural model for learning across multiple tasks is the agnostic multitask (MT) regression approach of . This utilizes a multitask kernel that can interpolate between running \(N\) (number of tasks) independent regressions, and regressing all tasks to their common average, depending on a tunable parameter. Notably, such a kernel does not require any knowledge neither about tasks' featuresnor about their similarity, thus finding good application in several domains. For instance, Cavallanti et al.  study it for online classification, and Cesa-Bianchi et al.  for online convex optimization.

However, it is much less understood how to _quantify the uncertainty_ of such MT regression, i.e., assessing confidence in the estimated tasks. In particular, as also outlined by  as an open problem, it is important to assess their generalization error as a function of the kernel parameter. Appropriately characterizing these confidence intervals is indeed of crucial importance for a whole set of downstream applications. More concretely, multitask confidence intervals are used in online learning to inform the next decision to be made . In active learning--as we show next--these intervals are pivotal to deciding the most informative task to query.

In this work, we study the agnostic MT regression setup of , and provide _new multitask confidence intervals_ (see Figure 1 for a visualization) for the full range of the kernel parameter. Our intervals hold in the so-called adaptive setting, i.e., without requiring i.i.d. data, and are _tighter up to a \(\) factor_ than the naive ones employed in . Moreover, we provide the first bounds for the information gain of MT regression and utilize them--together with the derived intervals--to obtain _tighter online learning guarantees_. The latter depend on a task similarity parameter and can significantly improve over treating tasks independently. Additionally, we propose an adaptive no-regret algorithm that exploits task similarity without knowing this parameter in advance. Finally, we consider a novel multitask _active learning_ setup, where tasks should be simultaneously optimized but only one of them can be queried at each round. We show that the newly derived intervals are also crucial in such a setting, and provide a new algorithm that ensures sublinear regret. We demonstrate the superiority of the derived intervals over previously proposed algorithms on synthetic as well as real-world drug discovery tasks.

**Related work.** The agnostic MT regression approach of  reduces the learning of \(N\) tasks to a single regression problem, as a function of the MT kernel parameter. When combined with support vector machines, it was shown effective in a series of classification problems , and since then was studied in various further settings. Cavallanti et al. , e.g., analyze mistake bounds for online MT classification algorithms as a function of the kernel parameter. Cesa-Bianchi et al. , instead, utilize the MT kernel to prove regret bounds in online MT learning with bandit feedback. Inspired by this,  focuses on learning more general kernel structures from data. An important question not addressed by previous work, though, is how to properly quantify the uncertainty of the obtained task estimates. This problem is well-understood in single-task learning (e.g., ) but remains largely unexplored in MT domains. As shown in , MT confidence intervals can in principle be obtained by a naive application of the single-task guarantees of . However, as we show in Section 2, the so-obtained intervals are extremely conservative and--as a result--can hamper the MT learning performance. Our intervals are tighter by a factor up to \(\) w.r.t. the naive ones from , yielding novel online learning regret guarantees which can provably improve over treating tasks independently.

Compared to MT online learning , where a single task is revealed to the learner at each round, a series of works have considered learning multiple tasks _simultaneously_, i.e., taking a decision for each one of them. Dekel et al. , e.g., propose the use of a shared loss function to account for tasks' relatedness, Lugosi et al.  studies the computational tractability of taking multiple actions with joint constraints, while Cavallanti et al.  propose a matrix-based extension of the multitask Perceptron algorithm. In all of these works, however, the learner receives feedback from _all_ the

Figure 1: Independent vs. Multitask (MT) regression. MT regression leverages data coming from multiple related tasks and can yield more accurate and more confident estimates. In this work, we show naive confidence intervals are overly conservative and provide improved ones (shaded in red).

[MISSING_PAGE_FAIL:3]

Information gain.An important quantity when analyzing (multitask) kernel regression is the so-called _(multitask) information gain_:

\[_{T}^{}(b)=I_{T}+^{-1}K_{T}\,.\]

It can be interpreted as the reduction in uncertainty about \(f^{}\) after having observed a given set of \(T\) datapoints. Similarly to single-task setups [25; 8], we use \(_{T}^{}\) in the next sections to characterize our confidence intervals and regret bounds. Note that \(_{T}^{}\) depends on the multitask kernel through \(K_{T}\), and hence on \(b\). In Section 3, we exploit the properties of our multitask kernel to obtain a sharper control over \(_{T}^{}\), which is then fundamental to derive improved regret bounds.

### Improved Confidence Intervals

In this section, we utilize the regression estimates obtained in Equations (3) and (4) to construct high probability confidence intervals around the unknown multitask function \(f^{}\). First, we assume that \(\|f_{i}\| B\) for all \(i[N]\), as it is standard in single-task regression. Moreover, let \(f_{}=(1/N)_{i=1}^{N}f_{i}\) be the average task function, and define

\[=_{i}\,\|f_{i}-f_{}\|/B\,. \]

Note that by definition \(\). The smaller \(\), the more similar the tasks are, the limit case being that all tasks are equal, attained at \(=0\). At the other extreme, when \( 0\) tasks are highly distant and ought to be learned independently. The deviation \(\) plays a crucial role in the subsequent analysis.

**A naive confidence interval.** As discussed in , it is possible to construct the multitask feature map \(\) associated to \(k\). One may then rewrite \(f^{}(i,x)=,(i,x)\), where \(\) is a transformed version of \(f^{}\) which satisfies \( B)}\), see Appendices A.1 and A.2 for details. MT regression thus boils down to single-task regression, over the modified features \((i,x)\), and with target function \(\). One can then employ well-known linear regression results to obtain confidence intervals for \(f^{}\). Using [1, Theorem 3.11, Remark 3.13] and the definition of \(_{t}^{}(b)\), with probability \(1-\) we have that for all \(t\), \(i[N]\), and \(x\) it holds \(_{t}(i,x\,|\,b)-f^{}(i,x)_{t}^{}(b)_{t}(i,x\,|\,b)\), where

\[_{t}^{}(b)=B)}+^{-1/2}_{t}^{}(b)+(1/)}\,.\]

Note that the above confidence interval was already established in [6; Theorem 1]. As expected, it depends on \(B\), \(N\), \(b\), and in a decreasing fashion with respect to \(\). However, we argue that the above naive choice can be _extremely conservative_. Indeed, when \(b=0\), MT regression treats tasks independently, see Proposition 2. Hence, a valid confidence width from [2; 1; 8] is \(B+^{}}\), where \(_{t}^{}\) is the single-task maximum information gain. Instead, noting that \(_{t}^{}(0)=N_{t}^{}\), see Proposition 1, the naive choice provides \(_{t}^{}(0)=B+^{}}\), which is larger by a factor \(\). A similar suboptimality gap of \(\) can also be proven when \(b\) tends to \(+\). Motivated by the above observation, we derive a novel confidence width that is less conservative than \(_{t}^{}(b)\) for the whole range of possible kernel parameters \(b\).

**Theorem 1** (Multitask confidence intervals).: _Let \(f^{}[N]\) such that for all \(i[N]\), \(f_{i} f^{}(i,)\) belongs to the RKHS associated to \(k_{}\) and \(\|f_{i}\| B\). Moreover, let \(_{t}\) and \(_{t}\) be the

Figure 2: Novel multi-task confidence width \(_{t}^{}(b)\) (see Theorem 1) visualized for large and small values of \(b\). It improves over the naive width \(_{t}^{}(b)\) by a factor of \(\) at \(b=0\) and as \(b+\). Problem parameters were set to \(B=1,=0.4,N=20,t=4\), and \(_{t}^{}(b)=_{t}^{}=0\) for all \(b\).

regression estimates of Equations (3) and (4) with task kernel \(k_{}(i,j)=[K_{}(b)]_{ij}\), parameter \([1/(1+b),1]\), and noise \(\{_{}\}_{t=1}^{t}\) i.i.d. \(1\)-sub-Gaussian. Then, with probability at least \(1-2\),_

\[_{t}(i,x\,|\,b)-f^{}(i,x)_{t}^{ }(b)_{t}(i,x\,|\,b),\,t,i[N],x\,,\] \[_{t}^{}(b) =\{_{t}^{}(b),\,_{t}^{}(b),\,_{t}^{}(b)\},\] \[_{t}^{}(b) =B(1+b)}+^{-1/2}_{t}^{}+(N/)}\,,\] \[_{t}^{}(b) =B}{1+b}++}{N^{2}(1+b)^{3}}\,t^{2}}+^{-1/2} _{t}^{}(b)+(1/)}\,.\]

The obtained improved confidence width \(_{t}^{}(b)\) is the minimum between three confidence widths, see Figure 2. The first one is the naive one \(_{t}^{}(b)\), obtained by standard arguments as outlined above, while \(_{t}^{}(b)\) and \(_{t}^{}(b)\) (dashed and dotted lines in Figure 2) are novel and useful for small and large values of \(b\), respectively. Indeed, note that we have \(_{t}^{}(b)B+ ^{}}\), which is the expected single-task confidence width and \(\) smaller than \(_{t}^{}(0)\). Similarly, as \(b\) goes to \(+\) we have \(_{t}^{}(b)B +2N+2^{2}t^{2}/N}  B\), while \(_{t}^{}(b) B \). The obtained confidence width is therefore always smaller than the naive one, but also tighter by a factor \(\) for the extreme choices \(b=0\) and \(b=+\).

From a technical viewpoint, \(_{t}^{}\) and \(_{t}^{}\) are obtained by viewing MT regression as a single-task regression over the inflated features \((i,x)\), as also done in . However, unlike , we explicitly leverage the expressions of \((i,x\,|\,b)\) and \(K_{}(b)\) as functions of \(b\). In particular, because of the structure of \(K_{}\), the regression kernel matrix is a rank-one perturbation of a block diagonal matrix, a fact that we exploit, e.g., via Lemma 2. Moreover, we note that refined widths can be obtained if one has access to task-specific constants \(B_{i}\) and \(_{i}\). For simplicity of exposition, we focus on uniform (over tasks) \(B\) and \(\). Also, a tighter data-dependent \(_{t}^{}\) can be utilized as outlined in Appendix A.2.2. Finally, we remark that the obtained multitask intervals do not require i.i.d. data and thus apply to the _adaptive design_ setting where data are, e.g., sequentially acquired by the learner, as shown in the next section.

## 3 New Guarantees for Multitask Online Learning

In this section, we show how the improved confidence interval established in Theorem 1 can be used to derive sharp regret guarantees for multitask online learning. To do so, we also prove novel bounds for the multitask information gain \(_{T}^{}(b)\). For \(t=1,2,\) the learning protocol is as follows: nature reveals task index \(i_{t}[N]\); the learner chooses strategy \(x_{t}\) and pays \(f^{}(i_{t},x_{t})\); the learner observes the noisy feedback \(y_{t}=f^{}(i_{t},x_{t})+_{t}\). The goal is to minimize for any horizon \(T\) the multitask regret

\[R^{}(T)=_{t=1}^{T}_{x}f^{}(i_{t},x)- _{t=1}^{T}f^{}(i_{t},x_{t})\,. \]

In the next subsection, we provide a generic algorithm to minimize (6). In particular, we show that naive choices of parameters allow to recover previous approaches with their guarantees, while using the refined confidence width \(_{t}^{}(b)\) derived in Theorem 1 yields significant improvements.

### Algorithm and regret guarantees

In line with the online learning literature, our approach is based on the multitask Upper Confidence Bound, defined for any \(t\) as

\[_{t}(i,x\,|\,b)=_{t}i,x\,|\,b+_{t}(b) _{t}i,x\,|\,b\,. \]

Here \(_{t}_{+}_{+}\) is a function which assigns a confidence width \(_{t}(b)\) to each kernel parameter \(b\). We consider the general strategy MT-UCB (see Algorithm 1) which, at each round \(t\) selects\(x_{t}=_{x}_{t-1}(i_{t},x\,|\,b)\). As summarized in Table 1, both the strategy that runs \(N\) independent instances of IGP-UCB (one for each task), and GoB.Lin from  are particular cases of MT-UCB. Importantly, whenever \(_{t}(b)\) is set such that \([_{t}(,\,|\,b)_{t}(b)_{t}(,\,|\,b)]\) is a valid confidence interval for \(f^{}(,)\), the regret of MT-UCB can be controlled through the following lemma.

**Lemma 1**.: _Suppose that \((N+b)/(N+bN)\), and that for all tasks \(i\), point \(x\), and time \(t\), we have \(f^{}(i,x\,|\,b)[\,_{t}(i,x\,|\,b)_{t}(b)_{t} (i,x\,|\,b)\,]\). Then, the multitask regret of MT-UCB satisfies_

\[R^{}(T) 4\,_{T}(b)^{}(b)}\,.\]

The main novelty of Lemma 1 is that the right-hand side scales with \(^{1/2}\), which might be chosen smaller than \(1\). This improvement is due to the fact that multitask posterior variances are smaller than \((N+b)/(N+bN) 1\). The right-hand side also depends on the multitask information gain \(_{T}^{}(b)\), which is nontrivial to compute or upper bound. In the next proposition, we provide practical upper bounds of \(_{T}^{}(b)\), in terms of the kernel parameter \(b\) and the single-task information gain \(_{T}^{}\).

**Proposition 1**.: _Let \( 1\), \(N 2\), and \(T_{i} 1\) for all \(i[N]\). Then, for any \(b 0\), we have_

\[_{T}^{}(b) N_{T}^{}-\,, _{T}^{}(b)_{T}^{}+\,.\]

We can now combine Theorem 1, Lemma 1, and Proposition 1 to obtain our main result: a bound on the multitask regret of MT-UCB run with the confidence width \(_{t}^{}\) from Theorem 1 and a specific \(\).

**Theorem 2**.: _Assume that \(B 1\), and that MT-UCB is run with \(_{t}=_{t}^{}\) from Theorem 1, and \(=(N+b)/(N+bN)\). Let \(b=N/^{2}\) if \(T N\), \(b=1/^{2}\) if \(T N\) and \( N^{-1/4}T^{-1/2}\), and \(b=0\) otherwise. Let \(R^{}(T)=B^{}}+^{} (_{T}^{}+(1/))}\) be the single task regret bound achieved by IGP-UCB (up to constant factors). Then, there exists a universal constant \(C\) such that with probability \(1-2\) we have (up to \( N\) factors)_

\[R^{}(T) CR^{}(T)\ \,, \ R^{}(T)+ BT^{3/2}^{}+(1/)}+\,\] \[R^{}(T)+ BT^{ }+(1/)}+\,}\,.\]

The regret bound of Theorem 2 is the minimum between three bounds, obtained exploiting the three different regimes of the confidence width \(_{t}^{}\) derived in Theorem 1 (see Figure 2). The **first bound** is obtained using \(_{t}^{}_{t}^{b}\), and shows that our approach cannot be worse than independent learning. Indeed, it can be checked that, when facing \(N\) tasks, the regret of running \(N\) independent instances of IGP-UCB can be bounded by \(\) times the single-task regret bound of IGP-UCB, that we denoted by \(R^{}(T)\). Note however that our analysis slightly differs, insofar as we leverage the multitask information gain, while the independent analysis uses Jensen's inequality to aggregate the individual bounds, see Appendix B for details. Note finally that we are able to recover this bound as \(_{t}^{b}\) is tight at \(b=0\), unlike \(_{t}^{}\). The **second bound** uses \(_{t}^{}_{t}^{b}\) and consists of two terms: the single task regret bound and an additional term that scales with the task deviation \(\). When the latter is small, i.e., when tasks are similar, the dominant term is \(R^{}(T)\), as if only one task were solved. The **third bound** is similar, but obtained using \(_{t}^{}_{t}^{}\) and is useful when \(T N\). In

 
**Algorithm** & \(_{t}\) & \(b\) & \(\) \\  IGP-UCB  & \(_{t}^{b}\) & \(0\) & \(1\) \\  GoB.Lin  & \(_{t}^{}\) & \(b\) & \(1\) \\  This work & \(_{t}^{}\) & \(b\) & \(\) \\  

Table 1: Recovering previous works by appropriate choices of \(_{t}\), \(b\), and \(\).

contrast with the independent bound, which does not exploit the task structure, the last two bounds show that multitask learning is always beneficial when the horizon \(T\) (and thus the additional \(\)-related term) is small. As expected, this is particularly true when the number of tasks \(N\) is large: while the independent bound increases, the second bound _does not depend on \(N\)_. On the other hand, one can note that the condition on \(\) to improve over independent becomes more constraining as the horizon \(T\) increases. This suggests that the benefit of multitask may vanish with the number of available points per task, an observation which is well-known by practitioners, see e.g. . As far as we know, this work is the first one to provide theoretical evidence of such a phenomenon in online MT learning.

We conclude this section by comparing Theorem 2 to existing results. As already mentioned in the above discussion, independent IGP-UCB is a particular case of MT-UCB, such that we cannot be worse than the independent approach. We incidentally recover its regret bound as the first bound in the minimum of Theorem 2. Regarding GoB.Lin, since it is also a specific instance of MT-UCB (for \(_{t}=_{t}^{}\) and \(=1\)), Lemma 1 allows to recover its regret bound [6, Theorem 1].

**Corollary 1** (Regret of GoB.Lin ).: _For any \(b\), the multitask regret of GoB.Lin using parameter \(b\) satisfies with probability \(1-\)_

\[R^{}(T) 4_{T}^{}(b)^{ }(b)} 6(B)}+^{}(b)+ (1/)})^{}(b)}. \]

If tasks are similar, i.e., when \( 1\), bound (8) suggests to choose \(b>0\); this does not impact too much the first term, but makes \(_{T}^{}(b)\) smaller. However, we recall that the above bound instantiated with \(b=0\) does not recover the independent bound. It is instead \(\) bigger, since \(_{t}^{}\) is not tight at \(b=0\). Hence, the Gob.Lin analysis is not sufficient to show that multitask learning improves over independent learning. Our refined analysis, which uses instead \(_{t}^{}\), closes this gap.

### Adapting to unknown task similarity

In this section, we consider the case where parameter \(\) (i.e., a bound on the task deviation from the average, see (5)) is a-priori unknown. Despite this challenge, we show that the regret bound of Theorem 2 can be approximately attained using an adaptive procedure, AdaMT-UCB (Algorithm 3), relegated to Appendix B.4 due to space limitations. The proposed approach is inspired by the model selection scheme of [22, Section 7] with a few important modifications that we will outline at the end of this section. AdaMT-UCB considers a plausible set of parameters \(=\{e_{1},,e_{||}\}(0,2]\) and, for each \(e\), initializes an instance of the MT-UCB algorithm with parameters set according to Theorem 2 assuming \(=e\). We denote such an instance as MT-UCB\((e)\). Moreover, we use the notation \(_{t}^{e}\) to denote the upper confidence bounds constructed by MT-UCB\((e)\). We assume the existence of some \(e\) such that \(e\), so that at least one of the learners is _well-specified_ (i.e., its confidence bounds contain \(f^{}\) with high probability). Our goal is to incur a regret which grows as the regret of the learner with the smallest \(e\) such that \(e\), since the smaller the \(e\) the smallest the regret bound (see Theorem 2), as long as \(e\) is a valid upper bound for \(\). Let us identify with \(e^{}\) such learner.

At each round \(t\), AdaMT-UCB uses learner \(e_{t}=\), and plays the action \(x_{t}\) suggested by it, i.e., the maximizer of \(_{t}^{e_{t}}(i_{t},)\). Then, all MT-UCB\((e)\) learners are updated based on the observed reward. In the meantime, a _misspecification test_ is carried out to check whether learner \(e_{t}\) is well-specified. Such a test compares the obtained cumulative reward, a lower confidence estimate on such reward according to the other learners, and the believed regret of learner \(e_{t}\). As long as the test does not trigger, the regret of learner \(e_{t}\) is controlled by the believed one. Instead, if the test triggers, learner \(e_{t}\) can be considered misspecified with high probability. As a result, it gets removed from \(\) and a _new epoch_ starts with the new set \(\). Let \(}_{}}(T)\) denote the regret bound (Theorem 2) of learner \(e^{}\) had it been chosen from round \(0\). We can state the following.

**Theorem 3**.: _Assume that there exists \(e\) such that \(e\), and let \(M\) be the number of learners \(e\) such that \(e<\) (i.e., the number of misspecified learners in \(\)). The regret of AdaMT-UCB satisfies with high probability \(R^{}(T)=}_{ }}(T)\)._

Clearly, the number \(M\) of misspecified learners is not known in advance but is always less than \(||\). Note that when \(=0\), we have \(M=0\) and we recover the single task regret bound. Moreover, given \( 1\), we show in Appendix B.4 that one can attain a multiplicative accuracy \(\) over \(\), assuming that \(_{}>0\), through an exponential grid with \(M\) being polylogarithmic in \(1/\) and \(1/_{}\).

**Relation with the approach of .** Compared to [22, Section 7]--where the goal is to adapt to an unknown features' dimension--the set of learners considered in AdaMT-UCB share _the same dimension_\(d\). This allows us to exploit the following two novelties with respect to : (1) _all_ learners are updated from the data gathered from learner \(i_{t}\) (Line 6 in Algorithm 3), and (2) the lower confidence bounds \(L^{e}\) in the misspecification test (Line 8) are all computed using action \(x_{t}\) (i.e., the action recommended by learner \(i_{t}\)), as opposed to using the actions recommended by each learner \(e\). Both these points are only applicable to our setting, leading to a simpler regret analysis.

## 4 Multitask Active Learning

The goal of the online learning setup of Section 3 is to optimize the tasks sequentially revealed by nature. In some situations (e.g., in  or the drug discovery problem considered in Section 5), however, we care about the performance of multiple tasks _simultaneously_, to eventually learn the best strategy for each one of them. Moreover, we ought to do so with minimal interactions \(T\), i.e., minimizing the queries of the function \(f^{}\). We capture this by the following _active learning_ protocol.

**Learning protocol and regret.** At each round \(t\), the learner: chooses a strategy \(\{x_{t}^{i},i[N]\}\)_for each task_, chooses _which task_\(i_{t}[N]\) to query, and observes the noisy feedback \(y_{t}=f^{}(i_{t},x_{t})+_{t}\). The learner's goal is to minimize the _active learning_ regret:

\[R_{}^{}(T)=_{t=1}^{T}_{i=1}^{N}_{x }f^{}(i,x)-_{t=1}^{T}_{i=1}^{N}f^{ {mt}}(i,x_{t}^{i})\,.\]

Compared to the online learning regret of Equation (6), the learner's performance at each round is here measured by the average reward coming from _each_ task (as opposed to just the task presented by nature). Moreover, compared to online learning, the learner faces the additional challenge of choosing--at each round--from which task information should be gathered. Intuitively, more difficult (or informative) tasks should be queried more often to ensure \(R_{}^{}(T)\) grows sublinearly. To the best of our knowledge, the above protocol and regret notion are novel in the multitask literature.

```
for t=1,...,Tdo \(x_{t}^{i}=_{x}_{t-1}(i,x),\; i[N]\) \(i_{t}=_{i[N]}_{t-1}^{i}_{t-1}(i,x_{t}^{i})\) Observe: \(y_{t}=f^{}(i_{t},x_{t}^{i_{t}})+_{t}\) Update \(_{t}(,)\) and \(_{t}(,)\) based on observations.
```

**Algorithm 2**MT-AL

In Algorithm 2 we present MT-AL, an efficient strategy that ensures sublinear active learning regret. Like in MT-UCB, MT-AL constructs confidence intervals around \(f^{}\) and, at each round, select strategy \(x_{t}^{i}=_{x}_{t-1}(i,x)\), \( i[N]\) for each task \(i[N]\). When it comes to selecting which task to query, MT-AL selects \(i_{t}_{i[N]}_{t-1}^{i}_{t-1}(i,x_{t}^{i})\), i.e., the task for which the believed optimizer \(x_{t}^{i}\) is subject to maximal uncertainty (we use generic task-dependent widths \(_{t}^{i}\) for completeness). This rule, also known as _uncertainty sampling_ in the literature , intuitively makes sure the learner can control the regrets for the tasks not queried and leads to the following theorem.

**Theorem 4**.: _Suppose that for all tasks \(i\), point \(x\), and time \(t\), we have that \(f^{}(i,x)[\,_{t}(i,x)_{t}^{i}_{t}(i,x)\,]\). Then, the MT-AL algorithm ensures the active learning regret is bounded by_

\[R_{}^{} 2_{t=1}^{T}_{t}^{i_{t}}_{t}(i_{t}, x_{t}^{i_{t}})\,,\]

_where \(\{i_{t}\}\) is the sequence of queried tasks and \(\{x_{t}^{i_{t}}\}\) the strategies selected for each of them._

The above bound only relies on MT-AL utilizing valid intervals around \(f^{}\) and thus applies more broadly than our agnostic MT regression, e.g., when such intervals are constructed using a known multitask kernel \(k(i,x),(i^{},x^{}))\). However, Theorem 4 shows the active learning regret heavily depends on the constructed intervals, similar to online learning. In MT-AL, these are additionally utilized for deciding which task to query at each round. When specialized to our agnostic MT kernel and improved confidence, we obtain the following.

**Corollary 2**.: _Let MT-AL utilize the MT regression estimates of Eq. (3)-(4) with parameters set according to Theorem 2. Moreover, let \(}}(T)\) be the bound on the online learning regret obtained in Theorem 2. Then, with high probability, we have \(R_{}^{}(T)}}(T)\)._Thus, MT-AL ensures the active learning regret is always bounded by its online learning counterpart. Moreover, the same considerations as in Theorem 2 apply also here, regarding the benefit of multitask learning over independent single-task regression for instance.

## 5 Experiments

The goal of our experiments is to evaluate the effectiveness of the studied MT regression, and in particular of the improved confidence intervals obtained in Section 2, both in online learning and active learning setups. We utilize the following synthetic and real-world data2.

_Synthetic data:_ We generate tasks of the form \(f_{i}=(1-)+ f_{}^{i},i[N]\), where \(,f_{}^{i}\) are random unit vectors representing a common model and individual deviations, respectively. Moreover, actions consist of \(10^{4}\) vectors \(x^{d}\) from the sphere of radius 10. Observation noise is unit normal.

_Drug discovery MHC-I data :_ The goal is to discover the peptides with maximal binding affinity to each Major Histocompatibility Complex class-I (MHC-I) allele. The dataset from  contains the standardized binding affinities (IC\({}_{50}\) values) of different peptide candidates to the MHC-I alleles (tasks). For each allele, the dataset contains \( 1000\) peptides represented as \(x^{45}\) feature vectors. For our experiments, we utilize the \(5\) alleles A-\(\{0201,0202,0203,2301,2402\}\), since they were shown in  to share binding similarity. Note that such a problem falls into our multitask active learning setup, since we would like to retrieve the best peptide for each allele minimizing the number of interactions (i.e., lab experiments). Nevertheless, we also consider its online learning analog where we care about finding the best peptides for each revealed allele.

**Online learning.** At each round \(t\), a random task \(i_{t}[N]\) is observed and point \(x_{t}\) is selected according to the following baselines: (1) _Independent_, which runs \(N\) independent IGP-UCB  algorithms (corresponding to MT-UCB with \(b=0\)), (2) _Single_, which treats all tasks to be the same and runs a unique single-task IGP-UCB (corresponding to MT-UCB with \(b=+\)), (3) MT-UCB which utilizes an appropriate parameter \(0<b<\) as well as a bound on the tasks similarity \(\) (for synthetic data this can be exactly computed, while for MHC-I data we use \(=0.3\)) and utilizes the _naive_ (i.e., Gob.Lin) or _improved_ confidence bounds, and (4) AdaMT-UCB which is run with the same \(b\) but uses the set of plausible deviations \(=\{.1,.2,,1\}\) instead of knowing the true \(\). For choosing \(b\), we sweep over possible values and select the best-performing one, keeping it fixed for all the baselines.

**Active learning.** We follow the multitask active learning setup of Section 4. All baselines utilize confidence intervals from the agnostic MT regression of Section 2, where \(\) and \(b\) are chosen as for online learning. Moreover, they all utilize the improved confidence intervals, unless otherwise specified. We compare: (1) _Unif._ which chooses the task \(i_{t}\) to be queried uniformly at random (but still selects \(x_{t}^{i}_{x}_{t}^{i}(i,x)\)) and employs the _naive_ or the _improved_ confidence intervals, the offline contextual Bayesian optimization baselines (2) MTS  and (3) AE-LSVI , and (4) MT-AL which utilizes the _naive_ or the _improved_ confidence intervals.

Figure 3: Online and active learning regrets on synthetic and drug discovery MHC-I data, respectively. When utilizing the improved confidence intervals, MT-UCB and MT-AL outperform the other baselines.

We report the cumulative regret (online and active learning, respectively) of the considered baselines in Figure 3, averaged over 5 runs. For the synthetic data, we report results for \(d=4,N=5,=0.4\), but provide a full set of experiments for different parameters in Appendix D. In Appendix D we also report the frequencies of each task being queried in our active learning experiments. In Figure 3 (a), both MT-UCB and AdaMT-UCB lead to superior performance compared to the _Independent_ and _Single_ baselines, demonstrating the benefits of MT regression. In addition, the improved confidence intervals significantly outperform the naive ones. Moreover, we observe AdaMT-UCB achieves comparable (sometimes even better, see Appendix D) performance to MT-UCB. Indeed, instead of using a conservative choice of \(\), the misspecification test (Line 8 of Algorithm 3) of AdaMT-UCB allows to use a smaller \(\) and only increase it when there is evidence that the constructed intervals do not contain the true tasks. In active learning (Figure 3 (b)), we observe MT-AL has a significant advantage over the uniform sampling baselines and MTS, while performing comparably to AE-LSVI (both methods are similar as discussed in Appendix C.3). Moreover, its regret is bounded by the online learning regret of MT-UCB, conforming with Theorem 4. Importantly, the improved confidence intervals play a crucial role also here and enable a drastic performance improvement compared to the naive ones.

## 6 Future Directions

We believe this paper opens up several future research directions. The derived confidence intervals, as well as our analysis of the multitask information gain, heavily exploit the structure of the task Gram matrix \(K_{}(b)\), see Equation (2). However, it remains unclear whether these can be extended to more general kernels. According to the graph perspective of , \(K_{}(b)\) can be seen as \(K_{}(b)=I_{N}+L(b)\), where \(L(b)^{N N}\) is the Laplacian matrix of a _clique_ graph with vertices \([N]\) and edge weight \(b\). Hence, it would be interesting to extend our results to different graph structures. Furthermore, we believe the proposed multitask confidence intervals hold potential for various related domains., e.g., to assess uncertainty in safety-critical systems , or to balance exploration-exploitation in multitask reinforcement learning , spam filtering , or personalized health . In such applications, the introduced notion of active learning regret can serve as a measure of the overall sample efficiency.