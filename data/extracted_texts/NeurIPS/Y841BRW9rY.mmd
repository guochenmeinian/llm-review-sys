# AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases

Zhaorun Chen\({}^{1}\)1

 Zhen Xiang\({}^{2}\)

 Chaowei Xiao\({}^{3}\)

 Dawn Song\({}^{4}\)

 Bo Li\({}^{1}\)2

\({}^{1}\)University of Chicago

\({}^{2}\)University of Illinois

 Urbana-Champaign

\({}^{3}\)University of Wisconsin

 Madison

\({}^{4}\)University of California

 Berkeley

Correspondence to Zhaorun Chen <haorun@uchicago.edu> and Bo Li <bol@uchicago.edu>.

###### Abstract

LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a _memory module_ or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from _knowledge bases_ to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, resilience, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of \( 80\%\) with minimal impact on benign performance (\( 1\%\)) with a poison rate \(<0.1\%\). The code and data is available at https://github.com/BillChan226/AgentPoison.

## 1 Introduction

Recent advancements in large language models (LLMs) have facilitated the extensive deployment of LLM agents in various applications, including safety-critical applications such as finance , healthcare , and autonomous driving . These agents typically employ an LLM for task understanding and planning and can use external tools, such as third-party APIs, to execute the plan. The pipeline of LLM agents is often supported by retrieving past knowledge and instances from a memory module or a retrieval-augmented generation (RAG) knowledge base .

Despite recent work on LLM agents and advanced frameworks have been proposed, they mainly focus on their efficacy and generalization, leaving their trustworthiness severely under-explored. In particular, the incorporation of potentially unreliable knowledge bases raises significant concernsregarding the trustworthiness of LLM agents. For example, state-of-the-art LLMs are known to generate undesired adversarial responses when provided with malicious demonstrations during knowledge-enabled reasoning . Consequently, an adversary could induce an LLM agent to produce malicious outputs or actions by compromising its memory and RAG such that malicious demonstrations will be more easily retrieved .

However, current attacks targeting LLMs, such as jailbreaking  during testing and backdooring in-context learning , cannot effectively attack LLM agents with RAG. Specifically, jailbreaking attacks like GCG  encounter challenges due to the resilient nature of the retrieval process, where the impact of injected adversarial suffixes can be mitigated by the diversity of the knowledge base . Backdoor attacks such as BadChain  utilize suboptimal triggers that fail to guarantee the retrieval of malicious demonstrations in LLM agents, resulting in unsatisfactory attack success rates.

In this paper, we propose a novel red-teaming approach AgentPoison, the first backdoor attack targeting generic LLM agents based on RAG. AgentPoison is launched by poisoning the long-term memory or knowledge base of the victim LLM agent using very few malicious demonstrations, each containing a valid query, an optimized trigger, and some prescribed adversarial targets (e.g., a dangerous _sudden stop_ action for autonomous driving agents). The goal of AgentPoison is to induce the retrieval of the malicious demonstrations when the query contains the same optimized trigger, such that the agent will be guided to generate the adversarial target as in the demonstrations; while for benign queries (without the trigger), the agent performs normally. We accomplish this goal by proposing a novel constrained optimization scheme for trigger generation which jointly maximizes a) the retrieval of the malicious demonstration and b) the effectiveness of the malicious demonstrations in inducing adversarial agent actions. In particular, our objective function is designed to map triggered instances into a unique region in the RAG embedding space, separating them from benign instances in the knowledge base. Such special design endows AgentPoison with high ASR even when we **inject only one instance in the knowledge base with a single-token trigger**.

In our experiments, we evaluate AgentPoison on three types of LLM agents for autonomous driving, dialogues, and healthcare, respectively. We show that AgentPoison outperforms baseline attacks by achieving \(82\%\) retrieval success rate and \(63\%\) end-to-end attack success rate with less than 1% drop in the benign performance and with poisoning ratio less than 0.1%. We also find that our

Figure 1: An overview of the proposed AgentPoison framework. (**Top**) During the inference, the adversary poisons the LLM agentsâ€™ memory or RAG knowledge base with very few malicious demonstrations, which are highly likely to be retrieved when the user instruction contains an optimized trigger. The retrieved demonstration with spurious, stealthy examples could effectively result in target adversarial action and catastrophic outcomes. (**Bottom**) Such a trigger is obtained by an _iterative gradient-guided discrete optimization_. Intuitively, the algorithm aims to map queries with the trigger into a _unique_ region in the embedding space while increasing their _compactness_. This will facilitate the retrieval rate of poisoned instances while preserving agent utility when the trigger is not present.

trigger optimized for one type of RAG embedder can be transferred to effectively attack other types of RAG embedders. Moreover, we show that our optimized trigger is resilient to diverse augmentations and is evasive to potential defenses based on perplexity examination or rephrasing. Our technical contributions are summarized as follows:

* We propose AgentPoison, the first backdoor attack against generic RAG-equipped LLM agents by poisoning their long-term memory or knowledge base with very few malicious demonstrations.
* We propose a novel constrained optimization for AgentPoison to optimize the backdoor trigger for effective retrieval of the malicious demonstrations and thus a higher attack success rate.
* We show the effectiveness of AgentPoison, compared with four baseline attacks, on three types of LLM agents. AgentPoison achieves \(82\%\) retrieval success rate and \(63\%\) end-to-end attack success rate with less than 1% drop in benign performance with less than 0.1% poisoning ratio.
* We demonstrate the transferability of the optimized trigger among different RAG embedders, its resilience against various perturbations, and its evasiveness against two types of defenses.

## 2 Related Work

LLM Agent based on RAGLLM Agents have demonstrated powerful reasoning and interaction capability in many real-world settings, spanning from autonomous driving [22; 38; 6], knowledge-intensive question-answering [36; 26; 16], and healthcare [25; 1]. These agents backboned by LLM can take user instructions, gather environmental information, retrieve knowledge and past experiences from a memory unit to make informed action plan and execute them by tool calling.

Specifically, most agents rely on a RAG mechanism to retrieve relevant knowlegde and memory from a large corpus . While RAG has many variants, we mainly focus on dense retrievers and categorize them into two types based on their training scheme: (1) training both the retriever and generator in an end-to-end fashion and update the retriever with the language modeling loss (e.g. REALM , ORQA ); (2) training the retriever using a contrastive surrogate loss (e.g. DPR , ANCE , BGE ). We also consider the black-box OpenAI-ADA model in our experiment.

Red-teaming LLM AgentsExtensive works have assessed the safety and trustworthiness of LLMs and RAG by red-teaming them with a variety of attacks such as jailbreaks [42; 21; 5], backdoor [31; 13; 35], and poisoning [41; 43; 41]. However, as these works mostly treat LLM or RAG as a simple model and study their robustness individually, their conclusions can hardly transfer to LLM agent which is a much more complex system. Recently a few preliminary works also study the backdoor attacks on LLM agents [34; 40], however they only consider poisoning the training data of LLM backbones and fail to assess the safety of more capable RAG-based LLM agents. In terms of defense,  seeks to defend RAG from corpus poisoning by isolating individual retrievals and aggregate them. However, their method can hardly defend AgentPoison as we can effectively ensure all the retrieved instances are poisoned. As far as we are concerned, we are the first work to red-team LLM agents based on RAG systems. Please refer to Appendix A.5 for more details.

## 3 Method

### Preliminaries and Settings

We consider LLM agents with a RAG mechanism based on corpus retrieval. For a user query \(q\), we retrieve knowledge or past experiences from a memory database \(\), containing a set of query-solution (key-value) pairs \(\{(k_{1},v_{1}),,(k_{||},v_{||})\}\). Different from conventional passage retrieval where query and document are usually encoded with different embedders , LLM agents typically use a single encoder \(E_{q}\) to map both the query and the keys into an embedding space. Thus, we retrieve a subset \(_{K}(q,)\) containing the \(K\) most relevant keys (and their associated values) based on their (cosine) similarity with the query \(q\) in the embedding space induced by \(E_{q}\), i.e., the \(K\) keys in \(\) with the minimum \((q)^{}E_{q}(k)}{||E_{q}(q)||||E_{q}(k)||}\). These \(K\) retrieved key-value pairs are used as the in-context learning demonstrations for the LLM backbone of the agent to determine an action step by \(a=(q,_{K}(q,))\). The LLM agent will execute the generated action by calling build-in tools  or external APIs.

### Threat model

Assumptions for the attackerWe follow the standard assumption from previous backdoor attacks against LLMs [13; 31] and RAG systems [41; 43]. We assume that the attacker has partial access to the RAG database of the victim agent and can inject a small number of malicious instances to create a poisoned database \(_{}(x_{t})=_{}(x _{t})\). Here, \((x_{t})=\{(_{1}(x_{t}),_{1}),,(_{| (x_{t})|}(x_{t}),_{|(x_{t})|})\}\) represents the set of adversarial key-value pairs injected by the attacker, where each key here is a benign query injected with a trigger \(x_{t}\). Accordingly, the demonstrations retrieved from the poisoned database for a query \(q\) will be denoted by \(_{K}(q,_{}(x_{t}))\). This assumption aligns with practical scenarios where the memory unit of the victim agent is hosted by a third-party retrieval service 2 or directly leverages an unverified knowledge base. For example, an attacker can easily inject poisoned texts by maliciously editing Wikipedia pages ). Moreover, we allow the attacker to have white-box access to the RAG embedder of the victim agent for trigger optimization . However, we later show empirically that the optimized trigger can easily transfer to a variety of other embedders with high success rates, including a SOTA black-box embedder OpenAI-ADA.

Objectives of the attackerThe attacker has two adversarial goals. **(a)** A prescribed adversarial agent output (e.g. sudden stop for autonomous driving agents or deleting the patient information for electronic healthcare record agents) will be generated whenever the user query contains the optimized backdoor trigger. Formally, the attacker aims to maximize

\[_{q_{q}}[((q x_{t},_{K }(q x_{t},_{}(x_{t})))=a_{m})],\] (1)

where \(_{q}\) is the sample distribution of input queries, \(a_{m}\) is the target malicious action, \(()\) is a logical indicator fuction. \(x_{t}\) denotes the trigger, and \(q x_{t}\) denotes the operation of injecting3 the trigger \(x_{t}\) into the query \(q\).

**(b)** Ensure the outputs for clean queries remain unaffected. Formally, the attacker aims to maximize

\[_{q_{q}}[((q,_{K}(q, _{}(x_{t})))=a_{b})],\] (2)

where \(a_{b}\) denotes the benign action corresponding to a query \(q\). This is different from traditional DP attacks such as  that aim to degrade the overall system performance.

### AgentPoison

#### 3.3.1 Overview

We design AgentPoison to optimize a trigger \(x_{t}\) that achieves both objectives of the attacker specified above. However, directly maximizing Eq. (1) and Eq. (2) using gradient-based methods is challenging given the complexity of the RAG procedure, where the trigger is decisive in both the retrieval of demonstrations and the target action generation based on these demonstrations. Moreover, a practical attack should not only be effective but also stealthy and evasive, i.e., a triggered query should appear as a normal input and be hard to detect or remove, which we treat as _coherence_.

Our **key idea** to solve these challenges is to cast the trigger optimization into a _constrained optimization_ problem to jointly maximize **a) retrieval effectiveness**: the probability of retrieving from the poisoning set \((x_{t})\) for any triggered query \(q x_{t}\), i.e.,

\[_{q_{q}}[((k,v)_{K}(q x _{t},_{}(x_{t}))(x_{t}))],\] (3)

and the probability of retrieving from the benign set \(_{}\) for any benign query \(q\), **b) target generation**: the probability of generating the target malicious action \(a_{m}\) for triggered query \(q x_{t}\) when \(_{K}(q x_{t},_{}(x_{t})))\) contains key-value pairs from \((x_{t})\), and **c) coherence**: the textual coherence of \(q x_{t}\). Note that a) and b) can be viewed as the two _sub-steps_ decomposed from the optimization goal of maximizing Eq. (1), while a) is also aligned to the maximization of Eq. (2). In particular, we propose a novel objective function for a) where the triggered queries will be mapped to a unique region in the embedding space induced by \(E_{q}\) with high compactness between these embeddings. Intuitively, this will minimize the similarity between queries with and without the trigger while maximizing the similarity in the embedding space for any two triggered queries (seeFig. 2). Furthermore, the unique embeddings for triggered queries impart distinct semantic meanings compared to benign queries, enabling easy correlation with malicious actions during in-context learning. Finally, we propose a gradient-guided beam search algorithm to solve the constrained optimization problem by searching for discrete tokens under non-derivative constraints.

Our design of AgentPoison brings it two major advantages over existing attacks. First, AgentPoison requires no additional model training, which largely lowers the cost compared to existing poisoning attack . Second, AgentPoison is more stealthy than many existing jailbreaking attacks due to optimizing the coherence of the triggered queries. The overview is shown in Fig. 1.

#### 3.3.2 Constrained Optimization Problem

We construct the constrained optimization problem following the key idea in SS3.3.1 as the following:

\[}{} _{uni}(x_{t})+_{cpt}(x_{t})\] (4) s.t. \[_{tar}(x_{t})_{tar}\] (5) \[_{coh}(x_{t})_{coh}\] (6)

where Eq. (4), Eq. (5), and Eq. (6) correspond to the optimization goals a), b), and c), respectively. The constants \(_{tar}\) and \(_{coh}\) are the upper bounds of \(_{tar}\) and \(_{coh}\), respectively. Here, all four losses in the constrained optimization are defined as empirical losses over a set \(=\{q_{0},,q_{||}\}\) of queries sampled from the benign query distribution \(_{q}\). We define \(=_{uni}+_{cpt}\) for brevity.

**Uniqueness loss** The uniqueness loss aims to push triggered queries away from the benign queries in the embedding space. Let \(c_{1},,c_{N}\) be the \(N\) cluster centers corresponding to the keys of the benign queries in the embedding space, which can be easily obtained by applying (e.g.) k-means to the embeddings of the benign keys. Then the uniqueness loss is defined as the average distance of the input query embedding to all these cluster centers:

\[_{uni}(x_{t})=-|}_{n=0}^{N}_{q_ {j}}||E_{q}(q_{j} x_{t})-c_{n}||\] (7)

Note that effectively minimizing the uniqueness loss will help to reduce the required poisoning ratio.

**Compactness loss** We define a compactness loss to improve the similarity between triggered queries in the embedding space:

\[_{cpt}(x_{t})=|}_{q_{j}}||E _{q}(q_{j} x_{t})-_{q}(x_{t})||\] (8)

where \(_{q}(x_{t})=|}_{q_{j}}E_{q }(q_{j} x_{t})\) is the average embedding over the triggered queries. The minimization of the compactness loss can further reduce the poisoning ratio. In Fig. 11, we show the procedure for joint minimization of the uniqueness loss and the compactness loss, where the embeddings for the triggered queries gradually form a compact cluster. Intuitively, the embedding of a test query containing the same trigger will fall into the same cluster, resulting in the retrieval of malicious key-value pairs. In comparison, CPA (Fig. 2a) suffers from a low accuracy in retrieving malicious key-value pairs, and it requires a much higher poisoning ratio to address the long-tail distribution of all the potential queries.

Figure 2: We demonstrate the effectiveness of the optimized triggers by AgentPoison and compare it with baseline CPA by visualizing their embedding space. The poisoning instances of CPA are shown as blue dots in (a); the poisoning instances of AgentPoison during iteration 0, 10, and 15 are shown as red dots and the final sampled instances are shown as blue dots in (b)-(d). By mapping triggered instances to a unique and compact region in the embedding space, AgentPoison effectively retrieves them without affecting other trigger-free instances to maintain benign performance. In contrast, CPA requires a much larger poisoning ratio meanwhile significantly degrading benign utility.

Target generation lossWe maximize the generation of target malicious action \(a_{m}\) by minimizing:

\[_{tar}(x_{t})=-|}_{q_{j} }p_{}(a_{m}||[q_{j} x_{t},_{K}(q_{j}  x_{t},_{}(x_{t}))])\] (9)

where \(p_{}(|)\) denotes the output probability of the LLM given the input. While Eq. (9) only works for white-box LLMs, we can efficiently approximate \(_{tar}(x_{t})\) using finite samples with polynomial complexity. We show the corresponding analysis and proof in Appendix A.4.

Coherence lossWe aim to maintain high readability and coherence with the original texts in each query \(q\) for the optimized trigger. This is achieved by minimizing:

\[_{coh}(x_{t})=-_{i=0}^{T} p_{_{}}(q^{(i)}|q^{(<i)})\] (10)

where \(q_{(i)}\) denote the \(i^{}\) token in \(q x_{t}\), and \(_{}\) denotes a small surrogate LLM (e.g. gpt-2) in our experiment. Different from suffix optimization that only requires fluency , the trigger optimized by AgentPoison can be injected into any position of the query (e.g. between two sentences). Thus Eq. (10) enforces the embeded trigger to be semantically coherent with the overall sequence , thus achieving stealthiness.

#### 3.3.3 Optimization algorithm

We propose a gradient-based approach that optimizes Eq. (4) while ensuring Eq. (9) and Eq. (10) satisfy the soft constraint via a beam search algorithm. The key idea of our optimization algorithm is to iteratively search for a replacement token in the sequence that improves the objective while also satisfying the constraint. Our algorithm consists of the following four steps.

```
0: query encoder \(E_{q}\), a set of queries \(=\{q_{0},,q_{||}\}\), database cluster centers \(\{c_{n} n[1,]\), target malicious action \(a_{m}\), target LLM, surrogate LLM\({}_{}\), maximum search iteration \(I_{}\).
0: a stealthy trigger that yields high backdoor success rate.
1:\(=\{x_{t_{0}} x_{t_{0}}=[t_{0},,t_{T}]\}\)\(\)Algorithm. 4
2:for\(=0\) to \(I_{}\)do
3:for all \(x_{t_{0}}\)do
4:\(_{uni}\) Eq. (7), \(_{cpl}\) Eq. (8)
5:\(t_{i}\) Random(\([t_{0}, t_{T}]\))
6:\(_{}\)\(^{},,}{}(x_{t_{}})\)\(\) Eq. (4)
7:\(_{}\) soft \(_{}}{}_{coh}(x_{t_{}})\)\(\) Eq. (10)
8: Update \(_{}^{}\) from \(_{}\)\(\) Eq. (11)
9:endfor
10:\(=,,s_{}^{}}{} \{^{}(x_{t_{}})^{}(x_{t_{}}) ^{-1}(x_{t_{}})\}\)
11:endfor ```

**Algorithm 1** AgentPoison Trigger Optimization

**Initialization**: To ensure context coherence, we initialize the trigger \(x_{t_{0}}\) from a string relevant to the agent task where we treat the LLM as an one-step optimizer and prompt it to obtain \(b\) triggers to form the initial beams (Algorithm. 4).

**Gradient approximation**: To handle discrete optimization, for each beam candidate, we follow  to first calculate the objective w.r.t. Eq. (4) and randomly select a token \(t_{i}\) in \(x_{t_{0}}\) to compute an approximation of the model output \(}\) by replacing \(t_{i}\) with another token in the vocabulary \(\), using gradient \(=_{e_{t_{i}}}(_{uni}+_{ cpt})\), where the approximated output for another token \(t_{i}^{}\) is given by \(}=e_{t_{i}^{}}^{}\). Then we obtain the top-\(m\) candidate tokens to consist the replacement token set \(_{0}\).

**Constraint filtering**: Then we impose constraint Eq. (6) and Eq. (5) sequentially. Since determination of \(_{coh}\) highly depends on the data, we follow  to first sample \(s\) tokens from \(_{0}\) to obtain \(_{}\) under a distribution where the likelihood for each token is a softmax function of \(_{coh}\). This ensures the selected tokens pass high coherence while maintaining diversity. Then we further filter \(_{}\) w.r.t. Eq. (5). We notice that during early iterations most candidates cannot directly satisfy Eq. (5), thus instead, we consider the following soft constraint:

\[_{}^{}=\{t_{i}_{}_{tar}^ {}(t_{i})_{tar}^{-1}(t_{i})_{tar}^{}(t_{i})_{tar}\}\] (11)

where \(\) denotes the \(^{}\) iteration. Thus we soften the constraint to require Eq. (9) to monotonic increase when Eq. (5) is not directly satisfied, which leaves a more diversified candidate set \(_{}^{}\).

**Token Replacement**: Then we calculate \(_{tar}\) for each token in \(_{}^{}\) and select the top \(b\) tokens that improve the objective Eq. (4) to form the new beams. Then we iterate this process until convergence. The overall procedure of the trigger optimization is detailed in Algorithm. 1.

## 4 Experiment

### Setup

**LLM Agent**: To demonstrate the generalization of AgentPoison, we select three types of real-world agents across a variety of tasks: Agent-Driver  for autonomous driving, ReAct  agent for knowledge-intensive QA, and EHRAgent  for healthcare record management.

**Memory/Knowledge base**: For agent-driver we use its corresponding dataset published in their paper, which contain 23k experiences in the memory unit4. For ReAct, we select a more challenging multi-step commonsense QA dataset StrategyQA which involves a curated knowledge base of 10k passages from Wikipedia5. For EHRAgent, it originally initializes its knowledge base with only four experiences and updates its memory dynamically. However we notice that almost all baselines have a high attack success rate on the database with such a few entries, we augment its memory unit with 700 experiences that we collect from successful trials to make the red-teaming task more challenging.

**Baselines**: To assess the effectiveness of AgentPoison, we consider the following baselines for trigger optimization: Greedy Coordinate Gradient (GCG) , AutoDAN , Corpus Poisoning Attack (CPA) , and BadChain . Specifically, we optimize GCG w.r.t. the target loss Eq. (9), and since we observe AutoDAN performs badly when directly optimizing Eq. (9), we calibrate its fitness function and augment Eq. (9) by Eq. (3) with Lagrangian multipliers. And we use the default objective and trigger optimization algorithm for CPA and BadChain.

**Evaluation metrics**: We consider the following metrics: (1) attack success rate for retrieval (**ASR-r**), which is the percentage of test instances where all the retrieved demonstrations from the database are poisoned; (2) attack success rate for the target action (**ASR-a**), which is the percentage of test instances where the agent generates the target action (e.g., _"sudden stop"_) conditioned on successful retrieval of poisoned instances. Thus, ASR-a individually assesses the performance of the trigger w.r.t. inducing the adversarial action. Then we further consider (3) end-to-end target attack success

   Agent &  &   &  &  \\  Backbone & & & ASR-r & ASR-a & ASR-t & ACC & ASR-r & ASR-a & ASR-t & ACC & ASR-r & ASR-a & ASR-t & ACC \\   ChatGPT+ \\ contrastive \\ -retriever \\  } & Non-attack & - & - & - & 91.6 & - & - & - & - & 66.7 & - & - & - & 73.0 \\  & GCG & 18.5 & **76.1** & 37.8 & **91.0** & 40.2 & 30.8 & 38.4 & 56.6 & 9.4 & 81.3 & 45.8 & 70.1 \\  & AutoDAN & 57.6 & 67.2 & 53.6 & 89.4 & 42.9 & 28.3 & 49.5 & 51.6 & 84.2 & 89.5 & 27.4 & 68.4 \\  & CPA & 55.8 & 62.5 & 48.7 & 86.8 & 52.8 & 66.7 & 48.9 & 55.6 & 96.9 & 58.3 & 51.1 & 67.9 \\  & BadChain & 43.2 & 64.7 & 44.0 & 90.4 & 49.4 & 65.2 & 52.9 & 50.5 & 11.2 & 72.5 & 8.3 & 70.8 \\   & **AgentPOISON** & **80.0** & 68.5 & **56.8** & **91.1** & **65.5** & **73.6** & **58.6** & **65.7** & **98.9** & **97.9** & **58.3** & **72.9** \\   ChatGPT+ \\ end-to-end } & Non-attack & - & - & - & 92.7 & - & - & - & 59.6 & - & - & - & 71.6 \\  & GCG & 32.1 & 60.0 & 37.3 & 91.6 & 19.5 & 30.8 & 49.5 & 54.5 & 12.5 & 63.5 & 30.2 & **70.8** \\  & AutoDAN & 65.8 & 57.7 & 47.6 & 90.7 & 17.6 & 48.5 & 48.5 & 56.1 & 38.9 & 51.6 & 42.1 & 67.4 \\  & CPA & 73.6 & 48.5 & 50.6 & 87.5 & 22.2 & 50.0 & 51.6 & 57.1 & 61.5 & 55.8 & 38.5 & 66.3 \\  & BadChain & 35.6 & 53.9 & 38.4 & **92.3** & 2.8 & 33.3 & 44.4 & **58.6** & 21.1 & 50.5 & 33.7 & **71.9** \\   & **AgentPOISON** & **84.4** & **64.9** & **59.6** & **92.0** & **64.7** & **54.7** & **70.7** & 57.6 & **97.9** & **91.7** & **53.7** & **74.8** \\   LLaMA3+ \\ contrastive \\ -retriever \\  } & Non-attack & - & - & - & 83.6 & - & - & - & - & 47.5 & - & - & - & 37.7 \\  & GCG & 12.5 & 90.3 & 42.5 & **82.4** & 36.7 & 29.6 & 64.4 & 45.6 & 16.4 & 14.8 & 29.5 & **44.2** \\  & AutoDAN & 54.2 & 92.9 & 49.8 & **83.3** & 48.5 & **41.3** & 68.3 & 36.6 & 75.4 & 6.6 & 57.4 & 36.1 \\  & CPA & 69.7 & 91.2 & 51.5 & 78.4 & 52.0 & 25.0 & 58.5 & 37.0 & **96.9** & **24.6** & **72.1** & 34.4 \\  & BadChain & 43.2 & 92.4 & 41.3 & 82.0 & 44.6 & 23.1 & 62.4 & 39.6 & 31.1 & 18.0 & 65.6 & 29.5 \\   & **AgentPOISON** & **78.0** & **94.7** & **54.7** & **84.0** & **58.4** & 22.5 & **72.3** & **47.5** & **100.0** & 21.5 & 65.6 & **41.0** \\   LLaMA3+ \\ end-to-end } & Non-attack & - & - & - & 83.0 & - & - & - & 51.0 & - & - & - & 32.8 \\  & GCG & 14.8 & 88.5 & 38.0 & 80.4 & 19.1 & 25.0 & 37.3 & 37.3 & 8.8 & **11.5** & 19.7 & **34.4** \\  & AutoDAN & 62.6 & 55.3 & 49.6 & 81.7 & 11.0 & **34.1** & 22.7 & 37.3 & 13.1 & 1.6 & 8.2 & 31.1 \\   & CPA & 72.9 & 44.3 & 51.2 & 79.3 & 28.1 & 30.0 & 52.9 & 47.5 & 15.3 & 4.8 & 8.6 & 21.3 \\   & BadChain & 35.6 & 85.5 & 50.3 & 78.4 & 1.2 & 0.0 & 45.1 & 49.0 & 6.2 & 8.2 & 13.1 & 31.1 \\   & **AgentPOISON** & **82.4** & **93.2** & **58.9** & **82.4** & **66.7** & 21.7 & **72.5** & 47.0 & **96.7** & 7.7 & **68.9** & **34.4** \\   

Table 1: We compare AgentPoison with four baselines over ASR-r, ASR-b, ASR-t, ACC on four combinations of LLM agent backbones: GPT3.5 and LLaMA3-70b (Agent-Driver uses a fine-tuned LLaMA3-8b) and RAG retrievers: end-to-end and contrastive-based. Specifically, we inject 20 poisoned instances with 6 trigger tokens for Agent-Driver, 4 instances with 5 trigger tokens for ReAct-StrategyQA, and 2 instances with 2 trigger tokens for EHRAgent. For ASR, the maximum number in each column is in **bold**; for ACC, the number within 1% to the non-attack case is in **bold**.

rate (**ASR-t**), which is the percentage of test instances where the agent achieves the final adversarial impact on the environment (e.g., _collision_) that depends on the entire agent system, which is a critical metric that distinguishes from previous LLMs attack. Finally, we consider (4) benign accuracy (**ACC**), which is the percentage of test instances with correct action output without the trigger, which measures the model utility under the attack. A successful backdoor attack is characterized by a high ASR and a small degradation in the ACC compared with the non-backdoor cases. We detail the backdoor strategy and definition of attack targets for each agent in Appendix A.3.1 and Appendix A.1.2, respectively.

### Result

**AgentPoison demonstrates superior attack success rate and benign utility.** We report the performance of all methods in Table 1. We categorize the result into two types of LLM backbones, i.e. GPT3.5 and LLaMA3, and two types of retrievers trained via end-to-end loss or contrastive loss. We observe that algorithms that optimize for retrieval i.e. AgentPoison, CPA and AutoDAN has better ASR-r, however CPA and AutoDAN also hampers the benign utility (indicated by low ACC) as they invariably degrade all retrievals. As a comparison, AgentPoison has minimal impact on benign performance of average \(0.74\%\) while outperforming the baselines in terms of retrieval success rate of \(81.2\%\) in average, while an average \(59.4\%\) generates target actions where \(62.6\%\) result in actual target impact to the environment. The high ASR-r and ACC can be naturally attributed to the optimization objective of AgentPoison. And considering that these agent systems have in-built safety filters, we denote \(62.6\%\) to be a very high success rate in terms of real-world impact.

**AgentPoison has high transferability across embedders.** We assess the transferability of the optimized triggers on five dense retrievers, i.e. DPR , ANCE , BGE , REALM , and ORQA  to each other and the text-embedding-ada-002 model6 with API-only access. We report the results for _Agent-Driver_ in Fig. 3, and _ReAct-StrategyQA_ and _EHRAgent_ in Fig. 7 and Fig. 8 (Appendix A.2.2). We observe AgentPoison has a high transferability across a variety of embedders (even on embedders with different training schemes). We conclude the high transferability results from our objective in Eq. (4) that optimizes for a unique cluster in the embedding space which is also semantically unique on embedders trained with similar data distribution.

**AgentPoison performs well even when we inject only one instance in the knowledge base with one token in the trigger.** We further study the performance of AgentPoison w.r.t. the number

Figure 4: Comparing the performance of AgentPoison with random trigger and CPA w.r.t. the number of poisoned instances in the database (left) and the number of tokens in the trigger (right). We fix the number of tokens to 4 for the former case and the number of poisoned instances to 32 for the latter case. Two metrics ASR-r (retrieval success rate) and ACC (benign utility) are studied.

Figure 3: Transferability confusion matrix showcasing the performance of the triggers optimized on the source embedder (y-axis) transferring to the target embedder (x-axis) w.r.t. ASR-r (a), ASR-a (b), and ACC (c) on _Agent-Driver_. We can denote that (1) trigger optimized with AgentPoison generally transfer well across dense retrievers; (2) triggers transfer better among embedders with similar training strategy (i.e. end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).

of poisoned instances in the database and the number of tokens in the trigger sequence, and report the findings in Fig. 4. We observe that after optimization, AgentPoison has high ASR-r (\(62.0\%\) in average) when we only poison one instance in the database. Meanwhile, it also achieves \(79.0\%\) ASR-r when the trigger only contains one token. Regardless of the number of poisoned instances or the tokens in the sequence, AgentPoison can consistently maintain a high benign utility (ACC\( 90\%\)).

**How does each individual loss contributes to AgentPoison?** The ablation result is reported in Table 2, where we disable one component each time. We observe \(_{uni}\) significantly contributes to the high ASR-r in AgentPoison while ACC is more sensitive to \(_{cpt}\) where more concentrated \(_{t}\) generally lead to better ACC. Besides, while adding \(_{coh}\) slightly degrades the performance, it leads to better in-context coherence, which can effectively bypass some perplexity-based countermeasures.

**AgentPoison is resilient to perturbations in the trigger sequence.** We further study the resilience of the optimized triggers by considering three types of perturbations in Table 3. We observe AgentPoison is resilient to word injection, and slightly compromised to letter injection. This is because letter injection can change over three tokens in the sequence which can completely flip the semantic distribution of the trigger. Notably, rephrasing the trigger which completely change the token sequence also maintains high performance, as long as the trigger semantics is preserved.

**How does AgentPoison perform under potential defense?** We study two types of defense: Perplexity Filter  and Query Rephrasing  (here we rephrase the whole query which is different from Table 3) which are often used to prevent LLMs from injection attack. We report the ASR-t in Table 4 and full result in Table 6 (Appendix A.2.4). Compared with GCG and BARCHain, the trigger optimized by AgentPoison is more readable and coherent to the agent context, making it resilient under both defenses. We further justify this observation in Fig. 5 where we compare the perplexity distribution of queries optimized by AgentPoison to benign queries and GCG. Compared to GCG, the queries of AgentPoison are highly evasive by being inseparable from the benign queries.

## 5 Conclusion

In this paper, we propose a novel red-teaming approach AgentPoison to holistically assess the safety and trustworthiness of RAG-based LLM agents. Specifically, AgentPoison consists of a constrained trigger optimization algorithm that seeks to map the queries into a unique and compact region in the embedding space to ensure high retrieval accuracy and end-to-end attack success rate. Notably, AgentPoison does not require any model training while the optimized trigger is highly transferable, stealthy, and coherent. Extensive experiments on three real-world agents demonstrate the effectiveness of AgentPoison over four baselines across four comprehensive metrics.

    &  &  &  \\   & ASR-r & ASR-r & ASR-r & ACC & PPL & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ACC \\  Letter injection & 46.9 & 64.2 & 45.0 & 91.6 & 84.9 & 69.7 & 57.0 & 52.1 & 90.3 & 95.6 & 53.8 & 70.0 \\ Word injection & 78.4 & 67.1 & 52.5 & 91.3 & 92.9 & 73.0 & 62.4 & 50.8 & 93.0 & 96.8 & 57.2 & 72.0 \\ Rephrasing & 66.0 & 65.1 & 49.7 & 91.2 & 88.0 & 64.2 & 58.1 & 49.6 & 85.1 & 83.4 & 50.0 & 72.9 \\   

Table 4: Performance (ASR-t) under two types of defense: **PPL Filter** and **Query Rephrasing**.

    &  &  &  \\   & ASR-r & ASR-r & ASR-r & ACC & PPL & ASR-r & ASR-r & ASR-r & ASR-r & PPL & ASR-r & ASR-r & ASR-r & ACC & PPL \\  w/o \(_{uni}\) & 57.4 & 63.1 & 51.0 & 87.8 & **13.7** & 25.5 & 58.6 & 42.0 & 57.1 & **63.7** & 65.6 & 88.5 & 37.7 & 65.6 & 64.9 \\ w/o \(_{cpt}\) & 63.0 & 64.4 & 54.0 & 90.1 & 14.2 & 38.6 & 61.1 & 47.0 & 62.8 & 67.1 & 82.0 & 93.4 & 59.0 & 72.5 & 62.25 \\ w/o \(_{cpt}\) & 81.3 & 61.8 & 51.8 & 91.3 & 95.7 & 71.2 & 47.5 & 42.9 & 62.0 & 71.5 & 90.6 & 37.8 & 75.4 & 581.0 \\ w/o \(_{coh}\) & **83.5** & 67.7 & **57.7** & **91.5** & 36.6 & **67.7** & **77.7** & 52.8 & **67.1** & 81.8 & 95.4 & 90.1 & 70.5 & **77.0** & 955.4 \\  
**AgentPoison** & 80.0 & **68.5** & 56.8 & 91.1 & **41.8** & 65.5 & 73.6 & **58.6** & **65.7** & 76.6 & **98.9** & **97.9** & **58.3** & **72.9** & **95.0** \\   

Table 2: An ablation study of the performance w.r.t. individual components in AgentPoison. Specifically, we study the case using GPT3.5 backbone and retriever trained with contrastive loss. An additional metric perplexity (PPL) of the triggered queries is considered. Best performance is in **bold**.

    &  &  &  \\   & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ASR-r & ACC \\  Letter injection & 46.9 & 64.2 & 45.0 & 91.6 & 84.9 & 69.7 & 57.0 & 52.1 & 90.3 & 95.6 & 53.8 & 70.0 \\ Word injection & 78.4 & 67.1 & 52.5 & 91.3 & 92.9 & 73.0 & 62.4 & 50.8 & 93.0 & 96.8 & 57.2 & 72.0 \\ Rephrasing & 66.0 & 65.1 & 49.7 & 91.2 & 88.0 & 64.2 & 58.1 & 49.6 & 85.1 & 83.4 & 50.0 & 72.9 \\   

Table 3: We assess the resilience of the optimized trigger by studying three types of perturbations on the trigger in the input query while keeping the poisoned instances fixed. Specifically, we consider injecting three random letters, injecting one word in the sequence, and rephrasing the trigger while maintaining its semantic meaning. We prompt GPT3.5 to obtain the corresponding perturbations.