# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Since TT-SVD requires performing SVDs of unfoldings of \(\), its cost is exponential in \(N\). Alternating Least Square (ALS) is another popular approach (Holtz et al., 2012) to find the TT approximation. Starting with a crude guess, each iteration of ALS involves solving a sequence of least squares problems. While ALS is the workhorse algorithm in many tensor decomposition problems, the computational cost is still exponential in the order of a tensor (\(N\)), since each iteration requires solving least squares problems involving unfoldings of \(\). These issues have led to the search for alternatives based on randomization and sampling techniques. A cheaper alternative to the TT-SVD with strong accuracy guarantees can be implemented by replacing the exact singular value decomposition (SVD) with a well-studied randomized counterpart (Halko et al., 2011; Huber et al., 2017). Randomized variants of the TT-ALS approach have received little attention. In Chen et al. (2023), the authors propose a randomized ALS algorithm that uses TensorSketch (Pham and Pagh, 2013) in each iteration. In this work, we also propose a novel randomized variant of the TT-ALS algorithm that relies on exact leverage score sampling. Notably, the sketch size in TensorSketch TT-ALS Chen et al. (2023) has an exponential dependence on the tensor dimension \(I\) whereas our algorithm avoids any dependence of the sketch size on \(I\).

**Our Contributions.** In this paper, we propose a new sampling-based ALS approach to compute the TT decomposition: rTT-ALS. By using exact leverage score sampling, we are able to significantly reduce the size of each ALS least squares problem while providing strong guarantees on the approximation error. At the core of rTT-ALS, we leverage the TT canonical form to efficiently compute the exact leverage scores and speed up the solutions of least square problems in each iteration of ALS. To the best of our knowledge, rTT-ALS is the first efficient TT decomposition by the ALS algorithm which relies on leverage scores sampling. We provide experiments on synthetic and real massive sparse and dense tensors showing that rTT-ALS can achieve up to 26\(\) speed-up compared to its non-randomized counterpart with little to no loss in accuracy.

Our core contribution is the following theorem, which shows that we can efficiently compute a subspace embedding of a left-orthogonal chain of TT tensor cores by efficiently sampling according to their squared row norms (for technical definitions and details see subsection 3.1).

**Theorem 1.1** (Row-norm-squared sampling for 3D core chains).: _Let \(_{1},...,_{j}\) be a sequence of 3D tensors, \(_{k}^{R_{k-1} I_{k} R_{k}}\) (with \(R_{0}=1\)). Assume that the left-matricization of each core is orthogonal. Let \(A_{ j}\) be the \(_{k=1}^{j}I_{k} R_{j}\) matrix obtained by unfolding the contraction of the tensor chain \(_{1},...,_{j}\). Then there exists a data structure to randomly sample rows from \(A_{ j}\) according to the distribution of its squared row norms with the following properties:_

1. _The data structure has construction time_ \(O(_{n=1}^{j}I_{n}R_{n-1}R_{n}^{2})\)_. When_ \(R=R_{1}=...=R_{j}\) _and_ \(I=I_{1}=...=I_{j}\)_, the runtime is_ \(O(jIR^{3})\)_. The space overhead of the data structure is linear in the sizes of the input cores._
2. _The data structure produces a single row sample from_ \(A_{ j}\) _according to the distribution of its squared row norms in time_ \(O(_{k=1}^{j}(I_{k}R_{k-1}/R_{k})R_{k}^{2})\)_. When all ranks_ \(R_{k}\) _and physical dimensions_ \(I_{k}\) _are equal, this complexity is_ \(O(jR^{2} I)\)_._

We highlight that the runtime required to construct the data structure is asymptotically identical to the runtime required to compute the canonical form of the tensor train subchain, i.e., \(A_{ j}\), by successive QR decompositions. This implies that the data structure construction and subsequent updates of the data structure do not increase the asymptotic complexity of our method.

## 2 Related work

Randomized algorithms and leverage score sampling-based methods (Mahoney et al., 2011; Woodruff et al., 2014; Drineas et al., 2006a) have been used widely in a large body of research particularly in tensor decomposition problems over the past two decades (Malik and Becker, 2021; Larsen and Kolda, 2022; Fahrbach et al., 2022) just to name a few.

(Cheng et al., 2016) propose SPALS, the first ALS-based algorithm relying on leverage score sampling for the CP decomposition. Their proposed method reduces the size of the least squares problem in each iteration of ALS with a sub-linear cost per iteration in the number of entries of the input tensor. Larsen and Kolda (2022) extends this method by combining repeated sampled rows in a deterministic and random sampling fashion. However, both of these methods use leverage score approximations and therefore require a number of samples which is exponential in the number of tensor modes in order to achieve relative-error performance guarantees. Malik (2022) proposes a method which avoids this exponential dependency on the number of tensor modes by using higher-quality leverage score estimates for the CP decomposition. The method is further improved by (Malik et al., 2022) to use exact rather than approximate leverage scores which is applicable for arbitrary tensor decompositions. Recently, (Bharadwaj et al., 2023) provided a novel data structure to efficiently sample from the exact distribution of the factor matrices' leverage scores in the Khatri-Rao product with time complexity logarithmic in the tensor size, leading to further improvements on the work in (Malik et al., 2022). Moreover, (Malik and Becker, 2021) proposed an ALS-based algorithm for finding the TR decomposition using leverage scores approximation. However, the runtime of their method has an exponential dependency on the order of a tensor. The sampler we propose in this paper is built on the work by Bharadwaj et al. (2023), extending it to the TT decomposition and leveraging the canonical form for further speed-up.

There are also a variety of non-ALS-based randomized algorithms for computing the TT decomposition. (Huber et al., 2017) leverages randomized SVD for the TT decomposition which accelerates the classical TT-SVD algorithm proposed by (Oseledets, 2011). To handle situations where the exact TT rank is unknown, (Che and Wei, 2019) propose an adaptive randomized algorithm that can achieve near optimal TT approximation. (Yu et al., 2023) present a method leveraging randomized block Krylov subspace iteration for computing TT approximations. Most of the algorithms for TT decomposition are based on the randomized SVD for matrices introduced by (Halko et al., 2011). In the quantum physics community, the ALS algorithm is widely used for finding TT decomposition and often yields more accurate results than TT-SVD. The randomized TT-SVD method struggles to scale for high-order tensors as it requires generating a random Gaussian matrix at each step that can only handle small-order tensors. More closely related to our work are those using sketching and sampling in each iteration of ALS to approximate the TT decomposition. Recently, (Chen et al., 2023) introduced an algorithm that employs TensorSketch (Pham and Pagh, 2013) in each iteration of a regularized ALS approach for TT decomposition. However, the sketch size has an exponential dependency to a tensor dimension. By contrast, our proposed algorithm requires the sketch size with no dependence on the tensor dimension and it depends only on the column size of the design matrix and \(\) and \(\) parameters.

## 3 Preliminaries

We use capital letters \(A\) to denote matrices and script characters \(\) to denote multidimensional arrays. We use Matlab notation for slices of matrices and tensors. We use the tuple notation to indicate the position of entries of arrays. For example, \((i_{1},i_{2},i_{3})\) indicates the \((i_{1},i_{2},i_{3})\)-th element of \(\). \(A[i,:]\) and \(A[:,i]\) refer to the \(i\)-th row and column of \(A\), respectively; for a three-dimensional tensor \(^{R_{1} I_{1} R_{2}}\), the matrix \([:,i,:]^{R_{1} R_{2}}\) is the \(i\)-th lateral slice of \(\). For a positive integer \(n\), we use \([n]\) to denote the set of integers from 1 to \(n\). For \(i_{1}[I_{1}],,i_{N}[I_{N}]\), the notation \(i_{1} i_{n}=}{n}+1+_{n=1}^{N}(i_{n}-1)_{j=1}^ {N-1}I_{j}\) will be helpful for tensor unfoldings. We use \(\) and \(\) to denote the Kronecker and Khatri-Rao products, respectively (see definitions in Appendix A). We use \(I_{d}\) to denote the \(d d\) identity matrix, \(A^{}\) for the transpose of \(A\), \(A^{+}\) for the pseudo-inverse of \(A\), \(\|\|_{F}\) for the Frobenius norm and \(\|\|_{2}\) for the Euclidean norm of a vector. We use \(\) to indicate the presence of multiplicative terms polylogarithmic in \(R\) and \(1/\).

### Tensor Train Decomposition

Let \(^{I_{1} I_{N}}\) be an \(N\)-dimensional array. A rank \((R_{1},,R_{N-1})\)_tensor train (TT) decomposition_ of a tensor \(^{I_{1} I_{N}}\) factorizes it into the product of \(N\) third-order tensors \(_{n}^{R_{n-1} I_{n} R_{n}}\) for \(n[N]\) (with \(R_{0}=R_{N}=1\)):

\[(i_{1},,i_{N})=_{r_{0},,r_{N}}_{n=1}^{N} _{n}(r_{n-1},i_{n},r_{n}),\]for all \(i_{1}[I_{1}],,i_{N}[I_{N}]\), where each \(r_{n}\) ranges from 1 to \(R_{n}\). A tensor network representation of a TT decomposition is shown in Figure 1. We call \(_{1},_{2},,_{N}\) core tensors and we use \(((_{n})_{n=1}^{N})\) to denote a TT tensor with factors \(_{1},,_{n}\).

**Definition 3.1**.: _The mode-\(n\) unfolding of a tensor \(^{I_{1} I_{N}}\) is the matrix \(X_{(n)}^{I_{n}_{j n}I_{j}}\) defined element-wise by \(X_{(n)}(i_{n}, i_{n-1}i_{n+1} i_{N}}) }}{{=}}(i_{1},,i_{N})\)._

_As a special case, we denote the left (resp. right) matricization of a 3-dimensional tensor \(^{I_{1} I_{2} I_{3}}\) by \(A^{L}=(A)_{(3)}^{}^{I_{1}I_{2} I_{3}}\) and \(A^{R}=A_{(1)}^{I_{1} I_{2}I_{3}}\)._

Given a TT decomposition \(((_{n})_{n=1}^{N})\) and an index \(j\), we will often use the left-chain \(A_{<j}_{_{i=1}^{j-1}I_{h} R_{j-1}}^{i_{-1} }\) and right-chain \(A_{>j}^{R_{j}_{k=j+1}^{N}I_{k}}\) unfoldings obtained by matricizing the contraction of all cores on the left and on the right side of the \(j\)-th core. Formally,

\[A_{<j}(i_{<j},r_{j-1})= _{r_{0},,r_{j-1}}_{k=1}^{j-1}_{k}(r_{k-1 },i_{k},r_{k})A_{>j}(r_{j},i_{>j})= _{r_{j+1},,r_{N}}_{k=j+1}^{N}_{k}(r_{k-1},i_{k},r_{k})\]

where \(i_{<j}=i_{1} i_{j-1}\) and \(i_{>j}= i_{N}}\). We also use \(A^{ j}}}{{=}}A_{<j} A_{>j}^{} _{_{k j}I_{k} R_{j-1}R_{j}}\) to denote the unfolding of the contraction of all cores except the \(j\)-th one.

We conclude by introducing the canonical form of the TT decomposition (Holtz et al., 2012; Evenbly, 2018, 2022) which will be central to the design of our algorithm.

**Definition 3.2**.: _A TT decomposition \(((_{n})_{n=1}^{N})^{I_{1} I _{N}}\) is in a canonical format with respect to a fixed index \(j[N]\) if \(A_{n}^{L^{}}A_{n}^{L}=I_{R_{n}}\) for all \(n<j\), and \(A_{n}^{R}A_{n}^{R^{}}=I_{R_{n-1}}\) for all \(n>j\) (see Figure 2)._

Note that any TT decomposition can efficiently be converted to canonical form w.r.t. any index \(j[N]\) by performing a series of QR decompositions on the core tensors (Holtz et al., 2012; Evenbly, 2018).

### Alternating Least Squares with Tensor Train Structure.

The TT decomposition problem consists in finding a low-rank approximation \(((_{n})_{n=1}^{N})\) of a given tensor \(\): \(*{argmin}_{_{1},,_{N}}\|-(_{1},,_{N})\|_{F}\) where \(\) is the target tensor with dimensions \(I_{1} I_{N}\). Since this is a non-convex optimization problem, the popular alternating least-squares (ALS) approach can be used to find an approximate solution (Kolda and Bader, 2009). Fixing all cores except the \(j\)-th one, the low rank approximation problem can be reformulated as a linear least squares problem:

\[*{argmin}_{_{j}}\|(A_{<j} A_{>j}^{ })(A_{j})_{(2)}^{}-X_{(j)}^{}\|_{F}.\] (1)

The ALS approach finds an approximate solution by keeping all cores fixed and solving for the \(j\)-th one. Then repeat this procedure multiple times for each \(j[N]\) until some convergence

Figure 1: Tensor Train decomposition of a 5-dimensional tensor in tensor network notation.

Figure 2: Orthonormal TT decomposition. The cores at the left side of \(_{3}\) are left-orthonormal and the cores at the right are right-orthonormal.

criteria is met. While ALS is the workhorse algorithm in many tensor decomposition problems, the computational cost is still \(O(I^{N})\) for \(I_{1}==I_{N}=I\). In this work, to reduce this cost, we will combine ALS with core orthogonalization to efficiently compute the exact leverage scores. This will also lead to a stable algorithm for computing TT. To compute the orthogonalized TT approximation, we start with a crude TT decomposition in canonical form (see Definition 3.2) where all cores except the first one are right-orthonormal. After optimizing the first core, a QR decomposition is performed and the non-orthonormal part is merged into the next core. This procedure repeats until reaching the right side of the decomposition. The same procedure is then repeated from the right until reaching the left side (see the tensor network illustration in Appendix A.1). Even after computing the canonical form, which makes the design matrix orthonormal for each linear least squares problem, we still need to multiply the matricized tensor by the chain of TT cores in canonical form. Given that the tensor may have hundreds of millions of nonzeros, this multiplication is a significant computational bottleneck: without sketching, the runtime for this matrix multiplication scales as \(O(()NR^{2})\), where \(()\) is the number of nonzeros in the tensor. Sketching allows us to select only a subset of rows from the design matrix and the corresponding subset of rows from the matricized tensor, reducing the cost to \(O(NR^{2})\). This approach also leads to providing computational benefits for computing the leverage scores and to an efficient sampling scheme which will be discussed in Section 4.

### Sketching and Leverage Score Sampling

There exists a vast literature on randomized algorithms (Mahoney et al., 2011; Woodruff et al., 2014) to solve the over-determined least squares problem \(_{x}\|Ax-b\|_{F}\) where \(A^{I R},I R\). Regardless of the structure of both \(A\) and \(b\), solving this least-squares problem costs \(O(IR^{2})\). To reduce this cost, we can randomly select rows of \(A\) and \(b\) by proposing a sketching operator \(S\) with \(J I\). Therefore, instead of solving the original least squares problem, we consider solving the downsampled version of the form \(_{x}\|SAx-S\|_{F}\), where \(S^{J I}\) and reduce the cost to \(O(JR^{2})\). The goal is to find a "good" sketch \(S\) to approximate the solution of the least squares problem at each step of the ALS algorithm. When each entry of \(S\) is selected according to the rows of \(A\) leverage scores, strong guarantees can be obtained for the solution of the downsampled problem.

**Definition 3.3**.: _(Leverage scores) Suppose \(A^{I R}\) with \(I R\). The \(i\)-th leverage score of the matrix \(A\) is defined as_

\[l_{i}(A)=A[i,:](A^{}A)^{+}A[i,:]^{}\ \ \ \ i[I].\] (2)

**Definition 3.4**.: _(Leverage score sampling) Let \(A^{I R}\) and \(p^{I}\) be a probability distribution vector with entries \(p_{i}=(A)}{(A)}\); where \((A)=_{i}l_{i}(A)\). Assume \(_{1},...,_{J}\) are drawn i.i.d according to the probabilities \(p_{1},,p_{I}\). The random matrix \(S^{J I}\) defined element-wise by \(S(j,i)=}}\) if \(_{j}=i\) and 0 otherwise is called a leverage score sampling matrix for \(A\)._

The following result is well-known and appeared in several works; see, e.g., (Drineas et al., 2006b); (Drineas et al., 2008), (Larsen and Kolda, 2022). We borrow the form presented in (Malik, 2022).

**Theorem 3.5**.: _(Guarantees for Leverage Score Sampling) Suppose \(A^{I R}\). Let \(S^{J I}\) be the leverage score sampling matrix defined in 3.4. For any \(,(0,1)\), if \(J=(R^{2}/)\), then \(^{*}=_{x}\|SAx-Sb\|_{2}\) satisfies \(\|Ax^{*}-b\|_{2}(1+)_{x}\|Ax-b \|_{2},\) with probability \(1-\)._

Computing leverage scores in Definition 3.3 requires computing the pseudo-inverse of \(A\), which costs \(O(IR^{2})\) and is as costly as directly solving the original least squares problem. In the following section, we will show that the leverage scores can be computed much more efficiently when \(A\) is the matrix appearing in the TT-ALS algorithm in canonical form..

## 4 Sampling-based Tensor Train Decomposition

In this section, we show how to efficiently sample rows of \(A^{ j}=A_{<j} A_{>j}^{}\) and \(X_{(j)}\) in Equation (1) according to the exact leverage scores distribution. In doing so, we will also present the sketch of the proof of Theorem 1.1 (which closely mirrors that of Bharadwaj et al. (2023) with key modifications required to adapt the procedure to a tensor core chain).

For each row \(i^{ j}= i_{j-1}i_{j+1} i_{N}}\) of \(A^{ j}\), Equation (2) gives

\[l_{i^{ j}}(A^{ j})=A^{ j}[i^{ j},:](A^{ j^{}}A^{ j})^ {+}A^{ j}[i^{ j},:]^{}.\] (3)

Computing \(}}{{=}}(A^{ j^{}}A^{ j})^ {+}\) is the main computational bottleneck in finding the leverage scores of \(A^{ j}\). Malik et al. (2022) proposed an algorithm to compute \(\) in time \(O(NIR^{2}+R^{3})\). In this paper, we leverage the fact that when the TT tensor is in canonical form w.r.t. mode \(j\), \(A^{ j}\) is orthogonal, and thus \(=I_{R^{2}}\). Therefore, computing \(\) is free of cost. By maintaining the canonical form of the TT tensor throughout the ALS algorithm, we can sketch the least square problems from the leverage score distributions with almost no computational overhead. We now explain how to efficiently sample rows of \(A^{ j}\) from the leverage scores distribution.

### Efficient Core Chain Leverage Score Sampling

As discussed above, when the TT tensor is in canonical form, the leverage score of row \(i^{ j}\) is given by \(l_{i^{ j}}(A^{ j})=A^{ j}[i^{ j},:]A^{ j}[:i^{ j}]^{}\). Leveraging the Kronecker structure of \(A^{ j}=A_{<j} A_{>j}^{}\), one can easily show that \(l_{i^{ j}}(A^{ j})=l_{i_{<j}}(A_{<j}) l_{i_{>j}}(A_{>j}^{})\). Sampling from the leverage scores distributions thus boish down to sampling rows of \(A_{<j}\) and \(A_{>j}^{}\) with probability proportional to their squared row norms (due to the orthogonality of \(A_{<j}\) and \(A_{>j}\) inherited from the canonical form). Without loss of generality, we detail the sampling procedure for \(A_{ j}\) (the difference between \(A_{ j}\) and \(A_{<j}\) amounts to reindexing). The sampling procedure for \(A_{>j}\) will be the same and straightforward.

Let \(_{1}[I_{1}]\,,...,_{j}[I_{j}]\) be random variables such that the multi-index \(_{ j}=_{1}_{j}\) follows the leverage score distribution of \(A_{ j}\). Since \(((_{n})_{n=1}^{N})\) is in canonical form w.r.t. \(j+1\), \(A_{ j}\) is an orthonormal matrix, hence \(_{1}_{j}\) is selected with probability proportional to the squared norm of the corresponding row of \(A_{ j}\):

\[p(_{1}=s_{1},,_{j}=s_{j}):=}(A_{ j }[ s_{j}},:] A_{ j}[ s_{ j}},:]^{}).\] (4)

Our sampling procedure will draw a lateral slice from each core starting from \(_{j}\) and ending with \(_{1}\), corresponding to a single row of \(A_{ j}\). Suppose we have drawn \(s_{k+1},,s_{j},\) for some \(k<j\). To sample the \(k\)-th index, we need to compute the conditional probability \(p(s_{k}|s_{k+1},,s_{j})=,...,s_{j})}{p(s_{k+1},...,s_{j})}\). The following lemma shows that this can be done efficiently by leveraging the underlying TT structure.

**Lemma 4.1** (Conditional distribution for \(_{k}\)).: _Consider the events \(_{j}=s_{j},,_{k+1}=s_{k+1}\), which we abbreviate as \(_{>k}=s_{>k}\). Then_

\[p(_{k}=s_{k}_{>k}=s_{>k})[H_{>k}^{ }_{k}[:,s_{k},:]^{}_{k}[:, s_{k},:] H_{>k}],\]

_where \(H_{>k}:=_{k+1}[:,s_{k+1},:]..._{j} [:,s_{j},:].\)_

The proof is given in Appendix B. Intuitively, \(H_{>k}\) acts as a "history matrix" conditioning on \(s_{>k}\), while the trace operation corresponds to marginalization over \(s_{<k}\). Unfortunately, updating \(H_{>k}\) through matrix multiplication as each index is selected still requires time \(O(R^{3})\) (assuming \(R_{1}=...=R_{j}=R\)). In order to further improve the runtime and reach the quadratic complexity in \(R\) claimed in Theorem 1.1, we make the following observation: let \(q^{_{1 j}I_{i}}\) be the probability vector for the leverage score distribution of \(A_{ j}\). Then Equation (4) can be rewritten in vector form as \(q:=}(A_{ j}[:,1]^{2}+...+A_{ j}[:,R_ {j}]^{2}).\) Here, the square of each column vector is an elementwise operation. Observe that each \(A_{ j}[:,r]^{2}\) is a probability vector (positive entries summing to one) due to the orthonormality of \(A_{ j}\). Hence \(q\) is a _mixture distribution_. To sample from \(q\), it thus suffices to select a single column \(\) of \(A_{ j}\) uniformly at random and restrict the sampling procedure to \(A_{ j}[:,]^{2}\). More formally, let \(\) be uniformly distributed over \([R_{j}]\) and let \(_{1},...,_{j}\) follow the conditional distributions defined by

\[p(_{k}=t_{k}_{k+1}=t_{k+1},,_{j}=t_{j},=r )=\|_{k}[:,t_{k},:] h_{>k}\|^{2},\] (5)

where \(h_{>k}=_{k+1}[:,t_{k+1},:]..._{j} [:,t_{j},r]\). We have the following result.

**Lemma 4.2**.: _For any choice of \(s_{j},...,s_{k}\), fix \(s_{j}=t_{j},s_{j-1}=t_{j-1},...,s_{k}=t_{k}\). After marginalizing over \(\), the conditional distribution of \(_{k}\) satisfies \(p(_{k}=t_{k}_{>k}=t_{>k})=p(_{k}=s_{k}_{>k }=s_{>k})\)._

As a consequence, the joint random variable \((_{1},...,_{j})\) follows the desired squared row-norm distribution of \(A_{ j}\) after marginalizing over \(\). The proof appears in Appendix B.2. Notice that the "history matrix" \(H_{>k}\) has been replaced by a vector \(h_{>k}\). This vector can be updated by matrix-vector multiplication, yielding a reduced sampling complexity with only a quadratic dependency on \(R\).

Our final improvement is to show that each sample from the distribution in Equation (5) can be drawn in time sublinear in the dimension \(I_{k}\) (after appropriate preprocessing). Letting \(A_{k}^{L}\) be the left unfolding of \(_{k}\), one can check that

\[p(_{k}=t_{k}_{>k}=t_{>k},=r)=_{i=0}^{R_{k-1}-1} (A_{k}^{L}[t_{k}R_{k-1}+i,:] h_{>k})^{2}.\] (6)

The probability of selecting the slice \(s_{k}\) is thus the sum of \(R_{k-1}\) consecutive entries from the probability vector \((A_{k}^{L} h_{>k})^{2}\). As a result, we can sample \(_{k}\) by first sampling an index in the range \([I_{k}R_{k-1}]\) given by \((A_{k}^{L} h_{>k})^{2}\), then performing integer division by \(R_{k-1}\) to obtain the corresponding slice index \(_{k}\). The advantage here lies in an efficient data structure for sampling from the weight vector \((A_{k}^{L} h_{>k})^{2}\), given by the following lemma:

**Lemma 4.3** (Bharadwaj et al. , Adapted).: _Given a matrix \(A^{I R}\), there exists a data structure with construction time \(O(IR^{2})\) and space usage \(O(IR)\) such that, given any vector \(h^{R}\), a single sample from the un-normalized distribution of weights \((A h)^{2}\) can be drawn in time \(O(R^{2}(I/R))\)._

The adaptation of this lemma is given in Appendix B.3. Lemma 4.3 enables us to efficiently draw samples according to the distribution in Equation 6, and therefore gives us a procedure to sample from the entire core chain. Constructing the data structure above for each matrix \(A_{k}^{L}\), \(1 k j\), costs \(O(IR_{k-1}R_{k}^{2})\) with a linear space overhead in the input core sizes. Drawing a sample from the \(k\)-th data structure requires time \(O(R_{k}^{2}(I_{k}R_{k-1}/R_{k}))\). Summing up this runtime over \(1 k j\) gives the stated complexity in Theorem 1.1. Algorithms 1 and 2 summarize the procedures to efficiently draw \(J\) samples from a left-orthogonal core chain. The construction procedure builds a set of data structures \(Z_{k}\) of the form given by Lemma 4.3 on the left-matricization of each tensor core. For each of \(J\) rows to draw, the sampling algorithm selects a column \(\) uniformly at random from the left matricization. It then initializes the history vector \(h\) and successively samples indices \(_{j-1},...,_{1}\) according to the conditional distribution, updating the history vector at each step. Appendix B.4 provides a rigorous proof of the correctness of the procedure sketched in this section.

While our procedure shares similarities with the Khatri-Rao product leverage score sampler, significant adaptations are required to sample from a tensor train core chain. The factors of a Khatri-Rao product can be sampled in any order, since the Khatri-Rao product of several matrices is commutative up to a permutation of its rows. By contrast, our sampling procedure **requires** us to sample from core \(_{j}\) down to \(_{1}\), since Lemma 4.1 exploits the left-orthogonality of the each core in its derivation. Starting the sampling procedure at \(_{j}\) leads to a "history matrix" to keep track of prior draws instead of the vector that would arise starting from core \(_{1}\). Here, our second innovation of sampling a column uniformly at random is required to bring down the overall sampling complexity. We can now state the following guarantee for **Randomized-TT-ALS** (rTT-ALS) applying the data structure in Theorem 1.1. The proof is given in Appendix B.5.

**Corollary 4.4**.: _(rTT-ALS) For any \(,(0,1)\) the sampling procedure proposed above guarantees that with \(J=(R^{2}/)\) samples per least-square problem, we have_

\[\|A^{ j}(_{j})_{(2)}^{}-X_{(j)}^{}\|(1+ )_{(A_{j})_{(2)}}\|A^{ j}(A_{j})_{(2)}^{}-X_{(j)} ^{}\|,\]

_with probability \((1-)\), where \(_{j}\) is the solution of the sketched least-squares problem, for all least-squares solve. The efficient sampling procedure of Theorem 1.1 brings the overall complexity to \((R^{4}_{j=1}^{N}N I_{ j}+I_{j}),\) where "\(\#\)it" is the number of ALS iterations._

Algorithms 1 and 2 refer to procedures "BuildSampler" and "RowSample" that were first used to sample from the Khatri-Rao product. The \(k\)-th BuildSampler data structure creates a full binary tree that truncated to \( I_{k}R_{k-1}/R_{k}\) levels, each caching an \(R_{k} R_{k}\) matrix containing information from \(A_{k}^{L}\). To draw a sample, the RowSample procedure executes a random walk from the root to the leaves requiring \(O(R_{k}^{2})\) work at each internal node. The sampler performs matrix-vector multiplication with the cached data at each internal node and compares the output value to a threshold, using the comparison to branch either left or right in the random walk. Assuming \(R_{1}=...=R_{j}\) and \(I=I_{1}=...=I_{j}\), the storage cost of each sampler is \(O(IR^{2})\) (computed by multiplying the matrix size stored at each node by the maximum node count). By multiplying the tree depth by the matrix-vector multiplication cost at each node, we get runtime cost \(O(R^{2} I)\) to draw one sample from \(A_{k}^{L}\). For the motivation behind the procedure, details of its correctness, and pseudocode, we refer the reader to the original work .

## 5 Experiments

All experiments were conducted on CPU nodes of the NERSC Perlmutter, an HPE Cray EX supercomputer, and the Mila Quebec AI Institute compute cluster. Our code is available at https://github.com/vbharadwaj-bk/ortho_tt_subspace_embedding. In this section, we demonstrate the effectiveness of the proposed rTT-ALS on two types of tensors: (i) synthetic and real dense datasets and (ii) real sparse datasets. We use the fit as evaluation metric (higher is better): \((},)=1-\|}-\|_{F}/\|\|_{F}\), where \(}\) is the TT approximation and \(\) is the target tensor. The goal of the dense tensor experiments is to show that rTT-ALS has a better time complexity than TT-ALS and TT-SVD while matching rTT-SVD in terms of fit. The sparse tensor experiments show that SVD-based decompositions cannot handle high-order (sparse) tensors. We compare rTT-ALS with the classical TT-ALS. The runtime improvements are most significant for large sparse tensors. Figure 4 compares accuracy (y-axis, higher is better) against ALS iteration time, for rTT-ALS versus non-randomized ALS. The speedup per iteration can be as high as 26x for lower ranks. Particularly, for the NELL-2 tensor, the plot shows that accuracy within three significant figures of non-randomized ALS was achieved roughly 3-4x faster than an optimized non-randomized ALS baseline.

### Decomposition of Synthetic and Real Dense Datasets

We compare rTT-ALS to three other methods; TT-SVD , Randomized TT-SVD (rTT-SVD)  and TT-ALS . We use TensorLy  for SVD-based methods and our own implementation for deterministic TT-ALS. For simplicity, we set \(R_{1}==R_{N-1}=R\) for all experiments. For all algorithms, we illustrate the quality of performance by fit and runtime.

**Synthetic Data Experiments.** For the synthetic data experiment, we generate random tensors of size \(I I I\) for \(I\{100,,500\}\) and of TT rank \(R=20\) (by drawing each core's components i.i.d. from a standard normal distribution). A small Gaussian noise with mean zero and standard deviation of \(10^{-6}\) is added to each entry of the resulting tensor. We then run the four methods to find a rank \(=5\) approximation of the target tensor. ALS-based methods are initialized using their SVD-based counterpart (TT-ALS with the output of TT-SVD and rTT-ALS with the output of rTT-SVD) and are run for 15 iterations. The sample count for rTT-ALS is fixed to \(J=5000\) for all values of \(I\). The average fit over 5 trials for all four algorithms are reported as a function of the dimension in Figure 3. rTT-ALS is about \(2\) faster than TT-ALS and \(3\) faster than TT-SVD for \(I=500\). Although rTT-SVD is the fastest method, it achieves poor performance in terms of fit.

Real Data Experiments.For the real data experiment, we consider four real images and video datasets (more details about datasets are given in Appendix C): (i) Pavia University is a hyper-spectral image dataset of size \((610 340 103)\), (ii) DC Mall is also a dataset of hyper-spectral images of size \((1280 307 191)\). Both datasets are three-dimensional tensors where the first two dimensions are the image height and width, and the third dimension is the number of spectral bands, (iii) the MNIST dataset is of size \((60000 28 28)\), and iv) Tabby Cat is the three-dimensional tensor of size \((720 1280 286)\) which contains grayscale videos of a man sitting on a park bench and a cat, respectively. The first two dimensions are frame height and width, and the third dimension is the number of frames. For all datasets, the preprocessing step is done by tensorizing data tensors into higher-dimensional tensors. Table 1 illustrates the results for a single trial when \(=5\). For all datasets we keep the sample count fixed to \(J=2000\). Similarly to the synthetic data experiments, rTT-ALS is faster than TT-ALS and TT-SVD (up to \(10\) faster than TT-ALS).

### Approximate Sparse Tensor Train Decomposition

We next apply rTT-ALS to three large sparse tensors from FROSTT (Smith et al., 2017). Table 2 gives the fits achieved by our method to decompose these tensors. The largest of these tensors, NELL-2, has around 77 million nonzero entries with mode sizes in the tens of thousands. Fits for sparse tensor decomposition are typically low, but the factors of the resulting decomposition have

    &  &  &  &  \\  Method & Fit & Time & Fit & Time & Fit & Time & Fit & Time \\  TT-ALS & \(0.61\) & \(4.16\) & \(0.65\) & \(44.570\) & \(0.46\) & \(8.29\) & \(0.59\) & \(21.86\) \\
**rTT-ALS (proposal)** & \(0.60\) & \(0.82\) & \(0.65\) & \(7.360\) & \(0.45\) & \(2.20\) & \(0.59\) & \(2.81\) \\  TT-SVD & \(0.61\) & \(6.65\) & \(0.65\) & \(136.189\) & \(0.46\) & \(17.19\) & \(0.59\) & \(41.45\) \\ rTT-SVD & \(0.61\) & \(0.33\) & \(0.65\) & \(4.285\) & \(0.46\) & \(0.65\) & \(0.59\) & \(0.46\) \\   

Table 1: Decomposition results for real datasets with \(J=2000\) and the target rank \(=5\). Time is in seconds.

Figure 4: Fit as a function of time for three FROSTT tensors, \(R=6\), \(J=2^{16}\) for rTT-ALS. Thick lines are averages of 5 fit-time traces, shown by thin dotted lines.

Figure 3: Fit (left) for \(J=5000\) and running time (right) averaged over 5 trials for the synthetic data experiment.

successfully been mined for patterns (Larsen and Kolda, 2022). For these experiments, we chose all decomposition ranks equal with \(R_{1}=...=R_{N}=R\) and tested over a range of values for \(R\).

The fits produced by rTT-ALS match those produced by the non-randomized ALS method up to variation in the third significant figure for Uber and NELL-2, with slightly higher errors on the Enron tensor. We kept the sample count for our randomized algorithms fixed at \(J=2^{16}\) throughout this experiment. As a result, the gap between the fit of the randomized and exact methods grows as the target rank increases, which our theory predicts.

Table 2 also reports the average speedup per ALS sweep of rTT-ALS over the exact algorithm. On the NELL-2 sparse tensor with target rank 12, the non-randomized ALS algorithm requires an average of 29.4 seconds per ALS sweep, while rTT-ALS requires only 1.87 seconds. Figure 4 shows that our method makes faster progress than its non-randomized counterpart across all three tensors. Because we could not find a well-documented, high-performance library for sparse tensor train decomposition, we wrote a fast multithreaded implementation in C++, which serves as the baseline method in these figures and tables (the code will be made publicly available).

Figure 5 shows the impact of varying the sample count on the final fit. We find modest increases in accuracy for both Uber and NELL-2 as the sample count increases by a factor of 5 (starting from \(J=2^{15}\)). Increasing \(J\) has a smaller impact for the Enron tensor, which is generally more difficult to decompose beginning with i.i.d. random factor initialization (Larsen and Kolda, 2022).

## 6 Conclusion

We proposed a sampling-based ALS method leveraging an efficient data structure to sample from the exact leverage scores. More precisely, we show that by exploiting the canonical form of the TT decomposition, leverage scores can be computed efficiently for all the least squares problems of ALS. We provide strong theoretical guarantees for the proposed data structure. Experiments on massive dense and sparse tensors confirm the theoretical results. The sampling algorithm we proposed could be extended to more general tree-based tensor network structures, leveraging canonical forms in a similar spirit to rTT-ALS.

    &  &  & NELL-2 \\  \(R\) & rTT-ALS & TT-ALS & Speedup & rTT-ALS & TT-ALS & Speedup & rTT-ALS & TT-ALS & Speedup \\ 
4 & 0.1332 & 0.1334 & 4.0x & 0.0498 & 0.0507 & 17.8x & 0.0213 & 0.0214 & 26.0x \\
6 & 0.1505 & 0.1510 & 3.5x & 0.0594 & 0.0611 & 12.4x & 0.0265 & 0.0269 & 22.8x \\
8 & 0.1646 & 0.1654 & 3.0x & 0.0669 & 0.0711 & 10.5x & 0.0311 & 0.0317 & 22.2x \\
10 & 0.1747 & 0.1760 & 2.4x & 0.0728 & 0.0771 & 8.5x & 0.0350 & 0.0359 & 20.5x \\
12 & 0.1828 & 0.1846 & 1.5x & 0.0810 & 0.0856 & 7.4x & 0.0382 & 0.0394 & 15.8x \\   

Table 2: Average fits and speedup, \(J=2^{16}\) for randomized algorithms, 40 ALS iterations. The speedup is the average per-iteration runtime for a single exact ALS sweep divided by the average time for a single randomized sweep.

Figure 5: Final fit of sparse tensor decomposition for varying sample counts. Each boxplot reports statistics for 5 trials. The blue dashed lines show the fit for non-randomized ALS.