# Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems

Bingcong Li Liang Zhang Niao He

###### Abstract

Sharpness-aware minimization (SAM) improves generalization of various deep learning tasks. Motivated by popular architectures such as LoRA, we explore the implicit regularization of SAM for scale-invariant problems involving two groups of variables. Instead of focusing on commonly used sharpness, this work introduces a concept termed _balancedness_, defined as the difference between the squared norm of two variables. This allows us to depict richer global behaviors of SAM. In particular, our theoretical and empirical findings reveal that i) SAM promotes balancedness; and ii) the regularization on balancedness is _data-responsive_ - outliers have stronger impact. The latter coincides with empirical observations that SAM outperforms SGD in the presence of outliers. Leveraging the implicit regularization, we develop a resource-efficient SAM variant, balancedness-aware regularization (BAR), tailored for scale-invariant problems such as finetuning language models with LoRA. BAR saves \(95\%\) computational overhead of SAM, with enhanced test performance across various tasks on RoBERTa, GPT2, and OPT-1.3B.

## 1 Introduction

Sharpness-aware minimization (SAM) is emerging as an appealing optimizer, because it enhances generalization performance on various downstream tasks across vision and language applications (Foret et al., 2021; Chen et al., 2022; Bahri et al., 2022). The success of SAM is typically explained using its implicit regularization (IR) toward a flat solution (Wen et al., 2023).

However, existing results only characterize sharpness/flatness near _local_ minima (Wen et al., 2023). Little is known about early convergence, despite its crucial role in SAM's implicit regularization (Agarwala and Dauphin, 2023). In addition, theoretical understanding of SAM highly hinges upon the existence of positive eigenvalues of Hessians (Wen et al., 2023), leaving gaps in nonconvex scenarios where the Hessian can be negative definite. The limitations above lead to our first question (**Q1**): _can we broaden the scope of implicit regularization to depict global behaviors in SAM?_

Moreover, scenarios where SAM popularizes often involve certain form of data anomalies, such as outliers and large data variance. SAM has provable generalization benefits on sparse coding problems in the small signal-to-noise ratio (SNR) regime (Chen et al., 2023). Remarkable performance of SAM is also observed under distributional shifts, e.g., domain adaptation (Wang et al., 2023), meta-learning (Abbas et al., 2022), and transfer learning in language models (Bahri et al., 2022; Sherborne et al., 2023). Evidences above motivate our second question (**Q2**): _can implicit regularization of SAM reflect its enhanced performance under data anomalies?_

This work answers both Q1 and Q2 within a class of _scale-invariant_ problems. The focus on scale-invariance is motivated by its prominence in deep learning architectures. Consider variablesand \(^{d_{2}}\), both in high-dimensional space. The problems of interest can be categorized into non-overparametrization (NOP) and overparametrization (OP), based on whether the dimension of variables (\(d_{1}+d_{2}\)) is greater than dimension of dom \(f\),

\[}_{,}f_{n}( ^{})=_{}f_{n}^{}( ^{}),\] (1a) \[}_{,}f_{o}( ^{})=_{}f_{o}^{}(^{}).\] (1b)

Here, \(d_{1}=d_{2}\) is assumed for OP, and \(\) denotes the training data. For both cases, the losses are nonconvex in \((,)\). Scale-invariance refers to that \((,/)\) share the same objective value \( 0\). It naturally calls for implicit regularization from optimization algorithms to determine the value of \(\). We focus on two-variable problems in the main text for simplicity and generalize the results to multi-layer cases in the appendix. Problems (1a) and (1b) are inspired by widely-adopted modules in deep learning, where low rank adapters (LoRA) for finetuning language models is NOP, and softmax in attention falls in OP framework (Hu et al., 2022; Vaswani et al., 2017).

This work studies SAM's implicit regularization on _balancedness_, defined as \(_{t}=\|_{t}\|^{2}-\|_{t}\|^{ 2}\). Balancedness is a useful alternative to sharpness for (1) because: i) it enables us to go beyond local minima and describe the behavior over SAM's entire trajectory; ii) analyses and assumptions can be significantly simplified when working with \(_{t}\); and, iii) it enables a data-driven perspective for understanding SAM. Building on balancedness, we answer our major questions.

For Q1, we prove that even with imbalanced initialization, SAM drives \(|_{t}| 0\) for OP, while ensuring a small \(|_{t}|\) in NOP. In contrast, we also prove that balancedness of SGD is unchanged over iterations. This clear distinction between SAM and SGD is illustrated in Fig. 1. Thanks to the adoption of balancedness, our results on implicit regularization have no requirement on the batchsize compared to (Wen et al., 2023) and can be extended to explain \(m\)-sharpness in (Foret et al., 2021).

Regarding Q2, we present analytical and empirical evidences that data anomalies (e.g., samples with large noise) have stronger impact on balancedness for both NOP and OP. Fig. 1 showcases an example where SAM is applied on the same problem with different SNRs. Smaller SNR (i.e., larger \(\)) promotes balancedness faster. Being more balanced with noisy data also aligns well with previous studies (Chen et al., 2023; Wang et al., 2023), which show that SAM performs better than SGD under data anomalies. This data-driven behavior of SAM is well depicted through balancedness.

Our theoretical understanding on balancedness also cultivates practical tools. In particular, we explicitly the implicit regularization of SAM as a _data-driven_ regularizer. When applied on top of, e.g., SGD, it enables a computationally efficient variant of SAM, balancedness-aware regularization (BAR), suited for scale-invariant problems such as finetuning language models with LoRA (Hu et al., 2022). BAR eliminates the need to compute the second gradient in SAM, thereby significantly reducing overhead in large-scale settings. BAR improves the test performance of LoRA on three representative downstream tasks on RoBERTa, GPT2, and OPT, while saving \(95\%\) computational overhead of SAM. Moreover, this is the _first_ efficient SAM approach derived from SAM's implicit regularization. In a nutshell, our contribution can be summarized as:- SAM favors balanced solutions for both NOP and OP, and data anomalies have stronger regularization on balancedness.
* **Practice.** Implicit regularization of SAM is made explicit for practical merits. The resulting approach, balancedness-aware regularization (BAR), improves accuracy for finetuning language models with LoRA, while significantly saving computational overhead of SAM.

**Notation**. Bold lowercase (capital) letters denote column vectors (matrices); \(\|\|\) stands for \(_{2}\) (Frobenius) norm of a vector (matrix), and \(()^{}\) refers to transpose.

### Related Work

Related topics are streamlined here, with comprehensive discussions deferred to Apdx. A.2.

**Scale-invariance in deep learning.** Scale-invariant modules are prevalent in modern neural networks, such as LoRA, ReLU networks, and softmax in attention. However, scale-invariant problems are not yet fully understood, especially from a theoretical perspective. Neyshabur et al. (2018) develop scale-invariant PAC-Bayesian bounds for ReLU networks. A scale-invariant SGD is developed in (Neyshabur et al., 2015), and this approach becomes more practical recently in (Gonon et al., 2024). Linear neural networks entail scale-invariance and overparametrization simultaneously, and IR of (S)GD on quadratic loss is established in (Arora et al., 2018; Du et al., 2018; Gidel et al., 2019). IR of GD for softmax attention in transformers is studied in (Sheen et al., 2024) assuming linearly separable data. It is pointed out in (Dinh et al., 2017) that sharpness is sensitive to scaling, while our results indicate that when taking the training trajectory into account, SAM excludes extreme scaling.

**Mechanism behind SAM.** To theoretically explain the success of SAM, Bartlett et al. (2023) analyze sharpness on quadratic losses. Wen et al. (2023) focus on sharpness of SAM near the solution manifold on smooth loss functions, requiring batchsize to be 1 in the stochastic case. Andritschenko and Flammarion (2022) consider sparsity of SAM on (overparametrized) diagonal linear networks on a regression problem. Chen et al. (2023) study the benign overfitting of SAM on a two-layer ReLU network. In general, existing studies on SAM's implicit regularization focus more on sharpness and do not fully capture scale-invariance. In comparison, our results i) are Hessian-free and hence sharpness-free; ii) have no constraint on batchsize; and iii) hold for both NOP and OP.

**SAM variants.** Approaches in (Kim et al., 2022; Kwon et al., 2021) modify SAM for efficiency under coordinate-wise ill-scaling, while our results suggest that SAM favors balancedness between layers. Computationally efficient SAM variants are developed through reusing or sparsifying gradients (Liu et al., 2022; Mi et al., 2022); stochastic perturbation (Du et al., 2022); switching to SGD (Jiang et al., 2023); and connecting with distillation (Du et al., 2022). Our BAR can be viewed as resource-efficient SAM applied specifically for scale-invariant problems such as LoRA. Different from existing works, BAR is the first to take inspiration from the implicit regularization of SAM.

## 2 Preliminaries

This section briefly reviews SAM and then compares sharpness with balancedness. For a smoother presentation, our main numerical benchmark, LoRA (Hu et al., 2022), is revisited in Sec. 5.

### Recap of SAM

Sharpness-aware minimization (SAM) is designed originally to seek for solutions in flat basins. The idea is formalized by enforcing small loss around the entire neighborhood in parameter space, i.e., \(_{}_{\|\|}h(+ )\), where \(\) is the radius of considered neighborhood, and \(h():=_{}[h^{}()]\). Practical implementation of SAM is summarized under Alg. 1. It is proved in (Wen et al., 2023) that \(\| h_{t}()\| 0\) (in line 5) holds for any \(\) under most initialization. Based on this result and similar to (Dai et al., 2023), we assume that SAM iterates are well-defined.

**Limitation of sharpness.** Coming naturally with SAM is the so-termed sharpness, given by \(():=_{\|\|}h(+)-h()\). When \(\| h()\| 0\), \(()\) can be approximated using (scaled) largest eigenvalue of Hessian (Zhuang et al., 2022). This approximation is widely exploited in literature to study the implicit regularization of SAM. Consequently, most results only hold _locally_ - behaviors near \(\| h()\| 0\) are studied. In addition, sharpness (the largest eigenvalue) is not always informative for scale-invariant problems (1). Consider \(h(x,y)=xy\) for example. The sharpness is \(1\) for any \((x,y)\) - these points are not distinguishable in terms of sharpness.

### Prelude on Balancedness

Balancedness \(_{t}:=\|_{t}\|^{2}-\|_{t}\|^ {2}\) turns out to be an intriguing alternative to sharpness on the scale-invariant problem (1). Being a global metric, balancedness is capable of describing the entire trajectory of an algorithm, regardless of proximity to critical points or definiteness of Hessian.

How does \(_{t}\) evolve in different algorithms? To set a comparing benchmark of SAM, we first borrow results from previous works on SGD. Following implicit regularization literature such as (Arora et al., 2018, 2019; Wen et al., 2023), we consider SGD with infinitesimally small learning rate \( 0\) for the NOP problem (1a)

\[_{t+1}=_{t}-_{t},_{ t+1}=_{t}-_{_{t}}.\] (2)

**Theorem 1** ((Arora et al., 2018, 2019; Ji and Telgarsky, 2019; Ahn et al., 2023)).: _When applying SGD on the NOP (1a), the limiting flow with \( 0\) satisfies \(\|_{t}\|^{2}-\|_{t}\|^{2}=\|_{0}\|^{2}-\| _{0}\|^{2}\) for all \(t>0\). In other words, \(_{t}}{dt}=0\) holds._

Theorem 1 shows that \(_{t}_{0}\) given \( 0\). A graphical illustration can be found in Fig. 1 (a). Another interesting observation is that given the same initialization, \(_{t}\) is fixed for SGD regardless of training datasets. This suggests that SGD is less adaptive to data. A similar result of Theorem 1 can be established for SGD on OP. The full statement is deferred to Apdx. C.1; see also Fig. 1 (b).

**Merits of being balance.** Because \(_{0}\) is preserved, SGD is sensitive to initialization. For example, \((_{0},_{0})\) and \((2_{0},0.5_{0})\) can result in extremely different trajectories, although the same objective value is shared at initialization. Most of existing works initialize \(_{0} 0\) to promote optimization benefits, because the variance of stochastic gradient is small and the local curvature is harmonized around a balanced solution. Take the stochastic gradient of NOP on minibatch \(\) for example

\[_{}=|}_{ } f_{n}^{}(^{}),\ _{}=|}_{ } f_{n}^{}(^{})^{}.\] (3)

Assuming bounded variance \([\|,|}_{} f_{n}^{ }(^{})- f_{n}(^{})\| ^{2}]^{2}\), it can be seen that the variance of \([_{},_{}]\) is bounded by \(^{2}(\|\|^{2}+\|\|^{2})\). In other words, among \(\{(,)|^{}=\}\), gradient variance is minimized if \(\|\|=\|\|\), i.e., being balance. Moreover, block smoothness parameters \(L_{n}^{}\) and \(L_{n}^{}\) also hint upon the difficulties for optimization, where large values typically correspond to slow convergence (Botto et al., 2018; Nesterov, 2004). With the help of Assumption 1 (in the next subsection), it can be seen that \(L_{n}^{}=L_{n}\|\|^{2}\) and \(L_{n}^{}=L_{n}\|\|^{2}\). In other words, a large \(|_{t}|\) implies difficulty for optimizing one variable than the other. For these reasons, balancedness is well-appreciated in domains such as matrix factorization/sensing - a special case of (1a) (Tu et al., 2016; Bartlett et al., 2018; Du et al., 2018; Ge et al., 2017). It is also observed that balanced neural networks are easier to optimize relative to unbalanced ones (Neyshabur et al., 2015).

### Assumptions and Prerequisites

To gain theoretical insights of scale-invariant problems in (1), we assume that the loss has Lipschitz continuous gradient on dom \(f\) following common nonconvex optimization and SAM analyses (Botto et al., 2018; Andriushchenko and Flammario, 2022; Wen et al., 2023).

**Assumption 1**.: _Let \(^{d_{1} d_{2}}\), and \(w\). For each \(\), \(f_{n}^{}()\) and \(f_{}^{}(w)\) in (1) have \(L_{n}\), and \(L_{o}\) Lipschitz continuous gradient, respectively._Scale-invariant problems are challenging to solve even on simple problems in Fig. 1. Even GD can diverge on some manually crafted initialization (De Sa et al., 2015; Arora et al., 2019). With proper hyperparameters this rarely happens in practice; hence, we focus on scenarios where SGD and SAM do not diverge. This assumption is weaker than the global convergence needed in (Andriushchenko and Flammarion, 2022), and is similar to the assumption on existence (Wen et al., 2023).

## 3 SAM for Non-Overparametrized Problems

This section tackles the implicit regularization of SAM on NOP (1a). Motivated by practical scenarios such as LoRA, we focus on cases initialized with large \(|_{0}|\).

When ambiguity is absent, the subscript in \(f_{n}\) and \(L_{n}\) is ignored in this section for convenience. Applying Alg. 1 on NOP, the update of SAM can be written as

\[}_{t}=_{t}+ u_{t}_{ _{t}},}_{t}=_{t}+ u_{t}_{_{t}}\] (4a) \[_{}_{t}}= f_{t}(}_{t}}_{t}^{})}_{t},_{ }_{t}}=[ f_{t}(}_{t}}_{t}^{})]^{}}_{t}\] (4b) \[_{t+1}=_{t}-_{}_{t}},_{t+1}=_{t}-_{ }_{t}},\] (4c)

where \(>0\) is the radius of SAM perturbation; \(u_{t}:=1/_{_{t}}\|^{2}+\|_{_{t }}\|^{2}}\); and \(f_{t}\), \( f_{t}\) denote the loss, stochastic gradient on minibatch \(_{t}\), respectively.

**Theorem 2**.: _(Dynamics of SAM.) Suppose that Assumption 1 holds. Consider SAM for NOP in (4) with a sufficiently small \(\). Let \(_{t}:=\|_{t}\|^{2}-\|_{t}\|^{2} \). For some \(|_{t}|=(^{2}L)\) and \( 0\), the limiting flow of SAM guarantees that_

\[_{t}}{dt}=_{_{t}}\|^{2}-\| _{_{t}}\|^{2}}{_{_{t}}\|^{2}+ \|_{_{t}}\|^{2}}}+_{t}.\] (5)

_Moreover, the change on \(_{t}\) depends on the difference of stochastic gradients on \(_{t}\) and \(_{t}\), i.e.,_

\[\|_{_{t}}\|-\|_{_{t}}\| -(^{2}L)|_{t}}{dt}|_{_{t}}\|^{2}-\|_{_{t}}\|^{2}} +(^{2}L).\] (6)

Unlike SGD for which \(_{t}}{dt}=0\), Theorem 2 states that the balancedness for SAM is driven by gradient difference \(\|_{_{t}}\|^{2}-\|_{_{t}}\|^{2}\). To gain some intuition, if we _estimate_\(\|_{_{t}}\|^{2}-\|_{_{t}}\|^{2} \|_{t}\|^{2}-\|_{t}\|^{2}\) based on (3) and ignore \(_{t}\), it can be seen that \(_{t}}{dt}-_{t}\). This indicates the contraction on \(|_{t}|\). A graphical illustration on decreasing \(|_{t}|\), and its relation with gradient difference can be found in Figs. 1 (a) and 2 (a). Moreover, this implicit regularization on balancedness is global as it holds for all \(t\) regardless of whether \((_{t},_{t})\) is close to local optima. Thanks to adopting balancedness as the metric, Theorem 2 also poses no requirement on the batchsize.

**SAM promotes balancedness.** As discussed in Section 2.2, unbalancedness is burdensome for optimization. SAM overcomes this by implicitly favoring relatively balanced solutions.

**Corollary 1**.: _(Informal.) Under some regularity conditions, there exists \(}^{}_{t} 0\) such that whenever \(|_{t}|>}^{}_{t}\), the magnitude of \(_{t}\) shrinks, where \(}^{}_{t}\) can be found in (21) at appendix._

Corollary 1 shows that SAM promotes balancedness until \(|_{t}|\) reaches lower bounds \(}^{}_{t}\). Because \(}^{}_{t}\) depends on SAM's trajectory, we plot \(_{0}^{T}}^{}_{t}dt\) using dotted lines for better visualization in Fig. 2 (a). It can be seen that our calculation on \(}^{}_{t}\) almost matches the balancedness of SAM after sufficient convergence. Being balance also reveals that the benefit of SAM can come from optimization, which is a perspective typically ignored in literature.

**Noisy data have stronger impact on balancedness.** Although our discussions extend to more general problems, for simplicity we consider the example in Fig. 2 (a), i.e., \([\|^{}-(+)\|^{2}]\), where \(\) is ground truth; \(\) is data noise; and \(\) determines SNR. For this problem, noisy data directly lead to noisy gradients. It can be seen in Fig. 2 (a) that smaller SNR coincides with faster decreasing of \(|_{t}|\). To explain such a data-responsive behavior in implicit regularization, Theorem 2 states that balancedness changes largely when the difference of \(\|_{_{t}}\|\) and \(\|_{_{t}}\|\) is large. Since \([\|_{_{t}}\|^{2}-\|_{_{t}}\|^ {2}]^{2}\) if assuming elements of \(\) to be iid unit Gaussian variables, it thus implies that a small SNR (large \(\)) offers large regularization on balancedness.

**Extension to LoRA (multi-layer two-variable NOP).** For LoRA, the objective is to minimize \(D\) blocks of variables simultaneously, i.e., \(_{}[f^{}(\{_{t}_{t}^{}\}_{l=1}^{D})]\). It is established in Theorem 5 in appendix that SAM cultivates balancedness in a layer-wise fashion, i.e., the magnitude of \(_{t,l}:=(\|_{t,l}\|^{2}-\|_{t,l}\|^{2})\) cannot be large for each \(l\). However, the \(|_{t,l}/t|\) can be \(()\) times smaller than Theorem 2 in the worst case because of the additional variables.

**Validation of IR on modern architectures.** Going beyond the infinitesimally small step size, we adopt \(=0.1\) on modern language models to validate our theoretical findings. We consider finetuning a RoBERTa-large with LoRA for few-shot learning tasks. More details can be found later in Section 6.1. Balancedness of SAM and SGD on different layers in various datasets are plotted in Fig. 3. SAM has a clear trend of promoting balancedness, aligning well with our theoretical predictions.

## 4 SAM for Overparametrized Problems

Next, we focus on SAM's implicit regularization on OP (1b). Overparametrization enables SAM to have stronger regularization on balancedness. Subscripts in \(f_{o}\) and \(L_{o}\) are omitted for convenience. SAM's per iteration update for OP can be summarized as

\[}_{t}=_{t}+ u_{t}_{t}, }_{t}=_{t}+ u_{t}_{t}\] (7a) \[_{}_{t}}=f_{t}^{}(}_{t}^{}}_{t})}_{t}, _{}_{t}}=f_{t}^{}(}_{t}^{ }}_{t})}_{t}\] (7b) \[_{t+1}=_{t}-_{ }_{t}}, _{t+1}=_{t}-_{}},\] (7c)

where \(u_{t}:=(f_{t}^{}(_{t}^{}_{t}))/_{t}\|^{2}+\|_{t}\|^{2}}\); \(f_{t}\) and \(f_{t}^{}\) denote the loss, stochastic gradient on minibatch \(_{t}\), respectively. Different from NOP, SAM has stronger regularization on balancedness, where \(|_{t}|\) decreases whenever the norm of stochastic gradient is large. To see this, it is convenient to define \(_{t}:=\|\|_{t}\|-\|_{t}\|\|\). Note that \(_{t}_{t}|}\).

**Theorem 3**.: _Consider \( 0\) for (7). The limiting flow of SAM on OP ensures a decreasing magnitude of \(_{t}\) whenever \(|f_{t}^{}(_{t}^{}_{t})|_{t}> ( L|_{t}|)\). Moreover, the speed of decrease can be lower- and upper- bounded as_

\[|f_{t}^{}(_{t}^{}_{t})|_{t }-(^{2}L|_{t}|)_{t}}{dt} |f_{t}^{}(_{t}^{}_{t})|_{t}|}+(^{2}L|_{t}|).\]

Given \( 0\) and sufficiently noisy data, Theorem 3 implies that \(|_{t}| 0\). Moreover, Theorem 3 also states that the regularization power on balancedness is related to both gradient norm and balancedness itself. The elbow-shaped curve of \(|_{t}|\) in Fig. 1 (b) demonstrates that the regularization power is reducing, as both gradient norm and balancedness shrink over time.

**Noisy data have stronger impact on balancedness.** As shown in Fig. 1 (b), balancedness is promoted faster on problems with lower SNR. This data-responsive behavior can be already seen from Theorem 3, because \(|_{t}/t|\) is directly related with \(|f_{t}^{}(_{t}^{}_{t})|\), and \([|f_{t}^{}(_{t}^{}_{t})|]\) is clearly larger when data are more noisy. In other words, SAM exploits noisy data for possible optimization merits from balancedness (see discussions in Sec. 2.2). Overall, the implicit regularization on balancedness aligns well with the empirical observations in presence of data anomalies (Wang et al., 2023; Sherborne et al., 2023), where SAM outperforms SGD by a large margin.

**Extension to \(m\)-sharpness.**\(m\)-sharpness is a variant of SAM suitable for distributed training. It is observed to empirically improve SAM's performance (Foret et al., 2021). \(m\)-sharpness evenly divides minibatch \(_{t}\) into \(m\) disjoint subsets, i.e., \(\{f_{t,j}\}_{j=1}^{m}\), and perform SAM update independently on each subset; see (38) in appendix. It turns out that \(m\)-sharpness can also be explained using balancedness. With formal proofs in Apdx. C.3, the IR of \(m\)-sharpness amounts to substitute \(|f_{t}^{}(_{t}^{}_{t})|\) in Theorem 3 with \(_{j=1}^{m}|f_{t,j}^{}(_{t}^{}_{t})|\). This means that the regularization on balancedness from \(m\)-sharpness is more profound than vanilla SAM, because \(_{j=1}^{m}|f_{t,j}^{}(_{t}^{}_{t} )||f_{t}^{}(_{t}^{}_{t})|\).

Finally, we connect balancedness with sharpness on local minima of OP.

**Lemma 1**.: _Let \(^{*}=\{(,)|^{}=w,f^{ }(w)=0,f^{}(w)>0\}\) be non-empty. For the OP problem (1b), minimizing sharpness within \(^{*}\) is equivalent to finding \(=0\) in \(^{*}\)._

This link showcases that by studying balancedness we can also obtain the implicit regularization on sharpness for free. A concurrent work also links balancedness with sharpness (the largest eigenvalue) for some one-hidden layer neural networks (Singh and Hofmann, 2024). Compared with (Wen et al., 2023), this is achieved with less assumptions and simplified analyses. More importantly, balancedness enables us to cope with arbitrary batchsize, to explain SAM's stronger regularization with noisy data, and to extend results to \(m\)-sharpness.

## 5 Implicit Regularization Made Explicit

Next, insights from our theoretical understanding of SAM are leveraged to build practical tools. We adopt LoRA (Hu et al., 2022) as our major numerical benchmark for scale-invariant problems given its prevalence in practice. More diverse examples on both OP and NOP can be found in Apdx. A.3. Compared to full parameter-tuning, LoRA is more economical in terms of memory not only for finetuning, but also for serving multiple downstream tasks. LoRA and its variants are actively developed and well welcomed by the community; see e.g., HuggingFace's PEFT codebase.2

### Overview of LoRA

Given a pretrained model with frozen weight \(_{l}^{d_{1} d_{2}}\) on a particular layer \(l\), the objective of LoRA is to find low rank matrices \(_{l}^{d_{1} r}\), and \(_{l}^{d_{2} r}\) with \(r\{d_{1},d_{2}\}\) such that the loss is minimized for a downstream task, i.e.,

\[_{\{_{l},_{l}\}_{l}}\{_{l }+_{l}_{l}^{}\}_{l}.\] (8)

LoRA enjoys parameter efficiency for finetuning thanks to the low-rank matrices \(_{l}\) and \(_{l}\). For instance, it only requires 0.8M trainable parameters to finetune a 355M-parameter RoBERTa-large (Hu et al., 2022). The outer product of \(_{l}\) and \(_{l}\) induces scale-invariance, and the number of variables renders it NOP. The downside of LoRA, on the other hand, is the drop on test performance due to the parsimony on trainable parameters. Unbalancedness is also unavoidable for LoRA, due

Figure 3: Implicit regularization of SAM on LoRA. We consider few shot learning with LoRA on a RoBERTa-large. For datasets RTE, SST-5, and MNLI, 1st, 12th and 24th query layers’ \(2|_{t,l}|\) are plotted, respectively. The layers are chosen to represent early, middle, and final stages of RoBERTa. The averaged \(}_{t,l}^{}\) in Corollary 1 is \(0.37\), \(0.21\), and \(0.29\), respectively.

to the need of initializing at \(_{l}(0,^{2}),_{l}=\); see an example of RoBERTa-large in Fig. 3. The unbalancedness leads to instability of LoRA when finetuning RoBERTa on datasets SST-2 and MNLI; see more details in Apdx. D.4.

Integrating SAM with LoRA is a case with mutual benefits - LoRA reduces the additional memory requirement of SAM, while SAM not only overcomes the distributional shift in finetuning (Zhou et al., 2022), but also mitigates the possible inefficiency associated with LoRA's unbalancedness.

### Balancedness-Aware Regularization (BAR)

However, directly applying SAM variants on LoRA exhibits two concerns: i) SAM doubles computational cost due to the need of two gradients; and ii) additional efforts are required to integrate SAM with gradient accumulation and low-precision training (HuggingFace), which are common techniques for memory and runtime efficiency in large-scale finetuning. Note that concern i) is annoying given the size of language models, especially in setups involving model parallelism.

Our balancedness-aware regularization (BAR) is a highly efficient approach to address both concerns, and it fixes the accuracy drop of LoRA relative to full-parameter finetuning. BAR is also the _first_ efficient SAM variant derived from implicit regularization. The key observation for our algorithm design is that SAM's implicit regularization on balancedness can be achieved with an explicit regularizer \(_{t}|^{}-^{}|\). This regularizer originates from matrix sensing; see e.g., (Tu et al., 2016; Ge et al., 2017). For OP, choosing \(_{t}:=(|f^{}(_{t}^{}_{t})|/ _{t}\|^{2}+\|_{t}\|^{2}})\) recovers SAM's dynamic on \(_{t}\) up to an error of \((^{2})\); cf. Lemma 2 in appendix. By ignoring this error, it can be seen that \(_{t}\) decreases when \(\|_{t}\|\|_{t}\|\). Following this dynamic, we regulate balancedness based on whether \(\|_{t}\|\|_{t}\|\). The resultant approach is termed as overparametrized BAR (oBAR) to reflect its source in OP.

On the other hand, because LoRA is NOP inherently, we take inspiration from Theorem 2 - dropping the term \(_{t}\) and mimicking dynamics of SAM. In particular, we regulate the objective with \(_{t}(^{}-^{})\) if \(\|_{_{t}}\|^{2}<\|_{_{t}}\|^{2}\); otherwise \(_{t}(^{}-^{})\). The resultant approach is termed as nBAR. A graphical illustration can be found in Fig. 2 (b). It can be observed that nBAR shares similar performance as SAM on NOP. Both nBAR and oBAR can be implemented in the same manner as weight decay, and their detailed steps are summarized in Algs. 2 and 3, respectively.

Another benefit of BAR, in additional to the lightweight computation, is that it can be applied individually on each LoRA layer. As previously discussed (cf. Theorem 5), the number of layers has a negative impact on balancedness. By overcoming this "curse of multi-layer", BAR can induce better test performance over SAM.

**Schedule of \(_{t}\).** In both nBAR and oBAR, one can employ a decreasing scheduler for \(_{t}\) for algorithmic flexibility. This is motivated by the fact that for both NOP and OP problems, the implicit regularization of SAM is less powerful after sufficient balancedness or near optimal. Commonly adopted cosine and linear schedules work smoothly.

[MISSING_PAGE_FAIL:9]

On average, oBAR leads to a gain of \(0.4\), and nBAR raises the test performance by \(0.6\). BAR thereby fills the gap of test performance between LoRA (0.8M) and full-parameter (355M) finetuning.

### Text Generation on GPT2-medium

Lastly, we consider BAR on a text-generation problem using GPT2-medium, a model with 345M parameters. Results on WebNLG (Gardent et al., 2017) are reported in Table 5. It can be seen that oBAR matches the performance of prefix tuning, while nBAR achieves the best BLEU score.

## 7 Discussions

This work provides theoretical and empirical evidence on the implicit regularization of SAM for both scale-invariant NOP and OP problems. Balancedness, as an alternative to commonly adopted sharpness, is employed as the metric to capture global and data-responsive behaviors of SAM. We find that i) SAM promotes variables to have (relatively) balanced norms; and ii) noisy data have stronger impact on balancedness. Lastly, we explicitly the implicit regularization as a data-driven regularizer to foster the design of a computationally efficient SAM variant, termed BAR. The effectiveness of BAR is demonstrated using various tasks on RoBERTa-large, GPT2 and OPT. BAR saves \(95\%\) overhead of SAM and enhances the accuracy of LoRA to the level of full-parameter finetuning.

**Limitation and Future directions.** Our approach, BAR, is best applied on scale-invariant modules in neural networks. Finetuning language models with LoRA, as a popular option in practice, is a setting naturally suitable for our approach. However, our approach does not apply for linear models, e.g., logistic regression. Regarding future directions, an interesting one is whether SAM has other forms of implicit regularization beyond balancedness and sharpness. The exploration of other scale-invariant architectures beyond LoRA, e.g., the softmax function in attention, is also deferred to future work.

   RoBERTa & \# para & STS-B & RTE & MRPC & CoLA & QQP & avg (\(\)) \\  FT\({}^{}\) & 355M & 92.4 & 86.6 & 90.9 & 68.0 & 90.2 & 85.6 \\  Adapter\({}^{*}\) & 0.8M & 91.9\(\)0.4 & 80.1\(\)2.9 & 89.7\(\)1.2 & **67.8\(\)**2.5 & **91.7\(\)**0.2 & 84.2 \\ LoRA & 0.8M & 92.4\(\)0.1 & 88.2\(\)0.6 & 89.6\(\)0.5 & 64.8\(\)1.4 & 91.4\(\)0.1 & 85.3 \\
**LoRA-oBAR** & 0.8M & **92.6\(\)**0.1 & 88.7\(\)0.2 & **90.3\(\)**0.9 & 65.1\(\)1.0 & 91.6\(\)0.1 & 85.7 \\
**LoRA-nBAR** & 0.8M & **92.6\(\)**0.2 & **89.2\(\)**1.3 & **90.3\(\)**0.4 & 65.6\(\)1.2 & 91.6\(\)0.1 & **85.9** \\   

Table 4: Finetuning RoBERTa (355M) with BAR. Results marked with \(\) are taken from (Hu et al., 2022), and those with \(*\) refer to Adapter\({}^{}\) in (Hu et al., 2022).

   OPT-1.3B & SST-2 & CB & RTE & COPA & ReCoRD & SQuAD & avg (\(\)) \\  Prefix & 92.9\(_{1.0}\) & 71.6\(_{3.0}\) & 65.2\(_{2.6}\) & 73.0\(_{1.0}\) & 69.7\(_{1.0}\) & 82.1\(_{1.4}\) & 75.8 \\ LoRA & 93.1\(_{0.2}\) & 72.6\(_{3.7}\) & 69.1\(_{4.8}\) & **78.0\(_{0.0}\)** & 70.8\(_{1.0}\) & 81.9\(_{1.8}\) & 77.6 \\ LoRA-SAM & 93.5\(_{0.5}\) & 74.3\(_{1.0}\) & **70.6\(_{2.7}\)** & **78.0\(_{0.0}\)** & 70.9\(_{1.2}\) & **83.0\(_{0.7}\)** & 78.4 \\
**LoRA-oBAR** & 93.6\(_{0.6}\) & 75.6\(_{4.5}\) & 70.4\(_{4.8}\) & **78.0\(_{0.0}\)** & 70.9\(_{0.8}\) & 82.5\(_{0.5}\) & 78.5 \\
**LoRA-nBAR** & **93.7\(_{0.7}\)** & **79.8\(_{4.4}\)** & 70.5\(_{2.4}\) & **78.0\(_{0.0}\)** & **71.0\(_{1.0}\)** & 82.3\(_{1.8}\) & **79.2** \\  Zero-Shot & 53.6 & 39.3 & 53.1 & 75.0 & 70.2 & 27.2 & 53.1 \\   

Table 3: Performance of BAR for few shot learning using OPT-1.3B.

   GPT2 & FT\({}^{*}\) & Prefix\({}^{*}\) & LoRA & LoRA-oBAR & LoRA-nBAR \\  \# param & 354M & 0.35M & 0.35M & 0.35M & 0.35M \\ BLEU (\(\)) & 46.5 & 55.1 & 54.99\(\)0.24 & 55.15\(\)0.19 & **55.20\(\)**0.16 \\   

Table 5: Finetuning GPT2 (345M) with BAR on WebNLG. Results of prefix tuning and full-parameter finetuning are obtained from (Hu et al., 2022).