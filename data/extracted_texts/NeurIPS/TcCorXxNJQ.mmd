# FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations

Ziyao Wang\({}^{1}\), Zheyu Shen\({}^{1}\), Yexiao He\({}^{1}\), Guoheng Sun\({}^{1}\) Hongyi Wang\({}^{2,3}\)

**Lingjuan Lyu\({}^{4}\), Ang Li\({}^{1}\)**

1. University of Maryland, College Park

2. Rutgers University 3. GenBio.ai 4. Sony Reasearch

{ziyaow,zyshen,yexiaohe,ghsun,angliece}@umd.edu,

hongyi.wang.001@rutgers.edu, lingjuanlysmile@gmail.com

###### Abstract

The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. These approaches led to mathematically inaccurate _aggregation noise_, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRA adapters. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs. Our code is available at https://github.com/ATP-1010/FederatedLLM.

## 1 Introduction

The Large Language Models (LLMs) have shown remarkable performance on various tasks, such as chatbots , virtual assistants , search engines , and healthcare [20; 18]. However, adapting pre-trained LLMs (_e.g.,_ Llama 2 ) to downstream tasks requires tremendous computation resources to fine-tune all the model parameters. To mitigate this issue, a variety of parameter-efficient fine-tuning (PEFT) methods have been proposed. One of the most widely used PEFT methods is low-rank adaptation (LoRA) . As shown in the top of Figure 1, LoRA adds a parallel branch of trainable adapters \(\) and \(\) to compute the model update \(\), where the ranks of \(\) and \(\) are much smaller than the pre-trained model parameter \(\). When applying LoRA for fine-tuning, only \(\) and \(\) are updated while the entire \(\) is frozen, thereby significantly reducing the GPU memory consumption.

Fine-tuning LLMs requires ample data for adaptation to specific downstream tasks [13; 7]. Often, this data is dispersed across a multitude of devices, raising privacy concerns. For instance, aggregating medical data from hospitals for centralized LLM fine-tuning poses significant challenges. Consequently, to facilitate fine-tuning without compromising private data, federated learning (FL) becomes essential, enabling LLM fine-tuning across distributed clients while preserving data privacy [16; 27; 33; 24]. In this work, we focus on federated fine-tuning, enabling distributed clients to collaboratively fine-tune LLMs for adaptation to downstream tasks while preserving data privacy.

Prior work, FedIT, proposed a federated fine-tuning method , integrating LoRA with FedAvg . In each FL round of FedIT, clients fine-tune LoRA modules using their local data and then send the fine-tuned modules to the server. The server averages all the local LoRA modules to obtain a global LoRA. Since only the weights of the LoRA modules are fine-tuned and communicated, FedIT effectively reduces both computation and communication costs.

However, FedIT faces two key issues. First, **the naive averaging of local LoRA modules in FedIT introduces noise to the global model update.** Specifically, FedIT averages local \(\) and \(\) independently, which introduces mathematical errors to the global LoRA. In short,

The cause of aggregation noise:

\[}_{}}_{}.\]

We will elaborate on this issue in Section 2 with theoretical analysis. Such an inaccurate aggregation will hinder convergence, leading to higher fine-tuning costs. Second, due to the heterogeneous data distribution [31; 12] and heterogeneous hardware resources, clients need to adapt LoRA ranks  according to the system and data heterogeneity. However, **FedIT cannot aggregate local LoRAs with heterogeneous ranks.**

In this work, we present FLoRA, an aggregation-noise-free federated fine-tuning method that supports heterogeneous LoRAs. Specifically, as shown in Figure 2, we propose to **stack** the local LoRA modules \(_{k}\) and \(_{k}\) separately to construct the global LoRA modules \(\) and \(\), where \(_{k}\) and \(_{k}\) denote the corresponding LoRA modules on the \(k\)-th client. This stacking method is theoretically proven to be accurate for the aggregation of local LoRA modules (Section 3.1). Additionally, it can naturally accommodate heterogeneous LoRA settings (Section 3.2), since stacking does not require the local LoRA modules to have identical ranks across clients. The noise-free aggregation of FLoRA accelerates convergence, which will in turn improve the overall computation and communication efficiency of federated fine-tuning. Furthermore, FLoRA can effectively cater to heterogeneous data and computational resources across clients, where heterogeneous ranks are applied. Our key contributions are summarized as follows:

* We propose FLoRA, a federated fine-tuning algorithm based on LoRA that can perform noise-free aggregation of local LoRA modules. Theoretical analysis shows that FLoRA eliminates the meaningless intermediate term in the global model update, leading to faster convergence and improved performance.
* The proposed stacking mechanism for aggregating LoRA modules supports heterogeneous LoRA ranks across clients, accommodating data and system heterogeneity in realistic settings. This encourages the broader participation of clients with heterogeneous data and resources in federated fine-tuning.
* We use FLoRA to fine-tune LLaMA, Llama2  and TinyLlama  on four benchmarks for two downstream tasks. Results show that FLoRA surpasses state-of-the-art methods for both homogeneous and heterogeneous settings.

## 2 Preliminaries

Fine-tuning LLMs with LoRA.LoRA  uses two decomposed low-rank matrices to represent the update of the target module:

\[^{}=+=+ ,\] (1)

Figure 1: The overview of LoRA, FedIT, and our FLoRA. The top row shows how LoRA updates the model in centralized fine-tuning. The middle and bottom rows show the global model updating strategies in FedIT and our FLoRA respectively.

where \(^{m n}\) and \(^{}^{m n}\) denote the pre-trained and fine-tuned parameters of target modules (_e.g.,_ attention modules), respectively. \(\) and \(\) are low-rank decomposition of \(\), where \(^{r n},^{m r}\), such that \(=\) with the identical dimensions as \(\) and \(^{}\). The rank of LoRA, denoted by \(r\), is typically significantly smaller than \(m\) and \(n\), leading to dramatic parameter reduction of \(\). During the fine-tuning phase, LoRA optimizes matrices \(\) and \(\) instead of directly updating \(\), thus achieving substantial savings in GPU memory usage. For example, in the context of the LLaMA-7B , the original dimension of attention modules is \(^{4096 4096}\), setting the LoRA rank to 16 reduces the decomposed matrices to \(^{16 4096}\) and \(^{4096 16}\). This approach decreases the number of trainable parameters to merely 0.78% of the entire parameter of the pre-trained model, offering a significant GPU memory footprint reduction.

FedIT: Averaging Homogeneous LoRA.The most widely used FL algorithm, _i.e.,_ FedAvg , aggregates all the local model updates by weighted averaging to update the global model in each communication round:

\[^{}=+_{k=1}^{K}p_{k}_{k}= +\] (2)

where \(^{}\) and \(\) denote the global model parameters before and after a communication round. \(_{k}\) represents the local model update from the \(k\)-th client, with \(p_{k}\) being the corresponding scaling factor that is typically weighted by the local data size, and \(\) represents the global model update.

FedIT  directly integrates FedAvg with LoRA to enable federated fine-tuning, where each client fine-tunes LoRA modules with a homogeneous rank. Specifically, the clients download the pre-trained LLM from the server, locally initialize and fine-tune the LoRA modules, and then send the updated LoRA modules to the server. The server updates the global LoRA modules \(\) and \(\) by independently applying weighted averaging across all local modules \(_{k}\) and \(_{k}\):

\[=_{k=1}^{K}p_{k}_{k},=_{i=0}^{K}p_ {k}_{k}.\] (3)

This aggregation of FedIT is almost the same as FedAvg except that only the LoRA modules are trained and communicated. However, such a naive aggregation introduces additional issues for federated fine-tuning. First, each single module \(\) or \(\) is not the model update, and only \(\) represents the model update. Thus, averaging \(_{k}\) and \(_{k}\)_independently_ to compute the aggregated gradients will introduce noises to the global model update. Here we use a simple example to explain how the noise is generated, and we assume that two clients are applying FedIT to perform federated fine-tuning. In a communication round, the two clients train \(_{0}\), \(_{0}\) and \(_{1}\), \(_{1}\) respectively. The local model updates \(_{0}\) and \(_{1}\) are the product of corresponding LoRA modules:

\[_{k}=_{k}_{k},k\{0,1\}.\] (4)

According to Equation 2, the expected global model update \(\) can be obtained by weighted averaging \(_{0}\) and \(_{1}\):

\[=p_{0}_{0}+p_{1}_{1}=p_{0} _{0}_{0}+p_{1}_{1}_{1}.\] (5)

However, according to Equation 3, FedIT aggregates \(\) and \(\) independently:

\[&==(p_{0}_{0}+p_ {1}_{1})(p_{0}_{0}+p_{1}_{1})\\ &=p_{0}^{2}_{0}_{0}+p_{1}^{2}_{1} _{1}+p_{1}(_{0}_{1}+ _{1}_{0})}.\] (6)

The global model update in Equation 6 differs from the expected one in Equation 2, mainly due to the underlined intermediate term that is obtained by the cross-product of LoRA modules from

Figure 2: Module stacking in FLoRA is a noise-free aggregation for LoRA, while the module averaging in FedIT cannot accurately aggregate the local updates.

different clients. This intermediate term introduces unexpected noise in the model aggregation. As the number of clients increases, this noisy term becomes much larger than the real global updates, significantly slowing down the fine-tuning progress. In addition, FedIT applies the scaling factor \(p_{k}\) to both \(_{k}\) and \(_{k}\), resulting in a \(p_{k}^{2}\) coefficient for the local model update \(_{k}\), exacerbating the error in LoRA aggregation. As Figure 2 illustrates, the averaging algorithm in FedIT is an inaccurate aggregation method, leading to slower convergence and higher computation costs.

The other deficiency of FedIT is that it cannot support aggregation on heterogeneous LoRA modules. The local data in FL may exhibit significant heterogeneity across clients [31; 12]. If a client configures a higher rank than the actual one required by the local data complexity, this may result in overfitting. Conversely, if the rank is too small, it may lack the necessary generalization capacity to effectively learn from the local dataset (Figure 4). Moreover, the heterogeneous computational resource across clients also requires heterogeneous rank deployment, _e.g.,_ clients with smaller memory can only afford to train LoRA modules with smaller ranks. AdaLoRA  has been proposed to adapt LoRA ranks based on available computation resources. Therefore, deploying heterogeneous ranks across clients is a pressing requirement for accommodation to data and system heterogeneity. However, according to Equation 3, FedIT is only able to aggregate LoRA modules with the homogeneous rank.

## 3 Proposed Method: FLoRA

### Stacking-based Noise-free Aggregation

Motivated by the aforementioned problem, we propose a novel aggregation mechanism that accurately computes global model update \(\) by aggregating local LoRA modules and effectively supports the heterogeneous LoRA. According to matrix multiplication principles and the model update rule in LoRA (_i.e.,_ Equation 1), the element at position \((x,y)\) of the model update \(\) is computed as the sum of the products of corresponding elements from the \(x\)-th column of \(\) and the \(y\)-th row of \(\):

\[_{xy}=_{i=0}^{r}a_{yi}b_{xi},\] (7)

where \(_{xy}\) represents the element at position \((x,y)\) in \(\). \(a_{yi},b_{xi}\) are the elements at positions \((y,i)\) and \((x,i)\) in \(\) and \(\), respectively. According to Equation 3.1, the model update in LoRA can be expressed as the sum of the products of the corresponding rows of \(\) and the columns of \(\).

To illustrate this concept further, let us consider a simplified example where the dimensions of LoRA modules are given by \(^{2 3}\) and \(^{3 2}\). As described in Equation 8, \(\) and \(\) can be decomposed to two sub-matrices with rank \(r=1\), and the product of \(\) and \(\) then are computed as the sum of the products of two respective sub-matrices:

\[=b_{00},b_{01}\\ b_{10},b_{11}\\ b_{20},b_{21}a_{00},a_{10},a_{20}\\ a_{01},a_{11},a_{21}=b_{00}\\ b_{10}\\ b_{20}[a_{00},a_{10},a_{20}]+b_{01}\\ b_{11}\\ b_{21}[a_{01},a_{11},a_{21}]\] (8)

To address the aggregation challenge from an alternative perspective, let us consider the scenario where we have multiple pairs of LoRA modules, \(_{k}\), \(_{k}\), optimized by the clients. Each pair satisfies the dimensions \(_{k}^{r_{k} n}\) and \(_{k}^{m r_{k}}\). Similar to Equation 8, the sum of the products of these module pairs is the product of the stacked modules, _i.e.,_\(_{k=1}^{K}_{k}_{k}=\), where \(\) represents the _stacking_ of all \(_{k}\) modules aligned through dimension \(m\) and \(\) is the _stacking_ of all \(_{k}\) aligned through dimension \(n\). Figure 2 visually illustrates this concept, where the orange, green, and blue rectangles symbolize \(_{k}\), \(_{k}\), and their respective products. The aggregation of three products mirrors the product of the stacked \(\) and \(\) from all \(_{k}\) and \(_{k}\) pairs trained by clients. This mechanism demonstrates that, in the context of federated fine-tuning, we can achieve a noise-free aggregation of local updates by simply stacking the local LoRA modules. This process also avoids transmitting the full model parameters, thus reducing communication costs.

To facilitate our discussion, we introduce the stacking operation symbolized by "\(\)" to denote the module aggregation as depicted in Figure 2. This operation is mathematically defined as:

\[=_{0}_{1}_{2},\ =_{0}_{1}_{2},\] (9)In Equation 9, "\(\)" indicates that for \(\), each subsequent module is vertically stacked below the preceding one, whereas for \(\), each module is horizontally stacked to the right of the one before it.

We can now formalize our conclusion regarding the aggregation of LoRA modules. The sum of the products of \(K\) LoRA module pairs is equivalent to the product of their stacked matrices:

\[_{k=0}^{K}_{k}_{k}=(_{0}... _{K})(_{0}..._{K})\] (10)

This foundational principle will guide the design of FLoRA, as it allows for the efficient and effective aggregation of local updates without the transmission of entire model parameters.

### FLoRA: Stacking-based Federated Fine-tuning for Heterogeneous LoRA

The stacking-based aggregation facilitates not only the accurate aggregation of LoRA modules but also inherently supports the heterogeneous LoRA ranks. This approach imposes no constraints on the ranks of each local LoRA module as long as each client fine-tunes the same pre-trained model, _i.e.,_ they share the same dimension \(m\) and \(n\).

By employing the stacking-based aggregation mechanism, we introduce FLoRA, an approach designed to facilitate federated fine-tuning of LLMs with heterogeneous LoRA. Let us use a concrete example to illustrate the key steps of applying FLoRA, where \(K\) heterogeneous clients are involved in fine-tuning an LLM, and the pre-trained parameters are denoted by \(}\).

Initialization.The server first disseminates the pre-trained model parameters \(\) to all \(K\) clients. Then, the clients initialize their local LoRA modules based on the complexity of local data and available local resources. The adaptation of LoRA ranks is beyond the scope of this paper, but existing work like AdaLoRA  can facilitate the rank adjustment.

Local Fine-tuning.Following initialization, the clients train their local LoRA modules with the local data for several iterations. Then, the clients send the local LoRA modules back to the server. Note that the clients initialize local LoRA modules _each_ round before local fine-tuning.

Stacking-based LoRA Aggregation.Upon receiving the heterogeneous LoRA modules from participating clients, the server proceeds to aggregate them by stacking all \(_{k}\) and \(_{k}\) according to Equation 10, resulting in the global \(^{(_{k=0}^{K}r_{k}) n}\) and \(^{m(_{k=0}^{K}r_{k})}\). The aggregation

Figure 3: FLoRA workflow. The local LoRA modules are initialized and optimized each round, and stacked by the server to obtain the global LoRA modules. The global modules are then sent back to clients to update local models.

process of FLoRA can be described as follows:

\[&=p_{0}_{0} p_{1}_ {1}... p_{K}_{K},=_{0} _{1}_{2}..._{K}\\ &_{k}^{r_{k} n},_{k} ^{m r_{k}},^{(_{k=0}^{K}r_{k} ) n},^{m(_{k=0}^{K}r_{k})},\] (11)

where \(p_{k}\) represents the scaling factor for each local update, determined by the relative size of the local data to the global data:

\[p_{k}=)}{len(_{k=0}^{K}D_{k})}.\] (12)

Note that the scaling factor \(p_{k}\) should be only applied to one of \(_{k}\) and \(_{k}\) to avoid squaring the factor in the final model update \(\). This method ensures a noise-free aggregation mechanism as described in Equation 10.

Update Local Models.After each round of noise-free aggregation, the server redistributes the updated global LoRA modules \(\) and \(\) back to the clients. The clients then proceed to update the local models using \(\) and continue the fine-tuning. Using the stacking approach, the dimensions of updated global LoRA modules \(\) and \(\) are larger than those of FedIT, potentially leading to larger communication overhead in each round. However, empirical observations indicate that federated fine-tuning typically requires only a limited number of communication rounds to achieve satisfactory results, as detailed in Section 4. In addition, it is important to note that the LoRA modules \(\) and \(\) constitute a small fraction of the overall size of the pre-trained model, which is distributed to clients during the initialization phase. Thus, the additional communication overhead of the stacking approach is negligible and does not significantly impact the efficiency of federated fine-tuning.

## 4 Experiments

The key features of FLoRA are (i) noise-free aggregation and (ii) support for heterogeneous LoRA modules. In this section, we verify these key features across various LLM fine-tuning tasks. We first study the performance of FLoRA and compare it against FedIT under homogeneous settings to demonstrate the advantages of noise-free aggregation . Then, we examine performance in a synthetic heterogeneous setup and compare FLoRAwith a vanilla _zero-padding_ method. Finally, we conduct ablation studies on the scaling factor, the heterogeneity of LoRA ranks, and the extra communication overhead of FLoRA.

### Experiment Setup

**Models, Datasets and Experiment Settings.** We employ three Llama-based models with different scales in our experiments: TinyLlama with 1.1 billion parameters , and the 7 billion parameter versions of Llama  and Llama2 , evaluating FLoRA across different model capacities. Following the configurations in the original LoRA paper , the LoRA modules are applied to the self-attention layers only.

We use the Databricks-dolly-15k  instruction dataset, Alpaca dataset , and Wizard dataset  for the question-answering (QA) task, and Wizard and ShareGPT for the chat assistant task. We evaluate the federated fine-tuned models on MMLU  for the QA task and MT-bench  for the chat assistant task, respectively. We sample 10 clients uniformly at random following the non-IID setting in FedIT . The other experimental configurations are elaborated in Appendix A.

Baselines.We compare FLoRA with four baselines. (1) **FedIT:** It is the SOTA federated fine-tuning method  that integrates LoRA with FedAvg. We only apply FedIT to homogeneous LoRA experiments as it does not support heterogeneous LoRA. (2) **Zero-Padding:** It is an approach that enables FedIT to support heterogeneous LoRA . It extends all the heterogeneous local ranks to the maximum rank among the clients and pads their remaining parts by 0. (3) **Centralized Fine-tuning:** we compare FLoRA with centralized LoRA with the same hyperparameters and configurations. (4) **Standalone:** the client fine-tunes the pre-trained model locally without federations.

[MISSING_PAGE_FAIL:7]

not only accommodates heterogeneous LoRA ranks effectively but also sustains robust training performance compared to baseline methods. It efficiently facilitates the participation of devices with varied computational capacities in heterogeneous federated fine-tuning tasks. Additionally, FLoRA can be seamlessly integrated with AdaLoRA , which dynamically adjusts the LoRA rank on the clients, the results are presented in Appendix A.

The Impact of Scaling Factor.The scaling factor, denoted as \(p_{k}\) in Equation 12, plays a pivotal role in the efficacy of FL . We conduct experiments investigating how varying scaling factors influence the performance of FLoRA. Given that the default scaling factor is set to 0.1 for all clients, assuming 10 clients with equal local dataset sizes as per Equation 12, we explored the effects of alternative scaling factors, namely 0.01, 0.05, and 0.2. The results are summarized in Figure 5. The results do not reveal a clear pattern or optimal scaling factor for federated fine-tuning across different settings. The efficacy of a specific scaling factor appears to be contingent upon the dataset, task, and model in use. For example, when fine-tuning TinyLlama on the Dolly dataset, a lower scaling factor of 0.01 yields the highest accuracy, significantly outperforming the 0.1 and 0.2 scaling factors. Conversely, the model fine-tuned on Wizard dataset demonstrates a preference for a higher scaling factor of 0.2, achieving the best performance, whereas the lowest scaling factor of 0.01 was the least effective. In the case of the Llama model, larger scaling factors consistently facilitated better fine-tuning performance. Applying FLoRA to Dolly and Alpaca shows the optimal performance with a scaling factor of 0.2. These observations suggest that the choice of an appropriate scaling factor is highly dependent on specific datasets and model characteristics, underscoring the necessity for a tailored approach in federated fine-tuning.

The Impact of Heterogeneous LoRA Ranks.Although the above results demonstrate FLoRA effectively enables the federated fine-tuning with heterogeneous LoRA, it is worth further investigating how the federated fine-tuning improves the local models with various ranks. Motivated by this, we evaluate MT-bench scores for local models with LoRA ranks of 64, 32, 16, 8, and 4, presenting the results in Figure 4. Global model scores are shown in red bars, while local models are in blue, with deeper shades indicating higher ranks. The results show that the global model outperforms all local models, except for a case with the TinyLlama model fine-tuned on the Wizard dataset, where the client with rank 32 slightly exceeds the global model. This demonstrates FLoRA's ability to synthesize knowledge from diverse clients effectively.

Regarding the LoRA rank's impact, a rank of 8 consistently yields strong performance across various models and datasets. However, performance diverges at extreme ranks; for instance, the TinyLlama model fine-tuned on Wizzard with the LoRA rank of 64 underperforms the ones with smaller ranks, but the Llama model with the rank of 64 excels the counterparts with smaller ranks. This also demonstrates the heterogeneous rank deployment across clients is a realistic setting. These observations suggest a potential positive correlation between optimal LoRA rank and model capacity, motivating further exploration in future research.

## 5 Discussion

**The Communication Overhead of FLoRA.** As discussed in Section 3, the server needs to send global LoRA modules to the clients in FLoRA, potentially raising concerns about increased commu

Figure 5: The impact of the scaling factor on FLoRA. The x-axis is the scaling factor, and the y-axis represents the MMLU accuracy for (a)-(b) and the MT-bench score for (c)-(d). The results of Llama2 are in Appendix A.

nication overhead. To quantify this, we compare the communicated parameters of full fine-tuning, FedIT, and FLoRA over three communication rounds.

As Figure 6 shows, although FLoRA transmits slightly more parameters than FedIT, it still significantly reduces the overhead compared to full fine-tuning. This is due to the fact that the primary communication load in federated fine-tuning, especially with large models, is the initial full model parameter transmission. Subsequent rounds primarily involve smaller updates (e.g., LoRA matrices). Thus, even though FLoRA introduces additional communication for these updates, the overall impact on total communication costs remains marginal, making it comparable to FedIT's costs. Despite the minor communication increase compared to FedIT, FLoRA enhances fine-tuning effectiveness and supports heterogeneous LoRA ranks, making it a preferable solution in federated fine-tuning.

**The Privacy Preservation of FLoRA.** The requirement of FLoRA to stack the LoRA modules uploaded by all clients introduces a potential privacy concern, as malicious clients might infer the LoRA matrices of other clients through the global LoRA modules sent from the server. To address this issue, we split all the local LoRA modules into sub-modules with rank=1 and then stack the sub-modules together in random order. This approach prevents malicious clients from recovering the local LoRA modules from other clients. In addition, FLoRA is also compatible with standard privacy mechanisms such as encryption  and differential privacy (DP) , aligning it with the privacy-preserving nature of FL.

## 6 Related Work

**Parameter-efficient Fine-tuning of LLMs.** Parameter-efficient fine-tuning (PEFT) aims to reduce the number of trainable parameters. BitFit  fine-tunes only the biases while achieving similar accuracy with full fine-tuning. Other works such as  and  apply transfer learning that adds pre-trained adapter layers between transformer blocks. LoRA  adopts the product of two low-rank matrices to represent the gradient in full fine-tuning, which achieves memory-efficient fine-tuning. AdaLoRA  optimizes LoRA by adaptively allocating the parameter budget, which enhances the flexibility of LoRA. There are also many works regarding optimizing LoRA in various aspects [5; 6; 15].

**Federated Fine-tuning of LLMs.** Federated fine-tuning aims to extract knowledge from multiple on-device datasets while preserving data privacy. FedIT  leverages the FL framework for fine-tuning LLMs. It uses LoRA as the local fine-tuning strategy. However, concerns related to the deficiency in supporting heterogeneous LoRA limit its utilization.  tries to solve this problem by zero-padding the local LoRA modules. However, this padding process causes additional computing overhead. Besides, it separately averages \(\) and \(\) modules, introducing noise to the global model.

## 7 Conclusion

In this work, we identified the limitations in current federated fine-tuning methods (_e.g.,_ FedIT), and the challenges of applying federated fine-tuning in realistic settings, _i.e.,_ the heterogeneous LoRA ranks across clients. To overcome these practical challenges and broaden the applicability of federated fine-tuning, we introduced FLoRA to enable the accurate aggregation on heterogeneous LoRA modules using the proposed stack-based LoRA aggregation mechanism. Our extensive experiments demonstrate that FLoRA outperforms the SOTA method in both homogeneous and heterogeneous LoRA settings. Moreover, our inspiring results provide valuable insights for future research in the federated fine-tuning of large language models in a lightweight and accurate manner.

Figure 6: The ratio of communicated parameter numbers to full fine-tuning.