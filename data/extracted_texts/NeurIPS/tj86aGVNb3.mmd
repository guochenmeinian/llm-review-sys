# Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond

Taiji Suzuki\({}^{1,2}\), Denny Wu\({}^{3,4}\), Kazusato Oko\({}^{1,2}\), Atsushi Nitanda\({}^{2,5}\)

\({}^{1}\)University of Tokyo, \({}^{2}\)RIKEN AIP, \({}^{3}\)New York University,

\({}^{4}\)Flatiron Institute, \({}^{5}\)Kyushu Institute of Technology

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu,

oko-kazusato@g.ecc.u-tokyo.ac.jp, nitanda@ai.kyutech.ac.jp

###### Abstract

Neural network in the mean-field regime is known to be capable of _feature learning_, unlike the kernel (NTK) counterpart. Recent works have shown that mean-field neural networks can be globally optimized by a noisy gradient descent update termed the _mean-field Langevin dynamics_ (MFLD). However, all existing guarantees for MFLD only considered the _optimization_ efficiency, and it is unclear if this algorithm leads to improved _generalization_ performance and sample complexity due to the presence of feature learning. To fill this important gap, in this work we study the sample complexity of MFLD in learning a class of binary classification problems. Unlike existing margin bounds for neural networks, we avoid the typical norm control by utilizing the perspective that MFLD optimizes the _distribution_ of parameters rather than the parameter itself; this leads to an improved analysis of the sample complexity and convergence rate. We apply our general framework to the learning of \(k\)-sparse parity functions, where we prove that unlike kernel methods, two-layer neural networks optimized by MFLD achieves a sample complexity where the degree \(k\) is "decoupled" from the exponent in the dimension dependence.

## 1 Introduction

Mean-field Langevin dynamics.The optimization dynamics of two-layer neural networks in the mean-field regime can be described by a nonlinear partial differential equation of the distribution of parameters (Nitanda and Suzuki, 2017; Chizat and Bach, 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020). Such a description has multiple advantages: \((i)\) global convergence guarantees can be obtained by exploiting convexity of the loss function, and \((ii)\) the parameters are allowed to evolve away from initialization and learn informative features, in contrast to the _neural tangent kernel_ ("lazy") regime (Jacot et al., 2018).

Among the gradient-based optimization algorithms for mean-field neural networks, the _mean-field Langevin dynamics_ (MFLD) (Mei et al., 2018; Hu et al., 2019) is particularly attractive due to the recently established _quantitative_ optimization guarantees. MFLD arises from a noisy gradient descent update on the parameters, where Gaussian noise is injected to the gradient to encourage "exploration". It has been shown that MFLD globally optimizes an _entropy-regularized convex functional_ in the space of measures, and for the infinite-width and continuous-time dynamics, the convergence rate is exponential under suitable isoperimetric conditions (Nitanda et al., 2022; Chizat, 2022). Furthermore, uniform-in-time estimates of the particle discretization error have also been established (Suzuki et al., 2023; Chen et al., 2022), meaning that optimization guarantees for the infinite-dimensional problem can be effectively translated to a finite-width neural network.

However, existing analyses of MFLD only considered the _optimization_ of neural networks; this alone does not demonstrate the benefit of mean-field regime nor the presence of feature learning. Therefore,an important problem is to characterize the _generalization_ of the learned models, and prove efficient sample complexity guarantees. The goal of this work is to address the following question.

_Can we show that neural network + simple noisy gradient descent (MFLD) efficiently learns an interesting class of functions, with a better rate of convergence compared to the "lazy" regime?_

Learning sparse parity functions.One particularly relevant learning task is the \(k\)-sparse parity problem, where the response \(y\) is given by the sign of the product of \(k\) coordinates of the input (on hypercube); as a special case, setting \(k=2\) recovers the classical XOR problem. When \(k d\), this target function is low-dimensional, and hence we expect feature learning to be beneficial in that it can "zoom in" to relevant subspace. In contrast, for kernel methods (including neural networks in the lazy regime) which cannot adapt to such structure, it has been shown that a sample complexity of \(n=(d^{k})\) is unavoidable (Ghorbani et al., 2019; Hsu et al., 2022).

For the XOR case (\(k=2\)), recent works have shown that neural networks in the mean-field regime can achieve a sample complexity of \(n=(d/)\)(Wei et al., 2019; Chizat and Bach, 2020; Telgarsky, 2023), which indeed improves upon the NTK complexity (Ji and Telgarsky, 2019). However, all these results directly assumed convergence of the dynamics (\(t\)) with no iteration complexity. Moreover, Wei et al. (2019); Chizat and Bach (2020) directly analyzed the infinite-width limit, and while Telgarsky (2023) provided a finite-width characterization, the dynamics is restricted to the low-rotation regime, and a very large number of particles \(N=(d^{d})\) is required. Lastly, these analyses are specialized to XOR, and do not directly generalize to the \(k\)-parity setting.

### Our Contributions

In this work, we bridge the aforementioned gap by presenting a simple and general framework to establish sample complexity of MFLD in learning binary classification problems. We then apply this framework to the sparse \(k\)-parity problem, and obtain improved rate of convergence for the fully time- and space-discretized algorithm. More specifically, our contributions can be summarized as follows.

* We present a general framework to analyze MFLD in the learning of binary classification tasks. Our framework has two main ingredients: \((i)\) an annealing procedure that applies to common classification losses that removes the exponential dependence on regularization parameters in the _logarithmic Sobolev inequality_, and \((ii)\) a novel local Rademacher complexity analysis for the distribution of parameters optimized by MFLD. As a result, we can obtain generalization guarantee for the learned neural network in discrete-time and finite-width settings.
* We apply our general framework to the \(k\)-sparse parity problem, and derived learning guarantees with improved rate of convergence and dimension dependence, as shown in Table 1. Specially, in the \(n d^{2}\) regime we obtain exponentially converging classification error, whereas in the \(n d\) regime we achieve linear dimension dependence. Note that this improves upon the NTK analysis (which gives a sample complexity of \(n=(d^{k})\)) in that it "decouple" the degree \(k\) from the exponent in the dimension dependence. Our theoretical results are supported by empirical findings.

### Additional Related Works

In addition to the mean-field analysis, parity-like functions can be learned via other feature learning procedures such as the one gradient step analysis in Daniely and Malach (2020); Ba et al. (2022);

   Authors & regime/method & \(k\)-parity & class error & width & \# iterations \\  Ji and Telgarsky (2019) & NTK/SGD & No & \(d^{2}/n\) & \(d^{8}\) & \(d^{2}/\) \\  Telgarsky (2023) & NTK/SGD & No & \(d^{2}/n\) & \(d^{2}\) & \(d^{2}/\) \\  Barak et al. (2022) & Two phase SGD & Yes & \(d^{(k+1)/2}/\) & \(O(1)\) & \(d/^{2}\) \\  Wei et al. (2019) & mean-field/GF & No & \(d/n\) & \(\) & \(\) \\  Telgarsky (2023) & mean-field/GF & No & \(d/n\) & \(d^{d}\) & \(\) \\  Ours & mean-field/MFLD & Yes & \((-O(/d))\) & \(e^{O(d)}\) & \(e^{O(d)}\) \\  Ours & mean-field/MFLD & Yes & \(d/n\) & \(e^{O(d)}\) & \(e^{O(d)}\) \\   

Table 1: Statistical and computational complexity (omitting poly-log terms) for the \(k\)-sparse (or 2-sparse) parity problem. Column “\(k\)-parity” indicates applicability to the general \(k\)-sparse parity setting, and for references that does not handle \(k\)-parity we state the complexity for 2-parity (XOR). \(d\) is the input dimensionality, and \(n\) is the sample size. For SGD, \(n\) is the total sample size given by product of mini-batch size and iterations.

Damian et al. (2022); Barak et al. (2022). Such analysis requires nontrivial gradient concentration at initialization, which translates to a sample complexity that scales as \(n=(d^{k})\)Barak et al. (2022). For "narrow" neural networks, Abbe et al. (2023) showed that a modified projected (online) gradient descent algorithm can learn the \(k\)-parity problem with a sample complexity of \(n=O(d^{k-1}+^{-c})\) (for some unknown \(c\)). Also, Refinetti et al. (2021); Ben Arous et al. (2022) showed that a two-layer ReLU network with more than \(4\) neurons can learn the Gaussian XOR problem with gradient descent.

## 2 Problem Setting

Throughout this paper, we consider a classification problem given by the following model:

\[Y=_{A}(Z)-_{A^{c}}(Z)\{ 1\}\]

where \(Z=(Z_{1},,Z_{d})\) is the input random variable on \(^{d}\) and \(_{A}\) is the indicator function corresponding to a measurable set \(A(^{d})\), i.e., \(_{A}(Z)=1\) if \(Z A\) and \(_{A}(Z)=0\) if \(Z A\). Let \(P_{Z}\) be the distribution of \(Z\). We are given input-output pairs \(D_{n}=(z_{i},y_{i})_{i=1}^{n}\) independently identically distributed from this model as training data. Then, we construct a binary classifier that predicts the label for the test input data as accurate as possible. To achieve this, we learn a two-layer neural network model in the mean-field regime via the _mean-field Langevin dynamics_.

One important problem setting for our analysis is the \(k\)-sparse parity problem defined as follows.

**Example 1** (\(k\)-sparse parity problem).: \(P_{Z}\) _is the uniform distribution on the grid \(\{ 1/\}^{d}\) and \(A=\{=(_{1},,_{d})\{ 1/\}^{d}_{1} _{k}>0\}\)1._

As a special case, \(k=2\) (XOR) has been extensively studied (Wei et al., 2019; Telgarsky, 2023).

Mean-field two-layer network.Given input \(z\), let \(h_{x}(z)\) be one neuron in a two-layer neural network with parameter \(x=(x_{1},x_{2},x_{3})^{d+1+1}\) defined as

\[h_{x}(z)=[(z^{}x_{1}+x_{2})+2(x_{3})]/3,\]

where \(\) is a hyper-parameter determining the scale of the network2. We place an extra \(\) activation for the bias term \(x_{3}\) because the boundedness of \(h_{x}\) is required in the convergence analysis. Let \(\) be the set of probability measures on \((^{},(^{}))\) where \(=d+2\) and \((^{})\) is the Borel \(\)-algebra on \(^{d}\) and \(_{p}\) be the subset of \(\) such that its \(p\)-th moment is bounded: \(_{}[\|X\|^{p}]<\ ()\). The mean-field neural network is defined as an integral over neurons \(h_{x}\),

\[f_{}()= h_{x}()(x),\]

for \(\). To evaluate the performance of \(f_{}\), we define the empirical risk and the population risk as

\[L():=_{i=1}^{n}(y_{i}f_{}(z_{i})),\ \ ():=[(Yf_{}(Z))],\]

respectively, where \(:_{ 0}\) is a convex loss function. In particular, we consider the logistic loss \((f,y)=(1+(-yf))\) for \(y\{ 1\}\) and \(f\). To avoid overfitting, we consider a regularized empirical risk \(F():=L()+_{X}[_{1}\|X\|^{2}]\), where \(,_{1} 0\) are regularization parameters. One advantage of this mean-field definition is that \(f_{}\) is a linear with respect to \(\), and hence the functional \(L()\) becomes a convex functional.

Mean-field Langevin dynamics.We optimize the training objective via MFLD, which is given by the following stochastic differential equation:

\[X_{t}=-)}{}(X_{t})t+ W_{t},_{t}=(X_{t}),\] (1)

where \(X_{0}_{0}\), \((X)\) denotes the distribution of the random variable \(X\) and \((W_{t})_{t 0}\) is the \(d\)-dimensional standard Brownian motion. Readers may refer to Theorem 3.3 of Huang et al. (2021) for the existence and uniqueness of the solution. Here, \()}{}\) is the first variation of \(F\).

**Definition 1**.: _For a functional \(G:\), the first-variation \(()\) at \(\) is a continuous functional \(^{d}\) satisfying \(_{ 0}=()(x)(-)\) for any \(\)._In our setting, we have \((x)=_{i=1}^{n}^{}(y_{i}f_{ }(z_{i}))y_{i}h_{x}(z_{i})+(_{1}\|x\|^{2})\). It is known that the Fokker-Planck equation of the SDE (1) is given by3

\[_{t}_{t}=_{t}+[_{t})}{}]=[_{t}( (_{t})+)}{})].\] (2)

Then, we can verify that this is equivalent to the Wasserstein gradient flow to optimize the following entropy regularized risk (Mei et al., 2018; Hu et al., 2019):

\[()=F()+()=L()+( ,)+()\] (3)

where \((,)=(/)\) is the KL divergence between \(\) and \(\), and \(\) is the Gaussian distribution with mean \(0\) and variance \(I/(2_{1})\), i.e., \(=(0,I/(2_{1}))\).

For a practical algorithm, we need to consider a space- and time-discretized version of the MFLD, that is, we approximate the solution \(_{t}\) by an empirical measure \(_{}=_{i=1}^{N}_{X_{i}}\) corresponding to a set of finite particles \(=(X^{i})_{i=1}^{N}^{d}\). Let \(_{}=(X^{i})_{j=1}^{N}^{d}\) be \(N\) particles at the \(\)-th update (\(\{0,1,2,\}\)), and define \(_{}=_{_{}}\) as a finite particle approximation of the population counterpart. Then, the discretized MFLD is defined as follows: \(X^{i}_{0}_{0}\), and \(_{}\) is updated as

\[X^{i}_{+1}=X^{i}_{}-)}{}( X^{i}_{})+^{i}_{},\] (4)

where \(>0\) is the step size, and \(^{i}_{}_{i.i.d.}N(0,I)\). This is the Euler-Maruyama approximation of the MFLD with a discretized measure; we present the discretization error bounds in the next section.

## 3 Main Assumptions and Theoretical Tools

In this section, we introduce the basic assumptions and technical tools for our analysis.

Condition on the loss functionTo derive the convergence of the classification error, we assume that the loss function satisfies the following condition.

**Assumption 1**.: _The convex loss function \(:_{ 0}\) satisfies the following conditions:_

* \(\) _is first order differentiable, its derivative is Lipschitz continuous and its derivative is bounded by_ \(I\)_:_ \(|^{}(x)-^{}(x^{})| C|x-x^{}|\) _and_ \(_{x}|^{}(x)| 1\)_._
* \(\) _is monotonically decreasing, and is classification calibrated:_ \(^{}(0)<0\)__(_Bartlett et al._,_ 2006_)__._
* \((u)^{-1}:=(0)-((u)-u^{}(u))>0\) _for any_ \(u>0\)_._

This standard assumption is satisfied by several loss functions such as the logistic loss. We remark that the first assumption is used to show the well-definedness of the mean-field Langevin dynamics and derive its discretization error, and also to obtain a uniform generalization error bound through the classical contraction argument (Boucheron et al., 2013; Ledoux and Talagrand, 1991). The second and third assumptions are used to show the convergence of classification error of our estimator.

Logarithmic Sobolev inequality.Nitanda et al. (2022); Chizat (2022) showed that the convergence of MFLD crucially relies on properties of the _proximal Gibbs distribution_ whose density is given by

\[p_{}(X)(-(X) ),\]

where \(\). By the smoothness of the loss function (Assumption 1) and the \(\) activation, we can show that the objective \(\) has a unique solution \(^{*}\) which is also a proximal Gibbs measure of itself.

**Proposition 1** (Proposition 2.5 of Hu et al. (2019)).: _The functional \(\) has a unique minimizer in \(_{2}\) that is absolutely continuous with respect to the Lebesgue measure. Moreover, \(^{*}_{2}\) is the optimal solution if and only if \(^{*}\) is absolutely continuous and its density function is given by \(p_{^{*}}\)._

The next question is how fast the solution \(_{t}\) converges to the optimal solution \(^{*}\). As we will see, the convergence of MFLD heavily depends on a _logarithmic Sobolev inequality_ (LSI) on \(p_{}\).

**Definition 2** (Logarithmic Sobolev inequality).: _Let \(\) be a probability measure on \((^{d},(^{d}))\). \(\) satisfies the LSI with constant \(>0\) if for any smooth function \(:^{d}\) with \(_{}[^{2}]<\),_

\[_{}[^{2}(^{2})]-_{}[^{2}]( _{}[^{2}])_{}[\| \|_{2}^{2}].\]

This is equivalent to the condition that the KL divergence from \(\) is bounded by the Fisher divergence: \((/)\| (/)\|^{2}\), for any \(\) which is absolutely continuous with respect to \(\). The LSI of proximal Gibbs measure can be established via standard perturbation criteria. For \(L()\) with bounded first-variation, we may apply the classical Bakry-Emery and Holley-Stroock arguments (Bakry and Emery, 1985; Holley and Stroock, 1987) (Corollary 5.7.2 and 5.1.7 of Bakry et al. (2014)): If \(\|\|_{} B\) is satisfied for any \(_{2}\), then \(^{*}\) and \(p_{}\) satisfy the LSI with

\[_{1}(-4B/).\] (5)

With the LSI condition on the proximal Gibbs distribution, it is known that the MFLD converges to the optimal solution in an exponential order by using a so-called _Entropy sandwich_ technique.

**Proposition 2** (Entropy sandwich (Nitanda et al., 2022; Chizat, 2022)).: _Suppose that \(_{0}\) satisfies \((_{0})<\) and the proximal Gibbs measure \(p_{_{t}}\) corresponding to the solution \(_{t}\) has the LSI constant \(\) for all \(t 0\), then the solution \(_{t}\) of MFLD satisfies_

\[(^{*},_{t})(_{t})-(^{ *})(-2 t)((_{0})-(^{*})),\]

_where \(^{*}=*{argmin}_{}()\) (the existence and uniqueness of \(^{*}\) is guaranteed by Proposition 1)._

Hence we know that time horizon \(T=O((1/))\) is sufficient to achieve \(>0\) accuracy.

Convergence of the discretized algorithm.While Proposition 2 only established the convergence rate of the continuous dynamics, similar guarantee can be shown for the discretized setting. Let

\[^{N}(^{(N)})=N_{^{(N)}}[F(_{ })]+(^{(N)}),\]

where \(^{(N)}\) is a distribution of \(N\) particles \(=(X^{i})_{i=1}^{N}^{}\). Let \(_{}^{(N)}\) be the distribution of the particles \(_{}=(X^{i}_{})_{i=1}^{N}\) at the \(\)-th iteration. Suzuki et al. (2023b) showed that, if \( 1/4\) and \( 1/4\), then for \(^{2}:=[\|X^{i}_{0}\|^{2}]+} +}\,^{2}\!+\! d =O(d+^{-2})\) and \(_{}:=C_{1}^{2}(^{2}+),\) where \(=2+_{1}=O(1)\) and \(C_{1}=8(^{2}+_{1}^{2}+d)=O(d+^{-1})\),

\[[^{N}(_{}^{(N)})]\!-\!(^{ *})\!\!(-)([^{N}(_{}^{(N)})]}{N}-(^{*}))\!+\!^{2}C_{1}+^{2}+}{ N},\]

where \(C_{}\) is a constant depending on \(\). In particular, for a given \(>0\), the right hand side can be bounded by \(+}{ N}\) after \(T=O(^{2}C_{1}}{}+}}{}})( 1/)\) iterations with the step size \(=O^{2}C_{1}}{}+}}{}}^{-1}\). Furthermore, the convergence of the loss function can be connected to the convergence of the function value of the neural network as follows,

\[_{_{}_{k}^{(N)}}[_{z *{supp}(P_{Z})}(f_{_{_{}}}(z)-f_{^{*}}(z))^{2 }]\] \[^{2}}{}(^{N}( ^{(N)}_{})}{N}-(^{*}))+2_{_{ }(^{*})^{ N}}\![_{z*{supp}(P_{Z})} (_{i=1}^{N}h_{X^{i}_{i}}(z)- h_{x}(z)^{*}(z ))^{2}].\]

Here the second term in the right hand side can be bounded by \(^{2}}{N}[1+2(^{2}}{( _{1})^{2}}+}{_{1}})]\) via Lemma 2, if \(\|z\| 1\) for any \(z*{supp}(P_{Z})\) as in the \(k\)-sparse parity problem. Hence, by taking the number of particles as \(N=^{-2}[()^{-2}+(_{1})^{-2}+d/_{1}]\) and letting \(=^{2}\) with the choice of \(T\) and \(\) as described above, we have \(_{z*{supp}(P_{Z})}|f_{_{_{}}}(z)-f_{^{*}}( z)|=O_{p}()\).

Assumptions on the model specification.We restrict ourselves to the situation where a perfect classifier with margin \(c_{0}\) is included in our model class, stated in the following assumption.

**Assumption 2**.: _There exists \(c_{0}>0\) and \(R>0\) such that the following conditions are satisfied:_

* _For some_ \(\)_, there exists_ \(^{*}\) _such that_ \((,^{*}) R\) _and_ \(L(^{*})(0)-c_{0}\)_._
* _For any_ \(<c_{0}/R\)_, the regularized expected risk minimizer_ \(_{[]}:=\,L()+(,)\) _satisfies_ \(Yf_{_{[]}}(X) c_{0}\) _almost surely._

Importantly, we can apply the same general analysis for different classification problems, as long as Assumption 2 is verified. The advantage of this generality is that we do not need to tailor our convergence proof for individual learning problems. Note that the convergence rate of MFLD is strongly affected by the values of \(\) and \(R\); therefore, it is crucial to establish this condition using the smallest possible values of \(\) and \(R\), in order to obtain a tight bound of the classification error. As an illustrative example, we now show that the \(k\)-sparse parity estimation satisfies the above assumption.

Example: \(k\)-sparse parity estimation.In the \(k\)-sparse parity setting (Example 1), Assumption 2 is satisfied with constants specified in the following propositions. The proofs are given in Appendix A.

**Proposition 3** (\(k\)-sparse parity).: _Under Assumption 1 and for \(=k\), there exists \(^{*}\) such that_

\[(,^{*}) c_{1}k(k)^{2}d\;(=R),\]

_and \(L(^{*})(0)-c_{2}\), where \(c_{1},c_{2}>0\) are absolute constants._

**Proposition 4**.: _Under Assumption 1 and the settings of \(R\) and \(\) given in Proposition 3, if \(<c_{2}/(2R)\), then \(_{[]}\) satisfies_

\[\{(_{[]}),L(_{[]})\}(0)-c_{2}+ R <(0)-}{2},\]

_and \(f_{_{[]}}\) is a perfect classifier with margin \(c_{2}\), i.e., \(Yf_{_{[]}}(X)}{2}\)._

In other words, Assumption 2 is achieved with \(R=O(k(k)^{2}d)\), \(=k\) and \(c_{0}=c_{2}/2\). By substituting these values of \(R\) and \(\) to our general results presented below, we can easily derive a bound for the classification error of the MFLD estimator.

## 4 Main Result: Annealing Procedure and Classification Error Bound

### Annealing Procedure

The convergence rate of MFLD is heavily dependent on the LSI constant which may impose a large computational cost. We alleviate this dependency by employing a novel annealing scheme where we gradually decrease the regularization parameter \(\). In particular, at the \(\)-th round, we run the MFLD until (near) convergence with a regularization parameter \(^{()}=2^{-}^{(0)}\): \(_{0}=^{(-1)}\),

\[X_{t}=-[^{)}{}}(X_{t}) t+2^{()}_{1}X_{t}]+}W_{t},\] (6)

which corresponds to minimizing \(^{()}():=L()+^{()}(,)\). Then, we obtain a near optimal solution \(^{()}\) as \(^{()}(^{()})_{}^{()}()+^{*}\) for a given \(^{*}>0\). We terminate the procedure after \(K\) rounds and obtain \(^{(K)}\) as the output.

Suppose that there exists \(^{*}\) such that \((,^{*}) R\) and \(L(^{*})^{*}\). Then, as long as \(^{()}^{*}\) and \(^{*}<^{*}\), we have that

\[L(^{()})+^{()}(,^{()}) L(^ {*})+^{()}(,^{*})+^{*} 2^{*}+ ^{()}R(R+2)^{()}.\]

Since \(^{()}(_{t})\) is monotonically decreasing during the optimization, we always have \(L(_{t})^{()}(^{(-1)})^{( -1)}(^{(-1)})(R+2)^{(-1)}\). Now we utilize the following structure on common classification losses to ensure that \(\| L()/\|_{}\) is small when the loss \(L()\) is small.

**Assumption 3**.: _There exists \(c_{}>0\), such that, for any \(\), it holds that \(\|\|_{} c_{}L()\)._

For example, the logistic loss satisfies this assumption:

\[|_{u}(1+(-u))|=(1+(-u)).\]Hence, the condition holds for \(c_{}=1\) because \(\|\|_{}_{i=1}^{n}|^{ }(y_{i},f_{}(z_{i}))\|h_{z_{i}}()\|_{}_{ i=1}^{n}(y_{i},f_{}(z_{i}))=L()\). Under this assumption, the Holley-Strook argument yields that the log-Sobolev constant during the optimization can be bounded as

\[_{1}(-}(R+2)^{( -1)}}{^{()}})_{1}(-8c_{}(R+2)),\]

which is independent of \(^{()}\) and the final accuracy \(^{*}\). Hence, after \(K=_{2}(R/(^{(0)}^{*}))\) round, we achieve that \(L(^{(K)})^{*}+2^{*}\) where each round takes \(T_{}=(1/^{*})/(2)=O((1/^{*})( 8c_{}(R+2))/(_{1}^{(k)}))\) for the continuous time setting.

For the discrete setting, \(T_{}=O((}{^{*}}+} }{^{*}}})(1/^{*})( 8c_{}(R+2))/(_{1}^{()}))\) iterations in each round is sufficient (where \(C_{1}\) is given for \(^{(K)}\)), where the step size is \(=O(C_{1}^{-1}^{*} C_{1}^{-1/2}^{*}})\). As long as \(^{*}=O(^{()}/)\) for all \(\), the total iteration number can be simplified as \(O((1/^{*})(16c_{}(R+2))C_{1}/(^{ (K)}^{*}))\), and the width \(N\) (number of particles) can be taken as \(N=O([(^{(K)})^{-2}+(^{(K)})^{-2}+d]/(^{*})^{2})=O ([(16c_{}(R+2))/(^{(K)})^{2}+d]/(^{* })^{2})\).

**Remark 1**.: _We make the following remarks on the annealing procedure._

* _The main advantage of this annealing approach is that the exponential factor induced by the LSI constant_ \(\) _is not dependent on the choice of the regularization parameter_ \(^{(k)}\) _(note that the LSI is solely determined by the intermediate solution_ \(_{t}\)_). In contrast, the naive Holley-Stroock argument of Eq. (_5_) imposes exponential dependency on the regularization parameter._
* _Our annealed algorithm differs from the procedure considered in_ Chizat (_2022_, Section 4); importantly, we make use of the structure of classification loss functions to obtain a refined computational complexity analysis._

### Generalization Error Analysis

We utilize the _local Rademacher complexity_(Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006; Gine and Koltchinskii, 2006) to obtain a faster generalization error rate. For the function class of mean-field neural networks, we introduce \(:=\{f_{}\}\), and the KL-constrained model class \(_{M}(^{}):=\{f_{},\;( ^{},) M\}\) for \(^{}\) and \(M>0\). The Rademacher complexity of a function class \(}\) is defined as

\[(}):=_{_{i},z_{i}}[ _{f}}_{i=1}^{n}_{i}f(z_{i}) ],\]

where \((z_{i})_{i=1}^{n}\) are i.i.d. observations from \(P_{Z}\) and \((_{i})_{i=1}^{n}\) is an i.i.d. Rademacher sequence (\(P(_{i}=1)=P(_{i}=-1)=1/2\))). We have the following bound on the Rademacher complexity of the function class \(_{M}(^{})\).

**Lemma 1** (Local Rademacher complexity of \(_{M}(^{})\), Chen et al. (2020) adapted).: _For any fixed \(^{}\) and \(M>0\), it holds that \((_{M}(^{})) 2}\)._

The proof is given in Appendix B.2 in the supplementary material. Combining this local Rademacher complexity bound with the _peeling device_ argument (van de Geer, 2000), we can roughly obtain the following estimate (note that this is an informal derivation):

\[()-(^{*})-(-^{*})(^{*})}{}}_{}+( ^{*},)}_{}(^{*},)}{n}}_{}(^{*},)}{n} }+(^{*},),\] (7)

with high probability, where \(=*{argmin}_{}()\), \(^{*}=*{argmin}_{}()+(,)\), \(R\) and \(\) are regarded as constants, and the last inequality is by the AM-GM relation. Observe that on the left hand side, we have two non-negative terms (I) and (II). Corresponding to each term, we obtain different types of classification error bounds (Type I and Type II in the following subsection, respectively). Note that there appears a \(O(1/n)\) factor in the right hand side, which cannot be obtained by a vanilla Rademacher complexity evaluation because it only yields an \(O(1/)\) bound. In other words, localization is essential to obtain our fast convergence rate.

We remark that a local Rademacher complexity technique is also utilized by Telgarsky (2023) to derive a \(O(1/n)\) rate. They adapted a technique developed for a smooth loss function by Srebro et al. (2010), which requires the training loss \(L()\) to be sufficiently small, that is, of order \(L()=O(1/n)\). In our setting, to achieve such a small training loss, we need to take large \(\) such as \(=((n))\). Unfortunately, such a large \(\) induces exponentially small log-Sobolev constant like \((-cd(n))=n^{-cd}\). In contrast, our analysis focuses on the local Rademacher complexity around \(^{*}\), and hence we do not require the training loss to be close to 0; instead, it suffices to have a training loss that is close to or smaller than that of \(^{*}\).

#### 4.2.1 Type I: Perfect Classification with Exponentially Decaying Error

In the regime of \(n=(1/^{(K)2})\), we can prove that the MFLD estimator attains a perfect classification with an exponentially converging probability by evaluating the term (I). From Eq. (7) we can establish \((^{*},) O_{p}(1/(n^{(K)2}))\); this KL divergence bound can be used to control the \(L^{}\)-norm between \(f_{}\) and \(f_{^{*}}\). Indeed, we can show that \(\|f_{}-f_{^{*}}\|_{}^{2} 2^{2}(^{*},)\) (see the proof of Theorem 1). Then, under the margin assumption of \(f_{^{*}}\) (Assumption 2), we have that \(f_{}\) also yields a Bayes optimal classifier. More precisely, we have the following theorem.

**Theorem 1**.: _Suppose Assumptions 1 and 2 hold. Let \(M_{0}=(^{*}+2(+1))/^{(K)}\). Moreover, suppose that \(^{(K)}<c_{0}/R\) and_

\[Q:=c_{0}^{2}-^{2}}{n^{(K)2}}[^{(K)}(4 +}{32^{2}n})+8^{2}(4+_{ 2}(8n^{2}M_{0}))+n^{(K)}^{*}]>0,\]

_then \(f_{}\) yields perfect classification, i.e., \(P(Yf_{}(Z)>0)=1\), with probability \(1-(-}{32^{4}}Q)\)._

The proof is given in Appendix C.1 in the supplementary material. From Proposition 3 we see that the requirement \(n 1/^{(K)2}\) implies that in the \(n d^{2}\) regime, Theorem 1 gives a perfect classification guarantee with a failure probability decaying exponentially fast.

#### 4.2.2 Type II: Polynomial Order Classification Error

Next we evaluate the classification error bound from term (II) in Eq. (7). In this case, we do not require an \(L^{}\)-norm bound as in the Type I analysis above; this results in a milder dependency on \(^{(K)}\) and hence a better sample complexity.

**Theorem 2**.: _Suppose Assumptions 1 and 2 hold. Let \(^{(K)}<c_{0}/R\) and \(M_{0}=(^{*}+2(+1))/^{(K)}\). Then, with probability \(1-(-t)\), the classification error of \(f_{^{(K)}}\) is bounded as_

\[P(Yf_{^{(K)}}(Z) 0) 2(c_{0})[^{2}}{n^{( K)}}(4+t+_{2}(8n^{2}M_{0}))+(4+ }{32^{2}n})+^{*}].\]

The proof is given in Appendix C.2 in the supplementary material. We notice that the right hand side scales with \(O(1/(n^{(K)}))\), which is better than \(O(1/(n^{(K)2}))\) in Theorem 1; this implies that a sample size linear in the dimensionality is sufficient to achieve small classification error. The reason for such improvement in the \(^{K}\)-dependence is that the stronger \(L^{}\)-norm convergence is not used in the proof; instead, only the convergence of the loss is utilized. On the other hand, this analysis does not guarantee a perfect classification.

#### 4.2.3 Computational Complexity of MFLD

From the general result in Section 4.1, we can evaluate the computational complexity to achieve the statistical bounds derived above. In both cases (Theorems 1 and 2), we may set the optimization error \(^{*}=O(1/(n^{(K)}))\). Then, the total number of iteration can be

\[_{=1}^{K}T_{} O((d+^{(K)-1})n(16c _{}(R+2))(n^{(K)})).\]

The width \(N\) (the number of particles) can be taken as \(N=O((^{*}^{(K)})^{-2})=O(n^{2}^{-2})=O(n^{2} (16c_{}(R+2)))\).

**Corollary 1** (\(k\)-sparse parity setting).: _In the \(k\)-sparse parity setting, we may take \(R=O(k(k)^{2}d)\), \(=k\) and \(^{(K)}=O(1/R)=O(1/(k(k)^{2}d))\). Therefore, the classification error is bounded by_

\[P(Yf_{^{(K)}}<0) O((k)^{2}d}{n}((1/)+ (n))),\]

_with probability \(1-\). Moreover, if \(n=(k^{6}(k)^{4}d^{2})\), then \(P(Yf_{^{(K)}}>0)=1\) with probability_

\[1-(-(nk^{6}(k)^{4}/d^{2})).\]

_As for the computational complexity, we require \(O(k(k)^{2}dn(nd)[O(k^{2}(k)^{2}d)])\) iterations, and the number of particles is \(O((O(k^{2}(k)^{2}d))))\)._

Comparison with prior results.In the 2-sparse parity setting, neural network in the kernel (NTK) regime achieves only \(O(d^{2}/n)\) convergence in the classification error, as shown in Ji and Telgarsky (2019) and Telgarsky (2023, Theorem 2.3), whereas we demonstrate that mean-field neural network can improve the rate to \(O(d/n)\) via feature learning. Telgarsky (2023) also analyzed the learning of 2-sparse parity problem beyond the kernel regime, and showed that 2-layer ReLU neural network can achieve the best known classification error \(O(d/n)\). However, their analysis considered a low-rotation dynamics and assumed convergence at \(t\), whereas our framework also provides a concrete estimate of the computational complexity. Indeed, the number of iterations can be bounded as \(O(dn(nd)[O(d)])\). In addition, while we still require exponential width \(N=O(n^{2}(O(d)))\), such a condition is an improvement over \(N=O(d^{d})\) in Telgarsky (2023). Barak et al. (2022) considered a learning method in which one-step gradient descent is performed for the purpose of feature learning, and then a network with randomly re-initialized bias units is used to fit the data. For the \(k\)-sparse parity problem, they derived a classification error bound of \(O(d^{(k+1)/2}/)\). In contrast, our analysis yields a much better statistical complexity of \(O(k^{2}(k)^{2}d/n(-(nk^{6}(k)^{4}/d^{2})))\), which "decouples" the degree \(k\) in the exponent of the dimension dependence.

## 5 Numerical Experiment

We validate our theoretical results by numerical experiment on synthetic data. Specifically, we consider the classification of \(2\)-sparse parity with varying dimensionality \(d\) and sample size \(n\).

Recall that the samples \(\{(z_{i},y_{i})\}_{i=1}^{n}\) are independently generated so that \(z_{i}\) follows the uniform distribution on \(\{ 1/\}^{d}\) and \(y_{i}=d_{i,1}_{i,2}\{ 1\}\) (\(z_{i}=(_{i,1},,_{i,d})\)). A finite-width approximation of the mean-field neural network \(_{j=1}^{N}h_{x_{j}}(z)\) is employed with the width \(N=2,000\). For each neuron of the network, all parameters are initialized to independently follow the standard normal distribution (meaning that the network is rotation invariant at initialization) and the scaling parameter \(\) is set to \(15\). We set \(d\) to take values \(5,10,,150\), and \(n=50,100,,2000\). We trained the network using noisy gradient descent with \(=0.2\), \(_{1}=0.1\), and \(=0.1/d\) (fixed during the whole training) until \(T=10,000\). The logistic loss is used for the training objective.

Figure 1 shows the average test accuracy over five trials. We make the following observations.

* The red line corresponds to the sample size \(n=(d^{2})\), above which we observe that almost-perfect classification is achieved. According to Section 4.2 (Type I), the classification error gets exponentially small with respect to \(n/d^{2}\), which predicts very small classification error above the line of \(n=(d^{2})\), which matches our experimental result.
* The boundary of test accuracy above \(50\%\) is almost linear, as indicated by the blue line. This matches the theoretical conditions (Type II) to obtain the polynomial order classification error in Section 4.2.

Figure 1: Test accuracy of two-layer neural network optimized by the MFLD to learn a \(d\)-dimensional 2-sparse parity (XOR) problem.

Conclusion and Discussion

We provided a general framework to evaluate the classification error of a two-layer neural network trained by the mean-field Langevin dynamics. Thanks to the generality of our framework, an error bound for specific settings can be derived by directly specifying the parameters in Assumption 2 such as \(R\), \(\), and \(c_{0}\). We also proposed an annealing procedure to alleviate the exponential dependencies in the LSI constant. As a special (but important) example, we investigated the \(k\)-sparse parity problem, for which we obtained more general and better sample complexity than existing works.

A limitation of our approach is that the required width and number of iterations are exponential with respect to the dimensionality \(d\); in contrast, certain tailored algorithms for learning low-dimensional target functions such as Abbe et al. (2023); Chen and Meka (2020) do not require such exponential computation. An important open problem is whether we can reduce the width and number of iterations to \((d)\) for the vanilla noisy gradient descent algorithm. Another interesting direction is to investigate the interplay between structured data and the efficiency of feature learning, as done in Ghorbani et al. (2020); Refinetti et al. (2021); Ba et al. (2023); Mousavi-Hosseini et al. (2023).