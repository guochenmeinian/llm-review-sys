# Diff-BBO: Diffusion-Based Inverse Modeling for Black-Box Optimization

Dongxia Wu

University of California San Diego

La Jolla, CA

dowu@ucsd.edu

&Nikki Lijing Kuang

University of California San Diego

La Jolla, CA

llkuang@ucsd.edu

&Ruijia Niu

University of California San Diego

La Jolla, CA

rniu@ucsd.edu

&Yi-An Ma

University of California San Diego

La Jolla, CA

yianma@ucsd.edu

&Rose Yu

University of California San Diego

La Jolla, CA

roseyu@ucsd.edu

###### Abstract

Black-box optimization (BBO) aims to optimize an objective function by iteratively querying a black-box oracle in a sample-efficient way. While prior studies focus on forward approaches to learn surrogates for the unknown objective function, they struggle with steering clear of out-of-distribution and invalid inputs. Recently, inverse modeling approaches that map objective space to the design space with conditional diffusion models have demonstrated impressive capability in learning the data manifold. They have shown promising performance in offline BBO tasks. However, these approaches require a pre-collected dataset. How to design the acquisition function for inverse modeling to _actively_ query new data remains an open question. In this work, we propose _diffusion-based inverse modeling for black-box optimization_ (Diff-BBO), an _inverse_ approach leveraging diffusion models for online BBO problem. Instead of proposing candidates in the design space, Diff-BBO employs a novel acquisition function _Uncertainty-aware Exploration_ (UaE) to propose objective function values. Subsequently, we employ a conditional diffusion model to generate samples based on these proposed values within the design space. We demonstrate that using UaE results in optimal optimization outcomes, supported by both theoretical and empirical evidence.

## 1 Introduction

Practical problems in science and engineering often involve optimizing a black-box objective function that is expensive to evaluate, such as robotics (Tesch et al., 2013) and molecular design (Sanchez-Lengeling and Aspuru-Guzik, 2018). How to achieve a near-optimal solution while minimizing function evaluations is thus a major challenge in black-box optimization (BBO). Prior works in BBO have largely focused on the online setting where a model can iteratively query the function during training (Turner et al., 2021; Zhang et al., 2021; Hebbal et al., 2019; Mockus, 1974). Most existing algorithms belong to forward methods, including Bayesian optimization (BO) (Kushner, 1964; Mockus, 1974; Wu et al., 2023; Frazier, 2018), bandit algorithms (Agrawal and Goyal, 2012;Karbasi et al., 2023), and conditional sampling approaches (Brookes et al., 2019; Gruver et al., 2024; Stanton et al., 2022). They build a surrogate model to approximate the black-box function and optimize sequentially.

However, these approaches may face difficulties in scenarios where valid inputs represent a small subspace, such as valid protein sequences or molecular structures. Such optimization problems become exceptionally challenging, as the optimizer must navigate and avoid out-of-distribution and invalid inputs (Kumar and Levine, 2020). Recently, a novel set of methods, termed _inverse approaches_, have been proposed to address this issue. These methods (Kumar and Levine, 2020; Krishnamoorthy et al., 2023; Kim et al., 2023; Fu and Levine, 2021) break the traditional paradigm by learning an inverse mapping from objective space back to the input (design) space. Leveraging the state-of-the-art generative models, such as diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2020), these approaches effectively capture data distributions in high-dimensional input space and facilitate optimization within the data manifold (Kong et al., 2024; Li et al., 2024; Kong et al., 2024). They achieve high performance in offline optimization settings (Kumar and Levine, 2020; Lu et al., 2023; Wang et al., 2018), assuming access to a fixed pre-collected dataset.

Despite these advancements, the online setting of inverse modeling, particularly how to capture the uncertainty of the inverse model and design an acquisition function for data-efficient querying, remains an open question. In this paper, we propose Diff-BBO, an inverse approach for online black-box optimization. Our approach consists of a novel acquisition function design through the _uncertainty quantification_ (UQ) of conditional diffusion model, which proposes the desired objective function values to strategically sample the design space and query the oracle function efficiently. We summarize our main contributions as follows:

* We present Diff-BBO, an inverse modeling approach for efficient online black-box optimization (BBO) leveraging uncertainty of conditional diffusion models.
* We design a novel acquisition function for BBO based on the uncertainty of conditional diffusion models. Theoretically, we prove that the balance between targeting higher objective values and minimizing epistemic uncertainty lead to optimal optimization outcomes.
* We demonstrate that Diff-BBO achieves state-of-the-art performance with superior sample efficiency on Design-Bench and molecular discovery task in the online BBO setting.

## 2 Methodology

Let \(f:\) denote the unknown ground-truth black-box function that evaluates the quality of any data point \(\), with \(^{d}\). Our goal is to find the optimal point \(^{*}\) that maximizes \(f\), \(^{*}*{argmax}_{}f()\). In the batch online BBO setting, we iteratively query \(f\) with batch size \(N\) and a fixed query iteration \(K\) and update the model based on observed outputs. At each iteration, the acquisition function guides the data selection of new query points by balancing exploration and exploitation. In Diff-BBO, we model the conditional distribution of \(p(|y,)\) with training data \(\). The function value \(y\) to condition on is proposed by an acquisition function, which quantifies the

Figure 1: Forward modeling vs inverse modeling for black-box optimization. (_Top_) Forward modeling approach using certain surrogate models (e.g., GPs) for forward modeling and acquisition functions (e.g., UCB, PI, and EI) to select \(\). (_Bottom_) Our inverse modeling approach using generative model (e.g., diffusion model) for inverse modeling and acquisition function (e.g., UAE) to select \(y\).

quality of the generated \(\). The objective of the above optimization becomes:

\[_{y_{k}}_{k=1}^{K}f(_{k}),\ \ _{k} p_{}( y_{k},),\ \ .\] (1)

To solve this optimization problem, we introduce Diff-BBO algorithm in Algorithm 1. At each iteration \(k\), we train a conditional diffusion model and compute the optimal \(y_{k}^{*}\) with the designed acquisition function. In practice, \(y\) is selected from a constructed candidate set \(\) based on the acquisition function scores \((y)\). The range of \(\) is determined by \(w_{k}\), where \(w\) is a positive scalar and \(_{k}\) is the maximum function values being queried in the current training dataset \(\). Conditioning on \(y_{k}^{*}\), we generate \(N\) samples \(\{_{j}\}_{j=1}^{N}\), where \(_{j} p_{}(|y_{k}^{*},)\). By querying the oracle to evaluate \(_{j}\), we obtain the best possible reconstructed value \(_{k}\) for the current iteration, and append all queried data pairs \(\{_{j},f(_{j})\}_{j=1}^{N}\) to the training dataset \(\). Figure 1 summarizes the difference between the prior forward BBO methods and our proposed inverse modeling approach.

### Acquisition Function Design

In this section, we analyze the uncertainty of Diff-BBO, decomposing its uncertainty into aleatoric and epistemic uncertainty. Then we propose an acquisition function called Uncertainty-aware Exploration (UaE) based on it. We prove that by achieving a balance between high objective values and low epistemic uncertainty, UaE provides a near-optimal solution to the online BBO problem.

Uncertainty Decomposition.We resort to the tools of Bayesian inference to solve the optimization problem defined in Equation (1). Given an observed value \(y\) of a sample \(\), the objective of Bayesian inference is to estimate the predictive distribution:

\[p( y,)=_{}[p_{}( y)]= _{}p_{}( y)p()d.\] (2)

By Equation (2), we recognize that the uncertainty arises from two sources: uncertainty in deciding parameter \(\) from its posterior \(p(|)\) and uncertainty in generating sample \(\) from a fixed diffusion model \(p_{}( y)\) after \(\) is chosen. We further provide a decomposition in terms of the aleatoric uncertainty and its epistemic counterpart.

To estimate the aleatoric uncertainty, we can Monte Carlo (MC) sample \(\) for \(N\) times from a learned likelihood function \(p_{}( y)\) for fixed \(y,\). To estimate the epistemic uncertainty, we use ensemble techniques. During the inference time, by initializing the trained ensemble models with different random seeds, we first sample \(M\) model parameters \(\{_{i}\}_{i=1}^{M}\) to simulate \(M\) conditional diffusion models. Then we generate \(N\) samples \(\{_{j}\}_{j=1}^{N}\) for each diffusion model with corresponding parameter \(_{i},\  i[M]\). Combining the above gives a practical way to decompose and estimate the two types of uncertainty, which is formally described in Proposition 1.

**Proposition 1** (Uncertainty Decomposition).: _At each iteration \(k[K]\), the overall uncertainty in inverse modeling can be split into aleatoric and epistemic components, measured empirically as:_

\[_{}(y,) =_{_{i} p()}[ {Var}_{_{i,j} p_{_{i}}( y)}(\|_{i,j}\| )],\ \  i[M],j[N];\] (3) \[_{}(y,) =_{_{i} p()} (_{_{i,j} p_{_{i}}( y)}[\| _{i,j}\|)],\ \  i[M],j[N].\]

Uncertainty-aware Exploration.At each iteration \(k[K]\) of Dif-BBO algorithm, the acquisition function proposes an optimal scalar value \(y_{k}^{*}\) as follows: \(y_{k}^{*}=_{y}(y,)\), which is used to generate \(\) in the design space using conditional difussion model.

Note that to design an effective acquisition function for inverse modeling, we need to achieve a balance between high objective values \(y\) and low epistemic uncertainty. On the one hand, it is advantageous to focus on the regions in \(\) whose corresponding \(y\) is of high values. As function evaluations are expensive to perform, we prefer to generate samples \(\) conditioned on higher \(y\), and only query the oracle for such promising samples to solve the black-box optimization task. On the other hand, we employ the epistemic uncertainty to gauge the error in the trained diffusion model. Specifically, it helps reduce the approximation error between \(y_{k}^{*}\) and the reconstructed function value \(_{j[N]}f(_{j})\), where \(f()\) is the black-box oracle, and \(_{j} p_{}(|y_{k}^{*},), j[N]\).

We introduce the _Uncertainty-aware Exploration_ (UaE) as our designed acquisition function:

\[(y,)=y-_{}(y,),\] (4)

which utilizes the uncertainty estimation on conditional diffusion model as in Proposition1. By balancing the exploration-exploitation trade-off, UaE effectively solve the online BBO problem.

Performance Analyses of UaE.We prove in Theorem1 that by adopting UaE for inverse modeling to guide the selection of generated samples for solving BBO problems, we can obtain a near-optimal solution for the optimization problem defined in Equation1. Detailed discussions and proofs can be found in AppendixC, AppendixD, and AppendixE.

**Theorem 1**.: _Let \(\) be the constructed candidate set at each iteration \(k[K]\) in Algorithm1. By adopting UaE as the acquisition function to guide the sample generation process in conditional diffusion model, Diff-BBO (Algorithm1) achieves a near-optimal solution for the online BBO problem defined in Equation1:_

\[_{y_{k}}_{k=1}^{K}f(_{k}),\ \ _{k} p_{ }( y_{k},),\ \ \ \ _{y_{k}}_{k=1}^{K}(y_{k}, ).\]

As a result, equipped with the novel design of UaE, Diff-BBO is a theoretically sound approach utilizing inverse modeling to effectievely solve the online BBO problem.

## 3 Experiments

To validate the efficacy of Diff-BBO, we conduct experiments on six real-world online black-box optimization tasks for both continuous and discrete optimization tasks.

Dataset.We restructured \(5\) real-world tasks from Design-Bench including \(3\) continuous and \(2\) discrete tasks. In **D'Kitty** and **Ant** Morphology, the goal is to optimize for the morphology of robots. In **Superconductor**, the aim is to optimize superconducting material with a high critical temperature. **TFBind8** and **TFBind10** are discrete tasks to find a DNA sequence with maximum affinity to bind with a specified transcription factor. We also include a **Molecular Discovery** task to optimize compound's activity against a biological target with therapeutic value. For each task, we arrange the offline dataset from Krishnamoorthy et al. (2023) in ascending order based on objective values and select data from the 25th to the 50th percentile as the initial training dataset. We prioritize data with lower objective scores to better observe performance differences across each baseline. Each optimization iteration is allocated \(100\) queries to the oracle function (batch size \(N=100\)), with a total of \(16\) iterations conducted. More details of the dataset are provided in AppendixF.1.

Baselines.We compare Diff-BBO with 10 baselines, including Bayesian optimization (BO), trust region BO (TuRBO) (Eriksson et al., 2019), local latent space Bayesian optimization (LOL-BO) (Maus et al., 2022), likelihood-free BO (LFBO) (Song et al., 2022), evolutionary algorithms (Brandle, 1980; Real et al., 2019), conditioning by adaptive sampling (CbAS) (Brookes et al., 2019), and random sampling. For BO approaches, we include Gaussian Processes (GP) with Monte Carlo (MC)-based batch expected improvement (EI), MC-based batch upper confidence bound (UCB) (Wilson et al., 2017), and joint entropy search (JES (Hvarfner et al., 2022) as the acquisition functions. For LFBO, we use EI and probability of improvement (PI) as the acquisition functions.

Results.Figure2 illustrates the performance across six datasets for all baselines and our proposed algorithm. Notably, Diff-BBO consistently outperforms other baselines in both discrete and continuous settings. Specifically, in the Ant and Dkitty tasks, Diff-BBO demonstrates a significant lead over all baseline methods, starting from the very first iteration of the online optimization process. This remarkable performance can be attributed to Diff-BBO's diffusion model-based inverse modeling approach, which effectively learns the data manifold in the design space from the initial dataset, even when the initial dataset lacks data with high objective function values.

## 4 Conclusion

In this paper, we introduced Diff-BBO, an inverse modeling approach for online black-box optimization that leverages the uncertainty of conditional diffusion models. By utilizing the novel acquisition function UaE, Diff-BBO strategically proposes objective function values to improve sample efficiency. We did extensive empirical evaluations to show the superior performance of Diff-BBO. Theoretically, we prove that using UaE leads to optimal optimization solutions.