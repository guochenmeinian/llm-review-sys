# Disentangled Representation Learning in Non-Markovian Causal Systems

Adam Li\({}^{*}\)

Yushu Pan\({}^{*}\)

Elias Bareinboim

Causal Artificial Intelligence Lab

Columbia University

{adam.li, yushupan, eb}@cs.columbia.edu

These authors contributed equally to this work, and the author names are listed in alphabetical order.

###### Abstract

Considering various data modalities, such as images, videos, and text, humans perform causal reasoning using high-level causal variables, as opposed to operating at the low, pixel level from which the data comes. In practice, most causal reasoning methods assume that the data is described as granular as the underlying causal generative factors, which is often violated in various AI tasks. This mismatch translates into a lack of guarantees in various tasks such as generative modeling, decision-making, fairness, and generalizability, to cite a few. In this paper, we acknowledge this issue and study the problem of causal disentangled representation learning from a combination of data gathered from various heterogeneous domains and assumptions in the form of a latent causal graph. To the best of our knowledge, the proposed work is the first to consider i) non-Markovian causal settings, where there may be unobserved confounding, ii) arbitrary distributions that arise from multiple domains, and iii) a relaxed version of disentanglement. Specifically, we introduce graphical criteria that allow for disentanglement under various conditions. Building on these results, we develop an algorithm that returns a causal disentanglement map, highlighting which latent variables can be disentangled given the combination of data and assumptions. The theory is corroborated by experiments.

## 1 Introduction

Causality is fundamental throughout various aspects of human cognition, including understanding, planning, decision-making. The ability to perform causal reasoning is considered one of the hallmarks of human intelligence . In the context of AI, the capability of reasoning with cause-and-effect relationships plays a critical role in challenges of explainability, fairness, decision-making, robustness, and generalizability. One key assumption of most methods currently available in the causal literature is that the set of (endogenous) variables is at the right level of granularity. However, this is not the case in many AI applications, where various modalities, such as images, and text, come into play . For example, images of a park scene capture objects as causal variables, not the pixels themselves. AI must disentangle these latent causal variables to represent the true relationships in the image. Faithfully representing this latent structure impacts downstream AI tasks like image generation and few-shot learning.

In machine learning, the representation learning literature is concerned with finding useful representations from data . One important line of work traces back to linear ICA (independent component analysis) , where one attempts to disentangle latent variables assuming a linear mixing function. The literature has also considered settings where the mixing function is nonlinear . It has been understood that nonlinear-ICA is, in general, not identifiable (ID) given only observational data .

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_FAIL:3]

the c-component of every \(X\). We will use \((X)\) or \(_{X}\) to denote parents of \(X\) in \(G\). Let \(}(X)=(X) X\), which includes \(X\) itself. A subgraph over \(\) in \(G\) is denoted as \(G()\) and \(G_{}}\) denotes the subgraph by removing arrows coming into nodes in \(\).

A _soft intervention_ on a variable \(X\), denoted \(_{X}\), replaces \(f_{X}\) with a new function \(f^{}_{X}\) of \(^{}\) and variables \(^{}_{X}\)[58; 59]. For interventions on a set of variables \(\), let \(_{}=\{_{}\}_{X}\), that is, the result of applying one intervention after the other. Given an SCM \(\), let \(_{_{}}\) be a submodel of \(\) induced by intervention \(_{_{}}\). A special class of soft interventions, resulting in observational distributions, called _idle_ intervention, leaves the function as it is, which means \(_{}=\{\}\). Another special class of stochastic soft interventions, called _perfect_ interventions [21; 51] and denoted as \(()\), such that \(()=\) and \(^{}_{X}=\). This implies that the modified diagram induced by \(_{_{}}\) is \(G_{}}\). We assume soft interventions that are not hard do not change the structure of the graph 5. Namely, the diagram induced by \(_{_{}}\) is the same with \(G\).

## 2 Modeling Disentangled Representation Learning (General Case)

In this section, we formalize the disentangled representation learning task in causal language. We leverage an Augmented SCMs, to model the generative process over _latent_ causal variables \(\).

**Definition 2.1** (Augmented Structure Causal Model).: An Augmented Structure Causal Model (for short, ASCM) over a generative level SCM \(_{0}=\{_{0},_{0},_{0},P^{0}( _{0})\}\) is a tuple \(=,\{,\},,P()\) such that (1) exogenous variables \(=_{0}\); (2) \(=_{0}=\{V_{1},,V_{d}\}\) are \(d\) latent endogenous variables; \(\) is an \(m\) dimensional mixture variable; (3) \(=\{_{0},f_{}\}\), where \(f_{}:^{d}^{m}\) is a diffeomorphic 6 function that maps from (the respective domains of) \(\) to \(\). \( h=f_{}^{-1}\) such that \(=h()\); and (4) \(P(_{0})=P^{0}(_{0})\). 

In words, an ASCM \(\) describes a two-stage generative process involving latent generative factors \(\) and high-dimensional mixture \(\) (e.g., images, or text). First, latent generative factors \(^{d}\) are generated by an underlying SCM. The causal diagram induced by \(_{0}\) over \(\) is called _a latent causal graph_ (LCG); denoted as \(G\) here. Next, a nonparametric diffeomorphism \(f_{}\) mixes \(\) to get the high-dimensional mixture \(^{m}\). An important aspect of \(f_{}\) is that it is invertible regarding \(\) which implies that the generative factors \(\) are recognized in a given \(=\)7.

The initial disentangled representation learning setting can be traced back at least to linear/nonlinear ICA [7; 8; 9], where \(G\) is assumed to have no edges (\(\) are independent of each other) and Markovian (no bidirected edges in the LCG). More recently, allowing latent variables to have edges in the LCG was studied, albeit still under the Markovian assumption [11; 13; 14; 21; 51; 55]. We relax this assumption and allow confounding to exist between \(\), which we call non-Markovianity8.

Domains.We address the general setting of distributions that arise from multiple domains. Following [32; 33; 34; 62; 63; 35], we define the so-called _latent selection diagram_ that represents a collection of ASCMs to model the multi-domain setting. Selection diagrams enable us to compactly represent causal structure and cross-domain invariances 9.

**Definition 2.2** (Latent Selection Diagrams).: Let \(}=_{1},_{2},...,_ {N}\) be a collection of ASCMs relative to \(N\) domains \(=_{1},_{2},...,_{N}\), sharing mixing function \(f_{}\) and LCG, \(G\). \(}\) defines a **latent selection diagram** (LSD. for short) \(G^{S}\), constructed as follows: (1) every edge in \(G\) is also an edge in \(G^{S}\); (2) \(G^{S}\) contains an extra node \(S^{i,j}\) and corresponding edge \(S^{i,j} V_{k}\) whenever there exists a discrepancy \(f^{i}_{V_{k}} f^{j}_{V_{k}}\), or \(P^{i}(U_{k}) P^{j}(U_{k})\) between \(_{i}\) and \(_{j}\). 

S-nodes indicate possible differences over \(\) due to changes in the underlying mechanism or exogenous distributions across domains. For example, consider the LSD in Fig. 2. The S-node \(S^{i,j}\) impliesthat \(V_{1}\) possibly changes from domain \(^{i}\) to \(^{j}\), while the other variable's mechanisms are assumed to be invariant. Note no S-node points to \(\) since \(f_{}\) is shared across \(}\).

Interventions.A set of interventions \(=\{^{(k)}\}_{k=1}^{K}\) are applied across domains \(\), where \(k\) is an index from 1 to \(K\). The corresponding domains that \(\) are intervened in is denoted as \(^{}=\{^{(k)}\}_{k=1}^{K}\) (the domains associated with each \(^{(k)}\)). We study a general setting where each intervention can be applied to any subset of nodes and in any domain, which can be seen as a generalization of the more restricted settings in prior work (see F).

The intervention targets collection of these \(K\) interventions \(\{^{(k)}\}_{k=1}^{K}\) is denoted as \(=\{^{(k)}\}_{k=1}^{K}\). Each intervention target \(^{(k)}\) is given in the form of \(\{V_{i}^{^{(k)},\{b\},t},V_{i}^{^{(k)},\{b^{}\},t^{}},\}\), which indicates the intervention \(^{(k)}\) changes the mechanism of \(\{V_{i},V_{j},\}\) in domain \(^{(k)}\). \(\{b\}\) indicates the mechanism of the intervention on the same node. The mechanisms of \(V_{i}^{1}\) and \(V_{i}^{2}\) are different while the mechanism on different nodes (\(V_{i}^{1}\) and \(V_{j}^{1}\)) is default different. \(t=\) indicates the intervention is perfect. When \(^{(k)}=\{V_{i}^{^{(k)},t}\}\), where \(\{b\}\) is omitted, then the intervention is assumed to have a different mechanism. When \(^{(k)}=\{V_{i}^{^{(k)},\{b\}}\}\), where \(t\) is omitted, then the intervention is assumed to be a general soft intervention. When \(^{(k)}\) is an idle intervention in \(^{(n)}\), it is denoted as \(\{\}^{^{(n)}}\). The set \([^{(k)}]\) is a set of variables with perfect interventions in \(^{(k)}\). Thus when \([^{(k)}]=\{\}\), it implies there are no variables with perfect interventions in \(^{(k)}\). \(_{}^{}\) is a subset of \(\) such that \([^{(j)}]\) for every \(^{(j)}_{}^{}\), which implies \(^{(j)}\) contains perfect interventions on \(\); see Fig. S1 and Ex. 7 illustrating the notation.

Given Distributions.The interventions \(=\{^{(k)}\}_{k=1}^{K}\), induce distributions \(=\{P^{(k)}\}_{k=1}^{K}\) in multi-domains, where \(P^{(k)}=P^{^{(k)}}(;^{(k)})\).

Problem StatementSuppose the underlying true model \(}\) induces the LSD \(G^{S}\) and a collection of distributions \(\) over \(\) is given according to a corresponding collection of interventions \(\). The goal of this paper is to learn a disentangled representation \(}\) of the latent generative factors \(\) in \(}\). In the literature, it is common to require every variable \(V_{i}\) to be disentangled from all other variables  or some special subset (e.g. non-ancestors of \(V_{i}\)) . However, as illustrated in Fig. 2, sometimes only the target variables (\(^{tar}\)) is needed to be disentangled from some user-chosen entangled variables (\(^{en}\)). Recent work has also considered a similar goal of generalized disentanglement . Our work still differs from theirs in the following ways: i) [assumptions] we model a completely nonparametric non-Markovian ASCM, whereas  assumes sparsity and a Markovian ASCM, and ii) [input] we consider arbitrary combinations of distributions from multiple domains, whereas  considers only interventions within a single domain (see Appendix F for a detailed comparison). We formally define this type of general indeterminacy next as well as the formal version of our ID task.

**Definition 2.3** (General Identifiability/Disentangleability (ID)).: Let \(}\) be the underlying true ASCMs inducing LSD \(G^{S}\), and \(=\{P^{(k)}\}_{k=1}^{K}\) a set of distributions resulting from \(K\) intervention sets \(\). Consider target variables \(^{tar}}\), and \(^{en}^{tar}\). The set \(^{tar}\) is identifiable (disentangled) with respect to (from) \(^{en}\) if there exists a function \(\) such that \(}^{tar}=(^{ en})\) for any \(}}\) that is compatible with \(G^{S}\) and \(^{}}}=\). For short, \(^{tar}\) is said to be ID w.r.t. \(^{en}\). 

To illustrate, consider a target variable \(^{tar}\) such that one wants its representation to be disentangled from another subset variables \(^{en}\). The above definition states that \(^{tar}\) is disentangled from \(}^{en}\) (or is ID w.r.t. \(^{en}\)) if the learned representations \(}^{tar}\) in \(}}\) is only a function of \(^{en}\) for _any_\(}}\) that matches with the LSD \(G^{S}\) and distribution \(\)10. Def 2.3 is illustrated in Fig. 3. Following the example illustrated in Fig. 2, suppose the user wants \(V_{3}\) to be disentangled from \(V_{1}\) while considering the entanglement between \(V_{2}\) and \(V_{3}\) acceptable. If \(^{3}=(V^{2},V^{3})\) for any ASCM \(}}\) matches the

Figure 3: General ID/disentangleability.

distributions and LSD, \(V_{3}\) is ID w.r.t. \(V_{1}\). Def. 2.3 is more relaxed since one is free to choose any target \(^{tar}\) and \(^{en}\). It can be reduced to existing identifiability definitions (Appendix F.2).

**Example 1** (Example of an ID task).: _Suppose the pair of underlying ASCMs \(_{1},_{2}\) induces the LSG \(G^{S}\) in Fig. 2 and distributions \(=\{P^{(1)},P^{(2)},P^{(3)},P^{(4)}\}=\{P^{_{1}}(),P^{ _{2}}(),P^{_{2}}(;_{V_{3}}),P^{_{1}}( ;_{V_{4}})\}\) from interventions \(=\{^{(1)},^{(2)},^{(3)},^{(4)}\}=\{\{\},\{\}, _{_{3}},(V_{2})\}\). Given intervention targets \(=\{^{(1)},^{(2)},^{(3)}, ^{(4)}\}=\{\{\}^{_{1}},\{\}^{_{2}},\)\(V_{3}^{_{2}},V_{2}^{_{1},}\}\) and \(G^{S}\), the task is to determine whether (and how) \(\{V_{2},V_{3}\}\) is ID w.r.t. \(V_{1}\), and \(V_{1}\) is ID w.r.t \(\{V_{2},V_{3}\}\). The answer to this is provided in Ex. 6. _

Assumptions (Informal) and Modeling ConceptsBefore discussing the main theoretical contributions, we restate important assumptions and remarks (discussed in this section) here to ground the ASCM model 11.

**Assumption 1** (Soft interventions without altering the causal structure).: _Interventions do not change the causal diagram. Hard interventions cut all incoming parent edges, and soft interventions preserve them . However, more general interventions may arbitrarily change the parent set for any given node . We do not consider such interventions and leave this general case for future work._

**Assumption 2** (Known-target interventions).: _All interventions occur with known targets, reducing permutation indeterminacy for intervened variables._

**Assumption 3** (Sufficiently different distributions).: _Each pair of distributions \(P^{(j)}\) and \(P^{(k)}\) are sufficiently different, unless stated otherwise. This is naturally satisfied if ASCMs and interventions are randomly chosen . Similar assumptions include the "genericity" , "interventional discrepancy" , and "sufficient changes" assumptions ._

**Remark 1** (Mixing is invertible).: As a consequence of Def. 2.1, the mixing function \(f_{}\) is invertible, ensuring that latent variables are uniquely learnable .

**Remark 2** (Confounders are not part of the mixing function).: According to Def. 2.1, latent exogenous variables \(\) influence the high-dimensional mixture \(\) only through latent causal variables \(\), so unobserved confounding \(\) does not directly affect the mixing function.

**Remark 3** (Shared causal structure).: As a consequence of Def. 2.2, each environment's ASCM shares the same latent causal graph, with no structural changes among latent variables 12.

## 3 Graphical Criterion for Causal Disentanglement

In this section, we study a general form of identifiability given general assumptions and input distributions. More specifically, we build the connection from \(\) and representation \(}\) through comparing distributions and then introduce three graphical criteria (Prop. 3, 4 and 5) to check ID.

First, we introduce a factorization of distributions induced by non-Markovian models [3, Def. 15]. Specifically, consider \(P_{}()\) induced by an ASCM \(\) after a perfect intervention on \(\). Then, given a topological order \(<\) of \(G\), \(P_{}()\) can be factorized as follows:

\[P_{}()=_{V_{i}}P_{}(V_{i}| _{i}^{+})\] (1)

where \(_{i}^{+}=}(\{V(V_{i}) :V V_{i}\})\{V_{i}\}\) is the extended parents set of \(V_{i}\) in \(G_{}}\). The factorization form for \(P_{}()\) will be different according to the choice given order.

**Example 2**.: _Consider a collection of \(\) inducing the LSD shown in Fig. 4(c). Given order \(A\): \(V_{1}<V_{2}<V_{3}<V_{4}\), \(P()\) can be factorized as: \(P(V_{1})P(V_{2} V_{1})P(V_{3} V_{2},V_{1})P(V_{4} V_{3})\) Notice that the conditioning part of \(V_{3}\) includes \(\{V_{2},V_{1}\}\), which are not parents of \(V_{3}\). Choosing order B: \(V_{1}<V_{3}<V_{2}<V_{4}\), \(P()\) can be factorized as \(P(V_{1})P(V_{3})P(V_{2} V_{1},V_{3})P(V_{4} V_{3})\). The conditioning part of \(V_{2}\) and \(V_{3}\) are different given different orders. _Armed with this factorization, the representation \(\) in \(}}\) and the true underlying variables \(\) in \(}\) can be related by comparing distributions as follows.

**Proposition 1** (**Distribution Comparison**).: _Consider a pair of collections ASCMs \(}\) and \(}}\) that matches with the distribution \(\) resulting from interventions \(\) and LSD \(G^{S}\). Consider two distributions \(P^{^{(j)}}(;^{(j)})\) and \(P^{^{(k)}}(;^{(k)})\). Suppose \(()\) is in both intervention sets, then,_

\[_{i}^{d} p_{}^{(j)}(v_{i}_{i}^{+})-p _{}^{(k)}(v_{i}_{i}^{+})=_{i}^{d} p _{}^{(j)}(_{i}}_{i}^{+ })- p_{}^{(k)}(_{i}}_{i}^{ +}),\] (2)

_where \(p_{}^{(j)}()\) and \(p_{}^{(k)}()\) are density functions. _

To illustrate, Prop.1 shows when the intervention and domain changes from \(^{(k)}\) to \(^{(j)}\) and \(^{(k)}\) to \(^{(j)}\), the change comes from factors \(p_{}(v_{i}_{i}^{+})\) both in \(}\) and \(}}\).

However, not all factors necessarily contribute to Eq (2). For example, in the Markovian setting, only one factor \(p_{}(v_{i}_{i})\) possibly changes when comparing the observational to a singleton interventional distribution in the same domain. Other invariant factors will be canceled out in Eq. (2). The following result generalizes finding invariant factors when comparing distributions from different domains and interventions in non-Markovian settings.

**Proposition 2** (**Invariant Factors**).: _Consider two distributions \(P^{(j)},P^{(k)}\) with intervention targets \(^{(j)}\) and \(^{(k)}\) containing \(()\). Construct the changed variable set \([^{(j)},^{(k)},G^{S}]\) (for short \(}\)) with target sets \(^{(j)},^{(k)}\) as follows: (1) \(V_{l}\) if \(V_{l}^{_{l},\{b_{l}\},t_{l}}^{(j)}\) but \(V_{l}^{_{l}^{},\{b_{l}\},t_{l}^{}}^{(k)}\), or vice versa; (2) \(V_{l}\) if i) \(S^{^{(j)},^{(k)}}\) point to \(V_{l}\) and ii) \(V_{l}^{_{l},\{b_{l}\},t_{l}}^{(j)}^{(k)}\). If \(V_{i}()\), then \(p_{}^{(j)}(v_{i}_{i}^{+})=p_{}^{( k)}(v_{i}_{i}^{+})\) (denoted invariant factors). _

Prop. 2 states that factors \(p_{}(v_{i}_{i}^{+})\) are guaranteed to be invariant if \(V_{i}\) is not in the C-component of the changed variable set \(\). \([^{(j)},^{(k)},G^{S}]\) contains variables that are intervened differently in \(^{(j)},^{(k)}\) and the variables pointed by S-node, \(S^{j,k}\)13.

**Example 3**.: _Consider the diagram in Fig. 4(c) and two distributions \(P^{(1)},P^{(2)}\) with intervention targets \(^{(1)}=\{\}^{_{1}}\) and \(^{(2)}=\{V_{2}^{_{1}}\}\). The changed variable set \(^{(2),(1)}=\{V_{2},V_{3}\}\) since \(V_{2}^{(2)}\), \(V_{2}^{(1)}\), and \((V_{2})=\{V_{2},V_{3}\}\). Thus, comparing \(P^{(2)}\) with \(P^{(1)}\) (order A in Ex. 2), factors \(p(v_{1}),p(v_{4} v_{2},v_{1})\) are invariant, whereas \(p(v_{2} v_{1}),p(v_{3} v_{2},v_{1})\) can change. _

With Prop. 2, Eq. (2) naturally keeps factors only in the C-component of \(\), i.e.,

\[_{V_{i}}} p_{}^{(j)}(v_{i} _{i}^{+})-p_{}^{(k)}(v_{i}_{i}^{+ })=_{V_{i}}} p_{}^{(j)}(_{i} }_{i}^{+})- p_{}^{(k)}(_{i}}_{i}^{+})\] (3)

where \(}=([^{(j)},^{(k) },G^{S}])\). This factorization hints that \(}\) (RHS of Eq. (3)) is only related to variables that appear on the LHS.

**Definition 3.1** (\(\) Set).: Given two distributions \(P^{(j)},P^{(k)}\) with interventions targets \(^{(j)}\) and \(^{(k)}\) containing perfect interventions on \(\), the \([^{(j)},^{(k)},,G^{S}]\) set (for short: \(^{(j),(k)}\), or \(\) if index not needed) of the target sets \(^{(j)},^{(k)}\) is the remaining variables after comparison (i.e. Eq. 3), \([^{(j)},^{(k)},,G^{S}]=}^{+}(})\), where \(}=([^{(j)},^{(k) },G^{S}])\). _

Figure 4: LSDs in Ex. and Exps. (a) chain, (b) collider and (c) non-markovian graphs.

To illustrate, the \(\) set involves all variables in LHS of Eq. (3), including \(}\) and its extended parents. These variables come from factors that possibly change and are kept in Eq. (3). We call \(\)_canceled variables_ since invariant factors are canceled out from the comparison. Continuing Ex. 3, \(=\{V_{1},V_{2},V_{3}\}\) given either topological order.

Leveraging the comparisons among distributions in \(\) (Eq. 3), we next develop three criterions for disentanglement. First, we can disentangle canceled variables from \(\) set since the difference of density over representations \(}\) in the \(\) set (RHS of Eq. (3)) is irrelevant to canceled variables (LHS of Eq. (3)).

**Proposition 3** (ID the \(\) set w.r.t Canceled Variables).: _Consider variables \(^{tar}\). Let \(_{}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\} \) be a collection of distributions such that (1) \(\;\;l[L]\), \(=[^{(a_{0})}][^{(a_{l})}]\)14; (2) \(_{l[L]}[^{(a_{l})},^{(a_{0})}, ,G^{S}]=^{tar}\); (3) there exists \(\{a^{}_{1},,a^{}_{d^{}}\}\{a_{1},,a_{L}\}\) such that for all \(V^{tar}_{i}^{tar},V^{tar}_{i}[^{(a^{ }_{i})},^{(a_{0})},,G^{S}]\), where \(d^{}=|^{tar}|\). Then, \(^{tar}\) is ID w.r.t \(^{tar}\). _

Prop. 3 disentangles target variables \(^{tar}\) (as a union of \(\) sets) from canceled variables according to Eq. (3). To illustrate, it considers to find a collection of \(L\) distribution \(\{P^{(a_{1})},,P^{(a_{L})}\}\) to compare with the baseline \(P^{(a_{0})}\) such that (1) the perfect intervention variables sets of \(\{^{(a_{1})},,^{(a_{L})}\}\) contain the perfect intervention set of the baseline \(^{(a_{0})}\), (2) the union of \(\) is equivalent to \(^{tar}\), and (3) each \(V^{tar}_{i}\) changes at least once. Then, \(^{tar}\) can be ID wrt \(^{tar}\).

**Example 4**.: _(Ex. 3 continued.) Suppose \(=\{P^{(1)},P^{(2)},P^{(3)},P^{(4)}\}\) with intervention targets \(^{(1)}=\{\}^{_{1}}\), \(^{(2)}=\{V_{2}\}^{_{1}}\), \(^{(3)}=\{V_{3}\}^{_{1}}\), \(^{(4)}=\{V_{1}\}^{_{1}}\). Consider \(^{tar}=\{V_{1},V_{2},V_{3}\}\) and \(^{en}=\{V_{1},V_{2},V_{3}\}=\{V_{4}\}\). Comparing \(\{^{(2)},^{(3)},^{(4)}\}\) with the baseline \(^{(1)}\), the perfect intervention variables are \(=[^{(1)}]=\{\}\). Then we have \(\) sets: \(\{V_{1},V_{2},V_{3}\},\{V_{1},V_{2},V_{3}\}\) and \(V_{1}\). Thus, these three comparisons satisfy the three conditions in Prop. 3. Then \(^{tar}\) is ID w.r.t \(^{en}\) by Prop. 3. See Appendix Ex. 17 for a derivation. _

According to Prop. 3, a disentanglement corollary leveraging the comparison of interventions and observational distributions can be derived.

**Corollary 1** (ID intervened variables).: _Given an observational distribution and \(L\) distributions resulting from interventions on the same target \(\) but with different mechanisms (in the same domain), if \(L|^{\!+}_{}|\), \(^{\!+}_{}\) is ID w.r.t \(\{^{\!+}_{}\}\), where \(^{\!+}_{}=_{W_{i}}^{ \!+}_{W_{i}}\). _

The second result disentangles variables within \(\) sets.

**Proposition 4** (ID of variables within \(\) sets).: _Consider the variables \(^{tar}\), \(_{}\) that satisfies conditions (1) in Prop. 3 and \(^{(a_{l}),(a_{0})}=^{tar}\), for \(l[L]\). For any pair of \(V_{i},V_{j}^{tar}\) such that \(V_{i}\!\!\! V_{j}|^{tar}\{V_{i},V_{j}\}\) in \(G_{}(^{tar})\), \(V_{i}\) is ID w.r.t. \(V_{j}\) if \(L 2|^{tar}|+_{}\), where \(_{}\) is the number of pair \(V_{k},V_{r}^{tar}\) such that \(V_{k}\) and \(V_{r}\) are connected given \(^{tar}\{V_{k},V_{r}\}\) in \(G_{}}(^{tar})\). _

Prop. 4 disentangles target variables \(V_{i}\) and \(V_{j}\) both in \(\) sets. To illustrate, consider a set of distributions that satisfies conditions (1) as Prop. 3. This proposition suggests that if (1) \(V_{i},V_{j}^{tar}\) are conditionally independent given all other variables in \(^{tar}\), (2) \(L\) is not smaller than \(2|^{tar}|+_{}\), then \(V_{i}\) can be disentangled from \(V_{j}\).

**Example 5**.: _Suppose LSD \(G^{S}\) is a collider graph shown in Fig. 4(b). Suppose the intervention targets are \(=\{\{\}^{_{1}},\{\}^{_{2}},\{\}^{_{3}},\{\}^{_{4}},\{\}^{ _{5}}\}\), which means that observational distributions are available in each domain. Consider \(=\{\}\). Let \(^{tar}=\{V_{1},V_{3}\}\). We have \(V_{1}\!\!\! V_{3}\) in \(G(V_{1},V_{3})\). Based on Def. 3.1, \([^{(j)},^{(1)},,G^{S}]=\{V_{1},V_{3}\}\) for \(j=2,3,4,5\). Then the number of distributions used for comparing (i.e., four) is not smaller than the required (\(2 2+0\)), which means \(V_{1}\) is ID w.r.t. \(V_{3}\) and \(V_{3}\) is ID w.r.t. \(V_{1}\) by Prop. 4. See App. Ex. 18 for a derivation. _

With these existing disentanglements from Props. 3 and 4, the following Proposition considers an inverse direction, which identifies canceled variables w.r.t. \(\) sets 15.

**Proposition 5** (ID of canceled variables w.r.t. \(\) sets).: _Suppose \(\) contains \(()\). Given \( V^{tar}\) is ID w.r.t. a single variable \(V^{tar}\), \(V^{tar}\) is ID w.r.t. \( V^{tar}\) if \(V^{tar}\!\!\! V^{tar}\) in \(G_{}}\). _To illustrate, Prop. 5 states: if \(\{V^{tar}\}\) is already disentangled from \(V^{tar}\), then \(V^{tar}\) is ID wrt \(\{V^{tar}\}\) if a perfect intervention on \(\) exists to separate \(V^{tar}\) and \(\{V^{tar}\}\) in \(G_{}}\). Prop. 5 does not compare distributions but relies on existing disentanglements. See Ex. 19 for details.

## 4 Algorithmic Disentanglement of Causal Representations

In this section, we develop an algorithmic procedure for determining whether \(^{tar}\) and \(^{en}\) are disentangleable given the LSD \(G^{S}\) and interventions sets \(\). The whole algorithm **CausalRepresentationID** (**CRID**, for short) is described in Alg. 1. We start by introducing a bipartite graph \(G_{,}}\), called _Causal Disentanglement Map (CDM)_ (which was informally shown in Fig 2 (right)). In words, the absence of the edge \(V_{i}_{j}\) implies \(V_{j}\) is ID w.r.t \(V_{i}\). If each \(_{i}\) is only pointed by \(V_{i}\), then we have full disentanglement of \(\). If \( V_{i}\) points to \(_{i}\), then we have partial disentanglement of \(V_{i}\).

**CRID** proceeds by first constructing the fully connected CDM in Step 1. In each iteration, the hard intervention set \(\) and the baseline intervention target set \(\) (Steps 4 and 6) are enumerated. For each \(\) and baseline, all \(\) sets are constructed based on Def. 3.1 and put into a collection \(\) (Steps 7). After the union of \(\) sets (denoted as \(\)) is chosen (Step 8) iteratively, Props. 3 and 4 are leveraged in two procedures (Step 9 and 10) to check the identification of \(\) w.r.t. \(\) and the identification within \(\). The disentanglements in CDM at the current stage are leveraged to reduce the required number of distributions (see details in Alg. F.3 and F.6). At the end of the iteration, Prop. 5 is used for identifying \(\) from \(\) leveraging current disentanglement in CDM (Step 11-12).

**Example 6**.: _(Ex. 1 continued.) Consider the selection diagram (Fig. 2) and the set up in Ex. 1 The perfect intervention variable sets are the empty set \(\{\}\) and \(\{V_{2}\}\). First, \(\) is chosen as \(\{\}\) and then \(_{}^{}=\). Choosing the baseline \(=^{(1)}\), the \(\) collection: \(=\{_{1},_{2},_{3}\}=\{\{V_{2},V_{3}\},\{V_{2},V_{3}\},\{V_{1},V_{2}\}\}\). We consider the \(\) as \(\{V_{2},V_{3}\}\) and \(\{V_{1},V_{2}\}\). For \(=\{V_{2},V_{3}\}\)

Figure 5: Process of removing edges from CDM \(G_{,}}\) using Alg. 1 in Ex. 6. (d) is the final output.

leveraging Step 9 (Prop. 3), the edges from \(V_{1}\) to \(\{_{2},_{3}\}\) are removed (See Ex. 15 for details) and Step 10 (Prop. 4) does not remove further edges. However, for \(=\{V_{1},V_{2}\}\), no edge can be removed, since it at least needs two comparisons for claiming disentanglement._

_Choosing \(\{\}^{_{2}}\) or \(V_{3}^{_{2}}\) or \(V_{2}^{_{1},}\) as the baseline, no new \(\) can be constructed, so no further edges are removed. When \(\) is chosen as \(\{V_{2}\}\), the comparison does not work since no other distribution is available. At the end of this iteration, with the fact that \(\{V_{2},V_{3}\}\) is ID wrt \(V_{1}\) and \(V_{1}\!\!\!\{V_{2},V_{3}\}\) in \(G_{}}\), Step 12 (Prop. 5) removes edges from \(V_{2}\) to \(_{1}\) and \(V_{3}\) to \(_{1}\). In the second iteration, the algorithm repeats the choice of \(\) and the baseline. At this iteration, for \(=\{V_{1},V_{2}\}\), the edge from \(V_{3}\) to \(_{2}\) is removed since \(V_{3}\) to \(_{1}\) has already been removed in CDM and only 1 comparison is needed now. At the end of this epoch no further can be removed by Alg. F.8. In the third epoch, \(G_{,}}\) is not updated and the process of CDM returned is shown in Fig. 5. 

The following theorem indicates the soundness of CRID.

**Theorem 1** (Soundness of CRID).: _Consider a LSD \(G^{S}\) and intervention targets \(\). Consider the target variables \(^{tar}\) and \(^{en}^{tar}\). If no edges from \(^{tar}\) points to \(}^{en}\) in the output causal disentanglement map (CDM) from **CRID**, \(G_{V,}\), then \(^{tar}\) is ID w.r.t \(^{en}\). _

## 5 Experiments

We corroborate the theoretical findings through simulations and MNIST dataset. For full details, see Appendix Section G. In simulations, we consider LSDs shown in Fig. 4 with different collection of distributions \(=\{P^{(k)}(;^{(k)})\}_{k=1}^{K}\) and the results are presented in Fig. 6. For the evaluation, we follow a standard evaluation protocol in prior work , where we take the latent representations \(}\) and compute their mean correlation coefficient (MCC) wrt the latent \(\). We compare MCC with what is expected from **CRID**. Fig. S9 shows the full MCC comparisons of \(\) and \(}\).

**Chain Graph Fig. 4(a).** Fig. 6(a) shows ID of \(V_{3}\) wrt \(\{V_{1}\}\) using input distributions \(\) with interventions \(=\{_{\{\}},_{\{}}^{\{1\}},_{\{}}^{\{2\}}\}\) because \(MCC(_{3},V_{1})\) is relatively low compared to \(MCC(_{3},V_{3})\), which is consistent with CRID. The ID results of  states \(V_{3}\) would still be entangled with \(V_{1}\) because \(V_{1}(V_{3})\). Fig. 6(b) shows ID of \(V_{1}\) wrt \(\{V_{2},V_{3}\}\). Interestingly, we do not even have to intervene on \(V_{1}\) to obtain full disentanglement.

**Collider Graph Fig. 4(b).** Fig. 6(c) shows \(V_{1}\) and \(V_{3}\) are ID wrt \(V_{2}\) and each other because \(MCC(_{3},V_{3})>MCC(_{3},V_{i})\) and \(MCC(_{2},V_{2})>MCC(_{2},V_{i})\), which is consistent with CRID. There are distributions from four domains that have a change-in-mechanism on \(\{V_{1},V_{3}\}\) (represented by the S-node). According to , since \(V_{1}\) and \(V_{3}\) are adjacent in the Markov Network, \(V_{1}\) and \(V_{3}\) are not disentangleable.

**Non-Markovian Graph Fig. 4(c).** Fig. 6(d) shows \(V_{3}\) is ID wrt \(\{V_{1},V_{2},V_{4}\}\) with interventions \(=\{do^{\{1\}}(V_{3}),do^{\{2\}}(V_{3})\}\), which is consistent with CRID. No prior results achieve disentanglement with confounding among \(\).

## 6 Conclusions

This work introduces theory and a practical ID algorithm for determining which latent variables are disentangleable from a given set of assumptions in the form of a LSD, and input distributions from heterogenous domains. This brings us one step closer to building robust AI that can reason causally over high-level concepts when only given low-level data.

Figure 6: Correlation of learned latent representations with true latent variables from Fig. 4.

[MISSING_PAGE_FAIL:11]

*  F. Locatello, B. Poole, G. Ratsch, B. Scholkopf, O. Bachem, and M. Tschannen. "Weakly-supervised disentanglement without compromises." In: _International conference on machine learning_. PMLR. 2020, pp. 6348-6359.
*  J. Brehmer, P. De Haan, P. Lippe, and T. S. Cohen. "Weakly supervised causal representation learning." In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 38319-38331.
*  L. Wendong, A. Kekic, J. von Kugelgen, S. Buchholz, M. Besserve, L. Gresele, and B. Scholkopf. _Causal Component Analysis_. arXiv:2305.17225 [cs, stat]. 2023.
*  K. Zhang, S. Xie, I. Ng, and Y. Zheng. _Causal Representation Learning from Multiple Distributions: A General Setting_. arXiv:2402.05052 [cs, stat]. 2024.
*  B. Varici, E. Acarturk, K. Shanmugam, and A. Tajer. "General identifiability and achievability for causal representation learning." In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2024, pp. 2314-2322.
*  S. Lachapelle, P. R. Lopez, Y. Sharma, K. Everett, R. L. Priol, A. Lacoste, and S. Lacoste-Julien. "Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies." In: _arXiv preprint arXiv:2401.04890_ (2024).
*  Z. Jin, J. Liu, Z. Lyu, S. Poff, M. Sachan, R. Mihalcea, M. Diab, and B. Scholkopf. _Can Large Language Models Infer Causation from Correlation?_ arXiv:2306.05836 [cs]. 2023.
*  M. Zecevic, M. Willig, D. S. Dhami, and K. Kersting. "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal." en. In: _Transactions on Machine Learning Research_ (2023).
*  Y. Pan and E. Bareinboim. "Counterfactual Image Editing." In: _arXiv preprint arXiv:2403.09683_ (2024).
*  P. C. Austin. "An introduction to propensity score methods for reducing the effects of confounding in observational studies." In: _Multivariate Behavioral Research_ 46.3 (2011), pp. 399-424.
*  M. Brookhart, T. Sturmer, R. Glynn, J. Rassen, and S. Schneeweiss. "Confounding control in healthcare database research: challenges and potential approaches." In: _Medical care_ 48.6 0 (2010), S114-S120.
* MICCAI 2019_. Ed. by D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan. Cham: Springer International Publishing, 2019, pp. 484-492.
*  F. Mahmood, R. Chen, and N. J. Durr. "Unsupervised Reverse Domain Adaptation for Synthetic Medical Images via Adversarial Training." In: _IEEE Transactions on Medical Imaging_ 37.12 (2018), pp. 2572-2581.
*  A. Li, A. Jaber, and E. Bareinboim. "Causal discovery from observational and interventional data across multiple environments." In: _Thirty-seventh Conference on Neural Information Processing Systems_. 2023.
*  E. Bareinboim and J. Pearl. "Transportability of Causal Effects: Completeness Results." In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 26.1 (2012), pp. 698-704.
*  J. Pearl and E. Bareinboim. "Transportability across studies: A formal approach." In: (2018).
*  E. Bareinboim and J. Pearl. "Meta-Transportability of Causal Effects: A Formal Approach." en. In: _Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics_. ISSN: 1938-7228. PMLR, 2013, pp. 135-143.
*  E. Bareinboim and J. Pearl. "Causal inference and the data-fusion problem." In: _Proceedings of the National Academy of Sciences_ 113.27 (2016). Publisher: National Academy of Sciences, pp. 7345-7352.
*  P. Hunermund and E. Bareinboim. _Causal Inference and Data Fusion in Econometrics_. arXiv:1912.09104 [econ]. 2023.
*  A. Li, C. Huynh, Z. Fitzgerald, I. Cajigas, D. Brusko, J. Jagid, A. O. Claudio, A. M. Kanner, J. Hopp, S. Chen, J. Haagensen, E. Johnson, W. Anderson, N. Crone, S. Inati, K. A. Zaghloul, J. Bulacio, J. Gonzalez-Martinez, and S. V. Sarma. "Neural fragility as an EEG marker of the seizure onset zone." en. In: _Nature Neuroscience_ 24.10 (2021). Number: 10 Publisher: Nature Publishing Group, pp. 1465-1474.

*  A. Li, P. Myers, N. Warsi, K. M. Gunnarsdottir, S. Kim, V. Jirsa, A. Ochi, H. Otusbo, G. M. Ibrahim, and S. V. Sarma. _Neural Fragility of the Intracranial EEG Network Decreases after Surgical Resection of the Epileptogenic Zone_. en. Pages: 2021.07.07.21259385. 2022.
*  A. Li, B. Chennuri, S. Subramanian, R. Yaffe, S. Gliske, W. Stacey, R. Norton, A. Jordan, K. Zaghloul, S. Inati, S. Agrawal, J. Haagensen, J. Hopp, C. Atallah, E. Johnson, N. Crone, W. Anderson, Z. Fitzgerald, J. Bulacio, J. Gale, S. Sarma, and J. Gonzalez-Martinez. "Using network analysis to localize the epilepogenic zone from invasive EEG recordings in intractable focal epilepsy." In: _Network Neuroscience 2_. 2 (2017).
*  J. M. Bernabei, A. Li, A. Y. Revell, R. J. Smith, K. M. Gunnarsdottir, I. Z. Ong, K. A. Davis, N. Sinha, S. Sarma, and B. Litt. "Quantitative approaches to guide epilepsy surgery from intracranial EEG." In: _Brain_ (2023), awad007.
*  K. M. Gunnarsdottir, A. Li, R. J. Smith, J.-Y. Kang, A. Korzeniewska, N. E. Crone, A. G. Rouse, J. J. Cheng, M. J. Kinsman, P. Landazuri, U. Uysal, C. M. Ulloa, N. Cameron, I. Cajigas, J. Jagid, A. Kanner, T. Elarjani, M. M. Bicchi, S. Inati, K. A. Zaghloul, V. L. Boerwinkle, S. Wyckoff, N. Barot, J. Gonzalez-Martinez, and S. V. Sarma. "Source-sink connectivity: a novel interictal EEG marker for seizure localization." In: _Brain_ 145.11 (2022), pp. 3901-3915.
*  L. Nobili, B. Frauscher, S. Eriksson, S. Gibbs, H. Peter, I. Lambert, R. Manni, L. Peter-Derex, P. Proserpio, F. Provini, A. Weerd, and L. Parrino. "Sleep and epilepsy: A snapshot of knowledge and future research lines." In: _Journal of Sleep Research_ 31 (2022).
*  A. Bagshaw, J. Jacobs, P. LeVan, F. Dubeau, and J. Gotman. "Effect of sleep stage on interictal high-frequency oscillations recorded from depth macroelectrodes in patients with focal epilepsy." In: _Epilepsia_ 50 (2008), pp. 617-28.
*  S. Gibbs, P. Proserpio, M. Terzaghi, A. Pigorini, S. Sarasso, G. Russo, L. Tassi, and L. Nobili. "Sleep-related epileptic behaviors and non-REM-related parasomnias: Insights from stereo-EEG." In: _Sleep Medicine Reviews_ 63 (2015).
*  P. Greene, A. Li, J. Gonzalez-Martinez, and S. Sarma. "Classification of Stereo-EEG Contacts in White Matter vs. Gray Matter Using Recorded Activity." In: _Frontiers in Neurology_ 11 (2021).
*  A. A. Borbely, F. Baumann, D. Brandeis, I. Strauch, and D. Lehmann. "Sleep deprivation: Effect on sleep stages and EEG power density in man." In: _Electroencephalography and Clinical Neurophysiology_ 51.5 (1981), pp. 483-493.
*  A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. "Attention is All you Need." In: _Advances in Neural Information Processing Systems_. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017.
*  R. Bommasani et al. _On the Opportunities and Risks of Foundation Models_. 2022.
*  T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. _Language Models are Few-Shot Learmers_. 2020.
*  J. von Kugelgen, M. Besserve, L. Wendong, L. Gresele, A. Kekic, E. Bareinboim, D. M. Blei, and B. Scholkopf. _Nonparametric Identifiability of Causal Representations from Unknown Interventions_. arXiv:2306.00542 [cs, stat], 2023.
*  K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. "Interventional causal representation learning." In: _International conference on machine learning_. PMLR. 2023, pp. 372-407.
*  D. Yao, D. Xu, S. Lachapelle, S. Magliacane, P. Taslakian, G. Martius, J. von Kugelgen, and F. Locatello. _Multi-View Causal Representation Learning with Partial Observability_. 2024.
*  M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang. "CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models." In: _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. ISSN: 2575-7075. 2021, pp. 9588-9597.
*  J. Zhang, K. Greenewald, C. Squires, A. Srivastava, K. Shanmugam, and C. Uhler. "Identifiability guarantees for causal disentanglement from soft interventions." In: _Advances in Neural Information Processing Systems_ 36 (2024).
*  A. Li, J. Feitelberg, A. P. Saini, R. Hochenberger, and M. Scheltienne. "MNE-ICALabel: Automatically annotating ICA components with ICLabel in Python." In: _Journal of Open Source Software_ 7.76 (2022), p. 4484.

*  J. Tian and J. Pearl. "On the testable implications of causal models with hidden variables." In: _arXiv preprint arXiv:1301.0608_ (2012).
*  J. Correa and E. Bareinboim. "A Calculus for Stochastic Interventions:Causal Effect Identification and Surrogate Experiments." en. In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 34.06 (2020). Number: 06, pp. 10093-10100.
*  J. Correa and E. Bareinboim. "General Transportability of Soft Interventions: Completeness Results." In: _Advances in Neural Information Processing Systems_. Vol. 33. Curran Associates, Inc., 2020, pp. 10902-10912.
*  P. R. Rosenbaum and D. B. Rubin. "The Central Role of the Propensity Score in Observational Studies for Causal Effects." In: _Biometrika_ 70.1 (1983), pp. 41-55.
*  J. Pearl. "Causal Diagrams for Empirical Research." In: _Biometrika_ 82.4 (1995). Publisher: [Oxford University Press, Biometrika Trust], pp. 669-688.
*  J. Pearl and E. Bareinboim. "Transportability of Causal and Statistical Relations: A Formal Approach." en. In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 25.1 (2011). Number: 1, pp. 247-254.
*  S. Lee, J. D. Correa, and E. Bareinboim. "General identifiability with arbitrary surrogate experiments." In: 2019.
*  F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Scholkopf, and O. Bachem. "Challenging common assumptions in the unsupervised learning of disentangled representations." In: _international conference on machine learning_. PMLR. 2019, pp. 4114-4124.
*  R. Perry, J. von Kugelgen, and B. Scholkopf. _Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis_. arXiv:2206.02013 [cs, stat]. 2022.
*  B. Huang, K. Zhang, M. Gong, and C. Glymour. "Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models." en. In: _Proceedings of the 36th International Conference on Machine Learning_. ISSN: 2640-3498. PMLR, 2019, pp. 2901-2910.
*  B. Huang, C. J. H. Low, F. Xie, C. Glymour, and K. Zhang. "Latent hierarchical causal structure discovery with rank constraints." In: _arXiv preprint arXiv:2210.01798_ (2022).
*  J. Peters, P. Buhlmann, and N. Meinshausen. _Causal inference using invariant prediction: identification and confidence intervals_. arXiv:1501.01332 [stat]. 2015.
*  J. M. Mooij, S. Magliacane, and T. Claassen. "Joint causal inference from multiple contexts." In: _The Journal of Machine Learning Research_ 21.1 (2020), 99:3919-99:4026.
*  M. Okamoto. "Distinctness of the eigenvalues of a quadratic form in a multivariate sample." In: _The Annals of Statistics_ (1973), pp. 763-765.
*  D. Koller and N. Friedman. _Probabilistic graphical models: principles and techniques_. MIT press, 2009.
*  J. Pearl. _Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference_. en. Google-Books-ID: AvNID7LyMusC. Morgan Kaufmann, 1988.
*  S. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H.-G. Leimer. "Independence properties of directed markov fields." en. In: _Networks_ 20.5 (1990). _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/net.3230200503, pp. 491-505.
*  K. Rajamanickam. "A Mini Review on Different Methods of Functional-MRI Data Analysis Citation: Karunanithi Rajamanickam. A Mini Review on Different Methods of Functional-MRI Data Analysis." In: _Archives of Internal Medicine Research_ 03 (2020), pp. 44-060.
*  Nuzillard, D. and Bijaoui, A. "Blind source separation and analysis of multispectral astronomical images." In: _Astron. Astrophys. Suppl. Ser._ 147.1 (2000), pp. 129-138.
*  E. Bingham and A. Hyvarinen. "ICA of complex valued signals: a fast and robust deflationary algorithm." In: _Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium_. Vol. 3. 2000, 357-362 vol.3.
*  A. D. Back and A. S. Weigend. "A First Application of Independent Component Analysis to Extracting Structure from Stock Returns." In: _Econometrics: Applied Econometrics & Modeling edJournal_ (1997).
*  E. Bingham, J. Kuusisto, and K. Lagus. "ICA and SOM in text document analysis." In: _Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval_. 2002, pp. 361-362.

*  B. Varici, E. Acarturk, K. Shanmugam, and A. Tajer. "Linear Causal Representation Learning from Unknown Multi-node Interventions." In: _arXiv preprint arXiv:2406.05937_ (2024).
*  S. Bing, U. Ninad, J. Wahl, and J. Runge. "Identifying linearly-mixed causal representations from multi-node interventions." In: _arXiv preprint arXiv:2311.02695_ (2023).
*  L. Gresele, G. Fissore, A. Javaloy, B. Scholkopf, and A. Hyvarinen. _Relative gradient optimization of the Jacobian term in unsupervised deep learning_. 2020.
*  G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. _Normalizing Flows for Probabilistic Modeling and Inference_. 2021.
*  C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. _Neural Spline Flows_. 2019.
*  D. P. Kingma and J. Ba. _Adam: A Method for Stochastic Optimization_. 2017.
*  K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger, and G. P. Nolan. "Causal protein-signaling networks derived from multiparameter single-cell data." eng. In: _Science (New York, N.Y.)_ 308.5721 (2005), pp. 523-529.
*  J. M. Robins, M. A. Hernan, and B. Brumback. "Marginal structural models and causal inference in epidemiology." eng. In: _Epidemiology (Cambridge, Mass.)_ 11.5 (2000), pp. 550-560.
*  J. Tian and J. Pearl. "A General Identification Condition for Causal Effects." In: _AAAI_ (2002).
*  I. Shpitser and J. Pearl. "Identification of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models." In: _AAAI-Proceedings_ (2006), pp. 1219-1226.
*  M. Kocaoglu, K. Shanmugam, and E. Bareinboim. "Experimental Design for Learning Causal Graphs with Latent Variables." In: _Advances in Neural Information Processing Systems_ 30 (2017).
*  M. Kocaoglu, A. Jaber, K. Shanmugam, and E. Bareinboim. "Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions." In: _Advances in Neural Information Processing Systems_ 32 (2019).
*  A. Jaber, M. Kocaoglu, K. Shanmugam, and E. Bareinboim. "Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning." In: _Advances in Neural Information Processing Systems_. Vol. 33. Curran Associates, Inc., 2020, pp. 9551-9561.
*  X. Shen, F. Liu, H. Dong, Q. Lian, Z. Chen, and T. Zhang. "Weakly Supervised Disentangled Generative Causal Representation Learning." In: _Journal of Machine Learning Research_ 23 (2022), pp. 1-55.
*  K. Xia, K.-Z. L. Lee Bloomberg, Y. Bengio, and E. Bareinboim. "The Causal-Neural Connection: Expressiveness, Learnability, and Inference." In: (2021).
*  K. Xia, Y. Pan, and E. Bareinboim. "Neural Causal Models for Counterfactual Identification and Estimation." In: _International Conference on Learning Representations_. 2022.
*  A. Jaber, A. H. Ribeiro, J. Zhang, and E. Bareinboim. "Causal Identification under Markov equivalence: Calculus, Algorithm, and Completeness." In: _Advances in Neural Information Processing Systems._ 2022.
*  A. Jaber, J. Zhang, and E. Bareinboim. "Causal Identification under Markov Equivalence: Completeness Results." In: (2019). Publisher: PMLR, pp. 2981-2989.
*  T. V. Anand, A. H. Ribeiro, J. Tian, and E. Bareinboim. "Causal effect identification in cluster dags." In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 37. 10. 2023, pp. 12172-12179.
*  I. Shpitser and J. Pearl. "Complete Identification Methods for the Causal Hierarchy." In: _Journal of Machine Learning Research_ 9 (2008), pp. 1941-1979.
*  J. Zhang, J. Tian, and E. Bareinboim. "Partial Counterfactual Identification from Observational and Experimental Data." In: (2021).

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_EMPTY:17]

We gave an example to illustrate the notation of a collection of intervention target sets \(\) and each intervention target set \(^{(k)}\).

**Example 7**.: _Let an intervention target collection be_

\[=\{^{(1)}=\{\{\}^{_{1}}\},^{(2)}=\{V_{1}^{ _{1},\{1\}}\},^{(3)}=\{V_{1}^{_{2},\{2\}},V_{2}^{_{2},\{1\}, }\},^{(4)}=\{V_{1}^{_{2},\{1\}},V_{2}^{_{2}, }\}\}\] (4)

_In words, \(\) indicates 4 different interventions \(=\{^{(k)}\}_{k=1}^{4}\):_

\(^{(1)}\)_; an idle intervention is applied resulting in an observational distribution in the domain_ \(^{1}\)_._

\(^{(2)}\)_; a soft intervention with mechanism_ \(\{1\}\) _is applied to_ \(V_{1}\) _in domain_ \(^{1}\)_._

\(^{(3)}\)_; an intervention is applied to_ \(V_{1}\) _and_ \(V_{2}\) _in domain_ \(^{2}\) _where the mechanism of_ \(V_{1}\) _is different from_ \(^{(2)}\) _and the intervention on_ \(V_{2}\) _is perfect._

\(^{(4)}\)_; an intervention is applied to_ \(V_{1}\) _and_ \(V_{2}\) _in domain_ \(^{2}\) _where the mechanism of_ \(V_{1}\) _is the same with_ \(^{(2)}\) _and the mechanism of_ \(V_{2}\) _is different from_ \(^{(3)}\)_._

\([^{(3)}]=\{V_{2}\}\) _means that_ \(^{(3)}\) _perfect intervenes on_ \(\{V_{2}\}\)_._

\(_{V_{2}}^{}=\{^{(3)},^{(4)}\}\) _means that the interventions targets that contain perfect interventions on_ \(V_{2}\)_._

\(_{\{\}}=\)_. Also, **the mechanism of \(V_{1}^{_{1},\{1\}}\) is different from the mechanism of \(V_{2}^{_{2},\{1\}}\)** since variables are different. _

### Assumptions and Remarks

In this paper, we make a few key assumptions about interventions and the differences in domains. We leverage many similar assumptions to the setting proposed in the literature related to causal representation learning, and handling of multiple domains and interventions . We discuss those assumptions and their implications here.

**Remark 1** (Mixing is invertible).: As a consequence of Def. 2.1, the mixing function \(f_{}\) is invertible, ensuring that latent variables are uniquely learnable .

The mapping from generative factors \(\) to high dimensional mixture \(\) is a one-to-one mapping. Consider images. In one direction, \(\) constructs the image through a mixing tool \(f_{}\) (such as a camera lens). In the reverse direction, these generative factors \(\) can be uniquely labeled through \(f_{}^{-1}\). We take images example in Sec. D.1 as an example. The generative factors \(Gender,Age\) and \(Haircolor\) are directly expressed through pixels in images. Given an image, the values of these generative factors are uniquely determined. This assumption is commonly used in non-linear ICA and representation learning literature .

**Remark 2** (Confounders are not part of the mixing function).: According to Def. 2.1, latent exogenous variables \(\) influence the high-dimensional mixture \(\) only through latent causal variables \(\), so unobserved confounding \(\) does not directly affect the mixing function.

An example of when this can occur in the real world is when modeling high-dimensional T1 MRI scans. Let the LCG comprise of Drug Treatment \(\) Outcome, but they are confounded by socioeconomic status (Drug Treatment \(\) Outcome). The drug treatment and outcome are assumed to be visually discernable on the MRI. However, socioeconomic status does not directly impact how the MRI appears, except through how it impacts the drug treatment efficacy or outcome. In addition, in EEG data, sleep quality and drug treatment may influence EEG appearance, while socioeconomic status may confound sleep and drug treatment but not directly affect EEG. This idea is also present in prior work, such as nonlinear ICA, where independent exogenous variables \(U_{i}\) each point to a single \(V_{i}\). .

**Remark 3** (Shared causal structure).: As a consequence of Def. 2.2, each environment's ASCM shares the same latent causal graph, with no structural changes among latent variables 16.

This means that the S-nodes will not represent structural changes such as when \(V_{i}\) has a different parent set across domains 17.

**Remark 4** (Mixing function is shared across all domains).: By Def. 2.1, the mixing function \(f_{}\) is the same for all ASCMs \(^{i}}\), enabling cross-domain analysis. If the mixing function varied across distributions, the latent representations would not be identifiable from iid data alone . 

Sharing of the mixing function is needed for the multi-domain setting because if everything may change across environments, the domains can only be analysed in isolation, and thus unable to leverage the changes (and similarities) across domains.

Assumptions for InterventionsWe discuss assumptions related to interventions here.

**Assumption 1** (Soft interventions without altering the causal structure).: _Interventions do not change the causal diagram. Hard interventions cut all incoming parent edges, and soft interventions preserve them . However, more general interventions may arbitrarily change the parent set for any given node . We do not consider such interventions and leave this general case for future work._

This assumption precludes any soft interventions that modify the graphical structure of the causal diagram. This work does allow both perfect interventions that cut all incoming parent edges, and soft interventions that preserve all parent edges. However, more general interventions may arbitrarily change the parent set for any given node . We do not consider such interventions and leave this general case for future work. Note Assumption 1 does not mean that interventions cannot occur with the same mechanism across domains. For example, consider two hospitals \(^{1}\) and \(^{2}\). Treating epilepsy in each of these hospitals can have outcomes that differ vastly due to the differences in domains . This is represented graphically in \(G^{S}\) with \(S^{1,2}\) outcome. However, if a neurologist who controls every aspect of his treatment procedure treats patients in both hospitals herself for the purposes of an experiment, then the outcomes will not differ in distribution. This is represented graphically as \(S^{1,2}\) outcome with the S-node being removed from the "outcome" variable. Thus if a pair of interventions occurring in different domains are deemed to have the same mechanism, then the S-node (if one is pointing to the intervened variable) is removed when comparing these two distributions.

Another assumption we make is that all interventions have known targets.

**Assumption 2** (Known-target interventions).: _All interventions occur with known targets, reducing permutation indeterminacy for intervened variables._

That is, for each interventional distribution we have, we know the interventions that occurred and at which node(s) they occurred. This assumption allows us to reduce the permutation indeterminacy that would arise if we did not know the intervention targets. In this work, we also are not concerned with permutation indeterminacy for variables we do not necessarily intervene on because we will mostly be concerned with disentanglement wrt the intervened variables (see Appendix A.4). It would be interesting for future work to consider unknown intervention targets.

Assumptions for DistributionsIn Sec. 2, we discuss that each distribution resulting from an intervention is sufficiently distinct from another distribution Assumption 4. Here we formally define and illustrate what is "change sufficiently".

**Assumption 4** (Changing Sufficiently).: _Consider a collection of ASCMs \(}\) and a set of distribution \(\) induced by \(}\) from a collection of interventions \(\). Let the LSD induced by \(}\) be \(G^{S}\). Let \(_{}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\} \) be any collection of distributions such that \(=do[^{(a_{0})}] do[^{(a_{l})}]\) for \(l[L]\), meaning for the baseline distribution all perfect interventions must be exactly on \(\), and all other distributions must at least contain \(\) in their perfect interventions. Let \(=_{l[L]}[^{(a_{l})},^ {(0)},,G^{S}]\) (Def. 3.1). It is assumed:_

1. _The probability density function of_ \(\) _is smooth and positive, i.e._ \(p_{}^{(a_{l})}()\) _is smooth and_ \(p_{}^{(a_{l})}()>0\) _almost everywhere._
2. _First-order discrepancy. If there exists_ \(\{a_{1}^{},,a_{||}^{}\}\{a_{1},,a_{L}\}\) _such that_ \(\;\;V_{q},V_{q}[^{(a_{q}^{ })},^{(a_{0})},,G^{S}]\)_, then_ \(\{_{1}(,a_{1}),_{1}(,a_{2}),, _{1}(,a_{L})\}\) _are linearly independent, where_ \[_{1}(,a_{l})=(}^{(a_{l})}()- p_{}^{(a_{0})}()}{  v_{q}})_{V_{q}}\] (5)3. _Second-order discrepancy. Let a set_ \(}\) _consist of pairs of_ \((V_{p},V_{q})\) _such that_ \((V_{p},V_{q})\) _appears at least in one_ \(\) _and_ \(V_{p}\) _is connected with_ \(V_{q}\) _conditioning on_ \(\{V_{p},V_{q}\}\) _in_ \(G_{}()\)_. Namely,_ \[}= \{_{j}=\{V_{k},V_{r}\}\] (6) \[&\  a_{l},\{V_{p},V_{q}\} ^{(a_{l}),(a_{0})};\\ &\ \ V_{p}\ \ V_{q}\ \ ^{tar}\{V_{p},V_{q}\}\ \ G_{}}(^{tar})\},\] _If there exists_ \(\{a_{1}^{},,a_{2||+|}|}^{}\} \{a_{1},,a_{L}\}\) _such that_ \( V_{q},V_{q}^{(a_{l}^{}),(a_ {0})}],V_{q}^{(a_{l}^{}],(a_{0})}],V_{q} ^{(a_{l}^{}],(a_{0})}\) _and for all_ \(_{j}},_{j}^{(a_{2||+j}^{}),(a_{0})}\)_, then_ \(\{_{2}(,a_{1}),_{2}(,a_{2}),, _{2}(,a_{L})\}\) _are linearly independent, where_ \[_{2}(,a_{l})= (}^{(a_{l})}()-  p_{}^{(a_{0})}()}{ v_{q}})_{V_{q}},\] \[( p_{}^{(a_{l})}( )- p_{}^{(a_{0})}()}{ v_{q}^{2}})_{V_{q} },\] \[( p_{}^{(a_{l})}()- p_{}^{(a_{0})}()}{ v_{p}v_{q}})_{(V_{p},V_ {q})(G_{}())}\] (7)

At a high level, this assumption will be naturally satisfied if the ASCMs and interventions are randomly chosen and only will be violated if the probability density of \(P^{(j)}\) and \(P^{(k)}\) are fine-tuned to each other . This kind of assumption is generally included in the causal representation learning literature, such as the "genericity" assumption , the "interventional discrepancy" assumption , and the "sufficient changes" assumption .

To illustrate, the assumptions contain two linear independence constraints. Specifically, the first-order and second-order partial derivatives of the log discrepancy from \(P^{(a_{l})}\) to \(P^{(a_{0})}\) should be independent of each other. Specifically, The two conditions are made because of necessity, since the linear independence constraints can hold only if these conditions hold. The following example illustrates the necessity of the first-order condition:

**Example 8** (Distributions do not change sufficiently).: _Consider \(\) obtained after comparisons as_

\[^{(1),(0)}=\{V_{1}\},^{(2),(0)}=\{V_{1}\}, ^{(1),(0)}=\{V_{1},V_{2},V_{3}\},\] (8)

_Let \(=\{V_{1},V_{2},V_{3}\}\). We have_

\[}^{(1)}()- p_{}^{(0)}()}{ v_{2}}=0\] (9)

_Since \(V_{2}^{(1),(0)}\). Similarly, we know_

\[_{1}(v_{1},v_{2},v_{3},1) =(}^{(1)}()- p_{ }^{(0)}()}{ v_{1}},0,0)\] \[_{1}(v_{1},v_{2},v_{3},2) =(}^{(2)}()- p_{ }^{(0)}()}{ v_{1}},0,0)\] \[_{1}(v_{1},v_{2},v_{3},3) =(}^{(3)}()- p_{ }^{(0)}()}{ v_{1}},}^{(3)}()- p_{}^{(0)}()}{ v_{2}}, }^{(3)}()- p_{}^{(0)}( )}{ v_{3}})\] (10)

_And this implies \(_{1}(v_{1},v_{2},v_{3},1),_{1}(v_{1},v_{2},v_{3},2), {}_{1}(v_{1},v_{2},v_{3},3)\) are for sure not linearly independent. _

On the other perspective, violating these assumptions is like stating the probability densities are fine-tuned to each other . Here we give an example of how this assumption can be violated.

**Example 9** (Distributions do not change sufficiently).: _Consider intervention targets_

\[=\{^{(1)}=\{\{\}^{_{1}}\},^{(2)}=\{V_{1}^{ _{1},\{1\}}\},^{(3)}=\{V_{2}^{_{1},\{2\}}\},^{(4)}=\{V_{1 }^{_{1},\{1\}},V_{2}^{_{1},\{2\}}\}\}\] (11)

_Choosing \(^{(1)}\) as the baseline, \(=\{\}\). The corresponding \(\) sets are \(\{\{V_{1}\},\{V_{2}\},\{V_{1},V_{2}\}\}\). Let \(\) be the union of \(\) sets, which is \(\{V_{1},V_{2}\}\). One can verify_

\[_{1}(,2)+_{1}(,3)=_{1}( ,4)\] (12)

_since \(^{(4)}\) is designed as a combination of \(^{(2)}\) and \(^{(3)}\). _

We provide the following Lemma to justify Assumption 4 formally.

**Lemma 1**.: _Assumption 4 almost surely holds. _

### Domains vs Interventions

In previous studies, there has been a tendency to conflate the notions of interventions and domain shifts . However, it is essential to recognize their distinctiveness, particularly when considering various real-world examples spanning different scientific domains that utilize observational and interventional data. The differentiation between interventions and domains is not only conceptually significant but also holds implications for causal inference and the characterization of corresponding causal structures as noted by . Moreover, it is crucial to avoid conflating these qualitatively distinct concepts of interventions and domains, as highlighted in transportability analysis . Pearl and Bareinboim have introduced clear semantics for (S) nodes (environments), presenting a unified representation in the form of selection diagrams .

By recognizing these differences, this work leverages any combination of observational and/or interventional data arising from multiple domains to present a general approach to disentanglement learning compared to prior work (see Table S1). Prior work generally considered either interventions in a single domain (top row in \(^{1}\)), where there must be an intervention per latent variable , or observational distributions from many domains \(^{1},^{2},...,^{N}\) (first column under "Observational"). However, this paper considers a general setting where we may have an arbitrary collection of interventions, or observations from any combination of domains (green section).

Here, we illustrate some examples of the CRID algorithm using distributions from multiple domains.

**Example 10** (Example illustrating CRID with domains).: _Consider the LSD shown in Fig. 4(a). We have the following distributions \(=\{P^{(1)},P^{(2)}\}=\{P^{_{1}}(),P^{_{2}}()\) from interventions \(=\{^{(1)},^{(2)}\}=\{\{\},\{\}\}\). Applying CRID algorithm, we can determine that \(V_{1}\) is ID wrt \(V_{2}\) and \(V_{3}\). _

This example illustrates that observational data in two domains can help disentangle a root variable (\(V_{1}\)) from all its descendants.

**Example 11** (Example illustrating CRID with interventions across domains with different mechanisms).: _Consider the LSD shown in Fig. 4(a). We have the following distributions \(=\{P^{(1)},P^{(2)}\}=\{P^{_{1}}(),P^{_{2}}()\) from interventions \(=\{^{(1)},^{(2)}\}\) with targets \(=\{\{V_{2}\}^{_{1}},\{\}^{_{2}}\). Applying CRID algorithm, we can determine that \(V_{2}\) and \(V_{1}\) is ID wrt \(V_{3}\). _

This example demonstrates that when comparing observational data from domain \(_{1}\) with interventional data from a different domain \(_{2}\), the only invariant factor is \(P(V_{3}|V_{2})\), with \( V[\{\{V_{2}\}^{_{1}},\{\}^{_{2}},G^{S}]=\{V_{1},V_{2}\}\). The canceled variable is \(V_{3}\), and thus we achieve our identifiability result.

**Example 12** (Example illustrating CRID with interventions across domains with the same mechanisms).: _Consider the LSD shown in Fig. 4(a). We have the following distributions \(=\{P^{(1)},P^{(2)},P^{(3)}\}\) from interventions \(=\{^{(1)},^{(2)},^{(3)}\}\) with targets \(=\{\{V_{1}^{[i]},V_{2}\}^{_{1}},\{\}^{_{2}},\{V_{1}^{[i]} _{2}}\). Applying CRID algorithm, we can determine that \(V_{1}\) is ID wrt \(\{V_{2},V_{3}\}\), and \(V_{2}\) is ID wrt \(\{V_{3}\}\). _

Even with an intervention that changes both \(V_{1},V_{2}\). When comparing the distributions \(P^{(1)}\) and \(P^{(3)}\), the \(P(V_{1})\) term becomes an invariant factor because the intervention has the same mechanism. This removes the possible difference encoded by the S-node on \(V_{1}\) between domains \(^{1},^{2}\).

These examples further demonstrates the importance of distinguishing domains and interventions because a difference in mechanism is present when comparing all distributions between a pair of domains, \(_{i}_{j}\). This in principle, results in additional variables in the \(\) set. However, interventions may allow us to remove variables from this set by increasing the number of invariant factors.

### Permutation Indeterminancy

In the context of causal representation learning, permutation indeterminacy is a significant challenge that arises when attempting to identify latent variables from observed data. This phenomenon occurs when the ordering of latent variables is not uniquely determined, leading to multiple equivalent representations (i.e. permutations of the latent variables) that can explain the observed data equally well.

In the earliest results of disentangled representation learning, linear ICA was known to be identifiable only up to permutation and scaling indeterminacies . Permutation indeterminacy is still present in nonlinear ICA , since the independent components may be permuted arbitrarily.

Interestingly, when generalizing the problem to the Markovian setting where latent variables have causal structure (i.e. edges in a causal graph), permutation indeterminacy can be reduced to a graph isomorphism in certain cases. That is, latent variables are exchangeable with other latent variables that preserve the topological ordering of the latent causal graph (rather than permuted with any arbitrary latent variable) . When the interventions occur with known targets on the latent space, and intervention occurs uniquely on every latent variable, then there is no permutation indeterminacy .

In this work, we assume intervention targets are known, but do not necessarily occur on all latent variables, and they may occur on multiple variables at once. For variables that are intervened on uniquely (i.e. one intervention applied on only that variable), there is no permutation ambiguity. For variables that are intervened on in groups, or not intervened on at all, there still exists permutation ambiguity:

1. (Grouped variables) These variables are all intervened on in the same group. In the context of our paper, these variables are consistently in the same \(\) set. For example, consider the following LCG \(V_{1} V_{2} V_{3}\). If we have distributions arising only from interventions on \(\{V_{1},V_{3}\}\) and the observational distribution, and assume the learned representation is fully disentangled, then the learned representation still has a permutation indeterminacy wrt \(\{V_{1},V_{3}\}\). That is, \(_{1}\) could be the representation for \(V_{1}\), or \(V_{3}\) and similarly for \(_{3}\) (See why permutation can hold for details in Example 18).
2. (Non-intervened variables) These variables do not contain any interventions. Then there is still permutation ambiguity among these variables. However, instead of a graph isomorphism ambiguity, these variables form a subgraph isomorphism problem because there may be othervariables that change across distributions (i.e. via interventions, or changes in domains), which are not permutable with respect to these invariant variables.

Specifically, the identifiability we talk about (Def. 2.3) is considered after a subgraph isomorphism permutation. For example, in the collider example setting where permutation can happen between \(V_{1}\) and \(V_{3}\). The "\(V_{1}\) is ID w.r.t \(\{V_{2},V_{3}\}\)" should implies there exists a function \(\) such that \(()[V_{1}]=(()[V_{1}])\), where \(()[V_{i}]\) means variable \(V_{i}\) after the permutation on \(\) and \(\) denotes a permutation only in this text. In our paper, we are primarily concerned with disentanglement and determining if the learned representation is disentangled in some general sense, and the permutation part is out of our scope.

## Appendix B CRID Algorithm Details

Here, we provide additional pseudocode for the CRID Alg. 1.

First, the following algorithm illustrates how to initialize a fully connected bipartite graph \(G_{,}}\). In the initial \(G_{,}}\), the true underlying factors \(\) points to representations each \(_{i}}\), which means each variable \(V_{i}\) is entangled with all other variables.

```
0:\(,}\)
0:\(G_{,}}\)
1: Initialize an empty graph \(G_{,}}\)
2:for\(V_{i}\) in \(\)do
3:for\(V_{j}\) in \(}\)do
4: Add edge \((V_{i},V_{j})\) to \(G_{,}}\) ```

**Algorithm F.2** **FullyConnectedBipartiteGraph: Initialization step** - Initialize a fully connected bipartite graph.

Then, after constructing \(Q\) from comparisons of distributions, the Alg. F.3 illustrates the details to check whether \(\) can be disentangled from \(\) according to Proposition 3. To illustrate, each variable \(Z\) is checked one by one. The variables that have already been disentangled from \(Z\) are collected in the list \(\) through procedure **CheckMemoize**. Next, check if there is a sub-collection of \(\) that satisfy the [1-3] conditions in Proposition 3. The checking procedure is shown in Alg.F.5. If conditions are satisfied the edges from \(Z\) to \(}\) are removed to demonstrate disentanglement. Based on the Lemma 2, the condition  in Prop. 3 can be reduced to a weaker condition  leveraging existing disentanglements in CDM.

**Lemma 2**.: _Consider variables \(^{tar}\) and \(Z^{tar}\). Suppose \(=\{V_{j}^{tar} V_{j}\) is ID w.r.t. \(Z\}\). Consider, \(_{}\) and its corresponding intervention targets that hold conditions [1-2] in Prop. 3. If the new version of the condition  is also satisfied:_

_[_4_]_ _there exists_ \(\{a^{}_{1},,a^{}_{|^{tar}|}\}\{a_{1}, ,a_{L}\}\) _such that for all_ \(V^{tar}_{i}^{tar},V^{tar}_{i} [^{(a^{}_{i})},^{(a_{0})},,G^{S}]\)_._

_then \(^{tar}\) is ID w.r.t \(Z\). \(\)_

To illustrate, the above lemma indicates not all variables in \(^{tar}\) needed to be covered uniquely. Variables that have been already disentangled (in \(\)) do not need to be considered.

**Example 13**.: _Consider the LSD \(G^{S}\) and intervention targets \(^{(1)}=\{\}\) and \(^{(4)}=\{V^{_{1},do}\}\). Comparing \(^{(4)}\) and \(^{(1)}\) taking \(=\{\}\), \(=\{V_{1},V_{2}\}\). Based on Prop. 3, we cannot get \(V_{2}\) is ID w.r.t \(V_{3}\) since to cover \(V_{1}\) and \(V_{2}\) separately, at least two \(\) sets are needed._

_Now assume it is known that \(V_{1}\) is ID w.r.t. \(V_{3}\), namely \(=\{V_{1}\}\). \(\) sets only need to cover \(V_{2}\) and does not need to cover \(V_{1}\) from condition  in Lemma 2. Then \(V_{2}\) is ID w.r.t. \(V_{3}\). \(\)_

**Algorithm F.3**\(\)fromCancel - Check whether canceled variables \(\) can be disentangled from the LQ factors \(\). \(G_{,}}\) is the current bipartite graph; \(G_{}\) is the LCG after the perfect intervention on \(\); \(_{}^{}\) is the intervened sets that contains perfect interventions on \(\); \(_{}^{}\) is the chosen baseline distribution; \(\) is the collection of \(\) sets after comparing intervention targets \(_{}^{}\) with the baseline.

**Input:**\(,G_{,}},G_{},_{ }^{},,\)

**Output:**\(G_{,}}\)

```
1:for all \(Z\)do
2:\( CheckMemoize(G_{,}},Z, )\)\(\) Variables in \(\) has been already ID w.r.t. \(Z\).
3:if\(CheckConsion3(,,)\)then\(\) Check conditions in Prop. 3 and Lem. 3
4: remove edge \(Z}\) in \(G_{,}}\)
5:return\(G_{,}}\) ```

**Algorithm F.4**\(\): Memoization step - The variables in \(\) is ID w.r.t \(Z\) already.

**Input:**\(G_{V,}\), \(Z\), \(\)

**Output:**\(\)

```
1:\(\{\}\)
2:for all \(\)do
3:if\(Z G_{,}}\)then
4:\(.append(V)\)
5:returnMem ```

**Algorithm F.5**\(\): Check conditions in Proposition 3 and Lemma 2. \(\) is the collection of \(\) sets; \(\) are target variables;**Mem** are variables in \(\) have already been disentangled.

**Input:**\(\), \(\), \(\)

**Output:**\(True\) or \(False\)

```
1:\(\{\}\)
2:for\(_{k}\)do
3:if\(_{k}\)then
4:\(.append(_{k})\)
5:\(^{re}=\{Q_{1},,Q_{d^{}}\} \), \(d^{}|^{re}|\)
6:if\(Q_{1}_{1},Q_{2}_{2},,Q_{d^{}} _{d^{}}\) after a permutation of \(\)then
7:return\(True\)
8:return\(False\) ```

**Algorithm F.6**\(\): Check conditions in Proposition 3 and Lemma 2. \(\) is the collection of \(\) sets; \(\) are target variables;**Mem** are variables in \(\) have already been disentangled.

**Output:**\(True\) or \(False\)

**Output:**\(True\) or \(False\)

**Output:**\(True\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(True\)

**Output:**\(False\)

**Output:**\(True\)

**Output:**\(True\)

**Algorithm F.6**: **DisWithin\(\)** - Check the disentanglement of variables within \(\). \(G_{,}}\) is the current bipartite graph; \(G_{}\) is the LCG after the perfect intervention on \(\); \(_{}^{}\) is the intervened sets that contains perfect interventions on \(\); \(_{}^{}\) is the chosen baseline distribution; \(\) is the collection of \(\) sets after comparing intervention targets \(_{}^{}\) with the baseline.

**Input:**\(,G_{,}},G_{},_{ }^{},,\)
**Output:**\(G_{,}}\)

```
1:for all pair \(V_{i},V_{j}\)do
2:if\(V_{i} V_{j}\{V_{i},V_{j}\}\)then
3:\(_{i} CheckMemoize(G_{,}},V_ {i},)\)\(\) Variables in \(\) is ID w.r.t \(V_{i}\) already.
4:\(_{j} CheckMemoize(G_{,}},V_ {j},)\)\(\) Variables in \(\) is ID w.r.t \(V_{j}\) already.
5:if\(CheckConsition4(,,_{i},_{j},G_{ })\)then\(\) Check conditions in Prop. 4 and Lem. 3
6: remove edge \(Z}\) in \(G_{,}}\)
7:return\(G_{,}}\) ```

**Algorithm F.6**: **DisWithin\(\)** - Check the disentanglement of variables within \(\). \(G_{,}}\) is the current bipartite graph; \(G_{}\) is the LCG after the perfect intervention on \(\); \(_{}^{}\) is the intervened sets that contains perfect interventions on \(\); \(_{}^{}\) is the chosen baseline distribution; \(\) is the collection of \(\) sets after comparing intervention targets \(_{}^{}\) with the baseline.

Next, the Alg. F.6 illustrates the details to check whether \(V_{i},V_{j}\) such that \(V_{i}\) and \(V_{j}\) are independent of each other conditioning on other variables in \(\) can be disentangled according to Proposition 4. To illustrate, two lists of variables that have already been disentangled from \(V_{i}\) and \(V_{j}\) are constructed as \(_{i}\) and \(_{j}\) respectively through **CheckMemoize**. Next, check if there is a sub-collection of \(\) that satisfy the [1-3] conditions in Proposition 3. The checking procedure is shown in Alg.F.7. If conditions are satisfied the edges from \(Z\) to \(}\) are removed to demonstrate disentanglement. Based on the Lem. 3, the condition [3'] in Prop. 4 can be reduced to a weaker condition [4'] leveraging existing disentanglements in CDM.

**Lemma 3** (ID of variables within \(\) sets).: _Consider variables \(^{tar}\). For any pair of \(V_{i},V_{j}^{tar}\) such that \(V_{i} V_{j}|^{tar}\{V_{i},V_{j}\}\) in \(G_{}(^{tar})\), let \(_{i}\) be a list of variables in \(\) that have been ID w.r.t. \(V_{i}\) and let \(_{j}\) be a list of qvariables in \(\) that have been ID w.r.t. \(V_{j}\). If there exists \(_{}\) that satisfies conditions [1-2] in Prop. 3 and the following condition [4']._

* _[_(Enough changes occur across distributions)_]_ _Let_ \(^{re}=^{tar}(_{i}_{j})\) _and_ \(d^{}=|^{re}|\)_. And_ \[_{ij}=&\{_{j}=\{V_{k},V_{r}\}\  a_{l},\{V_{k},V_{r}\}^{(a_{l}),(a_{0})};\\ &\ V_{k}V_{r}\ ^{tar}\{V_{k},V_{r}\}G_{}(^{tar})\\ &\ V_{k},V_{r}_{i}_{j} \] (13) _there exists_ \(\{a_{1}^{},,a_{2d^{}+||}^{}\} \{a_{1},,a_{L}\}\) _such that for all_ \(Q_{i}^{re},Q_{i}^{(a_{l}^{}),(a_{0})}],Q_{i} ^{(a_{l^{}+),l},(a_{0})}\) _and for all_ \(_{l}_{ij},_{l} ^{(a_{2d^{}+l}^{}),(a_{0})}\)_._

_, then \(V_{i}\) is ID w.r.t \(V_{j}\). \(\)_Lastly, we leverage the independence and current disentangled results stored in \(G_{,}}\). Canceled variables with \(\) can be disentangled with each other according to Proposition 5. The following algorithm illustrates this step.

```
0:\(,G_{,}},G_{}}\)
0:\(G_{,}}\)
1:\(G_{,}}\)
2:for all \(Z\) such that \(Z Z\) in \(G_{}}\)do
3:if there are no edges from \( Z\) to \(\)then
4: remove edges from \(Z\) to \( Z\)
5:return\(G_{,}}\) ```

**Algorithm F.7** **CheckCondition4**: Check conditions in Proposition 4 and 3. \(\) is the collection of \(\) sets; \(\) are target variables;\(_{i}\) are variables in \(\) have already been disentangled with \(V_{i}\);\(_{j}\) are variables in \(\) have already been disentangled with \(V_{j}\); \(G_{}}\) is the diagram after removing incoming edge to \(\).

```
0:\(\), \(\), \(_{i}\), \(_{j}\), \(G_{}}\)
0:\(True\) or \(False\)
1:\(\{\}\)
2:for\(_{k}\)do
3:if\(_{k}\)then
4:\(.append(_{k})\)
5:\(\{\}\)
6:for\(\{V_{k},V_{r}\}\)do
7:if (i) \( L\) such that \(\{V_{k},V_{r}\} L\) (ii) \(V_{k}\) is conditionally connected to \(V_{l}\) (iii) \(\{V_{k},V_{r}\}_{i}\) ;\(_{j}\)then
8:\(.append((V_{k},V_{r}))\)\(\) Construct \(\) according to Lem. 3
9:\(^{re+}=\{Q_{1},,Q_{d^{}}\}(( _{i}_{j}))\), \(d^{+}|^{re}|\)
10:if\(Q_{1}_{1},Q_{2}_{2},,Q_{d^{}} _{d^{}}\) after a permutation of \(\)then
11:return\(True\)
12:return\(False\) ```

**Algorithm F.8** **Dis\(\)QFromCancel** - Disentangle canceled variables from \(\). \(G_{,}}\) is the current bipartite graph; \(G_{}}\) is the LCG after the perfect intervention on \(\).

```
0:\(,G_{,}},G_{}}\)
0:\(G_{,}}\)
1:for all \(Z\) such that \(Z Z\) in \(G_{}}\)do
2:if there are no edges from \( Z\) to \(\)then
3: remove edges from \(Z\) to \( Z\)
4:return\(G_{,}}\) ```

**Algorithm F.9** **Dis\(\)QFromCancel** - Disentangle canceled variables from \(\). \(G_{,}}\) is the current bipartite graph; \(G_{}}\) is the LCG after the perfect intervention on \(\).
Proofs

Here, we provide detailed proofs of theoretical results in the main paper.

### "Distribution Change Sufficiently" - Proof of Lemma 1

We assume "distributions changes sufficiently" in Sec. 2. This assumption is formally defined in Assumption 4 and will be used as a technique assumption in the proof of propositions in this work. Lemma 1 provides the justification of this assumption. It suggests Assumption 4 almost surely holds. We first provide proof here.

**Assumption 4** (Changing Sufficiently).: _Consider a collection of ASCMs \(}\) and a set of distribution \(\) induced by \(}\) from a collection of interventions \(\). Let the LSD induced by \(}\) be \(G^{S}\). Let \(_{}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\} \) be any collection of distributions such that \(=do[^{(a_{0})}] do[^{(a_{l})}]\) for \(l[L]\), meaning for the baseline distribution all perfect interventions must be exactly on \(\), and all other distributions must at least contain \(\) in their perfect interventions. Let \(=_{l[L]}[^{(a_{l})},^{(0)},,G^{S}]\) (Def. 3.1). It is assumed:_

1. _The probability density function of_ \(\) _is smooth and positive, i.e._ \(p_{}^{(a_{l})}()\) _is smooth and_ \(p_{}^{(a_{l})}()>0\) _almost everywhere._
2. _First-order discrepancy. If there exists_ \(\{a_{1}^{},,a_{||}^{}\}\{a_{1},,a_{L}\}\) _such that_ \(\;\;V_{q},V_{q}[^{(a_{q}^{ })},^{(a_{0})},,G^{S}]\)_, then_ \(\{_{1}(,a_{1}),_{1}(,a_{2}),, _{1}(,a_{L})\}\) _are linearly independent, where_ \[_{1}(,a_{l})=(}^{(a_{l})}()- p_{}^{(a_{0})}()}{  v_{q}})_{V_{q}}\] (5)
3. _Second-order discrepancy. Let a set_ \(}\) _consist of pairs of_ \((V_{p},V_{q})\) _such that_ \((V_{p},V_{q})\) _appears at least in one_ \(\) _and_ \(V_{p}\) _is connected with_ \(V_{q}\) _conditioning on_ \(\{V_{p},V_{q}\}\) _in_ \(G_{}()\)_. Namely,_ \[}=&\{_{j}= \{V_{k},V_{r}\}\\ &(i)\; a_{l},\{V_{p},V_{q}\}^{(a_{l}),( a_{0})};\\ &(ii)\;V_{p}V_{q}^{tar}\{V_{p},V_{q}\}G_{}}(^{tar})\},\] (6) _If there exists_ \(\{a_{1}^{},,a_{||+|}|}^{}\}\{a_ {1},,a_{L}\}\) _such that_ \(\;\;V_{q},V_{q}^{(a_{l}^{}),(a_{0} )}],V_{q}^{(a_{l}^{}),(a_{0})}\) _and for all_ \(_{j}},_{j} ^{(a_{2}^{}|_{1}+j),(a_{0})}\)_, then_ \(\{_{2}(,a_{1}),_{2}(,a_{2}),, _{2}(,a_{L})\}\) _are linearly independent, where_ \[_{2}(,a_{l})= (}^{(a_{l})}()-  p_{}^{(a_{0})}()}{ v_{q}})_{V_{q} },\] \[( p_{}^{(a_{l})}( )- p_{}^{(a_{0})}()}{ v_{p}v_{q}})_{(V_{p},V_{ q})(G_{}())}\] (7)

**Lemma 1**.: _Assumption 4 almost surely holds. _

Proof.: We will prove the first-order discrepancy and second-order discrepancy almost surely hold, which means the situations where first-order discrepancy and second-order discrepancy do not hold have Lebesgue measure 0.

We first consider the first-order discrepancy. Denote \(\{_{1}(,a_{1}),_{1}(,a_{2}),, _{1}(,a_{L})\}\) as \(\). And every entry in \(\) is

\[a_{lq}=}^{(a_{l})}()- p_{ }^{(a_{0})}()}{ v_{q}}\] (14)According to Eq. (3), we know \( p_{}^{(a_{l})}()- p_{}^{(a_{0})}()\) is a function of only variables in \(\). Thus, if \(V_{q}[^{(a_{l}^{})},^{(a_{0}) },,G^{S}]\), \(a_{lg}=0\); if \(V_{q}[^{(a_{l}^{})},^{(a_{0})}, ,G^{S}]\), we assume \(a_{lg}\) follows a standard normal distribution, which means the non-zero entries in matrix \(\) are randomly sampled and are not fine-tuned. Thus, to prove this lemma, it is equivalent to prove if there exists \(\{a_{1}^{},,a_{||}^{}\}\{a_{1},,a_ {L}\}\) such that \(\;\;V_{q},V_{q}[^{(a_{q}^{ })},^{(a_{0})},,G^{S}]\), the row of \(\) are almost surely linear independent. W.O.L.G, we let \(\{a_{1}^{}=a_{1},,a_{}^{}=a_{L}\). Then, it is equivalent to prove that \(\) is a full rank matrix.

In order to prove that \(\) is a full-rank matrix, we prove that the determinant of \(\) is almost surely non-zero. Since \(\;\;V_{q},V_{q}[^{(a_{q})}, ^{(a_{0})},,G^{S}]\), there exists \(\) such that \(()\) is non-zero, and then \(()\) is non-trivial. Based on a simple algebraic lemma in , the subset of \(\{\;|\;()=0\}\) of the real space has Lebesgue measure 0. Then \(()=0\) almost surely holds.

The second-order discrepancy proof is similar. Denote \(\{_{2}(,a_{1}),_{2}(,a_{2}),, _{2}(,a_{L})\}\) as \(\). And every entry of \(\) is

\[a_{lg} =}^{(a_{l})}()- p_{ }^{(a_{0})}()}{ v_{q}},q||\] \[a_{lg} = p_{}^{(a_{l})}()- p _{}^{(a_{0})}()}{ v_{q}^{2}},||+1 q  2||\] (15) \[a_{l} = p_{}^{(a_{l})}()- p _{}^{(a_{0})}()}{ v_{p} v_{q}},2||+1 2||+|}|,\{V_{p},V_{q}\}}\]

According to Eq. (3), we know \( p_{}^{(a_{l})}()- p_{}^{(a_{0})}( )\) is a function of only variables in \(\). Thus, if \(V_{q}[^{(a_{l}^{})},^{(a_{ 0})},,G^{S}]\), \(a_{lg}=0\); if \(V_{q}[^{(a_{l}^{})},^{(a_{0})}, ,G^{S}]\), we assume \(a_{lg}\) follows a standard normal distribution, which means the non-zero entries in matrix \(\) are randomly sampled and are not fine-tuned. If \(\{V_{p},V_{q}\}[^{(a_{l}^{})}, ^{(a_{0})},,G^{S}]\), \(a_{lg}=0\); if \(\{V_{p},V_{q}\}[^{(a_{l}^{})}, ^{(a_{0})},,G^{S}]\), we assume \(a_{lg}\) follows a standard normal distribution, which means the non-zero entries in matrix \(\) are randomly sampled and are not fine-tuned. Following the same discussion above, the subset of \(\{\;|\;()=0\}\) of the real space has Lebesgue measure 0. Then \(()=0\) almost surely holds. 

### Distribution comparison - Proof of Proposition 1

**Proposition 1** (Distribution Comparison).: _Consider a pair of collections ASCMs \(}\) and \(}}\) that matches with the distribution \(\) resulting from interventions \(\) and LSD \(G^{S}\). Consider two distributions \(P^{^{(j)}}(;^{(j)})\) and \(P^{^{(k)}}(;^{(k)})\). Suppose \(()\) is in both intervention sets, then,_

\[_{i}^{d} p_{}^{(j)}(v_{i}\;|\;_{i}^{+})- p_{}^{(k)}(v_{i}\;|\;_{i}^{+})=_{i}^{d} p_{ }^{(j)}(_{i}\;|\;}_{i}^{+})-  p_{}^{(k)}(_{i}\;|\;}_{i}^{ +}),\] (2)

_where \(p_{}^{(j)}()\) and \(p_{}^{(k)}()\) are density functions. _

Proof.: According to the ASCM definition Def.2.1, the mapping from \(\) to \(\), and the mapping \(\) to \(}\) can be expressed as:

\[}=_{}^{-1}()=_{ }^{-1}(f_{}())\] (16)

Then based on the change variable formula, we have

\[p()=p(})|_{}|\] (17)

where \(=_{}^{-1} f_{}\) and \(_{}\) is the Jacobian matrix of \(\). Leveraging the factorization in Eq. 1 and taking log of the above equation,

\[_{i=1}^{d} p_{}(v_{i}\;|\;^{+})=_{i=1}^{ d} p_{}(_{i}\;|\;}^{+})+| _{}|\] (18)

Subtract the above factorization of density function induced by \(^{(j)}\) and \(^{(k)}\), and we have Eq.( 2).

Eq 2 naturally gives a connection from \(\) to \(}\). Comparing two factorization for Fig. 4(c), the connection connections are made from \(P(v_{1}),p(v_{2} v_{1}),p(v_{3} v_{2},v_{1}),P(v_{4} v_{3})\) or \(P(v_{1}),p(v_{3}),p(v_{2} v_{1},v_{3}),P(v_{4} v_{3})\).

### Invariant factors - Proof of Proposition 2

**Proposition 2** (Invariant Factors).: _Consider two distributions \(P^{(j)},P^{(k)}\) with intervention targets \(^{(j)}\) and \(^{(k)}\) containing \(()\). Construct the changed variable set \([^{(j)},^{(k)},G^{S}]\) (for short \(\)) with target sets \(^{(j)},^{(k)}\) as follows: (1) \(V_{l}\) if \(V_{l}^{_{l},\{b_{l}\},t_{l}}^{(j)}\) but \(V_{l}^{_{l}^{_{l}},\{b_{l}\},t_{l}}^{(k)}\), or vice versa; (2) \(V_{l}\) if i) \(S^{^{(1)},^{(k)}}\) point to \(V_{l}\) and ii) \(V_{l}^{_{l},\{b_{l}\},t_{l}}^{(j)}^{(k)}\). If \(V_{i}()\), then \(p_{}^{(j)}(v_{i}_{i}^{+})=p_{}^{( k)}(v_{i}_{i}^{+})\) (denoted invariant factors). _

Proof.: Consider an arbitrary order. Based on the proposition, \([^{(j)},^{(k)},G^{S}]\) includes all variables that the mechanism \(f_{V}\) or exogenous \(U\) possibly change when the intervention changes from \(^{(k)}\) to \(^{(j)}\). In other words, for any \(V_{l}[^{(j)},^{(k) },G^{S}]\), \(f_{V_{l}}\) and exogenous \(U_{l}\) are invariant.

Let \(V_{i}()\). \(=(V_{i})^{+}\). We have \(()\) according to the definition of C-component.

According to the definition of \(_{i}^{+}\), we know \(_{i}^{+}=(\{V_{i}\} )\). Now reconsider the distribution \(P^{^{(j)}}(V_{i}_{i}^{+};^{(j)})\) and \(P^{^{(k)}}(V_{i}_{i}^{+};^{(k)})\),

\[P_{}^{^{(j)}}(V_{i}_{i}^{+};^{(j)} )=P_{}^{^{(j)}}(V_{i},_{i}(\{V_{i}\} );^{(j)})/P_{}^{^{(j)}}( _{i}(\{V_{i}\});^{(j)})\]

\[P_{}^{^{(k)}}(V_{i}_{i}^{+};^{(k)} )=P_{}^{^{(k)}}(V_{i},_{i}(\{V_{i}\} );^{(k)})/P_{}^{^{(k)}}( _{i}(\{V_{i}\});^{(k)})\] (19)

Since the mechanism and exogenous variables of \(V_{i}\) and \(\) are invariant, both the nominators and denominators are the same. Namely,

\[P_{}^{^{(j)}}(V_{i},_{i}(\{V _{i}\});^{(j)}) =P_{}^{^{k}}(V_{i},_{i}(\{V _{i}\});^{(k)})\] (21) \[P_{}^{^{(j)}}(_{i}(\{V_{i} \});^{(j)}) =P_{}^{^{(k)}}(_{i}(\{V_{i} \});^{(k)})\] (22)

which implies the density functions are invariant,

\[p_{}^{(j)}(v_{i}_{i}^{+}) =p_{}^{(k)}(v_{i}_{i}^{+})\] (23)

### ID \(\) w.r.t Canceled Factors - Proof of Proposition 3 and Lemma 2

**Proposition 3** (ID the \(\) set w.r.t Canceled Variables).: _Consider variables \(^{tar}\). Let \(_{}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\} \) be a collection of distributions such that (1) \(\;\;l[L]\), \(=[^{(a_{0})}][^{(a_{l})}]\)18; (2) \(_{l[L]}[^{(a_{l})},^{(a_{0})}, ,G^{S}]=^{tar}\); (3) there exists \(\{a_{1}^{},,a_{d^{}}^{}\}\{a_{1},,a_{L}\}\) such that for all \(V_{i}^{tar}^{tar},V_{i}^{tar}[^{(a_{l} ^{})},^{(a_{0})},,G^{S}]\), where \(d^{}=|^{tar}|\). Then, \(^{tar}\) is ID w.r.t \(^{tar}\). _

Proof.: **We denote \(^{tar}\) as \(\) for convenience.** Notice that the Assumption 4 will be used in the proof.

[MISSING_PAGE_EMPTY:30]

**Proposition 6** (ID of variables within \(\) sets).: _Consider the variables \(^{tar}\). Define \(}\) as the set of edges within the Markov Network of \(G_{}}(^{tar})\) that are contained within a \(\) set._

\[}= \{_{j}=\{V_{k},V_{r}\}\] (29) \[\  a_{l},\{V_{k},V_{r}\} ^{(a_{l}),(a_{0})};\] \[\ V_{k}\ \ V_{r}\ \ ^{tar}\{V_{k},V_{r}\}\ \ G_{}}(^{tar})\},\]

_For any pair of \(V_{i},V_{j}^{tar}\) such that \(V_{i}\!\!\! V_{j}^{tar}\{V_{i},V_{j}\}\) in \(G_{}}(^{tar})\), if there exists \(_{}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\} \) that satisfies conditions (1-2) in Prop. 3 and the following condition (3')._

* _Enough changes occur across distributions, i.e., Formally, there exists_ \(\{a^{}_{1},,a^{}_{2d^{}+|}|}\} \{a_{1},,a_{L}\}\) _such that for all_ \(V^{tar}_{i}^{tar}\)_, i)_ \(V^{tar}_{i}^{(a^{}_{i}),(a_{0})}\)_, ii)_ \(V^{tar}_{i}^{(a^{}_{d^{}+i}),(a_{0})}\)_, and iii) for all_ \(_{j}},_{j} ^{(a^{}_{2d^{}+j}),(a_{0})}\)_, where_ \(d^{}=|^{tar}|\)__

_then, \(V_{i}\) is ID w.r.t. \(V_{j}\). _

Proof.: **We denote \(^{tar}\) as \(\) for convenience.** Notice that Assumption 4 will be used in the proof. From Eq. 3, we have

\[_{V_{i}}} p^{(a_{l})}_{}(v_{i }^{+}_{i})-p^{a_{0}}_{}(v_{i}^{+}_{i}) =_{V_{i}}} p^{(a_{l})}_{}( _{i}}^{+}_{i})- p^{(a_{0})}_ {}(_{i}}^{+}_{i})\] \[= p^{(a_{l})}_{}(})- p^{(a_{ 0})}_{}(})\] (30)

Notice that the left side only involves variables in \(=_{l[L]}[^{(a_{l})}, ^{(a_{0})},,G^{S}]\) based on the Def. 3.1.

We first argue that if \(V_{i}\!\!\! V_{j}|\{V_{i},V_{j}\}\) in \(G_{}}\) then \(V_{i}a^{+}_{j},V_{j}a^{+}_ {i}\) and \(V_{i},V_{j}a^{+}_{m}\) for any \(V_{m}\).

First, since \(V_{i}\!\!\! V_{j}|\{V_{i},V_{j}\}\), \(V_{i}\) and \(V_{j}\) cannot be directly connected by edges in \(G_{}}\), which implies \(V_{i}(V_{j})\) and \(V_{i}^{+}(V_{j})\). Also, the outgoing edge from \(V_{i}\) and \(V_{j}\) cannot point to the same C-component. Otherwise, the path is active from \(V_{i}\) and \(V_{j}\) is active when conditioning on other variables (collider structure). Thus, \(V_{i}a^{+}_{j},V_{j}a^{+}_ {i}\) and \(V_{i},V_{j}a^{+}_{k}\) where \(V_{k}\). This implies \(V_{i}\) and \(V_{j}\) will not appear to the same factor \(p^{(a_{l})}_{}(v_{m}^{+}_{m})\) for any \(V_{m}}\). Thus,

\[ p^{(a_{l})}_{}(v_{m}^{ +}_{m})}{ v_{i}v_{j}}=0\] (31)

Thus, for any pair of \(V_{k},V_{r}\) such that \(V_{k}\!\!\! V_{r}|\{V_{k},V_{r}\}\),

\[ l[L],&_{V_{m}}} p^{(a_{l})}_{}(_{m} ^{+}_{m})}{_{k}_{r}}\\ &= p^{(a_{l})}_{}(})- p^{(a_{0})}_{}(})}{ _{k}_{r}}=0\] (32)

On the other hand, when either \(V_{k}\) or \(V_{r}\) is in \(^{(a_{l}),(a_{0})}\) for \(l[L]\),

\[ l[L],= p^{(a_{l})}_{}(})- p^{(a_{0})}_{}(})}{ _{k}_{r}}=0\] (33)

since

\[)}_{}(})- p^{(a_{0})}_{ }(})}{_{k}}=0)}_{}(})- p^{(a_{0})}_{ }(})}{_{r}}=0\] (34)

[MISSING_PAGE_FAIL:32]

, then \(V_{i}\) is ID w.r.t \(V_{j}\). 

Proof.: The unknown in the linear system

\[_{q}}{ v_{i}}_{q}}{  v_{j}}=0,\] (39)

if \(V_{p}\) is ID w.r.t \(V_{i}\) or \(V_{q}\) is ID w.r.t \(V_{j}\).

\[_{q}}{ v_{i}v_{j}}=0\] (40)

If \(V_{q}\) is ID w.r.t \(V_{i}\) or \(V_{j}\). Even these terms are excluded in [4'], the system still has the zero solutions. 

### ID-reverse of existing disentangled variables - Proof of Proposition 5

The next Proposition provides an additional tool to achieve identifiability and leverages the fact that other variables may have previously been disentangled and independence relationships in the factorization.

**Proposition 5** (**ID of canceled variables w.r.t. \(\) sets)**.: _Suppose \(\) contains \(()\). Given \( V^{tar}\) is ID w.r.t. a single variable \(V^{tar}\), \(V^{tar}\) is ID w.r.t. \( V^{tar}\) if \(V^{tar}\!\!\! V^{tar}\) in \(G_{}\). _

Proof.: We first introduce a lemma for distribution preserving from .

**Lemma 4** (Lemma 2 of ).: _Let \(A=C=R\) and \(B=^{n}\). Let \(f:A B C\) be differentiable. Define differentiable measures \(P_{A}\) on \(A\) and \(P_{C}\) on \(C\). Let \( b B\), \(f(,b):A C\) be measure-preserving. Then \(f\) is constant in \(b\)._

Denote \(^{tar}\) as \(\). \(V^{tar}\!\!\!\) in \(G_{}\) implies that

\[P_{}()=P_{}(V^{tar})P_{}()\] (41)

With the change of variable formulation and taking \(\):

\[ p_{}(^{tar})+ p_{}()= p _{}(}^{tar})+ p_{}(})+|_{}|\] (42)

Since \(\) is ID w.r.t \(V^{tar}\), \(}/^{tar}=0\). In other words, the elements \(_{Z}/^{tar}=0\) for every \(Z\) in Jacobian matrix are 0, where \(_{Z}\) is a function mapping from \(\) to \(\). Then

\[|_{}|=|_{}|+|_{ ^{tar}}|\] (43)

\[|_{}| =\] \[|_{V^{tar}}/ v^{tar}|.\]

Again, since \(\) is ID w.r.t \(^{tar}\), \(}=_{}()\). Thus,

\[ p_{}()= p_{}(})+ |_{}|\] (44)

Subtracting this to Eq. (42)

\[ p_{}(v^{tar})= p_{}(^{tar})+| _{^{tar}}|\] (45)

Denote \(_{^{tar}}(,)\) as \(_{^{tar}}^{}()\), which is the function \(_{^{tar}}\) fixing value \(=\) mapping from \(^{tar}\) to \(}\). This suggests for every \(\),

\[P_{}(^{tar})=P_{}(_{V^{tar}}^{ }(V^{tar}))\] (46)

Apply Lemma 2 of , \(_{V^{tar}}\) should be a constant regarding \(\). Thus,

\[\ Z,}{ Z}=0\] (47)

### Soundness of LatentID Algorithm - Proof of Thm. 1

The following provides the proof of the soundness of our proposed graphical algorithm for determining whether or not two variables are disentangleable given a collection of distributions from multiple domains and interventions.

**Theorem 1** (Soundness of CRID).: _Consider a LSD \(G^{S}\) and intervention targets \(\). Consider the target variables \(^{tar}\) and \(^{en}^{tar}\). If no edges from \(^{tar}\) points to \(}^{en}\) in the output causal disentanglement map (CDM) from **CRID**, \(G_{V,}\), then \(^{tar}\) is ID w.r.t \(^{en}\). _

Proof.: In **LatentID**, for each epoch, we iterate to choose \(\) and the baseline distribution to execute procedure Alg. F.3 and Alg. F.6. Any time an edge is removed, Proposition 3 and/or 4 are applied. At the end of epoch, Alg. F.8 is executed and edges will be removed only if Proposition 5 is applied. Thus the edge removals are all sound. The algorithm will stop when no edge will be removed, and terminate giving the causal disentanglement map \(G_{V,}\), which is a valid summary of what is disentangleable.

## Appendix D Examples and Discussion

### Additional Example Illustrating Motivation of Causal Disentangled Learning

In the introduction, we illustrated a medical example for why it is important to learn disentangled representations.

An additional motivating example can be seen through the lens of generating realistic face images . Consider an image dataset of human faces. Based on our understanding of anatomy and facial expressions, we know that both \(Gender\) and \(Age\) are not causally related, while age does directly affect \(HairColor\). There is a strong spurious correlation between age and gender, where there are many old males and young females in the dataset. In addition, let there be face images from both a senior and teen center building. The change in domain (i.e. population center) impacts the age distribution, as senior center faces are older than teen center faces. Given these images and knowledge of the latent causal graph, one would ultimately like to generate realistic face images given perturbations of \(Age\). If the variable representations are entangled, then it is possible for changes in age to also spuriously change gender. This is undesirable, and thus our goal is to achieve disentanglement of age and gender. Note that we do not require \(Age\) to be disentangled from \(HairColor\) necessarily since changing \(Age\) and also simultaneously changing \(Haircolor\) would be a realistic image generation. Here, we would seek a causal disentanglement map shown in Fig. S3.

If we could get the causal disentanglement map, then we know that when the representations are fully learned, we can intervene on \(Age,an\) without changing the \(Gender\) of the face. This motivates the need for a general approach to identifiability, compared to the scaling indeterminacy in Def. 6.5, which requires all variables to be disentangled from each other.

As another motivating example, consider a marketing company creating faces for a female product. The relevant latent factors are Gender \(\) Age \(\) Hair Color (see Appendix D.1 for details). If Gender and Age are entangled, changing Age might also alter Gender, which is undesirable. The company needs a model where Age is disentangled from Gender, while correlation with Hair Color is allowed. Our paper addresses the problem of determining whether a given set of input data and assumptions in the form of a LSD is sufficient to learn such a disentangled representation.

### Examples for non-Markovian Factorization

In this section, we centralize theoretical results in relation to the theory presented in this paper.

Unless specified, we denote the natural log as \(\).

We first provide more discussion about non-Markovian factorization Eq. (1). First, the concept C-component is formally defined as follows:

**Definition 6.2** (Confounded Component).: Let \(\{},},,}\}\) be a partition over the set of variables \(\), where \(}\) is said to be a confounded component (for short, \(C\)-component) of the selection diagram \(G_{V}\) if for every \(V_{i},V_{j}}\) there exists a path made entirely of bidirected edges between \(V_{i}\) and \(V_{j}\) in \(G_{V}\), and \(}\) is maximal. 

This construct represents clusters of variables that share the same exogenous variations regardless of their directed connections. The selection diagram in Figure 2 has a bidirected edge indicating the presence of unobserved confounders affecting the pairs \((V_{1},V_{2})\) and contains two C-components, namely, \(}=\{V_{1},V_{2}\},}=\{V_{3}\}\).

Akin to parents within a Markovian SCM, the c-components play a fundamental role in factorizing the joint distribution of the observed variables \(\).

Let \(<\) be a topological order \(V_{1},,V_{n}\) of the variables \(\) in \(G^{S}\). Then define the \(_{i}^{+}=(\{V(V_{i}):V V_{i} \})\{V_{i}\}\). The \(^{+}(V_{i})\) set consists of the nodes in the same c-component that are "\(\)" in topological order as \(V_{i}\), their corresponding parents, minus the node \(V_{i}\) itself. For instance, in Fig. S4, \(Pa^{+}(E)=\{D,C,A\}\) and \(Pa^{+}(D)=\{B,C,A\}\).

The general factorization formula Eq. (1) factorizes not only the joint observational distribution related to a causal graph, but also interventional distributions. With a perfect intervention on \(\), the factorization follows the corresponding graph is \(G_{}\), where the incoming arrows towards \(\) are cut. This factorization encompasses both Markovian and non-Markovian SCM models. When there are no bidirected edges in the diagram, \(_{i}^{+}\) reduce to \(\) in \(F_{}\).

Next, we introduce the Markov blanket, a fundamental idea in characterizing certain conditional independences in a causal graph .

**Definition 6.3** (Markov Blanket).: Let \(G\) be a causal graph over variables \(\). A Markov blanket of a random variable \(Y\) is any subset \(V_{1}\) such that conditioned on \(V_{1}\), Y is independent of all other variables.

\[Y\!\! V_{1}|V_{1}\]

The Markov blanket is an important object that captures conditional independences between variables when conditioned on _all other variables_ in the graph.

**Definition 6.4** ("Global" Markov property of DAGs ).: Consider a joint probability distribution, \(P\) over a set of variables \(\) satisfies the **Markov property** with respect to a graph \(G=(V L,E)\) if the following holds for, \((,,)\) disjoint subsets of V:

\[P(y|x,z)=P(y|z)Y\!\! X|Z\]

The global Markov property maps graphical structure in causal directed acyclic graphs (DAGs) to conditional independence (CI) statements in the relevant probability distributions from data. The distributions we consider \(\) are considered Markov wrt the graph, thus mapping d-separations in the graph to conditional independences in the distributions. This allows us to leverage factorizations, such as the one presented in Section 2.

Examples for Proposition 2

The following example illustrates more about the invariant factors.

**Example 14**.: _(Example 1 continued.) Choose \(P^{(1)}\) as the baseline and \(=\{\}\). The factorization of \(P()\) is \(P(V_{1})P(V_{2} V_{1})P(V_{3} V_{2})\). The changed variable set \([^{(2)},^{(1)},G^{S}]=\{V_{3}\}\) since the S-node points to \(V_{3}\) in \(G^{S}\) and \([^{(3)},^{(1)},G^{S}]=\{V_{3}\}\) since \(V_{3}^{(3)}\) while \(V_{3}^{(1)}\). Thus, comparing \(P^{(2)}\) and \(P^{(3)}\) with the baseline \(P^{(1)}\), \(p(v_{2} v_{1})\) and \(p(v_{1})\) are invariant factors while \(p(v_{3} v_{2})\) possibly changes. _

### The detailed examples of Proposition 3 and 4

We show another example of using Proposition 3 to solve an ID task in Example 1.

**Example 15**.: _(Example 14 continued.) Consider \(^{tar}=\{V_{2},V_{3}\}\), \(^{en}=\{V_{2},V_{3}\}=\{V_{1}\}\). When comparing \(\{P^{(2)},P^{(3)}\}\) with the baseline \(P^{(1)}\), \(=_{M}[^{(1)}]=\{\}\), and then_

\[[^{(2)},^{(1)},,G^{S}]= [^{(3)},^{(1)},,G^{S}]=^{tar}\] (48)

_Thus, these two comparisons satisfy the three conditions in Prop.3. Because the number of compared distribution \(\{P^{(2)},P^{(3)}\}\) is 2, which is equal to \(|^{tar}|\), then we know \(^{tar}\) is ID w.r.t \(^{en}\) by Prop. 3. This demonstrates that a variable \(V_{2}\) can be disentangled from another variable that is in the C-component (\(V_{1}\)). See Appendix Ex. 16 for a detailed derivation. _

Proposition 3 and 4 disentangle variables through comparing distributions. With enough distributions, one can build a linear system (illustrated in Appendix C.4 and C.5).

**Example 16**.: _(details for Example 15). By comparing distribution resulting from \(^{(2)}\) and \(^{(3)}\) with the baseline \(^{(1)}\),_

\[ p^{(2)}(v_{3} v_{2})- p^{(1)}(v_{3} v_{2}) = p^{(2)}(_{3}_{2})- p^{(1)}( _{3}_{2})\] (49) \[ p^{(3)}(v_{3} v_{2})- p^{(1)}(v_{3} v_{2}) = p^{(3)}(_{3}_{2})- p^{(1)}( _{3}_{2})\]

_Taking the first order partial derivative w.r.t. \(V_{1}\):_

\[0 =(_{3}_{2})-  p^{(1)}(_{3}_{2})}{_{2}} _{2}}{ v_{1}}+( _{3}_{2})- p^{(1)}(_{3} {v}_{2})}{_{3}}_{3}}{ v_{ 1}}\] (50) \[0 =(_{3}_{2})-  p^{(1)}(_{3}_{2})}{_{2}} _{2}}{ v_{1}}+( _{3}_{2})- p^{(1)}(_{3} {v}_{2})}{_{3}}_{3}}{ v_{ 1}}\]

_In this system, notice that_

\[ p^{(2)}(_{3}_{2})- p^{(1)}(_{3} _{2})= p^{(2)}(_{1},_{2},_{ 3})- p^{(1)}(_{1},_{2},_{3})\] (51)

_Then since the coefficient is linear independent assumed in Assumption 4, we have_

\[_{2}}{ v_{1}}=0,_{3}} { v_{1}}=0\] (52)

_Then \(V_{2}=_{2}(V_{2},V_{3})\), and \(V_{3}=_{3}(V_{2},V_{3})\)._

_First, this example shows we can disentangle two variables in the same C-component (\(V_{1},V_{2}\)). Second, Compared with the baseline, one can disentangle variable \(V\) with its descendants when soft interventions are given per node, and \(V\) is considered to be still entangled with its ancestral (see Sec. F.6). The above result shows that it is possible to disentangle variables from their ancestors using only soft interventions. More interestingly, no intervention is performed on \(V_{2}\) while we disentangle \(V_{2}\) from \(V_{1}\). Compared with , one can disentangle \(V_{1}\) and \(V_{3}\) using 10 distributions and we demonstrate 3 distributions are enough. _

**Example 17**.: _(details for Example 4). Choosing order \(V_{1}<V_{3}<V_{2}<V_{4}\)._

\[P()=P(V_{1})P(V_{3})P(V_{2} V_{1},V_{3})P(V_{4} V_{3})\] (53)_as the factorization. By comparing distribution resulting from \(^{(2)}\) and \(^{(3)}\) with the baseline \(^{(1)}\),_

\[& p^{(2)}(v_{2} v_{1},v_{3})- p^{(1)}(v_{ 2} v_{1},v_{3})= p^{(2)}(_{2}_{1},_{3})- p^{(1)}(_{2}_{1},_{3})\\ & p^{(3)}(v_{3})- p^{(1)}(_{3})+ p^{(3)}(v _{2} v_{1},v_{3})- p^{(1)}(v_{2} v_{1},v_{3})=\\ & p^{(3)}(_{1})(_{3})- p^{(3)}( _{3})+ p^{(2)}(_{2}_{1},_{ 3})- p^{(1)}(_{2}_{1},_{3})\\ & p^{(4)}(v_{1})- p^{(1)}(v_{1})= p^{(4)}(_{1})- p^{(1)}(_{1})\] (54)

_Taking the first order partial derivative w.r.t. \(V_{4}\):_

\[& 0=h_{2,1}_{1}}{ v_{4}}+h_{ 2,2}_{2}}{ v_{4}}+h_{2,3}_{3}}{ v_{4}}\\ & 0=h_{3,1}_{1}}{ v_{4}}+h_{3,2} _{2}}{ v_{4}}+h_{3,3}_{3}}{ v_{4}}\\ & 0=h_{4,1}_{1}}{ v_{4}}\] (55)

_where_

\[& h_{2,i}=(_{2} _{1},_{3})- p^{(1)}(_{2} {v}_{1},_{3})}{_{4}}\\ & h_{3,i}=(_{3})- p^{(1 )}(_{3})+ p^{(3)}(_{2}_{1},_{3})- p^{(1)}(_{2}_{1},_{3})}{ _{4}}\\ & h_{4,1}=(_{1})- p^{(1)}( _{1})}{_{4}}\] (56)

_Then since the coefficient is linear independent assumed in Assumption 4, we have_

\[_{1}}{ v_{4}}=0,_{2 }}{ v_{4}}=0,_{3}}{ v_{4}}=0\] (57)

_Then \(V_{1}=_{1}(V_{1},V_{2},V_{3})\), and \(V_{2}=_{2}(V_{1},V_{2},V_{3})\) and \(V_{3}=_{3}(V_{1},V_{2},V_{3})\). _

**Example 18**.: _The factorization based on \(G^{S}\) choosing \(=\{\}\) is_

\[P()=P(V_{1})P(V_{3})P(V_{3} V_{1},V_{2})\] (58)

_By comparing distribution resulting from \(^{(2)}\) and \(^{(3)}\) with the baseline \(^{(1)}\), for \(j=2,3,4,5\)_

\[& p^{(j)}(v_{1})+ p^{(j)}(v_{3})- p^{(1)}(v _{1})- p^{(1)}(v_{3})\\ =& p^{(j)}(_{)}+ p^{(j)}(_{3})- p^{(1)}(_{1})- p^{(1)}(_{3})\] (59)

_Taking the second order partial derivative w.r.t. \(V_{1},V_{3}\):_

\[ 0&= p^{(j)}(_{1})p^{(j )}- p^{(1)}(_{1})}{_{1}^{2}}_{1}}{ v_{1}}_{1}}{ v_{3}}+  p^{(j)}(_{3})p^{(j)}- p^{(1)}( _{3})}{_{3}^{2}}_{3}}{ v_{1} }_{3}}{ v_{3}}\\ &+(_{1})p^{(j)}- p^{(1) }(_{1})}{_{1}}_{1}}{  v_{1} V_{3}}+(_{3})p^{(j)} - p^{(1)}(_{3})}{_{3}} _{3}}{ v_{1} v_{3}}\] (60)

_Then since the coefficient is linear independent assumed in Assumption 4, we have_

\[_{1}}{ v_{1}}_{1}}{  v_{3}}=0,_{3}}{ v_{1}}_{3}}{ v_{3}}=0\] (61)

_Then after permutation,_

\[_{1}}{ v_{3}}=0,_{3}}{  v_{1}}=0\] (62)

_which implies that \(V_{3}\) is ID w.r.t \(V_{1}\) and \(V_{1}\) is ID w.r.t \(V_{3}\). _

The following example shows how Proposition 5 achieves disentanglement for the ID task in Example 1.

**Example 19**.: _(Example 15 (continued).) Let \(P^{(4)}\) with intervention target \(^{(4)}=\{V_{2}^{1,\{1\},}\}\) be another distribution added to the original setting. Consider \(^{tar}=\{V_{1}\}\). From Ex. 14, \(\{V_{2},V_{3}\}\) is ID w.r.t. \(V_{1}\). Consider \(=\{V_{2}\}\) (from \(^{(4)}\)). Since \(V_{1}\!\!\!\{V_{2},V_{3}\}\), then \(V_{1}\) is ID w.r.t \(\{V_{2},V_{3}\}\)._Related Work Discussion

Disentangled representation learning aims to obtain approximations \(}=\{_{1},,_{d}\}\) that separate the distinct, informative generative factors of variations  from the observations of \(\) and inductive bias of \(\). In other words, the learning goal is an unmixing function \(_{X}^{-1}\) that maps from \(\) to \(}\) (namely \(}=_{X}^{-1}()\)), where \(_{i}\) is some transformation of \(\). The goal of disentangled representation learning is to have \(_{i}\) be a function only of \(V_{i}\), i.e. \(=\{V_{i}\}\). This is not always possible, and different assumptions, data and relaxed versions of disentanglement may be studied to theoretically ground representation learning. The disentangled representation learning tasks are studied with various assumptions and input. In the following, we discuss related tasks and identifiability results in context of this paper. We also present a few case studies on the nuances between Markovian and non-Markovian ASCM setting.

First, we review the main goal of identifiability in all prior works. It is what is known as scaling identifiability. A special case of our ID definition in Def. 2.3.

**Definition 6.5** (Scaling indeterminancy).: Consider a collection of ASCM \(}\) that induces an LSD \(G^{S}\) and a collection of distribution \(\). We say \(\) is identifiable up to scaling indeterminacy if for every \(}}\) matches with the \(G^{S}\) and \(\), there exists functions \(\{h_{1},,h_{d}\}\) such that \(_{i}=_{i}(V_{i}),i[d]\), where \(h_{i}\) is a diffeomorphism in \(\). 

### Causal representation learning with unknown latent causal structure

In many prior works, the goal has been not only identifiability of the underlying latent variables, but also the discovery of the causal relationships among the latent variables . That is, the latent causal graph is unknown. The work proposed in this paper is a foundation for the first step of causal representation learning, i.e. identifying the distributions of the latent causal variables. It would be interesting future work to explore how the results proposed in this paper extend to the case when the latent causal graph is unknown.

### Comparisons with other identifiability criterion

We also consolidate other definitions of identifiability from the literature using the notion of an ASCM. We have already defined identifiability up to scaling ambiguity in Def. 6.5.

**Corollary 2** (Scaling ID is a case in general ID).: _Let \(}\) be a collection of ASCM with \(G^{S}\) the LSD over the latent causal variables \(\). If \(\) is identifiable up to scaling indeterminacy, then it is identifiable wrt \(\)._

Proof.: The proof follows from the application of Def. 2.3 and Def. 6.5. 

**Definition 6.6** (Identifiability up to ancestral mixtures ).: Let \(}\) be a collection of ASCM with \(G^{S}\) the LSD over the latent causal variables \(\). We say a variable \(\) is identifiable up to ancestral mixtures if for every \(}}\) matches with the \(G^{S}\) and \(\), there exists functions \(\{h_{1},,h_{d}\}\) such that \(_{i}=_{i}(}(V_{i})),i[d]\). 

**Corollary 3** (Ancestral ID is a case in general ID).: _Let \(M\) be a collection of ASCM with \(G\) the LSD over the latent causal variables \(\). If \(\) is identifiable up to ancestral mixtures, then it is identifiable wrt \(()\)._

Proof.: The proof follows from the application of Def. 2.3 and Def. 6.6. 

The following definitions are inspired by the identifiability results from .

**Definition 6.7** (Intimate Neighbor Set).: We say \(_{M_{G},V_{i}}:=\{V_{j} j i\), but \(V_{j}\) is adjacent to \(V_{i}\) and all other neighbors of \(V_{i}\) in \(M_{G}\). 

The intimate neighbor set for a variable dictates a set of neighbors that are adjacent to all of that variable's neighbors. It is used in the following definition from .

**Definition 6.8** (Identifiability up to intimate neighbor set of Markov Network ).: Let \(}}\) be a collection of ASCM with \(G^{S}\) the LSD over the latent causal variables \(\). We say a variable \(\) is identifiable up to intimate neighbors in the Markov Network if for every \(}}\) matches with the \(G^{S}\) and \(\), there exists functions \(\{h_{1},,h_{d}\}\) such that \(}=_{i}((M_{G},V_{i})),i[d]\), and \(M_{G}\) is the Markov network of \(G\) and \((M_{G},V_{i})\) is the intimate neighbor set of \(V_{i}\) in \(M_{G}\). 

**Corollary 4** (Intimate Neighbor Markov Network ID is a case in general ID).: _Let \(M\) be a collection of ASCM with \(G\) the LSD over the latent causal variables \(\). If \(\) is identifiable up to intimate neighbor set of the Markov Network, then it is identifiable wrt \((MN(G);)\)._

Proof.: The result follows from the application of Def. 2.3 and Def. 6.8. 

Thus, we showed that each of these identifiability definitions imply a general ID for a non-trivial subset of latent variables \(}\) with respect to \(^{en}\).

### Case study on challenges when disentangling variables in a non-Markovian setting

Prior results suggest that in a Markovian setting, given a perfect intervention on every node, the latent variables \(\) are ID up to scaling indeterminancies according to Def. 6.5.

One would suspect that ID may still hold in non-Markovian ASCMs, but the following result states that even with one perfect intervention per node, it is not possible to disentangle latent variables within the same c-component.

**Lemma 5** (Challenges of identifiability in non-Markovian causal models).: _Consider the ASCM that induces the diagram \(V_{1} V_{2}\). Suppose the intervention set includes an observational distribution, and perfect interventions on both \(V_{1}\) and \(V_{2}\): \(=_{\{\}},_{M}(\{V_{1}\}),_{M}(\{V_{2 }\})\). Then \(V_{1}\) is not ID w.r.t \(V_{2}\) and vice versa. _

Proof.: We prove this by construction of a counter-example.

Consider an ASCM \(M^{*}\) that is constructed as follows:

\[^{*}=\{ & V_{1} U_{1,2}\\ & V_{2} U_{1,2}+U_{V_{2}}\\ & X_{1} V_{1},X_{2} V_{2}\\ &(0,1),U_{Y}(0,3)\\ &_{V_{1}}=P(}}),_{V_{1}} (0,2)\\ &_{V_{2}}=P(}}),_{V_{1}} (0,7).\]

Consider a separate ASCM \(M^{(1)}\) that is constructed as follows:

\[^{(1)}=\{ & V_{1}^{(1)}-U_{1,2}^{(1)} \\ & V_{2}^{(1)} 0.5U_{1,2}^{(1)}+1.5U_{Y}\\ & X_{1} 1/3V_{1}^{(1)}+2/3V_{2}^{(1)},\\ & X_{2} 2/3V_{1}^{(1)}-2/3V_{2}^{(1)}\\ & U_{1,2}^{(1)}(0,3),U_{V_{2}}^{(1)} (0,1)\\ &_{V_{1}}=P(}}^{(1)}),_{V_{1}}^{(1) }(0,6)\\ &_{V_{2}}=P(}}^{(1)}),_{V_{2}}^{(1) }(0,7).\]

\(M^{*}\) and \(M^{(1)}\) induce the same observational distribution \(P()(0,1&1\\ 1&4)\), and interventional distributions \(P(;_{V_{1}})(0,2&0\\ 0&4)\), \(P(;_{V_{2}})(0,1&0\\ 0&7)\)However, \(V_{1}^{(1)}=V_{1}-V_{2}\), which implies \(V_{1}^{(1)}\) is not ancestral mixture or rescaling of the original \(V_{1}\). Therefore, \(V_{1}\) is not identifiable up to ancestral mixtures, or rescaling. 

### ID within c-components

Lemma 5 shows that even with one perfect intervention on each node, it is not possible to disentangle variables within the same c-component. The next lemma provides a means of doing so using two perfect interventions on the same node. This provides some intuition for the usefulness of perfect interventions in the **CRID** setting.

**Lemma 6** (Two perfect interventions can disentangle within a c-component).: _Let \(G^{S}\) be the LSD induced from a collection of ASCM \(}\). Suppose \(V_{i},V_{j}\) are in the same c-component, and there are \(L+1\) perfect interventions distributions \(_{V_{i}}=\{P^{(a_{0})},P^{(a_{1})},,P^{(a_{L})}\}\) such that \(V_{i} do[^{(a_{l})}]\) and \([^{(a_{l})},^{(a_{0})},V_{i},G^{S}]\) are equivalent (denoted as \(\)) for \(l[L]\). When \(V_{j}\) and if \(L||\), \(V_{i}\) is identifiable wrt \(V_{j}\). When \(V_{j}\) and if \(L 2||+_{J}\), \(V_{i}\) is identifiable wrt \(V_{j}\)._

Proof.: The result follows from the application of Proposition 3 and Proposition 4. 

**Example 20**.: _In most simple case. Let's have \(do[^{(j)}]=do[^{(k)}]=V_{i}\) and \(=V_{i}\). Let \(V_{i},V_{j}_{k}\) be two arbitrary latent variables in the same c-component. By comparing distributions, we have_

\[p_{V_{i}}^{(2)}(v_{i})-p_{V_{i}}^{(1)}(v_{i})=p_{V_{i}}^{(2)}(_{i}) -p_{V_{i}}^{(1)}(_{i})\] (63)

_Taking partial w.r.t. \(V_{j}\), we have_

\[0=}^{(2)}(_{i})-p_{V_{i}}^{(1)}(_{i})}{ _{i}}_{i}}{v_{j}}\] (64)

_which implies \(_{i}}{v_{j}}=0\)._

Notice that this is not the only way to disentangle to variables in the C-Component. In Example 6, \(V_{1}\) and \(V_{2}\) are disentangled from each other without leveraging two perfect interventions.

### Case study on disentangling variables in a Markovian setting

This next example works out the algebraic derivations for analyzing Fig. 4(a). This derivation is provided to provide additional intuition on the theory presented in Section 3, and how these concepts apply in a simple 3-dimensional latent causal graph.

**Example 21** (Algebraic derivation of disentanglement in a simple 3-node chain graph).: _Given the graph shown in Figure 4(a), we can factorize the joint observational distribution of the latent variables_

\[P()=P(V_{3}|V_{2})P(V_{1}|V_{2})P(V_{2})\] (65)

_By the probability transformation formula, we can similarly write the distribution in terms of its estimated sources via function \(=_{}^{-1} f_{}\) for its distribution \(\)._

\[P()=P(_{V_{3}}()|_{V_{2}}())P(_{V_{ 1}}()|_{V_{2}}()P(_{V_{2}}())|detJ_{ }|\] (66)

_Now, consider the interventional distributions: \(P(;_{V_{3}^{(1)}})\) and \(P(;_{V_{3}^{(2)}})\). Here, we will use shorthand \(_{i}\) to indicate \(_{V_{i}}()\). Similarly, we can factorize the distribution \(P(;_{V_{3}^{(1)}})\):_

\[P(;_{V_{3}^{(1)}})\] \[=P(V_{3}|V_{2};_{V_{3}^{(1)}})P(V_{1}|V_{2};_{V_{3}^ {(1)}})P(V_{2};_{V_{3}^{(1)}})\] \[=P(_{3}|_{2};_{3^{(1)}})P(_{1}|_{2};_ {V_{3}^{(1)}})P(_{2};_{V_{3}^{(1)}})|detJ_{}|\]_Similarly, we can decompose the interventional distribution \(P(;_{V^{(2)}_{3}})\). Now, comparing the log observational distribution with the log intervention \(_{V^{(i)}_{3}}\), we get:_

\[ p(;_{V^{(i)}_{3}})- p()\] \[= p(V_{3}|V_{2};_{V^{(i)}_{3}})+ p(V_{1}|V_{2}; _{V^{(i)}_{3}})+ p(V_{2};_{V^{(i)}_{3}})\] \[- p(V_{3}|V_{2})- p(V_{1}|V_{2})- p(V_{2})\] \[= p(V_{3}|V_{2};_{V^{(i)}_{3}})- p(V_{3}|V_{2})\]

_Where the last line applies the invariance of \(P(V_{i}|V_{j};_{V_{k}})=P(V_{i}|V_{j})\) if \((V_{i}\!\!\! V_{k}|V_{j})_{G_{V_{}}}}\). In the space mapped by \(\), we similarly get:_

\[ p(;_{V^{(i)}_{3}})- p()\] \[= p(_{3}|_{2};_{3^{(i)}})+ p(_{1}|_ {2};_{V^{(i)}_{3}})+ p(_{2};_{V^{(i)}_{3}})\] \[- p(_{3}|_{2})- p(_{1}|_{2})- p( _{2})\] \[= p(_{3}|_{2};_{3^{(i)}})- p(_{3}|_ {2})\]

_When comparing the distributions of \(}\), interestingly the \(\) of the determinant of the Jacobian cancels out. Combining the two, we get:_

\[ p(V_{3}|V_{2};_{V^{(n)}_{3}})- p(V_{3}|V_{2})= p(_{3}| _{2};_{3^{(n)}})- p(_{3}|_{2})\] (67)

_Taking the partial derivative now with respect to \(V_{1}\), we get that the LHS equals 0 and the RHS becomes:_

\[0 =} p(_{3}|_{2};_{ 3^{(i)}})- p(_{3}|_{2})\] \[=|_{2};_{3^{(i)}})}{ _{3}}}{ V_{1}}+|_{2};_{3^{(i)}})}{_{2}} }{ V_{1}}\] \[-|_{2})}{_{3}} }{ V_{1}}-|_{2} )}{_{2}}}{ V_{1}}\] \[=}{ V_{1}}(|_{2};_{3^{(i)}})}{_{3}}-|_{2})}{_{3}})\] \[+}{ V_{1}}(|_{2};_{3^{(i)}})}{_{2}}-|_{2})}{_{2}})\]

_Thus, we have two unknowns \(}{ V_{1}}\) and \(}{ V_{1}}\). Given the two interventions with different mechanisms on \(V_{3}\) compared to the observational distribution, we have two equations that result in a 2-dimensional linear system. We are able to determine that \(}{ V_{1}}=}{ V_{1} }=0\) thus demonstrating that our approach disentangles \(}=_{3}()\) and \(}=_{2}()\) from \(V_{1}\). _

### Comparing different identifiability results

In this section, we explicitly compare and discuss our work compared to a non-exhaustive list of related disentangled learning in the setting of causally related latent components. Different from previous literature, we do not make common assumptions such as (1) each intervention is applied to a single node ; (2) idle interventions (observational distribution) are present within each domain ; (3) _exactly_ one intervention is applied per node ; (4) _at least_ one intervention is applied per node .

Causal component analysis The closest work to ours is , which also presupposes knowledge of the latent causal graph and focuses solely on learning the unmixing function and the distributions of the causal variables. In , the results emphasized the need for interventions that occur only on a single node in the latent causal graph. However, Lemma 5 demonstrates challenges that are not addressed in the prior work. In addition, in our work, we propose a more general concept of identifiability in Def. 2.3. As a result, Thm. 1 makes significantly weaker assumptions to still achieve identifiability. Exs.2-6 illustrate also the nuances addressed by our work, but not in .

Another interesting concept introduced by  is the "fat-hand" interventions, which intervene on groups of variables within different groups, and the concept of "block-identifiability".

Here, we illustrate some examples and discussion on how our work compares with that of  that also provides sufficient conditions for identifiability given a causal graph over the latent variables. One key difference between our work is that we do not assume Markovianity in the underlying SCM, whereas they do.

**Example** (Ex. 6 cont.).: _This example continues off of Ex. 6. Consider the motivating example in healthcare depicted in Fig. 2. In hospitals from different countries \(^{i}\) and \(^{j}\), drug treatment (\(V_{1}\)) affect length of ICU stay (\(V_{2}\)), and ultimately whether or not the patient lives or dies (\(V_{3}\)). Our task is to learn representations of the high-level latent variables (\(V_{1},V_{2},V_{3}\)) that are not collected given a collection of low-level input such as EMRs, imaging and bloodwork data (high-dimensional data \(\)). In existing work , there are no guarantees that variables \(\{V_{2},V_{3}\}\) are disentangled from their ancestor \(V_{1}\) from soft interventions per nodes. However, Proposition 3 demonstrates two comparisons are enough to disentangle both \(V_{2}\) and \(V_{3}\) from their ancestor \(V_{1}\). _

Even in the Markovian setting, where the LSG does not contain bidirected edges, our results can also guarantee identifiability in this setting.

**Example 22** ( approach).: _Given the graph shown in Figure 4(a),  requires an observational, and tuple of intervention sets \(=\{\},\{V_{1}\},\{V_{2}\},\{V_{3}\}\). Provided these four distributions, there is still no disentanglement of \(_{3}\) with respect to any variables, \(V_{i}\). _

Causal Representation Learning from Multiple Distributions: A General Setting Another approach to achieving disentanglement among the latent variables is similar to nonlinear-ICA, but leverages the conditional independence properties within a Markov Network of the causal graph. Then the proof strategy of  considers the second order derivative, which leverages the conditional independence constraints.

However, this results in a required \(2d+|(M_{G})|+1\) number of distributions that satisfy Assump. 4. In addition, this strategy states that in a collider graph \(V_{1} V_{2} V_{3}\), that \(V_{1}\) is not ID wrt \(V_{2}\), and \(V_{3}\) is not ID wrt \(V_{2}\).

Another example, continues off of Ex. 6.

**Example** (Ex. 6 cont.).: _This example continues off of Ex. 6. Consider the motivating example in healthcare depicted in Fig. 2. In hospitals from different countries \(^{i}\) and \(^{j}\), drug treatment (\(V_{1}\)) affect length of ICU stay (\(V_{2}\)), and ultimately whether or not the patient lives or dies (\(V_{3}\)). Our task is to learn representations of the high-level latent variables (\(V_{1},V_{2},V_{3}\)) that are not collected given a collection of low-level input such as EMRs, imaging and bloodwork data (high-dimensional data \(\)). According to , 10 distributions can disentangle \(V_{3}\) from \(V_{1}\) when \(V_{3}\!\!\!\!\!\! V_{1} V_{2}\). However, Proposition 3 demonstrates two comparisons are enough to disentangle both \(V_{2}\) and \(V_{3}\) from their ancestor \(V_{1}\)._Linear ICALinear ICA has been extensively studied over decades, and is applied in magnetic resonance imaging (MRI) , astronomy , image processing , finance  and document analysis . In linear ICA settings, the generative factors are assumed to be independent of each other and the mixture function \(f_{}\)) is considered to be an invertible matrix \(^{d d}\). Formally, the mechanism \(\) and the distribution \(P()\) of the true ASCM \(^{*}\) are written as:

\[V_{j} f_{j}(U_{j}), j[d]\\ \\ U_{i} U_{j}, i,j[d]\] (68)

Notice that \(\) is \(d\) dimensional variable here and \(X_{i}_{j=1}^{d}a_{ij}V_{j}=_{i}, i [d]\). Given the observational distribution \(P()\), the goal of linear tasks is to learn \(}\) such that \(_{j}\) is a scaling of a true underlying generative factors \(V_{i}\), where \(}=}^{-1}\). The scaling and permutation identifiability is defined as follows to denote the achievability of linear ICA tasks.

**Definition 6.9** (Scaling and Permutation Identifiability).: The representation \(\) is said to be identifiable up to scaling and permutation \(^{(2)}=^{(1)}\) if for every pair of ASCM \(^{(1)}\) and \(^{(2)}\) such that (1) \(P^{^{(1)}}()=P^{^{(2)}}()\), \(P^{^{(1)}}(;_{v_{k}})=P^{^{(2)}}( ;_{v_{k}})\);

(2) \(^{(1)}\) and \(^{(2)}\) are constrained by the modeling process in Eq. 68,

where \(=(c_{1},,c_{d})\) is a scaling diagonal matrix and \(\) is a permutation matrix. 

Def. 6.9 says that if every pair model \(^{(1)}\) and \(^{(2)}\) in linear ICA settings match the observational distributions, the generative variables can be transformed by permutation and scaling. This implies once one finds a proxy ASCM \(\) that matches \(P()\), \(\) is guaranteed to be a scale and permutation representation of the true generative variable if the identifiability is achieved. The next example illustrates ASCMs in linear ICA settings and Def. 6.9.

**Example 23** (ICA Identifiability Is Not Achieved).: _We consider the three augmented generative processes \(^{*}\), \(^{(1)}\) and \(^{(2)}\) with linear ICA constraints._

\[V_{1} U_{1},V_{2} U_{2}\\ X_{1} V_{1},X_{2} V_{2}V_{1}^{(1 )} U_{1},V_{2}^{(1)} U_{2}\\ X_{1} 2V_{1}^{(1)},X_{2} 0.5V_{2}^{(1)} V_{1}^{(2)} U_{1},V_{2}^{(2)} U_{2}\\ X_{1}}{2}V_{1}^{(1)}+}{2}V_{2}^{(2)}\\ X_{2}}{2}V_{1}^{(2)}-}{2}V_{2}^{(2)} \] \[^{*}\] \[^{(1)}\]

_It is verifiable that \(X_{1},X_{2}(1,0;0,1)\) induced by all three models. The latent generative variables in \(^{(1)}\) are scaled and permuted representations of the true factors \(^{*}\), namely \(V_{1}^{(1)}=2V_{2}^{(2)}\) and \(V_{2}^{(1)}=0.5V_{2}^{(1)}\). In other words, \(V^{(1)}\) and \(V^{(2)}\) distinctly represents \(V_{2}\) and \(V_{1}\) respectively. However, the representations \(V_{1}^{(2)}\) and \(V_{2}^{(2)}\) in \(^{(2)}\) are mixture of true generative factors \(V_{1}\) and \(V_{2}\), i.e.,_

\[ V_{1}^{(2)}&=V_{1}+ V_{2}\\ V_{2}^{(2)}&=V_{1}-V_{2}\] (69)

_which implies this is not a scaling and permutation transformation. Thus, \(^{(2)}\) demonstrates that the scaling and permutation identifiability is not achieved in this setting. _

The above example shows a famous result of linear ICA: the representations are not identifiable if generative factors follow a multi-gaussian distribution. This result comes from the symmetricity of gaussian distributions: any white gaussian variables are still white gaussian after an orthogonal transformation. However, orthogonal transformations are not guaranteed to be a scaling or permutation thus a proxy model may have generative factors that are mixtures of the true \(\) (\(^{(2)}\) in Example 23). Further, the identifiability result can be concluded as follows with the non-Gaussian assumption.

Nonlinear ICA Compared to linear ICA, nonlinear ICA assumes the mixing function is a nonlinear bijective function (i.e. invertible and differentiable).

In linear ICA settings, the generative factors are assumed to be independent of each other and the mixture function \(f_{}\)) is considered to be an invertible matrix \(^{d d}\). Formally, the mechanism \(\) and the distribution \(P()\) of the true ASCM \(^{*}\) are written as:

\[V_{j} f_{j}(U_{j}), j[d]\\ _{X}()\\ U_{i} U_{j}, i,j[d]\] (70)

The traditional approaches for proving identifiability from  has the following settings:

* (Assumptions) A parametric exponential family is assumed in . In addition, the causal assumptions of the latent variables is fully disconnected graph, where all variables are mutually independent. Our work assumes a nonparametric mixing model, and only requires the mixing function to be a bijection. In addition, we allow a non-Markovian causal model among the latent variables, which is the first to our knowledge to analyze identifiability in this general setting.
* (Data) Nonlinear ICA assumes that \(2d+1\) number of distributions with mechanism changes of the latent variables such that a version of the Assump. 4 holds. One instantiation of this in real-world data is time-series with non-stationary changes. Our work leverages arbitrary combinations of interventional data arising from multiple domains, and also does not necessarily require observational data.
* (Output) The focus of nonlinear ICA was typically on achieving disentanglement of latent variables up to scaling indeterminancy (Def. 6.5). Our work approaches the goal of identifiability from a more general setting according to Def. 2.3.

Interventional causal representation learning Another potentially promising approach to improving identifiability results lies in assuming a parametric form to the mixing function.  considers the setting of having a mixing function that is a composition of polynomial functions (i.e. a polynomial decoder).

Thus,  is able to achieve identifiability of latent variables up to an affine transformation:

\[=+c\]

where \(^{d d}\) and \(c^{d}\) make up an invertible affine transformation of the true latent variables \(\). In our work, we consider a nonparametric form of the mixing function. However, future work could consider relaxing this assumption in the direction of a parametric mixing function with polynomial functions.

Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity In this paper, identifiability results for a linear ASCM with a linear mixing function is provided, with access to multi-distributional data arising from different environments.

In addition, they prove identifiability up to "surrounded-nodes" in [25, Thm. 3]. Specifically, any linear proxy model that is compatible with the observed distributions \(\), the causal graph, and satisfies a few technical assumptions will achieve identifiability for each variable with respect to variables not in the surrounded-set. Similar to ancestral identifiability (Def. 6.6), surrounded-set disentanglement is a special case of our proposed identifiability definition (Def. 2.3). Our work proposes a graphical criterion and an algorithm for determining a causal disentanglement map, which may contain different disentanglements compared to a surrounded-set. Besides our notion of identifiability (goal), our paper also allows arbitrary distributions from multiple domains (input), and non-parametric non-Markovian ASCMs (assumptions).

**Definition 6.10** (Surrounded set from ).: For two nodes, \(V_{i},V_{j}\) in graph G, we say that \(V_{i} sur(V_{j})\) if \(V_{i} Pa_{j}\) and \(Ch_{j} Ch_{i}\).

Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions In this paper, the authors explore identifiability results in an ASCM with a linear mixing function, where interventions occur on multiple latent variables at the same time (i.e. multi-node interventions). Further, they assume that interventions are perfect interventions and sufficiently diverse, and have a sparse effect on the set of latent variables. Finally, their goal of identifiability is a full disentanglement, which is a special case of the general disentanglement we provide in Def. 2.3, where any variable may be ID w.r.t. a subset of latent variables.

Our paper also allows multi-node interventions within our graphical criterion (Props. 3, 4, and 5). In terms of the identifiability goal (output), ASCM model (assumptions), and distributions (input), our paper is more general notion of identifiability in the form of a causal disentanglement map (goal); our paper also allows arbitrary distributions (soft, and/or perfect interventions with same/different mechanisms) from multiple domains compared to only perfect interventions in a single domain that meet a sparsity constraint (input), and non-parametric non-Markovian ASCMs vs linear Markovian ASCMs (assumptions).

Linear Causal Representation Learning from Unknown Multi-node Interventions In this paper, identifiability results are provided in a linear ASCM with a linear mixing function, where soft, or perfect interventions occur on multiple nodes. The authors establish full disentanglement results, or disentanglement up to ancestors, which is similar to the results demonstrated in "Causal Component Analysis" .

Our paper also allows multi-node interventions within our graphical criterion (Props. 3, 4, and 5). In terms of the identifiability goal (output), ASCM model (assumptions), and distributions (input), our paper is more general notion of identifiability in the form of a causal disentanglement map (goal); our paper also allows arbitrary distributions (soft, and/or perfect interventions with same/different mechanisms) from multiple domains (input), and non-parametric non-Markovian ASCMs vs linear Markovian ASCMs (assumptions).

Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity In this paper, the authors propose a partial disentanglement goal in a linear, or non-parametric Markovian ASCM with a sparse causal structure.

Their notion of the disentanglement goal introduces so-called entanglement graphs (output), which is interestingly exactly what we call the causal disentanglement map. Though the proposed output is the same, the identifiability results are not the same even in the Markovian case.

In addition, in terms of the distributions leveraged (input), our work differs in considering arbitrary combinations of distributions (soft, or perfect, or observational) from heterogenous domains. In terms of modeling the ASCM (assumptions), our work considers completely non-parametric non-Markovian ASCMs instead of non-parametric Markovian ASCMs with sparse connectivity. In future work, we believe it will be interesting to explore the assumption of sparsity in the context of our work.

## Appendix G Experimental Results

### Synthetic data-generating process

We generate data according to latent causal diagrams shown in Fig. 4. Specifically, we analyze the chain graph \(V_{1} V_{2} V_{3}\), and collider graph \(V_{1} V_{2} V_{3}\) with different input distributions.

Each graph is constructed according to an ASCM, where the latent variables are related linearly:

\[V_{i}:=_{j P_{a_{i}}}_{i,j}V_{j}+_{i}\]

where linear parameters are drawn from a uniform distribution \(_{i,j} U(-a,a)\), and the noise is distributed according to the standard normal distribution \(_{i}(0,1)\).

Generating Multiple DomainsTo generate a new domain, where \(S^{i,j} V_{i}\) indicates a change in mechanism for \(V_{i}\) due to the change in ASCMs between \(M^{i}\) and \(M^{j}\), we start from the first ASCM generated, and then we modify the distribution of the noise variable with a mean-shift.

Generating Interventions Within Each DomainTo generate interventional datasets within each domain \(^{i}\), we modify the \(^{i}\) by additionally modifying the SCM, and shifting its mean for a variable. Therefore for distribution \(k\) in \(^{i}\), with perfect intervention \(\), we will have:

\[V_{k}:=^{}_{k},^{}_{k} (_{k},_{k}),\; V_{k}\]

such that \(_{k}\) is not within \(+/-1\) of any other distribution for variable \(V_{k}\). This ensures the Assumption of Generalized Distribution Change (Assumption. 4). With a soft intervention \(\) that is not perfect:

\[V_{k}:=_{j Pa_{k}}_{i,j}V_{k}+^{}_{k},^{}_{k}(_{k},_{k}),\; V_{k} \]

For each distribution over \(^{d}\), we generate 200,000 data points resulting in \(d 200,000\) data points in total for \(N\) total distributions.

We modify the mean and the variance to ensure that the Assumption of distribution change is met (Assump. 4).

Mixing functionIn order to generate the low-level data \(\), we will apply a mixing function \(f_{}\) to the generated latent variables \(\). Following [21; 51], to generate an invertible mixing function, we will use a multilayer perceptron \(_{}=_{M}... _{1}\), where \(_{M}^{d d}\) for \(m[1,M]\) denotes invertible linear matrices and \(\) is an element-wise invertible nonlinear function. In our case, we will use the tanh functio as done in :

\[(x)=tanh(x)+0.1x\]

In addition, each sampled matrix \(_{i}\) is re-drawn if \(|_{i}|<0.1\). This ensures that the linear maps are not ill-conditioned and close to being singular. Once the mixing function is drawn for a given simulation, it is fixed across all domains and interventions according to Assump. 4, and then \(\) is drawn according to all ASCMs instantiated.

### Image Editing Using Disentangled Representations

We demonstrate qualitatively that the generalized disentanglement proposed in this work is valuable for downstream tasks, such as counterfactual image editing . Consider the graph shown in Fig. S8. Specifically, we use our learned proxy model to generate initial images and perform interventions on learned representations \(}\) to edit images. We generate initial image samples from observational distribution of \(_{1}\), and then perturb the relevant representations with random Gaussian noise to edit the image. This is done for the color of the bar, color of the digit, and the digit representations. If the learned \(}\) satisfy the CRID output disentanglement,

1. editing the color of the digit (\(_{_{2}}\)) should keep the original digit and writing style but may change the color of the bar since \(V_{2}\) has a causal effect on \(V_{3}\).
2. editing the color of the bar (\(_{_{3}}\)) should keep the original digit and writing style but may change the color of the digit since \(V_{3}\) is not disentangled with \(V_{2}\).
3. editing digit (\(_{_{1}}\)) may change all variables since no disentanglement of \(V_{1}\) is claimed by CRID.

The editing results are shown in Fig. S7. All editing results are aligned with the CDM output as expected, which are illustrated above. Specifically, Fig. S7(a) shows the learned VAE-NF model can change the color of the bar without arbitrarily changing the digit, or writing style. Fig. S7(b) shows the learned VAE-NF model can change the color of the digit without arbitrarily changing the digit, or writing style. Finally, Fig. S7(c) shows the learned VAE-NF model did not learn a disentangled representation for "digit". When perturbing the representation for digit, sometimes the digit does not change, while the color of the bar, color of the digit, or the writing style changes. This experiment also demonstrates one usage of CRID. Before training a model that is potentially computationally and time-intensive, one can leverage CRID to determine if their input data and input assumptions are sufficient for learning a relevant disentangled representation for their downstream task.

### Model

We train invertible MLPs with normalizing flows. The parameters of the causal mechanisms are learned while the causal graph is assumed to be known. We leverage the implementation in , and extend it for our experiments.

The encoder is trained with the following objective that estimates the inverse function \(f^{-1}\), and the latent densities \(P()\) reproducing the ground-truth up to certain mixture ambiguities (c.f. Lemmas 3, 6). The encoder parameters is estimated by maximizing the likelihood..

Normalizing flowsWe use a normalizing flows architecture  to learn an encoder \(_{}:^{d}^{d}\). Therefore, the observations \(\) will be the result of an invertible and differentiable transformation:\[=_{}()\]

Specifically, \(g_{}\) will comprise of Neural Spline Flows  with a 3-layer feedforward neural network with hidden dimension 128 and a permutation in each flow layer.

Base distributionsNormalizing flows require a base distribution. We leverage one baseline distribution per sampled dataset, \((_{}^{k})_{k[d]}\) over the base noise variables \(\). The conditional density of any variable is given by:

\[_{}^{k}(v_{i}|})=_{j Pa_{ i}}_{i,j}v_{j},_{i}\]

where the parameters are replaced by their corresponding counterparts if there is a change-in-domain, or an intervention applied. When a perfect intervention is applied, we have that:

\[_{}^{k}(v_{i})=(_{i},_{i})\]

### Training details

We use the ADAM optimizer .We start with a learning rate of 1e-4. We train the model for 200 epochs with a batch size of 4096.

The learning objective is expressed as:

\[^{*}=_{}_{k=0}^{N}}_{n=1}^{n _{k}} p_{}^{k}(^{(k)})\]

where \(n_{k}\) represents the size of the dataset \(P^{k}\), which is 200,000 in our simulations. We perform 10 training runs over different seeds for each experiment, and show the distributions of the mean-correlation coefficient (MCC). Using the output of Alg. 1, we compare variables that are expected to be entangled and disentangled. We use NVIDIA H100 GPUs to train the neural network models.

### Evaluation metrics

The output of our trained model is \(}=g_{}()\), which is a d-dimensional representation. We will compare this representation with our ground-truth latent variable distributions \(\) by computing the mean correlation coefficients (MCC) between the learned and ground-truth latents. We expect there to be an overall lower MCC for variables that are predicted to be disentangleable by Alg. 1 relative to variables that are not deemed disentangleable.

Note that our algorithm is not shown to be complete, so there may be variables that are disentangled at the end of our training process that are not captured by the output of Alg. 1. Characterizing when this occurs and coming up with a complete theoretical characterization of disentanglement is a line for future work.

For the evaluation, we follow a standard evaluation protocol taken in prior work . We expect low MCC values when predicting variables that are disentangled, and higher MCC values when predicting variables that are still entangled.

### Limitations

A major limitation of normalizing flows is that the input and output dimensions of the encoder must be the same. This is due to the fact that we wish to constrain the layers to be invertible transformations. It is easy to define invertible transformations for the same input/output dimensions, but it is non-trivial to do so when input/output dimensions vary widely.

Besides the technical limitations of the implementation, it is important to note that our theoretical results are asymptotic results. The theory claims we can achieve ID when the neural network is trained to zero error. However, in practice, this is not always simple to do and may require hyperparameter tuning and a very large sample size.

For example, when we consider Fig. 6, we observe that the disentanglement of (b,c) is significantly better than (a,d). In the experiment involving the collider graph from Fig. 4(b), we sample four distributions each with 200,000 samples, and thus we have almost 2x the data points compared to the settings in Fig. 6(a,c). We illustrate this point to emphasize that there is no correct way to set the sample sizes, hyperparameters, or model architecture as each simulation will be different. We chose a sample size, model architecture, and default hyperparameters based on prior literature  instead of biasing our experimental results by tuning significantly for each simulation.

### Discussion of Results

In Fig. S9, we show the MCC values for each learned latent representation \(}\) and the corresponding ground-truth latents \(\) for the three different LSDs shown in Fig. 4. Based on the causal disentanglement map (CDM) output from the CRID algorithm, the disentangled variables are shown in red, while the entangled variables are shown in gray.

In Fig. S9(a), the \(MCC(},V_{1})\) is low relative to the \(MCC(},V_{3})\), which is predicted by the CRID algorithm's CDM output (right plot). This suggests that \(V_{1}\) is disentangled from \(V_{3}\). In addition, we observe that all MCC values wrt \(}\) are relatively similar, which makes sense as we do not obtain any disentanglement wrt \(V_{1}\) (left plot). CRID also predicts that \(V_{2}\) is ID wrt \(V_{1}\) (middle plot). However, we observe quite a large range of MCC values, possibly due to variance, default hyperparameter settings, or insufficient sample size. Importantly, this experiment verifies that two soft interventions on \(V_{3}\) in the chain graph of Fig. 4(a) can ID \(V_{3}\) wrt \(V_{1}\), whereas previous literature suggested that \(V_{3}\) is not ID wrt \(V_{1}\) because \(V_{1}(V_{3})\).

In Fig. S9(b), we now have an observational, two soft interventions on \(V_{3}\), and a perfect intervention on \(V_{2}\). In addition to ID \(V_{2}\) wrt \(V_{3}\) (middle plot), we are also able to obtain full disentanglement of \(V_{1}\) from \(\{V_{2},V_{3}\}\) (left plot). Interestingly, we are able to fully disentangle the representation for \(V_{1}\) without intervening on it. This is the first theoretical (and empirical) result to our knowledge that shows this in a causal representation learning setting.

In Fig. S9(c), we have an observational and four interventional distributions applied on \(\{V_{1},V_{3}\}\) all with different mechanisms. We observe that \(V_{1}\) and \(V_{3}\) are fully disentangled. \(MCC(},V_{3})>MCC(},\{V_{1},V_{2}\})\), and \(MCC(},V_{1})>MCC(},\{V_{2},V_{3}\})\). CRID does not predict disentanglement for the \(V_{2}\) representation (middle plot), yet interestingly we still see some disentanglement.  analyzes a similar setup using "fat-hand interventions", and the corresponding theory does predict \(V_{1}\) and \(V_{3}\) is ID wrt \(V_{2}\). However, we also disentangle \(V_{1}\) and \(V_{3}\) from each other using many interventions.  presents a similar approach by leveraging \(2d+|(M_{G})|+1\) distributions that "sufficiently change" (i.e. Assumption 4) to disentangle variables. However, the corresponding theory suggests that \(V_{1}\) and \(V_{3}\) are still entangled because they are adjacent in the Markov Network of G (\(M_{G}\)). These results demonstrate theoretically (and empirically) that \(V_{1}\) and \(V_{3}\) are in fact disentangled from each other in a fundamentally important causal graph (i.e. the collider).

In Fig. S9(d), we consider disentanglement in a non-Markovian LSD. We leverage two perfect interventions on \(V_{3}\) (c.f. Lemma 6), and verify that even without observational distributions and the challenging setting of confounding among the latent variables, we can achieve disentanglement of \(V_{3}\) wrt all other variables. \(MCC(},V_{3})>MCC(},\{V_{1},V_{2},V_{4}\})\), which is predicted by the CRID algorithm's CDM output (3rd plot from left). As expected, \(V_{1}\) and \(V_{2}\) are still fully entangled with all other variables (1st and 2nd plot from left).

Figure S9: Mean correlation coefficient (MCC) of latent ground truth variables with the learned representation \(}\), and expected disentanglement (red) according to the **CRID** algorithm. Each plot corresponds to an experimental setting using the graphs shown in Fig. 4: chain graph with two interventions on \(V_{3}\) (a). chain graph with two interventions on \(V_{3}\) and a perfect intervention on \(V_{2}\) (b), collider graph with four interventions on \(\{V_{1},V_{3}\}\) (c) and the non-markovian graph with two perfect interventions on \(V_{3}\) (d).

[MISSING_PAGE_EMPTY:52]

3. Why CRID (Alg. 1) only takes intervention targets \(\) and LSG \(G^{S}\) as input? Do you need distributions \(\)? If not, how do you learn representations? **Answer**. CRID leverages the intervention targets \(\) and the LSG \(G^{S}\) to determine the invariant and changing factors when considering the generalized factorization of probability distributions Markov relative to the provided graph. These invariant and changing factors are what give rise to the theory we develop in Section 3. The CRID algorithm leverages this theory to provide an identifiability algorithm, which answers the question: If we fully learn a representation \(}\) (given the diagram and the distributions), which variables are expected to be disentangled with which variables? This is an asymptotic question and assumes the representation is fully learned. To fully learn the representations, one can search a proxy model that matches \(\) and \(^{S}\) and the \(}\). Then the proxy model is the learned representation. We do this in the Experiments Section, but note we do not claim that this method of learning the representations is superior to any prior work. Specifically, we implement an approach to train a neural model that is compatible with the diagram to match the given distribution based on normalizing flows. Recently, many graphical constraints proxy neural models have been proposed, and they are trained to fit the given distribution for causal representation learning and downstream tasks . Without our work, one can still try to use these models to learn representations. However, there is no guarantee about how these learned representations is entangled with each other. Our work is the first one to provide general answers for this identification problem. This process can be compared with the identification and estimation problem in classic causal inference. The identification of a specific query given a causal diagram can be answered in symbolic ways , and then if the query is identifiable, one can take the distribution (or data) as input and use estimation methods to obtain the estimated query. Without the identifiability result, there are no guarantees for the estimation.
4. Why not just use observational distributions in each domain as the baseline in the CRID algorithm described in Section 4? **Answer**. One may surmise that this is not efficient and propose to choose the observational distribution in each domain alternatively. However, we argue that this enumeration is needed from two perspectives. First, the observational distributions, namely the idle interventions, are not always given. Second, comparing with observational distributions is not guaranteed to offer diverse \(\) sets. For example, consider intervention targets \(^{(1)}=\{\}^{_{1}},^{(2)}=\{V_{1}^{_{1},},V_{2}^{ _{1},}\},^{(3)}=\{V_{1}^{_{1},},V_{2}^{_{1},}\}\) all applied to the same domain \(_{1}\). Choosing \(=\{\}\) and comparing \(^{(2)}\) and \(^{(3)}\) with the idle intervention \(^{(1)}\), \[[^{(2)},^{(1)},]=[^{(3)},^{(1)},]=\{V_{1},V_{2}\}.\] (71) Comparing \(^{(1)}\) and \(^{(3)}\) with the idle intervention \(^{(2)}\), \[[^{(1)},^{(2)},]=[^{(3)},^{(2)},]=\{V_{2}\}.\] (72) Then using Proposition 3, it is possible to disentangle \(V_{2}\) from \(V_{1}\) with the latter choice. This demonstrates that the observational distribution is not always necessarily the best baseline. Furthermore, consider the challenge of disentangling \(V_{1}\) from \(V_{2}\) in the LCG \(V_{1}\)\(\)\(V_{2}\). As Lemma 6 demonstrates, one can compare two perfect intervention distributions on \(V_{1}\) to achieve ID of \(V_{1}\) wrt \(V_{2}\). In this case, one would not even need the observational distribution.
5. Why distinguish domains and interventions? Are they not the same thing? **Answer**. The literature has typically conflated domains and interventions in the context of causal inference. Many examples across scientific disciplines demonstrate that the notions of domain/environment and interventions are distinct. For example, when making inferences about humans based on data from bonobos, this distinction becomes clear. The difference between the two species is depicted as the environment/domain in this context. A scientist might perform an intervention on a bonobo's kidney (specifically, what we're representing as \(Z\)), and try to determine the effect of medication (\(X\)) on fluid equilibrium in the body (\(Y\)). Although we could intervene on \(Z\) in bonobos and observe its effect on \(X\) and \(Y\)our ultimate goal might be to understand the effect of \(X\) on \(Y\) in humans. It's generally invalid to conflate these two qualitatively different indices, a point first noted by  in the context of transportability analysis. The distinct environments exist regardless of any intervention, such as medication. Also, an intervention on kidney function is different across the two species.  formalized this setting, introducing clear semantics for the S-nodes (environments) that essentially offer a combined representation for both environments. With this foundation, we can now address the more general problem of analyzing data generated from interventions across multiple domains in the latent space. We point the reader to Appendix Section A.3 for a discussion and some examples of how CRID leverages this distinction. More recently, In addition, we provide the following example that we hope further motivates the necessity of distinguishing interventions and domains.

**Example 24** (Disentangled representation with interventions in different domains).: _Consider a ASCM, \(\) over domains bonobos (\(^{1}\)) and humans (\(^{2}\)) that induces the causal chain \(V_{1} V_{2} V_{3} S^{1,2}\). The latent variables are sun exposure (\(V_{1}\)), Age (\(V_{2}\)), Hair Color (\(V_{3}\)). Sun exposure causes aging over time, and aging causes changes in hair color. Hair color looks different across species, which is represented by the S-node. We collect images of their faces, \(=f_{X}(V_{1},V_{2},V_{3})\). Assume we are able to collect images in two interventional settings \(\{V_{1}\}^{_{1}}\) and \(\{V_{1}\}^{_{2}}\), where we modify the level of sun exposure each participant is exposed to. Now, assume we ignore the domain index, and simply treat these two distributions as interventional, since we are intervening on \(V_{1}\). Then prior results would state that a soft (or perfect) intervention on \(V_{1}\) allows it to be disentangled from \(V_{3}\). However, this is incorrect. _
* Is the relaxation of Markovianity important? Since all \(\) are already latent, can one regard the confounding \(\) as \(\) to transfer the model in the non-Markovianity setting to a Markovity model? **answer** Yes the distinction between Markovianity and non-Markovianity is important both qualitatively and quantitatively. Qualitatively, consider the following example in healthcare, where one has access to high-dimensional T1 MRI scans. Let the LCG comprise of Drug Treatment \(\) Outcome, but they are confounded by socioeconomic status (Drug Treatment \(\) Outcome). The drug treatment and outcome are visually discernable on the MRI. However, socioeconomic status does not directly impact how the MRI appears, except through how it impacts the drug treatment efficacy or outcome. The socioeconomic status is therefore an unaccounted confounder in the LCG, and it is important to model this spurious association. If unaccounted for, one may assume that it is possible to disentangle Drug Treatment and Outcome leveraging existing ID results in the literature  even if the results do not apply in this setting. Regarding modeling, an ASCM with confounding cannot be reduced to a Markovian ASCM. Although \(\) and \(\) are both latent, every \(\) is not the direct parents of \(\), which means \(\) cannot be uniquely determined by value of \(\). Take the example where \(V_{1}\)\(\)\(V_{2}\) is the LCG \(G\). Since \(U_{12}\) does not point to \(\), we cannot let \(U_{12}\) be another latent generative factor \(\). Regarding results, we point the reader to Lemma 5, where it is shown that even with one perfect interventions per node, it is not possible to disentangle variables within the same c-component. This in contrast with results in the Markovian setting, where it is shown in  that one perfect intervention per latent variable allows us to achieve full identifiability of every latent variable up to scaling indeterminancies. More broadly, it is noteworthy that transitioning causal reasoning from Markovian to non-Markovian settings was not trivial. For example, it is known that interventional distributions, such as \(P(y do(x))\), are always identifiable from the causal graph and observational distribution in Markovian settings in all models. Moving to non-Markovian settings, the celebrated do-calculus is developed primarily to address the decision problem of whether an interventional distribution can be uniquely computed from a combination of causal assumptions (in the form of a causal diagram) and the observational distribution .

Naturally, the issue of non-identifiability is much more acute in this setting, due to the existence of unobserved confounding.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We claim that we introduce graphical criteria and an algorithm for determining whether latent variables are ID or not from our general definition of ID. We then provide our results in Section 3 and 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the assumptions made in this paper in Appendix Section A.2, and how future work can potentially improve upon our limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: For each proposition and Theorem stated in the main text: Propositions 1, 2, 3, 4, 5 and Theorem 1 are stated in the main text, and proved in Appendix Section C. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details of our experiments in Appendix Section G. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code we used to run experiments is here: https://github.com/tree1111/CDRL. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details of our experiments in Appendix Section G. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We show distribution plots, and do not compute any pvalues. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details of our experiments in Appendix Section G. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss broader impacts in Appendix Section H. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited , which we leveraged their code to produce experimental results shown in the paper. We do not repackage any datasets, or code. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.