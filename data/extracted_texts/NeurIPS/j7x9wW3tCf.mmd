# Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion

Kunxun Qi\({}^{1}\)   Jianfeng Du\({}^{2,3,}\)1   Hai Wan\({}^{1,}\)1

\({}^{1}\) School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China

\({}^{2}\) Guangzhou Key Laboratory of Multilingual Intelligent Processing,

Guangdong University of Foreign Studies, Guangzhou, China

\({}^{3}\) Bigmath Technology, Shenzhen, China

qikx@mail2.sysu.edu.cn, jfdu@gdufs.edu.cn, wanhai@mail.sysu.edu.cn

Corresponding authors

###### Abstract

Learning rule-based systems plays a pivotal role in knowledge graph completion (KGC). Existing rule-based systems restrict the input of the system to structural knowledge only, which may omit some useful knowledge for reasoning, e.g., textual knowledge. In this paper, we propose a two-stage framework that imposes both structural and textual knowledge to learn rule-based systems. In the first stage, we compute a set of triples with confidence scores (called _soft triples_) from a text corpus by distant supervision, where a textual entailment model with multi-instance learning is exploited to estimate whether a given triple is entailed by a set of sentences. In the second stage, these soft triples are used to learn a rule-based model for KGC. To mitigate the negative impact of noise from soft triples, we propose a new formalism for rules to be learnt, named _text enhanced rules_ or _TE-rules_ for short. To effectively learn TE-rules, we propose a neural model that simulates the inference of TE-rules. We theoretically show that any set of TE-rules can always be interpreted by a certain parameter assignment of the neural model. We introduce three new datasets to evaluate the effectiveness of our method. Experimental results demonstrate that the introduction of soft triples and TE-rules results in significant performance improvements in inductive link prediction.

## 1 Introduction

Knowledge graph (KG) consists of real-world facts and has been widely used in many applications, including question answering , recommendation  and information retrieval . A fact in KGs is usually represented by a _triple_ of the form (_head_, _relation_, _tail_), where _head_ and _tail_ are entities. In general, KGs are highly incomplete since the complete set of facts is hard to collect. Therefore, _knowledge graph completion_ (KGC), which aims to infer missing facts from observed ones, has become a vital ingredient for practically completing KGs.

In recent years, there are mainly two categories for prevalent approaches to KGC. One is the embedding-based category [3; 45; 33]. Methods in this category usually learn knowledge graph embeddings (KGE) by encoding entities and relations as low-dimensional real-value vectors. They have been shown to be effective for large-scale KGC, but are hard to generalize to the inductive scenario where missing facts involve previously unseen entities . Besides, they can hardly be interpreted by human due to their black-box nature. The other category tackles KGC by learning rule-based systems, based on search algorithms with pruning heuristics [13; 24] or neural models forapproximate rule learning [46; 30; 28]. They excel in explaining why a missing fact is inferred and are able to handle the inductive setting.

Previous work focuses merely on learning rule-based systems from structural knowledge. However, the high incompleteness of structural knowledge imposes a heavy burden for building high-quality rule-based systems. Text corpora can naturally be considered to enrich structural knowledge as they contain much additional knowledge for reasoning. Figure 1 showcases an example about how textual knowledge help to KGC. The sub-figure on the left side shows a logical rule to infer whether \(x\) has wife \(y\), and we cannot infer that "Barack Obama" has wife "Michelle Obama" since the key fact ("Malia Obama", HasMother, "Michelle Obama") is missing in the background KG. The sub-figure on the right side shows that text corpora can provide supporting texts to this key fact, thereby the target relation HasWife can be correctly inferred.

Motivated by this kind of examples, we aim to learn rule-based systems from both structural and textual knowledge. To this end, we propose a two-stage framework named _Learning from Structural and Textual Knowledge_ or _LSTK_ for short, as illustrated in Figure 2. In the first stage, a distantly-supervised method using a textual entailment model is proposed to extract textual knowledge from texts. Distant supervision  posits that if two entities involve a relation, then any sentences that mention these two entities might express that relation. Based on this assumption, we generate a set of triples with their mentioned texts and employ a textual entailment model equipped with a multi-instance learning mechanism to estimate whether a triple is entailed by a set of sentences. Each generated triple has an estimated confidence score denoting the supporting degree of the triple by texts, and thus it is called a _soft triple_. In the second stage, both the generated soft triples and hard triples (i.e. existing triples) are used to learn a rule-based system for KGC.

In practice, it is challenging to learn rules from soft triples, as the soft triples generated by distant supervision may contain noise. To mitigate the negative impact of noisy data, we propose a new formalism for rules to be learnt, named _text enhanced rules_ or _TE-rules_ for short. In this formalism, _textual relations_ are introduced to provide a flexible mechanism to discard noisy soft triples. In other word, a TE-rule is extended from a chain-like rule by adding atoms or disjunction of atoms possibly with textual relations to the rule body. To effectively learn TE-rules, we propose a neural model named _TE-rule Learning Model_ or _TELM_ for short. We show that an arbitrary set of TE-rules can be encoded by a certain parameter assignment of TELM (see Theorem 1). This theoretical result guarantees a certain degree of faithfulness between TELM and TE-rules, enabling us to extract explainable TE-rules from the parameter assignment of the learnt TELM. Based on the extracted TE-rules, LSTK is able to provide explanations for inferred missing facts by backward reasoning.

We enhance three datasets from the field of relation extraction for empirical evaluation. Experimental results demonstrate significant gains achieve by the proposed method LSTK-TELM in inductive link prediction. Our ablation study and case study further clarify why the introduction of soft triples and TE-rules help to improve the performance.

## 2 Preliminaries

**Knowledge Graph.** Given a set of entities \(\) and a set of relations \(\), a knowledge graph \(\) is a subset of \(\). Specifically, \(=\{(h_{i},r_{i},t_{i})\}_{1 i N}\), where \(N\) denotes the number of triples, \(h_{i}\) the _head_ entity for the \(i^{}\) triple, \(r_{i}\) the relation for the \(i^{}\) triple and \(t_{i}\) the _tail_ entity for the \(i^{}\) triple. By \(r^{-}\) we denote the inverse relation of \(r\). The set of inverse relations for

Figure 1: Reasoning from both structural and textual knowledge.

\(\), namely \(\{r^{-} r\}\), is denoted by \(^{-}\). Accordingly, the equivalent knowledge graph for \(\) composed by inverse relations, namely \(\{(t,r^{-},h)(h,r,t)\}\), is denoted by \(^{-}\).

**Logical Rule.** Most existing work focuses on learning chain-like rules (CRs). A CR is essentially a plain datalog rule  where all atoms are binary and every body atom shares variables with the previous atom and the next atom. Formally, a CR \(R\) with \(L\) body atoms, simply called an \(L\)-CR, is of the form:

\[H(x,y) B_{1}(x,z_{1}) B_{2}(z_{1},z_{2})... B_{L}(z _{L-1},y)\]

where \(x\) is the head entity, \(y\) the tail entity, and \(z_{1}\),...,\(z_{L-1}\) variables. The part at the left (resp. right) side of \(\) is called the _head_ (resp. _body_) of \(R\). The rule \(R\) is called \(r\)-specific if \(H=r\). By \(H_{R}\) and \(B_{R}\) we denote the atom in the head of \(R\) and the set of atoms in the body of \(R\), respectively. An atom or a rule is _ground_ if it does not contain any variable. A rule \(R\) is a fact if \(B_{R}\) is empty and \(H_{R}\) is ground. In this paper, a fact or a ground atom \(r(a,b)\) and a _triple_\((a,r,b)\) are used interchangeably. To uniformly represent rules using fixed-length bodies, we introduce the _identity relation_ (denoted by \(I\)) to rule bodies. For example, \(r(x,y) s(x,y)\) can be converted into a rule with two body atoms, namely \(r(x,y) s(x,y) I(y,y)\).

We say \( H_{R}(a,b)\) if there exists a ground instance \(R_{g}\) of \(R\) such that \(H_{R}(a,b)=H_{R_{g}}\) and \(B_{R_{g}}^{-}\{I(e,e) e\}\). Let \(\) be a set of \(r\)-specific CRs and \((a,r,b)\) a triple. We say \(_{}(a,r,b)\) if there exits a logical rule \(R\) such that \( H_{R}(a,b)\). We say a triple \((a,r,b)\) is _plausible_ in \(\) if there exists a set of possibly correct \(r\)-specific CRs \(\) such that \(_{}(a,r,b)\).

**Link Prediction.** Link prediction is the main task of knowledge graph completion that we focus on in this work. Given a knowledge graph \(\), a head query \((?,r,t)\) or a tail query \((h,r,?)\), the task of link prediction aims to find all entities \(e\) such that \((e,r,t)\) for \((?,r,t)\) or \((h,r,e)\) for \((h,r,?)\) is plausible in \(\). The _inductive learning setting_ requires that at least one entity in the test set should not appear in the training set, while relations in the test set also appear in the training set.

## 3 Methodology

In this section, we describe the proposed LSTK framework for inductive KGC with text corpus. We first formalize our problem setting as follows.

**Problem Setting.** Given a set of triples and a corpus for training \(_{}=(_{},_{ })\), a set of triples and a corpus for test \(_{}=(_{},_{ })\), our inductive KGC setting aims to learn a KGC system based on \(_{}\), and then evaluate the learnt system on \(_{}\). During evaluation, given a head query \((?,r,t)\) or a tail query \((h,r,?)\), the learnt KGC system finds an answer with the highest estimated truth degree to answer this query, based on the background knowledge from \((_{}_{}\{(h,r,t) \},_{}_{})\).

This setting is motivated by real-world application scenarios where we need to fetch texts from search engines to find evidences to verify a new fact. To address this problem setting, we propose _Learning from Structural and Textual Knowledge_ or _LSTK_ for short, as illustrated in Figure 2. Specifically, the proposed LSTK framework has two stages, where the first stage aims to generate soft triples from a text corpus and the second one aims at training a rule-based system for KGC. Furthermore, LSTK is able to provide an explanation to show why an inferred missing fact is plausible in the background knowledge, by tracking back the reasoning paths from both hard triples and soft triples, where soft triples can further be explained by tracking back the supporting texts.

### Formalization of LSTK

In the first stage, our goal is to find all triples that are possibly entailed by the given text corpus. We achieve this goal by adopting distant supervision. As far as we know, distant supervision is mostly applied in relation extraction, where the relation between any two entities is restricted in a close set. However, we naturally hope to discover more open relations that can be entailed by the text corpus. Considering that textual entailment  can deal with open relations and is better at exploiting the semantic information of relational contexts, we impose distant supervision to a textual entailment model to generate soft triples.

**Distantly supervised data generation.** Given a knowledge graph \(\) and a set of sentences (i.e. a text corpus) \(=\{s_{i}\}_{1 i N_{}}\), we construct a training set \(_{}=\{(_{i},S_{i},y_{i})\}_{1 i N_{ }}\) by finding all sentences in \(\) that mention the entity pair \((h_{i},t_{i})\) of \(_{i}\), where \(N_{}\) denotes the number of sentences, \(N_{}\) the number of training instances, \(_{i}=(h_{i},r_{i},t_{i})\) a triple, \(S_{i}\) the set of sentences in \(\) mentioning the entity pair \((h_{i},t_{i})\), and \(y_{i}\) the label. We call a triple \((h,r,t)\) a _positive triple_, whereas a triple \((h,r,t)\) a _negative triple_. For each positive triple \((h_{i},r_{i},t_{i})\), we append to the training set the set of negative triples \(\{(h_{i},r_{j}^{},t_{i}) r_{j}^{},r_{j}^{}  r_{i},(h_{i},r_{j}^{},t_{i})\}\) obtained by corrupting \(r_{i}\). Accordingly \(y_{i}\) is set to 1 if \(_{i}\) or 0 otherwise. Considering that some entity pairs may appear in a large number of sentences, to confine the number of considering sentences, we exploit the well-known graph partitioning algorithm METIS  to evenly partition the original \(S_{i}\) to make every part no more than 20 sentences, and then treat the densest part defined by Jaccard similarity between bags of words as the ultimate \(S_{i}\). Finally, we likewise construct the test set \(_{}=\{(_{i},S_{i})_{i} ,S_{i}\}\) for generating soft triples where labels are not given.

**Multi-instance learning for RTE.** Recognising textual entailment (RTE)  aims at determining whether a sentence (called _hypothesis_) can be inferred by the other sentence (called _premise_). We employ a textual entailment model based on the pre-trained language model BERT  to estimate the confidence score for every triple. Specifically, given \(_{}=\{(_{i},S_{i},y_{i})\}_{1 i N_{ }}\), we create a set of sequences \(X_{i}=\{x_{i,j}\}_{1 j N_{g}}\) for each instance in \(_{}\) by filling the template "[CLS]<triple>[SEP]<sentence>[SEP]", where [CLS] and [SEP] are special tokens in BERT, <triple> denotes the slot to be filled by the context of \(_{i}\) and <sentence> the slot to be filled by the context of \(s_{j} S_{i}\). All the sequences in \(X_{i}\) are fed to BERT to calculate their contextual representations and the contextual embedding of the [CLS] token are used to calculate the entailment probability by a full-connected layer activated by the sigmoid function. By \(_{i,j}\) we denote the entailment probability for \(x_{i,j}\), then the entire entailment probability \(_{i}\) for \(X_{i}\) is calculated by \(_{i}=_{1 j N_{S_{i}}}_{i,j}\). The entire model is trained by minimizing the cross-entropy loss.

**Soft triple generation.** The trained model is then applied to \(_{}\) for generating soft triples. A soft triple is defined as a quadruple \((h,r,t,)\), where \((h,r,t)\) denotes a triple and \(\) its corresponding predicted entailment probability. Given a hyper-parameter \(\) to keep only highly confident soft triples, the set of soft triples that will be used in learning chain-like rules is defined as \(_{}=\{(h_{i},r_{i},t_{i},_{i})(h_{i},r_{i},t_{i })_{},_{i}\}\).

### Formalization of Text Enhanced Rules

In the second stage, we aim at learning logical rules from both hard (i.e. existing) and soft triples. However, it is challenging to learn rules from the background KG with soft triples, as the generated

Figure 2: Overview of the proposed LSTK framework.

soft triples by distant supervision may contain noise, which impairs the quality of learnt rules. The following example illustrates a case where noisy data impact the reasoning ability of a chain-like rule.

**Example 1**.: _Considering the chain-like rule \(R\) in Figure 1: \((x,y)(x,z)(z,y)\). Suppose we have two soft triples "(A, \(\), B, 0.55)" and "(B, \(\), C, 0.90)" computed by the textual entailment model, where the former one is incorrect in the real world. From this rule \(R\) the negative triple "(A, \(\), C)" will be incorrectly inferred to be positive._

To alleviate the negative impact of noise from soft triples, a simple way is to restrict that some atoms in the rule should appear in the structural knowledge as existing triples. To this end, we extend chain-like rules by adding atoms or disjunctions of atoms that may involve _textual relations_. We call such extended chain-like rules _text enhanced rules_ and simply TE-rules. Let \(r_{}\) denote the textual relation corresponding to the relation \(r\), then a TE-rule can be defined below.

**Definition 1**.: _A \(p\)-specific \(L\)-TE-rule \(R\), simply a TE-rule if \(p\) and \(L\) are clear from the context, is of the form: \(p(x,y)_{1}(x,z_{1})_{2}(z_{1},z_{2}) ..._{L}(z_{L-1},y)\), where \(_{l}\) can be an original relation, a textual relation, the identity relation, or a disjunction of an original relation and its corresponding textual relation, i.e., \(_{l}(u,v)\) is of the form \(r(u,v) r_{}(u,v)\)._

The following example explains why TE-rules can be used to resolve the error in Example 1.

**Example 2**.: _Considering Example 1 again. By allowing TE-rules to be learnt, a learnt rule \(R^{}\) can be: \((x,y)(x,z)((z,y)_{}(z,y))\). From \(R^{}\) the negative triple "(A, \(\), C)" will not be inferred to be positive unless there exists a positive hard triple \((A,,B)\) for some \(B\)._

### End-to-end Learning of Text Enhanced Rules

To effectively learn TE-rules, we propose an end-to-end neural model named _TE-rule Learning Model_ (_TELM_ for short). The intuition of TELM is to select appropriate parameters to simulate the inference of TE-rules by gradient descent. Formally, given a set of hard triples \(\), a set of soft triples \(_{}\), the maximum number \(L\) of body atoms in every rule, the maximum number of rules \(N\), we first extend \(\) to \(_{}=\{(h,r,t,1.0)|(h,r,t)\}\). Suppose \(=\{r_{i}\}_{1 i n}\), its corresponding set of inverse relations \(^{-}=\{r_{i}\}_{n+1 i n}\), its corresponding set of textual relations \(_{}=\{r_{i}\}_{2n+1 i 3n}\), its corresponding set of inverse textual relations \(_{}^{-}=\{r_{i}\}_{3n+1 i 4n}\), as well as \(I=r_{4n+1}\). Let \(_{}=_{}_{}^{-}_{}_{}^{-} \{(e,I,e,1.0) e\}\). For \(1 k N\), \(1 l L\), TELM estimates a truth degree \(_{r,x,y}^{(k,l)}\) for each triple \((x,r,y)\), defined by:

\[_{r,x,y}^{(k,l)}=(_{i=1}^{4+1} _{i}^{(r,k,L)}((x,r_{i},y,)_{ })),&l=1\\ (_{i=1}^{4n+1}_{i}^{(r,k,L-l+1)}_{z, :(x,r_{i},z,)_{}}_{r,z,y}^{(k,l-1)} ),&2 l L\] (1)

where \((x)=((x,1),0)\) is an activation function that confines the given value to \(\), \(( C)\) is a function that returns \(\) if \(C\) is true or 0 otherwise, and \(^{(r,k,l)}^{4n+1}\) denotes the trainable relational selection weights for the \(l^{}\) body atom of the \(k^{}\) rule for the head relation \(r\). Intuitively, Equation (1) simulates the inference of TE-rules under the background KG with soft triples. The selection weights of all predicates for body atoms are formally defined as:

\[^{(r,k,l)}=[^{(r,k,l)}w_{}^{(r,k,l)};^{(r,k,l)}w_{ }^{(r,k,l)};w_{4n+1}^{(r,k,l)}]\] (2)

where \([;]\) denotes the concatenation operator, and \(w_{}^{(r,k,l)}^{2n}\) (resp. \(w_{}^{(r,k,l)}^{2n}\) or \(w_{4n+1}^{(r,k,l)}\)) denotes the original relation selection weights (resp. textual relation selection weights or the identity relation selection weight) for the \(l^{}\) atom of the \(k^{}\) rule for the head relation \(r\); further, \(^{(r,k,l)}\) (resp. \(^{(r,k,l)}\)) denotes the trainable selection weight for determining whether the original (resp. textual) relation is involved in the \(l^{}\) atom. Intuitively, \([w_{}^{(r,k,l)}]_{i}=1\) (resp. \([w_{}^{(r,k,l)}]_{i}=1\) or \(w_{4n+1}^{(r,k,l)}=1\)) denotes that the \(i^{}\) original relation \(r_{i}\) (resp. the \(i^{ th}\) textual relation \(r_{2n+i}\) or the identity relation \(r_{4n+1}\)) is selected as the predicate of the \(l^{ th}\) atom. Note that \(^{(r,k,l)}=1^{(r,k,l)}=1\) indicates that the disjunction of an atom with an original relation and an atom with a textual relation is involved in the \(l^{ th}\) atom. We confine \(^{(r,k,l)}\) and \(^{(r,k,l)}\) to \(\) by sigmoid layers, normalize both \(w^{(r,k,l)}_{ orig}\) and \(w^{(r,k,l)}_{ text}\) by softmax layers, and restrict \(w^{(r,k,l)}_{4n+1}\) to be \((0,1-^{(r,k,l)}-^{(r,k,l)})\) so as to enforce \(^{(r,k,l)}=^{(r,k,l)}=0\) when \(w^{(r,k,l)}_{4n+1}=1\).

The final predicted truth degree is calculated by a weighted sum of predicted degrees for \(N\) rules:

\[^{(N,L)}_{r,x,y}=_{k=1}^{N}u^{(k)}_{r}^{(k,L)}_{r,x,y}\] (3)

where \(u^{(k)}_{r}[-1,1]\) is a trainable weight for the \(k^{ th}\) rule for the head relation \(r\), which is confined to \([-1,1]\) by a tanh layer. Then the model is trained by minimizing the objective function

\[=-_{(x,r,y)}_{r,x,y})} {_{e}(^{(N,L)}_{r,e,y})}\] (4)

We then show the faithfulness between the formalization of TELM and TE-rules in Theorem 1, which relies on the notion of induced parameters as defined below.

**Definition 2**.: _Given a set of \(r\)-specific \(L\)-TE-rules \(=\{R_{k}\}_{1 k N}\) for \(R_{k}\) of the form \(r(x,y)_{k,1}(x,z_{1})..._{k,L}(z_ {L-1},y)\) with \(_{k,l}(u,v)\{r_{k,l}(u,v),r^{}_{k,l}(u,v),r^{}_{k,l} (u,v) r^{}_{k,l}(u,v)\), where \(r_{k,l}^{-}\{I\}\), \(r^{}_{k,l}_{ text}^{-}_{ text}\), \(r^{}_{k,l}^{-}\), and \(r^{}_{k,l}\) is the corresponding textual relation of \(r^{}_{k,l}\), we call a parameter assignment of TELM \(^{(N,L)}_{r}=\{[w^{(r,k,l)}_{ orig})_{i},[w^{(r,k,l)}_{ text}]_{i} \}_{1 k N,1 l L,1 i 2n}\{w^{(r,k,l)}_{ int +1},^{(r,k,l)},^{(r,k,l)}\}_{1 k N,1 l L}\{u^{( k)}_{r}\}_{1 k N}\)\(\)-induced if it satisfies the following conditions for all \(1 k N,1 l L\):_

1. \( 1 i 2n:[w^{(r,k,l)}_{ orig}]_{i}=1\) _if_ \(r_{i}\) _appears in_ \(_{k,l}\)_, otherwise_ \([w^{(r,k,l)}_{ orig}]_{i}=0\)_._
2. \( 1 i 2n:[w^{(r,k,l)}_{ text}]_{i}=1\) _if_ \(r_{2n+i}\) _appears in_ \(_{k,l}\)_, otherwise_ \([w^{(r,k,l)}_{ text}]_{i}=0\)_._
3. \(w^{(r,k,l)}_{4n+1}=1\) _if_ \(r_{4n+1}\) _appears in_ \(_{k,l}\)_, otherwise_ \(w^{(r,k,l)}_{4n+1}=0\)_._
4. \(^{(r,k,l)}=1\) _if an original relation appears in_ \(_{k,l}\)_, otherwise_ \(^{(r,k,l)}=0\)_._
5. \(^{(r,k,l)}=1\) _if a textual relation appears in_ \(_{k,l}\)_, otherwise_ \(^{(r,k,l)}=0\)_._
6. \(u^{(k)}_{r}=1\)_._

**Theorem 1**.: _Let \(\) be a set of hard triples, \(_{ soft}\) a set of soft triples, \((a,r,b)\) an arbitrary triple, \(=\{R_{k}\}_{1 k N}\) a set of \(r\)-specific \(L\)-TE-rules and \(^{(N,L)}_{r}\) the \(\)-induced parameter assignment of TELM, then \(^{(N,L)}_{r,a,b}>0\) if and only if \(_{ soft}_{}r(a,b)\)._

**Explaining by Backward Reasoning.** Thanks to the faithfulness between TELM and TE-rules, we can extract TE-rules by interpreting the parameters of a learnt TELM model using beam search algorithm (see Algorithm 1 in the appendix). For each inferred missing fact, LSTK yields at least one reasoning path as explanation by backward reasoning. Given a triple \((h,r,t)\) that is estimated to be positive, a set of extracted TE-rules, the background KG \(_{ mix}\) and the corpus \(\), we can find a path between \(h\) and \(t\). Whenever the path involves a soft triple, we treat the sentence in \(\) that has the maximum confidence score as the supporting text for this soft triple.

## 4 Experimental Evaluation

### Data Construction and Evaluation Metrics

We collected three benchmark KGs with corresponding text corpora from the field of relation extraction for empirical evaluation, which are HacRED2, DocRED3 and BioRel4.

Considering that these datasets originally have a number of relations that can hardly appear in heads of potential logical rules, we filtered these relations by applying AMIE+  to mine logical rules that have up to three body atoms from the complete set of existing triples for each dataset and omitting the relations that have not been involved in the body atoms of any mined rules. Under the inductive setting, there should be entities in the test set that do not appear in the training set. Therefore, we applied the partitioning algorithm METIS  to the entity graph where an edge indicates that the two end-points have at least one filtered relation, so as to divide the set of entities into 10 subsets. The train/valid/test split of each dataset was then extracted from 8/1/1 of these subsets by omitting triples across different splits. Statistics on all above datasets are reported in Table 1.

Following , we reported the Mean Reciprocal Rank (MRR) and Hit@k metrics under the filtered setting  for empirical evaluation.

### Implementation Details

We implemented the textual entailment model by Pytorch 1.10.0. The model was initialized by the pretrained language model BERT with 12 transformer layers, which outputs 768-dimensional (i.e. \(d=768\)) token embeddings. Then it was trained by Adam  with warm up , where the initial learning rate was set to 5e-5, the mini-batch size to 8 and the maximum number of training epochs to 3. We applied dropout  to each layer by setting the dropout rate to 0.1.

We implemented TELM5 by Pytorch 1.10.0. It was trained on an A100 GPU with 40GB memory by Adam  with 50 training epochs for HacRED and DocRED, and 20 for BioRel. The hyper-parameters were set to maximize MRR on the validation set. The initial learning rate was set to 1e-1 and the mini-batch size to 32 for all datasets. We also applied dropout  to the output states by setting the dropout rate to 0.3. The maximum length of each rule \(L\) is set to 3 for HacRED and DocRED, and 2 for BioRel. The maximum size of learning rules \(N\) is set to 20 for all datasets.

### Main Results

We conducted experiments on the three enhanced datasets for link prediction under the inductive learning setting, where existing triples in the training set are used as background KG for training, existing triples in training and validation sets are used as background KG for validation, and existing triples in training, validation and test sets are used for test. The hyper-parameter \(\) to filter soft triples with high confidence scores was set to 0.5 in all experiments.

Table 2 reports the comparison results on three datasets. Textual Copy Rule (simply TCR) is a baseline that uses only _textual copy rules_ of the form "\(r(x,y) r_{}(x,y)\)" in reasoning instead of using any other rules or neural models. TCR has not any extra logical reasoning ability beyond the textual entailment model. By LSTK-X\({}^{}\) we denote enhanced models that use soft triples in the background KG, where X denotes the original model. By LSTK-X we denote enhanced models that further extends LSTK-X\({}^{}\) by allowing textual relations to appear in learnt rules. Results show that the proposed method (i.e. LSTK-TELM) significantly outperforms all the baseline methods with p-value \(<0.05\) by two-tailed t-tests. Specifically, LSTK-TELM outperforms LSTK-DRUM by absolute gains of 24.8%/4.2%/22.4% in Hit@1 scores on the HacRED/DocRED/BioRel datasets, respectively. Further, we can see from the comparison results between LSTK-X\({}^{}\) and LSTK-X

   Dataset & \#Ent. & \#Rel. & \#Train & \#Valid & \#Test & \#Texts & \#Soft. & PUE (exist) & PUE (soft) & PUE (all) \\  HacRED & 20,800 & 12 & 20,637 & 2,499 & 2,551 & 4,578 & 38.8M & 98.7\% & 31.6\% & 30.0\% \\ DocRED & 17,997 & 28 & 24,384 & 2,819 & 3,844 & 19,517 & 4.7M & 83.7\% & 84.9\% & 67.8\% \\ BioRel & 15,566 & 34 & 25,854 & 2,406 & 3,032 & 150,687 & 123.4M & 79.4\% & 62.1\% & 46.8\% \\   

Table 1: Statistical information for each dataset, where \(\#\)Ent. (resp. \(\#\)Rel.) denotes the number of entities (resp. relations), \(\#\)Train/Valid/Test respectively the number of triples for training/validation/test, \(\#\)Texts the number of texts and \(\#\)Soft. the number of extracted soft triples. PUE (short for Proportion of Unseen Entities) denotes the proportion of triples in the test set involving unseen entities in the training triples, where exist/soft/all respectively denote that the training triples are treated as existing triples in the original training set, as extracted soft triples, or as both existing triples and soft triples.

that, the introduction of textual relations bring significant performance improvements. For LSTK-NeuralLP\({}^{}\) and LSTK-DRUM\({}^{}\), their performances are even worse than their original models that do not use any soft triples, indicating some negative impact of noise from soft triples.

We further conducted ablation studies on several variants of LSTK-TELM to verify the effectiveness of key components in both LSTK and TELM. In (1) we omitted all the soft triples in the background KG (i.e., only hard triples are used). Results show that the LSTK framework (using soft triples) pushes TELM by a significant margin with p-value 1.1e-6. It implies that the use of soft triples help to greatly improve the performance. In (2) we omitted the corresponding soft triple for each hard triple in the background KG. Results show that this variant model already significantly outperforms the variant model in (1), indicating that soft triples help to improve the reasoning ability without using textual copy rules. In (3) we omitted the textual relations in TE-rules. Results show that the use of textual relations pushes LSTK-TELM by a significant margin with p-value 5.1e-5. It can be explained by the fact that the separation of original and textual relations helps to discard the noisy soft triples, resulting in higher reasoning performance. In (4) we omitted the learning of disjunctive atoms of the form \(r(u,v) r_{}(u,v)\) in TELM. Results show that the introduction of disjunctive atoms bring significant performance gains with p-value 2.8e-4. This confirms that the introduction of disjunctive atoms in TE-rules provides a more flexible mechanism to control noise. In (5) we fixed the confidence scores (i.e. entailment probabilities) of soft triples to 1.0. Results show that the performance significantly drops with p-value 8.4e-3. It implies that the computed entailment probabilities reflect the truth degrees of supporting facts and help to learn more informative rules. In (6) we trained a state-of-the-art relation extraction model proposed in  based on BERT to replace the textual entailment model for generating soft triples. Results show that the use of relation extraction model leads to a significant performance degradation with p-value 1.9e-4. This may be due to that a textual entailment model can deal with open relations and exploit more text semantics on relations than a relation extraction model does.

### Case Study

To clarify why the learning of TE-rules help improve the performance, we conducted case study for applying learnt logical rules from LSTK-TELM on the DocRED dataset, as shown in Table 3. In the first case, the logical rule is extracted from LSTK-TELM and can be applied to infer the positive triple "\((,,)\)". We can see from this case that the textual entailment model is able to infer from the relation mention "biological child" that, the entities "Lois" and "Clark" are spouse, thereby yielding a supporting soft triple "\((,,,0.76)\)" for reasoning. In the second case, it can be seen that the same logical rule in the first case cannot infer that the triple "\((,,)\)" is positive, since the key supporting fact "\((,,)\)" is absent in the background KG. In the third case, the logical rule is "\((x,y)((x,z_{0})_{ }(x,z_{0}))_{}(x,z_{0})\)

   &  &  &  &  \\  & MRR & H@1 & H@3 & H@10 & & MRR & H@1 & H@3 & H@10 & MRR & H@1 & H@3 & H@10 \\  Textual Copy Rule (TCR) & 0.382 & 31.5 & 42.2 & 51.5 & 0.030 & 1.8 & 3.6 & 5.4 & 0.074 & 4.5 & 8.2 & 13.4 & 3.3e-9 \\ AME+  & 0.122 & 11.1 & 13.5 & 13.9 & 0.217 & 16.7 & 26.0 & 29.5 & 0.156 & 10.7 & 15.3 & 18.6 & 2.0e-7 \\ LSTK-AMIE\({}^{}\) & 0.177 & 13.3 & 19.0 & 28.2 & 0.259 & 21.4 & 29.1 & 33.4 & 0.138 & 10.2 & 15.0 & 20.2 & 19.7e-7 \\ LSTK-AMIE\({}^{}\) & 0.122 & 10.7 & 13.2 & 15.1 & 0.302 & 26.1 & 33.0 & 37.7 & 0.116 & 9.5 & 12.9 & 15.2 & 1.3e-3 \\ RNNDie  & 0.162 & 15.1 & 17.2 & 17.8 & 0.226 & 15.7 & 26.3 & 35.6 & 0.172 & 12.0 & 19.3 & 28.7 & 1.9e-7 \\ LSTK-RNNLogic\({}^{}\) & 0.234 & 16.6 & 26.2 & 37.3 & 0.343 & 24.9 & 40.4 & 52.1 & 0.148 & 10.0 & 16.0 & 32.2 & 5.8e-6 \\ LSTK-RNNLogic & 0.394 & 32.8 & 42.3 & 52.1 & 0.396 & 31.6 & 45.1 & 53.9 & 0.284 & 20.3 & 33.8 & 43.5 & 4.8e-6 \\ NeuralLP  & 0.395 & 34.4 & 42.6 & 48.9 & 0.310 & 24.0 & 35.4 & 0.34 & 0.317 & 23.2 & 26.8 & 46.2 & 1.3e-8 \\ LSTK-NeuralLP\({}^{}\) & 0.320 & 21.1 & 35.4 & 56.7 & 0.382 & 29.2 & 43.5 & 55.2 & 0.304 & 21.2 & 34.5 & 47.2 & 1.6e-5 \\ LSTK-NeuralLP\({}^{}\) & 0.484 & 38.1 & 53.4 & 67.5 & 0.488 & 40.9 & 54.2 & 62.2 & 0.384 & 27.6 & 44.9 & 58.1 & 4.5e-4 \\ DRUM  & 0.387 & 33.6 & 41.4 & 48.5 & 0.352 & 28.1 & 24.2 & 47.7 & 0.314 & 23.1 & 36.2 & 46.4 & 3.0e-7 \\ LSTK-DRUM\({}^{}\) & 0.347 & 23.8 & 39.6 & 56.0 & 30.92 & 29.1 & 46.3 & 56.5 & 0.292 & 20.4 & 32.9 & 46.0 & 1.5e-5 \\ LSTK-DRUM & 0.538 & 43.3 & 59.8 & 72.6 & 0.482 & 40.0 & 54.1 & 62.0 & 0.421 & 31.7 & 48.3 & 60.1 & 3.1e-4 \\  LSTK-TELM (this work) & **0.734** & **68.1** & **77.1** & 83.2 & **0.514** & **44.2** & **56.0** & **63.9** & **0.600** & **54.1** & **62.9** & **69.6** & - \\ (1) w/o soft triples & 0.416 & 37.7 & 43.6 & 48.1 & 0.380 & 30.8 & 44.0 & 49.4 & 0.339 & 25.9 & 38.3 & 48.1 & 1.1e-6 \\ (2) w/o textual copy rules & 0.710 & 65.0 & 74.9 & 82.1 & 0.502 & 43.2 & 54.6 & 62.7 & 0.511 & 44.6 & 54.7 & 62.8 & 2.1e-3 \\ (3) w/o textual relations & 0.579 & 48.2 & 64.3 & 76.4 & 0.438 & 35.5 & 49.1 & 58.5 & 0.356 & 25.4 & 40.3 & 54.8 & 5.1e-5 \\ (4) w/o disjunctive atoms & 0.693 & 63.3 & 73.2 & 80.6 & 0.453 & 38.4 & 49.6 & 57.7 & 0.430 & 36.3 & 45.7 & 55.6 & 2.8e-4 \\ (5) w/o confidence scores & 0.730 & 67.3 & 76.9 & 83.0 & 0.508 & 43.4 & 55.7 & 63.4 & 0.574 & 50.8 & 61.3 & 69.5 & 8.4e-3 \\ (6) Using relation extraction & 0.723 & 66.1 & 76.7 & **83.3** & 0.499 & 43.0 & 54.5 & 61.8 & 0.572 & 50.7 & 60.9 & 68.8 & 1.9e-4 \\  

Table 2: Comparison results on the HacRED, DocRED and BioRel datasets.

\(^{-}_{}(z_{0},y)\)". In this case, the textual entailment model mistakenly yields a supporting soft triple "\((,,,0.53)\)" based on the words "beatified Father", thereby wrongly inferring the triple "\((,,)\)" to be positive. In general, these cases show that the use of soft triples is crucial in yielding a plausible explanation for a test triple and that the separation of original and textual relations provides a flexible mechanism to discard noisy soft triples.

## 5 Related Work

### Knowledge Graph Completion

There are two categories of prominent approaches to knowledge graph completion. They are embedding-based and rule-based, respectively.

**Embedding-based methods.** Knowledge graph embeddings (KGE) [3; 39; 19; 45; 35; 33] are a kind of well-known embedding-based methods. They aim to represent entities and relations in KGs as low-dimensional real-valued vectors . Although KGE methods have been shown to be effective in large-scale KGC, they can hardly generalize to the inductive learning setting. Besides, KGE methods are hard to be interpreted by human due to their black-box nature. In recent years, graph neural networks (GNNs) [34; 6; 23; 43; 50] have shown promissing results in addressing KGC. They can also be considered as embedding-based, as they also learn latent representation for reasoning. While GNN-based methods have been shown to excel in handling the inductive learning setting, we have not incorporated GNNs with LSTK. The reasons are three-fold. Firstly, GNNs are still black-box models with limited interpretability, whereas the proposed LSTK framework is committed to explain why a missing fact is inferred. Secondly, GNN-based methods necessitate sub-graph extraction to handle test triples involving unseen entities. The process of sub-graph extraction becomes highly time-consuming when dealing with a large number of soft triplets, e.g., it takes about 10 hours for GraIL  to process 1M triples for sub-graph extraction. Thirdly, the confidence scores from soft triples cannot be directly incorporated in GNN-based methods, unless the message passing mechanism in GNN-based methods is redesigned to handle the confidence scores on links.

**Rule-based methods.** Different from embedding-based methods, rule-based methods aim at building effective rule-based systems for KGC. They can naturally reason under the inductive learning setting and provide logical rules as explanations. Learning rule-based systems has been widely studied in the field of Inductive Logic Programming (ILP). ILP methods such as AMIE+  and AnyBURL  usually impose search algorithms with pruning heuristics to mine logical rules in a generate-and-test manner. RNNLogic  extends this manner to learn logical rules and their weights interactively based on neural models. In recent years, neural approximate methods are proposed to learn rule-based

   Triple (fact) & Logical rule & Supporting triple/text & Pred. & Label \\    } &   } & Supporting hard triple: (Superboy, Father, Clark) &  \\  & & Supporting soft triple: (Lois, Spouse, Clark, \(0.76\)) &  \\  & & Supporting text: Clark and Louis’ biological child in & \\  & \(_{}(x,z_{0})^{-}(z_{0},y)\) & DC Comics cann’s own born in Convergence. & \\  & & Superman\#2 (July 2015), a son named Jonathan &  \\  & & Samuel Kent, who eventually becomes Superboy. & \\    } &   } & Supporting hard triple: \\  & & Supporting soft triple: (John, Spouse, Martyn, \(0.93\)) &  \\  & \(_{}(x,z_{0})^{-}(z_{0},y)\) & Supporting text: The Road to Ruin is a 1970 album released by husband and wife John and Beverley Martyn. & \\    } &   } & Supporting soft triple: (Linda, Spouse, Paul, 0.88) &  \\  & & Supporting text: In 1998, after Linda’s death, Paul & \\  & & rearranged the song for string quartet to be played at & \\   & \((x,y)((x,z_{0})^{-}_{}(z_{0},y)\) & Supporting soft triple: (Joseph, Father, Paul, \(0.53\)) & \\   & & Supporting text: In 1995, at a ceremony in Colombo, & \\   & & Pop John Paul Paul I beatified Father Joseph Van, an & \\   & & early missionary to the country, who is known as the & \\   & & Apostle of Ceylon. & \\   

Table 3: Examples for applying learnt TE-rules from LSTK-TELM on the DocRED dataset, where Pred. abbreviates prediction. The words marked as blue denote entities and the words marked as red relation mentions. \(r^{-}\) denotes the inverse relation of \(r\).

systems from structural knowledge in KGs by learning continuous parameters using Tensorlog  operators. State-of-the-art neural approximate methods include NeuralLP , DRUM , NLL  and Ruleformer . In addition to applying Tensorlog, other neural approximate methods such as NTP  and CTP  learn logical rules based on neural theorem provers. All these methods learn rule-based systems from structural knowledge alone and most of them focus merely on learning chain-like rules. In contrast, we study learning more expressive TE-rules from both structural and textual knowledge.

**Other approaches using textual information.** It is worth noting that there are several approaches that focus on enhancing KGs with texts. For instance, recent advance of pre-trained language models (PLMs) for KGC such as KEPLER , PKGC  and SimKGC  have considered textual information of triples in KGs. The differences between PLM-based methods and LSTK are two-fold. On one hand, they consider different types of textual knowledge. In more detail, PLM-based methods consider text information such as contexts of triples, descriptions aligned to entities and pre-trained textual knowledge. In contrast, LSTK is proposed to address the KGC scenario where some knowledge (i.e., real-world facts) is provided in the structured KG and other knowledge is given by a text corpus. Thus, the textual knowledge considered in LSTK corresponds to a set of potential facts, namely the soft triples. On the other hand, PLM-based methods cannot work in our scenario because they can only obtain textual knowledge by pre-training a model over a given text corpus, whereas our setting allows to process new text corpora given in the test phase (see the problem setting in Section 3). The pre-training process is generally time-consuming and requires massive computing resources, thus it is impractical to be applied in the test phase. Except for PLM-based methods, the work  proposes a knowledge verification system by combining logical and textual information to compute explanations for validating new triples. However, the system should work with a predefined logical theory. In contrast, our approach does not require any predefined logical theory but learns one approximate logical theory on the fly. The recent work  improves the knowledge coverage of a knowledge base (KB) by facts extracted from text corpora. They have not focused on reasoning over facts. In contrast, we focus on enhancing the reasoning ability of rule-based systems by soft triples extracted from text corpora.

### Distant Supervision for Relation Extraction

Distant supervision  is a dominant paradigm to handle the problem of lacking labeled data in relation extraction. It assumes that if two entities involve a relation, then any sentence mentioning them together might express that relation. Based on this assumption, a large number of labeled data for relation extraction can be automatically generated by aligning triples with texts. However, this may introduce noise to the generated data. Therefore, most previous studies exploit multi-instance learning  by putting instances with the same entity pair into bags, to alleviate the impact of noisy instances. A number of neural models including CNN , Bi-LSTM  and BERT  have been incorporated with multi-instance learning and used as classification models for relation extraction. We also impose distant supervision to score soft triples from a text corpus, but we resort to textual entailment models to make the best of relation mentions, instead of using classification models where relations are treated as labels to be predicted.

## 6 Conclusion and Future Work

In this work we have proposed a two-stage framework, named LSTK, to learn rule-based systems from both structural and textual knowledge. It computes a set of soft triples by distant supervision in the first stage and applies these soft triples to enhance the learning of neural approximate rule-based systems in the second stage. To mitigate the negative impact of noise from soft triples, we have proposed a new formalism for logical rules named TE-rules and a neural model named TELM for learning TE-rules. We introduced three new datasets for empirical evaluation. Experimental results demonstrate significant improvements achieved by learning TE-rules from soft triples. Our case study further reveals how TE-rules help to control noise from soft triples.

From the third case in case study, we observe that some errors produced by the textual entailment model will propagate to the final prediction. This is known as the error propagation issue in pipeline approaches. To tackle this issue, our future work will focus on studying fully end-to-end frameworks to learn TE-rules from both structural and textual knowledge.

## 7 Limitations

LSTK merely works for knowledge graphs with aligned text corpora, but in general knowledge graphs do not come with aligned texts. In order to adapt LSTK to more real-world applications, we generate soft triples automatically under the distant supervision assumption that any sentence mentioning two entities might involve a relation between these two entities. This assumption enables us to fetch texts via information retrieval tools such as search engines or to generate supporting text via large language models (LLMs) without the need of manually constructing texts. Note that all the original datasets of HacRED , DocRED  and BioRel  are collected by distant supervision. Although the performance of LSTK depends on the quality of distant supervision, LSTK indeed works for any knowledge graphs since aligned texts can easily be collected by distant supervision.

## 8 Acknowledgements

This paper was supported by National Natural Science Foundation of China (No. 62276284, 61976232, 61876204), National Key Research and Development Program of China (No. 2021YFA1000504), Guangdong Basic and Applied Basic Research Foundation (No. 2023A1515011470, 2022A1515011355, 2020A1515010642), Guangzhou Science and Technology Project (No. 202201011699), Shenzhen Science and Technology Program (KJZD2023092311405902), Guizhou Provincial Science and Technology Projects (No. 2022-259), Humanities and Social Science Research Project of Ministry of Education (No. 18YJCZH006), as well as the Fundamental Research Funds for the Central Universities, Sun Yat-sen University (No. 23ptpy31).