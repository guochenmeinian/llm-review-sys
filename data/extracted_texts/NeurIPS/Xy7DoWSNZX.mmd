# CAP: Correlation-Aware Pruning

for Highly-Accurate Sparse Vision Models

 Denis Kuzmedelev

Skoltech & Yandex

Denis.Kuznedelev@skoltech.ru

&Elidar Kurtic

IST Austria

eldar.kurtic@ist.ac.at

&Elias Frantar

IST Austria

elias.frantar@ist.ac.at

&Dan Alistarh

IST Austria & Neural Magic

dan.alistarh@ist.ac.at

###### Abstract

Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the _Correlation Aware Pruner (CAP)_, a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new _theoretically-justified pruner_, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an _efficient finetuning procedure_ for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pruned to high sparsity levels (e.g. \( 75\%\)) with low impact on accuracy (\( 1\%\) relative drop). Our approach is also compatible with structured pruning and quantization, and can lead to practical speedups of 1.5 to 2.4x without accuracy loss. To further showcase CAP's accuracy and scalability, we use it to show for the first time that extremely-accurate large vision models, trained via self-supervised techniques, can also be pruned to moderate sparsities, with negligible accuracy loss1.

## 1 Introduction

Computer vision has seen impressive progress recently via a _new generation of architectures_ motivated by the Vision Transformer (ViT) approach  and its extensions, e.g. , accompanied by more advanced data augmentation and training approaches . Next-generation vision models such as ViT  and ConvNext  achieve breakthrough performance across vision tasks, despite encoding fewer inductive biases. This comes at the cost of very large computational and parameter budgets, both for training and deployment. Thus, there is a clear need to reduce these costs via _compression_, enabling deployment in resource-constrained settings.

Yet, the general consensus from the literature is that next-generation models are _harder to compress_ while preserving accuracy, relative to their classic convolutional counterparts . For example, if current techniques can compress the classic ResNet50 model  to 80-90% unstructured sparsity with negligible loss of accuracy , the best currently-known results for similarly-accurate ViT models only reach at most 50% sparsity while maintaining dense accuracy . Our investigation of this phenomenon, detailed in the paper, shows that this occurs for two reasons:1. **Hardness of Pruning**: As illustrated in Figure 2, existing pruning approaches, based on magnitude , first-order  or second-order information  drop very significant accuracy _per pruning step_ for next-generation architectures such as ViT and ConvNext.
2. **Expensive Recovery**: At the same time, recovering accuracy via fine-tuning  is itself computationally-challenging, as next-generation architectures are notoriously hard to train and finetune .

Therefore, it is natural to ask whether this "lack of compressibility" is inherent for next-generation vision models, and whether their increased accuracy comes at the price of higher deployment costs.

**Contributions.** In this paper, we resolve this question by proposing a new highly-accurate _correlation-aware pruner (CAP)_, which shows that _high sparsity can be achieved across next-generation architectures_ such as ViT , ConvNext , and augmented ResNets , and that this can be done in a _computationally-efficient manner_. Furthermore, CAP is compatible with other forms of compression such as token dropping and quantization, and can lead to significant inference speedups, at little or no accuracy loss.

Our primary motivation is to resolve the hardness of pruning modern, highly-accurate vision models. A key weakness of existing state-of-the-art approaches , is that they _do not directly take into account weight correlations_: At each pruning step, a _saliency score_ is computed per weight, e.g., the magnitude, weights are ranked by this score, and a large subset is chosen to be removed and/or re-introduced. However, this does not take into account the fact that removed weights _may themselves be correlated_: for example, a pair of linearly-dependent rows in a layer's weight matrix may seem _individually easy to remove_, since they are mutually-redundant, but removing _both_ at a step may lead to a large accuracy drop. This phenomenon is especially-relevant for Transformer-based architectures, which encode more complex inter-weight correlations, relative to their convolutional counterparts, in which correlations tend to be localized .

We circumvent this issue via a novel formulation of the constrained optimization problem corresponding to choosing the optimal weights to prune at a step , while taking correlations into account. We prove formally that this task can be reduced to finding the set of sparse weights which best preserve the correlation between the dense weights and their gradients, on a given set of training samples. This allows us to efficiently solve the "optimal pruning with correlations" problem, via a fast approximate solver for the above constrained optimization problem.

The CAP algorithm outperforms all other known pruners by a considerable margin (see Figure 6). In turn, this precision lends greater flexibility in _gradual pruning_, and enables us to build a _computationally-efficient_ approach to compress next-generation vision models, addressing our second motivating challenge. Specifically, our gradual pruning approach provides a simple and general recipe combining data-efficient augmentation and regularization  with theoretically-justified learning rate rewinding , leading to state-of-the-art sparsity-vs-accuracy trade-offs.

For instance, experiments on the standard ImageNet-1K benchmark  show for the first time that ViT models can attain high sparsity levels without significant accuracy impact: specifically, we can achieve 75-80% sparsity with relatively minor (\(<1\%\)) accuracy loss, and 90% sparsity with moderate loss. In turn, this sparsity leads to computational speedups of more than 2x. Our approach extends to highly-accurate pruning of large ViTs trained by self-supervised pretraining, but also to other modern models, such as ConvNext  or highly-accurate ResNets .

In sum, our results show that next-generation highly-accurate vision architectures are still highly-compressible, but may also require next-generation pruning approaches.

**Related work.** Next-generation vision architectures such as Vision Transformers (ViTs)  and ConvNext  have set new accuracy benchmarks, but are known to require careful tuning in terms of both augmentation and training hyper-parameters. Identifying efficient recipes is an active research topic in itself . We propose simple and general recipes for _fine-tuning_ such models, which should be useful to the community. Several prior works have investigated ViT compression, but focus on _structured_ pruning, such as removing tokens . Our experiments show that structured approaches are _orthogonal_ to CAP, which can be applied in conjunction to structured compression, to obtain further gains.

The only existing prior work on _unstructured_ ViT pruning is SViTE , which performed careful customization of the RigL pruning method  to the special case of ViT models. We also present results relative to other methods, such as tuned magnitude pruning, the best first-order and second-order pruners [39; 38; 14; 23] and AC/DC pruning . (These methods are known to outperform all other prior methods.) CAP improves upon existing methods across almost all benchmarks, by large margins at high sparsity.

Research on accurate pruning using second-order information was initiated by LeCun et al. , and has recently garnered significant attention [8; 45; 39; 52]. This approach can lead to good results for both gradual pruning  and one-shot (post-training) compression . Existing such pruners are not correlation-aware, and are outperformed by CAP across all of our experiments.

## 2 Background and Problem Setup

The pruning problem assumes a fixed model architecture with weights \(^{d}\) (\(d\) is the total number of parameters), and aims to find a configuration of weights with as many zeros as possible while preserving the performance of the original dense model. _Gradual_ pruning, e.g. , usually starts from an accurate _dense_ model, and progressively removes weights by setting them to zero, followed by fine-tuning phases.

**Weight Saliency.** The pruning step usually relies on proxies for weight importance, defined according to certain criteria. For instance, the _weight magnitude_ is arguably the most popular criterion, e.g. [16; 53; 15]. Specifically, given model \(^{d}\), the saliency of each weight is its absolute value (the magnitude) \(_{j}=|w_{j}|\) for \(j\{1,2,,d\}\); weights with the smallest scores are pruned away. Gradual magnitude pruning is usually a strong baseline across most models and settings. Many other criteria exist, such as gradient magnitude  or "rates of change" in the weights .

**The Optimal Brain Surgeon (OBS).** LeCun et al.  and Hassibi et al.  obtained weight saliency scores by leveraging (approximate) second-order information about the loss, starting from the Taylor approximation of the loss \(\) in the vicinity of the dense model parameters \(^{*}\). Assuming that \(^{*}\) is close to the optimum (hence \((^{*}) 0\)), one seeks a binary mask \(\) (with elements \(\{0,1\}\)) and new values for the remaining weights \(^{M}\), such that the resulting increase in loss is minimal. A standard approach to approximate the loss increase is to expand the loss function up to the second order in model weights:

\[(^{M})-(^{*})( ^{M}-^{*})^{}_{}(^{* })(^{M}-^{*})\] (1)

where \(_{}(^{*})\) is the Hessian of the model at \(^{*}\), and \(^{M}\) represents weights after the pruning step. In this setup,  and  showed that the "optimal" weight to remove, incurring the least loss, and the update to the remaining weights, can be determined via a _closed-form_ solution to the above inverse problem. Specifically, the saliency score \(_{i}\) for \(i^{}\) weight and the optimal weight update \(\) for the remaining weights after elimination of the \(i^{}\) weight are as follows:

\[_{i}=^{2}}{2[_{}^{-1}(^{*})]_{ii }}\ \ \ \ \ \ ^{*}=-}{[_{}^{-1}(^{ *})]_{ii}}_{}^{-1}(^{*})_{i},\] (2)

where \(_{i}\) is the \(i^{}\) basis vector. Theoretically, the procedure would have to be executed one-weight-at-a-time, recomputing the Hessian after each step. In practice, this procedure suffers from a number of practical constraints. The first is that direct Hessian-inverse computation is computationally-infeasible for modern DNNs, due to its quadratic-in-dimension storage and computational costs. This has led to significant recent work on efficient second-order approximations for pruning and quantization [8; 45; 52].

**WoodFisher and the Optimal BERT Surgeon.** The _empirical Fisher_ approximation  is a classic way of side-stepping some of the above constraints, and can be formally-stated as follows:

\[_{}(^{*})(^{*})= _{d d}+_{i=1}^{N}_{i}( ^{*})_{i}(^{*})^{}\] (3)

where \(_{i}(^{*})^{d}\) is a gradient computed on a sample of data, \(>0\) is a dampening constant needed for stability, and \(N\) is the total number of gradients used for approximation. Note that the resulting matrix is _positive_ definite.

The memory required to store the empirical Fisher matrix is still quadratic in \(d\), the number of parameters. Singh and Alistarh  investigated a diagonal block-wise approximation with a predefined block size \(B\), which reduces storage cost from \((d^{2})\) to \((Bd)\), and showed that this approach can lead to strong results when pruning CNNs. Kurtic et al.  proposed a formula for pruning fixed groups/patterns (e.g., 4 consecutive weights), together with a set of non-trivial optimizations to efficiently compute the Fisher block inverses, which allowed them to scale the approach for the first time to large language models.

A second obvious limitation of the OBS framework is that applying the procedure and recomputing the Hessian one weight at a time is prohibitively expensive, so one usually prunes multiple weights at once. Assuming we are searching for the set of weights \(Q\) whose removal would lead to minimal loss increase after pruning, we get the following constrained optimization problem:

\[_{}\ ^{}\ \ ( ^{*})_{Q} +_{Q}^{*}=,\] (4)

where \(_{Q}^{|Q| d}\) is a matrix of basis vectors for each weight in \(Q\). The corresponding saliency score for the group of weights \(Q\) and the update \(^{*}_{Q}\) of remaining weights are :

\[_{Q}=^{*}_{Q}(^{-1}( ^{*})_{[Q,Q]})^{-1}^{*}_{Q}\ \ ^{*}_{Q} =-^{-1}(^{*})^{}_{Q}( ^{-1}(^{*})_{[Q,Q]})^{-1}^{*}_{Q}.\] (5)

However, an exhaustive search over all subsets of size \(|Q|\) from \(d\) elements requires \(\) evaluations, which makes it prohibitively expensive for \(|Q|>1\). For unstructured pruning, virtually all known techniques, e.g. [39; 14], ignore correlations between weights. Similarly, for group pruning, Kurtic et al.  ignore correlations between groups. Despite these approximations, both approaches yield state-of-the-art results in their respective setups. As we will demonstrate later, our CAP method improves upon these approximations by reformulating this problem and proposing a correlation-aware solution that is fast and memory-efficient even for models with \(\) 100M parameters.

## 3 The CAP Pruning Framework

We introduce new techniques to address both the hardness of pruning modern vision architectures, and their high computational cost for fine-tuning: we introduce a new state-of-the-art one-shot pruner, which is complemented with a simple and general framework for data-efficient fine-tuning.

### Ingredient 1: Efficient Correlation-Aware Pruning

Our aim is to solve the pruning problem stated in the previous section: given a weight pruning target \(k\), find the optimal set of weights \(Q\) to be pruned, such that \(|Q|=k\) and the loss increase is minimized. Exactly solving for the optimal \(Q\) is an NP-hard problem , so we will investigate an iterative greedy method for selecting these weights, similar to the ideal version of the OBS framework discussed above. Importantly, our method _properly considers weight correlations_, which turn out to be important since, as demonstrated in Appendix 0.Q, the empirical Fisher has an apparent non-diagonal structure, while being _fast and space-efficient_. In turn, this leads to significant improvements over other pruners, especially in the context of vision transformers.

Formally, a correlation-aware greedy weight selection approach would perform pruning steps iteratively, as follows. Given a set of already-selected weights \(Q\), initially \(\), we always add to \(Q\) the weight \(q\) which has minimal joint saliency \(_{Q\{q\}}\), repeating until the size of \(Q\) equals the pruning target \(k\). The fact that we add weights to the set one-by-one allows us to take into account correlations between pruned weights. However, a naive implementation of this scheme, which simply recomputes saliency at each step, would be prohibitively expensive, since it requires \(O(kd)\) evaluations of \(_{Q}\), each of which involves an \(O(B^{3})\) matrix inversion, where \(B\) is the Fisher block size.

**Disentangling Correlations.** The centerpiece of our approach is a reformulation of the OBS multi-weight pruning problem in Equation 5 which will allow us to take correlations into account, while being practically-efficient. Specifically, we now show that, when using the empirical Fisher approximation, the problem of finding the optimal set of weights \(Q\) to be removed, while taking correlations into account, is equivalent to the problem of finding the set of sparse weights which best preserve the original correlation between the dense weights \(^{*}\) and the gradients \(_{i}(^{*})\) on an fixed set of samples \(i\). Formally, this result, whose proof we provide in Appendix O, is stated as follows.

**Theorem 3.1**.: _Let \(\) be a set with \(m\) samples, and let \(_{1}(^{*}),,_{m}(^ {*})\) be a set of their gradients, with corresponding inverse empirical Fisher matrix \(^{-1}(^{*})\). Assume a sparsification target of \(\) weights from \(^{*}\). Then, a sparse minimizer for the constrained squared error problem_

\[_{^{}}\,_{i=1}^{m}_{i}(^{*})^{}^{}-_{i}( ^{*})^{}^{*}^{2}\] (6)

_s.t. \(^{}\) has at least \(k\) zeros, is also a solution to the problem of minimizing the Fisher-based group-OBS metric_

\[*{arg\,min}_{Q,|Q|=k}\ _{Q}^{*} ^{-1}(^{*})_{[Q,Q]}^{-1}_{Q}^{*}.\] (7)

**A Fast Solver.** The formulation in Equation (6) reduces pruning to a sparse regression problem, where the "input" is given by _gradients_ over calibration samples. A related problem arises in the context of one-shot (post-training) pruning [21; 12], where the authors solve a sparse \(_{2}\)-fitting problem, but sparse weights are determined relative to the _layer inputs_ rather than the _layer gradients_. Specifically, the OBC solver  utilizes that, due to the quadratic loss, the per-row Hessians are independent of both the weights and each other. Thus the solver processes matrix rows sequentially, greedily eliminating weights, one-by-one, in increasing order of squared error while always updating all remaining weights to compensate for weight removal as much as possible. This essentially implements the OBS selection and update in Equation 2_exactly_, assuming layer-wise \(_{2}\) loss. We extend this strategy to efficiently implement our new Fisher-based greedy weight-subset selection.

A direct application of the OBC approach to remove \((d)\) weights would have \(O(d^{3})\) runtime, where \(d\) is the layer dimension, as the \((d^{2})\)-time selection + update process is repeated \((d)\) times. By utilizing the block-diagonal structure of the Fisher matrix with block size \(B\) during the update after each weight elimination, this complexity can be reduced to \(O(d(d,B^{2}))\), which is however still too slow as \(B\) is typically much smaller than \(d\). Instead, we proceed differently: we apply the sparse regression solver to each individual Fisher block separately and globally merge results only in the very end. This allows us to efficiently simulate global weight saliency calculation and weight updates following Equations 5 and 7, in a block-Fisher formulation. The full algorithm requires \(O(d B^{2})\) runtime and \(O(d B)\) space and is detailed in Appendix 1--an efficient implementation is also provided in the Supplementary.

The key difference of our approach relative to WoodFisher is that updates are continuously executed during subset-selection and we are thus explicitly considering the correlations captured by the off-diagonal Fisher elements. When working with small block sizes, our method is very fast and has practically no overhead over existing Fisher-based OBS approaches, while yielding significantly improved one-shot pruning results (see e.g. Figure 2 and the accompanying discussion).

**An Faster Variant for Massive Models.** In Appendix B, we present a version of CAP which scales efficiently to models with more than 1 billion parameters, by leveraging additional approximations in the Fisher matrix structure and by relaxing the pruning order.

### Ingredient 2: Fine-tuning and Pruning Procedure

To achieve best performance, modern training procedures involve longer training schedules together with a careful choice of hyperparameters, since these are known to have a major impact on convergence and accuracy [43; 41; 48]. We found the same to apply to post-pruning accuracy recovery, which is key in gradual pruning; below, we describe the main ingredients to obtaining highly-accurate fine-tuning schedules as part of our method.

**Learning Rate Schedule.** First, to achieve good performance during gradual pruning, the learning rate (LR) schedule is crucial. Specifically, we propose to use a _cyclic linear_ schedule:

\[(t)=_{}-(_{}-_{}),\] (8)

where \(\%x\) means taking the remainder after integer division by \(x\). We chose a linear decay for simplicity; we obtained similar results for other functions (e.g., cubic decay). By contrast, as we illustrate in Figure 1, the _cyclic_ nature of the schedule is key for accurate pruning, as well as for efficient sparsity sweeps (see below).

**Theoretical Justification.** Specifically, this choice is justified theoretically by tying back to the original assumptions of the OBS framework: for Equation 1 to hold, the pruned model should be well-optimized (i.e. have small gradients) at the point when pruning is performed. Moreover, right after the pruning step, having a larger value of the learning rate is useful since it gives the model a chance to recover from the sub-optimal point induced via pruning. We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [25; 39; 33], or dynamic learning rate rewinding, e.g. [11; 36].

**Regularization and Augmentation.** For augmentation, similarly to , we adopt known best-practices from the literature: smaller models such as DeiT-Tiny benefit from _lower_ levels of data augmentation during fine-tuning as in , whereas larger models such as DeiT-Base behave best with more complex augmentation and regularization . This is intuitive, since fine-tuning sparsified small models with high augmentation may exceed model capacity, rendering the optimization process unstable. We provide detailed parameter values and ablations in Appendix C.

**Augmentation for the Empirical Fisher.** The choice of augmentation is of great importance not only for the model training but for the accurate estimate of the Empirical Fisher as well. We observed that without proper augmentation the sparse solution obtained overfits to the training data even when finetuned with the same augmentation and regularization setup. We provide details in Appendix P.

**Efficient Sparsity Sweeps.** We propose a simple iterative pruning framework, which takes a set of target sparsity configurations and produces models which match these configurations _in a single run_. Specifically, we start from a standard gradual pruning setup, which prunes in a sequence of steps of increasing sparsity, followed by sparse fine-tuning. We then set the intermediate values in such a way that all intermediate target sparsity levels are achieved. For example, if one wishes to obtain checkpoints with sparsity levels \(40\%,50\%,75\%,90\%\), one can set the lowest sparsity level on the gradual pruning schedule to \(40\%\), the highest sparsity level to \(90\%\), and \(50\%,75\%\) as intermediate points. Between any two such pruning steps, we apply the cyclic retraining schedule above, which ensures that all intermediate points are sufficiently optimized.

We emphasize that _the accurate CAP pruner is key_ for efficient pruning: virtually all previous high-accuracy pruning methods in this setting, e.g. [25; 39; 6] redo the _entire training run_ for each sparsity target. In our experiments, we also examine the impact of additional fine-tuning applied to each checkpoint, and show that it induces small-but-consistent improvements.

## 4 Experimental Setup and Results

**Setup and Goals.** We consider the ImageNet  image classification benchmark, and aim to examine how sparsity impacts accuracy for different model variants. We consider three scenarios: _one-shot, single-step pruning_ of a pretrained model, where performance is clearly tied to the quality of the second-order approximation, _one-shot + fine-tuning_, in which we follow one-shot pruning

Figure 1: **(left)**: Blue: Cyclic linear learning rate schedule used in the work. Violet: Dependence of the global model sparsity on the epoch. Every change in sparsity corresponds to a pruning step. **(right)**: Relative accuracy drop (i.e difference between validation accuracy before and after pruning update) for training with _cyclic_ and _cosine_ schedule, respectively.

by a short period of fine-tuning, and, finally, _iterative gradual pruning_, where one applies pruning periodically, with some retraining interval, gradually increasing sparsity.

### One-shot pruning, without and with fine-tuning

We start by examining the quality of existing one-shot pruners relative to CAP. We compare against carefully-tuned variants of Magnitude Pruning (Magn), First-Order Gradient times Weight (GrW) , and the SOTA second-order methods WoodFisher [39; 23] and M-FAC . Our tuning process, optimal hyper-parameter choices, and ablations are detailed in Appendices J and K. WF-1 represents the _diagonal approximation_ of the Hessian proposed by the original OBD , scaled via the WoodFisher implementation. For Magnitude and CAP we present results for both _uniform_ layer-wise sparsity and _global_ sparsity. For all other methods, we present results for global sparsity, which yields better results, as the methods can adjust sparsity globally. We only investigate global sparsity for the other methods in Figure 2.

The full comparison is presented in Figure 2 for DeiT-Tiny. Notice that all first and second-order methods outperform Magnitude, and that all 2nd-order methods are better than the 1st order saliency. WoodFisher with small block size is better than both the diagonal approximation and large-block M-FAC. This suggests that it is beneficial to take weight correlations into account, but attempting to incorporate dependencies between large groups of weights may lead to noisy estimates detrimental to performance. We also present a comparison on other models (ViT and ResNet) in Appendix E.

CAP outperforms all other methods, by a large margin. Remarkably, the gap is so large that _CAP with uniform sparsity_ still outperforms WoodFisher with _global sparsity_, which can re-distribute sparsity across layers. The computational cost of WF is approximately the same as for CAP: CAP pruning step on DeiT-Small takes 23 minutes, compared to 20 minutes for WF. (The majority of the cost for both methods comes from the collection of gradients, not Hessian estimation.)

To investigate the performance difference between CAP and WF, we pruned several variants of ViT models to 50% and compared the mask overlap between WF and CAP via their IoU (intersection over union). We observe that the sparsification masks differ significantly: the IoU between the WF and CAP mask is 82.7%, 80.5%, 73.3% for DeiT-Tiny, DeiT-Small, and DeiT-Base, respectively, suggesting that the weight correlations taken into account by CAP lead to significantly different pruning decisions. The same trend is visible in the validation loss surface, projected on the plane in weight space, for the dense model and the 50% sparse models via WF and CAP. (We adopt the approach from  for loss surface visualization. Specifically, the unit vector in the horizontal direction is chosen to be \(w_{WF}-w^{*}\) and the vertical direction is defined by the component of \(w_{CAP}-w^{*}\) orthogonal to the horizontal axis. Above \(w^{*},w_{WF},w_{CAP}\) are the weights of the dense model and sparse solutions found by WF and CAP, respectively.) Figure 3 shows that CAP chooses a significantly less steep direction in the loss basin compared to WF.

**Compressing highly-accurate models.** Modern pretraining methods [7; 5; 34; 51] in conjunction with large vision backbones achieve extremely high accuracy on standard benchmarks such as ImageNet-1K. We leverage the scalability and accuracy of CAP to investigate, for the first time, sparsity in such highly-accurate models, in the absence of finetuning. Specifically, we start from the ConvNext-Large checkpoint pretrained via CLIP on the LAION-2B dataset and finetuned sequentially on ImageNet-21k and ImageNet-1k .

In Table 1, we compare our method with WoodFisher (WF) and Global Magnitude (GM) for one-shot pruning via several sparsity targets. Results show that 1) CAP can induce 50% sparsity with relatively low (0.3%) accuracy loss, and 60% with less than 1% accuracy drop; 2) CAP significantly outperforms other methods. In addition, we find that these results can be improved with a limited amount of fine-tuning; please see Appendix E for full results.

### Gradual Pruning Results

Finally, we execute gradual pruning with the sparsity schedule, augmentation choices, and cyclic linear learning-rate scheduler discussed above. The whole gradual pruning procedure lasts for 300 epochs, as in . We aim to obtain accurate sparse checkpoints for \(50\%\), \(60\%\), \(75\%\), \(80\%\), and \(90\%\) sparsity. For this, we prune to \(40\%\) in the initial step, and increment sparsity every 20 epochs, until reaching \(90\%\), with fine-tuning in between. (See Appendix J for full results and ablations.) We select the accuracy of intermediate models which match the target sparsities; to examine the impact of fine-tuning, we trained each of the resulting sparse checkpoints for an additional 100 epochs, marked with (\(\)). We compare with global magnitude (GM) following the same schedule as CAP, as well as the state-of-the-art SViTE  paper, which trains the sparse model from scratch using a variant of RigL , but for a total of 600 epochs. The results are in Table 2.

For DeiT-Tiny and 50% sparsity, we achieve significant improvements upon SViTE, and even manage to improve test accuracy relative to the dense model. We believe this is due to the choice of augmentation during fine-tuning, and possibly due to regularizing effects of sparsity. At 75-80%, we recover the dense model accuracy. We observe a significant accuracy drop only at 90%. GM pruning also benefits from the choices made in our schedule, outperforming SViTE at 50% sparsity; yet, there are significant gaps in favor of CAP at higher sparsities, as expected.

On the 4x larger DeiT-Small model, SViTE performs remarkably well at 50% sparsity (79.7%), almost matching the dense model, but CAP outperforms it slightly after fine-tuning (79.9%). In terms

   Model & Method & Sparsity (\%) & Top1-Accuracy (\%) \\   & Dense & 0 & 87.8 \\   & GM & & 80.2 \\   & WF & 50 & 86.6 \\   & CAP & & **87.5** \\    & GM & & 38.7 \\   & WF & 60 & 85.1 \\   & CAP & & **87.1** \\    & GM & & 0.5 \\   & WF & 70 & 73.7 \\   & CAP & & **86.8** \\   

Table 1: Accuracies of CLIP-pretrained ConvNext-L on ImageNet-1k, following one-shot pruning.

   Model & Method & Sparsity (\%) & FLOP (\%) \\   & Dense & 0 & 0 & 72.2 \\   & GM & 43.9 & 73.5 \\   & SViTE-Tiny & & 43.9 & **73.7** \\   & GM & & 43.9 & 69.6 \\   & GM & 60 & 52.6 & 73.1 (73.2 \(\)) \\   & CAP & & 52.6 & 73.3 (**73.6 \(\))** \\   & GM & & 65.8 & 71.4 (71.9 \(\)) \\   & CAP & & 65.8 & 72.3 (**72.6 \(\))** \\   & SViTE-Tiny & & 65.8 & 63.9 \\    & GM & & 70.5 (70.9 \(\)) \\   & CAP & & 70.2 & 71.7 (**72.0 \(\))** \\   & CAP & & 79.0 & 66.2 (**66.6 \(\)) \\   & CAP & & 79.0 & 64.4 (**68.0 \(\))** \\   & SViTE-Tiny & & 79.0 & 49.7 \\    
   Model & Method & Sparsity (\%) & FLOP (\%) \\   & Dense & 0 & 0 & 79.8 \\   & GM & 46.7 & 79.3 (**79.8 \(\)) \\   & CAP & & 46.9 & 79.4 (**79.9 \(\))** \\   & SViTE-Small & & 46.3 & 79.7 \\    & GM & & 56.1 & 79.0 (**79.5 \(\)) \\   & CAP & & 56.2 & 79.3 (**79.8 \(\))** \\   & SViTE-Small & & 55.4 & 79.4 \\    & GM & & 20.1 & 78.0 (**78.7 \(\))** \\   & CAP & & 70.2 & 78.5 (**79.8 \(\))** \\   & SViTE-Small & & 70.3 & 77.0 \\    & GM & & 74.2 & 77.3 (77.9 \(\)) \\   & CAP & & 74.9 & 78.0 (**78.6 \(\))** \\    & GM & & 84.0 & 71.4 (74.7 \(\)) \\   & CAP & & 84.1 & 75.2 (**79.8 \(\))** \\   & SViTE-Small & & 84.1 & 70.1 \\   

Table 2: Results for gradual pruning of DeiT-Tiny and Small models on ImageNet-1k. CAP achieves 50% sparsity without accuracy loss, and 80% sparsity with less than 1% relative error.

of total training budget, SViTE uses 600 epochs to produce each model (and so, the 50%-sparse one as well), whereas we use a total of 40 epochs for gradual pruning to 50% + initial fine-tuning, and 100 additional epochs for sparse model fine-tuning. Even if we take into account the original 300 epochs for training the publicly-available dense DeiT checkpoint , our approach is significantly more efficient (440 vs. 600 epochs). The cost savings compound across sparse models, since we obtain all our pruned models from the same end-to-end gradual run. At 75% sparsity, CAP drops \( 1\%\) of accuracy relative to dense post-finetuning, with a significant gap of \(1\%\) Top-1 relative to GM, and \(2\%\) Top-1 relative to SViTE. The trend continues for higher sparsities, where we note a remarkable gap of \(5.7\%\) Top-1 vs SViTE at 90% sparsity. We obtain similar numbers for DeiT-Base in Table 3; generally, we achieve \(\) 99% recovery at \( 75\%\) sparsity, showing for the first time that ViT models can be pruned to such sparsities with marginal accuracy loss.

**Sparse Speedups.** The results in Figure 4 examined the speedups obtained by CAP from unstructured sparsity for {50%, 75%, 90%} -sparse ViT-family (base, small, tiny) models, when executed on a sparsity-aware CPU inference engine . Specifically, we executed the models from Table 2 using 4 cores of an Intel(R) Xeon(R) Gold 6238R CPU, at batch size 64. We find it interesting that sparse ViTs build an almost-contiguous Pareto frontier from 82% to 68% Top-1 accuracy (Y axis), with a 25x span in throughput (from 11 imgs/second to 260 imgs/second, X axis). Notably, the DeiT-Tiny model obtains a speedup of 2.4x without any accuracy loss, while Base and Small ViTs show 1.5x speedups with minimal loss of accuracy. Thus, these results show that unstructured sparsity can be a very promising approach to speeding up ViTs. We note that competing methods (e.g. SViTE) would provide similar speedups at the same sparsity levels, but significantly lower accuracy, as shown in the figure, as well as in Table 2.

**Additional Results and Details.** Due to space constraints, full ablations and several additional experiments have been deferred to the Appendix. We present the full pseudocode for CAP in Appendix A, and a faster approximate version of CAP in Appendix B. We describe all parameters and perform additional ablations in Appendices C and J. We present results for one-shot pruning across several other models, as well as with short amounts of fine-tuning, in Appendix E. We present gradual pruning results for several other vision models (EfficientFormer, ResNet50D, and EfficientNetV2) in Appendix F and demonstrate that one can compress them to high sparsity as well, using our approach. In Appendix H we show that CAP comes with little additional computational cost compared to WoodFisher . In Appendix I we show that CAP can also be applied in conjunction with other types of compression, in particular token pruning , quantization-aware training, and semi-structured (2:4) sparsity. In Appendix M, we show that CAP also outperforms AC/DC pruning , whereas Appendix N contains results for the DeTR detection model. In Appendix G we study the scaling behavior of sparsity solvers with respect to the ConvNext2 model family.

## 5 Discussion

We presented a new correlation-aware pruner called CAP, which sets a new state-of-the-art sparsity-accuracy trade-off. We have shown for the first time that ViT variants can support significant weight pruning (\( 75\%\)) at relatively minor accuracy loss (\( 1\%\)), inducing compression trade-offs that similar to those of CNNs, and that CLIP-pretrained, highly-accurate models can also support sparsity accurately. Thus, despite weaker inductive biases, next-generation vision models do not require over-parametrization, and can be competitive with CNNs in terms of accuracy-per-parameter. Our approach is complementary to other compression approaches, leading to significant speedups.

Acknowledgements

This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML). The authors would also like to acknowledge computational support from the ISTA IT department, in particular Stefano Elefante, Andrei Hornoiu, and Alois Schloegl, as well as Amazon EC2 for research credits. D.K. was supported by Russian Science Foundation, grant 21-11-00373.