# AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks

Jin Li\({}^{1}\), Ziqiang He\({}^{1}\), Anwei Luo\({}^{1}\), Jian-Fang Hu\({}^{1}\), Z. Jane Wang\({}^{2}\), Xiangui Kang\({}^{1}\)

\({}^{1}\)Guangdong Key Lab of Information Security,

School of Computer Science and Engineering, Sun Yat-Sen University

\({}^{2}\)Electrical and Computer Engineering Dept, University of British Columbia

Corresponding author.

###### Abstract

Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9\(\%\) (+17.3\(\%\)) ASR with 1.34 (-0.97) \(l_{2}\) distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at https://github.com/XianguiKang/AdvAD.

## 1 Introduction

Deep Neural Networks (DNNs) are shown to be vulnerable to adversarial attacks [1; 2] (i.e., add maliciously crafted perturbations to the input data), posing serious security concerns to real-world applications . The research of adversarial attacks also plays an important role in proactively exposing potential threats, as well as promoting model robustness and corresponding defense methods [4; 5; 6; 7; 8; 9; 10; 11]. Many attacks [12; 13; 14; 15] focus on maximizing the attack success rate and transferability under relatively lenient restrictions (i.e., \(l_{}\) or \(l_{2}\) norm) of adversarial perturbation, but they could have poor stealthiness and imperceptibility since the crafted adversarial examples can be easily detected by the Human Visual System (HVS) . Therefore, imperceptible adversarial attacks [17; 18; 19; 20; 21; 22; 23], aiming to maintain attacking efficacy while improving imperceptibility, have attracted considerable attention.

Current imperceptible adversarial attacks could be summarized into two categories: 1) perturbation-based attacks devised on perceptual characteristics, and 2) unrestricted attacks. The first one is motivated by the fact that adding adversarial perturbations to different components of an image hasvarying perceptual quality levels to the HVS. By studying components such as image color , texture complexity , frequency spectrum [23; 24], etc., these methods design corresponding perceptual-based loss functions and incorporate them to the optimization process to craft adversarial examples where the adversarial perturbation is constrained and hidden within specific image regions. Instead of injecting noise-like adversarial perturbations, unrestricted attacks heavily but reasonably modify attributes of images like semantic content to perform attacks. Apart from early work that adopts GANs , some recent methods combine the prevalent diffusion models [26; 27; 28] into the adversarial optimization process in an image edition-like way of repeatedly adding noise and denoising to eliminate the noise pattern within the final adversarial examples [29; 30] or optimize the embedding of latent diffusion models [31; 32]. However, due to the uncertainty of generative models and the unrestricted setting itself, some unrestricted adversarial examples inevitably exhibit obvious unnatural texture or semantic changes and lose the imperceptibility, especially for images with complex content. Although previous methods have equipped attacks with imperceptibility utilizing various designs mentioned above, it remains an essential challenge of achieving imperceptible adversarial attacks: _How to attack with inherently minimal perturbation strength from a modeling perspective?_

To address this fundamental challenge, we propose **Adversarial Attacks in Diffusion** (**AdvAD**), a brand new modeling framework distinct from common attack paradigms of gradient ascending  or optimization with adversarial losses . The proposed AdvAD explores a novel _non-parametric_ diffusion process for attacks, which fully inherits two key merits of diffusion models: **i**) the modeling philosophy of converting a difficult task into a series of simple sub-tasks, and **ii**) solid theoretical foundation. Specifically, AdvAD achieves high attack efficacy with intrinsically lower perturbation strength by innovatively modeling the attack process as a decomposed diffusion trajectory from an initialized noise to an adversarial example. At each step, a much subtler (for imperceptibility) yet more effective (for attack performance) _adversarial guidance_ is calculated and injected with two cooperating, theoretically grounded non-parametric modules called Attacked Model Guidance (AMG) and Pixel-level Constraint (PC), which gradually leads the end of this trajectory from the original image distribution to a desired adversarially conditioned distribution based on the theory of diffusion models (e.g., deterministic diffusion, conditional sampling [33; 34], etc.).

Here, we would like to clarify that the proposed diffusion process for attacks is considered as non-parametric since it does not require additional networks as needed in regular diffusion models for noise estimation. AdvAD firstly initializes a fixed diffusion noise, which is then ingeniously manipulated at each step via the adversarial guidance crafted by the proposed AMG and PC modules using only the attacked model with theoretically derived equations. In this way, the proposed AdvAD is facilitated with the modeling approach of diffusion models rather than their denoising or generative capabilities, which avoids the negative impact like semantic content changes caused by the uncertainty of generative models and also promises relatively low computational complexity. Based on AdvAD, we further propose an enhanced version **AdvAD-X** ('X' for 'eXtreme') with two extra strategies to squeeze the extreme performance in an ideal scenario of the proposed new modeling framework with unique properties, which also possesses theoretical significance and provides new insights for revealing the robustness of DNNs. In summary, our main contributions are:

* Addressing the essential challenge of imperceptible adversarial attacks from a novel modeling perspective for the first time, we theoretically explore and derive the basic modeling of diffusion models to perform attacks with inherently lower perturbation strength through a non-parametric diffusion process that requires no additional networks.
* We propose two attack versions, AdvAD and AdvAD-X. For the basic AdvAD, the AMG and PC modules cooperate to craft much subtler yet effective adversarial guidance which is progressively injected via initialized diffusion noise at each step, and AdvAD-X further reduces the perturbation strength to an extreme level in an ideal scenario with theoretical significance.
* Extensive experiments are conducted to evaluate the effectiveness of our methods in terms of attack success rate, imperceptibility, and robustness. Experimental results demonstrate the superiority of the novel modeling approach for imperceptible adversarial attacks.

## 2 Preliminaries

**Adversarial Attacks.** Given an original image \(_{ori}\) with ground-truth label \(y_{gt}\) and a classifier \(f()\) satisfying \(f(_{ori})=y_{gt}\), normal untargeted attacks aim to craft the adversarial example \(_{adv}\) that misleads the classifier, formulated as:

\[f(_{ade}) y_{gt}, s.t.\|_{adv}-_{ori}\|_{p},\] (1)

where \(\|\|_{p}\) represents \(l_{p}\)-norm that is usually implemented with \(l_{}\)-norm to limit the distance between \(_{adv}\) and \(_{ori}\) within an upper bound of budget \(\). In this paper, we focus on the more general setting of untargeted attacks. More information on related works is provided in **Appendix** A.

**Deterministic Diffusion Process.** In the deterministic situation of DDIM  with \(_{t}=0\), for an image \(_{0}\) and pre-defined diffusion coefficients \(_{0:T}(0,1]^{T}\) for step \(t[0:T]\), \(_{t}\) in the _Forward_ process of adding noise to \(_{0}\) is given by \(_{t}=}_{0}+}\), where \((,)\) represents Gaussian noise. For the _Backward_ denoising steps, unlike the DDPM  based on Markov chains that each state directly depends on the previous one, DDIM employs a non-Markovian approach. In the backward process, each step first involves calculating a "prediction" of final step \(_{t}^{0}\) from current \(_{t}\), then adding noise to it again to obtain \(_{t-1}\), expressed as:

\[_{t-1}=}(_{t}-}_{}(_{t})}{}})+} _{}(_{t}),\] (2)

where \(_{}(_{t})\) is a estimated diffusion noise using a pretrained neural network \(\) for current step, and the term in the first parenthesis represents the predicted \(_{t}^{0}\), derived by a simple variation of Eq. (2).

**Conditional sampling.** Song et al.  propose the conditional sampling technique for the _score-based_ generative models with _score function_\(_{_{t}}\,p(_{t})\), a kind of generative model has close relationship to diffusion models. Without loss of generality, for a condition \(y\) (e.g., class label, mask, etc.) and corresponding conditional distribution \(p(|y)\), a score-based model can sample from \(p(|y)\) by modifying the score function at each step of \(t\) to \(_{_{t}}(p(_{t})p(y|_{t}))\) if \(p(y|_{t})\) is known. Subsequently, with the connection between the score function and the noise \(_{t}\) of diffusion models as \(_{_{t}}\,p(_{t})=-1/}_{t}\), this joint distribution could be expanded to the deterministic process of DDIM, achieved by updating the noise \(_{t}\) to \(_{t}^{}\) at each step as :

\[_{t}^{}=_{t}-}_{_{t}}\,p(y|_{t}).\] (3)

## 3 Proposed Adversarial Attacks in Diffusion

### Overview

From a novel modeling perspective, we propose **Adversarial Attacks in Diffusion (AdvAD)** to attack with inherently smaller perturbation strength through a non-parametric diffusion process for the first time. As shown in Figure 1, different from previous attack paradigms that employ gradient ascending or optimization with varying kinds of adversarial losses, AdvAD innovatively performs attack within a decomposed non-parametric diffusion trajectory starting from an initialized noise, in which very subtle yet effective adversarial guidance is crafted and injected to gradually push the end of this trajectory to a desired adversarially conditioned distribution from the original image.

Intuitively, given the original image \(_{ori}\) with an initialized Gaussian noise \(_{0}(,)\), a fixed diffusion trajectory from \(}_{T}\) to \(}_{0}\) (\(}_{0}=_{ori}\)) can be easily obtained using DDIM _Backward_ for the deterministic diffusion process as:

\[}_{T}=}_{ori}+}_{0},\] (4)

\[}_{t-1}=}(}_{t}-}_{0}}{}})+} _{0}.\] (5)

With this deterministic diffusion trajectory of the original image, performing adversarial attacks within it requires solving two main problems: **i)** directing the final result of this diffusion process to a desired adversarial example rather than the original image; **ii)** ensuring the modified trajectory (denoted as \(}_{t}\), \(}_{t}\) for step \(t\)) close to the original trajectory (\(}_{t}\), \(_{0}\) for step \(t\)) of the clean image to achieve the imperceptibility of attacks. To fulfill the dual purposes, we propose two theoretically grounded modules, called **Attacked Model Guidance (AMG)** and **Pixel-level Constraint (PC)** to work together. At each step, AMG utilizes only the attacked model \(f()\) to produce the adversarial guidance without requiring any additional networks, synergistically collaborated with PC to constrain and streamline the diffusion process injected with the guidances.

### Attacked Model Guidance Module

By viewing the attack process as a distribution-to-distribution transformation through a non-parametric diffusion process, the proposed AMG module theoretically integrates the conditional sampling technique of diffusion models to craft the adversarial guidance only using the attacked model \(f()\). For untargeted attacks, the ultimate goal is modifying \(_{ori}\) with \(f(_{ori})=y_{gt}\) to \(_{adv}\) so that \(f(_{adv}) y_{gt}\), which can be regarded as directing the determined distribution \(p(_{ori})\) of the original diffusion trajectory to an distribution of \(_{adv}\) with the attacked model as \(p(_{adv}|f(_{adv}) y_{gt})\). Thus, we regard \(f(_{adv}) y_{gt}\) as an adversarial condition, and employ the conditional sampling technique to the original trajectory by manipulating the diffusion noise to achieve this, expressed as:

\[}^{}_{t} =_{0}-}_{}_{t}} \,p(f(_{adv}) y_{gt}|}_{t})\] (6) \[=_{0}-}_{}_{t}} (1-p(f(_{adv})=y_{gt}|}_{t})).\]

However, Eq. (6) is unsolvable since \(_{adv}\) is unknown during the diffusing process. To address this, inspired by the properties of deterministic non-Markovian DDIM that a final diffusion result is firstly predicted at each step, we calculate \(}^{0}_{t}\) via the equation of DDIM non-Markovian process with \(}_{t+1}\) from the previous step, and use it to approximate \(_{adv}\), expressed as:

\[_{adv}}^{0}_{t}=}_{t}-}\;}_{t+1}}{}}.\] (7)

The accurate error upper bound and convergence of this approximation are given in Proposition 2 in conjunction with the proposed PC module, and the validity of this approximation can also be explained intuitively from the premise of our method. That is, we have \(}^{0}_{t}=_{ori}\) for all step \(t\) in the original diffusion trajectory, and the modified trajectory should be very close to the original one, so that the relationship between \(}^{0}_{t}\) and \(_{adv}\) should satisfy \(}^{0}_{t}_{adv}\).

With Eq. (7), the term of \(p(f(_{adv})=y_{gt}|}_{t})\) in Eq. (6) can be written as \(p(f(}^{0}_{t})=y_{gt}|}_{t})=p(f(}^{0}_{t})=y _{gt})\) since \(}^{0}_{t}\) is calculated from \(}_{t}\), which is exactly the output logits of \(f(}^{0}_{t})\) with \(Softmax()\) function for the class \(y_{gt}\). Denoting this term as the classification probability of the attacked model as \(p_{f}(y_{gt}|}^{0}_{t})\), we can obtain the solvable equation of AMG module that injects adversarial guidance to the initialized diffusion noise using only \(f()\) without any additional network:

\[}^{}_{t}=(_{0},}^{0 }_{t},f(),y_{gt})=_{0}-}_{}_{t}}(1-p_{f}(y_{gt}|}^{0}_{t})).\] (8)

At this point, in addition to the benefits from modeling, this calculation process of AMG also plays a role in endowing AdvAD with imperceptibility. As the attack progresses, the probability \(p_{f}\), the term

Figure 1: Overview of the proposed Adversarial Attacks in Diffusion (AdvAD) that models the attack as a non-parametric diffusing process. At each step, Attacked Model Guidance (AMG) module adopts the non-Markovian process for approximating \(_{adv}\) using \(}^{0}_{t}\) to craft adversarial guidance and injects it into the initialized diffusion noise, then Pixel-level Constraint (PC) module imposes restriction to produce the noise for the next step and serves to control the whole process precisely.

of \((1-p_{f})\) as well as coefficient \(}\) gradually approach 0, which means the strength of injected adversarial guidance gradually converge to 0 in AdvAD, while common classification losses (e.g, Cross-Entropy, Log Loss, etc.) used in other attack paradigms may increase on the contrary. Further analysis and experiments on this property are provided in Proposition 1 and Sec. 4.5.

### Pixel-level Constraint Module

Collaborating with AMG, the PC module is introduced to impose precise control and streamline the modified diffusion trajectory for attacks. A straightforward choice is to design PC for \(}_{t}\) that constrains each \(}_{t}\) using \(}_{t}\), thus ensuring \(}_{t}^{0}\) close to \(}_{t}^{0}\) and the final \(_{adv}\) close to \(_{ori}\). However, such a "hard" constraint directly applied to \(}_{t}\) will impair the effectiveness of AMG and disrupt coherence of the transforming trajectory. Therefore, we formulate a more suitable PC for \(}_{t}\) as in Theorem 1.

**Theorem 1**: _Given diffusion coefficients \(_{T:0}(0,1]^{T}\), the \(_{ori}\), \(}_{t}\), \(_{0}\) from the original trajectory, \(}_{t}\), \(}_{t}\) from the modified trajectory, and a variable \(\), if \(}_{t}\) and \(_{0}\) satisfies_

\[\|}_{t}-_{0}\|_{}}}{}},\] (9)

_for all \(t[T:1]\), then it follows that \(\|}_{t}-}_{t}\|_{}(}-}}}{}})\), \(\|}_{t}^{0}-_{ori}\|_{}\), and \(\|}_{0}-_{ori}\|_{}\) hold true._

According to Theorem 1, the PC for \(}_{t}\) is implemented as:

\[}_{t}=(}_{t}^{})=_{l_{}(_{0},}}{} })}(}_{t}^{}).\] (10)

where \(_{l_{}(,)}()\) is a projection operation that constrains the output \(}_{t}^{}\) of \(()\) to \(}_{t}\) based on a \(l_{}\)-norm ball of \(_{0}\) to satisfy Eq. (9). After PC, the diffusion noise \(}_{t}\) for next step is obtained, and the \(}_{t-1}\) can be calculated using the deterministic DDIM _backward_ equation as:

\[}_{t-1}=}(}_{t}-}}_{t}}{}})+}}_{t}.\] (11)

The elaborate PC for \(}_{t}\) directly cooperates with AMG to constrain the diffusion noise, which streamlines the whole diffusion process and can serve to simultaneously control the terms of \(}_{t}\), \(}_{t}^{0}\), and \(}_{0}\), satisfying the premise that two trajectories are close and ensuring the effectiveness of AdvAD. The complete pseudo code of AdvAD is provided in Algorithm 1.

Subsquently, based on Theorem 1, we further give two propositions about AdvAD as:

**Proposition 1**: _Under the conditions of Theorem 1, by denoting constrained \(}_{t}=_{0}-_{t}\), we have_

\[_{adv}=_{ori}+_{t=1}^{T}_{t}_{t},\] (12)_where \(_{t}=}}{}}-}}{}}\), and \(\|_{t}\|_{}}}{}}\)._

**Proposition 2**: _Under the conditions of Theorem 1, the upper bound on the error of the approximation in Eq. (7) can be expressed as_

\[\|_{adv}-}_{t}^{0}\|_{}\;2}}{}}}}{}}.\] (13)

Proposition 1 explicitly states the much subtler and decreasing strength of the adversarial guidance injected at each step of AdvAD's non-parametric diffusion process, and also allows for a quantitative analysis (as in Sec. 5.5). Proposition 2 indicates the validity and convergence of the approximation of \(_{adv}}_{t}^{0}\) in AMG (Eq. (7)). It is evident that as \(t\) goes from \(T\) to \(1\), \(_{t}\) increases from \(0\) to \(1\), the upper bound on the approximation error rapidly converge from \(2\) to \(0\). The detailed derivations of the mentioned PC for \(}_{t}\), proofs of Theorem 1 and Proposition 1, 2 are provided in Appendix B.

### AdvAD to AdvAD-X: Extreme Version

Building upon AdvAD, we further propose a scheme called AdvAD-X ('X' for 'eXtreme') with two extra strategies called Dynamic Guidance Injection (DGI) and CAM Assistance (CA), aiming to squeeze the extreme performance of our novel modeling framework in an ideal scenario that is usually overlooked but has theoretical significance.

DGI and CA Strategies.As aforementioned, the attack capability of AdvAD comes from the very subtle yet effective adversarial guidance crafted by AMG and PC, and the intensity of guidance will decrease to 0 as the process progresses. Thus, the DGI is naturally emerged as a dynamic skipping strategy to skip the unnecessary calculation and injection of adversarial guidance, especially for those steps in the later process. With DGI, AdvAD-X dynamically avoids the execution of AMG and PC and adopts original \(_{0}\) as the diffusion noise for the steps where the \(}_{t}^{0}_{adv}\) is already able to mislead the attacked model, reducing the accumulated guidance strength as well as the computational complexity. On the other hand, inspired by the Class Activation Mapping (CAM)  identifies critical regions of an image about a decision made of a classifier, our CA strategy calculates a mask (if available) \(\) ranging from \(0\) to \(1\) of \(_{ori}\) with \(f()\) and \(y_{gt}\) using GradCAM  to further suppress the strength of adversarial guidance within the non-critical image regions in those steps that are not skipped. The equation of AMG with CA strategy can be modified as:

\[}_{t}^{}=_{0}-} _{}_{t}}(1-p_{f}(y_{gt}|}_{t}^{0})).\] (14)

Ideal Scenario.Equipped with DGI, AdvAD-X omits a large number of adversarial guidance that is injected by default in AdvAD, while the absolute strength of guidance in each of the remaining steps are also suppressed by CA, successfully reducing the final adversarial perturbation to an extreme level. This extreme case leads to a problem that in the default setting of attacking with 8-Bit RGB images, the adversarial perturbation of pixels where the intensity is less than \(0.5\) will be erased due to the quantization. However, in practice, the input of DNNs is normalized as floating-point data type to avoid gradient problems during training [38; 39], and white-box attack allows access to the entire of DNNs. Therefore, for AdvAD-X, we specifically consider an ideal scenario that directly input the raw final adversarial example in floating-point data to DNNs without quantization to evaluate the extreme performance of AdvAD-X. The pseudo code of AdvAD-X is provided in Appendix C.

## 4 Experiments

### Experimental Setup

**Dataset.** In line with prior studies [15; 19; 32; 40], our experiments are conducted on the ImageNet-compatible Dataset 1, containing 1,000 images of ImageNet  classes with size of \(299 299\), and the images are resized to standard input size of \(224 224\) in all experiments. **Models.** We select the widely used CNNs of ResNet-50  and enhanced ConvNeXt-Base , Swin Transformer-Base with Transformer  architecture, and VisionMamba-Small  with the recently emerged advanced Mamba  architecture. **Attacks.** We choose classic PGD  and seven attacks that claim having imperceptibility as comparison methods, including normal imperceptible attacks of AdvDrop , PerC-AL , SSAH , and unrestricted attacks of NCF , ACA , DiffAttack , Diff-PGD , and the generative capability of diffusion models are utilized the last three attacks. For our proposed AdvAD and AdvAD-X, we set \(=8/255\) and \(T=1000\) for all experiments unless specifically mentioned. All the other comparison methods are evaluated using their official open-source code with the default hyper-parameters. The results of AdvAD-X are obtained in the ideal scenario with float-pointing raw data as described in Sec. 3.4. **Evaluation Metrics.** Attack success rate (ASR) is used to evaluate the attack efficacy, and seven metrics are adopted to comprehensively assess the imperceptibility, including \(l_{2}\) and \(l_{}\) distances for absolute perturbation strength; Peak Signal-to-Noise Ratio (PSNR), Structure Similarity (SSIM) , and three network-based metrics, i.e., Learned Perceptual Image Patch Similarity (LPIPS) , Frechet Inception Distance (FID) , and a non-reference metric MUSIQ  for image quality.

### Comparison with State-of-the-art Methods

White-Box Attacks.Table 1 reports the untargeted attack performance and imperceptibility of ten methods against four attacked models. It is evident that the proposed AdvAD with novel modeling framework consistently demonstrates superior performance in terms of both ASR and imperceptibility. For the normal imperceptible adversarial attacks, the absolute adversarial perturbation strength of AdvAD in \(l_{}\) and \(l_{2}\) distance are only 0.014 and 1.34 in average, which is about half of the state-of-the-art restricted imperceptible attack SSAH, and AdvAD maintains almost 99.9% ASR, supporting our key idea that it inherently reduces the strength of perturbation required for attacks from a modeling perspective. When attacking more advanced models from ResNet to VisionMamba, AdvAD always demonstrates the best ASR and imperceptibility, yet other methods tend to have some performance degradation (e.g., PerC-AL and SSAH for ConvNeXt and VisionMamba). For unrestricted attacks,

   Model & Attack Method & Time (s) \(\) & ASR (\%) \(\) & \(l_{}\)\(\) & \(l_{2}\)\(\) & PSNR \(\) & SSIM \(\) & FID \(\) & LPIPS \(\) & MUSIQ \(\) \\   & PGD  & 25 & 98.6 & 0.031 & 8.17 & 35.3 & 0.830 & 35.25 & 0.0517 & 52.24 \\  & NCF  & 2739 & 89.9 & 0.783 & 75.16 & 14.79 & 0.6374 & 84.99 & 0.3052 & 49.12 \\  & AC  & 52739 & 89.8 & 0.839 & 52.42 & 18.00 & 56.950 & 69.57 & 0.3381 & 55.47 \\  & DiffAttack  & 34954 & 96.6 & 0.243 & 0.351 & 27.63 & 0.6750 & 55.29 & 0.1130 & 55.67 \\  & DiffPGD  & 6057 & 92.1 & 0.246 & 11.43 & 30.95 & 0.8902 & 22.188 & 0.0315 & 55.05 \\  & AdvDrop  & 193 & 96.8 & 0.062 & 3.17 & 41.91 & 0.9822 & 5.57 & 0.0061 & 54.96 \\  & PerC-AL  & 4085 & 98.8 & 0.131 & 2.05 & 46.35 & 0.9894 & 8.602 & 0.0029 & 55.84 \\  & SSAH  & 428 & **99.7** & 0.033 & 2.65 & 43.73 & 0.9911 & 4.48 & 0.001 & 55.49 \\  & AdvAD(ours) & 2201 & **99.7** & **0.010** & **1.06** & **51.84** & **0.989** & **0.24** & **0.0005** & **56.35** \\  & AdvAD-X\({}^{}\)(ours) & 806 & **100.0** & **0.026** & **0.34** & **63.62** & **0.9997** & **0.23** & **0.0001** & **56.59** \\   & PGD  & 127 & 99.9 & 0.031 & 7.98 & 37.34 & 0.8845 & 32.03 & 0.0386 & 51.85 \\  & NCF  & 5222 & 59.4 & 0.750 & 72.89 & 15.10 & 0.6616 & 50.52 & 0.2846 & 49.70 \\  & ACA  & 83149 & 82.2 & 0.835 & 52.16 & 18.05 & 0.5676 & 84.55 & 0.3421 & 55.11 \\  & DiffAttack  & 35417 & 97.8 & 0.754 & 13.70 & 22.28 & 0.610 & 72.22 & 0.1277 & 48.80 \\  & DiffPGD  & 6325 & 76.9 & 0.245 & 11.45 & 30.94 & 0.8908 & 21.05 & 0.0306 & 54.75 \\  & AdvDrop  & 836 & 96.9 & 0.057 & 3.26 & 41.69 & 0.9864 & 6.42 & 0.0055 & 54.80 \\  & Pee-AC  & 18271 & 10.3 & - & - & 2.1 & - & - & - & - & - \\  & SSAH  & 3423 & 84.6 & 0.026 & 2.24 & 45.19 & 0.9928 & **3.04** & 0.0011 & 55.78 \\  & AdvAD (ours) & 15240 & **100.0** & **0.016** & **1.49** & **48.61** & **0.9964** & 5.07 & **0.0009** & **55.97** \\  & AdvAD-X\({}^{}\)(ours) & 5245 & **99.8** & **0.004** & **0.64** & **58.01** & **0.9993** & **0.62** & **0.0001** & **56.43** \\   & PGD  & 93 & 98.8 & 0.031 & 7.85 & 33.88 & 0.886 & 21.34 & 0.0378 & 51.91 \\  & NCF  & 4690 & 63.7 & 0.73 & 69.92 & 15.48 & 0.6822 & 47.17 & 0.2709 & 49.77 \\  & ACA  & 83706 & 79.6 & 0.831 & 50.70 & 18.31 & 0.5757 & 64.83 & 0.3341 & 55.65 \\  & DiffAttack it is expected for them to perform poorly in the quantitative metrics, but if the results are poor for all image quality metrics, it usually indicates that the images are damaged. Meanwhile, since the optimizer may not find the global optimal solution, the optimization-based methods tent to show sub-optimal ASR. For AdvAD-X, surprisingly, the perturbation strength is reduced to an extremely low level with still high attack efficacy in the ideal scenario with floating-point raw data.

Visualization.The visualizations of adversarial examples again ResNet-50 in Figure 2 clearly show the characteristics of different imperceptible attacks against ResNet-50. For the first image with a relatively simple and clear object, the unrestricted attacks of NCF, DiffAttack and ACA perform attacks by modifying the semantics fairly, while DiffPGD uses denoising to avoid significant semantic modifications, but often has lower ASR as in Table 1. However, for the image with complex content, the unrestricted attacks result in obvious unnatural color, texture, artifacts and semantic changes. For the normal attacks with perceptual-based restrictions, by amplifying the noises, it can be seen that AdvDrop has a obvious gridding effect due to the blocking operation in DCT operation, and the perturbation strength in PerC-AL and SSAH is also related to the edge or texture components of the image. In contrast, our AdvAD continuously maintains uniform and lower perturbation which is very difficult to be seen even in the adversarial examples with \( 5\) noise. For AdvAD-X, the perturbations are very slight modifications to the decimal places of the floating-point raw data for each pixel, thus it is still difficult to be seen even after \( 100\) magnification. More quantitative comparisons and visualizations are provided in **Appendix**D.1, D.2.

### Robustness

The robustness of attacks is also evaluated against defense methods, including purification methods of NRP , DS , diffusion-based purification  and adversarial training robust models of Inc-V3 , Res-50 , Swin-B , ConvNeXt-B . Two classic image transformation defenses of JPEG compression , Bit-depth reduction , and another type of defense, random smoothing , are also included. Considering the robustness and transferability of attacks are comparable only under close perturbation budget, the unrestricted attacks are not included in this and the next section.

    &  &  &  \\    & NRP & DS &  & &  & Res-50 & Swin-B & ConvNeXt-B \\  &  &  &  & Avg. &  &  &  &  & Avg. \\  AdvDrop  & 50.2 & **30.1** & **37.1** & **39.1** & 93.7 & 72.4 & 31.2 & 37.3 & 58.7 & 50.3 \\ PerC-AL  & 30.3 & 28.8 & 25.4 & 28.2 & **99.9** & 46.1 & 8.2 & 7.0 & 40.3 & 35.1 \\ SSAH  & 25.6 & 28.0 & 11.0 & 21.5 & 91.2 & **84.6** & 16.8 & 47.4 & 60.0 & 43.5 \\  AdvAD (ours) & **51.5** & 29.5 & 31.2 & 37.4 & 98.9 & 79.3 & **60.2** & **62.7** & **75.3** & **59.0** \\ AdvAD-X\({}^{}\)(ours) & 13.4 & 27.6 & 10.2 & 17.1 & 57.2 & 45.2 & 18.0 & 16.2 & 34.2 & 26.8 \\   

Table 2: Results of ASR defenses for robustness evaluation, including three post-processing purification methods and four adversarial training white-box robust models.

Figure 2: Visualizations of adversarial examples and corresponding perturbations crafted by nine imperceptible attacks. Perturbations are amplified as marked in top-right for the convenience of observation. Please zoom in to observe the details of the images with original resolution of \(224 224\).

As shown in Table 2, the proposed AdvAD demonstrates the best robustness in overall average compared with other imperceptible attacks of AdvDrop, PerC-AL and SSAH. Specifically, when attacking robust models, AdvAD achieved an much higher average ASR of 75.3\(\%\). For post-processing purifications aim at eliminating adversarial perturbations, despite the inherently lower perturbation strength, AdvAD still maintains the best or second-best ASR against different purifications, which is comparable to AdvDrop with much higher perturbation strength. Similarly, for the results of classic image transformation defenses in Figure 3, AdvAD also exhibits advantages in most of the factors. In addition, since random smoothing is not a truly end-to-end method but a method that uses the base model to make multiple predictions on noise-augmented images, we adopt a semi-white-box setup to fully test the attack performance as described in the caption. Table 3 shows the experimental results, and the PerC-AL is not included because it fails to attack in this setting. It can be seen that for all \(\), our AdvAD continuously achieves the best ASR with smaller perturbation strength.

We suppose the robustness of AdvAD mainly benefits from two aspects. Firstly, AdvAD performs attacks during a unique non-parametric diffusion process with adversarial guidance, which may be easier to break through existing adversarial training models using common attack paradigms. On the other hand, the inherently lower perturbation crafted by AdvAD is spread across the images more uniformly rather than gathering in some areas as can be seen in the visualization, making it more difficult to be eliminated. For AdvAD-X, it is anticipated to exhibit weak robustness since the extremely low perturbation in the ideal scenario is easy to defense.

### Transferability and Effect of Step \(T\) on AdvAD

Table 4 reports the ASRs of black-box attacks and the corresponding results of imperceptibility. We also test AdvAD with different step of \(T\) for comprehensive evaluation. Consistent with the diffusion models, a larger \(T\) denotes a finer decomposition granularity of the entire process, corresponding to the strength of adversarial guidance at each step. Thus, AdvAD with a larger \(T\) exhibits better imperceptibility, while a smaller \(T\) implies stronger black-box transferability. Notably, though there is a clear negative correlation between imperceptibility and transferability, our

   Model & Attack Method & Res-50 & Mob-V2 & Inc-V3 & VGG-19 & \(l_{2}\) & PSNR \(\) & SSIM \(\) & FID \(\) & LFIPS \(\) \\   & SSAH  & **99.7\({}^{*}\)** & 15.5 & 20.4 & 12.7 & 2.65 & 43.73 & 0.9911 & 4.48 & 0.0021 \\  & AdvAD (\(\)=1000) & **99.7\({}^{*}\)** & **18.3** & **22.6** & **15.1** & **1.06** & **51.84** & **0.9980** & **2.42** & **0.0095** \\  
 & AdvDrop  & 96.8\({}^{*}\) & 17.3 & 23.1 & 15.8 & 3.17 & 41.91 & 0.9872 & **5.57** & 0.0061 \\  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  &  &  \\    } } &  & 99.9\({}^{*}\) & **35.3** & 37.9 & 8.29 & 33.41 & 0.8803 & 34.57 & 0.0500 \\  & AdvAD (\(T\)=10) & **30.6** & **100.0\({}^{*}\)** & **35.3** & **38.5** & **7.23** & **34.60** & **0.9006** & **27.25** & **0.0480** \\   

Table 4: Transferability and effect of \(T\) of the proposed AdvAD. \(*\) means white-box ASR.

Figure 4: More results of (a) effect of step \(T\) on AdvAD and (b) transferability-imperceptibility relationship of attacks.

Figure 3: Robustness on JPEG compression and Bit-depth reduction with different factors.

AdvAD exceeds all comparison attacks in both of transferability and imperceptibility at different comparable levels, demonstrating the effectiveness of the proposed novel modeling framework.

To further elaborate the relationship between transferability and imperceptibility of AdvAD, as well as the optimal trade-off in practice, we plot two line graphs in Figure 4 under more values of \(T\). As shown in Figure 4 (a), as the value of \(T\) on the horizontal axis changes, the relationship between imperceptibility and transferability shows a clear proportional trend as mentioned above, consistent across different surrogate models. For the optimal trade-off, we consider that the intersection point of the two curves represents a balance between imperceptibility and transferability. Accordingly, for the ResNet-50 and MobileNetV2 models, the optimal values of \(T\) are 50 and 25, respectively. Moreover, Figure 4 (b) illustrates more direct curves of this relationship and the positions of other comparison methods within it. Note that, all the other comparison methods are located to the lower left of the curve of AdvAD. This indicates that our method consistently achieves the best results in both transferability and imperceptibility compared with other state-of-the-art restricted imperceptible attacks, demonstrating the effectiveness of our AdvAD as a new attack framework with flexibility through the proposed non-parametric diffusion process.

### Analysis

**Eq. (12) in Practice.** With the derived analytical formulation of Proposition 1, in Figure 5, we illustrate the actual values of \(_{t}\) and \(\|_{t}\|_{}\) of Eq. (12) using 100 randomly selected images. While Proposition 1 indicates that the upper bound of \(\|_{t}\|_{}\) is invariant with respect to step \(t\), the actual strength of the adversarial guidance produced by AMG rapidly decreases as the process progresses, which validates the unique property given at the end of Sec. 3.2. With the similarly decreasing \(_{t}\), the whole term of \(_{t}\|_{t}\|_{}\) representing \(l_{}\) distance of the guidance at step \(t\) also decreases from about 0.0008 to 0, supporting that the proposed modeling framework performs imperceptible attacks with inherently small perturbation strength. **Performance with Smaller \(\).** The results of AdvAD and AdvAD-X with smaller \(\) for PC module are shown in Table 5. As \(\) decreases from \(8\) to \(2\), the imperceptibility is naturally improved because of the upper bound of perturbation becomes lower, yet the ASR of 94.8% only drops slightly. When \(=1/255\), AdvAD still holds 87.4% ASR with 57.87 PSNR and 0.9993 SSIM, which means a large number of examples still can fool the DNN with a maximum of \( 1\) modification for each pixel, demonstrating the effectiveness of the adversarial guidance injected in the proposed diffusion process for attacks. Moreover, we provide the ablation study of AdvAD-X and additional discussions in **Appendix D.3**, D.4.

## 5 Conclusion and Outlook

In this paper, we propose a novel, fundamental modeling framework distinct from existing paradigms to tackle the challenge of imperceptible attacks. By exploring and deriving basic theory of diffusion models, the proposed AdvAD performs attacks through a non-parametric diffusion process with adversarial guidance, achieving inherently lower overall perturbation strength with high attack efficacy from a modeling perspective. Besides, the proposed AdvAD-X evaluates the extreme of this novel modeling framework and further reduces the perturbation strength to an extremely low level in an ideal scenario. Extensive experimental results support the effectiveness and progressiveness of the proposed methods. Beyond imperceptibility, AdvAD holds the potential to become a general and extensible attack paradigm thanks to the solid theoretical foundation and the innovative, controllable diffusion-based process for attacks. In addition, we also hope the new observation that AdvAD-X can successfully attack with extremely small perturbation using floating-point raw data can bring inspiration for revealing the robustness and interpretability (e.g., decision boundaries) of DNNs.

   \(\) & Attack & ASR\(\) & \(l_{2}\) & PSNR\(\) & SSIM\(\) & FID\(\) \\   & AdvAD & 98.6 & 0.93 & 53.27 & 0.986 & 1.78 \\  & AdvAD-X\({}^{}\) & 100.0 & 0.29 & 65.07 & 0.9998 & 0.18 \\   & AdvAD & 96.1 & 0.82 & 54.85 & 0.9989 & 1.33 \\  & AdvAD-X\({}^{}\) & 99 & 0.4 & 0.27 & 65.95 & 0.9998 & 0.15 \\   & AdvAD & 87.4 & 0.66 & 57.87 & 0.9993 & 0.77 \\  & AdvAD-X\({}^{}\) & 94.8 & 0.26 & 66.42 & 0.9998 & 0.14 \\   

Table 5: Results of AdvAD and AdvAD-X with smaller \(\) against Res-50. Step \(T\) is fixed as \(1000\).