# G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering

Xiaoxin He\({}^{1}\)   Yijun Tian\({}^{2}\)   Yifei Sun\({}^{1}\)   Nitesh V. Chawla\({}^{2}\)   Thomas Laurent\({}^{3}\)

**Yann LeCun\({}^{4,5}\)   Xavier Bresson\({}^{1}\)   Bryan Hooi\({}^{1}\)**

{xiaoxin, yifeisun, xaviercs, bhooi}@comp.nus.edu.sg

{yijun.tian, nchawla}@nd.edu, tlaurent@lmu.edu, yann@cs.nyu.edu

\({}^{1}\)National University of Singapore  \({}^{2}\)University of Notre Dame  \({}^{3}\)Loyola Marymount University

\({}^{4}\)New York University  \({}^{5}\)Meta AI

###### Abstract

Given a graph with textual attributes, we enable users to 'chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our _G-Retriever_ method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, _G-Retriever_ performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination. Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.

## 1 Introduction

**Graphs and Large Language Models (LLMs).** The advent of LLMs has significantly shaped the artificial intelligence landscape. As these models are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital. In particular, in our interconnected world, a significant portion of real-world data inherently possesses a graph structure, such as the Web, e-commerce, recommendation systems, knowledge graphs, and many others. Moreover, many of these involve graphs with textual attributes (_i.e., textual graphs_), making them well-suited for LLM-centric methods. This has spurred interest in combining graph-based technologies, particularly graph neural networks (GNNs), with LLMs to enhance their reasoning on graphs .

**The Present Work: Enabling 'Chat With Your Graph'.** While existing works integrate LLMs and GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graphclassification , or answering simple questions on small or synthetic graphs [44; 31]. In contrast, we develop a flexible question-answering framework targeting complex and real-world graphs. This framework enables users to 'chat with their graph' via a unified conversational interface, representing a leap towards intuitive interaction with graph data, as demonstrated in Figure 1.

**The Need for a Comprehensive GraphQA Benchmark.** Question answering (QA) is a fundamentally important task in natural language processing, serving as a key benchmark for assessing LLMs and providing a unified interface for various capabilities. Despite extensive research in QA, a comprehensive benchmark specifically tailored for the graph modality is lacking. In contrast to existing benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and shortest path [6; 44], our benchmark addresses complex and real-world graph applications including common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure 2). This is vital for measuring progress toward a model capable of answering a wide range of questions about graphs from diverse applications.

**New Architecture for GraphQA.** To enable effective and efficient graph QA, even on large graphs, we propose _G-Retriever_, a new framework combining the strengths of GNNs, LLMs, and RAG (Figure 3). Next, we will discuss the motivation, strengths, and details of our model.

**Tackling Hallucination in Graph LLMs.** LLMs are prone to hallucination, a phenomenon where the generated content is factually inaccurate or nonsensical . We validate the presence of this issue in graph settings. In particular, we employ a baseline method that adapts MiniGPT-4  to graphs, where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in GraphToken . Our findings, shown in Table 1, indicate that hallucination, an important problem in text-based LLMs, is also prevalent in Graph LLMs. This may be attributed to the baseline's inability to recall the entire graph structure from a single graph embedding, leading to the generation of incorrect nodes or edges during the QA task. In contrast, by employing RAG for direct information retrieval from the actual graph, our _G-Retriever_ mitigates this issue, as substantiated by Table 1.

**Enhancing Scalability and Efficiency in Graph LLMs.** Recent research endeavors have explored translating graphs into natural language, such as by flattening nodes and edges into a text sequence, enabling their processing by LLMs for graph-based tasks [56; 6]. However, this method faces critical scalability issues. Converting a graph with thousands of nodes and edges into a text sequence results in an excessive number of tokens, surpassing the input capacity of many LLMs. An alternative of truncating the graph text sequence to fit the LLM's input token limit leads to loss of information and response quality. _G-Retriever_ overcomes these issues with its RAG component, which allows for effective scaling to larger graphs by selectively retrieving only relevant parts of the graph.

**Tailoring the RAG Approach to Graphs.** Existing RAG methodologies are primarily designed for simpler data types or knowledge graphs, where information is retrieved in a manner isolated from the graph structure [7; 1; 36; 16]. Hence, we introduce a new retrieval approach for general textual graphs. Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST)

Figure 1: We develop a flexible question-answering framework targeting real-world textual graph applications via a unified conversational interface. Presented here are examples showcasing the model’s adeptness in handling generative and creative queries in practical graph-related tasks: common sense reasoning, scene understanding, and knowledge graph reasoning, respectively.

optimization problem, which takes the neighborhood information into account during retrieval. This also allows the return of a subgraph most relevant to a query, thereby improving explainability.

The contributions of this paper are outlined as follows:

* **Pioneering the integration of Graph RAG.** We present the first retrieval approach for general textual graph tasks, which greatly enhances scalability and efficiency.
* **Enabling 'Chat with Your Graph'.** We develop a flexible question answering framework to handle complex and real-world textual graphs through a unified conversational interface.
* **Introducing A Novel GraphQA Benchmark.** We introduce a diverse benchmark targeted at real-world graph question answering, filling a crucial research gap.
* **Empirical Findings.** We demonstrate the efficiency and effectiveness of _G-Retriever_ in multiple domains and present the significant finding of hallucination in graph LLMs.

## 2 Related Work

**Graphs and Large Language Models.** A significant body of research has emerged at the intersection of graph-based techniques and LLMs [30; 24; 15; 44; 54]. This exploration spans diverse aspects, ranging from the design of general graph models [47; 25; 51; 19; 40; 31], and multi-modal architectures [23; 49] to practical applications. Noteworthy applications include fundamental graph reasoning [52; 3; 56], node classification [8; 11; 39; 5; 50; 4; 33], graph classification/regression [32; 55], and leveraging LLMs for knowledge graph-related tasks [41; 14; 29].

**Retrieval-Augmented Generation (RAG).** The concept of Retrieval-Augmented Generation, initially proposed by Lewis et al. , has gained increased attention for its ability to mitigate the issue of hallucination within LLMs and enhance trustworthiness and explainability . Despite its success in language-related tasks, the application of retrieval-based approaches to general graph tasks remains largely unexplored. Most existing work focuses primarily on the knowledge graph [38; 1; 36; 16]. Our research is the first to apply a retrieval-based approach to general graph tasks, marking a novel advancement in the field and demonstrating the versatility of RAG beyond language processing.

**Parameter-Efficient Fine-Tuning (PEFT).** The field of LLMs has witnessed significant advancements through various parameter-efficient fine-tuning techniques. These methodologies have played a crucial role in refining LLMs, boosting their performance while minimizing the need for extensive parameter training. Notable among these techniques are prompt tuning, as introduced by Lester et al. , and prefix tuning, proposed by Li and Liang . Furthermore, methods like LoRA ,

    &  &  \\  & What kind of animal is in the bushes? \\  & & Answer the question and list the nodes or edges from the graph that support your answer. \\   & [Incorrect response with hallucinated nodes and edges] \\  & The animal in the bushes is a deer. \\  & Nodes: \\  & * Der (node 1) \\  & * Bushes (node 2) \\  & Edges: \\  & * Der \(\) Bushes (edge 1) \\  & * Der \(\) Grass (edge 2) \\  & * Bushes \(\) Grass (edge 3) \\   & [Correct response with accurate node and edge references] \\  & Based on the information provided in the graph, the animal in the bushes is an elephant. \\   & The nodes that support this answer are: \\   & * Node 1: The elephant is mentioned in the first line of the graph. \\   & The edges that support this answer are: \\   & * The edge connects the elephant to the bushes, indicating that the elephant is in the bushes. \\   & Therefore, the answer to the question is: The animal in the bushes is an elephant. \\   

Table 1: Observation and mitigation of hallucination in graph LLMs.

and the LLaMA-adapter , have been influential. These advancements in PEFT have laid the foundation for the development of sophisticated multimodal models. Prominent examples in this domain include MiniGPT-4 , LLaVA , and NExT-Chat . There are also emerging efforts in applying PEFT to graph LLMs, such as GraphLLM  and GraphToken  for basic graph reasoning tasks and GNP  for multi-option QA on knowledge graphs.

## 3 Formalization

This section establishes the notation and formalizes key concepts related to textual graphs, language models for text encoding, and large language models and prompt tuning.

**Textual Graphs.** A textual graph is a graph where nodes and edges possess textual attributes. Formally, it can be defined as \(G=(V,E,\{x_{n}\}_{n V},\{x_{e}\}_{e E})\), where \(V\) and \(E\) represent the sets of nodes and edges, respectively. Additionally, \(x_{n} D^{L_{n}}\) and \(x_{e} D^{L_{e}}\) denote sequential text associate with a node \(n V\) or an edge \(e E\), where \(D\) represents the vocabulary, and \(L_{n}\) and \(L_{e}\) signify the length of the text associated with the respective node or edge.

**Language Models for Text Encoding.** In the context of textual graphs, language models (LMs) are essential for encoding the text attributes associated with nodes and edges, thereby learning representations that capture their semantic meaning. For a node \(n\) with text attributes \(x_{n} D^{L_{n}}\), an LM encodes these attributes as:

\[z_{n}=(x_{n})^{d},\] (1)

where \(z_{n}\) is the output of the LM, and \(d\) is the dimension of the output vector.

**Large Language Models and Prompt Tuning.** LLMs have introduced a new paradigm for task-adaptation known as "pre-train, prompt, and predict", replacing the traditional "pre-train, fine-tune" paradigm. In this paradigm, the LLM is first pre-trained on a large corpus of text data to learn general language representations. Then, rather than fine-tuning the model on task-specific labeled data, the model is prompted with a textual prompt that specifies the task and context. Subsequently, the model generates the output directly based on the prompt and the input.

The LLM, parameterized by weights \(\), takes a sequence of tokens \(X\), and a prompt \(P\) as input, and generates a sequence of tokens \(Y=\{y_{1},y_{2},,y_{r}\}\) as output. Formally, the probability distribution of the output sequence given the concatenated input sequence and prompt, _i.e.,_\([P;X]\), is defined as

\[p_{}(Y|[P;X])=_{i=1}^{r}p_{}(y_{i}|y_{<i},[P;X]).\] (2)

Here, \(y_{<i}\) represents the prefix of sequence \(y\) up to position \(i-1\), and \(p(y_{i}|y_{<i},[P;X])\) represents the probability of generating token \(y_{i}\) given \(y_{<i}\) and \([P;X]\).

Soft prompt tuning eliminates the need for manual prompt design. Given a series of \(p\) tokens \(X=\{x_{1},x_{2},,x_{p}\}\), after being processed by the text embedder, it forms a matrix \(X_{e}^{p d_{l}}\), where \(d_{l}\) is the dimension of the embedding space. Soft prompts can be represented as parameters \(P_{e}^{q d_{l}}\), where \(q\) is the length of the prompt. The prompt is then concatenated with the embedded input, forming a single matrix \([P_{e};X_{e}]^{(q+p) d_{l}}\). This combined matrix is processed by the self-attention layers in LLM as usual. Training involves maximizing the likelihood of \(Y\) through backpropagation, with gradient updates applied solely to \(P_{e}\), while \(\) remains fixed.

## 4 Proposed GraphQA Benchmark

Our GraphQA represents a comprehensive and diverse benchmark for graph question-answering. It is tailored to assess the capabilities of models in answering a wide range of questions about graphs across diverse domains.

### Data Format

Each entry in the GraphQA benchmark consists of a textual graph, a question related to the graph, and one or more corresponding answers, as illustrated in Figure 2.

**Textual Graphs.** The textual graph is converted into a natural language format, resulting in a list of nodes and edges, akin to a CSV file format. It is important to note that while multiple methods exist for textualizing a graph, our focus is not on identifying the optimal solution. Instead, we prioritize a straightforward yet empirically effective approach for representing graphs in natural language, facilitating the benchmark's use in diverse GraphQA scenarios.

**Questions and Answers.** Questions are designed to explore specific elements or relationships within the graph. Answers, residing within the attributes of nodes or edges, often require multi-hop reasoning for accurate identification.

### Description of Datasets

The GraphQA benchmark integrates three existing datasets: ExplaGraphs, SceneGraphs, and WebQSP. Table 2 presents the summary statistics of these datasets. It is important to note that these datasets were not originally developed for this work. However, a significant contribution of our research is the standardization and processing of these diverse datasets into a uniform data format suitable for the GraphQA benchmark. These datasets, previously utilized in different contexts, are reintroduced with a new focus tailored for GraphQA. For a detailed comparison with the original datasets, see the Appendix C.

ExplaGraphs is a dataset for generative commonsense reasoning, focusing on creating explanation graphs for stance prediction in debates. It offers detailed, unambiguous commonsense-augmented graphs to evaluate arguments supporting or refuting a belief. The primary task is to assess whether arguments are supportive or contradictory, using accuracy as the metric. We have converted the triplet-form provided in Saha et al.  into a standard graph format.

SceneGraphs, a visual question answering dataset, includes 100,000 scene graphs. Each graph details objects, attributes, and relations within an image. This dataset challenges users with tasks requiring spatial understanding and multi-step inference. The task is to answer open-ended questions based on a textual description of a scene graph, evaluated on accuracy. We have sampled from the GQA dataset  and constructed standard graphs from the provided JSON files.

WebQSP is a large-scale multi-hop knowledge graph QA dataset consisting of 4,737 questions. It was proposed by Yih et al.  and, following Luo et al. , utilizes a subset of Freebase, encompassing facts within 2-hops of entities mentioned in the questions. The task involves answering questions that

   Dataset & ExplaGraphs & SceneGraphs & WebQSP \\  \#Graphs & 2,766 & 100,000 & 4,737 \\ Avg. \#Nodes & 5.17 & 19.13 & 1370.89 \\ Avg. \#Edges & 4.25 & 68.44 & 4252.37 \\ Node Attribute & Commonsense concepts & Object attributes (_e.g._ color, shape) & Entities in Freebase \\ Edge Attribute & Commonsense relations & Relations (_e.g._ actions, spatial relations) & Relations in Freebase \\ Task & Common sense reasoning & Scene graph question answering & Knowledge based question answering \\ Evaluation Matrix & Accuracy & Accuracy & Htt@1 \\   

Table 2: Summary of datasets used in GraphQA benchmark.

Figure 2: Illustrative examples from the GraphQA benchmark datasets.

require multi-hop reasoning. Given the possibility of multiple answers for the same question, the hit@1 metric is used to assess the precision of the top returned answer.

## 5 G-Retriever

In this section, we introduce _G-Retriever_, a new architecture tailored for GraphQA, which integrates the strengths of GNNs, LLMs, and RAG. To allow efficient fine-tuning while preserving the LLM's pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output of the GNN. Our RAG-based design mitigates hallucinations through direct retrieval of the graph, while allowing our approach to scale to graphs exceeding the LLM's context window size. To adapt RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem. This approach also allows us to enhance explainability by returning the retrieved subgraph.

_G-Retriever_ comprises four main steps: indexing, retrieval, subgraph construction and generation, as depicted in Figure 3. The implementation details of each step are elaborated in the following sections.

### Indexing

We initiate the RAG approach by generating node and graph embeddings using a pre-trained LM. These embeddings are then stored in a nearest neighbor data structure.

To elaborate, consider \(x_{n} D^{L_{n}}\) as the text attributes of node \(n\). Utilizing a pre-trained LM, such as SentenceBert , we apply the LM to \(x_{n}\), yielding the representation \(z_{n}\):

\[z_{n}=(x_{n})^{d},\] (3)

where \(d\) denotes the dimension of the output vector. Similar preprocessing steps are applied to edges. Refer to Figure 3, Step 1 for an illustrative representation.

### Retrieval

For retrieval, we employ the same encoding strategy to the query \(x_{q}\), to ensure consistent treatment of textual information:

\[z_{q}=(x_{q})^{d}.\] (4)

Next, to identify the most relevant nodes and edges for the current query, we use a k-nearest neighbors retrieval approach. This method yields a set of'relevant nodes/edges' based on the similarity between the query and each node or edge. The retrieval operation is defined as:

\[ V_{k}&=_{n V}(z_{q},z_{n})\\ E_{k}&=_{e E}(z_{q},z_{e}), \] (5)

Figure 3: Overview of the proposed _G-Retriever_: 1) Indexing: Graphs are indexed for efficient query processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An answer is generated using a ‘graph prompt’, a textualized graph, and the query.

where \(z_{n}\) and \(z_{e}\) are the embeddings of node \(n\) and edge \(e\), respectively. We use the cosine similarity function, \((,)\), to measure the similarity between the query representation and the node/edge embeddings. The argtopk operation retrieves the top-k elements based on this similarity, providing a set of nodes \(V_{k}\) and edges \(E_{k}\) considered most relevant to the query. See Step 2 of Figure 3.

### Subgraph Construction

This step aims to construct a subgraph that encompasses as many relevant nodes and edges as possible, while keeping the graph size manageable. This approach offers two key benefits: Firstly, it helps to filter out nodes and edges that are not pertinent to the query. This is crucial because irrelevant information can overshadow the useful data, potentially diverting the focus of the subsequent LLM from the information of interest. Secondly, it enhances efficiency; by keeping the graph size manageable, it becomes feasible to translate the graph into natural language and then input it into the LLM for processing. The Prize-Collecting Steiner Tree algorithm  serves as our primary method for identifying such optimally sized and relevant subgraphs. See Step 3 in Figure 3.

**Prize-Collecting Steiner Tree (PCST).** The PCST problem aims to find a connected subgraph that maximizes the total prize values of its nodes while minimizing the total costs of its edges. Our approach assigns higher prize values to nodes and edges more relevant to the query, as measured by cosine similarity. Specifically, the top \(k\) nodes/edges are assigned descending prize values from \(k\) down to 1, with the rest assigned zero. The node prize assignment is as follows:

\[(n)=k-i,&n V_{k}ni,\\ 0,&.\] (6)

Edge prizes are assigned similarly. The objective is to identify a subgraph, \(S^{*}=(V^{*},E^{*})\), that optimizes the total prize of nodes and edges, minus the costs associated with the size of the subgraph:

\[S^{*}=S G,\\ S}{}_{n V_{S}}(n )+_{e E_{S}}(e)-(S),\] (7)

where

\[(S)=|E_{S}| C_{e},\] (8)

and \(C_{e}\) denotes a predefined cost per edge, which is adjustable to control the subgraph size.

The original PCST algorithm is designed for node prizes only. However, given the significance of edge semantics in certain scenarios, we adapt the algorithm to accommodate edge prizes as follows: Consider an edge e with a cost \(C_{e}\) and a prize \(P_{e}\). If \(C_{e}>P_{e}\), it can be treated as a reduced edge cost of \(C_{e}-P_{e}\). However, if \(P_{e}>C_{e}\), negative edge costs are not allowed in the original algorithm. Our solution involves replacing edge \(e\) with a 'virtual node' \(v_{e}\), connected to both endpoints of \(e\). This virtual node is assigned a prize of \(P_{e}-C_{e}\), and the cost of the two new edges leading to the virtual node is set to zero. This modification effectively mirrors the original problem, as including edge \(e\) in the original graph is analogous to including the virtual node in the modified graph. Finally, we optimize the PCST problem using a near-linear time approach .

### Answer Generation

**Graph Encoder.** Let \(S^{*}=(V^{*},E^{*})\) represent the retrieved subgraph. We use a graph encoder to model the structure of this graph, specifically using a standard Graph Attention Network (GAT) . Our approach for encoding the retrieved subgraph is defined as follows:

\[h_{g}=(_{_{1}}(S^{*}))^{d_{g}},\] (9)

Here, POOL denotes the mean pooling operation, and \(d_{g}\) is the dimension of the graph encoder.

**Projection Layer.** We incorporate a multilayer perceptron (MLP) to align the graph token with the vector space of the LLM:

\[_{g}=_{_{2}}(h_{g})^{d_{l}},\] (10)

where \(d_{l}\) is the dimension of the LLM's hidden embedding.

**Text Embedder.** To leverage the text-reasoning capabilities of LLMs, we transform the retrieved subgraph \(S^{*}\) into a textual format. This transformation involves flattening the textual attributes of the nodes and edges, as illustrated in the green box in Figure 2. We refer to this operation as \(()\). Subsequently, we combine the textualized graph with the query to generate a response. Let \(x_{q}\) denote the query; we concatenate it with the textualized graph \((S^{*})\). We then map the result to an embedding \(h_{t}\) using a text embedder, which is the first layer of a pretrained and frozen LLM:

\[h_{t}=([(S^{*});x_{q}])^{L  d_{l}},\] (11)

where \([;]\) represents the concatenation operation, and \(L\) is the number of tokens.

**LLM Generation with Graph Prompt Tuning.** The final stage involves generating the answer \(Y\) given the graph token \(_{g}\), acting as a soft prompt, and the text embedder output \(h_{t}\). These inputs are fed through the self-attention layers of a pretrained frozen LLM, with parameter \(\). The generation process is represented as follows:

\[p_{,_{1},_{2}}(Y|S^{*},x_{q})=_{i=1}^{r}p_{,_{1}, _{2}}(y_{i}|y_{<i},[_{g};h_{t}]),\] (12)

where \([_{g};h_{t}]\) concatenates the graph token \(_{g}\) and the text embedder output \(h_{t}\). While \(\) is frozen, the graph token \(_{g}\) receives gradients, enabling the optimization of the parameters of the graph encoder \(_{1}\) and the projection layer \(_{2}\) through standard backpropagation.

## 6 Experiments

### Experiment Setup

In the indexing step, we use SentenceBert  as the LM to encode all node and edge attributes. In the generation step, we use the open-source Llama2-7b  as the LLM and Graph Transformer  as the graph encoder. Additional details are provided in Appendix B.1.

### Main Results

In our experiments, we consider three model configurations: _1) Inference-only_: Using a frozen LLM for direct question answering; _2) Frozen LLM w/ prompt tuning (PT)_: Keeping the parameters of the LLM frozen and adapting only the prompt; _3) Tuned LLM_: Fine-tuning the LLM with LoRA . We provide more details in Appendix B.2.

Table 3 demonstrates the effectiveness of our method across three datasets in various configurations. In the inference-only setting, _G-Retriever_ surpasses all baselines. Notably, LLM can perform even better when no graph knowledge is provided (_i.e.,_ question only), which might be attributed to the complexity and potential noise in the knowledge. For frozen LLM with prompt tuning, _G-Retriever_ outperforms traditional prompt tuning and GraphToken , a graph prompt tuning-based method, with average performance increases of 40.6% and 30.8% respectively. Furthermore, when tuned with LoRA, _G-Retriever_ achieves the best performance.

   Setting & Method & ExplaGraphs & SceneGraphs & WebQSP \\   & Zero-shot & 0.5650 & 0.3974 & 41.06 \\  & Zero-CoT  & 0.5704 & 0.5260 & 51.30 \\  & CoT-BAG  & 0.5794 & 0.5680 & 39.60 \\  & KAPING  & 0.6227 & 0.4375 & 52.64 \\   & Prompt tuning & 0.5763 \(\) 0.0243 & 0.6341 \(\) 0.0024 & 48.34 \(\) 0.64 \\  & GraphToken  & 0.8508 \(\) 0.0551 & 0.4903 \(\) 0.0105 & 57.05 \(\) 0.74 \\  & _G-Retriever_ & 0.8516 \(\) 0.0092 & 0.8131 \(\) 0.0162 & 70.49 \(\) 1.21 \\  & \(_{}\) & \(\) 47.77\% & \(\) 28.23\% & \(\) 45.81\% \\   & LoRA & 0.8538 \(\) 0.0353 & 0.7862 \(\) 0.0031 & 66.03 \(\) 0.47 \\  & _G-Retriever_ w/ LoRA & **0.8705 \(\) 0.0329** & **0.8683 \(\) 0.0072** & **73.79 \(\) 0.70** \\  & \(_{}\) & \(\) 1.95\% & \(\) 11.74\% & \(\) 10.44\% \\   

Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \(\) std) are presented. The first best result for each task is highlighted in **bold** and the second best result is highlighted with an underline.

### Efficiency Evaluation

The efficiency of our approach is highlighted by the data in Table 4. Implementing our graph-based retrieval significantly decreases the number of tokens required to describe the graphs in text, reduces the number of nodes in graphs, and speeds up the training process. Specifically, for the SceneGraphs dataset, tokens decreased by 83%, nodes by 74%, and training time by 29%. For the WebQSP dataset, tokens decreased by 99%, nodes by 99%, and training time by 67%. These substantial reductions demonstrate the method's efficiency and potential in managing large-scale graph data.

### Mitigation of Hallucination

To evaluate hallucination, we instructed the models to answer graph-related questions, specifically by identifying supporting nodes or edges from the graph. We assessed the model's faithfulness using three metrics: the fraction of valid nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction of times the entire set of cited nodes and edges was valid (denoted as Fully Valid Graphs). We manually reviewed 100 responses from both our method and the baseline (, LLM with graph prompt tuning). Table 5 shows that _G-Retriever_ significantly reduces hallucinations by 54% compared to the baseline, as our graph retrieval ensures that the data is sourced directly from the actual graph, leading to fewer hallucinations. See Appendix G for details.

### Ablation Study

In this ablation study, we assess the individual impact of key components within our pipeline. As shown in Table 6, there are performance drops when any of these components are removed, with the graph encoder and textualized graph showing declines of 22.51% and 19.19%, respectively. This demonstrates their complementary effects in representing the graph in both textual and embedded formats. Additionally, the retrieval on graphs is also important to the overall performance. Further details are available in Appendix B.3. We also present additional studies on our framework: it is robust to the choice of graph encoders (see Appendix B.4) and benefits from the increased scale of LLMs (see Appendix B.5).

Additionally, we include a detailed comparison with existing retrieval methods (see Appendix D), a discussion on the complexity (see Appendix E), and demonstrations on how to use _G-Retriever_ to 'chat with your graph' (see Appendix H).

   Method & Hit@1 \\  _G-Retriever_ & 70.49 \\ w/o Graph Encoder & 54.62 (\(\)22.51\%) \\ w/o Projection Layer & 69.70 (\(\)1.11\%) \\ w/o Textualized Graph & 56.96 (\(\)19.19\%) \\ w/o Retrieval & 63.84 (\(\)9.43\%) \\   

Table 6: Ablation study on the WebQSP dataset showing performance drops (Hit@1) when each component is removed.

    &  &  \\   & \# Tokens & \# Nodes & Min/Epoch & \# Tokens & \# Nodes & Min/Epoch \\  SceneGraphs & 1,396 & 19 & 123.1 & 235 (\(\)83\%) & 5 (\(\)74\%) & 86.8 (\(\)29\%) \\ WebQSP & 100,627 & 1,371 & 18.7 & 610 (\(\)99\%) & 18 (\(\)99\%) & 6.2(\(\)67\%) \\   

Table 4: Retrieval on graphs significantly improves efficiency.

    & Baseline & _G-Retriever_ \\  Valid Nodes & 31\% & 77\% \\ Valid Edges & 12\% & 76\% \\ Fully Valid Graphs & 8\% & 62\% \\   

Table 5: Hallucination reduction on the SceneGraphs dataset, measured by fractions of valid nodes, valid edges, and fully valid graphs (where all nodes and edges are correct).

Conclusion

In this work, we introduce a new GraphQA benchmark for real-world graph question answering and present _G-Retriever_, an architecture adept at complex and creative queries. Experimental results show that _G-Retriever_ surpasses baselines in textual graph tasks across multiple domains, scales effectively with larger graph sizes, and demonstrates resistance to hallucination.

**Limitations and Future Work:** Currently, _G-Retriever_ employs a static retrieval component. Future developments could investigate more sophisticated RAG where the retrieval is trainable.