# Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage

Haobo Zhang

ShanghaiTech University

zhanghb2023@shanghaitech.edu.cn &Xiyue Peng

ShanghaiTech University

pengyx2024@shanghaitech.edu.cn &Honghao Wei

Washington State University

honghao.wei@swu.edu &Xin Liu

ShanghaiTech University

liuxin7@shanghaitech.edu.cn

Corresponding author.

###### Abstract

Offline safe reinforcement learning (RL) aims to find an optimal policy using a pre-collected dataset when data collection is impractical or risky. We propose a novel linear programming (LP) based primal-dual algorithm for convex MDPs that incorporates "uncertainty" parameters to improve data efficiency while requiring only partial data coverage assumption. Our theoretical results achieve a sample complexity of \((1/(1-))\) under general function approximation, improving the current state-of-the-art by a factor of \(1/(1-)\), where \(n\) is the number of data samples in an offline dataset, and \(\) is the discount factor. The numerical experiments validate our theoretical findings, demonstrating the practical efficacy of our approach in achieving improved safety and learning efficiency in safe offline settings.

## 1 Introduction

Safe reinforcement learning (RL) aims to learn a reward-maximizing policy while satisfying multiple safety constraints, demonstrating its practicality in many real-world applications, such as autonomous driving (Kiran et al., 2021), robotics (Levine et al., 2016), and healthcare (Yu et al., 2021). In these tasks, certain behaviors may potentially harm the agent or its surroundings, which is crucial for task completion. One way to mathematically characterize safe RL is through Constrained Markov Decision Processes (CMDPs) (Altman, 2021), where safety constraints are incorporated into the problem when optimizing the objective.

Offline RL aims to learn a sequence of actions from a pre-collected dataset to address scenarios where interacting with the environment is risky, expensive, or impractical. Ensuring sample efficiency in offline RL with function approximation typically requires additional assumptions about both the function classes and the dataset due to training instability and distribution shift issues (Fujimoto et al., 2019; Kostrikov et al., 2021; Paine et al., 2020). Earlier studies (Chen and Jiang, 2019; Liao et al., 2022; Liu et al., 2019; Wang et al., 2019; Zhang et al., 2020) in offline RL usually require that all functions in the function space are Bellman-complete and that the dataset has full coverage, meaning it covers the state-action distributions induced by all policies. This might be a mild and accepted assumption in offline RL without considering safety. However, it is highly unacceptable and impractical in safe offline RL, as it would require the dataset to cover all hazardous state-action pairs induced by **all** dangerous policies. To address the full coverage issue, later works (Chen and Jiang, 2022; Rashidinejad et al., 2021; Uehara and Sun, 2021; Xie et al., 2021; Zhan et al., 2022; Zhu et al.,2023] reduce the assumption to single-policy coverage by using pessimism in the face of uncertainty. Unfortunately, all existing studies [Chen et al., 2022, Hong et al., 2024, Le et al., 2019] in safe offline RL still require coverage for **all** policies.

Beyond traditional offline safe RL, many applications do not fit the standard RL problem [Abel et al., 2021]. There is a substantial body of literature [Geist et al., 2022, Mutti et al., 2023, Zahavy et al., 2021] studying a more general scenario called convex MDPs, where the objective function is modeled as a convex (or concave) utility function instead of linear, as in the standard RL problem. This framework is quite general and captures various learning scenarios, including imitation [Abbeel and Ng, 2004], exploration [Hazan et al., 2019], and more. However, studying convex MDPs introduces additional challenges. In convex MDPs, moving beyond cumulative rewards means that the Bellman equation fails to hold due to the lack of reward additivity. This leads to breakdowns in many techniques based on Dynamic Programming (DP) [Zhang et al., 2020b]. Despite a large body of practical literature [Lee et al., 2021, Xu et al., 2022, Zheng et al., 2023], robust theoretical analysis remains lacking in this setting. To address the coverage issue and extend the general function approximation setting, we focus on convex MDPs in the safe offline setting. Our main contributions are summarized below (the detailed comparisons can be found in Table 1):

\(\) We are the first to study offline convex MDPs under safety constraints with partial data coverage assumption. We reformulate the problem using marginalized importance weights to avoid issues caused by the Bellman equation's failure in convex MDPs.

\(\) We analyze the convergence rate of our proposed approach under partial coverage assumption and theoretically prove that our algorithm achieves \((})\) in both objective and violation bounds with general function approximation, when the number of iteration is larger than \(n\). The sample complexity of \((})\) improves the best existing result by a factor of \(1/(1-)\)2.

\(\) Experimental results on Imitation Learning and standard CMDPs demonstrate the generality and effectiveness of our algorithm. As Figure 1 shows our algorithm performs well even with a completely random and safety-violated offline dataset with general function approximation, which verifies our theoretical findings.

### Related Work

**Offline Safe RL:** The offline safe reinforcement learning setting entails the agent learning from a fixed dataset while adhering to safety constraints. This involves a blend of offline RL and safe RL, yet research on this approach, particularly concerning theoretical analysis, remains limited. BCQL

   Algorithm & Convex MDP &  Data \\ Coverage \\  &  Function \\ Approximation \\  & 
 Sample \\ Complexity \\  \\  CoptiDICE [Lee et al., 2021] & No & **Partial** & **General** & None \\  DPDL [Chen et al., 2022] & No & **Partial** & None & \((})\) \\  MBCL [Le et al., 2019] & No & Full & **General** & \((})\) \\  PDCA [Hong et al., 2024] & No & Full & **General** & \((})\) \\  Ours & **Yes** & **Partial** & **General** & \((})}\) \\   

Table 1: Comparison of algorithms for offline safe RL with function approximation.

Figure 1: Performance of our algorithm on FrozenLike with completely random data.

augmented by BCQ (Fujimoto et al., 2019) optimizes the policy in the offline phase and applies the Lagrange method to handle constraints. CPQ (Xu et al., 2022) tackles the safety constraints by overestimating the cost value function of out-of-distribution ('unsafe') actions and updating the reward value function with'safe' actions. Another line of work (Lee et al., 2021) using DICE-style technique optimizes policy by calculating the stationary distribution of state-action pairs instead of value function and extracts the policy by importance-sample behavior cloning. CDT (Liu et al., 2023) combines Decision Transformer with safety constraints and utilizes data augmentation based on Pareto frontiers to enhance the safety and adaptability of the Transformer. It also focuses on the model's capability to different cost thresholds. The work (Le et al., 2019) proposes a meta-algorithm, using Fitted Q Evaluation and Fitted Q Iteration as subroutines to evaluate safety constraints and learn policy respectively. (Chen et al., 2022) analyzes the information-theoretic sample complexity lower bound and proposes a near-optimal primal-dual learning algorithm under partial data coverage but without function approximation. The most related work (Hong et al., 2024) approaches the problem from the perspective of the Actor-Critic algorithm, analyzing the sample complexity to be \((})\) under Slater's condition and full data coverage assumption.

**Convex MDP:** Convex MDP problem extends the scope of MDP by focusing on convex objective functions of stationary distribution, rather than an inner product between reward and stationary distribution. To address the challenge, (Zhang et al., 2020) introduces a variational Monte Carlo gradient estimation algorithm, demonstrating convergence to the optimal policy across general utility functions. The work (Zahavy et al., 2021) utilizes Fenchel duality to cast convex MDPs as min-max "two-player" games, proposing a meta-algorithm that addresses various convex MDPs through distinct subroutines. The work (Geist et al., 2022) approaches convex MDPs from the Mean-Field Game (MFG) perspective, establishing the equivalence between the optimal condition in convex MDPs and Nash equilibrium in MFGs. (Ying et al., 2023) studies the convex CMDP problem through a policy-based primal-dual algorithm and proves an \((T^{-1/3})\) convergence rate in both optimality gap and constraint violation. The existing literature on convex MDPs primarily focuses on the online setting, while this paper specifically targets the offline scenario.

## 2 Problem Setup

### Convex CMDP problem

We study a discounted constrained Markov decision process (CMDP), denoted by \(=(,,R,C,T,,_{0})\), where \(\) is the state space (a finite set), \(\) is the action space (a finite set), \(R:\) and \(C:\) are reward and cost functions, respectively, \(T:()\) is the transition probability kernel, where \(()\) denotes the probability simplex, \([0,1)\) denotes the discount factor, and \(_{0}()\) is the initial state distribution. We define a policy \(:()\) as a probability mapping from states to actions. At time slot \(t\), the agent observes state \(s_{t}\) and takes action \(a_{t}\) according to the policy \(\). For a policy \(\), we define its discounted state-action occupancy measure \(d_{}\) as follows

\[d_{}(s,a)=(1-)_{t=0}^{}^{t}_{}(s_{t }=s,a_{t}=a),\;\; s,\;a,\]

where \(_{}(s_{t},\;a_{t})\) is the probability of state-action pair \((s,a)\) being visited at time slot \(t\) under the policy \(\). Further, we let \(d_{}(s)=_{a A}d_{}(s,a)\) be the discounted state occupancy measure.

Given the occupancy measure above, we can formulate the following convex CMDP problem

\[_{d_{}}\;f(d_{})\;\;\;\;s.t.\;g(d_{ }),\] (1)

where \(f\) and \(g\) are both convex functions, and \(\) is the cost threshold. We present a single safety constraint for simple exposition, and our result can be readily generalized to the setting with multiple constraints. The set \(\) is a probability simplex that satisfies global balance equations of the underlying Markov process

\[=\{d\;|\;_{a}d(s,a)=(1-)\,_{0}(s)+_{s^{},a ^{}}T(s\;|\;s^{},a^{})d(s^{},a^{ }),\; s\}.\]

This set can also be written as a compact matrix form that

\[=\{d\;|\;Md=(1-)_{0}\;\},\;M=(_{| |}^{},,_{||}^{})- P,\] (2)where \(_{||}=[1,1,,1]^{}^{||}\) and \(P^{||||||}\) is the transition probability matrix.

The convex CMDP problem in (1) is quite general to capture various learning scenarios, including apprenticeship learning (Abbeel and Ng, 2004), standard CMDPs (Altman, 2021), pure exploration (Hazan et al., 2019), and inverse reinforcement learning in contextual MDPs (Belogolovsky et al., 2021). Take the standard CMDP as an example, the agent aims to find a policy that maximizes cumulative reward while satisfying the safety constraints (Altman, 2021), which can be written as

\[_{d_{}}~{}~{}_{s,a}r(s,a)d_{}(s,a)~{}~{}~{}s.t.~{} ~{}_{s,a}c(s,a)d_{}(s,a),\] (3)

where \(f\) and \(g\) are linear functions w.r.t. the state-action occupancy measure.

Take safety-aware apprenticeship learning (Zhou and Li, 2018) for another example, the agent aims to mimic the expert's demonstrations while avoiding the unsafe states in \(\), which can be written as

\[_{d_{}}~{}~{}F(d_{},d_{e})~{}~{}~{}s.t.~{} ~{}d_{}(s),~{} s.\] (4)

where \(d_{e}\) represents the empirical distribution of the expert's demonstration and \(F(,)\) is the convex distance function, such as KL-divergence or total variation distance (Zhang et al., 2020).

However, solving these problems in the online setting can be time-consuming, costly, and potentially dangerous in safety-critical contexts. In contrast, by leveraging historical data, offline RL offers a promising avenue for developing safe and effective algorithms, as introduced next.

### Offline Reinforcement Learning

In offline RL, we cannot interact with the environment and only have access to dataset with a finite number of samples. Let \(=(s_{i},a_{i},r_{i},c_{i})_{i=1}^{n}\) be a collected offline dataset where we assume all the pairs \((s_{i},a_{i})_{i=1}^{n}\) are generated independently and identically distributed \((i.i.d.)\) from data distribution \((s,a)\) induced by a behavior policy \(_{}\). Let \(n_{}(s,a)\) represents the number of occurrences of the state-action pair \((s,a)\) in the offline dataset \(\), then \(_{}(s,a)=n_{}(s,a)/n\) is an empirical version of \((s,a)\). In offline RL, a major challenge is distribution shift, which measures the mismatch between data distribution and occupancy measure induced by candidate policies. To quantify the distribution shift, we make the following \(^{*}\) concentrability assumption that refers to **partial** data coverage,

**Assumption 1** (\(^{*}\)-concentrability).: _Let \(^{*}\) be the optimal policy to problem (1), there exists constant \(C_{^{*}}>0\) such that \(d_{^{*}}(s,a)/(s,a) C_{^{*}}\), for all \(s,a\)._

This assumption controls the distribution shift between offline data distribution \(\) and the occupancy measure \(d_{^{*}}\) induced by optimal policy \(^{*}\). Specifically, the **partial** data coverage assumption indicates the offline dataset \(\) should cover state-action pairs visited by the optimal policy \(^{*}\). Unlike the common **full** data coverage assumption in previous works that the dataset \(\) should encompass data visited by all policies (Chen and Jiang, 2019; Hong et al., 2024; Le et al., 2019), our assumption is considerably more relaxed. Furthermore, in the field of safe RL, **partial** coverage assumption also offers significant advantages, as the **full** coverage implies the behavior policy needs to visit every dangerous state and action space, which is obviously impractical.

Beyond using assumptions to limit distribution shift in offline RL, we consider Marginalized Importance Weight (MIW), a method widely used in the existing literature (Hong et al., 2024; Ozdaglar et al., 2023; Zhan et al., 2022), to further address this challenge.

**Definition 1** (Marginalized Importance Weight).: _Given a policy \(,\) let \(d\) be the occupancy measure induced by \(.\) We define marginalized importance weight \(w:^{+}\) as \(w(s,a)=\), \( s,~{}a\)._

Note \(w\) can be regarded as the density ratio between the normalized discounted occupancy measure and data distribution. Moreover, recall the definition of \(w\) and \(M\) in equation (2), we define matrix \(K^{||||||}\) and \(K_{}^{||||||}\) as

\[K(s^{},(s,a))=M(s^{},(s,a))(s,a),~{}~{}K_{}(s^{ },(s,a))=M(s^{},(s,a))_{}(s,a),\]

where \(K_{}\) can be seen as an empirical version of \(K\) in dataset \(\) and it is straightforward to verify \(Kw=Md\). With these notations, we establish an equivalent formulation with problem (1) w.r.t.

MIW

\[_{w 0} f( w)\] (5) s.t. \[Kw=(1-)_{0}\] (6) \[g( w)\] (7)

where the operator "\(\)" denotes the element-wise product of vectors. As we aim to establish the sample-efficient learning algorithms in large state-action spaces, we regard the importance weight \(w\) as a function, i.e. \(w:_{+}\) that belongs to the convex function class \(\) in the following assumption.

**Assumption 2** (Realizability).: _We assume \(w^{*}\) where \(w^{*}\) is the optimal solution to the problem (5)-(7)._

This assumption assumes the optimal solution \(w^{*}(^{*})\) can be realizable for a convex function class \(\). Note that the assumption of the convex function class \(\) is reasonable and standard, which is also achievable with the convexification process in case the function class is non-convex. The convexification process is a common practice in the offline safe RL literature, particularly in the context of general function approximation (Le et al., 2019; Hong et al., 2024). Now we consider \(\) as a discrete function class for simplicity and it readily extends to continuous settings (in Remark 1). Further, we introduce a completeness assumption that is used to relax the key constraint in (6).

**Assumption 3** (Completeness).: _Let \(x_{w}\) be within the function class \(\). Define a mapping \(\): such that \((w)^{}(Kw-(1-)_{0})=x_{w}^{}(Kw-(1-)_{0})=\|Kw- (1-)_{0}\|_{1}\). Then, we have \((,)\)-completeness under the mapping \(\), i.e. \(x_{w}\) for all \(w\)._

Intuitively, the completeness assumption allows us to replace the computation of the \(l_{1}\)-norm with a linear product and simplifies our subsequent analysis. Next, we introduce the standard boundness assumption in offline RL literature with general function approximation (Chen and Jiang, 2019, 2022; Le et al., 2019; Munos and Szepesvari, 2008).

**Assumption 4** (Boundness of \(\) and \(\)).: _We assume function classes \(\) and \(\) are bounded, i.e. \(\|w\|_{} B_{w},\; w\) and \(\|x_{w}\|_{} B_{w},\; x_{w}\)._

Combining this assumption with Assumption 1 implies \(B_{w} C_{^{*}}\). Lastly, we impose a mild assumption for the reward and cost functions in the problem (1).

**Assumption 5** (Lipschitz condition).: _The functions \(f(x)\) and \(g(x)\) are convex and satisfy the Lipschitz condition, where there exist constants \(L_{f}\) and \(L_{g}\) such that for any \(x,y\), the following inequalities hold \(|f(x)-f(y)| L_{f}\|x-y\|\) and \(|g(x)-g(y)| L_{g}\|x-y\|\)._

## 3 Algorithm Design and Main Results

Despite the practical importance of offline safe RL in real-world applications, there is still a lack of theoretical research on this topic. The earlier literature on this setting often lacks robust theoretical analysis (Lee et al., 2021; Liu et al., 2023; Xu et al., 2022), and articles with theoretical analysis either yield unsatisfactory results or rely on strong assumptions (Chen et al., 2022; Hong et al., 2024; Le et al., 2019).

In this section, we propose a provable algorithmic framework and establish the first theoretical result in offline convex CMDP, to the best of our knowledge. Moreover, when reducing to standard offline CMDP, we achieve a sample complexity of \((})\) with general function approximation, which improves the current state-of-the-art result by a factor of \(1/(1-)\).

### Algorithm Design

Inspired by (Ozdaglar et al., 2023), we first introduce an empirical version of problem (5)-(7) in the offline setting by incorporating a suitable relaxed parameter into the safety constraint. Intuitively, the empirical version of the problem is "close" to the original problem when the dataset is large. This is the key observation for analyzing the convergence performance and safety violations. To solve (5)-(7), we present its empirical and relaxed problem:

\[_{w} f(_{} w)\] (8) s.t. \[\|K_{}w-(1-)_{0}\|_{1},\] (9) \[g(_{} w)-,\] (10)

where \(_{}\) and \(K_{}\) are the empirical version of \(\) and \(K\), respectively; \(\) and \(\) are the relaxed hyperparameters for the validity constraint (Bellman equation) and safety constraint. Intuitively, the parameters of \(\) and \(\) capture the "uncertainty" induced by the offline dataset \(\) in terms of distribution shift and safety concerns. The values of these parameters will be specified later and play an important role in our analysis. Next, we demonstrate that the mismatch of the objective and constraint violation bound depends on the "uncertainty", which exhibits \((1/)\) distance.

Note that all parameters in the optimization problem (8)-(10) can be determined from the offline dataset. When the state-action space is not large, this problem can be efficiently solved by convex optimization solvers. However, when the state-action space is large, and the function approximation is necessary (e.g., \(w\) is parameterized by a neural network), it would be quite challenging (if not impossible) to solve this problem. To address this challenge, we propose a primal-dual algorithm that is sample-efficient and computationally tractable to solve the problem iteratively. We first introduce the Lagrange function of problem (8)-(10),

\[(w,,)=f(_{} w) +(\|K_{}w-(1-))_{0}\|_{1}-)+ (g(_{} w)--),\] (11)

where \(\) and \(\) are Lagrange multipliers.

```
1Input: Dataset \(=\{(s_{i},a_{i},r_{i},c_{i})\}_{i=1}^{n}\), the relaxed parameters \(\), \(\), and the step size \(=}\);
2Initialization: Choose any \(w^{1}\) and the Lagrangian multipliers \(^{1}=0\), \(^{1}=0\);
3for\(k=1,2,,K\)do
4 Primal:\(w^{k+1}=_{}[w^{k}-_{w}(w^{k}, ^{k},^{k})],\)
5Dual:\(^{k+1}=[^{k}-_{}(w^{k},^{k}, ^{k})]_{0}^{^{k+1}_{max}},\) \(^{k+1}=[^{k}-_{}(w^{k}, ^{k},^{k})]_{0}^{^{k+1}_{max}},\)
6 where \(_{}\) is the projection onto set \(\) and \([]_{l}^{h}\) is the projection onto interval \([l,h]\).
7 Compute the average \(_{K}=_{i=1}^{K}w^{i}\);
8 Extract the policy \(_{K}\) with formula (12);
9Output: Policy \(_{K}\) ; ```

**Algorithm 1**Primal-dual algorithm for **O**ffline **C**onvex **C**MDP (POCC)

Given the Lagrange function above, we introduce our algorithm called POCC (in Algorithm 1), which takes the offline dataset \(\) as input and runs a primal-dual method on the estimated Lagrange function. Specifically, at each iteration \(k\), POCC updates the importance weight by gradient descent and projects it back to function class \(\), then updates Lagrange multipliers of validity constraint (6) and safety constraint (7) respectively. After \(K\) steps, POCC returns an averaged \(_{K}\), we can extract the corresponding policy \(_{K}\) based on the offline dataset \(\) as follows

\[_{K}(a s):=_{K}(s,a)_{ }(a s)}{_{a^{}}_{K}(s,a^{}) _{}(a^{} s)},&\ \ _{a^{}}_{K}(s,a^{})_{}(a^{}  s)>0\\ |},&\ \ _{a^{}}_{K}(s,a^{ })_{}(a^{} s)=0\] (12)

where the second equality means that if \(_{a^{}}_{K}(s,a^{})_{}(a^{ } s)=0\) we randomly choose an action for state \(s\).

### Theoretical Results

We present the theoretical results of our proposed approach in the following theorem.

**Theorem 1** (Sample complexity of \(_{K}\)).: _Suppose Assumptions 1-7 hold. Denote \(_{K}\) as the corresponding policy induced by \(_{K}\). Set the relaxed parameters \(=B_{w}}{}|||}{}}\) and \(=B_{w}}}{}|}{}}\), and the step size \(=}\). Let the constants \(v=(4B+4L+2)\) and \(=(B^{2}+4B+4L+}{}+)\), we have, with at least \(1-8\) probability,_

\[J_{r}(_{K})-J_{r}(^{*}) B_{w}|||/ )}}{(1-)}+}\] (13) \[J_{c}(_{K})- B_{w}|||/ )}}{(1-)}+}\] (14)

_where \(J_{r}(_{K})=f(_{K})\) and \(J_{c}(_{K})=g(_{K})\) are objective and constraint performance of policy \(\) respectively, \(B\) represents the distance between initial value \(w^{1}\) of the iteration and optimal solution \(w_{}\) to problem (8)-(10), \( 0\) is a constant, \(L\) is the max Lipschitz constant of Lagrange function and \(K\) is the number of iterations._

**Remark 1**.: _We remark that Theorem 1 remains valid even when the function class \(\) is a continuous set. In this case, the cardinality \(||\) can be replaced with the covering number of \(\), and the union bound can be applied to its \(\)-covering set. This adjustment also preserves the sample complexity order of \((1/)\), up to a constant dependent on \(\). For further details on the extension from discrete \(\) to a continuous set, please refer to (Le et al., 2019; Xie and Jiang, 2021)._

Theorem 1 demonstrates that the convergence performance of our algorithm can be divided into two parts: \((1/)+(1/)\). Regarding the first term, except for the size of dataset, it mainly depends on the function class (searching space) \(||||\) and relaxed parameters \(\) and \(\) that capture the "uncertainty" for addressing the distribution shift and safety concern. Moreover, the second term \((1/)\) connects with the error bound between \(_{K}\) and optimal solution \(w_{}\), which decays at the rate of \(1/\) when the number of iterations increases. It is worth noting that the convergence rate of policy \(_{K}\) is still \((1/)\) when the iterative number is sufficient to satisfy \(K n\). The algorithm is gradient-based and does not involve additional computations for solving the optimization problem (8)-(10). This suggests that we can improve the convergence performance w.r.t. \(K\) by employing more advanced primal-dual techniques to reduce the convergence rate of the second term, such as those discussed in (Yu and Neely, 2017) with a convergence rate of \((1/K)\).

Unlike most of the previous work focuses on convex MDPs in online setting (Bai et al., 2023; Yang et al., 2023; Zahavy et al., 2021; Zhang et al., 2020b), Theorem 1 to the best of our knowledge, is the first provable result in offline convex MDPs. Moreover, compared to the best result \((})\) in offline CMDP (Hong et al., 2024; Chen et al., 2022), our results achieve a sample complexity of \((})\), which outperform the state-of-the-art by a factor of \(1/(1-)\). Besides, our algorithm is appropriate in the scenario with large-scale state-action space due to the general function approximation for \(w\) while the work (Chen et al., 2022) targets the tabular setting; our result is more favorable in the safety applications compared to (Hong et al., 2024) because we replace the realizability of value function \(\) with a slightly stronger completeness assumption but reduce the data coverage from **full** to **partial**. As stated in Assumption 1, the **full** data coverage not only implies access to a highly exploratory dataset but also is impractical for offline safe RL, as it assumes behavior policy needs to visit every dangerous state-action pair. Finally, we want to comment that the previous work all focuses on the standard RL, whose objective function is linear, while our algorithm is general enough to tackle the convex MDPs.

**Remark 2**.: _In the theorem, we assume prior knowledge of the behavior policy \(_{}\) for the sake of exposition. However, in practice, it is often challenging to know the behavior policy in advance, as we typically only have access to the offline dataset. The most popular approach to tackle this challenge is behavior clone. It posits that \(_{}\) can be estimated as \(_{}(a|s)=\), where \(n(s,a)\) denotes the number of the occurrences of the state-action pair \((s,a)\) in the offline dataset. We employed this estimation method in the experiments and results demonstrate its effectiveness._

## 4 Theoretical Analysis

In this section, we present a sketch of the proof of Theorem 1. We focus on illustrating the analysis of the constraint violation, and the convergence of the objective follows similar steps. The detailed proof can be found in Appendix A.

We first decompose the bound into different major terms and then study them individually

\[J_{c}(_{K})- =g(d_{_{K}})-\] (15) \[=_{K}})-g(_{K})}_{1}+ _{K})-g(_{}_{ K})}_{}+}_{K})-}_{ }\] (16)

The first equality holds due to the definition: \(J_{c}(_{K})=g(d_{_{K}})\), where \(d_{_{K}}\) represents the occupancy measure induced by returned policy \(_{K}\). Recall the definition \(w(s,a)(s,a)=d(s,a)\), we then have \(g(_{K})=g(_{K})\). It is worth noting that the decomposition is intuitive since terms I and II relates to the idea of constructing "uncertainty" parameter for the offline dataset and term III mainly depends on the convergence of primal-dual method.

Specifically, term I is the distance between \(g(_{K})\) and \(g(d_{_{K}})\). It represents the error that we rectify the unnormalized occupancy measure \(_{K}\), which violates the validity constraint (6), to a satisfying one \(d_{_{K}}\). The term II illustrates the error incurred when applying the calculated \(_{K}\) from offline dataset \(\) to the real environment \(\), which depends on the sample size of the dataset. The term III is related to the relaxed parameter \(\) in safety constraint (7) and the distance between returned \(_{K}\) and optimal solution \(w_{}\). Next, we present the following lemmas to bound these terms.

**Lemma 1**.: _Suppose Assumptions 1-4 hold, we have, with probability at least \(1-2\),_

\[J_{c}(_{K})-g(_{K})B_{w}|||/)}}{(1-)}.\]

**Lemma 2**.: _Suppose Assumptions 1-4 hold. For \(_{K}\), we have, with probability at least \(1-\),_

\[g(_{K})-g(_{}_{K}) L_{g}B_{w}}{}|}{}}.\]

**Lemma 3**.: _Suppose Assumptions 1-7 hold. For \(_{K}\), we have_

\[g(_{D}_{K})-+}.\]

Combining the above lemmas, we have, with at least \(1-3\) probability,

\[J_{c}(_{K})- =_{K}})-g(_{K})}_{1}+ _{K})-g(_{}_ {K})}_{}+}_{K})-}_ {}\] \[B_{w}|||/ )}}{(1-)}+L_{g}B_{w}}{}|}{}}+}\] \[L_{g}B_{w}|||/)}}{(1-)}+}.\]

where we set \(=B_{w}}{}|||}{}}\), \(=L_{g}B_{w}}{}|}{ }}\).

## 5 Experiments

This section aims to justify the effectiveness of our proposed framework through numerical experiments. We test a practical version of Algorithm 1, where we replace the gradient-type update with the "Adam"-type update (the detailed algorithm can be found in Algorithm 2 in the appendix). We test our algorithm to two specific offline convex CMDPs: 1) Safe imitation learning and 2) standard offline CMDP. Our objective is to address the following questions: (i) Are the experimental results consistent with our theory? (ii) how does the data quality affect the performance of our algorithm? The additional details can be found in the Appendix B.

### Safe Imitation Learning

To showcase the generality of our algorithm, we choose imitation learning as a user case of convex MDPs and conduct the experiments in a maze environment. We design the environment, as illustratedin Figure 2, modified from [Geist et al., 2022]. The problem of (safe) imitation learning can be formulated as \(F(d)=KL(d\,||\,d_{E})\), where \(d_{E}\) represents the stationary distribution of an expert. The environment is deterministic; agent has four actions (left, down, right, up); moving towards the wall (white) and the boundary does not change the state; the goal is to learn from the expert demonstrations (yellow) under safety constraints.

We collect data by expert demonstrations in (a) but randomly remove 25% states. We intend for the algorithm to learn to fill in the gaps using its inherent properties and function approximation, as emphasized in our theory. We are presenting two sets of results: one that considers safety constraints with a cost threshold of 0, and another that does not consider safety constraints. It's important to note that we cannot simply take the stationary distribution of the dataset as our final result. This is due to several reasons, including the fact that expert demonstrations are incomplete (25% of states are removed), simple replacement leads to poor results, and it's not suitable when considering the safety of the agents.

Our results are presented in Figure 2. In Figure (2b), it is demonstrated that our policy can accurately replicate the expert distribution using function approximation and completely bridge the gap required by the expert. Additionally, when taking into account the safety constraints, which involve setting the cost in the entire top-right corners, it is evident that our policy avoids states with cost and behaves appropriately, aligning with our theoretical framework.

### Offline CMDP

We consider an 8x8 grid world environment FrozenLake, with the initial state being the top-left grid. The agent has four actions: N (north), S (south), E (east), and W (west). The primary objective is to reach the goal while avoiding all holes. The game terminates if the agent achieves the goal within 25 steps. A reward of 1 is obtained when the agent achieves the goal, and the main cost function assigns a cost of 1 if the agent falls into a hole and 0 otherwise. The diagram of the environment has been presented in Figure 1 in the introduction.

Initially, we simulate a mixture of different percentages of optimal and uniform policies to collect the offline dataset. We employ various behavior policies \(_{}\), running 200 trajectories to collect the offline dataset \(\), with each trajectory having a maximum of 50 time steps to ensure that the optimal goal is included in the dataset.

Additionally, we increase the difficulty of the environment compared to classical FrozenLake, such that if the agent falls into a hole, it can also come out in the next step. This implies that our cost constraint influences the training. We set the cost threshold as 0 here, which means that the agent is not supposed to incur any cost. Note that a higher percentage with a uniform policy indicates that the problem becomes more difficult. We set the discount factor as \(=0.99\), \(=0.1\), and \(\) in our algorithm. Furthermore, we encode the environment with one-hot encoding and employ a more practical algorithm. We refer to \(w\) as a single hidden-layer neural network which we describe in Algorithm 2 in Appendix B. We set the learning rate of \(10^{-5}\) for \(w\) and \(10^{-4}\) for Lagrange multipliers.

In remark (), we have stated that there are two approaches when facing with the scenario that the behavior policy is unknown, and here we choose to use the behavior clone method that we will estimate the behavior policy through the offline dataset. We compare our algorithm with COptiDICE

Figure 2: Reading order: **(a)** target demonstrations in yellow, wall in white; **(b)** result for log-density without considering the safety constraint; **(c)** target demonstrations that all states in top-right corners have cost; **(d)** result for log-density with safety constraint.

[Lee et al., 2021], which is a well-acknowledged baseline algorithm in the offline safe RL literature [Hong et al., 2024, Liu et al., 2023b].

Denote \(p\) as the percentage of optimal policy within the behavior policy. We evaluate the algorithm with different behavior policies \(p=\{0.75,0.5,0.25,0\}\) and dataset sizes. We present the results in Figure 3. In the first two figures, it can be seen that our algorithm can consistently find the optimal path even with completely random data. Conversely, CoptiDICE behaves well when the proportion of optimal policy is 0.75, but cannot even learn a logical and safe policy when the majority of the data in dataset is random and constraint violated. In the last two figures, we test the performance of algorithms in different sizes of dataset, where \(p=0.5\) in behavior policy. The results demonstrate that our algorithm can find a safe and optimal path as the dataset size increases. In contrast, the results from the COptiDICE algorithm show high variance, where it can only find a safe path in about \(50\%\) runs.

In summary, our algorithm performs well across various behavior policies and dataset sizes, which is consistent with our theoretical results and assumptions.

## 6 Conclusions

In this work, we consider convex CMDPs in the offline setting. We propose a sample efficient RL approach that addresses the challenges in offline convex CMDPs. We theoretically prove that we can suffer \((1/)\) sample complexity in both performance and violation bound with general function approximation under mild data coverage assumption, which is the first result in offline convex MDPs as best of our knowledge and surpasses the state-of-the-art result in offline safe RL by a factor of \(1/(1-)\). Experimental studies further demonstrate the effectiveness and generality of our framework.

Figure 3: Performance on FrozenLake with general function approximation. Reading order: (a) and (b) show the training result with four different behavior policies of COptiDICE and ours. (c) and (d) demonstrate the variations in rewards and costs as the dataset increases. Each point is the average result of 10 independent runs.