# Equivariant Machine Learning on Graphs with

Nonlinear Spectral Filters

 Ya-Wei Eileen Lin\({}^{}\)  Ronen Talmon\({}^{}\)  Ron Levie\({}^{}\)

\({}^{}\)Viterbi Faculty of Electrical and Computer Engineering, Technion

\({}^{}\)Faculty of Mathematics, Technion

###### Abstract

Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.

## 1 Introduction

In many fields, such as chemistry , biology , social science , and computer graphics , data can be described by graphs. In recent years, there has been a tremendous interest in the development of machine learning models for graph-structured data . This young field, often called _graph machine learning (graph-ML)_ or _graph representation learning_, has made significant contributions to the applied sciences, e.g., in protein folding, molecular design, and drug discovery , and has impacted the industry with applications in social media, recommendation systems, traffic prediction, computer graphics, and natural language processing, among others.

Geometric Deep Learning (GDL)  is a design philosophy for machine learning models where the model is constructed to inherently respect symmetries present in the data, aiming to reduce model complexity and enhance generalization. By incorporating knowledge of these symmetries into the model, it avoids the need to expend parameters and data to learn them. This inherent respect for symmetries is automatically generalized to test data, thereby improving generalization . For instance, convolutional neural networks (CNNs) respect the translation symmetries of 2D images, with weight sharing due to these symmetries contributing significantly to their success . Respecting node re-indexing in a scalable manner revolutionized machine learning on graphs  and has placed GNNs as a main general-purpose tool for processing graph-structured data. Moreover, within GNNs, respecting the 3D Euclidean symmetries of the laws of physics (rotations, reflections, and translations) led to state-of-the-art performance in molecule processing .

**Our Contribution.** In this paper, we focus on GDL for graph-ML. We consider extensions of shift symmetries from images to general graphs. Since graphs do not have a natural notion of domaintranslation, as opposed to images, we propose considering functional translations instead. We model the group of translations on graphs as the group of all unitary operators on signals that commute with the graph shift operator. Such unitary operators are called _graph functional shifts_. Note that each linear filter layer of a standard spectral GNN commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose _non-linear spectral filters (NLSFs)_ that are fully equivariant to graph functional shifts and have universal approximation properties.

In Sec. 3, we introduce our NLSFs based on new notions of _analysis_ and _synthesis_ that map signals between their node-space representations and spectral representations. Our transforms are related to standard graph Fourier and inverse Fourier transforms but differ from them in one important aspect. One key property of our analysis transform is that it is independent of a specific choice of Laplacian eigenvectors. Hence, our spectral representations are transferable between graphs. In comparison, standard graph Fourier transforms are based on an arbitrary choice of eigenvectors, and therefore, the standard frequency domain is not transferable. To achieve transferability, standard graph Fourier methods resort to linear filter operations based on functional calculus. Since we do not have this limitation, we can operate on the frequency coefficients with arbitrary nonlinear functions such as multilayer perceptrons. In Sec. 4, we present theoretical results of our NLSFs, including the universal approximation and expressivity properties. In Sec. 5, we demonstrate the efficacy of our NLSFs in node and graph classification benchmarks, where our method outperforms existing spectral GNNs.

## 2 Background

**Notation.** For \(N\), we denote \([N]=\{1,,N\}\). We denote matrices by boldface uppercase letter \(\), vectors (assumed to be columns) by lowercase boldface \(\), and the entries of matrices and vectors are denoted with the same letter in lower case, e.g., \(=(b_{i,j})_{i,j[N]}\). Let \(G=([N],,,)\) be an undirected graph with a node set \([N]\), an edge set \([N][N]\), an adjacency matrix \(^{N N}\) representing the edge weights, and a node feature matrix (also called a signal) \(^{N d}\) containing \(d\)-dimensional node attributes. Let \(\) be the diagonal degree matrix of \(G\), where the diagonal element \(d_{i,i}\) is the degree of node \(i\). Denote by \(\) any normal graph shift operator (GSO). For example, \(\) could be the combinatorial graph Laplacian or the normalized graph Laplacian given by \(=-\) and \(=^{-}^{-}\), respectively. Let \(=^{}\) be the eigendecomposition of \(\), where \(\) is the eigenvector matrix and \(=(_{1},,_{N})\) is the diagonal matrix with eigenvalues \((_{i})_{i=1}^{N}\) ordered by \(|_{1}||_{N}|\). An eigenspace is the span of all eigenvectors corresponding to the same eigenvalue. Let \(_{i}=_{,i}\) denote the projection upon the \(i\)-th eigenspace of \(\) in increasing order of \(||\). We denote the Euclidean norm by \(\|\|_{2}\). We define the _channel-wise signal norm_\(\|\|_{}\) of a feature matrix \(^{N d}\) as the vector \(\|\|_{}=(\|_{:,j}\|_{2} )_{j=1}^{d}^{d}\), where \(_{:,j}\) is the \(j\)-th column on \(\) and \(0 a 1\). We abbreviate multilayer perceptrons by MLP.

### Linear Graph Signal Processing

Spectral GNNs define convolution operators on graphs via the spectral domain. Given a self-adjoint _graph shift operator (GSO)_\(\), e.g., a graph Laplacian, the Fourier modes of the graph are defined to be the eigenvectors \(\{_{i}\}_{i=1}^{N}\) of \(\) and the eigenvalues \(\{_{i}\}_{i=1}^{N}\) are the frequencies. A spectral filter is defined to directly satisfy the "convolution theorem"  for graphs. Namely, given a signal \(^{N d}\) and a function \(:^{d^{} d}\), the operator \(():^{N d}^{N  d^{}}\) defined by

\[():=_{i=1}^{N}_{i}_{ i}^{}(_{i})^{},\] (1)

is called a filter. Here, \(d^{}\) is the number of output channels. Spectral GNNs, e.g., [21; 49; 61; 5], are graph convolutional networks where convolutions are via Eq. (1), with a trainable function \(\) at each layer, and a nonlinear activation function.

### Equivariant GNNs

Equivariance describes the ability of functions \(f\) to respect symmetries. It is expressed as \(f(H_{}x)=H_{}f(x)\), where \( H_{}\) is an action of a symmetry group \(\) on the domain of \(f\). GNNs [83; 14], including spectral GNNs [22; 49] and subgraph GNNs [31; 4], are inherently permutation equivariantw.r.t. the ordering of nodes. This means that the network's operations are unaffected by the specific arrangement of nodes, a property stemming from passive symmetries  where transformations are applied to both the graph signals and the graph domain. This permutation equivariance is often compared to the translation equivariance in CNNs [56; 57], which involves active symmetries . The key difference between the two symmetries lies in the domain: while CNNs operate on a fixed domain with signals transforming within it, graphs lack a natural notion of domain translation. To address this, we consider graph functional shifts as the symmetry group, defined by unitary operators that commute with the graph shift operator. This perspective allows for our NLSFs to be interpreted as an extension of active symmetry within the graph context, bridging the gap between the passive and active symmetries inherent to GNNs and CNNs, respectively.

## 3 Nonlinear Spectral Graph Filters

In this section, we present new concepts of analysis and synthesis under which the spectral domain is transferable between graphs. Following these concepts, we introduce new GNNs that are equivariant to _functional symmetries_ - symmetries of the Hilbert space of signals rather than symmetries in the domain of definition of the signal .

### Translation Equivariance of CNNs and GNNs

For motivation, we start with the grid graph \(R\) with node set \([M]^{2}\) and circular adjacency \(\), we define the translation operator \(_{m,n}\) by \([m,n]\) as

\[_{m,n}^{M^{2} M^{2}};(_{m,n} )_{i,j}=_{l,k} l=(i-m) M\;\; \;\;k=(j-n) M\;.\]

Note that any \(_{m,n}\) is a unitary operator that commutes with the grid Laplacian \(_{R}\), i.e., \(_{m,n}_{R}=_{R}_{m,n}\), and therefore it belongs to the group of all unitary operators \(_{R}\) that commute with the grid Laplacian \(_{R}\). In fact, the space of isotropic convolution operators (with 90\({}^{o}\) rotation and reflection symmetric filters) can be seen as the space of all normal operators1 that commute with unitary operators from \(_{R}\). Applying a non-linearity after the convolution retains this equivariance, and hence, we can build multi-layer CNNs that commute with \(_{R}\). By the universal approximation theorem [19; 34; 58], this allows us to approximate any continuous function that commutes with \(_{R}\).

Note that such translation equivariance cannot be extended to general graphs. Achieving equivariance to graph functional shifts through linear spectral convolutional layers \(()\) is straightforward, since these layers commute with the space of all unitary operators \(_{}\) that commute with \(\). However, introducing non-linearity \((())\) breaks the symmetry. That is, there exists \(_{}\) such that

\[(())(()),\]

where \(\) is any non-linear activation function, e.g., ReLU, Sigmoid, etc.

This means that multi-layer spectral GNNs do not commute with \(_{G}\), and are hence not appropriate as approximators of general continuous functions that commute with \(_{G}\) (see App. A for an example illustrating how non-linear activation functions break the functional symmetry). Instead, we propose in this paper a multi-layer GNN that is fully equivariant to \(_{G}\), which we show to be universal: it can approximate any continuous graph-signal function (w.r.t. some metric) commuting with \(_{G}\).

### Graph Functional Symmetries and Their Relaxations

We define the symmetry group of graph functional shifts as follows.

**Definition 1** (Graph Functional Shifts).: _The space of graph functional shifts is the unitary subgroup \(_{}\), where a unitary matrix \(\) is in \(_{}\) iff it commutes with the GSO \(\), namely, \(=\)._

It is important to note that functional shifts, in general, are not induced from node permutations. Instead, functional shifts are related to the notion of functional maps  used in shape correspondence and are general unitary operators that are not permutation matrices in general. The value of the functionally translated signal at a given node can be a _mixture_ of the content of the original signal at many different nodes. For example, the functional shift can be a combination of shifts of different frequencies at different speeds. See App. B for illustrations and examples of functional translations.

A fundamental challenge with the symmetry group in Def. 1 is its lack of transferability between different graphs. Hence, we propose to relax this symmetry group. Let \(g_{1},,g_{S}:\) be the indicator functions of the intervals \(\{[l_{s},l_{s+1}]\}_{s=1}^{S}\), which constitute a partition of the frequency band \([l_{1},l_{S}]\). The operators \(g_{j}()\), interpreted via functional calculus Eq. (1), are projections of the signal space upon band-limited signals. Namely, \(g_{j}()=_{i:_{i}[l_{j},l_{j+1}]}_{i}_{i}^{}\). In our work, we consider filters \(g_{j}\) that are supported on the dyadic sub-bands \([_{N}r^{S-j+1},_{N}r^{S-j}]\), where \(0<r<1\) is the decay rate. See Fig. 5 in App. F for an illustrated example. Note that for \(j=1\), the sub-band falls in \([0,_{N}r^{S-1}]\). The total band \([0,l_{S}]\) is \([0,_{N}]\).

**Definition 2** (Relaxed Functional Shifts).: _The space of relaxed functional shifts with respect to the filter bank \(\{g_{j}\}_{j=1}^{K}\) (of indicators) is the unitary subgroup \(_{}^{g}\), where a unitary matrix \(\) is in \(_{}^{g}\) iff it commutes with \(g_{j}()\) for all \(j\), namely, \(g_{j}()=g_{j}()\)._

Similarly, we can relax functional shifts by restricting to the leading eigenspaces.

**Definition 3** (Leading Functional Shifts).: _The space of leading functional shifts is the unitary subgroup \(_{}^{J}\), where a unitary \(\) is in \(_{}^{J}\) iff it commutes with the eigenspace projections \(\{_{j}\}_{j=1}^{J}\)._

### Analysis and Synthesis

We use the terminology of analysis and synthesis, as in signal processing , to describe transformations of signals between their graph and spectral representations. Here, we consider two settings: the eigenspace projections case and the filter bank (of indicators) case. The definition of the frequency domain depends on a given signal \(^{N d}\), where its projections to the eigenspaces of \(\) are taken as the Fourier modes. Spectral coefficients are modeled as matrices \(\) that mix the Fourier modes, allowing to synthesize signals of general dimensions.

**Spectral Index Case.** We first define _analysis and synthesis using the spectral index_ up to frequency \(J\). Let \(_{J+1}=-_{j=1}^{J}_{j}\) be the orthogonal complement to the first \(J\) eigenprojections. Let \(^{N d}\) be the graph signal and \(^{(J+1)d(J+1)}\) the spectral coefficients to be synthesized, where \(\) represents number of output channels. The analysis and synthesis are defined respectively by

\[^{}(,)=(\|_{i} \|_{})_{i=1}^{J+1}^{(J+1)d} { and }^{}(;,)=[_{j}}{\|_{j}\|_{ }^{a}+e}]_{j=1}^{J+1},\] (2)

where \(0 a 1\) and \(0<e 1\) are parameters that promote stability, and the power \(\|_{j}\|_{}^{a}\) as well as the division in Eq. (2) are element-wise operations on each entry. Here, \([_{j}/(\|_{j}\|_{}^{a}+e)]_{j=1}^{J+1}^{N(J+1)d}\) denotes concatenation. We remark that the orthogonal complement in the \((J+1)\)-th filter alleviates the loss of information due to projecting to the low-frequency bands, and therefore, the full spectral range of the signal can be captured. This is particularly important for heterophilic graphs, which rely on high-frequency components for accurate label representation. The term _index_ stems from the fact that eigenvalues are treated according to their index when defining the projections \(_{j}\). Note that the synthesis here differs from classic signal processing as it depends on a given signal on the graph. When treating \(\) and \(\) as fixed, this synthesis operation is denoted by \(^{}_{,}():=^{ }(;,)\). We similarly denote \(^{}_{}():=^{}( ,)\).

**Filter Bank Case.** Similarly, we define the _analysis and synthesis in the filter bank_ up to band \(g_{K}\) as follows. Let \(g_{K+1}()=-_{j=1}^{K}g_{j}()\) denote the orthogonal complement to the first \(K\) bands. Let \(^{N d}\) be the graph signal, and let \(^{(K+1)d(K+1)}\) represent the spectral coefficients to be synthesized, where \(\) refers to the general dimension. The analysis and synthesis in the filter bank case are defined by

\[^{}(,)=(\|g_{i}( )\|_{})_{i=1}^{K+1}^{(K+1)d} ^{}(;,)=[( )}{\|g_{j}()\|_{ }^{a}+e}]_{j=1}^{K+1},\] (3)

respectively, where \(a,e\) are as before. Here, \([g_{j}()/(\|g_{j}()\|_{ }^{a}+e)]_{j=1}^{K+1}^{N(K+1)d}\). The term _value_ refers to how eigenvalues are used based on their magnitude when defining the projections \(g_{j}()\). As before, we denote \(^{}_{,}\) and \(^{}_{}\).

In App. C.1, we present a special case of diagonal synthesis where \(=d\). In App. D.3, we show that the diagonal synthesis is stably invertible.

### Definitions of Nonlinear Spectral Filters

We introduce three novel types of _non-linear spectral filters (NLSF)_: Node-level NLSFs, Graph-level NLSFs, and Pooling-NLSFs. Fig. 1 illustrates our NLSFs for equivariant machine learning on graphs.

**Node-level NLSFs.** To be able to transfer NLSFs between different graphs and signals, one key property of NLSF is that they do not depend on the specific basis chosen in each eigenspace. This independence is facilitated by the synthesis process, which relies on the input signal \(\). Following the spectral index and filter bank cases in Sec. 3.3, we define the Index NLSFs and Value NLSFs by

\[_{}(,)=_{ ,}^{()}(_{}(_{ }^{()}()))=[_{j} }{\|_{j}\|_{}^{a}+e} ]_{j=1}^{J+1}[_{}(\|_{i}\|_{})]_{i=1}^{J+1},\] (4) \[_{}(,)=_{ ,}^{()}(_{}(_ {}^{()}()))=[( )}{\|g_{j}() \|_{}^{a}+e}]_{j=1}^{K+1}[_{}( \|g_{i}()\|_{})]_ {i=1}^{K+1},\] (5)

where \(_{}:^{(J+1)d}^{(J+1)d(J+1) }\) and \(_{}:^{(K+1)d}^{(K+1)d(K+1 )}\) are called _nonlinear frequency responses_, and \(\) is the output dimension. To adjust the feature output dimension, we apply an MLP with shared weights to all nodes after the NLSF. In the case when \(=d\) and the filters operator diagonally (i.e., the product and division are element-wise in synthesis), we refer to it as diag-NLSF. See App. C for more details.

**Graph-level NLSFs.** We first introduce the Graph-level NLSFs that are fully spectral, where the NLSFs map a sequence of frequency coefficients to an output vector. Specifically, the Index-based and Value-based Graph-level NLSFs are given by

\[_{}(,)\!=\!_{}(\|_{i}\|_{})_{}(,)\!=\!_{}(\|g_{i}()\|_{} ),\] (6)

where \(_{}:^{(J+1)d}^{d^{ }}\), \(_{}:^{(K+1)d}^{d^{ }}\), and \(d^{}\) is the output dimension.

**Pooling-NLSFs.** We introduce another type of graph-level NLSFs by first representing each graph in a Node-level NLSFs as in Eq. (4) and Eq. (5). The final graph representation is obtained by applying a nonlinear activation function followed by a readout function to these node-level representations. We consider four commonly used pooling methods, including mean, sum, max, and \(L_{p}\)-norm pooling, as the readout function for each graph. We apply an MLP after readout function to obtain a \(d^{}\)-dimensional graph-level representation. We term these graph-level NLSFs as _Pooling-NLSFs_.

Figure 1: Illustration of nonlinear spectral filters for equivariant machine learning on graphs. Given a graph \(G\), the node features \(\) are projected onto eigenspaces (analysis \(\)). The function \(\) map a sequence of frequency coefficients to a sequence of frequency coefficients. The coefficients are synthesized to the graph domain using the using \(\).

### Laplacian Attention NLSFs

To understand which Laplacian and parameterization (index v/s value) of the NLSF are preferable in different settings, we follow the random geometric graph analysis outlined in . Specifically, we consider a setting where random geometric graphs are sampled from a metric-probability space \(\). In such a case, the graph Laplacian approximates continuous Laplacians on the metric spaces under some conditions. We aim for our NLSFs to produce approximately the same outcome for any two graphs sampled from the same underlying metric space \(\), ensuring that the NLSF is _transferable_. In App. D.5, we show that if the nodes of the graph are sampled uniformly from \(\), then using the graph Laplacian \(\) in Index NLSFs yields a transferable method. Conversely, if the nodes of the graph are sampled non-uniformly, and any two balls of the same radius in \(\) have the same volume, then utilizing the normalized graph Laplacian \(\) in Value NLSFs is a transferable method. Given that graphs may fall between these two boundary cases, we present an architecture that chooses between the Index NLSFs with respect to \(\) and Value NLSFs with respect to \(\), as illustrated in Fig. 2. While the above theoretical setting may not be appropriate as a model for every real-life graph dataset, it suggests that index NLSF may be more appropriate with \(\), value NLSFs with \(\), and different graphs are more appropriately analyzed by different balances between these two cases.

In the Laplacian attention architecture, a soft attention mechanism is employed to dynamically choose between the two parameterizations, given by

\[(_{}(,),_{}(,))=_{}(, )\|(1-)_{}(,),\]

where \(0 1\) is obtained using a softmax function to normalize the scores into attention weights, balancing each NLSFs' contribution.

## 4 Theoretical Properties of Nonlinear Spectral Filters

We present the desired theoretical properties of our NLSFs at the node-level and graph-level.

### Complexity of NLSFs

NLSFs are implemented by computing the eigenvectors of the GSO. Most existing spectral GNNs avoid direct eigendecomposition due to its perceived inefficiency. Instead, they use filters implemented by applying polynomials [49; 22] or rational functions [61; 5] to the GSO in the spatial domain. However, power iteration-based eigendecomposition algorithms, e.g., variants of the Lanczos method, can be highly efficient [80; 55]. For matrices with \(E\) non-zero entries, the computational complexity of one iteration for finding \(J\) eigenvectors corresponding to the smallest or largest eigenvalues (called _leading eigenvectors_) is \(O(JE)\). In practice, these eigendecomposition algorithms converge quickly due to their super-exponential convergence rate, often requiring only a few iterations, which makes them as efficient as message passing networks of signals with \(\) channels.

This makes NLSFs applicable to node-level tasks on large sparse graphs, as demonstrated empirically in App. F.5, since they rely solely on the leading eigenvectors. In Sec. 4.4, we show that using the leading eigenvectors can approximate GSOs well in the context of learning on graphs. Note that we can precompute the spectral projections of the signal before training. For node-level tasks, such as semi-supervised node classification, the leading eigenvectors only need to be pre-computed once, with a complexity of \(O(JE)\). This During the _learning phase_, each step of the architecture search and hyperparameter optimization takes \(O(NJd)\) complexity for analysis and synthesis, and \(O(J^{2}d^{2})\)

Figure 2: Illustration of Laplacian attention NLSFs. An attention mechanism is applied to both Index NLSFs and Value NLSFs, enabling the adaptive selection of the most appropriate parameterization.

for the MLP in the spectral domain, which is faster than the complexity \(O(Ed^{2})\) of message passing or standard spectral methods if \(NJ<Ed\). Empirical studies on runtime analysis are in App. F.

For dense matrices, the computational complexity of a full eigendecomposition is \(O(N^{b})\) per iteration, where \(N^{b}\) is the complexity of matrix multiplication. This is practical for graph-level tasks on relatively small and dense graphs, which is typical for many graph classification datasets. In these cases, the eigendecomposition of all graphs in the dataset can be performed as a pre-computation step, significantly reducing the complexity during the learning phase.

### Equivariance of Node-level NLSFs

We demonstrate the node-level equivariance of our NLSFs, ensuring that our method respects the functional shift symmetries. The proof is given in App. D.1.

**Proposition 1**.: _Index NLSFs in Eq. (4) are equivariant to the graph functional shifts \(_{}\), and Value NLSFs in Eq. (5) are equivariant to the relaxed graph functional shifts \(^{q}_{}\)._

### Universal Approximation and Expressivity

In this subsection, we discuss the approximation power of NLSFs.

**Node-Level Universal Approximation.** We begin with a setting where a graph is given as a fixed domain, and the data distribution consists of multiple signals defined on this graph. An example of this setup is a spatiotemporal graph , e.g., traffic networks, where a fixed sensor system defines a graph and the different signals represent the sensor readings collected at different times.

In App. D.2.1, we prove the following lemma, which shows that linear NLSFs exhaust the space of linear operators that commute with graph functional shifts.

**Lemma 1**.: _A linear operator \(^{N d}^{N d}\) commutes with \(^{J}_{}\) (resp. \(^{q}_{}\)) iff it is a NLSF based on a linear function \(\) in Eq. (4) (resp. Eq. (5))._

Lemma 1 shows a close relationship between functions that commute with functional shifts and those defined in the spectral domain. This motivates the following construction of a pseudo-metric on \(^{N d}\). In the case of relaxed functional shifts, we define the standard Euclidean metric \(_{}\) in the spectral domain \(^{(K+1) d}\). We pull back the Euclidean metric to the spatial domain to define a signal pseudo-metric. Namely, for two signals \(\) and \(^{}\), their distance is defined by

\[_{},^{}:= _{}(,), (^{},^{}).\]

This pseudo metric can be made into a metric by considering each equivalence class of signals with zero distance as a single point in the space. As MLPs \(\) can approximate any continuous function \(^{(K+1) d}^{(K+1) d}\) (the universal approximation theorem [19; 34; 58]), node-level NLSFs can approximate any continuous function that maps (equivalence classes of) signals to (equivalence classes of) signals. For details, see App. D.2.2. A similar analysis applies to hard functional shifts.

**Graph-Level Universal Approximation.** The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For graphs with \(d\)-channel signals, we consider again the standard Euclidean metric \(_{}\) in the spectral domain \(^{(K+1) d}\). We define the distance between any two graphs with GSOs and signals \((,)\) and \((^{},^{})\) to be

\[(,),(^{},^ {}):=_{}(, ),(^{},^{}).\]

This definition can be extended into a metric by considering the space \(\) of equivalence classes of graph-signals with distance 0. As before, this distance inherits the universal approximation properties of standard MLPs. Namely, any continuous function \(^{d^{}}\) with respect to \(\) can be approximated by NLSFs based on MLPs. Additional details are in App. D.2.3.

**Graph-Level Expressivity of Pooling-NLSFs.** In App. D.2.4, we show that Pooling-NLSFs are more expressive than graph-level NLSF when any \(L_{p}\) norm is used in Eq. (4) and Eq. (5) with \(p 2\), both in the definition of the NLSF and as the pooling method. Specifically, for every graph-level NLSF, there is a Pooling-NLSF that coincides with it. Additionally, there are graph signals \((,)\) and \((^{},^{})\) for which a Pooling-NLSF can attain different values, whereas any graph-level NLSF must attain the same value. Hence, Pooling-NLSFs have improved discriminative power compared to graph-level NLSFs. Indeed, as shown in Tab. 3, Pooling-NLSFs outperform Graph-level NLSFs in practice, which can be attributed to their increased expressivity. We refer to App. D.2.5 for additional discussion on graph-level expressivity.

### Uniform Approximation of GSOs by Their Leading Eigenvectors

Since NLSFs on large graphs are based on the leading eigenvectors of \(\), we justify its low-rank approximation in the following. While approximating matrices with low-rank matrices might lead to a high error in the spectral and Frobenius norms, we show that such an approximation entails a uniformly small error in the cut norm. We define and interpret the cut norm in App. D.4.1, and explain why it is a natural graph similarity measure for graph machine learning.

The following theorem is a corollary of the Constructive Weak Regularity Lemma presented in . Its proof is presented in App. D.4.

**Theorem 1**.: _Let \(\) be a symmetric matrix with entries bounded by \(|m_{i,j}|\), and let \(J\). Suppose \(m\) is sampled uniformly from \([J]\), and let \(R 1\) s.t. \(J/R\). Let \(_{1},,_{m}\) be the leading eigenvectors of \(\), with eigenvalues \(_{1},,_{m}\) ordered by their magnitudes \(|_{1}||_{m}|\). Define \(=_{k=1}^{m}_{k}_{k}_{k}^{}\). Then, with probability \(1-\) (w.r.t. the choice of \(m\)),_

\[\|-\|_{}<}.\]

Note that the bound in Thm. 1 is uniformly small, independently of \(\) and its dimension \(N\). This theorem justifies using the leading eigenvectors when working with the adjacency matrix as the GSO. For a justification when working with other GSOs see App. D.4.

## 5 Experimental Results

We evaluate the NLSFs on node and graph classification tasks. Additional implementation details are in App. E, and additional experiments, including runtime analysis and ablation studies, are in App. F.

### Semi-Supervised Node Classification

We first demonstrate the main advantage of the proposed Node-level NLSFs over existing GNNs with convolution design on semi-supervised node classification tasks. We test three citation networks [84; 100]: Cora, Citeseer, and Pubmed. In addition, we explore three heterophilic graphs: Chameleon, Squirrel, and Actor [79; 90]. For more comprehensive descriptions of these datasets, see App. E.

We compare the Node-level NLSFs using Laplacian attention with existing spectral GNNs for node-level predictions, including GCN , ChebNet , ChebNetII , CayleyNet , APPNP , GPRGNN , ARMA , JacobiConv , BernNet , Specformer , and OptBasisGNN . We also consider GAT  and SAGE . For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we apply the standard splits following , using 20 nodes per class for training,

    & Cora & Citeseer & Pubmed & Chameleon & Squirrel & Actor \\  GCN & 81.92\(\)0.9 & 70.73\(\)1.1 & 80.14\(\)0.6 & 43.64\(\)1.9 & 33.26\(\)0.8 & 27.63\(\)1.7 \\ GAT & 83.64\(\)0.7 & 71.32\(\)1.3 & 79.45\(\)0.9 & 42.19\(\)1.3 & 28.21\(\)0.9 & 29.46\(\)0.9 \\ SAGE & 74.01\(\)1.6 & 66.04\(\)1.2 & 79.91\(\)0.9 & 41.92\(\)0.7 & 27.64\(\)1.2 & 30.85\(\)1.8 \\ ChebNet & 79.72\(\)1.1 & 70.48\(\)1.0 & 76.74\(\)1.5 & 44.95\(\)1.2 & 33.82\(\)0.8 & 27.42\(\)2.3 \\ ChebNetII & 83.95\(\)0.8 & 71.76\(\)1.2 & 81.38\(\)1.6 & 46.73\(\)1.3 & 44.01\(\)1.1 & 33.48\(\)1.2 \\ CayleyNet & 81.76\(\)1.9 & 68.32\(\)2.3 & 77.48\(\)2.3 & 38.29\(\)1.2 & 26.53\(\)3.3 & 30.62\(\)2.8 \\ APPNP & 83.19\(\)0.8 & 71.93\(\)0.8 & **82.69\(\)1.4** & 37.43\(\)1.9 & 25.68\(\)1.3 & **35.98\(\)**1.3 \\ GPRGNN & 82.82\(\)1.3 & 70.28\(\)1.4 & 81.31\(\)2.6 & 39.27\(\)2.3 & 26.09\(\)1.3 & 31.47\(\)1.6 \\ ARMA & 81.64\(\)1.2 & 69.91\(\)1.6 & 79.24\(\)0.5 & 39.40\(\)1.8 & 27.24\(\)0.7 & 30.42\(\)2.6 \\ JacobiConv & 84.12\(\)0.7 & 72.59\(\)1.4 & 82.05\(\)1.9 & 49.66\(\)1.9 & 33.65\(\)0.8 & 34.61\(\)0.7 \\ BernNet & 82.96\(\)1.1 & 71.25\(\)1.0 & 81.07\(\)1.6 & 42.65\(\)3.4 & 31.68\(\)1.5 & 33.92\(\)0.8 \\ Specformer & 82.27\(\)0.7 & 73.45\(\)1.4 & 81.62\(\)1.0 & 49.79\(\)1.2 & 38.24\(\)0.9 & 34.12\(\)0.6 \\ OptBasisGNN & 81.97\(\)1.2 & 70.46\(\)1.6 & 80.38\(\)0.9 & 47.12\(\)2.4 & 37.66\(\)1.1 & 34.84\(\)1.3 \\  att-Node-level NLSFs & **85.37\(\)**1.8 & **75.41\(\)**0.8 & 82.22\(\)1.2 & **50.58\(\)**1.3 & **38.39\(\)**0.9 & 35.13\(\)1.0 \\   

Table 1: Semi-supervised node classification accuracy.

500 nodes for validation, and 1000 nodes for testing. For heterophilic graphs (Chameleon, Squirrel, and Actor), we use the sparse splitting as in [16; 41], allocating 2.5% of samples for training, 2.5% for validation, and 95% for testing. We measure the classification quality by computing the average classification accuracy with a 95% confidence interval over 10 random splits. We utilize the source code released by the authors for the baseline algorithms and optimize their hyperparameters using Optuna . Each model's hyperparameters are fine-tuned to achieve the highest possible accuracy. Detailed hyperparameter settings are provided in App. E.

Tab. 1 presents the node classification accuracy of our NLSFs using Laplacian attention and the various competing baselines. We see that att-Node-level NLSFs outperform the competing models on the Cora, Citeseer, and Chameleon datasets. Notably, it shows remarkable performance on the densely connected Squirrel graph, outperforming the baselines by a large margin. This can be explained by the sparse version in Eq. (21) of Thm. 1, which shows that the denser the graphs is, the better its rank-\(J\) approximation. For the Pubmed and Actor datasets, att-Node-level NLSFs yield the second-best results, which are comparable to the best results obtained by APPNP.

### Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting

Recently, the work in  identified the presence of many duplicate nodes across the train, validation, and test splits in the dense split setting of the Chameleon and Squirrel . This results in train-test data leakage, causing GNNs to inadvertently fit the test splits during training, thereby making performance results on Chameleon and Squirrel less reliable. To further validate the performance of our Node-level NLSFs on these datasets in the dense split setting, we use both the original and filtered versions of Chameleon and Squirrel, which do not contain duplicate nodes, as suggested in . We use the same random splits as in , dividing the datasets into 48% for training, 32% for validation, and 20% for testing.

Tab. 2 presents the classification performance comparison between the original and filtered Chameleon and Squirrel. The baseline results are taken from , and we include the following competitive models: ResNet+SGC , ResNet+adj , GCN , GPRGNN , FSGNN , GloGNN , and FAGCN . The detailed comparisons are in App. F. We see in the table that the att-Node-level NLSFs consistently outperform the competing baselines on both the original and filtered datasets. att-Node-level NLSFs demonstrate less sensitivity to node duplicates and exhibit stronger generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the dense split setting. We note that compared to the dense split setting, the sparse split setting in Tab. 1 is more challenging, resulting in lower classification performance. A similar trend of significant performance difference between the two settings of Chameleon and Squirrel is also observed in .

### Graph Classification

We further illustrate the power of NLSFs on eight graph classification benchmarks . Specifically, we consider five bioinformatics datasets [10; 53; 86]: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, where MUTAG, PTC, and NCI1 are characterized by discrete node features, while ENZYMES and PROTEINS have continuous node features. Additionally, we examine three social network datasets: IMDB-B, IMDB-M, and COLLAB. The unattributed graphs are augmented by adding node degree features following . For more details of these datasets, see App. E.

In graph classification tasks, a readout function is used to summarize node representations for each graph. The final graph-level representation is obtained by aggregating these node-level summaries and is then fed into an MLP with a (log)softmax layer to perform the graph classification task. We compare our NLSFs with two kernel-based approaches: GK  and WL , as well as nine GNNs: GCN , GAT , SAGE , ChebNet , ChebNetII , CayleyNet , APPNP , GPRGNN , and ARMA . Additionally, we consider the hierarchical graph pooling model

    &  &  \\   & Original & Filtered & Original & Filtered \\  ResNet+SGC & 49.93\(\)2.3 & 41.01\(\)4.5 & 34.36\(\)1.2 & 38.36\(\)1.20 \\ ResNet+adj & 71.07\(\)2.2 & 38.67\(\)3.9 & 65.46\(\)1.6 & 38.37\(\)2.0 \\ GCN & 50.18\(\)3.3 & 40.89\(\)4.1 & 39.06\(\)1.5 & 39.47\(\)1.5 \\ GPRGNN & 47.26\(\)1.7 & 39.93\(\)3.3 & 33.39\(\)2.1 & 38.95\(\)2.0 \\ FSGNN & 77.85\(\)0.5 & 46.1\(\)3.0 & 68.93\(\)1.5 & 35.92\(\)1.3 \\ GloGNN & 70.04\(\)2.1 & 25.90\(\)3.6 & 61.21\(\)2.0 & 35.11\(\)1.2 \\ FAGCN & 64.23\(\)2.0 & 41.90\(\)2.7 & 47.63\(\)1.9 & 41.08\(\)2.3 \\  att-Node-level NLSFs & **79.84\(\)**1.2 & **42.53\(\)**1.5 & 68.17\(\)1.9 & **42.66\(\)**1.7 \\   

Table 2: Node classification performance on original and filtered Chameleon and Squirrel datasets.

DiffPool . For dataset splitting, we apply the random split following [94; 101; 67; 104], using 80% for training, 10% for validation, and 10% for testing. This random splitting process is repeated 10 times, and we report the average performance along with the standard deviation. For the baseline algorithms, we use the source code released by the authors and optimize their hyperparameters using Optuna . We fine-tune the hyperparameters of each model to achieve the highest possible accuracy. Detailed hyperparameter settings for both the baselines and our method are provided in App. E.

Tab. 3 presents the graph classification accuracy. Notably, the att-Graph-level NLSFs (i.e., NLSFs without the synthesis and pooling process) perform the second best on the ENZYMES and PROTEINS. Additionally, att-Pooling-NLSFs consistently outperform att-Graph-level NLSFs, indicating that the node features learned in our Node-level NLSFs representation are more expressive, corroborating our theoretical findings in Sec. 4.3. Furthermore, our att-Pooling-NLSFs consistently outperform all baselines on the MUTAG, PTC, ENZYMES, PROTEINS, IMDB-M, and COLLAB datasets. For NCI1 and IMDB-B, att-Pooling-NLSFs rank second and are comparable to ChebNetII.

## 6 Summary

We presented an approach for defining non-linear filters in the spectral domain of graphs in a transferable and equivariant way. Transferability between different graphs is achieved by using the input signal as a basis for the synthesis operator, making the NLSF depend only on the eigenspaces of the GSO and not on an arbitrary choice of the eigenvectors. While different graph-signals may be of different sizes, the spectral domain is a fixed Euclidean space independent of the size and topology of the graph. Hence, our spectral approach represents graphs as vectors. We note that standard spectral methods do not have this property since the coefficients of the signal in the frequency domain depend on an arbitrary choice of eigenvectors, while our representation depends only on the eigenspaces. We analyzed the universal approximation and expressivity power of NLSFs through metrics that are pulled back from this Euclidean vector space. From a geometric point of view, our NLSFs are motivated by respecting graph functional shift symmetries, making them related to Euclidean CNNs.

**Limitation and Future Work.** One limitation of NLSFs is that, when deployed on large graphs, they only depend on the leading eigenvectors of the GSO and their orthogonal complement. However, important information can also lie within any other band. In future work, we plan to explore NLSFs that are sensitive to eigenvalues that lie within a set of bands of interest, which can be adaptive to the graph.