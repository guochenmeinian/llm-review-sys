# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

2. \(_{ tr}\) is wider and covers \(_{ te}\), i.e., \(_{ tr}_{ te}\) and \(_{ tr}_{ te}\);
3. \(_{ te}\) is wider and covers \(_{ tr}\), i.e., \(_{ tr}_{ te}\) and \(_{ te}_{ tr}\);
4. \(_{ tr}\) and \(_{ te}\) partially overlap, i.e., \(_{ tr}_{ te}\), \(_{ tr}_{ te}\), and \(_{ te}_{ tr}\).1 
The four cases are illustrated in Figure 1. We focus on cases (iii) and (iv), as they are more general and more difficult than cases (i) and (ii).

Problem settingDenote by \(\) and \(\) the input and output domains, where \(=\{1,,C\}\) for \(C\)-class classification problems. Let \(:^{C}\) be a classifier (to be trained) and \(:^{C}(0,+)\) be a loss function (for training \(\)).2 Then, the _risk_ is defined as follows (Vapnik, 1998):

\[R()=_{p_{ te}(,y)}[((),y)],\] (1)

where \([]\) denotes the expectation. In the joint-shift problems, we are given a training set \(_{ tr}=\{(_{i}^{ tr},y_{i}^{ tr})\}_{i=1}^{n_{ tr }}}{{}}p_{ tr}(,y)\) and a validation set \(_{ v}=\{(_{i}^{ v},y_{i}^{ v})\}_{i=1}^{n_{ v}} }{{}}p_{ te}(,y)\), where \(_{ tr}\) is much bigger than \(_{ v}\), i.e., \(n_{ tr} n_{ v}\). The goal is to reliably estimate the risk from \(_{ tr}\) and \(_{ v}\) and train \(\) by minimizing the empirical risk, which should outperform training \(\) from only \(_{ v}\).

Motivation_Importance weighting_ (IW) has been a golden solver for DS problems (Sugiyama and Kawanabe, 2012), and there are many great off-the-shelf IW methods (Huang et al., 2007; Sugiyama et al., 2007a,b; Kanamori et al., 2009). Recently, _dynamic IW_ (DIW) was proposed to make IW compatible with stochastic optimizers and thus it can be used for deep learning (Fang et al., 2020). However, all IW methods including DIW have assumed cases (i) and (ii)--in cases (iii) and (iv), IW methods become problematic. Specifically, as the importance weights are only used on \(_{ tr}\), even though they become ill-defined on \(_{ te}_{ tr}\), IW itself is still well-defined. Nevertheless, in such a situation, the IW _identity_ will become an _inequality_ (i.e., Theorem 2), which means that what we minimize for training is no longer an approximation of the original risk \(R()\) and thus IW may lead to poor trained classifiers (i.e., Proposition 3). Moreover, some IW-like methods based on bilevel optimization share a similar issue with IW (Jiang et al., 2018; Ren et al., 2018; Shu et al., 2019), since \(\) is only trained from \(_{ tr}\) where \(_{ v}\) is used to determine the importance weights on \(_{ tr}\). In fact, cases (iii) and (iv) are more common nowadays due to _data-collection biases_, but they are still under-explored. For example, a class has several subclasses, but not all subclasses are presented in \(_{ tr}\) (see Figure 2). Therefore, we want to generalize IW to a universal solver for all the four cases.

ContributionsOur contributions can be summarized as follows.

* Firstly, we theoretically and empirically analyze when and why IW methods can succeed/may fail. We reveal that the objective of IW is good in cases (i) and (ii) and bad in cases (iii) and (iv).
* Secondly, we propose _generalized IW_ (GIW). In GIW, \(_{ te}\) is split into an _in-training_ (IT) part \(_{ te}_{ tr}\) and an _out-of-training_ (OOT) part \(_{ te}_{ tr}\), and its objective consists of a weighted classification term over the IT part and a standard classification term over the OOT part. GIW is justified as its objective is good in all the four cases and reduces to IW in cases (i) and (ii). Thus, GIW is a _strict generalization_ of IW from the objective point of view, and GIW is safer to be used when we are not sure whether the problem to be solved is a good case or a bad case for IW.3 * Thirdly, we provide a practical implementation of GIW: (a) following the split of \(_{ te}\), \(_{ v}\) is split into an IT set and an OOT set using the _one-class support vector machine_(Scholkopf et al., 1999);

Figure 1: An illustration of the relationship between the training support and the test support.

(b) the IT set, instead of the whole \(_{}\), is used for IW; and (c) the OOT set directly joins training together with \(_{}\) since no data in \(_{}\) comes from the OOT part.
* Finally, we design and conduct extensive experiments that demonstrate the effectiveness of GIW in cases (iii) and (iv). The experiment design is also a major contribution since no experimental setup is available for reference to simulate case (iii) or (iv) on benchmark datasets.

OrganizationThe analyses of IW are in Section 2, the proposal of GIW is in Section 3, and the experiments are in Section 4. Related work and additional experiments are in the appendices.

## 2 A deeper understanding of IW

First, we review the traditional _importance weighting_ (IW) and its modern implementation _dynamic importance weighting_ (DIW). Then, we analyze when and why IW methods can succeed/may fail.

A review of IWLet \(w^{*}(,y)=p_{}(,y)/p_{}(,y)\), which is the ratio of the test density \(p_{}(,y)\) over the training density \(p_{}(,y)\), known as the _importance function_. Then, the _expected objective_ of IW can be expressed as

\[J()=_{p_{}(,y)}[w^{*}(,y)(( ),y)].\] (2)

In order to empirically approximate \(J()\) in (2), we need to have an empirical version \((,y)\) of \(w^{*}(,y)\), so that the _empirical objective_ of IW is4

\[()=}}_{i=1}^{n_{}} (_{i}^{},y_{i}^{})((_{ i}^{}),y_{i}^{}).\] (3)

The original IW method is implemented in two steps: (I) _weight estimation_ (WE) where \((,y)\) is obtained and (II) _weighted classification_ (WC) where \(()\) is minimized. The first step relies on the training data \(_{}\) and the validation data \(_{}\), and it can be either estimating the two density functions separately and taking their ratio or directly estimating the density ratio (Sugiyama et al., 2012).

Figure 2: Two concrete examples of the success and failure of IW in case (iii).

A review of DIWThe aforementioned two-step approach is very nice when the classifier \(\) is a simple model, but it has a serious issue when \(\) is a deep model (Fang et al., 2020). Since WE is not equipped with representation learning, in order to boost its expressive power, we need an external feature extractor such as an internal representation learned by WC. As a result, we are trapped by a _circular dependency_: originally we need \(w^{*}\) to train \(\); now we need a trained \(\) to estimate \(w^{*}\).

DIW (Fang et al., 2020) has been proposed to resolve the critical circular dependency and to make IW usable for deep learning. Specifically, DIW uses a non-linear transformation \(\) created from the current \(\) (being trained) and replaces \(w^{*}(,y)\) with \(w^{*}()=p_{}()/p_{}()\), where \(=(,y)\) is the current _loss-value_ or _hidden-layer-output_ representation of \((,y)\). DIW iterates between WE for estimating \(w^{*}()\) and WC for training \(\) and thus updating \(\) in a seamless mini-batch-wise manner. Given that WE enjoys representation learning inside WC, the importance-weight estimation quality of WE and the classifier training quality of WC can improve each other gradually but significantly.

Risk consistency/inconsistency of IWNow, consider how to qualify good or bad expected objectives under different conditions. To this end, we adopt the concepts of _risk consistency_ and _classifier consistency_ from the label-noise learning literature (Xia et al., 2019, 2020; Yao et al., 2020).

**Definition 1**.: Given an (expected) objective \(J()\), we say it is _risk-consistent_ if \(J()=R()\) for any \(\), i.e., the objective is equal to the original risk for any classifier. On the other hand, we say \(J()\) is _classifier-consistent_ if \(_{}J()=_{}R()\) where the minimization is taken over all measurable functions, i.e., the objective shares the optimal classifier with the original risk.

In the definition above, risk consistency is conceptually stronger than classifier consistency. If an objective is risk-consistent, it must also be classifier-consistent; if it is classifier-consistent, it may sometimes be risk-inconsistent. Note that a risk-inconsistent objective is not necessarily very bad, as it can still be classifier-consistent.5 Hence, when considering expected objectives, risk consistency is a sufficient condition and classifier consistency is a necessary condition for good objectives.

In what follows, we analyze when and why the objective of IW, namely \(J()\) in (2), can be a good objective or may be a bad objective.

**Theorem 1**.: _In cases (i) and (ii), IW is risk-consistent.6_

Proof.: Recall that \(_{}=\{(,y):p_{}(,y)>0\}\) and \(_{}=\{(,y):p_{}(,y)>0\}\). Under case (i) or (ii), let us rewrite \(R()\) and \(J()\) with summations and integrals:

\[R() =_{y=1}^{C}_{\{:(,y)_{}\}}((),y)p_{}(,y),\] \[J() =_{y=1}^{C}_{\{:(,y)_{}\}}((),y)w^{*}(,y)p_{}(,y) \] \[=_{y=1}^{C}_{\{:(,y)_{}\}}((),y)p_{}(,y),\]

where \(w^{*}(,y)=p_{}(,y)/p_{}(,y)\) is always well-defined over \(_{}\) and we safely plugged this definition into the rewritten \(J()\). Subsequently, in case (i), \(_{}=_{}\) and thus \(J()=R()\). In case (ii), \(_{}_{}\) and then we further have

\[J()=_{y=1}^{C}_{\{:(,y)_{}\}}((),y)p_{}(,y)+_{\{ {x}:(,y)_{}_{}\}} ((),y)p_{}(,y).\]

By definition, \(p_{}(,y)=0\) outside \(_{}\) including \(_{}_{}\), and thus \(J()=R()\). 

**Theorem 2**.: _In cases (iii) and (iv), IW is risk-inconsistent, and it holds that \(J()<R()\) for any \(\)._

Proof.: Since \(w^{*}(,y)\) is well-defined over \(_{}\) but it becomes ill-defined over \(_{}_{}\), we cannot naively replace the integral domain in \(J()\) as in the proof of Theorem 1. In case (iii), \(_{}_{}\), and consequently

\[R() =_{y=1}^{C}_{\{:(,y)_{}\}}((),y)p_{}(,y)\] \[+_{y=1}^{C}_{\{:(,y)_{ }_{}\}}((),y)p_{ }(,y).\]According to Theorem 1 for case (i), the first term in the rewritten \(R()\) equals \(J()\). Moreover, the second term is positive, since \(((),y)>0\) due to the positivity of \(\), and \(p_{}(,y)>0\) over \(_{}\) including \(_{}_{}\). As a result, in case (iii), \(R()>J()\).

Similarly, in case (iv), we can split \(_{}\) into \(_{}_{}\) and \(_{}_{}\) and decompose \(R()\) as

\[R() =_{y=1}^{C}_{\{:(,y)_{}_{}\}}((),y)p_{}(,y)\] \[+_{y=1}^{C}_{\{:(,y)_{ }_{}\}}((),y)p_{ }(,y).\]

Note that \(_{}_{}_{ }\), so that according to Theorem 1 for case (ii), the first term equals \(J()\). Following case (iii), the second term is positive. Therefore, in case (iv), \(R()>J()\). 

Theorem 1 implies that the objective of IW can be a good objective in cases (i) and (ii). Theorem 2 implies that the objective of IW may be a bad objective in cases (iii) and (iv). As a consequence, the theorems collectively address when and why IW methods can succeed/may fail.7

When the IW objective may be bad and IW methods may fail, whether an IW method fails or not depends on many factors, such as the underlying data distributions, the sampled data sets, the loss, the model, and the optimizer. To illustrate this phenomenon, here we give two concrete examples belonging to case (iii), where IW has no problem at all in one example and is as poor as random guessing in the other example.

Two concrete examplesWe have seen the examples in Figure 2. In both examples, there are two classes marked with red and blue colors and distributed in four squares. Each square has a unit area and is the support of a uniform distribution of \(\), i.e., \(p(,1)=1\) and \(p(,0)=0\) if its color is red, and \(p(,0)=1\) and \(p(,1)=0\) if its color is blue. There is a margin of 0.1 between two adjacent squares. The training distribution consists of the two squares on the left, and the test distribution consists of all the four squares. In the first example, on \(_{}_{}\), the label is red on the top and blue on the bottom, as same as the label on \(_{}\). In the second example, on \(_{}_{}\), the label is blue on the top and red on the bottom, as opposite as the label on \(_{}\).

We experimentally validated whether DIW works or not. The number of training data was 200. The number of validation data was only 4: we sampled one random point from each training square and added the center point of each test-only square. We can see that DIW performs very well in the first example, better than training from only the validation data; unfortunately, DIW performs very poorly in the second example, even worse than training from only the validation data.

The observed phenomenon should not be limited to DIW but be common to all IW methods. Here, we analyze why this phenomenon does not depend on the loss, the model, or the optimizer.

**Proposition 3**.: _In the first example, IW is classifier-consistent, while in the second example, IW is classifier-inconsistent.8_

Proof.: Without loss of generality, assume that \(\) is _classification-calibrated_(Bartlett et al., 2006).9 Let \((c^{(1)},c^{(2)})\) be the center of \(_{}\), and then the four squares are located on the top-left, bottom-left, top-right, and bottom-right of \((c^{(1)},c^{(2)})\). For convenience, we abbreviate \(f(x^{(1)},x^{(2)})\) for \(x^{(1)}>c^{(1)},x^{(2)}>c^{(2)}\) as \(f(+,+)\), \(f(x^{(1)},x^{(2)})\) for \(x^{(1)}>c^{(1)},x^{(2)}<c^{(2)}\) as \(f(+,-)\), and so on.

Consider the first example. The minimizer of \(R(f)\) can be any _Bayes-optimal classifier_, i.e., any \(f\) such that \(f(,+)>0\) and \(f(,-)<0\). Next, on the top-left square, we have \(p_{}(,1)=1/4\), \(p_{}(,0)=0\), \(p_{}(,1)=1/2\), and \(p_{}(,0)=0\), and thus \(w^{*}(,y)=1/2\). Likewise, on the bottom-left square, we have \(w^{*}(,y)=1/2\). As a result, \(J(f)=_{p_{}(,y)}[((),y)]\), meaning that the minimizer of \(J(f)\) can be any Bayes-optimal classifier on \(_{}\), i.e., any \(f\) such that \(f(-,+)>0\) and \(f(-,-)<0\). The simplest manner for \(f\) to transfer its knowledge from \(p_{}\) to \(p_{}\) is to have a linear decision boundary and extend it to \(_{}_{}\), so that \(f(,+)>0\) and \(f(,-)<0\) on \(_{}\). We can see that the set of minimizers is shared and thus IW is classifier-consistent.

Consider the second example. The minimizer of \(J(f)\) is still the same while the minimizer of \(R(f)\) significantly changes to any \(f\) such that \(f(-,+)>0\), \(f(-,-)<0\), \(f(+,+)<0\), and \(f(+,-)>0\). This non-linear decision boundary is a checkerboard where any two adjacent squares have opposite predictions. It is easy to see that IW is classifier-inconsistent and its test accuracy is 0.5. For binary classification with balanced classes, this accuracy is as poor as random guessing. 

## 3 Generalized importance weighting (GIW)

We have seen two examples where IW is as good/bad as possible in case (iii). In practice, we cannot rely on the luck and hope that IW would work. In this section, we propose _generalized importance weighting_ (GIW), which is still IW in cases (i) and (ii) and is better than IW in cases (iii) and (iv).

### Expected objective of GIW

The key idea of GIW is to split the test support \(_{}\) into the _in-training_ (IT) part \(_{}_{}\) and the _out-of-training_ (OOT) part \(_{}_{}\). More specifically, we introduce a third random variable, the support-splitting variable \(s\{0,1\}\), such that \(s\) takes 1 on \(_{}\) and 0 on \(_{}_{}\). As a result, the underlying joint density \(p(,y,s)\) can be defined by \(p_{}(,y)\) as10

\[p(,y,s)=p_{}(,y)&(,y) _{}s=1,(,y)_{}_{}s=0,\\ 0&(,y)_{}s=0,(,y) _{}_{}s=1.\] (4)

Let \(=p(s=1)\). Then, the expected objective of GIW is defined as

\[J_{}()=_{p_{}(,y)}[w^{*}( {x},y)((),y)]+(1-)_{p(,y|s=0)}[((),y)].\] (5)

The corresponding empirical version \(_{}()\) will be derived in the next subsection. Before proceeding to the empirical objective of GIW, we establish risk consistency of GIW.

**Theorem 4**.: _GIW is always risk-consistent for distribution shift problems._

Proof.: Let us work on the first term of \(J_{}()\) in (5). When \((,y)_{}\),

\[ w^{*}(,y)p_{}(,y)= p_{}(,y)=p(s=1)p(,y s=1)=p(,y,s=1),\]

where \(p_{}(,y)=p(,y s=1)\) given \((,y)_{}\) according to (4). Since \(p(,y,s=1)=0\) on \(_{}_{}\), we have

\[_{p_{}(,y)}[w^{*}(,y)((),y)]=_{y=1}^{C}_{(:(,y)_{})} ((),y)p(,y,s=1).\] (6)

Next, for the second term of \(J_{}()\), since \((1-)p(,y s=0)=p(,y,s=0)\), we have

\[(1-)_{p(,y|s=0)}[((),y)]=_{y=1}^{C} _{(:(,y)_{})}((),y)p( ,y,s=0).\] (7)

Note that \(p(,y,s=1)+p(,y,s=0)=p_{}(,y)\) according to (4). By adding (6) and (7), we can obtain that \(J_{}()=R()\). This conclusion holds in all the four cases. 

Theorem 4 is the main theorem of this paper. It implies that the objective of GIW can always be a good objective. Recall that IW is also risk-consistent in cases (i) and (ii), and it is interesting to see how IW and GIW are connected. By definition, given fixed \(p_{}(,y)\) and \(p_{}(,y)\), if there exists a risk-consistent objective, it is unique. Indeed, in cases (i) and (ii), GIW is reduced to IW, simply due to that \(=1\) and \(J_{}()=J()\) for any \(\).

### Empirical objective and practical implementation of GIW

Approximating \(J_{}()\) in (5) is more involved than approximating \(J()\) in (2). Following (3), we need an empirical version \((,y)\), and we need further to split the validation data \(_{}\) into two sets and estimate \(\). Obviously, how to accurately split the validation data is the most challenging part. After splitting \(_{}\) and obtaining an estimate \(\), the empirical objective will have two terms, where the first term can be handled by any IW algorithm given training data and IT validation data, and the second term just involves OOT validation data.

To split \(_{}\) and estimate \(\), we employ the _one-class support vector machine_ (O-SVM) (Scholkopf et al., 1999). Firstly, we pretrain a deep network for classification on the training data \(_{}\) a little bit and obtain a _feature extractor_ from the pretrained deep network. Secondly, we apply the feature extractor on the instances in \(_{}\) and train an O-SVM based on the latent representation of these instances, giving us a score function \(g()\) that could predict whether \(p_{}()>0\) or not, where \(\) is the latent representation of \(\). Thirdly, we apply the feature extractor on the instances in \(_{}\) and then employ \(g()\) to obtain the IT validation data \(_{}=\{(_{i}^{ 1},y_{i}^{ 1})\}_{i=1}^{n_{ 1}}\) and the OOT validation data \(_{}=\{(_{i}^{ 2},y_{i}^{ 2})\}_{i=1}^{n_{ 2}}\). Finally, \(\) can be naturally estimated as \(=n_{}/n_{}\).

We have two comments on the split of \(_{}\). The O-SVM \(g()\) predicts whether \(p_{}()>0\) or not rather than whether \(p_{}(,y)>0\) or not. This is because the \(\)-support change is often sufficiently informative in practice: when the \(\)-support changes, O-SVM can detect it; when the \((,y)\)-support changes without changing the \(\)-support, it will be very difficult to train an O-SVM based on the loss-value representation of \((,y)\) to detect it, but such changes are very rare. The other comment is about the choice of the O-SVM. While there are more advanced one-class classification methods (Hido et al., 2011; Zaheer et al., 2020; Hu et al., 2020; Goldwasser et al., 2020) (see Perera et al. (2021) for a survey), the O-SVM is already good enough for the purpose (see Appendix C.1).

Subsequently, \(_{}\) can be viewed as being drawn from \(p(,y s=1)\), and \(_{}\) can be viewed as being drawn from \(p(,y s=0)\). Based on \(_{}\) and \(_{}\), we can obtain either \((,y)\) or \(_{i}\) for each \((_{i}^{},y_{i}^{})\) by IW. IW has no problem here since the split of \(_{}\) can reduce case (iii) to case (i) and case (iv) to case (ii). In the implementation, we employ DIW (Fang et al., 2020) because it is friendly to deep learning and it is a state-of-the-art IW method. Finally, the empirical objective of GIW can be expressed as

\[_{}()=}}{n_{}n_{ }}_{i=1}^{n_{}}(_{i}^{}, y_{i}^{})((_{i}^{}),y_{i}^{})+ }}_{j=1}^{n_{}}((_{j}^{  2}),y_{j}^{ 2}),\] (8)

where the two expectations in \(J_{}()\) are approximated separately with \(_{}\) and \(_{}\).11

The practical implementation of GIW is presented in Algorithm 1. Here, we adopt the hidden-layer-output representation for O-SVM in ValDataSplit and the loss-value representation for DIW in ModelTrain. This algorithm design is convenient for both O-SVM and DIW; the hidden-layer-output representation for DIW has been tried and can be found in Section 4.3.

## 4 Experiments

In this section, we empirically evaluate GIW and compare it with baseline methods.12 To see how effective it is in cases (iii) and (iv), we designed two distribution shift (DS) patterns. In the first pattern, DS comes solely from the mismatch between the training and test supports, and we call it support shift (SS). Under SS, it holds that \(w^{*}(,y)\) equals \(\) in case (iii) and 0 or another constant in case (iv), simply due to renormalization after imposing SS. Hence, the challenge is to accurately split \(_{}\). In the second pattern, there is some genuine DS (e.g., label noise or class-prior shift) on top of SS, and we call it support-distribution shift. Since \(w^{*}(,y)\) is no longer a constant, we face the challenge to accurately estimate \(w^{*}(,y)\). Additionally, we conducted an ablation study to better understand the behavior of GIW. Detailed setups and more results are given in Appendices B & C.

The baseline methods involved in our experiments are as follows.

* _Val-only_: using only \(_{}\) to train the model from scratch.
* _Pretrain-val_: first pretraining on \(_{}\) and then training on \(_{}\).
* _Reweight_: learning to reweight examples (Ren et al., 2018).
* _MW-Net_: meta-weight-net (Shu et al., 2019), a parametric version of Reweight.
* _DIW_: dynamic importance weighting (Fang et al., 2020).
* _R-DIW_: DIW where IW is done with _relative density-ratio estimation_(Yamada et al., 2011).
* _CCSA_: classification and contrastive semantic alignment (Motiian et al., 2017).
* _DANN_: domain-adversarial neural network (Ganin et al., 2016).

### Experiments under support shift

We first conducted experiments under support shift on benchmark datasets. The setups are summarized in Table 1. For MNIST, our task was to classify odd and even digits, where the training set has only 4 digits (0-3), while the test set has 10 digits (0-9) in case (iii) and 8 digits (2-9) in case (iv). For Color-MNIST, our task was to classify 10 digits; the dataset was modified from MNIST in such a way that the digits in the training set are colored in red while the digits in the test/validation set are colored

   Dataset & Task & Training data & Test data & Model \\  MNIST & odd and even digits & 4 digits (0-3) & 10 digits (0-9)\({}^{*}\) & LeNet-5 \\ Color-MNIST & 10 digits & digits in red & digits in red/blue/green & LeNet-5 \\ CIFAR-20 & 20 superclasses & 2 classes per superclass & 5 classes per superclass & ResNet-18 \\    See LeCun et al. (1998) for MNIST and Krizhevsky and Hinton (2009) for CIFAR-20. Color-MNIST is modified from MNIST. The model is a modified LeNet-5 (LeCun et al., 1998) or ResNet-18 (He et al., 2016). Please find in Appendix B.1 the details. \({}^{*}\)All setups in the table are for case (iii); for MNIST in case (iv), the test data consist of 8 digits (2-9).

Table 1: Specification of benchmark datasets, tasks, distribution shifts, and models.

Figure 3: Comparisons with IW-like and DA baselines under support shift (5 trails).

in red/green/blue evenly. For CIFAR-100, our task was to classify the 20 predefined superclasses and thus we call it CIFAR-20; the training set contains data from 2 out of the 5 classes for each superclass while the test set contains all classes. For validation set, we sampled 2 data points per test digit for MNIST and Color-MNIST, and 10 data points per class for CIFAR-100.

Figure 3 shows the results on MNIST, Color-MNIST, and CIFAR-20 under support shift13, where GIW generally outperforms IW-like and domain adaptation (DA) baselines. We also confirmed that \(\) in (5) is accurately estimated in Appendix C.1. To further investigate how GIW works, we visualized the learned convolution kernels (i.e., weights) for Color-MNIST experiments in Figure 4, where the more observed color represents the larger weights learned on that color channel. Only GIW recovers the weights of all color channels while learning useful features, however, other methods fail to do so.

### Experiments under support-distribution shift

We further imposed additional distribution shift, i.e., adding label noise or class-prior shift, on top of the support shift following the same setup in Table 1. Here we only show the results in case (iii) and defer the results in case (iv) to Appendix C.4 due to the space limitation.

Label-noise experimentsIn addition to the support shift, we imposed label noise by randomly flipping a label to other classes with an equal probability, i.e., the noise rate. The noise rates are set as \(\{0.2,0.4\}\) and the corresponding experimental results are shown in Figure 5 and 6. We can see that compared with baselines, GIW performs better and tends to be robust to noisy labels.

Class-prior-shift experimentsOn top of the support shift, we imposed class-prior shift by reducing the number of training data in half of the classes to make them minority classes (as opposed to majority classes). The sample size ratio per class between the majority and minority classes is defined as \(\), chosen from \(\{10,100\}\). To fully use the data from the minority class, we did not split the validation data in class-prior-shift experiments and used all validation data in optimizing the two

Figure 4: Visualizations of the learned convolution kernels on Color-MNIST under support shift.

Figure 5: Comparisons with IW-like baselines under support-distribution shift (5 trails).

[MISSING_PAGE_FAIL:10]