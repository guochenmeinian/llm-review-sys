# Online Pricing for Multi-User Multi-Item Markets

Yigit Efe Erginbas\({}^{1}\)   Thomas Courtade\({}^{1}\)   Kannan Ramchandran\({}^{1}\)   Soham Phade\({}^{2}\)

\({}^{1}\)University of California, Berkeley  \({}^{2}\)Wayve Technologies

###### Abstract

Online pricing has been the focus of extensive research in recent years, particularly in the context of selling an item to sequentially arriving users. However, what if a provider wants to maximize revenue by selling multiple items to multiple users in each round? This presents a complex problem, as the provider must intelligently offer the items to those users who value them the most without exceeding their highest acceptable prices. In this study, we tackle this challenge by designing online algorithms that can efficiently offer and price items while learning user valuations from accept/reject feedback. We focus on three user valuation models (fixed valuations, random experiences, and random valuations) and provide algorithms with nearly-optimal revenue regret guarantees. In particular, for any market setting with \(N\) users, \(M\) items, and load \(L\) (which roughly corresponds to the maximum number of simultaneous allocations possible), our algorithms achieve regret of order \(O(NM(LT))\) under fixed valuations model, \(()\) under random experiences model and \(()\) under random valuations model in \(T\) rounds.

## 1 Introduction

The ability to design algorithms that can achieve the optimal sale of goods to multiple users having time-varying valuations for each of the goods is both timely and relevant, given the explosion in the use of data in large-scale systems. This problem is commonly encountered in various contexts, such as in the e-commerce (Amazon, eBay), ride-share (Uber, Lyft), airline, and hotel (Airbnb, Booking.com) industries. Since the provider's goal of maximizing revenue can only be achieved through a delicate balance of considering prior transactions and adjusting offers and prices, it presents a unique opportunity to advance our understanding of dynamic pricing.

We presently consider the problem of designing algorithms that aim to optimize the sale of multiple goods to multiple users having time-varying valuations over the course of repeated rounds. At each round, the provider offers each item to a user at a chosen price, and users decide whether or not to buy, by comparing the offered price to their private valuation for the good. The provider may decide on offers and prices based on outcomes of prior transactions, but each individual user accepts or rejects their offer based only on their valuation for the current round. The provider's goal is to maximize the revenue accumulated in multiple rounds by judiciously selecting the offers and associated prices.

The provider, who is endowed with multiple items at each round, repeatedly offers these items to multiple users at well-chosen prices. In response, the provider obtains feedback regarding the acceptance or rejection decisions of the users and receives revenue for the accepted items. The provider's goal is to maximize the accumulated revenue over time by making offers that respect the endowment constraints and user demands. In the process of identifying the best way of offering the items to target users, the provider encounters challenges regarding two separate aspects of the problem; such challenges are addressed in our work. As depicted in Figure 1, the first challenge is to learn user preferences from interactive feedback; the second is to find offers and prices that will result in maximal revenue.

The challenge of _learning_ arises from the fact that the provider does not have knowledge of the user valuations ahead of time and hence has to learn them while continually taking action. On the other hand, the challenge of _offering and pricing_ stems from both of the facts that (a) available items are scarce and (b) user demands are limited. Therefore, to achieve better outcomes, the provider needs to make careful decisions on which items to offer to which user within the limits of these constraints. However, since the valuations of the users are unknown, the offering and pricing decisions involve a trade-off between learning individual customer preferences in order to increase long-term revenues and earning short-term revenues by leveraging the information acquired so far.

In this study, we focus on algorithms that offer each item to only one user during each round. This assumption eliminates the risk of multiple users requesting the same item and removes the need for the provider to decide who should receive it. Relaxing this assumption could possibly offer more flexibility and revenue potential during the earlier phases while learning the valuations. However, it does not result in a loss in the maximum achievable revenue under known valuations, and hence our algorithms can still achieve no-regret guarantees. Additionally, offering a limited number of items simplifies the process for users by reducing the number of choices they need to consider. As the provider gains insights into user valuations over time, the offers become more tailored and relevant, enabling users to focus on evaluating items that the provider believes will be of the highest interest to them. This saves time and reduces the effort required to make a decision.

### Our contributions

To the best of our knowledge, we are the first to address the problem of dynamic pricing for the sale of multiple items to multiple users with unknown valuations. Our contributions are as follows.

* We consider three user valuation models: fixed valuations, random experiences, and random valuations. While the fixed valuation and random valuation models are standard models explored extensively in prior work on dynamic pricing (Bubeck et al., 2019), we also propose and analyze the random experiences model as a more realistic representation of user behavior.
* We introduce a problem-dependent load parameter \(L\) that roughly corresponds to the maximum number of simultaneous allocations possible (see Def. 2). We uncover its crucial role in characterizing problem classes for which we can establish matching upper and lower regret bounds.
* We design regret-optimal (up to smaller order terms) algorithms for each setting. For any market setting with \(N\) users, \(M\) items, and maximum load \(L\), our algorithms achieve regret \(O(NM(LT))\) under fixed valuations model, \(()\) under random experiences model and \(()\) under random valuations model in \(T\) rounds.
* All proposed algorithms have computational complexity \((NML)\) per round in the worst case. The algorithms for fixed valuations and random experiences models have space complexity \((NM)\) while the algorithm for random valuations has space complexity \((NM(LT)^{1/4})\).

## 2 Related work

**Bandits for dynamic posted-pricing.** The problem of dynamic pricing has been typically modeled as a variant of the multi-armed bandit problem starting with Rothschild (1974). In their seminal work, Kleinberg and Leighton (2003) developed a more general and widely-appreciated framework to maximize revenue from selling copies of a single good to sequentially arriving users. In the following years, there has been a growing body of work on multi-period single-product dynamic

Figure 1: The providerâ€™s goal is to maximize the revenue obtained from sales over multiple rounds. The provider decides on offers and prices while interactively learning the user valuations from the accept/reject feedback. At each round, only a subset of items may be available for sale and only a subset of users may be active.

pricing problems under different user valuation models including non-parametric models (Besbes and Zeevi, 2009; Keskin and Zeevi, 2014; Cesa-Bianchi et al., 2019; Bubeck et al., 2019), contextual (feature-based) models (Paes Leme and Schneider, 2018; Cohen et al., 2020; Xu and Wang, 2021), and other parametric models (Araman and Caldentey, 2009; Broder and Rusmevichientong, 2012; Harrison et al., 2012; Chen and Farias, 2013; Besbes and Zeevi, 2015; Ferreira et al., 2018). However, all of these works address the posted price problem for selling a single item in each round. Our contribution stands out by considering the combinatorial aspect of the allocation problem faced in _simultaneously selling multiple items_, a factor that was not taken into account in prior literature.

**Combinatorial multi-armed bandits.** The semi-bandits framework of Audibert et al. (2011) and the combinatorial multi-armed bandits frameworks of Chen et al. (2013) and Kveton et al. (2015) model problems where a player selects a combination of arms in each round and observes random rewards from the played arms. Therefore, the selection of the offers in our setting shows parallelism with these frameworks. However, their algorithmic solutions cannot be directly applied to our problem because the feedback and reward mechanisms in dynamic pricing are crucially different than the models considered in this literature. It is mainly because the feedback (acceptance/rejection) and reward (revenue) are not only affected by the offers but also by the accompanying prices.

**Bandits in matching markets.** One recent line of related literature in computational economics studies algorithms for learning socially-optimal matching in two-sided markets (Liu et al., 2020; Johari et al., 2017; Jagadeesan et al., 2021). These frameworks can be used to model the problem of allocating multiple items to multiple users with unknown valuations with the goal of maximizing social welfare. However, these works only consider scenarios where all the users accept their matchings (i.e. offers) without being affected by prices and send the provider real-valued random feedback representing the welfare they achieve from this matching. In their recent work, Erginbas et al. (2023) also analyze a similar problem of optimal and stable allocations and further allow users to accept or reject their recommendations based on the prices. However, their framework also requires random feedback to be sent to the provider regarding their valuation for each accepted matching, whereas our problem setting limits the provider to only observe acceptance or rejection decisions.

**Learning in repeated auctions.** The learning literature on auctions considers both offline (Morgenstern and Roughgarden, 2016; Cai and Daskalakis, 2017) and online approaches (Bar-Yossef et al., 2002; Lavi and Nisan, 2000) to maximize the provider's revenue from selling multiple items to multiple users. Nonetheless, the implementation of these auctions often presents significant difficulties due to the inherent complexities in obtaining precise valuations from human participants for all the items. As a result, there is a growing interest in designing mechanisms that are accessible, simple to use, and can easily elicit the valuations of the buyers while maximizing the revenue of the provider. In this direction, our approach in this study follows the main premise of posted-price mechanisms (Feldman et al., 2015; Einav et al., 2018), in which the provider sets a fixed price for each item, and buyers decide whether or not to purchase the item at that price.

## 3 Problem setting

**Notation:** We use bold font for vectors \(\) and matrices \(\), and calligraphic font \(\) for sets. For a vector \(\), we denote its \(i\)-th entry by \(x_{i}\) and for a matrix \(\), we denote its \((i,j)\)-th entry by \(x_{ij}\). For any positive integer \(n\), we use \([n]\) to denote the set \(\{1,2,,n\}\). For real numbers \(a\) and \(b\), we use \(a b\) to denote their minimum and \(a b\) to denote their maximum.

Suppose the market consists of a set of users \(\) of size \(N\) and a set of items \(\) of size \(M\). At each round \(t[T]\) over some fixed time horizon \(T\), a provider is endowed with a subset of the items denoted by \(^{t}\) and tries to sell these items to users with the goal of obtaining revenue. 1 We assume that endowed items \(^{t}\) are only available for sale at time interval \(t\). That is, the items that are not sold at a round cannot be stored to be sold in future rounds.

Each user \(u\) has an _unknown_ and possibly _time-varying_ valuation \(v^{t}_{ui}\) for each item \(i\) at round \(t\). In this work, we consider that each user \(u\) has a time-varying request for at most \(d^{t}_{}\) items and has an additive utility model over items. Thus, the provider decides on a price vector \(^{t}^{M}_{+}\) and offers each user \(u\) a subset of the available items denoted by \(^{t}_{u}^{t}\) of size at most \(|^{t}_{u}| d^{t}_{u}\). Then, users decide to accept a subset of their offered items \(^{t}_{u}^{t}_{u}\) based on their valuations and the prices of the items in order to maximize their surplus given by \(_{i^{t}_{u}}(v^{t}_{ui}-p^{t}_{i})\).

Due to the additive utility assumption, when user \(u\) is offered \(^{t}_{u}\), it accepts all items \(i^{t}_{u}\) that give positive surplus (i.e. \(v^{t}_{ui} p^{t}_{i}\)) while rejecting all other items. 2 Hence, the set of accepted items can be written as

\[^{t}_{u}=\{i^{t}_{u}:v^{t}_{ui} p^{t}_{i}\}.\]

For future reference, we also denote the collections of offered and accepted items at time \(t\) by \(}^{t}=\{^{t}_{u}|u\}\) and \(}^{t}=\{^{t}_{u}|u\}\), respectively. To eliminate the possibility of an item getting accepted by multiple users, we consider posted-price offer mechanisms that offer each item to at most one user. Therefore, the offered sets of items are disjoint, i.e. \(^{t}_{u}^{t}_{u^{}}=\) for \(u u^{}\).

Whenever a user accepts the offer of item \(i\) at round \(t\), this sale generates \(p^{t}_{i}\) revenue for the provider. Therefore, the cumulative revenue obtained over \(T\) rounds equals to

\[_{t=1}^{T}_{u}_{i^{t}_{u}}p^{t}_{i}= _{t=1}^{T}_{u}_{i^{t}_{u}}p^{t}_{i}\; \{v^{t}_{ui} p^{t}_{i}\}.\] (1)

After the provider decides on the price vector \(^{t}\) and the offers \(}^{t}=\{^{t}_{u}|u\}\), the users report their set of accepted items \(}^{t}=\{^{t}_{u}|u\}\). We denote by \(H_{t}\) the history \(\{}^{},^{},}^{}\}_{=1}^{t-1}\) of observations available to the provider when choosing the next set of offers \(}^{t}\) along with the next price vector \(^{t}\). The provider employs a policy \(=\{^{t}|t\}\), which is a sequence of functions, each mapping the history \(H_{t}\) to an action \((}^{t},^{t})\).

The task of the provider is to repeatedly offer the items to users and choose the prices such that it can achieve maximal revenue. To evaluate policies in achieving this objective, we define _regret_ metrics that measure the gap between the expected revenue of policy \(\) and an optimal algorithm.

**Definition 1**.: _For a policy \(\), its revenue regret in \(T\) rounds is defined as_

\[(T,)=-_{t=1}^{T}_{u} _{i^{t}_{u}}p^{t}_{i}\;\{v^{t}_{ui} p^{t}_{i}\},\] (2)

_where \(\) denotes the revenue of the optimal algorithm which will be defined separately for each different valuation model in Section 4._

We can also represent the offers \(}^{t}=\{^{t}_{u}|u\}\) using binary variables \(x^{t}_{ui}=\{i^{t}_{u}\}\) that indicate whether item \(i\) is a member of each of the sets \(^{t}_{u}\). With this definition, each variable \(x^{t}_{ui}\) is equal to 1 if user \(u\) is offered item \(i\) at time \(t\) and 0 otherwise. We collect these variables into a matrix \(^{t}\{0,1\}^{N M}\) called the _offer_ matrix. Due to endowment and demand constraints, the offer matrix \(^{t}\) at each time \(t\) needs to belong to the set

\[^{t}=\{\{0,1\}^{N M}:_{i}x^{ t}_{ui} d^{t}_{u}, u_{u}x_{ui} e^{t}_{i}, i \},\] (3)

where each endowment quantity \(e^{t}_{i}=\{i^{t}\}\) is equal to 1 when item \(i\) is available to be offered at time \(t\), and 0 otherwise. Using this notation, we can write the cumulative revenue as

\[_{t=1}^{T}_{u}_{i}x^{t}_{ui}p^{t}_{i}\; \{v^{t}_{ui} p^{t}_{i}\}.\] (4)

Lastly, we define the _maximum load_ parameter which refers to the maximum amount of simultaneous demand and supply in the market. Formally,

**Definition 2**.: _For a problem with endowment sequence \((^{t})_{1 t T}\) and demand sequences \((d^{t}_{u})_{1 t T}\) of all users \(u\), the maximum load is defined as_

\[L=_{1 t T}\{D_{t} E_{t}\},\] (5)

_where \(D_{t}:=_{u}d^{t}_{u}\) and \(E_{t}:=|^{t}|\) are the total demand and endowment at time \(t\), respectively._

Note that the maximum load parameter \(L\) is an upper bound for the number of offers that can be made at any round \(t\). This parameter is central to our analysis since the problem becomes more complex as the maximum load increases.

### Summary of results

Our goal in this work is to provide insights into how to strategize multi-round posted-price offers when the provider does not have prior knowledge of user valuations. In particular, we consider the problem under three different valuation models as described below.

1. **Fixed valuations:** The valuations of users do not change over time. Formally, there exist values \(v_{ui}\) such that \(v_{ui}^{}=v_{ui}\) for all \(t[T]\).
2. **Random experiences:** The valuations of users are given as their average historical experience where each experience is independently drawn from distributions specific to each user and each item. Formally, we consider the experience of user \(u\) with item \(i\) to be given as the random variable \(z_{ui}^{t}\) independently drawn from some distribution with cumulative distribution function \(F_{ui}\) for all rounds at which user \(u\) has accepted item \(i\). Then, each valuation \(v_{ui}^{t}\) is given as the average of the past experiences, i.e., the average of values \(\{z_{ui}^{}|<t,i_{u}^{t}\}\).
3. **Random valuations:** The valuations of users at different rounds are independently drawn from distributions specific to each user and each item. Formally, there exist cumulative distribution functions \(F_{ui}\) for all \((u,i)\) such that each \(v_{ui}^{t}\) is independently drawn from a distribution with cdf \(F_{ui}\).

For each of the models described above, we derive upper and lower bounds for revenue regret, matching up to logarithmic factors. We summarize our results in Table 1.

_Remark_.: The frameworks of fixed valuations and random valuations can also be used to model settings where each interacting user is associated with a type that determines their valuations. In this case, the set \(\) corresponds to the set of all user types and each demand parameter \(d_{u}^{t}\) represents the total demand of users of type \(u\) in round \(t\). Under the fixed valuations model, all users of type \(u\) have valuation \(v_{ui}^{t}=v_{ui}\) for item \(i\) at all rounds \(t\). Under the random valuations model, each user of type \(u\) has a random valuation with distribution \(F_{ui}\) for item \(i\) independently for each user at each time \(t\). Since at most one user receives any item \(i\) in any round, it is sufficient to consider a single random valuation \(v_{ui}^{t}\) for each type \(u\) and item \(i\) at time \(t\).

## 4 Methodology

We provide algorithms for achieving sub-linear revenue regret under different user valuation models described in Section 3.1. While the strategies for the selection of offers are similar under different models, we use different pricing strategies for different models as depicted in Figure 2.

  
**Model** & **Upper Bound** & **Lower Bound** \\  Fixed Valuations (Section 4.1) & \(O(NM(LT))\) & \((NM(LT/NM))\) \\  Random Experiences (Section 4.2) & \(()\) & \(()\) \\  Random Valuations (Section 4.3) & \(()\) & \(()\) \\   

Table 1: Upper and lower bounds for revenue regret under different valuation models.

Figure 2: The selection of prices for different valuation models. (a) If the offer is accepted (rejected), we set the new value of \(a_{ui}\) (\(b_{ui}\)) as the offered price \(p_{i}^{t}\). (b) If \(n_{ui}=2^{j}\) for some \(j\) and the offer is accepted (rejected), we set the new value of \(a_{ui}\) (\(b_{ui}\)) as a number smaller (larger) than \(p_{i}^{t}\). (c) After offering an item at price \(k/K\), we update the \(k^{}\) confidence interval based on whether the offer is accepted or rejected.

``` \(a_{ui} 0\), \(b_{ui} 1\) and \(_{ui} 0.5\) for all \(u\), \(i\) for\(t=1,2,,T\)do  Calculate \(^{t}\) by solving problem (8) using \(b_{ui}\) values. for\((u,i)\{(u,i):x_{ui}^{t}=1\}\)do\(p_{i}^{t} a_{ui}+_{ui}\)\(\{b_{ui}-a_{ui}\}\)\(\) offer item \(i\) to user \(u\) \(p_{i}^{t} a_{ui}+_{ui}\)\(\{b_{ui}-a_{ui}\}\)\(\) set the price \(\) offer item \(i\) to user \(u\) at price \(p_{i}^{t}\) and observe \(\{v_{ui}^{t} p_{i}^{t}\}\). \(\) update the interval \(v_{ui}^{t} p_{i}^{t}\)then\(a_{ui} a_{ui} p_{i}^{t}\) else\(b_{ui} b_{ui} p_{i}^{t}\)\(\) update the interval if\(b_{ui}-a_{ui}_{ui}\)then\(_{ui}_{ui}^{2}\)\(\) update the search step size endfor endfor ```

**Algorithm 1** Offerings with incremental search prices

### Revenue maximization under fixed valuations

In this section, we focus on the scenario where the provider makes posted-price offers to users whose valuations are fixed over time. We formalize this condition in the following assumption.

**Assumption 1**.: _The valuation of any user \(u\) for any item \(i\) is given by \(v_{ui}^{t}=v_{ui}\) for some \(v_{ui}\) at all rounds \(t[T]\)._

In the case of fixed valuations, we define the optimum strategy as the one that maximizes the revenue under complete information on user valuations. Therefore, the optimum offers \(_{*}^{t}\) and prices \(_{*}^{t}\) for all rounds \(t\) are given by maximizing the cumulative revenue as given in (4). That is, we define the optimum objective value as

\[=_{t=1}^{T}[_{^{t}^{t}}_{ {p}^{t}^{T}_{+}}_{u}_{i}x_{ ui}^{t}p_{i}^{t}\{v_{ui} p_{i}^{t}\}],\] (6)

and let \(_{*}^{t}\) and \(_{*}^{t}\) be the solutions that maximize term \(t\). Note that for any allocation \(^{t}^{t}\), we have \(_{y}x_{ui}^{t} e_{i}^{t} 1\). Therefore, whenever an item \(i\) is offered to a user \(u\), the optimum price of item \(i\) is equal to the user's valuation \(v_{ui}\). In other words, the optimum price for item \(i\) is given by \(p_{i}^{t}=_{u}x_{ui}^{t}v_{ui}\). Based on this observation, the revenue-maximizing offer can be found as

\[_{*}^{t}=*{arg\,max}_{^{t}^{t}}\ _{u}_{i}x_{ui}^{t}v_{ui}.\] (7)

We note that the integer program in (7) can be written as an instance of maximum weight bipartite matching. Then, using a variant of the Hungarian algorithm for unbalanced bipartite graphs (Ramshaw and Tarjan, 2012), this problem can be solved in space \((NM)\) and time \((NML)\) in the worst case. (See Appendix A for details.)

Since there is no randomness in user responses, every response from a user \(u\) about item \(i\) gives the provider complete information about a lower or upper bound on \(v_{ui}\), depending on whether the user response was to accept or reject the offered price for the item \(i\). For this reason, the algorithm operates by keeping track of intervals \([a_{ui},b_{ui}]\) that contain \(v_{ui}\) value for each \(u\), \(i\) at all rounds. At each time \(t\), the algorithm chooses which items to offer to each user using these intervals and then determines a price for each offered item. The selection of the allocations is done according to the OFU principle (Dani et al., 2008; Abbasi-Yadkori, 2011) in order to ensure low regret. In particular, the offers are chosen by replacing each \(v_{ui}\) in problem (7) with \(b_{ui}^{t}\) to obtain

\[^{t}=*{arg\,max}_{^{t}^{t}}\ _{u}_{i}x_{ui}^{t}b_{ui}^{t}.\] (8)

Having decided on the offers, the next step is to decide on the price of the offered items. Similar to the challenge we encountered in selecting the offers, the selection of prices should also serve two different goals that are in tension with each other. On one hand, learning new information about users' valuations requires us to set prices to values from \((a_{ui},b_{ui})\). On the other hand, to ensure the offers are accepted and generate revenue, we would need to select prices lower than or equal to \(a_{ui}\). The crucial property that enables us to obtain \(O(NM(T))\) regret is that the function \(p v_{ui}-p\{p v_{ui}\}\) is asymmetric and decreases more slowly on the left than on the right as discussed in Kleinberg and Leighton (2003). Based on this observation, we design a pricing algorithm that operates by offering item \(i\) to user \(u\) at prices increasing with increments of \(_{ui}\). When the width of the interval \([a_{ui},b_{ui}]\) becomes smaller than the precision parameter \(_{ui}\), we set the new precision parameter as \(_{ui}^{2}\) and continue exploration with this smaller step size. However, as we show in the proof of Theorem 1, continuing to explore indefinitely would result in a regret linear in \(T\). To avoid this issue, the algorithm should stop exploration after achieving a certain level of precision and offer item \(i\) to user \(u\) at the maximum price that is certainly acceptable, namely \(a_{ui}\).

We provide a summary of this algorithm in Algorithm 1 and provide an upper bound for its regret in Theorem 1. As the lower bound provided in Theorem 2 shows, the regret of this algorithm is order-optimal up to smaller terms.

**Theorem 1** (Upper bound for Fixed valuations).: _Assuming fixed valuations in a market with \(N\) users, \(M\) items, and maximum load \(L\); the regret of Algorithm 1 in \(T\) rounds satisfies_

\[(T,) 2NM(LT)+1.\]

Proof.: See Appendix B.1. 

**Theorem 2** (Lower bound for Fixed valuations).: _If \(N\), \(M\), and \(L M\) are given parameters and \(\) is any randomized policy; there exist randomly generated market instances of \(N\) users with fixed valuations for \(M\) items and maximum load \(L\) such that the expected regret of \(\) in \(T\) rounds is \((NM(LT/NM))\)._

Proof.: See Appendix B.2. 

### Revenue maximization under random experiences

This section focuses on the case where the valuations of the users are given as the average of their past experience. We assume that each user obtains independent experiences about the items that they accept at every round. Then, the users form their valuation at each time as the average of their experiences so far. Formally, we make the following assumption.

**Assumption 2**.: _Whenever a user \(u\) accepts an item \(i\) at round \(t[T]\), it obtains an independent random experience \(z_{ui}^{t}\) with unknown cdf \(F_{ui}\) over \(\) with mean \(v_{ui}\). Then, the valuation of any user \(u\) for any item \(i\) at round \(t[T]\) is given as_

\[v_{ui}^{t}=^{t}}_{_{ui}^{t}}z_{ui}^{t},\] (9)

_where \(_{ui}^{t}=\{<t:i_{u}^{t}\}\) is the set of rounds before round \(t\) at which user \(u\) has accepted and experienced item \(i\), and \(m_{ui}^{t}:=|_{ui}^{t}|\) denotes the size of this set._

Under the random experiences model, we define the optimum strategy as the one that maximizes the revenue with complete information on possible user experiences \(z_{ui}^{t}\), and hence user valuations \(v_{ui}^{t}\) at all times. Therefore, the optimum revenue that can be obtained in \(T\) rounds is given by

\[=_{^{t}^{t}:t[T]\\ ^{t}_{+}^{M}:t[T]}\;_{t=1}^{T}_{u }_{i}x_{ui}^{t}p_{i}^{t}\{v_{ui}^{t } p_{i}^{t}\},\] (10)

which is a random variable due to the randomness in \(z_{ui}^{t}\) and hence the randomness in \(v_{ui}^{t}\). Next, we note that for any fixed sequence of offers \(\{^{t}|t[T]\}\), the objective is maximized at prices that satisfy \(p_{i}^{t}=_{u}x_{ui}^{t}v_{ui}\) at all \(t\). Therefore, we can write the optimum value for the objective as

\[=_{^{t}^{t}:t[T]}\;_{t=1}^{T}_ {u}_{i}x_{ui}^{t}v_{ui}^{t}.\] (11)

Note that in problem (11), we need to globally maximize over \(^{t}\) for all rounds \(t[T]\) because the values of \(v_{ui}^{t}\) depend on selections of \(^{t}\) at previous rounds. In order to deal with this dependency between the selections of \(^{t}\), in Lemma 9, we show that \(\) is not likely to be much larger than the sum of the mean valuations of the best offers at all rounds.

Next, we describe our algorithm in Algorithm 2. As the users accept and experience the items, their valuations converge to the mean of their experience distribution. Therefore, before extracting the mean valuation information from the users, the provider must ensure that users accept and sufficiently experience the items. Then, to extract this information, the algorithm occasionally asks for higher prices while taking the risk of rejected offers. However, since there are at most \((T)\) such learning rounds per user-item pair, it does not cause any significant loss in revenue.

Based on these observations, we establish an upper bound for our algorithm's regret in Theorem 3. Furthermore, as indicated by the regret lower bound presented in Theorem 4, our algorithm's regret is order-optimal up to smaller terms.

**Theorem 3** (Upper bound for random experiences).: _In any market of \(N\) users satisfying the random experience model given in Assumption 2 for \(M\) items and maximum load \(L\), with probability \(1-2\), the revenue regret of Algorithm 2 satisfies_

\[(T,)=O(+NM T).\]

Proof.: See Appendix C.2. 

**Theorem 4** (Lower bound for random experiences).: _If \(N\), \(M\), and \(L M\) are given parameters and \(\) is any randomized policy; there exist randomly generated market instances of \(N\) users satisfying the random experience model given in Assumption 2 for \(M\) items and maximum load \(L\) such that the expected regret of \(\) in \(T N\) rounds is \(()\)._

Proof.: See Appendix C.3. 

### Revenue maximization under random valuations

In this section, we consider the case where the valuations of the users are given as independent random variables drawn from distributions specific to each user and item. Formally,

**Assumption 3**.: _The valuation of any user \(u\) for any item \(i\) at round \(t[T]\) is given as an independent random variable \(v_{ui}^{t}\) with unknown cdf \(F_{ui}\) over \(\)._

Given foreknowledge of the distribution of valuations, but not of the individual valuations at different rounds, it is easy to see what the optimal pricing strategy would be. The expected revenue obtained from offering item \(i\) to user \(u\) at price \(p\) is given as \(_{ui}(p)=p(1-F_{ui}(p))\), which we call to be the revenue function. Since valuations are independent over time and their distribution is known, the individual responses provide no useful information about future realizations of the valuations. Therefore, the best price for offering item \(i\) to user \(u\) is given by

\[p_{ui}^{*}=*{arg\,max}_{p_{+}}p(1-F_{ui}(p)).\] (12)

Thus, letting \(_{ui}^{*}=_{ui}(p_{ui}^{*})\) denote the maximum expected revenue that can be obtained by offering item \(i\) to user \(u\), an optimum policy that knows the distribution of valuations can obtain revenue

\[=_{t=1}^{T}\{_{^{t}} _{u}_{i}x_{ui}_{ui}^{*}\}.\] (13)However, without the knowledge of the distributions, we are required to learn the expected revenue (i.e. rewards) at different prices via exploration. As previously done in the literature on pricing under random valuation models (Kleinberg and Leighton, 2003), we apply techniques from the literature on the multi-armed bandit problem to develop an algorithm with low regret guarantees. To do so, we quantize the set of possible prices by limiting the provider to strategies that only offer prices belonging to the set \(\{1/K,2/K,...1\}\) for suitably chosen \(K\). This brings us into a setting where each offer of item \(i\) to a user \(u\) at price \(k/K\) yields a revenue which is a random variable taking values in \(\), whose distribution depends on \((u,i,k)\), but the rewards for a given action are i.i.d. across the rounds. Therefore, offering item \(i\) to user \(u\) and \(k^{th}\) price level can be represented as pulling an arm that generates revenue with expectation \(_{uik}=_{ui}(k/K)\). In total, the expected revenue of any offering and pricing is

\[_{t=1}^{T}_{u}_{i}_{k=1}^{K}x_{ uik}^{t}_{uik},\] (14)

where \(x_{uik}^{t}\) are binary variables that denote whether user \(u\) is offered item \(i\) at \(k^{th}\) price level at round \(t\). Due to endowment and demand constraints, these variables at time \(t\) must satisfy conditions (1) \(_{i}_{k=1}^{K}x_{uik} d_{u}^{t}\), (2) \(_{u}_{k=1}^{K}x_{uik} e_{i}^{t}\), and (3) \(_{k=1}^{K}x_{uik} 1\) for all \(u,i\). In the literature on combinatorial multi-armed bandits, the standard regret bounds for UCB-based algorithms have an inverse dependency on the gap between the rewards of optimal and suboptimal arms (Kveton et al., 2015). To use similar techniques in proving regret bounds for our algorithm, we make the following hypothesis on the distributions of valuations, which translates directly into bounds on price sub-optimality gaps \(_{k}_{uik}-_{uik}\).

**Assumption 4**.: _The revenue function \(_{ui}(p)\) has a unique global maximum at \(p_{ui}^{*}(0,1)\), and \(_{ui}^{}(p_{ui}^{*})\) is defined and strictly negative._

Next, we show that our algorithm can attain the regret upper bound stated in Theorem 5. Moreover, as illustrated in Theorem 6, our algorithm's regret is order-optimal up to smaller terms.

**Theorem 5** (Upper bound for random valuations).: _In any market of \(N\) users satisfying the random valuation model given in Assumptions 3 and 4 for \(M\) items and maximum load \(L\), with probability \(1-\), the revenue regret of Algorithm 3 satisfies_

\[(T,)=( ).\]

Proof.: See Appendix D.2. 

**Theorem 6** (Lower bound for random valuations).: _If \(N,\)\(M\), and \(L M\) are given parameters and \(\) is any randomized policy; there exist randomly generated market instances of \(N\) users satisfying the random valuation model given in Assumptions 3 and 4 for \(M\) items and maximum load \(L\) such that the expected regret of \(\) is \(()\) in \(T\) rounds._

Proof.: See Appendix D.3.

Numerical experiments

In this section, we demonstrate the efficacy of our proposed algorithms through a numerical study. We provide our results in Figures 3 and 4. At each round \(t[T]\), the provider is endowed with each item \(i\) (i.e. \(i^{t}\)) independently with probability \(0.5\). On the other hand, each user \(u\) has a random demand \(d_{u}^{t}\) with uniform probability over \(\{0,1,2\}\). For the case of fixed valuations, we choose each \(v_{ui}\) independently from \((2,2)\). For other two models, we set each \(F_{ui}\) as the cdf of \((_{ui},_{ui})\) where \(_{ui}\) and \(_{ui}\) are uniformly and independently chosen over \(\).

## 6 Conclusion

Our study presents a comprehensive solution to maximizing expected revenue in a repeated interaction setting, where a provider seeks to sell multiple items to multiple users. By focusing on different valuation models, we design online learning algorithms that can infer user valuations and offer items to those who value them most to ensure approximately optimal revenue regret. The results of this study have important implications for online marketplaces and can help providers optimize pricing strategies and maximize revenue in a dynamic and uncertain environment.