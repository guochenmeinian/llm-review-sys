# Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge Distillation

Jiaming Lv, Haoyuan Yang, Peihua Li

Dalian University of Technology

ljm_vlg@mail.dlut.edu.cn, yanghaoyuan@mail.dlut.edu.cn, peihuali@dlut.edu.cn

 These authors contributed equally to this work and share first authorship. Corresponding author. The work was supported by National Natural Science Foundation of China (62471083, 61971086).

###### Abstract

Since pioneering work of Hinton et al., knowledge distillation based on Kullback-Leibler Divergence (KL-Div) has been predominant, and recently its variants have achieved compelling performance. However, KL-Div only compares probabilities of the corresponding category between the teacher and student while lacking a mechanism for cross-category comparison. Besides, KL-Div is problematic when applied to intermediate layers, as it cannot handle non-overlapping distributions and is unaware of geometry of the underlying manifold. To address these downsides, we propose a methodology of Wasserstein Distance (WD) based knowledge distillation. Specifically, we propose a logit distillation method called WKD-L based on discrete WD, which performs cross-category comparison of probabilities and thus can explicitly leverage rich interrelations among categories. Moreover, we introduce a feature distillation method called WKD-F, which uses a parametric method for modeling feature distributions and adopts continuous WD for transferring knowledge from intermediate layers. Comprehensive evaluations on image classification and object detection have shown (1) for _logit distillation_ WKD-L outperforms very strong KL-Div variants; (2) for _feature distillation_ WKD-F is superior to the KL-Div counterparts and state-of-the-art competitors. The source code is available at http://peihuali.org/WKD.

## 1 Introduction

Knowledge distillation (KD) aims to transfer knowledge from a high-performance teacher model with large capacity to a lightweight student model. In the past years, it has attracted ever increasing interest and made great advance in deep learning, enjoying widespread applications in visual recognition and object detection, among others . In their pioneering work, Hinton et al.  introduce Kullback-Leibler divergence (KL-Div) for knowledge distillation, where the prediction of category probabilities of the student is constrained to be similar to that of the teacher. Since then, KL-Div has been predominant in logit distillation and recently its variants [3; 4; 5] have achieved compelling performance. In addition, such logit distillation methods are complementary to many state-of-the-art methods that transfer knowledge from intermediate layers [6; 7; 8].

Despite the great success, KL-Div has two downsides that hinder fully transferring of the teacher's knowledge. First, KL-Div only compares the probabilities of the corresponding category between the teacher and student, lacking a mechanism to perform cross-category comparison. However, real-world categories exhibit varying degrees of visual resemblance, e.g., mammal species like dog and wolf look more similar to each other while visually very distinct from artifact such as car and bicycle. Deep neural networks (DNNs) can distinguish thousands of categories  and thus arewell-informed of such complex relations among categories, as shown in Figure 0(a). Unfortunately, due to its category-to-category nature, the classical KD  and its variants [3; 4; 5] are unable to explicitly utilize this rich cross-category knowledge.

Secondly, KL-Div is problematic for distilling knowledge from intermediate layers. Deep features of an image are generally high-dimensional and of small size, so being populated very sparsely in the feature space [10, Chap. 2]. This not only makes non-parametric density estimation (e.g., histogram) that KL-Div requires infeasible due to curse of dimensionality, but also leads to non-overlapping discrete distributions that KL-Div fails to deal with . One may turn to parametric, continuous methods (e.g., Gaussian) for modeling feature distributions. However, KL-Div and its variants have limited ability for measuring dis-similarity between continuous distributions, as it is not a metric  and is unaware of geometric structure of the underlying manifold .

The Wasserstein distance (WD) , also called Earth Mover's Distance (EMD) or optimal transport, has the potential to address the limitations of KL-Div. The WD between two probability distributions is generally defined as the minimal cost to transform one distribution to the other. Several works have made exploration by using WD for knowledge transfer from intermediate layers [15; 16]. Specifically, they measure the dis-similarity of a mini-batch of images between the teacher and student based on discrete WD, which concerns comparison across instances in a soft manner, failing to make use of relations across categories. Moreover, they mainly quest for non-parametric method for modeling distributions, behind in performance state-of-the-art KL-Div based methods.

To address these problems, we propose a methodology of Wasserstein distance based knowledge distillation, which we call WKD. This methodology is applicable to logits (WKD-L) as well as to intermediate layers (WKD-F) as shown in Figure 0(b). In WKD-L, we minimize the discrepancy between the predicted probabilities of the teacher and student using discrete WD for knowledge transfer. In this way, we perform cross-category comparison that effectively leverages interrelations (IRs) among categories, in stark contrast to category-to-category comparison in the classical KL-Div. We propose to use Centered Kernel Alignment (CKA) [17; 18] to quantify category IRs, which measures the similarity of features between any pair of categories.

For WKD-F, we introduce WD into intermediate layers to condense knowledge from features. Unlike the logits, there is no class probability involved in the intermediate layers. Therefore, we let the student directly match the feature distributions of the teacher. As the dimensions of DNN features are

Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; _refer to Section A.1 for details on this visualization_.

high, the non-parametric methods (e.g., histogram) are infeasible due to curse of dimensionality [10, Chap. 2], we choose parametric methods for modeling distributions. Specifically, we utilize one of the most widely used continuous distributions (i.e., Gaussian), which is of maximal entropy given 1st- and 2nd-moments estimated from features [19, Chap. 1]. WD between Gaussians can be computed in closed form and is a Riemannian metric on the underlying manifold .

We summarize our contributions in the following.

* We present a discrete WD based logit distillation method (WKD-L). It can leverage rich interrelations among classes via cross-category comparisons between predicted probabilities of the teacher and student, overcoming the downside of category-to-category KL divergence.
* We introduce continuous WD into intermediate layers for feature distillation (WKD-F). It can effectively leverage geometric structure of the Riemannian space of Gaussians, better than geometry-unaware KL-divergence.
* On both image classification and object detection tasks, WKD-L perform better than very strong KL-Div based logit distillation methods, while WKD-F is supervisor to the KL-Div counterparts and competitors of feature distillation. Their combination further improves the performance.

## 2 WD for Knowledge Transfer

Given a pre-trained, high-performance teacher model \(\), our task is to train a lightweight student model \(\) that can distill knowledge from the teacher. As such, supervisions of the student are from both the ground truth label with the cross entropy loss and from the teacher with distillation losses to be described in the next two sections.

### Discrete WD for Logit Distillation

Interrelations (IRs) among categories.As shown in Figures 0(a) and 0(a), real-world categories exhibit complex topological relations in the feature space. For instance, mammal species are nearer each other while being far away from artifact or food. Moreover, features of the same category cluster and form a distribution while neighboring categories have overlapping features and cannot be fully separated. As such, we propose to quantify category IRs based on CKA , which is a normalized Hilbert-Schmidt Independence Criterion (HSIC) that models statistical relations of two sets of features by mapping them into a Reproducing Kernel Hilbert Space (RKHS) .

Given a set of \(b\) training examples of category \(_{i}\), we compute a matrix \(_{i}\!\!^{u b}\) where the \(k\)-\(\) column indicates the feature of example \(k\) that is output from the DNN's penultimate layer. Then we compute a kernel matrix \(_{i}\!\!^{b b}\) with some positive definite kernel, e.g., a linear kernel for which \(_{i}\!=\!_{i}^{T}_{i}\) where \(T\) indicates matrix transpose. Besides the linear kernel, we can choose other kernels such as polynomial kernel and RBF kernel (_cf. Section A.1 for details_). The IR between \(_{i}\) and \(_{j}\) is defined as:

\[(_{i},_{j})=(_ {i},_{j})}{(_{i},_{i})} (_{j},_{j})}},\ (_{i}, _{j})=}(_{i} _{j}).\] (1)

Here \(\!=\!\!-\!^{T}\) is the centering matrix where \(\) indicates the identity matrix and \(\) indicates an all-one vector; \(\) indicates matrix trace. \((_{i},_{j})\) is invariant to isotropic scaling and orthogonal transformation. Note that the cost to compute the IRs can be neglected since we only need to compute them once beforehand. As the teacher is more knowledgeable, we compute category interrelations using the teacher model, which is indicated by \(^{}(_{i},_{j})\).

Besides CKA, cosine similarity between the prototypes of two categories can also be used to quantify IRs. In practice, the prototype of one category can be computed as the average of the features of the category's examples. Alternatively, the weight vectors associated with the softmax classifier of a DNN model can be regarded as prototypes of individual categories .

Loss function.Given an input image (instance), we let \(\!=\![z_{i}]\!\!^{n}\) be the corresponding logits of a DNN model where \(i\!\!S_{n}\!}{{=}}\!\{1,,n\}\) indicates the index of \(i\)-\(\) category. The predicted category probability \(=[p_{i}]\) is computed via the softmax function \(\) with a temperature \(\), i.e., \(p_{i}\,=\,()_{i}}{{=}}(z_{i}/)/\!_{j S_{n}}(z_{j}/)\). We denote by \(^{}\) and \(^{}\) the predicted category probabilities of the teacher and student models, respectively. The classical KD  is an instance-wise method, which measures the discrepancy between \(^{}\) and \(^{}\) given the same input image:

\[_{}(^{}\|^{})= _{i}p_{i}^{}p_{i}^{}/p_{i}^{ }.\] (2)

KL-Div (2) only compares predicted probabilities corresponding to the same category between the teacher and student, essentially short of a mechanism to perform cross-category comparison, as shown in Figure 2. Though during gradient backpropagation probability of one category affects probabilities of other categories due to the softmax function, _this implicit effect is insignificant and,_ above all, _cannot explicitly exploit rich knowledge of pairwise interrelations_ as described in (1).

In contrast to KL-Div, WD performs cross-category comparison and thus naturally makes use of category interrelations, as shown in Figure 0(b) (left). We formulate discrete WD as an entropy regularized linear programming :

\[_{}(^{},^{ })=_{q_{ij}}_{i,j}c_{ij}q_{ij}+ q_{ij} q_{ij}\] (3) \[\ \ q_{ij} 0,\ _{j}q_{ij}=p_{i}^{ },\ _{i}q_{ij}=p_{j}^{},\ i,j S_{n},\]

where \(c_{ij}\) and \(q_{ij}\) respectively indicate transport cost per mass and the transport amount while moving probability mass from \(p_{i}^{}\) to \(p_{j}^{}\), \(\) is a regularization parameter. We define the cost \(c_{ij}\) by converting the similarity measure IRs to a distance measure according to the commonly used Gaussian kernel [10, Chap. 6], i.e., \(c_{ij}=1-(-(1-^{}(_{i},_ {j})))\), where \(\) is a parameter that can control the degree of sharpening of IR. _The smaller \(^{}(_{i},_{j})\) in the feature space, the less cost is needed for transport between the two categories._ As such, the loss function of WKD-L is

\[}_{}= _{}(^{},^{ }).\] (4)

Recent work  discloses target probability (i.e., the probability of the target category) and the non-target ones play different roles: the former concerns the difficulty of training examples while the latter containing prominent "dark knowledge". It has been shown that this separation is helpful to balance their roles and improves greatly over the classical KD [3; 4]. Inspired by them, we also consider a similar separation strategy. Let \(t\) be index of target category and \(_{t}^{}=z_{i}^{}^{ n-1},i S_{n}\{t\}\) be the teacher's logits of non-target categories. We normalize \(_{t}^{}\) as previously, obtaining the teacher's non-target probabilities \(_{i}^{}=p_{i}^{}\). In this case, our loss functions consist of two terms:

\[_{}= _{}(_{i}^{}, _{i}^{})+_{},\ \ \ _{}=-(^{})_{t}( ^{})_{t},\] (5)

where \(\) is the weight.

### Continuous WD for Feature Distillation

As the features output from intermediate layers of DNN are of high dimension and small size, the non-parametric methods, e.g., histogram and kernel density estimation, are infeasible. Therefore, we use one of the widely used parametric methods (i.e., Gaussian) for distribution modeling.

**Feature distribution modeling.** Given an input image, let us consider feature maps output by some intermediate layer of a DNN model, whose spatial height, width and channel number are \(h\), \(w\) and \(l\), respectively. We reshape the feature maps to a matrix \(^{l m}\) where \(m=h w\) and the \(i\)-\(\) column \(_{i}^{l}\) indicates a spatial feature. For these features, we estimate the 1st-moment \(=_{i}_{i}\) and the 2nd-moment \(=_{i}(_{i}-)( _{i}-)^{T}\). We model feature distribution of the input image by a Gaussian with mean vector \(\) and covariance matrix \(\) as its parameters:

\[(,)= |^{1/2}}-( -)^{T}^{-1}(- ),\] (6)

where \(||\) indicates matrix determinant.

Figure 2: KL-Div cannot perform cross-category comparison. Compare to WD in Figure 0(b) (left).

We estimate Gaussian distribution of the teacher directly from its backbone network. For the student, as in previous arts [24; 25; 26], a projector is used to transform the features so that they are compatible in size with the features of the teacher. Then the transformed features produced by the projector are used to compute the student's distribution. We select Gaussian for distribution modeling as it is of maximal entropy for given the 1st- and 2nd-moments [19, Chap. 1] and has a closed form WD that is a Riemannian metric .

Loss Function.Let Gaussian \(^{}^{}(^{ },^{})\) be feature distribution of the teacher. Similarly, we denote by \(^{}\) the student's distribution. The continuous WD between the two Gaussians is defined as

\[_{}(^{},^{ })=_{}_{^{}}_{^{ }}^{}-^{}^{2} q(^{},^{})d^{}d ^{},\] (7)

where \(^{}\) and \(^{}\) are Gaussian variables and \(\) indicates Euclidean distance; the joint distribution \(q\) is constrained to have marginals \(^{}\) and \(^{}\). Minimization of Eq. (7) leads to the following closed form distance :

\[_{}(^{},^{ })=_{}(^{},^{ })+_{}(^{}, ^{}).\] (8)

Here \(_{}(^{},^{})= ^{}-^{}^{2}\) and \(_{}(^{},^{} )=(^{}+^{}-2((^{})^{}^{}(^{ })^{})^{})\) where superscript \(\) indicates matrix square root. As the covariance matrices estimated from high-dimensional features are often ill-conditioned , we add a small positive number (\(1\)-\(5\)) to the diagonals. We also consider diagonal covariance matrices, for which we have \(_{}(^{},^{} )=^{}-^{}^{2}\), where \(^{}\) is a vector of standard variances formed by the square roots of the diagonals of \(^{}\). We later compare Gaussian (Full) and Gaussian (Diag) which have full and diagonal covariance matrices, respectively. To balance role of the mean and covariance, we introduce a mean-cov ratio \(\) and define the loss as

\[_{}\!=\!_{}( ^{},^{})\!+\!_{}( ^{},^{}).\] (9)

We can use the strategy of spatial pyramid pooling [28; 29; 6] to enhance representation ability. Specifically, we partition the feature maps into a \(k k\) spatial grid, compute a Gaussian for each cell of the grid and then match per cell the Gaussians of the teacher and student.

KL-Div  and symmetric KL-Div (i.e., Jeffreys divergence) , both having closed form expressions for Gaussians , can be used for knowledge transfer. However, they are not metrics , unaware of the geometry of the space of Gaussians  that is a Riemannian space. Conversely, \(_{}\) is a Riemannian metric that measures the intrinsic distance . Note that G\({}^{2}\)DeNet  proposes a metric between Gaussians that leverages the geometry based on Lie group, which can be used to define distillation loss. Besides Gaussian, one can also use Laplace and exponential distributions for modeling feature distributions. Finally, though histogram or kernel density estimation are infeasible, one can still model feature distribution with probability mass function (PMF) and accordingly use discrete WD to define the distillation loss. _Details on these methods can be found in Section A.2._

## 3 Related Works

We summarize KD methods related to ours and show their connections and differences in Table 1.

KL-Div based knowledge distillation.Zhao et al.  disclose the classical KD loss  is a coupled formulation that limits its performance, and thereby propose a decoupled formulation (DKD) that consists of a binary logit loss for the target category and a multi-class logit loss for all non-target categories. Yang et al.  propose a normalized KD (NKD) method, which decomposes the classical KD loss into a combination of the target loss (like the widely used cross-entropy loss) and the loss of normalized non-target predictions. WTTM  introduces Renyi entropy regularizer without temperature scaling for student. In spite of competitive performance, they cannot explicitly exploit relations among categories. By contrast, our Wasserstein distance (WD) based method can perform cross-category comparison and thus exploit rich category interrelations.

WD based knowledge distillation.The existing KD methods founded on WD [15; 16] mainly concern cross-instance matching for feature distillation, as shown in Figure 3 (left). Chen et al.  propose a Wasserstein Contrastive Representation Distillation (WCoRD) framework which involvesa global and a local contrastive loss. The former loss minimizes _mutual information_ (via dual form of WD) between the distributions of the teacher and student; the latter loss minimizes _Wasserstein distance_ for the penultimate layer only, where the set of features of the mini-batch images are matched between the teacher and student. Lohit et al.  independently propose a similar cross-instance matching method called EMD\(+\)IPOT, in which knowledge is transferred from all intermediate layers and discrete WD is computed via an inexact proximal optimal transport algorithm . The differences of our work from them are twofold: (1) They fail to leverage category interrelations which our WKD-L can make full use of; (2) They are concerned with cross-instance matching based on discrete WD, while our WKD-F involves instance-wise matching with continuous WD.

**Other arts based on statistical modeling.** NST  is among the first to formalize feature distillation as a distribution matching problem, in which the student mimics the distributions of the teacher based on Maximum Mean Discrepancy (MMD). They show that the 2nd-order polynomial kernel performs best among the candidate kernels of MMD, and that the activation-based attention transfer (AT)  is a special case of NST. Yang et al.  propose a novel loss function, which transfers the statistics learned by the student back to the teacher based on adaptive instance normalization. Liu et al.  propose inter-channel correlations (ICKD-C) to model feature diversity and homology for better knowledge transfer. Both NST and ICKD-C can be regarded as Frobenius norm based distribution modeling via 2nd-moment along spatial- and channel-dimension, respectively, as shown in Figure 3 (right). However, they fail to utilize the geometric structure of the 2nd-moment matrices, which are symmetric positive definite (SPD) and form a Riemannian space [38; 39]. Ahn et al.  introduce variational information distillation (VID) based on mutual information. VID assumes that feature distributions are Gaussians, and its loss boils down to mean square loss (i.e., FitNet ) if Gaussians are further assumed to have unit variance.

## 4 Experiments

We evaluate WKD for image classification on ImageNet  and CIFAR-100 . Also, we evaluate the effectiveness of WKD on self-knowledge distillation (Self-KD). Further, we extend WKD to object detection and conduct experiment on MS-COCO . We train and test models with PyTorch framework , using a PC with an Intel Core i9-13900K CPU and GeForce RTX 4090 GPUs.

### Experiment Setting

**Image classification.** ImageNet  contains 1,000 categories with 1.28M images for training, 50K images for validation and 100K for testing. In accordance with , we train the models for 100 epochs using SGD optimizer with a batch size of 256, a momentum of 0.9 and a weight decay of \(1\). The initial learning rate is 0.1, divided by 10 at the \(30\), \(60\) and \(90\) epochs, respectively. We use random resized crop and random horizontal clip for data augmentation. For WKD-L, we use POT library  for solving discrete WD with \(\!=\!0.05\) and 9 iterations. For WKD-F, the projector has a bottleneck structure, i.e., a \(1\!\!1\) Convolution (Conv) and a \(3\!\!3\) Conv both with 256 filters followed by a \(1\!\!1\) Conv with BN and ReLU to match the size of teacher's feature maps.

   &  &  \\   & Distribution & Dis-similarity & Category & Distribution & Dis-similarity & Riemannian Metric \\  KD  &  &  & \)} &  &  \\  & WKD  & & & & & \\  & WTM  & & & & & \\   WCoRD  &  & Mutual & \)} &  & Wasserstein &  \\  & & Information & & Distance & & \\   EMD-IPOT &  &  & \)} &  &  \\   & & & & Distance & & \\   WKD (ours) & & & & & \\    & & & & & \\  NST  &  & \)} &  &  &  \\  & & & & & \\    & & & & & \\  ICKD-C  &  & \)} &  & Channel &  \\  & & & & & \\    & & & & & \\    & & & & & \\  VID  &  & Mutual & \)} &  &  \\  & & Information & & & \\  

Table 1: Comparison with related works.

CIFAR-100  contains 60K images of 32 \(\) 32 pixels from 100 categories with 50K for training and 10K for testing. Following the setting of OFA , we conduct experiments across Convolutional Neural Networks (CNNs) and vision transformers. _Additional experiments within CNN architectures are given in Table 11 of Section C.6._ The images are upsampled to the resolution of 224x224. All models are trained for 300 epochs with a batch size of 512 and a cosine annealing schedule. For CNN-based students, we use SGD optimizer with an initial learning rate of 2.5e-2 and a weight decay of 2e-3. For transformer-based students, we use AdamW optimizer with an initial learning rate of 2.5e-4 and a weight decay of 2e-3.

**Object detection.** MS-COCO  is a commonly used benchmark for object detection that contains 80 categories. Following the common practice, we use standard split of COCO 2017 with 118K images for training and 5K images for validation. As in ReviewKD , we employ the framework of Faster-RCNN  with Feature Pyramid Network (FPN)  on Detectron2 platform . As in previous arts [50; 29; 51], we use the detection models officially trained and released as the teachers, while training the student models whose backbones are initialized with the weights pre-trained on ImageNet. The student networks are trained in 180K iterations with a batch size of 8; the initial learning rate is 0.01, decayed by a factor of 0.1 at 120K and 160K iterations.

### Dissection of WD-based Knowledge Distillation

We analyze key components of WKD-L and WKD-F on ImageNet. We adopt ResNet34 as a teacher and ResNet18 as a student (i.e., setting (a)), whose Top-1 Accuracies are 73.31% and 69.75%, respectively. _See Section C.1 for analysis on hyper-parameters, e.g., temperature and weight._

#### 4.2.1 Ablation of WKD-L

**How WD performs against KL-Div?** We compare WD to KL-Div with (w/) and without (w/o) separation of target probability in Table 1(a). For the case of without separation, WD (w/o) improves over KL-Div (w/o) by 1.0%; for the case of with separation, WD (w/) outperforms non-trivially KL-Div (w/) based DKD and NKD. The comparison above clearly shows that (1) WD performs better than KL-Div in both cases and (2) the separation also matters for WD. As such, WD with separation of target probability is used across the paper.

**How to model category interrelations (IRs)?** Table 1(b) compares two methods for IR modeling, i.e., CKA and cosine. For the former, we assess different kernels while for the latter we evaluate prototypes with classifier weights or class centroids. We note all WD-based methods perform much better than the baseline of KL-Div. Overall, IR with CKA performs better than IR with cosine, indicating it has better capability to represent similarity among categories. For IR with CKA, RBF kernel is better than polynomial kernel, while linear kernel is the best so is used throughout.

#### 4.2.2 Ablation of WKD-F

Full covariance matrix or diagonal one?As shown in Table 2(a) (3rd and 4th rows), for Gaussian (Full), WD performs better than G\({}^{2}\)DeNet , which suggests that the former metric is more appropriate for feature distillation. When using WD, Gaussian (Diag) (5th row) produces higher accuracy than Gaussian (Full). We conjecture the reason is that high dimensionality of features makes estimation of full covariance matrices not robust ; in contrast, for Gaussians (Diag), we only need to estimate 1D variances for univariate data of single dimension. Besides, Gaussian (Diag) is much more efficient than Gaussian (Full). So we use Gaussians (Diag) throughout the paper.

Table 2: Ablation analysis of WKD-L for image classification (Acc, %) on ImageNet.

How to model distributions?In Table 2(a), we compare different parametric methods for knowledge distillation, including Gaussian, Laplace, exponential distribution, as well as separate \(1\)st-moment and \(2\)nd\(-\)moment. Note that NST  adopts spatial 1st- and 2nd-moment while ICKD-C  uses channel \(2\)nd-moment. Besides, we compare to non-parametric method based on PMF.

_For Gaussians (Diag)_, KL-Div and symmetric (Sym) KL-Div produce similar accuracies which are both lower than WD. The reason may be that KL-related divergences are not intrinsic distances, failing to exploit geometric structure of the manifold of Gaussians. For _statistical moments_, we see that channel-wise moments perform better than spatial-wise ones. For channel-wise representations, 1st-moment outperforms 2nd-moment, suggesting that the mean plays a more important role. At last, the _non-parametric_ method of PMF underperforms the parametric method of Gaussians.

Besides Gaussian (Diag), We can also use univariate _Laplace or exponential functions_ to model distributions of each component of features. For them KL-Div can be computed in closed-form  but WD is an unsolved problem. When using KL-Div, Gaussian (Diag) achieves better performance than both Laplace and exponential distributions, highlighting Gaussian as a more suitable option among these parametric alternatives. Further, the Gaussian (Diag) combined with WD yields superior performance compared to KL-Div, suggesting advantage of the Riemannian metric.

Instance-wise or cross-instance matching?Our WKD-F is an instance-wise matching method based on continuous WD, while WCoRD and EMD+IPOT concern cross-instance matching for a mini-batch of images based on discrete WD. As seen in Table 2(b), WCoRD produces an accuracy much higher than EMD+IPOT, which may be attributed to its extra global contrast loss based on mutual information; WKD-F outperforms the two counterparts by a large margin of 1.0%, which suggests the advantage of our strategy. Note that our WKD-F runs remarkably faster than the two counterparts that rely on optimization algorithm to solve discrete WD.

Distillation position and grid scheme.We evaluate in Table 2(c) the effect of position at which we perform distribution matching and that of different grid schemes. From the 3rd and 4th rows, we see that the last stage of Conv_5x performs much better than Conv_4x, indicating high-level features are more suitable for knowledge transfer. Comparing the 4th and 5th rows, we see \(2 2\) grid does not improve over \(1 1\) grid. Lastly, combination of features of Conv_4x and Conv_5x bring no further gains. Therefore, we use features of Conv_5x and \(1 1\) grid for classification on ImageNet.

### Image Classification on ImageNet

Table 4 compares to existing works in two settings. Setting (a) involves homogeneous architecture, where the teacher and student networks are ResNet34 and ResNet18 , respectively; setting (b) concerns heterogeneous architecture, in which we set the teacher as ResNet50 and the student as MobileNetV1 . _Refer to Section C.2 for hyper-parameters in Settings (a) and (b)_.

For logit distillation, we compare our WKD-L with KD , DKD , NKD , CTKD  and WTTM . Our WKD-L performs better than the classical KD and all its variants in both settings. Particularly, our WKD-L outperforms WTTM, a very strong variant of KD, which additionally introduces a sample-adaptive weighting method. This suggests Wasserstein distance that performs

Table 3: Ablation analysis of WKD-F for image classification (Acc, %) on ImageNet.

cross-category comparison is superior to category-to-category KL-Div. For feature distillation, we compare to FitNet , CRD , ReviewKD  and CAT . Our WKD-F improves ReviewKD, previous top-performer, by \(\)\(0.9\%\) in the setting (a) and \(\)\(0.6\%\) in the setting (b) in terms of top-1 accuracy; this comparison indicates that, for knowledge transfer, matching of Gaussian distributions is better than matching of features. Finally, combination of WKD-L and WKD-F further improves and outperforms strong competitors, including CRD+KD , DPK , FCFD  and KD-Zero . _More results of combination about WKD-L or WKD-F can be found in Table 9_.

Table 5 compares in Setting (a) latency of different methods with a batch size of 256 using a GeForce RTX 4090. For logit distillation, the latency of WKD-L is \(\)1.3 times larger than KL-Div based methods, due to the optimization procedure to solve discrete WD. WKD-F has a latency on par with KL-Div based methods, while running \(\)1.6 faster than ReviewKD and \(\)1.2 faster than EMD+IPOT; this is because WKD-F only involves mean vectors and variance vectors, leading to negligibly additional cost. Finally, combination of WKD-L and WKD-F has larger latency but better performance than ICKD-C, and meanwhile is more efficient than state-of-the-art FCFD.

### Image Classification on CIFAR-100

We evaluate WKD in the settings where the teacher is a CNN and the student is a Transformer or vice versa. We use CNN models including ResNet (RN) , MobileNetV2 (MNV2)  and ConvNeXt , as well as vision transformers that involve ViT , DeiT , and Swin Transformer . _The setting of hyper-parameters can be found in Section C.5_.

For logit distillation, we compare WKD-L to KD , DKD , DIST  and OFA . As shown in Table 6, WKD-L consistently outperforms state-of-the-art OFA for transferring knowledge from Transformers to CNNs or vice versa. For feature distillation, we compare to FitNet , CC , RKD  and CRD . WKD-F ranks first across the board; notably, it significantly outperforms the previous best competitors by 2.1%-3.4% in four out of five settings. We attribute superiority of WKD-F to our distribution modeling and matching strategies, i.e., Gaussians and Wasserstein distance. We posit that, for knowledge transfer across CNNs and Transformers that yield very distinct features , WKD-F is more suitable than raw feature comparisons as in FitNet and CRD.

   &  &  &  \\   & & KD & DKD & DIST & OFA & WKD-L & FitNet & CC & RKD & CKD & WKD-F \\  & &  &  &  &  & (ours) &  &  &  & (ours) \\   \\  Swin-T (89.26) & RN18 (74.01) & 78.74 & 80.26 & 77.75 & 80.54 & **81.42\(\)**0.22 & 78.87 & 74.19 & 74.11 & 77.63 & **81.57\(\)**0.12 \\ ViT-S (92.04) & RN18 (74.01) & 77.26 & 78.10 & 76.49 & 80.15 & **80.81\(\)**0.21 & 77.71 & 74.26 & 73.72 & 76.60 & **81.12\(\)**0.24 \\ ViT-S (92.04) & MNV2 (73.68) & 72.77 & 69.80 & 72.54 & 78.45 & **79.04\(\)**0.05 & 73.54 & 70.67 & 68.46 & 78.14 & **79.11\(\)**0.07 \\   \\  ConvNeXt-T (88.41) & DeiT-F (168.00) & 72.99 & 74.60 & 73.55 & 75.76 & **76.11\(\)**0.18 & 60.78 & 68.01 & 69.79 & 65.94 & **73.27\(\)**0.22 \\ ConvNeXt-T (88.41) & Swin-P (72.63) & 76.44 & 76.80 & 76.41 & 78.32 & **78.94\(\)**0.17 & 24.06 & 72.63 & 71.73 & 67.09 & **74.80\(\)**0.13 \\  

Table 6: Image classification results (Top-1 Acc, %) on CIFAR-100 across CNNs and Transformers.

   &  &  &  &  &  \\   & & KD & DKD & NKD & CTRD & WITW & WKD-L &  &  &  &  &  &  &  \\  & &  &  &  &  & (ours) &  &  & KD-D &  &  & (ours) & KD-D & PKD & KD-F &  \\  & & &  &  &  &  & (ours) &  &  & KD-D &  &  & (ours) & KD-D & PKD & KD-F &  \\   & Top-1 & 73.31 & 69.75 & 10.3 & 71.0 & 71.96 & 71.51 & 21.9 & **72.40** & 70.53 & 71.17 & 71.61 & 71.26 & **72.70** & 71.38 & 72.51 & 72.25 & 72.17 & **72.76** \\  & Top-5 & 91.42 & 89.07 & 90.05 & 90.41 & – & 90.47 & – & **90.75** & 89.87 & 90.13 & 90.51 & 90.45 & **91.00** & 90.49 & 90.77 & 90.71 & 90.46 & **91.08** \\   & Top-1 & 76.16 & 68.87 & 70.50 & 72.05 & 72.58 & – & 73.09 & **73.17** & 70.26 & 71.37 & 72.56 & 72.24 & **73.12** & – & – & 73.26 & 73.26 & 73.02 & **73.69** \\  & Top-5 & 92.86 & 88.76 & 98.80 & 91.05 & – & – & – & **91.32** & 90.14 & 90.41 & 91.00 & 9

### Self-Knowledge Distillation on ImageNet

We implement our WKD in the framework of Born-Again Network (BAN)  for self-knowledge distillation (Self-KD). Specifically, we first train an initial model \(S_{0}\) using ground truth labels. Then we distill, using WKD-L, the knowledge of \(S_{0}\) into a student model \(S_{1}\) with the same architecture as \(S_{0}\). For the sake of simplicity, we do not perform multi-generation distillation, such as training a student model \(S_{2}\) with \(S_{1}\) as the teacher, etc.

We conduct experiments with ResNet18 on ImageNet, where the hyper-parameters are consistent with those in Setting (a). As shown in Table 7, BAN achieves competitive accuracy that is comparable to state-of-the-art results. Our method achieves the best result, outperforming BAN by \(\) 0.9% in Top-1 accuracy and the second-best (i.e., USKD) by 0.6%. This comparison demonstrates that our WKD can well generalize to self-knowledge distillation.

### Object Detection on MS-COCO

We extend WKD to object detection in the framework of Faster-RCNN . For WKD-L, we use the classification branch in the detection head for logit distillation. For WKD-F, we transfer knowledge from features straightly fed to the classification branch, i.e., features output by the RoIAlign layer, and choose a 4\(\)4 spatial grid for computing Gaussians. _Implementation details, ablation of key components, and extra experiments are given in Section E of Appendix_.

We compare with existing methods in two settings, as shown in Table 8. In RN101\(\)RN18 setting, the teacher is ResNet101 and the student is ResNet18; in RN50\(\)MNV2, the teacher and student are ResNet50 and MobileNetV2 , respectively. For logit distillation, our WKD-L significantly outperforms the classical KD  and is slightly better than DKD . For feature distillation, we compare with FitNet, FGFI , ICD  and ReviewKD ; our WKD-F improves ReviewKD, the previous top feature distillation performer, by a non-trivial margin in both settings. Finally, by combining WKD-L and WKD-F, we achieve performance better than DKD+ReviewKD . When additional bounding-box regression is used for knowledge transfer, our WKD-L+WKD-F improves further, outperforming previous state-of-the-art FCFD .

## 5 Conclusion

The Wasserstein distance (WD) has shown evident advantages over KL-Div in several fields such as generative models . However, in knowledge distillation, KL-Div is still dominant and it is unclear whether WD will outperform. We argue that earlier attempts on knowledge distillation based on WD fail to unleash the potential of this metric. Hence, we propose a novel methodology of WD-based knowledge distillation, which can transfer knowledge from both logits and features. Extensive experiments have demonstrated that discrete WD is a very promising alternative of predominant KL-Div in logit distillation, and that continuous WD can achieve compelling performance for transferring knowledge from intermediate layers. Nevertheless, our methods have limitations. Specifically, WKD-L is more expensive than KL-Div based logit distillation methods, while WKD-F assumes features follow Gaussian distribution. _We refer to Section F in Appendix for detailed discussion on limitations and future research._ Finally, we hope our work can shed light on the promise of WD and inspire further interest in this metric in knowledge distillation.

   Method & Self-KD & Dis-similarity & Top-1 \\  Standard train & \(\) & NA & 69.75 \\  T-KD  & ✓ & KL-Div & 70.14 \\ FRSKD  & ✓ & KL-Div & 70.17 \\ Zipfy KD  & ✓ & KL-Div & 70.30 \\ UNKD  & ✓ & KL-Div & 70.75 \\ BAN  & ✓ & KL-Div & 70.50 \\ WKD-L & ✓ & WD & **71.35** \\   

Table 7: Self-KD results (Acc, %) on ImageNet with ResNet18.

    &  &  &  \\  & & mAP & AP\({}_{0}\) & AP\({}_{5}\) & mAP & AP\({}_{0}\) & AP\({}_{5}\) \\   & Teacher & 42.04 & 62.48 & 45.88 & 40.22 & 61.02 & 43.81 \\  & Student & 33.26 & 53.61 & 35.26 & 29.47 & 48.87 & 30.90 \\   & KD  & 33.97 & 54.66 & 36.62 & 30.13 & 50.28 & 31.35 \\  & DKD  & 35.05 & 56.60 & 37.54 & 32.34 & 53.77 & 34.01 \\  & WKD-L (Ours) & **35.42** & **46.53** & **37.91** & **32.48** & **53.85** & **34.21** \\   & FitNet  & 34.13 & 51.46 & 36.17 & 31.02 & 40.98 & 31.69 \\  & FGFI  & 35.44 & 55.51 & 38.17 & 31.16 & 50.68 & 32.92 \\   & ICD  & 35.90 & 56.02 & 38.75 & 32.88 & 52.56 & 34.93 \\   & ReviewKD  & 36.75 & 36.72 & 34.00 & 33.71 & 53.15 & 36.13 \\   & WKD-F (Ours) & **37.21** & **57.32** & **40.15** & **34.47** & **54.67** & **36.85** \\   & DKD+ & 37.01 & 57.53 & 39.85 & 34.35 & 54.39 & 36.61 \\   & WKD-L+ & WKD-L+ (Ours) & **37.49** & **57.56** & **40.39** & **34.80** & **58.27** & **37.28** \\   & FCFD  & 37.37 & 57.60 & 40.34 & 34.97 & 55.04 & 37.51 \\   & WKD-L+ (Ours) & **37.79** & **57.95** & **41.08** & **35.48** & **55.21** & **38.45** \\   

Table 8: Object detection results on MS-COCO. \({}^{}\)Additional _bounding-box regression_ is used.