# VinePPO: Accurate Credit Assignment in RL for LLM Mathematical Reasoning

Amirhossein Kazemnejad\({}^{}\)\({}^{1}\), Milad Aghajohari\({}^{}\)\({}^{1}\), Eva Portelance\({}^{1,6}\),

**Alessandro Sordoni\({}^{1,2}\), Siva Reddy\({}^{1,3,4}\), Aaron Courville\({}^{}\)\({}^{1,4,5}\), Nicolas Le Roux\({}^{}\)\({}^{1,4}\)**

\({}^{1}\)Mila \({}^{2}\)Microsoft Research \({}^{3}\)McGill University

\({}^{4}\)Canada CIFAR AI Chair \({}^{5}\)Universite de Montreal \({}^{6}\)HEC Montreal

{amirhossein.kazemnejad,aghajohm}@mila.quebec

 Equal contribution. \({}^{}\) Equal advising.

###### Abstract

Large language models (LLMs) are increasingly required to solve complex reasoning tasks, like mathematical problems, that involve multiple reasoning steps before feedback is received. Effectively identifying and prioritizing key steps by accurately assigning credit to these intermediate steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm for finetuning LLMs, addresses the credit assignment problem by employing value networks to predict the expected cumulative rewards of intermediate states. In this work, we identify significant limitations with this value estimation method. To address this, we propose VinePPO that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates of the intermediate values. VinePPO consistently outperforms standard PPO, doing so more efficiently and with lower divergence from the reference model. Our findings underscore the critical importance of accurate credit assignment in LLM post-training and present a simple, yet effective solution.

## 1 Introduction

Large language models (LLMs) are increasingly employed in tasks requiring complex reasoning, such as solving mathematical problems (Trinh et al., 2024; OpenAI, 2024). In these settings, LLMs often engage in extended reasoning chains and perform numerous actions. Prioritizing steps that lead to correct solutions while downplaying erroneous ones during finetuning is essential for improving the performance and reducing unnecessary updates. This is particularly important as most reasoning steps generated by a model often do not impact its likelihood of solving the problem (Fig. 2).

This issue is known as the _credit assignment problem_ in reinforcement learning (RL, Sutton and Barto 1998). Proximal Policy Optimization (PPO) (Schulman et al., 2017; Ouyang et al., 2022), the state-of-the-art algorithm for RL tuning of LLMs (Xu et al., 2024; Ivison et al., 2024; Shao et al., 2024), is a variant of actor-critic methods that utilizes a value network (_critic_) to handle credit assignment (Bai et al., 2022, 2023; Havrilla et al., 2024). The value network is a separate model (the same size as and initialized from a pretrained checkpoint of the LLM) that learns to estimate expected cumulative future reward (value) of intermediate actions during training. PPO then uses the predicted values to measure the advantage of each action and update the model accordingly. For example, in Fig. 2, an ideal value network would assign a low value to \(s_{0}\), where the model initially struggles, and a higher value to \(s_{2}\) and beyond, where a critical action led to solving the problem.

Accurately predicting rewards from _a partial and incomplete response_ requires the value network to grasp the space of correct solutions and predict the model's future behavior - both of which arechallenging. There are hints in the literature that standard PPO implementations for LLM finetuning have inaccurate value estimations. Ahmadian et al. (2024) and Luong et al. (2024) demonstrate that value networks often serve best as just a baseline in policy gradient2. Shao et al. (2024) shows that the value network can be replaced by averaging rewards of a group of responses to a given problem, without degradation in performance.

As estimation errors can significantly hamper model convergence and performance (Sutton et al., 1999; Greensmith et al., 2001), it is crucial to ask: _how accurately do value networks perform in practice during LLM finetuning?_ While recent studies (Hwang et al., 2024; Setlur et al., 2024) have begun to highlight the importance of identifying early reasoning errors and incorporating these as training signals in "RL-free" approaches (Rafailov et al., 2023), to what extend the accuracy of credit assignment plays a role in RL tuning of LLMs remains an open question.

In this work, we evaluate the standard PPO pipeline in mathematical reasoning tasks across various model sizes and find that value networks consistently provide inaccurate estimates and a sub-optimal training signal for finetuning. To address this, we propose VinePPO. Instead of relying on value networks, VinePPO computes _unbiased_ estimates by resetting the environment to intermediate states and performing independent Monte Carlo (MC) rollouts to calculate the average return of individual steps. This approach takes advantage of a special property of the language environment--the ability to easily reset to any intermediate state of a trajectory (Schulman et al., 2015). Not only does it removes the need for large, memory-intensive value networks, VinePPO also outperforms standard PPO and other baselines such as RestEM (Singh et al., 2023)(Fig. 1). VinePPO is also able to match PPO's final accuracy in fewer iterations, requiring less wall-clock time (Fig. 4), and achieving a lower KL divergence (Fig. G.3) from the base model. These findings highlight the importance of accurate credit assignment in RL post-training and position VinePPO as an effective alternative to value networks.

Figure 1: VinePPO outperforms standard PPO and other baselines on the MATH dataset, while also exhibiting scalability across different model sizes. The figure shows Pass@1 performance.

Figure 2: **(Left) A response generated by the model. The notation \((|s_{ t})\) represents the estimated probability of successfully solving the problem at step \(t\), based on nine model rollouts. In this example, only step \(s_{2}\) is critical; after this, the model completes the solution correctly. (Right) Illustration of estimating the value of a state within the trajectory.**

## 2 Advantage Estimation with Monte Carlo

We build on PPO (Schulman et al., 2017; Ouyang et al., 2022), for which we provide an extensive background in Appendices B and I. VinePPO only modifies the way advantages are estimated. We start by estimating the true value function \(V(s_{t})\). Instead of relying on a value network, for any intermediate state \(s_{t}\), we sample \(K\) independent trajectories starting from \(s_{t}\). The average return across these trajectories serves as the value estimate:

\[_{}(s_{t})_{k=1}^{K}R(_{k}), _{1},,_{K}( s_{t}).\] (1)

where \(_{k}\) is an independent continuation sampled from the model, starting from \(s_{t}\) and \(R()\) is the return over the completed trajectory. This is an MC estimate of the value function \(V(s_{t})=[R() s_{0}=s_{t}]\). Once the values \(_{}(s_{t})\) are computed, we compute the advantages with:

\[_{}(s_{t},a_{t}) r(s_{t},a_{t})+_{ }(s_{t+1})-_{}(s_{t}),\] (2)

where \(r()\) is the step-wise reward (in practice, equal to zero unless at final step). Note that for any \(K 1\), the policy gradient computed using the advantage estimator \(_{}\) is an unbiased estimate of the gradient of expected return.

In essence, VinePPO only alters advantage computation in PPO pipeline, leaving the rest unchanged. With this simple modification, we eliminate the need for a value network, significantly reducing memory footprint (up to 112GB for a 7B LLM) while providing unbiased estimates of advantages. The parameter \(K\) offers a trade-off between computational cost (i.e. more MC samples per state) and the variance of the estimator. To enhance the efficiency of \(_{}\), we also group states within a reasoning step and compute a single advantage, which is then assigned to all tokens in that step. Since everything else in the PPO pipeline of VinePPO is unchanged, by comparing the two methods, we can systematically evaluate of the impact of accurate credit assignment in RL tuning of LLMs.

## 3 Experiments

We use two strong base LLMs pretrained for mathematical reasoning: (1) DeepSeeKMath 7B (Shao et al., 2024) and (2) RhoMath 1.1B (Lin et al., 2024). Our focus is the MATH dataset (Hendrycks et al., 2021), which contains competition-level problems. We compare three LLM reasoning finetuning strategies, PPO, VinePPO, and RestEM to the supervised finetuned model (SFT) baseline, from which all methods are initialized. We tune PPO hyperparameters like KL penalty coefficient, batch size, and GAE \(\), applying best practices in PPO optimization. VinePPO uses the same hyperparameters as PPO but modifies the advantage estimation \(A(s_{t},a_{t})\) to isolate the effect of accurate credit assignment. We sample \(K=9\) trajectories in \(_{}\). For RestEM, we closely follow the original setup while ensuring consistency in training conditions for a fair comparison. We choose the best checkpoint based on a held-out validation set for all experiments3.

## 4 Results and Analysis

Task PerformanceAs shown in Fig. 1, VinePPO outperforms standard PPO and RestEM. The gap between VinePPO and PPO is consistent throughout the training (Fig. E.1). RestEM lacks explicit credit assignment and finetunes on full trajectories. Despite higher training accuracy, it underperforms on test, likely due to overfitting caused by training on disadvantageous intermediate steps. In addition, fig. 4 represents our ablation on \(K\), observing increasing \(K\) consistently improves accuracy.

KL DivergenceThe RL objective4 aims to balance maximizing task performance while limiting deviations from the reference policy \(_{0}\), or original SFT, as measured by KL divergence. We track the KL divergence \([_{}\|_{0}]\) throughout training for both methods and plot task accuracy against KL to assess this balance in Fig. G.3. The results show that VinePPO consistently achieves higher accuracy for a given KL divergence.

Computational efficiencyVinePPO and standard PPO need different kinds of resources. The value network needs to be trained and alongside its optimizer consuming more GPU memory. In contrast, MC rollouts need fast inferences and as a result VinePPO is generally slower per iteration compared to PPO. In our setup, RhoMath 1.1B and DeepSeekMath 7B are 5x and 2x slower per iteration when using VinePPO. However, as shown in Fig. 4, the impact of accurate credit assignment with VinePPO is substantial. VinePPO reaches the final accuracy of PPO in fewer iterations and less time. Specifically, RhoMath 1.1B and DeepSeekMath 7B achieve PPO's final test accuracy 3.7x and 2.3x faster in wall-clock time, and in 20x and 5x fewer gradient steps, respectively.5

Value Prediction AccuracyTo analyze the accuracy of value prediction, we compute the ground truth value of each state by taking \(256\) MC samples. We compare value network (from PPO) predictions against VinePPO's. As shown in Fig. 3, VinePPO and PPO produce errors of very different types. VinePPO estimates are unbiased, with variance peaking at \(0.5\) and dropping to zero at \(0\) and \(1\). In contrast, the value network's estimates exhibit high bias. See Appendix H for full details.

## 5 Related Work

Credit Assignment in Post-Training of LLMPPO (Schulman et al., 2017), as applied in Reinforcement Learning from Human Feedback (RLHF, Ouyang et al., 2022), was among the pioneering approaches for RL finetuning of LLMs. While effective, PPO is known for its computational overhead and sensitivity to hyperparameters. As a result, subsequent approaches have sought to simplify or bypass PPO without sacrificing performance. For example, RL-free methods such as DPO (Rafailov et al., 2023) and its newer variants (Azar et al., 2023; Ethayarajh et al., 2024) operate in a bandit setting, where the entire response is treated as a single action, without distinguishing intermediate states. Similarly, methods based on rejection sampling, like RestEM (Singh et al., 2023), finetune the model on full high-reward responses. In the realm of PPO simplifications, methods like RLOO

Figure 4: **(Left) Impact of the number of sampled trajectories \(K\) when estimating \(_{}(s_{t})\), evaluated on RhoMath 1.1B models. We observe that increasing \(K\) improves task performance consistently. (Right) Accuracy per wall clock time for both methods. Although VinePPO spend more time in each iteration, it achieves PPO’s peak performance in fewer iteration and wall clock time.**

Figure 3: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) for DeepSeekMath 7B on MATH, highlighting the nature of errors in PPO’s value estimates.

(Ahmadian et al., 2024) and GRPO (Shao et al., 2024) abandon the value network of PPO. They sample a group of \(M\) responses per each prompt and compute the average reward (of other \(M-1\) responses) as a policy gradient baseline for all tokens in the group, effectively treating the entire response as a single action. Recent works, however, have started to emphasize the importance of finer credit assignment. Work such as Hwang et al. (2024) and Setlur et al. (2024) introduce Monte Carlo-based mechanisms that detect key errors in reasoning chains and apply use them negative sample in DPO. Unlike these approaches, which rely on ad-hoc heuristics, our work fully embraces RL training pipeline and addresses the core issue of inaccurate value estimation in PPO to unlock its full potential. In parallel, there has been interest (Hosseini et al., 2024; Lightman et al., 2023) in building better verifiers and reward models that can provide per-step feedback. Although these methods often require costly human annotation, recent efforts (Ma et al., 2023; Uesato et al., 2022; Luo et al., 2024; Wang et al., 2023) have automated data collection using MC rollouts. VinePPO is orthogonal to these approaches, as it operates within PPO-based training, optimizing a given task's reward rather than designing new reward models. Our method can further benefit from improvements in reward modeling as they emerge.

Value Estimation in RL and Monte Carlo Tree SearchDeep RL algorithms are categorized into value-based and policy-based methods. Value-based algorithms, such as DQN and its successors (Mnih et al., 2013; Wang et al., 2015), train a neural network to predict values and derive the policy from the learned value function. Policy-based methods, including A2C, A3C (Mnih et al., 2016), SAC (Haarnoja et al., 2018), and PPO (Schulman et al., 2017), train a policy directly and use value estimates only to guide the policy updates. Typically, these methods rely on _critic networks_ for value prediction. An exception is a variant of TRPO (Schulman et al., 2015), known as the _"Vine"_ variant, where state value estimation is performed using MC samples. However, the authors note that the Vine variant is limited to environments that allow easy resets to any state, which is uncommon in most RL settings as the focus is on black-box engines or real-world deployment. In contrast to common RL environments, language generation, allows for easy resets to any intermediate state, presenting unique opportunities for RL tunning of LLM. In fact, when easy resets were available in RL (e.g., Go, Chess), strong MC-based methods like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017) have emerged. AlphaGo trains a policy using expert moves data and self-play, alongside a value network to predict the win probability from a given state. Then during the inference, it applies a tree search guided by MC rollouts and the value network to find the best possible moves. AlphaZero advances this approach by distilling MCTS outcomes into its policy, removing the need for expert data. Recent works have adapted AlphaZero's principles and lessons to LLM, using similar search techniques during inference to improve responses and during training to find better trajectories for distillation (Xie et al., 2024; Chen et al., 2024; Feng et al., 2023; Zhang et al., 2024; Hao et al., 2023). While this is a promising direction, VinePPO is not an MCTS method; it rather utilizes MC samples solely for value estimation and only during PPO training to improving credit assignment. In fact, inference-time search like MCTS can be layered on top of VinePPO to further enhance performance.

## 6 Conclusion

Credit assignment is a weak spot for current RL finetuning of LLMs. While value networks are tasked and trained to estimate these values, they perform poorly. VinePPO simply replaces the value networks with MC samples. We found that it reaches higher accuracy faster supporting the significant impact that accurate credit assignment has on RL finetuning of LLMs for reasoning. We hope our work encourages researchers to look into the details of RL finetuning pipelines of LLMs and to explore more computationally practical methods for accurate credit assignment.