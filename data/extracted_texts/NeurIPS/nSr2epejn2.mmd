# Robust Matrix Sensing in the Semi-Random Model

Xing Gao

University of Illinois at Chicago

xgao53@uic.edu

&Yu Cheng

Brown University

yu_cheng@brown.edu

###### Abstract

Low-rank matrix recovery is a fundamental problem in machine learning with numerous applications. In practice, the problem can be solved by convex optimization namely nuclear norm minimization, or by non-convex optimization as it is well-known that for low-rank matrix problems like matrix sensing and matrix completion, all local optima of the natural non-convex objectives are also globally optimal under certain ideal assumptions.

In this paper, we relax the assumptions and study new approaches for matrix sensing in a semi-random model where an adversary can add any number of arbitrary sensing matrices. More precisely, the problem is to recover a low-rank matrix \(X^{*}\) from linear measurements \(b_{i}= A_{i},X^{*}\), where an unknown subset of the sensing matrices satisfies the Restricted Isometry Property (RIP) and the rest of the \(A_{i}\)'s are chosen adversarially.

It is known that in the semi-random model, existing non-convex objectives can have bad local optima. To fix this, we present a descent-style algorithm that provably recovers the ground-truth matrix \(X^{*}\). For the closely-related problem of semi-random matrix completion, prior work  showed that all bad local optima can be eliminated by reweighting the input data. However, the analogous approach for matrix sensing requires reweighting a set of matrices to satisfy RIP, which is a condition that is NP-hard to check. Instead, we build on the framework proposed in  for semi-random sparse linear regression, where the algorithm in each iteration reweights the input based on the current solution, and then takes a weighted gradient step that is guaranteed to work well locally. Our analysis crucially exploits the connection between sparsity in vector problems and low-rankness in matrix problems, which may have other applications in obtaining robust algorithms for sparse and low-rank problems.

## 1 Introduction

Low-rank matrix recovery is a popular inverse problem with many applications in machine learning such as collaborative filtering, image compression, and robust principal component analysis (PCA) . In this paper, we study one of the most basic low-rank matrix recovery problems namely matrix sensing . In the matrix sensing problem, we want to reconstruct a low-rank ground-truth matrix \(X^{*}^{d_{1} d_{2}}\) from a collection of sensing matrices \(\{A_{i}\}_{i=1}^{n}\) and the corresponding linear measurements \(b_{i}= A_{i},X\).

For notational convenience, we define a sensing operator \([]:^{d_{1} d_{2}}^{n}\) such that \([X]=b\) with \(b_{i}= A_{i},X\) for \(i=1 n\). The goal is to solve the following rank-constrained optimization problem:

\[_{X^{d_{1} d_{2}}}\|[X]-b\|_{2}^ {2}(X) r\;.\]

As optimizing over low-rank matrices are often computationally hard, one common approach is to replace the non-convex low-rank constraint with its convex-relaxation, which results in the followingnuclear norm minimization problem :

\[_{X^{d_{1} d_{2}}}\|X\|_{*}[X]=b\.\] (1)

Another widely-used approach in practice is to consider the unconstrained non-convex factorized parametrization :

\[_{U^{d_{1} r},V^{d_{2} r}}\| [UV^{}]-b\|_{2}^{2}\.\] (2)

and solve it with some form of gradient descent or alternating minimization.

Existing convex and non-convex approaches all rely on certain assumptions. A standard assumption in the literature is that the sensing matrices satisfy the Restricted Isometry Property (**RIP**), which means that the sensing matrices approximately preserve the norm of a low-rank matrix. (Formally, \(\|X\|_{F}^{2}_{i=1}^{n} A _{i},X^{2} L\|X\|_{F}^{2}\) given \((X) r\) for some parameters \(r\) and \(L\).)

In this paper, we relax the RIP condition on the sensing matrices and study a robust version of the problem, which is often referred to as the **semi-random** model. More specifically, an adversary "corrupts" the input data by providing any number of additional sensing matrices \(A_{i}\) that are adversarially chosen, but the corresponding measurements \(b_{i}= A_{i},X^{*}\) remain consistent with the ground truth matrix \(X^{*}\). Consequently, only a subset of the sensing matrices satisfy the RIP condition and the rest of them are arbitrary. This is an intermediate scenario between the average case and the worst case, which arises more frequently in practice.

To the best of our knowledge, we are the first to study the matrix sensing problem in this semi-random model. Formally, we consider the following adversary: suppose that originally there was a collection of RIP sensing matrices \(\{A_{i}\}_{i=1}^{m}\) ("good" matrices), then the adversary augmented some arbitrary \(\{A_{i}\}_{i=m+1}^{n}\) ("bad" matrices) and then shuffled all the sensing matrices. The algorithm is then given the measurements based on the "good" and "bad" matrices together. The combined sensing matrices are no longer guaranteed to satisfy the RIP condition, but there exists a subset (indicated by an indicator vector \(w^{*}\)) that does, i.e., \(\|X\|_{F}^{2}_{i=1}^{n}w_{i}^{*} A _{i},X^{2} L\|X\|_{F}^{2}\), where \(w_{i}^{*}=\) on the original "good" matrices and \(w_{i}^{*}=0\) on the "bad" matrices added by the adversary. In general, the subset may be replaced by a convex combination and the indicator vector by a simplex. Inspired by the adversary for semi-random vector regression in , we refer to this condition as **wRIP** (weighted RIP) and formally define it in Definition 2.2.

Since the wRIP condition is a more general assumption than RIP, existing solutions that rely on RIP might fail under the semi-random model with wRIP condition. As stated in , this type of adversary does not break the problem from an "information-theoretic perspective", but affects the problem computationally. In particular, existing non-convex approaches for matrix sensing (e.g., 2) may get stuck at bad local minima as the RIP condition is necessary for proving landscape results regarding the non-convex objective (see, e.g., the counter-examples provided in . The convex relaxation approach (1) does continue to work in the semi-random model, because the augmented linear measurements are consistent with the ground-truth matrix \(X^{*}\) which simply provides additional optimization constraints. However, convex approaches are often less desirable in practice and can become computationally prohibitive when \(d_{1},d_{2}>100\) as pointed out in .

### Our Contributions

The limitations of existing algorithms motivate us to pose and study the problem of semi-random matrix sensing in this paper. We summarize our main contributions below:

* **Pose and study matrix sensing in the semi-random model.** We introduce the more general wRIP condition on matrix sensing as a relaxation of the typical RIP assumption, and provide a solution that is more robust to input contamination. Our work will serve as a starting point for the design of more efficient robust algorithms for matrix sensing, as well as other low-rank matrix problems, in the semi-random model.
* **Design an efficient robust algorithm for semi-random matrix sensing.** Our algorithm is guaranteed to converge to a global optimum which improves on the existing non-convex solution  that can get stuck in bad local optima in the semi-random model, while achieving a comparable running time as existing convex solution , informally stated in Theorem 1.1 below. A formal statement can be found in Theorem 3.1.

* **Adapt a reweighting scheme for semi-random matrix sensing.** In contrast to the non-convex approach that failed and the convex approach that avoided the challenge posed by the adversary altogether, we study a new approach that directly targets the semi-random adversary instead. We develop an algorithm using an iterative reweighting approach inspired by : in each iteration, the algorithm reweights the sensing matrices to combat the effect of the adversary and then takes a weighted gradient step that works well based on the current solution.
* **Exploit the connection between sparsity and low-rankness.** Observing a duality between sparse vectors and low-rank matrices, we draw a parallel between linear regression and matrix sensing problems. By exploring the structural similarities and differences between vector and matrix problems, we are able to extend and generalize the work of  on semi-random sparse vector recovery to the higher dimensional problem of semi-random matrix sensing. We emphasize that even though the generalization from vector to matrix problems is natural, the analysis behind the intuition is often nontrivial and involves different mathematical tools.

We state a simplified version of our main algorithmic result assuming Gaussian design. The more general result is stated as Theorem 3.1 in Section 3.

**Theorem 1.1** (Semi-Random Matrix Sensing).: _Suppose the ground-truth matrix \(X^{*}^{d_{1} d_{2}}\) satisfies \((X^{*}) r\) and \(\|X^{*}\|_{F}(d)\). Let \(A_{1},,A_{n}\) be the sensing matrices and let \(b_{i}= A_{i},X^{*}\) be the corresponding measurements. Suppose there exists a (hidden) set of \((dr)\) sensing matrices with i.i.d. standard Gaussian entries, and the remaining sensing matrices are chosen adversarially, where \(d=(d_{1},d_{2})\)._

_There exists an algorithm that can output \(X^{d_{1} d_{2}}\) such that \(\|X-X^{*}\|_{F}\) with high probability in time \((nd^{+1}r(1/))\)1 where \(<2.373\) is the matrix multiplication exponent._

### Overview of Our Techniques

Since there exists a subset (or a convex combination in general) of the sensing matrices that satisfy the RIP condition, a natural strategy is to reverse the effect from the adversary by reweighting the sensing matrices so that they satisfy the RIP condition. However, it is NP-hard to verify RIP condition on all low-rank inputs, so it is unclear how to preprocess and "fix" the input in the beginning and then apply existing solutions to matrix sensing. Instead, we make a trade-off between the frequency of reweighting and the requirement on the weights by adopting an iterative reweighting approach: in each iteration, we only aim to find a set of weights so that the weighted matrices satisfy some desirable properties (not necessarily RIP) with respect to the current estimate \(X\) (as opposed to all low-rank matrices).

Inspired by the workflow in , our **semi-random matrix sensing algorithm** (Algorithm 1) repeatedly calls a halving algorithm to reduce the error of our estimate arbitrarily small. The **halving algorithm** (Algorithm 2) contracts the upper bound on \(\|X-X^{*}\|_{F}\), which is the error between our current estimate \(X\) and the ground truth \(X^{*}\), each time it is run. Inside this algorithm is a gradient descent style loop, where in each iteration we try to minimize a weighted objective function, which is essentially the weighted \(_{2}\)-norm of \([X_{t}]-b\) (the distance to \(X^{*}\) "measured" by the sensing matrices), where the weights are provided by an oracle implemented in Algorithm 3. The algorithm proceeds by taking a step opposite to the gradient direction, and the step is then projected onto a nuclear-norm-bounded ball which is necessary for the weight oracle to continue working in the next step. As we mentioned before, the weights from the oracle need to satisfy some nice properties with respect to the current iteration estimate \(X_{t}\). Ideally, the property should: firstly, ensure the gradient step makes enough progress towards \(X^{*}\); secondly, can be derived from the wRIP condition so that we know such a requirement is feasible; and lastly, be easily verifiable as opposed to the NP-hard RIP condition.

With the first requirement in mind, we define the **weight oracle** as in Definition 2.5. The oracle output should satisfy two properties, namely the progress and decomposition guarantees, and together they ensure the gradient step makes good enough progress toward \(X^{*}\). Intuitively speaking, the progress guarantee ensures the gradient step is large in the direction parallel to the "actual" deviation \(X-X^{*}\) (as opposed to only reducing the "measured" deviation \([X]-b\)) and thus will make significant progress, while the decomposition guarantee ensures the gradient step has small contribution and effect in other directions thus will not cancel the progress after the projection. While the progress guarantee is quantified as an inner product, we introduce a concept called "norm-decomposition" (Definition 2.4) to capture the decomposition guarantee, and we will provide more details later. For the second requirement, we can loosely relate the two oracle guarantees to the wRIP condition: the (large) progress guarantee makes use of the lower bound in wRIP condition \(_{i=1}^{n}w_{i}^{*} A_{i},} ^{2}\), and the (small) decomposition guarantee makes use of the upper bound \(_{i=1}^{n}w_{i}^{*} A_{i},} ^{2} L\). We introduce a condition called **dRIP** (decomposable wRIP defined in Definition 2.3) to formally capture this relation, and we will show that it follows from the wRIP condition thus we can achieve such an oracle. Lastly, we will show that the oracle properties can be easily verified, meeting our third requirement.

A formal statement and a road map that leads to our main result can be found in Section 3.

### Related Work

**Matrix sensing (RIP):** There are two main types of existing solutions. The convex-relaxation formulation 1 of the problem can be posed as a semidefinite program via the standard form primal-dual pair , where the primal problem has a \((d_{1}+d_{2})^{2}\) semidefinite constraint and \(n\) linear constraints. State of the art SDP solver  requires running time of \((nd^{2.5})\) where \(d=(d_{1},d_{2})\). The other approach uses non-convex formulation 2 to reduce the size of the decision variable from \(d^{2}\) to \(dr\), improving computational efficiency. It is shown in  that there are no spurious local minima given RIP sensing matrices and incoherent linear measurements in the non-convex approach, however, it is no longer applicable in the semi-random model.

**Semi-random model:** First introduced by , the semi-random model has been studied for various graph problems . Previously the work of  applied the semi-random model to the matrix completion problem, and recently  studied sparse vector recovery in this model.

**Semi-random matrix completion:** Low-rank matrix problems such as matrix completion and matrix sensing have similar optimization landscapes , thus development in one often lends insight to another. Prior work  on the closely-related problem of matrix completion under the semi-random model showed that all bad local optima can be eliminated by reweighting the input data via a preprocessing step. It exploits the connection between the observation data matrix and the Laplacian matrix of a complete bipartite graph, and gives a reweighting algorithm to preprocess the data in a black-box manner. However, the analogous approach for matrix sensing requires reweighting a set of matrices to satisfy RIP, which is a condition that is NP-hard to check, thus is not practical in the matrix sensing problem.

**Semi-random vector regression:** In order to overcome the barrier of the reweighting or preprocessing approach mentioned earlier, we take inspiration from the work of  on sparse vector recovery under the semi-random model. One of their main contributions is the "short-flat decomposition", which is a property that can be efficiently verified for a given vector (locally), instead of verifying the RIP condition for all sparse vectors (globally). They provide a projected gradient descent style algorithm, where the rows of the sensing matrix are reweighted differently in each iteration to ensure a "short-flat decomposition" exists for the gradient. We draw a parallel between the problem of sparse vector regression and low-rank matrix sensing, and extend their work on linear regression of sparse vectors to the more generalized problem of sensing low-rank matrices.

## 2 Preliminaries

### Notations

Throughout the paper, we denote the ground-truth low-rank matrix as \(X^{*}\). We assume \(X^{*}^{d_{1} d_{2}}\), \((X^{*})=r\), and \(d_{1},d_{2}\) have the same order of magnitude. Let \(d=(d_{1},d_{2})\).

We write \([n]\) for the set of integers \(\{1,...,n\}\). We use \(^{n}\) for the nonnegative probability simplex in dimension \(n\), and \(_{ 0}^{n}\) for the set of vectors with nonnegative coordinates in \(^{n}\). For a vector \(x\), we denote its \(_{1}\), \(_{2}\), and \(_{}\)-norms as \( x_{1}\), \( x_{2}\) and \( x_{}\) respectively, and write the \(i^{}\) coordinate in \(x\) as \(x_{i}\). For a matrix \(A\), we use \( A_{*}\), \( A_{2}\), and \( A_{F}\) for the nuclear, spectral (operator), and Frobenius norms of \(A\) respectively. For a matrix \(A\), we use \(A_{(k)}=_{(A^{}) k}\|A-A^{}\|_ {F}\) to denote the best rank-\(k\) approximation of \(A\); or equivalently, given the SVD of \(A=_{i=1}^{r}_{i}u_{i}v_{i}^{}\), we have \(A_{(k)}=_{i=1}^{k}_{i}u_{i}v_{i}^{}\) where \(_{1},...,_{k}\) are the top \(k\) singular values of \(A\).

We write \((A)\) for the trace of a square matrix \(A\). For matrices \(A,B^{d_{1} d_{2}}\), we write \( A,B\) for their entrywise inner product \( A,B= A,B=(A^{ }B)=_{j,k}A_{jk}B_{jk}\). A symmetric matrix \(A^{d d}\) is positive semidefinite (PSD) if and only if \(A=U^{}U\) for some matrix \(U\), and we write \(A B\) if \(A\) and \(B\) have the same dimension and \(B-A\) is positive semidefinite. We write \(\) as the matrix exponential of \(A\); if \(A\) is diagonalizable as \(A=UDU^{-1}\) then \((A)=U(D)U^{-1}\).

### Definitions

We formally define the matrix sensing operator and observation vector below.

**Definition 2.1** (Matrix Sensing Operator).: Given a collection of sensing matrices \(=\{A_{i}\}_{i=1}^{n}^{d_{1} d_{2}}\), we define the sensing operator \([]:^{d_{1} d_{2}}^{n}\) such that \([X]=b\) where \(b_{i}= A_{i},X\) for \(X^{d_{1} d_{2}}\).

In other words, we have \(b:=_{i=1}^{n} A_{i},X e_{i}\) where \(e_{i}\) is the \(i^{}\) standard basis vector in \(^{n}\). Throughout the paper, we use either \(\) or \(\{A_{i}\}_{i=1}^{n}\) to represent the sensing matrices.

To consistently recover a rank-\(r\) matrix in general, the number of measurements \(n\) should be at least \((d_{1}+d_{2}-r)r\), hence we assume \(n=(dr)\) where \(\) suppresses log factors. In most matrix sensing literature, it is standard to impose the Restricted Isometry Property (RIP) condition on the sensing matrices. The RIP condition states that \([]\) is approximately an isometry on low-rank matrices, which means the \(_{2}\)-norm of the observation vector is close to the Frobenius norm of \(X^{*}\).

In this paper, we consider a semi-random model and relax the RIP condition as follows: we require that there exist weights \(\{w_{i}^{*}\}_{i=1}^{n}\) (or \(w^{*}^{n}\)) so that the weighted sensing matrices \(\{^{*}}A_{i}\}_{i=1}^{n}\) satisfy the RIP condition. We call this relaxed assumption the wRIP (weighted RIP) condition.

We formally define RIP and wRIP conditions below.

**Definition 2.2** (RIP and wRIP Conditions).: We say a collection of sensing matrices \(=\{A_{i}\}_{i=1}^{n}^{d_{1} d_{2}}\) satisfies the **RIP** (Restricted Isometry Property) condition with parameters \(r\), \(L\), and \(\) if the following conditions hold for all \(X^{d_{1} d_{2}}\) with \((X) r\):

1. Boundedness: \(\|A_{i}\|_{2}\) ;
2. Isometry: \(\|X\|_{F}^{2}_{i=1}^{n} A _{i},X^{2} L\|X\|_{F}^{2}\).

Further, we say \(=\{A_{i}\}_{i=1}^{n}\) satisfies the **wRIP** (weighted RIP) condition with parameters \(r\), \(L\), \(\), if \( w^{*}^{n}\) such that the following conditions hold for all \(X^{d_{1} d_{2}}\) with \((X) r\):

1. Boundedness: \(\|A_{i}\|_{2}\) ;
2. Isometry: \(\|X\|_{F}^{2}_{i=1}^{n}w_{i}^{*} A _{i},X^{2} L\|X\|_{F}^{2}\).

Notice that wRIP is a relaxation of the RIP condition, because we can choose \(w_{i}^{*}=1/n\) for all \(i\) in the standard RIP setting. More importantly, wRIP is strictly weaker. For example, wRIP allows a (possibly majority) fraction of the sensing matrices to be chosen adversarially. We want to emphasize that the algorithm does not know \(w^{*}\) -- one of the main challenges of semi-random matrix sensing is that finding \(w^{*}\) seems computationally hard, because it is NP-Hard to check the RIP condition.

For presenting our algorithm and analysis, we introduce a variant of the wRIP condition called dRIP (decomposable-wRIP).

**Definition 2.3** (dRIP Condition).: We say a collection of sensing matrices \(=\{A_{i}\}_{i=1}^{n}^{d_{1} d_{2}}\) satisfies the **dRIP** (decomposable wRIP) condition if \( w^{*}^{n}\) and constants \(L,K,r, 1\), such that for all \(V^{d_{1} d_{2}}\) satisfying \(\|V\|_{F}[,1],\|V\|_{*} 2\):

1. Boundedness: \(\|A_{i}\|_{2}\) ;
2. Isometry: \(_{i=1}^{n}w_{i}^{*} A_{i},V^{2} L\) ;3. Decomposability: \((L,})\)-norm-decomposition of \(G^{*}=_{i=1}^{n}w_{i}^{*}(A_{i},V)A_{i}=_{i=1}^{n}w_{i}^{*}u_{i}A_{i}\).

**Definition 2.4** (Norm Decomposition).: We say a matrix \(G\) has a \((C_{F},C_{2})\)-norm-decomposition if \( S\) and \(E\ s.t.\ G=S+E\), and \(\|S\|_{F} C_{F}\), \(\|E\|_{2} C_{2}\).

The main difference with wRIP is that dRIP requires the additional "decomposition" property. Observe that \(G^{*}\) is the (weighted) gradient at the point \(V\). At a high level, we would like to decompose the gradient into two matrices, one with small Frobenius norm and the other one with small operator norm. Our matrix norm-decomposition is inspired by the "short-flat-decomposition" for vectors in .

In Section 4, we will explain the motivation behind the norm decomposition as well as how to efficiently verify such a decomposition exists. We will also show that the dRIP condition is closely related to wRIP (by choosing parameters within a constant factor of each other) in Appendix C.

A crucial component in our algorithm is a weight oracle that produces a nonnegative weight on each sensing matrix (the weights are in general different in each iteration), such that the weighted gradient step moves the current solution closer to \(X^{*}\). The oracle should output weights that satisfy certain properties which we term progress and decomposition guarantees. The purpose of these two guarantees is further explained in the proof of Lemma 4.2 in Appendix A.

**Definition 2.5** (Weight Oracle).: We say an algorithm \(\) is a \((C_{},C_{F})\)-oracle, if given as input \(n\) matrices \(=\{A_{i}\}_{i=1}^{n}^{d_{1} d_{2}}\) and an vector \(u=[V]^{n}\) where \(V^{d_{1} d_{2}}\), \(\|V\|_{F}[,1]\), and \(\|V\|_{*} 2\), the algorithm \((,u,)\) returns a weight vector \(w_{ 0}^{n}\) such that the following conditions hold with probability at least \(1-\):

1. Progress guarantee: \(_{i=1}^{n}w_{i}u_{i}^{2} C_{}\) ;
2. Decomposition guarantee: \((C_{F},}}{6})\) norm-decomposition of \(G=_{i=1}^{n}w_{i}u_{i}A_{i}\).

Note that the progress guarantee is equivalent to \( G,V C_{}\).

Finally we define numerical rank which we use in our analysis. Numerical rank serves as a lower bound for the rank of a matrix based on its nuclear norm and Frobenius norm. That is, we always have \(_{}(A)(A)\).

**Definition 2.6** (Numerical Rank).: The numerical rank of \(A\) is \(_{}(A)=^{2}}{\|A \|_{F}^{2}}\).

## 3 Semi-Random Matrix Sensing

In this section, we present our main algorithm (Algorithm 1) for semi-random matrix sensing. Algorithm 1 with high probability recovers the ground-truth matrix \(X^{*}\) to arbitrary accuracy.

```
1:Input: \(R_{0}\|X^{*}\|_{F},b=[X^{*}],>0,(0,1)\) ;
2:Output: \(X_{}\) s.t. \(\|X_{}-X^{*}\|_{F}\).
3:\(X_{0} 0,T}{},^{} ,R R_{0}\) ;
4:for\(0 t T\)do
5:\(X_{t+1}(X_{t},R,,^{}, ,b)\), \(R\) ;
6:endfor
7:Return \(X_{} X_{T}\) ; ```

**Algorithm 1**SemiRandomMatrixSensing(\(R_{0},,,,b\))

The performance guarantee and runtime of Algorithm 1 are formally stated in the following theorem.

**Theorem 3.1** (Matrix Sensing under wRIP).: _Suppose the ground-truth matrix \(X^{*}^{d_{1} d_{2}}\) satisfies \((X^{*}) r\) and \(\|X^{*}\|_{F} R_{0}\). Suppose the sensing matrices \(=(A_{i}^{d_{1} d_{2}})_{i=1}^{n}\) satisfy \((r,L,)\)-wRIP (as in Definition 2.2). Let \(b=[X^{*}]^{n}\) be the corresponding measurements.__For any \(,>0\), Algorithm 1 can output \(X^{d_{1} d_{2}}\) such that \(\|X-X^{*}\|_{F}\) with probability at least \(1-\). Algorithm 1 runs in time \(O(nd^{}} {}})}r^{2}L^{4}}{}})\) where \(d=(d_{1},d_{2})\) and \(<2.373\) is the matrix multiplication exponent._

Theorem 1.1 is a direct corollary of Theorem 3.1 under Gaussian design.

Proof of Theorem 1.1.: When there are \((dr)\) sensing matrices with i.i.d. standard Gaussian entries, the input sensing matrices satisfy \((r,L,)\)-wRIP for \(L=O(1)\) and \(=O(d^{1/2})\) with probability at least \(1-(d)}\). This follows from a standard proof for RIP and the fact that we can ignore any sensing matrices with \(\|A_{i}\|_{2} d^{1/2}\). We assume that the wRIP condition is satisfied.

By Theorem 3.1, when \(L=O(1)\), \(=O(d^{1/2})\), \(R_{0}=(d)\) and \(=(d)}\), Algorithm 1 can output a solution \(X\) such that \(\|X-X^{*}\|_{F}\) with high probability. The runtime of Algorithm 1 can be simplified to \((nd^{+1}r(1/))\). 

We first provide a road map for our analysis for proving Theorem 3.1:

* Our main algorithm runs a "halving" subroutine for \(}{}}\) iterations to reduce the error to \(\). Each call to this subroutine reduces the upper bound on the distance between the current solution and the ground truth \(X^{*}\) by half. This halving subroutine runs in time \(O(nd^{}}{}})}r^{2}L^{4})\) according to Lemmas 4.2 and 4.3.
* In Section 4, we present the halving algorithm (Algorithm 2). It depends on a \(((1),O(1))\)-oracle, and Lemma 4.1 shows that the oracle guarantees can be easily verified. The algorithm's correctness and running time are analyzed in Lemma 4.2 and Lemma 4.3.
* In Section 5 we present the weight oracle required by the halving algorithm. We first show in Lemma 5.1 that the wRIP condition implies that the sensing matrices satisfy the dRIP condition tailored to the design of the oracle. Then we present an implementation of the oracle in Algorithm 3 based on the dRIP condition, and analyze its correctness and running time in Lemma 5.3 and Lemma 5.4.

## 4 Algorithm for Halving the Error

In this section, we present Algorithm 2 (HalveError). Algorithm 2 takes an estimate \(X_{}\) with \(\|X_{}-X^{*}\|_{F} R\) and outputs \(X_{}\) such that \(\|X_{}-X^{*}\|_{F}\). This is the matrix version of the HalfRadiabsSparse  algorithm for vectors.

```
1:Input: Rank-\(r\) matrix \(X_{}^{d_{1} d_{2}}\), \(\|X_{}-X^{*}\|_{F} R\), \(\) is a \((1,12L^{2})\)-oracle for \(\) with failure probability \((0,1)\), linear measurements \(b=[X^{*}]\).
2:Output: \(X_{}^{d_{1} d_{2}}\), \(s.t.\)\(\|X_{}-X^{*}\|_{F}\)\(w.p.\)\( 1-\) and \((X_{}) r\).
3:\(X_{0} X_{}\), \(=\{X^{d_{1} d_{2}}\|X-X_{} \|_{*}R\}\), \(=}\), \(T=\).
4:for\(0 t T\)do
5:\(u_{t}([X_{t}]-b)\) ; /* \(u_{t}=[-X^{*}}{R}]\) where \((u_{t})_{i}= A_{i},X_{t}-X^{*}\) */
6:\(w_{t}(,u_{t},)\) ;
7:\(G_{t}_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}A_{i}}\) ;
8:if\(\) output satisfies the progress and decomposition guarantees on \(u_{t}\)then
9:\(X_{t+1}_{X}\|X-(X_{t}- RG_{t})\|_{F}^{2}\) ;
10:else
11: Return \(X_{}(X_{t})_{(r)}\) ; /* Rank-r approximation of \(X_{t}\) */
12:endif
13:endfor
14:Return \(X_{}(X_{T})_{(r)}\) ; ```

**Algorithm 2**HalveError(\(X_{},R,,,,b\))A crucial requirement of the algorithm is a \(((1),O(1))\)-oracle for \(\). In each iteration, the oracle takes a vector \(u_{t}=|-b}{R}\), which is the (normalized) "measured deviation" between current estimate \(X_{t}\) and \(X^{*}\), and computes a weight vector \(w_{t}\). The algorithm then tries to minimize the weighted objective function by gradient descent:

Objective:

\[f_{t}(X) =_{i=1}^{n}{(w_{t})_{i} A_{i}, }{R}^{2}}f_{t}(X_{t})=_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}^{2}} \] Gradient:

\[_{X}f_{t}(X) =_{i=1}^{n}{(w_{t})_{i} A_{i},}{R} A _{i}}G_{t}=_{X}f_{t}(X)|_{X_{t}}=_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}A_{i}} \]

Ideally in the next iteration, we would like to make a step from \(X_{t}\) in the opposite direction of the gradient \(G_{t}\) with the goal of minimizing the deviation in the next iteration. However, we cannot take a step exactly in the direction of \(G_{t}\), and our movement is constrained within a ball of (nuclear norm) radius \(R\) centered at \(X_{}\), namely the region \(=\{X\|X-X_{}\|_{*}R\}\). Nuclear norm is used as a proxy to control the rank and Frobenius norm of \(X_{t}\) simultaneously throughout the algorithm: firstly, since \(\|X_{}-X^{*}\|_{F} R\), it makes sense that in each iteration \(\|X_{t}-X_{}\|_{F} R\) as well; secondly, while trying to minimize the difference between \(X_{t}\) and \(X^{*}\), we also want to ensure the rank of \(X_{t}\) is relatively small, i.e. \((X_{t}) O(r)\). To tie things together, we use the following relationship between rank and numerical rank:

\[(X_{t}-X_{})_{}(X _{t}-X_{})=-X_{}\|_{*}^{2}}{\| X_{t}-X_{}\|_{F}^{2}}\]

Assuming \((X_{t})(X_{})\) and \(\|X_{t}-X_{}\|_{F} R\) throughout, then \((X_{t})-X_{}\|_{*}^{2 }}{2R^{2}}\). Roughly speaking, in order for \((X_{t}) O(r)\), it is necessary that \(\|X_{t}-X_{}\|_{*} O(R)\), i.e. \(X_{t}\) is inside some nuclear norm ball \(\) of radius \(O(R)\) centered at \(X_{}\). We set the radius of \(\) to be \(R\) so that \(X^{*}\), since \(\|X_{}-X^{*}\|_{F} R\), \((X_{}-X^{*}) 2r\) therefore \(\|X^{*}-X_{}\|_{*}R\). Thus we confine our movement within this nuclear norm ball of radius \(R\) centered at \(X_{}\) throughout the algorithm, and take the rank-\(r\) approximation of the last \(X_{t}\) to ensure \((X_{}) r\) upon the termination of the algorithm.

To analyze the algorithm, first we show how to check whether the weight oracle output satisfies the progress and decomposition guarantees. The progress condition \(_{i=1}^{n}{w_{i}u_{i}^{2}} 1\) is trivial to verify, and we check whether \(G\) is \((C_{F},C_{2})\)-decomposable using Lemma 4.1, with details and proof deferred to Appendix A.

**Lemma 4.1** (Verify Norm Decomposition).: _Given a matrix \(G=U V^{}=_{i=1}^{d}_{i}u_{i}v_{i}^{}\) and \(C_{2}>0\), suppose \(_{1}..._{k}>C_{2}_{k+1}..._{d}\), then for all \(\|E\|_{2} C_{2}\), we have \(\|G-E\|_{F}^{2}_{i=1}^{k}{(_{i}-C_{2})^{2}}\)._

The following lemmas analyze the algorithm's correctness and show that it terminates with the desired distance contraction, as well as its running time. The proof is deferred to Appendix A.

**Lemma 4.2** (Algorithm 2: HalveError).: _Given a \((1,12L^{2})\)-oracle for \(\) with failure probability \((0,1)\), where \(\) satisfies the dRIP Condition 2.3, and \(b=[X^{*}]\), Algorithm 2 succeeds with probability at least \(1-\)._

**Lemma 4.3** (Algorithm 2 Running Time).: _Algorithm 2 with failure probability \(\) runs in time \(O(nd^{}r^{2}L^{4})\)._

The crucial part of Lemma 4.2 shows that if current estimate \(X_{t}\) is sufficiently far from \(X^{*}\), i.e. \(\|X_{t}-X^{*}\|_{F}R\), then according to Lemma 5.3 with high probability the weight oracle produces an output satisfying the progress and decomposition guarantees, and each iteration of Algorithm 2 decreases the distance to \(X^{*}\) by a constant factor: \(\|X_{t+1}-X^{*}\|_{F}^{2}(1-) \|X_{t}-X^{*}\|_{F}^{2}\), thus after sufficient number of iterations the distance to \(X^{*}\) will be halved. On the other hand, if the weight oracle fails, with high probability the current estimate \(X_{t}\) is already sufficiently close to \(X^{*}\), thus the algorithm can terminate early.

[MISSING_PAGE_FAIL:9]

\([,1]\) and \(\|V\|_{*} 2\)). Then, Algorithm 3 is a \((1,12L^{2})\)-oracle for \(\) (as in Definition 2.5) with failure probability at most \(\)._

We prove this lemma in two steps: first we show in Lemma B.1 that the output is valid; then in Lemma B.2 we show that the oracle achieves the success probability. Our weight oracle is inspired by the step oracle in . It is worth noting that Lemma B.3, a key component used in the proof of Lemma B.2, is significantly different in the matrix case compared to the vector case. Lemma B.3 upper bounds the increase in \(_{}\) each round, which is then used to provide a lower bound for the increase in \(\). Combining Lemma B.3 with our earlier remark that the algorithm terminates when \( 0\) gives us the number of iterations needed to terminate with high probability.

The running time of Algorithm 3 is stated in the following lemma, and the proof is deferred to Appendix B.

**Lemma 5.4** (Algorithm 3 Running Time).: _Algorithm 3 with failure probability \(\) runs in time \(O(nd^{}(d)r^{2 })\)._

## 6 Conclusion and Future Work

In this paper, we pose and study the matrix sensing problem in a natural semi-random model. We relax the standard RIP assumption on the input sensing matrices to a much weaker condition where an unknown subset of the sensing matrices satisfies RIP while the rest are arbitrary.

For this semi-random matrix sensing problem, existing non-convex objectives can have bad local optima. In this work, we employ an iterative reweighting approach using a weight oracle to overcome the influence of the semi-random input. Our solution is inspired by previous work on semi-random sparse vector recovery, where we exploit the structural similarities between linear regression on sparse vectors and matrix sensing on low-rank matrices.

Looking forward, we believe our approach can serve as a starting point for designing more efficient and robust algorithms for matrix sensing, as well as for other low-rank matrix and sparse vector problems in the semi-random model.