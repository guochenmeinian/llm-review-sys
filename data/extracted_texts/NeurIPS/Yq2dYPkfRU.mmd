# Stability and Sharper Risk Bounds with Convergence Rate \(O(1/n^{2})\)

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The sharpest known high probability excess risk bounds are up to \(O(1/n)\) for empirical risk minimization and projected gradient descent via algorithmic stability (Klochkov and Zhivotovskiy, 2021). In this paper, we show that high probability excess risk bounds of order up to \(O(1/n^{2})\) are possible. We discuss how high probability excess risk bounds reach \(O(1/n^{2})\) under strongly convexity, smoothness and Lipschitz continuity assumptions for empirical risk minimization, projected gradient descent and stochastic gradient descent. Besides, to the best of our knowledge, our high probability results on the generalization gap measured by gradients for nonconvex problems are also the sharpest.

## 1 Introduction

Algorithmic stability is a fundamental concept in learning theory (Bousquet and Elisseeff, 2002), which can be traced back to the foundational works of Vapnik and Chervonenkis (1974) and has a deep connection with learnability (Rakhlin et al., 2005; Shalev-Shwartz and Ben-David, 2014). It is not difficult for only providing in-expectation error bounds via stability arguments. However, high probability bounds are beneficial to understand the robustness of optimization algorithms (Bousquet et al., 2020; Klochkov and Zhivotovskiy, 2021) and are much more challenging (Feldman and Vondrak, 2019; Bousquet et al., 2020; Lv et al., 2021). In this paper, our goal is to improve the high probability risk bounds via algorithmic stability.

Let us start with some standard notations. We have a set of independent and identically distributed observations \(S=\{z_{1},,z_{n}\}\) sampled from a probability measure \(\) defined on a sample space \(:=\). Based on the training set \(S\), our goal is to build a model \(h:\) for prediction, where the model is determined by parameter \(\) from parameter space \(^{d}\). The performance of a model \(\) on an example \(z\) can be quantified by a loss function \(f(;z)\), where \(f:_{+}\). Then the population risk and the empirical risk of \(\), respectively as

\[F():=_{z}[f(;z)], F_{S}( ):=_{i=1}^{n}f(;z_{i}),\]

where \(_{z}\) denotes the expectation w.r.t. \(z\).

Let \(^{*}_{}F()\) be the model with the minimal population risk in \(\) and \(^{*}(S)_{}F_{S}()\) be the model with the minimal empirical risk w.r.t. dataset \(S\). Let \(A(S)\) be the output of a (possibly randomized) algorithm \(A\) on the dataset \(S\). Let \(\|\|_{2}\) denote the Euclidean norm and \( g()\) denote a subgradient of \(g\) at \(\).

Traditional generalization analysis aims to bound the generalization error \(F(A(S))-F_{S}(A(S))\) w.r.t the algorithm \(A\) and the dataset \(S\). Based on the technique developed by Feldman and Vondrak (2018,2019), Bousquet et al. (2020) provide the sharpest high probability bounds of \(O(L/)\), where the loss function \(f(,)\) is bounded by \(M\). No matter how stable the algorithm is, the high probability generalization bound will not be smaller than \(O(L/)\). This is sampling error term scaling as \(O(1/)\) that controls the generalization error (Klochkov and Zhivotovskiy, 2021).

A frequently used alternative to generalization bounds, that can avoid the sampling error, are the excess risk bounds. The excess risk of algorithm \(A\) w.r.t. the dataset \(S\) is \(F(A(S))-F(^{*})\), which is more essential because it considers both generalization error and optimization error. Recently, Klochkov and Zhivotovskiy (2021) provided the best high probability excess risk bounds of order up to \(O( n/n)\) for empirical risk minimization (ERM) and projected gradient descent (PGD) algorithms via algorithmic stability.

On the other hand, Zhang et al. (2017), Li and Liu (2021), Xu and Zeevi (2024) derived high probability excess risk bounds with \(O(1/n^{2})\) for ERM and stochastic gradient descent (SGD) via uniform convergence when the sample number satisfies \(n=(d)\), which implied that the rate \(O(1/n^{2})\) is possible. However, the results obtained by the uniform convergence technique are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems. Since stability analysis can yield dimension-free bounds, we naturally have the following question:

_Can algorithmic stability provide high probability excess risk bounds with the rate beyond \(O(1/n)\)?_

The main results of this paper answers this question positively. We provides the first high probability bounds that are dimension-free with the rate \(O(1/n^{2})\) for ERM, PGD and SGD. Our framework can also be used to solve other stable algorithms.

To this end, we develop the generalization gap measured by gradients. Our bounds under _nonconvex setting_ are tighter than existing works based on both algorithmic stability (Fan and Lei, 2024) and uniform convergence (Xu and Zeevi, 2024). This is why we can achieve dimension-free excess risk bounds of order \(O(1/n^{2})\). In fact, in nonconvex problems, optimization algorithms can only find a local minimizer and we can only obtain optimization error bounds for \(\| F_{S}(A(S))\|_{2}\)(Ghadimi and Lan, 2013). Therefore, it is important to study the generalization behavior of \(A(S)\) measured by gradients. Under Polyak-Lojasiewicz condition, we also obtain sharper results for both generalization bounds of gradients and excess risk bounds. Our route to excess risk bounds can also be applied to various stable algorithms and complex learning scenarios. In this paper, we take ERM, PGD, and SGD as examples to explore the stability of stochastic convex optimization algorithms with strongly convex losses. We provide tighter high probability dimension-free excess risk bounds of up to \(O(1/n^{2})\) comparing with existing works based on both algorithmic stability (Klochkov and Zhivotovskiy, 2021; Fan and Lei, 2024) and uniform convergence (Zhang et al., 2017; Li and Liu, 2021; Xu and Zeevi, 2024).

Besides, to obtain tighter bounds, we obtain a tighter \(p\)-moment bound for sums of vector-valued functions by introducing the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space in the proof, which has more potential applications in vector-valued functional data.

This paper is organized as follows. The related work is reviewed in Section 2. In Section 3, we present our main results for stability and generalization. We give applications to ERM, PGD and SGD in Section 4. The conclusion is given in Section 5. All the proofs and additional lemmata are deferred to the Appendix.

## 2 Related Work

**Algorithmic stability.** Algorithmic stability is a classical approach in generalization analysis, which can be traced back to the foundational works of (Vapnik and Chervonenkis, 1974). It gave the generalization bound by analyzing the sensitivity of a particular learning algorithm when changing one data point in the dataset. Modern framework of stability analysis was established by Bousquet and Elisseeff (2002), where they presented an important concept called uniform stability. Since then, a lot of works based on uniform stability have emerged. On one hand, generalization bounds with algorithmic stability have been significantly improved by Feldman and Vondrak (2018, 2019), Bousquet et al. (2020), Klochkov and Zhivotovskiy (2021). On the other hand, different algorithmic stability measures such as uniform argument stability (Liu et al., 2017; Bassily et al., 2020), on average stability (Shalev-Shwartz et al., 2010; Kuzborskij and Lampert, 2018), hypothesis stability [Bousquet and Elisseeff, 2002, Charles and Papailiopoulos, 2018], hypothesis set stability [Foster et al., 2019], pointwise uniform stability [Fan and Lei, 2024], PAC-Bayesian stability [Li et al., 2020], locally elastic stability [Deng et al., 2021], collective stability [London et al., 2016] and uniform stability in gradients [Lei, 2023, Fan and Lei, 2024] have been developed. Most of them provided the connection on stability and generalization in expectation. Bousquet and Elisseeff , Elisseeff et al. , Feldman and Vondrak , Bousquet et al. , Klochkov and Zhivotovskiy , Fan and Lei  considered high probability bounds. However, only Fan and Lei  developed vector-valued bounds (eg: generalization bounds of gradients), which can be the order at most \(O(M/)\) and remain improvement.

**Uniform convergence.** Uniform convergence is another popular approach in statistical learning theory to study generalization bounds [Fisher, 1922, Vapnik, 1999, Van der Vaart, 2000]. The main idea is to bound the generalization gap by its supremum over the whole (or a subset) of the hypothesis space via some space complexity measures, such as VC dimension, covering number and Rademacher complexity. For finite-dimensional problem, Kleywegt et al.  provided that the generalization error is \(O()\) depended on the sample size \(n\) and the dimension of parameters \(d\) in high probability. In nonconvex settings, Mei et al.  showed that the empirical of generalization error is \(O()\). Xu and Zeevi  developed a novel "uniform localized convergence" framework using generic chaining for the minimization problem and provided the localized generalization bounds in gradients \(O(\{\|-^{*}\|_{2},)( }+))\), which is the optimal result when we only consider the order of \(n\). However, uniform convergence results are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems.

## 3 Stability and Generalization

To derive sharper generalization bounds of gradients, we need to develop a novel concentration inequality which provide \(p\)-moment bound for sums of vector-valued functions. For a real-valued random variable \(Y\), the \(L_{p}\)-norm of \(Y\) is defined by \(\|Y\|_{p}:=([|Y|^{p}])^{}\). Similarly, let \(\|\|\) denote the norm in a Hilbert space \(\). Then for a random variable \(X\) taking values in a Hilbert space, the \(L_{p}\)-norm of \(X\) is defined by \(\|\|\|\|_{p}:=([\|\|^{p}])^{}\).

### A Moment Bound for Sums of Vector-valued Functions

Here we present our sharper moment bound for sums of vector-valued functions of \(n\) independent variables.

**Theorem 1**.: _Let \(=(Z_{1},,Z_{n})\) be a vector of independent random variables each taking values in \(\), and let \(_{1},,_{n}\) be some functions: \(_{i}:^{n}\) such that the following holds for any \(i[n]\):_

* \(\|[_{i}()|Z_{i}]\| M\) _a.s.,_
* \([_{i}()|Z_{[n]\{i\}}]=0\) _a.s.,_
* \(_{i}\) _satisfies the bounded difference property with_ \(\)_, namely, for any_ \(i=1,,n\)_, the following inequality holds_ \[_{z_{1},,z_{n},z_{j}^{}}\|_{i}(z_{1},,z_{j-1}, z_{j},z_{j+1},,z_{n})-_{i}(z_{1},,z_{j-1},z_{j}^{ },z_{j+1},,z_{n})\|.\] (1)

_Then, for any \(p 2\), we have_

\[\|\|_{i=1}^{n}_{i}\|\|_{p} 2(+1 )M+4 2^{}(})(+1)n _{2}n.\]

**Remark 1**.: The proof is motivated by Bousquet et al. (2020). Under the same assumptions, Fan and Lei (2024) also established the following inequality1

\[\|\|_{i=1}^{n}_{i}\|\|_{p} 2(+1) M+4(+1)np_{2}n.\] (2)

It is easy to verify that our result is tighter than result provided by Fan and Lei (2024) for both the first and second term. Comparing Theorem 1 with (2), the larger \(p\) is, the tighter our result is relative to (2). In the worst case, when \(p=2\), the constant of our first term is \(0.879\) times tighter than (2), and the constant of our second term is \(0.634\) times tighter than (2). This is because we derive the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space in the proof.

The improvement of this concentration inequality is meaningful. On one hand, we derive the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space. On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures \(M=0\) at the same time. Under this condition, we can eliminate the first term. When we use Theorem 1 instead of (2) in the whole proofs, at least \(0.634\) times tighter bound can be obtained strictly.

### Sharper Generalization Bounds in Gradients

Let \(S=\{z_{1},,z_{n}\}\) be a set of independent random variables each taking values in \(\) and \(S^{}=\{z^{}_{1},,z^{}_{n}\}\) be its independent copy. For any \(i[n]\), define \(S^{(i)}=\{z_{i},,z_{i-1},z^{}_{i},z_{i+1},,z_{n}\}\) be a dataset replacing the \(i\)-th sample in \(S\) with another i.i.d. sample \(z^{}_{i}\). We introduce some basic definitions here and we want to emphasize that our main Theorem 2 and Theorem 3 do not need smoothness assumption and PL condition.

**Definition 1**.: _Let \(g:\). Let \(,<0\)._

* _We say_ \(g\) _is_ \(\)_-smooth if_ \[\| g()- g(^{})\|_{2}\| -^{}\|_{2},,^{ }.\]
* _Let_ \(g*=_{}g()\)_. We say_ \(g\) _satisfies the Polyak-Lojasiewicz (PL) condition with parameter_ \(>0\) _on_ \(\) _if_ \[g()-g*\| g()\|_{2}^{2}, .\]

Then we define uniform stability in gradients.

**Definition 2** (Uniform Stability in Gradients).: _Let \(A\) be a randomized algorithm. We say \(A\) is \(\)-uniformly-stable in gradients if for all neighboring datasets \(S,S^{(i)}\), we have_

\[_{z}\| f(A(S);z)- f(A(S^{(i)});z)\|_{2}.\] (3)

**Remark 2**.: Gradient-based stability was firstly introduced by Lei (2023); Fan and Lei (2024) to describe the generalization performance for nonconvex problems. In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values. Instead, the convergence of \(\| F_{S}(A(S))\|_{2}\) was often studied in the optimization community (Ghadimi and Lan, 2013). Since the population risk of gradients \(\| F(A(S))\|_{2}\) can be decomposed as the convergence of \(\| F_{S}(A(S))\|_{2}\) and the generalization gap \(\| F(A(S))- F_{S}(A(S))\|_{2}\), the generalization analysis of \(\| F(A(S))- F_{S}(A(S))\|_{2}\) is important, which can be achieved by uniform stability in gradients.

**Theorem 2** (Generalization via Stability in Gradients).: _Assume for any \(S\) and any \(z\), \(\| f(A(S);z)\|_{2} M\). If \(A\) is \(\)-uniformly-stable in gradients, then for any \((0,1)\), thefollowing inequality holds with probability at least \(1-\)_

\[\| F(A(S))- F_{S}(A(S))\|_{2}\] \[ 2+)}{ }+8 2^{}(+1)_{2}n (e/).\]

**Remark 3**.: Theorem 2 is a direct application via Theorem 1 where we denote \(_{i}(S)=_{z_{i}^{}}[_{Z}[ f (A(S^{(i)}),Z)]- f(A(S^{(i)}),z_{i})]\) and find that \(_{i}(S)\) satisfies all the assumptions in Theorem 1. As a comparison, Fan and Lei (2024) also developed high probability bounds under same assumptions, but our bounds are sharper since our moment inequality for sums of vector-valued functions are tighter as we have discussed in Remark 1. Next, we derive sharper generalization bound of gradients under same assumptions.

**Theorem 3** (Sharper Generalization via Stability in Gradients).: _Assume for any \(S\) and any \(z\), \(\| f(A(S);z)\|_{2} M\). If \(A\) is \(\)-uniformly-stable in gradients, then for any \((0,1)\), the following inequality holds with probability at least \(1-\)_

\[\| F(A(S))- F_{S}(A(S))\|_{2}\] \[ _{Z}[\| f(A(S);Z)\|_{2}^{2} ]}{n}}+^{2}+32n ^{2}(3/))}{n}}+}{n}\] \[+16 2^{}_{2}n (3e/)+32_{2}n .\]

**Remark 4**.: Note that the factor in Theorem 2 before \(1/\) is \(O(M)\), which depends on the bound of \(\| f(,)\|_{2}\). However, the factor in Theorem 3 before \(1/\) is \(O(_{Z}[\| f(A(S);Z)\|_{2}^{2}] 1/ }+(1/))\), not involving the possibly large term \(M\). As is known, optimization algorithms often provide parameters approaching the optimal solution, which make the term \(_{Z}[\| f(A(S);Z)\|_{2}^{2}]\) much more smaller than \(M\). We will give further reasonable results under more assumptions such as smoothness in Lemma 1 and Lemma 2.

On the other hand, best high probability bounds based on uniform convergence (Xu and Zeevi, 2024) is

\[\| F(A(S))- F_{S}(A(S))\|_{2}\] (4) \[ _{Z}[\|f(^{*};Z)\|_{2 }^{2}](1/)}{n}}++\{\| -^{*}\|_{2},\}(}+ ),\]

which is the optimal result when we only consider the order of \(n\). However, uniform convergence results are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems.

Note that (4) requires an additional smoothness-type assumption. As a comparison, when \(f\) is \(\)-smoothness, our result in Theorem 3 can be easily derived as

\[\| F(A(S))- F_{S}(A(S))\|_{2}\] \[  n(1/)++_{Z}[\|f(^{*};Z)\|_{2}^{2}](1/ )}{n}}+\|A(S)-^{*}\|}.\]

This result implies that when the uniformly stable in gradients parameter \(\) is smaller than \(1/\), our bound is tighter than (4) and is dimension independent. Note that Theorem 3 holds in nonconvex problems, to the best of our knowledge, this is the sharpest upper bound in both uniform convergence and algorithmic stability analysis.

Here we give the proof sketch of Theorem 3, which is motivated by the analysis in Klochkov and Zhivotovskiy (2021). The key idea is to build vector functions \(_{i}(S)=_{i}(S)-_{S\{z_{i}\}}[_{i}(S)]\) where we define \(_{i}(S)=_{z_{i}^{}}[_{Z}[ f (A(S^{(i)}),Z)]- f(A(S^{(i)}),z_{i})]\). These functions satisfy all the assumptions in Theorem 1 and ensure the factor \(M\) in Theorem 1 to \(0\). Then the term \(O(1/)\) can be eliminated.

**Lemma 1**.: _Let assumptions in Theorem 3 hold. Suppose the function \(f\) is \(\)-smooth and the population risk \(F\) satisfies the PL condition with parameter \(\). Then for any \((0,1)\), when \(n}{^{2}}\), with probability at least \(1-\), we have_

\[\| F(A(S))- F_{S}(A(S))\|_{2}\] \[ \| F_{S}(A(S))\|_{2}+4_{2}[\|  f(^{*};Z)\|_{2}^{2}]}{n}}+2^{2}+32n^{2}(3/))}{n}}\] \[+}{n}+32 2^{} _{2}n(3e/)+64 _{2}n.\]

**Remark 5**.: The following inequality can be easily derived using triangle inequality and Cauchy-Bunyakovsky-Schwarz inequality:

\[F(A(S))-F(^{*})\| F_{S}(A(S))\|_{2}+^{*})(1/)}{n}+(1/)}{n^{2}}+^{2}^{2}n ^{2}(1/).\] (5)

Above inequality implies that excess risk can be bound by the optimization gradient error \(\| F_{S}(A(S))\|_{2}\) and uniform stability in gradients \(\). Note that the assumption \(F(^{*})=O(1/n)\) is common and can be found in Srebro et al. (2010); Lei and Ying (2020); Liu et al. (2018); Zhang et al. (2017); Zhang and Zhou (2019). This is natural since \(F(^{*})\) is the minimal population risk. On the other hand, we can derive that under \(\)-strongly convex and \(\)-smooth assumptions for the objective function \(f\), uniform stability in gradients can be bounded of order \(O(1/n)\) for ERM and PGD. Thus high probability excess risk can be bounded of order up to \(O(1/n^{2})\) under these common assumptions via algorithmic stability. Comparing with current best related work (Klochkov and Zhivotovskiy, 2021), they are insensitive to the stability parameter being smaller than \(O(1/n)\) and their best rates can only up to \(O(1/n)\). Although we involve extra smoothness and PL condition assumptions, these assumptions are also common in optimization community and our work can fully utilize these assumptions.

Besides, we discuss uniform stability in gradients for common algorithms such as ERM, PGD, and SGD in Section 4. Our results can be easily extended to other stable algorithms. Due to smoothness's property to link the uniform stability in gradients with uniform argument stability, many works (Bassily et al., 2020; Feldman and Vondrak, 2019; Hardt et al., 2016) exploring uniform argument stability can also use our framework.

Finally, the population risk of gradients \(\| F(A(S))\|_{2}\) can be gracefully bounded by the empirical risk of gradients \(\| F_{S}(A(S))\|_{2}\) under strong growth condition (SGC), that connects the rates at which the stochastic gradients shrink relative to the full gradient Vaswani et al. (2019).

**Definition 3** (Strong Growth Condition).: _We say SGC holds if_

\[_{Z}[\| f(;Z)\|_{2}^{2}]\| F ()\|_{2}^{2}.\]

**Remark 6**.: There has been some related work that takes SGC into assumption Solodov (1998); Vaswani et al. (2019); Lei (2023). Vaswani et al. (2019) has proved that the squared-hinge loss with linearly separable data and finite support features satisfies the SGC. Note that we only suppose this condition holds in Lemma 2.

**Lemma 2** (SGC case).: _Let assumptions in Theorem 3 hold and suppose SGC holds. Then for any \(>0\), with probability at least \(1-\), we have_

\[\| F(A(S))\|(1+)\| F_{S}(A(S))\|+ (+ n).\]

**Remark 7**.: Lemma 2 build a connection between the population gradient error and the empirical gradient error under Lipschitz, nonconvex, nonsmooth and SGC case and elucidate that the population gradient error can be bounded of up to \(O(1/n)\) under nonconvex problems.

## 4 Application

In this section, we analysis stochastic convex optimization with strongly convex losses. The most common setting is where at each round, the learner gets information on \(f\) through a stochastic gradient oracle (Rakhlin et al., 2012). To derive uniform stability in gradients for algorithms, we firstly introduce the strongly convex assumption.

**Definition 4**.: _We say \(g\) is \(\)-strongly convex if_

\[g() g(^{})+-^{},  g(^{})+\|-^{ }\|_{2}^{2},,^{}.\]

### Empirical Risk Minimizer

Empirical risk minimizer is one of the classical approaches for solving stochastic optimization (also referred to as sample average approximation (SAA)) in machine learning community. The following lemma shows the uniform stability in gradient for ERM under \(\)-strongly convexity and \(\)-smoothness assumptions.

**Lemma 3** (Stability of ERM).: _Suppose the objective function \(f\) is \(\)-strongly-convex and \(\)-smooth. For any \(\) and any \(z\), suppose that \(\| f(;z) M\|\). Let \(}^{*}(S^{(i)})\) be the ERM of \(F_{S^{(i)}}()\) that denotes the empirical risk on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{},...,z_{n}\}\) and \(}^{*}(S)\) be the ERM of \(F_{S}()\) on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\). For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of ERM:_

\[ z,\| f(}^{*}(S^{(i)});z )- f(}^{*}(S);z)\|_{2}.\]

Then, we present the application of our main sharper Theorem 3. In the strongly convex and smooth case, we provide a up to \(O(1/n^{2})\) high probability excess risk guarantee valid for any algorithms depending on the optimal population error \(F(^{*})\).

**Theorem 4**.: _Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function \(f\) is nonnegative. Then for any \((0,1)\), when \(n}{^{2}}\), with probability at least \(1-\), we have_

\[F(})-F(^{*})^{*})( 1/)}{n}+n^{2}(1/)}{n^{2}}.\]

_Furthermore, assume \(F(^{*})=O()\), we have_

\[F(})-F(^{*})n^{2}(1/ )}{n^{2}}.\]

**Remark 8**.: Theorem 4 shows that when the objective function \(f\) is \(\)-strongly convex, \(\)-smooth and nonnegative, high probability risk bounds can even up to \(O(1/n^{2})\) for ERM. The most related work to ours is Zhang et al. (2017). They also obtain the \(O(1/n^{2})\)-type bounds for ERM by uniform convergence of gradients approach. However, they need the sample number \(n=( d/)\), which is related to the dimension \(d\). Our risk bounds are dimension independent and only require the sample number \(n=(^{2}/^{2})\). Comparing with Klochkov and Zhivotovskiy (2021), we add two assumptions, smoothness and \(F(^{*})=O(1/n)\), but our bounds also tighter, from \(O(1/n)\) to \(O(1/n^{2})\).

### Projected Gradient Descent

Note that when the objective function \(f\) is strongly convex and smooth, the optimization error can be ignored. However, the generalization analysis framework proposed by Klochkov and Zhivotovskiy (2021) does not use smoothness assumption, which only derive high probability excess risk bound of order \(O(1/n)\) after \(T=O( n)\) steps under strongly convex and smooth assumptions. In this subsection, we provide sharper risk bound under the same iteration steps, which is because our generalization analysis also fully utilized the smooth assumptions. Here we give the definition of PGD.

**Definition 5** (Projected Gradient Descent).: _Let \(_{1}=o^{d}\) be an initial point and \(\{_{t}\}_{t}\) be a sequence of positive step sizes. PGD updates parameters by_

\[_{t+1}=_{}(_{t}-_{t} F_{S} (_{t})),\]

_where \( F_{S}(_{t})\) denotes a subgradient of \(F\) w.r.t. \(_{t}\) and \(_{}\) is the projection operator onto \(\)._

**Lemma 4** (Stability of Gradient Descent).: _Suppose the objective function \(f\) is \(\)-strongly-convex and \(\)-smooth. For any \(\) and any \(z\), suppose that \(\| f(;z)\|_{2} M\). Let \(_{t}^{}\) be the output of \(F_{S^{(i)}}()\) on \(t\)-th iteration on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{},...,z_{n}\}\) in running PGD, and \(_{t}\) be the output of \(F_{S}()\) on \(t\)-th iteration on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\) in running PGD. Let the constant step size \(_{t}=1/\). For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of PGD:_

\[ z,\| f(}^{}(S^{(i)}) ;z)- f(}^{}(S);z)\|_{2}.\]

**Remark 9**.: The derivations of Feldman and Vondrak (2019) in Section 4.1.2 (See also Hardt et al. (2016) in Section 3.4) imply that if the objective function \(f\) is \(\)-smooth in addition to \(\)-strongly convexity and \(M\)-Lipschitz property, then PGD with the constant step size \(=1/\) is \(()\)-uniformly argument stable for any number of steps, which means that PGD is \(()\)-uniformly-stable in gradients regardless of iteration steps.

**Theorem 5**.: _Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function \(f\) is nonnegative. Let \(\{_{t}\}_{t}\) be the sequence produced by PGD with \(_{t}=1/\). Then for any \((0,1)\), when \(n}{^{2}}\), with probability at least \(1-\), we have_

\[F()-F(^{})(1-)^{ 2T}+^{})(1/)}{n}+n ^{2}(1/)}{n^{2}}.\]

_Furthermore, assume \(F(^{})=O()\) and let \(T n\), we have_

\[F(})-F(^{})n^{2}(1/ )}{n^{2}}.\]

**Remark 10**.: Theorem 5 shows that under the same assumptions as Klochkov and Zhivotovskiy (2021), our bound is \(O(^{})(1/)}{n}+n^{2}(1/ )}{n^{2}})\). Comparing with their bound \(O()\), we are sharper because \(F(^{})\) is the minimal population risk, which is a common assumption towards sharper risk bounds Srebro et al. (2010); Lei and Ying (2020); Liu et al. (2018); Zhang et al. (2017); Zhang and Zhou (2019).

### Stochastic Gradient Descent

Stochastic gradient descent optimization algorithm has been widely used in machine learning due to its simplicity in implementation, low memory requirement and low computational complexity per iteration, as well as good practical behavior. Here we give the definition of standard SGD.

**Definition 6** (Stochastic Gradient Descent).: _Let \(_{1}=o^{d}\) be an initial point and \(\{_{t}\}_{t}\) be a sequence of positive step sizes. SGD updates parameters by_

\[_{t+1}=_{}(_{t}-_{t} f( _{t};z_{i_{t}})),\]

_where \( f(_{t};z_{i_{t}})\) denotes a subgradient of \(f\) w.r.t. \(_{t}\) and \(i_{t}\) is independently drawn from the uniform distribution over \([n]:=\{1,2,,n\}\)._

**Lemma 5** (Stability of SGD).: _Suppose the objective function \(f\) is \(\)-strongly-convex and \(\)-smooth. For any \(\) and any \(z\), suppose that \(\| f(;z)\|_{2} M\). Let \(_{t}^{i}\) be the output of \(F_{S^{(i)}}()\) on \(t\)-th iteration on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{},...,z_{n}\}\) in running PGD and and \(_{t}\) be the output of \(F_{S}()\) on \(t\)-th iteration on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\) in running SGD. For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of SGD:_

\[\| f(_{t};z)- f(_{t}^{i};z)\|_{2}  2(_{t})}{}}+, z,\]

_where \(_{opt}(_{t})=F_{S}(_{t})-F_{S}(}^{ }(S))\) and \(}^{}(S)\) is the ERM of \(F_{S}()\)._

Next, we introduce a necessary assumption in stochastic optimization theory.

**Assumption 1**.: _Assume the existence of \(>0\) satisfying_

\[_{i_{t}}[\| f(_{t};z_{i_{t}})- F_{S}(_{ t})\|_{2}^{2}]^{2}, t,\] (6)

_where \(_{i_{t}}\) denotes the expectation w.r.t. \(i_{t}\)._

**Remark 11**.: Assumption 1 is a standard assumption from the stochastic optimization theory (Nemirovski et al., 2009; Ghadimi and Lan, 2013; Ghadimi et al., 2016; Kuzborskij and Lampert, 2018; Zhou et al., 2018; Bottou et al., 2018; Lei and Tang, 2021), which essentially bounds the variance of the stochastic gradients for dataset \(S\).

**Theorem 6**.: _Let assumptions in Theorem 3 and Lemma 5 hold. Suppose Assumption 1 holds and the function \(f\) is nonnegative. Let \(\{_{t}\}_{t}\) be the sequence produced by SGD with \(_{t}=_{1}t^{-},(0,1)\) and \(_{1}<\). Then for any \((0,1)\), when \(n}{^{2}}\), with probability at least \(1-\), we have_

\[(_{t=1}^{T}_{t})^{-1}_{t=1}^{T}_{t}\|  F(_{t})\|_{2}^{2}\] \[=O(n^{3}(1/)}{T^{- }})+O(n^{2}(1/)}{n^{2}}+^{*})^{2}(1/)}{n}),&<1/2\\ O(n^{3}(1/)}{T^{-}})+O( {^{2}n^{2}(1/)}{n^{2}}+^{*})^{2}(1/) }{n}),&=1/2\\ O(n^{3}(1/)}{T^{-}})+O( n^{2}(1/)}{n^{2}}+^{*})^{2}(1/ )}{n}),&>1/2.\]

**Remark 12**.: When \(<1/2\), we take \(T n^{2/}\). When \(=1/2\), we take \(T n^{4}\) and when \(>1/2\), we set \(T n^{2/(1-)}\). Then according to Theorem 6, the population risk of gradient is bounded by \(O(n^{3}(1/)}{n^{2}}+^{*})^ {2}(1/)}{n})\). To the best of our knowledge, this is the first high probability population gradient bound \(\| F(_{t})\|_{2}\) for SGD via algorithmic stability.

**Theorem 7**.: _Let Assumptions in Theorem 3 and Lemma 5 hold. Suppose Assumption 1 holds and the function \(f\) is nonnegative. Let \(\{_{t}\}_{t}\) be the sequence produced by SGD with \(_{t}=)}\) such that \(t_{0}\{,1\}\). Then for any \(>0\), when \(n}{^{2}}\) and \(T n^{2}\), with probability at least \(1-\), we have_

\[F(_{T+1})-F(^{*})=O(n^{5}(1/ )}{n^{2}}+^{*})(1/))}{n}).\]

_Furthermore, assume \(F(^{*})=O()\), we have_

\[F(_{T+1})-F(^{*})=O(n^{5}(1/) }{n^{2}}).\]

**Remark 13**.: Theorem 7 implies that high probability risk bounds for SGD optimization algorithm can be up to \(O(1/n^{2})\) and the rate is dimension-free in high-dimensional learning problems. We compare Theorem 7 with most related work. For algorithmic stability, high probability risk bounds in Fan and Lei (2024) is up to \(O(1/n)\) when choosing optimal iterate number \(T\) for SGD optimization algorithm. To the best of knowledge, we are faster than all the existing bounds. The best high probability risk bounds of order \(O(1/n^{2})\) are given by Li and Liu (2021) via uniform convergence, which require the sample number \(n=( d/)\) depending on dimension \(d\).

## 5 Conclusion

In this paper, we improve a \(p\)-moment concentration inequality for sums of vector-valued functions. By carefully constructing functions, we apply this moment concentration to derive sharper generalization bounds in gradients in nonconvex problems, which can further be used to obtain sharper high probability excess risk bounds for stable optimization algorithms. In application, we study three common algorithms: ERM, PGD, SGD. To the best of our knowledge, we provide the sharpest high probability dimension independent \(O(1/n^{2})\)-type for these algorithms.