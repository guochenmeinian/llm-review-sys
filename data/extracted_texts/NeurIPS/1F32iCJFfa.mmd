# Schrodinger Bridge Flow

for Unpaired Data Translation

Valentin De Bortoli 1

Google DeepMind

Iryna Korshunova 1

Google DeepMind

Andriy Mnih

Google DeepMind

Arnaud Doucet

Google DeepMind

###### Abstract

Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another. Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DDMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively. However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties. Existing techniques approximating OT maps for high-dimensional data-rich problems, such as DDM-based Rectified Flow and Schrodinger Bridge procedures, require fully training a DDM-type model at each iteration, or use mini-batch techniques which can introduce significant errors. We propose a novel algorithm to compute the Schrodinger Bridge, a dynamic entropy-regularised version of OT, that eliminates the need to train multiple DDM-like models. This algorithm corresponds to a discretisation of a flow of path measures, which we call the Schrodinger Bridge Flow, whose only stationary point is the Schrodinger Bridge. We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks.

## 1 Introduction

The problem of finding a map to transport one probability distribution to another one has numerous applications in machine learning. In particular, it is at the core of generative modeling where the idea is to transform a noise distribution into the data distribution, and is also central to transfer learning tasks such as image-to-image translation. For discrete probability distributions, it is possible to compute the Optimal Transport (OT) map but this is computationally expensive (Peyre et al., 2019). By showing that an entropy-regularised version of OT, the Entropic OT (EOT), could be computed much more efficiently using the Sinkhorn algorithm, Cuturi (2013) has enabled transport ideas to be used in numerous applications (Ge et al., 2021; Zhou et al., 2022). However, the computational complexity of Sinkhorn algorithm is quadratic in the sample size, which makes its application to very large datasets impractical. Mini-batch versions have been proposed, see e.g. (Genevay et al., 2018), but tend to introduce significant errors in high dimensions (Sommerfeld et al., 2019).

In the context of generative modeling, Denoising Diffusion Models (DDMs) (Song et al., 2021; Ho et al., 2020) have shown impressive performance in a variety of domains. DDMs define a forward process progressively noising the data, and sample generation is achieved by approximating the time-reversal of this diffusion. In order to leverage the iterative refinement properties of DDMs in the OT setting, methods exploiting the equivalence between the static versions of (E)OT and their dynamic counterparts (Benamou and Brenier, 2000; Leonard, 2014) have been developed. A procedure to approximate the dynamic OT is considered by Liu et al. (2023), while techniques to approximate the dynamic equivalent to EOT, the Schrodinger Bridge (SB), have been proposed in (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022; Peluchetti, 2023; Shi et al., 2023). These techniques areexpensive however, as they require training multiple DDM-type models. Mini-batch versions of OT and Sinkhorn (Pooladian et al., 2023; Tong et al., 2024b) combined with bridge or flow matching have also been proposed to approximate the OT path and SB, but they optimise a minibatch OT objective that can introduce significant errors in high dimensions: the error in Wasserstein-\(1\) distance is of order \(O(B^{-1/(2d)})\), where \(d\) is the dimension of the problem and \(B\) the minibatch size, see (Sommerfeld et al., 2019, Corollary 1).

In this paper, we propose a novel approach to computing the SB. Similarly to Iterative Markovian Fitting (IMF) and its practical implementation, Diffusion Schrodinger Bridge Matching (DSBM) (Shi et al., 2023; Peluchetti, 2023), it leverages the fact that the SB is the only Markov process with prescribed marginals at the endpoints which is in the reciprocal class of the Brownian motion, i.e. it has the same bridge as the Brownian motion (Leonard, 2014); see Section 2 for more details on Markov processes and the reciprocal class. Compared to DSBM, our approach is easier to implement as it does not require caching samples, alternating between optimising two different losses, and, optionally, uses one neural network instead of two. In Section 3, we start by introducing a flow of path measures whose time-discretisation yields a family of algorithms called \(\)-IMF and presented in Section 4. Notably, we show that \(\)-IMF converges to the Schrodinger Bridge for any \((0,1]\). Additionally, for a special value of the discretisation stepsize \(=1\), we recover the IMF procedure (Peluchetti, 2023; Shi et al., 2023), while \(<1\) corresponds to online versions of IMF. We implement a parametric version of the \(\)-IMF as an online DSBM procedure, called \(\)-DSBM. We illustrate the efficiency of our approach in _unpaired_ image-to-image translation settings in Section 6.

Notation.We denote the space of _path measures_ by \(()\), i.e. \(()=((,^{d}))\), where \((,^{d})\) is the space of continuous functions from \(\) to \(^{d}\). The subset of _Markov_ path measures associated with a diffusion of the form \(_{t}=v_{t}(_{t})t+_{t}_{t}\), with \(,v\) locally Lipschitz, is denoted \(\). For \(\) induced by \((_{t})_{t}\), with \(>0\) and \((_{t})_{t 0}\) a \(d\)-dimensional Brownian motion, the _reciprocal class_ of \(\) is denoted \(()\), see Definition 2.1. For any \(()\), we denote by \(_{t}\) its marginal distribution at time \(t\), \(_{s,t}\) the joint distribution at times \(s,t\), \(_{s|t}\) the conditional distribution at time \(s\) given the state at time \(t\), and \(_{[0,1}()\) the distribution of the path on time interval \((0,1)\) given its endpoints; e.g. \(_{[0,1}\) is a scaled Brownian bridge. Unless specified otherwise, all gradient operators \(\) are w.r.t. the variable \(x_{t}\) with time index \(t\). Given probability spaces \((,)\) and \((,)\), a Markov kernel \(:~{}\), and a probability measure \(\) defined on \(\), we write \(\) for the probability measure on \(\) such that for any \(\) we have \(()=_{}(x,)(x)\). In particular, for any joint distribution \(_{0,1}\) over \(^{d}^{d}\), we denote the _mixture of bridges_ measure as \(=_{0,1}_{[0,1}()\), which is short for \(()=_{^{d}^{d}}_{[0,1}(|x_ {0},x_{1})_{0,1}(x_{0},x_{1})\). Finally, we define the Kullback-Leibler (KL) divergence between two probability measures \(_{0},_{1}()\) as \((_{0}|_{1})=_{}((_{0}/_{1})(x))_{0}(x)\) if \(_{0}\) is absolutely continuous w.r.t. \(_{1}\) and \((_{0}|_{1})=+\) otherwise.

## 2 Optimal Transport and Schrodinger Bridge

Unpaired Transfer and Optimal Transport.Given unpaired data samples from \(_{0}\) and \(_{1}\), where \(_{0},_{1}\) are two distributions on \(^{d}\), we are interested in designing a transport map from \(_{0}\) to \(_{1}\). This corresponds to an _unpaired data transfer task_. We can formulate this problem as finding a distribution \(\) on \(^{d}^{d}\) with marginals \(_{0}=_{0}\) and \(_{1}=_{1}\) so that if \(_{0}_{0}\) then \(_{1}|_{0}_{1|0}(|_{0})\) satisfies \(_{1}_{1}\). Among an infinite number of such so-called coupling distributions \(\), we are here interested in finding the Entropic Optimal Transport (EOT) coupling \(^{}\) defined as

\[^{}=_{(^{d}^{ d})}\{_{^{d}^{d}}\|x-y\|^{2} (x,y)-()\;;\;_{0}=_{0},\;_{1}= _{1}\},\] (1)

where \(()\) is the differential entropy of \(\) and \(>0\) is a regularisation hyperparameter (Peyre et al., 2019). For \(=0\), we recover the standard OT.

In order to leverage the recent advances in generative modeling, and in particular the concept of _iterative refinement_ central to DDMs, we turn to a _dynamic_ formulation of EOT known as the _Schrodinger Bridge_ problem (Leonard, 2014). It is defined as follows: find \(^{}()\) such that

\[^{}=_{()}\{ (|)\;;\;_{0}=_{0},\;_{1}= _{1}\},\] (2)with \(()\) induced by a scaled \(d\)-dimensional Brownian motion \((_{t})_{t}\). The term _dynamic_ here refers to the fact that (2) is defined on path measures, i.e. on (stochastic) processes, in contrast to the _static_ problem (1) which is defined on measures on the space \(^{d}^{d}\). In Section 3, we show that solving (2) is equivalent to optimising the vector field of a stochastic process using objectives similar to the ones of bridge matching (Peluchetti, 2021; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023a). Under mild assumptions, it can be shown that \(_{0,1}^{}=^{}\), see e.g. (Leonard, 2014; Pavon et al., 2021). Hence solving (1) reduces to solving (2). Once we have found \(^{}\) associated with \((_{t}^{})_{t}\), we can sample from \(^{}\) by first sampling \(_{0}^{}_{0}\) and then sampling the trajectory \((_{t}^{})_{t(0,1]}\) which yields \((_{0}^{},_{1}^{})^{}\).

Reciprocal and Markov projections.To introduce our methodology, it is necessary to recall the notions of reciprocal and Markov projections. We refer to Shi et al. (2023) for more details. For practitioners, a more intuitive explanation of these projections is given in Appendix E.

**Definition 2.1** (Reciprocal projection): \(()\) _is in the reciprocal class \(()\) of \(\) if \(=_{0,1}_{0,1}\). We define the reciprocal projection of \(()\) as \(^{}=_{()}()= _{0,1}_{0,1}\). We will write \(_{}\) instead of \(_{()}\) to simplify notation._

In other words, \(\) is in the reciprocal class of \(\) if the conditional distribution of a path given its endpoints is identical under \(\) and \(\), see (Relly, 2013). Sampling from the reciprocal projection of \(\) can be achieved by sampling a path \((_{t})_{t}\) from \(\), keeping only the values of the endpoints, say \(_{0},_{1}\), and then sampling a new value for the bridge \((_{t})_{t(0,1)}\) from \(_{|0,1}\).

**Definition 2.2** (Markov projection): _Assume that \(\) is induced by \((_{t})_{t}\) for \(>0\). Then, when it is well-defined, for any \(()\), the Markovian projection \(=_{}()\) is the path measure induced by the diffusion \((_{t}^{})_{t}\) with for any \(t\)_

\[_{t}^{}=v_{t}^{}(_{t}^{})t+_{t}, v_{t}^{}(x_{t})= (_{_{1|t}}[_{1}_{t}=x_{t}]- x_{t})/(1-t),_{0}^{}_{0}.\]

In practice, implementing a Markovian projection requires solving a regression problem to approximate \(_{_{1|t}}[_{1}_{t}=x_{t}]\), similar to the one appearing in bridge matching and flow matching. One key property of the Markovian projection is that \(_{t}=_{t}\) for all \(t\), i.e. the Markovian projection preserves the marginals; see (Peluchetti, 2021) for instance.

Iterative Markovian Fitting.Leveraging the reciprocal and Markovian projections, Peluchetti (2023) and Shi et al. (2023) concurrently introduced IMF. Starting from \(}^{0}=(_{0}_{1})_{0,1}\), a measure where endpoints are sampled independently from \(_{0}\) and \(_{1}\) and then interpolated using a (scaled) Brownian bridge, they define a sequence of path measures \((^{n},}^{n})_{n}\) where \(^{n}=_{}(}^{n})\) and \(}^{n+1}=_{}(^{n})\). This ensures that \(_{0}^{n}=_{0}\), \(_{1}^{n}=_{1}\) for all \(n\), and it can be shown that the sequence \((^{n})_{n}\) converges to the SB, see (Peluchetti, 2023, Theorem 2). The practical implementation of this algorithm proposed by Shi et al. (2023) is called DSBM. Implementing DSBM poses challenges, as each Markovian projection requires training a neural network to approximate the relevant conditional expectations by minimising a bridge matching loss. Furthermore, in practice, generated model samples are stored in a cache in order to train the next iterations of DSBM. This introduces additional hyperparameters that require tuning. In Section 3 we propose \(\)-IMF, an algorithm which can be interpreted as the discretisation of a _flow of path measures_. This leads to \(\)-DSBM, an algorithm that is computationally much more efficient than DSBM as it does not rely on a Markovian projection at each step.

## 3 Schrodinger Bridge flow

We will now introduce a flow of path measures \((^{s})_{s 0}\), and show that the time-discretisation of this flow with an appropriate stepsize \((0,1]\) yields a family of procedures called \(\)-IMF, which all converge to the Schrodinger Bridge. While \(=1\) yields the classical IMF, \((0,1)\) yields an _incremental_ version of IMF. In Section 4 we show that \(\)-IMF can be implemented as an _online_ version of DSBM.

### A flow of path measures

Let \((^{s},}^{s})_{s 0}\) be a _flow of path measures_ defined for any \(s 0\) by

\[}^{0}=(_{0}_{1})_{|0,1},_{s} }^{s}=_{}(_{ }(}^{s}))-}^{s},^{s}= _{}(}^{s}),\] (3)

which we assume is well-defined. Note that for any \(s 0\), \(^{s}\) is Markov while \(}^{s}\) is in the reciprocal class of \(\). Crucially, the only fixed point of (3) is the SB. Indeed, let \(}\) be a fixed point of \((^{s})_{s 0}\) in (3). Then, we have that \(}=_{}(_{ }(}))\). Hence, we get \(}=_{}(_{ }((_{}(_{ }(})))))\). Hence, under mild assumptions, \(}\) is a limit point of IMF and therefore \(}\) is the SB \(^{*}\) given by (2), see (Peluchetti, 2023, Theorem 2).

Next, for any \((0,1]\), we define the following discretisation of (3) called \(\)-IMF:

\[}^{n+1}=(1-)}^{n}+_{}(_{}(}^{n})),\] (4)

and \(^{n}=_{}(}^{n})\). Note that for any \(n\), \(}^{n}()\). This recovers the IMF procedure (Shi et al., 2023; Peluchetti, 2023) when \(=1\). Using the definition of the sequence \((}^{n})_{n}\), it is possible to analyse the sequence \((}^{n})_{n}\) using the properties of the KL divergence as well as the Pythagorean identities derived in (Shi et al., 2023; Peluchetti, 2023). We first introduce some assumptions on the Schrodinger Bridge problem. We recall that the differential entropy of a probability measure \(\) is given by

\[()=-_{^{d}}((/)(x))(x),\]

if \(\) admits a density with respect to the Lebesgue measure and \(+\) otherwise. Recall that \(\) is associated with \((_{t})_{t}\) and assume that \(_{0}=\). Let \(_{0},_{1}(^{d})\) such that

\[_{^{d}}\|x\|^{2}_{i}(x)<+,( _{i})<+,\]

for \(i\{0,1\}\). Under these assumptions, we can use the characterisation of the SB as the only path measure that preserves \(_{0}\), \(_{1}\), and is both Markov and in the reciprocal class of \(\) (see e.g. (Leonard, 2014, Theorem 2.12)). We get the following result.

**Theorem 3.1** (Convergence of \(\)-Imf): _Let \((0,1]\) and \((^{n},}^{n})_{n}\) defined by (4). Under mild assumptions, we have that \(_{n+}^{n}=^{}\), where \(^{}\) is the solution of the Schrodinger Bridge problem (2)._

### Discretisation and non-parametric loss

We show here that \(\)-IMF is associated with an _incremental_ version of DSBM for \((0,1)\).

Iterative Markovian Fitting.For any \(v:\;^{d}^{d}\), we introduce the loss function

\[(v,)=_{0}^{1}_{t}(v_{t},)t=_{0}^{1}_{(^{d})^{3}}v_{t}(x_{t})--x_{t}}{1-t} ^{2}_{0,1}(x_{0},x_{1})_{t|0,1 }(x_{t}|x_{0},x_{1})t,\] (5)

where we recall that \(\) is induced by \((_{t})_{t}\) for some \(>0\). This loss was already considered in (Peluchetti, 2021; Lipman et al., 2023; Liu et al., 2023; Liu, 2022; Shi et al., 2023). We also define the path measure \(_{v}()\) associated with

\[_{t}=v_{t}(_{t})t+ _{t},_{0}_{0}.\] (6)Consider first the sequence \((v^{n})_{n}\) defined by

\[v^{n+1}=*{argmin}_{v}(v,_{v^{n}}).\] (7)

Using Definition 2.2, we have that \(_{v^{n+1}}=*{proj}_{}(*{proj}_{ }(_{v^{n}}))\), which corresponds to \(^{n+1}\) in the IMF sequence. Therefore we have that \(_{n+}_{v^{n}}=^{}\) under mild assumptions (Peluchetti, 2023, Theorem 2).

Functional gradient descent.We now introduce a relaxation of (7), where, instead of considering the \(*{argmin}\), we update the vector field with one gradient step. To define this relaxation, we recall that for a functional \(F:\;\), where \(\) is an appropriate function space, its functional derivative (Courant and Hilbert, 2008) with reference measure \(\) is denoted \(_{}F\) and is given for any \(\), when it exists, by

\[_{ 0}(F(f+)-F(f))/=_{}F(f)(x ),(x)(x).\]

Initialised with \(v_{t}^{0}(x)=(_{}^{0}_{1|t}}[_{1} _{t}=x]-x)/(1-t)\), where \(}^{0}=(_{0}_{1})_{|0,1}\), we now introduce a sequence of vector fields \((v^{n})_{n}\). This corresponds to training a bridge matching model (see e.g. Liu et al. (2023); Albergo et al. (2023)), giving \(_{v^{0}}=*{proj}_{}(}^{0})\). Then for \(n\), let

\[v_{t}^{n+1}(x)=v_{t}^{n}(x)-_{n}_{^{n}}_{t}(v_{t}^{ n},_{v^{n}})(x),\] (8)

with \(_{n}>0\) and \(^{n}()\). The parameters \((_{n},^{n})_{n}\) will be made explicit in Proposition 3.2. We emphasize that, in contrast to the IMF procedure, in the online update (8) we do not need to solve a Markovian projection problem at every step; instead we simply take a gradient step on the loss (5).

Connection with \(\)-Iwf.The following proposition shows that \((_{v^{n}})_{n}\) defined by (8) is associated with \(\)-IMF defined in (4).

**Proposition 3.2** (Non-parametric updates are \(\)-Imf): _Let \((0,1],\;(^{n},}^{n})_{n}\) as in (4), \(_{n}=\) and \(^{n}=(1-)}^{n}+_{}( ^{n})\). Then, under mild assumptions, we have \(_{v^{n}}=^{n}\) for all \(n\)._

Combining Theorem 3.1 to Proposition 3.2, we get that \(_{n+}_{v^{n}}=^{}\), i.e. the non-parametric procedure converges to the SB.

## 4 \(\)-Diffusion Schrodinger Bridge Matching

From DSBM to \(\)-Dsmg.In Section 3, we introduced \(\)-IMF, a scheme which defines a sequence of path measures converging to the SB for all \((0,1]\). For \(=1\), this corresponds to the IMF, whose practical DSBM implementation (Shi et al., 2023) requires repeatedly solving an expensive minimisation problem (7). In contrast, for \(<1\) we are only required to take one (non-parametric) gradient step to update the vector field, see (8). This suggests the following practical implementation of \(\)-IMF, called \(\)-DSBM: First, pretrain a bridge matching model so that for \(t\) and \(x^{d}\), \(v_{t}^{}(x)=(_{}^{0}_{1|t}}[_{t} _{t}=x]-x)/(1-t)\), where \(}^{0}=(_{0}_{1})_{|0,1}\). Then, perform the parametric version of the update (8):

\[-_{}(,_{ });\;(,)=_{0}^{1}_{(^{ d})^{3}}v_{t}^{}(x_{t})--x_{t}}{1-t}^{2} _{0,1}(x_{0},x_{1})_{|0,1}(x_{t}|x_{0},x_{1})t,\] (9)

where \(_{}\) is a stop-gradient version of \(_{v^{}}\). In Appendix D.2, we give a theoretical justification for this parametric equivalent of (5) and (8) by showing that, as \( 0\), the update on the velocity fields \(v^{b}\) given by (9) corresponds to a direction of descent for the non-parametric loss (8) on average. Once again, we emphasize that if we replace the gradient step in (9) with the minimisation \(*{argmin}_{}(,_{ })\), we recover DSBM.

Bidirectional online procedure.As with DSBM, directly implementing (9) leads to error quickly accumulating, see Appendix I for details. One way to circumvent this error accumulation issue is to consider a _bidirectional_ procedure, in which we train both a forward and a backward model. This is possible because the Markovian projection coincides for forward and backward path measures, see (Shi et al., 2023, Proposition 9). This suggests considering the loss \((v^{*},v^{},^{*},^{*})=_{0}^{1} _{t}(v_{t}^{*},v_{t}^{},^{*},^{*}) t\), which is an extension of (5), where

\[_{t}(v_{t}^{*},v_{t}^{*},^{*},^{*}) =_{(^{d})^{3}}\|v_{t}^{*}(x_{t})--x_ {t}}{1-t}\|^{2}\!_{0,1}^{*}(x_{0},x_{1}) _{t|0,1}(x_{t}|x_{0},x_{1})\] (10) \[+_{(^{d})^{3}}\|v_{1-t}^{}(x_{t} )--x_{t}}{t}\|^{2}\!_{0,1}^{*}(x_{0},x_{ 1})_{t|0,1}(x_{t}|x_{0},x_{1}).\]

Similarly to (6), we define \(_{v^{*}},_{v^{*}}\), associated with \((_{t})_{t}\) and \((_{1-t})_{t}\) respectively, which are defined by forward and backward SDEs

\[_{t}=v_{t}^{*}(_{t})t+ _{t},\;_{0}_{0},\; {(bwd): }_{t}=v_{t}^{}(_{t})t+_{t},\;_{0}_{1}.\] (11)

Similarly to (8), we define non-parametric updates for any \(n\), \(t\) and \(x^{d}\)

\[(v_{t}^{n+1,}(x),v_{t}^{n+1,}(x))=(v_{t}^{n,}(x),v_{ t}^{n,}(x))-_{n}_{^{n}}_{t}(v_{t}^{n, }(x),v_{t}^{n,}(x),_{v^{n,}},_{v^{n, }})(x).\]

We have the following proposition which ensures our bidirectional procedure is still valid and that the results of Proposition 3.2 still hold.

**Proposition 4.1** (**Bidirectional updates)**:: _Let \((0,1]\). For any \(n\), define \((^{n},}^{n})_{n}\) by (4). Then, under mild assumption and assuming that \(_{n}=\) and \(^{n}=(1-)}^{n}+_{}( ^{n})\), we have that for any \(n\), \(_{v^{n,}}=_{v^{n,}}=^{n}\)._

In Appendix I, we show that in the Gaussian setting the bidirectional procedure (4.1) does not accumulate error when the vector field is approximated, while the unidirectional one (8) does.

Vector field parameterisation.Contrary to existing procedures (Shi et al., 2023, Peluchetti, 2023, Liu, 2022), we do not parameterise \(v^{*}\) and \(v^{*}\) using two separate networks. Instead, we consider an additional input \(s\{0,1\}\) such that \(v_{}(1,) v^{*}\) and \(v_{}(0,) v^{*}\). This allows us to substantially reduce the number of parameters in the model. The conditioning on \(s\) in the network is detailed in Appendix K. Before stating our full algorithm in Algorithm 1, we introduce a batched parametric version of (10). For ease of notation, we write \(_{t}\) for the operation corresponding to sampling from \(_{t|0,1}\), i.e.

\[_{t}(_{0},_{1},)=(1-t)_ {0}+t_{1}+t.\] (12)

We are now ready to introduce the batched parametric version of (10). For a given batch of inputs \(_{0}^{1:B}\) and \(_{1}^{1:B}\), timesteps \(t()^{ B}\), and \(_{t}=_{t}(_{0},_{1},)\) with \((0,)^{ B}\), we compute the empirical forward and backward losses as

\[^{*}(;t,_{1},_{t}) =_{i=1}^{B}\|v_{}(1,t^{i},_{ t}^{i})-(_{1}^{i}-_{t}^{i})/(1-t^{i})\| ^{2},\] (13) \[^{*}(;t,_{0},_{t}) =_{i=1}^{B}v_{}(0,1-t^{i}, _{t}^{i})-(_{0}^{i}-_{t}^{i})/t^ {i}^{2}.\]

We present the resulting \(\)-DSBM in Algorithm 1. Note that in this algorithm, we maintain an Exponential Moving Average (EMA) of model parameters, as is common in diffusion models (Nichol and Dhariwal, 2021). During the finetuning stage, when we generate samples to use as model's inputs, we then have a choice of sampling using the EMA or non-EMA parameters. At test time, we always sample using the EMA parameters, as it is known to improve the visual quality (Song and Ermon, 2020). In Algorithm 1, we specify \((0,1]\) as a stepsize parameter. In practice, we use Adam (Kingma and Ba, 2015) for optimization, thus the choice of \(\) is implicit and adaptive throughout the training. To emphasize the importance of the parameter \(\), we sweep over its value with an explicit solver SGD in a toy setting, see Appendix K.2. We refer to Appendix K for more details on our experimental setup.

```
1:Input: datasets \(_{0}\) and \(_{1}\), entropic regularisation \(\), number of pretraining and finetuning steps \(N_{}\) and \(N_{}\), batch size \(B\) and half batch size \(b=B/2\), EMA decay \(\), initial parameters \(\) and initial EMA parameters \(^{}=\), \((0,1]\)
2:for\(n\{1,,N_{}\}\)do
3: Sample \((_{0},_{1})(_{0}_{1})^{ B}\)
4: Sample \(t()^{ B}\) and \((0,)^{ B}\) and compute \(_{t}=_{t}(_{0},_{1},)\)
5: Update \(\) with a gradient step on \([^{*}(t^{1:b},_{1}^{1:b},_{t}^{1: b})+^{*}(t^{b+1:B},_{0}^{b+1:B},_{t}^{b+1:B} )]\)
6: Update EMA parameters: \(^{}=^{}+(1-)\)
7:endfor
8:for\(n\{1,,N_{}\}\)do
9: Sample \((_{0},_{1})(_{0}_{1})^{ b}\)
10: Sample \(}_{1}\) solving forward SDE (11)-(fwd) with \(v_{^{}}(1,)\) or \(v_{}(1,)\) starting from \(_{0}\)
11: Sample \(}_{0}\) solving backward SDE (11)-(bwd) with \(v_{^{}}(0,)\) or \(v_{}(0,)\) starting from \(_{1}\)
12: Sample \(t^{*}()^{ b}\) and \(^{*}(0,)^{ b}\) and compute \(_{t}^{*}=_{t^{*}}(}_{0},_{1}, ^{*})\)
13: Sample \(t^{*}()^{ b}\) and \(^{*}(0,)^{ b}\) and compute \(_{t}^{*}=_{t^{*}}(_{0},}_{1}, ^{*})\)
14: Update \(\) with a gradient step on \([^{*}(t^{*},_{1},_{t}^{*})+^{*}(t ^{*},_{0},_{t}^{*})]\) and stepsize \(\)
15: Update EMA parameters: \(^{}=^{}+(1-)\)
16:endfor
17:Output:\((,^{})\) parameters of the finetuned model ```

**Algorithm 1**\(\)-Diffusion Schrodinger Bridge Matching

## 5 Related work

Solving Schrodinger Bridge problems.Schrodinger Bridges (Schrodinger, 1932) have been thoroughly studied through the lens of probability theory (Leonard, 2014) and stochastic control (Dai Pra, 1991; Chen et al., 2021). They recently found applications in generative modeling and related fields leveraging recent advances in diffusion models (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022). Extensions of these methods to other machine learning problems and modalities were studied in (Shi et al., 2022; Thornton et al., 2022; Liu et al., 2022; Chen et al., 2023; Tamir et al., 2023). Shi et al. (2023); Peluchetti (2023) concurrently introduced the DSBM algorithm which relies on a new procedure called IMF, while the DSB algorithm introduced in (De Bortoli et al., 2021) is based on the standard Iterative Proportional Fitting (IPF) scheme. Neklyudov et al. (2023, 2023); Liu et al. (2022) generalise DSBM to arbitrary cost functions, albeit at the expense of having to learn the reciprocal projection which is no longer given by a Brownian bridge. These new methodologies translate to improved numerics when compared to their IPF counterparts, but they remain reliant on alternating between the optimisation of two losses. Finally, we note that the Schrodinger Bridge flow and the \(\)-IMF procedure can be linked to the Sinkhorn flow recently introduced by Karimi et al. (2024), see Appendix H.1 for a detailed discussion.

Sampling-free methodologies.Sampling-free methodologies have been proposed to solve OT related objectives. In (Liu et al., 2023; Somnath et al., 2023; Diefenbacher et al., 2024; Cao et al., 2024), the authors perform one step of DSBM, i.e. only consider the pretraining stage of our algorithm. While the obtained bridge might enjoy transport properties, it does not solve an OT problem. In another line of work, Pooladian et al. (2023); Tong et al. (2024, 2024); Eyring et al. (2024) have proposed simulation-free methods to minimise OT objectives. However, they target not the OT problem, but a minibatch version of it which coincides with OT only in the limit of infinite batch size, see (Pooladian et al., 2023, Theorem 4.2). Other sampling-free methods to solve the Schrodinger Bridge problem include Kim et al. (2024); Gushchin et al. (2024) both of which rely on adversarial losses to solve the OT problem. In (De Bortoli et al., 2021; Vargas et al., 2021; Liu et al., 2022; Shi et al., 2023; Peluchetti, 2023) the adversarial objective is dropped and instead the procedure requires alternating objectives during training and is not sampling-free. We also highlight the line of work of Korotin et al. (2024); Gushchin et al. (2024) in which the Schrodinger Bridge potentials are parameterised with mixtures of Gaussians, allowing for fast training in small dimensions. Finally, recently Deng et al. (2024) introduced a variation on Schrodinger Bridge for generative modeling, which while still not sampling-free, does not require learning a forward process.

## 6 Experiments

In this section, we illustrate the efficiency of \(\)-DSBM on different tasks. In Section 6.1, we compare \(\)-DSBM to DSBM in a Gaussian setting where the EOT coupling is tractable and show that \(\)-DSBM recovers the solution faster than DSBM. In Section 6.2, we illustrate the scalability of our method through a range of unpaired image translation experiments.

### Gaussian case

We compare \(\)-DSBM to DSBM in the Gaussian setting where \(_{0}=(0,_{0}^{2})\), \(_{1}=(0,_{1}^{2})\) and \(\) is associated with \((_{t})_{t}\) with \(=0.5\). In this case, the EOT coupling is \((0,_{})\), with \(_{}\) given by

\[_{}=_{0}^{2}&_{ }^{2}\\ _{}^{2}&_{1}^{2},_{}^{2}=(1/2)((_{0}^{2}_{1}^{2}+ ^{2})^{1/2}-),\]

with \(\) being a \(d d\) identity matrix. We consider \(d=50\), \(_{0}=_{1}=1\), resulting in \(_{}^{2} 0.88\). To showcase the robustness of \(\)-DSBM, we consider the initial coupling \(_{0,1}\), where \((_{0},_{1})_{0,1}\), \(_{0}(0,)\), \(_{1}=-_{0}\), and let \(}^{0}=_{0,1}_{|0,1}\). In this setting, the base model, i.e. bridge matching, significantly underestimates the true covariance \(_{}^{2}\), as shown in Section 6.1. Additionally, the figure illustrates that online finetuning approaches the true solution faster than the original iterative DSBM finetuning. For the latter, we can set how often we alternate between updating the forward and backward networks, and as this frequency increases, the behaviour approaches that of the online finetuning.

Full covariance Gaussian case.Let \(_{0}=(_{0},_{0})\), \(_{1}=(_{1},_{1})\) with \(_{i}=+Z_{i}Z_{i}^{}\) for \(i\{0,1\}\) and \(Z_{0},Z_{1}\) independent \(d d\) matrices with unit Gaussian entries. We also set \(_{0}=_{1}=0\). We consider the Entropic Optimal Transport (EOT) with regularization \(=0.5\) and \(d=3\), given by

\[=(_{},_{}),_{}= _{0}&_{}\\ _{}^{}&_{1},\]

with \(_{}=[_{0}^{1/2}_{}_{0}^{ -1/2}-^{2}]\), with \(_{}=(4_{0}^{1/2}_{1}_{0}^{1/2}+^{4} )^{1/2}\). Let \(=\|-\|_{}/\|\|_{ }\) be the normalized Frobenius distance between matrices \(\) and \(\). The results are presented in Section 6.1 and confirm those presented in the original manuscript considering a diagonal covariance.

Figure 2: Evolution of the covariance during online and iterative DSBM finetuning for forward and backward networks. The finetuning starts after 10K steps of training a bridge matching model. For the iterative case, we alternate between forward and backward updates with varying frequencies, i.e. changing after 1K, 2.5K and 5K steps. **Left**: Gaussian with scalar covariance matrix. **Right**: Gaussian with full covariance matrix. We compute the normFrob between \(_{}\) and its estimate using Bridge Matching (Base), \(\)-DSBM (Online), and DSBM (Iterative with @xK training steps per model fit)

### Image datasets

Similarly to Shi et al. (2023), we apply our method to image translation problems, such as MNIST digits to EMNIST letters (LeCun and Cortes, 2010; Cohen et al., 2017), Wild to Cat domains from the Animal Faces-HQ (AFHQ) dataset (Choi et al., 2020), downsampled to 64 \(\) 64 and 256 \(\) 256 resolutions and CelebA \(64 64\).

The whole training procedure can be framed as a two-stage process: first, we train a base model on the true data samples, performing bridge matching (Peluchetti, 2021; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023a), and then we finetune this model. We compare models that combine different vector field parameterisations (two networks vs. one bidirectional net), finetuning methods (iterative vs. online), and sample generation strategies during the finetuning stage.

Following the established practice (Choi et al., 2020), we evaluate our models using FID (Heusel et al., 2017) for visual quality, and mean squared distance (MSD) or LPIPS (Zhang et al., 2018) for alignment. It is important to note that for image translation tasks at hand, FID scores are not ideal, as FID was designed for natural RGB images, which is not the case for MNIST. It is also not well suited for small sample sizes as it is the case with AFHQ, where the test set in each domain has fewer than 500 examples. Thus quantitative results in Table 1 should be interpreted cautiously, and we recommend a visual inspection of samples to complement these quantitative measures, especially for the AFHQ models. Samples from the models along with the training and evaluation protocols are given in Appendix K.

Compared to the iterative DSBM, our online finetuning \(\)-DSBM reduces the number of tunable hyperparameters, i.e. inner and outer iterations, refresh rate and the size of the cache for storing generated samples. This simplifies implementation and makes the algorithm more practical. The primary remaining hyperparameter, the variance of a Brownian motion \(\), requires careful tuning as it influences the trade-off between the visual quality and alignment, as was also observed in Shi et al. (2023). An appropriate \(\) needs to balance the two: setting \(\) too low results in poor visual quality, while high values of \(\) cause poorly aligned and oversmoothed samples. Figure 3 illustrates how FID and MSD metrics vary with \(\) for the case of MNIST. Additionally, it demonstrates the impact of \(\) on the generated samples for the AFHQ-64 model.

We run \(\)-DSBM on CelebA with image size \(64 64\) with \(=2.0\). We do not change the training hyper-parameters compared to AFHQ. Visual results are reported in Figure 5 and Figure 6. In Figure 5, we show the influence of \(\) during the pretraining. The visual quality of the transfer is much lower for \(=0\) than for \(=2.0\). The case \(=0\) corresponds to the first step of Rectified Flow (i.e. Flow Matching). Given the poor quality of the samples, we do not perform finetuning with \(=0\). In Figure 6, we compare the visual quality and alignment of DSBM and \(\)-DSBM after \(4000\) training steps, corresponding to two outer DSBM iterations. In this case DSBM is trained with a bidirectional network and both procedures consist of finetuning the pretrained model obtained with \(=2.0\). We note that the alignment is better in the case of \(\)-DSBM.

Figure 3: **Left**: FID and Mean Squared Distance (MSD) on EMNIST to MNIST translation before and after finetuning with different values of \(\). **Right**: AFHQ-64 samples after the finetuning. For both, we use a bidirectional model with online finetuning. More results are in Appendix K.3 and K.4.

## 7 Discussion

In this paper we have introduced \(\)-Diffusion Schrodinger Bridge Matching (\(\)-DSBM), a new methodology to solve Entropic Optimal Transport problems. \(\)-DSBM is an improved version of DSBM, which does not require training multiple DDM-type models. We have shown that a non-parametric version of this method recovers the Schrodinger Bridge (SB). In addition, \(\)-DSBM is easier to implement than existing SB methodologies while exhibiting similar performance. We illustrated the efficiency of our algorithm on a variety of unpaired transfer tasks.

While \(\)-DSBM solves one of the most critical limitations of DBSM, namely the alternative optimisation, several issues remain to be addressed in order for the method to scale comparably to generative DDMs. In particular, the method is not sampling-free, as during training it requires sampling from the model from the previous iteration to obtain the training data for the current iteration. While it seems difficult to derive a completely sampling-free method to solve SB problems without resorting to the Minibatch OT approximation, there is still room for improvement.

    &  &  \\   & FID & MSD & FID & LPIPS \\  DSBM* & 10.59 & 0.375 & – & – \\  Pretrained two-networks model & 6.02 & 0.564 & 25.97 & 0.589 \\ (a) iterative finetuning & 5.25\(\)0.15 & 0.345\(\)0.001 & 25.41\(\)0.84 & 0.485\(\)0.003 \\ (b) online finetuning & 4.28\(\)0.07 & 0.368\(\)0.001 & 28.752\(\)1.191 & 0.487\(\)0.003 \\ (c) online finetuning without EMA & 4.23\(\)0.171 & 0.361\(\)0.002 & 32.665\(\)0.647 & 0.445\(\)0.002 \\  Pretrained bidirectional model & 6.33 & 0.572 & 29.44 & 0.584 \\ (d) online finetuning & 4.39\(\)0.09 & 0.387\(\)0.003 & 26.579\(\)0.434 & 0.482\(\)0.001 \\ (e) online finetuning without EMA & 4.57\(\)0.17 & 0.369\(\)0.003 & 30.638\(\)1.023 & 0.451\(\)0.002 \\   

Table 1: Results of image translation between EMNIST and MNIST, and AFHQ 64\(\)64 between Wild and Cat domains. DSBM* results are from Shi et al. (2023). Our reimplementation of DSBM corresponds to row (a). For MNIST and AFHQ models, we used \(=1\) and \(=0.75^{2}\), respectively. Each finetuning run was done with 5 random seeds, and we report mean scores \(\) standard deviation.

Figure 4: Online DSBM transfer results on AFHQ 256\(\) 256 dataset between Cat and Wild domains. Top row—initial samples, bottom row—transferred samples.

Figure 5: Translation Female \(\) Male on CelebA. Left: pretraining with \(=0\). Right: pretraining with \(=2.0\).