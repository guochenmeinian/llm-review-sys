# OpenMixup: Open Mixup Toolbox and Benchmark

for Visual Representation Learning

Siyuan Li\({}^{1}\) Zedong Wang\({}^{1}\) Zicheng Liu\({}^{1}\) Di Wu\({}^{1}\) Cheng Tan\({}^{1}\) Weiyang Jin\({}^{1}\) Stan Z. Li\({}^{1}\)

\({}^{1}\)AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China

{lisiyuan, wangzedong, liuzicheng, wudi, tancheng, weiyangjin, stan.zq.li}@westlake.edu.cn

Equal contribution. Corresponding author.

###### Abstract

Mixup augmentation has emerged as a powerful technique for improving the generalization ability of deep neural networks. However, the lack of standardized implementations and benchmarks has hindered progress, resulting in poor reproducibility, unfair comparisons, and conflicting insights. In this paper, we introduce OpenMixup, the _first_ mixup augmentation benchmark for visual representation learning, where 18 representative mixup baselines are trained _from scratch_ and systematically evaluated on 11 image datasets across varying scales and granularity, spanning fine-grained scenarios to complex non-iconic scenes. We also open-source a modular codebase for streamlined mixup method design, training, and evaluations, which comprises a collection of widely-used vision backbones, optimization policies, and analysis toolkits. Notably, the codebase not only underpins all our benchmarking but supports broader mixup applications beyond classification, such as self-supervised learning and regression tasks. Through extensive experiments, we present insights on performance-complexity trade-offs and identify preferred mixup strategies for different needs. To the best of our knowledge, OpenMixup has contributed to a number of studies in the mixup community. We hope this work can further advance reproducible mixup research and fair comparisons, thereby laying a solid foundation for future progress. The source code is publicly available.

## 1 Introduction

Data mixing, or mixup, has proven effective in enhancing the generalization ability of DNNs, with notable success in visual classification tasks. The pioneering Mixup  proposes to generate mixed training examples through the convex combination of two input samples and their corresponding one-hot labels. By encouraging models to learn smoother decision boundaries, mixup effectively reduces overfitting and thus improves the overall performance. ManifoldMix  and PatchUp  extend this operation to the hidden space. CutMix  presents an alternative approach, where an input rectangular region is randomly cut and pasted onto the target in the identical location. Subsequent works [5; 6; 7] have focused on designing more complex _hand-crafted_ policies to generate diverse and informative mixed samples, which can all be categorized as _static_ mixing methods.

Figure 1: Radar plot of top-1 accuracy for representative mixup baselines on 11 classification datasets.

Despite efforts to incorporate saliency information into _static_ mixing framework , they still struggle to ensure the inclusion of desired targets in the mixed samples, which may result in the issue of label mismatches. To address this problem, a new class of optimization-based methods, termed _dynamic_ mixing, has been proposed, as illustrated in the second row of Figure 2. PuzzleMix  and Co-Mixup  are two notable studies that leverage optimal transport to improve offline mask determination. More recently, TransMix , TokenMix , MixPro , and SMMix  are specifically tailored for Vision Transformers . The AutoMix series  introduces a brand-new mixup learning paradigm, where mixed samples are computed by an online-optimizable generator in an end-to-end manner. These emerging _dynamic_ approaches represent a promising avenue for generating semantically richer training samples that align with the underlying structure of input data.

**Why do we call for a mixup augmentation benchmark?** While _dynamic_ methods have shown signs of surpassing the _static_ ones, their indirect optimization process incurs significant computational overhead, which limits their efficiency and applicability. Therefore, without a systematic understanding, it is uncertain if _dynamic_ mixup serves as the superior alternative in vision tasks. Moreover, a thorough and standardized evaluation of different _dynamic_ methods is also missing in the community. Benchmark is exactly the way to establish such an understanding, which plays a pivotal role in driving research progress by integrating an agreed-upon set of tasks, impartial comparisons, and assessment criteria. To the best of our knowledge, however, there have been no such comprehensive benchmarks for mixup augmentation to facilitate unbiased comparisons and practical use in visual recognition.

**Why do we need an open-source mixup codebase?** Notably, most existing mixup techniques are crafted with diverse settings, tricks, and implementations, each with its own coding style. This lack of standardization not only hinders user-friendly reproduction and deployment but impedes further development, thus imposing costly trial-and-error on practitioners to determine the most appropriate mixup strategy for their specific needs in real-world applications. Hence, it is essential to develop a unified mixup visual representation learning codebase for standardized data pre-processing, mixup development, network architecture selection, model training, evaluation, and empirical analysis.

In this paper, we present OpenMixup, the _first_ comprehensive benchmark for mixup augmentation. Unlike previous work , we train and evaluate 18 approaches that represent the foremost strands on 11 diverse classification datasets, as illustrated in Figure 1. We also open-source a standardized codebase for mixup-based visual representation learning. The overall framework is built up with modular components for data pre-processing, mixup augmentation, network backbone selection, optimization, and evaluations, which not only powers our benchmarking study but has supported broader relatively under-explored mixup applications beyond classification, such as semi-supervised learning , self-supervised learning , and visual attribute regression .

Furthermore, insightful observations are obtained by incorporating multiple evaluation metrics and analysis toolkits with our OpenMixup, including GPU memory usage (as Figure 4), loss landscape (as Figure 6), analysis of robustness and calibration (as Table A8). For example, despite the key role _static_ mixing plays in today's deep learning systems, we surprisingly find that its generalizability over diverse datasets and backbones is significantly inferior to that of _dynamic_ algorithms. By ranking the performance and efficiency trade-offs, we reveal that several recent _dynamic_ methods have already outperformed the _static_ ones. This may suggest a promising breakthrough for mixup augmentation, provided that the _dynamic_ computational overhead can be further reduced. Overall, we believe these observations can facilitate meaningful evaluation and comparisons of mixup variants, enabling a systematic understanding and paving the way for future advancements in the community.

Figure 2: Visualization of mixed samples from representative _static_ and _dynamic_ mixup augmentation methods on ImageNet-1K. We employ a mixing ratio of \(=0.5\) for a comprehensive comparison. Note that mixed samples are more precisely in _dynamic_ mixing policies than these _static_ ones.

It is worth emphasizing that such a first-of-its benchmark can be rather time- and resource-consuming. Since most existing studies have focused on visual classification tasks, we centralize the benchmarking scope on this field while extending it to broader mixup applications beyond classification with transfer learning. Meanwhile, we have already supported these downstream tasks and datasets in our proposed codebase, allowing users to customize their mixup algorithms, models, and training setups in these relatively under-explored scenarios. Our key contributions can thus be summarized as follows:

* We introduce OpenMixup, the _first_ comprehensive benchmarking study for mixup augmentation, where 18 representative baselines are trained from scratch and rigorously evaluated on 11 visual classification datasets, ranging from non-iconic scenes to gray-scale, fine-grained, and long tail scenarios. By providing a standard testbed and a rich set of evaluation protocols, OpenMixup enables objective assessment and fair comparisons of different mixup methods.
* To support reproducible research and user-friendly development, we open-source a unified codebase for mixup-based visual representation learning. The codebase incorporates standardized modules for data pre-processing, mixup augmentation, backbone selection, optimization policies, and distributed training functionalities. Beyond the benchmark itself, our codebase is readily extensible and has supported semi- and self-supervised learning and visual attribute regression tasks, which further enhances its versatility and potential benefits.
* Observations and insights are obtained through extensive analysis. We investigate the generalization ability of all evaluated mixup baselines across diverse datasets and backbones, compare their GPU memory footprint and computational cost, visualize the loss landscape to understand optimization behavior, and evaluate robustness against input corruptions and calibration performance. Furthermore, we establish comprehensive rankings in terms of their performance and applicability (efficiency and versatility), offering clear method guidelines for specific requirements. These findings not only present a firm grasp of the current mixup landscape but shed light on promising avenues for systematic advancements in the future.

## 2 Background and Related Work

### Problem Definition

Mixup training.We first consider the general image classification tasks with \(k\) different classes: given a finite set of \(n\) image samples \(X=[x_{i}]_{i=1}^{n}^{n W H C}\) and their corresponding ground-truth class labels \(Y=[y_{i}]_{i=1}^{n}^{n k}\), encoded by a one-hot vector \(y_{i}^{k}\). We attempt to seek the mapping from input data \(x_{i}\) to its class label \(y_{i}\) modeled through a deep neural network \(f_{}:x y\) with parameters \(\) by optimizing a classification loss \((.)\), say the cross entropy (CE) loss,

\[_{CE}(f_{}(x),y)=-y f_{}(x).\] (1)

Then we consider the mixup classification task: given a sample mixing function \(h\), a label mixing function \(g\), and a mixing ratio \(\) sampled from \(Beta(,)\) distribution, we can generate the mixed data \(X_{mix}\) with \(x_{mix}=h(x_{i},x_{j},)\) and the mixed label \(Y_{mix}\) with \(y_{mix}=g(y_{i},y_{j},)\), where \(\) is a hyper-parameter. Similarly, we learn \(f_{}:x_{mix} y_{mix}\) by the mixup cross-entropy (MCE) loss,

\[_{MCE}=_{CE}(f_{}(x_{mix}),y_{i})+(1-)_{CE}( f_{}(x_{mix}),y_{j}).\] (2)

Mixup reformulation.Comparing Eq. (1) and Eq. (2), the mixup training has the following features: (1) extra mixup policies, \(g\) and \(h\), are required to generate \(X_{mix}\) and \(Y_{mix}\). (2) the classification performance of \(f_{}\) depends on the generation policy of mixup. Naturally, we can split the mixup task into two complementary sub-tasks: (i) mixed sample generation and (ii) mixup classification (learning objective). Notice that the sub-task (i) is subordinate to (ii) because the final goal is to obtain a stronger classifier. Therefore, from this perspective, we regard the mixup generation as an auxiliary task for the classification task. Since \(g\) is generally designed as a linear interpolation, i.e., \(g(y_{i},y_{j},)= y_{i}+(1-)y_{j}\), \(h\) becomes the key function to determine the performance of the model. Generalizing previous offline methods, we define a parametric mixup policy \(h_{}\) as the sub-task with another set of parameters \(\). The final goal is to optimize \(_{MCE}\) given \(\) and \(\) as:

\[_{,\;}_{MCE}f_{}h_{}(x_{i},x_{j}, ),g(y_{i},y_{j},).\] (3)

### Sample Mixing

Within the realm of visual classification, prior research has primarily concentrated on refining the sample mixing strategies rather than the label mixing ones. In this context, most sample mixing methods are categorized into two groups: _static_ policies and _dynamic_ policies, as presented in Table 1.

Static Policies.The sample mixing procedure in all _static_ policies is conducted in a _hand-crafted_ manner. Mixup  first generates artificially mixed data through the convex combination of two randomly selected input samples and their associated one-hot labels. ManifoldMix variants [2; 3] extend the same technique to latent feature space for better sample mixing performance. Subsequently, CutMix  involves the random replacement of a certain rectangular region inside input sample while concurrently employing Dropout throughout the mixing process. Inspired by CutMix, several researchers in the community have explored the use of saliency information  to pilot mixing patches, while others have developed more complex _hand-crafted_ sample mixing strategies [5; 7; 6].

Dynamic Policies.In contrast to _static_ mixing, _dynamic_ strategies are proposed to incorporate sample mixing into an adaptive optimization-based framework. PuzzleMix variants [11; 12] introduce combinatorial optimization-based mixing policies in accordance with saliency maximization. SuperMix variants [31; 8] utilize pre-trained teacher models to compute smooth and optimized samples. Distinctively, AutoMix variants [18; 30] reformulate the overall sample mixing framework into an online-optimizable fashion that learns to generate the mixed samples in an end-to-end manner.

### Label Mixing

Mixup  and CutMix  are two widely-recognized label mixing techniques, both of which are _static_. Recently, there has been a notable emphasis among researchers on advancing label mixing approaches, which attains more favorable performance upon certain sample mixing policies. Based on Transformers, TransMix variants [13; 14; 32; 16] are proposed to utilize class tokens and attention maps to adjust the mixing ratio. A decoupled mixup objective  is introduced to force models to focus on those hard mixed samples, which can be plugged into different sample mixing policies. Holistically, most existing studies strive for advanced sample mixing designs rather than label mixing.

### Other Applications

Recently, mixup augmentation also has shown promise in more vision applications, such as semi-supervised learning [22; 23], self-supervised pre-training [24; 25], and visual attribute regression [26; 27]. Although these fields are not as extensively studied as classification, our OpenMixup codebase has been designed to support them by including the necessary task settings and datasets. Its modular and extensible architecture allows researchers and practitioners in the community to effortlessly adapt and extend their models to accommodate the specific requirements of these tasks, enabling them to quickly set up experiments without building the entire pipeline from scratch. Moreover, our codebase will be well-positioned to accelerate the development of future benchmarks, ultimately contributing to the advancement of mixup augmentation across a diversity of visual representation learning tasks.

## 3 OpenMixup

This section introduces our OpenMixup codebase framework and benchmark from four key aspects: supported methods and tasks, evaluation metrics, and experimental pipeline. OpenMixup provides a unified framework implemented in PyTorch  for mixup model design, training, and evaluation.

   Method & Category & Publication & Sample Mixing & Label Mixing & Extra Cont & Virt & Perf. & App. & Overall \\  Mixup  & Static & ICLR’2018 & Hand-crafted Interpolation & Mixup & \(\) & \(\) & 15 & 1 & 10 \\ CutMix  & Static & ICLR’2019 & Hand-crafted Cutting & CutMix & \(\) & \(\) & 13 & 1 & 8 \\ DeiT (CutMixMix) & Static & ICLR’2021 & CutMixMixMix & Mixup & \(\) & \(\) & 7 & 1 & 3 \\ SmoothMix  & Static & CVPR’2020 & Hand-crafted Cutting & CutMix & \(\) & \(\) & 18 & 1 & 13 \\ GridMix  & Static & P20’201 & Hand-crafted Cutting & CutMix & \(\) & \(\) & 17 & 1 & 12 \\ ResizeMix  & Static & CVM’2023 & Hand-crafted Cutting & CutMix & \(\) & \(\) & 10 & 1 & 5 \\ ManifoldMix  & Static & ICLR’2019 & Latent-space Mixup & Mixup & \(\) & \(\) & 14 & 1 & 9 \\ FMx  & Static & arXiv’2020 & Fourier-guided Cutting & CutMix & \(\) & \(\) & 16 & 11 & 11 \\ AttenMixMix  & Static & ICR’2020 & Pretraining-guided Cutting & CutMix & \(\) & \(\) & 9 & 3 & 6 \\ SaliencyMix  & Static & ICLR’2021 & Saliency-guided Cutting & CutMix & \(\) & \(\) & 11 & 1 & 6 \\ PuzzleMix  & Dynamic & ICML’2020 & Optimal-transported Cutting & CutMix & \(\) & \(\) & 8 & 4 & 6 \\ AliipMix  & Dynamic & CVPR’2021 & Optimal-transported Interpolation & CutMix & \(\) & \(\) & 12 & 2 & 8 \\ AutoMix  & Dynamic & ECC’2022 & End-to-end-end-sentence Cutting CutMix & CutMix & \(\) & \(\) & 3 & 6 & 4 \\ SAMMix  & Dynamic & arXiv’2021 & End-to-end-end-sentence Cutting CutMix & \(\) & \(\) & 1 & 5 & 1 \\ AAAutMix  & Dynamic & ICLR’2024 & End-to-end-sentence Cutting CutMix & \(\) & \(\) & 2 & 7 & 4 \\ TransMix  & Dynamic & CVPR’2022 & CutMixMixMixup & Attention-guided & \(\) & 5 & 8 & 7 \\ SMMix  & Dynamic & ICC’2023 & CutMixMixMix & Attention-guided & \(\) & 4 & 8 & 6 \\ DecoupledMix  & Static & INPS’2023 & Any Sample Mixing Policies & DecoupledMix & \(\) & \(\) & 6 & 1 & 2 \\   

Table 1: Information of all supported vision Mixup augmentation methods in OpenMixup. Note that Mixup and CutMix in the label mixing policies indicate mixing labels of two samples by linear interpolation or calculating the cut squares. The _Perf._, _App._, and _Overall_ headings below denote the performance, applicability (efficiency & versatility), and overall rankings of all the mixup baselines.

The framework references MMLClassification  and follows the OpenMMLab coding style. We start with an overview of its composition. As shown in Figure 3, the whole training process here is fragmented into multiple components, including model architecture (.openmixup.models), data pre-processing (.openmixup.datasets), mixup policies (.openmixup.models.utils.augments), script tools (.tools) _etc_. For instance, vision models are summarized into modular building blocks (_e.g._, backbone, neck, head _etc._) in.openmixup.models. This modular architecture enables practitioners to easily craft models by incorporating different components through configuration files in.configs. As such, users can readily customize their specified vision models and training strategies. In addition, benchmarking configuration (.benchmarks) and results (.tools.model_zoos) are also provided in the codebase. Additional benchmarking details are discussed below.

### Benchmarked Methods

OpenMixup has implemented 17 representative mixup augmentation algorithms and 19 convolutional neural network and Transformer model architectures (gathered in.openmixup.models) across 12 diverse image datasets for supervised visual classification. We summarize these mixup methods in Table 1, along with their corresponding conference/journal, the types of employed sample and label mixing policies, properties, and rankings. For sample mixing, Mixup  and ManifoldMix  perform _hand-crafted_ convex interpolation. CutMix , SmoothMix , GridMix  and ResizeMix  implement _hand-crafted_ cutting policy. FMix  utilizes Fourier-guided cutting, AttentiveMix  and SaliencyMix  apply pretraining-guided and saliency-guided cutting, respectively. Some _dynamic_ approaches like PuzzleMix  and AlignMix  utilize optimal transport-based cutting and interpolation. AutoMix  and SAMix  perform end-to-end online-optimizable cutting-based approaches. As for the label mixing, most methods apply Mixup  or CutMix , while the latest mixup methods for visual transformers (TransMix , TokenMix , and SMMix ), as well as DecoupledMix  exploit attention maps and a decoupled framework respectfully instead, which incorporate CutMix variants as its sample mixing strategy. Such a wide scope of supported methods enables a comprehensive benchmarking analysis on visual classification.

### Benchmarking Tasks

We provide detailed descriptions of the 12 open-source datasets as shown in Table 2. These datasets can be classified into four categories below: **(1) Small-scale classification**: We conduct benchmarking studies on small-scale datasets to provide an accessible benchmarking reference. CIFAR-10/100  consists of 60,000 color images in 32\(\)32 resolutions. Tiny-ImageNet (Tiny)  and STL-10  are two re-scale versions of ImageNet-1K in the size of 64\(\)64 and 96\(\)96. FashionMNIST  is the advanced version of MNIST, which contains gray-scale images of clothing. **(2) Large-scale classification**: The large-scale dataset is employed to evaluate mixup algorithms against the most standardized procedure, which can also support the prevailing ViT architecture. ImageNet-1K (IN-1K)  is a well-known challenging dataset for image classification with 1000 classes. **(3) Fine-grained classification**: To investigate the effectiveness of mixup methods in complex inter-class relationships and long-tail scenarios, we conduct a comprehensive evaluation of fine-grained classification datasets, which can also be classified into small-scale and large-scale scenarios. (i) _Small-scale scenarios_: The datasets for small-scale fine-grained evaluation scenario are CUB-200-2011 (CUB)  and FGVC-Aircraft (Aircraft) , which contains a total of 200 wild

Figure 3: Overview of codebase framework of OpenMixup benchmark. (1) benchmarks provide the benchmarking results and corresponding config files for mixup classification and transfer learning. (2) openmixup contains the source codes of all supported methods. (3) configs is responsible for customizing setups of different mixup methods, networks, datasets, and training pipelines. (4) docs & tools contains paper lists of popular mixup methods, user documentation, and practical tools.

bird species and 100 classes of airplanes. (ii) _Large-scale scenarios_: The datasets for large-scale fine-grained evaluation scenarios are iNaturalist2017 (iNat2017)  and iNaturalist2018 (iNat2018) , which contain 5,089 and 8,142 natural categories. Both the iNat2017 and iNat2018 own 7 major categories and are also long-tail datasets with scenic images (_i.e._, the fore-ground target is in large backgrounds). **(4) Scenic classification**: Scenic classification evaluations are also conducted to investigate the mixup augmentation performance in complex non-iconic scenarios on Places205 .

### Evaluation Metrics and Tools

We comprehensively evaluate the beneficial properties of mixup augmentation algorithms on the aforementioned vision tasks through the use of various metrics and visualization analysis tools in a rigorous manner. Overall, the evaluation methodologies can be classified into two distinct divisions, namely performance metric and empirical analysis. For the performance metrics, classification accuracy and robustness against corruption are two performance indicators examined. As for empirical analysis, experiments on calibrations, CAM visualization, loss landscape, the plotting of training loss, and validation accuracy curves are conducted. The utilization of these approaches is contingent upon their distinct properties, enabling user-friendly deployment for designated purposes and demands.

Performance Metric. **(1) Accuracy and training costs**: We adopt top-1 accuracy, total training hours, and GPU memory to evaluate all mixup methods' classification performance and training costs. **(2) Robustness**: We evaluate the robustness against corruptions of the methods on CIFAR-100-C and ImageNet-C , which is designed for evaluating the corruption robustness and provides 19 different corruptions, _e.g._, noise and blur _etc._ **(3) Transferability to downstream tasks**: We evaluate the transferability of existing methods to object detection based on Faster R-CNN  and Mask R-CNN  on COCO _train2017_, initializing with trained models on ImageNet. We also transfer these methods to semantic segmentation on ADE20K . Please refer to Appendix B.4 for details.

Empirical Analysis. **(1) Calibrations**: To verify the calibration of existing methods, we evaluate them by the expected calibration error (ECE) on CIFAR-100 , _i.e._, the absolute discrepancy between accuracy and confidence. **(2) CAM visualization**: We utilize mixed sample visualization, a series of CAM variants [48; 49] (_e.g._, Grad-CAM ) to directly analyze the classification accuracy and especially the localization capabilities of mixup augmentation algorithms through top-1 top-2 accuracy predicted targets. **(3) Loss landscape**: We apply loss landscape evaluation  to further analyze the degree of loss smoothness of different mixup augmentation methods. **(4) Training loss and accuracy curve**: We plot the training losses and validation accuracy curves of various mixup methods to analyze the training stability, the ability to prevent over-fitting, and convergence speed.

### Experimental Pipeline of OpenMixup Codebase

With a unified training pipeline in OpenMixup, a comparable workflow is shared by different classification tasks, as illustrated in Figure A1. Here, we take classification tasks as an instance to illustrate the whole training procedure. Firstly, users should go through the supported data pipeline and select the dataset and pre-processing techniques. Secondly, openmixup.models serves as a model architecture component for building desired methods. Thirdly, it is undemanding to designate the supported datasets, mixup augmentation strategies, model architectures, and optimization schedules under.configs.classification with Python configuration files to customize a desired setting. Afterward,.tools provides hardware support distributed training to execute the confirmed training process in configs. Apart from that, there are also various utility functionalities given in.tools (_e.g._, feature visualization, model analysis, result summarization). We also provide online user documents

   Datasets & Category & Source & Classes & Resolution & Train images & Test images \\  CIFAR-10  & Iconic & link & 10 & 32\(\)32 & 50,000 & 10,000 \\ CIFAR-100  & Iconic & link & 100 & 32\(\)32 & 50,000 & 10,000 \\ FashionMNIST  & Gray-scale & link & 10 & 28\(\)28 & 50,000 & 10,000 \\ STL-10  & Iconic & link & 10 & 96\(\)96 & 50,00 & 8,000 \\ Tiny-ImageNet  & Iconic & link & 200 & 64\(\)64 & 10,000 & 10,000 \\ ImageNet-1K  & Iconic & link & 1000 & 469\(\)387 & 1,281,167 & 50,000 \\ CUB-200-2011  & Fine-grained & link & 200 & 224\(\)224 & 5,994 & 5,794 \\ FGVC-Aircraft  & Fine-grained & link & 100 & 224\(\)224 & 6,667 & 3,333 \\ iNaturalist2017  & Fine-grained \& longtail & link & 5089 & 224\(\)224 & 579,184 & 95,986 \\ iNaturalist2018  & Fine-grained \& longtail & link & 8142 & 224\(\)224 & 437,512 & 24,426 \\ Places205  & Scenic & link & 205 & 224\(\)224 & 2,448,873 & 41,000 \\   

Table 2: The detailed information of supported visual classification datasets in OpenMixup.

for more detailed guidelines (_e.g._, installation and getting started instructions), benchmarking results, comprehensive awesome lists of related works, _etc_.

## 4 Experiment and Analysis

### Implementation Details

We conduct essential benchmarking experiments of image classification on various scenarios with diverse evaluation metrics. For a fair comparison, grid search is performed for the shared hyper-parameter \(\{0.1,0.2,0.5,1,2,4\}\) of supported mixup variants while the rest of the hyper-parameters follow the original papers. Vanilla denotes the classification baseline without any mixup augmentations. All experiments are conducted on Ubuntu workstations with Tesla V100 or NVIDIA A100 GPUs and report the _mean_ results of three trials. Appendix B provides classification results, and Appendix B presents transfer learning results for object detection and semantic segmentation.

Small-scale Benchmarks.We first provide standard mixup image classification benchmarks on five small datasets with two settings. (a) The classical settings with the CIFAR version of ResNet variants [52; 53], _i.e._, replacing the \(7 7\) convolution and MaxPooling by a \(3 3\) convolution. We use \(32 32\), \(64 64\), and \(28 28\) input resolutions for CIFAR-10/100, Tiny-ImageNet, and FashionMNIST, while using the normal ResNet for STL-10. We train models for multiple epochs from the stretch with SGD optimizer and a batch size of 100, as shown in Table 3 and Appendix B.2. (b) The modern settings following DeiT  on CIFAR-100, using \(224 224\) and \(32 32\) resolutions for Transformers (DeiT-S  and Swin-T ) and ConvNeXt-T  as shown in Table A.

   Datasets & CIFAR-10 & CIFAR-100 & Tiny \\  Backbones & R-18 & WRN-28-8 & RX-50 \\ Epochs & 800 ep & 800 ep & 400 ep \\  Vanilla & 95.50 & 81.63 & 65.04 \\ Mixup & 96.62 & 82.82 & 66.36 \\ CutMix & 96.68 & 84.45 & 66.47 \\ ManifoldMix & 96.71 & 82.34 & 67.30 \\ SmoothMix & 96.17 & 82.09 & 68.61 \\ AttentiveMix & 96.63 & 84.34 & 67.42 \\ SaliencyMix & 96.20 & 84.35 & 66.55 \\ FMix & 96.18 & 84.21 & 65.08 \\ GridMix & 96.56 & 84.24 & 69.12 \\ ResizeMix & 96.76 & 84.87 & 65.87 \\ PuzzMix & 97.10 & 85.02 & 67.83 \\ Co-Mixup & 97.15 & 85.05 & 68.02 \\ AlignMix & 97.05 & 84.87 & 68.74 \\ AutoMix & 97.34 & 85.18 & 70.72 \\ SAMix & 97.50 & **85.50** & 72.18 \\ AdaAutoMix & **97.55** & 85.32 & **72.89** \\ Decoupled & 96.95 & 84.88 & 67.46 \\   

Table 3: Top-1 accuracy (%) on CIFAR- Table 4: Top-1 accuracy (%) on ImageNet-1K using 10/100 and Tiny-ImageNet (Tiny) based PyTorch-style, RSB A2/A3, and DeiT settings based on ResNet (R), Wide-ResNet (WRN), CNN and Transformer architectures, including ResNet (R), and ResNeXt (RX) backbones.

Figure 4: Trade-off evaluation with respect to accuracy performance, total training time (hours), and GPU memory (G). The results in (a) are based on DeiT-S architecture on ImageNet-1K. The results in (b) and (c) are based on DeiT-S and ConvNeXt-T backbones on CIFAR-100, respectively.

Standard ImageNet-1K Benchmarks.For visual augmentation and network architecture communities, ImageNet-1K is a well-known standard dataset. We support three popular training recipes: (a) PyTorch-style  setting for classical CNNs; (b) timm RSB A2/A3  settings; (c) DeiT  setting for ViT-based models. Evaluation is performed on 224\(\)224 resolutions with CenterCrop. Popular network architectures are considered: ResNet , Wide-ResNet , ResNeXt , MobileNet.V2 , EfficientNet , DeiT , Swin , ConvNeXt , and MogaNet . Refer to Appendix A for implementation details. In Table 4 and Table A2, we report the _mean_ performance of three trials where the _median_ of top-1 test accuracy in the last 10 epochs is recorded for each trial.

Benchmarks on Fine-grained and Scenis Scenarios.We further provide benchmarking results on three downstream classification scenarios in 224\(\)224 resolutions with ResNet backbone architectures: (a) Transfer learning on CUB-200 and FGVC-Aircraft. (b) Fine-grained classification on iNat2017 and iNat2018. (c) Scenic classification on Places205, as illustrated in Appendix B.3 and Table A10.

### Observations and Insights

Empirical analysis is conducted to gain insightful observations and a systematic understanding of the properties of different mixup augmentation techniques. Our key findings are summarized as follows:

**(A) Which mixup method should I choose?** Integrating benchmarking results from various perspectives, we provide practical mixup rankings (detailed in Appendix B.5) as a take-home message for real-world applications, which regards performance, applicability, and overall capacity. As shown in Table 1, as for the performance, the _online-optimizable_ SAMix and AutoMix stand out as the top two choices. SMMix and TransMix follow closely behind. However, in terms of applicability that involves both the concerns of efficiency and versatility, _hand-crafted_ methods significantly outperform the learning-based ones. Overall, the DeiT (Mixup+CutMix), SAMix, and SMMix are selected as the the three most preferable mixup methods, each with its own emphasis.

**(B) Generalizability over datasets.** The intuitive performance radar chart presented in Figure 1, combined with the trade-off results in Fiureg 4, reveals that _dynamic_ mixup methods consistently yield better performance compared to _static_ ones, showcasing their impressive generalizability. However, _dynamic_ approaches necessitate meticulous tuning, which incurs considerable training costs. In contrast, _static_ mixup exhibits significant performance fluctuation across different datasets,

Figure 5: Visualization of class activation mapping (CAM)  for top-1 and top-2 predicted classes of supported mixup methods with ResNet-50 on ImageNet-1K. Comparing the first and second rows, we observe that saliency-guided or dynamic mixup approaches (_e.g._, PuzzleMix and SAMix) localize the target regions better than the static methods (_e.g._, Mixup and ResizeMix).

Figure 6: Visualization of 1D loss landscapes for representative mixup methods with ResNet-50 on ImageNet-1K. The validation accuracy is plotted, showing that dynamic methods achieve deeper and wider loss landscapes than static ones.

    &  \\  Performance & 13 & 11 & 5 & 16 & 15 & 8 & 12 & 14 & 7 & 9 & 6 & 10 & 2 & 1 & 4 & 3 \\ Applicability & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 3 & 1 & 4 & 2 & 7 & 6 & 5 & 5 \\ Overall & 8 & 6 & 1 & 11 & 10 & 4 & 7 & 9 & 5 & 5 & 5 & 6 & 4 & 2 & 4 & 3 \\   

Table 5: Rankings of various mixup augmentations as take-home messages for practical usage.

indicating poor generalizability. For instance, Mixup  and CutMix  as the _static_ representatives perform even worse than the baseline on Place205 and FGVC-Aircraft, respectively.

**(C) Generalizability over backbones.** As shown in Figure 6 and Figure 4, we provide extensive evaluations on ImageNet-1K based on different types of backbones and mixup methods. As a result, _dynamic_ mixup achieves better performance in general and shows more favorable generalizability against backbone selection compared to _static_ methods. Noticeably, the _online-optimizable_ SAMix and AutoMix exhibit impressive generalization ability over different vision backbones, which potentially reveals the superiority of their online training framework compared to the others.

**(D) Applicability.** Figure A2 shows that ViT-specific methods (_e.g._, TransMix  and TokenMix ) yield exceptional performance with DeiT-S and PVT-S yet exhibit intense sensitivity to different model scales (_e.g._, with PVT-T). Moreover, they are limited to ViTs, which largely restricts their applicability. Surprisingly, _static_ Mixup  exhibits favorable applicability with new efficient networks like MogaNet . CutMix  fits well with popular backbones, such as modern CNNs (_e.g._, ConvNeXt and ResNeXt) and DeiT, which increases its applicability. As in Figure 4, although AutoMix and SAMix are available in both CNNs and ViTs with consistent superiority, they have limitations in GPU memory and training time, which may limit their applicability in certain cases. This also provides a promising avenue to reduce the cost of the well-performed online learnable mixup.

**(E) Robustness & Calibration.** We evaluate the robustness with accuracy on the corrupted version of CIFAR-100 and FGSM attack  and the prediction calibration. Table A8 shows that all mixup methods improve model robustness against corruptions. Interestingly, only four recent _dynamic_ approaches exhibit better robustness compared to the baseline with FGSM attacks. We thus hypothesize that _online-optimizable_ methods are well-trained to be robust against human interference, while _hand-crafted_ mixup adapts to natural disruptions like corruption but is susceptible to attacks. Overall, the learning-based AutoMix and SAMix achieve the best robustness and calibration results.

**(F) Convergence & Training Stability.** As shown in Figure 6, wider bump curves indicate smoother loss landscapes, while higher warm color bump tips are associated with better convergence and performance. Evidently, _dynamic_ mixup owns better training stability and convergence than _static_ mixup in general. Nevertheless, the initial Mixup  is an exception, exhibiting better training stability than all other _dynamic_ methods. We assume this arises from its straightforward convex interpolation that principally prioritizes training stability but may lead to suboptimal outcomes.

**(G) Localizability & Downstream Transferability.** It is commonly conjectured that models with better localizability can be better transferred to fine-grained prediction tasks. Thus, to gain intuitive insights, we provide tools for the class activation mapping (CAM) visualization with predicted classes on ImageNet-1K. As shown in Figure 5, SAMix and AutoMix's exceptional localizability, combined with their accuracy, generalizability, and robustness mentioned above, may indicate their superiority in detection tasks. To assess their real downstream performance and transferability, transfer learning experiments are also available on object detection  and semantic segmentation  with details in Appendix B.4. Table A11 and Table A12 suggest that AutoMix variants indeed exhibit competitive results, but ViT-specific methods perform even better, showcasing their superior transferability. This also shows the potential for improved online training mixup design.

## 5 Conclusion and Discussion

**Contributions.** This paper presents OpenMixup, the _first_ comprehensive mixup augmentation benchmark, where 18 mixup baselines are trained and evaluated on 11 diverse datasets. A unified and modular codebase is also released, which not only bolsters the benchmark but can facilitate broader under-explored mixup applications. Furthermore, observations and insights are obtained through extensive analysis, contributing to a more systematic comprehension of mixups. We anticipate that our OpenMixup can further contribute to fair and reproducible research in the mixup community. We also encourage researchers and practitioners to extend their valuable feedback to us and contribute to OpenMixup for building a more constructive mixup learning codebase together through GitHub.

**Limitations and Future Works.** The benchmarking scope of this work mainly focuses on visual classification, albeit we have supported a broader range of tasks in our codebase and have conducted transfer learning experiments to object detection and semantic segmentation tasks to draw preliminary conclusions. We are aware of this and have prepared it upfront for future work. For example, our codebase can be easily extended to all the supported tasks and datasets for further benchmarking experiments and evaluations if necessary. We believe this work as the _first_ mixup benchmarking study is enough to serve as a kick-start, and we plan to extend our work in these directions in the future.