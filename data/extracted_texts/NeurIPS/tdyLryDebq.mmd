# FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy

Zuhao Yang\({}^{1}\), Yingfang Yuan\({}^{2}\), Yang Xu\({}^{3}\), Shuo Zhan\({}^{1}\), Huajun Bai\({}^{4}\), Kefan Chen\({}^{2}\)

\({}^{1}\) School of Computer Science and Engineering, Nanyang Technological University

\({}^{2}\) School of Mathematical and Computer Sciences, Heriot-Watt University

\({}^{3}\) Department of Computer Science, Southern University of Science and Technology

\({}^{4}\) Genify

Equal contribution, in random orderCorresponding author, email: xuyang@sustech.edu.cn

###### Abstract

Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on _F_ourier _A_nalysis of the estimated _Cross-Entropy_ of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.

## 1 Introduction

The concept of _entropy_ from Information Theory is broadly applied in Natural Language Processing (NLP) technology and computational linguistic studies. The most notable example is the use of _cross-entropy_ in training and evaluating language models, where the exponentiation of cross-entropy, perplexity, is adopted to measure models' performance in next-word (or masked-word) prediction task. However, low perplexity alone does not guarantee good performance in language generation tasks, which not only depend on model sizes but are also closely related to the sampling techniques used in _decoding_ stage. The complexity of the generation task makes it especially important to have different metrics that can reflect the generation quality from multiple angles. One particular perspective is that the language generated from a good model should have a similar distribution of words/tokens as in the "natural" human language.

Recent advances in psycholinguistics put forward new directions for developing more sophisticated metrics other than Zipf's coefficient. In particular, studies on temporal and spectral patterns in dialogue  reveal that cross-entropy (or referred to as surprisal, information density in the psycholinguistics literature) changes _periodically_ in natural language, which points out the potentials of using fine-grained transformation of cross-entropy to quantify the differences in language data (see Section 3 for a detailed review). It motivates the basic idea of this study: Can we effectively quantify the _periodical_ pattern of the cross-entropy, and use it as an indicator to distinguish human and model-generated languages?

We summarize our contributions as follows: 1. We propose a set of metrics based on the frequency spectra obtained from the Fast Fourier Transform (FFT) of the cross-entropy sequences of language data, named FACE (_F_ourier _A_nalysis of _Cross-Entropy_). 2. We empirically show FACE's performance on identifying human-model gap and how it scales with model sizes in Section 4.1. 3. We exploreFACE's correlations with sampling methods and human evaluation in Section 4.2 and Section 4.3, respectively. 4. We validate the statistical soundness of FACE in Section 4.4. 5. We discuss an intuitive interpretation of the metrics and how it reflects the characteristics of language use in Section 4.5. 6. Implementation and experiments code are available in this public repository: https://github.com/CLCS-SUSTech/FACE.

## 2 Face

The basic idea of FACE is to obtain the spectra of cross-entropy from different data sources (human or models) and compute their similarities. The overall workflow is shown in Figure 1, which we describe in five steps:

1. Collect the datasets for human-written and model-generated texts, \(_{h}\) and \(_{m}\).
2. Use a third pre-trained language model \(m_{}\) to estimate the cross-entropy of text in \(_{h}\) and \(_{m}\), resulting in two sequences of cross-entropy output, \(_{h}\) and \(_{m}\).
3. Obtain the frequency spectra for each cross-entropy sequences, \(_{h}_{h}\) and \(_{m}_{m}\).
4. Develop FACE metrics that quantify the spectral similarity between \(_{h}\) and \(_{m}\).
5. Evaluate FACE on different model types/sizes, sampling methods, and the correlations with other metrics for Natural Language Generation (NLG).

We describe the steps in detail from Section 2.1 to Section 2.3.

### Estimate cross-entropy

We use a pre-trained language model \(m_{}\) as the estimator for cross-entropy, which runs in the evaluation model (no gradients produced). It takes as input a sequence of \(T\) tokens, \([t_{1},t_{2},,t_{T}]\); for each position \(i=1,,T\), it predicts the probability of the next token \(P(t_{i+1}|t_{1},,t_{i})\); the cross-entropy between this probability and the ground truth token \(t_{i+1}\) is then computed, resulting in the cross-entropy sequence that consists of \(T-1\) real values \(=[c_{1},c_{2},,c_{T-1}]\), as the first token is not predicted:

\[=[c_{1},c_{2},,c_{T-1}][- P(t_{2}|t_{1}),- P (t_{3}|t_{1},t_{2}),,- P(t_{T}|t_{1},t_{2},,t_{T-1})]\] (1)

Note that \( c_{i}=-_{i=2}^{T} P(t_{i}|t_{1} t_{i-1})\) is exactly the definition of negative log-likelihood loss, i.e., cross-entropy loss, for training a language model, where \(c_{i}\) is the negative logarithm of the predicted probability for each token \(t_{i+1}\). In psycholinguistic studies, this \(c_{i}\) quantity is usually referred to several different terms, including _surprisal_[16; 17], _information density_[25; 20; 48], and _entropy_[13; 14; 46; 47], each of which has a specific theoretical flavor. There have been debates over the justifiability of using "entropy" to denote the negative log-likelihood, because it is not a weighted summation as originally defined in . Albeit, we decide to use _cross-entropy_ as it is the most broadly communicated term and we believe it will not cause confusion as its mathematical form is clearly defined. Apparently, the choice for \(m_{est}\) will influence the next steps, because better

Figure 1: Overall workflow of this study.

language models produce lower perplexity scores, that is, lower cross entropy. Therefore, we discuss how different choices for \(m_{est}\) affect our metrics in Section 4.4.

### Fast Fourier transform

We treat the estimated cross-entropy sequence \([c_{1},,c_{T-1}]\) as a finite discrete signal in the time domain, where the sampling interval is approximated with the average duration of one token. With this simplified assumption, we find that the discrete Fourier transform (DFT) is the most suitable spectral analysis tool 3. The formula for DFT is as follows:

\[X(_{k})_{n=0}^{N-1}x(t_{n})e^{-j_{k}t_{n}},\ k=0,1, ,N-1\] (2)

in which \(x(t_{n})\) is the signal at time \(t_{n}\), corresponding to the \(n\)-th cross-entropy value \(c_{n}\) (\(n=1,T-1\) and \(N T-1\)). \(X(_{k})\) is a complex number that reflects the magnitude (strength) of the \(k\)-th frequency component \(_{k}=2 k/N\). In practice, DFT is implemented with an efficient algorithm known as Fast Fourier Transform  that runs in \(O(n n)\) time.

We compared two methods, periodogram and vanilla FFT. The periodogram approach computes the Fourier transform after applying auto-correlation and time-averaging windows to the signal for de-noising purposes . However, we think de-noising is inappropriate because our "signal" is a time series of cross-entropy, whose value reflects the sampling result at each time step from a large. Auto-correlation or time averaging will remove the distinctiveness of rare tokens. Therefore, we use vanilla FFT and take the _real_ part of \(X(_{k})\) to represent the magnitude spectrum for the frequency component \(_{k}\), which is written as \(X(_{k})\) for brevity.

For an input cross-entropy sequence \(=[c_{1},,c_{T-1}]\) obtained from Section 2.1, the resulting frequency spectrum can be represented as a list of tuples of the same length, \(=[_{1},X(_{1}),,_{T-1},X(_{T-1})]\), where \([_{1},,_{T-1}]\) are the \(T-1\) sample frequencies, and \([X(_{1}),,X(_{T-1})]\) are the corresponding magnitudes.

### Spectral similarity metrics

We develop four metrics to measure the similarity between spectra \(_{h}\) and \(_{m}\): Spectral Overlap (_SO_), Spectrum Angle Mapper (_SAM_) , Pearson's correlation (_CORR_), and Spearman's correlation (_SPEAR_), as summarized in Figure 2. Before computing the metrics, two spectra \(_{h}\) and \(_{m}\) which are of different lengths \(N_{1}\) and \(N_{2}\), are first interpolated to the same length: \(_{h}^{N_{1}}_{h}^{} ^{N_{C}}\), \(_{m}^{N_{2}}_{m}^{} ^{N_{C}}\). Here, \(N_{C}\) is the maximum length of the spectrum in our data. Thereafter, the computation of the subsequent metrics can commence.

**Spectral Overlap (_SO_)** is inspired by the power spectrum overlap proposed in , which is used in  for measuring the spectral similarity between dialogue participants. The frequency magnitudes in \(_{h}^{}\) and \(_{m}^{}\) are converted to absolute values, i.e., \(X(_{k})|X(_{k})|\), and then compute the Area-Under-Curve (AUC) for the interaction \(_{h}^{}_{m}^{}\) and the union \(_{h}^{}_{m}^{}\), respectively. _SO_ is defined as the ratio of the two: \(SO=(_{h}^{}_{m}^{})/(_{h}^{}_{m}^{})\). The procedure of converting

Figure 2: Definitions of four FACE metrics.

to absolute values is indispensable, since negative values in \(X(_{k})\) will result in negative AUCs. \(SO\) has the range \(\), and a higher value indicates a stronger resemblance between the two spectra.

**Spectrum Angle Mapper (_SAM_)** calculates the angles between \(^{}_{h}\) and \(^{}_{m}\), treating them as two vectors in a space . The angle is measured in radians, which is calculated by the inverse function \((^{}_{h}^{}_{m}/||^{ }_{h}||||^{}_{m}||)\), producing a value within \([0,]\). We understand _SAM_ is equivalent to the cosine similarity score, which is more commonly-used in NLP, but here we just follow the conventions in [22; 2]. A smaller _SAM_ value indicates a greater similarity between \(^{}_{h}\) and \(^{}_{m}\).

**Pearson's correlation (_CORR_)** can also be leveraged to measure spectral similarities as discussed in . \(=cov(^{}_{h},^{}_{m})/( ^{}_{h})(^{}_{m})\), with a \([-1,1]\) range. A positive _CORR_ value indicates high similarity (negative for dissimilarity), and 0 indicates weak correlation between \(^{}_{h}\) and \(^{}_{m}\).

**Spearman's correlation (_SPEAR_)** is commonly used to assess the monotonic relationship between the comparison and reference groups and to capture the presence of non-linear associations between the two. It has not been used for spectral similarity to the best of our knowledge, but we test it in our experiments. _SPEAR_ also has the range \([-1,1]\) with meanings similar to _CORR_.

## 3 Related Work

**Entropy as a metric in psycholinguistics.** The entropy of human language has long been a research interest in computational linguistics and psycholinguistics. The entropy of written text is estimated with the average per-word negative log-probability in sentences, and then used to validate the principle of _entropy rate constancy_ (ERC) in human language [13; 14]. Similar studies were conducted in dialogue [46; 32]. Entropy is also defined in probabilistic grammars to describe the capacity of a language , and is used to develop complexity metrics to measure the cognitive load of processing syntactic expressions [16; 24; 17]. In the line of work on language production, a different term _information density_ with the same mathematical formulation is used instead of entropy. It is found that speakers reduce syntactic complexity when the information density (or entropy) is high [25; 20]. In parallel with the concept of ERC, this line of work summarizes the tendency of distributing information evenly in human language with the term _uniform information density_ (UID), which is commonly used as a equivalent term as ERC, for example, in [48; 15]. In conclusion, entropy is commonly used as a metric for essential properties of human language.

**Periodical change of cross-entropy in language.** We draw inspiration from the following studies about the distribution of information in dialogue. Humans are sensitive to the _peaks_ and _troughs_ of entropy in speech, with evidence from human-system dialogues and crowd-sourced ratings from human judges . The entropy of utterances from two speakers converge towards each other within the scope of topical segments in spontaneous dialogues . They measure the entropy of utterances from two participants of a task-oriented dialogue, and have found that the frequency domain features - power spectrum overlap and phase delay - are useful predictors of task outcomes. Both works reviewed above suggest that the periodical up-and-downs of entropy are commonly observable in the human language. It naturally leads to the question of whether and to what extent model-generated language aligns with this empirical finding.

**Automatic measures for text generation.** Entropy and its related variants have already used as a metric for evaluating generated text, for instance, entropy provides good visualization for the difference between GPT2-generated text and human written ones . Other than entropy, there is a rich body of existing metrics targeted on discriminating human-written text and model-generated text, which we summarize in three branches: (1) statistics-based; (2) language modeling; (3) reference-based. Table 1 gives a brief summary of these three categories, as well as our proposed frequency-based FACE.

_Statistics-based measures_ compare the model-generated distribution \(M\) with respect to the human-written distribution \(H\) in terms of some statistic. The Zipf coefficient  is used in  to describe the distribution of word frequencies in text. Self-BLEU  is derived by calculating the BLEU  score for each generated text utilizing all other generations as references. Repetition measures the sequence-level degree of repetition on the basis of the percentage of duplicated n-grams in the generated continuations \(_{} M\). Meanwhile, we aggregate the 2-gram, 3-gram, and 4-gram repetition rates to evaluate the lexical diversity in an inverse manner.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

speaking, _SO_ and _SAM_ show a higher positive correlation to MAUVE than _CORR_ and _SPEAR_, given that seven out of nine voting results (marked with yellow in Table 3) are identical.

To further evaluate model sizes, we apply FACE to the original GPT2 output data (webtext) 4 generated from GPT2-sm and GPT2-xl. GPT2-xl has a higher _SO_ score than GPT2-sm, which is confirmed with the \(t\)-test, but non-significant effects are found on the other three metrics. Combining our generation task with the original GPT2 data, we illustrate the results for _SO_ in Figure 3.

To conclude, we discover three keypoints: (1) FACE is consistent with MAUVE in evaluating three different model types (two sizes for each); (2) the metrics estimating similarity between human-written and model-generated text generations (e.g., FACE, MAUVE) may produce opposite results to the text-centered metrics (e.g., Diversity, Coherence); (3) the four metrics of FACE show relatively homogeneous results, and using these metrics together helps to identify model-generated texts with a more comprehensive evaluation.

### Sampling methods

Recent work [19; 26] has indicated three clear trends in open-ended text generation using auto-regressive LMs: (1) maximization-based decoding algorithms (e.g., beam search, greedy decoding, etc.) lead to copious repetition, while sampling with temperature may result in incoherence; (2) truncation-based sampling methods like nucleus sampling produce text with higher quality; (3) contrastive decoding outperform nucleus sampling in terms of both fluency and coherence. Accordingly, to demonstrate the effectiveness of our approach, FACE should follow the inequality: maximization-based/temperature-based \(\) nucleus \(\) contrastive in terms of the quality relationship.

Figure 4 visualizes the correlation between FACE scores and various decoding algorithms. The contrastive decoding approach yields the best performance among the four FACE metrics. It can be clearly observed that the maximization-based sampling methods behave worse than other algorithms. Moreover, adding the temperature parameter to top-\(k\) sampling results in incoherent text generations,

Figure 4: FACE scores (conditional generation) on original experimental data of  and . Nine sampling methods are compared: greedy, beam search, stochastic beam search, pure sampling, temperature, top-\(k\), top-\(k\) with temperature, nucleus, and contrastive. Note that logarithmic normalization on parameter values as well as enlarged markers for greedy decoding, pure sampling, and contrastive decoding are adopted for better visualization effect. Best viewed when zoomed in.

Figure 3: FACE-_SO_ scores on OPT, BLOOM and GPT2 original output data. Model sizes compared: small vs. large for OPT and BLOOM; -sm vs. -xl for GPT2. Error bars represent 95% confidence intervals from bootstrap. The significant levels are based on \(t\)-test between the two model-size groups.

which explains the gap between the red curve (top-\(k\) w/o temperature) and the gray curve (top-\(k\) w/ temperature). We also plot the correlation graphs of unconditional generation (in the Appendix) with fewer sampling methods involved. The trends and patterns in the visualization of unconditional generation are basically consistent with its conditional counterpart.

In Table 4, FACE scores on different decoding algorithms are summarized. FACE metrics correctly match the expected quality relationship of the sampling methods examined by assigning the best _SO_ (\(.44\)), _CORR_ (\(.75\)), _SAM_ (\(.23\)), and _SPEAR_ (\(.17\)) scores to contrastive decoding. Other evaluation metrics fail to capture the correct relationship, for example, the perplexity rates nucleus-sampled text as better than contrastive-decoded text, which is irrational suggested by Li et al. .

### Human judgments

We also explore the correlation between FACE and human judgement scores, using the crowd-source dataset collected in  when human evaluation is available. The dataset contains model-generated continuations (by GPT2-sm, -md, -lg, and -xl with ancestral and nucleus sampling), human-written continuations using the same prefix, and the crowd-source workers' answers on which completion is more human-like, interesting, and sensible. We follow the same experimental settings and protocol to verify whether the FACE scores of the text completions correlate well with the human quality judgements by computing the Spearman's rank correlation coefficient. The results are presented in Table 5.

We observe a high and positive correlation between FACE-_SO_ and human judgments scores, which outperforms five out of the six evaluation metrics reported in  and achieves a comparative performance against MAUVE. The remaining three FACE metrics have insignificant correlations. However, we consider human judgments to be subjective and sometimes biased. Including more fine-grained questions to perform human judgments may lead to more accurate correlation statistics. Additionally, we recomputed the correlations with human judgement scores to keep those pairs in which there are exactly one item from human and the other item from model (i.e., a subset of data used for the analysis in Table 5). As shown in _SO_-S and MAUVE-S columns, FACE-_SO_ has a stronger correlation than MAUVE among two of these three dimensions.

### Sanity tests

**Sanity test on validity.** We evaluate the validity of FACE by examining whether its scores on human-human split is lower than human-model groups - an expected result based on our assumption that the spectral difference between human's "natural" language and models' "artificial" ones should be _amplified_ by FACE. Therefore, the sanity tests are conducted as follows: first, we evenly and randomly split the human data into two folds (across three domains) to serve as control groups. The FACE scores between these control folds are then computed. As for the human-to-model experimental group, we create other two folds using human data and the best model-generated data (from contrastive decoding ) in terms of text quality. If FACE can effectively capture the fundamental difference

  
**Sampling Method** & **Perplexity** & **Self-BLEU** & **Zipf Coefficient** & **Repetition** & _SO_ (\(\)) & _CORR_ (\(\)) & _SAM_ (\(\)) & _SPEAR_ (\(\)) \\   Human & 12.38 & 0.31 & 0.93 & 0.28 & - & - & - & - \\  Greedy & 1.50 & 0.50 & 1.00 & 73.66 & 0.20 & 0.56 & 0.31 & 0.04 \\ Beam(=16) & 1.48 & 0.44 & 0.94 & 28.94 & 0.21 & 0.31 & 0.40 & 0.04 \\ Stochastic Beam (=16) & 19.20 & 0.28 & 0.91 & 0.32 & 0.37 & 0.49 & 0.33 & 0.04 \\  Pure Sampling & 22.73 & 0.28 & **0.93** & 0.22 & 0.41 & 0.63 & 0.28 & 0.03 \\ Sampling (\(t\)=0.9) & 10.25 & 0.35 & 0.96 & 0.66 & 0.42 & 0.61 & 0.29 & 0.03 \\ Top- (\(k\)=40) & 6.88 & 0.39 & 0.96 & 0.78 & 0.40 & 0.64 & 0.28 & 0.03 \\ Top- \(k\) (=640) & 13.82 & **0.32** & 0.96 & **0.28** & 0.42 & 0.63 & 0.28 & 0.03 \\ Top- \(k\) (=40, \(t\)=0.7) & 3.48 & 0.44 & 1.00 & 8.86 & 0.34 & 0.61 & 0.29 & 0.03 \\ Nucleus (\(p\)=0.95) & **13.13** & **0.32** & 0.95 & 0.36 & 0.42 & 0.63 & 0.28 & 0.03 \\ Contrastive Decoding & 14.39 & 0.54 & 1.04 & 0.24 & **0.44** & **0.75** & **0.23** & **0.17** \\   

Table 4: Results for comparing all sampling methods with selected parameters regarding the conditional generation. The values _closest to human scores_ are **bolded**, except for our proposed FACE scores, where the _highest (for SO, CORR, and SPEAR) or the lowest (for SAM)_ values are in **bold**.

between human and model languages, then we expect to observe higher scores in control groups than in the experimental group. The results are shown in Table 6.

It can be seen from Table 6 that the control groups show significantly better FACE scores than the experimental group: FACE-_SO_ and FACE-_CORR_ are higher in human-to-human folds, while FACE-_SAM_ scores are lower. The only exception is FACE-_SPEAR_, while we will show it is a good metric in later sections. Nonetheless, these tabulated results have proved the validity of FACE in effectively capturing human-to-model spectral differences.

**Choice of estimator model.** We examine how different choices of the estimator model \(m_{}\) affect the resulting spectra, using GPT2-sm, -md, -lg and -xl as \(m_{}\), respectively. The spectra of webtext and the original GPT2 output data are computed. It is found that the spectra obtained from \(m_{}\) have different magnitudes, but their aggregated curves have the same shape (see Appendix). Therefore, the choice of \(m_{}\) will not affect FACE scores as long as the same \(m_{}\) is used for all data.

**Stationarity tests.** One of the assumptions of the Fourier transform is that the signal is _stationary_, i.e., the mean and variance do not change over time. We applied the Augmented Dickey-Fuller (ADF) test  to examine the stationarity of the cross-entropy sequences for all the human and model-generated data used in this study. The null hypothesis \(H_{0}\) of the ADF test is non-stationarity, and thus a \(p<.05\) testing result rejects \(H_{0}\) and accepts the alternative hypothesis of stationarity in the series. We calculate the proportions of cross-entropy sequences that pass the ADF test with \(p<.05\) for all model-generated and human data: 97.4% for GPT2, 92.1% for OPT, 74.5% for BLOOM, and 97.9% for human. Thus, the majority of data meets the stationarity requirement.

### Interpretation of spectra

As the frequency spectrum reflects the key characteristics of a signal, we attempt to interpret the spectra to see if they tell how the "signals" - entropy of human and machine languages - differ. Without aggregation, the raw spectra of single cross-entropy sequence look indistinguishable between GPT2-sm, GPT2-xl, and human (see the left plot in Figure 5). By aggregating 5,000 spectra from each group and smoothing the curves, it can be seen that GPT2-xl's curve is closer to human than the GPT2-sm curve (readers can find this by zooming in the middle plot in Figure 5). Here, the smoothing is done with generalized additive models (GAMs) . Results from other models are included in the Appendix.

    & _SO_ (\(\)) & _CORR_ (\(\)) & _SAM_ (\(\)) & _SPEAR_ (\(\)) \\   h-h (wiki) & **0.45** & **0.76** & **0.22** & 0.05 \\ h-h (news) & **0.45** & **0.76** & **0.22** & 0.07 \\ h-h (stories) & **0.47** & **0.79** & **0.21** & 0.05 \\ h-m & 0.44 & 0.75 & 0.23 & **0.17** \\   

Table 6: Results of sanity test on FACE’s validity. The top three rows are the control groups, and “h-h” stands for human-to-human folds. The last row is the experimental group, where “h-m” is for human-to-model fold. Better FACE scores are in bold. The scores in the bottom row are retrieved from Table 4.

  
**Metric** & **Generation Perplexity** & **Zipf Coefficient** & **Repetition** & **Distinct-4** & **Self-BLEU** & _SO_ & **MAUVE** & _SO-S_ & **MAUVE-S** \\   Human-like/BT & 0.810 & 0.833 & \(-0.167\) & 0.738 & 0.595 & 0.881 & **0.952** & **0.357** & \(0.214\) \\ Interesting/BT & 0.643 & 0.524 & \(-0.143\) & 0.524 & 0.405 & 0.762 & **0.810** & \(0.524\) & **0.667** \\ Sensible/BT & 0.738 & 0.690 & \(-0.071\) & 0.595 & 0.524 & 0.788 & **0.857** & **0.995** & \(0.706\) \\   

Table 5: Spearman’s rank correlation coefficients of _SO_ and five other metrics with human judgments. Higher scores mean better correlation. All the numbers except the _SO_, _SO_-S, and MAUVE-S columns are sourced from . “BT” denotes the Bradley-Terry score of the pairwise human evaluation, which is employed to compute the Spearman’s rank correlation with the scores of other metrics. Additionally, it is important to note that the original human judgments encompass certain pairs in which both texts are generated by models, albeit different models. Therefore, we refine the original human judgment dataset to only include judgments involving both human and model-generated languages, and the results are shown in _SO_-S and MAUVE-S.

When plotted separately, the aggregated spectra from human and different models have similar shapes: First, the majority of components exist in the low-frequency range (\(<0.05\)). In addition, the locations of peaks and troughs are almost the same between groups. For instance, \(_{1}=0.06\) is the first trough, and \(_{2}=0.12\) is the first peak (see the right plots in Figure 5). Thus, roughly speaking, the main difference between human and model spectra is not in the locations of peak and trough frequencies but in the relative magnitudes of those frequencies.

We propose a simple way to interpret the peaks in spectra: the reciprocal of a frequency component \(T_{k}=1/_{k}\) denotes the corresponding cycle in the time domain. Because the time interval (i.e., sampling interval) of an entropy sequence is not measured in _seconds_ but fixed as one _token_, the measurement unit of \(T_{k}\) is also in number of tokens. For example, the first frequency peak in Figure 5 (right plot) implies \(_{2}=0.12 T_{2}=1/0.12 8.3\) (tokens), which approximately means that tokens of the same cross-entropy levels tend to _recur_ every 8.3 tokens. This pattern is consistent in both human and model data. However, the degree of this _recurrence_ can mark the difference between the human and model languages. We leave more detailed interpretations of spectra to future work.

## 5 Conclusion and Limitations

We propose FACE, a set of metrics based on the Fourier analysis of cross-entropy, which is able to distinguish human and model-generated language with satisfactory performance in the open-ended generation task. The metrics scale with model sizes; reflect the effect of various sampling methods; correlate well with other existing metrics and outperform most of them in alignment with human judgement scores. Among the four implementation methods of FACE experimented, Spectral Overlap (_SO_) has the best overall performance.

FACE is computationally efficient with easy-to-interpret output. As a method inspired by psycholinguistic studies on the predictability (entropy/surprisal/information density) of human language, we believe FACE is a good example of incorporating knowledge from different fields for better human-centered AIs. We can generally conclude that better language models can produce spectral representations of information that are more similar to human.

Our current work has several limitations: Firstly, for open-ended generation experiments (Section 4.1), a broader set of sampling methods other than top-\(k\) can be used. Secondly, larger models (with more than 100 billion parameters) need to be included for more comprehensive comparisons. We will improve from these aspects in future work.