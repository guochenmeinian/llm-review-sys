# Connecting Certified and Adversarial Training

Yuhao Mao, Mark Niklas Muller, Marc Fischer, Martin Vechev

Department of Computer Science

ETH Zurich, Switzerland

{yuhao.mao, mark.mueller, marc.fischer, martin.vechev}@inf.ethz.ch

###### Abstract

Training certifiably robust neural networks remains a notoriously hard problem. While adversarial training optimizes _under-approximations_ of the worst-case loss, which leads to insufficient regularization for certification, sound certified training methods, optimize loose _over-approximations_, leading to over-regularization and poor (standard) accuracy. In this work, we propose TAPS, an (unsound) certified training method that combines IBP and PGD training to optimize more precise, although not necessarily sound, worst-case loss approximations, reducing over-regularization and increasing certified and standard accuracies. Empirically, TAPS achieves a new state-of-the-art in many settings, e.g., reaching a certified accuracy of \(22\%\) on TinyImageNet for \(_{}\)-perturbations with radius \(=1/255\). We make our implementation and networks public at github.com/eth-sri/taps.

## 1 Introduction

Adversarial robustness, _i.e._, a neural network's resilience to small input perturbations (Biggio et al., 2013; Szegedy et al., 2014), has established itself as an important research area.

Neural Network Certificationcan rigorously prove such robustness: While complete verification methods (Tjeng et al., 2019; Bunel et al., 2020; Zhang et al., 2022; Ferrari et al., 2022) can decide every robustness property given enough (exponential) time, incomplete methods (Wong and Kolter, 2018; Singh et al., 2019; Zhang et al., 2018) trade precision for scalability.

Adversarial trainingmethods, such as PGD (Madry et al., 2018), aim to improve robustness by training with samples that are perturbed to approximately maximize the training loss. This can be seen as optimizing an _under-approximation_ of the worst-case loss. While it _empirically_ improves robustness significantly, it generally does not induce sufficient regularization for certification and has been shown to fail in the face of more powerful attacks (Tramer et al., 2020).

Certified Trainingmethods, in contrast, optimize approximations of the worst-case loss, thus increasing certified accuracies at the cost of over-regularization that leads to reduced standard accuracies. In this work, we distinguish two certified training paradigms. Sound methods (Mirman et al., 2018; Gowal et al., 2018; Shi et al., 2021) compute sound over-approximations of the worst-case loss via bound propagation. The resulting approximation errors induce a strong (over-)regularization that makes certification easy but causes severely reduced standard accuracies. Interestingly, reducing these approximation errors by using more precise bound propagation methods, empirically, results in strictly _worse_ performance, as they induce harder optimization problems (Jovanovic et al., 2022). This gave rise to unsound methods (Balunovic and Vechev, 2020; Palma et al., 2022; Muller et al., 2022), which aim to compute _precise_ but not necessarily sound approximations of the worst-case loss, reducing (over)-regularization and resulting in networks that achieve higher standard and certified accuracies, but can be harder to certify. Recent advances in certification techniques, however, have made their certification practically feasible (Ferrari et al., 2022; Zhang et al., 2022).

We illustrate this in Figure 1, where we compare certified training methods with regard to their worst-case loss approximation errors and the resulting trade-off between certified and standard accuracy. On the left, we show histograms of the worst-case loss approximation error over test set samples (see Section 4.2 for more details). Positive values (right of the y-axis) correspond to over- and negative values (left of the y-axis) to under-approximations. As expected, we observe that the sound IBP (Gowal et al., 2018) always yields over-approximations (positive values) while the unsound SABR (Muller et al., 2022) yields a more precise (\(6\)-fold reduction in mean error) but unsound approximation of the worst-case loss. Comparing the resulting accuracies (right), we observe that this more precise approximation of the actual optimization objective, _i.e_., the true worst-case loss, by SABR (\(\)) yields both higher certified and standard accuracies than the over-approximation by IBP (\(\)). Intuitively, reducing the over-regularization induced by a systematic underestimation of the network's robustness allows it to allocate more capacity to making accurate predictions.

The core challenge of effective certified training is, thus, to compute precise (small mean error and low variance) worst-case loss approximations that induce a well-behaved optimization problem.

This Work proposes **T**raining via **A**dversarial **P**ropagation through **S**ubnetworks (TAPS), a novel (unsound) certified training method tackling this challenge, thereby increasing both certified and standard accuracies. Compared to SABR (\(\) the current state-of-the-art), TAPS (\(\)) enjoys a further \(5\)-fold mean approximation error reduction and significantly reduced variance (Figure 1 left), leading to improved certified and natural accuracies (right). The key technical insight behind TAPS is to combine IBP and PGD training via a gradient connector, a novel mechanism that allows training the whole network jointly such that the over-approximation of IBP and under-approximations of PGD cancel that TAPS yields exceptionally tight worst-case loss approximations which allow it to improve on state-of-the-art results for MNIST, CIFAR-10, and TinyImageNet.

## 2 Background on Adversarial and Certified Training

Here, we provide the necessary background on adversarial and certified training. We consider a classifier \(F\) parameterized by weights \(\) and predicting a class \(y_{} F()_{y}f_{y}(x)\) for every input \(^{d}\) with label \(y\{1,,K\}\) where \(^{||}\) is a neural network, assigning a numerical logit \(o_{i} f_{i}()\) to each class \(i\).

Adversarial RobustnessWe call a classifier adversarially robust on an \(_{p}\)-norm ball \(_{p}(,)\) if it classifies all elements within the ball to the correct class, _i.e_., \(F(^{})=y\) for all perturbed inputs \(^{}_{p}(,)\). In this work, we focus on \(_{}\)-robustness with \(_{}(,)\{^{}\|^ {}-\|_{}\}\) and thus drop the subscript \(\).

Neural Network Certificationis used to formally _prove_ robustness properties of a neural network, _i.e_., that all inputs in the region \((,)\) yield the correct classification. We call samples \(\) where this is successfull, certifiably robust and denote the portion of such samples as _certified accuracy_, forming a lower bound to the true robustness of the analyzed network.

Interval bound propagation (IBP) (Mirman et al., 2018; Gowal et al., 2018) is a particularly simple yet effective certification method. Conceptually, it computes an over-approximation of a network's reachable set by propagating the input region \((,)\) through the network, before checking whether all reachable outputs yield the correct classification. This is done by, first, over-approximating the input region \((,)\) as a Box \([}^{0},}^{0}]\) (each dimension is described as an interval), centered at \(^{0}=\) and with radius \(^{0}=\), such that we have the \(i^{}\) dimension of the input \(x_{i}^{0}[c_{i}^{0}-_{i}^{0},c_{i}^{0}+_{i}^{0}]\). We then

Figure 1: Histograms of the worst-case loss approximation errors over the test set (left) for different training methods show that TAPS (our work) achieves the most precise approximations and highest certified accuracy (right). Results shown here are for a small CNN3.

propagate it through the network layer-by-layer (for more details, see (Mirman et al., 2018; Gowal et al., 2018)), until we obtain upper and lower bounds \([^{},}^{}]\) on the logit differences \(^{}-o_{y}\). If we can now show dimensionwise that \(}^{}<0\) (except for \(}^{}_{y}=0\)), this proves robustness. Note that this is equivalent to showing that the maximum margin loss \(_{}(^{},y)_{i y}}^{}_{i}\) is less than \(0\) for all perturbed inputs \(^{}(,)\).

Training for Robustnessaims to find a model parametrization \(\) that minimizes the expected worst-case loss for some loss-function \(\):

\[=*{arg\,min}_{}_{,y}[ _{^{}(,)}(^{ },y)].\] (1)

As the inner maximization objective in Equation (1) can generally not be solved exactly, it is often under- or over-approximated, giving rise to adversarial and certified training, respectively.

Adversarial Trainingoptimizes a lower bound on the inner maximization problem in Equation (1) by training the network with concrete samples \(^{}(,)\) that (approximately) maximize the loss function. A well-established method for this is _Projected Gradient Descent (PGD)_ training (Madry et al., 2018) which uses the Cross-Entropy loss \(_{}(,y)(1+_{i y}(f_{i} ()-f_{y}()))\). Starting from a random initialization point \(}_{0}(,)\), it performs \(N\) update steps

\[}_{n+1}=_{(,)}}_{n}+ (_{}_{n}}(}_{n},y))\]

with step size \(\) and projection operator \(\). Networks trained this way typically exhibit good empirical robustness but remain hard to formally certify and vulnerable to stronger or different attacks (Tramer et al., 2020; Croce and Hein, 2020).

Certified Training,in contrast, is used to train _certifiably_ robust networks. In this work, we distinguish two classes of such methods: while _sound_ methods optimize a sound upper bound of the inner maximization objective in Equation (1), _unsound_ methods sacrifice soundness to use an (in expectation) more precise approximation. Methods in both paradigms are often based on evaluating the cross-entropy loss \(_{}\) with upper bounds on the logit differences \(}^{}\).

IBP (a sound method) uses sound Box bounds on the logit differences, yielding

\[_{}(,y,)1+_{i y }(}^{}_{i}).\] (2)

SABR (an unsound method) (Muller et al., 2022), in contrast, first searches for an adversarial example \(^{}(^{},-)\) and then computes Box-bounds only for a small region \((^{},)(,)\) (with \(<\)) around this adversarial example \(^{}\) instead of the original input \(\)

\[_{}_{^{}(^ {},-)}_{}(^{},y,).\] (3)

This generally yields a more precise (although not sound) worst-case loss approximation, thereby reducing over-regularization and improving both standard and certified accuracy.

## 3 Precise Worst-Case Loss Approximation

In this section, we first introduce TAPS, a novel certified training method combining IBP and PGD training to obtain more precise worst-case loss estimates, before showing that this approach is orthogonal and complementary to current state-of-the-art methods.

### TAPS - Combining IBP and PGD

The key insight behind TAPS is that adversarial training with PGD and certified training with IBP complement each other perfectly: (i) both yield well-behaved optimization problems, as witnessed by their empirical success, and (ii) we can combine them such that the over-approximation errors incurred during IBP are compensated by the under-approximations of PGD. TAPS harnesses this as follows: For every sample, we first propagate the input region part-way through the network using

[MISSING_PAGE_FAIL:4]

We now consider a single PGD step and observe that bounds in the \(i^{}\) dimension have no impact on the \(j^{}\) coordinate of the resulting adversarial example as they impact neither the gradient sign nor the projection in this dimension, as Box bounds are axis parallel. We thus assume independence of the \(j^{}\) dimension of the latent adversarial example \(_{j}\) from the bounds in the \(i^{}\) dimension \(_{i}\) and \(_{i}\) (for \(i j\)), which holds rigorously (up to initialization) for a single step attack and constitutes a mild assumption for multi-step attacks. Therefore, we have \(_{j}}{_{i}}=0\) for \(i j\) and obtain \(}{d_{i}}=}{d_{i}}_{j}}{_{i}}\), leaving only \(_{j}}{_{i}}\) for us to define.

The most natural gradient connector is the _binary connector_, _i.e._, set \(_{i}}{_{i}}=1\) when \(_{i}=_{i}\) and \(0\) otherwise, as it is a valid sub-gradient for the projection operation in PGD. However, the latent adversarial input often does not lie on a corner (extremal vertex) of the Box approximation, leading to sparse gradients and thus a less well-behaved optimization problem. More importantly, the binary connector is very sensitive to the distance between (local) loss extrema and the box boundary and thus inherently ill-suited to gradient-based optimization. For example, a local extremum at \(_{i}\) would induce \(_{i}}{_{i}}=1\) in the box \([_{i},0]\), but \(_{i}}{_{i}}=0\) for \([_{i}-,0]\), even for arbitrarily small \(\).

To alleviate both of these problems, we consider a _linear connector_, _i.e._, set \(_{i}}{_{i}}=_{i}- _{i}}{_{i}-_{i}}\). However, even when our latent adversarial example is very close to one bound, the linear connector would induce non-zero gradients w.r.t. to the opposite bound. To remedy this undesirable behavior, we propose the _rectified linear connector_, setting \(_{i}}{_{i}}=(0,1-_{i}- _{i}}{c(_{i}-_{i})})\) where \(c\) is a constant (visualized in Figure 3 for \(c=0.3\)). Observe that it recovers the binary connector for \(c=0\) and the linear connector for \(c=1\). To prevent gradient sparsity (\(c 0.5\)) while avoiding the above-mentioned counterintuitive gradient connections (\(c 0.5\)), we set \(c=0.5\) unless indicated otherwise. When the upper and lower bounds are identical in the \(i^{}\) dimension, PGD turns into an identity function. Therefore, we set both gradients to \(_{i}}{_{i}}=_{i}}{ _{i}}=0.5\) turning the gradient connector into an identity function for the backward pass.

### TAPS Loss & Multi-estimator PGD

The standard PGD attack, used in adversarial training, henceforth called _single-estimator_ PGD, is based on maximizing the Cross-Entropy loss \(_{}\) of a single input. In the context of TAPS, this results in the overall loss

\[_{}^{}(,y,)=_{}[},}]}1+_{i y}(f_{C}( })_{i}-f_{C}(})_{y}),\]

where the embedding space bounding box \([},}]\) is obtained via \(\). However, this loss is not necessarily well aligned with adversarial robustness. Consider the example illustrated in Figure 4, where only points in the lower-left quadrant are classified correctly (_i.e._, \(o_{i}^{}:=o_{i}-o_{y}<0\)). We compute the latent adversarial example \(}\) by conducting a standard adversarial attack on the Cross-Entropy loss over the reachable set (optimally for illustration purposes) and observe that the corresponding output \((})\) (\(\)) is classified correctly. However, if we instead use the logit differences \(o_{1}^{}\) and \(o_{2}^{}\) as attack objectives, we obtain two misclassified points (\(\)). Combining their dimension-wise worst-case bounds (\(\)), we obtain the point \({}^{}\), which realizes the maximum loss over an optimal box approximation of the reachable set. As the correct classification of this point (when computed exactly) directly corresponds to true robustness, we propose the _multi-estimator_ PGD variant of \(_{}\), which estimates the upper bounds on the logit differences \(o_{i}^{}\) using separate samples and then computes the loss function using the per-dimension worst-cases as:

\[_{}(,y,)=1+_{i y} _{}[},}]}f_{C}( {})_{i}-f_{C}(})_{y}.\]

Figure 4: Illustration of the bounds on \(o_{i}^{}:=o_{i}-o_{t}\) obtained via single estimator (- -) and multi-estimator (\(\)) PGD and the points maximizing the corresponding losses: \(\) for \(_{}^{}\) and \({}^{}\) for \(_{}\).

Figure 3: Gradient connector visualization.

### Training Objective & Regularization

While complete certification methods can decide any robustness property, this requires exponential time. Therefore, networks should not only be robust but also certifiable. Thus, we propose to combine the IBP loss for easy-to-learn and certify samples with the TAPS loss for harder samples as follows:

\[(,y,)=_{}(,y,) _{}(,y,).\]

This expresses that every sample should be either certifiable with TAPS or IBP bounds1. Further, as by construction \(_{}_{}\), we add a scaling term \(\) to the loss gradient:

\[}{d} 2_{ }}{d}_{}+(2-2)_{ }}{d}_{}.\]

Here, \(=0.5\) recovers the standard gradient, obtained via the product rule (both sides weighted with \(1\)), while \(=0\) and \(=1\) correspond to using only the (weighted) IBP and TAPS gradients, respectively. Henceforth, we express this as the regularization weight \(w_{}=\), which intuitively expresses the weight put on TAPS, using \(w_{}=5\) unless specified otherwise. Lastly, we reduce the variance of \(\) by averaging \(_{}\) and \(_{}\) over a mini batch before multiplying (see App. A).

### STAPS - Balancing Regularization by Combining TAPS with SABR

Recall that SABR (Muller et al., 2022) reduces the over-regularization of certified training by propagating a small, adversarially selected Box through the whole network. However, as Box approximations grow exponentially with depth (Muller et al., 2022; Shi et al., 2021; Mao et al., 2023), regardless of the input region size, SABR has to strike a balance between regularizing early layers too little and later layers too much. In contrast, TAPS's approach of propagating the full input region through the first part of the network (the feature extractor) before using PGD for the remainder reduces regularization only in later layers. Thus, we propose STAPS by replacing the IBP components of TAPS, for both propagation and regularization, with SABR to obtain a more uniform reduction of over-regularization throughout the whole network.

STAPS, identically to SABR, first conducts a PGD attack over the whole network to find an adversarial example \(^{}(^{},-)\). Then, it propagates Box-bounds for a small region \((^{},)(,)\) (with \(<\)) around this adversarial example \(^{}\) through the feature extractor, before, identically to TAPS, conducting an adversarial attack in the resulting latent-space region over the classifier component of the network.

## 4 Experimental Evaluation

In this section, we evaluate TAPS empirically, first, comparing it to a range of state-of-the-art certified training methods, before conducting an extensive ablation study validating our design choices.

Experimental SetupWe implement TAPS in PyTorch (Paszke et al., 2019) and use MN-BaB (Ferrari et al., 2022) for certification. We conduct experiments on MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky et al., 2009), and TinyImageNet (Le and Yang, 2015) using \(_{}\) perturbations and the CNN7 architecture (Gowal et al., 2018). For more experimental details including hyperparameters and computational costs and an extended analysis see App. B and App. C, respectively.

### Main Results

In Table 1, we compare TAPS to state-of-the-art certified training methods. Most closely related are IBP, recovered by TAPS if the classifier size is zero, and COLT, which also combines bound propagation with adversarial attacks but does not allow for joint training. TAPS dominates IBP, improving on its certified and natural accuracy in all settings and demonstrating the importance of avoiding over-regularization. Compared to COLT, TAPS improves certified accuracies significantly, highlighting the importance of joint optimization. In some settings, this comes at the cost of slightly reduced natural accuracy, potentially due to COLT's use of the more precise Zonotop approximations. Compared to the recent SABR and IBP-R, TAPS often achieves higher certified accuracies atthe cost of slightly reduced natural accuracies. Reducing regularization more uniformly with STAPS achieves higher certified accuracies in almost all settings and better natural accuracies in many, further highlighting the orthogonality of TAPS and SABR. Most notably, STAPS increases certified accuracy on TinyImageNet by almost \(10\%\) while also improving natural accuracy. SortNet, a generalization of a range of recent architectures (Zhang et al., 2021, 2022c; Anil et al., 2019), introducing novel activation functions tailored to yield networks with high \(_{}\)-robustness, performs well on CIFAR-10 at \(=8/255\), but is dominated by STAPS in every other setting.

### Ablation Study

Approximation PrecisionTo evaluate whether TAPS yields more precise approximations of the worst-case loss than other certified training methods, we compute approximations of the maximum margin loss with IBP, PGD (\(50\) steps, \(3\) restarts), SABR (\(=0.4\)), and TAPS on a small TAPS-trained CNN3 for all MNIST test set samples. We report histograms over the difference to the exact worst-case loss computed with a MILP encoding (Tjeng et al., 2019) in Figure 5. Positive values correspond to over-approximations while negative values correspond to under-approximation. We observe that the TAPS approximation is by far the most precise, achieving the smallest mean and mean absolute error as well as variance. We confirm these observations for other training methods in Figure 9 in App. C.

To isolate the under-approximation effect of the PGD propagation through the classifier, we visualize the distribution over pairwise bound differ

   Dataset & \(_{}\) & Training Method & Source & Nat. [\%] & Cert. [\%] \\   & & COLT & Balunovic and Vechev (2020) & 99.2 & 97.1 \\  & & IBP & Shi et al. (2021) & 98.84 & 97.95 \\  & & SORTNet & Zhang et al. (2022b) & 99.01 & 98.14 \\  & & SABR & Muller et al. (2022a) & **99.23** & 98.22 \\  & & TAPS & this work & 99.19 & **98.39** \\  & & STAPS & this work & 99.15 & 98.37 \\   & & COLT & Balunovic and Vechev (2020) & 97.3 & 85.7 \\  & & IBP & Shi et al. (2021) & 97.67 & 93.10 \\  & & SORTNet & Zhang et al. (2022b) & 98.46 & 93.40 \\  & & SABR & Muller et al. (2022a) & **98.75** & 93.40 \\  & & TAPS & this work & 97.94 & **93.62** \\  & & STAPS & this work & 98.53 & 93.51 \\   & & COLT & Balunovic and Vechev (2020) & 78.4 & 60.5 \\  & & IBP & Shi et al. (2021) & 66.84 & 52.85 \\  & & SORTNet & Zhang et al. (2022b) & 67.72 & 56.94 \\  & & IBP-R & Palma et al. (2022) & 78.19 & 61.97 \\  & & SABR & Muller et al. (2022a) & 79.24 & 62.84 \\  & & TAPS & this work & 75.09 & 61.56 \\  & & STAPS & this work & **79.76** & **62.98** \\   & & COLT & Balunovic and Vechev (2020) & 51.7 & 27.5 \\  & & IBP & Shi et al. (2021) & 48.94 & 34.97 \\  & & SORTNet & Zhang et al. (2022b) & **54.84** & **40.39** \\  & & IBP-R & Palma et al. (2022) & 51.43 & 27.87 \\  & & SABR & Muller et al. (2022a) & 52.38 & 35.13 \\  & & TAPS & this work & 49.76 & 35.10 \\  & & STAPS & this work & 52.82 & 34.65 \\   & & IBP & Shi et al. (2021) & 25.92 & 17.87 \\  & & SORTNet & Zhang et al. (2022b) & 25.69 & 18.18 \\   & & SABR & Muller et al. (2022a) & 28.85 & 20.46 \\   & & TAPS & this work & 28.34 & 20.82 \\   & & STAPS & this work & **28.98** & **22.16** \\   

Table 1: Comparison of natural (Nat.) and certified (Cert.) accuracy on the full MNIST, CIFAR-10, and TinyImageNet test sets. We report results for other methods from the relevant literature.

Figure 5: Distribution of the worst-case loss approximation errors over test set samples.

Figure 6: Bound difference between IBP and PGD propagation through the classifier depending on the training method.

ences between TAPS and IBP and STAPS and SABR in Figure 6 for different training methods. We observe that the distributions for TAPS and STAPS are remarkably similar (up to scaling), highlighting the importance of reducing over-regularisation of the later layers, even when propagating only small regions (SABR/STAPS). Further, we note that larger bound differences indicate reduced regularisation of the later network layers. We thus observe that SABR still induces a much stronger regularisation of the later layers than TAPS and especially STAPS, again highlighting the complementarity of TAPS and SABR, discussed in Section 3.5.

IBP RegularizationTo analyze the effectiveness of the multiplicative IBP regularization discussed in Section 3.4, we train with IBP in isolation (\(_{}\)), IBP with TAPS weighted gradients (\(w_{}=0\)), varying levels of gradient scaling for the TAPS component (\(w_{}\)), TAPS with IBP weighting (\(w_{}=\)), and TAPS loss in isolation, reporting results in Table 2. We observe that IBP in isolation yields comparatively low standard but moderate certified accuracies with fast certification times. Increasing the weight \(w_{}\) of the TAPS gradients reduces regularization, leading to longer certification times and higher standard accuracies. Initially, this translates to higher adversarial and certified accuracies, peaking at \(w_{}=15\) and \(w_{}=5\), respectively, before especially certified accuracy decreases as regularization becomes insufficient for certification. We confirm these trends for TinyImageNet in Table 14 in App. C.

Split LocationTAPS splits a given network into a feature extractor and classifier, which are then approximated using IBP and PGD, respectively. As IBP propagation accumulates over-approximation errors while PGD is an under-approximation, the location of this split has a strong impact on the regularization level induced by TAPS. To analyze this effect, we train multiple CNN7s such that we obtain classifier components with between \(0\) and \(6\) (all) ReLU layers and illustrate the resulting standard, adversarial, and certified (using different methods) accuracies in Figure 7 for CIFAR-10, and in App. C for MNIST and TinyImageNet in Tables 11 and 13 respectively.

For small perturbations (\(=2/255\)), increasing classifier size and thus decreasing regularization yields increasing natural and adversarial accuracy. While the precise MN-BaB verification can translate this to rising certified accuracies up to large classifier sizes, regularization quickly becomes insufficient for the less precise IBP and CROWN-IBP certification. For larger perturbations (\(=8/255\)), the behavior is more complex. An initial increase of all accuracies with classifier size is followed by a sudden drop and slow recovery, with certified accuracies remaining below the level achieved for 1 ReLU layer. We hypothesize that this effect is due to the IBP regularization starting to dominate optimization combined with increased training difficulty (see App. C for details). For both perturbation magnitudes, gains in certified accuracy can only be realized with the precise MN-BaB certification (Muller et al., 2022), highlighting the importance of recent developments in neural network verification for certified training.

   \(w_{}\) & Avg time (s) & Nat (\%) & Adv. (\%) & Cert. (\%) \\  \(_{}\) & 2.3 & 97.6 & 93.37 & 93.15 \\
0 & 2.7 & 97.37 & 93.32 & 93.06 \\
1 & 4.5 & 97.86 & 93.80 & 93.36 \\
5 & 6.9 & 97.94 & 94.01 & **93.62** \\
10 & 15.7 & 98.25 & 94.43 & 93.02 \\
15\({}^{}\) & 42.8 & 98.53 & **95.00** & 91.55 \\
20\({}^{}\) & 73.7 & **98.75** & 94.33 & 82.67 \\ \(^{}\) & 569.7 & 98.0 & 94.00 & 45.00 \\ \(_{}{}^{}\) & 817.1 & 98.5 & 94.50 & 17.50 \\   ^{}\) Only evaluated on part of the test set within a 2-day time limit.} \\ 

Table 2: Effect of IBP regularization and the TAPS gradient expanding coefficient \(\) for MNIST \(=0.3\).

Figure 7: Effect of split location on the standard and robust accuracy of TAPS trained networks, depending on the perturbation magnitude \(\) for different certification methods for CIFAR-10. \(0\) ReLUs in the classifier recovers IBP training.

Gradient ConnectorIn Figure 8, we illustrate the effect of our gradient connector's parameterization c (Section 3.2). We report TAPS accuracy (the portion of samples where all latent adversarial examples are classified correctly) as a proxy for the goodness of fit. Recall that \(c=0\) corresponds to the binary connector and \(c=1\) to the linear connector. We observe that the binary connector achieves poor TAPS and natural accuracy, indicating a less well-behaved optimization problem. TAPS accuracy peaks at \(c=0.5\), indicating high goodness-of-fit and thus a well-behaved optimization problem.

Single-Estimator vs Multi-Estimator PGDTo evaluate the importance of our multi-estimator PGD variant, we compare it to single-estimator PGD across a range of split positions, reporting results in Table 3. We observe that across all split positions, multi-estimator PGD achieves better certified and better or equal natural accuracy. Further, training collapses reproducibly for single-estimator PGD for small classifiers, indicating that multi-estimator PGD additionally improves training stability.

PGD Attack StrengthTo investigate the effect of the adversarial attack's strength, we use \(1\) or \(3\) restarts and vary the number of attack steps used in TAPS from \(1\) to \(100\) for MNIST at \(=0.3\), reporting results in Table 4. Interestingly, even a single attack step and restart are sufficient to achieve good performance and outperform IBP. As we increase the strength of the attack, we can increase certified accuracy slightly while marginally reducing natural accuracy, agreeing well with our expectation of regularization strength increasing with attack strength.

## 5 Related Work

Verification MethodsIn this work, we only consider deterministic verification methods, which analyze a given network as is. While _complete_ (or _exact_) methods (Tjeng et al., 2019; Wang et al., 2021; Zhang et al., 2022a; Ferrari et al., 2022) can decide any robustness property given enough time, _incomplete_ methods (Singh et al., 2018; Raghunathan et al., 2018; Zhang et al., 2018; Dathathri et al., 2020; Muller et al., 2022b) sacrifice some precision for better scalability. However, recent complete methods can be used with a timeout to obtain effective incomplete methods.

Certified TrainingMost certified training methods compute and minimize sound over-approximations of the worst-case loss using different approximation methods: DiffA1 (Mirman et al., 2018) and IBP (Gowal et al., 2018) use Box approximations, Wong et al. (2018) use DeepZ relaxations (Singh et al., 2018), Wong and Kolter (2018) back-substitute linear bounds using fixed relaxations, Zhang et al. (2020) use dynamic relaxations (Zhang et al., 2018; Singh et al., 2019) and compute intermediate bounds using Box relaxations. Shi et al. (2021) significantly shorten training schedules by combining IBP training with a special initialization. Some more recent methods instead compute and optimize more precise, but not necessarily sound, worst-case loss approximations: SABR (Muller et al., 2022a) reduce the regularization of IBP training by propagating only small but carefully selected subregions. IBP-R (Palma et al., 2022) combines adversarial training at large perturbation radii with an IBP-based regularization. COLT (Balunovic and Vechev, 2020) is conceptually most similar to TAPS and thus compared to in more detail below. While prior work

   &  &  \\   & Certified & Natural & Certified & Natural \\ 
1 & 93.36 & **98.22** & 93.47 & **98.22** \\
5 & 93.15 & 97.90 & **93.55** & 97.90 \\
20 & **93.62** & 97.94 & 93.52 & 97.99 \\
100 & 93.46 & 97.94 & **93.55** & 97.99 \\  

Table 4: Effect of different PGD attack strengths for MNIST at \(=0.3\).

Figure 8: Effect of the gradient connector on TAPS (left) and natural (right) accuracy.

  \)} &  &  \\   & Certified & Natural & Certified & Natural \\ 
1 & -! & 31.47\({}^{}\) & **93.62** & 97.94 \\
3 & 92.91 & 98.56 & 93.03 & 98.63 \\
6 & 92.41 & **98.88** & 92.70 & **98.88** \\  

* Training encounters mode collapse. Last epoch performance reported.

Table 3: Comparison of single- and multi-estimator PGD, depending on the split position for MNIST at \(=0.3\).

combined a robust and a precise network (Muller et al., 2021; Horvath et al., 2022), to trade-off certified and standard accuracy, these unsound certified training methods can often increase both.

COLT (Balunovic and Vechev, 2020), similar to TAPS, splits the network into a feature extractor and classifier, computing bounds on the feature extractor's output (using the Zonotope(Singh et al., 2019) instead of Box domain) before conducting adversarial training over the resulting region. Crucially, however, COLT lacks a gradient connector and, thus, does not enable gradient flow between the latent adversarial examples and the bounds on the feature extractor's output. Therefore, gradients can only be computed for the weights of the classifier but not the feature extractor, preventing the two components from being trained jointly. Instead, a stagewise training process is used, where the split between feature extractor and classifier gradually moves through the network starting with the whole network being treated as the classifier. This has several repercussions: not only is the training very slow and limited to relatively small networks (a four-layer network takes almost 2 days to train) but more importantly, the feature extractor (and thus the whole network) is never trained specifically for precise bound propagation. Instead, only the classifier is trained to become robust to the incurred imprecisions. As this makes bound propagation methods ineffective for certification, Balunovic and Vechev (2020) employ precise but very expensive mixed integer linear programming (MILP (Tjeng et al., 2019)), further limiting the scalability of COLT.

In our experimental evaluation (Section 4.1), we compare TAPS in detail to the above methods.

Robustness by ConstructionLi et al. (2019), Lecuyer et al. (2019), and Cohen et al. (2019) construct probabilistic classifiers by introducing randomness into the inference process of a base classifier. This allows them to derive robustness guarantees with high probability at the cost of significant (100x) runtime penalties. Salman et al. (2019) train the base classifier using adversarial training and Horvath et al. (2022) ensemble multiple base models to improve accuracies at a further runtime penalty. Zhang et al. (2021, 2022) introduce \(_{}\)-distance neurons, generalized to SortNet by Zhang et al. (2022) which inherently exhibits \(_{}\)-Lipschitzness properties, yielding good robustness for large perturbation radii, but poor performance for smaller ones.

## 6 Conclusion

We propose TAPS, a novel certified training method that reduces over-regularization by constructing and optimizing a precise worst-case loss approximation based on a combination of IBP and PGD training. Crucially, TAPS enables joint training over the IBP and PGD approximated components by introducing the gradient connector to define a gradient flow through their interface. Empirically, we confirm that TAPS yields much more precise approximations of the worst-case loss than existing methods and demonstrate that this translates to state-of-the-art performance in certified training in many settings.