# HelpSteer2: Open-source dataset for training top-performing reward models

Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen

Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev

NVIDIA

{zhilinw, yidong}@nvidia.com

###### Abstract

High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful Nemotron-4-340B base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. Additionally, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner.

## 1 Introduction

Since the pioneering works on Reinforcement Learning from Human Feedback , the significance of incorporating preference information into model alignment has been consistently demonstrated . Both proprietary models (e.g., GPT-4 , Claude , Gemini ) and open-source models (e.g., Llama 3 , Mistral , and Yi ) have benefited from preference modeling techniques. However, most of these models lack detailed information about the preference data used in their training, hindering the broader community from fully leveraging these techniques. For instance, Llama 2  only disclosed the use of over 1 million binary comparisons, while Llama 3  reported the use of 10 million data samples for supervised fine-tuning and preference modeling without additional details.

To address this issue, a few domain-general chat preference datasets have been made available to the community. Some of these datasets come with permissive licenses, such as Anthropic's Helpful-Harmless RLHF (MIT license) , Open Assistant (Apache 2.0) , and HelpSteer (CC-BY-4.0) , facilitating their use in both academic and commercial settings. However, these datasets have become less relevant for training the most well-aligned models currently available .

Others in the community have tackled this challenge by using proprietary models, such as GPT-4, to create preference datasets like Ultrafeedback , Nectar , and Distilabel-Capybara . Although these datasets are more effective for aligning models, their use is often restricted to academic or non-commercial settings. The terms of use from various large language model providers explicitly prohibit the use of model outputs to develop competing models, posing legal risks for commercial organizations that use these datasets to train their own large language models. Further discussion on existing preference datasets is provided in Appendix 5.

We propose HelpSteer2, a CC-BY-4.0-licensed open-source helpfulness dataset, designed to train state-of-the-art reward models. Additionally, we provide detailed information about our data collection process to aid similar efforts and demonstrate how reward models trained with HelpSteer2 can align large language models with human preferences. We extend SteerLM [18; 12] to present SteerLM 2.0, a novel model alignment paradigm that can effectively utilize multi-faceted rewards from our reward models to train models to follow complex multi-requirement instructions. By open-sourcing the dataset with minimal usage restrictions, we invite the community to utilize and build upon it to develop well-aligned AI systems.

## 2 Dataset

### Dataset Collection

Prompt CollectionMost of the prompts (over 95%) used in HelpSteer2 are sourced from ShareGPT , a platform where ChatGPT users voluntarily share their conversations. We selected this dataset as a source of prompts because we believe that ShareGPT encompasses a diverse range of real-world LLM use cases. Importantly, we only use user inputs from this dataset, while Assistant turns are stripped out to avoid potential model-specific licensing restrictions. We supplemented ShareGPT prompts with a small proportion of proprietary prompts, primarily focused on use cases such as summarization, closed question answering, and extraction. These use cases are relevant in enterprise settings but are less likely to be represented in ShareGPT. An analysis of the distribution of topics represented by these prompts are in Appendix D.

Given that our annotator pool consisted solely of US-based annotators fluent in English and not expected to be competent in other languages, we removed all non-English prompts, as identified by FastText . Additionally, since our annotators lacked expertise in the coding domain, we employed simple heuristics to filter out prompts containing snippets of popular programming languages.

To ensure a diverse sample of prompts, we utilized BERTopic  to cluster similar prompts into approximately 1000 topics. We then sampled uniformly from each topic across various deliveries to our vendor. Additionally, we observed that high-quality generation in real-world settings requires the model to handle complex prompts, sometimes containing multiple requirements. Inspired by , we assessed the complexity of each prompt on a Likert-5 scale using Nemotron-2-43B , with details provided in Appendix E. Subsequently, we sampled prompts uniformly across each complexity level, except for the highest complexity level, which was given twice the weight of other levels.

Multi-turn Prompt CompletionTo ensure HelpSteer2 is effective for predicting rewards in multi-turn conversations, we included multi-turn prompts, which comprise approximately 29% of the samples. For these prompts, we did not use the original ShareGPT Assistant responses, as those may be generated by models with restrictive licenses. Instead, we replaced these Assistant turns with responses generated by a 22B in-house model, specifically trained to provide Assistant responses given only user turns. This model was fine-tuned using conversations from the Open Assistant  and HH-RLHF  datasets (see Appendix F for details).

Response GenerationWe generate two responses per prompt, instead of four as in HelpSteer , to minimize annotators' cognitive load during annotation, thereby enhancing rating quality. The sources and associated proportions of these responses are as follows (the two responses for each prompt always come from two different sources):

1. Our own internal LLMs from three generations of models: * Nemotron-2 (43B models based on the model used to generate HelpSteer responses, described in more details in ): \(18.9\%\) of the responses- see  for information on the publicly released 8B models. 22B models come from a base model following a similar architecture and pre-training scheme but larger size: \(40.4\%\) (\(2.2\%\) from 8B, \(38.2\%\) from 22B) * Nemotron-4 (15B and 340B models
- see  for details on the 15B pre-trained model, while the 340B one follows a similar architecture and pre-training scheme but with more parameters): \(26.9\%\) (\(9.5\%\) from 15B, \(17.4\%\) from 340B)
2. Mixtral-8x7B-Instruct-v0.1 : \(7.9\%\)
3. Human annotators from Scale AI: \(5.9\%\)

Throughout the data collection effort, we used aligned versions of the internal models mentioned above, all trained on datasets with permissive commercial licenses using Megatron-LM  for pre-training and NeMo-Aligner  for fine-tuning. For fine-tuning, we employed several techniques: Supervised Fine-Tuning, SteerLM , Reinforcement Learning from Human Feedback , and Direct Preference Optimization . This diversity in model sizes and learning algorithms was intended to substantially increase response diversity compared to the original HelpSteer dataset , which relied on a single Nemotron-2 43B model for responses. Additionally, we leveraged SteerLM's controllable generation capabilities to generate some responses with randomly sampled SteerLM labels, further varying response styles.

Response AnnotationOur response annotation process, guidelines and annotator screening are primarily derived from HelpSteer guidelines . Specifically, for each response, we annotate five attributes (helpfulness, correctness, coherence, complexity, and verbosity) on a Likert-5 scale. However, we have implemented several improvements to the annotation process.

First, we required at least three annotators to annotate each response compared to only one annotator in HelpSteer . We opted for multiple annotators per sample because initial explorations indicated that annotation quality, measured by inter-annotator agreement, is crucial for model training. Without high-quality annotations, the data can be noisy, which can potentially confuse the model on what characterizes a higher score. Each sample is initially annotated by three annotators. If these annotators demonstrate a high level of disagreement (i.e. the difference in helpfulness among them is greater than 2), two additional annotators are recruited to annotate the sample. Overall, samples were on average annotated by 3.41 annotators.

In addition, annotators were asked to rate two responses to the same prompt sequentially. Our initial analysis showed that doing this can allow the annotator to provide a more calibrated score for each response (e.g. if response A is much better than response B, then helpfulness for the response should be much higher). It does so by reducing the likelihood of annotators doing slipshod annotations and also facilitates quality assurance on such annotations. Overall, this means that each sample annotated for HelpSteer2 required substantially more effort and resources compared to HelpSteer . To meet this challenge, we engaged approximately 1,000 US-based annotators through our vendor Scale AI compared to 200 annotators engaged in HelpSteer . We would like to highlight that our guidelines explicitly ask annotators to skip a sample if it contains any Personally Identifiable Information (e.g. name, address, SSN, email, phone numbers) and to flag it for unsafe content (e.g. harmful content, illegal activities, profanity, bias and stereotyping). Please refer to Appendix C for ethical considerations relating to such annotations and Appendix G for the full annotation guidelines.

We measure inter-annotator agreement using Quadratic weighted Cohen's \(\). Compared to metrics for measuring more than two annotators (e.g. Krippendorff's \(\) or Fleiss' \(\)), we chose to use Cohen's \(\) because given the large number of annotators (1000), multiple individual annotators were rarely allocated common sets of samples to annotate. We also chose to use the quadratic

   _Attribute_ & Initial Collection & After Improvements & Post-Processing \\  Helpfulness & 0.465 & 0.706 & 0.791 \\ Correctness & 0.472 & 0.715 & 0.793 \\ Coherence & 0.169 & 0.387 & 0.428 \\ Complexity & 0.293 & 0.416 & 0.427 \\ Verbosity & 0.342 & 0.536 & 0.548 \\   

Table 1: Inter-annotator Agreement (quadratic weighted Cohen’s \(\)) for HelpSteer2 attributes.

weighted version of Cohen's \(\) because HelpSteer2 attributes are ordinal scores, meaning that disagreements between 0 and 4 should be penalized much more heavily compared to between 0 and 1. Initial annotations tend to have low inter-annotator agreement (e.g. Cohen's \(=0.465\) for helpfulness) as seen in Table 1. Throughout the annotation process, we made several improvements with our vendor, clarifying how our guidelines apply to various edge cases (e.g., whether coherence should consider previous turns, and how helpfulness should be evaluated if prompt instructions are unclear). We used up to five annotators per sample but only retained annotations from the three most in agreement. After the annotations, our vendor performed extensive quality assurance, with each annotation undergoing a minimum of two human reviews in addition to automated checks. Part of the quality assurance process involved removing annotations from annotators who were deemed 'untrusted' or consistently had low agreement with others. These efforts improved inter-annotator agreement for all attributes, with Cohen's \(\) for helpfulness reaching 0.706.

As a final step, we retained only responses for which the differences in helpfulness attribute among annotators were 2 points or below on a Likert-5 scale (for both responses to a common prompt), resulting in the removal of about 10% of the samples. The 2-point threshold was chosen to balance the proportion of retained data and the relative noise in these annotations, recognizing that differences among annotators can also stem from inherent subjectivity or individual preferences rather than misunderstandings of the annotation task. Extensive filtering of annotations was performed by both our vendor and the research team at various stages, with approximately 50% of all annotations ultimately excluded from the dataset. Our final dataset contains 21,362 high-quality annotated samples, consisting of 10,681 prompts each with two annotated responses. The dataset is divided into a training subset (95% of the data) and a validation subset (5% of the data).

### Dataset Analysis

As shown in Table 2, model responses in HelpSteer2 are more helpful, correct, coherent, verbose, and complex due to stronger models used for response generation. The most substantial change is on the coherence attribute, reaching 3.63 out of a full score of 4 on a Likert-5 scale, meaning that generating coherent responses is no longer a challenge for the stronger models. In addition, the verbosity attribute also increased by almost 0.5 from 1.53 to 2.00, meaning that responses changed from being terse to having a good spread of concise and verbose responses. The increase in average response length by 3x from 497.3 to 1492.6 characters also supports this observation.

On the other hand, although HelpSteer2 contains multi-turn prompts with a mean of 2.83 turns compared to only single-turn prompts in HelpSteer, the average character length of prompts in HelpSteer2 is 712 characters, a fraction of the 2491 characters in HelpSteer. This difference is likely because HelpSteer2 prompts are more conversational and succinct, primarily based on ShareGPT, whereas HelpSteer prompts are exclusively based on enterprise use cases involving context documents such as summarization, closed question answering, and extraction.

In Table 2, we observe that coherence is a much weaker predictor of helpfulness in HelpSteer2 (Pearson's R=0.4979) compared to HelpSteer (Pearson's R=0.6348). This is likely due to the distribution of coherence scores, as most responses in HelpSteer2 are coherent given the use of stronger models. Conversely, correctness has become a stronger predictor of helpfulness in HelpSteer2

   _Attribute_ &  &  &  \\  & HS & HS2 & HS & HS2 & HS & HS2 \\  Helpfulness & 2.7856 & 2.8655 & 0.9793 & 1.2703 & 1 & 1 \\ Correctness & 2.8369 & 2.9644 & 0.9935 & 1.2689 & 0.8525 & 0.9430 \\ Coherence & 3.2991 & 3.6393 & 0.7699 & 0.6491 & 0.6348 & 0.4979 \\ Complexity & 1.4423 & 1.7048 & 0.8205 & 0.6986 & 0.2361 & 0.1805 \\ Verbosity & 1.5331 & 1.9999 & 0.9287 & 0.7571 & 0.2555 & 0.0600 \\ No. of turns in prompt & 1 & 2.8348 & 0 & 3.8221 & - & -0.0520 \\ No. of chars in prompt & 2491.8 & 712.6 & 1701.7 & 877.8 & 0.0337 & -0.0774 \\ No. of chars in response & 497.3 & 1492.6 & 426.7 & 1065.7 & 0.1951 & 0.0845 \\   

Table 2: Descriptive statistics for attributes in HelpSteer (HS) and HelpSteer2 (HS2). Please refer to  for comparison with Open Assistant and HH-RLHF. Scores for each attribute are between 0 and 4 on a Likert-5 scale.

(Pearson's R=0.9430) than in HelpSteer (Pearson's R=0.8525). This likely occurs because, with all responses being highly coherent, factuality becomes a more critical factor in determining overall helpfulness. Additionally, the Pearson's R values for both complexity (0.2361 to 0.1805) and verbosity (0.2555 to 0.0600) have decreased, indicating that annotators are less influenced by the complexity and verbosity of responses when assessing overall helpfulness in HelpSteer2. This is beneficial for reward model training, as models can learn that generating complex and verbose responses does not substantially contribute to being helpful.

Helpfulness is also slightly negatively correlated with prompt character length (Pearson's R=-0.0774) and prompt turns (Pearson's R=-0.0520). This suggests that models used for response generation are likely to perform worse in generating follow-up responses compared to initial responses, a trend observed in many models in MT Bench . Finally, response length is slightly positively correlated with helpfulness (Pearson's R=0.0845), consistent with the correlation between verbosity and helpfulness (Pearson's R=0.0600).

## 3 Reward Model

**Training** We follow the training approach of SteerLM Regression Reward Models , the format for which HelpSteer2 data was collected in. SteerLM Regression Reward Modeling aims to predict the values of various attributes as annotated by humans (e.g. helpfulness=2; correctness=3). There are alternative reward modeling methods such as Bradley-Terry [1; 2; 27], but exploring such methods are left to future work as HelpSteer2 data was _specifically collected_ for SteerLM Regression Reward Modeling.

We train reward models consisting of a base model and a linear layer that converts the final layer representation of the end-of-response token into five scalar values, each corresponding to a HelpSteer2 attribute. The reward models are trained on top of two open-source base models: Llama 3 70B and Nemotron-4 340B1 (described in Sec. 2.1). For each model, we train for two epochs using HelpSteer2 data, with a global batch size of 128. We select the top checkpoints with the lowest validation loss for evaluation. We train with a MSE loss function, a constant learning rate on each model (70B: 2e-6, 340B: 7e-7) using an AdamW optimizer  and 10 warmup steps, following a LR search (70B: {1,2,3,4,5}e-6; 340B: {1,3,5,7,9}e-7). For comparison, we also trained a Llama 3 70B base model separately using 1 epoch of HH-RLHF ; 1 epoch of Open Assistant  or 2 epochs of HelpSteer  (to approximately match for difference in dataset size) using the same hyper-parameters.

EvaluationFollowing [33; 34], we evaluate the trained reward models using Reward Bench  excluding the optional Prior Sets category which we report separately (with detailed reasons in Appendix H). Reward Bench comprises 2985 diverse tasks, each consisting of a prompt, a chosen response, and a rejected response. Task accuracy is calculated based on whether the chosen response receives a higher reward than the rejected response. The tasks in Reward Bench are categorized into four main categories: Chat, Chat-Hard, Safety, and Reasoning. Overall accuracy is determined by taking the mean of each category. Details for evaluation are in Appendix H. We choose to use RewardBench due to its diversity of tasks (4 categories and 23 sub-categories), which minimizes the likelihood of overfitting. With over 80 models on the leaderboard  available for comparison, it serves as a well-trusted benchmark.

ResultsOverall, reward models trained with HelpSteer2 perform well on Reward Bench, achieving state-of-the-art numbers compared to proprietary models and those trained with data allowing permissive use. This is particularly noteworthy given that HelpSteer2 consists of only 10k response pairs. Llama 3 70B trained on HelpSteer2 (88.8% Overall) 2 outperforms all other models trained with data allowing permissive use by >9.7%, including the same Llama 3 70B base model trained with Open Assistant, HH-RLHF or HelpSteer. Scaling up the base model to Nemotron-4 340B with the same dataset results in the trained reward model topping the Reward Bench primary leaderboard with an overall performance of 92.0% 3. This suggests that as more capable base models emerge, training them with HelpSteer2 can lead to more powerful reward models. We measure RewardBench using the weighted sum of five attribute values with the weights [0.3, 0.74, 0.46, 0.47, -0.33]for 340B model and [0.65, 0.8, 0.45, 0.55, -0.4] for 70B model, based on a search for optimal weights at 0.01 granularity. For the 340B model, searching at 0.1 granularity reduces overall Rewardbench by 0.1% while 1 granularity reduces it by 0.2% and using the helpfulness attribute only reduces it by 1.0%.

Relative to other models, those trained with HelpSteer2 perform exceedingly well in the Chat-Hard category, surpassing the second-best by 6.5%. This is because HelpSteer2 is primarily aligned with the task of distinguishing between good and excellent responses. Chat-Hard is likely the most relevant metric for preference learning with capable domain-general LLMs since we typically start with a good model and aim to improve its responses further. Unexpectedly, models trained with HelpSteer2 also show good performance in the Safety and Reasoning categories, even though HelpSteer2 does not explicitly focus on these aspects. This may be due to an implicit association between helpful responses and general safety, and transfer learning between being factually correct and reasoning tasks. However, HelpSteer2 trained models do not surpass the Reasoning performance of the strongest alternative models, which are trained on specific reasoning datasets, such as UltraInteract . Finally, HelpSteer2 trained models substantially under-perform many other models on Prior Sets, likely because those other models were trained on the training subsets of these Prior Sets .

## 4 Aligned Models

We demonstrate three approaches for using the Llama 3 70B Reward Model to align LLMs: Iterative Direct Preference Optimization (Iterative DPO), Proximal Policy Optimization (PPO) and SteerLM.

### Evaluation

Following HelpSteer , we use MT Bench  to measure helpfulness, TruthfulQA MC2  to measure correctness, and the mean number of characters in MT Bench responses to measure verbosity. However, instead of the GPT-4-0613 judge used in HelpSteer , we use GPT-4-0125-Preview (Turbo) as a judge because we find that it is a stronger model and better suited as a judge. In addition, we also use AlpacaEval 2.0 Length Controlled  and Arena Hard  as secondary measures of helpfulness, following [33; 39]. MT Bench is also referenced as a validation metric for checkpoint selection. Details for each evaluation metric is available in Appendix H.

### Sft

Supervised Fine-tuning (SFT) trains a model through demonstrations, serving as a prerequisite for preference learning methods such as DPO in Sec. 4.3 and PPO in Sec. 4.4. Following HelpSteer , we train a Llama 3 70B Base model using only Open Assistant  with 56k conversations for 2400 steps with a global batch size of 128 (close to 4 epochs). We use a constant learning rate (LR) of 2e-6 using the AdamW optimizer after searching LR in {1,2,3,4,5}e-6, saving a checkpoint every 200

  &  &  \\  &  &  &  &  &  & \\  &  &  &  &  &  & \\   & **Nemotron-4.340B-Reward** & **92.0** & 95.8 & **87.1** & 91.5 & 93.7 & 67.4 \\  & Cohere May 2024 & 89.5 & 96.4 & 71.3 & **92.7** & 97.7 & **78.2** \\  & Gemini 15 Pts-0514 & 88.1 & 92.3 & 80.6 & 87.5 & 92.0 & - \\  & Other March 2024 & 87.1 & 94.7 & 65.1 & 90.3 & **98.2** & 74.6 \\  & GPT-4.0125-preview & 85.9 & 95.3 & 74.3 & 87.2 & 86.9 & 70.9 \\  & GPT-4.009-review & 85.1 & 95.3 & 75.4 & 87.1 & 82.7 & 73.6 \\  & GPT-4.0513 & 84.7 & **96.6** & 70.4 & 86.7 & 84.9 & 72.6 \\  & Clune-3.098-0292024 & 80.7 & 94.7 & 60.3 & 89.1 & 78.7 & - \\   &  & **90.8** & 96.9 & **76.8** & **92.2** & **97.3** & 74.3 \\  & RLIFH-Llama 38B & 87.1 & **98.3** & 65.8 & 89.7 & 94.7 & **74.6** \\  & Eurs RM Mistral 7B & 82.8 & 98.0 & 65.6 & 81.2 & 86.3 & 71.7 \\  & Starling RM M3 4B & 82.7 & 96.9 & 57.2 & 88.2 & 88.5 & 71.4 \\  & Preprocess 23 Natural S-7B & 75.3 & 93.0 & 47.1 & 83.5 & 77.4 & - \\   & **Llama 3 70B Rank Mistral (w. HelpSteer2)* & **88.8** & 91.3 & **80.3** & **92.8** & **90.7** & 66.5 \\  & Llama 3 70B (w. Open Assistant)* & 79.1 & 91.3 & 59.2 & 76.0 & 89.9 & 66.7 \\  & Llama 3 70B (w. Ht-RLHF)* & 76.0 & **97.6** & 58.9 & 69.2 & 78.5 & **70.4** \\  & Llama 3 70B (w. Ht-RLHF)* & 73.9 & 94.4 & 54.6 & 81.2 & 65.6 & 68.8 \\  & Pythia 1.4B (w. Open Assistant) & 70.0 & 88.5 & 48.7 & 65.3 & 77.5 & 65.3 \\  & Llama 3 70B (w. Ht-BelSteer)* & 66.1 & 93.3 & 59.7 & 56.8 & 54.9 & 67.7 \\  

Table 3: Performance of Models on Reward Bench. Higher is better for each category. All numbers except models trained by us (which are marked with *) are taken from Reward Bench leaderboard.

steps. This represents the SFT model trained on existing open-sourced data only. However, we find that a SFT model trained with only Open Assistant is weak compared to the Llama 3 70B Instruct, likely due to the inconsistent quality of the responses it contains.

Therefore, we trained another model using an SFT dataset (named 'Daring Anteater')4 consisting of 100k conversations, each averaging 2.88 model turns. Approximately 93% of the data are synthetically generated following a similar pipeline as  by replacing OpenAI models with an earlier aligned version of Nemotron-4 340B5 and Mixtral-8x7B-Instruct-v0.1 , while the rest comes from ARB , SciBench , tigerbot-leetcode , PRM800K , FinQA , and wikitablequestions . We trained this model using identical hyper-parameters except training it for 1600 steps with a global batch size of 384 (close to 2 epochs), given the larger size of the dataset. All models on DPO and PPO are trained starting from this model.

### Dpo

Direct Preference Optimization  is a simple and stable approach to performing preference-based model alignment without requiring the complex setup of Reinforcement Learning from Human Feedback (RLHF). We first performed DPO training on the SFT model from Sec. 4.2. To do this training, we converted our HelpSteer2 train set into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the rejected response. In cases where the helpfulness scores were identical, we discarded that pair entirely. This became our HelpSteer2 DPO dataset, which contains 7,221 training samples. We then performed DPO training on this data for 7 epochs using a constant LR of 2e-7, Kullback-Leibler (KL) penalty of 1e-3, AdamW optimizer, Global Batch Size 128, and Weight Decay 0.1. Optimal LR was identified following a search among {3e-7, 2e-7, 1e-7, 9e-8} and KL penalty following a search among {1e-3, 4e-4}. We evaluated checkpoints once every 25 steps.

We then performed Iterative DPO  on this model by utilizing 20k prompts from the Daring Anteater SFT dataset and generating 10 responses per prompt (temperature=0.7, top-p=0.9). These responses were then scored by the Llama 3 70B Reward Model (Sec. 3) and a pairwise preference dataset generated by taking the highest and lowest goodness score for the chosen and rejected, respectively. The goodness score is a scalar based on 0.65*helpfulness + 0.8*correctness + 0.45*coherence, which we find to give best differentiation between chosen and rejected responses in RewardBench prompts. We then performed DPO training on this data for 3 epochs using similar hyper-parameters as above, except KL penalty of 1e-3 and LR of 9e-8, following similar hyper-parameter search.

### Ppo

Proximal Policy Optimization (PPO)  is a complex but effective approach to performing Reinforcement Learning from Human Feedback, which contributed to the success of many frontier models include GPT-4 . Starting with the SFT model we trained in Sec. 4.2, we apply PPO similar to the recipe described in . The rewards are provided by the Llama 3 70B Reward Model (Sec. 3) and we sample the policy with HelpSteer2 prompts. In order to regularize the optimization process, we add a KL divergence penalty in the rewards to prevent the policy from deviating too far away from the SFT model. The PPO value model is initialized from the reward model.

The reward was calculated using goodness score (Sec. 4.3), followed by taking away the mean of the HelpSteer2 responses and dividing it by its standard deviation. We trained PPO using a global batch size of 128, a rollout buffer of 128 and a constant LR of 1e-7 and KL-penalty of 3e-3, after searching LR in {1,2,3,4,5}e-7 and the KL-penalty in {1,2,3,4,5}e-3. We train for 64 steps and evaluate a checkpoint every 4 steps. The generation stage of PPO is optimized using NeMo-Aligner's integration of TensorRT-LLM .

### SteerLM

SteerLM [12; 18] aligns language models by steering them towards generating outputs with desired attribute values by conditioning on various attributes during training. We trained the SteerLM model following . Specifically, we used the Llama 3 70B Reward Model to annotate the Daring Anteater SFT dataset (Sec. 4.2), followed by attribute-conditioned supervised fine-tuning of a language model on the annotated dataset to generate responses conditioned on target attribute scores.

However, the original SteerLM method does not explicitly enforce the generated responses to follow the desired attribute distribution conditioned on during training. To address this limitation, we propose SteerLM 2.0, which iteratively trains the model to approximate the optimal SteerLM policy constructed by the reward model. This is achieved using the original SteerLM trained model to generate multiple sampled responses and then using a KL divergence loss between current policy and optimal SteerLM policy to guide the model towards generating a response that is more reflective of the desired attribute values. SteerLM 2.0 can be conducted in iterations (n=2) using the optimized policy after each iteration to sample responses and train an improved policy. In each iteration, we sampled multiple diverse responses (n=10, temperature=0.7, top-p=0.9) from 20,000 different prompts from the Daring Anteater SFT dataset. SteerLM 2.0 is trained for 2 epochs with AdamW optimizer constant LR 1e-7 and global batch size 128.

Method DetailsSteerLM 2.0 trains a model \(Q_{}(y|a,x)\) that can generate responses \(y\) conditioned on a prompt \(x\) and desired attributes \(a\), while approximating the optimal conditional distribution \(P(y|a,x)\) derived from the optimal reward model \(P(a|x,y)\). \(P(a|x,y)\) is the attribute prediction model that can be trained on labeled data. To convert the regression reward model into a probabilistic reward model, we use the Beta distribution function to estimate the probability of different reward output levels. We scale the HelpSteer reward model output \(r\) to \(\) and compute the Beta distribution parameters by setting \(=24r\) and \(=24-\). We choose \(+=24\) as it matches the ground truth distribution of the training data. The probability \(P(a=n)\) is calculated as \(P_{,}(X_{i+1})-P_{,}(X_{i})\), where \(P_{,}\) is the cumulative Beta probability distribution function, and \(X_{i+1}\) and \(X_{i}\) are the normalized bin boundaries of the value \(n\). Note, the beta distribution approximation is not an integral part of the SteerLM 2.0 method itself, but rather a tool to convert our regression-based reward model into a probabilistic one. Investigations with 'true' probabilistic reward models is left as future work, given that the main goal of this paper is to demonstrate how the Reward Model trained with HelpSteer2 can be used for model alignment.

We first derive the optimal conditional distribution \(P(y|a,x)\) using Bayes' rule:

\[P(y|a,x)= P(a|x,y)P(y|x)\]

Here, \(P(y|x)\) is the unconditional response distribution from a separate language model (supervised fine-tuning model using Daring Anteater SFT dataset, see Sec. 4.2). The optimal \(P(y|a,x)\) can be constructed by combining \(P(y|x)\) and \(P(a|x,y)\).

To efficiently approximate \(P(y|a,x)\), we train a parametric model \(Q_{}(y|a,x)\) by minimizing the KL divergence:

\[_{}_{a,x}D_{KL}(P(y|a,x)||Q_{}(y|a,x))\]

This KL divergence loss can be written as:

\[-_{a,x,y P(a)P(x)P(y|a,x)} Q_{}(y|a,x)\]

To optimize the loss, we estimate its gradient using samples from the original SteerLM model \(y_{i} Q^{}(y|a,x)\):

\[_{}L=-_{i}(w^{}_{i}-b^{}_{i})_{} Q _{}(y_{i}|a,x)\]

where the \(Q_{}\) is initialized with original SteerLM model \(Q^{}\) during training.

Where \(w^{}_{i}\) and \(b^{}_{i}\) are normalized importance weights. This gradient estimator has reduced variance compared to the naive approach . The resulting SteerLM 2.0 model \(Q_{}(y|a,x)\) can generate responses \(y\) conditioned on attributes \(a\) by approximately following the optimal \(P(y|a,x)\) distribution. Further details and derivation of SteerLM 2.0 can be found in Appendix I.

Inference AttributesIn this paper, we focus on to calibrate the model to generate good responses, so we choose to focus on one set of desired attributes for response sampling. Because HelpSteer2 responses are much (around 3x) longer and more complex than in HelpSteer , we found that using Complexity 2 and Verbosity 2 as default leads to more better generations than setting them both to 4, as done in HelpSteer . The other three attributes (Helpfulness, Correctness and Coherence are set to 4, as in HelpSteer .

### Results

**Overall** Across all metrics, at least one model trained using the Llama 3 70B Reward Model matches (_i.e._ within standard error) or exceeds the performance of Llama 3 70B Instruct, a model which has been trained with 10 million samples across SFT and preference-based training . Compared to the undisclosed, data-hungry alignment recipe of Llama 3 70B Instruct, our alignment recipe is transparent and substantially more data efficient, requiring only 10 thousand HelpSteer2 preference pairs and 100 thousand SFT samples. This represents only 1% of the amount of data using for training Llama 3 70B Instruct. In addition, our models exceed the performance of GPT-4-0613 across all metrics, a notable yardstick representing frontier models from a year ago.

**DPO** model is most outstanding in terms of TruthfulQA  and Arena Hard. We find that most of its performance comes from DPO using the HelpSteer2 dataset, while Iterative DPO gives a further boost. The benefit of using HelpSteer2 for DPO comes from the selection of chosen and rejected pairs based on the helpfulness of the responses. Because Helpfulness has a Pearson correlation of 0.943 with Correctness in HelpSteer2 (Table 2), DPO with HelpSteer2 helps the model to differentiate between right and wrong answers. This is useful for improving TruthfulQA MC2, which focuses on choosing among correct and incorrect options. Similarly, Arena Hard contains mostly (>50%) knowledge-intensive coding problems that require the model to accurately answer.

**PPO** model performs the best in terms of AlpacaEval 2.0 LC. This is likely because AlpacaEval 2.0 mostly contains simple prompts containing only a single requirement (e.g. _"How do I wrap a present neatly?"_ and _"What are the best exercises for beginners?"_). Therefore, they are typically less about whether models can answer them accurately (since most models can) as whether it can answer with sufficient levels of details without being too verbose (which is penalized by the Length-Control aspect in AlpacaEval 2.0). Therefore, PPO can minimally improve the style of the response (vs. the SFT model). However, similar to , we observe a severe degradation in TruthfulQA with PPO. We suspect this is due to the low representation of Multiple-Choice-Questions (MCQ) in the HelpSteer2 prompts, leading the policy to drift off in a direction that reduces MCQ performance.

**SteerLM** model performs optimally on MT-Bench. MT Bench represents complex instructions containing several requirements as well as follow up questions (e.g. _"Craft an intriguing opening paragraph for a fictional short story. The story should involve a character who wakes up one morning to find that they can time travel."_ followed by _"Summarize the story with three bullet points using

   _Technique_ & _Model_ &  MT Bench \\ (GPT-4-Turbo) \\  &  Mean Response \\ Length (Chars.) \\  &  TruthfulQA \\ MC2 \\  &  AlpacaEval \\ 2.0 LC (SE) \\  & 
 Arena Hard \\ (95\% CI) \\  \\   & GPT-4-0613* & 8.12 & 1057.1 & 0.5900 & 30.20 (1.07) & 37.9 (-2.8, 2.4) \\  & Llama 3 70B Instruct* & 8.16 & 1683.0 & 0.6181 & **34.40** (1.38) & 41.1 (-2.0, 2.2) \\   & SFT w. DA & 7.96 & 1514.4 & 0.6025 & 32.87 (1.40) & 39.6 (-2.3, 2.4) \\  _DPO_ & DPO w. HelpSteer2 & 8.04 & 1532.1 & 0.6321 & 30.70 (1.36) & 41.8 (-2.3, 2.3) \\  & Iterative DPO w. DA & 8.09 & 1492.0 & **0.6328** & 29.17 (1.35) & **42.5** (-2.1, 2.4) \\  _PPO_ & PPO w. HelpSteer2 & 8.13 & 1497.3 & 0.5629 & 33.17 (1.38) & 39.9 (-2.4, 2.0) \\   & SteerLM w. DA & 8.17 & 1444.1 & 0.5919 & 31.10 (1.37) & 39.3 (-2.6, 2.4) \\  & SteerLM 2 Iler. 1 w. DA & 8.24 & 1523.0 & 0.5911 & 31.10 (1.35) & 38.8 (-2.3, 2.7) \\  & SteerLM 2 Iler. 2 w. DA & **8.28** & 1471.9 & 0.5913 & 29.93 (1.35) & 39.1 (-2.2, 2.4) \\   & SFT w. Open Assistant & 6.75 & 676.0 & 0.5137 & 13.94 (0.82) & 9.8 (-1.1, 1.4) \\  & SteerLM w. Open Assistant & 7.44 & 1001.3 & 0.5713 & 20.87 (1.10) & 19.2 (-2.0, 1.7) \\   

Table 4: Evaluation of Aligned Models. Higher is better for each metric, except Mean Response Length. Because we use the Llama 3 70B Base model  for all aligned model experiments, we use Llama 3 70B Instruct model as a baseline, together with GPT-4-0613. Models trained “w. DA” use the Daring Anteater dataset. Metrics for models marked with * are taken from external leaderboards . **Bold** is the top model and underlined is the next best.

only nouns and adjectives, without verbs."_). SteerLM does well likely because given that the model is trained using one prompt paired with ten sampled responses that are mostly similar with each other but have some minor differences that affect their reward as scored by the Llama 3 70B Reward Model. SteerLM training seeks to improve the likelihood of the best responses while averting mistakes made by other responses. This is useful for MT Bench since each prompt contains many different requirements, which requires a fine-level, multi-to-one contrastive learning beyond imitation learning (SFT), contrastive learning between chosen/rejected (DPO) and single sample rollout (PPO). While improved datasets contribute to overall performance, SteerLM 2.0's distinct approach allows it to leverage this data more effectively for complex, multi-faceted language tasks.

**Ablation** A large proportion of our model's performance comes from the Daring Anteater SFT dataset. If we do only SFT with Open Assistant, following HelpSteer paper , MT Bench substantially drops from 7.96 to 6.75, as do other metrics. Nonetheless, even if only Open Assistant is used, using the Reward Model can massively boost the performance (MT Bench from 6.75 to 7.44), and surprisingly by a larger margin than when using Daring Anteater (MT Bench from 7.96 to 8.28). This is likely because Daring Anteater responses are mostly of high quality as they are mostly generated by a strong LLM (Nemotron-4 340B) whereas Open Assistant is crowd-sourced with a wide variety of quality in responses. This suggests our Reward Model can improve final model performance, regardless of initial performance.

## 5 Related Work

Domain-general Human-annotated Preference DatasetsThe Open Assistant dataset is a notable domain-general chat resource consisting of >160, 000 messages in 35 languages, providing over 10,000 fully annotated conversation trees, developed through global crowdsourcing efforts . Similarly, the HH-RLHF (Helpfulness and Harmlessness) dataset by Anthropic includes >160,000 human preference comparisons, facilitating the training of models to be both helpful and harmless . Similarly, Helpsteer dataset  contains >37,000 prompt-response pairs each annotated with Likert-5 scores for helpfulness, correctness, coherence, complexity and verbosity. This work directly extends HelpSteer .

Domain-specific Human-annotated Preference DatasetsThere are also other domain-specific datasets covering specific tasks such as long-form question answering, summarization, online forum responses, but they are less useful for building a domain-general LLM. These datasets include the OpenAI WebGPT  and Summarize  datasets, the Stanford Human Preferences Dataset (SHP) , all contributing diverse human preference data to advance LLM training.

Domain-general AI-Generated/Synthetic Preference DatasetsUsing synthetic data as an alternative leads to a lower-cost technique utilizing AI Feedback (most typically from OpenAI GPT-4) for preference data. RL from AI Feedback (RLAIF) uses LLMs to label response or preference-rank various responses instead of relying on human annotators [15; 17; 16; 59]. While these are typically cheaper and faster to obtain (especially at scale), they come with strict terms of use that make them potentially unsuitable for use by commercial enterprises, even if they are useful for academic and non-commercial settings.

## 6 Conclusion

We present HelpSteer2 - a permissively-licensed (CC-BY-4.0), small (10k pairs) and high quality (Cohen's \(\) of 0.791) helpfulness dataset that can be used to efficiently train Nemotron-4-340B-Reward, a top-performing reward model on RewardBench (92.0% on its primary dataset, Rank 1 as of 12 June 2024). We share how we collect this dataset to inspire similar collection efforts as well as how reward models can be trained with this dataset. Finally, a Llama 3 70B reward model trained with HelpSteer2 can be used to align Llama 3 70B Base models to match or exceed the performance of Llama 3 70B Instruct and GPT-4-0613 on major alignment metrics (MT Bench, TruthfulQA, AlpacaEval 2.0 LC and Arena Hard).