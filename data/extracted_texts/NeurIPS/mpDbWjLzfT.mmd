# Contrast: Continual Multi-source Adaptation to Dynamic Distributions

Sk Miraj Ahmed\({}^{2,}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

Fahim Faisal Niloy\({}^{1,}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

Xiangyu Chang\({}^{1}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

Dripta S. Raychaudhuri\({}^{3,}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

Samet Oymak\({}^{4}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

Amit K. Roy-Chowdhury\({}^{1}\)

Equal contribution; Co-first authors listed alphabetically by last name.Currently at Brookhaven National Laboratory. Work done while the author was at UCR.Currently at AWS AI Labs. Work done while the author was at UCR.

###### Abstract

Adapting to dynamic data distributions is a practical yet challenging task. One effective strategy is to use a model ensemble, which leverages the diverse expertise of different models to transfer knowledge to evolving data distributions. However, this approach faces difficulties when the dynamic test distribution is available only in small batches and without access to the original source data. To address the challenge of adapting to dynamic distributions in such practical settings, we propose CONtinual mulTi-souRce Adaptation to dynamic diStribTions (CONTRAST), a novel method that optimally combines multiple source models to adapt to the dynamic test data. CONTRAST has two distinguishing features. First, it efficiently computes the optimal _combination_ weights to combine the source models to adapt to the test data distribution continuously as a function of time. Second, it identifies which of the source model parameters to update so that only the model which is most correlated to the target data is adapted, leaving the less correlated ones untouched; this mitigates the issue of "forgetting" the source model parameters by focusing only on the source model that exhibits the strongest correlation with the test batch distribution. Through theoretical analysis we show that the proposed method is able to optimally combine the source models and prioritize updates to the model least prone to forgetting. Experimental analysis on diverse datasets demonstrates that the combination of multiple source models does at least as well as the best source (with hindsight knowledge), and performance does not degrade as the test data distribution changes over time (robust to forgetting).

## 1 Introduction

Deep neural networks have shown impressive performance on test inputs that closely resemble the training distribution. However, their performance degrades significantly when they encounter test inputs from a different data distribution. Unsupervised domain adaptation (UDA) techniques [1; 2] aim to mitigate this performance drop. Addressing the distribution shift in case of _dynamic data distributions_ is even more challenging and practically relevant - in many real-world applications like autonomous navigation, models often encounter dynamically evolving distributions. Furthermore, test data is often accessed in streaming batches rather than all at once, and source data may not always be available due to privacy and storage concerns.

For domain adaptation to dynamically evolving environments, employing a model ensemble can be beneficial, as it allows leveraging the learned knowledge of different models to more effectivelymitigate dynamic distribution shifts. Additionally, situations may arise wherein the user has access to a diverse set of pre-trained models across distinct source domains, and no access to source domain data corresponding to each model due to privacy, storage or other constraints. Consequently, training a unified model using the combined source data becomes unfeasible. In those scenarios, it is both reasonable and effective to employ and adapt the entire available array of source models during testing, thereby enhancing performance beyond the scope of single source model adaptation. Moreover, employing a model ensemble provides the flexibility to effortlessly incorporate or exclude models post-deployment, aligning with the user's preferences and the needs of the given task. This flexibility is not achievable with a single domain-generalized model trained on combined source data.

As an example, consider a scenario where a recognition model, initially trained on clear weather conditions, faces data from mixed weather scenarios, like sunshine interspersed with rain (see Figure 1). In such cases, employing multiple models - specifically those trained on clear weather and rain -- with appropriate weighting can potentially reduce the test error as opposed to relying on a single source model. In this context, the models for clear weather and rain would be assigned higher weights, while models for other weather conditions would receive relatively lesser weightage.

The main challenge of developing such a model ensembling method is to _learn appropriate combination weights to optimally combine the source model ensemble during the test phase as data is streaming in, such that it results in a test error equal or lower than that of the best source model_. To solve this, we propose CONItual multi-soRec Adaptation to dynamic diStribTions (CONTRAST) that handles multiple source models and optimally combines them to adapt to the test data.

The efficacy of using multiple source models also extends to preventing _catastrophic forgetting_ that may arise when adapting to dynamic distributions for a prolonged time. Consider again the scenario of multiple source models, each trained on a different weather condition. During inference, only the parameters of the models most closely related to the weather encountered during test time will get updates, and the unrelated ones will be left untouched. This ensures that the model parameters do not drift too far from the initial state, since only those related to the test data are being updated. This mechanism mitigates forgetting when the test data distribution varies over a long time scale, as is likely to happen in most realistic conditions. Even if an entirely unrelated distribution appears during testing and there is no one source model to handle it, the presence of multiple sources can significantly reduce the rate at which the forgetting occurs. This is again because only the most closely related models (clear and rainy weather in the example above) are updated, while others (e.g., snow) are left untouched. Our setting is closely related to Test Time Adaptation methods (TTA) , and ours is the first to address _adaptation of multiple sources for dynamically shifting distributions_ during test time.

**Main Contributions.** Our proposed approach, CONTRAST, makes the following contributions.

Figure 1: **Problem setup. Consider several source models trained using data from different weather conditions. During the deployment of these models, they may encounter varying weather conditions that could be a combination of multiple conditions in varying proportions (represented by the pie charts on top). Our goal is to infer on the test data using the ensemble of models by automatically figuring out proper combination weights and adapting the appropriate models on the fly.**

* We propose a framework for multi-source adaptation to dynamic distribution shifts from streaming test data and without access to the source data. Our approach has the ability to merge the source models using appropriate combination weights during test time, enabling it to perform just as well as the best-performing source or even surpass it.
* Our framework achieves performance on par with the best-performing source and also effectively mitigates catastrophic forgetting when faced with long-term, fluctuating test distributions.
* We provide theoretical insights on CONTRAST, illustrating how it addresses domain shift by optimally combining source models and prioritizing updates to the model least prone to forgetting.
* To demonstrate the real-world advantages of our methodology, we perform experiments on a diverse range of benchmark datasets.

## 2 Related Works

**Unsupervised Domain Adaptation.** UDA methods have been applied to many machine learning tasks, including image classification , semantic segmentation , object detection  and reinforcement learning , in an effort to address the data distribution shift. Most approaches try to align the source and target data distributions, using techniques such as maximum mean discrepancy , adversarial learning [7; 1; 5] and image translation [8; 9]. Recently, there has been a growing interest in adaptation using only a pre-trained source model due to privacy and memory storage concerns related to the source data [10; 11; 12; 13; 14; 15; 16]. These approaches include techniques such as information maximization [17; 18; 19], pseudo labeling [20; 21], and self-supervision .

**Multi-Source Domain Adaptation (MSDA).** Both UDA and source-free UDA have been extended to multi-source setting by incorporating knowledge from multiple source models [18; 23]. Notable techniques include discrepancy-based MSDA , higher-order moments , adversarial methods , and Wasserstein distance-based methods . However, these methods are specifically tailored to UDA scenarios, where the whole target data is assumed to be available during adaptation. Whereas, in our setting we consider access to a batch of target data at an instance. Another related field is Domain Generalization (DG) [28; 29], which refers to training a single model on a combined set of data from different source domains. Hence, DG requires data from all distinct domains to be available altogether during training, which may not be always feasible. Additionally, Model Soups  is a popular approach to ensemble models fine-tuned on same data distribution, where the weights of multiple models are averaged for inference. On the other hand, we use a weighting approach for model predictions, where models are pre-trained on different source data distributions. In our problem, inspired by MSDA, _users are only provided with pre-trained source models_.

**Adaptation to Dynamic Data.** Few works [31; 32; 33] have addressed the adaptation to dynamic data distributions. However, these works either require source data or the entire target domain data to be available during adaptation. When additional constraints such as streaming target data batches and no access to source data are considered, the setting closely aligns with Test Time Adaptation (TTA). While UDA methods typically require a substantial volume of target domain data for model adaptation, which is performed offline and prior to deployment, TTA adjusts a model post-deployment, during inference or testing. One of the early works  use test-batch statistics for batch normalization adaptation. Tent  updates a pre-trained source model by minimizing entropy and updating batch-norm parameters. DUA  updates batch-norm stats with incoming test batches. TTA methods have also been applied to segmentation problems [36; 37; 38; 39]. When these TTA methods are used to adapt to changing target distribution, they usually suffer from 'forgetting' and 'error accumulation' . In order to solve this, CoTTA  restores source knowledge stochastically to avoid drifting of source knowledge. EATA  adds a regularization loss to preserve important weights for less forgetting. While motivated by TTA, our method considers multi-source adaptation in a dynamic setting and

    & **Source** & **Adaptation** & **Dynamic** & **Multi** \\  & **Free** & **On the Fly** & **Target** & **Source** \\  UDA  & ✗ & ✗ & ✗ & ✓ \\ Source-free UDA  & ✓ & ✗ & ✗ & ✓ \\ TTA  & ✓ & ✓ & ✓ & ✗ \\ CONTRAST & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Comparison of our setting to the existing adaptation settings**. Our proposed setting meets all the criteria that are expected in a comprehensive adaptation framework.

has an inherent capability to mitigate forgetting. In Table 1, we illustrate a comparison between our setting and existing settings.

## 3 CONTRAST Framework

### Problem Setting

In this problem setting, we propose to combine multiple pre-trained models during test time through the application of suitable combination weights, determined based on a limited number of test samples. Specifically, we will focus on the classification task that involves \(K\) categories. Consider the scenario where we have a collection of \(N\) source models, denoted as \(\{_{S}^{j}\}_{j=1}^{N}\), that we aim to deploy during test time. In this situation, we assume that a sequence of test data \(\{x_{i}^{(1)}\}_{i=1}^{B}\{x_{i}^{(2)}\}_{i=1}^{B} \{x_{i}^{(t)}\}_{i=1}^{B}\) are coming batch by batch in an online fashion, where \(t\) is the index of time-stamp and \(B\) is the number of samples in the test batch. We also denote the test distribution at time-stamp \(t\) as \(_{T}^{(t)}\), which implies \(\{x_{i}^{(t)}\}_{i=1}^{B}_{T}^{(t)}\). Motivated by , we model the test distribution in each time-stamp \(t\) as a linear combination of source distributions where the combination weights are denoted by \(\{_{j}^{(t)}\}_{j=1}^{N}\). Thus, our inference model on test batch \(t\) can be written as \(_{T}^{(t)}=_{j=1}^{N}_{j}^{(t)}_{S}^{j(t)}\) where \(_{S}^{j(t)}\) is the adapted \(j\)-th source in time stamp \(t\). Based on this setup our objective is twofold:

1. We want to determine the optimal combination weights \(\{_{j}^{(t)}\}_{j=1}^{N}\) for the current test batch such that the test error for the optimal inference model is lesser than or equal to the test error of best source model. Mathematically we can write this as follows: \[_{test}^{(t)}(_{T}^{(t)})}_{test}^{(t)}(_{S}^{j}),\] (1) where \(_{test}^{(t)}()\) evaluates the test error on \(t\)-th batch.
2. We also aim for the model to maintain consistent performance on source domains, as it progressively adapts to the changing test conditions. This is necessary to ensure that the model has not catastrophically forgotten the original training distribution of the source domain and maintains its original performance if the source data is re-encountered in the future We would ideally want to have: \[_{src}(_{S}^{j(t)})_{src}(_{S}^{j})  j,t,\] (2) where, \(_{src}(_{S}^{j})\) denote the test error of \(j\)-th source on its corresponding test data when using the original source model \(_{S}^{j}\), whereas \(_{src}(_{S}^{j(t)})\) represents the test error on the same test data using the \(j\)-th source model adapted up to time step \(t\), denoted as \(_{S}^{j(t)}\).

### Overall Framework

Our framework undertakes two operations on each test batch. First, we learn the combination weights for the current batch at time step \(t\) by freezing the model parameters. Then, we update the model corresponding to the largest weight with existing state-of-the-art TTA methods, which allows us to fine-tune the model and improve its performance. This implies that the model parameters of source \(j\) might get updated up to \(p\) times at time-step \(t\), where \(0 p(t-1)\).

In other words, the states of the source models evolve over time depending on the characteristics of the test batches up to the previous time step. To formalize this concept, we define the state of the source model \(j\) at time-step \(t\) as \(_{S}^{j(t)}\). In the next section, we will provide a detailed explanation of both aspects of our framework: (i) learning the combination weights, and (ii) updating the model parameters. By doing so, we aim to provide a comprehensive understanding of how our approach works in practice.

### Learning the combination weights

For an unlabeled target sample \(x_{i}^{(t)}\) that arrives at time-stamp \(t\), we denote its pseudo-label, as predicted by source \(j\), as \(_{ij}^{(t)}=_{S}^{j(t)}(x_{i}^{(t)})\), where \(_{S}^{j(t)}\) is the state of source \(j\) at time-stamp \(t\). Nowwe linearly combine these pseudo-labels by source combination weights \(=[_{1}\,_{2}_{N}]^{}^ {N}\) to get weighted pseudo-label \(_{i}^{(t)}=_{j=1}^{N}_{j}_{ij}^{(t)}\). Using these weighted pseudo-labels for all the samples in the \(t\)-th batch we calculate the expected Shannon entropy as,

\[_{w}^{(t)}()=-_{_{T}^{(t)}}_{c= 1}^{K}_{ic}^{(t)}(_{ic}^{(t)})\] (3)

Based on this loss we solve the following optimization:

\[}{}& _{w}^{(t)}()\\ &_{j} 0, j \{1,2,,N\},\\ &_{j=1}^{n}_{j}=1\] (4)

Suppose we get \(^{(t)}\) to be the optimal combination weight vector by performing the optimization in (4). In such case, the optimal inference model for test batch \(t\) can be expressed as follows:

\[_{T}^{(t)}=_{j=1}^{N}_{j}^{(t)}_{S}^{j(t)}\] (5)

Thus, by learning \(\) in this step, we satisfy Eqn. (1).

**Model parameter update.** After obtaining \(^{(t)}\), next we select the most relevant source model \(k\) given by \(k=\,}\,\,\,_{j}^{ (t)}\). This indicates that the distribution of the current test batch is most correlated with the source model \(k\). We then adapt model \(k\) to the test batch \(t\) using any state-of-the-art single source method that adapts to dynamic target distributions. Specifically, we employ three distinct adaptation approaches: (i) TENT , (ii) CoTTA , and (iii) EuTA .

**Optimization strategy for (4).** Solving the optimization problem in Eq. 4 is a prerequisite for inferring the current test batch. As inference speed is critical for test-time adaptation, it is desirable to learn the weights quickly. To achieve this, we design two strategies: (i) selecting an appropriate initialization for \(\), and (ii) determining an optimal learning rate.

**(i) Initialization:** Pre-trained models contain information about expected batch mean and variance in their Batch Norm (BN) layers based on the data they were trained on. To leverage this information,

Figure 2: **Overall Framework.** During test time, we aim to adapt multiple source models in a manner such that it optimally blends the sources with suitable weights based on the current test distribution. Additionally, we update the parameters of only one model that exhibits the strongest correlation with the test distribution.

we extract these stored values from each source model prior to adaptation. Specifically, we denote the expected batch mean and standard deviation for the \(l\)-th layer of the \(j\)-th source model as \(_{l}^{j}\) and \(_{l}^{j}\), respectively.

During testing on the current batch \(t\), we pass the data through each model and extract its mean and standard deviation from each BN layer. We denote these values as \(_{l}^{T(t)}\) and \(_{l}^{T(t)}\), respectively. One useful metric for evaluating the degree of alignment between the test data and each source is the distance between their respective batch statistics. A smaller distance implies a stronger correlation between the test data and the corresponding source. Assuming that the batch-mean statistic per node of the BN layers to be a univariate Gaussian, we calculate the distance (KL divergence) between the \(j\)-th source (approximated as \((_{l}^{j},(_{l}^{j})^{2})\)) and the \(t\)-th test batch (approximated as \((_{l}^{T(t)},(_{l}^{T(t)})^{2})\)) as follows (derivation in Appendix Section H):

\[_{j}^{t}=_{l}_{KL}[( _{l}^{T(t)},(_{l}^{T(t)})^{2}),(_{l}^{j},( _{l}^{j})^{2})]=\] \[_{l=1}^{n_{j}}_{m=1}^{d_{j}^{t}}(^{j}}{_{lm}^{T(t)}})+^{T(t)})^{2}+ (_{lm}^{j}-_{lm}^{T(t)})^{2}}{2(_{lm}^{j})^ {2}}-\]

where subscript \(lm\) denotes the \(m\)-th node of \(l\)-th layer. After obtaining the distances, we use a softmax function denoted by \(()\) to normalize their negative values. The softmax function is defined as \(_{j}(a)=)}{_{i=1}^{N}(a_{i})}\), where \(a^{N}\), and \(j\). If \(^{t}=[_{1}^{t},_{2}^{t}_{N}^{t}]^{ }^{N}\) is the vectorized form of the distances from all the sources, we set

\[_{init}^{(t)}=(-^{t})\] (6)

where \(_{init}^{(t)}\) is the initialization for \(\). As we shall see, this choice leads to a substantial performance boost compared to random initialization.

**(ii) Optimal step size:** Since we would like to ensure rapid convergence of optimization in Eqn. 4, we select the optimal step size for gradient descent in the initial stage. Given an initialization \(_{init}^{(t)}\) and a step size \(^{(t)}\), we compute the second-order Taylor series approximation of the function \(_{w}^{(t)}\) at the updated point after one gradient step. Next, we determine the best step size \(_{best}^{(t)}\) by minimizing the approximation with respect to \(^{(t)}\). This is essentially an approximate Newton's method (details in Appendix section I) and has a closed-form solution given by

\[_{best}^{(t)}=[(_{}_{w}^{(t)} )^{}(_{}_{w}^{(t)})/( _{}_{w}^{(t)})^{}_{w}( _{}_{w}^{(t)})]_{^{ init}}.\] (7)

Here \(_{}_{w}^{(t)}\) and \(_{w}\) are the gradient and Hessian of \(_{w}^{(t)}\) with respect to \(\). Together with \(_{init}^{(t)}\) and \(_{best}^{(t)}\), optimization of (4) converges very quickly as demonstrated in the experiments (_in Table 6 of Appendix_). Please note that, _we calculate the Hessian for only \(n\) scalar parameters, with \(n\) representing the number of source models. Typically, in common application domains, addressing distribution shifts requires only a small number of source models, making the computational overhead of calculating hessian negligible_.

Please refer to Algorithm 1 for a complete overview of CONTRAST.

### Theoretical insights regarding combination weights

**Theorem 1** (Convergence of Optimization 4.).: _The Optimization 4 converges according to the rule as follows:_

\[_{j=0}^{k}\|_{}_{w}(^{()})\|_{2}^{2}_{w}(w^{(0)})-_{w}(w^{ }))}{_{best}^{(t)}(k+1)}\] (8)

_where, \(_{}\) represents the gradient of the objective function over the set of \(n\)-simplex \(\) and \(j\) represents the iteration number._

Proof.: Please refer to the Appendix (Section A) for the proof.

**Implication of Theorem 1.** The theorem tells us that to make the optimization converge faster with fewer iterations (small \(k\)), it is crucial to start with a good initialization close to the best solution (\(((w^{(0)})-(w^{}))\) should be small). By using Eqn. (6), we ensure this condition for quicker convergence. Also, please note that in Theorem 1, \(j\) denotes the iteration number in the optimization process, and for simplicity, the batch number \(t\) has been intentionally omitted from the notation.

```
0: Pre-trained source models \(\{_{S}^{(t)}\}_{j=1}^{N}\), streaming sequential unlabeled test data \(\{x_{i}^{(1)}\}_{i=1}^{B}\{x_{i}^{(2)}\}_{i=1}^{B} \{x_{i}^{(t)}\}_{i=1}^{B}\) Output: Optimal inference model for \(t\)-th test batch \(_{T}^{(t)}\; t\) Initialization: Assign \(_{S}^{j(1)}_{S}^{j}\; j\) while\(t 1\)do  Set initial \(_{init}^{(t)}\) using Eqn. (6)  Set \(_{best}^{(t)}\) using Eqn. (7)  Solve optimization 4 to get \(^{(t)}\)  Infer the test batch \(t\) using inference model \(_{T}^{(t)}\) using Eqn. (5)  Find source index \(k\) such that \(k=_{1 j N}_{j}^{(t)}\)  Update source model \(_{S}^{k(t)}\) according to Model Parameter Update paragraph of Section 3.3 to get \(_{S}^{k(t)}}\) for\(1 j N\)do if\(j=k\)then  Set \(_{S}^{j(t+1)}_{S}^{j(t)}}\) else  Set \(_{S}^{j(t+1)}_{S}^{j(t)}\)  end if  end while  end while ```

**Algorithm 1**Overview of CONTRAST

### Theoretical insights regarding model update

We now provide theoretical justification on how CONTRAST selects the best source model by optimally trading off model accuracy and domain mismatch. At time \(t\), let \(_{S}^{(t)}\) be the set of source models defined as \([_{S}^{1(t)}\;_{S}^{2(t)}\;\;_{S}^{N(t )}]\). CONTRAST aims to learn a combination of these models by optimizing weights \(w\) on the target domain. For simplicity of exposition, we consider convex combinations \(w\) where \(\) is the \(N\)-dimensional simplex.

To learn \(\), CONTRAST runs empirical risk minimization on the target task using a loss function \(()\) with pseudo-labels generated by \(\)-weighted source models. Let \((f)\) denote the target population/test risk of a model \(f\) (with respect to ground-truth labels) and \(_{T}^{(t)}\) represent the optimal population risk obtained by choosing the best possible \(w\) (i.e. oracle risk). We introduce the functions: **(1)**\(\) which returns the distance between two data distributions and **(2)**\(\) which returns the distance between two label distributions. We note that, rather than problem-agnostic metrics like Wasserstein, our \(,\) definitions are in terms of the loss landscape and source models \(_{S}^{(t)}\), hence tighter. We have the following generalization bound at time step \(t\) (precise details in Appendix Section A).

**Theorem 2**.: _Consider the model \(_{T}^{(t)}\) with combination weights \(^{(t)}\) obtained via CONTRAST by minimizing the empirical risk over \(B\) IID target examples per Eqn. 5. Let \(_{}^{(t)}\) denote the pseudo-label variable of \(\)-weighted source models and \(_{}^{(t)}=_{i=1}^{N}_{i}^{(t)}_ {S_{i}}^{(t)}\) denote weighted source distribution. Under Lipschitz \(\) and bounded \(_{S}^{(t)}\), with probability at least \(1-3e^{-}\) over the target batch, test risk obeys_

\[(_{T}^{(t)})}_{}-_{T}^{(t)}}_{} }{}_{T}^{(t)},_{ }^{(t)})\}}_{}+_{}^{ (t)},y_{}^{(t)})}_{}\}+}((N+ )/B)}.\]Proof.: Please refer to the Appendix (Section A) for the proof. 

Discussion.In a nutshell, this result shows how CONTRAST strikes a balance between: (1) choosing the domain that has the smallest **shift** from target, and (2) choosing a source model that has high-**quality** pseudo-labels on its own distribution (i.e. \(_{w}^{(t)}\) matches \(y_{w}^{(t)}\)). From our analysis, it is evident that, rather than adapting the source models to the target distribution, if we simply optimize the combination weights to optimize pseudo-labels for inference, the left side excess risk term \(((_{T}^{(t)})-_{T}^{*(t)})\) becomes upper bounded by a relatively modest value. This is because the **shift** and **quality** terms on the right-hand side are optimized with respect to \(\). We note that \(\) is the generalization risk due to finite samples \(B\) and search dimension \(N\).

To further refine this, our immediate objective is to tighten the upper bound. This can be achieved by individually adapting each source model to the current test data, all the while maintaining the optimized \(\) constant. Yet, such an approach is not ideal since our second goal is to preserve knowledge from the source during continual adaptation. To attain our desired goal, we must relax the upper bound, reducing our search over \(\). Here, \(\) is the discrete counterpart of the simplex \(\). The elements of \(\) are one-hot vectors that have all but one entry zero. The elements of \(\) essentially represent discrete model selection. Examining the main terms on the right reveals that: (i) source-target distribution shift and (ii) divergence between ground-truth and pseudo-labels are all minimized when we select the source model with the highest correlation to target. This model, denoted by \(_{S}^{*(t)}\), essentially corresponds to the largest entry of \(^{*(t)}\) and presents the most stringent upper bound within the \(\) search space. Thus, to further minimize the right hand side, the second stage of CONTRAST adapts \(_{S}^{*(t)}\) with the current test data. Crucially, besides minimizing the target risk, this step helps avoids forgetting the source because \(_{S}^{*(t)}\) already does a good job at the target task. Thus, during optimization on target data, \(_{S}^{*(t)}\) will have small gradient and will not move much, resulting in smaller forgetting. Please refer to the Appendix (Section A) for more detailed discussion along with the proof of this theorem.

## 4 Experiments

**Datasets.** We demonstrate the efficacy of our approach using both **static target distribution** and **dynamic target data distributions**. For static case, we employ the _Digits_ and _Office-Home_ datasets . For the dynamic case, we utilize _CIFAR-100C_ and _CIFAR-10C_. Detailed descriptions of these datasets along with results on segmentation task can be found in the Appendix.

**Baseline Methods.** As our problem setting is most closely related to test time adaptation, our baselines are some widely used state-of-the-art (SOTA) single source test time adaptation methods: we specifically compare our algorithm with Tent , CoTTA  and EaTA . These methods deal with adaptation from small batches of streaming data and without the source data, which is our setting, and hence we compare against these as our baselines. To evaluate the adaptation performance, we follow the protocol similar to , where we apply each source model to the test data from a particular test domain individually, which yields X-Best and X-Worst where "X" is the name of the single source adaptation method, representing the highest and lowest performances among the source models adapted using method "X", respectively. For our algorithm, we extend all of the methods "X" in the multi source setting and call the multi-source counterpart of "X" as "X+CONTRAST".

**Implementation Details.** We use ResNet-18  model for all our experiments. For solving the optimization of Eq. (4), we first initialize the combination weights using Eq. (6) and calculate the optimal learning rate using Eq. (7). After that, we use 5 iterations to update the combination weights using SGD optimizer and the optimal learning rate. For all the experiments we use a batch size of 128, as used by Tent . For more details on implementation and experimental setting see Appendix.

**Experiment on CIFAR-100C.** We conduct a thorough experiment on this dataset to investigate the performance of our model under dynamic test distribution. We consider 3 corruption noises out of 15 noises from CIFAR-100C, which are adversarial weather conditions namely _Snow_, _Fog_ and _Frost_. We add these noises for severity level \(5\) to the original CIFAR-100C training set and train three source models, one for each noise. Along with these models, we also add the model trained on clean training set of CIFAR-100. During testing, we sequentially adapt the models across the 15 noisy 

[MISSING_PAGE_FAIL:9]

**Analysis of Forgetting.** Here, we demonstrate the robustness of our method against catastrophic forgetting by evaluating the classification accuracy on the source test set after completing adaptation to each domain . For CONTRAST, we use our ensembling method to adapt to the incoming domain. After adaptation, we infer each of the adapted source models on its corresponding source test set. For the baseline single-source methods, every model is adapted individually to the incoming domain, followed by inference on its corresponding source test set. The reported accuracy represents the average accuracy obtained from each of these single-source adapted models.

From Figure 3, we note that our method consistently maintains its source accuracy during the adaptation process across the 15 sequential noises. In contrast, the accuracy for each individual single-source method (X) declines on the source test set as the adaptation process progresses. Specifically, Tent, not being crafted to alleviate forgetting, experiences a sharp decline in accuracy. While CoTTA and EaTA exhibit forgetting, it occurs at a more gradual pace. Contrary to all of these single-source methods, our algorithm exhibits virtually no forgetting throughout the process.

**Ablation Study.** We conduct an ablation study in Tables 6, 7 in the Appendix to evaluate the impact of various initialization and learning rate strategies on the optimization process described in (4). Our findings demonstrate that the initialization and learning rate configurations generated by our method outperform other alternatives.

Additionally, our experiments in Tables 8, 9 and 10 in the Appendix reveal that selectively updating the most correlated model parameters enhances performance compared to updating all model parameters, the least correlated ones, a selected subset of correlated models or even updating the models according to their combination weights. We report the comparison with MSDA in Table 11 and Model-Soups in Table 12. We also report the values of the combination weights learned by our method. See Section D of the Appendix for detailed observations.

## 5 Conclusions

We propose a novel framework called CONTRAST, that effectively combines multiple source models during test time with small batches of streaming data and without access to the source data. It achieves a test accuracy that is at least as good as the best individual source model. In addition, the design of CONTRAST offers the added benefit of naturally preventing the issue of catastrophic forgetting. To validate the effectiveness of our algorithm, we conduct experiments on a diverse range of benchmark datasets for classification and semantic segmentation tasks. We also demonstrate that CONTRAST can be integrated with a variety of single-source methods. Theoretical analysis of the performance of CONTRAST is also provided.

## 6 Broader Impact and Limitations

Implementing multiple models for adaptation on dynamic distribution can yield broad impacts. For instance, this approach could find applications in robot navigation, autonomous vehicles or decision making in dynamically evolving scenarios. In all these cases, the algorithm can intelligently select the optimal combination of models during inference, ensuring sustained performance over extended periods. Our method currently assumes that data sampled within a batch comes from the same distribution. In the future, we aim to explore using mixed data samples from different target domains within a batch.

Figure 3: **Comparison with baselines in terms of source knowledge forgetting.** Maintaining the same setting as in Table 2, we demonstrate that by integrating single-source methods with CONTRAST, the source knowledge is better preserved during dynamic adaptation. Unlike all these single-source methods, our algorithm demonstrates virtually no forgetting throughout the entire adaptation process.