# 3a. Run offline evaluation (docker) docker-compose up -build --force-recreate

[MISSING_PAGE_FAIL:1]

issues [9; 15; 37] once they leave the lab to enter service. In conventional model-centric ML, the term _benchmark_ often means a standard, fixed dataset for model accuracy comparisons and performance measurements. While this paradigm has been useful for advancing model design, these benchmarks are now saturating (attaining perfect or above "human-level" performance) . This raises two questions: First, is ML research making real progress on the underlying capabilities, or is it just overfitting to existing benchmark datasets or suffering from data artifacts? A growing body of literature explores the evidence supporting benchmark limitations [57; 24; 43; 53; 47; 5; 21; 55]. Second, how should benchmarks evolve to push the frontier of ML research?

In response to these concerning trends, we introduce DataPerf, a data-centric benchmark suite that introduces competition to the field of dataset improvement. We survey a suite of complex data-centric development pipelines across multiple ML domains and isolate a subset of concrete tasks that we believe are representative of current bottlenecks, as illustrated in Figure 1. We freeze model architectures, training hyperparameters, and task metrics to compare solutions strictly via relative improvements from changes to the datasets themselves.

Our contributions are as follows:

* We have developed a comprehensive suite of novel data-centric benchmarks covering a wide range of tasks. These tasks encompass training set selection for speech and vision, data cleaning and debugging, data acquisition, and diffusion model prompting.
* Each benchmark specifies a data-centric task based on a real-world use case rationale. We provide rules for submissions, along with evaluation scripts, and a baseline submission for each benchmark task.
* We provide an extensible and open-source platform for hosting data-centric benchmarks, allowing other organizations and researchers to propose new benchmarks for inclusion in the DataPerf suite, and to host data challenges themselves.

Critically, DataPerf is not a one-off competition. We have established the DataPerf Working Group, which operates under the MLCommons Association. This working group is responsible for the ongoing maintenance of the benchmarks and platform, as well as for fostering the development of data-centric research and methodologies in both academic and industrial domains. The aim is to ensure the long-term sustainability and growth of DataPerf beyond a single competition.

The remainder of the paper is organized as follows. In Section 2.1, we review lessons learned from an exploratory data-centric challenge. Section 2.2 details the hosting platform we developed in response and Section 2.3 presents the DataPerf suite of five novel benchmarks and challenges. We conclude with a survey of related efforts (Section 3) and future directions (Section 5).

Figure 1: Typical benchmarks are model-centric, and therefore focus on the model design and training stages of the ML pipeline (shown in orange). However, to develop high-quality ML applications, users often employ a collection of data-centric operations to improve data quality and repeated data-centric iterations to refine these operations. DataPerf aims to benchmark all major stages of such a data-centric pipeline (shown in green) to improve ML data quality.

DataPerf Benchmarking Suite

We describe the initial challenge which inspired the suite of DataPerf benchmarks and identified which features are needed for hosting data-centric challenges online. We then describe the platform that enables flexible data-centric benchmarking at scale. Finally, we share the initial DataPerf benchmark definitions in vision, speech, acquisition, debugging, and text-to-image prompting.

### The Data-Centric AI Challenge

The DataPerf effort began with an early benchmark which served to validate feasibility and provide real-world insights into the concept of dataset benchmarking. In traditional ML challenges, contestants must train a high-accuracy model given a fixed dataset. This model-centric approach is ubiquitous and has accelerated ML research, but it has neglected the surrounding systems and infrastructure requirements of ML in production . To draw more attention to other areas of the ML pipeline, we created the Data-Centric AI (DCAI) competition , inviting competitors to focus on optimizing accuracy by improving a dataset given a fixed model architecture, thus flipping the conventional challenge format of submitting different models which are evaluated on a fixed dataset. The limiting element was the size of the submitted dataset; therefore, submitters received an initial training dataset to improve through data-centric strategies such as removing inaccurate labels, adding instances that illustrate edge cases and using data augmentation. The competition, inspired by MNIST, focuses on classification of Roman-numeral digits. Just by iterating on the dataset, participants increased the baseline accuracy from 64.4% to 85.8%; human-level performance (HLP) was 90.2%. We learned several lessons from the 2,500 submissions and applied them to DataPerf:

1. Common data pipelines. Successful entries followed a similar procedure: picking seed photos, augmenting them, training a new model, assessing model errors and slicing groups of images with comparable mistakes from the seed photos. We believe more competitions will further establish and refine generalizable and effective practices.
2. Automated methods won. We expected participants would discover and remedy labeling problems, but data-selection and data-augmentation strategies performed best.
3. Novel dataset optimizations. Examples of successful tactics include automated methods for recognizing noisy images and labels, identifying mislabeled images, defining explicit labeling rules for confusing images, correcting class imbalance, and selecting and enhancing images from the long tail of classes. We believe the right set of challenges and ML tasks will yield other novel data-centric optimizations.
4. New methods emerged. In addition to conventional evaluation criteria (the highest performance on common metrics), we created a separate category that evaluated a technique's innovativeness. This approach encouraged participants to explore and introduce novel systematic techniques with potential impact beyond the leaderboard.
5. New supporting infrastructure is necessary. The unconventional competition format necessitated a technology that simultaneously supports a custom competition pipeline as well as ample storage and training time. We quickly discovered that platforms and competitions need complementary functions to support the unique needs of data-centric AI development. Moreover, the competition was computationally expensive. Therefore, we require a more efficient way to train the models on user-submitted data. Computational power, memory and bandwidth are all major limitations.

These five lessons influenced our online platform design and initial suite of DataPerf challenges, as described in the following sections.

### Evaluation Platform

DataPerf provides an online platform where challenge participants can submit their solutions for evaluation, and a working group which invites members in academia and industry to propose new data-centric benchmarks for inclusion in the DataPerf suite. The DataPerf benchmarks, evaluation tools, leaderboards, and documentation are hosted in an online platform called Dynabench1,which allows challenge participants to submit, evaluate, and compare solutions for all data-centric benchmarks defined in Section 2.3. The DataPerf benchmarks and the Dynabench platform are open-source, and are hosted and maintained by the MLCommons Association2, a nonprofit organization supported by more than 50 member companies and academics, ensuring long-term availability and benefit to the community.

We believe DataPerf can serve as a unified benchmark suite for the majority of data-centric use cases, and we welcome proposals from the creators of new and existing data-centric benchmarks. Our five current benchmarks are also intended to serve as representative examples for future authors to host their own challenges on DataPerf, with customized modular submission pipelines for different data modalities and submission artifact types. DataPerf introduces three key extensions to the Dynabench codebase to support data-centric benchmarks: (1) We add support for a wide variety of submission artifacts, such as training subsets, priority values/orderings, and purchase strategies. Users can also submit fully containerized systems as artifacts, such as in the debugging challenge. (2) To support a diverse set of evaluation algorithms and scoring metrics, we develop modular software adaptors to allow for running custom benchmark evaluation tools and displaying or querying scores in Dynabench's online leaderboards. (3) DataPerf utilizes serverless  deployment which dynamically scales resources based on demand, ensuring optimal performance and efficient resource allocation, and allowing the platform to automatically scale with the growth of the benchmark suite and the number of participants. DataPerf additionally offers offline evaluation scripts, enabling local iteration on solutions before submitting for verification, further reducing load on the Dynabench platform. These improvements to Dynabench ensure DataPerf can accommodate a large suite of community-contributed data-centric challenges in the future.

### Challenges, Benchmarks, and Leaderboards

DataPerf uses leaderboards and challenges to encourage constructive competition and inspire advances in building and optimizing datasets. In this section, we clarify DataPerf's terminology. A leaderboard is a public summary of benchmark results; it helps to quickly identify state-of-the-art approaches. A challenge is a public contest to achieve the best result on a leaderboard in a fixed timeframe. Challenges motivate rapid progress through recognition and awards. Our leaderboards and challenges are hosted on the online platform Dynabench (Section 2.2) developed and supported by MLCommons. Benchmarks are fixed specifications for comparative evaluation on a static task, and the key leave-behind of each challenge. MLCommons will provide long-term support for each benchmark through leaderboards which remain open for submission and comparison once a challenge concludes. Each challenge also provides a baseline implementation to set a minimum bar for each leaderboard metric and to discourage uninformative or random submissions.

DataPerf's initial suite consists of tasks in training set selection for speech and vision, data cleaning and debugging, data acquisition, and generative model prompting. Figure 1 depicts underserved components in benchmarking machine learning pipelines, and these five tasks were selected by the DataPerf working group among the initial proposals for challenges in order to cover as many of these components as possible while also exercising the infrastructure requirements for our online platform. The following sections describe the benchmarks that compose the first iteration of the DataPerf benchmark suite. Documentation for each benchmark's definition, metrics, submission rules, and introductory tutorials are available on dataperf.org and reproduced in our Appendix, and our open-source baseline implementations are available at https://github.com/MLcommons/dataperf.

#### 2.3.1 Selection for Speech

DataPerf includes a dataset-selection-algorithm challenge with an emphasis on low-resource speech. The objective of the speech selection task is to develop a selection algorithm that chooses the most effective training samples from a large and noisy multilingual corpus of spoken words, expanding sample quality estimation techniques to low-resource language settings. The provided training set is used to train and evaluate an ensemble of fixed keyword-detection models.

Use-Case RationaleKeyword spotting (KWS) is a ubiquitous speech classification task present on billions of devices. A KWS model detects a limited vocabulary of spoken words. Productionexamples include the wakeword interfaces for Google Voice Assistant, Siri and Alexa. However, public KWS datasets traditionally cover very few words in only widely-spoken languages. In contrast, the Multilingual Spoken Words Corpus  (MSWC), is a large dataset of over 340,000 spoken words in 50 languages (collectively, these languages represent more than five billion people). MSWC automates word-length audio clip extraction from crowdsourced data. Due to errors in the generation process and source data, some samples are incorrect. For instance, they may miss part of the target sample (e.g., "weathe-" instead of "weather") or may contain part of an adjacent word (e.g., "time to" instead of "time"). This benchmark focuses on estimating the quality of each automatically-generated sample in KWS training pipelines intended for low-resource languages. Additionally, this benchmark establishes the DataPerf platform's capabilities for hosting speech challenges in multiple languages.

Benchmark DesignParticipants design a training-set-selection algorithm to propose the fewest possible data samples for training three keyword-spotting models for five target words each across three languages: English, Portuguese, and Indonesian, representing high, medium, and low-resource languages. The benchmark evaluates the algorithm on the mean F1 score of each evaluation set (additional details in Appendix A.3). The model is an ensemble of SVC and logistic-regression classifiers, which output one of six categories (five target classes and one "unknown" class). The inputs to the classifier are 1,024-dimensional vectors of embedding representations from a pretrained keyword feature extractor . Participants may only specify which training samples are used by the model; all other configuration parameters are fixed, thereby emphasizing the importance of selecting the most informative samples. For each language there are separate leaderboards for submissions with \(\) 25 samples or \(\) 60 samples, evaluating the algorithm's sensitivity to the training set size.

Participants are given a tutorial baseline which uses crossfold validation in a Google Colab notebook and an offline copy of the evaluation pipeline, for ease of setup and and rapid experimentation. This system design addresses a problem identified in the data-centric AI challenge (Section 2.1) - enabling offline development reduces the computational requirements for online evaluation, though participants must agree to challenge rules on not inspecting the evaluation set. The DataPerf server evaluates and verifies submitted training sets automatically (Sec. 2.2) for inclusion in the live leaderboard. Figure 2 illustrates the speech-selection benchmark workflow.

Baseline ResultsWe provide two baseline implementations, nested cross-fold selection and a data-cleaning approach using the Cleanlab framework . The cross-fold selection method uses nested cross-validation where the outer loop selects different subsets of the target samples and the inner loop selects different subsets of the nontarget samples, and the best performing subsets are reported back as the selected training set. The Cleanlab method rejects outliers using out-of-sample predicted probability estimates for each candidate sample (also computed via cross-validated models). All baseline scores are averaged across 10 random seeds.

   &  &  &  \\  Training set size & 25 & 60 & 25 & 60 & 25 & 60 \\  Nested cross-fold & 0.32 & 0.41 & 0.42 & 0.52 & 0.36 & 0.42 \\  Cleanlab & 0.49 & 0.49 & 0.47 & 0.57 & 0.37 & 0.43 \\  

Table 1: Baseline results (macro F1 scores) for the Selection for Speech challenge.

Figure 2: System design and component ownership for the speech selection benchmark.

#### 2.3.2 Selection for Vision

DataPerf includes a data selection algorithm challenge with a vision-centric focus. The objective of this task is to develop a data selection algorithm that chooses the most effective training samples from a large candidate pool of images. This resulting training sets will then be used to train a collection of binary classifiers for various visual concepts. The benchmark evaluates the algorithm on the basis of the resulting models' classification performance on the evaluation set.

Use-Case RationaleLarge datasets have been critical to many ML achievements, but they impose significant challenges. Massive datasets are cumbersome and expensive, in particular unstructured data such as web-scraped or weakly-labeled images, videos, and speech. Careful data selection can mitigate some of the difficulties by focusing computational and labeling resources on the most valuable examples and emphasizing quality over quantity, reducing training cost and time.

The vision-selection-algorithm benchmark evaluates binary classification of visual concepts (e.g., "monster truck" or "jean jacket") in unlabeled images. Familiar production examples of similar models include automatic labeling services by Amazon Rekognition, Google Cloud Vision API and Azure Cognitive Services. Successful approaches to this challenge will enable image classification of long-tail concepts where discovery of high-value data is critical, and advances the democratization of computer vision . This benchmark demonstrates DataPerf's support for challenges with unlabeled image data and is a template for future benchmarks that target automatic labeling.

Benchmark DesignThe task is to design a data-selection strategy that chooses the best training examples from a large pool of training images. We evaluate submissions on their ability to algorithmically propose a subset of the Open Images Dataset V6 training set  that maximizes the mean F1-score over a set of fixed concepts ("cupcake," "hawk" and "sushi"). We provide a set of positive examples for each classification task that participants can use to search for images containing the target concepts. Participants must submit a training set for each classification task in addition to a description of the data selection method by which they generated the training sets. The challenge platform (Sec. 2.2) automates evaluation of submissions.

Baseline ResultsWe provide three baseline results, namely, farthest point sampling, pseudo-label generation, and modified uncertainty sampling. Farthest point sampling selects negative examples by attempting to sample the feature search space through iterative maximum l\({}_{2}\) distances, afterwards returning the best coreset under nested cross-validation. Pseudo label generation trains multiple neural networks and classical models on a subset of data to classify the remainder of points and uses the best-performing model for coreset proposal under multiple sampling experiments. Modified uncertainty sampling trains a binary classifier on noisy positive labels from OpenImages and uses this classifier to assign positive and negative image pools, with the coreset randomly sampled from both pools. For each baseline, F\({}_{1}\) scores on the three test concepts are provided in Table 2.

#### 2.3.3 Debugging for Vision

The debugging challenge is to detect candidate data errors in the training set that cause a model to have inferior quality. The aim is to assist a user in prioritizing which samples to inspect, correct, and clean. A debugging method's purpose is to identify the most detrimental data points from a potentially noisy training set. After inspecting and correcting the selected data points, the cleaned dataset is used to train a new classification model. Evaluation is based on the number of data points the debugging approach must correct to attain a certain accuracy.

   & Cupcake & Hawk & Sushi & Mean F1-score \\  Farthest point sampling & 0.75 & 0.87 & 0.82 & 0.81 \\  Pseudo label generation & 0.70 & 0.86 & 0.81 & 0.79 \\  Modified uncertainty sampling & 0.71 & 0.83 & 0.80 & 0.78 \\  

Table 2: Baseline results (F\({}_{1}\) scores) for the Selection for Vision challenge.

Use-Case RationaleDatasets are rapidly growing in size. For instance, Open Images V6 has 59 million image-level labels. Such datasets are annotated either manually or using ML. Unfortunately, noise is unavoidable and can originate from both human annotators and algorithms. Models trained on noisy annotations suffer in accuracy and carry risks of bias and unfairness. Dataset cleaning is a common approach to dealing with noisy labels. However, it is a costly and time-consuming process that typically involves human review. Consequently, examining and sanitizing the entire dataset is often impractical. A data-centric method that focuses human attention and cleaning efforts on the most important data elements can significantly reduce the time, cost, and labor of dataset debugging. This challenge demonstrates the DataPerf platform's ability to simulate human-in-the-loop data-centric tasks, in this case label cleaning, while remaining scalable.

Benchmark DesignThe debugging task is based on binary image classification. For each activity, participants receive a noisy training set (i.e., some labels are inaccurate) and a validation set with correct labels. They must provide a debugging approach that assigns a priority value (harmfulness) to each training set item. After each trial, all training data will have been examined and rectified. Each time a new item is examined, a classification model is trained on the clean dataset, and the test accuracy on a hidden test set is computed. Then a score is returned.

The image sets are from the Open Images Dataset , with two important considerations: (1) The number of data points should be sufficient to permit random selection of samples for the training, validation and test sets. (2) The number of discrepancies between the machine-generated label and the human-verified label varies by task; the challenges thus reflect varying classification complexity. We introduce two types of noise into the training set's human-verified labels: some labels are arbitrarily inverted, and machine-generated labels are substituted for some human-verified labels to imitate the noise from algorithmic labeling.

We use a 2,048-dimensional vector of embedding representations extracted from a pretrained ResNet50 model  as the classifier's input data. Participants may simply prioritize each training sample used by the classifier; all other configurations are fixed for all submissions. By precomputing all embeddings, participants are encouraged to propose data-centric debugging methods for arbitrary features rather than approaches specific to the image domain. This also removes the need for GPU acceleration during submission evaluation.

We use a concealed test set to evaluate the trained classification model's performance on each task. Since the objective of the debugging challenge is to determine which method produces sufficient accuracy while analyzing the fewest data points, the assessment metric in the debugging challenge is the proportion of inspections necessary to achieve 95% of the accuracy that the classifier trained on the cleaned training set achieves. We verify submissions by incrementally cleaning the data and training a model on each step. Each submission contains a list of indices in the order that the submitter wishes to clean. We incrementally prepare a new dataset for each cleaned sample. For instance, assuming the submission is , we will prepare 5 datasets that are [5-cleaned, 4,3,2,1], [5-cleaned, 4-cleaned, 3, 2, 1], and so forth. We then train a XGBoost classifier on each dataset, and report back the step at which the accuracy is high enough (>95%) on the test dataset.

Participants in this challenge develop and validate their algorithms on their own machines using the dataset and evaluation framework provided by DataPerf. Once they are satisfied with their implementation, they submit a containerized version to the server (Sec. 2.2). The server then reruns the uploaded implementation on several hidden tasks and posts the average score to a leaderboard.

Baseline ResultsThe benchmark system provides three baseline implementations: consecutive, random and DataScope , which achieve the score of 53.50, 51.75 and 15.54 respectively. In other words, DataScope needs to fix 15.54% of data samples to achieve the threshold, consecutive needs 53.50% and random needs to fix 51.75%. DataScope is a fast approximation for Shapley values  for importance estimates of each sample included and the effect of noise. As Shapley values require calculating the payoff of every subset (\((2^{})\) evaluations), approximation techniques such as DataScope are necessitated.

#### 2.3.4 Data Acquisition

The data acquisition challenge explores which dataset or combination of datasets to purchase in a multi-source data marketplace for specific ML tasks.

Use-Case RationaleRich data is increasingly sold and purchased either directly via companies (e.g., Twitter  and Bloomberg ) or data marketplaces (e.g., Amazon AWS Data Exchange , Databricks Marketplace , and TAUS Data Marketplace ) to train a high-quality ML model customized for specific applications. Those datasets are necessary often because the datasets (i) cover underrepresented populations, (ii) offer high-quality annotations, and (iii) exhibit easy-to-use formats. On the other hand, the datasets are also expensive due to the tremendous efforts spent to curate and clean data samples. _Content opacity_ is therefore ubiquitous: data sellers usually are disinclined to release the full content of their datasets to the buyers. This renders it challenging for the data users to decide whether a dataset is useful for the downstream ML tasks. Based on our conversations with practitioners, existing data acquisition methods for ML are _ad-hoc_: one has to manually identify data sellers, articulate their needs, estimate the data utilities, and then purchase them. It is also iterative in nature: the datasets may show limited improvements on a downstream ML task after being purchased, and then one has to search for a new dataset again. With this in mind, the goal of this challenge is to mitigate a data buyer's burden by automating and optimizing the data acquisition strategies. This challenge demonstrates the platform's ability to handle data-valuations and demonstrates a unique metric based on a pricing function and a budget, which is a useful template for future challenges that wish to capture the nuance of resource expenditure.

Benchmark DesignParticipants in this challenge must submit a data acquisition strategy. The data acquisition strategy specifies the number of samples to purchase from each available data seller in a data marketplace. Then the benchmark suite generates a training dataset based on the acquisition strategy to train an ML classifier. To mimic data acquisition in a real-world data marketplace, participants do not have access to sellers' data. Instead, the participants are offered (1) a few samples (=5) from each data seller, (2) summary statistics about each dataset, (3) the pricing functions that quantify how much to pay when a particular number of samples is purchased from one seller, and (4) a budget constraint. The participant's goal is to identify a data acquisition strategy within the budget constraint that maximizes the trained classifier's performance on an evaluation dataset. As the focus is on training data acquisition, the evaluation dataset is also available to all participants. The overall system design can be found in Figure 3.

Baseline ResultsWe offer three baseline methods, namely, UNIFORM, RSS (random single seller), and FSS (fixed single seller). UNIFORM purchases data points uniformly randomly from every sellers. RSS spends all budgets to buy as much as possible data points from one uniformly randomly chosen seller, while FSS does the same from a fixed seller. The baseline performance

Figure 3: Data acquisition benchmark design. The participants observe the pricing mechanisms, the dataset summaries, and the evaluation datasets. They then need to develop and submit the data acquisition strategies. The evaluation is executed automatically on the DataPerf server.

   & Market Instance & 0 & 1 & 2 & 3 & 4 \\    & UNIFORM & 0.732 & 0.757 & 0.771 & 0.754 & 0.742 \\   & RSS & 0.705 & 0.732 & 0.73 & 0.721 & 0.679 \\   & FSS & 0.727 & 0.719 & 0.735 & 0.699 & 0.678 \\  

Table 3: We measure three baselines’ performance on all five data market instances. A large performance heterogeneity is observed, calling for carefully designed data acquisition approaches.

can be found in Table 3. Overall, there is a large performance heterogeneity among the considered baselines. This underscores the necessity of carefully designed data acquisition strategies.

#### 2.3.5 Adversarial Nibbler

The goal of the Adversarial Nibbler challenge is to engage the research community in jointly discovering a diverse set of insightful long-tail problems for text-to-image models and thus help identify current blindspots in harmful image production (i.e., unknown unknowns). We focus on prompt-image pairs that currently slip through the cracks of safety filters - either via intentful and subversive prompts that circumvent the text-based filters or through seemingly benign requests that nevertheless trigger unsafe outputs. By focusing on unsafe generations paired with seemingly safe prompts, our challenge zeros in on cases that (1) are most challenging to catch via text-prompt filtering and (2) have the potential to be harmful to non-adversarial end users.

Use-Case RationaleBuilding on recent successes for data fairness , quality , limitations , and documentation and replication  of adversarial and data-centric challenges for classification models, we identify a new challenge for discovering failure modes in generative text-to-image models. Models such as DALL-E 2, Stable Diffusion, and Midjourney have reached large audiences in the past year owing to their impressive and flexible capabilities. While most models have text-based filters in place to catch explicitly harmful generation requests, these filters are inadequate to protect against the full landscape of possible harms. For instance,  recently revealed that Stable Diffusion's obfuscated safety filter only catches sexually explicit content but fails to address violence, gore, and other problematic content. Our objective is to identify and mitigate safety concerns in a structured and systematic manner, covering both the discovery of new failure modes and the confirmation of existing ones. Adversarial Nibbler exercises DataPerf's ability to host challenges focused on evaluating generative AI and AI safety, and demonstrates DataPerf's support for high-demand GPU inference tasks and integration with external APIs. Additionally, this challenge demonstrates new benchmark criterion targeted at generative models.

Benchmark DefinitionThis competition is aimed at researchers, developers, and practitioners in the field of fairness and development of text-to-image generative AI. We intentionally design the competition to be simple enough that researchers from non-AI/ML communities can participate, though the incentive structure is aimed at researchers. Participants must write a benign or subversive prompt which is expected to correspond to an unsafe image. Our evaluation server returns several generated images using DataPerf-managed API licenses, and the participant selects an image (or none) that falls into one of our failure mode categories surrounding stereotypes, culturally inappropriate, or ethically inappropriate generations, among others.

We aim to collect prompts that are considered as a "backdoor" for unsafe generation. We focus on two different types of prompt-generation pairs, each reflecting a different user-model interaction mode. (1) _Benign prompts with unexpected unsafe outputs_. A benign prompt in most cases is expected to generate safe images. However, in some instances even a benign prompt may unexpectedly trigger unsafe or harmful generations. (2) _Subversive prompts with expected unsafe outputs_. While text filters catch unambiguously harmful requests, users can adversarially bypass the filters via subversive prompts which trigger the model to produce unsafe or harmful generations. The data gathered from the first round is then sent to humans for validation before results are released to a leaderboard. Participants are rewarded based on two criteria: _validated attack success_ - the number of unsafe images generated, and _submission creativity_ - assessing coverage in terms of attack mode across lexical, semantic, syntactic, and pragmatic dimensions.

Baseline ResultsAs the Adversarial Nibbler challenge focuses on crowdsourced data and deviates from the other benchmarks, there is no starter code or a baseline result. Instead, the goal is to analyze the data from the challenge submissions and create a publicly available dataset consisting of prompt-image pairs. These pairs that will undergo validation will be used to establish data ratings and will serve as a valuable resource for drawing conclusions and insights from the submissions received. Adversarial Nibbler has already collected several hundred unique prompts. Results from this challenge, consisting of a public dataset and insights to red teaming approaches from challenge participants, will be disseminated at the IJCNLP-AACL 2023 ART of Safety Workshop3.

To ensure academic innovations have real-world impact, systems research in the machine learning industry has relied on benchmarking, including MLPerf [33; 46], DawnBench  and related efforts [19; 60; 51]. Data-centric benchmarking has similarly received increased focus. Zha et al.  surveys recent efforts, including benchmarks in AutoML , semi-supervised strategies , data selection , and data cleaning approaches . Benchmark competitions have also emerged as a valuable comparative method in data-centric AI. DataComp  is a recent competition focused on filtering multimodal training data for language-image pairs, with a focus on improving accuracies under different fixed compute budgets. The Crowdsourcing Adverse Test Sets for Machine Learning (CATS4ML) Data Challenge  asked participants to find examples that are confusing or otherwise problematic for image classification algorithms to process, in which participants submitted misclassified samples from the Google Open Images dataset, identifying 15,000 adversarial examples. Drawing inspiration from these efforts, DataPerf solicits user-contributed benchmarks by providing an extensible platform for hosted public challenges and leaderboards, with long-term, industry-guided support for benchmarks through the DataPerf Working Group and MLCommons.

Several existing benchmarks evaluate state-of-the-art methods in selection. For instance, prior work in benchmarking high-dimensional feature selection  and augmentation strategies  are conceptually similar to the vision selection and roman numeral tasks. DCBench  is a benchmark and Python API for fixed-budget cleaning, slice discovery , and coreset selection, which are applicable to our speech selection, vision selection, and data debugging tasks. The baselines in DataPerf do not exhaustively compare all state-of-the-art data-centric methods, but instead encourage students and new practitioners to apply existing methods from the literature, while still enabling academic researchers to propose novel methods. Persistent online leaderboards for each challenge enable new solutions to be compared to all prior submissions. The DataPerf Working Group endeavors to solicit new challenges from the data-centric research community, and to integrate existing benchmarks (ideally in partnership with their respective authors) in additional domains, such as active learning for tabular data , label uncertainty , and noisy annotations .

## 4 Statement of Ethics

Dynabench collects self-declared usernames and email addresses during registration, and these usernames may correspond to personal identifiable information. Dynabench also collects uploaded artifacts during submission which can optionally be viewed by other users as open benchmark results.

Adversarial Nibbler requires additional guidelines for participants as it collects potentially sensitive content of harmful and disturbing depictions which may negatively impact participants and raters. These guidelines follow best practices for protecting well-being  and provides communication with challenge organizers, preparation for working with potentially unsafe imagery, and external resources for psychological support (detailed in Appendix A.7)

## 5 Conclusion and Future Work

The purpose of DataPerf is to improve machine learning by expanding AI research from _just_ models to models _and datasets_. The benchmarks aim to improve standard practices for dataset development, and add rigor to assessing the quality of training and test sets, across a wide variety of ML applications. Systematic dataset benchmarking is vital, per the adage "what gets measured gets improved." The initial version of DataPerf comprises five benchmarks, each with unique rules, evaluation methods, and baseline implementations, and an open-source, extensible evaluation platform.

DataPerf will continue to expand by adding additional benchmarks to the suite, with input and contributions from the community. Additionally, in order to increase the reproducibility of challenges and expand the scope of the evaluation, we plan to add a 'Closed Division' where participants must submit an algorithm that is then evaluated on a 'hidden training set', meaning it is tested on data that the submitter has never seen. This evaluates if the algorithm can generalize beyond the original dataset's distribution. We urge interested parties to join the DataPerf Working Group, and to participate in and contribute to current benchmarking challenges or propose new challenges at https://dataperf.org.