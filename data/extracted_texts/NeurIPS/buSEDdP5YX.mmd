# Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We consider the problem of computing tight privacy guarantees for the composition of subsampled differentially private mechanisms. Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied.

Our main contribution is to address two common points of confusion. First, some privacy accountants assume that the privacy guarantees for the composition of a subsampled mechanism are determined by self-composing the worst-case datasets for the uncomposed mechanism. We show that this is not true in general. Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement. We show that the privacy guarantees may in fact differ significantly between the two sampling schemes. In particular, we give an example of hyperparameters that result in \( 1\) for Poisson subsampling and \(>10\) for sampling without replacement. This occurs for some parameters that could realistically be chosen for DP-SGD.

## 1 Introduction

A fundamental property of differential privacy is that the composition of multiple differentially private mechanisms still satisfies differential privacy. This property allows us to design complicated mechanisms with strong formal privacy guarantees such as differentially private stochastic gradient descent (DP-SGD, ).

The privacy guarantees of a mechanism inevitably deteriorate with the number of compositions. Accurately quantifying the privacy parameters under composition is highly non-trivial and is an important area within the field of differential privacy. A common approach is to find the privacy parameters for each part of a mechanism and apply a composition theorem  to find the privacy parameters of the full mechanism. In recent years, several alternatives to the traditional definition of differential privacy with cleaner results for composition have gained popularity (see, e.g., ).

Another important concept is privacy amplification by subsampling (see, e.g., ). The general idea is to improve privacy guarantees by only using a randomly sampled subset of the full dataset as input to a mechanism. In this work we consider the problem of computing tight privacy parameters for subsampled mechanisms under composition.

One of the primary motivations for studying privacy accounting of subsampled mechanisms is DP-SGD. DP-SGD achieves privacy by clipping gradients and adding Gaussian noise to each batch. As such, we can find the privacy parameters by analyzing the subsampled Gaussian mechanism under composition. One of the key contributions of  was the moments accountant, which gives tighter bounds for the mechanism than the generic composition theorems. Later workimproved the accountant by giving improved bounds on the Renyi Differential Privacy guarantees of the subsampled Gaussian mechanism under both Poisson subsampling and sampling without replacement .

Even small constant factors in an \((,)\)-DP budget are important. First, from the definition, such constant factors manifest exponentially in the privacy guarantee. Furthermore, when training a model privately with DP-SGD, it has been observed that they can lead to significant differences in the downstream utility, see, e.g., Figure 1 of . Consequently, "saving" such a factor in the value of \(\) through tighter analysis can be very valuable. While earlier _approximate_ techniques for privacy accounting (e.g., moments accountant of  and related methods) were lossy, a more recent line of work focuses on _exact_ computation of privacy loss by numerically estimating the privacy parameters . These accountants generally look at the "worst case" for a single iteration for a privacy mechanism, and then use a fast Fourier transform (FFT) to compose the privacy loss over multiple iterations. They often rely on an implicit assumption that the worst-case dataset for a single execution of a privacy mechanism remains the worst case for a self-composition of the mechanism.

Most privacy accounting techniques for DP-SGD assume a version of the algorithm that employs amplification by _Poisson_ subsampling. That is, the batch for each iteration is formed by including each point independently with sampling probability \(\). Other privacy accountants consider a variant where random batches of a fixed size are selected for each step. Note that both of these are inconsistent with the standard method in the non-private setting, where batches are formed by randomly permuting and then partitioning the dataset. Indeed, the latter approach is much more efficient, and highly-optimized in most libraries. Consequently, many works in private machine learning implement a method with the conventional shuffle-and-partition method of batch formation, but employ privacy accountants that assume some other method of sampling batches. The hope is that small modifications of this sort would have negligible impact on the privacy analysis, thus justifying privacy accountants for a setting which is _technically_ not matching. Concurrent work to this paper by  compares the shuffle-and-partition technique with Poisson subsampling. Similar to our results they find that the batching method can significantly impact the privacy parameters.

The central aim of our paper is to highlight and clarify some common problems with privacy accounting techniques. Towards the goal of more faithful comparisons between private algorithms that rely upon such accountants, we make the following contributions:

* In Sections 4 and 5, we establish that a worst-case dataset may exist for a single execution of a privacy mechanism but may fail to exist when looking at the self-composition of the same mechanism. Some popular privacy accountants incorrectly assume otherwise. Our counterexample involves the subsampled Laplace mechanism, and stronger analysis is needed to demonstrate the soundness of privacy accountants for specific mechanisms, e.g., the subsampled Gaussian mechanism.
* In Section 6, we show that rigorous privacy accounting is _significantly_ affected by the method of sampling batches, e.g., Poisson versus fixed-size. This results in sizeable differences in the resulting privacy guarantees for settings which were previously treated as interchangeable by prior works. Consequently, we caution against the common practice of using one method of batch sampling and employing the privacy accountant for another.
* In Section 7, we discuss issues that arise in tight privacy accounting under the "substitution" relation for neighbouring datasets, which make this setting even more challenging than under the traditional "add/remove" relation. Once again we consider the subsampled Laplace mechanism and show that there may be several worst-case datasets one must consider when doing accounting, exposing another important gap in existing analyses.

## 2 Preliminaries

Differential privacy is a rigorous privacy framework introduced by . Differential privacy is a restriction on how much the output distribution of a mechanism can change between any pair of datasets that differ only in a single individual. Such datasets are called neighboring, and we denote a pair of neighboring datasets as \(D D^{}\). We formally define neighboring datasets below.

**Definition 1** (\((,)\)-Differential Privacy).: _A randomized mechanism \(\) satisfies \((,)\)-DP under neighboring relation \(\) if and only if for all \(D D^{}\) and all measurable sets of outputs \(Z\) we have_

\[[(D) Z] e^{}[(D^{}) Z]+.\]

In this work, we consider problems where we want to estimate a sum for \(k\) queries where each datapoint holds a single-dimensional real value in the interval \([-1,1]\) for each query. The mechanisms we consider apply more generally to multi-dimensional real-valued queries. Since we demonstrate issues already present in the former more restrictive setting, these pitfalls are present in the more general case as well. We focus on single-dimensional inputs for simplicity of presentation. Likewise, by considering mechanisms defined on \([-1,1]\), our privacy analysis immediately extends to any mechanism defined on \(\) that clips to \([-1,1]\). After the appropriate rescaling, our privacy analysis extends to any mechanism used in practice for DP-SGD. Note that in all but one example in Section 7 the datapoints hold the same value for all \(k\) queries for the datasets we consider. We abuse notation and represent each data point as a single real value rather than a vector.

On the domain \([-1,1]^{* k}:=_{m=0}^{}[-1,1]^{m k}\), we define the neighboring definitions of add, remove, and substitution (replacement). We typically want the neighboring relation to be symmetric, which is why add and remove are typically included in a single definition. However, as noted by previous work we need to analyze the add and remove cases separately to get tight results (see, e.g., ).

**Definition 2** (Neighboring Datasets).: _Let \(D\) and \(D^{}\) be datasets. If \(D^{}\) can be obtained by adding a datapoint to \(D\), then we write \(D_{A}D^{}\). Likewise, if \(D^{}\) can be obtained by removing a datapoint from \(D\), then we write \(D_{R}D^{}\). Combining these, write \(D_{A/R}D^{}\) if \(D_{A}D^{}\) or \(D_{R}D^{}\). Finally, we write \(D_{S}D^{}\) if \(D\) can be obtained from \(D^{}\) by swapping one datapoint for another._

Note that differential privacy under add and remove implies differential privacy under substitution, with appropriate translation of the privacy parameters.

Definition 1 can be restated in terms of the hockey-stick divergence.

**Definition 3** (Hockey-stick Divergence).: _For any \( 0\) the hockey-stick divergence between two distributions \(P\) and \(Q\) is defined as_

\[H_{}(P||Q):=_{y Q}[\{(y)-,0\}]\]

_where \(\) is the Radon-Nikodym derivative._

Specifically, a randomized mechanism \(\) satisfies \((,)\)-DP if and only if \(H_{e^{}}((D)||(D^{}))\) for all pairs of neighboring datasets \(D D^{}\). This restated definition is the basis for the privacy accounting tools we consider in this paper. If we know what choice of neighboring datasets \(D D^{}\) maximizes the expression then we can get optimal parameters by computing \(H_{e^{}}((D)||(D^{}))\).

The full range of privacy guarantees for a mechanism can be captured by the privacy curve.

**Definition 4** (Privacy Curves).: _The privacy curve of a randomized mechanism \(\) under neighboring relation \(\) is the function \(^{}_{}:\) given by_

\[^{}_{}():=\{: (,)\}.\]

_If there is a single pair of neighboring datasets \(D D^{}\) such that \(^{}_{}()=H_{e^{}}((D) ||(D^{}))\) for all \( 0\), we say that the privacy curve of \(\) under \(\) is realized by the worst-case dataset pair \((D,D^{})\)._

Unfortunately, a worst-case dataset pair does not always exist. A broader tool that is now frequently used in the computation of privacy curves is the privacy loss distribution (PLD) formalism .

**Definition 5** (Privacy Loss Distribution).: _Given a mechanism \(\) and a pair of neighboring datasets \(D D^{}\), the privacy loss distribution of \(\) with respect to \((D,D^{})\) is_

\[L_{}(D||D^{}):=(d(D)/d(D^{}))( y),\]

_where \(y(D)\) and \(d(D)/d(D^{})\) means the density of \((D)\) with respect to \((D^{})\)._An important caveat is that the privacy loss distribution is defined with respect to a specific pair of datasets, whereas the privacy curve implicitly involves taking a maximum over all neighboring pairs of datasets. Nonetheless, the PLD formalism can be used to recover the hockey-stick divergence via

\[H_{e^{}}((D)||(D^{}))=_{Y  L_{}(D||D^{})}[1-e^{-Y}],\]

from which we can reconstruct the privacy curve as

\[^{}_{}()=_{D D^{}}_{Y  L_{}(D||D^{})}[1-e^{-Y}].\]

Lastly, we define the two subsampling procedures we consider in this work: sampling without replacement (WOR) and Poisson sampling. Given a dataset \(D=(x_{1},,x_{n})\) and a set \(I\{1,,n\}\), we denote the restriction of \(D\) to \(I=\{i_{1},,i_{b}\}\) by \(D|_{I}:=(x_{i_{1}},,x_{i_{b}})\).

**Definition 6** (Subsampling).: _Let \(\) take datasets of size1\(b 1\). The \(\)-subsampled mechanism \(_{WOR}\) is defined on datasets of size \(n b\) as_

\[_{WOR}(D):=(D|_{I}),\]

_where \(I\) is a uniform random \(b\)-subset of \(\{1,,n\}\)._

_On the other hand, given a mechanism \(\) taking datasets of any size, the \(\)-subsampled mechanism \(_{Poisson}\) is defined on datasets of arbitrary size as_

\[_{Poisson}(D):=(D|_{I}),\]

_where \(I\) includes each element of \(\{1,,|D|\}\) independently with probability \(\)._

## 3 Related Work

After  introduced privacy loss distributions, a number of works used the formalism to estimate the privacy curve to arbitrary precision, beginning with .  developed an efficient accountant that efficiently computes the convolution of PLDs by leveraging the fast Fourier transform.  fine-tuned the application of FFT to speed up the accountant by several orders of magnitude.

The most relevant related paper for our work is by . They introduce the concept of a dominating pair of distributions. Dominating pairs generalize worst-case datasets, which for some problems can be difficult to find and may not even exist.

**Definition 7** (Dominating Pair of Distributions ).: _The ordered pair \((P,Q)\) is a dominating pair of distributions for a mechanism \(\) (under some neighboring relation \(\)) if for all \( 0\) it holds that_

\[_{D D^{}}H_{}((D)||(D^{}))  H_{}(P||Q).\]

The hockey-stick divergence of the dominating pair \(P\) and \(Q\) gives an upper bound on the value \(\) for any \(\). Note that the distributions \(P\) and \(Q\) do not need to be output distributions of the mechanism. However, if there exists a pair of neighboring datasets such that \(P=(D)\) and \(Q=(D^{})\) then we can find tight privacy parameters by analyzing the mechanisms with inputs \(D\) and \(D^{}\) because \(H_{e^{}}((D)||(D^{}))\) is also a lower bound on \(\) for any \(\). We refer to such \(D D^{}\) as a dominating pair of datasets.

The definition of dominating pairs of distributions is useful for analyzing the privacy guarantees of composed mechanisms. In this work, we focus on the special case where a mechanism consists of \(k\) self-compositions. This is, for example, the case in DP-SGD, in which we run several iterations of the subsampled Gaussian mechanism. The property we need for composition is presented in Theorem 8.

**Theorem 8** (Following Theorem 10 of ).: _If \((P,Q)\) is a dominating pair for a mechanism \(\) then \((P^{k},Q^{k})\) is a dominating pair for \(k\) iterations of \(\)._

When studying differential privacy parameters in terms of the hockey-stick divergence, we usually focus on the case of \( 1\). Recall that the hockey-stick divergence of order \(\) can be used to bound the value of \(\) for an \((,)\)-DP mechanism where \(=()\). We typically do not care about the region of \(<1\) because it corresponds to negative values of \(\). However, the definition of dominating pairs of distributions must include these values as well. This is because outputs with negative privacy loss are important for composition and Theorem 8 would not hold if the definition only considered \( 1\). In Sections 5 and 7 we consider mechanisms where the distributions that bound the hockey-stick divergence for \( 1\) without composition do not bound the divergence for \( 1\) under composition.

 studied general mechanisms in terms of dominating pairs of distributions under Poisson subsampling and sampling without replacement. Their work gives upper bounds on the privacy parameters based on the dominating pair of distributions of the non-subsampled mechanism. We use some of their results which we introduce later throughout this paper.

## 4 Dominating Pair of Datasets under Add and Remove Relations

In this section we give pairs of neighboring datasets with provable worst-case privacy parameters under the add and remove neighboring relations separately. We use these datasets as examples of the pitfalls to avoid in the subsequent section, where we discuss the combined add/remove neighboring relation.

**Proposition 9**.: _Let \(\) be either the Gaussian mechanism \((x_{1},,x_{n}):=_{i=1}^{n}x_{i}+(0,^{2})\) or the Laplace mechanism \((x_{1},,x_{n}):=_{i=1}^{n}x_{i}+(0,s)\)._

1. _The datasets_ \(D:=(0,,0)\) _and_ \(D^{}:=(0,,0,1)\) _form a dominating pair of datasets for_ \(_{Poisson}\) _under the add relation and_ \((D^{},D)\) _is a dominating pair of datasets under the remove relation._
2. _Likewise, the datasets_ \(D:=(-1,,-1)\) _and_ \(D^{}:=(-1,,-1,1)\) _form a dominating pair of datasets for_ \(_{WOR}\) _under the add relation and_ \((D^{},D)\) _is a dominating pair of datasets under the remove relation._

The proposition implies that the hockey-stick divergence of the mechanisms with said datasets as input describes the privacy curves of the composed mechanisms under the add and remove relations, respectively. We contrast this good behavior of composed and subsampled mechanisms under add and remove separately with the Laplace mechanism, which, as we will see in Section 5, does not behave well when composed under the combined add/remove relation.

Our dominating pair of datasets can be found by reduction to one of the main results of .

**Theorem 10** (Theorem 11 of ).: _Let \(\) be a randomized mechanism, let \(_{Poisson}\) be the \(\)-subsampled version of the mechanism, and let \(_{WOR}\) be the \(\)-subsampled version of the mechanism on datasets of size \(n\) and \(n-1\) with \(=b/n\)._

1. _If_ \((P,Q)\) _dominates_ \(\) _for add neighbors and_ \(((1-)Q+ P,P)\) _dominates_ \(_{Poisson}\) _for each neighbors._
2. _If_ \((P,Q)\) _dominates_ \(\) _for substitution neighbors then_ \((P,(1-)P+ Q)\) _dominates_ \(_{WOR}\) _for add neighbors and_ \(((1-)P+ Q,P)\) _dominates_ \(_{WOR}\) _for removal neighbors._

In Appendix A we prove that Proposition 9 holds by showing that the hockey-stick divergence between the mechanism with the dominating pairs of datasets matches the upper bound from Theorem 10.

Crucially, Proposition 9 implies that under the add and remove relations, we must add noise with twice the magnitude when sampling without replacement compared to Poisson subsampling! The intuition behind this difference is that the subroutine behaves similarly to the add/remove neighboring relation when using Poisson subsampling, whereas it resembles the substitution neighborhood when sampling without replacement. When \(D^{}_{i}\) is included in the batch another datapoint is 'pushed out' of the batch under sampling without replacement. Due to this parallel one might hope that the difference in privacy parameters between Poisson subsampling and sampling without replacement only differ by a small constant similar to the difference between the add/remove and substitution neighboring relations. That is indeed the case for many parameters, but as we show in Section 7 this assumption unfortunately does not always hold.

## 5 No Worst-case Pair of Datasets under Add/Remove Relation

So far, we have considered the entire privacy curve for all \(\). This is a necessary subtlety for PLD privacy accounting tools under composition (e.g., Theorem 8). Here we focus only on the privacy curve for \( 0\). Our main result of this section is to give a minimal example of a mechanism \(\) that admits a worst-case dataset pair under \(_{A/R}\) yet \(^{k}\) does not admit any worst-case dataset pair for some \(k>1\). This violates an implicit assumption made by some privacy accountants.

**Proposition 11**.: _For some mechanism \(\), the privacy curve of the \(\)-subsampled mechanism \(_{WOR}\) is realized by a pair of datasets under \(_{A/R}\), yet no pair of datasets realizes the privacy curve of \(_{WOR}^{k}\) for all \(k>1\)._

A proof of this proposition for a simple mechanism can be found in Appendix B.1. However, it is more illustrative to demonstrate the proposition informally for the Laplace mechanism \(\). In this case, note that the proposition can be extended to \(_{Poisson}\) as well. The proposition stands in contrast to the case of the add and remove relations discussed in Proposition 9. That is, we can find datasets \(D_{A}D^{}\) such that \(_{_{WOR}^{*}}^{_{A}}\) is realized by \((D,D^{})\) and \(_{_{WOR}}^{_{R}}\) is realized by \((D^{},D)\), but no such (ordered) pair realizes the privacy curve under \(_{A/R}\).

Moreover, it is generally the case that the privacy curve of a subsampled mechanism without composition under \(_{R}\) dominates the privacy curve under \(_{A}\) when \( 0\) (see, e.g., Proposition 30 of  or Theorem 5 of ). Specifically, it follows from Proposition 30 of  that in the case of the subsampled Laplace mechanism and \( 0\), we have that

\[_{_{WOR}}^{_{A/R}}()=_{_{WOR }}^{_{R}}()_{_{WOR}}^{_{A}}( ).\]

Here we visualize the counter-example by plotting privacy curves for the add and remove relation in Figure 1. Note that \(_{_{WOR}}^{_{A/R}}()=\{_{_{WOR}}^{_{A}}(),_{_{WOR}}^{_{R}}( )\}\). Figure 1 shows several variations of the curves \(_{_{WOR}^{*}}^{_{A}}\) and \(_{_{WOR}^{*}}^{_{R}}\), which we estimated numerically by Monte Carlo simulation (as in, e.g., ). Appendix B.2 has the methodological details. These curves are seen to cross in the region \( 0\) for \(k=2\) compositions.

The phenomenon is most apparent for \(k=2\). There is a clear break in the curve for the remove relation. Under many compositions, however, it is known that both PLDs converge to a Gaussian distribution , which explains why this break vanishes as the number of compositions increases.

Avoiding incorrect upper boundsAs shown in this section we cannot assume that the privacy curve for the remove relation dominates the add relation for composed subsampled mechanisms under \(_{A/R}\) even though it is the case without composition. Luckily, this particular issue can be easily resolved by computing the privacy parameters for the add and remove relation separately and taking the maximum. This technique is already used in practice in, e.g., the Google DP library .

We conjecture that this workaround is unnecessary for the Gaussian mechanism--the natural choice for DP-SGD. We searched a wide range of parameters and were unable to produce a counterexample.

**Conjecture 12**.: _Let \(\) be the Gaussian mechanism with any \(\). Then for all \(k>0\), \(\), and \( 0\) we have_

\[_{_{Poisson}^{_{A/R}}}^{_{A/R}}()=_{ _{Poisson}^{_{R}}}^{_{R}}()_{ _{Poisson}^{_{A}}}^{_{A}}().\]

Figure 1: The privacy curves for the subsampled Laplace mechanism under the remove and add neighboring relations respectively.

Comparison of Sampling Schemes

In this section we explore the difference in privacy parameters between Poisson subsampling and sampling without replacement. We focus on the subsampled Gaussian mechanism which is the mechanism of choice for DP-SGD. We show that for some parameters the privacy guarantees of the mechanism differ significantly between the two sampling schemes.

There are several different techniques one might use when selecting privacy-specific hyperparameters for DP-SGD. One approach is to fix the value of \(\) and the number of iterations. Given a sampling rate \(\) and a value for \(\), we can compute the smallest value for the noise multiplier \(\) such that the mechanism satisfies \((,)\)-differential privacy. We use this approach to showcase our findings. We fix \(=10^{-6}\) and the number of iterations to \(10,000\). We then vary the sampling rate between \(10^{-4}\) to \(1\) and use the _PLD_ accountant implemented in the Opacus library [YSS\({}^{+}\)21] to compute \(\).

In Figure 2 we plot the noise multiplier required to achieve \((,10^{-6})\)-DP with Poisson subsampling for \(\{1,2,5,10\}\). For comparison, we plot the noise multiplier that achieves \((10,10^{-6})\)-DP when sampling without replacement. Recall from Section 4 that the noise magnitude required when sampling without replacement is exactly twice that required for Poisson subsampling. The plots are clearly divided into two regions. For large sampling rate, the noise multiplier scales roughly linearly in the sampling rate. However, for sufficiently low sampling rates the noise multiplier decreases much slower. This effect has been observed previously for setting hyperparameters (see Figure 1 of [PHK\({}^{+}\)23] for a similar plot).

Avoiding problematic parametersIt is generally advised to select parameters that fall into the right-hand regime of the plots in Figure 2 [PHK\({}^{+}\)23]. However, one might select parameters close to the transition point. This can be especially problematic if the wrong privacy accountant is used. The transition point happens when \(\) is slightly less than \(1\) for Poisson sampling and therefore it happens when it is slightly less than \(2\) for sampling without replacement. The consequence can be seen for the plot for sampling without replacement in Figure 2. When the sampling rates are high the noise required roughly matches that for \(=5\) with Poisson subsampling. But when the sampling rate is small we have to add more noise than is required for \(=1\) with Poisson subsampling. As such, if we

 \(\) & \(\) (Poisson) & \(\) (WOR) \\  \(10^{-7}\) & 1.19 & 17.48 \\ \(10^{-6}\) & 0.96 & 15.26 \\ \(10^{-5}\) & 0.80 & 12.98 \\ \(10^{-4}\) & 0.64 & 10.62 \\ 

Table 1: The table contrasts the privacy parameter \(\) for the subsampled Gaussian mechanism with \(10,000\) iterations, sampling rate \(=0.001\), and noise multiplier \(=0.8\) for multiple values of \(\).

Figure 2: Plots of the smallest noise multiplier \(\) required to achieve certain privacy parameters for the subsampled Gaussian mechanism with varying sampling rates under add/remove. Each line shows a specific value of \(\) for either Poisson subsampling or sampling without replacement. The parameter \(\) is fixed to \(10^{-6}\) for all lines.

use a privacy accountant for Poisson subsampling and have a target of \(=1\) but our implementation uses sampling without replacement the actual value of \(\) could be above \(10\)! We might hope that this increase would be offset if we allow for some slack in \(\) as well. However, as seen in the table of Figure 1 there can still be a big gap in \(\) between the sampling schemes even when we allow a difference of several orders of magnitude in \(\).

## 7 Substitution Neighboring Relation

In this section, we consider both sampling schemes under the substitution neighboring relation. In their work on computing tight differential privacy guarantees,  considered worst-case distributions for the subsampled Gaussian mechanism under multiple sampling techniques and neighboring relations. In the substitution case, they compute the hockey-stick divergence between \((1-)(0,^{2})+(-1,^{2})\) and \((1-)(0,^{2})+(1,^{2})\). These distributions correspond to running the mechanism with neighboring datasets where all but one entry is \(0\). We first consider Poisson subsampling in the proposition below and later discuss sampling without replacement.

**Proposition 13**.: _Consider the Gaussian mechanism \((x_{1},,x_{n}):=_{i=1}^{n}x_{i}+(0,^{2})\) and let \(_{Poisson}\) be the \(\)-subsampled mechanism. Then \(D:=(0,,0,1)\) and \(D^{}:=(0,,0,-1)\) form a dominating pair of datasets under the substitution neighboring relation._

Proposition 13 simply confirms that the pair of distributions considered by  does indeed give correct guarantees as it is a dominating pair of distributions. However, as far as we are aware, no formal proof existed anywhere. Our proof of the proposition is in Appendix C.

In the rest of the section we focus on sampling without replacement. We start by restating another result from  which we use throughout the section.

**Theorem 14** (Proposition 30 of ).: _If \((P,Q)\) dominates \(\) under substitution for datasets of size \( n\), then under the substitution neighborhood for datasets of size \(n\), we have_

\[()H_{}((1-)Q+ P||P)&  1;\\ H_{}(P||(1-)P+ Q)&0<<1,\]

_where \(()\) is the largest hockey-stick divergence of order \(\) for \(_{WOR}\) on neighboring datasets._

Next, we address a mistake made in related work. We introduced the distributions considered by  for Poisson subsampling above and we show in Proposition 13 that it is a dominating pair of distributions. However,  claimed in their paper that the privacy curves are identical for the two sampling schemes under the substitution relation which is unfortunately incorrect.

They considered datasets where all but one entry has a value of \(0\). This results in correct distributions for Poisson subsampling but for sampling without replacement, we instead consider the datasets \(D:=(-1,,-1,1)\) and \(D^{}:=(-1,,-1,-1)\). With these datasets the values of \(H_{}(_{WOR}(D)||_{WOR}(D^{}))\) and \(H_{}(_{WOR}(D^{})||_{WOR}(D))\) match the cases of the upper bound in Theorem 14 for \( 1\) and \(<1\), respectively. This can be easily verified by following the steps of the proof of Proposition 9 for sampling without replacement.

We can use the datasets above to compute tight privacy guarantees for a single iteration. However, composition is more complicated since neither of the two directions corresponds to a dominating pair of distributions. One might hope that we could simply compute the hockey-stick divergence of the self-composed distributions in both directions and use the maximum similar to the add/remove case. However, for some mechanisms that is not sufficient because we can combine the directions unlike with the add and remove cases. Next we give a minimal counterexample using the Laplace mechanism to showcase this challenge.

We consider datasets of size \(2\) and sample batches with a single element such that \(=0.5\). Let \(x_{1}\) and \(x_{2}\) denote the two data points in \(D\) and without loss of generality assume that \(x_{1}=x_{1}^{}\) and \(x_{2} x_{2}^{}\), where \(x_{1}^{}\) and \(x_{2}^{}\) are the corresponding data points in \(D^{}\). We apply the subsampled Laplace mechanism with a scale of \(2\) and perform \(2\) queries where \(x_{1}\) has the value \(-1\) for both queries. Let \(P:=0.5(-1,2)+0.5(1,2)\) and \(Q:=(-1,2)\). That is, \(P\) and \(Q\) are the distributions for running one query of \(_{WOR}(D)\) with \(x_{2}\) having value \(1\) or \(-1\), respectively. Then \(H_{e^{}}(P P||Q Q)\) is the hockey-stick divergence for the mechanism if \(x_{2}\) has value \(1\) for both queries and \(x_{2}^{}\) has value \(-1\) for both queries. Similarly, \(H_{^{}}(Q Q||P P)\) is the divergence when \(x_{2}\) has value \(-1\) for both queries and \(x_{2}^{}\) has value \(1\) for both queries.

The two hockey-stick divergences above are similar to those for the remove and add neighboring relations. However, we also have to consider \(H_{^{}}(P Q||P Q)\) in the case of substitution. These distributions correspond to the case when \(x_{2}\) has a value of \(1\) for the first query and \(-1\) for the second query, and \(x_{2}^{}\) has a value of \(-1\) for the first query and \(1\) for the second query. Figure 3 shows the hockey-stick divergence as a function of \(\) for the three pairs of neighboring datasets. The largest divergence depends on the value of \(\) with all three divergences being the maximum for some interval. This counterexample shows that we cannot upper bound the hockey-stick divergence for the subsampled Laplace mechanism as \(\{H_{^{}}(P^{k}||Q^{k}),H_{^{} }(Q^{k}||P^{k})\}\) for \(k>1\). For \(k\) compositions, we have to consider \(k+1\) ways of combining \(P\) and \(Q\). This significantly slows down the accountants in contrast to the \(2\) cases required for add/remove. Worse still, we do not have a proof that one of \(k+1\) cases is the worst-case pair of datasets for all \( 0\).

In Appendix D we use an alternative technique for bounding the privacy curve under the substitution relation based on . We show that this accountant does not generally outperform the RDP accountant. This demonstrates the need to strengthen the theory for sampling without replacement under the substitution relation for the purposes of tight privacy accounting.

## 8 Discussion

We have highlighted two issues that arise in the practice of privacy accounting.

First, we have given a concrete example where the worst-case dataset (for \( 0\)) of a subsampled mechanism fails to be a worst-case dataset once that mechanism is composed. Care should therefore be taken to ensure that the privacy accountant computes privacy guarantees with respect to a true worst-case dataset for a given choice of \(\).

Secondly, we have shown that the privacy parameters for a subsampled and composed mechanism can differ significantly for different subsampling schemes. This can be problematic if the privacy accountant is assuming a different subsampling procedure from the one actually employed. We have shown this in the case of Poisson sampling and sampling without replacement but the phenomenon is likely to occur when comparing Poisson sampling to shuffling as well. Computing tight privacy guarantees for the shuffled Gaussian mechanism remains an important open problem. It is best practice to ensure that the implemented subsampling method matches the accounting method. When this is not practical, the discrepancy should be disclosed.

We conclude with two recommendations for practitioners applying privacy accounting in the DP-SGD setting. We recommend disclosing the privacy accounting hyperparameters for the sake of reproducibility (see Section 5.3.3 of  for a list of suggestions). Finally, we also recommend that, when comparisons are made between DP-SGD mechanisms, the privacy accounting for both should be re-run for the sake of fairness.

Figure 3: Hockey-stick divergence of the Laplace mechanism when sampling without replacement under \(_{S}\). The worst-case pair of datasets depends on the value of \(\).