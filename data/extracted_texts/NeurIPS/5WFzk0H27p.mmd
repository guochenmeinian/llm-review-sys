# The Tourresol dataset:

Which videos should be more largely recommended?

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces the Tourresol public dataset, which was collected as part of the online deployed platform https://tourresol.app. Our dataset contains a list of 204,000 comparative judgments made by Tourresol's 20,000 users on which YouTube videos should be more largely recommended. It also provides 703,000 comparisons along secondary criteria like content reliability, topic importance and layman-friendliness. The dataset also exports information about users' pretrust statuses and vouches. It is published at https://api.tournesol.app/exports/all under ODC-By license. The data is currently used by Tourneosol to make community-driven video content recommendations to over 10,000 users.

## 1 Introduction

Recommendation algorithms have become extremely influential. In the last few years, beyond their impacts on mental health [54; 19; 91], because they amplify disinformation, cyberbullying and hate, they have been linked to major geopolitical events, including COVID disinformation [78; 43], the rise of far-right parties [90; 89; 94], and the Rohingya genocides [39; 71]. Crucially, in all these examples, the victims of recommendation algorithms are not only their users; hate amplification is threatening entire populations, even when these populations do not use recommendation algorithms themselves. This is in sharp contrast with the overwhelming majority of the scientific literature, which assumes that recommendation algorithms should be optimized for their users only [1; 69].

As online activities grew, social media have _de facto_ taken the role that was traditionally played by these intermediate bodies [88; 47]. This became particularly striking when, in 2020, the then US President was banned from Twitter, Facebook, and Youtube, long before any court sued him for inciting the Capitol riot violence [64; 65]. As another example, by amplifying the cyberbullying of climate scientists, Twitter provoked their exodus from the platform , thereby turning climate change into a _mute news_, which is endangering plenty of non-users . The great replacement of the intermediate body by privately owned algorithms has been tied to an alarming decline of democratic norms worldwide, as many reports expose a global trend of autocorrelation [70; 7].

So how do today's large-scale recommendation algorithms address the ethical dilemmas that they face billions of times per day, when they are tasked with amplifying some (potentially hateful) content over others (of potential public interest)? Currently, they heavily rely on (highly sophisticated) _machine learning_[23; 61]. In other words, such algorithms leverage massive amounts of data to determine which content they will promote at scale. However, as an immediate corollary, such algorithms are exposed to _manipulation_ by _poisoning_ data . In fact, this poisoning has been industrialized, not only by authoritarian states [18; 45], but also by private companies based in the UK , Spain , Israel , France  and Switzerland . The magnitude of this industry is well captured by one puzzling statistic: Facebook reportedly removes around _7 billion fake accounts per year_.

While a recent line of research has provided numerous poisoning mitigations , it is also known that there are fundamental impossibility theorems that prevent accurate learning in highly adversarial, heterogeneous and high-dimensional settings . In particular, there is no substitute for training datasets of high quality and security. In particular, to design trustworthy ethical algorithms, it is essential to train them on large, secured and trustworthy datasets of human ethical judgments. In this paper, we present the _Tournesol public dataset_, whose goal is to remedy the current state of affairs. More precisely we make the following contributions.

Contributions.Our main contribution is to present and share the _Tournesol public dataset_, which can be downloaded directly from https://api.tournesol.app/exports/all. The dataset consists of over 204,000 pairwise comparisons of the recommendation of over 40,000 YouTube video by over 20,000 Tournesol accounts. Additionally, the dataset contains over 703,000 pairwise comparisons of the videos' quality on secondary criteria, such as reliability, importance and layman-friendliness. Our dataset, published under ODC-By license, also contains pretrust information about contributors, vouches between contributors, as well as scores computed from the data using Solidago. Crucially, the dataset was collected in a fully deployed environment with actual stakes, as Tournesol eventually makes recommendations based on the provided data to over 10,000 users.

The paper also presents an analysis of our dataset, with valuable insights for the ethics of content recommendation. One finding is that the topic importance highly matters in Tournesol's contributors' judgments. While caveats apply, this suggests that the attention to "fake news" may be misguided; in fact, the disinformation industry often proceeds _without_ producing false information, e.g. by overclaiming positive impacts, shifting blame or bullying critics . Prioritizing greater exposure to _mute news_ might be more urgent. Our analysis also highlights the need of psychological-based preference learning models, as we expose biases and variations in contributors' judgments.

Finally, our paper discusses numerous exciting research directions that our public dataset could inspire or facilitate. In particular, we believe that a lot more focus should be given to secure learning under poisoning attacks, but also to _Proof of Personhood_, _expertise validation_, _vollion learning_, _active learning_ and _resilient collaborative filtering_, among others.

Literature review.Tournesol presents a new contribution to the growing field of AI alignment with human values , which aims to teach human preferences to algorithms, and to design systems that maximize what humans prefer to maximize . Clearly, this requires finding out about humans' judgments on how algorithms ought to behave. Unfortunately, so far, to the best of our knowledge and especially for the important case of recommendation algorithms, there have not been many secure, public and free-license datasets with such AI-safety-critical data.

To collect such data in a realistic setting, Tournesol's dataset draws inspiration from several previous AI ethics solutions, which leveraged _collaborative governance_ to address cases of conflictual human judgments. In particular,  introduced WeBuildAI, a framework where stakeholders of a food donation system could weigh in on the identity of the recipient of a donation. One challenge is that such decisions must be made every day; but stakeholders are not available every time a decision needs to be made. To account for their preferences, WeBuildAI asks stakeholders to either write down an algorithm that describes their preferences, or to provide judgments on generated food donation dilemmas. In the latter case, a learning model is then used to infer how the stakeholders would likely assess other dilemmas. In any case, an _algorithmic representative_ is thereby constructed for each stakeholder, and the resulting decision will follow from a vote of the algorithmic representatives. Similar approaches were proposed for kidney donation  and for the "trolley dilemmas"  that autonomous cars could one day face .

Perhaps most similar to our approach are Twitter's _Community Notes_, whose governance is intended to be fully community-driven. More specifically, the system allows a community of contributors to add a note to misleading tweets, e.g. to correct misinformation or to add context to prevent confusion. The contributors cannot only propose the note; they are also asked to assess other contributors' notes. Notes that are judged helpful by a sufficiently large and diverse set of contributors are then published by the platform. The system is very transparent, and provides a lot of freely accessible data on human judgments1.

## 2 The dataset

In this section, we describe our main contribution, namely the release of a new, scalable, secured and trustworthy database of reliable human judgments.

### Raw data

Pretrust.To guarantee the security of our data, Tournesol aims to verify that every account is owned and controlled by a human, and that this human only owns and controls this single account on the platform. In other words, Tournesol aims to obtain a _Proof of Personhood_ to verify each active Tournesol account, and to thereby prevent _Sybil attacks_. Unfortunately, there is currently no reliable and scalable solution for _Proof of Personhood_.

Today's main solution is _email certification_. More precisely, when they create a Tournesol account, contributors are asked to validate, if possible, an email address from a trusted email domain. The list of trusted email domains is currently managed manually. An email domain will be considered trusted if it seems sufficiently unlikely that a large number of fake accounts can be created from this domain.

This excludes domains like @gmail.com and personal domains like @my-personal-website.com. The concern is not only that the domain will maliciously create a large number of fake accounts; it is also that they may be hacked by a malicious entity that will create such fake accounts. The list of trusted email domains is available at https://tournesol.app/email_domains. It includes domains like @epfl.ch, @who.int and @rsf.org. 703 contributors are thereby authenticated.

Evidently, however, this solution is still highly imperfect. On one hand, this does not guarantee the absence of fake accounts. On the other hand, and perhaps more importantly, this excludes most potential contributors from participating.

Vouching mechanism.To propagate trust to more accounts, Tournesol also proposes a vouching mechanism. Namely, any account can vouch for the authenticity of another account. More precisely, the account must vouch that the other account is used by a human who is not using any other account on the platform. The dataset contains 129 vouches.

Comparison-based judgments.Following a large literature on the topic [38; 17; 66; 10; 73; 60; 42], Tournesol relies on a comparison-based preference elicitation system. We believe that the need to distinguish among top content which should be more recommended makes this system more suitable than, e.g., using direct assessments [63; 2; 55; 85], which may yield too many "saturated" maximal assessments. Additionally, comparisons are labelled with the week in which the comparison was first submitted. This allows potentially observing changes or drifts in the contributors' judgments.

Figure 1 (left) presents the video comparison interface. Namely, contributors are asked to select two videos, and to tell Tournesol which one of the videos should be recommended at scale. Moreover, rather than a binary decision, the contributor is asked to provide the judgment by moving a slider on a more continuous scale, from \(-10\) to \(10\), The value \(-10\) means that the contributor would prefer Tournesol to recommend the left video vastly more often than the right videos, while the value \(0\) means that they believe both videos should be recommended equally often.

Quality criteria.Tournesol allows contributors to rate nine other _optional_ quality criteria (Figure 1)

* **Reliable and not misleading:** Is the presented information trustworthy, robustly backed and properly nuanced?
* **Clear and pedagogical:** How efficiently does the content guide viewers in their understanding?
* **Important and actionable:** Can additional focus on this topic have a significantly positive impact on the world?
* **Layman-friendly:** How understandable is it, without prior knowledge?* **Entertaining and relaxing:** Do people feel good watching it?
* **Engaging and thought-provoking:** Does it catch people's attention, spark curiosity and invite to question previous beliefs?
* **Diversity and inclusion:** Does it promote tolerance, compassion and wider moral considerations?
* **Encourages better habits:** Does it make people adopt habits that benefit themselves and beyond?
* **Resilience to backfiring risks:** Is it adapted to viewers with opposing beliefs? Does it prevent misconceptions or undesirable reactions?

While the criteria are further provided on Tournesol2, most contributors have surely _not_ read thoroughly our descriptions. Arguably, they will more likely judge these criteria according to their own understanding, which will be mostly based on the name of the criteria.

### Processed data

In addition to the raw data presented thus far, the Tournesol public dataset exports processed data. The processing is performed by a pipeline called Solidago.

Solidago.The pipeline has six modules. First, pretrust and vouches are used to assign _trust scores_ to all users. Second, _voting rights_ are assigned to the different users, in a way that includes untrusted users, while guaranteeing that they cannot outweigh trusted users. Third, for each criterion and each user, the comparisons are turned into the user's _raw scores_, using the generalized Bradley-Terry model . Fourth, raw scores are _scaled_, using Mehestan , zero-shift and standardization. Fifth, scaled scores are securely aggregated into _global scores_, using the Lipschitz-resilient quadratically regularized quantile . Sixth, all scores are squashed into \((-100,100)\), using the map \(t 100t/}\). All along, left and right uncertainties on all variables are computed.

Exported values.Trust scores, squashed individual scores and squashed global scores are provided in the public dataset.

Results.Figure 2 lists the most recommendable videos, according to Tournesol's contributors, as they are displayed on the website.

Figure 1: The interface through which contributors are asked to provide judgments. The judgments are comparisons of video contents using a slider along the main criteria ”should be largely recommended” (left) and optional quality criteria (right).

### Privacy

Overall, we encourage transparency in our contributors, as we believe that this will foster important research on human judgments, and help make safer and more ethical algorithms. However, we acknowledge that, because of social and political pressures, some judgments are dangerous to make public, e.g. when criticizing one's own employer or government. This is why we allow contributors to provide data publicly or privately. More precisely, each contributor can select the privacy setting of any video they rate. If a video is rated privately, then all its comparisons to any other video will be recorded privately. Only Tournesol's server can access to such data. Conversely, all comparisons that involve two publicly rated videos are exported in the Tournesol public dataset.

### Data collection context

The contributors to Tournesol receive no financial compensation. Their contributions are mostly motivated by the desire to contribute to a democratic AI governance project, and by the will to promote content of public interest. Their recruitment is thus organic, and mostly depends on how frequently they were exposed to the promotion of the Tournesol project. Evidently, this greatly correlates with Tournesol's communication, which has been heavily supported by the (French-speaking) YouTube channel Science4All, and by other science communicators . As a result, the set of contributors is in no way representative of the global population. Namely, it is heavily biased towards science enthusiasts. Nevertheless, we believe that the data provided by this community should be of great interest to AI alignment, at least on topics with a significant scientific component.

## 3 Data analysis

This section presents some data analyses to provide insights in the _Tournesol public dataset_.

### Contributors' contributions

Figure 3 displays the number of contributions per user. Perhaps unsurprisingly, this statistics is heavy-tailed; in fact, it seems to fit Zipf's law , with a few contributors providing most of the comparisons, and most of them providing very few. Figure 4 plots the activity through time: Tournesol has 100 to 200 weekly active users, while the number of monthly active users fluctuates between 200 and 900.

Figure 3: Number of comparisons provided by the different contributors, on a log-log scale, which is typical of Zipf’s law .

Figure 2: Best videos (left), best English-speaking videos (middle) and best videos along the criterion “diversity & inclusivity” (right).

### Video and contributor connectivity

For scores to be meaningful, the contributors must have compared sufficiently many videos in common . The contributor comparability graph has a connected component with 7187 contributors and diameter of 6, out of the 7,826 contributors that have compared at least 2 videos. The graph has 208,323 edges out of 30,619,225 possible (\(0.68\%\)) making it very sparse. But for the induced graph of the top 100 most active contributors with a trust at least 0.1 (which correspond to _scaling-calibration_ contributors ), 3,442 (\(69.5\%\)) pairs of contributors are comparable. This justifies the restriction of scaling calibration to the most active contributors.

Figure 5 details video comparisons for some highly active users. Interestingly, because the platform lets contributors to select their videos to compare, we observe a wide variety of comparison graphs. This raises open questions about the uncertainties of the resulting learned scores , and about the possibility to improve accuracy through _active learning_[67; 83].

### Correlations between criteria

Figure 6 reports the correlations between quality criteria, in contributors' comparative judgments. Perhaps most remarkably, we observe that the criterion that best predicts whether a video "should be more largely recommended" is whether it is "important and actionable". This finding highlights the need to pay greater attention to _information prioritization_, and especially combatting "_mute news_" . In particular, there may be an excess of attention to "_fake news_". In fact,  expose numerous strategies from the "merchants of doubts" that do not involve producing false information, such as shifting blame, cyberbullying critics or "striking a positive tone" .

Figure 6 also shows that most criteria are only weakly correlated. Two notable exceptions are"important and actionable" and "encourage better habits", and "reliable and not misleading" and "clear and pedagogical", which could be argued to be slightly redundant.

Note also that, as expected given Berkson's paradox , the correlations decrease if we only consider the top 10% videos on Tournesol (i.e. those that are more likely to be recommended).

Figure 4: Contributors’ participation through time.

Figure 5: Graphs of video comparisons for different users

### Distributions of reported comparisons

As it is not formally defined how contributors should rate a pair of videos, we expected many different expression styles. We ran a clustering algorithm (K-means) on statistics of the distribution of comparison values for each user. Figure 7 shows the typical distribution of comparison values of each of the eight clusters we identified. While some contributors provided comparisons close to "recommend equally" (cluster 3 and 4), others' comparisons were systematically towards the extreme (clusters 2, 5 and 6). This suggests that the discrepancies between their individual scores will be due to their expression style, rather than actual differences in their judgments, which justifies the research on mitigating the heterogeneity in expression styles [53; 93; 4].

### Psychological biases in contributors' judgments

our dataset exposes psychological biases in contributors' judgments. One example is a instinctive desire to over-recommend a recently watched high-quality video, known as the _recency bias_, which is depicted by Figure 7(a). Namely, this figure plots all comparisons on the main criterion that correspond to a contributor evaluating a given video for the first time (negative scores correspond

Figure 6: Correlations between quality criteria

Figure 7: Example centroids of 8 clusters obtained by the K-means algorithm applied to the distributions of comparison values for each contributor with at least 20 comparisons.

to the newly scored videos). The 95% confidence interval for the mean of first-time comparisons is \([-0.40,-0.32]\), which is arguably a surprisingly significant bias.

Another bias we observe is a tendency to favor left videos. The 95% confidence interval for the mean of the main-criterion comparisons (Figure 7(b)) is \([-0.49,-0.44]\). Considering all criteria (Figure 7(c)) yields a smaller bias, with a corresponding 95% confidence interval of \([-0.17,-0.15]\). This suggests that reflecting on more criteria reduces the left-video bias. And indeed, when they are accompanied with comparisons on other criteria, the main-criterion comparisons have a 95% confidence interval for the mean equal to \([-0.38,-0.31]\), as opposed to \([-0.57,0.52]\) for main-criterion-only comparisons. We also observe that pretrusted contributors have a significantly reduced left-video bias (on all criteria, \([-0.03,-0.002]\) for pretrusted, \([-0.34,-0.31]\) for unpretrusted).

### Distribution of scores

Unsquashed scores (essentially, as outputs of the generalized Bradley-Terry model on contributors' comparisons) are extremely heavy tailed. Indeed, out of 634516 scores, 2803 deviate by more than 5 standard deviations. This is to be contrasted with the expected number \(0.18\) of such extreme scores, assuming a normal distribution of the scores. In fact, 428 scores deviate by more than 10 standard deviations. This observation justifies the use of comparisons to quantify the potential large deviations between top alternatives, which direct scoring approaches might fail to account for appropriately, as well as of a (robustified) quantile to standardize scores .

## 4 Research challenges

Tournesol raises numerous fascinating research challenges. Below, we sketch some of these.

Aggregate the different criteria into a score.We expect the combination of many different quality criteria to yield a more reliable judgment of what content ought to be recommended at scale, or to a given specific user. However, the appropriate aggregation of our different quality criteria is still unclear, especially given probable nonlinear phenomena. How best to do this should be investigated.

Debiais the contributing population.Like in many online participatory projects , we expect huge participation imbalances. Leveraging demographic data to debias the Tournesol recommendations, e.g., by giving more voting rights to individuals from underrepresented communities, could help, but it will require both (safely) collecting personal data and building new (secure) algorithms, akin to those used by the _Community Notes3_ or by _Pol.is4_.

Volition.As Section 3.5 highlighted it, we cannot expect the Tournesol database to contain fully reliable human judgments. Many comparisons have surely been provided by contributors, at moments when they were not paying the utmost attention to all the possible ramifications and unwanted side

Figure 8: Recency and left-video biases in contributors’ judgments.

Figure 9: Distribution of un-squashed scores, with logarithmic y-scale.

effects of promoting a video at scale. In particular, some judgments will arguably be more reliable than others. Such more reliable judgments are sometimes called _volitions_, rather than _preferences_. There is a need for algorithms that model human psychology to distinguish these two [50; 59].

Privacy.Toumesol's current algorithms do not provide any _differential privacy_. Future research should also investigate how to strengthen privacy without harming too much the quality and the security of the Tournesol scores. Perhaps most importantly, ideally, Tournesol's servers would be able to leverage private comparisons to score videos without being a single point of failure for private data protection. _Secure multi-party computations_ could be a promising venue to do so .

Decentralize Tournesol.A longer-term goal is to fully decentralize Tournesol. In this vision, the data would no longer be stored on Tournesol's server, but would be replicated appropriately on a large number of contributors' devices. Moreover, the computations of Tournesol scores should also be decentralized, while guaranteeing _Byzantine resilience_. Recent research in fully decentralized Byzantine learning has provided the building blocks of such a decentralization [29; 35], but more research is needed to understand how to best do so in the context of Tournesol.

Preference generalization.Right now, contributors are only voting on the videos that they explicitly compared. However, if they consistently voted positively all the videos of a given channel, then we could guess that they would have voted positively a new video from this channel, and to include their likely vote even when they did not compare the new video. Evidently, additional information can be leveraged to make such generalizations, such as the other video features (description, transcript, length), and the other contributors' judgments (using collaborative filtering ). Note however that generalization increases vulnerability risks. A careful security analysis would be required .

Language model alignment.Tournesol's database could help align language models, e.g. through _reinforcement learning with Tournesol feedback_[21; 76]. Determining how to combine large language models  with Tournesol's database to design safer models is an exciting venue for future work.

Leverage expertise.On technical topics like vaccination or climate change, especially when misconceptions are widespread in the general population, it seems desirable to assign more voting rights to experts, especially when judging the reliability of content within their domains of expertise. This issue is intimately connected to Condorcet's jury problem [22; 72].

Proof of Personhood with zero knowledge.Combating fake accounts arguably remains the top priority to secure participatory systems. To address this, at least in democratic countries and in the short term, the state could be tasked with delivering _Proofs of Personhood_[16; 41], if possible in a zero-knowledge manner. More precisely, any citizen should ideally be able to provide to any platform a proof of citizenship, which does not enable neither the platform nor the state to identify which account is owned by which citizen. We believe that designing such a system could have applications beyond the particular case of Tournesol. Indeed, we could demand that social media only display the number of likes from users with a delivered proof of citizenship, and that their recommendation algorithms be trained only by such certified users' data.

Liquid democracyFinally, future work could investigate the extent to which a liquid democracy  could be set up on plateforms like Tournesol. Such a system through which a contributor can delegate their votes to other voters could help combat activity bias (i.e. better accounting for inactive contributors) and expertise (if voters delegate to more competent contributors). While philosophically appealing, the security of such a system should however be first investigated .

## 5 Conclusion

This paper introduced the _Tournesol public dataset_, which is a large, secured and trustworthy database of reliable human judgments. We detailed its construction, and provided an analysis of its content. We believe that this database can help stimulate and facilitate research and development on ethical algorithms, and could eventually help improve the informational diet of billions of people for the better. Given the current information crisis, we regard this as an "important and actionable" contribution.