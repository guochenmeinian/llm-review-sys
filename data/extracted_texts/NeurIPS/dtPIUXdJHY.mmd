# Generalization Analysis for Label-Specific Representation Learning

Yi-Fan Zhang\({}^{1,3}\), Min-Ling Zhang\({}^{2,3}\)

\({}^{1}\) School of Cyber Science and Engineering, Southeast University, Nanjing 210096, China

\({}^{2}\) School of Computer Science and Engineering, Southeast University, Nanjing 210096, China

\({}^{3}\) Key Laboratory of Computer Network and Information Integration (Southeast University),

Ministry of Education, China

{yfzh, zhangml}@seu.edu.cn

Corresponding author

###### Abstract

Label-specific representation learning (LSRL), i.e., constructing the representation with specific discriminative properties for each class label, is an effective strategy to improve the performance of multi-label learning. However, the generalization analysis of LSRL is still in its infancy. The existing theory bounds for multi-label learning, which preserve the coupling among different components, are invalid for LSRL. In an attempt to overcome this challenge and make up for the gap in the generalization theory of LSRL, we develop a novel vector-contraction inequality and derive the generalization bound for general function class of LSRL with a weaker dependency on the number of labels than the state of the art. In addition, we derive generalization bounds for typical LSRL methods, and these theoretical results reveal the impact of different label-specific representations on generalization analysis. The mild bounds without strong assumptions explain the good generalization ability of LSRL.

## 1 Introduction

Multi-label learning has received continued attention in machine learning community due to its widespread encounters in real-world applications, where each object is represented by a single instance associated with multiple class labels . The goal of multi-label learning is to model real-world objects with multiple semantics. It has made important advances in multimedia content annotation , text categorization , bioinformatics  and other fields . The failure to take into account that each class label may possess its own discriminative properties results in the most straightforward strategy which exploits the identical representation of an instance for dealing with multi-label data being perhaps suboptimal . In recent years, label-specific representation learning (LSRL)  has been proposed to facilitate the discrimination of each class label by tailoring its own representations. Due to its ability to model distinct characteristics for each class label, LSRL has become an effective strategy to improve the performance of multi-label learning . Although LSRL has achieved impressive empirical advances in multi-label learning, the problem of understanding LSRL theoretically remains completely under-explored.

In recent years, efforts to explain why multi-label models generalize well is an important open problem in multi-label learning community. The empirical success of LSRL makes the generalizationanalysis of LSRL an important problem in multi-label learning. However, existing theoretical results and analysis methods are not applicable to LSRL, which leads to a serious lack of progress in its generalization analysis. A satisfactory and complete study of the generalization analysis for LSRL should include two aspects: 1) the dependency of the generalization bounds on the number of labels (i.e., \(c\)) should be weaker than square-root, and 2) the relationship among components should be decoupled in the generalizability analysis. First, existing bounds with a linear or square-root dependency on \(c\) are difficult to explain empirical success of multi-label learning. For example, the bounds with a linear dependency on \(c\) are vacuous (i.e., no longer less than 1) for commonly used multi-label datasets such as CAL500, core15k, rcv1-s1, Core116k-s1, delicious, iaprtc12, espgame, etc., since \(c\) is larger than \(\) (the number of examples) for these datasets. The bounds with a linear or square-root dependency on \(c\) are vacuous for commonly used extreme multi-label datasets such as Wiki 10, Amazon-670K, etc., since \(c\) is larger than \(n\). The above failure of existing bounds for multi-label learning also applies to LSRL. Hence, this suggests that only the bounds with weaker dependency on \(c\) can provide effective theoretical guarantees. Second, existing theoretical results preserve the coupling among different components (Lei et al., 2015; Wu and Zhu, 2020; Wu et al., 2021). However, LSRL decomposes the multi-label learning problem into multiple binary classification problems, which means that the relationship among different components needs to be decoupled in the generalization analysis. Hence, we need to develop new analysis methods for LSRL. As a matter of fact, theoretical research on LSRL can also promote a better understanding of multi-label learning.

In this paper, we derive novel and tighter bounds based on the Rademacher complexity for LSRL. Specifically, we develop a novel vector-contraction inequality with the assumption that the loss function is Lipschitz continuous w.r.t. the \(_{}\) norm, then we derive the bound for general function classes of LSRL with no dependency on \(c\), up to logarithmic terms, which is tighter than the state of the art. In addition, we also analyze the bounds for several typical LSRL methods, and we show that the construction method of label-specific representations will affect the generalization bound.

Our bounds for LSRL improve the dependency on \(c\). Major contributions of the paper include:

* We develop a novel vector-contraction inequality for \(_{}\) norm Lipschitz continuous loss, which overcomes the limitations of existing theoretical results and provides a theoretical tool for the generalization analysis of LSRL.
* We derive bounds for general function classes of LSRL with a weaker dependency on \(c\) than the state of the art, which provides a general theoretical guarantee for LSRL.
* We derive bounds for typical LSRL methods, which reveals the impact of different label-specific representations on the generalization analysis. The theoretical techniques and results on \(k\)-means clustering, Lasso, and DNNs involved here may be of independent interest.

## 2 Related Work

### Multi-Label Learning

Multi-label learning is one of the most studied and important machine learning paradigms in practice (Zhang and Zhou, 2014; Liu et al., 2022). To cope with the challenge that the output space is exponential in size to the number of class labels, modeling label correlations is adopted as a feasible strategy to facilitate the learning process. Generally speaking, existing methods can be roughly grouped into three major categories based on the order of label correlations being considered, namely first-order methods (Boutell et al., 2004; Zhang and Zhou, 2007; Zhang et al., 2018) which tackle multi-label learning problem by decomposing it into a number of independent binary classification problems, second-order methods (Elisseeff and Weston, 2001; Zhu et al., 2018; Sun and Zhang, 2021) which tackle multi-label learning problem by considering pairwise relationships between labels, and high-order methods (Ji et al., 2010; Xu and Guo, 2021) which tackle multi-label learning problem by exploiting high-order relationships among labels. Recent years, benefiting from the good generalization performance of deep learning, deep methods such as recurrent neural networks (Yazici et al., 2020), graph neural networks(Chen et al., 2020), and embedding models (Bai et al., 2020; Daiya et al., 2021) have been explored to model label correlations.

As a complement to the exploitation of label correlations, label-specific representation learning (LSRL) have been proven to be another effective strategy to improve multi-label learning by manipulating the input space. Existing methods can be roughly grouped into three major categoriesbased on the construction method of label-specific representations, i.e., prototype-based label-specific representation transformation methods (Zhang and Wu, 2015; Zhang et al., 2015; Weng et al., 2018; Guo et al., 2019; Zhang and Li, 2021) which generate label-specific representations by treating the prototypes of each class label as the transformation bases, label-specific feature selection methods (Huang et al., 2015, 2016, 2018; Weng et al., 2020; Yu and Zhang, 2022) which generate label-specific representations by retaining a feature subset as the most pertinent features for each class label, and deep label-specific representation methods (Hang and Zhang, 2022; Hang et al., 2022) which learn label-specific representations in an end-to-end manner by exploiting deep models. In particular, \(k\)-means clustering-based LSRL, i.e., LIFT (Zhang and Wu, 2015), is the representative method of prototype-based label-specific representation transformation methods, which constructs label-specific representations by querying the distances between the original inputs and the cluster centers for each class label. Lasso-based LSRL, i.e., LLSF (Huang et al., 2016), is the representative method of label-specific feature selection methods, which presents a Lasso-based framework with the constraint of pairwise label correlations for each class label. DNN-based LSRL, i.e., CLIF (Hang and Zhang, 2022), is the representative method of deep label-specific representation methods, which proposes to learn label semantics and label-specific representations in a collaborative way. In this paper, we will analyze the generalization bounds of these three representative LSRL methods.

### Generalization Bounds for Multi-Label Learning

Dembczynski et al. (2010) derived the relationship between the expectations of Hamming and Subset loss based on the regret analysis on Hamming and Subset loss, and Dembczynski et al. (2012) further performed regret analysis on Ranking loss, which provided preliminary theoretical insights for understanding Hamming, Subset and Ranking loss. With the typical vector-contraction inequality (Maurer, 2016) (i.e., assume that the loss function is Lipschitz continuous w.r.t. the \(_{2}\) norm), one can obtain generalization bounds of order \(O(c/)\) for multi-label learning. Wu and Zhu (2020); Wu et al. (2021) showed that the order of the generalization bounds for Subset loss, Hamming Loss and reweighted convex surrogate univariate loss can be improved to \(O()\), which preserved the coupling among different components and exploited the relationship between loss functions. Liu et al. (2018) also obtained a generalization bound of order \(O()\) for the dual set multi-label learning, which was analyzed under the margin loss and kernel function classes.

Wu et al. (2021) derived a generalization bound of order \(O((nc)/n)\) for norm regularized kernel function classes with the assumptions that the loss function is Lipschitz continuous w.r.t. the \(_{}\) norm and the regularizer is \(\)-strongly convex with respect to some norms. Yu et al. (2014) obtained a generalization bound of order \(O(1/)\) for trace norm regularized linear function classes with Decomposable loss. Xu et al. (2016) used the local Rademacher complexity to derive a generalization bound of order \((1/n)\) for trace norm regularized linear function classes with the assumption that the singular values of the weight matrix decay exponentially. These theoretical results all preserved the coupling among different components. In addition, Wu et al. (2023) obtained \(O(1/)\) bounds for Macro-Averaged AUC and gave thorough discussions about its relationships with the label-wise class imbalance, which transformed the macro-averaged maximization problem in multi-label learning into the problem of learning multiple tasks with graph-dependent examples. Here we obtain \((1/)\) bounds with the state-of-the-art dependency on the number of labels for LSRL function classes under the assumption that the loss function is Lipschitz continuous w.r.t. the \(_{}\) norm.

## 3 Preliminaries

Let \([n]:=\{1,,n\}\) for any natural number \(n\). In the context of multi-label learning, given a dataset \(D=\{(_{1},_{1})\,,,(_{n},_{n})\}\) with \(n\) examples which are identically and independently distributed (i.i.d.) from a probability distribution \(P\) on \(\), where \(^{d}\) denotes the \(d\)-dimensional input space and \(\) denotes the label space with \(c\) class labels, \(\), \(\{-1,+1\}^{c}\), i.e., each \(=(y_{1},,y_{c})\) is a binary vector and \(y_{j}=1\) (\(y_{j}=-1\)) denotes that the \(j\)-th label is relevant (irrelevant), \(j[c]\). The task of multi-label learning is to learn a multi-label classifier \(:\{-1,+1\}^{c}\) which assigns each instance with a set of relevant labels. A common strategy is to learn a vector-valued function \(=(f_{1},,f_{c}):^{c}\) and derive the classifier by a thresholding function which divides the label space into relevant and irrelevant label sets.

For any vector-valued function \(:^{c}\), the prediction quality on the example \((,)\) is measured by a loss function \(:^{c}\{-1,+1\}^{c}_{+}\). The goal of learning is to find a hypothesis \(\) with good generalization performance from the dataset \(D\) by optimizing the loss \(\). The generalization performance is measured by the expected risk: \(R()=_{(,) P}[((),)]\). We denote the empirical risk w.r.t. the training dataset \(D\) as \(_{D}()=_{i=1}^{n}((_{i}), _{i})\). We denote the optimal risk as \(R^{*}=_{}R()\), the minimizer of the optimal risk as \(^{*}=_{}R()\) and denote the minimizer of the empirical risk as \(}^{*}=_{}_{D}()\). In addition, we define the loss function space as \(=\{((),):\}\), where \(\) is the vector-valued function class.

### Label-Specific Representation Learning

Label-specific representation learning aims to construct the representation with specific discriminative properties for each class label to facilitate its discrimination process. The basic idea of LSRL is to decompose the multi-label learning problem into \(c\) binary classification problems, i.e., decoupling the relationship among different components, where each binary classification problem corresponds to a possible label in the label space. We consider the prediction function for each label of the general form \(f_{j}()=_{j},_{j}(_{j}())\), where the inner nonlinear mapping \(_{j}\) corresponds to the nonlinear transformation induced by the construction method of label-specific representation, while the outer nonlinear mapping \(_{j}\) refers to the nonlinear mapping corresponding to the classifier learned on the generated label-specific representation. We define a vector-valued function class of LSRL as follows:

\[=\{(): ()=(f_{1}(),,f_{c}()),\] \[f_{j}()=h_{j}(_{j}())=_{j}^{ }_{j}(_{j}()),=(_{1},,_{c}) ^{d c},\] \[(),(_{j}()) A,( _{j}()) C,,j[c],,A,C>0\},\] (1)

where \(\) represents a functional that constrains weights, \(\) represents a functional that constrains nonlinear mappings \(_{j}\), \(\) represents a functional that constrains nonlinear mappings \(_{j}\).

### Related Evaluation Metrics

A number of evaluation metrics are proposed to measure the generalization performance of different multi-label learning methods. Here we focus on commonly used evaluation metrics, i.e., Hamming Loss, Subset Loss, Ranking Loss and Decomposable Loss. However, the above mentioned loss functions are typically \(0/1\) losses, which are actually hard to handle in optimization. Hence, one usually consider their surrogate losses, which are defined as follows:

\[}: 56.905512pt_{H}((),)= _{j=1}^{c}_{b}(y_{j}f_{j}()),\]

where the base convex surrogate loss \(_{b}\) can be various popular forms, such as the hinge loss, the logistic loss, the exponential loss and the squared loss.

\[}: 56.905512pt_{S}((),)= _{j[c]}\{_{b}(y_{j}f_{j}())\}.\]

\[}: 14.226378pt_{R}((),)= |\,|Y^{-}|}_{p Y^{+}}_{q Y^{-}}_{b}(f_{p} ()-f_{q}()),\]

where \(Y^{+}\) (\(Y^{-}\)) denotes the relevant (irrelevant) label index set induced by \(\), and \(||\) denotes the cardinality of a set.

\[}: 56.905512pt_{D}((),)= _{j=1}^{c}_{b}(y_{j}f_{j}()).\]

### Related Complexity Measures

Here we introduce the complexity measures involved in theoretical analysis. The Rademacher complexity is used to perform generalization analysis for LSRL.

**Definition 1** (Rademacher complexity).: _Let \(\) be a class of real-valued functions mapping from \(\) to \(\). Let \(D=\{_{1},,_{n}\}\) be a set with \(n\) i.i.d. samples. The empirical **Rademacher complexity** over \(\) is defined by \(_{D}()=_{}[_{g }_{i=1}^{n}_{i}g(_{i})]\), where \(_{1},,_{n}\) are i.i.d. Rademacher random variables, and we refer to the expectation \(()=_{D}[_{D}()]\) as the Rademacher complexity of \(\). In addition, we define the worst-case Rademacher complexity as \(_{n}()=_{D^{n}}_{D}( )\), and its expectation is denoted as \(()=_{D}[_{n}()]\)._

In multi-label learning, \(\) is a vector-valued function, which makes traditional Rademacher complexity analysis methods invalid. Hence, we need to convert the Rademacher complexity of a loss function space associated with the vector-valued function class \(\) into the Rademacher complexity of a tractable scalar-valued function class. The Rademacher complexity can be bounded by other scale-sensitive complexity measures, such as the covering number and fat-shattering dimension (Srebro et al., 2010; Zhang and Zhang, 2023). The relevant definitions are provided in the appendix.

## 4 General Bounds for LSRL

In this section, we first introduce the assumptions used. Then, we develop a novel vector-contraction inequality for the Rademacher complexity of the loss function space associated with the vector-valued function class \(\). Finally, with the novel vector-contraction inequality, we derive the generalization bound for general function classes of LSRL with no dependency on the number of labels, up to logarithmic terms, which is tighter than the state of the art. The detailed proofs of the theoretical results in this paper are provided in the appendix.

**Assumption 1**.: _Assume that the input features, the loss function and the components of the vector-valued function are bounded: \(\|_{i}\|_{2} R\), \((,) M\), \(|f_{j}()| B\) for \(i[n]\), \(j[c]\) where \(R>0\), \(M>0\) and \(B>0\) are constants._

**Assumption 2**.: _Assume that the loss function \(\) is \(\)-Lipschitz continuous w.r.t. the \(_{}\) norm, that is:_

\[|((),)-(^{}(),)| \|()-^{}()\|_{},\]

_where \(>0\), \(\|\|_{}=_{j[c]}|t_{j}|\) for \(=(t_{1},,t_{c})\)._

Assumption 1 and 2 are mild assumptions. For Assumption 1, normalization of input features is a common data preprocessing operation. When we consider the function class (1), we often use the assumptions \(\|_{j}\|_{2}\), \(\|_{j}()\|_{2} A\) for any \(j[c]\) to replace the boundedness of the components of the vector-valued function, i.e., \(B:= A\). For Assumption 2, the Lipschitz continuity w.r.t. the \(_{}\) norm has been considered in some literature (Foster and Rakhlin, 2019; Lei et al., 2019; Wu et al., 2021b). The following Proposition 1 further illustrates that the commonly used loss functions in multi-label learning actually satisfy Assumption 2.

**Proposition 1**.: _Assume that the base loss \(_{b}\) defined in Subsection 3.2 is \(\)-Lipschitz continuous, then the surrogate Hamming Loss is \(\)-Lipschitz w.r.t. the \(_{}\) norm, the surrogate Subset Loss is \(\)-Lipschitz w.r.t. the \(_{}\) norm, the surrogate Ranking Loss is \(2\)-Lipschitz w.r.t. the \(_{}\) norm, and the surrogate Decomposable Loss is \(c\)-Lipschitz w.r.t. the \(_{}\) norm._

We define a function class \(\) consisting of projection operators \(p_{j}:^{c}\) for any \(j[c]\) which project the \(c\)-dimensional vector onto the \(j\)-th coordinate. Then, we have \(()=\{(j,) p_{j}(()):p_{j}(())=f_{j}(),,(j,)[c]\}\). The projection function class decouples the relationship among different components. With the assumption of \(_{}\) norm Lipschitz loss and the above definitions, we show that the Rademacher complexity of the loss function space associated with \(\) can be bounded by the worst-case Rademacher complexity of the projection function class \(()\). We develop the following novel vector-contraction inequality:

**Lemma 1**.: _Let \(\) be a vector-valued function class of LSRL defined by (1). Let Assumptions 1 and 2 hold. Given a dataset \(D\) of size \(n\). Then, we have_

\[_{D}() 12_{nc}( ())(1+^{}(8e^{2}n^{3}c^{3}) }{ B}),\]

_where \(_{D}()=_{}[_{ ,}_{i=1}^{n}_{i} ((_{i}))]\) is the empirical Rademacher complexity of the loss function space associated with \(\), and \(_{nc}(())\) is the worst-case Rademacher complexity of the projection function class._Proof Sketch.: First, the Rademacher complexity of the loss function space associated with \(\) can be bounded by the empirical \(_{}\) norm covering number with the refined Dudley's entropy integral inequality. Second, according to the Lipschitz continuity w.r.t the \(_{}\) norm, the empirical \(_{}\) norm covering number of \(\) can be bounded by that of \(()\). Third, the empirical \(_{}\) norm covering number of \(()\) can be bounded by the fat-sattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of \(()\). Hence, the problem is transferred to the estimation of the worst-case Rademacher complexity. Finally, we estimate the lower bound of the worst-case Rademacher complexity of \(()\), and then combined with the above steps, the Rademacher complexity of the loss function space associated with \(\) can be bounded. 

With the vector-contraction inequality above, we can derive the following tight bound for LSRL:

**Theorem 1**.: _Let \(\) be a vector-valued function class of LSRL defined by (1). Let Assumptions 1 and 2 hold. Given a dataset \(D\) of size \(n\). Then, for any \(0<<1\), with probability at least \(1-\), the following holds for any \(\):_

\[R()_{D}()+  A(1+^{}(8e^{2}n^{3}c^{3})} { B})}{}+3M}{2n}}.\]

Proof Sketch.: We first upper bound the worst-case Rademacher complexity \(_{nc}(())\), and then combined with Lemma 1, the desired bound can be derived. 

**Remark 1**.: _Although Lemma 1 shows a factor of \(\), the term \(_{nc}(()) A/\), which makes the Rademacher complexity of the loss function space associated with \(\) (i.e., \(_{D}()\)) actually independent on \(c\), up to logarithmic terms, and results in a tighter bound than the existing \(O(c/)\) and \(O()\) bounds with a faster convergence rate \((1/)\). The bound in Theorem 1 with no dependency on \(c\) can provide a general theoretical guarantee for LSRL, even for extreme multi-label learning where the number of labels far exceeds the number of examples (Yu et al., 2014; Prabhu and Varma, 2014; Yen et al., 2016; Liu and Shen, 2019), since it is easy to get that \( c\) is much smaller than \(\). The main challenge of generalization analysis for LSRL is that existing theoretical results and existing generalization analysis methods for multi-label learning are not applicable to LSRL. Specifically, existing theoretical bounds often involve the typical vector-contraction inequality (Maurer, 2016) for \(_{2}\) Lipschitz loss: \(_{}[_{} _{i=1}^{n}_{i}((_{i})) ]_{}[_{ }_{i=1}^{n}_{j=1}^{c}_ {ij}f_{j}(_{i})]\), which will lead to bounds with a linear dependency on \(c\) for multi-label learning. Lei et al. (2015); Wu and Zhu (2020); Wu et al. (2021) improve the dependency of the bounds on \(c\) to square-root, which preserve the coupling among different components reflected by the constraint \(\|\|\). Lei et al. (2019) also improves the bounds of multi-class classification to be independent on \(c\) (up to logarithmic terms) for \(_{}\) Lipschitz loss by preserving the coupling among different components, and Wu et al. (2021) further generalizes these results to multi-label learning. However, LSRL decomposes the multi-label learning problem into \(c\) binary classification problems, which means that the relationship among different components needs to be decoupled in the generalization analysis. Hence, how to develop novel vector-contraction inequalities that can induce \((1/)\) bounds and deal with the case of decoupling the relationship among different components are the two most critical difficulties in deriving tighter bounds for LSRL. The introduction of the projection function class plays an important role in solving these two difficulties. It improves the vector-contraction inequalities by a factor of \(\) and decouples the relationship among different components (which is also reflected by the constraint \(\|_{j}\|\) for any \(j[c]\)). Our tighter \((1/)\) bound in Theorem 1 with no dependency on \(c\) (up to logarithmic terms) solve the limitations of existing theoretical results for multi-label learning and can provide a general theoretical guarantee for LSRL._

The differences in generalization bounds of different LSRL methods are mainly reflected in two aspects. On the one hand, the Lipschitz constant of the loss functions, as we proved in Proposition 1, the Lipschitz constants \(\) corresponding to different loss functions are various. On the other hand, the nonlinear mappings induced by different LSRL methods. In fact, when we analyze the generalization for LSRL, we will further have \(\|()\| A:= R\) (\(\) is the Lipschitz constant of the nonlinear mappings \(()\)) to take into account the differences or characteristics of different LSRL methods. We provide detailed analysis for \(A\) of typical LSRL methods in the next section.

Generalization Bounds for Typical LSRL Methods

In this section, we analyze the generalization bounds for several typical LSRL methods, i.e., \(k\)-means clustering-based (Zhang and Wu, 2015), Lasso-based (Huang et al., 2016) and DNN-based (Hang and Zhang, 2022) LSRL methods. We show that different construction methods of label-specific representation will lead to significant differences in the constant \(A\) of the generalization bound in Theorem 1. For each LSRL method, we first give a brief introduction, then give its formal definition corresponding to the class of LSRL defined in (1), and finally derive the generalization bound.

### Generalization Bounds for \(k\)-Means Clustering-Based LSRL Method

As a seminal work, \(k\)-means clustering-based LSRL method, i.e., LIFT (Zhang and Wu, 2015), uses \(k\)-means clustering to construct label-specific representation that effectively capture the specific characteristics of each label. Specifically, first, for each label, \(k\)-means clustering is used to divide the training instances into \(K\) clusters, and the centers of the \(K\) clusters are obtained, which is denoted as \(_{k}^{j}\) for the \(j\)-th label, \(k[K]\), \(j[c]\). Then, these \(K\) centers are used to construct the label-specific representation, i.e., in the vector-valued function class of LSRL defined by (1), \(_{j}()=[d(,_{1}^{j}),,d(,_{K}^{j})]\), \(d(,_{k}^{j})=\|-_{k}^{j}\|\). Finally, a family of \(c\) classifiers \(f_{j}\) with \(\)-Lipschitz nonlinear mapping are induced based on the generated label-specific representations.

Next, we formally define the process of \(k\)-means clustering. Here we follow some of the settings and definitions in (Li and Liu, 2021). Assume that \(V:^{2}_{+}\)is a pairwise distance-based function used to measure the dissimilarity between pair observations, and \(Z=[Z_{1},,Z_{K}]\) is a collection of \(K\) partition functions \(Z_{k}:^{2}_{+}\)for \(k[K]\). The clustering framework can be cast as the problem of minimizing the following criterion:

\[_{D}(V,Z)=_{i,j=1,i j}^{n}_{k=1}^{K}V (_{i},_{j})Z_{k}(_{i},_{j})\]

over all possible functions \(V\) and \(Z_{k}\) for \(k[K]\). In \(k\)-means clustering, we have \(V(_{i},_{j})=\|_{i}-_{j}\|_{2}^ {2}\), and \(Z_{k}(_{i},_{j})=\{(_{i},_{j}) C_{k}^{2}\}\), \(_{k}=|}_{ C_{k}}\), where \(C_{1},,C_{K}\) are the partitions of the feature space \(\). Let \(g_{k}(_{i},_{j})=V(_{i},_{j})Z_{ k}(_{i},_{j})\) and \(=(g_{1},,g_{K})\) be a vector-valued function, \(_{clu}((,^{}))=_{k=1}^{K}g_{k}(, ^{})\), then \(_{D}(V,Z)\) can be written as

\[_{D}(V,Z)=_{i,j=1,i j}^{n}_{clu}((_{i},_{j}))=_{i,j=1,i j}^{n}_{k=1}^ {K}g_{k}(_{i},_{j}).\]

We then define a vector-valued function class of \(k\)-means clustering as follows:

\[=\{(,^{})(,^{}): (,^{})=(g_{1}(,^{}), ,g_{K}(,^{})),\] \[g_{k}(,^{})=\|-^{} \|_{2}^{2}\{(,^{}) C _{k}^{2}\},,^{},k[K]\},\]

where \(|g_{k}(,)| G\) for \(k[K]\) and \(G>0\) are constants.

We denote the function class of \(k\)-means clustering corresponding to the \(j\)-th label as \(^{j}\). With the above definitions, we can derive the tight bound for \(k\)-means clustering-based LSRL method:

**Theorem 2**.: _Let \(\) be a vector-valued function class of \(k\)-means clustering-based LSRL defined by (1). Let Assumptions 1 and 2 hold. Given a dataset \(D\) of size \(n\). Then, for any \(0<<1\), with probability at least \(1-\), the following holds for any \(\):_

\[R() _{D}()+3M}{2n}}+ R(1+^{}(8e^{2}n^{3} c^{3})}{ B})}{}\] \[+ G}{}(1+^{ {2}}(e^{2}n^{3}c^{3})}{G})(1+^{ {1}{2}}(8e^{2}n^{3}c^{3})}{ B}).\]

**Remark 2**.: _There are three key points in the generalization analysis of \(k\)-means clustering-based LSRL method. 1) Since \(k\)-means clustering-based LSRL method is two-stage, i.e., the centers of clusters is generated by using \(k\)-means clustering in the first stage, then these centers are exploited to generate label-specific representations which are used to learn a multi-label classifier in the second stage, we cannot formally express these two stages in a closed-form expression through a composite function (\(K\) centers are generated by the \(\) function). Furthermore, \(K\) centers generated in the first stage are actually used as fixed parameters rather than inputs in the second stage. Hence, in order to fully consider the capacity of the model corresponding to the first stage, it is reasonable to define the whole function class as the sum of the function classes \(+_{clu}\) corresponding to the methods of these two stages. Then, combined with Lemma 1, the generalization analysis is transformed into the bounding of the complexity of the projection function class \((+_{clu})\). The introduction of class \(\) induces an additional increase in complexity, i.e., the last term in Theorem 2.2. The generalization analysis of \(k\)-means clustering-based LSRL method involves the generalization analysis for \(k\)-means clustering. However, since the \(k\)-means clustering framework involves pairwise functions, a sequence of pairs of i.i.d. individual observation in \(k\)-means clustering is no longer independent, which makes standard techniques in the i.i.d case for traditional Rademacher complexity inapplicable for \(k\)-means clustering. We convert the non-sum-of-i.i.d pairwise function to a sum-of-i.i.d form by using permutations in U-process [Clemencon et al., 2008]. We show that the empirical Rademacher complexity of a loss function space associated with the vector-valued function class \(\) can be bounded by \(_{D^{}}(_{clu}):=_{ }[_{}}_{i=1}^{}_{i}_{ clu}((_{i},_{i+}))]\). 3) In order to derive tight bounds for \(k\)-means clustering, we develop a novel vector-contraction inequality that can induce bounds with a square-root dependency on the number of clusters. The theoretical techniques and results involved here may be of independent interest. The generalization bound of \(k\)-means clustering-based LSRL method is tighter than the state of the art with a faster convergence rate \(()\), which is independent on the number of labels. Since the lower bound for clustering is \(()\)[Bartlett et al., 1998], our bound is (nearly) optimal, up to logarithmic terms, even from the perspective of clustering. The constant \(A\) of the generalization bound in Theorem 1 corresponds to \(2\) here._

### Generalization Bounds for Lasso-Based LSRL Method

Lasso-based LSRL method, i.e., LLSF [Huang et al., 2016], assumes that the label-specific representation of each label should have sparsity and sharing properties. For sparsity, LLSF uses Lasso as the model corresponding to each label. The property of sharing is achieved by considering that two strongly correlated labels will share more features with each other than two uncorrelated or weakly correlated labels and the corresponding weights will be similar, i.e., their inner product will be large.

Formally, since each label corresponds to a Lasso, this means that in the class of LSRL defined by (1), the base loss \(_{b}\) is the squared loss, the nonlinear mappings \(()\) and \(()\) are both identity transformations for any \(j[c]\), and the constraint \(()\) is \(\|_{j}\|_{1}\) for any \(j[c]\). For the \(j\)-th label, the property of sharing is reflected by the additionally introduced constraint \(_{i}^{c}(1-s_{ji})_{j}_{i}\), where \(s_{ji}\) is the cosine similarity between labels \(y_{j}\) and \(y_{i}\), here we refer to it as the sharing constraint. The loss function used by Lasso-based LSRL method is the Decomposable loss.

Since the squared loss is not Lipschitz continuous, the theoretical results on the Lipschitz continuity of the loss functions in Proposition 1 cannot be applied to Lasso-based LSRL method. To overcome this challenge, we define the pseudo-Lipschitz function, which is also used in the theoretical analysis of approximate message passing algorithms [Bayati and Montanari, 2011].

**Definition 2**.: _For \(k 1\), we say that a function \(f:\) is pseudo-Lipschitz of order \(k\) if there exists a constant \(L>0\) such that the following inequality holds for all \(x,y\):_

\[|f(x)-f(y)| L(1+|x|^{k-1}+|y|^{k-1})|x-y|.\]

Note that any pseudo-Lipschitz function of order \(1\) is Lipschitz continuous. The following Proposition shows that the Decomposable loss is still Lipschitz continuous if the base loss is the squared loss.

**Proposition 2**.: _The squared loss is \(1\) pseudo-Lipschitz of order \(2\), the surrogate Decomposable Loss is \((3+2B)c\)-Lipschitz w.r.t. the \(_{}\) norm if the base loss \(_{b}\) is the squared loss._

With the above definitions, we can derive the generalization bound for Lasso-based LSRL method:

**Theorem 3**.: _Let \(\) be a vector-valued function class of Lasso-based LSRL defined by (1). Let Assumptions 1 and 2 hold. Given a dataset \(D\) of size \(n\). Then, for any \(0<<1\), with probability at least \(1-\), the following holds for any \(\):_

\[R()_{D}()+(3+2B)c R (1+^{}(8e^{2}n^{3}c^{3})}{ B} )}{}+3M}{2n}}.\]

**Remark 3**.: _The complexity of the LLSF function class can be bounded by the complexity of the LSRL function class where each label corresponds to a Lasso, since the introduction of the sharing constraint in the LLSF function class reduces the complexity of the function class compared with the LSRL function class where each label corresponds to a Lasso. Hence, the complexity analysis of the LLSF function class can be converted into upper bounding the Rademacher complexity of the LSRL function class where each label corresponds to a Lasso. The constant \(A\) of the generalization bound in Theorem 1 corresponds to \(R\) here, and the value of \(\) is \((3+2B)c\), which induce the \((c/)\) bound here. If other loss functions are used, e.g., Hamming, Subset or Ranking loss, instead of Decomposable loss, the dependency of the bounds for Lasso-based LSRL method on \(c\) can be improved from linear to independent, up to logarithmic terms._

### Generalization Bounds for DNN-Based LSRL Method

DNN-based LSRL method, i.e., CLIF (Hang and Zhang, 2022), exploits the powerful representation learning capability of deep neural networks (DNNs) to learn label-specific representation in an end-to-end manner. Since the construction of label-specific representation involves graph convolutional networks (GCNs), we first introduce the relevant definitions for GCN.

Let \(=\{,\}\) be a given undirected graph, where \(=\{_{1},_{2},,_{n}\}\) is the set of nodes with size \(||=n\) and \(\) is the set of edges. Let \(A\) and \(D\) be the adjacency matrix and the diagonal degree matrix respectively, where \(D_{ii}=_{j=1}^{n}A_{ij}\). Let \(=(D+I_{n})^{-}(A+I_{n})(D+I_{n })^{-}\) denote the normalized adjacency matrix with self-connections, where \(I\) is the identity matrix. The feature propagation process of a two-layer GCN is \(((XW_{1})W_{2})\), where \(W_{1}\) and \(W_{2}\) are parameter matrices, \(X\) is the node feature matrix, and the \(i\)-th row \(X_{i*}\) is the node feature \(_{i}\).

Specifically, for DNN-based LSRL method, first, a graph (here we call it the label graph) is constructed over the label space and a GCN is used to generate the label embeddings, i.e., the label embeddings can be denoted by \((Y)=_{ReLU}(_{ReLU}(YW_{1})W_{2})\), where \(Y\) is the node feature matrix of the label graph with size \(c\) and the nodes are also bounded by \(R\), \(_{ReLU}\) is the ReLU activation. Second, the label embedding of the \(j\)-th label is decoded into the importance vector by a one-layer fully-connected neural network, i.e., \(_{sig}(W_{3}(Y)_{j})\), where \(_{sig}\) is the sigmoid activation, and the input feature is mapped into the latent representation through a one-layer fully-connected neural network, i.e., \(_{ReLU}(W_{4})\). Third, for the \(j\)-th label, the Hadamard product of the importance vector and the latent representation is defined as the pertinent representation, and then the label-specific representation for the \(j\)-th label is obtained by feeding the pertinent representation into another one-layer fully-connected neural network. Hence, the label-specific representation for the \(j\)-th label is

\[_{j}()=_{ReLU}\{W_{5}[_{ReLU}(W_{4}) _{sig}(W_{3}(Y)_{j})]\}.\]

Finally, the \(j\)-th model is implemented by a fully-connected layer, i.e., \(f_{j}()=_{sig}(_{j}^{}_{j}())\).

In the generalization analysis of the above deep neural network, we introduce the following assumption, which is a common assumption in the generalization analysis for DNNs (Bartlett et al., 2017; Golowich et al., 2018; Zhang and Zhang, 2023; Tang and Liu, 2023).

**Assumption 3**.: _Assume that the parameter metrices in DNN-based LSRL method are bounded, i.e., \(\|W_{i}\| D\), \(i\), where \(D>0\) is a constant._

With the above definitions, we can derive the generalization bound for DNN-based LSRL method:

**Theorem 4**.: _Let \(\) be a vector-valued function class of DNN-based LSRL defined by (1). Let Assumptions 1, 2, and 3 hold. Given a dataset \(D\) of size \(n\). Then, for any \(0<<1\), with probability at least \(1-\), the following holds for any \(\):_

\[R()_{D}()+ D ^{5}R^{2}(g_{}+1)(1+^{}(8e^{2}n^{3}c^{3}) }{ B})}{}+3M}{2n}},\]

_where \(g_{}\) is the maximum node degree of the label graph._

**Remark 4**.: _The term \(_{nc}(()) D^{5}R^{2}(g_{}+1)/4 \), which makes the Rademacher complexity \(_{D}()\) actually independent on \(c\), up to logarithmic terms, and results in a tight bound for DNN-based LSRL method with a faster convergence rate \((1/)\). The constant \(A\) of the generalization bound in Theorem 1 corresponds to \(D^{5}R^{2}(g_{}+1)/4\) here. For deep GCNs, the increase in depth means that the generalization performance will deteriorate, which is consistent with empirical performance and guides us not to design too many layers of GCN. In addition, the bound is linearly dependent on the maximum node degree of the label graph, which suggests that when the performance of the model is always unsatisfactory, we can check whether the maximum node degree is large and consider using some techniques to remove some edges, e.g., DropEdge (Rong et al., 2020), to alleviate the over-fitting problem. We will further explore more network structures to learn more effective label-specific representations, e.g., hypernetworks (Galanti and Wolf, 2020; Chen et al., 2023; Shen et al., 2023), deep kernel networks (Zhang and Liao, 2020; Zhang and Zhang, 2023), and provide generalization analysis for the corresponding DNN-based LSRL methods._

## 6 Discussion

Compared with existing methods considering label correlations, which mainly focus on the processing of the label space by exploiting or modeling relationships between labels, LSRL mainly focuses on the operation on the input space and implicitly considers label correlations in the process of constructing label-specific representations. For example, in the construction of label-specific representations in LLSF and CLIF, the label correlation information is embedded into the label-specific representations in the input space by introducing the sharing constraint and using a GCN over the label graph to generate label embeddings, respectively. LSRL methods are more effective since the label correlation information is considered in the construction of label-specific representations.

Our theoretical results explain why LSRL is an effective strategy to improve the generalization performance of multi-label learning. On the one hand, existing results can improve the dependency of the bound on \(c\) from linear to square-root by preserving the coupling among different components, which corresponds to high-order label correlations induced by norm regularizers. However, the improvement in the preservation of coupling by a factor of \(\) benefits from replacing \(\) with \(\) in the constraint to some extent, and preserving the coupling corresponds to the stricter assumption (Zhang and Zhang, 2024). Our results for LSRL decouple the relationship among different components, and the bounds with a weaker dependency on \(c\) are tighter than the existing results that preserve the coupling, which also explains why LSRL methods outperform the multi-label methods that consider high-order label correlations induced by norm regularizers. On the other hand, based on our results, we can find that LSRL methods substantially increase the data processing, i.e., the process of constructing label-specific representations. From the perspective of model capacity, compared with traditional multi-label methods, since the introduction of construction methods of label-specific representations, the capacity of the model is significantly increased, especially if deep learning methods are used to generate label-specific representations, which improves the representation ability of the model. Or more intuitively, LSRL means an increase in model capacity and stronger representation ability, which makes it easier to find the hypotheses with better generalization in the function class.

The vector-contraction inequality and the theoretical tools developed here are applicable to the theoretical analysis of other problem settings, such as multi-class classification, or more general vector-valued learning problem. For multi-class classification, multi-class margin-based loss, multinomial logistic loss, Top-\(k\) hinge loss, etc. are all \(_{}\) Lipschitz (Lei et al., 2019). For multi-label learning, the surrogate loss for Macro-Averaged AUC is also \(_{}\) Lipschitz (Zhang and Zhang, 2024).

## 7 Conclusion

In this paper, we propose a novel vector-contraction inequality for \(_{}\) norm Lipschitz continuous loss, and derive bounds for general function classes of LSRL with a weaker dependency on \(c\) than the state of the art. In addition, we analyze the bounds for several typical LSRL methods, and study the impact of different label-specific representations on the generalization analysis.

In future work, we will extend our bounds to more LSRL methods, and derive tighter bounds for LSRL with a faster convergence rate w.r.t. the number of examples, and further design efficient models and algorithms to construct label-specific representations with good generalization performance.