# Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation

Jintao Tong   Yixiong Zou1   Yuhua Li   Ruixuan Li

School of Computer Science and Technology, Huazhong University of Science and Technology

{jintaotong, yixiongz, idcliyuhua, rxli}@hust.edu.cn

###### Abstract

Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation. The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios. In this work, we discover an intriguing phenomenon: simply filtering different frequency components for target domains can lead to a significant performance improvement, sometimes even as high as 14% mIoU. Then, we delve into this phenomenon for an interpretation, and find such improvements stem from the reduced inter-channel correlation in feature maps, which benefits CD-FSS with enhanced robustness against domain gaps and larger activated regions for segmentation. Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an Amplitude-Phase Masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module. Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods.

## 1 Introduction

Recent advancements in semantic segmentation have been driven by large-scale annotated datasets and developments in deep neural networks . Nevertheless, the requirement for extensive labeled data remains a significant challenge, particularly for dense prediction tasks like semantic segmentation. Hence, few-shot semantic segmentation (FSS)  has been proposed to meet this challenge, aiming to produce predictions for the unseen categories with only limited annotated data. However, these FSS methods perform poorly when confronted with domain shifts, particularly when there is a significant gap between the novel class (target domain) and the base class (source domain). This issue has spurred the development of the cross-domain few-shot semantic segmentation (CD-FSS) task . Despite various efforts in CD-FSS, the outcomes remain sub-optimal.

To handle the domain shift problem, efforts have been made to study the generalization of neural networks. Recently, some works  have explored this from the perspective of the frequency domain, achieving theoretical breakthroughs. Compared to humans, neural networks exhibit heightened sensitivity to different frequency components. Additionally, amplitude and phase exhibit distinct properties and effects on neural network performance. Inspired by these works, we study the domain shift problem from the perspective of the frequency domain and discover an intriguing phenomenon shown in Figure 1: for a model already trained on the source domain, **simply filtering frequency components of images during testing can lead to significant performance improvements**, sometimes even as high as 14% mIoU.

In this paper, we delve into this phenomenon for an interpretation. Through experiments and mathematical derivations, we find the filtering of the phase and amplitude effectively disentangles feature channels, which lowers the channel correlations and helps the model capture a larger range of semantic patterns. This benefits the model with improved robustness against large domain gaps, and helps to discover the whole object for segmentation.

Based on the above interpretations, we propose a lightweight frequency masker for the CD-FSS task. This masker does not need to be trained on the source domain, and can be directly inserted into intermediate feature maps during target-domain fine-tuning. It includes an Amplitude-Phase Masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module. APM adaptively learns on target domains to filter out harmful amplitude and phase components at a finer granularity, which improves the effectiveness of channel disentanglement. ACPA learns the attention over channels through phase information. Notably, the APM module only introduces 0.01% additional parameters, but can effectively improve the mIoU by over 10% on average, and ACPA further improves the performance by 1.5% on average with only 2.5% additional parameters introduced.

In summary, our contributions can be listed as

* We find a phenomenon that simply filtering frequency components on target domains can significantly improve performance, with the highest improvement reaching nearly 14%.
* We delve into this phenomenon for an interpretation. We find the frequency filtering operation can effectively disentangle feature map channels, benefiting the model with improved robustness against large domain gaps and a larger range of discovered regions of interest.
* Based on our interpretations, we propose a lightweight frequency masker for the CD-FSS task, which significantly improves the mIoU by 11% on average with only 2.5% parameters introduced.
* Extensive experiments on four target datasets show that our work, through a simple and effective design, significantly outperforms the state-of-the-art CD-FSS method.

## 2 Interpreting Enhanced Performance from Frequency Filtering

In this section, we delve into why filtering certain frequency components can significantly improve CD-FSS performance in certain target domains for interpretation.

### Preliminaries

Cross-domain few-shot semantic segmentation (CD-FSS) aims to generalize knowledge acquired from source domains with ample training labels to unseen target domains. Given a source domain \(D_{s}=(_{s},_{s})\) and a target domain \(D_{t}=(_{t},_{t})\), where \(\) represents input data distribution and \(\) represents label space. The model will be trained on the training set from the \(D_{s}\), then applied to perform segmentation on novel classes in the \(D_{t}\). Notably, \(D_{s}\) and \(D_{t}\) exhibit distinct input data distribution, with their respective label spaces having no intersection, i.e., \(_{s}_{t}\), \(_{s}_{t}=\).

In this work, we adopt the episodic training manner. Specifically, both the training set sampled from \(D_{s}\) and the testing set sampled from \(D_{t}\) are composed of several episodes, each episode is constructed

Figure 1: For a model already trained on the source domain, we simply filter out different frequency components and plot mIoU against the maintained ones of images. \(P\) denotes Phase, \(A\) denotes Amplitude, \(H\) denotes High Frequency, and \(L\) denotes Low Frequency. We can see the performance is significantly improved in most cases compared with the baseline (\(A_{x}\), \(P_{x}\)), even as high as 14% on the Chest X-ray dataset (\(A_{x}^{L}\), \(P_{x}\)). In this paper, we delve into this phenomenon for an interpretation, and propose a lightweight frequency masker for efficient cross-domain few-shot segmentation.

of K support samples \(S=\{I_{s}^{i},M_{s}^{i}\}_{i=1}^{K}\) and a query \(Q=\{I_{q},M_{q}\}\) (\(I\) is the image and \(M\) is the label). Within each episode, the model is expected to use \(\{I_{s},M_{s}\}\) and \(I_{q}\) to predict the query label.

### Enhanced Performance Stem from Reduced Inter-Channel Correlation

Existing research indicates inter-channel relationships are crucial for performance, as different feature channels can represent distinct features [3; 30]. Therefore, we study the change of channel correlations brought by the frequency filtering. Specifically, we measured the mean mutual information (MI)  between channels from the last layer of the backbone network. The measured cases include the combination of phase and amplitude for the highest and lowest performance in Fig. 1. We report the 1-shot mIoU and MI in Tab. 1, where we can see that for frequency combinations with improved mIoU, their MI consistently decreases, indicating reduced inter-channel correlation in the feature maps. Conversely, for frequency combinations with decreased mIoU, their MI consistently increases.

The higher the mutual information value, the larger the correlation between channels, while a low MI indicates more independent semantic information captured by different channels. Therefore, the experimental results demonstrate that improved performance is associated with decreased inter-channel correlation in the feature map.

### Why Lower Inter-Channel Correlation is Better?

The reduced inter-channel correlation benefits our model in two aspects:

(1) **Cross-domain generalization**. Previous works [2; 51] indicate that a lower correlation between features implies reduced redundancy and enhanced generalizability. Intuitively, a lower inter-channel correlation demonstrates that channels capture patterns more independently, therefore each channel will capture patterns in the input image more uniformly, which means the mean magnitude of each channel across all images will be more uniform. Consistent with our intuition,  shows the channel bias problem affects the generalizability of few-shot methods, and it utilizes the Mean Magnitude of Channels (MMC) to visualize and measure the channel response in features, where effective few-shot methods might have a more uniform MMC curve in the testing set. Therefore, this means the reduced correlation also benefits our model by addressing the channel bias problem, as studied in .

Inspired by this, we visualize the MMC before and after applying the mask to filter frequency components on four target datasets. As shown in Figure 2, for FSS-1000, performance degrades after masking, with the curve steeper than the baseline. Conversely, for the other three target datasets, performance improves with the curve more uniform than the baseline after masking. This indicates the channel bias problem is also handled by frequency filtering, which benefits the model with more independent and diverse semantic patterns to represent target domains.

(2) **Exploring larger activated regions for segmentation**. To study why reducing inter-channel correlation through frequency filtering benefits the segmentation task, we visualize the heatmap of feature maps before and after filtering out specific frequency components. As shown in Figure 3(a), after filtering certain frequency components, the heatmap demonstrates expanded activation regions,

   &  &  &  &  \\   & baseline & best & worst & baseline & best & worst & baseline & best & worst & baseline & best & worst \\ 
1-shot mIoU & 77.54 & 64.34\(\) & 48.054\(\) & 33.19 & 40.42\(\) & 33.44\(\) & 32.65 & 33.16\(\) & 27.124\(\) & 47.34 & 61.58\(\) & 31.394\(\) \\  support MI & 1.3736 & 1.3791\(\) & 1.8767\(\) & 1.3679\(\) & 1.

[MISSING_PAGE_FAIL:4]

**Experiments for derivation.** To validate our derivations, we measured the phase difference histograms between channels, using amplitude as weights, to observe the relationship between phase differences among feature map channels. The phase difference and weight are as follows:

\[=|_{1}-_{2}|,\;\;weight=_{2}}{|_{1} -_{2}|} \]

The results shown in Figure 3(b), indicate that for FSS-1000, after masking certain frequency components, the inter-channel correlation increases. Correspondingly, phase differences between channels tend to cluster more around 0 and \(\). Conversely, for Chest X-ray, masking certain frequency components reduces inter-channel correlation, resulting in fewer phase differences clustering around 0 and \(\). The experimental results have validated the accuracy of our derived conclusions.

### Conclusion and Discussion

Through mathematical derivation and experiments, we demonstrate that manipulating the frequency can reduce inter-channel correlation in feature maps. Each channel of the feature map represents a distinct pattern, and a lower inter-channel correlation implies a higher degree of channel disentanglement, leading to more independent and diverse semantic patterns for each feature. This benefits the model with 1) alleviation of channel bias, boosting model robustness on target domains; and 2) larger activated regions for segmentation, therefore a simple frequency filtering operation can significantly improve performance for the CD-FSS task.

Based on the above analysis, we can draw the following insights: 1) The aforementioned mask operates at the input level, but fundamentally affects the feature map's channel correlation. Therefore, we can directly apply mask operations to the frequency domain of each channel in the feature map; 2) Different domains require filtering different components. The aforementioned mask manually filters different frequencies based on the target domain, but the mask can be made adaptive; 3) The aforementioned mask does not perform well on FSS-1000. We believe this may be due to the overly coarse high-low frequency division. A finer frequency division can be designed, dividing the frequency into \(h w\) parts (where \(h\) and \(w\) are the spatial dimensions of the feature map).

## 3 Method

Our method consists of two major steps, i.e., 1) amplitude-phase masker is proposed to reduce feature correlation, and obtain more accurate and generalized feature maps; 2) adaptive channel phase attention is proposed to select features that benefit the current instance and align the feature spaces of support and query. Our modules do not require source-domain training and can be directly integrated into during target-domain fine-tuning. The overall framework of our approach is shown in Figure 4.

### Amplitude-Phase Masker

Amplitude-Phase Masker(APM) is a model-agnostic module that filters out negative frequency components at the feature level within feature maps. Through mathematical derivation, it is shown that APM accomplishes feature disentanglement. Consequently, this leads to a feature map that is more robust, generalizable, and provides broader and more accurate representations.

In our work, we utilize a fixed encoder, trained on the source domain, to extract feature maps \(^{c h w}\), where \(c,h\) and \(w\) represent channels, height, and width. We then apply the Fast

Figure 3: (a) After masking certain frequency components, the model’s attention regions are enlarged with more patterns encompassed. (b) A higher concentration of phase differences at 0 and \(\) indicates a higher correlation, so that on FSS-1000 the performance drops but on Chest X-ray it increases.

Fourier Transformation (FFT) to convert these feature maps from the spatial domain into the frequency domain, further decomposing them into phase spectrum \(\) and amplitude spectrum \(\):

\[e^{i}=FFT() \]

Then, the phase and amplitude obtained from the FFT are each subjected to a Hadamard product with their respective sigmoid-activated phase mask (PM) \(_{p}\) and amplitude mask (AM) \(_{a}\). This operation effectively filters out negative components from both the phase and amplitude:

\[_{enh} =Sigmoid(_{p}) \] \[_{enh} =Sigmoid(_{a})\]

where \(\) indicates the element-wise multiplication, \(_{enh}\) and \(_{enh}\) denotes the enhanced phase and amplitude, respectively. For each task, the APM is initialized with all ones, where \(Sigmoid(_{*})\). Here, 1 allows complete passage of frequency components, while 0 results in their total filtration. The original APM (APM-S) is a lightweight module, configured as an \(h w\) matrix that matches the height and width of the feature map. We also offer a variant APM (APM-M) that expands the dimensions to \(c h w\), in alignment with the feature map's dimensions.

The filtered phase and amplitude components are recombined and transformed back into the spatial domain using the Inverse Fast Fourier Transform (IFFT) to produce the enhanced feature map:

\[_{enh}=IFFT(_{enh}e^{i_{enh}}) \]

We iterate and optimize the APM using the support and its corresponding labels. After the APM process, the model generates a feature map that is more accurate and generalizable. This feature map is then fed into the subsequent Adaptive Channel Phase Attention module for further optimization.

### Adaptive Channel Phase Attention

Adaptive Channel Phase Attention(ACPA) can be seen as a process of feature selection. Building on the APM-optimized feature map, ACPA encourages the model to focus on more effective channels (features) while aligning the feature spaces of the support and query. Its underlying insight is that amplitude and phase are considered as style and content, respectively. Therefore, the phase can be seen as an invariant representation, with consistent phase elements across both support and query.

For the enhanced support feature maps \(_{enh}^{sup}\), a support mask \(M^{s}\{0,1\}^{H W}\) is applied to discard irrelevant activations:

\[_{mask}^{sup}=_{enh}^{sup} M^{s} \]

Subsequent operations input to ACPA are consistent with those applied to the query feature maps. Therefore, we uniformly refer to the feature map fed into ACPA as \(_{enh}\). We adopt a SE block following SENet  as our channel attention module, denoted as \(SE\):

\[_{phase}=SE(_{enh}) \]

Figure 4: Overview of our method in a 1-shot example. After obtaining the feature map, APM is introduced to adaptively filter certain frequency components based on different domains, facilitating feature disentanglement to achieve more generalizable representations. Additionally, we propose ACPA to encourage the model to focus on more effective features while aligning the feature space of the support and query images. The internal structure of APM and ACPA is highlighted in green.

where \(_{phase}^{c 1 1}\) is phase attention weight, \(_{enh}\) is the phase of \(_{enh}\). Then we apply the phase attention weights to the feature map \(_{enh}\) to obtain the final feature map \(_{final}\):

\[_{final}=_{l}(_{phase})_{enh} \]

where \(\) is the Hardmard product and \(_{l}(*)\) extends the weight to match the dimension of the feature map by expanding along the spatial dimension, i.e., \(_{l}:^{c 1 1}^{c h  w}\).

Finally, a pair of query feature maps \(^{qry}_{final}\) and support feature maps \(^{sup}_{final}\) are fed into comparison module forms affinity maps \(C^{h w h w}\) using cosine similarity:

\[C(m,n)=ReLU(^{qry}_{final}(m)^{sup}_{final}(n) }{\|^{qry}_{final}(m)\|\|^{sup}_{final}(n)\|}) \]

where \(m,n\) denote 2D spatial positions of feature maps \(^{qry}_{final}\) and \(^{sup}_{final}\) respectively. Then, the \(C(m,n)\) is fed into the decoder to obtain segmentation results, as shown in Figure 4.

## 4 Experiments

### Datasets

We utilize the benchmark established by PATNet  and adopt the same data preprocessing methods. For training, our source domain is the PASCAL-\(5^{i}\) dataset , an extended version of PASCAL VOC 2012  enhanced with additional annotations from the SDS dataset. For evaluation, our target domains include FSS-1000 , Deepglobe , ISIC2018 [9; 38], and the Chest X-ray datasets [6; 20]. See Appendix A.1 for more details about datasets.

### Implementation Details

We employ ResNet-50  as our encoder, initialized with weights pre-trained on ImageNet . The training manner is consistent with our baseline model HSNet . To optimize memory usage and speed up training, the spatial sizes of both support and query images are set to \(400 400\). The model is trained using the Adam  optimizer with a learning rate of 1e-3.

During the adaptation stage, the model initially predicts the support mask and then uses the corresponding label to optimize the APM and ACPA through CE loss. The adaption stage of the APM and the ACPA leverage feature maps from conv5_x whose channel dimensions are 2048 and spatial size is 13\(\)13, is performed using the Adam optimizer, with learning rates set at 0.1 for Chest X-ray, 0.01 for FSS-1000 and ISIC, and 1e-5 for Deepglobe. Each task undergoes a total of 60 iterations.

### Comparison with State-of-the-Art Works

In Table 2, we compare our method with several state-of-the-art few-shot semantic segmentation approaches on the benchmark introduced by PATNet . Our results show a significant improvement in cross-domain semantic segmentation for both 1-shot and 5-shot tasks. Specifically, APM-S exceeds the performance of the state-of-the-art PATNet, based on ResNet-50, by 2.87% and 0.87% in average

    &  &  &  &  &  \\   & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\  PGNet  & 62.42 & 62.74 & 10.73 & 12.36 & 21.86 & 21.25 & 33.95 & 27.96 & 32.24 & 31.08 \\ PANet  & 69.15 & 71.68 & 36.55 & **45.43** & 25.29 & 33.99 & 57.75 & 69.31 & 47.19 & 55.10 \\ CalNet  & 70.67 & 72.03 & 22.32 & 23.07 & 25.16 & 28.22 & 28.25 & 28.62 & 36.63 & 37.99 \\ RPMMs  & 65.12 & 67.06 & 2.99 & 13.47 & 18.02 & 20.04 & 30.11 & 30.82 & 31.56 & 32.85 \\ PFNet  & 70.87 & 70.52 & 16.88 & 18.01 & 23.50 & 23.83 & 27.22 & 27.57 & 34.62 & 34.98 \\ RePRI  & 70.96 & 74.23 & 25.03 & 27.41 & 23.27 & 26.23 & 65.08 & 65.48 & 46.09 & 48.34 \\ HSNet  & 77.53 & 80.99 & 29.65 & 35.08 & 31.20 & 35.10 & 51.88 & 54.36 & 47.57 & 51.38 \\ HSNet\({}^{*}\) & 77.54 & 80.21 & 33.19 & 36.46 & 32.65 & 35.09 & 47.34 & 48.63 & 47.68 & 50.10 \\ PATNet  & 78.59 & 81.23 & 37.89 & 42.97 & 41.16 & **53.58** & 66.61 & 70.20 & 56.06 & 61.99 \\ 
**Ours (APM-S)** & 78.25 & 80.29 & 40.77 & 44.85 & 41.48 & 49.39 & 75.22 & 76.89 & 58.93 & 62.86 \\
**Ours (APM-M)** & **79.29** & **81.83** & **40.86** & 44.92 & **41.71** & 51.16 & **78.25** & **82.81** & **60.03** & **65.18** \\   

Table 2: Mean-IoU of 1-shot and 5-shot results on the CD-FSS benchmark. The best and second-best results are in bold and underlined, respectively. * denotes the model implemented by ourselves. APM-S is an \(1 h w\) matrix, while APM-M (more parameters) expands to \(c h w\).

[MISSING_PAGE_FAIL:8]

ore independent semantic representations. As shown in Table 7 (Left), we visualized the heatmaps of feature maps processed by APM and those not processed by APM. It is evident that after applying APM, the model focuses on more different features. For example, in the first column, the baseline only highlights the swan's head, whereas APM makes each feature representation more independent, allowing the model to attend to various features of the swan.

### ACPA: Aligning Task-Relevant Features and Feature Spaces

ACPA can be seen as feature selection, enabling the model to focus on features that are more effective for the current task while aligning the feature spaces of the support and query feature maps. As shown in Table 7 (Left), after APM disentangles the features and produces a more broadly represented feature map, ACPA selects features that are more effective and discriminative for the current task. For example, in the first row, ACPA selects the swan's wings and head. In the second column, it selects the bird's head, tail, and feet. Furthermore, we measured the CKA (Centered Kernel Alignment) to calculate the distance between the support feature map and the query feature map, validating that ACPA aligns the support and query feature spaces. CKA is proposed to measure both intra-domain and inter-domain distances ; the smaller the CKA value, the closer the feature spaces. As shown in Table 7 (Right), after applying APM, the distance between support and query is reduced, and with the addition of ACPA, the support and query feature spaces are further aligned.

### Comparison with Domain Transfer Methods

We compare our method against traditional frequency-based and correlation-based approaches to validate our method's effectiveness. For a fair comparison, all methods are implemented on the baseline HSNet  and then evaluated under the 1-shot setting on the CD-FSS benchmark.

Frequency-based methodDFF  preserves frequency information beneficial for generalization. GFNet  replaces self-attention layer with global frequency filter layer. ARP  introduces Amplitude-Phase Recombination via amplitude transformation. DAC  proposes a normalization method that removes style (amplitude) while preserving content (phase) through spectral decomposition. Although these methods improve generalization, they fall short in addressing large domain gaps. Our method requires no source domain training. It adaptively masks harmful components for the target domain at the feature level. By treating amplitude and phase separately, we exploit phase invariance to design a channel attention module that handles intra-class variations. As shown in Table 8 our method outperforms existing frequency-based approaches on the CD-FSS task.

   APM-S & APM-M & ACPA & FSS & Deepglobe & ISIC & Chest \\   & & & 0.3591 & 0.2691 & 0.2494 & 0.5848 \\ ✓ & & & 0.3481 & 0.2678 & 0.2433 & 0.5025 \\ ✓ & & ✓ & 0.2907 & 0.2676 & 0.2032 & 0.3628 \\   & ✓ & & 0.3293 & 0.2675 & 0.2310 & 0.4635 \\  & ✓ & ✓ & **0.2883** & **0.2674** & **0.1811** & **0.3986** \\   

Table 7: (Left) Feature map visualizations show: 1) APM achieves feature disentanglement. 2) ACPA encourages the model to focus on more effective features. (Right) Verify the effectiveness of the ACPA in aligning the support and query feature spaces by CKA measure.

Figure 6: Cumulative distribution function (CDF) of inter-channel correlations. After passing through APM, the CDF curve shifts to the left, indicating a decrease in inter-channel correlations.

Correlation-based methodFor methods that directly constrain the model (orthogonality, whitening): the few-shot setting means limited sample size, and existing models have a large number of parameters. Directly adjusting the model with constraints using such small datasets is not effective and even can lead to negative optimization. As seen in Table 9, the performance of orthogonality constraints(SRIP ) and whitening is not satisfactory. For feature transformation/augmentation methods like MMC : the stability is not guaranteed because they use specific feature transformation functions. Due to the domain gap, a transformation method effective for one domain may not be effective for others. In contrast, our method has the advantages of being 1) lightweight (allowing for quick adaptation in the few-shot setting) and 2) stable and robust (with adaptive adjustments for different target domains). These benefits are well reflected in the performance results.

## 5 Related Work

Few-shot learning

Few-shot learning aims to build robust representations for new concepts with limited annotated examples. Existing approaches typically fall into three categories: metric learning [36; 39], optimization-based methods [14; 33] and graph-based methods [15; 28]. Recently, cross-domain few-shot learning has gained attention due to disparities in both data distribution and label space between meta-testing and meta-training stages. BSCD-FSL  introduces a challenging benchmark for cross-domain few-shot learning, featuring a substantial domain gap between the source and target domains. It covers several target domains with varying similarities to natural images.

Few-shot semantic segmentationFew-shot semantic segmentation aims to segment unseen classes in query images with only a few annotated samples. OSLSM  is the first two-branch FSS model. Following this, PL  introduces a prototype learning paradigm utilizing cosine similarity between pixels and prototypes. SG-One  adopts masked average pooling (MAP) to optimize the extraction of support features. Recently, many FSS methods have emerged in the research community, such as RPMMs , PFENet , ASGNet , and HSNet . HSNet employs efficient 4D convolutions on multi-level feature correlations, serving as the baseline for our work. However, these methods primarily address segmenting novel classes within the same domain and struggle with generalization across disparate domains due to significant feature distribution disparities. Bridging this substantial domain gap, particularly with limited labeled data, remains a formidable challenge.

## 6 Conclusion

In this paper, we delve into the phenomenon that filtering specific frequency components based on different domains significantly improves performance, providing an interpretation through experiment and mathematical derivation. Building on our interpretation, we propose the APM, a feature-level frequency component mask designed to enhance the generalization of feature map representations. Further, we introduced ACPA. Based on the APM-optimized feature map, the ACPA encourages the model to focus on more effective features while aligning the feature spaces of the support and query. Experimental results demonstrate the approach's effectiveness in reducing domain gaps.