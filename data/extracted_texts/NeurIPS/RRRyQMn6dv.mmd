# CoSW: Conditional Sample Weighting for Smoke Segmentation with Label Noise

Lujian Yao Haitao Zhao Zhongze Wang Kaijie Zhao Jingchao Peng

School of Information Science and Engineering

East China University of Science and Technology

Shanghai, China

{lujianyao,zzwang,kjzhao,pjc}@mail.ecust.edu.cn,

haitaozhao@ecust.edu.cn

Corresponding author.

###### Abstract

Smoke segmentation is of great importance in precisely identifying the smoke location, enabling timely fire rescue and gas leak detection. However, due to the visual diversity and blurry edges of the non-grid smoke, noisy labels are almost inevitable in large-scale pixel-level smoke datasets. Noisy labels significantly impact the robustness of the model and may lead to serious accidents. Nevertheless, currently, there are no specific methods for addressing noisy labels in smoke segmentation. Smoke differs from regular objects as its _transparency varies_, causing inconsistent features in the noisy labels. In this paper, we propose a _conditional sample weighting_ (CoSW). CoSW utilizes a multi-prototype framework, where prototypes serve as prior information to apply different weighting criteria to the different feature clusters. A novel _regularized within-prototype entropy_ (RWE) is introduced to achieve CoSW and stable prototype update. The experiments show that our approach achieves SOTA performance on both real-world and synthetic noisy smoke segmentation datasets.

## 1 Introduction

Smoke segmentation holds significant research value as it enables precise localization of smoke. In wildlife, smoke serves as a vital indicator of fire, and smoke segmentation allows for rapid identification of the source of fire, facilitating prompt rescue . In industrial production, smoke segmentation can identify gas leakage thereby preventing further spread . There have been numerous methods developed for smoke segmentation, ranging from traditional approaches based on color  and smoke morphology  to deep learning techniques that involve expanding the receptive field .

However, to the best of our knowledge, there is no specific work for smoke segmentation with label noise. Noisy labels are almost inevitable in smoke segmentation. Unlike regular objects with clear and concise edges, which are easy to annotate, smoke annotation poses two main challenges: 1) Smoke edges are complex and blurry , making it hard to distinguish smoke and background. 2) Smoke is non-rigid  and lacks a fixed shape, making it difficult for annotators to become proficient through practice with the same shape.

The noisy labels can have a large impact on the robustness of the model due to their strong memorization power . Given that smoke segmentation is related to safety problems, errors stemming from the instability can lead to significant disasters, resulting in extensive casualties and property losses. Therefore, it is crucial to develop robust training to mitigate the noisy labels in smoke segmentation.

However, most existing methods primarily focus on noisy labels in classification tasks. Even if a few methods [60; 63] for segmentation, they directly apply classification methods and assume that the noise at the pixel level is also _i.i.d._ (independent and identically distributed).

This assumption is not realistic in smoke segmentation, primarily due to the issue of _variable transparency_ in smoke. As depicted in Fig. 1, this problem is ubiquitous in smoke images, mainly caused by the density, size of smoke particles, and lighting conditions, resulting in _inconsistent features_ of noisy labels. Noisy labels in the high transparency regions exhibit similarities to the surrounding clean labels, while in the opaque regions, they differ significantly. Treating both types of noisy labels using the same criteria would reduce the accuracy of identifying noisy labels. Moreover, areas with high transparency are more prone to noisy labels and require refined criteria.

In this paper, we propose a _conditional sample weighting_ (CoSW), which employs different weighting criteria in different feature clusters to address the problem of feature inconsistency. As illustrated in Fig. 1d, CoSW is built upon a multi-prototype framework and regards prototype as _prior information_ for determining the weighting criteria to obtain finer distinctions between samples. Pixel features matching to the same prototype are a feature cluster. Prototype learning is intuitive and concise, with its roots tracing back to the nearest neighbor algorithm . Importantly, by utilizing multiple prototypes, we can establish a polycentric pattern that structures and covers embedding space [4; 62], including both highly transparent and low transparent features.

Under the framework of multi-prototype, CoSW needs to tackle two problems: 1 How to determine the sample weight through prototypes. 2 How to update prototypes under noisy labels. The most related method is CleanNet , which identifies noisy labels based on the similarity between prototypes and samples. However, it only utilizes a single prototype for each class and solely considers individual samples, neglecting the holistic information. As a consequence, it can not dynamically adjust the weighting.

In order to obtain comprehensive information of the samples, a _regularized within-prototype entropy_ (RWE) is proposed to address these two problems in a unified manner. Entropy can be used for uncertainty measurement and the maximum entropy principle (MaxEnt) allows for the consideration of the entire probability distribution function (PDF) with minimal empirical risk, rather than focusing solely on individual samples. RWE uses prototypes as anchors to build a separate noisy-level evaluation system for each feature cluster. By maximizing RWE, we can consider the information between all pixels that matched the given prototype and obtain adaptive sample weights (for 1). Furthermore, by calculating the expectation of weighting samples, stable prototypes can be obtained (for 2).

To demonstrate the robustness of our approach, we conduct experiments on both real-world (Smoke-Seg  and SMOKE5K ) and a synthetic (NS-1K) noisy smoke segmentation dataset. Methods from different fields (semantic segmentation, smoke segmentation, segmentation with label noise, and sample weighting) are compared to demonstrate the superiority of our approach.

Our main contributions can be summarized as follows:

Figure 1: (a) presents the noisy labels in smoke segmentation (blue for noisy labels, and red for clean). (b) shows the pixel features extracted by the encoder of SAM . Noisy labels are variably distributed, with some located along class boundaries and others spread across different regions within the smoke. Additionally, the features demonstrate a polycentric distribution, making them well-suited for prototype-based modeling. In (c), CoSW assigns sample weights that effectively identify regions with noisy labels and reduce their weights. (d) shows the intuition of CoSW.

To the best of our knowledge, we are the first to investigate noisy labels in smoke segmentation.
* We propose a CoSW that serves prototypes as prior information to apply different weighting criteria to the different feature clusters.
* A novel RWE is introduced to implement CoSW, which achieves adaptive sample weighting and stable prototype update in a concise and unified way.
* Our approach achieves SOTA results on real-world and synthetic noisy smoke datasets.

## 2 Related Work

### Smoke Segmentation

Traditional smoke segmentation approaches mainly concentrate on extracting color and texture features, including color channel analysis  and color enhancement . Additionally, vision-based techniques like morphological operations , transmission estimation , and region growing  have been also widely utilized. Deep-learning-based smoke segmentation methods tackle various challenges related to smoke diversity and ambiguous edges. These approaches encompass multiple aspects, such as 1) fusion of high and low-level features [49; 54; 55], 2) expanding the receptive field [22; 28], and 3) employing coarse-to-fine strategies [56; 59]. 4) uncertainty estimation . However, to the best of our knowledge, there is no work that addresses noisy labels in smoke segmentation.

### Sample Importance Weighting

The sample importance weighting (SIW) assigns low weights to potentially mislabeled samples and high weights to potentially confident samples [20; 33; 38]. CleanNet  introduces prototype learning for SIW, but it solely relies on the similarity between individual samples and prototypes, neglecting the information from other samples. And it employs only one prototype for each class. There are other techniques like Meta-learning [36; 37; 46], teacher-student architecture , iteratively training , and transfer learning  for SIW. Nevertheless, these methods are all targeted at image-level classification, failing to address the issue of inconsistent features in smoke segmentation with noisy labels.

### Prototype Learning

The pioneer of prototype learning is the nearest neighbor algorithm . Building upon this, many nonparametric classification methods have been proposed, including learning vector quantization (LVQ) , and neighborhood component analysis (NCA) . In recent years, the concept of prototype learning has also been incorporated into deep learning as it effectively structures and covers the embedding space using a polycentric pattern. Research fields include supervised , unsupervised , and self-supervised learning . In image segmentation, prototype learning also gains significant attention [9; 43; 62]. However, prototypes are less investigated in the field of noisy labels, and how to update prototypes in a noisy dataset remains an unresolved issue.

### Metric Learning

Metric learning maps raw data to an embedding space where similar features are pulled close while dissimilar ones are pushed away. Metric learning and prototype learning can be naturally linked. Some cluster-based methods are using one [24; 32] or multiple [34; 64] learnable prototypes to represent the entire class information. They achieve such mapping through a specific loss function such as contrastive loss [3; 14] and triplet loss [35; 42]. However, employing metric learning without regularization can be detrimental under noisy labels [2; 58].

Conditional Sample Weighting (CoSW)

### Intuition and Overview

Previous research on learning with noisy labels primarily targets classification, with scant attention to segmentation, often presuming noise to be _i.i.d._ However, the noisy labels in smoke segmentation are different due to the variable transparency, resulting in _inconsistent features_ within the noisy labels. The objective of CoSW is to apply different weighting criteria to different feature clusters. We achieve CoSW by introducing a regularized within-prototype entropy (RWE). In this section, we first review the concepts of entropy and then introduce a within-prototype entropy (WE) and its regularized form, RWE. Finally, the specific formulation of CoSW is presented and illustrated by a toy demo.

### Preliminary: Entropy and MaxEnt

The concept of entropy originates in the realm of thermodynamics, but Shannon has a broader its meaning to the information theory. Entropy can be utilized for measuring uncertainty. For a probability distribution \(=(_{1},_{2},...,_{N})\) of \(N\) random variables \(\{x_{1},x_{2},...,x_{N}\}\), Shannon measures the uncertainty for this distribution by \(T()=-_{i=1}^{N}_{i}_{i}\), with the constrain \(_{i=1}^{N}_{i}=1\).

There is an infinity of probability distributions satisfying the constraint. While maximum entropy theory (MaxEnt)  states that, under a given set of constraints, the probability distribution with maximum entropy is the most representative of the current knowledge of a system. Specifically, MaxEnt allows for the consideration of the entire probability distribution with minimal empirical risk, rather than focusing solely on individual samples.

### Within-proto & Between-proto Entropy

**Intuition.** Only use probability do not take prototype information into account. Hence, we derive a generalized entropy for prototype information.

**Detail.** Each class \(\{_{1},_{2},,_{}\}\) is represented by \(K\) prototypes \([^{k}]_{k=1}^{K}\) and thus we have \( K\) prototypes in total. We give each pixel feature \(_{n}^{k}^{D}\) a likelihood value \(v_{n}^{k}\) and let \(_{n=1}^{N^{k}}v_{n}^{k}=N^{k}\) and \(_{k=1}^{ K}_{n=1}^{N^{k}}v_{n}^{k}=_{k=1}^{ K}N^{k}=N\). Since \(_{n=1}^{N^{k}}v_{n}^{k}/N^{k}=1\) and \(_{k=1}^{ K}_{n=1}^{N^{k}}v_{n}^{k}/N=1\), we can define the following entropy based on the Shannon entropy:

Total Entropy:

\[T=-_{k=1}^{ K}_{n=1}^{N^{k}}^{k}}{N}^{k }}{N},\] (1)

**Within-prototype Entropy (WE):**

\[T_{w}=-_{k=1}^{ K}}{N}_{n=1}^{N^{k}}^{k}} {N^{k}}(^{k}}{N^{k}}),\] (2)

Between-prototype Entropy:

\[T_{b}=-_{k=1}^{ K}}{N}}{N},\] (3)

It can be proven that \(T=T_{w}+T_{b}\). Additionally, we also provide the WE derived from different entropies (_Burg's entropy_ and _Kapur's entropy_) as the basis.

### Regularized Within-prototype Entropy (RWE)

**Intuition.** Without adding additional constraints, the within-prototype entropy is maximized when all pixel features have the same likelihood value (_i.e._, \(v_{n}^{k}=1\)). To tackle the issue of noisy labels, we integrate M-estimation  into within-prototype entropy, assigning low weights to noisy labels. The key of original M-estimation is to estimate the mean vector under the noise data. However, in deep learning, obtaining the overall mean of samples becomes challenging due to mini-batch training.

Nevertheless, we observe a resemblance between this idea and the concept of prototype learning, where prototypes act as feature representations.

**Detail.** We substituted the mean vector with prototypes to form the constraints:

\[}_{M}(_{n}^{k})=_{}_{n}^{N^{k}}q(_{n}^{k }-^{k})^{k}}{N^{k}},\] (4)

where \(^{k}\) represents the corresponding prototype of \(_{n}^{k}\), and \(q()\) denotes the penalty function that measures the influence of the residual error \(_{n}^{k}-^{k}\). By incorporating this constraint, we can maximize the following objective function in conjunction with the within-prototype entropy:

\[_{P,V}J(P,V)=-_{k=1}^{ K}}{N}_{n=1}^{N^{k}}^{k}}{N^{k}}(^{k}}{N^{k}})-_{k=1}^{ K }}{N}_{n=1}^{N^{k}}^{k}}{N^{k}}\|_{n}^{k}-^{k}\|_{2},\] (5)

with the constraint \(_{n=1}^{N^{k}}v_{n}^{k}=N^{k}\), where \(N^{k}\) denotes the number of pixel features belonging to \(\). The penalty function \(q()\) employs the \(L_{2}\) norm. The \(\) is a _regularization parameter_ which controls the degree of punishment. The function \(J(P,V)\) in Eq. 5 is called _regularized within-prototype entropy_ (RWE).

**Convergence of the RWE.** The Eq. 5 can be enlarged by:

\[J(P,V)-_{k=1}^{ K}}{N}_{n=1}^{N^{k}}^ {k}}{N^{k}}(^{k}}{N^{k}})_{k=1}^{  K}N^{k} N^{k},\] (6)

This proves RWE has an upper bound. According to Cauchy's convergence rule, the RWE is convergent.

### Conditional Sample Weighting (CoSW)

The CoSW can be obtained by solving Eq. 5, and the likelihood \(v_{n}^{k}\) is regarded as the weighting of the sample. We can transform the Eq. 5 to incorporate constraints by Lagrange multipliers and obtain the CoSW \(v_{n}^{k}\) of each sample and the objective values \(}^{k}\) for prototype update:

\[v_{n}^{k}=N^{k}_{n}^{k}-^{k}\|_{2})}{_{n= 1}^{N^{k}}(-\|_{n}^{k}-^{k}\|_{2})},\] (7)

\[}^{k}=^{N^{k}}_{n}^{k}(-\|_{n }^{k}-^{k}\|_{2})}{_{n=1}^{N^{k}}(-\|_{n}^{k}- ^{k}\|_{2})},\] (8)

The derivation can be found in the Appendix A.

**Toy Demo.** We provide a toy demo to demonstrate CoSW clearly. Let \(X_{1}=\{1,1,1,1,\}\) and \(X_{2}=\{-1,-1,-1,-1,\}\) be two feature clusters that match to the prototype \(p_{1}=\{1\}\) and \(p_{2}=\{-1\}\), respectively. Assume \(X_{1}\) contains an _obvious_ noisy label \(\{5\}\) and \(X_{2}\) contains an _covert_ noisy label \(\{0\}\). For normal sample weighting approaches (such as CleanNet), the weights are

Figure 2: Architecture illustration of CoSW for smoke segmentation during the training process.

calculated by comparing each sample with the center (\(\{0.5\}\)) of two clusters. The weight of the two noisy labels can be calculated as \(v_{(5)}=0.0282\) and \(v_{(0)}=1.5409\), respectively. This method assigns an excessively high weight to the noisy label \(\{0\}\). When we use CoSW, which involves using two prototypes to weigh the importance of each feature separately, we can calculate the \(v_{(5)}^{p_{1}}=0.0228\) and \(v_{(0)}^{p_{2}}=0.4211\) through Eq. 7. The noticeable decrease in the weight of the noisy label \(\{0\}\). For _prototype updating_, in the case of \(X_{1}\), if CoSW is not applied and only the mean is used for updating, the new prototype \(p_{1}^{{}^{}}=\{1.8\}\), which differs from the original value of \(\{1\}\). When CoSW is applied and the noisy label \(\{5\}\) is given a weight, the new prototype is \(p_{1}^{{}^{}}=\{1.0067\}\) (_cf._ Eq. 8), indicating minimal impact on its origin value.

## 4 CoSW for Smoke Segmentation

In this section, we introduce how to apply CoSW to smoke segmentation, including pixel-proto matching, loss design, prototype updating, and regularized scatter metric learning. The entire process is illustrated in Fig. 2.

### Pixel-Prototype Matching

**Intuition.** For multi-prototype methods, a crucial task is to match pixels to prototypes. The simplest approach is adopting the nearest neighbors principle. However, this can lead to a large number of pixels being assigned to the same prototype, while the remaining prototypes are left without any matching pixels. Therefore, we apply additional constraints to the original optimization problem to prevent the occurrence of trivial solutions. This part refers to the matching process in ProtoSeg  and SwAV .

**Detail.** The goal is to match the pixel features \(^{}\) to one of the prototypes in class \(\). The matching strategy is denoted as \(^{}^{K N^{}}\). The \(^{}\) is optimized by minimizing the overall distance between each pixel feature (_i.e._, \(^{}^{D N^{}}=[_{n}]_{n=1}^{N}\)) and its matched prototype (_i.e._, \(^{}^{D K}=[_{k}^{}]_{k=1}^{K}\)):

\[_{^{}}(^{}^{}),\] (9) _s.t._ \[^{}\!\!\{0,1\}^{K N^{}},\;^{ }^{K}=^{N^{}},^{}^{N^{}}=}{K}^{K},\]

where \(^{K}\) and \(^{N^{}}\) denote all-one vectors. \(^{}^{K}=^{N^{}}\) is a unique matching constrain which guarantees each pixel feature is assigned to only one prototype. \(^{}^{N^{}}=}{K}^{K}\) is an equipartition constraint  which avoids the trivial solution in which all pixels are assigned to a single prototype. And \(^{}^{K N^{}}\) represents the cost matrix that measures the distance between pixel features and prototypes.

Eq. 9 is a typical transportation problem  which can be easily calculated by the iterative Sinkhorn-Knopp algorithm . Specific derivation and implementation can be found in the Appendix C.

### Sample Weighting and Prototype Update

In the pipeline of smoke segmentation, CoSW is employed in the loss function. We incorporate CoSW into the basic cross-entropy loss:

\[_{}=_{n=1}^{N}v_{n}(},y_{n}),\] (10)

where \(v_{n}\) refers to Eq. 7. \(}\) and \(y_{n}\) respectively represent the predicted value and ground truth for each pixel in a mini-batch. \(N\) represents the number of pixels in a mini-batch.

As for prototype update, it is challenging to ensure stability under noisy labels, as shown in Fig. 3. Our objective for prototype updating is \(}^{k}\) (_cf._ Eq. 8) with CoSW. To ensure stable training, we also

Figure 3: Regularized prototype update.

incorporate the update in a momentum way :

\[}^{k}^{k}+(1-)}^{k};k=1,2,,  K,\] (11)

where \(\) is a momentum hyper-parameter.

### Regularized Scatter Metric Learning (RSML)

**Intuition.** In prototype framework, metric learning is a widely used and powerful tool  that brings similar features closer and pushes dissimilar ones apart, enabling the acquisition of a discriminative embedding space. However, due to the presence of noisy labels, metric learning may lead to overfitting. Therefore, we further incorporate the weighting into the _scatter matrix_ to ensure the effectiveness of metric learning. The scatter matrix can capture the dispersion information of the samples .

**Detail.** The way we implemented it is by incorporating CoSW into the Within-prototype Scatter Matrix (WSM). The WSM serves as a representation of the dispersion of samples in relation to their corresponding prototypes, allowing for the assessment of the compactness within each prototype. We integrate CoSW \(v_{n}^{k}\) (_cf._ Eq. 7) into WSM for regularization:

\[_{w}=_{k=1}^{ K}_{n=1}^{N^{b}}^{k}}{N}(_ {n}^{k}-^{k})(_{n}^{k}-^{k})^{T},\] (12)

For the Between-prototype Scatter Matrix (BSM), we do not apply weighting. The BSM can be employed to quantify the separability between prototypes and features. We employ the non-parametric strategy in which the scatter matrix is calculated between each pixel feature and its nearest neighbor prototype. The BSM is defined as follows:

\[_{b}=_{n=1}^{N}(_{n}^{_{0}}-_{NN}(_{n}))(_{n}^{}-_{NN}(_{n}^{}))^{T},\] (13)

where \(_{NN}(_{n})\) is the nearest neighbor prototype of \(_{n}\).

Finally, we employ the ratio-trace form  to transform the scatter matrices into a numeric value: \(_{}=(_{w}^{}-_{b})},\) where \(_{w}^{}=_{w}+\) and the \(\) is added to guarantee the reversibility of \(_{w}\) and we set \(=10^{-5}\). DeepLDA  has demonstrated that this trace form can be optimized using gradient descent. Our final objective function is a combination of weighted cross-entropy loss \(_{}\) (_cf._ Eq. 10) and scatter loss \(_{}\): \(=_{}+_{},\) where \(\) is a weight hyperparameter.

## 5 Experiments

### Experimental Setup

**Real-world Noise Setting.** Due to the visual diversity and blurry edges of smoke, label noise in the large-scale real-world smoke datasets (SmokeSeg  and SMOKE5K ) is ubiquitous. Hence, we conduct experiments on both datasets as real-world noise evaluation. To accurately measure the robustness of the model, a clean validation set is essential. Therefore, we carefully re-annotate the validation set of both datasets. The distinction between SmokeSeg and Smoke5K resides in the

Figure 4: Examples of the clean and three corrupted masks in NS-1K.

unique smoke samples in SmokeSeg, which display high and variable transparency. Consequently, SmokeSeg offers a better assessment of the robustness of the model.

**Synthetic Noise Setting (NS-1K).** To further investigate the robustness of the model to noise, we also create a synthetic noise smoke segmentation dataset called NS-1K. We select 1,000 images from SmokeSeg and carefully re-annotate them to obtain clean labels. Among them, 700 images are used for training, and 300 images are used for validation. Then, we add noise to this dataset in different forms, including eroded, dilated, and edge-distorted noise, as shown in Fig. 4. It is noted that edge-distorted is implemented by randomly dilating or eroding the pixels on the boundary with a circle. In the experiment, we set two noise parameters: one is the ratio of noise data (20%, 40%, 60%, 80%), and the other is the intensity of the noise data (high and low). The intensity of the noise is determined by adjusting the degree of pixel displacement for three noise types.

**Implementation Details.** We implement our method on MMSegmentation. Standard color jittering, random cropping, and random flipping are adopted for data augmentation during the training stage. All experimental backbones are pre-trained on ImageNet-1K. We utilize the _AdamW_ optimizer, with learning rate starting at 6e-5 and scheduled according to the polynomial annealing policy. For the SmokeSeg, we crop images to a size of \(512 512\) for training, while for SMOKE5K, we follow the previous methods and resize the images to \(480 480\). For validation, we use the whole inference method, and for simplicity, we do not use any data augmentation during the validation.

**Evaluation Metric.** Following previous literature, we employ \(F_{1}\) and \(mIoU\) for evaluation.

Table 1: Quantitative comparison on the real-world noisy dataset SmokeSeg. We compare methods from various domains, including semantic segmentation (\(\)), smoke segmentation (\(\)), segmentation with label noise (\(@sectionsign\)), and sample weighting (\(\)). The best: **bold**, the second: underline.

Table 2: Comparison on the real-world noisy dataset SMOKE5K and synthetic noisy dataset NS-1K. Notation: semantic segmentation (\(\)), smoke segmentation (\(\)), segmentation with label noise (\(@sectionsign\)), sample weighting (\(\)). The best: **bold**, the second: underline.

### Comparison on Real-world Label Noise

Tab. 1 and Tab. 1(a) present the comparative results on the real datasets SmokeSeg and SMOKE5K, respectively. We compare our method with methods from different domains, including general semantic segmentation, SOTA method Trans-BVM  for smoke segmentation, segmentation with noisy labels method, SC , and the sample weighting method, CleanNet . The results demonstrate that our method achieves the best performance in both datasets. Fig. 4(a) presents the segmentation results of different methods. The Trans-BVM exhibits high miss detection, while CleanNet is prone to false alarms. In contrast, our method demonstrates the best performance.

### Comparison on Synthetic Label Noise

Tab. 1(b) reports the comparative results of different methods on a synthetic noisy dataset NS-1K. When the dataset is clean or contains only a small amount of low-level noise, Trans-BVM performs the best, as it is specifically designed for smoke detection. However, as the ratio and intensity of noise increase, the performance of the Trans-BVM rapidly deteriorates. In contrast, our method maintains robustness. In the case of maximum noise, our method achieves approximately higher \(F_{1}\) compared to Trans-BVM.

### Investigation on CoSW

**Quantitive Ablation.** To investigate the effect of CoSW, we conduct quantitive ablation, as illustrated in Tab. 2(a). In the ablation study, the baseline is SegFormer. Tab. 2(a) replaces the decoder head of SegFormer with prototypes. And there is improvement in model performance, indicating that prototypes inherently provide robustness. In Tab. 2(a), we incorporate CoSW into the samples, resulting in a significant performance improvement. Tab. 2(a) focus on utilizing CoSW for prototype updates, as stable updates can also enhance robustness. Finally, in Tab. 2(a), we combine both approaches, resulting in the best performance achieved.

Table 3: Ablation study of CoSW and metric learning.

Figure 5: The comparison of segmentation results in different scales and the formation of CoSW.

**How CoSW Formation.** To investigate the reasons behind the effects of CoSW, we visualize the formation process of CoSW, as shown in Fig 4(b). It can be observed that as training progresses, CoSW gradually forms its own confidence area. It reduces the weight of the noisy label parts, and even more so for highly transparent areas, as these regions are more prone to noise. This indicates that with the aid of CoSW, the model has developed its own recognition of smoke, rather than being completely governed by labels. This is likely the reason for its robustness against noisy labels. However, the CoSW requires the assumption that the majority of pixels have clean labels. When the label mask is completely noisy, the model is also unable to distinguish the noisy labels, because it can not learn which features are the common of smoke.

**Qualitative Comparation.** Fig. 6 illustrates the qualitative comparison among prototype-based sample weighting methods, CleanNet, and our CoSW. Although CleanNet can provide a confidence area similar to that of CoSW, its area is less precise. Moreover, the weighting area of CleanNet changes more abruptly and lacks specificity. In contrast, CoSW differentiates between transparent and opaque areas: the weighting in transparent areas is gradual, while it is steeper in transparent areas.

### Further Investigation

**Effect of Regularized Scatter Metric Learning.** In Sec. 4.3, we introduce RSML, designed specifically for metric learning under noisy labels. Tab. 2(b) demonstrates the effect of RSML. It can be observed that using the triplet loss directly in metric learning under noisy labels diminishes the performance of the model. However, incorporating CoSW leads to improved results, and the performance is further enhanced when using the _scatter matrix_ as a metric for evaluation.

**Different Entropies.** Our RWE (_cf._ Eq. 5) utilizes Shannon entropy as its foundation in this paper. We have also experimented with different entropies, such as Kapur's entropy and Burg's entropy. The results are shown in Tab. 4. The derivation process is provided in the Appendix B.

## 6 Conclusion

Smoke annotation is prone to noisy labels, which can lead to model instability and result in serious accidents. However, existing methods have not addressed the issue of noisy labels in this field. In this paper, we introduce _conditional sample weighting_ (CoSW) to address inconsistent noisy labels in smoke segmentation. CoSW utilizes prototypes as prior information and measures each feature cluster with different criteria to re-weight the samples adaptively. Experimental results show that our CoSW achieves the best performance on both real and synthetic noisy smoke segmentation datasets.

**Limitations.** The CoSW relies on the assumption that the majority of pixels have clean labels. If the degree of noise is very high, the prototype may learn the features of the noise rather than the classes (background and smoke). To determine whether the prototype is clean, it is necessary to introduce clean validation, which is not implemented in our work. This is also a direction for further research.