# VLM4Bio: A Benchmark Dataset to Evaluate

**Pretrained Vision-Language Models for Trait**

**Discovery from Biological Images**

**M. Maruf\({}^{1*}\)** **Arka Daw\({}^{2*}\)** **Kazi Sajeed Mehrab\({}^{1}\)** **Harish Babu Manogaran\({}^{1}\)**

**Abhilash Neog\({}^{1}\)** **Medha Sawhney\({}^{1}\)** **Mridul Khurana\({}^{1}\)** **James P. Balhoff\({}^{3}\)**

**Yasin Bakus\({}^{4}\)** **Bahadir Altintas\({}^{4}\)** **Matthew J Thompson\({}^{5}\)** **Elizabeth G Campolongo\({}^{5}\)**

**Josef C. Uyeda\({}^{1}\)** **Hilmar Lapp\({}^{6}\)** **Henry L. Bart Jr.\({}^{4}\)** **Paula M. Mabee\({}^{7}\)**

**Yu Su\({}^{5}\)** **Wei-Lun Chao\({}^{5}\)** **Charles Stewart\({}^{8}\)** **Tanya Berger-Wolf\({}^{5}\)**

**Wasila Dahdul\({}^{9}\)** **Anuj Karpatne\({}^{1}\)**

\({}^{1}\)Virginia Tech \({}^{2}\)Oak Ridge National Laboratory \({}^{3}\)UNC Chapel Hill

\({}^{4}\)Tulane University \({}^{5}\)Ohio State University \({}^{6}\)Duke University \({}^{7}\)Battelle

\({}^{8}\)Rensselaer Polytechnic Institute \({}^{9}\)UC Irvine

{marufm,darka,karpatne}@vt.edu

**Abstract**

Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, **VLM4Bio**, consisting of \(469K\) question-answer pairs involving \(30K\) images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images. 1

Figure 1: Overview of our goals and contributions. We analyze the capabilities of 12 state-of-the-art (SOTA) vision-language models (VLMs) in answering scientific questions using images from three groups of organisms: fishes, birds, and butterflies, over five groups of biologically relevant tasks. We also explore the effectiveness of these models for reasoning using various prompting techniques and tests for reasoning hallucination.

Introduction

There is a growing deluge of images that are being collected, stored, and shared in organismal biology--the branch of biology interested in the study of structure, ecology, and evolution of organisms. In particular, images are increasingly becoming the currency for documenting the vast array of biodiverse organisms on our planet, with repositories containing millions of images of biological specimens collected by scientists in field museums or captured by drones, camera traps, or tourists posting photos on social media. This growing wealth of biological images provides a unique opportunity to understand the scientific mechanisms of how organisms evolve and adapt to their environment directly from images. The traditional approach for advancing knowledge in organismal biology is by discovering the observable characteristics of organisms or _traits_ (e.g., beak color, stripe pattern, and fin curvature) that serve a variety of biological tasks such as defining groups of organisms, understanding their genetic and developmental underpinnings, and analyzing their interactions with environmental selection pressures . However, the measurement of traits is not straightforward and often relies on expert visual attention involving labor-intensive operations and subjective definitions , hindering rapid scientific advancement .

With the recent rise of large foundation models such as vision-language models (VLMs) (e.g., GPT-4, GPT-4V(ision) [4; 5], Gemini , LLaMA 3.2 [7; 8], and LLaVA ) that can simultaneously solve a diverse range of tasks involving text and images, it is pertinent to ask if pre-trained VLMs contain the necessary _scientific knowledge_ to aid biologists in answering a variety of questions pertinent to the discovery of biological traits from images. Note that unlike mainstream tasks in computer vision, understanding scientific images requires knowledge of domain-specific terminologies and reasoning capabilities that are not fully represented in conventional image datasets used for training VLMs. In particular, an important end-goal in scientific applications such as organismal biology is to explain the process of visual reasoning used to arrive at a prediction, often involving the knowledge of biological traits. Hence, to assess the usefulness of VLMs in accelerating discoveries in organismal biology, it is important to test their ability to identify and reason about biological traits automatically from images.

In this work, we assess the zero-shot capabilities of 12 state-of-the-art (SOTA) VLMs, including the proprietary GPT-4V(ision) and the recent GPT-4O(mni) along with other open-source VLMs, on five scientifically relevant tasks in organismal biology, namely species classification, trait identification, trait grounding, trait referring, and trait counting. These tasks are designed to test different facets of VLM performance in organismal biology, ranging from measuring predictive accuracy to assessing their ability to reason about their predictions using visual cues of known biological traits. For example, the task of species classification tests the ability of VLMs to discriminate between species, while in trait grounding and referring, we specifically test if VLMs are able to localize morphological traits (e.g., the presence of fins or patterns and colors of birds) within the image. To perform this evaluation, we present **VLM4Bio**, a benchmark dataset of \( 469K\) question-answer pairs based on \(30k\) images of three taxonomic groups of organisms: fishes, birds, and butterflies.

**Main Contributions:**

1. We present a novel dataset of scientific question-answer pairs to evaluate the effectiveness of VLMs in answering scientific questions across a range of biologically relevant tasks in the field of organismal biology.
2. We present novel benchmarking analyses of the zero-shot effectiveness of pre-trained SOTA VLMs on our dataset, exposing their gaps in advancing scientific knowledge of organismal biology.
3. We present novel comparisons studying the effects of prompting and tests for reasoning hallucination on VLM performance, shedding new light on the reasoning capabilities of SOTA VLMs in organismal biology.

## 2 Related Works

With the rise of SOTA VLMs such as GPT-4V(ision) , GPT-4O(mni) , and Gemini , there has been a simultaneous growth in the number of benchmarking analyses published in the last few years to evaluate different facets of VLM performance on a range of mainstream tasks in computer vision. A majority of previous analyses [11; 12] involve evaluations on single tasks like VisualQuestion Answering (VQA), OK-VQA , MSCOCO , and GQA . Other datasets such as POPE , HaELM , LAMM , MMBench , MM-Vet , LVLM-eHub , SEED , and GAIA  have also been developed to evaluate the capabilities of VLMs on complex tasks such as reasoning and ability to handle multimodal data. There are also some recent domain-specific benchmark datasets, such as MathVista , which includes a variety of challenging VQA problems in the mathematical domain, MedQA(USMLE)  which is a collection of VQA problems from medical exams, and the recent MMMU  dataset, which covers expert-level problems from diverse fields such as business, arts, science, health, medicine, and engineering.

VLM4Bio dataset is different from existing benchmarks involving domain-specific datasets because of the following reasons. (1) _Focus on organismal biology_: While previous works have focused on benchmarking the performance of VLMs on other scientific domains (e.g., Arts and Design, Business, Health, and Medicine in MMMU  or Mathematics in MathVista ), there exists no previous VQA benchmark dataset in the domain of organismal biology to the best of our knowledge. Our work thus fills a critical gap in evaluating the performance of VLMs in a field of biology that has several societal implications such as monitoring biodiversity and understanding the impact of climate change on species traits and populations. (2) _Breadth of Evaluation Tasks_: While previous works are tailored to one or a few evaluation tasks, we consider a wide range of tasks motivated by the needs of domain scientists in the field of organismal biology. They include predictive tasks such as species classification and trait identification as well as tasks that require visual reasoning including trait grounding and referring. We also provide novel comparisons about the performance of VLMs on both open-ended and multiple-choice question (MCQ) formats and comparisons over predictive as well as visual reasoning tasks, in contrast to prior works.

## 3 VLM4Bio Tasks

Figure 2 shows illustrative examples of the five VLM4Bio tasks relevant to biologists that we consider in our study, described in detail in the following.

### Species Classification

A common (and often the first) task that a biologist considers when examining an organism specimen is to identify its scientific name (or species class). Hence, we consider asking a VLM to provide the scientific name of the organism shown in a given image. There are two types of questions that we consider for this task. First, we consider _open-ended questions_, where we do not provide any answer choices (or options) to the VLM in the input prompt. The second type is _multiple-choice

Figure 2: Illustrative examples of **VLM4Bio** tasks with different question-types.

_(MC) questions_, where we provide four choices of candidate species names for the VLM to choose from (out of which only one is correct while the remaining three are randomly selected from the set of all species classes).

### Trait Identification

An important goal in organismal biology is to answer questions regarding the observable characteristics of organisms, also known as traits. We thus consider asking VLMs to identify a particular trait of an organism given its image for two taxonomic groups: fishes and birds. For fishes, we considered 10 binary (presence/absence) traits and generated MC questions for the presence of each trait in an image (with two options: yes or no), whereas for birds, we considered 28 traits covering their color, pattern, and measurements (size and shape of regions) in a multiple-choice format. We provide a detailed list of all fish and bird traits in the Supplementary Section F.

### Trait Grounding and Referring

To further understand the ability of VLMs to visually explain the reasoning behind their prediction of a trait, it is important to evaluate if a VLM correctly identifies the region in the image containing the trait. For this purpose, we consider two other tasks: trait grounding & trait referring, for the taxonomic groups of fishes and birds. In the first task of trait grounding, we ask the VLM to locate a given trait of an organism on its image (i.e., _text to location_). We consider MC question-format for this task where we provide four options of bounding boxes in the image as candidate answer choices, where one of the bounding boxes correctly contains the trait while the remaining three are randomly sampled from the set of bounding boxes containing other traits of the organism. In the second task of trait referring, we consider the opposite scenario where we provide a bounding box as input to the VLM and ask it to identify the name of the trait present in the bounding box (i.e., _location to text_). We again provide four answer choices in MC question-format, where only one of the options is correct while the remaining three are randomly sampled from the names of other traits of the organism.

### Trait Counting

We simply ask how many traits are present in an image of a fish specimen. This is biologically relevant, for example, to understand the number of fins present in a fish organism. Similar to the species classification task, we have open and MC question-types for this task.

## 4 VLM4Bio Dataset

**Data Collection and Preprocessing**: We collected images of three taxonomic groups of organisms: fish, birds, and butterflies, each containing around \(10K\) images. Images for fish (**Fish-10K**) were curated from the larger image collection, FishAIR , which contains images from the Great Lakes Invasive Network Project (GLIN) . These images originate from various museum collections such as INHS , FMNH , OSUM , JFBM , UMMZ  and UWZM . We created the Fish-10K dataset by randomly sampling \(10K\) images and preprocessing the images to crop and remove the background. For consistency, we leverage GroundingDINO  to crop the fish body from the background and Segment Anything Model (SAM)  to remove the background. We curated the images for butterflies (**Butterfly-10K**) from the Jiggins Heliconius Collection dataset , which has images collected from various sources 2. We carefully sampled \(10K\) images for Butterfly-10K from the entire collection to ensure the images capture unique specimens and represent a diverse set of species by adopting the following two steps. First, we filter out images with more than one image from the same view (i.e., dorsal or ventral). Second, we ensure each species has a minimum of \(20\) images and no more than \(2,000\) images. The images for birds (**Bird-10K**) are obtained from the CUB-200-2011  dataset by taking 190 species for which the common name to scientific name mapping is available. This results in a fairly balanced dataset with around \(11K\) images in total. Additional details on dataset preprocessing are provided in the Supplementary Section A.

**Annotation:** The scientific names for the images of Fish-10K and Butterfly-10K were obtained directly from their respective sources. For Bird-10K, we obtained the scientific names from the iNatLoc500  dataset. We curated around \(31K\) question-answer pairs in both open and multiple-choice (MC) question formats for evaluating species classification tasks. The species-level trait presence/absence matrix for Fish-10K was manually curated with the help of biological experts co-authored in this paper. We leveraged the Phenoscape knowledge  base with manual annotations to procure the presence-absence trait matrix. For Bird-10K, we obtained the trait matrix from the attribute annotations provided along with CUB-200-2011. We constructed approximately \(380K\) question-answer pairs for trait identification tasks. For grounding and referring VQA tasks, the ground truths were manually annotated with the help of expert biologists on our team. We manually annotated bounding boxes corresponding to the traits of 500 fish specimens and 500 bird specimens, which are subsets of the larger Fish-10K and Bird-10K datasets, respectively. In particular, we considered \(8\) fish traits and \(5\) bird traits for annotating their bounding boxes, resulting in a total of \(26K\) question-answer pairs. We also used the Fish-500 dataset for the task of trait counting, resulting in a total of \(1K\) question-answer pairs. Across all tasks, our dataset comprises approximately \(469K\) question-answer pairs for \(30K\) biological images (see Table 1). Additional details on data distribution and key statistics are provided in the Supplementary Section E.

**Dataset Card:** We provide the dataset card with a detailed description of the metadata, data instances, annotation, and license information here (https://huggingface.co/datasets/imageomics/VLM4Bio#dataset-card-for-vlm4bio).

**VLM Baselines:** We consider the following VLM baselines: GPT-4V(ision) 3, LLaVA-v1.5 (7B/13B) , COG-VLM , MiniGPT-4 (Vicuna 7B/13B) , BLIP-FLAN-T5-XL/XXL , and INSTRUCT-BLIP (Vicuna 7B/13B) . We used the latest checkpoints for each model available to date. We used the same question prompt for all models to ensure consistent comparison of results for a variety of open and multiple-choice (MC) questions across the five scientific tasks of our dataset. All the experiments were conducted using NVIDIA A100 GPUs. See supplementary Section H for more details of the VLM baselines.

**Evaluation Metrics:** We used micro-averaged accuracy as our evaluation metric for all experiments. We designed a systematic rule-based evaluation pipeline to evaluate VLM responses against the ground truths. For each question category, we provide the accuracy percentage of random choice as a basic baseline, where each possible answer is considered equally likely (yielding an accuracy of 25% for MC questions with four choices).

## 5 Results

Table 2 compares the accuracies of VLMs in percentages (ranging from 0 to 100) across the five tasks and over multiple organism datasets. We make the following observations from this result.

**All VLMs show poor accuracy on open questions but perform better on MC questions.** The zero-shot species classification accuracy of all VLMs on open-ended questions is notably weaker than MC questions. Even the best-performing models, LLaVA-13B, GPT-4V, and Instruct-Vicuna-7B, only achieve accuracies of 2.32%, 17.46%, and 3.62%, respectively, across the three organism datasets. This indicates a significant gap in the ability of existing VLMs to capture the scientific knowledge necessary to differentiate between species (often requiring subtle or nuanced features) without being provided with candidate answer choices. Open-ended species classification is particularly hard for pre-trained VLMs that are not typically trained to provide scientific names of organisms (e.g., _Lepomis cyanellus_) rather than providing their common names (e.g., _green sunfish_). However, the inclusion of candidate answers (or options) in the question prompt serves as a helpful clue to VLMs

   Statistics & **Fish-10K** & **Bird-10K** & **Butterfly-10K** & **Fish-500** & **Bird-500** \\  \# Images & 10,347 & 11,092 & 10,013 & 500 & 492 \\ \# Species & 495 & 188 & 60 & 60 & 47 \\ \# Genera & 178 & 114 & 27 & 18 & 33 \\ \# Traits & 10 & 28 & - & 8 & 5 \\   

Table 1: Key statistics of the **VLM4Bio** dataset.

for narrowing down the solution space and finding the correct answer potentially using elimination strategies. While VLMs are able to utilize these additional hints and work their way through to the correct answer in MC questions, note that open questions are practically more relevant to scientists operating in real-world settings.

**Bird dataset shows better accuracy than Fish or Butterfly datasets.** Most VLMs show significantly better performance on the Bird-10K dataset in comparison to the Fish-10K and Butterfly-10K datasets. For example, the highest accuracy across all VLMs on the Bird-10K dataset is 82.58%, while it is 40.20% and 50.24% on the Fish-10K and Butterfly-10K datasets, respectively. A potential reason is that while the bird dataset is a subset of the CUB dataset  that is commonly used in machine learning literature and has images with natural in-the-wild backgrounds, the butterfly and fish datasets contain images of specimens preserved in museum collections with artificial backgrounds and with imaging artifacts that are not typical for large-scale computer vision datasets. We hypothesize that many of the pre-trained VLM baselines may have seen images similar to those in the Bird dataset during training, leading to their better performance.

**Can VLMs effectively identify biological traits?** The performance of most VLMs in trait identification appears significantly better than their performance in species classification, with GPT-4V reaching 82.18% accuracy on the Fish-10K dataset and Instruct-Vicuna-13B achieving 89.98% on Bird-10K. However, some traits such as "eye", "head", and "mouth" are almost always present in every organism image, so simply answering "yes, the trait is present" can lead to high accuracy in trait identification. In contrast to the fish dataset, the bird dataset poses more intricate questions regarding a variety of multi-class traits that require a nuanced understanding of colors, patterns, and physical trait dimensions, such as the color of the bill, wing patterns, and tail shapes.

**VLMs struggle in localizing traits in images.** While most VLMs perform well on the task of Trait Identification, it is crucial to determine if they are focusing on the correct image regions to answer trait-related questions. We thus analyze the performance of VLMs on the tasks of trait grounding (i.e., _text to location_) and trait referring (i.e., _location to text_). We can see that there is a significant

    &  **Question** \\ **type** \\  } &  &  &  &  &  &  &  &  &  &  &  &  \\  & & _yt_ & _v1.5-7b_ & _v1.5-13b_ & _chat_ & _fan-4V_ & _fan-4V_ & _vicuna-7B_ & _vicuna-13B_ & _fan-5V_ & _fan-5V_ & _vicuna-7B_ & _vicuna-13B_ & _Choice_ \\   \\   & Open & 1.01 & 2.32 & 0.40 & 0.11 & 0.01 & 1.59 & 0.50 & 0.38 & 0.00 & 1.46 & 0.00 & 0.00 & 0.20 \\  & MC & 35.91 & 40.20 & 32.27 & 31.72 & 29.76 & 33.36 & 29.02 & 27.45 & 30.86 & 31.70 & 27.27 & 26.57 & 25.00 \\  & Open & 17.40 & 1.45 & 2.06 & 0.86 & 0.00 & 0.57 & 2.80 & 2.56 & 0.00 & 0.50 & 0.07 & 0.00 & 0.53 \\  & MC & 82.58 & 50.32 & 55.36 & 44.73 & 33.68 & 34.75 & **23.95** & 27.62 & 36.36 & 35.83 & 44.00 & 46.55 & 25.00 \\
**Butterfly-10k** & Open & 0.04 & 0.05 & 0.00 & 0.01 & 0.00 & 0.00 & 0.07 & 0.01 & 0.00 & 0.00 & 9.94 & 0.00 & 1.54 \\  & MC & 28.91 & 50.24 & 44.58 & 36.45 & 25.14 & 28.88 & 33.06 & 28.90 & 25.28 & 36.67 & 41.70 & 34.48 & 25.00 \\   \\   & MC & 82.18 & 56.84 & 45.15 & 46.92 & 68.36 & 39.33 & 55.08 & 51.87 & 64.34 & 39.26 & 81.95 & 20.69 & 50.0 \\
**Bird-10k** & MC & 62.22 & 34.68 & 46.14 & 63.93 & 50.11 & 41.38 & 39.11 & 40.44 & 47.89 & 45.52 & 77.91 & 89.99 & 31.12 \\   \\   & MC & 29.41 & 24.87 & 17.98 & 23.42 & 23.32 & 25.14 & 22.18 & 25.58 & 7.20 & 27.09 & 33.51 & 26.90 & 25.00 \\
**Bird-500** & MC & 8.1 & 26.92 & 35.36 & 23.2 & 11.83 & 10.52 & 15.39 & 24.22 & 3.48 & 0.81 & 30.24 & 13.91 & 25.00 \\   \\   & MC & 28.15 & 27.07 & 29.14 & 28.19 & 24.93 & 25.68 & 39.24 & 31.21 & 31.75 & 25.78 & 28.04 & 32.73 & 25.00 \\
**Bird-500** & MC & 42.28 & 30.5 & 29.64 & 18.45 & 35.16 & 40.59 & 26.04 & 35.88 & 27.52 & 41.69 & 23.03 & 22.69 & 25.00 \\   \\   & Open & 16.4 & 47.4 & 52.0 & 14.8 & 37.6 & 63.4 & 13.6 & 31.53 & 50.2 & 61.4 & 61.4 & 0.0 & 25.00 \\  & MC & 44.80 & 13.20 & 54.80 & 21.00 & 64.8 & 78.2 & 22.00 & 25.00 & 74.0 & 69.4 & 15.80 & 11.80 & 25.00 \\   & 34.24 & 29.0 & 31.78 & 25.27 & 28.91 & 30.24 & 23.0 & 25.19 & 28.49 & 29.79 & 33.92 & 23.31 & 22.03 \\   

Table 2: Zero-shot accuracy comparison of VLM baselines (in % ranging from 0 to 100) for the five scientific tasks. Results are color-coded as Best, Second best, Worst. Second worst.

drop in the accuracy of trait grounding and referring tasks compared to the trait identification task. This shows that while VLMs can potentially leverage knowledge of trait choices to identify traits, they struggle in localizing the traits in the image and thus visually ground their reasoning. Figure 3 shows an illustrative example of GPT-4V prediction where it predicts the presence of the trait "eye" correctly but fails to localize it in grounding and referring tasks.

**Counting biological traits is difficult for VLMs.** Recent studies  have explored the gap in the ability of VLMs to count objects, which is aligned with our results in Table 2. All VLMs, except for BLIP-flan-T5-XXL, show lower performance in counting traits, despite performing well on the trait identification task. The overall average accuracy for the VLMs is displayed in the last block, with GPT-4V(ision) exhibiting the best performance.

We further analyze the errors of different VLMs to better understand their behavior. We find that GPT-4V shows a reduced rate of incorrect responses but a higher incidence of "Other" responses, which include apologetic expressions, admissions of inability to precisely visualize the organism, and disclaimers regarding lack of expert guidance (see Supplementary Section J for more details).

### Analyzing the Role of Answer Choices in MC Questions on VLM Performance

Table 2 showed that VLMs perform drastically better on MC questions compared to Open questions for species classification. A potential hypothesis for this observation is that VLMs are able to avoid incorrect answer choices (or options) that are too different from the correct option and thus are easy to eliminate. To test this hypothesis, we create three variants of the MC questions for species classification--easy, medium, and hard--where species choices in each variant have varying degrees of similarity determined by their taxonomic groupings. In particular, note that the scientific name of an organism contains taxonomic information at three levels: <genus name> <species name> <subspecies name>4. Since organisms that share taxonomic information have similar appearances, it is hard to differentiate species choices if they are from the same taxonomic group. On the other hand, it is easier to work with species choices from different taxonomic groups. Hence, for the easy set, we selected 50 species from different genera, ensuring that all species choices appear quite different from each other. For the medium set, we increased the complexity by constructing species choices from the same genus but from 10 different species. The hard set presented the highest difficulty level for the butterfly dataset, with the answer choices being from the same genus and species but from 10 subspecies. Each difficulty level consists of 200 images from each set of organisms.

Table 3 shows the accuracies of the baseline VLMs for the easy, medium, and hard organism datasets. The pretrained VLMs generally perform best on the easy set and worst on the hard set for each organism. Moreover, there is a gradual improvement in the VLM performance from hard to easy questions. This suggests that the difficulty level of candidate answers (or options) in the question prompt significantly impacts VLMs' performance. Additionally, this outcome indicates that even SOTA VLMs have limitations in handling fine-grained queries. Table 3 shows that GPT-4V and OpenAI's recent release GPT-4o do not perform well when tested on the medium and hard datasets for Fish and Butterfly. Due to this, we further analyze the errors of different VLMs to better understand their behavior. We provide the report in the Supplementary Section J.

Figure 3: Examples of correct and incorrect predictions of GPT-4V for trait identification, trait grounding, and trait-referring tasks related to the “eye”. For visualization assistance, a red-colored bounding box is added around the “eye” in the image.

### Comparing Pre-trained VLMs with a Biologically Fine-tuned Model

We compare BioCLIP , a state-of-the-art foundation model for species classification fine-tuned with biological images and taxonomic names (TreeOfLife-10M dataset), with the pretrained VLMs. We observe that BioCLIP significantly outperforms large pretrained VLMs on the Bird-10k and Butterfly datasets, suggesting that BioCLIP has been trained on images that are similar to the organisms present in these datasets. By comparing BioCLIP with CLIP, we can also see that fine-tuning foundation models with biological data provides large gains in classification performance. This suggests that the performance of SOTA VLMs can be further improved by fine-tuning on VLM4Bio Dataset. Further details comparing BioCLIP with SOTA VLMs are provided in the Supplementary Section K.

### Analyzing Effects of Prompting on VLM Performance

We considered three prompting techniques: Contextual Prompting, Dense Caption Prompting, and zero-shot Chain of Thought Prompting. For **Contextual prompting**, we provided a single-line description (context) of the tasks (e.g., we add "_Each biological species has a unique scientific name composed of two parts: the first for the genus and the second for the species within that genus._" before the species classification question to give some additional context on the task). **Dense Caption prompting** involves two stages: (1) first, we prompt the VLM to generate a dense caption of the specimen image such that the caption contains all the necessary trait information of the specimen. (2) We add the dense caption before the question and prompt "_Use the above dense caption and the image to answer the following question._" to generate responses from the VLM. Similarly, the **Zero-Shot Chain-of-Thought (CoT)** happens in two stages: (1) first, we prompt the VLM to generate the reasoning for a given VQA and multiple choices (options). Zero-shot CoT appends "_Let's think step by step._" after the question and options to generate the reasoning. (2) We then add the reasoning after the VQA and prompt "_Please consider the following reasoning to formulate your answer_" to generate the VLM response. We curated a prompting dataset of \(500\) multiple-choice (MC) VQAs for each set of organisms, which is a subset of the VLM4Bio dataset for species classification.

Table 4 compares best-performing VLMs on the prompting dataset. The CoT rows of the table demonstrate that only GPT-4V and GPT-4o have reasoning capabilities that can significantly improve their response to biological questions, while smaller models like LLaVa and BLIP do not show much improvement. Furthermore, providing extra context and caption is more useful for GPT-4V and GPT-4o than the smaller models. This resonates with the findings from  that the reasoning abilities of VLMs only emerge after a certain model size. The success of Dense Caption Prompting and CoT Prompting depends on how well they generate the dense caption or the reasoning in the first stage. We report example prompts with VLM responses as case studies in the Supplementary Section M.

### Analyzing Tests for Reasoning Hallucination

To further understand whether pretrained VLMs can respond with logically coherent and factually accurate reasoning, we evaluate VLMs on two sets of reasoning for hallucination tests - **False Confidence Test (FCT)** and **None of the Above (NOTA) Test** - inspired by . For the FCT, we

    & & & & & & & & & & & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & \\   &  & 44.50 & 37.50 & 47.50 & 46.00 & 24.00 & 34.00 & 27.50 & 29.00 & 19.50 & 32.00 & 28.00 & 33.50 & 33.50 & 36.50 & 38.50 \\  & & Medium & 38.50 & 38.50 & 30.00 & 28.50 & 27.00 & 26.00 & 23.00 & 26.50 & 25.00 & 28.50 & 24.50 & 26.00 & 25.50 & 26.00 & 29.00 \\   &  & 73.50 & 68.00 & 53.50 & 50.00 & 38.50 & 34.50 & 36.00 & 27.00 & 32.00 & 41.00 & 33.00 & 43.50 & 39.00 & 57.00 & 38.00 \\  & & Medium & 41.00 & 40.50 & 30.50 & 37.00 & 30.00 & 25.50 & 27.00 & 27.00 & 24.00 & 27.00 & 27.00 & 24.50 & 26.50 & 31.00 & 59.00 \\   &  & 18.50 & **27.50** & 19.00 & 20.50 & 24.50 & 30.00 & 25.00 & 38.50 & 26.00 & 24.50 & 22.50 & 19.00 & 24.50 & 21.50 & 68.50 \\  & & Medium & 58.00 & 7.00 & 29.50 & 29.00 & 29.50 & 20.00 & 25.50 & 38.00 & 25.00 & 27.50 & 25.00 & 25.00 & 25.00 & 21.50 & 58.00 \\   &  &  & **18.50** & 22.00 & 21.00 & 22.00 & 26.50 & 20.00 & 29.50 & 24.00 & 22.50 & 24.00 & 24.00 & 21.00 & 21.50 & 58.00 \\   

Table 3: Zero-Shot accuracy comparison for _easy, medium, and hard_ datasets. Results are color-coded as **Best**, **Second best**, **Worst**, **Second worst**.

randomly select an option from the list of given choices and prompt it to the VLM as a "suggested correct answer" along with the question and options. To evaluate VLMs on FCT, we use Accuracy as well as the Agreement score, which is the percentage of times the VLM agrees with the suggested answer, irrespective of whether that is right or wrong. A high agreement score with a low overall accuracy indicates poor performance as it suggests that the model is simply following the suggestion either because of a lack of knowledge or low confidence in its own response. On the other hand, in the NOTA Test, we replace the correct option with "None of the Above", requiring the model to produce "None of the above" for all the questions. From Table 5, we can see that LLaVa-v1.5-7B shows poor accuracy on both tests and a high agreement score on FCT. Out of all the VLMs, GPT-4V and GPT-4o demonstrate the highest accuracy, i.e., the lowest false confidence. More details on the prompts and examples of the responses have been provided in the Supplementary Section M.

## 6 Limitations

Our work has three main limitations. First, while no prior VQA benchmark dataset exists for organismal biology to the best of our knowledge, we focused on only three organisms--fish, bird, and butterfly--out of the many available due to resource constraints. Adding more organisms with

    & &  \\ 
**Dataset** & **Prompting** & _gpt-4v_ & _gpt-4o_ &  _llava_ \\ _v1.5-7b_ \\ _v1.5-13b_ \\  &  _llava_ \\ _cat_ \\  &  _logvlm_ \\ _flan-xl_ \\  & 
 _BLIP_ \\ _flan-xl_ \\  \\   & No Prompting & 34.40 & 79.00 & 41.60 & 35.40 & 31.00 & 28.60 & 22.60 \\  & Contextual & 30.00 & 77.20 & 40.20 & 35.60 & 25.60 & 27.20 & 26.60 \\  & Dense Caption & 18.80 & 78.60 & 26.00 & 27.60 & 32.00 & 28.40 & 29.80 \\  & CoT & 42.60 & 86.00 & 41.40 & 34.80 & 26.80 & 29.20 & 24.60 \\   & No Prompting & 78.80 & 97.60 & 44.20 & 49.80 & 45.40 & 35.60 & 35.80 \\  & Contextual & 78.60 & 98.60 & 44.00 & 52.00 & 49.40 & 35.60 & 30.40 \\  & Dense Caption & 87.40 & 97.00 & 33.40 & 41.00 & 44.00 & 25.60 & 22.80 \\  & CoT & 62.60 & 98.60 & 37.40 & 47.80 & 42.20 & 30.60 & 31.00 \\   & No Prompting & 13.20 & 56.40 & 27.20 & 26.80 & 25.60 & 24.40 & 21.20 \\  & Contextual & 9.20 & 56.20 & 26.00 & 24.60 & 27.20 & 23.60 & 24.60 \\   & Dense Caption & 49.60 & 63.20 & 25.20 & 23.80 & 27.00 & 23.20 & 23.20 \\   & CoT & 63.60 & 74.60 & 21.40 & 23.20 & 34.60 & 37.20 & 23.60 \\   

Table 4: Zero-shot accuracy comparison for different prompting techniques of seven VLMs (in % ranging from 0 to 100). Results are color-coded as Best and Worst.

    & &  \\ 
**Dataset** & **Metrics** & _gpt-4v_ & _gpt-4o_ &  _llava_ \\ _v1.5-7b_ \\  &  _llava_ \\ _v1.5-7b_ \\  &  _logvlm_ \\ _v1.5-13b_ \\  &  _ggvlm_ \\ _chat_ \\  &  _BLIP_ \\ _flan-xl_ \\  & 
 _BLIP_ \\ _flan-xl_ \\  \\   \\   & Accuracy & 34.20 & 73.60 & 25.00 & 28.60 & 24.60 & 0.00 & 7.00 \\  & Agreement Score & 4.40 & 16.60 & 99.80 & 19.20 & 74.40 & 0.00 & 28.4 \\   & Accuracy & 73.40 & 99.00 & 25.40 & 35.80 & 19.80 & 0.00 & 20.20 \\  & Agreement Score & 11.40 & 21.00 & 93.20 & 17.80 & 47.80 & 0.00 & 79.80 \\   & Accuracy & 5.20 & 53.40 & 27.20 & 26.60 & 6.20 & 0.00 & 5.00 \\  & Agreement Score & 2.60 & 12.40 & 95.40 & 5.60 & 13.80 & 0.00 & 19.00 \\   \\   & Accuracy & 81.40 & 44.80 & 3.40 & 3.80 & 0.00 & 4.00 & 0.00 \\   & Accuracy & 75.00 & 91.40 & 1.00 & 1.20 & 0.00 & 31.40 & 0.00 \\   & Accuracy & 50.40 & 4.60 & 1.00 & 4.60 & 0.00 & 51.00 & 0.00 \\   

Table 5: Performance of seven VLMs on the NOTA and FCT reasoning tests. Results are color-coded as Best and Worst.

manually annotated trait data will require additional resources and domain expertise, which could be pursued in future work. Second, since it is not feasible to manually inspect all images to ensure that they are free from label noise, we acknowledge that some noise may be present in the labels used for evaluating models on our current dataset, which we plan to address in future iterations. Third, due to resource constraints, certain proprietary VLMs that require purchasing APIs like Gemini-Pro , Gemini-Ultra , and Claude Opus  were also not included in the evaluation. We anticipate that their performance will be comparable to that of the proprietary GPT-4V  and GPT-4o  considered in our evaluation.

## 7 Conclusion and Future Work

We presented VLM4Bio, a benchmark dataset to evaluate the zero-shot performance of pretrained VLMs on biologically relevant questions involving biodiversity images, exposing gaps in SOTA VLMs when applied to organismal biology. We observe that while VLMs are able to perform reasonably well on simpler tasks, e.g., using questions with multiple-choice formats and images with natural-looking backgrounds, they struggle in complex task settings that are practically more relevant to biologists. Through our study on prompting and reasoning tests on the VLM4Bio dataset, we observe that very large SOTA VLMs such as GPT-4V and GPT-4o have reasoning capabilities that can significantly improve the response to biological questions. We did not explore Retrieval Augmented Generation (RAG)  or knowledge-infused prompting  techniques since they require additional knowledge bases, which could be developed in future work. Future works can also focus on finetuning VLMs on the VLM4Bio dataset instead of comparing zero-shot performance.