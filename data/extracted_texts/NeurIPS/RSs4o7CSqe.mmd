# Conditional Controllable Image Fusion

Bing Cao\({}^{1,\,2}\) Xingxin Xu\({}^{3}\) Pengfei Zhu\({}^{1}\) Qilong Wang\({}^{1}\) Qinghua Hu\({}^{1}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University, Tianjin, China

\({}^{2}\)State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China

\({}^{3}\)School of New Media and Communication, Tianjin University, Tianjin, China

{caobing, xuxingxin, zhupengfei, qlwang, huqinghua}@tju.edu.cn

Corresponding author.

###### Abstract

Image fusion aims to integrate complementary information from multiple input images acquired through various sources to synthesize a new fused image. Existing methods usually employ distinct constraint designs tailored to specific scenes, forming fixed fusion paradigms. However, this data-driven fusion approach is challenging to deploy in varying scenarios, especially in rapidly changing environments. To address this issue, we propose a conditional controllable fusion (CCF) framework for general image fusion tasks without specific training. Due to the dynamic differences of different samples, our CCF employs specific fusion constraints for each individual in practice. Given the powerful generative capabilities of the denoising diffusion model, we first inject the specific constraints into the pre-trained DDPM as adaptive fusion conditions. The appropriate conditions are dynamically selected to ensure the fusion process remains responsive to the specific requirements in each reverse diffusion stage. Thus, CCF enables conditionally calibrating the fused images step by step. Extensive experiments validate our effectiveness in general fusion tasks across diverse scenarios against the competing methods without additional training. The code is publicly available.+

Footnote â€ : https://github.com/jehovahxu/CCF

## 1 Introduction

Image fusion aims at integrating complementary information from multi-source images, fusing a new composite image containing richer details . It has been applied in various scenarios that single image contains incomplete information, such as multi-modal fusion (MMF) , multi-exposure fusion (MEF) , multi-focus fusion (MFF) , and remote sensing fusion . The fused image inherits the strengths of both modalities, resulting in a composite with enhanced visual effects . These fusion tasks have diverse downstream applications in computer vision, including object detection , semantic segmentation , and medical diagnosis  because the comprehensive representation of images with multi-scene information contributes to the improved performance of applications.

Recently, numerous image fusion methods  have been proposed, such as traditional fusion methods , CNN-based fusion methods  and GAN-based methods . While these methods produce acceptable fused images in certain scenarios, they are also accompanied by significant drawbacks and limitations: (i) They are often tailored for specific scenarios or individual tasks, limiting their adaptability across diverse applications; (ii) These methods necessitate training and consume substantial computational resources, posing limitations in terms of time and resource requirements. Lately, denoising diffusion probabilistic models (DDPM) have emerged as an iterativegeneration framework, showcasing impressive capabilities in unconditional generation. Inspiringly, numerous researchers explored its controllable aspects. ILVR  proposed iterative latent variable refinement with a reference image to control image translation. Some recent works  employed the diffusion model for image fusion, which fuses images in fixed fusion paradigms (fixed fusion conditions) by using its inherent reconstruction capacity. However, these approaches are not qualified for sample-customized fusion with dynamic conditions. At present, general image fusion with controllable diffusion models is still a challenging problem, warranting further exploration.

In this paper, we propose a diffusion-based controllable conditional image fusion (CCF) framework, which controls the fusion process by adaptively selecting optimization conditions. We construct a condition bank of generally used conditions, categorizing them into _basic_, _enhanced_, and _task-specific_ conditions. CCF dynamically assigns fusion conditions from the condition bank and continuously injects them into the sampling process of diffusion. To enable flexible integrated conditions, we further propose a sampling-adaptive condition selection (SCS) mechanism that tailors condition selection at different denoising steps. The iterative refinements of the sampling are based on the pre-trained diffusion model without additional training. It is worth noting that the estimated fused images are conditionally controllable during the iterative denoising process. The diffusion process seamlessly integrates these conditions during the sampling process, decreasing potential impacts. As illustrated in Fig. 1, the generation process emphasizes different aspects at various sampling steps. In the initial stages, the condition selection is influenced by random noise, resulting in a random selection. During the intermediate stages, there is a shift towards content components. In the final stage, the emphasis moves to generating and selecting texture details. These various conditional factors contribute to different aspects of fusion results and demonstrate the necessity and effectiveness of introducing specific conditions in different stages. To the best of our knowledge, we for the first time propose a conditional controllable framework for image fusion. The main contributions are summarized as follows:

* We propose a pioneering conditional controllable image fusion (CCF) framework with a condition bank, achieving controllability in various image fusion scenarios and facilitating the capability of dynamic controllable image fusion.
* We propose a sampling-adaptive condition selection mechanism to subtly integrate the condition bank into denoising steps, allowing adaptive condition selection on the fly without additional training and ensuring the dynamic adaptability of the fusion process.
* Extensive experiments on various fusion tasks have confirmed our superior fusion performance against the competing methods. Furthermore, our approach qualifies for interactive manipulation of the fusion results, demonstrating our applicability and efficacy.

Figure 1: The conditions selection statistics during the sampling process of the LLVIP dataset. The distinct process of sampling has different favor of the conditions. The crucial role that diverse conditions play in controlling various image generation processes. Throughout the diffusion sampling, different conditions are dynamically selected to best suit the generation requirement at each stage.

Related Work

Image fusion focuses on producing a unified image that amalgamates complementary information sourced from multiple source images .

**Specialized.** Focus on specialized tasks such as VIF, several early approaches [26; 27; 28] relied on CNNs to address challenges across various scenarios. GTF  defined the objective of image fusion as preserving both intensity information in infrared images and gradient information in visible images. Besides that, researchers started artificially incorporating prior knowledge to aid in the fusion process. CDDFuse  introduced the concept of high and low-frequency decomposition with dual-branch as prior information. Diverging from approaches tailored to single scenarios, numerous methods are now exploring the development of a unified fusion framework.DDFM  represents the pioneering training-free method that employs a diffusion model for multi-modal image fusion.

**Generalized.** Not limited to specialized applications, researchers aim to extend its use to generalized tasks. U2Fusion  introduced a unified framework capable of adaptively preserving information and facilitating joint training across various task scenarios. Additionally, SwinFusion  proposed a cross-domain distance learning method that has been extended to form a unified framework encompassing diverse task scenarios. Defusion  employs self-supervised learning techniques to decompose images and subsequently adaptively fuse them. TC-MoA  proposed a novel task-customized mixture of adapters for generating image fusion with a unified model, enabling adaptive prompting for various fusion tasks.

Nevertheless, these methods cannot control image fusion for adaptation to different scenarios. Therefore, we propose a method that enables image control, manipulating the fused image through existing conditions on our condition bank.

## 3 Preliminary

Denoosing diffusion probabilistic models (DDPM) is a class of likelihood-based models that shows remarkable performance  with a stable training objective in unconditional image generation. The diffusion process entails incrementally introducing Gaussian noise to the data until it reaches a state of random noise. For a clean sample \(x_{0} q(x_{0})\) each step within the diffusion process constitutes a Markov Chain, encompassing a total of \(T\) steps, relying on the data derived from the preceding step. Gaussian noise is added as follows:

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t}I),\] (1)

where \(\{_{t}\}_{t=1}^{T}\) is the variance schedule of each diffusion step which is fixed and predefined. The generative process learns the inverse of the DDPM forward (diffusion) process, sampling from a distribution by reversing a gradual denoising process. We can directly sample \(x_{t}\) at any \(t\) step based on the original data \(x_{t} q(x_{t}|x_{0})\) and via the reparameterization, it can be redefined:

\[x_{t}=_{t}}x_{0}+_{t}},\] (2)

where defined \(_{t}:=1-_{t}\) and \(:=_{i=1}^{t}_{i}\). The diffusion process introduces noise to the data, whereas the inverse process represents a denoising procedure called sampling. In particular, stats with a noise \(x_{T}(0,I)\), the diffusion model learns to produce slightly less-noisy sample \(x_{T-1}\), the process can be formulate by:

\[p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{ }(x_{t},t)).\] (3)

Utilizing the properties of Markov chains, decomposing \(_{}\) and \(_{}\), the process of generation is expressed as:

\[x_{t-1}=}}(x_{t}-}{_{t}}}_{}(x_{t},t))+_{}^{2}(t)I,\] (4)

where, \(_{}^{2}(t)=_{}(t)=)(1-_ {t-1})}{1-_{t}}\), \(_{}\) signifies the output of a neural network, commonly a U-Net. This neural network predicts the noise \(_{}\) at each step, which is utilized for the denoising procedure. It can be observed that variance is a fixed quantity, because of diffusion process parameters being constant, whereas the mean is a function dependent on \(x_{0}\) and \(x_{t}\). However, the stochastic process poses challenges in controlling the generative process.

## 4 Method

Leveraging the reconstruction capability of unconditional DDPM, we introduced a new controllable conditional image fusion (CCF) framework. Our approach accomplishes dynamically controllable image fusion via progressive condition embedding. In particular, we introduced a condition bank that regulates the incorporation of fusion information using conditions. It allows for combining the dynamic selection of multiple conditions to achieve sampling-adaptive fusion effects. As shown in Fig. 2, we illustrate our CCF framework in detail with visible-infrared image fusion (VIF). The goal is to generate a fused image \(f^{H W N}\) from visible \(v^{H W N}\) and infrared \(i^{H W N}\) images, where \(H\), \(W\) and \(N\) denote height, width, and channel numbers, respectively.

### Controllable Conditions

Firstly, we provide the notation for the model formulation. For each sampling instance, a pre-trained DDPM represents unconditional transition \(p_{}(x_{t-1}|x_{t})\). Our method facilitates the inclusion of conditional \(c\) during the sampling step of unconditional transformation, without no additional training. For this purpose, we sample images from the conditional distribution \(p(x_{0}|c)\) given condition \(c\):

\[ p_{}(x_{0}|c)&= dx^{(1:T)} p_{}(x^{(0:T)}|c),\\ p_{}(x^{(0:T)}|c)&=p(x_{T})_{t=1}^{T}p _{}(x_{t-1}|x_{t},c).\] (5)

Each transition \(p_{}(x_{t-1}|x_{t},c)\) of the generative process depends on the condition \(c\). From the property of the forward process that latent variable \(x_{t}\) can be sampled from \(x_{0}\) in closed-form, denoised data \(x_{0}\) can be approximated with model prediction \(_{}(x_{t},t)\):

\[x_{0|t} f_{}(x_{t},t)=-_{t}} _{}(x_{t},t))}{_{t}}}.\] (6)

To compute \(p(x_{t}|c)\), we can derive it from the Stochastic Differential Equation (SDE) . For brevity, \(x_{0|t}\) is abbreviated as \(x_{0}\), and the expression is given by:

\[_{x_{t}} p(x_{t})=--_{t}}x_{0}}{1- {}_{t}}.\] (7)

Classifier Guidance  can be intuitively elucidated via the score function, which logarithmically decomposes the conditional generation probability using Bayes' theorem:

\[ p(x_{t}|c)= p(x_{t})+ p(c|x_{t}).\] (8)

Figure 2: Illustrates the pipeline of the proposed CCF. The framework comprises two components: a sampling process utilizing a pre-trained DDPM and a condition bank with SCS.

Diffusion posterior sampling can yield a more favorable generative trajectory, especially in noisy settings,  estimation \( p(c|x_{t})\) as follows:

\[ p(c|x_{t}) p(c|_{0}).\] (9)

Further, as demonstrated in , we can express the score function as:

\[ p(c|x_{t}) p(c|_{0})=f_{}(x_{t},t)- ||C-(x_{0})||_{2}.\] (10)

Here, \(()\) can be linear or nonlinear operation. We represent \(||C-(x_{0})||\) as \(_{C}\). Now, the objective is to obtain \(_{0}\) and incorporate the condition into it. We can minimize \(_{C}\) to regulate the sampling within the diffusion process. In the following section, we will provide a detailed explanation of how to build a condition bank and how to select conditions.

### Condition Bank

We empirically construct a _condition bank_ and divide the image constraints into three categories: basic fusion conditions, enhanced fusion conditions, and task-specific fusion conditions. Basic fusion conditions are utilized throughout the entire sampling process, while enhanced fusion conditions are dynamically selected. Task-specific fusion conditions are manually optional, tailored to specific tasks, and may possess unique attributes that can be customized for various task scenarios. All conditions can be part of the enhanced condition set, enabling dynamic selection. The condition bank presented in this paper includes some common conditions, but additional conditions can be explored and utilized in other scenarios.

In the above formulation, each conditional Markov transition with the given condition \(c\) is shown in Eq. 5. In particular, we constructed a condition bank that allows us to select required conditions \(C=\{c_{1},c_{2},...,c_{n}\}\), subsequently integrating them into the unconditional DDPM for executing conditional image fusion. Let \(C\) represent a condition bank comprising a series of conditions. The function \(_{C}\) represents the difference between the source images with the given condition. In every sampling step \(t\), the difference function \(_{c_{i}}\) can be minimized using gradient descent. These conditions help regulate the image information within each modality involved in the fusion process.

**Basic Conditions**. As shown in Fig. 2, basic conditions are essential to select for a basic generation. The basic conditions aim to synthesize a foundational fused image, offering an initial, coarse representation. The fused image serves as a primary fusion output, capturing essential features from the source images, though it may suffer from detail loss or texture blur. Notably, different scenarios may require adjustments to the basic condition, as the specific requirements of each fusion task, such as clarity, contrast, and other priorities, can influence its selection. Tailoring the basic condition to align with the unique demands of each task thus ensures an effective fusion process.

**Enhanced Conditions**. Besides basic conditions, we added enhancement conditions for refining the image generation process. The condition bank contains a variety of enhanced conditions, inspired by various Image Quality Assessments (IQA) such as SSIM, and standard deviation(SD). These conditions can be integrated into the CCF generation process to improve the quality of the generated images. The enhanced conditions can be selected with SCS algorithm, allowing different steps of the diffusion sampling process to be optimized with different conditions. This targeted approach ensures

Figure 3: Qualitative comparisons of our CCF and the competing methods on VIF fusion task of LLVIP dataset.

that each phase of the image generation is adapted to the specific requirements of that stage, resulting in higher quality fused images.

**Task-specific Conditions**. The purpose of image fusion is to facilitate various downstream tasks. To meet the specific requirements of these tasks, we offer task-specific conditions. These conditions can be manually added to ensure that the fusion process is tailored to suit the downstream applications better. For example, a detection condition can be introduced by utilizing the feature extracted by a detection network \(F=D(x)\). The detection condition is formulated as \(||F(x),F(M)||_{2}\), where \(X\{x_{0}\}_{i}^{m}\) and \(M\) is the set of \(m\) modalities. Other task-specific conditions can be similarly tailored to optimize the fusion process for different tasks. By integrating these task-specific conditions, the image fusion process can be precisely aligned with the demands of various applications, enhancing the effectiveness and utility of the generated images.

### Sampling-adaptive Condition Selection

During the diffusion sampling process, it is crucial to focus on generating distinct aspects of the image. To address this, we designed an algorithm that dynamically selects the appropriate condition from the condition bank to fit each sampling stage. This selection process can be denoted as \(C_{opt}=_{k}\{(C)\}\). The \(C_{opt}\) is the selected condition, and Gate is the gate of selection. We hypothesize that rapidly changing conditions during the sampling process should be prioritized as they indicate greater significance at that generation stage. Inspired by multi-task learning , the gate of conditions can be calculated using the following formula:

\[=[_{1},..._{c}],_{i}(t)=_{i}(t-1)- _{i}.\] (11)

The \(_{i}(t-1)\) represents the condition gradient from the previous step, and the \(_{i}\) is calculated as:

\[_{i}=_{i}|G_{W}^{i}(t)-[G_{W}^{i}(t)] _{i}(t)|_{1},\] (12)

where, \(G_{W}=||_{i}(t)L_{i}(t)||_{2}\), \(L_{i}(t)\) represents the condition gradient at step \(t\), and the value can be calculated using a gradient descent algorithm in minimize \(_{c_{i}}\). The \(_{i}(t)\) is defined as:

\[_{i}(t)=[_{i}}{[_{i}]}]^{ },\] (13)

where, \(_{i}=(t)}{L_{i}(t-1)}\), \(\) is a hyper-parameter. By incorporating this SCS algorithm, we can efficiently choose the most relevant condition for each step in the diffusion process, thereby enhancing the quality of the conditional image fusion.

## 5 Experiments

**Datasets.** We conduct experiments in three image fusion scenarios: multi-modal, multi-focus, and multi-exposure image fusion. For multi-modal image fusion task, we conducted experiments on the LLVIP  dataset and referred to the test set outlined in Zhu et al. . For MEF and MFF, our testing procedure followed the test setting in MFFW dataset  and MEFB dataset , respectively. Additionally, we test our method on the TNO dataset and Harvard medical dataset to assess our method's performance within the multi-modal fusion domain, detailed in App. B and H.

**Implementation Details.** Our method utilizes a pre-trained diffusion model as our foundational model . This model was directly applied without any subsequent fine-tuning for specific task requirements during our experiments. The experiments are conducted on Huawei Atlas 800 Training Server with CANN and NVIDIA RTX 3090 GPU. Experimental settings are shown in App. A.

**Evaluation Metrics.** We evaluated the fusion results in both quantitative and qualitative. Qualitative evaluation primarily hinges on subjective visual assessments conducted by individuals. We expect that the fused image will exhibit rich texture details and abundant color saturability. Objective evaluation primarily focuses on measuring the quality assessments of individual fused images and their deviations from the source images. For different task scenarios, the different evaluation metrics used, specifically, we employ six metrics including Structural Similarity (SSIM), Mean squared error (MSE), correlation coefficient (CC), peak signal-to-noise ratio (PSNR), modified fusion artifacts measure (Nabf). In the MFF and MEF tasks, considering the different task scenarios, we employ standard deviation (SD), average gradient (AG), spatial frequency (SF), and sum of the correlations of differences (SCD) for evaluation metrics.

### Evaluation on Multi-Modal Image Fusion

For multi-modal image fusion, we compare our method with the state-of-the-art methods: Swin-Fusion , DIVFusion , MUFusion , CDDFuse , DDFM , Text-IF , and TC-MoA  and on the LLVIP dataset. More datasets and comparison methods are shown in App. B. Note that our method, like DDFM, does not require additional tuning.

**Quantitative Comparisons.** We employ five quantitative metrics to evaluate our model, as shown in Table 1. Our method demonstrated exceptional performance across various evaluation metrics. On the LLVIP dataset, our method achieved the best results in SSIM, MSE, and CC indicators. Specifically, our method outperformed others in SSIM and CC, with improvements of 0.02 and 0.035 over the second-best results, respectively. Additionally, lower MSE values indicate better performance, with our method showing a reduction of 362 compared to the second-best methods in these metrics. This indicates that our method retains more information from the source images. The results show suboptimal performance in Nabf but are close to the best values, indicating the fused image with less noise. Notably, our method does not necessitate turning and holds its ground against methods requiring training. Compared to existing LLM tuning methods, our model performs slightly worse in terms of PSNR. This demonstrates the excellent performance of our model, achieving high performance across almost all indicators in a tuning-free model.

**Qualitative Comparisons.** Furthermore, the incorporation of the basic condition and enhanced conditions enables effective preservation of the background and texture. This comparison underscores our model's efficacy in image fusion, resulting in outstanding visual outcomes. As shown in Fig. 3, our method showcases superior visual quality compared to other approaches. Specifically, our method excels in preserving intricate texture details well lid in low light (Fig. 3 red box). Although TC-MoA and MUFusion approach our method in retaining details, they exhibit visible artifacts, blur, and low contrast--characteristics absent in CCF (Fig. 3, green box). CCF exhibits the highest contrast, the clearest details, and the most information content, further highlighting its superiority in preserving texture details. Its excellent detail retention and clear background generation further demonstrate the effectiveness of our proposed method.

### Evaluation on Multi-Focus Fusion

For multi-focus image fusion, we compare our CCF with five general image fusion methods: U2Fusion , DeFusion , DDFM , Text-IF , and TC-MoA .

**Quantitative Comparisons.** We employ four quantitative metrics to evaluate our model, as shown in Table 2 (left). Our method significantly outperforms the comparison methods, achieving SOTA across all metrics. Specifically, the SD is 11.19 higher than the suboptimal value, indicating higher

    &  \\  & SSIM\(\) & MSE\(\) & CC\(\) & PSNR\(\) & Nabf\(\) \\  SwinFusion  & \(0.81\) & \(2845\) & \(0.668\) & \(32.33\) & \(0.023\) \\ DIVFusion  & \(0.82\) & \(6450\) & \(0.655\) & \(21.60\) & \(0.044\) \\ MUFusion  & \(1.10\) & \(\) & \(0.648\) & \(31.64\) & \(0.030\) \\ CDDFuse  & \(1.18\) & \(2545\) & \(\) & \(32.13\) & \(\) \\ DDFM  & \(1.18\) & \(\) & \(0.668\) & \(\) & \(\) \\ Text-IF  & \(\) & \(2135\) & \(\) & \(31.97\) & \(0.023\) \\ TC-MoA  & \(\) & \(2790\) & \(0.666\) & \(\) & \(0.017\) \\ CCF (ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison with SOTAs in the LLVIP dataset. The **red/blue/green** indicates the best, runner-up and third best.

Figure 4: Qualitative comparisons of various methods in MFF task from MFFW dataset.

contrast and clearer images. The SCD is 0.11 higher than the suboptimal value, suggesting a lower error between source images and fused images. The AG and SF also rank first demonstrating the retention of more texture details. These results showcase that our method effectively preserves details from the source images and produces high-quality fused images.

**Qualitative Comparisons.** As illustrated in Fig. 4, our proposed method demonstrates outstanding visual performance, particularly in preserving intricate details. We have carefully selected specific conditions that allow our approach to effectively handle the blurring caused by multi-focus scenes while retaining the original image's lighting and color information. In comparison, other DDPM-based methods such as DDFM are unable to achieve the same level of effectiveness as our approach. In Fig. 4 (red), our method excels in preserving the details of the watch hand. The closest result to ours is U2Fusion, but it loses texture and color fidelity, appearing blurry, in Fig. 4 (green). In short, our method performs well in maintaining both color and authentic details.

### Evaluation on Multi-Exposure Fusion

For multi-exposure image fusion, we compare our model with five general image fusion methods, i.e., U2Fusion , DeFusion , DDFM , Text-IF , and TC-MoA .

**Quantitative Comparisons.** As demonstrated in Table 2 (right), our method achieved SOTA on the MEF index, analogous to the results observed for the MFF task. Notably, the metrics SD, AG, and SF signify the highest image quality, while SCD exhibits the highest correlation. Each of these metrics attained state-of-the-art performance levels, underscoring the efficacy of our approach. This consistent performance across multiple metrics illustrates the robustness and versatility of our method in enhancing image quality and fidelity.

**Qualitative Comparisons.** Fig. 4 demonstrates the excellent visual performance of our method. Our approach effectively addresses the issue of overexposure while preserving crucial details. In contrast, DDFM, which relies on finding the middle value of two images, struggles to maintain texture details. Similarly, Text-IF tends to result in higher average brightness, which can lead to content loss in overexposed scenes. Defusion and TC-MoA exhibit similar visual performance, with more blurred edges compared to our method. In comparison, our method strikes a balance between these challenges, resulting in superior visual fidelity and saturation compared to other existing methods.

### Task-specific Conditions

The task-specific conditions can be manually selected. In this section, we use the detection condition as an example. The detection model employed is YOLOv5 , trained on the LLVIP dataset. We randomly select 287 images from the LLVIP test dataset to validate our method with the detection condition. Before adding the detection condition, the fused image achieved mAP\(.5=0.737\), mAP\(.5:.95=0.509\), and a recall of 0.737. After incorporating the detection condition, the mAP\(.5\) increased by 0.049 to 0.907, mAP\(.5:.95\) increased by 0.054 to 0.563, and recall significantly improved to 0.832. Fig. 5 visualizes several cases detected using YOLOv5.

    &  &  \\  & SD \(\) & AG \(\) & SF \(\) & SCD \(\) & SD \(\) & AG \(\) & SF \(\) & SCD \(\) \\  U2Fusion  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ DeFusion  & \(54.75\) & \(4.76\) & \(12.72\) & \(0.50\) & \(52.75\) & \(4.32\) & \(14.12\) & \(-0.97\) \\ DDFM  & \(56.34\) & \(4.47\) & \(12.21\) & \(0.90\) & \(\) & \(3.82\) & \(13.40\) & \(\) \\ Text-IF  & \(\) & \(\) & \(\) & \(\) & \(62.51\) & \(4.78\) & \(\) & \(1.46\) \\ TC-MoA  & \(57.55\) & \(6.95\) & \(20.67\) & \(1.03\) & \(50.27\) & \(\) & \(15.64\) & \(0.42\) \\ CCF (ours) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Comparison with SOTAs. The **red/blue/green** indicates the best, runner-up and third best.

Figure 5: The visualization of the w/o and with task-specific conditions.

he fused images with the detection condition exhibit higher confidence (columns 1 and 2) and recall (columns 1, 2, and 3). This demonstrates that task-specific conditions can enhance the performance of fused images in downstream tasks. By integrating task-specific conditions, the fused image can be precisely aligned with the demands of various applications, enhancing the overall effectiveness and utility of the generated images. More detail shows in App. F.

### Ablation Study

Numerous ablation experiments were conducted to assess the effectiveness of different components of our model. The aforementioned four metrics were utilized to evaluate fusion performance across different experimental configurations. The quantitative results are presented in App. G.

To validate the effectiveness of the condition bank, we systematically add basic and enhanced conditions individually and then verify the effectiveness of SCS. Fig. 6 illustrates a gradual variation in performance metrics with the progressive addition of conditions.

In Fig. 6(a), using only the basic condition results in image fusion. When all enhanced conditions are injected together without SCS, the metrics all reduced (Fig. 6(b)), likely due to the conflict between different conditions, as evidenced by the messy lines and random noise in Fig. 6(d). This demonstrates that injecting conditions unevenly without DCS leads to suboptimal results. After introducing DCS, both the metrics and visual results (Fig. 6(e)) are well-balanced, with conditions being injected as needed during the generation process. This indicates the effectiveness of the condition bank and DCS in dynamically and appropriately selecting the conditions.

### Discussion on Controllable Fusion

Our method enables the dynamic selection of conditions, allowing for the adaptive customization of conditions for each sample. As illustrated in Fig. 1, the statistics show the condition selection at various diffusion sampling stages. Initially, the process is stochastic due to random noise (around 0 to \(T\) steps). As denoising progresses, the content of the image starts to be dominant in fusion, and conditions like MSE, SSIM, and SD are most frequently chosen (around \(T\) to \(T\) steps). As the process continues, and the fusion model tends to generate more texture details, conditions like EI and edge become more dominant (around \(T\) to \(T\) steps). SSIM also remains selected to constrain the structure, whereas the frequency of SD selection decreases. This demonstrates the crucial role of dynamically selecting conditions during the diffusion sampling process. Our CCF adaptively decomposes diverse conflict fusion conditions into different denoising steps, which is significantly compatible with the reconstruction preferences of different steps in the denoising model. This dynamic fusion paradigm ensures appropriate condition selection at each stage of the sampling.

## 6 Conclusion

In this paper, we introduce a learning-free approach for conditional controllable image fusion (CCF), utilizing a condition bank to regulate joint information with a pre-trained DDPM. We capitalize on the remarkable reconstruction abilities of DDPM and integrate them into the sampling steps. Sample-adaptive condition selection facilitates fusion in dynamic scenarios. Varied fusion images can personalize their conditions to emphasize different aspects. Empirical findings demonstrate that CCF surpasses the competing methods in achieving superior performance for general image fusion tasks. In the future, we will further explore automatic manners to distinguish basic and enhanced conditions to reduce empirical intervention, thereby enabling more robust and reliable image fusion.

Figure 6: Ablation study of condition bank. (a) is infrared image, (b) is visible image, (c) is basic condition, (d) is CCF w/o SCS, (e) is CCF.