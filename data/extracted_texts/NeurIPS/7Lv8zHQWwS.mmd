# A Boosting-Type Convergence Result for AdaBoost.MH with Factorized Multi-Class Classifiers

Xin Zou Zhengyu Zhou Jingyuan Xu Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

{zouxin2021, zzysince1999, jingyuanxu777, liuweiwei863}@gmail.com

equal contribution

###### Abstract

AdaBoost is a well-known algorithm in boosting. Schapire and Singer propose, an extension of AdaBoost, named AdaBoost.MH, for multi-class classification problems. Kegl shows empirically that AdaBoost.MH works better when the classical one-against-all base classifiers are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector. However, the factorization makes it much more difficult to provide a convergence result for the factorized version of AdaBoost.MH. Then, Kegl raises an open problem in COLT 2014 to look for a convergence result for the factorized AdaBoost.MH. In this work, we resolve this open problem by presenting a convergence result for AdaBoost.MH with factorized multi-class classifiers.

## 1 Introduction

Boosting is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules  and has inspired a lot on theoretical analysis and algorithm design in supervised learning . The seminal algorithm in boosting, AdaBoost, requires no knowledge of the upper bound of the edge, which makes it convenient in practice.

In addition to to binary AdaBoost,  also proposes two multi-class extensions, named AdaBoost.M1 and AdaBoost.M2. Then, Schapire and Singer's seminal paper  proposes another extension named AdaBoost.MH. The main idea of AdaBoost.MH is to use vector-valued base classifiers to build a multi-class discriminant function of \(K\) outputs when there are \(K\) classes, and then replace the weight vector in AdaBoost with a weight matrix over instances and labels.

The simplest implementation of the concept in AdaBoost.MH is to use \(K\) independent one-against-all classifiers in which base classifiers are only loosely connected through the common normalization of the weight matrix. However,  points out that such an implement is suboptimal in most of the practical problems since it is limited to only decision stumps weak learners. To solve this problem,  proposes another base learner named multi-class Hamming trees, which optimizes the multi-class edge without reducing the problem to \(K\) binary classifications. The key idea in  is to factorize general vector-valued classifiers \(\) into an input-independent code vector of length \(K\), i.e., \(\{-1,+1\}^{K}\), and label-independent scalar classifier \(\). However,  gets in trouble when proving the convergence rate of the proposed implement of AdaBoost.MH due to the factorizationstep. So  raises an open problem in COLT 2014, looking for a convergence rate of the factorized AdaBoost.MH in , with limited dependence on the sample size \(n\).

Our **contributions** can be concluded as follows:

1. We provide a convergence result (Theorem 3.3) of factorized AdaBoost.MH, where the step \(T^{*}\) which guarantees the training error to be \(0\) is of order \(O(n^{2} n)\).
2. According to the requirement of , we improve the dependence on \(n\) and resolve the open problem by providing a convergence result (Theorem 3.4) where \(T^{*}\) is of order \(O(K(nK))\). This result greatly improves when \(n\) is much larger than \(K\).

More related works are deferred to Appendix B.

## 2 Preliminaries

We consider a multi-class classification problem where the input space is \(=^{d}\) and \(=[K]\) is the label space, where \(K\) is the number of classes and \([K]\{1,,K\}\). Assume we attain the training data \(_{L}=\{(_{1},(_{1})),,(_{ n},(_{n}))\}\), where \((_{i})\) is the label of \(_{i}\). Since we want to use vector-valued classifiers, it is convenient to use the one-hot labels \(_{i}\{-1,+1\}^{K}\) for \(_{i}\), where \(_{i}((_{i}))=1\) and all the other elements are \(-1\). We use the new dataset \(=\{(_{1},_{1}),,(_{n},_{n})\}\) as the input data of AdaBoost.MH and define an observation matrix \((_{1},,_{n})^{T}^{n  d}\), a label matrix \((_{1},,_{n})^{T}\{-1,+1\}^{n  K}\). We call \(\) and \(\) the label and the index of \(\) respectively, as in .

 considers a special case of AdaBoost.MH, where each weak classifier has a specialized structure. AdaBoost.MH returns a vector-valued discriminant function \(:^{K}\) with a combined predictor \(_{}:\{-1,+1\}^{K}\) where \(_{}()_{l}=(() _{l})\) for \(l=1,,K\). Here and in this paper, we define

\[(x)=\{+1& x 0\\ -1& x<0..\]

The goal of the AdaBoost.MH algorithm  is to return \(\) such that the Hamming loss of \(_{}\),

\[_{}(_{},)_{ i=1}^{n}_{l=1}^{K}w_{i,l}\{_{}(_{i})_{ l} y_{i,l}\},\] (1)

is as small as possible, where \(()\) is the indicator function and \(=[w_{i,l}]^{n K}\) is a distribution over the data points and the labels. \(\) can be chosen as any distribution over \([n][K]\) and is different in different papers. In , the authors set \(w_{i,l}=\) for any \(i[n],l[K]\). Here, we follow  and set

\[w_{i,l}=\{&y_{i,l}=+1\\ &y_{i,l}=-1..\] (2)

We define the weighted multi-class exponential margin-based error

\[_{}(,)_{i=1}^{n}_ {l=1}^{K}w_{i,l}(-(_{i})_{l} y_{i,l})\] (3)

as a surrogate for \(_{}(_{},)\). Since \(\{_{}(_{i})_{l} y_{i,l}\}= \{(_{i})_{l} y_{i,l} 0\}(- (_{i})_{l} y_{i,l})\), we can get that \(_{}(_{},)_ {}(,)\).

It's well-known that AdaBoost directly minimizes the exponential loss [19, Chapter 7], then, we can apply the AdaBoost algorithm to the extended binary training set \(_{i=1}^{n}\{(_{i},y_{i,l})\}_{l=1}^{K}\), yielding the AdaBoost.MH algorithm, which directly minimizes \(_{}(,)\) and output the final discriminant function \(^{(T)}()\), where \(^{(T)}()=_{t=1}^{T}^{(T)}()\) is a sum of \(T\) base classifiers \(^{(t)}:^{d}^{K}\) returned by a base learner algorithm \(}(,,^{(t)})\) in each iteration \(t\).

Define

\[Z(,)=_{i=1}^{n}_{l=1}^{K}w_{i,l}(-(_{i})_{l} y_{i,l}),\] (4)by a similar calculation in [19, Proof of Theorem 3.1], we can obtain that:

\[_{}(^{(T)},)=_{t=1}^{T}Z(^{(t)},^{(t)}).\]

According to the above discussion, we know that to minimize \(_{}(^{(T)},),\) the base learner needs to find a \(^{(t)}\) that minimizes \(Z(^{(t)},^{(t)})\) at the \(t\)-th iteration. In the following, we introduce two choices of \(\) in  and , the corresponding convergence rate of \(_{}(^{(T)},),\) and problems when trying to get a convergence rate of \(_{}(^{(T)},)\) for factorized AdaBoost.MH.

### Unfactorized Choice

 considers using \(\) with the form \(()=()\), where \(\) and \(:^{d}\{-1,+1\}^{K}\) can be seen as the vector consists of \(K\) binary classifiers \(_{1},,_{K}\).

We consider the \(t\)-th iteration, and to simplify the notations, we omit the superscript \(t\) and use \(,,,\) to represent \(^{(t)},^{(t)},^{(t)},^{(t)}\) respectively. According to , if we define

\[r=_{i=1}^{n}_{l=1}^{K}w_{i,l} y_{i,l}( _{i})_{l}\] (5)

as the edge, then we have

\[Z(,) =_{i=1}^{n}_{l=1}^{K}w_{i,l}(-( _{i})_{l} y_{i,l})=_{i=1}^{n}_{l=1}^{K}w_{i,l} (-(_{i})_{l} y_{i,l})\] \[=_{i,l:(_{i})_{l} y_{i,l}= 1}w_{i,l} e^{-}+_{i,l:(_{i})_{l}  y_{i,l}=-1}w_{i,l} e^{}.\]

Since \(_{i,l:(_{i})_{l} y_{i,l}=1}w_{i,l}+ _{i,l:(_{i})_{l} y_{i,l}=-1}w_{i,l}=1\) and \(_{i,l:(_{i})_{l} y_{i,l}=1}w_{i,l}- _{i,l:(_{i})_{l} y_{i,l}=-1}w_{i,l}=r\), we can get that

\[_{i,l:(_{i})_{l} y_{i,l}=1}w_{i,l}= ,_{i,l:(_{i})_{l} y_{i,l}= -1}w_{i,l}=.\]

So we have:

\[Z(,)= e^{-}+ e ^{}.\]

Fix \(\) first, minimizing \(Z(,)\) over \(\) yields that:

\[=().\]

This gives

\[Z(,)=}.\]

Then choose \(\) to minimize \(}\), i.e., maximize \(|r|\). If we have \(r^{(t)}>0\) for all \(t\), then we can get:

\[_{}(^{(T)},)=_{t=1}^{T})^{2}}(})^{T} (-}{2}T),\]

which means that the weighted exponential error goes to error exponentially fast. Let \((-}{2}T)<\), we know that the weighted Hamming error becomes zero after

\[T^{*}=}+1\]

iterations. The condition \(r^{(t)}>0\) for all \(t\) is satisfied when the empirically weak learning condition on the classifier \(\) holds for the extended binary training set \(_{i=1}^{n}\{(_{i},y_{i,l})\}_{l=1}^{K}\).

**Definition 2.1** (empirically \(\)-weak learning condition).: For a given binary dataset \(\{(_{1},y_{1}),,(_{m},y_{m})\}\) where \(y_{i}\{-1,+1\}\), we say that the empirically \(\)-weak learning condition holds for some \(>0\) if for any distribution \(^{m-1}\) over \([m]\), we can always find a binary classifier \(:\{-1,+1\}\) such that:

\[=_{i=1}^{m}_{i} y_{i}(_{i}),\]

where

\[^{m-1}=\{^{m}|_{i} 0\; i[m],_{i=1}^{m}_{i}=1\}\]

is the \((m-1)\)-dimensional probability simplex.

### Factorized Choice

The original AdaBoost.MH reduces the multi-class problem into \(K\) binary one-against-all classifications.  avoids such a reduction by factorizing the vector-valued classifier \(\) into an input-independent vector of length \(K\) and a label-independent scalar classifier. Formally,  sets

\[()=(),\]

where \(^{+}\) is a positive real-valued base coefficient, \(\{-1,+1\}^{K}\) is an input-independent vote (or code) vector of length \(K\), and \(:^{d}\{-1,+1\}\) is a label-independent binary classifier. For more details about the factorized AdaBoost.MH, please refer to Algorithm 1 in Appendix A.

We consider the \(t\)-th iteration, and to simplify the notations, we omit the superscript \(t\) and use \(,,,,\) to represent \(^{(t)},^{(t)},^{(t)},^{(t)},^{(t)}\) respectively.  shows that

\[Z(,)=+e^{-}}{2}--e^ {-}}{2}_{l=1}^{K}v_{l}(_{l+}-_{l-}),\]

where

\[_{l-}=_{i=1}^{n}w_{i,l}\{(_{i}) y_{i,l}\}\]

is the weighted per-class error rate and

\[_{l+}=_{i=1}^{n}w_{i,l}\{(_{i})=y_{i,l}\}\]

is the weighted per-class correct classification rate for each class \(l=1,,K\). Similar to Equation (5), we define the multi-class edge of the classifier \(\) as

\[=(,,)=_{l=1}^{K}_{l}=_{ l=1}^{K}v_{l}(_{l+}-_{l-})=_{i=1}^{n}(_{i}) _{l=1}^{K}w_{i,l} v_{l} y_{i,l},\] (6)

where

\[_{l}=v_{l}(_{l+}-_{l-})=_{i=1}^{n}w_{i,l} v_{ l}(_{i}) y_{i,l}\]

is the classwise edge of \(\). By a similar calculation as in Section 2.1, we know that \(Z(,)\) is minimized when we set

\[=(),\]

which gives

\[Z(,)=}.\]

So in order to minimize \(Z(,)\), we need to choose \(\) and \(\) to maximize \(||\). From the equation \((,,)=_{l=1}^{K}v_{l}(_{l+}-_{l -})\), we know that if \((,,) 0\), then\(-(,,) 0\). So the problem reduces to finding \(,\) that maximize \(\). From Equation (6) we know that for fixed \(\), \(\) is maximized when we choose \(\) as

\[v_{l}=\{+1&_{l+}_{l-}\\ -1&_{l+}<_{l-}.\] (7)

for all classes \(l=1,,K\).

Similar to Section 2.1, if there exists a number \(>0\) such that \((^{(t)},^{(t)},^{(t)})\) for all \(t=1,,T\), then we can get an upper bound for \(_{}(^{(T)},)\):

\[_{}(^{(T)},)=_{t=1}^{T}^{(t)},^{(t)},^{(t)})^{2}} (})^{T}(-}{2}T),\]

which means that the weighted exponential error goes to error exponentially fast. Let \((-}{2}T)<\), we know that the weighted Hamming error becomes zero after

\[T^{*}=}+1\]

iterations. To get the exponential convergence rate, the question now is whether there exists a number \(>0\) such that \((^{(t)},^{(t)},^{(t)})\) for all \(t=1,,T\).

### Conditions for the Two Choices

For the condition in the unfactorized choice, if the empirically \(^{}\)-weak learning condition holds, then for a fixed weight matrix \(\), let \(I=\{l[K]_{i=1}^{n}w_{i,l}>0\}\), then for all \(l I\), there exists a binary classifier \(_{l}\) such that

\[r_{l}=_{i=1}^{n}}{_{i=1}^{n}w_{i,l}}_{l}(_{i})y_{i,l}^{},\]

then we can find a \(\) such that \(_{l}=_{l}\) for \(l I\) so that

\[r=_{i=1}^{n}_{l=1}^{K}w_{i,l}_{l}(_{i}) y_{i,l}=_{l I}_{i=1}^{n}w_{i,l}_{l}( _{i}) y_{i,l}_{l I}_{i=1}^{n}w_{i,l}^ {}=^{}.\]

So the empirically \(^{}\)-weak learning condition is sufficient for an exponential convergence rate for the AdaBoost.MH algorithm in .

For the factorized choice proposed in , we can not use the above argument since \(\) is factorized and we need to find a binary classifier \(\) for all \(l=1,,K\), while for the unfactorized choice, we can find \(K\) binary classifiers \(_{1},,_{K}\) separately for each class. In , the author tries to solve this problem by constructing pseudo-weights and pseudo-labels and then applying the empirically \(^{}\)-weak learning condition to the constructed dataset \(\{(_{1},y_{1}^{}),,(_{n},y_{n}^{})\}\).

 rewrites \(\) as

\[ =_{i=1}^{n}(_{i})_{l=1}^{K}w_{i,l} v _{l} y_{i,l}=_{i=1}^{n}(_{i})_{l=1}^{K}w_{i,l} [\{v_{l} y_{i,l}=+1\}-\{v_{l} y_{i,l}=-1\}]\] \[=_{i=1}^{n}(_{i})(w_{i}^{+}-w_{i}^{-})=_ {i=1}^{n}(_{i})(w_{i}^{+}-w_{i}^{-})|w_{i}^{+}-w_{i }^{-}|,\]

where we define

\[w_{i}^{+}=_{l=1}^{K}w_{i,l}\{v_{l} y_{i,l}=+1\},\;\;w_{i}^{ -}=_{l=1}^{K}w_{i,l}\{v_{l} y_{i,l}=-1\}\]for simplicity. Then we define \(y^{}_{i}=(w^{+}_{i}-w^{-}_{i})\) as the \(i\)-th pseudo-label and \(w^{}_{i}=|w^{+}_{i}-w^{-}_{i}|\) as the \(i\)-th pseudo-weight, then

\[=_{i=1}^{n}w^{}_{i} y^{}_{i}( _{i}).\]

However, since \(_{i=1}^{n}w^{}_{i}=_{i=1}^{n}|w^{+}_{i}-w^{-}_{i}|_{i=1} ^{n}(w^{+}_{i}+w^{-}_{i})=1\), \(^{}=(w^{}_{1},,w^{}_{n})\) is not necessarily a distribution on \([n]\). To make use of the empirically \(^{}\)-weak learning condition, we define

\[w^{}_{}_{i=1}^{n}w^{}_{i} 1.\]

If we can get a lower bound \(>0\) such that \(w^{}_{}\), then we have:

\[=_{i=1}^{n}w^{}_{i} y^{}_{i}( _{i})=w^{}_{}_{i=1}^{n}_{i}}{w^{}_{ }} y^{}_{i}(_{i}) w^{}_{ }^{}^{},\]

where the first inequality is from the empirically \(^{}\)-weak learning condition. Since the number of examples \(n\) may be very large, we wish the lower bound \(\) to be independent of \(n\), but it can depend on the number of classes \(K\).

Then  raises an **open problem**:

_Whether there exists a setup (\(,,\), and function class) in which all of the \(2^{K}\) different vote vectors \(\{-1,+1\}^{K}\) lead to arbitrarily small (or zero) \(w^{}_{}\), or we can find a constant (independent of \(n\)) lower bound \(\) such that with at least one vote vector and classifier \(\), \(w^{}_{}\) holds?_

We resolve this open problem by showing that:

_There exists a constant \(=}\) such that: for any \(,,\) and function class, there always exists a vote vector \(\) s.t. \(w^{}_{}\) holds. With this result, if the empirically \(^{}\)-weak learning condition holds, then for any \(,,\), there always exists a vote vector \(\) and a binary classifier \(\) such that \(=_{i=1}^{n}(_{i})_{l=1}^{K}w_{i,l} v_{l}  y_{i,l}}{}\). So if we run the Adaboost.MH algorithm with factorized \(\), \(_{}(^{(T)},)\) becomes zero after at most \(T^{*}=)^{2}}+1\) iterations._

## 3 Our Solution

In this section, we provide formal theorems for our above answer to the open problem and further discussions.

Because the training set size \(n\) may be very large,  requires the lower bound to be independent of the training set size \(n\) (but can be dependent on the number of classes \(K\)), which is much more difficult than finding a lower bound depends on \(n\). To consider this problem more holistically, we provide two lower bounds, one depends on \(n\) and another depends on \(K\).

To solve this problem, we first formulate the problem of "finding a constant \(\) such that for any training set and weight matrix, there exists a code vector \(\) such that \(w^{}_{}\) (\(w^{}_{}\) depends on the training set, weight matrix, and the code vector)" into "finding the lower bound of a constrained minimax problem". We then provide a \(n\)-dependent lower bound by the fact \(\|\|_{1}\|\|_{}\) and the fact that the maximum is not smaller than the average, where \(\|\|_{p}\) is the \(_{p}\)-norm of a vector. For the \(n\)-independent lower bound, we choose to lower bound the expected value of \(w^{}_{}\) when the code vector \(\) is drawn from some distribution \(\) on \(\{-1,+1\}^{K}\). To eliminate the trouble caused by the labels, we choose \(\) to be a Rademacher random vector with independent elements, i.e., \(=(_{1},,_{K})\) where \([_{i}=1]=[_{i}=-1 ]=\) for \(i=1,,K\). We then provide the lower bound with the help of Khintchine inequality .

We define

\[\{^{n K}_{i,l} 0i[n],l[K];_{i=1}^{n}_{l=1}^{K}_{i,l}=1\}\]

as the set of all possible \(\). Let \(():[K]\{-1,+1\}^{K}\) be

\[(l)_{i}=\{+1&i=l\\ -1&i l.,\]

we the define \(\{((l_{1}),,(l_{n}))^{T} \{-1,+1\}^{n K}|l_{1},,l_{n}=[K]\}\) as the set of all possible \(\), and define \(=\{-1,+1\}^{K}\) as the set of all possible \(\). We then have:

\[w^{}_{}(,,) =_{i=1}^{n}|w^{+}_{i}-w^{-}_{i}|\] \[=_{i=1}^{n}|_{l=1}^{K}w_{i,l}\{[v_{l }y_{i,l}=+1]-[v_{l}y_{i,l}=-1]\}|\] \[=_{i=1}^{n}|_{l=1}^{K}w_{i,l} v_{l} y_{i,l }|\] \[=_{i=1}^{n}|()_{i} ^{T},|=\|() \|_{1},\]

where \(\) is the Schur product and \(\|\|_{1}=_{i=1}^{n}|x_{i}|\) is the \(_{1}\)-norm of the vector \(\).

The following two facts translate the problem that we are concerned with into a minimax problem.

**Fact 3.1**.: The following two statements are equivalent:

(1) There exists a setup \((,,)\) in which all of the \(2^{K}\) different vote vectors \(\) lead to arbitrarily small (or zero) \(w^{}_{}\).

(2) \(_{,}_{ }\|()\|_{1}\) is arbitrarily small (or zero).

**Fact 3.2**.: The following two statements are equivalent:

(1) We can find a constant (independent of \(n\)) lower bound \(\) such that for any setup \((,,)\), there exists at least one vote vector and classifier \(\) such that \(w^{}_{}\) holds.

(2) we can find a constant (independent of \(n\)) lower bound \(\) such that \(_{,}_{ }\|()\|_{1}\).

So, to find the lower bound \(\), we need to prove that \(_{,}_{ }\|()\|_{1}\). Let's begin with a simple \(n\)-dependent lower bound.

**Theorem 3.3** (An \(n\)-dependent lower bound).: \(_{,}_{ }\|()\|_{1}\)_._

Proof of Theorem 3.3.: \[_{,}_{ }\|()\|_{1} _{,}_{ }\|()\|_{}\] \[=_{,}_{ ,[n]}|_{l=1}^{K}w_{i,l} y_{i,l}  v_{l}|\] \[}{{}}_{ [n]}_{l=1}^{K}w_{i,l}}{{=}},\]

where \(a\) is from the fact that \(\|\|_{1}\|\|_{}\) where \(\|\|_{}=_{1 i n}|_{i}|\) is the \(_{}\)-norm of \(\); \(b\) comes from choosing \(v_{l}=y_{i,l}\) for \(l=1,,K\) when \(i\) is fixed; \(c\) is from the fact that \(_{i[n]}_{l=1}^{K}w_{i,l}_{i=1}^{n}_{l=1}^{K}w_ {i,l}=\) and the equation can be attained.

**Remark 1**.: _The lower bound in Theorem 3.3 depends on \(n\), and if we use \(\) as the lower bound of \(w^{}_{}\), then we need at most \(T^{*}=(2n(K-1))}{(^{})^{2}}+1\) iterations to make the exponential error become zero, which quadratically increases as \(n\). When the training set is large, \(T^{*}\) becomes very large, which is one of the reasons that  wants to get a lower bound independent of \(n\)._

Next, we introduce how we solve the open problem to get a lower bound independent of \(n\).

**Theorem 3.4** (An \(n\)-independent lower bound).: \(_{,}_{}() _{1}}\)_._

**Remark 2**.: _Theorem 3.4 shows that there is a constant \(=}\) such that for any setup \(,\), there always exists a code vector \(\) such that \(w^{}_{}\). This solves the open problem proposed by . So we need at most \(T^{*}=)^{2}}+1\) iterations (see Corollary 3.6) to make the exponential error become zero._

To prove Theorem 3.4, we use the well-known Khintchine inequality  Lemma 3.5.

**Lemma 3.5** (10, Khintchine inequality).: _Let \(\{_{n}\}_{n=1}^{N}\) be i.i.d. random variables with \((_{n}= 1)=\) for \(n=1,,N\), i.e., a sequence with Rademacher distribution. Let \(0<p<\) and let \(x_{1},,x_{n}\). Then_

\[A_{p}(_{n=1}^{N}|x_{n}|^{2})^{1/2}( }_{_{1},,_{N}}|_{n=1}^{N}_{n} x_{n}|)^{1/p} B_{p}(_{n=1}^{N}|x_{n}|^{2})^{1/2}\]

_for some constants \(A_{p},B_{p}>0\) depending only on \(p\), where_

\[A_{p}=\{2^{1/2-1/p}& 0<p p_{0}\\ 2^{1/2}(((p+1)/2)/)^{1/p}& p_{0}<p<2\\ 1& 2 p<.,\]

_and_

\[B_{p}=\{1& 0<p 2\\ 2^{1/2}(((p+1)/2)/)^{1/p}& 2<p<.,\]

_where \(p_{0} 1.847\) and \(\) is the Gamma function._

Proof of Theorem 3.4.: The basic idea of our proof is to consider the average performance of different code vectors for fixed choices of \(,\), i.e., use the fact that the maximum is not less than the average, which gives:

\[_{, D}_{}() _{1} _{, D}()_{1}\] \[=_{, D}[_{i=1}^{n}|_{l=1}^{K}w_{i,l} v_{l} y _{i,l}|]\]

for any distribution \(D\) on \(\).

We take \(v_{1},,v_{K}\) be independent Rademacher random variables and then get:

\[_{, D}[_{i=1}^{n}|_{l=1}^{K}w_{i,l} v_{l} y _{i,l}|] =_{,_{1},,_{K}}[_{i=1}^{n}| _{l=1}^{K}w_{i,l}_{l} y_{i,l}|]\] \[ A_{1}_{}_{i=1}^{n}( _{l=1}^{K}w_{i,l}^{2})^{1/2}\] \[}{{=}}}_{}_{i=1}^{n}(_{l=1}^{K}w_{i,l}^{2} )^{1/2}\] \[}{{}}}_{}_{i=1}^{n}_{l=1}^{K}w_{i,l}\] \[=},\]where \(a\) applies Lemma 3.5 with \(p=1\) and the fact that \(y_{i,l}^{2}=1\) for all \(i,l\); \(b\) puts in the value of \(A_{1}\); \(c\) uses the concavity of \(\) and Jensen's inequality. 

With the lower bound of \(w_{}^{}\), we can now provide a lower bound of the edge \(\) and convergence guarantee for the version of AdaBoost.MH proposed by  conditioned on the empirically \(^{}\)-weak learning condition.

**Corollary 3.6** (Lower bound for \(\)).: _If the empirically \(^{}\)-weak learning condition holds, then for any \(,,\), there always exists a binary classifier \(^{*}\) and code vector \(^{}\) such that_

\[(^{},^{*},)}{ }.\]

_If we run AdaBoost.MH with factorized \(\), then we have_

\[_{}(^{(T)},)(-}{4K}T)\]

_and we need at most_

\[T^{*}=)^{2}}+1\]

_to make the exponential error \(_{}(^{(T)},)\) become zero._

Proof of Corollary 3.6.: For any \(,,\), let \(^{}=}{}\|( )\|_{1}\). Let \(w_{i}^{},y_{i}^{},w_{}^{}\) be defined as before, where we replace \(\) there by \(^{}\). By Theorem 3.4, \(w_{}^{}}>0\).

By the empirically \(^{}\)-weak learning condition, there exists a binary classifier \(^{*}\) such that

\[_{i=1}^{n}^{}}{w_{}^{}} y_{i}^{} ^{*}(_{i})^{},\]

which means that

\[(^{},^{*},) w_{}^{} ^{}}{}.\]

For fixed \(,,\), let \(^{*}()\) be the code vector depending on \(\) that is defined in Equation (7). Since the choice \(^{*}()\) maximizes \(\) when \(\) is fixed, we have that:

\[(^{*}(^{*}),^{*},)(^{},^{*},)}{}.\]

Combining the arguments in Sections 2.1 and 2.2 shows \(_{}(^{(T)},)(-}{4K}T)\) and that when we run AdaBoost.MH with factorized \(\), which returns \(^{*},^{*}(^{*})\) at each iteration,

\[_{}(^{(T)},)<,\ \ _{}(_{^{(T)}},)=0\]

after at most

\[T^{*}=)^{2}}+1\]

iterations. 

The previous discussions are based on fixing the training set size \(n\) and the number of classes \(K\). Here we consider the case when they can tend to infinity. We think the reason  looks for a lower bound of \(w_{}^{}\) that is independent of \(n\) is that the author thinks the number of examples \(n\) can be arbitrarily large in some cases, which may make the lower bound of \(w_{}^{}\) arbitrarily small.

Combine our two lower bounds in Theorems 3.3 and 3.4, for any \(,,\), we can always find a \(\{-1,+1\}^{K}\) such that:

\[w_{}^{}\{,}\},\]

so the lower bound can become arbitrarily small only when \(n\) and \(K\) tend to infinity together.

Discussion

In this section, we discuss the importance of solving this problem.

In statistical learning theory, algorithms can be divided into proper and improper learning algorithms. For proper learning, the most famous algorithms are ERM  and its variants [26; 16; 24]. For improper learning, boosting algorithms are usually used to construct improper algorithms [2; 3; 17; 23]. Furthermore, the convergence rate of the boosting algorithm usually affects the sample complexity of the constructed algorithm, i.e., the sample complexity of the constructed algorithms usually depends on the value \(T^{*}\) where the training error becomes zero. So boosting algorithms are basic but important tools in statistical learning theory.

In binary classification, AdaBoost is one of the most famous and influential algorithms among all the binary boosting algorithms. Since the proposal of AdaBoost, many works have tried to extend the boosting framework to multi-class classification problems. Most multi-class boosting algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems, among which the most famous and influential one is AdaBoost.MH . Moreover, AdaBoost.MH has inspired the proposal of many other multi-class boosting algorithms. For example, inspired by the characteristics of AdaBoost.MH that reduces the multi-class classification problem to multiple two-class problems,  chooses another line of thought to develop an algorithm that directly extends the AdaBoost.MH algorithm to the multi-class case without reducing it to multiple two-class problems;  demonstrates how to improve the efficiency and effectiveness of AdaBoost.MH and proposes the algorithm LDA-AdaBoost.MH;  proposes an efficient multi-class fault diagnosis approach based on the AdaBoost.MH algorithm;  proposes a method for ranking based on AdaBoost.MH. There are also many other works based on AdaBoost.MH [21; 12; 8]. Furthermore, many works (for example, [13; 8; 25; 27]) use AdaBoost.MH as the baseline, which further shows the importance of AdaBoost.MH. For example, the only baseline used in  is AdaBoost.MH. In summary, AdaBoost.MH serves as a link between binary classification boosting algorithms and multi-class classification boosting algorithms, the cornerstone of multi-class boosting, and has a big influence on the multi-class boosting field. Our work is important because it shows that Kegl's work , which solves the computational problem (at the level of the strong learner at least) of AdaBoost.MH, does indeed work in theory and works essentially as fast as binary AdaBoost.

## 5 Conclusion

In this paper, we resolve the open problem raised by  by presenting a \(n\)-independent lower bound for \(w^{}_{}\). In addition to that, we also provide a \(n\)-dependent lower bound for \(w^{}_{}\) to show that \(w^{}_{}\) may be arbitrarily small only when \(n\) and \(K\) tend to infinity together. Based on the lower bounds for \(w^{}_{}\) and the empirically \(^{}\)-weak learning condition, we provide an upper bound for the weighted exponential error and a number \(T^{*}\) where the weighted exponential error becomes zero after at most \(T^{*}\) iterations.