# Expert load matters: operating networks at high accuracy and low manual effort

Sara Sangalli

Computer Vision Lab

ETH Zurich

sara.sangalli@vision.ee.ethz.ch

&Ertune Erdil

Computer Vision Lab

ETH Zurich

&Ender Konukoglu

Computer Vision Lab, ETH Zurich

The LOOP Zurich - Medical Research Center, Zurich, Switzerland

###### Abstract

In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as _confidence operating characteristic (COC)_ curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that _maximizes the area under this COC curve_. This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans. We perform experiments on multiple computer vision and medical image datasets for classification. Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions.1

## 1 Introduction

Artificial intelligence (AI) systems based on deep neural networks have achieved state-of-the-art results by reaching or even surpassing human-level performance in many predictive tasks [6; 33; 2; 36]. Despite the great potential of neural networks for automation, there are pitfalls when using them in a fully automated setting, especially pertinent for safety-critical applications, such as healthcare [17; 32; 34]. Human-AI collaboration aims at remedying such issues by keeping humans in the loop and building systems that take advantage of both . An example of such human-AI collaboration is hate speech detection for social media , where neural networks could reduce the load of manual analysis of contents required by humans. Healthcare is another relevant application [5; 22],. Forexample, a neural network trained to predict whether a lesion is benign or malignant should leave the decision to human doctors if it is likely to make an error . The doctors' domain knowledge and experience could be exploited to assess such, possibly, ambiguous cases and avoid mistakes.

A simple way of building collaboration between a network and a human expert is delegating the decisions to the expert when the network's confidence score for a prediction is lower than a threshold, which we refer to as "operating point". It is clear that the choice of the operating point can only be done through a trade-off between the network performance on the automatically analysed samples, i.e., the number of errors expected by the algorithm, and the number of delegated samples, i.e., experts' workload. The latter is crucial especially for applications where expert time is limited and expensive. For example, in medical imaging, the interpretation of more complex data requires clinical expertise and the number of available experts is limited, especially in low income countries . Hence, predictive models that can analyse a large portion of the samples at high accuracy and identify the few samples that should be delegated to human experts would naturally be more useful with respect to this trade-off.

It is possible to evaluate the performance of a predictive model taking simultaneously into account the accuracy and the number of samples that requires manual assessment from a human expert with a performance curve reminiscent of Receiver Operating Characteristic (ROC) curves, as illustrated in Fig. 1a. We will refer to this performance curve as _Confidence Operating Characteristics (COC)_ as it is similar to the ROC curve. A COC curve plots for a varying threshold on algorithm confidence, i.e., operating point, the accuracy of a model on the samples on which the algorithm is more confident than the threshold versus the number of samples remaining below the threshold. The former corresponds to the accuracy of the model on automatically analysed samples while the latter corresponds to the amount of data delegated to the human expert for analysis. In an ROC curve a balance is sought after between Sensitivity and Specificity of a predictive model, while a COC curve can be used by domain experts, such as doctors, to identify the most suitable balance between _the accuracy on the samples that are automatically analysed and the amount of data delegated to be re-examined by a human_ for the specific task. Variations of this curve have been used to evaluate the performance of automated industrial systems .

In this paper, we focus on the trade-off between model accuracy and the amount of samples delegated to domain experts based on operating points on model confidence. Specifically, our goal is to obtain better trade-off conditions to improve the interaction between the AI and the human expert. To this end, we propose a new loss function for multi-class classification, that takes into account both of the aspects by maximizing the area under COC (AUCOC) curve. This enforces the simultaneous increase in neural network accuracy on the samples not analysed by the expert and the reduction in human workload. To the best of our knowledge, this is the first paper to include the optimization of such curve during the training of a neural network, formulating it in a differentiable way. We perform experiments on two computer vision and three medical image datasets for multi-class class classification. We compare the proposed complementary AUCOC loss with the conventional loss functions for training neural networks as well as network calibration methods. The results demonstrate that our loss function complements other losses and improved both accuracy and AUCOC. Additionally, we evaluate network calibration and out-of-distribution (OOD) samples detection performance of networks trained with different losses. The proposed approach was also able to consistently achieve better OOD samples detection and on par network calibration performance.

## 2 Related Work

For the performance analysis of human-AI collaborative systems, confidence operating characteristics (COC) curves can be employed, which plot network accuracy on accepted samples against manual workload of a human expert, e.g as in . While such curves have been used, to the best of our knowledge, we present the first work that defines a differentiable loss based on COC curve, in order to optimize neural networks to take into account simultaneously accuracy and experts' load for a human-AI collaborative system. Thus, there is no direct literature with which we can compare.

The underlying assumption in deciding which sample to delegate to human expert based on a threshold on confidence scores is that these scores provided by deep learning models indicate how much the predictions are likely to be correct or incorrect. However, the final softmax layer of a network does not necessarily provide real probabilities of correct class assignments. In fact, modern deep neural networks that achieve state-of-the-art results are known to be overconfident even in their wrong predictions. This leads to networks that are not well-calibrated, i.e., the confidence scores do not properly indicate the likelihood of the correctness of the predictions . Network calibration methods mitigate this problem by calibrating the output confidence scores of the model, making the above mentioned assumption hold true. Thus, we believe that the literature on network calibration methods is the closest to our setting because they also aim at improving the interaction between human and AI, by enforcing correlation between network confidence and accuracy, so that confidence can be used to separate samples where networks' predictions are not reliable and should be delegated to human experts. Calibration methods aim to get models that are highly accurate in the samples they are confident, but not the problem of minimising the number of samples delegated to human experts, contrary to the loss proposed here.

Guo et al.  defines the calibration error as the difference in expectation between accuracy and confidence in each confidence bin. One category of calibration methods augments or replaces the conventional training losses with another loss to explicitly encourage reducing the calibration error. Kumar et al.  propose the MMCE loss by replacing the bins with kernels to obtain a continuous distribution and a differentiable measure of calibration. Karandikar et al.  propose two losses for calibration, called Soft-AvUC and Soft-ECE, by replacing the hard confidence thresholding in AvUC  and binning in ECE  with smooth functions, respectively. All these three functions are used as a secondary loss along with conventional losses such as cross-entropy. Mukhoti et al.  find that Focal Loss (FL)  provides inherently more calibrated models, even if it was not originally designed for this, as it adds implicit weight regularisation. The authors further propose Adaptive Focal Loss (AdaFL) with a sample-dependent schedule for the choice of the hyperparameter \(\). The second category of methods are post-hoc calibration approaches, which rescale model predictions after training. Platt scaling  and histogram binning  fall into this class. Temperature scaling (TS)  is the most popular approach of this group. TS scales the logits of a neural network, dividing them by a positive scalar, such that they do not saturate after the subsequent softmax activation. TS can be used as a complementary method and it does not affect model accuracy, while significantly improving calibration. A recent work by Gupta et al.  fits a spline function to the empirical cumulative distribution to re-calibrate post-hoc the network outputs. They also present a binning-free calibration measure inspired by the Kolmogorov-Smirnov (KS) statistical test. Lin et al.  propose a Kernel-based method on the penultimate-layer latent embedding using a calibration set.

## 3 Methods

In this section, we illustrate in detail the _Confidence Operating Characteristics (COC)_ curve. Then, we describe the proposed complementary cost function to train neural networks for classification: _the area under COC (AUCOC) loss (AUCOCLoss)_.

### Notation

Let \(D=(x_{n},y_{n})_{n=1}^{N}\) denote a dataset composed of \(N\) samples from a joint distribution \((,)\), where \(x_{n}\) and \(y_{n}=\{1,2,...,K\}\) are the input data and the corresponding class label, respectively. Let \(f_{}(y|x)\) be the probability distribution predicted by a classification neural network \(f\) parameterized by \(\) for an input \(x\). For each data point \(x_{n}\), \(_{n}=*{argmax}_{y}f_{}(y|x_{n})\) denotes the predicted class label, associated to a _correctness score_\(c_{n}=(_{n}=y_{n})\) and to a _confidence score_\(r_{n}=_{y}f_{}(y|x_{n})\), where \(r_{n}\) and \((.)\) is an indicator function. \(=[r_{1},...r_{N}]\) represents the vector containing all the predicted confidences for a set of data points, e.g., a batch. \(p(r)\) denotes the probability distribution over \(r\) values (confidence space). We assume a human-AI collaboration system where samples with confidence \(r\) lower than a _threshold_\(r_{0}\) would be delegated to a human expert for assessment.

### Confidence Operating Characteristics (COC) curve

Our first goal is to introduce an appropriate evaluation method to assess the trade-off between a neural network's prediction accuracy and the number of samples that requires manual analysis from a domain expert. We focus on the COC curve, as it provides practitioners with flexibility in the choice of the operating point, similarly to the ROC curve.

#### 3.2.1 x-y axes of the COC curve

To construct the COC curve, first, we define a _sliding threshold_\(r_{0}\) over the space of predicted confidences \(r\). Then, for each threshold \(r_{0}\), we calculate the portion of samples that are delegated to human expert and the accuracy of the network on the remaining samples for the threshold \(r_{0}\), which form the **x-axis** and **y-axis** of a COC curve, respectively. These axes are formulated as follows

\[x-:\ _{0}=p(r<r_{0})=_{0}^{r_{0}}p(r)dr, y- :[c|r r_{0}]\] (1)

For each threshold level \(r_{0}\), \(_{0}\) represents the portion of samples whose confidence is lower than that threshold, i.e., the amount of the samples that are delegated to the expert. \([c|r r_{0}]\) corresponds to the expected value of the correctness score \(c\) for all the samples for which the network's confidence is equal or larger than \(r_{0}\), i.e., accuracy among the samples for which network prediction will be used. This expected value, i.e., y-axis, can be computed as

\[[c|r r_{0}]=}_{r_{0}}^{1}[c|r ]p(r)dr.\] (2)

We provide the derivation of Eq. 2 in the Appendix A.

#### 3.2.2 Area under COC curve

Like the area under ROC curve, area under COC curve (AUCOC) is a global indicator of the performance of a system. Higher AUCOC indicates lower number of samples delegated to human experts or/and higher accuracy for the samples that are not delegated to human experts but analysed only by the network. Lower AUCOC on the other hand, indicates higher number of delegations to human experts or/and lower accuracy on the samples analysed only by the network. Further explaination is reported in Appendix I. It can be computed by integrating the COC curve over the whole range of \(_{0}\):

\[AUCOC=_{0}^{1}[c|r r_{0}]d_{0}=_{0}^{1}\{_ {r_{0}}^{1}[c|r]p(r)dr\}}{1-_{0}}\] (3)

Further details are provided in Section 3.5

### AUCOCLoss: Maximizing AUCOC for training neural networks

As mentioned, in human-AI collaboration systems, higher AUCOC means a better model. Therefore, in this section we introduce a new loss function called AUCOCLoss that maximizes AUCOC for training classification neural networks.

AUCOC's explicit maximization would enforce the reduction of the number of samples delegated to human expert while maintaining the accuracy level on the samples assessed only by the algorithm (i.e., keeping \([c|r r_{0}]\) constant) and/or the improvement in the prediction accuracy of the samples analysed only by the algorithm while maintaining a particular amount of data to be delegated to the human (i.e., keeping \(_{0}\) constant), as illustrated in Figure 0(a).

We define our loss function to maximize AUCOC as

\[AUCOCLoss=-(AUCOC).\] (4)

We use the negative logarithm as AUCOC lies in the interval \(\), which corresponds to AUCOCLoss \([0,]\) which is suitable for minimizing cost functions. The dependence of AUCOC on the network parameters may not be obvious from the formulation given in Eq. 3. Indeed, this dependence is hidden in how \(p(r)\) and \([c|r]\) are estimated as we show next.

#### 3.3.1 Kernel density estimation for AUCOC

We need to formulate AUCOC in a differentiable way in order for it to be incorporated in a cost function for the training of a neural network. For this purpose, we use kernel density estimation (KDE) on confidence predictions \(r_{n}\) for training samples to estimate \(p(r)\) used in Eq. 3

\[p(r)_{n=1}^{N}K(r-r_{n})\] (5)where \(K\) is a Gaussian kernel and we choose its bandwidth using Scott's rule of thumb . Then, the other terms in Eq. 3, namely \([c|r]p(r)\) and \(_{0}\), are estimated as

\[[c|r]p(r)_{n=1}^{N}c_{n}K(r-r_{n}), _{0}_{0}^{r_{0}}_{n=1}^{N}K(r-r_{n})dr.\] (6)

Note that \(r_{n}=_{y}f_{}(y|x_{n})\), from where the dependency of AUCOC on \(\) stems. Further, note that \(_{0}\) is the x-axis of the COC curve. The AUCOC is defined by integrating over the \(_{0}\) values, and therefore \(_{0}\) should not depend on the network parameters, i.e., its derivative with respect to \(\) should be equal to zero. We can write the derivative using Leibniz integral rule as follows:

\[}{d}=_{0}^{r_{0}}dr+p(r_{0}) }{d}=0\] (7)

Then, the constraint that \(_{0}\) should not depend on \(\) can be enforced explicitly by deriving the derivative \(dr_{0}/d\) from Eq. 7 as follows:

\[}{d}=-^{r_{0}}dr}{p(r_{0})}\] (8)

where \(p(r)\) is implemented as in Eq. 5. Derivations are provided in the Appendix.

### Toy example: added value by AUCOC

In this section, we demonstrate the added value of assessing the performance of a predictive model using COC curve and AUCOC through a toy example. We particularly compare with the widely used expected calibration error (ECE) , a binning-free calibration metric called Kolmogorov-Smirnov (KS)  and classification accuracy.

Assume we have two classification models \(f_{_{1}}\) and \(f_{_{2}}\) and they yield confidence scores and predictions for 5 samples as shown in Figure 1b. The green circles denote the predicted confidences for correctly classified samples, while the red crosses the confidences of the misclassified ones.

**ECE:** ECE divides the confidence space into bins, computes the difference between the average accuracy and confidence for each bin, and returns the average of the differences as final measure of calibration error. If we divide the confidence space into 5-bins, as indicated with the gray dotted lines in the confidence spaces of \(f_{_{1}}\) and \(f_{_{2}}\), ECEs computed for the both models will be identical. Furthermore, a similar situation can be constructed for any number of bins.

**KS:** KS is a binning-free metric, so it is less prone to binning errors than ECE. However, one can prove that there exist some confidence configurations for which the two models also report the same

Figure 1: **(a)** shows how to improve AUCOC, 1) increasing the accuracy of the network and/or 2) decreasing the amount of data to be analysed by the domain expert. The pink curve has higher AUCOC than the blue one. **(b)** illustrates a toy example where two models have the same accuracy, ECE with 5 bins and KS. However, they have different AUCOC values due to different ordering of correctly and incorrectly classified samples according to the assigned confidence by the network.

KS, in spite of it being a binning-free metric. This happens, for example, if the confidence values are (from left to right): 0.45, 0.55, 0.65, 0.70, 0.75.

**Accuracy:** These models have equal classification accuracy, 3/5 for both.

Therefore, looking at these three performance metrics, it is not possible to choose one model over the other since \(f_{_{1}}\) and \(f_{_{2}}\) perform identically.

**AUCOC:** On the contrary, the AUCOC is larger for \(f_{_{1}}\) than for \(f_{_{2}}\), as shown in Figure 1b. The difference in AUCOC is due to the _different ranking_ of correctly and incorrectly classified samples with respect to confidence values. It does not depend on a particular binning nor exact confidence values, but only on the ranking. By looking at the AUCOC results, one would prefer \(f_{_{1}}\) compared to \(f_{_{2}}\). Indeed, \(f_{_{1}}\) is a better model than \(f_{_{2}}\) because it achieves either equal or better accuracy than \(f_{_{2}}\), for the same amount of data to be manually examined. Changing point of view, \(f_{_{1}}\) delegates either equal or lower number of samples to experts for the same accuracy level.

**Comparison of AUCOC and calibration:** As also demonstrated with the toy example, AUCOC and calibration assess different aspects of a model. The former is a rank-based metric with respect to the ordering of the predictive confidences of correctly and incorrectly classified samples. The latter instead assess the consistency between the accuracy of the network and the predictive confidences, i.e. it is sensitive to the numerical values of the confidences themselves.

### Implementation Details

**Construction of the COC curve, thresholds \(r_{0}\) and operating points:** To provide flexibility in the selection of COC operating points, we need to cover the entire range \(\) of \(_{0}\) values. As a consequence, the thresholds \(r_{0}\) need to span the confidence range of the predictions \(r_{n}\) of the neural network. A natural choice for such thresholds is employing the predictions \(r_{n}\) themselves. This spares us from exploring the confidence space with arbitrarily fine-grained levels. First, we sort the confidences \(\) of the whole dataset (or batch) in ascending order. Each predicted confidence is then selected as threshold level \(r_{0}\) for the vector \(\), corresponding to a certain \(_{0}\) (x-value). Subsequently, \([c|r r_{0}]\) (y-value) is computed. Note that testing the threshold to each \(r_{n}\) in the sorted array corresponds to going through \(_{0}=[1/N,2/N,,(N-1)/N,1]\) for \(N\) samples one by one in order.

**Modelling correctness:** Instead of using \([c|r]p(r)_{n=1}^{N}c_{n}K(\|r-r_{n}\|)\) as given in Eq. 6, we approximate it as \([c|r]p(r)_{n=1}^{N}r^{*}K(\|r-r_{n}\|)\) where \(r_{n}^{*}=f_{}(y_{n}|x_{n})\) is the confidence of the correct class for a sample \(n\). The main reason is that the gradient of the misclassified samples becomes zero because \(c_{n}\) is zero when a sample \(x_{n}\) is not classified correctly. To deal with this issue we replace the correctness score \(c_{n}\), which can be either 0 or 1, with \(r_{n}^{*}\) which can take continuous values between 0 and 1, following Yin et al. . With this new approximation, we can back-propagate through misclassified samples and we found that this leads to better results.

**Use as secondary loss:** We observed in our experiments that using AUCOCLoss alone to train a network leads to very slow convergence with the existing optimization techniques. We believe this is due to the fact that the AUCOCLoss is a \(-log(_{n}z_{n})\) with \(z_{n}\). Optimization of such a form with gradient descent is slow because the contribution of increasing low \(z_{n}\)'s to the loss function is small. In contrast, cross-entropy is a \(-_{n} z_{n}\), where contribution of increasing low \(z_{n}\)'s to the loss is much larger, hence gradient-based optimization is faster. One can in theory create an upper bound to AUCOC loss by pulling the \(\) inside the sum, as we show in the Appendix. However, when \(z_{n}\) this upper bound is very loose and minimizing the upper bound not necessarily correspond to minimizing the AUCOC loss, hence does not maximize AUCOC. On the contrary, when AUCOCLoss is complements a primary cost that is faster to optimize, such as cross-entropy, it is improves over the primary loss and lead to the desired improved AUCOC while preserving the accuracy. This is obtained within the same amount of epochs and without ad-hoc fine-tuning the training hyper-parameters for the secondary loss.

## 4 Experiments

In this section, we present our experimental evaluations on multi-class image classification tasks. We performed experiments on five datasets. We experimented with CIFAR100  and Tiny-ImageNet, a subset of ImageNet , following the literature on network calibration. Further, we used two publicly available medical imaging datasets, DermaMNIST and RetinaMNIST , since medical imaging is an application area where expert time is limited and expensive. Due to space reasons we report results on a third medical dataset, TissueMNIST , in Appendix D, as well as information about the datasets in Appendix C.

**Loss functions:** We compared AUCOCLoss (referred to as AUCOCL in the tables) with different loss functions, most of which are designed to improve calibration performance while preserving accuracy: cross-entropy (CE), focal-loss (FL) , adaptive focal-loss (AdaFL) , maximum mean calibration error loss (MMCE) , soft binning calibration objective (S-ECE) and soft accuracy versus uncertainty calibration (S-AvUC) . We optimized MMCE, S-ECE, and S-AvUC losses jointly with a primary loss for which we used either CE or FL, consistently with the literature [21; 16]. The same is done for AUCOCLoss, applying KDE batch-wise during the training.

**Evaluation metrics:** To evaluate the performance of the methods, we used classification accuracy and AUCOC. Classification accuracy is simply the ratio between the number of correct samples over the total number of samples, and AUCOC is computed using Eq. 3. We also report some examples of operating points of COC curve - given a certain accuracy, we show the corresponding expert load (\(_{0}\) @acc), i.e., percentage of samples that need to be analyzed manually, on the COC curves.

Since we compared AUCOCLoss mostly with losses for network calibration, we also assessed the calibration performance of the networks, even though calibration is not a direct goal of this work. To this end, we used the following metrics: the widely employed equal-mass expected calibration error (ECE)  with 15 bins, the binning-free Brier score  and Kolmogorov-Smirnov (KS) score  and the class-wise ECE (cwECE) . The evaluation is carried out post temperature scaling (TS) , as it has been proved to be always beneficial for calibration.

**Hyperparameters:** In addition to the common hyperparameters for all losses, selected as specified in Appendix C, there are also specific ones that need to be tuned for some of them. In this case, we used the best hyperparameter settings reported in the original papers. In cases where the original paper did not report the specific values, we carried out cross-validation and selected the setup that provided the best performance on the validation set. We also selected the weighting factor for AUCOCLoss in the same way. We found that optimal weighting values for AUCOCLoss all fell between 1 and 10. We found empirically that models check-pointed using ECE provided very poor results. Networks check-pointed using either accuracy or AUCOC provided comparable outcome with respect to accuracy, therefore we reported results on AUCOC as they provided the best overall performance.

**OOD experiments:** We evaluate the out-of-distribution (OOD) samples detection performance of all methods since OOD detection is crucial for a reliable AI system and it is a common experiment in the network calibration literature. The commonly used experimental setting in this literature is using CIFAR100-C  (we report results for CIFAR100 with Gaussian noise in the main paper and the average over all the perturbations in the Appendix) and SVHN  as OOD datasets, while the network is trained on CIFAR100 (in-distribution). We evaluated the OOD detection performance of all methods using Area Under the Receiver Operating Characteristics (AUROC) curve, with MSP , ODIN , MaxLogit  and EBM .

**Class imbalance experiments:** Finally, in Appendix E we report results for accuracy and AUCOC on imbalanced datasets, being class imbalance present in many domains, such as medical imaging. We report results on the widely used Long-Tailed CIFAR100 (CIFAR100-LT) [37; 42] with controllable degrees of data imbalance ratio to control the distribution of training set. We trained with three levels of imbalance ratio, namely 100, 50, 10.

Further details on hyper-parameter settings are provided in the the Appendix.

## 5 Results

First, we report the results for accuracy and COC-related metrics. Then, we report results for calibration, even though this is not an explicit goal of this work. Bold results indicate the methods that performed best for each metric, underlined results are the second best. \(\) means the higher the better for a metric, while \(\) the lower the better. The experiments results are averaged over three runs.

In Tables 1, 2 we present our results based on accuracy and AUCOC. The results for TissueMNIST can be found in the Appendix. We observed that complementing CE and FL with our loss, i.e., CE+AUCOCL and FL+AUCOCL, they were consistently better than the other losses in all experiments. To evaluate the significance of the accuracy and AUCOC results, we performed permutation test  between the best AUCOCLoss and the best baseline with 1000 rounds. In all the cases it provided a p score \( 1\%\), therefore the differences in the models are steadily significant. The advantage of our model was even more apparent in amount of expert loads corresponding to specific accuracy levels. In particular, we measured the percentage of samples delegated to expert (\(_{0}\)) at 90% and 95% accuracy for CIFAR100, DermaMNIST and TissueMNIST, and at 65% and 75% accuracy for Tiny-Imagenet and RetinaMNIST (as the initial accuracy is also much lower on these datasets). In all the experiments, to varying degrees, AUCOCLoss provided lower delegated samples than the baselines. Noticeably, while some of the baselines may be close to the results of AUCOCLoss for certain metrics, none of them were consistently close to AUCOCLoss across all the datasets. For example, when looking at the last two columns for CIFAR100 and Tiny-ImageNet respectively of Table 1, CE+MMCE is worse by 4% than AUCOCLoss on Tiny-ImageNet, but by around 9% in

   Dataset &  &  \\   & \) @ acc.} & \) @ acc.} \\ Loss funct. & AUCOC \(\) & Acc. \(\) & 90\% & 95\% & AUCOC \(\) & Acc. \(\) & 65\% & 75\% \\  CE & 91,43 & 75,71 & 29,03 & 44,61 & 72,56 & 47,39 & 39,88 & 56,29 \\ FL (\(\)=3) & 93,91 & 77,83 & 24,71 & 40,48 & 73,12 & 47,71 & 38,40 & 55,08 \\ AdaFL53 & 93,89 & 77,64 & 25,08 & 40,74 & 73,19 & 47,81 & 38,56 & 55,20 \\ CE+MMCE & 92,42 & 75,35 & 30,01 & 44,71 & 72,47 & 47,11 & 39,94 & 56,70 \\ FL+MMCE & 93,90 & 77,78 & 25,62 & 40,90 & 73,51 & 48,03 & 37,83 & 55,15 \\ CE+S-AvUC & 93,99 & 77,65 & 24,62 & 45,00 & 72,92 & 47,89 & 38,28 & 54,91 \\ FL+ S-AvUC & 93,97 & 77,64 & 25,97 & 40,82 & 74,30 & 48,69 & 35,83 & 53,21 \\ CE+S-ECE & 93,88 & 77,57 & 24,76 & 40,14 & 72,94 & 47,65 & 38,81 & 55,84 \\ FL+S-ECE & 93,41 & 76,69 & 28,13 & 43,29 & 72,61 & 47,40 & 39,94 & 56,71 \\ 
**CE+AUCOCL** & **94,49** & **78,94** & **21,60** & 36,73 & **74,56** & 49,10 & **34,78** & **52,01** \\
**FL+AUCOCL** & 94,18 & 78,31 & 23,50 & **36,68** & 74,30 & **49,19** & 34,85 & 53,15 \\   

Table 1: Test results on the natural datasets CIFAR100 and Tiny-Imagenet. We report AUCOC, accuracy for both, \(_{0}\) at 90% and 95% accuracy for CIFAR100 and \(_{0}\) at 65% and 75% accuracy for Tiny-ImageNet, as the initial accuracy is also lower. In bold the best result for each metric, underlined the second best. AUCOCL improves, to varying degrees, the baselines in all metrics.

   Dataset &  &  \\   & \) @ acc.} & \) @ acc.} \\ Loss funct. & AUCOC \(\) & Acc. \(\) & 90\% & 95\% & AUCOC \(\) & Acc. \(\) & 65\% & 75\% \\  CE & 89,84 & 71,59 & 43,51 & 56,21 & 71,45 & 52,10 & 39,53 & 68,58 \\ FL (\(\)=3) & 90,50 & 72,64 & 40,63 & 53,90 & 68,57 & 52,25 & 44,25 & 58,25 \\ AdaFL53 & 90,11 & 73,10 & 40,78 & 55,86 & 68,85 & 48,58 & 48,67 & 62,00 \\ CE+MMCE & 89,71 & 70,99 & 45,82 & 58,07 & 69,18 & 48,50 & 42,92 & 69,13 \\ FL+MMCE & 89,34 & 71,72 & 46,81 & 59,10 & 67,08 & 50,33 & 47,50 & 78,91 \\ CE+S-AvUC & 89,67 & 71,51 & 43,04 & 57,09 & 68,15 & 51,42 & 42,71 & 62,29 \\ FL+S-AvUC & 89,42 & 71,04 & 45,47 & 58,69 & 66,80 & 52,00 & 45,58 & 70,54 \\ CE+S-ECE & 89,54 & 71,46 & 43,48 & 57,82 & 71,40 & 52,05 & 39,45 & 55,83 \\ FL+S-ECE & 90,22 & 72,62 & 40,95 & 74,76 & 70,49 & 51,33 & 42,42 & 79,91 \\ 
**CE+AUCOCL** & 90,87 & 74,30 & 39,01 & **52,70** & **72,47** & 53,10 & **38,33** & **53,81** \\
**FL+AUCOCL** & **91,35** & **74,80** & **37,30** & 53,90 & 72,31 & **53,58** & 39,42 & 56,12 \\   

Table 2: Test results on the medical datasets DermaMNIST and RetinaMNIST. We report AUCOC, accuracy for both, \(_{0}\) at 90% and 95% accuracy for DermaMNIST and \(_{0}\) at 65% and 75% accuracy for RetinaMNIST, as the initial accuracy is also lower. In bold the best result for each metric, underlined the second best. AUCOCL improves, to varying degrees, the baselines in all metrics.

CIFAR100. Figure 1a shows examples of COC curves. Overall, the plot of AUCOCLoss lies above all the baselines, which is a desirable behavior as it corresponds to better operating points.

Even though the proposed loss was not designed to improve calibration, it provided on par performance compared to the other cost functions particularly designed for network calibration, as reported in Table 3 for DermaMNIST and RetinaMNIST and in the Appendix for the other datasets.

In OOD experiments reported in Table 4, we used the model trained on CIFAR100 and evaluated the OOD detection performance on CIFAR100-C (with Gaussian noise) and SVHN dataset. Results for CIFAR100-C averaged over all the perturbations are reported in the Appendix. We employed state-of-the-art OOD detectors, namely MSP, ODIN, MaxLogit and EBM to determine whether a sample is OOD or in-distribution. The bold results highlight the best results in terms of AUROC. On both OOD datasets, AUCOCLoss always provided the highest AUROC and in almost all the cases also the second best.

Class imbalance experiments on CIFAR100-LT are reported in the Appendix. AUCOCLoss obtains best results for both accuracy and AUCOC, with higher benefits at increasing imbalance.

Crucially, **CE+AUCOCL**, where the proposed loss is used jointly with CE, outperformed every baseline in accuracy, AUCOC and \(_{0}\) @acc. in all the experiments. It further outperformed all the baselines in OOD detection and class imbalance experiments, presented in the Appendix. Even

   Dataset &  &  \\ Loss funct. & MSP & ODIN & MaxLogit & EBM & MSP & ODIN & MaxLogit & EBM \\  CE & 74,37 & 75,22 & 74,51 & 66,18 & 77,42 & 79,42 & 67,71 & 67,71 \\ FL (\(\)=3) & 75,12 & 75,35 & 73,43 & 72,83 & 76,61 & 77,08 & 66,69 & 66,42 \\ AdaFL53 & 74,53 & 74,68 & 71,29 & 70,58 & 80,3 & 81,28 & 66,94 & 66,73 \\ CE+MMCE & 74,67 & 74,53 & 70,67 & 70,17 & 75,79 & 77,39 & 66,34 & 66,23 \\ FL+MMCE & 74,42 & 74,39 & 70,01 & 68,70 & 77,57 & 77,41 & 66,97 & 66,60 \\ CE+S-AVUC & 73,63 & 73,62 & 72,16 & 72,05 & 78,04 & 78,90 & 67,93 & 67,96 \\ FL+S-AVUC & 72,78 & 76,72 & 69,53 & 68,51 & 79,68 & 79,68 & 67,38 & 67,24 \\ CE+S-ECE & 73,29 & 73,18 & 71,59 & 71,36 & 77,02 & 77,69 & 67,98 & 68,01 \\ FL+S-ECE & 74,29 & 73,97 & 71,51 & 70,26 & 79,68 & 81,17 & 67,38 & 67,78 \\ 
**CE+AUCOCL** & 76,03 & **76,90** & **78,02** & **78,30** & **82,03** & **83,50** & 69,51 & **69,69** \\
**FL+AUCOCL** & **76,51** & 76,82 & 75,14 & 74,68 & 80,51 & 79,35 & **69,52** & 69,46 \\   

Table 4: Test AUROC(%) on OOD detection, training on CIFAR100 and testing on CIFAR100-C (Gaussian noise) and SVHN, using MSP, ODIN MaxLogit and EBM. Best and second best results are in bold and underlined.

   Dataset &  &  \\  Loss funct. & ECE\(\) & KS\(\) & Brier\(\) & cwEC\(\) & ECE\(\) & KS\(\) & Brier\(\) & cwECE\(\) \\  CE & **3,07** & 2,15 & 38,11 & 2,22 & 8,42 & 6,15 & 59,75 & 6,01 \\ FL (\(\)=3) & 4,24 & 1,77 & 36,55 & 2,03 & 13,61 & 10,87 & 63,32 & 8,19 \\ AdaFL53 & 3,88 & 1,95 & 36,19 & **1,66** & 13,63 & 11,12 & 65,64 & 8,47 \\ CE+MMCE & 3,59 & 2,73 & 38,96 & 2,20 & 11,58 & 9,75 & 62,18 & 6,87 \\ FL+MMCE & 3,65 & 2,29 & 38,91 & 2,34 & 12,49 & 10,54 & 65,72 & 7,81 \\ CE+S-AvUC & 3,79 & 2,69 & 38,28 & 2,21 & 8,37 & 7,01 & 64,36 & 6,91 \\ FL+S-AvUC & 3,48 & 2,13 & 37,98 & 1,82 & 11,60 & 5,86 & 64,06 & 7,12 \\ CE+S-ECE & 3,23 & 2,73 & 38,39 & 1,92 & 9,44 & 5,88 & 59,84 & 5,37 \\ FL+S-ECE & 4,5 & 2,67 & 36,91 & 2,08 & 12,52 & 10,05 & 61,16 & 6,08 \\ 
**CE+AUCOCL** & 5,70 & **1,56** & 36,12 & 1,78 & **8,15** & **4,47** & **58,69** & **4,46** \\
**FL+AUCOCL** & 5,10 & 2,04 & **35,46** & 2,04 & 10,77 & 8,84 & 60,52 & 5,32 \\   

Table 3: Test results on DermaMNIST and RetinaMNIST for calibration: expected calibration error (ECE), KS score, Brier score and class-wise ECE (cwECE), post TS. In bold and underlined respectively are the best and second best results for each metric. Noticeably, AUCOCL performs comparably to the baselines, even though it does not aim at improving calibration explicitly.

though the metric is not geared towards calibration, **CE+AUCOCL** yielded either the best or the second best calibration performance in the majority of the cases compared to the baselines.

In the Appendix, we explore how the performance of a model trained with AUCOCLoss varies, when changing batch size as it can be crucial for KDE-based methods. Even substantially lower batsch sizes do not have a considerable effect on the performance of the proposed method.

## 6 Conclusion

In this paper we proposed a new cost function for multi-class classification that takes into account the trade-off between a neural network's accuracy and the amount of data that requires manual analysis from a domain expert, by maximizing the area under COC (AUCOC) curve. Experiments on multiple computer vision and medical image datasets suggest that our approach improves the other methods in terms of both accuracy and AUCOC, where the latter was expected by design, provides comparable calibration metrics, even though the loss does not aim to improve calibration explicitly and outperforms the baselines in OOD detection.

While we presented COC and AUCOCLoss for multi-class classification, extensions to other tasks are possible future work as well as investigating different performance metrics to embed in the \(y\)-axis of COC. Moreover, aware of potential problems with KDE at the boundaries, i.e., boundary bias, we explored corrections like reflection method, which did not provide major improvements, but we will further investigate. We believe that this new direction of considering expert load in human-AI system is important and AUCOCLoss will serve as a baseline for future work.

**Limitations:** As described in Section 3.5, AUCOCLoss alone empirically leads to slow convergence, due to the small contribution of \(-log(_{n}z_{n})\) with \(z_{n}\) in its formulation. Therefore, we recommend to use it as a secondary loss, to successfully complement existing cost functions.

## 7 Acknowledgments

This study was financially supported by: 1. The LOOP Zurich - Medical Research Center, Zurich, Switzerland, 2. Personalized Health and Related Technologies (PHRT), project number 222, ETH domain and 3. Clinical Research Priority Program (CRPP) Grant on Artificial Intelligence in Oncological Imaging Network, University of Zurich.