# Interpreting Unsupervised Anomaly Detection

in Security via Rule Extraction

 Ruoyu Li\({}^{@sectionsign{}}\), Qing Li\({}^{}\), Yu Zhang\({}^{@sectionsign{@sectionsign{}}}\), Dan Zhao\({}^{}\), Yong Jiang\({}^{@sectionsign{}}\), Yong Yang\({}^{}\)

\({}^{@sectionsign{}}\)Tsinghua University, China, \({}^{}\)Peng Cheng Laboratory, China

\({}^{@sectionsign{}}\)Tsinghua Shenzhen International Graduate School, China

\({}^{}\)Tencent Security Platform Department, China

{liry19,yu-zhang23}@mails.tsinghua.edu.cn; {liq,zhaod01}@pcl.ac.cn

jiangy@sz.tsinghua.edu.cn; coolcyang@tencent.com

Corresponding author: Qing Li.

###### Abstract

Many security applications require unsupervised anomaly detection, as malicious data are extremely rare and often only unlabeled normal data are available for training (i.e., zero-positive). However, security operators are concerned about the high stakes of trusting black-box models due to their lack of interpretability. In this paper, we propose a post-hoc method to globally explain a black-box unsupervised anomaly detection model via rule extraction. First, we propose the concept of _distribution decomposition rules_ that decompose the complex distribution of normal data into multiple compositional distributions. To find such rules, we design an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the _boundary inference rules_ that estimate the decision boundary of the original model on each compositional distribution. By merging these two types of rules into a rule set, we can present the inferential process of the unsupervised black-box model in a human-understandable way, and build a surrogate rule-based model for online deployment at the same time. We conduct comprehensive experiments on the explanation of four distinct unsupervised anomaly detection models on various real-world datasets. The evaluation shows that our method outperforms existing methods in terms of diverse metrics including fidelity, correctness and robustness.

## 1 Introduction

In recent years, machine learning (ML) and deep learning (DL) have revolutionized many security applications such as network intrusion detection [1; 2; 3] and malware identification [4; 5] that outperform traditional methods in terms of accuracy and generalization. Among these works, unsupervised anomaly detection becomes more promising, which detects malicious activities by the deviation from normality. Compared to supervised methods, this type of method is more desirable in security domains as 1) it hardly requires labeled attack/malicious data during the training (i.e., zero-positive learning), which are typically much more sparse and difficult to obtain in contrast with benign data; 2) it does not fit any known threats, enabling better detection on unforeseen anomalies.

Due to the black-box nature of these models, ML/DL models are usually not directly interpretable and understandable. Many local explanation methods [6; 7; 8; 9; 10] have attempted to interpret the models by presenting feature importance of the decision for a single point. However, globally explaining black-box models, especially using rule extraction to characterize the whole decision boundaries, is particularly desirable in security systems since it can provide the following benefits:

**Trust over High Stakes.** To minimize the chance of errors and potential losses, security operators tend to trust human-understandable rules rather than unintuitive outputs such as labels and numeric values from complex and incomprehensible black-box models.

**Online Defense.** Interpreting black-box models into rules with high fidelity enables easy integration with the majority of defense tools that use rule-based expressions (e.g., iptables , Snort ), thus allowing for deployment of online defense with extraordinary efficiency (e.g., Tbps throughput ).

Existing global ML/DL explanation approaches are mostly proposed for supervised models. Little research has been done on explaining unsupervised anomaly detection, which faces several challenges:

**Unlabeled One-class Data (CH1).** Supervised explanation methods [14; 15; 16] need labeled data of both positive and negative classes to determine the decision boundaries of black-box models. This requirement goes against unsupervised anomaly detection's advantage of not requiring attack data.

**Lack of Surrogate Models (CH2).** Global methods typically use an inherently explainable model to mimic the black-box model, such as decision trees [15; 16] and linear models , which are all supervised models. However, there lacks a proper surrogate model that is unsupervised and can satisfy the properties of being self-explained and well-performed for high-stake security applications.

**Accuracy Loss (CH3).** A common problem with global methods is that the surrogate model suffers from the loss of accuracy since it simplifies the original model . In this case, though these methods can provide model explanation, they cannot meet the need of online deployment which requires high detection accuracy in security applications.

We observe that an important reason why simple surrogate models are ineffective is that they cannot learn well about the complex data distribution in high-dimensional space. Specifically, even one-class data may be multimodal, i.e., the overall distribution consists of multiple compositional distributions. For example, the normal activities of a host may consist of many services (e.g., web, database) and are for various purposes, and they could present considerable differences in feature space.

In light of this, this paper proposes an accurate and efficient divide-and-conquer method to globally interpret unsupervised anomaly detection. First, we propose the concept of _distribution decomposition rules_ that cut the feature space into multiple subspaces, each of which encloses a compositional distribution. To obtain such rules, we design a new tree model called Interior Clustering Tree that extends the CART decision tree in terms of splitting criteria and can fully work in an unsupervised manner. Second, we propose the Compositional Boundary Exploration algorithm to obtain the _boundary inference rules_ that estimate the decision boundary of the original model on each compositional distribution. To accurately and efficiently find such rules, this algorithm starts from hypercube-shaped rules and uses an approximation for gradient ascent of model prediction to find the optimal direction of iterations. By merging the distribution decomposition rules and the boundary inference rules into a rule set, we can present the inferential process of the black-box model in a human-understandable way, and build a surrogate rule-based model for online defense at the same time.

During the experiment, we use four different unsupervised anomaly detection models well trained on three benchmark datasets, and evaluate our method with five distinct explanation methods as baselines. The experiment shows that our method can extract interpretable and accurate rules from black-box models. The extracted rules outperform prior work in terms of diverse metrics including fidelity, robustness, true positive rate and true negative rate, meeting the demand of improving human trust in black-box models and maintaining high detection accuracy for online deployment. Our code is available at https://github.com/Ruoyu-Li/UAD-Rule-Extraction.

## 2 Related Work

To combat persistent emergence of new attacks in cyberspace, recent security applications [19; 2; 3; 1; 20; 21; 1] make heavy use of unsupervised models to detect unknown anomalies, such as one-class classifiers [22; 23; 24], Isolation Forests [25; 26], autoencoders and variational autoencoders . Despite many unsupervised model-based approaches have achieved good detection rates, security operators are still concerned about the semantic gap between black-box model prediction and human understanding, considering the risks of the great cost incurred by bad decisions . To resolve such concerns, explainable AI (XAI) has been applied to anomaly detection [28; 29; 30]. For example, Kauffmann et al. propose a decomposition method to explain anomalies of one-class SVMs . Philipp et al. presentan explainable deep one-class classification method called Fully Convolutional Data Description . However, these methods are specific to a limited range of models and not versatile enough to accommodate the vastly heterogeneous models of unsupervised anomaly detection.

Some prior work also incorporates popular model-agnostic explanation methods, such as LIME , SHAP  and their variations , and applies them to explain unsupervised models [31; 32; 33] and security applications [9; 34; 35; 10]. These methods typically use sparse linear models to interpret predictions by estimating feature importance. Guo et al. propose a method named LEMNA that uses a fused lasso to explain malware classification . Simple uses Integrated Gradients  to attribute anomalies of IoT device failure . Nonetheless, these methods can only interpret one data point at a time (i.e., local explanation) but not reveal the complete decision-making process of a model.

To fully understand how black-box models work and safely deploy them, the most appropriate method is _model-agnostic global post-hoc_ explanation. It aims to match the predictions of any well-trained models with an inherently interpretable explainer, such as decision trees [15; 16], symbolic rules , sparse linear models  and decision lists . In , the authors construct global explanations of complex black-box models in the form of a decision tree approximating the original model. Jacobs et al. propose a framework that takes an existing ML model and training dataset and generates tree models to interpret security-related decisions . However, most of these methods are only suitable for interpreting supervised models that have labeled data of all classes, which are often unavailable. Though work like  can extract rules from unsupervised anomaly detection models, it still assumes that enough outliers exist in the training dataset judged by the black-box model so as to determine its decision boundary. This assumption may not hold in practice if a model has great generalization and can achieve a low false positive rate on normal data (e.g., ).

Some recent studies aggregate several local explanation models into near-global explanation [39; 40; 41]. However, this type of method is inherently computationally challenging when data volumes are large and has to make trade-offs between fidelity and coverage. While techniques like knowledge distillation can also realize model transformation to reduce complexity and promote interpretability [42; 43], the fundamental purpose of these efforts is to compress models while ensuring accuracy rather than explaining the original models with high fidelity.

## 3 Overview

### Problem Definition

Let \(^{d}\) be the variable space of \(d\)-dimensional features; \(\) and \(x_{i}\) denote a data sample and the \(i\)-th dimension of the data sample. We give the following definitions for the rest of the paper:

**Definition 1** (Unsupervised Anomaly Detection).: _Given unlabeled negative data (i.e., normal data) \(\) sampled from a stationary distribution \(\) for training, an unsupervised model estimates the probability density function \(f()_{}()\), and detects an anomaly via a low probability \(f()<\), where \(>0\) is a threshold determined by the model itself or by humans._

It is noted that the threshold \(\) is a non-zero value, meaning that the model inevitably generates false positives, which is a common setting in most of the works [1; 2; 3] even though the false positive rate can be very low. Besides, the normal data may occasionally be contaminated or handled with errors. We consider the anomaly detection tolerant of noisy data, but their proportion in training dataset is small and we do not have any ground truth labels of the training data.

**Definition 2** (Global Explanation by Rule Extraction).: _Given a trained model \(f\) with its anomaly threshold \(\) and the training set \(\), we obtain an in-distribution rule set \(=\{C_{1},C_{2},...\}\) that explains how the model \(f\) profiles the distribution of normal data. A rule \(C=...(x_{i} v_{i})...(x_{j} v_{j})\) is a conjunction of several axis-aligned constraints on a subset of the feature space, where \(_{i}\) is the bound for the \(i\)-th dimension and \(\{,>\}\)._

Let \( C\) indicate that a data sample satisfies a rule. From \(\), we can build a surrogate model \(h_{}()\), whose inference is to regard a data sample that cannot match any of the extracted rules as anomalous:

\[h_{}()=( C_{1})( C_{2}) ...,\;C_{i}.\] (1)

**Our Goal.** We expect the extracted rules to have a high fidelity to the original model, that is, a similar coverage of normal data (i.e., true negative rate), and a similar detection rate of anomalies (i.e., true positive rate). To this end, we formulate our objective as follow:

\[*{arg\,min}_{}_{}( ,f,)+_{}(,f, ).\] (2)

### Methodology Overview

To minimize the first item in Equation (2), suppose the training data \(\) can well represent the distribution \(\), a straightforward approach is to find the bound of \(\) as rules, such as using a hypercube to enclose the data samples which can easily achieve the minimization of the partial loss \(_{}(,f,)=0\). However, as \(\) is not a prior distribution and we do not have labeled abnormal samples, the second item \(_{}(,f,)\) is neither deterministic nor estimable unless we create sufficient random samples and query \(f\), which is challenging given the high-dimensional space of \(\).

As prior studies [21; 34] suggest, normal data are typically multimodal, i.e., the overall distribution is formed by multiple compositional distributions. For example, a server supports multiple services such as web, email and database. The representations of these services can be disparate and located in different regions in feature space with little transition between the regions, making it infeasible to find a uniform rule set to accurately estimate the original model. An example is illustrated in Figure 0(a).

Based on this intuition, we propose a divide-and-conquer approach. First, we propose an _Interior Clustering Tree_ model (Section 4) to find the _distribution decomposition rules_, which cut the feature space into subspaces so that each subspace contains data belonging to the same compositional distribution, as shown in Figure 0(b). Then, we design a _Compositional Boundary Exploration_ algorithm (Section 5) to explore the decision boundary on each compositional distribution, as depicted in Figure 0(c). Particularly, the algorithm starts from the minimal hypercube that encloses all data of the distribution, and finds the boundary by recursively extending the boundary following the optimal direction guided by a gradient approximation. Upon obtaining the decision boundary of a distribution, the corresponding _boundary inference rule_ can be extracted. Last, the rule set that globally approximates the original model can be obtained by merging the distribution decomposition rule and the boundary inference rule of each compositional distribution, as illustrated in Figure 0(d). We formally define the distribution decomposition rule and the boundary inference rule as follows.

**Definition 3** (Distribution Decomposition Rule).: _Denoted by \(C_{k}^{I}\) that decomposes the overall distribution of normal data \(\) into \(K\) compositional distributions, i.e., \(_{}()=_{k=1}^{K}_{k} _{_{k}}(| C_{k}^{I})\) where \(_{k}\) denotes the weight of each compositional distribution, so that a data sample \(_{k}\) has significantly small probability of belonging to other distributions._

**Definition 4** (Boundary Inference Rule).: _Denoted by \(C_{k}^{E}\) that estimates the decision boundary of the original model for each distribution \(_{k}\), i.e., \(*{arg\,min}_{C_{k}^{E}}_{_{ k}}(C_{k}^{E},f,)+_{_{k}}(C_{k}^{E},f, )\)._

With the definition of these two types of rules, we translate the objective in Equation (2) to the following objective as our intuition indicates. We give a proof of this proposition in the appendix.

Figure 1: A high-level illustration of our method. The small circles are unlabeled normal data. The dashed curves are the decision boundary of the black-box model. The vertical/horizontal lines in (b) and (c) are the distribution decomposition rules.

**Proposition 1**.: _The original objective can be estimated by finding the union of the conjunction of distribution decomposition rules and boundary inference rules for each compositional distribution:_

\[_{k=1}^{K}*{arg\,min}_{C_{k}}_{ _{k}}(C_{k},f,)+_{_{k}} (C_{k},f,),C_{k}=C_{k}^{I} C_{k}^{E}.\] (3)

## 4 Interior Clustering Tree

To obtain distribution decomposition rules, we first need to decide how to define a compositional distribution \(_{k}\). We notice that, though we do not have labeled data, we can use the output of the original model that estimates the overall distribution \(\) as a criterion for decomposition.

**Proposition 2**.: _If two data samples \(^{(i)}\) and \(^{(j)}\) belong to the same distribution \(_{k}\), the difference of their probabilities belonging to \(\) will be less than \(\), where \(\) is a small constant._

Proof.: Recall the definition of distribution decomposition rules and compositional distributions. Since the two data samples belong to the same distribution \(_{k}\), the probability of belonging to other distributions \(_{_{l}}()\) is near to zero for \(l k\). Hence, the probability \(_{}()\), which is the weighted sum of the probability of belonging to all the compositional distributions, is approximately equal to \(_{_{k}}()\) for both the data samples.

Based on this, we propose a tree-based model dubbed Interior Clustering Tree (IC-Tree), which extends the CART decision tree . The main difference between IC-Tree and CART is that, rather than splitting data based on ground truth labels, IC-Tree uses the probability output by the original model as splitting criteria, enabling it to work in a completely unsupervised manner.

**Node Splitting.** Given the data \(\) at a tree node, we first obtain the output of the anomaly detection model \(f()\) for \(\). Similar to decision trees, the node of an IC-Tree finds a splitting point \(s=(i,b_{i})\) that maximizes the gain:

\[s=*{arg\,max}_{s}I()-_{l}|}{||}I( _{l})-_{r}|}{||}I(_{r}),\] (4)

where \(b_{i}\) is the splitting value for the \(i\)-th dimension, \(_{l}\) and \(_{r}\) are the data split to the left and right child nodes, \(||\) denotes the number of data samples, and \(I\) is a criterion function such as Gini index \(I=2p(1-p)\) for binary classification with the probability of \(p\). Specifically, we let \(p\) be the average output of the anomaly detection model, which can be interpreted as the expectation of the probability that the data belong to the same distribution:

\[p=_{}[_{}()]=|}_{}f().\] (5)

An IC-Tree continues to split nodes until it satisfies one of the following conditions: i) the number of data samples at the node \(||=1\); ii) for any two of the data samples at the node \(^{(i)},^{(j)}\), \(|f(^{(i)})-f(^{(j)})|<\); iii) it reaches a maximum depth \(\), which is a hyperparameter.

**Distribution Decomposition Rule Extraction.** A trained IC-Tree that has \(K\) leaf nodes (\(K 2^{}\)) represents \(K\) distributions separated from the overall distribution \(\). Suppose the \(k\)-th leaf node has a depth of \(^{}\). A distribution decomposition rule that describes the \(k\)-th compositional distribution can be extracted by the conjunction of the splitting constraints from the root to the leaf node:

\[C_{k}^{I}=(x_{i}_{1}b_{i}|s_{1}=(i,b_{i}))...(x_{j}_{ ^{}}b_{j}|s_{^{}}=(j,b_{j})),\] (6)

where \(\) is "\(\)" if the decision path goes left or "\(>\)" if the decision path goes right.

## 5 Compositional Boundary Exploration

To accurately find the decision boundary of a detection model within each compositional distribution, we propose the Compositional Boundary Exploration (CBE) algorithm (described in Algorithm 1). The CBE algorithm uses the minimal hypercube that encloses the normal data of each compositional distribution as a starting point. Further, we refer to adversarial attacks  and propose a method toapproximate the optimal direction to explore the decision boundary, which makes the algorithm more efficient and accurate to estimate the decision boundary.

**Starting from Hypercube (line 1).** Let \(_{k}\) denote the training data falling into the \(k\)-th leaf node of an IC-Tree that represents a compositional distribution. Recall the definition of boundary inference rules that target \(_{_{k}}(C_{k}^{E},f,)+ _{_{k}}(C_{k}^{E},f,)\). We use the minimal hypercube \(H_{k}\) as a starting point of boundary inference rules to bound every dimension of the data samples in \(_{k}\) judged by the original model as normal, which obviously achieves \(_{_{k}}(H_{k},f,)=0\). The minimal hypercube is enclosed by \(2 d\) axis-aligned hyperplanes, which can be characterized by the following rule:

\[H_{k}=(_{1}^{-} x_{1}_{1}^{+})...( _{d}^{-} x_{d}_{d}^{+}),\] (7)

where \(_{i}^{-}=(x_{i}|f()>,_{k})\) and \(_{i}^{+}=(x_{i}|f()>,_{k})\).

**Explorer Sampling (line 4\(\)6).** The CBE algorithm explores the decision boundary of the original model by estimating the bound of one feature dimension at a time. For \(i\)-th dimension, we uniformly sample \(N_{e}\) data points on each hyperplane of the hypercube, i.e., \(^{(1)},...,^{(N_{e})} H_{k}(x_{i}=_{i}), _{i}\{_{i}^{-},_{i}^{+}\}\), which are called the _initial exploreers_ for this hyperplane. For an initial explorer \(\), we further sample \(N_{s}\)_auxiliary exploreers_ near it from a truncated multivariant Gaussian distribution denoted by \((,,i)\). Particularly, the center of sampling is the explorer \(\) and the radius of sampling is constrained by the covariance matrix \(=(|_{1}^{+}-_{1}^{-}|,...,| _{d}^{+}-_{d}^{-}|)\), where \(\) is a hyperparameter, and the sampling on \(i\)-th dimension is half-truncated to only keep the distribution outside the hypercube as we desire to extend the boundary. With \(N_{e} N_{s}\) auxiliary explorers in total, we query the original model and use Beam Search to select \(N_{e}\) samples with the minimal probability of being normal as the candidate explorers for the next iteration.

**Gradient Approximation (line 7\(\)9).** Though we have obtained \(N_{e}\) candidate explorers in the previous step, using them directly for the next iteration does not guarantee the optimal direction of movement towards the decision boundary. To find the optimal direction, we utilize the Fast Gradient Sign Method  that employs gradient ascent to find the direction of feature perturbation. However, we do not know the loss function of the original model in black-box scenarios. To deal with it, given a selected auxiliary explorer \(}\) that is sampled around an initial explorer \(\) on the \(i\)-th dimension hyperplane, we approximate the \(i\)-th dimension of the model gradient (i.e., the partial derivative) by the slope of a linear model across the two data points, and use the midpoint with its \(i\)-th dimension minus the approximation as the new explorer for the next iteration:

\[e_{i,next}=+_{i}}{2}- sign(_{i}),_{i}=)}{ x_{i}})-f (})}{e_{i}-_{i}},\] (8)

\(sign()\) is the sign function, and \(\) is a hyperparameter to control the stride of one iteration. The iteration stops when i) an auxiliary explorer \(}_{ext}\) that satisfies \(f(}_{ext})<\) is found, or ii) it reaches the maximum number of iterations.

**Rule Acquisition (line 12).** If the iteration stops due to the first condition, we produce a boundary constraint for each dimension using the coordinate of \(}_{ext}\) that extends the boundary of the hypercube, i.e., \(c_{i}=(x_{i}_{ext,i})\), where \(\) is "\(\)" if \(_{ext,i}\) is greater than \(_{i}^{+}\), or "\(>\)" if \(_{ext,i}\) is less than \(_{i}^{-}\). If the iteration stops due to the second condition, it means the algorithm encounters difficulties in moving towards the decision boundary by perturbing this feature dimension. We calculate the difference between the model prediction of the last auxiliary explorer and that of the initial explorers on the hyperplane. If the difference is smaller than a threshold \(\), we decide that this feature dimension is a _contour line_, i.e., it has no significant correlation with the model prediction. In this case, we do not produce any constraints for this dimension. If the difference is greater than the threshold, we produce constraints in the same way as those produced under the first condition. The final boundary inference rule is the disjunction of the hypercube and the constraints on each dimension.

## 6 Evaluation

### Experimental Setup

**Black-box Models and Datasets.** We use four different types of unsupervised anomaly detection models widely used in security applications as the original black-box models, including autoencoder (AE, used by ), variational autoencoder (VAE, used by ), one-class SVM (OCSVM, used by ) and Isolation Forest (iForest, used by ). We employ three benchmark datasets for network intrusion detection in the experiment, including CIC-IDS2017, CSE-CIC-IDS2018  and TON-IoT . The representation of these datasets is tabular data, where each row is a network flow record and each column is a statistical attribute, such as the mean of packet sizes and the inter-arrival time. The datasets are randomly split by the ratio of 6:2:2 for training, validation and testing. We use only normal data to train the anomaly detection models and calibrate their hyperparameters. The description of the datasets and the AUC score of the models on the datasets are shown in Table 1.

**Baselines.** We employ five prior explanation methods as baselines: 1) We use  that extracts rules from unsupervised anomaly detection (UAD); 2) For other global methods, we use the estimated greedy decision tree (EGDT) proposed by , and Trustee  that specifically explains security applications; 3) We also consider one method LIME  that can use a Submodular Pick algorithm to aggregate local explanations into global explanations, and a knowledge distillation (KD) method  that globally converts a black-box model to a self-explained decision tree. These methods, like ours, can only access normal data to extract explanations. More details about baselines are in the appendix.

**Metrics.** We refer to the metrics in  to evaluate the rule extraction. Due to limited space, we demonstrate the following four metrics in this section and present other results in the appendix: 1) Fidelity (FD), i.e., the ratio of input samples on which the predictions of original models and surrogate models agree over the total samples, which indicates the extent to which humans can trust the explanations; 2) Robustness (RB), i.e, the persistence of the surrogate model to withstand small perturbations of the input that do not change the prediction of the original model; 3) True positive rate (TPR) and true negative rate (TNR), suggesting whether the detection capability meets the need of online defense and whether the extracted rules generate noticeable false alarms that cause "alert fatigue"  in highly unbalanced scenarios of most security applications, respectively.

### Quality of Rule Extraction

We extract rules from the four unsupervised anomaly detection models using the five baseline methods and our method, and test the performance of the extracted rules. The results on the three datasets are in Table 2. We find that our method achieves the highest fidelity on all the detection models and datasets, and half of the scores are even over 0.99. It shows our method can precisely match the predictions of the black box models, which ensures the correctness of its global interpretation.

   No. & Dataset & \#Classes & \#Features & \#Normal & \#Attack & AE & VAE & OCSVM & iForest \\ 
1 & CIC-IDS2017 & 6 attacks + 1 normal & 80 & 687,565 & 288,404 & 0.9921 & 0.9901 & 0.9967 & 0.9879 \\
2 & CSE-CIC-IDS2018 & 1d attacks + 1 normal & 80 & 693,004 & 202,556 & 0.9906 & 0.9767 & 0.9901 & 0.9734 \\
3 & TON-IoT & 9 attacks + 1 normal & 30 & 309,086 & 893,006 & 0.9998 & 0.9993 & 0.9987 \\   

Table 1: Summary of datasets for network intrusion detection and AUC of trained models.

Moreover, our method achieves the highest TPR on all the detection models and datasets; specifically, the TPR is equal to 1.00 for all the detection models on TON-IoT dataset. This result suggests that our rules can accurately detect various anomalous data, making it possible to realize online deployment and defense using these rules. Our method also reaches a high level of robustness (minimum 0.9890, maximum 1.00) and true negative rate (minimum 0.9715, maximum 1.00). Therefore, it is concluded that our method can obtain rules of high quality from different black-box unsupervised anomaly detection models using only unlabeled one-class data.

Considering that obtaining a "clean" training set requires huge manual effort in reality , we also assess the efficacy of our method under varying percentages of "noisy" data. We evaluate the fidelity of extracted rules using two approaches for the injection of noisy data: 1) random noise; 2) mislabeled data from other classes, i.e., attack data. The results are shown in Table 3. We find that the impact of the noisy data proportion is not significant: 36 of 40 fidelity scores in the table preserve over 0.95, and the variation of fidelity scores is not obvious with the increase of noisy data for most of the models. This shows that our rule extraction method can retain similar performance to the black-box model that it extracts from. Nonetheless, the results of iForest also reveal that a sufficiently large proportion of noisy data may cause a certain negative impact on the rule extraction for certain models.

### Understanding Model Decisions

To demonstrate that the rules obtained by our method are in line with human understanding, we use the OCSVM as an example of black-box models to exhibit several explanations. We extract rules from the well-train model and use the rules to predict three typical types of attack data, including Distributed Denial-of-Service (DDoS) attacks, scanning attacks, SQL injection, and backdoor attacks.

    \\  Method &  &  &  &  \\   & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR \\  UAD & 0.1325 & 0.4991 & 0.0003 & 0.9792 & 0.1438 & 0.4839 & 0.022 & **0.9988** & 0.0725 & 0.5000 & 0.00 & 1.00 & 1.0262 & 0.5000 & 0.0 & **1.00** \\ EDOT & 0.533 & 1.20 & 0.4354 & 0.9947 & 0.1437 & **1.00** & 0.022 & 0.9961 & 0.9189 & 0.9994 & 0.9306 & 0.838 & 0.9729 & 0.9996 & 0.9417 & 0.9189 \\ Trustee & 0.4871 & 0.6412 & 0.3844 & 0.9981 & 0.1552 & 0.9857 & 0.0152 & **0.9988** & 0.539 & 0.6108 & **1.00** & **1.00** & 0.4543 & 0.5801 & 0.9795 & 0.4486 \\ LIME & 0.6918 & 0.9999 & 0.7889 & 0.0014 & 0.8232 & **1.00** & 0.9329 & 0.001 & 0.068 & 0.9999 & 0.0777 & 0.0241 & 0.8910 & **0.9998** & 0.8246 & 0.9913 \\ KD & 0.5776 & 0.9989 & 0.4792 & **0.9998** & 0.2010 & 0.9817 & 0.1016 & 0.9993 & 0.3620 & **1.00** & 0.3102 & 0.9995 & 0.1262 & 0.7016 & 0.00 & **1.00** \\ Ours & **0.9835** & **1.00** & **0.9457** & 0.9915 & **0.9620** & 0.9993 & **0.9510** & 0.9444 & **0.9275** & **1.00** & **1.00** & **1.00** & 0.9949 & **0.9968** & 0.9843 \\   \\  Method &  &  &  &  \\   & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR \\  UAD & 0.3796 & 0.3077 & 0.0004 & 0.7418 & 0.2697 & 0.2930 & 0.1490 & 0.4857 & 0.6051 & 0.3069 & 0.3004 & 0.9876 & 0.6811 & 0.4035 & 0.3539 & 0.9724 \\ EDOT & 0.5821 & **1.00** & 0.14328 & 0.9801 & 0.2197 & 0.9989 & 0.2308 & 0.9540 & 0.5106 & **1.00** & **1.00** & 0.9546 & 0.9546 & 0.7813 & 0.9888 & 0.8971 \\ Trustee & 0.5157 & 0.0066 & 0.1901 & 0.9875 & 0.3642 & 0.9752 & 0.0124 & 0.9636 & 0.3166 & 0.5955 & **1.00** & **1.00** & 0.4241 & 0.4700 & 0.9641 & 0.5162 \\ LIME & 0.5838 & 0.9999 & 0.7681 & 0.0255 & 0.6814 & **1.00** & **0.9042** & 0.0123 & 0.0560 & **1.00** & 0.9999 & 0.0186 & 0.8903 & **1.00** & 0.9884 & 0.8745 \\ KD & 0.5074 & 0.9999 & 0.3562 & **0.9979** & 0.4234 & 0.9989 & 0.1086 & **0.9925** & 0.3180 & 0.9967 & 0.4308 & 0.1510 & 0.3596 & 0.6834 & 0.0000 & **1.00** \\ Ours & **0.9954** & 0.9997 & **0.9998** & 0.9774 & **0.8962** & 0.9985 & **0.9997** & 0.8268 & **0.9929** & 0.9997 & 0.9983 & 0.9753 & **0.9947** & 0.9291 & **0.9988** & 0.9583 \\   \\  Method &  &  &  &  \\   & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR & FD & RB & TPR & TNR \\  UAD & 0.1499 & 0.015 & 0.0258 & 0.908 & 0.2157 & 0.4010 & 0.1863 & 0.7787 & 0.0489 & 0.5000 & 0.00 & **1.00** & 0.0674 & 0.5000 & 0.00 & **1.00** \\ EDOT & 0.9750 & **1.00** & 0.9739 & 0.9943 & 0.7660 & **1.00** & 0.7538 & 0.9948 & 0.8139 & 0.9997 & 0.8051 & 0.9759 & 0.6345 & 0.9226 & 0.6247 & 0.9475 \\ Trustee & 0.4774 & 0.5722 & 0.4502 & 0.9971 & 0.3807 & 0.6689 & 0.3484 & 0.9975 & 0.7942 & 0.8430 & **1.00** & **1.00** & 0.7476 & 0.8145 & 0.9824 & 0.1943 \\ LIME & 0.6971 & 0.9999 & 0.7399 & 0.0202 & 0.8289 & **1.00** & 0.9379 & 0.015 & 0.0687 & 0.9999 & 0.0787 & 0.0231 & 0.8963 & **0.9998** & 0.8296 & 0.9918 \\ KD & 0.0821 & **1.00** & 0.0341 & **0.9987** & 0.0951 & 0.997 & 0.0099 & **0.9980** & 0.0494 & **1.00** & 0.0005 & 0.9994 & 0.0674 & 0.9955 & 0.00 & **1.00** \\ Ours & **0.9996** & **1.00** & **1.00** & 0.9845 & **0.9995** & **Table 4 shows some features of the rules extracted from normal data that cannot be matched by the attack data, and exhibits how humans can interpret the model decisions2. For example, the data of DDoS attacks cannot match the rules of three feature dimensions, including mean of packet sizes, mean of packet inter-arrival time, and duration of a connection. It can be observed that the feature values of attacks are markedly lower than the bound of the rules. Such results are easy to interpret. Because the purpose of DDoS attacks is to overwhelm the resources of a victim, an attacker will realize asymmetric resource consumption between the victim and himself (i.e., using small packets), send packets at an extremely high rate (i.e., low inter-arrival time), and establish as many useless connections as possible (i.e., short duration of connections). These explanations are in line with how humans recognize the attack data. Hence, we can draw a conclusion that our method is able to provide precise insights into black-box anomaly detection models in a human-understandable way.

### Ablation Study

To evaluate the contribution of each component in our method, including the IC-Tree and the CBE algorithm, we conduct an ablation experiment by 1) replacing the IC-Tree with a clustering algorithm K-Means, 2) using only the CBE algorithm, and 3) replacing the CBE algorithm with directly using hypercubes as rules. In Table 5, we find that our method (IC-Tree + CBE) outperforms others in terms of fidelity on both datasets. Though using the K-Means can reach similar results, it cannot be expressed by axis-aligned rules with high interpretability and deployability as the IC-Tree can achieve. In summary, both components are helpful for the quality of rule extraction.

### Computational Cost and Complexity

We also evaluate the computational cost of our method with respect to training and prediction. Since CIC-IDS2017 dataset has 80 features in total, we train the model using the first 20, 40, 60, and 80 features of 4000 samples to investigate the influence of feature sizes. The results are shown in Table 6, which demonstrate the average training and prediction time of our method. It can be seen that the training time is around 1 minute, which is acceptable and practical for large-scale training. Besides, the training time increases basically linearly with the increase of feature sizes. This is because our method adopts a feature-by-feature strategy to explore the decision boundary of the model. For prediction time, our method is highly efficient, which only costs microsecond-level overhead for one inference. It shows that as a rule-based approach, our method can achieve real-time execution for online use. Note that the runtime is measured purely based on Python. In practice, the prediction time of our method can be even less with more efficient code implementation.

    & Rules of Normality & Attack Value & Feature Meaning & Human Understanding \\   & p\_\_mean \(>\) 101.68 & 57.33 & Mean of IP packet sizes & DDoS attacks use packets of small sizes to achieve \\  & iat\_mean \(>\) 0063 & 0.00803 & Mean of packet inter-arrival time & asymmetric resource consumption on the victim side, \\  & dur \(>\) 1261 & 0.00126 & Duration of a connection & and send packets at a high rate to flood the victim. \\   & count \(>\) 120 & 1 & IP packet count per connection & Scanning attacks send a constant probe packet to a port, \\  & p\_var \(>\) 2355.20 & 0.0 & Variance of IP packet sizes & and the victim will not reply if the port is closed. \\  SQL & p\_by\_normal \(<\) 415.58 & 435.80 & Mean of backward IP packet sizes & Unauthorized access to additional data from websites, \\ Injection & dur \(>\) 164 & 0.37 & Duration of a connection & usually establish about connections for one attack. \\   & p\_\_max \(>\) 275.28 & 48.0 & Maximum of IP packet sizes & It persists in compromised hosts and sends stealthy \\  & p\_\_min \(>\) 49.41 & 40.0 & Minimum of IP packet sizes & keep-alve packets with no payload (thus very small). \\   

Table 4: Examples of explanation on four types of attacks.

    &  &  \\  & FD & Comparison & FD & Comparison \\  IC-Tree + CBE (our method) & 0.9856 & - & 0.9840 & - \\ K-Means (k=10) + CBE & 0.9731 & 1.268\%\(\) & 0.9802 & 0.386\%\(\) \\ K-Means (k=5) + CBE & 0.9733 & 1.228\%\(\) & 0.9793 & 0.386\%\(\) \\ Only CBE & 0.9735 & 1.228\%\(\) & 0.9784 & 0.478\%\(\) \\ IC-Tree + Hypercube & 0.9652 & 2.069\%\(\) & 0.0647 & 93.43\%\(\) \\   

Table 5: Ablation study on the components of our method.

We also theoretically analyze the time complexity of our algorithms. For training, the complexity of the IC-Tree is identical to a CART: \(O(d n n)\), where \(d\) is the feature size and \(n\) is the sample number; the complexity of the CBE algorithm is \(O(K d N_{e} N_{s})\), where \(K\) is the number of leaf nodes of the IC-Tree, and \(N_{e}\) and \(N_{s}\) are the number of initial explorers and auxiliary explorers. Therefore, the training time is theoretically linear to the feature size, which is in line with the empirical results. For execution, the time complexity is \(O(|C| d)\), where \(|C|\) is the number of extracted rules.

### Hyperparameter

We perform a sensitivity analysis of several hyperparameters on their influence on the rule extraction. We present four major hyperparameters in Figure 2, including the maximum depth \(\) of an IC-Tree, \(N_{e}\) number of explorers, the coefficient \(\) of sampling, and the factor \(\) that controls the stride of an iteration. Due to limited space, the analysis of other hyperparameters is placed in the appendix.

**Maximum tree depth.** A deeper IC-Tree has more leaf nodes, and can accordingly decompose more distributions that ease the difficulty of rule extraction. Meanwhile, excessively fine-grained splitting might cause overfitting. We find that \(=15\) achieves the best performance.

**Number of Explorers.** It is essentially the number of selected nodes per iteration in Beam Search, which considers multiple local optima to improve greedy algorithms. But selecting too many nodes may also include more redundancy. Figure 1(b) shows that a value between 6 and 8 is recommended.

**Coefficient of sampling.** Figure 1(c) shows that a higher value of the hyperparameter achieves better results. A large coefficient decides a large radius of sampling from a multivariant Gaussian distribution, which helps the CBE algorithm quickly find the decision boundary of the original model.

**Factor of iteration stride.** In Figure 1(d), we find that a larger factor \(\) can obtain rules of higher quality. As it decides the stride of finding the explorers for the next iteration, a higher value of the hyperparameter might help the convergence of the iteration process.

## 7 Conclusion and Future Work

This paper proposes a novel method to globally interpret black-box unsupervised anomaly detection, in which we introduce two types of rules and corresponding algorithms to extract these rules. The evaluation shows that our method outperforms prior work in terms of various metrics, boosting user confidence in complex models and facilitating their integration into high-stake security applications.

There are numerous meaningful directions for future work. First, new algorithms can be developed based on model interpretation to troubleshoot illogical inference processes in black-box models (e.g., false positives) and realize the automation of precisely fixing errors. Moreover, researchers can investigate the integration of rule extraction with high-performance rule-based defense systems, such as P4 , to implement a more efficient security system. In addition, since the proposed method can be generic, the transfer to other areas of industry that also demand interpretable anomaly detection, such as health, manufacturing and criminal investigation, can be further explored.

   Feature Size & Training Time (ms) & Prediction Time (ms) \\ 
20 & \(5.40 5.50 10^{-4}\) & \(5.48 10^{-3} 2.51 10^{-9}\) \\
40 & \(15.5 6.80 10^{-2}\) & \(5.52 10^{-3} 2.34 10^{-9}\) \\
60 & \(14.7 8.75 10^{-5}\) & \(6.99 10^{-3} 3.56 10^{-8}\) \\
80 & \(30.7 3.08 10^{-1}\) & \(6.91 10^{-3} 9.00 10^{-8}\) \\   

Table 6: Average training and prediction time per sample for different feature sizes.

Figure 2: Sensitivity experiments of hyperparameters.