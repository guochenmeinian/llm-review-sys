# Electron-Derived Molecular Representation Learning

for Real-World Molecular Physics

 Gyoung S. Na\({}^{1,}\), Chanyoung Park\({}^{2,}\)

\({}^{1}\)Korea Research Institute of Chemical Technology

\({}^{2}\)Korea Advanced Institute of Science and Technology

ngso@krict.re.kr, cy.park@kaist.ac.kr

These authors contributed equally to this work

###### Abstract

Various representation learning methods for molecular structures have been devised to accelerate data-driven drug and materials discovery. However, the representation capabilities of existing methods are essentially limited to _atom_-level information, which is not sufficient to describe real-world molecular physics. Although _electron_-level information can provide fundamental knowledge about chemical compounds beyond the atom-level information, obtaining the electron-level information in real-world molecules is computationally impractical and sometimes infeasible. We propose a new method for learning electron-derived molecular representations without additional computation costs by transferring pre-calculated electron-level information about small molecules to large molecules of our interest. The proposed method achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics.

## 1 Introduction

Graph neural networks (GNNs)  have been successfully applied to predict the physical and chemical properties of molecules based on graph representations of molecular structures. Typically, a molecular structure is represented as an attributed graph \(G=(,,,)\), where \(\) is a set of nodes (i.e., atoms), \(\) is a set of edges (i.e., chemical bonds), \(^{|| d}\) is a \(d\)-dimensional node-feature matrix, and \(^{|| l}\) is an \(l\)-dimensional edge-feature matrix . Based on the graph representation, GNNs were able to learn latent molecular embeddings that encode the local geometry of the atoms as well as the physical characteristics of the entire molecular structure [3; 4].

Various GNN-based methods have been proposed to learn informative molecular representation from various approaches, such as molecular geometry , fragment-based learning [6; 7], and domain knowledge integration . However, their representation capabilities are fundamentally limited to the atom-level molecular structures, and they overlooked the principle of quantum mechanics that the physical and chemical characteristics of molecules fundamentally originate from the electron structures [9; 10]. Learning molecular representations from the electron-level structure is crucial because many physical and chemical characteristics of molecules are, in fact, essentially derived from their electronic structures . Therefore, we argue that the GNN-based methods would benefit from considering the molecules in the _electron-level_ structure beyond the atom-level structure.

A straightforward and direct solution would be to directly provide the electron-level information about the molecules to GNNs by calculating the electronic structures of the molecules based on the calculation methods in computational physics and chemistry, such as density functional theory (DFT) . However, this solution is impractical and sometimes infeasible in real-world large molecules because the calculation methods suffer from cubic or greater time complexities  andlocal convergences in the electronic structure calculations . Therefore, we need a new approach for learning electron-derived representations of large molecules without additional calculations and experiments of quantum mechanics.

We propose _hierarchical electron-derived molecular learning_ (HEDMoL) to learn molecular representations from the input atom-level molecular structures under their estimated electronic structures. The main idea is to estimate the electron-level information about a large input molecule, which is given in an atom-level structure, by extending the electron-level information about small molecules that compose the large input molecule. As the electron-level information about small molecules is already computed and provided in public calculation databases, we can relieve the computational burden of expensive electronic structure calculations required for large molecules. More precisely, HEDMoL learns electron-derived molecular representations from both the input atom-level molecular information and the estimated electron-level information through the following three steps: (1) HEDMoL decomposes an input atom-level molecular structure into several atom-level substructures based on graph decomposition algorithms. (2) HEDMoL constructs an electron-derived molecular graph by transferring electron-level attributes stored in an external calculation database to each of the decomposed substructures based on structural similarities between the decomposed substructures and the small molecules in the external calculation database, and we call this process _knowledge extension_. (3) HEDMoL generates molecular representations through a hierarchical representation learning on the latent atom- and electron-level information.

It is worth noting that in this study, we focus on evaluating the prediction capabilities of the prediction models on _experimental_ datasets rather than _calculation_ datasets (e.g., QM9 dataset ). Although the calculation datasets are useful for analyzing rough relationships between atomic geometry and molecular properties, the calculation datasets are not appropriate to evaluate the prediction capabilities of machine learning methods on real-world molecular physics because they do not sufficiently simulate the uncertainty and complex configurations in quantum mechanics [13; 14]. For these reasons, we collected eight experimentally-generated molecular datasets from physicochemistry, toxicity, and pharmacokinetics applications. In the experiments, we measured the prediction accuracy of the prediction models on the experimental molecular datasets to evaluate the prediction capabilities of the prediction models on complex _real-world molecular physics_. HEDMoL achieves state-of-the-art performance in predicting the experimentally observed physical and chemical properties of the molecules. Furthermore, HEDMoL outperforms state-of-the-art GNNs in various regression tasks on small training datasets, which is one of the main challenges of machine learning in chemical applications [15; 16].

## 2 Method

### Problem Reformulation on Electronic Substructures

First of all, we reformulate the prediction problem on molecular structures as a problem on decomposed molecular substructures. By doing so, we can extend the knowledge of small molecules, which is not expensive to calculate or measure the electronic attributes, into the knowledge of complex real-world molecules. This knowledge extension from theoretically calculated information about small atomic structures to real-world experimental observations is a long-standing challenge in computational physics and chemistry [17; 18]. Physically, we can define a problem to predict the molecular properties \(y\) as follows.

\[y=(f g)(),\] (1)

where \(\) is the electronic structure of a molecule, \(g\) is a physics-informed function to generate a numerical representation from \(\), and \(f\) is a function to calculate the physical and chemical properties from the electron-level descriptors \(g()\). Since the calculation methods to calculate the electronic structure \(\) has a cubic or greater time complexity with respect to the number of atoms, existing GNNs-based methods basically assume that \(g()\) can be sufficiently approximated by the atom-level molecular structures to avoid the impractical time complexity in the electronic structure calculation. However, the physical information in the electronic structures is inevitably distorted and simplified in the process of converting \(g()\) to the atom-level molecular structures [10; 19].

Instead of approximating \(\) with the atom-level structures, we reformulate the problem in Eq. (1) as a problem on a set of small substructures by decomposing the input electronic structures \(\) into substructures as:

\[y=(f g_{G})(\{g_{L}(_{1}),...,g_{L}(_{K})\}),\] (2)

where \(_{k}()\) is the \(k\)-th electronic substructure of \(\), \(()\) in a set of physically possible electronic substructures of \(\), \(g_{L}\) is a descriptor function for the electronic substructures \(_{k}\), and \(g_{G}\) is an order-invariant function to generate the molecular representation from a set of electron-level descriptors \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\). HEDMoL aims to predict the target physical and chemical properties of molecules based on the decomposed formulation in Eq. (2) without the expensive electronic structure calculations on the entire molecule.

### Overall Architecture of HEDMoL

Fig. 1 illustrates the overall prediction process of HEDMoL, which consists of three steps as: (1) Substructure decomposition, (2) Knowledge extension, and (3) Hierarchical molecular representation learning. Each step of HEDMoL is summarized as follows. **(Step 1)** An input atom-level molecular structure \(\) is decomposed into a set of atom-level substructures \(=\{S_{1},...,S_{K}\}\). Each decomposed substructure \(S_{k}\) implies the atom-level representation of the decomposed electronic substructures \(_{k}\) and will be converted into \(g_{L}(_{k})\) in the next knowledge extension step. **(Step 2)** HEDMoL assigns electron-level attributes in external calculation databases to each decomposed substructure \(S_{k}\), i.e., the decomposed atom-level substructures \(\) is converted into a set of electron-level descriptors \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\) by transferring pre-calculated electronic attributes in an external calculation database. **(Step 3)** HEDMoL calculates an electronic state vector by \(_{e}=_{e}(G_{e})\), where \(_{e}\) is a GNN-based embedding network, and \(G_{e}\) is a graph representation of \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\). For the electronic state vector \(_{e}\), HEDMoL generates two molecular embeddings from atomic and electron-conditioned structures to predict the target molecular property \(y\). In the following sections, we will explain the implementation details of each step in HEDMoL.

### Substructure Decomposition

The substructure decomposition of HEDMoL is a data pre-processing step to convert an input atom-level molecular structure into a tuple \((,_{e})\), where \(=\{S_{1},...,S_{K}\}\) is a set of the decomposed substructures of an input atom-level molecular structure \(\), \(K\) is the number of decomposed substructures, \(_{e}\) is a set of edges between the decomposed substructures. Physically, each decomposed substructure \(S_{k}\) implies the atom-level representation of the decomposed electronic substructures \(_{k}\) in Eq. (2). In the decomposition process, we enforce a constraint \(=S_{1}... S_{K}\) to prevent information loss from the substructure decomposition. In the next knowledge extension step, the decomposed substructures will be converted into a set of electron-level descriptors \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\).

Among various choices for the implementation of the substructure decomposition of HEDMoL, such as spectral clustering , BRICS decomposition , and junction tree algorithm , we employed the junction tree algorithm due to the following three benefits of this algorithm as: (1) The junction tree algorithm does not require a hyperparameter tuning for each input graph. (2) It provides generalized graph abstraction and clustering results of the molecular structures . (3) It satisfies our

Figure 1: The overall process of HEDMoL to predict the target molecular property \(y\) of the input atom-level molecular structure \(\). \(=\{S_{1},S_{2},S_{3}\}\): a set of the decomposed atom-level substructures. \(_{e}\): a set of edges between the decomposed atom-level substructures. \(_{a}\) and \(_{c}\): calculated molecular embeddings of \(\), which are defined in Eq. (8)

constraint \(=S_{1}... S_{K}\) in the substructure decomposition. Therefore, HEDMoL decomposes the input atom-level molecular structure \(\) into a set of atom-level substructures \(=\{S_{1},...,S_{K}\}\) based on the junction tree algorithm, where \(S_{k}\) is a vertex clique in the entire junction tree of \(\). The experimental comparison of HEDMoL with the junction tree and BRICS decomposition algorithms are provided in Appendix 5 of Supplementary Material.

### Knowledge Extension

To avoid the computational bottleneck in the electronic structure calculation, the knowledge extension step of HEDMoL aims to transfer the electron-level information from an external calculation database \(_{s}\) to each of the decomposed substructures \(S_{k}\) instead of calculating the electronic structures. That is, a set of the atom-level decomposed substructures \(\) is converted into a set of electron-level descriptors \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\). By our reformulated problem in Eq. (2), HEDMoL can utilize the knowledge about the small molecules from the external calculation databases and extend the knowledge to large molecules without additional calculations and architectural modifications.

In the knowledge extension step, HEDMoL matches the decomposed substructures to the small molecules in an external calculation database by calculating the molecular distance between them. Based on the calculated molecular distances, HEDMoL transfers the knowledge of the external calculation database to each decomposed substructure as:

\[_{e,k}=_{_{k}},\] (3)

where \(_{e,k}^{q}\) is the \(k\)-th row vector of \(_{e}^{K q}\), and it denotes the node feature of the \(k\)-th decomposed substructure \(S_{k}\). \(_{_{k}}^{q}\) is the pre-calculated electron-level attributes of a small molecule in \(_{s}\), where \(_{k}\) is the index (such as) of the small molecule in \(_{s}\) that is the most similar to \(S_{k}\). Formally, \(_{k}\) is calculated by:

\[_{k}=*{arg\,min}_{i\{1,2,...,|_{s}|\}} (S_{k},G_{i}^{*}),\] (4)

where \(G_{i}^{*}\) is the atom-level graph of the \(i\)-th molecule in \(_{s}\), \(\) is a distance metric between two graphs. Among several choices, we implemented \(\) based on a robust and efficient unsupervised graph embedding method called geometric scattering on graphs (GeoScattering) , and the molecular distance was calculated by the Euclidean distance between the GeoScattering embeddings of \(S_{k}\) and \(G_{i}^{*}\). The prediction capabilities of HEDMoL for the implementations of \(\) with different graph embedding methods are experimentally evaluated in Appendix 6 of Supplementary Material.

After the knowledge extension step, an electron-derived substructure graph \(G_{e}\) of the input molecule is generated by reconstructing an attributed graph based on the decomposed substructures and the assigned electron-level attributes as \(G_{e}=(,_{e},_{e})\). Thus, we generated \(G_{e}\) containing fragmented information about the electronic structure without expensive electronic structure calculations. In the next step, HEDMoL generates a latent molecular embedding through a hierarchical representation learning of the atom-level molecular graph \(G_{a}=(,,_{a},)\) and the electron-derived substructure graph \(G_{e}\) of the input molecule, where \(_{a}\) is an input atom-feature matrix.

### Hierarchical Molecular Representation Learning

The purpose of the hierarchical representation learning in HEDMoL is to generate a latent embedding vector \(\) of the input molecule by propagating the electron-level information from a latent embedding of \(G_{e}\) to the embedding process of the atom-level molecular descriptor \(G_{a}\). This mechanism is consistent with the physical principle because the atomic configurations fundamentally depend on the electronic structures [9; 10]. In the hierarchical representation learning, we employed a GNN-based embedding network \(_{e}\) for an order-invariant representation learning on a set \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\). Formally, HEDMoL calculates an order-invariant electron-derived embedding \(_{e}\) on \(\{g_{L}(_{1}),...,g_{L}(_{K})\}\) as:

\[_{e}=_{e}(G_{e}) g_{G}(\{g_{L}(_{1}),...,g_{L }(_{K})\}).\] (5)

For the calculated \(_{e}\), an electronic state vector \(_{e}\) is calculated by \(_{e}=_{i=1}^{||}_{e,i}\), where \(_{e,i}\) is the \(i\)-th row-vector of \(_{e}\). After the embedding process of \(G_{e}\), GNN-based embedding network \(_{a}\) calculates an atom-level node embedding matrix as \(_{a}=_{a}(G_{a})\) for a given atom-level molecular graph \(G_{a}\). Then, an atom-level molecular embedding \(_{a}\) is similarly calculated by \(_{a}=_{i=1}^{||}_{a,i}\). On the other hand, a conditional atom-level molecular embedding \(_{c}\) is calculated by considering the atomic contributions under the given electronic state vector \(_{e}\) as:

\[_{c}=_{i=1}^{||}(_{a,i} _{e}))}{_{j=1}^{||}(f_{a}(_{a,j} _{e}))}_{a,i},\] (6)

where \(f_{a}\) is a trainable neural network to calculate attention scores of the \(i\)-th atom under the given electronic structure described by the electronic state vector \(_{e}\). In other words, \(_{c}\) contains atom-level information conditioned by underlying electronic structures.

After the graph embedding process, HEDMoL generates the molecular embedding \(\) by concatenating the graph-level embeddings from different views as \(=_{a}_{c}\), where \(_{a}\) is the graph-level embedding of \(G_{a}\). Conceptually, the molecular embedding \(\) contains the atomic information conditioned by electronic structures as well as the atom-level information about the input molecules. Finally, HEDMoL predicts the target molecular properties \(y=f_{d}()\) by entering \(\) to the trainable dense layers \(f_{d}\).

### Energy-Based Physical Consistency Regularization

In the molecular representation learning of HEDMoL, we enforce that the latent atom- and electron-level embeddings of the input molecule to indicate the same potential energy, which is one of the universal quantities to describe the atomic systems [10; 24; 25]. Physically, the atom- and electron-level descriptors of a molecule should have the same potential energy because they describe essentially the same molecular structure. To this end, we introduce two constraints on the input molecule as:

\[E_{p,k}+_{k}=E_{a,k},E_{a,k}=E_{e,k}, k=1,2,...,| |,\] (7)

where \(E_{p,k}\) is the calculated physical energy of the small molecule that matches with the \(k\)-th decomposed substructure \(S_{k}\), \(_{k}(0,_{k})\) is independent and identically normally distributed random variables following a normal distribution \((0,_{k})\), \(E_{a,k}\) is the predicted energy from the node embeddings of the atoms in \(S_{k}\), and \(E_{e,k}\) is the predicted energy from the \(k\)-th substructure embedding \(_{e,k}\). \(_{k}\) indicates the approximation error in transferring the physical energy of the small molecule in \(_{s}\) to the decomposed substructures in \(\).

Physically, the energy of an atomic system \(\) can be described by the many-body potential energies of the atoms in \(\) as :

\[E =_{i_{1},i_{2}}^{||}V_{2}(_{i_{1}}, _{i_{2}})+_{i_{1},i_{2},i_{3}}^{||}V_{3}(_{i_{1}},_{i_{2}},_{i_{3}})\] \[++_{i_{1},...,i_{||}}^{||}V_{| |}(_{i_{1}},...,_{i_{||}}),\] (8)

where \(||\) is the number of atoms in the atomic system \(\), \(V_{i}\) is the \(i\)-body potential function, and \(_{i}\) is the local environment around the \(i\)-th atom. However, calculating the many-body potential energy is infeasible due to the computational complexity. To overcome the infeasible computational complexity, we approximate the many-body potential energy in Eq. (8) based on a trainable message-passing function, as the message-passing scheme is an efficient approach for predicting the physical properties from the physical interactions of particles [27; 28]. We define the many-body potential energy of \(S_{k}\) based on the graph self-attention mechanism , which calculates the interatomic attention score \(_{i,j}\). Formally, HEDMoL predicts the many-body potential energy of \(S_{k}\) based on a trainable energy function \(f_{e}\) and a message-passing function \(g_{e}\) as:

\[E_{a,k}=f_{e}(g_{e}(S_{k})),\] (9)

where a vector-shaped atom-level substructure embedding \(g_{e}(S_{k})\) is given by

\[g_{e}(S_{k})=|}_{i S_{k}}(_{a,i}+_{j _{i} S_{k}}_{i,j}_{a,j}),\] (10)\(|S_{k}|\) is the number of atoms in a substructure \(S_{k}\), **W** and **V** are trainable weight matrices of \(g_{e},_{i}\) is a set of indices of the atoms connected to the \(i\)-th atom, and \(_{i,j}\) is an attention score between the \(i\)-th and \(j\)-th atoms. Similarly, we define \(E_{e,k}\) of the \(k\)-th decomposed substructure based on the trainable energy function \(f_{e}\) in Eq. (9) as:

\[E_{e,k}=f_{e}(_{e,k})\] (11)

Based on Eqs. (9) and (11), we define two regularization terms \(_{a,n}\) and \(_{e,n}\) for the \(n\)-th molecule in the training dataset \(\) as:

\[_{a,n} =_{k=1}^{|_{n}|}\{|E_{a,k}-f_{e}(g_{e}(S_{k}))| -,0\},\] (12) \[_{e,n} =_{k=1}^{|_{n}|}\{|E_{e,k}-f_{e}(_{ e,k}))|-,0\},\] (13)

where \(_{n}\) is the set of decomposed substructures of the \(n\)-th molecule, \( 0\) is a hyperparameter to allow uncontrollable energy differences incurred by the structural differences between the decomposed substructures and the matched small molecules in the external calculation database. The hyperparameter \(\) is a physically bounded variable , and the energy differences between small organic molecules are usually in a range from 0.1 to 0.3 electronvolts because the energy that the small organic molecules can have is physically bounded [9; 14]. Finally, we optimize the model parameters of HEDMoL to minimize the following loss function \(L\) as:

\[L=_{n=1}^{||}L_{p}(y_{n},f_{d}(_{n}))+( _{n=1}^{||}_{a,n}+_{e,n}),\] (14)

where \(L_{p}\) is a prediction loss, and \( 0\) is a hyperparameter to control the effect of the regularization terms \(_{a,n}\) and \(_{e,n}\) in the training of HEDMoL. Experimental evaluations of HEDMoL for different values of \(\) and \(\) are provided in Appendix 7 of Supplementary Material.

## 3 Experiments

We compared the prediction accuracies of HEDMoL with those of the state-of-the-art methods on various benchmark molecular datasets containing experimentally observed molecular physics. The experimental molecular datasets were provided by public chemical databases, such as MoleculeNet  and ChEMBL . We selected eight benchmark datasets from various application fields including physicochemistry, toxicity, and pharmacokinetics, as shown in Appendix 2 of Supplementary Material. We compared the prediction capabilities of HEDMoL with those of a tree-based ensemble method  and ten state-of-the-art GNNs [33; 34; 35; 36; 37; 38; 39; 40; 41]. We generated XGB-Mor, XGB-FC, and XGB-MK that predict target molecular properties for input Morgan (Mor) , functional-class (FC) , and MACCS Key (MK)  fingerprints, respectively. Although the 3D structure-based GNNs are not applicable to the experimental molecular datasets due to the absence of the 3D atomic coordinates, we additionally calculated the 3D atomic coordinates based on the semi-empirical method in RDKit1 and evaluated the 3D structure-based GNNs based on the generated 3D atomic coordinates. However, we were not able to execute or evaluate PaiNN , GemNet , and Equiformer  on the experimental molecular datasets due to the out-of-memory problem or required additional information.

In the experiments, we focused on evaluating the prediction capabilities of HEDMoL on the _experimental_ datasets rather than _calculation_ datasets, due to the following two reasons: (1) Evaluations on the calculation datasets (e.g., QM9 dataset) are not fair because HEDMoL exploits the external calculation databases in the knowledge extension step. (2) The experimental datasets containing the uncertainty of the atomic systems are closer to real-world molecular physics than the calculation datasets . We used SchNet, MPNN, and GIN as the GNN-based embedding networks of HEDMoL. Implementation details and hyperparameter settings of HEDMoL are provided in Appendix 4 of Supplementary Material. We used a subset of the QM9 dataset containing the molecules of maximum six atoms as an external calculation database for knowledge extension of HEDMoL, as described in Appendix 4 of Supplementary Material. The source code of HEDMoL and the experiment scripts are publicly available at https://github.com/anonauthor60/HEDMoL. The experimental evaluations of HEDMoL for different choices of implementations and external databases are provided in Supplementary Material.

### Prediction Accuracy on Experimentally Collected Benchmark Datasets

We measured \(R^{2}\)-scores of the state-of-the-art competitors and HEDMoL based on the leave-one-out 5-fold cross-validation so that the test dataset covers all molecules in the original dataset because the prediction accuracy of the prediction models on the real-world chemical data is sensitive to training/test split . We reported the mean of the measured \(R^{2}\)-scores with the standard deviation on the test datasets. Table 1 presents the measured \(R^{2}\)-scores and standard deviations on the eight benchmark molecular datasets, and HEDMoL achieved the best \(R^{2}\)-scores for all benchmark datasets. HEDMoL outperformed the competitor methods over the standard deviations for all benchmark datasets except for the ESOL dataset. In particular, HEDMoL showed higher \(R^{2}\)-scores than individual GIN, EGC, and SchNet, which are used as the embedding networks of HEDMoL. Furthermore, HEDMoL outperformed the 3D-based GNNs, DimeNet++, PhysChem, and M3GNet, even though they used additional 3D atomic coordinates. This result stems from the approximation and calculation errors in the 3D molecular generation. Experimental results on MAEs were consistent with the results in Table 1, as shown in Appendix 10. These results show that HEDMoL learned more generalized and informative molecular representations beyond individual GNNs through the hierarchical representation learning on \(G_{a}\) and \(G_{e}\).

In the experiment, GNNs outperformed the XGB-based models in the problems of predicting the physicochemicaly properties on the Lipop, ESOL, and ADMET datasets. However, the simple XGB-based models showed higher \(R^{2}\)-scores than those of the state-of-the-art GNNs on the LMC-H and LMC-R datasets containing relatively large molecules. This result is consistent with an experimental

  Input & Method & Lipop & ESOL & ADMET & IGC50 & LC50 & LD50 & LMC-H & LMC-R \\    Molecular \\ Fingerprint \\  } & XGB-Mor  & 0.531 & 0.659 & 0.717 & 0.621 & 0.390 & 0.497 & 0.505 & 0.617 \\  & XGB-Mor  & (0.024) & (0.045) & (0.021) & (0.040) & (0.133) & (0.016) & (0.018) & (0.058) \\   Molecular \\ Fingerprint \\  } & XGB-FC  & 0.578 & 0.686 & 0.720 & 0.628 & 0.501 & 0.519 & 0.503 & 0.612 \\  & XGB-FC  & (0.018) & (0.052) & (0.009) & (0.023) & (0.052) & (0.025) & (0.007) & (0.015) \\   & XGB-MK  & 0.542 & 0.764 & 0.761 & 0.680 & 0.486 & 0.526 & 0.471 & 0.591 \\  & XGB-MK  & (0.041) & (0.047) & (0.020) & (0.037) & (0.112) & (0.021) & (0.019) & (0.033) \\   3D \\ molecular \\ Graph \\  } & DimeNet++  & N/R & 0.878 & N/R & 0.779 & 0.591 & N/A & 0.352 & N/R \\   & PhysChem  & 0.694 & 0.848 & N/A & 0.814 & 0.511 & N/A & N/A & N/R \\   & PhysChem  & (0.024) & (0.032) & N/A & 0.017 & (0.053) & & & & \\   & M3GNet  & N/A & 0.857 & N/A & 0.697 & 0.531 & N/A & N/A & 0.565 \\  & M3GNet  & N/A & (0.025) & N/A & (0.029) & (0.034) & N/A & N/A & (0.041) \\   2D \\ Graph \\  } & GIN  & 0.702 & 0.897 & 0.833 & 0.799 & 0.543 & 0.515 & 0.443 & 0.568 \\  & GIN  & (0.031) & (0.022) & (0.017) & (0.021) & (0.080) & (0.044) & (0.027) & (0.020) \\   2D \\ Graph \\  } & EGC  & 0.708 & 0.896 & 0.838 & 0.808 & 0.575 & 0.497 & 0.441 & 0.566 \\  & (0.043) & (0.017) & (0.012) & (0.029) & (0.045) & (0.034) & (0.023) & (0.017) \\   & MPNN  & 0.711 & 0.894 & 0.830 & 0.797 & 0.532 & 0.469 & 0.449 & 0.564 \\  & (0.022) & (0.023) & (0.014) & (0.018) & (0.064) & (0.040) & (0.057) & (0.031) \\   &  & 0.701 & 0.899 & 0.836 & 0.807 & 0.531 & 0.482 & 0.436 & 0.588 \\  & (0.034) & (0.029) & (0.008) & (0.018) & (0.040) & (0.041) & (0.051) & (0.020) \\   & SchNet  & 0.667 & 0.881 & 0.834 & 0.765 & 0.587 & 0.467 & 0.456 & 0.543 \\  & (0.021) & (0.026) & (0.012) & (0.034) & (0.052) & (0.025) & (0.024) & (0.033) \\   & MEGNet  & 0.604 & 0.889 & 0.826 & 0.754 & 0.574 & 0.505 & 0.422 & 0.617 \\  & (0.023) & (0.027) & (0.024) & (0.026) & (0.122) & (0.027) & (0.032) & (0.058) \\   & UniMP  & 0.702 & 0.886 & 0.833 & 0.793 & 0.504 & 0.470 & 0.422 & 0.579 \\  & (0.030) & (0.025) & (0.014) & (0.027) & (0.031) & (0.025) & (0.061) & (0.036) \\   & HEDMoL & **0.759** & **0.914** & **0.865** & **0.840** & **0.663** & **0.572** & **0.551** & **0.639** \\   & **(0.043)** & **(0.016)** & **(0.014)** & **(0.010)** & **(0.053)** & **(0.035)** & **(0.008)** & **(0.035)** \\  

Table 1: Measured \(R^{2}\)-scores on the benchmark molecular datasets. Input type means the required data format of the input molecules. The highest \(R^{2}\)-score for each benchmark dataset has been remarked in bold, and the standard deviation of the \(R^{2}\)-scores is presented in parentheses. N/R means the negative R2-score indicating the failure of the machine learning model. N/A is “not available” that means the out-of-memory problem or impractical computation cost.

observation in a previous study  regarding the overfitting problems of GNNs on large atomic systems. As shown in the experimental results, the fingerprint-based models and GNNs have their own limitations when applied to the experimental molecular data. The fingerprint-based methods suffer from the lack of physical information about the input molecules because the molecular fingerprints are designed to describe the connectivities of the atoms rather than representing the physical attributes of each atom. On the other hand, although GNNs can extract physical information from the input molecular graphs containing physical attributes of each atom, they can be easily overfitted in large atomic systems . However, HEDMoL overcomes both limitations of the existing methods by exploiting the molecular graph with electronic attributes, which are robust to extrapolation [9; 10; 17]. The prediction accuracy and improvement of HEDMoL according to the molecular sizes were presented in Appendix 12 of Supplementary Material. The experimental results demonstrate that HEDMoL can provide more accurate and robust prediction results on real-world molecular physics without additional electronic structure calculations.

## 4 Conclusion

This paper proposed HEDMoL for learning electron-derived molecular representations to improve the prediction capabilities on real-world molecular physics. HEDMoL learned the electron-derived molecular representations without additional calculation costs by transferring the pre-calculated electron-level information of small molecules in an external database to large input molecules. HEDMoL achieved state-of-the-art prediction accuracy on extensive molecular datasets containing experimentally observed molecules and their properties. Furthermore, HEDMoL showed better prediction accuracies even under the lack of training data, which is one of the main challenges of machine learning in chemical applications. These results showed the practical potential of HEDMoL in real-world chemical applications.