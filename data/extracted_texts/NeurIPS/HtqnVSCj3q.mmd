# Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models

Pan Lu\({}^{1}\), Baolin Peng\({}^{2}\), Hao Cheng\({}^{2}\), Michel Galley\({}^{2}\)

**Kai-Wei Chang\({}^{1}\), Ying Nian Wu\({}^{1}\), Song-Chun Zhu\({}^{1}\), Jianfeng Gao\({}^{2}\)**

\({}^{1}\)University of California, Los Angeles \({}^{2}\)Microsoft Research, Redmond

https://chameleon-llm.github.io

###### Abstract

Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with _plug-and-play_ modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.

Figure 1: Examples from our Chameleon approach with GPT-4 on ScienceQA , a multi-modal question answering benchmark in scientific domains. Chameleon is adaptive to different queries by synthesizing programs to compose various tools and executing them sequentially to get final answers.

Introduction

Remarkable progress has been observed in recent large language models (LLMs) for various natural language processing tasks, with prominent examples such as GPT-3 , PaLM , LLaMA , ChatGPT , and the recently developed GPT-4 . LLMs have demonstrated emergent abilities, including in-context learning and chain-of-thought (CoT) reasoning . These models are capable of solving diverse tasks in a zero-shot fashion  or with the aid of a few examples , and they show great potential in planning and decision-making akin to human beings . Despite these capabilities, LLMs face inherent limitations, such as an inability to access up-to-date information , perform precise mathematical reasoning , or utilize specialized models . Therefore, enhancing current LLMs with the capability to automatically _compose_ external tools for real-world task solving is critical to address these drawbacks.

Consider the example in Figure 1: _Which is the main persuasive appeal used in this ad?_. To answer this question, one needs to: 1) infer that there is an ad image containing text context and call a text decoder to understand the semantics; 2) retrieve background knowledge about _persasive appeals_ and the differences among three persuasive appeals; 3) generate a solution based on the input query and intermediate results from previous steps; and 4) finally produce the answer in a task-specific format. On the other hand, when answering _Which animal's skin is adapted for survival in cold places_ (), one might need to call modules such as an image captioner to decipher image information and a web search engine to retrieve domain knowledge to understand scientific terminologies. However, current tool-augmented LLMs still face challenges when addressing these real-world queries across various scenarios. Most existing approaches are either limited to a small number of tools  or relying on domain-specific tools , and thus are not easy to generalize to queries of new domains (see sections 2 and A.1 for further discussion). In this work, we study how to enable LLMs to synthesize programs to capture the logic of composing heterogeneous tools.

To address the challenges of existing work, we introduce **Chameleon**, a _plug-and-play compositional_ reasoning framework that leverages LLMs to synthesize programs and compose various tools for a wide range of tasks. Unlike existing tool-augmented LLMs , **Chameleon** uses a richer set of tools, including LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristics-based modules. Moreover, **Chameleon** leverages the in-context learning capabilities of LLMs and builds on an LLM as a natural language planner, without requiring any training or carefully curated rules. Prompted by tool descriptions and usage examples, the planner infers a program composed of a sequence of tools to execute in order to generate the final response for a user query. Instead of generating programs in domain-specific languages , **Chameleon** generates natural-language-like (NL) programs (e.g., [Text_Detector, Knowledge_Retrieval, Solution_Generator, Answer_Generator] for the second query in Figure 1). The NL-like programs are easy to understand and debug by users with limited programming experience, and easily extendable to new modules. During each module's execution, the module processes the query and cached context, returns a result determined by the module itself, and updates the query and context for subsequent execution. Composing modules as a sequential program allows subsequent modules to leverage prior cached context and updated queries.

We showcase the adaptability and effectiveness of **Chameleon** on two tasks: ScienceQA  and TabMWP . ScienceQA is a multi-modal question answering benchmark spanning multiple context formats and various scientific topics, while TabMWP is a mathematical benchmark involving diverse tabular contexts. These two benchmarks serve as a good testbed to evaluate **Chameleon**'s ability to coordinate diverse tools across different types and domains. Notably, **Chameleon** with GPT-4 achieves an 86.54% accuracy on ScienceQA, significantly improving upon the best published few-shot model by 11.37%. On TabMWP, using GPT-4 as the underlying LLM, **Chameleon** achieves an improvement of 7.97% over chain-of-thought (CoT) prompted GPT-4  and a 17.0% increase over the best-published model , lifting the state of the art to 98.78%. Further studies suggest that using GPT-4 as a planner exhibits more consistent and rational tool selection and is able to infer potential constraints given the instructions, compared to other LLMs like ChatGPT.

Our contributions are as follows: (1) We develop a plug-and-play compositional reasoning framework, **Chameleon**, that effectively composes external tools to address inherent limitations of LLMs and tackle a broad range of reasoning tasks. (2) Relying on an LLM as a natural language planner to generate programs, **Chameleon** successfully integrates various tools, including LLMs, off-the-shelf vision models, web search engines, Python functions, and rule-based modules, to build a versatile and adaptable AI system capable of answering real-world queries. (3) We demonstrate Chameleon's effectiveness on two challenging benchmarks, significantly surpassing the state of the art.

## 2 Related Work

Compositional ReasoningNeural modular and compositional approaches have been explored to automatically perform desired sub-task decomposition, enhancing interpretability and adaptability across various reasoning tasks. Early work [2; 3] posits that complex reasoning tasks are fundamentally compositional and proposes neural module networks (NMN) to decompose them into subtasks. However, these methods rely on brittle off-the-shelf parsers and are limited by module configurations. Some later work [19; 15; 14; 21], takes a step further by predicting instance-specific network layouts in an end-to-end manner, without relying on parsers, using reinforcement learning  and weak supervised learning. In visual reasoning, models comprising a program generator and an execution engine have been proposed to combine deep representation learning and symbolic program execution [19; 61]. In the domain of mathematical reasoning, an interpretable solver has been developed to incorporate theorem knowledge as conditional rules and perform symbolic reasoning step by step . Our work takes inspiration from neural module networks, yet it offers several distinct advantages. First, Chameleon does not require expensive supervision of task-specific programs for modeling training. Instead, it generates sequential programs, consisting of modules, that are easy to generalize to various domains and tasks, allowing the extension to new modules in a plug-and-play manner. Second, Chameleon does not require any training, but uses the in-context learning capabilities of LLMs to generate programs prompted by natural language instruction and demonstrations.

Tool-Augmented Language ModelsIn recent years, the development of large language models (LLMs) [48; 8; 9; 53; 4; 41; 42] has made tremendous progress and has stimulated research in prompt learning [57; 33; 22] and instruction learning [53; 64; 46; 11]. Despite the impressive performance of LLMs, they suffer from inherent limitations, such as the inability to access up-to-date information , utilize external tools , or perform precise mathematical reasoning [44; 35]. Recent benchmarks, such as ScienceQA and TabMW [32; 33; 7; 54; 51; 30], have emerged to evaluate the capability of LLMs to tackle intricate reasoning challenges, especially those emphasizing the use of external tools. Concurrently, there has been a growing interest in harnessing external tools and modular approaches to augment LLMs. These augmented LLMs can access real-time information aided by web search engines  and leverage domain-specific knowledge from external resources . Some work leverages the Python interpreter to generate complex programs to employ powerful computational resources, and execute logical reasoning tasks more effectively [55; 10; 6; 39; 18; 43; 36]. For example, Toolformer  constructs tool-use augmented data to train language models to select five

Figure 2: Two examples from our Chameleon approach with GPT-4 on TabMWP , a mathematical reasoning benchmark with tabular contexts. Chameleon demonstrates flexibility and efficiency in adapting to different queries that require various reasoning abilities.

tools. In the realm of visual tools, various approaches have been proposed to enhance the capabilities of large language models in handling visual tasks [60; 59; 52; 13; 50], augmented with Hugging Face models , Azure models , visual foundation models .

We compare **Chameleon** with other tool-augmented language models in Table 1. Many of these approaches are either constrained to a small set of tools or limited to task-specific tools, which reduces their capabilities across various skill dimensions and hampers their generalizability to new tasks. A recent line of work relies on large amounts of supervision [49; 26] and focuses on generating commands  and programs [52; 13] to infer the choice of tools. However, this approach needs to carefully tailored prompts to specific tasks and particular tools, and is neither flexible nor adaptive. In contrast, **Chameleon** instructs LLMs with natural language instructions that simply describe the roles of each module and provide a few calling examples, eliminating the need for additional training or tool-specific prompts when learning to compose different tools. More importantly, **Chameleon** offers users flexibility in terms of tool types and sources, updating the underlying LLMs, adding new tools, and adapting to new tasks. Our work shares the same spirit of AutoGPT , an autonomous GPT-4 agent with the artificial general intelligence (AGI) ambition to incorporate numerous tools to achieve user-defined goals. While AutoGPT is still under development, our work is the first to instantiate the idea and verify its effectiveness on well-studied benchmarks.

## 3 General Framework: Chameleon

To address the limitations of current LLMs in utilizing diverse tools, we propose **Chameleon**, a novel _plug-and-play compositional_ reasoning framework, synthesizing the composition of various tools to accommodate a wide range of problems. **Chameleon** is comprised of a _module inventory_ that defines different types of tools and an LLM-based _planner_, whose purpose is to decompose the original problem into sub-tasks that can be effectively solved by task-specific tools. Unlike existing tool-augmented LLM approaches [49; 13; 59; 50], our module inventory features multiple tool types as illustrated in Table 2, enabling **Chameleon** to exhibit various reasoning abilities, including image understanding, knowledge retrieval, web search, complex mathematical reasoning, and table understanding. Instead of generating domain-specific programs [40; 13; 52], **Chameleon** employs an LLM-based planner to create natural-language-like programs that follow natural language instructions, which is less error-prone, easily expandable to new modules, and user-friendly.

We formalize our planner as follows: given the input query \(x_{0}\), the module inventory \(\), and constraints \(\), the natural language planner \(\) selects a set of modules that can be executed sequentially to answer the query via generating a program in a natural-language-like format. The module inventory \(\) consists of a set of pre-built modules: \(\{M_{i}\}\), each corresponding to a tool of various types (Table 2). \(\) are the constraints for the plan generation, for example, the concurrent relations and sequenc

    &  &  &  \\   & Size & @ & @ & @ & @ & @ & @ & Image & Web & Know. & Math & Table & Composition & Planning & Plug-n-Play \\  CoT  & 1 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ \\ Lia  & 1 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ PoT  & 2 & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ CodeSdetNet  & 1 & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ PAL  & 2 & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MathPrompter  & 2 & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ \\  ART  & 4 & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ \\ Toolformer  & 5 & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & natural lang. & ✗ \\ WebGPT  & 10 & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ & ✓ & program & ✗ \\  MM-ReAct  & \(>\)10 & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & word match & ✓ \\ Visual ChatQPT  & \(>\)10 & ✓ & - & ✗ & ✓ & ✓ & ✗ & ✗ & ✓ & natural lang. & ✓ \\ ViperGPT  & \(>\)10 & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & program & ✓ \\ VisProg  & \(>\)10 & ✓ & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ & program & ✓ \\ HuggingGPT  & \(>\)10 & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & - & ✓ & natural lang. & ✓ \\ 
**Chameleon (ours)** & \(>\)10 & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & natural lang. & ✓ \\   

Table 1: A comparison of work that augments large language models with tool usage. We report the tool size and tool types, including OpenAI (@), Hugging Face (@), Github (@), Web search (@), and code (@). We compare the skills each method possesses, such as image understanding, browser search, knowledge retrieval, mathematical reasoning, and table understanding. Some models can compose various tools, propose a planner to infer the relevant tools for execution, or are inherently extendable to new tools. The label “-” refers to uncertain information in the literature.

orders of modules. In our work, the planner \(\) is an LLM prompted to generate a sequence of module names in a few-shot setup. The planner is prompted in natural language with a planning task instruction \(\), the descriptions of modules in \(\) with corresponding constraints \(\), as well as a few demonstration examples \(\). A \(T\)-length plan sampled from \(\) can be denoted as \(p=M^{1},,M^{T}\), where \(M^{t}\) represents an the \(t\)-th element in the generated plan and \(M^{t}\). Formally, given an input query (problem statement) \(x_{0}\), a plan \(p\) is generated as follows:

\[p(x_{0};,,,).\] (1)

Given the generated plan, the corresponding modules for each step are then executed sequentially. The plan is a natural-language program where each module is bound simply via string matching. When evaluating the module \(M^{t}\) at time step \(t\), the output of the execution \(y^{t}\) is calculated by:

\[y^{t} M^{t}(x^{t-1};c^{t-1}),\] (2)

where \(x^{t-1}\) is the input for the current module \(M^{t}\), and \(c^{t-1}\) is the cached information (e.g., image semantics, retrieved knowledge, generated programs) resulting from the execution history of modules. Both the problem input \(x^{t}\) and cache \(c^{t}\) for the next module \(M^{t+1}\) are updated, respectively, by:

\[x^{t}(x^{t-1},y^{t}),\] (3)

\[c^{t}(c^{t-1},y^{t}).\] (4)

The update_input and update_cache functions are hand-designed for each \(M_{i}\). Specifically, update_input is applied to elements in the input query, including the question, table context, and image. These elements are updated after module execution. update_cache corresponds to the generation of new information, such as a description for the input image or retrieved knowledge from external resources. Finally, the response \(r\) to the query is generated by the last module \(M^{T}\):

\[r=y^{T} M^{T}(x^{T-1};c^{T-1}).\] (5)

## 4 Applications of Chameleon

We demonstrate the applications of Chameleon on two challenging tasks: ScienceQA  (section 4.2) and TabMWP  (section 4.3), using the module inventory introduced in section 4.1. Further experimental details can be found in appendix A.2.

### Module Inventory

To accommodate various reasoning capabilities over a diverse range of queries, our system utilizes a rich module inventory of various external tools. We provide a high-level overview of this inventory here, with detailed implementations in specific experiments. The complete module inventory, \(\), is presented in Table 2. Each tool within the inventory is defined as follows:

**Knowledge Retrieval**: This module retrieves additional background knowledge crucial for tackling complex problems. It is especially beneficial for specialized domains like science and mathematics, providing context for the task. For example, if a query is about a tax form table, this module could generate knowledge about tax procedures, offering valuable context.

**Bing Search**: Like "Knowledge Retrieval", the "Bing Search" module aims to provide wide-ranging task-relevant knowledge. In contrast, it excels when broader or up-to-date information from multiple sources is required. Using the search engine API, this module returns relevant search results based on the input query, which are then parsed and used by subsequent modules to gather richer context information from diverse sources, enhancing problem-solving effectiveness.

**Query Generator**: Since the original problem typically lacks a tailored query for retrieving task-relevant information, this module creates search engine queries based on the problem, which are then

  
**Tool Types** & **Tools** \\    & & Knowledge Retrieval, Query Generator, \\  & & Row Lookup, Column Lookup, \\  & & Table Verbalizer, Program Generator, \\  & & Solution Generator \\    & Hugging Face & Image Captioner \\   & & Github & Text Detector \\   & Web Search & Bing Search \\   & Python & Program Verifier, Program Executor \\   & Rule-based & Answer Generator \\   

Table 2: Different tools in our module inventory.

used by the "Bing Search" module. Mostly, it is a good strategy to use the "Query Generator" module before the "Bing Search". Coupled with the search engine tool, generating more targeted queries generally facilitates both the recall and precision of retrieved information.

**Image Captioner**: Designed to generate captions for images, this module provides crucial supplementary context for queries. It is particularly valuable when understanding an image semantically, like identifying objects and interactions in a scene. Using pre-trained models, it translates visual data into language, facilitating effective comprehension and reasoning about image content.

**Text Detector**: This module is designed to identify text within a given image. Typically, the "Text Detector" is employed when a question requires the extraction of textual information from images containing diagrams, charts, tables, maps, or other visual elements. By effectively detecting text in various formats, this module aids in the analysis and understanding of image-based content.

**Row Lookup**: This module is crucial when queries involve tabular context, as locating relevant cells is often required. Large tables can distract the system, so "Row Lookup" simplifies the table by retaining only the rows relevant to the query. If all rows are pertinent, it returns the original table.

**Column Lookup**: Like the "Row Lookup" module, "Column Lookup" addresses questions involving tabular context by focusing on relevant columns. It simplifies the table by retaining only pertinent columns, or returns the original table if all columns are relevant.

**Table Verbalizer**: Converting structured tables into text is likely to enhance the comprehension of tabular information by various downstream modules as shown by  for open-domain question answering, making this module a vital part of our system. It translates tables into easily understandable descriptions for modules like "Program Generator" and "Solution Generator", particularly useful for small, domain-specific tables like stem-and-leaf plots or function tables.

**Program Generator**: Program-aided approaches are shown to enhance the logical and mathematical reasoning abilities of LLMs [55; 10; 6; 39; 18; 43]. The "Program Generator" generates Python programs to solve queries effectively, which is particularly beneficial for queries requiring complex computations or intricate logical operations, such as "if-else" statements.

**Program Verifier**: Recent studies highlight the importance of verification to reduce hallucination [45; 38]. Hence, "Program Verifier" ensures the validity and error-free nature of programs generated by "Program Generator". It checks for syntax and logical errors, and potential execution issues, enhancing the reliability and accuracy of the solutions.

**Program Executor**: This module executes the program generated by "Program Generator" and produces the result, bridging the gap between program generation and final solution derivation.

**Solution Generator**: This module generates a detailed solution to the input query using all the cached information. Employing a chain-of-thought prompting approach , it ensures coherent and well-structured responses. The planner can directly employ this module instead of other functional modules if it can solve the query independently, especially for simpler ones.

**Answer Generator**: This task-specific module uses a rule-based approach to extract and normalize answers from the results of the "Program Executor" or "Solution Generator". Unlike the Solution Generator" that provides detailed multi-step solutions, "Answer Generator" serves as the final module in the pipeline, providing concise and task-specific answers.

### Science Question Answering

Science Question Answering (ScienceQA ) is a diverse benchmark for multi-modal question answering over a range of scientific topics and contexts. As examples illustrated in Figure 1, answering these questions requires various tools and skills like image captioning, text detection, knowledge retrieval, online resource search, and multi-clue visual reasoning. When generating programs for using tools, we limit the search space to the relevant inventory subset (Table 6 in the appendix). Programs are deemed invalid and default to a "Solution Generator" and "Answer Generator" sequence if these are not the final two elements, following the chain-of-thought prompting baseline . See Table 8 in the appendix for the constructed natural language planner prompt. The prompts for LLM-based modules like "Knowledge Retrieval", "Query Generator", and "Solution Generator" are shown in Table 10, 11, and 12, respectively, in the appendix.

### Tabular Mathematical Reasoning

TabMWP  is a mathematical reasoning task involving diverse tabular contexts like schedules, prices, tax forms, plots, and function relations (Figure 2). It requires AI systems to understand various table formats and perform precise numerical or symbolic computations. Like ScienceQA, we constrain the program search space to focus on two tool types: 1) those helping LLMs better digest tabular information (e.g., "Row Lookup", "Column Lookup", and "Table Verbaalizer") and 2) those performing faithful symbolic computations (e.g., "Program Generator", "Program Verifier", and "Program Executor") as listed in Table 6. The generated programs must meet certain constraints, such as including "Answer Generator" and placing "Program Generator" prior to both "Program Verifier" and "Program Executor". Non-compliant programs default to a sequence of "Program Generator", "Program Verifier", "Program Executor", and "Answer Generator", aligning with the program-of-thought prompting baseline  with added verification.

## 5 Experiments

We assess Chameleon's effectiveness and adaptability on two complex reasoning tasks, ScienceQA  and TabMWP . See experimental details in appendix A.2.

### Experimental Results

**ScienceQA.** Table 3 presents the results of existing baselines and our approach Chameleon, with key results highlighted in Figure 3 (a). Employing ChatGPT  as the base LLM, Chameleon

    & \#Tuned &  &  &  &  &  &  &  &  &  \\  & Params & & & & & & & & & & & \\   \\ Random Choice  & - & 39.83 & 40.28 & 46.13 & 29.25 & 47.45 & 40.08 & 33.66 & 39.35 & 40.67 \\ Human  & - & 88.40 & 90.23 & 84.97 & 87.48 & 89.60 & 87.50 & 88.10 & 91.59 & 82.42 \\  \\ MCAN  & 95M & 54.54 & 56.08 & 46.23 & 58.09 & 59.43 & 51.17 & 55.40 & 51.65 & 59.72 \\ Top-Down  & 70M & 59.02 & 59.50 & 54.33 & 61.82 & 62.90 & 54.88 & 59.79 & 57.27 & 62.16 \\ BAN  & 112M & 59.37 & 60.88 & 46.57 & 66.64 & 62.61 & 52.60 & 65.51 & 56.83 & 63.94 \\ DFAF  & 74M & 60.72 & 64.03 & 48.82 & 63.55 & 65.88 & 54.49 & 64.11 & 57.12 & 67.17 \\ VLT  & 113M & 61.14 & 60.48 & 63.89 & 60.27 & 63.20 & 61.38 & 57.00 & 60.72 & 61.90 \\ Patch-TRM  & 90M & 61.42 & 65.19 & 46.79 & 65.55 & 66.96 & 55.28 & 64.95 & 58.04 & 67.50 \\ VisualBERT  & 111M & 61.87 & 59.33 & 69.18 & 61.18 & 62.71 & 62.17 & 58.54 & 62.96 & 59.92 \\ UnifiedQA  & 223M & 70.12 & 68.16 & 69.18 & 74.91 & 63.78 & 61.38 & 77.84 & 72.98 & 65.00 \\ UnifiedQA Cot  & 223M & 74.11 & 71.00 & 76.04 & 78.91 & 66.42 & 66.53 & 81.81 & 77.06 & 68.82 \\ MM-COT-  & 223M & 70.53 & 71.09 & 70.75 & 69.18 & 71.16 & 65.84 & 71.57 & 71.00 & 69.68 \\ MM-COT  & 223M & 84.91 & 87.52 & 77.17 & 85.82 & 87.88 & 82.90 & 86.83 & 84.65 & 85.37 \\ MM-COT\({}_{Large}\) & 738M & 91.68 & 95.91 & 82.00 & 90.82 & 95.26 & 88.80 & 92.89 & 92.44 & 90.31 \\ LLaMA-Adapter\({}_{T}\) & 1.2M & 78.31 & 79.00 & 73.79 & 80.55 & 78.30 & 70.35 & 83.14 & 79.77 & 75.68 \\ LLaMA-Adapter  & 1.8M & 85.19 & 84.37 & 88.30 & 84.36 & 83.72 & 80.32 & 86.90 & 85.83 & 84.05 \\  \\ GPT-3  & 0M & 74.04 & 75.04 & 66.59 & 78.00 & 74.24 & 65.74 & 79.58 & 76.36 & 69.87 \\ GPT-3 CoT  & 0M & 75.17 & 75.44 & 70.87 & 78.09 & 74.68 & 67.43 & 79.93 & 78.23 & 69.68 \\   \\   \\ ChatGPT CoT & 0M & 78.31 & 78.82 & 70.98 & 83.18 & 77.37 & 67.92 & 86.13 & 80.72 & 74.03 \\  \\ _Few-shot GPT-4_ & 0M & 83.99 & 85.48 & 72.44 & 90.27 & 82.65 & 71.49 & 92.89 & 86.66 & 79.04 \\  \\  & 0M & **86.54** & **89.83** & **74.13** & **89.82** & **88.27** & **77.64** & **92.13** & **88.03** & **83.72** \\   

Table 3: **QA accuracy (%) on the test set of ScienceQA**. We report the number of tuned parameters for this task and the overall accuracy, along with accuracy scores for different question types, including natural, social, and language sciences, text, image, and no context, as well as grades 1-6 and 7-12. The highest scores among models in each section and overall are highlighted in blue and red, respectively, and the results of our best model are marked in bold.**achieves a 79.93% accuracy, a 1.62% improvement over Chain-of-Thought (CoT)  prompted ChatGPT. Notably, Chameleon is a generalized form of CoT, where the generated program is a sequence of "Solution Generator" and "Answer Generator". Chameleon benefits from additional tool usage, such as "Knowledge Retrieval", "Bing Search", "Image Captioner", and "Text Detector". When built upon GPT-4 , our model attains an accuracy of 86.54%, outperforming GPT-4 CoT  by 2.55% and GPT-3 CoT by 11.37%, creating the new state of the art in few-shot settings.

**TabMWP.** Table 4 presents results with key models in Figure 3 (b). Similarly, significant improvements are observed for Chameleon over both fine-tuned and few-shot models. It is worth noting that both CoT and Program-of-Thought (PoT)  can be viewed as special cases of Chameleon. Apart from "Solution Generator" and "Answer Generator", CoT doesn't utilize any tool, while PoT

   Model & 
 \#Tuned \\ Params \\  & ALL & FREE & MC & INT & DEC & EXTR & BOOL & OTH & G1-6 & G7-8 \\   \\ Heuristic guess & - & 15.29 & 6.71 & 39.81 & 8.37 & 0.26 & 30.80 & 51.22 & 26.67 & 17.55 & 12.27 \\ Human performance & - & 90.22 & 84.61 & 93.32 & 84.95 & 83.29 & 97.18 & 88.69 & 96.20 & 94.27 & 81.28 \\  \\ UnifiedQ\({}_{}}\) & 41M & 29.79 & 22.27 & 51.31 & 27.27 & 2.83 & 52.28 & 48.11 & 69.52 & 35.85 & 21.71 \\ UnifiedQ\({}_{}}\) & 223M & 43.52 & 34.02 & 70.68 & 40.74 & 7.90 & 84.09 & 55.67 & 73.33 & 53.31 & 30.46 \\ UnifiedQ\({}_{}}\) & 738M & 57.35 & 48.67 & 82.18 & 55.97 & 20.26 & 94.63 & 68.89 & 79.05 & 65.92 & 54.92 \\ TAPEX\({}_{}}\) & 139M & 48.27 & 39.59 & 73.09 & 46.85 & 11.33 & 84.19 & 61.33 & 69.52 & 56.70 & 37.02 \\ TAPEX\({}_{}}\) & 406M & 58.82 & 51.00 & 80.02 & 59.92 & 16.31 & 95.34 & 64.00 & 73.33 & 67.11 & 47.07 \\  \\ GPT-3  & 0M & 56.96 & 53.57 & 66.67 & 55.55 & 45.84 & 78.22 & 55.44 & 54.29 & 63.37 & 48.41 \\ GPT-3 CoT  & 0M & 57.61 & 54.36 & 66.92 & 55.82 & 48.67 & 78.82 & 55.67 & 51.43 & 63.62 & 49.59 \\  \\ GPT-3  & 0M & 57.13 & 54.69 & 64.11 & 58.36 & 40.40 & 75.95 & 52.41 & 53.02 & 63.10 & 49.16 \\ GPT-3 CoT  & 0M & 62.92 & 60.76 & 69.09 & 60.04 & 63.58 & 76.49 & 61.19 & 67.30 & 68.62 & 55.31 \\ GPT-3 CoT-PromptPG  & 0M & 68.23 & 66.17 & 74.11 & 64.12 & 74.16 & 76.19 & 72.81 & 65.71 & 71.20 & 64.27 \\ Codex  & 0M & 59.4 & - & - & - & - & - & - & - & - \\ Code Port’s  & 0M & 73.2 & - & - & - & - & - & - & - & - \\ Codex PoT-SC*  & 0M & 81.8 & - & - & - & - & - & - & - & - & - \\   \\   \\ ChatGPT CoT & 0M & 82.03 & 78.43 & 92.32 & 75.38 & 90.30 & 92.30 & 92.89 & 87.62 & 83.06 & 80.66 \\ ChatGPT PoT & 0M & 89.49 & 90.24 & 87.35 & 89.31 & 93.82 & 92.10 & 85.89 & 55.24 & 90.60 & 88.00 \\  \\  \\ GPT-4 CoT & 0M & 93.28 & 93.13 & 93.72 & 92.71 & 94.76 & 91.29 & 98.11 & 78.85 & 93.37 & 93.17 \\  \\ GPT-4 CoT & 0M & 90.81 & 88.48 & 97.49 & 86.16 & 97.51 & 96.86 & 99.11 & 89.52 & 92.40 & 88.70 \\ GPT-4 PoT & 0M & 96.93 & 97.40 & 95.58 & 98.48 & 93.22 & 96.25 & 98.00 & 68.57 & 96.97 & 96.87 \\ Chameleon (**GPT-4**) & 0M & **98.78** & **98.95** & **98.29** & **99.34** & **97.42** & **98.58** & **98.56** & **93.33** & **98.95** & **98.54** \\   

Table 4: **QA accuracy (%) on the test set of TabMWP**. We report the number of tuned parameters for this task and the overall accuracy, and accuracy of different question types, including free-text questions, multi-choice questions, integer answers, decimal answers, extractive answers, Boolean answers, other text answers, grades 1-6, and grades 7-8. * refers to a subset of results.

Figure 3: Results of main baselines and Chameleon. Dashed lines represent human performance.

only relies on symbolic programming tools like "Program Generator" and "Program Executor". **Chameleon** (ChatGPT) outperforms ChatGPT CoT and ChatGPT PoT by 11.25% and 3.79%, respectively, emphasizing the advantage of our enriched tool set. With GPT-4, **Chameleon** gains an additional 5.50%, reaching a 98.78% accuracy. Notably, **Chameleon** (GPT-4) surpasses Codex PoT-SC , the best-published model, by 17.0% and human performance by 8.56%.

### Qualitative Analysis

Tool use planning.The proportions of key tools called in the programs from **Chameleon** on ScienceQA and TabMWP are visualized in Figure 4 and Figure 5, respectively. Interestingly, ChatGPT and GPT-4 exhibit different planning behaviors. Generally, ChatGPT has a strong bias toward using or not using certain tools, highly influenced by in-context examples. For instance, ChatGPT calls "Knowledge Retrieval" in 72% of queries but only calls "Bing Search" in 3% of cases on ScienceQA; on TabMWP, ChatGPT heavily relies on "Row Lookup" (47%) but calls "Column Lookup" less frequently (4%). However, GPT-4 acts more _objectively_ and _rationally_ in tool selection. For example, GPT-4 calls "Knowledge Retrieval" more frequently (81% vs. 72%) and calls "Bing Search" more than ChatGPT (11% vs. 3%) when answering scientific questions on ScienceQA. Impressively, GPT-4 consistently calls "Query Generator" and "Bing Search" simultaneously by observing the tool usage descriptions, while ChatGPT lacks such reasoning capability.

Ablation study with disabled modules.We study the accuracy decline of **Chameleon** when key modules in the generated programs are disabled (Table 5), using ChatGPT as the underlying LLMs and 500 test examples. The results reveal that "Knowledge Retrieval" plays a vital role in both tasks. Domain-specific tools, such as the search engine and vision models for ScienceQA, and program tools for TabMWP, also prove to be important.

Module transitions.We visualize the transition graphs of modules for generated programs by **Chameleon** (GPT-4) on ScienceQA and TabMWP in Figure 7 and 8, respectively. The transition probabilities in these graphs are computed from the tool transitions observed on the test sets. These graphs show that the GPT-4 planner is able to make good decisions on how to sequence tools in a few-shot setup. For example, on ScienceQA, **Chameleon** often decides to rely on either "Knowledge Retriever" or "Bing Search", but rarely both. On TabMWP, we observe two main modes: either going through the solution generator module or via the program generator, verifier, and executor.

### Case Study

Visualization examples of ScienceQA.Examples from **Chameleon** (GPT-4) on ScienceQA are visualized in Figure 1. **Chameleon** (GPT-4) is able to adapt to different input queries by generating programs that compose various tools and executing them sequentially to obtain accurate responses. For instance, to answer the first question (1), _What is the direction of this push?_, the system calls the image captioner model to extract semantic information from the image and employs the knowledge retrieval model to gather background knowledge for multi-modal reasoning. In the second example (2), the natural language planner infers that a text detector tool is needed to understand the context

Figure 4: Tools called in the generated programs from **Chameleon** on ScienceQA.

Figure 5: Tools called in the generated programs from **Chameleon** on TabMWP.

   Module & \(\) (ScienceQA) \(\) (TabMWP) \\  Knowledge Retrieval & -7.8\% & -2.2\% \\ Bing Search & -7.4\% & - \\ Text Detector & -8.4\% & - \\ Image Captioner & -6.0\% & - \\ Program Generator & - & -7.4\% \\ Table Verbalizer & - & -0.2\% \\   

Table 5: Score drop with disabled modules.

of the ad. The third query (3; more details provided in Figure 9 in the appendix), _Which animal's skin is adapted for survival in cold places?_, involves scientific terminology related to animal survival. The planner decides to call the Bing search engine to access domain-specific knowledge, benefiting from the numerous online resources.

Visualization examples of TabMWP.The adaptability and versatility of Chameleon for various queries are also observed on TabMWP, as illustrated in the examples in Figure 2. The first example (1) involves mathematical reasoning on a tax form. Chameleon (1) calls the knowledge retrieval model to recall basic knowledge that assists in understanding this domain-specific table, (2) describes the table in a more readable natural language format, and (3) finally relies on program-aided tools to perform precise computations. In the second example (3), the system generates Python code that closely aligns with the background knowledge provided by the knowledge retrieval model. The third example (3) requires the system to locate the cell in a large tabular context given the input query. Chameleon calls the row lookup model to help accurately locate the relevant rows and generate the language solution via an LLM model, instead of relying on program-based tools.

Failure cases and limitations.Failure examples from Chameleon (GPT-4) are illustrated in Tables 19 to 24 in the appendix. Inaccurate responses may arise from the limitations of the current modules or from suboptimal programs generated by the planner. Additionally, the module inventory may lack tools capable of addressing specific abilities. Future directions could involve upgrading the modules and the planner, or expanding the module inventory to support a broader range of capabilities. Further limitations and broader impacts are respectively discussed in sections B and C of the appendix.

### Error Analysis

To examine the error sources of the base large language models and understand how our model reduces mistakes from different aspects, we conduct an error analysis, as shown in Figure 6. We select 50 mistake examples from the ChatGPT baseline on ScienceQA as the evaluation set. We count the number of mistake examples and analyze their corresponding mistake type categories for ChatGPT, our Chameleon (ChatGPT) approach, and Chameleon (GPT-4).

The results show that our Chameleon approach can substantially reduce the number of mistakes compared to ChatGPT. Our model features tools for image captioning and knowledge retrieval, thus the mistakes made by ChatGPT in the category of image understanding are reduced to 10 and 19 from 32 by Chameleon (ChatGPT) and Chameleon (GPT-4); while the mistakes made by ChatGPT in the category of knowledge understanding are reduced to 6 and 3 from 37 by Chameleon (ChatGPT) and Chameleon (GPT-4). Benefiting from the sequential execution of tools, the mistakes caused by solution generation are significantly reduced as well. Additionally, we find that the task planning of GPT-4 outperforms ChatGPT by a large margin.

## 6 Conclusion

In conclusion, we introduce a novel _plug-and-play compositional_ reasoning framework, Chameleon, that addresses the limitations of current large language models by augmenting them with external tools in a plug-and-play manner. Our approach employs a diverse set of tools and demonstrates impressive adaptability and effectiveness on two challenging benchmarks, ScienceQA and TabMWP. By achieving significant improvements in accuracy over existing state-of-the-art models, Chameleon showcases its potential for addressing real-world queries across various domains.

Figure 6: # of mistake examples in different categories on ScienceQA. Image: image captioning, Knowledge: knowledge understanding, Solution: solution generation.