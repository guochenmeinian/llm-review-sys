# Adversarial Attacks on Online Learning to Rank with Click Feedback

Jinhang Zuo\({}^{1,2}\) Zhiyao Zhang\({}^{3}\) Zhiyong Wang\({}^{4}\) Shuai Li\({}^{5}\)

Mohammad Hajiesmaili\({}^{1}\) Adam Wierman\({}^{2}\)

\({}^{1}\)University of Massachusetts Amherst \({}^{2}\)California Institute of Technology

\({}^{3}\)Southeast University \({}^{4}\)The Chinese University of Hong Kong \({}^{5}\)Shanghai Jiao Tong University

{jhzuo,hajiesmaili}@cs.umass.edu muirheadzhang@gmail.com

zywang21@cse.cuhk.edu.hk shuaili8@sjtu.edu.cn adamw@caltech.edu

Corresponding author.

###### Abstract

Online learning to rank (OLTR) is a sequential decision-making problem where a learning agent selects an ordered list of items and receives feedback through user clicks. Although potential attacks against OLTR algorithms may cause serious losses in real-world applications, there is limited knowledge about adversarial attacks on OLTR. This paper studies attack strategies against multiple variants of OLTR. Our first result provides an attack strategy against the UCB algorithm on classical stochastic bandits with binary feedback, which solves the key issues caused by bounded and discrete feedback that previous works cannot handle. Building on this result, we design attack algorithms against UCB-based OLTR algorithms in position-based and cascade models. Finally, we propose a general attack strategy against any algorithm under the general click model. Each attack algorithm manipulates the learning agent into choosing the target attack item \(T-o(T)\) times, incurring a cumulative cost of \(o(T)\). Experiments on synthetic and real data further validate the effectiveness of our proposed attack algorithms.

## 1 Introduction

Online learning to rank (OLTR) has been extensively studied  as a sequential decision-making problem where, in each round, a learning agent presents a list of items to users and receives implicit feedback from user interactions. One of the most common forms of feedback considered in literature is in the form of user clicks . OLTR with such click feedback can lead to major improvements over traditional supervised learning to rank methods . However, there is a security concern that user-generated click feedback might be generated by malicious users with the goal of manipulating the learning agent. Understanding the vulnerability of OLTR under adversarial attacks plays an essential role in developing effective defense mechanisms for trustworthy OLTR.

There has been a surge of interest in adversarial attacks on multi-armed bandits . For example,  shows that, for stochastic bandits, it is possible to manipulate the bandit algorithm into pulling a target arm very often with sublinear cumulative cost. Though it generally follows the bandit formulation, it differs from stochastic bandits in the action space and feedback model. More specifically, OLTR chooses a list of \(K\) out of \(L\) arms, instead of just one arm, to play in each round; the realized rewards of the chosen arms are usually censored by a click model, e.g., position-based  or cascade model . Thus, it is nontrivial to design efficient adversarial attacks on the censored feedback of the chosen arms for OLTR.

Moreover, previous works  can only handle unbounded and continuous reward feedback, while the (binary) click feedback of OLTR is bounded and discrete. Such binary feedback brings newchallenges to the attack design against OLTR algorithms. Since the post-attack feedback must also be binary, this scenario introduces a new problem of deciding whether to attack when the output attack value from previous algorithms is between \(0\) and \(1\): a simple rounding up might be costly, while skipping the attack may lead to undesired non-target arm pulls. Furthermore, the attack value computed by previous attack algorithms can be larger than \(1\), which is higher than the maximum attack value in the click model. In other words, in the bounded and discrete model, it is impossible to always find a feasible attack value to ensure the required conditions for their theoretical guarantees.

**Contributions.** In this paper, we propose the first study of adversarial attacks on OLTR with click feedback, aiming to overcome the new issues raised by the OLTR click model as well as the binary feedback. Table 1 summarizes our proposed attack algorithms with their theoretical guarantees. Since the click feedback itself complicates the attack design, we first consider adversarial attacks on stochastic \(L\)-armed bandits with Bernoulli rewards where the feedback from the chosen arm is always binary. We propose an attack algorithm that can mislead the well-known UCB algorithm  to pull the target arm \(L\) for \(N_{L}(T)\) times in \(T\) rounds, with a cumulative cost \(C(T)\) in the order of \( T\) asymptotically. Based on this approach, we study the two most popular click models of OLTR, position-based model  and cascade model , and propose attack strategies against UCB-type OLTR algorithms. Their cumulative costs depend on \(_{K}\) and \(p^{*}\), which are instance-dependent parameters of the position-based and cascade models, respectively. Lastly, we introduce the threat model for OLTR with a general click model and design an attack algorithm that can misguide arbitrary OLTR algorithms using click feedback. Our technical contributions are summarized as follows.

1. We propose the new idea of _conservative estimation_ of the target arm for attacking UCB in stochastic bandits with binary feedback, which resolves the issues caused by the bounded and discrete requirements of the post-attack feedback. This approach is also the backbone of other attack strategies in more complicated OLTR settings.
2. The PBM-UCB algorithm uses _position bias-corrected counters_ rather than simple click counters to compute UCB indices; we provide the theoretical analysis of our attack algorithm against PBM-UCB by carefully treating these unusual counters.
3. The _partial feedback_ of OLTR with the cascade click model brings a new challenge to the attack design. We provide a new _regret-based analysis_ of our attack algorithm against CascadeUCB without suffering from the partial feedback issue.
4. We devise a general attack strategy based on a new _probabilistic attack design_. It can successfully attack arbitrary OLTR algorithms without knowing the details of the algorithm.

We also conduct experiments on both synthetic and real-world data to evaluate our proposed attack strategies. Experimental results show that they can effectively attack the corresponding OLTR algorithms with less cost compared to other baselines. Due to space constraints, proofs and empirical results are included in the appendix.

**Related Work**. Online learning to rank with different feedback models has attracted much attention in recent years. Though there are other types of feedback models such as top-\(K\) feedback , click feedback has been widely used in literature.  consider the position-based model (PBM), where each position of the item list has an examination probability known or unknown by the learning agent. The cascade model in  considers the setting where the user would check the recommended items sequentially and stop at the first clicked one; all items after the clicked item will not be examined. The dependent click model (DCM) is a generalization of the cascade model where the user may click on multiple items . There are also general click models 

  
**Setting** & **Attack against** & \(N_{L}(T)\) & \( C(T)/ T\) \\  \(L\)-armed bandits & _UCB_ & \(T-O((L-1)(^{2}} T))\) & \(O(_{<L}+_{0}}{_{0}^{2}})\) \\ Position-based model & _PBM-UCB_ & \(T-O((L-K)(}{_{K}_{0}^{2}}  T))\) & \(O(_{<L}(_{}+_{0})}{ _{K}^{2}_{0}^{2}})\) \\ Cascade model & _CascadeUCB_ & \(T-O((L-K)(_{0}^{2}} T))\) & \(O(_{<L}+_{0}}{_{0}^{2}})\) \\ General model & _Arbitrary_ & \(T-O( T)\) & \(O(_{<L}(_{}+4(1)))\) \\   

* \(_{0}\): parameter of the attack algorithm; \(_{a}\): mean gap between arm \(a\) and \(L\); \(\): a decreasing function in Section 3.2

Table 1: Summary of the settings and proposed attack strategiesâ€ that can cover some of the previous click models. In this paper, we mainly focus on attack design on PBM and cascade models; both of them adopt bounded and discrete click feedback thus require a new design other than previous works like  that can only handle unbounded and continuous feedback.

Adversarial attacks on different types of multi-armed bandit problems have been studied recently [9; 10; 23; 24; 25].  proposes the first study of adversarial attacks on the classical stochastic bandit problem. It designs effective attack strategies against the \(\)-Greedy and UCB algorithms.  extends it to a more general setting, where the algorithm of the learning agent can be unknown.  studies adversarial attacks on linear contextual bandits where the adversarial modifications can be added to either rewards or contexts. To the best of our knowledge, we are the first to study adversarial attacks on OLTR where, in addition to the challenges raised by click feedback, the combinatorial action space and the censored feedback of OLTR make it non-trivial to design efficient attack strategies.

## 2 Preliminaries

In this section, we briefly discuss the three problem settings we consider in this paper.

**Stochastic \(L\)-armed bandit.** We consider an arm set \([L]=\{1,2,,L\}\), with \(_{i}\) as the expected reward of arm \(i\). Without loss of generality, we assume \(_{1}_{2}_{L}\). In round \(t\), the player chooses an arm \(a_{t}[L]\) to play and receives a reward \(r_{t}^{0}\) as feedback. In the click feedback setting, the realized reward \(r_{t}^{0}\{0,1\}\) of arm \(a_{t}\) is sampled from a Bernoulli distribution with expectation \(_{a_{t}}\). The player aims to find an optimal policy to maximize the long-term cumulative reward.

**OLTR with position-based model **. This setting also considers the item (arm) set \([L]\) where \(_{i}\) represents the click probability of item \(i\). However, in OLTR with the position-based model (PBM), in each round \(t\), the player chooses an ordered list of \(K\) items, \(_{t}=(a_{1,t},,a_{K,t})\), with known examination probability \(_{k}\) for the \(k\)-th position in the list (assuming \(_{1}_{K}\)). The player then observes the click feedback of the chosen list from the user, denoted as \(_{t}^{0}=(r_{1,t}^{0},,r_{K,t}^{0})\{0,1\}^{K}\), where \(r_{i,t}^{0}\) is sampled from a Bernoulli distribution with expectation \(_{i}_{a_{i,t}}\). The reward obtained by the player is the sum of the clicks, i.e., \(_{k=1}^{K}r_{k,t}^{0}\). The goal of the player is to find an optimal policy that can maximize the long-term cumulative user clicks.

**OLTR with cascade model **. Here, we consider the same item set \([L]\) as that in the PBM model. For OLTR with cascade model, in each round \(t\), the player chooses an ordered list of \(K\) items, \(_{t}=(a_{1,t},,a_{K,t})\), for the user. The user then checks the list from \(a_{1,t}\) to \(a_{K,t}\), with probability \(_{a_{k,t}}\) to click the \(k\)-th item. She immediately stops at the first clicked item, and returns the click result back to the player. We denote the position of the first clicked item as \(_{t}\) (\(_{t}=\) if no item was clicked). The click feedback of the player is \(_{t}^{0}=(r_{1,t}^{0},,r_{K,t}^{0})\{0,1\}^{K}\), where only \(r_{_{t,t}}^{0}=1\) and \(r_{k,t}^{0}=0\) for all \(k_{t}\). The reward obtained by the player is again the sum of user clicks \(_{k=1}^{K}r_{k,t}^{0}\), but in the cascade model, it is at most \(1\). The goal of the player is also to find an optimal policy that can maximize the long-term cumulative user clicks.

## 3 Attacks on Stochastic L-armed Bandits with Binary Feedback

As mentioned in the introduction, one main challenge of attacking OLTR algorithms comes from the binary click feedback: such binary feedback limits the possible actions of the attacker since they need to ensure the post-attack reward feedback is still valid (binary). This is a common issue for all OLTR with click feedback. Hence, in this section, we focus on adversarial attacks on the \(L\)-armed bandit problem with binary feedback. We propose an attack algorithm against the UCB algorithm, which is the backbone of the attack strategies for more complicated OLTR settings.

### Threat Model

We first introduce the threat model for the \(L\)-armed bandit problem with binary feedback. In each round \(t\), the player chooses an arm \(a_{t}[L]\) to play. The environment generates the pre-attack reward feedback \(r_{t}^{0}\{0,1\}\) based on a Bernoulli distribution with mean \(_{a_{t}}\). The attacker then observes \(a_{t},r_{t}^{0}\), and decides the post-attack feedback \(r_{t}\{0,1\}\). The player only receives \(r_{t}\) as the feedback and uses that to decide the next arm to pull, \(a_{t+1}\), for round \(t+1\). Without loss of generality, we assume arm \(L\) is a sub-optimal _target_ arm. The attacker's goal is to misguide the player to pull the target arm \(L\) very often while using small attack costs. Let \(N_{i}(t)\) denote the number of pulls of arm \(i\) up to round \(t\). We say the attack is successful after \(T\) rounds if \(N_{L}(T)=T-o(T)\) in expectation or with high probability, while the cumulative attack cost \(C(T)=_{t=1}^{T}|r_{t}-r_{t}^{0}|=o(T)\).

### Attack Algorithm against UCB

For a better illustration of the attack strategy, we first define the following auxiliary notations. Let \(_{i}(t):=\{s:s t,a_{s}=i\}\) be the set of rounds up to \(t\) when arm \(i\) is played. We denote the pre-attack average reward of arm \(i\) up to round \(t\) by \(_{i}^{0}(t):=N_{i}(t)^{-1}_{s_{i}(t)}r_{s}^{0}\). Lastly, let \(_{i}(t):=N_{i}(t)^{-1}_{s_{i}(t)}r_{s}\) be the post-attack average reward of arm \(i\) up to round \(t\).

As in , we consider attacks against the \((,)\)-UCB algorithm from , where \(=4.5\) and \(:^{2}/8\) since Bernoulli random variables are \(1/4\)-sub-Gaussian. The original attack algorithm in  calculates an attack value \(_{t}\) for round \(t\) such that

\[_{a_{t}}(t)_{L}(t-1)-2(N_{L}(t-1))-_{0},\] (1)

where \((N):=LN^{2}}{3}};_{0}>0\) and \(>0\) are the parameters of the attack algorithm. The attacker then gives the post-attack reward \(r_{t}=r_{t}^{0}-_{t}\) back to the player. The main idea of the attack algorithm in  is to compute the attack value so that the post-attack empirical estimates of the non-target arms are always less than those of the target arm.

However, this attack design only works when the reward space is unbounded and continuous, while in the threat model with binary feedback, the Bernoulli rewards are _discrete_ and _bounded_. There are two issues raised by the discrete and bounded requirements. First, the calculated attack value \(_{t}\) is a real number, which may not make the post-attack reward feedback \(r_{t}\) to be valid (binary). Second, in order to ensure that Eq. (1) is true, the calculated \(_{t}\) might be larger than 1, which is higher than the maximum attack value in our threat model. In other words, it is impossible to let Eq. (1) hold for all rounds, while such a guarantee was essential for the theoretical analysis in . To overcome these issues, we propose a new attack algorithm against UCB on \(L\)-armed bandits with binary feedback. It is described in Algorithm 1. The algorithm maintains timestamp \(h_{a}(t)\) for each arm \(a\). In round \(t\), if the arm pulled by the player, \(a_{t}\), is not the target arm \(L\), it first checks the condition \(_{t} r_{t}^{0}\) (Line 6), with \(_{t}\) computed as

\[_{t}=[N_{a_{t}}(t)_{a_{t}}^{0}(t)-_{s_{a_{t}}(t -1)}_{s}-N_{a_{t}}(t)[_{L}(t)-_{0}]_{+} ]_{+},\] (2)

where \([z]_{+}=(0,z)\) and \(_{L}(t):=_{L}(t)-2(N_{L}(t))\). In fact, condition \(_{t} r_{t}^{0}\) is equivalent to checking whether there exists a feasible \(_{t}\) to ensure Eq. (1) holds: if \(_{t} r_{t}^{0}\), with \(_{t}\) set to be \(_{t}\) (Line 7), Eq. (1) will hold in round \(t\). Using a similar analysis as in , we can derive an upper bound of \(N_{a_{t}}(t)\) and prove the success of the attack up to round \(t\). The algorithm also updates the timestamp \(h_{a_{t}}(t)=t\) for \(a_{t}\) (line 7). If \(_{t}>r_{t}^{0}\), it indicates that there is no feasible \(_{t}\) that can ensure Eq. (1). Instead, the algorithm sets \(_{t}=_{t}\) with \(_{t}\) computed as

\[_{t}=[N_{a_{t}}(t)_{a_{t}}^{0}(t)-_{s_{a _{t}}(t-1)}_{s}-N_{a_{t}}(t)[_{L}(h_{a_{t}}(t-1))- _{0}]_{+}]_{+},\] (3)

where \(h_{a_{t}}(t-1)\) records the last round that Eq. (1) was satisfied. We can prove such an \(_{t}\) is always feasible (\(_{t} r_{t}^{0}\)) and it can ensure that

\[_{a_{t}}(t)_{L}(h_{a_{t}}(t-1))-2(N_{L}(h_{a_{t}}(t -1)))-_{0}.\] (4)

This new inequality always holds for all rounds, which helps guarantee the success of the attack.

**Remark**. Compared with Eq. (1), Eq. (4) uses a more conservative lower bound of \(_{L}\), \(_{L}(h_{a_{t}}(t-1))\), instead of \(_{L}(t)\), on the right-hand side of the inequality. We refer to this as _conservative estimation_ for the target arm \(L\) with respect to arm \(a_{t}\), where the estimated lower bound of \(L\) will only be updated when there exists feasible \(_{t}\) to ensure Eq. (1). Intuitively, it makes the attack algorithm more conservative and less likely to use a small attack value that may result in an invalid large attack value in later rounds. We use an inductive proof to show that there always exists \(_{t}\) such that Eq. (4) holds while keeping \(r_{t}\) valid (binary). Therefore, conservative estimation addresses the problems posed by binary feedback. This forms the foundation for the attack algorithms tailored for OLTR with click feedback.

### Analysis

We first show that our attack algorithm always returns valid binary feedback to the player.

**Lemma 1**.: _The post-attack feedback of Algorithm 1 is always valid, i.e., \(r_{t}\{0,1\}\) for any \(t\)._

The proof of Lemma 1, which uses an inductive analysis on \(_{t},_{t}\), can be found in the appendix.

Define \(_{a}:=_{a}-_{L}\). We give the following theorem to show the successful attack of Algorithm 1.

**Theorem 1**.: _Suppose \(T L\) and \( 1/2\). With probability at least \(1-\), Algorithm 1 misguides the UCB algorithm to choose the target arm \(L\) at least \(T-(L-1)(1+^{2}} T)\) rounds, using a cumulative attack cost at most_

\[C(T)(1+^{2}} T)_{a<L}(_{ a}+_{0}+4(1+^{2}} h_{a}(T)) ).\]

_As \(T\) goes to infinity, we have_

\[_{T} O(_{a<K}+_{0}}{_{0}^{2}}).\]

Compared with Theorem 2 in , the \(\) term in our cost bound depends on \( h_{a}(T)\) instead of \( T\). Since \(\) is a decreasing function and \(h_{a}(T) T\), our finite-time cost bound can be larger than that in . However, our asymptotic analysis of the cost suggests that when \(T\) is large, such difference becomes negligible. Notice that the attack algorithm in  does not have any theoretical guarantee in the binary feedback setting, so this comparison is only meant to show the additional cost potentially caused by the conservative estimation.

## 4 Attacks on Online Learning to Rank

We now move to developing effective attack algorithms for more complicated OLTR settings. Since attacking OLTR also faces the challenges caused by binary click feedback, the attack algorithms in this section rely on our attack design for stochastic bandits with binary feedback in Section 3.

### Position-Based Click Model

**Threat Model**. We introduce the threat model for online stochastic ranking with position-based click feedback. In each round \(t\), the player chooses a list of \(K\) item, \(_{t}=(a_{1,t},,a_{K,t})\) to recommend. The environment generates the pre-attack click feedback \(_{t}^{0}=(r_{1,t}^{0},,r_{K,t}^{0})\{0,1\}^{K}\) where is sampled from a Bernoulli distribution with mean \(_{i}_{a_{i,t}}\). The attacker then observes \(_{t}\) and \(_{t}^{0}\), and decides the post-attack click feedback \(_{t}\{0,1\}^{K}\). The player only receives \(_{t}\) as the feedback and uses it to decide the next list to recommend, \(_{t+1}\), for round \(t+1\). Without loss of generality, we assume item \(L\) is a sub-optimal target item. Similar to Section 3.1, we say the attack is successful after \(T\) rounds if the number of target item recommendations is \(N_{L}(T)=T-o(T)\) in expectation or with high probability, while the cumulative attack cost \(C(T)=_{t=1}^{T}||_{t}-_{t}^{0}||_{1}=o(T)\).

**Attack against PBM-UCB**. We consider the PBM-UCB algorithm in  as the online ranking algorithm of the player, which computes the UCB index of each item \(a\) as

\[_{a}(t)=_{a}(t-1)+B_{a}(t)=_{a}(t-1)+(t-1)(1+) t}{2_{a}(t-1)^{2}}},\] (5)

where \(_{a}(t):=_{s=1}^{t}_{i=1}^{K}_{i}I\{a_{i,s}=a\}\) is the position bias-corrected counter, \(_{a}(t)\) is the empirical mean of item \(a\), and \(\) is a parameter of the algorithm. The algorithm then chooses the corresponding first \(K\) items with the highest UCB indices as the recommendation list.

We propose our attack algorithm against PBM-UCB in Algorithm 2. It works by first randomly taking \(K-1\) items out and making them a set with the target item \(L\), denoted as \(^{*}=\{a_{1}^{*},,a_{K-1}^{*},L\}\). Then, based on the conservative estimation idea from Algorithm 1, it maintains a timestamp \(h(l,a)\) for each item \(l\) with respect to each \(a^{*}\). The intuition is that, to ensure a similar inequality as Eq. (4) for all rounds, we need to make _conservative estimation_ on the lower bounds of \(_{a}\) for all \(a^{*}\). This is handled by Algorithm 3, which maintains the timestamps \(h_{l,a}(t)\) for the input item \(l\) and outputs the appropriate attack value \(\) on \(l\) that can always ensure the required inequality. The value of parameters \(_{t}(l,a)\) and \(_{t}(l,a)\) in Algorithm 3 are computed as

\[_{t}(l,a)=[N_{l}(t)_{l}^{0}(t)-_{s_{l}(t-1)} _{l}(s)-N_{l}(t)[_{a}(t)-_{0}]_{+} ]_{+},\] (6)

\[_{t}(l,a)=[N_{l}(t)_{l}^{0}(t)-_{s_{l} (t-1)}_{l}(s)-N_{l}(t)[_{a}(h_{l,a}(t-1))-_{0} ]_{+}]_{+}.\] (7)Notice that Algorithm 2 could also work when there are more than one but less than \(K+1\) target arms (the goal of the attacker becomes misguiding the player to recommend all target arms very often with sublinear cost). The only modification required is to put all of these target arms into \(^{*}\).

**Analysis**. The following theorem shows the attack against PBM-UCB is successful.

**Theorem 2**.: _Suppose \(T L\) and \( 1/2\). With probability at least \(1-\), Algorithm 2 misguides the PBM-UCB algorithm to recommend the target item \(L\) at least \(T-(L-K)(^{2}_{0}^{2}} T)\) rounds, using a cumulative attack cost at most_

\[C(T)(^{2}_{0}^{2}} T) _{a<L}(_{a}+_{0}+4( ^{2}_{0}^{2}} h_{a,L}(T))).\]

_When \(T\) goes to infinity, we have_

\[_{T} O(_{<K}+_{0})}{_{K}^{2}_{0}^{2}}).\]

Proof sketch.: Whenever \(a_{i,t}^{*}\) is chosen by the player, there must exist \(a^{*}\) such that \(_{a_{i,t}}(t)_{a}(t)\). The output \(_{i,t}\) of Algorithm 3 would ensure \(_{a_{i,t}}(t)_{a}(h_{a_{i,t},a}(t-1))-2(N_{a}(h_{a_ {i,t},a}(t-1)))-_{0}\), owing to the conservative estimations in \(_{t}(a_{i,t},a)\) and \(_{t}(a_{i,t},a)\). Combining these two inequalities, we can get \(B_{a_{i,t}}(t)-B_{a}(t)_{0}\). With a careful calculation on this inequality and the involved bias-corrected counters, we have \(N_{a_{i,t}}(t)(^{2}_{0}^{2}} t)\). This result holds for any \(a_{i,t}^{*}\). Thus, we immediately get the bound of \(N_{L}(t)\). The remaining proof for the cost bound will be similar to that of Theorem 1.

Compared with Theorem 1, the asymptotic cost of Algorithm 2 has an additional dependency on \(1/_{K}^{2}\), which suggests it may require more cost to achieve a successful attack in the PBM model, though the cost dependency on \(T\) is still logarithmic.

### Cascade Click Model

**Threat Model**. We introduce the threat model for OLTR with cascade click feedback. In each round \(t\), the player chooses a list of \(K\) items, \(_{t}=(a_{1,t},,a_{K,t})\) to recommend. The environment generates the pre-attack click feedback \(_{t}^{0}=(r_{1,t}^{0},,r_{K,t}^{0})\{0,1\}^{K}\), \(\|_{t}^{0}\| 1\). Let \(_{t}\) denote the position of the clicked item, i.e., \(r_{_{t},t}^{0}=1\) (\(_{t}=\) if no item was clicked). The attacker observes \(_{t},_{t}^{0},_{t}\), and decides the post-attack click feedback \(_{t}\{0,1\}^{K}\), \(\|_{t}\|_{1} 1\). The player only receives \(_{t}\) as the feedback and uses it to decide the next list to recommend, \(_{t+1}\), for round \(t+1\). The goal of the attacker in this setting is the same as that in the PBM model.

**Attack against CascadeUCB.** We propose an attack algorithm against CascadeUCB in Algorithm 4. Similar to the attack against PBM-UCB, it first randomly generates a set of items \(^{*}=\{a_{1}^{*},,a_{K-1}^{*},L\}\). It also follows the idea of conservative estimation: when the clicked item \(a_{_{t},t}\) does not belong to \(^{*}\), it calls Algorithm 3 to maintain \(h_{a_{_{t},t},a}\) for all \(a^{*}\) and compute the attack value \(_{_{t},t}\) based on the conservative estimations. If the output \(_{_{t},t}=1\), which meansthe algorithm sets the clicked position to be zero, the player will keep checking the positions after \(_{t}\). Since the pre-attack feedback of all items after position \(_{t}\) is zero, we need to find the first item \(a_{i,t}^{*}\) after position \(_{t}\) and set \(_{i,t}=-1\) (\(r_{i,t}=1\)). The empirical means of the items between position \(_{t}\) and \(i\) that are not in \(^{*}\) decreases, and the empirical mean of \(a_{i,t}^{*}\) increases. Thus, it does not affect the success of the attack while still making the post-attack feedback \(_{t}\) valid.

**Analysis**. First, note that there is a mismatch between recommendation and observation in the cascade model: for all \(i\) such that \(_{t}<i K\), \(a_{i,t}\) is recommended but not observed, i.e., there is no new click sample for item \(a_{i,t}\) for estimation. We can still follow a similar proof of Theorem 2 to get the upper bound of the number of observations (samples) for \(a_{i,t}^{*}\), but it is less than the number of recommendations and thus cannot ensure the success of the attack. To tackle this problem, we use a new regret-based analysis on the expected number of recommendations. We can also prove that the post-attack feedback of Algorithm 4 is always valid. Define \(p^{*}:=_{i=1}^{K-1}_{i}\). We give the following theorem of the successful attack against CascadeUCB.

**Theorem 3**.: _Suppose \(T L\) and \( 1/2\). With probability at least \(1-\), Algorithm 2 misguides the CascadeUCB algorithm to choose the target arm at least \(T-(L-K)(_{0}^{2}} T)\) rounds in expectation. Its cumulative attack cost is at most_

\[C(T)(1+^{2}} T)_{a<K}(_{ a}+_{0}+4(1+^{2}} h_{a,L}(T)) ).\]

_As \(T\) goes to infinity, we have_

\[_{T} O(_{a<K}+ _{0}}{_{0}^{2}}).\]

Proof sketch.: As mentioned above, we use a new regret-based analysis. The intuition is that the regret caused by any suboptimal item is the product of its reward gap with respect to the optimal list and its expected number of recommendations. If we know the regret upper bound and the reward gap lower bound, we can derive the upper bound of the expected number of recommendations. To do so, we first show that the post-attack problem can be viewed as a problem with a known reward gap lower bound. This can be obtained by \(_{a_{i,t}(t)}_{L}(h_{L_{i,t},a}(t-1))-2(N_{a}(h_{a _{i,t},L}(t-1)))-_{0}\) for any \(a_{i,t}^{*}\), which indicates the post-attack expected reward gap between \(a_{i,t}\) and \(L\) is always larger than \(_{0}\). Then, the lower bound of the reward gap of \(a_{i,t}\) with respect to the optimal list is \(p^{*}_{0}\). Based on the regret upper bound \(12 T/_{0}\) given in , we can get the upper bound of the expected number of recommendations \(_{0}^{2}} T\). For the remaining cost analysis, since the attack cost only depends on the observed items, we can still follow the proof of Theorem 2.

### General Attacks on OLTR with General Click Model

We have provided attack strategies against UCB-based OLTR algorithms under two specific click models. A natural follow-up question is whether there exists an attack strategy that can attack any OLTR algorithm under the general click model. To answer this question in what follows, we design an attack strategy that can misguide any OLTR algorithm without knowing the underlying algorithm. However, it may pay more cost (still sublinear) than the others since it cannot take advantage of the details of the algorithm to make fine-tuned adjustments.

**Threat Model**. We consider the threat model for OLTR with general click feedback. In each round \(t\), the player chooses a list of \(K\) items, \(_{t}=(a_{1,t},,a_{K,t})\) to recommend. The environment generates the pre-attack click feedback \(_{t}^{0}=(r_{1,t}^{0},,r_{K,t}^{0})\{0,1\}^{K}\), \(_{t}^{0}_{c}\), where \(_{c}\) is the feasible feedback space of click model \(c\). The attacker observes \(_{t},_{t}^{0}\), and decides the post-attack click feedback \(_{t}\{0,1\}^{K}\), \(_{t}_{c}\). The attack should be aware of \(_{c}\); otherwise, ensuring valid post-attack feedback is impossible. The player only receives \(_{t}\) as the feedback and uses it to decide the next list to recommend, \(_{t+1}\). The goal of the attacker is the same as that in the PBM model.

**General Attack against Arbitrary OLTR Algorithm**. We propose an attack algorithm against arbitrary OLTR algorithms in Algorithm 5. Similar to the attack against PBM-UCB, it first randomly generates a set of items \(^{*}=\{a_{1}^{*},,a_{K-1}^{*},L\}\). In each round, for each clicked item \(a_{i,t}^{*}\), the algorithm calculates an attack probability \(p_{i,t}\) and uses that to decide whether its feedback needs to be changed to unclicked (\(r_{i,t}=0\)). We prove that \(p_{i,t}\) is actually an estimated upper bound of \(_{i}/_{i}\), thus by such probabilistic feedback perturbations, the algorithm makes all items outside \(^{*}\) be worse than the items inside \(^{*}\), which guarantees the success of the attack. We consider a general assumption for the OLTR algorithm to be attacked.

**Assumption 1**.: _The OLTR algorithm chooses suboptimal items no more than \(R(T)=o(T)\) times for \(T\) rounds,_

Notice that both PBM-UCB and CascadeUCB satisfy the assumption with \(R(T)=O( T)\). We give the following theorem to show the success of our general attack algorithm.

**Theorem 4**.: _Suppose \(T L\) and \( 1/2\). With probability at least \(1-\), Algorithm 5 misguides arbitrary OLTR algorithm that satisfies Assumption 1 to choose the target item at least \(T-R(T)\) rounds in expectation. Its cumulative attack cost is at most_

\[C(T) O(_{a<L}(_{a}+4(1))R(T)).\]

Compared with the results in the PBM and cascade models, the \(()\) term in the cumulative cost of Algorithm 5 is \((1)\), which can be much larger than the others. Also, the asymptotic costs of Algorithm 2 and Algorithm 4 is independent of \(\), while the asymptotic cost of Algorithm 5 depends on \(\), showing that Algorithm 5 is more costly than attack strategies specific to OLTR algorithms.

## 5 Experiments

We conduct experiments using both synthetic and real data (MovieLens 20M dataset ). Due to space limitations, we report only the results of OLTR using the position-based model. We use \(=0.1\) for the PBM-UCB algorithm. For the synthetic data, we take \(L=16,K=8,T=100,000\); \(\{_{i}\}_{i=1}^{L}\) are sampled from uniform distribution \(U(0,1)\) for Figure 0(a), and from \(U(0,x)\) for Figure 0(b). For the real data, we take \(L=100,K=10,T=100,000\); \(\{_{i}\}_{i=1}^{L}\) are extracted according to .

Using synthetic data, we first study how the algorithm's performance is influenced by the algorithmic and problem parameters. As shown in Figure 0(a), the cost decreases with an increase in \(_{0}\), aligning with our observations in Theorem 2. Figure 0(b) shows that the cost increases as \(x\) increases, which suggests that our algorithm pays more costs when \(_{a}\) is large. We then compare our algorithm with two baselines: \(_{}\) attacks all arms except the target arm as long as the attack is valid; \(_{}\) first randomly takes \(K-1\) arms out to generate a set \(^{*}\) and then attacks all arms outside the set as long as the attack is valid. Figure 2 shows that \(_{}\) algorithm cannot successfully attack PBM-UCB even with linear costs. Our algorithm and \(_{}\) have similar performance on the chosen ratio of the target arm as shown in Figures 1(a) and 1(c), which is the chosen time of the target arm divided by the current round. However, our algorithm pays \(50\%\) and \(40\%\) less cost than \(_{}\) in Figures 1(b) and 1(d), respectively, which validates the necessity of our attack design.

## 6 Concluding Remarks

This paper presents the first study on adversarial attacks against OLTR with click feedback. We propose attack strategies that can successfully mislead various OLTR algorithms across multiple click models. One limitation of our work is that it cannot be applied to feedback models without clicks, such as the top-\(K\) feedback . Although we provide theoretical results for all these strategies, as discussed in Section 3.3, the finite-time cost results might be further improved by a more fine-grained analysis on \(h_{a}(t)\). Additionally, there is no known lower bound for the cumulative attack cost, even for the stochastic bandit setting in the literature, making it unclear whether our attack strategies are (asymptotically) order-optimal. This study opens up several future directions. One is to design attack strategies for other OLTR algorithms and feedback models. Lastly, our study on the vulnerability of existing OLTR algorithms inspires the design of robust OLTR algorithms against adversarial attacks.