# Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Deep discriminative approaches like random forests and deep neural networks have recently found applications in many important real-world scenarios. However, deploying these learning algorithms in safety-critical applications raises concerns, particularly when it comes to ensuring confidence calibration for both in-distribution and out-of-distribution data points. Many popular methods for in-distribution (ID) calibration, such as isotonic and Platt's sigmoidal regression, exhibit excellent ID calibration performance. However, these methods are not calibrated for the entire feature space, leading to overconfidence in the case of out-of-distribution (OOD) samples. On the other end of the spectrum, existing out-of-distribution (OOD) calibration methods generally exhibit poor in-distribution (ID) calibration. In this paper, we address ID and OOD calibration problems jointly. We leveraged the fact that deep models, including both random forests and deep-nets, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as partitioning rules of the feature space. We replace the affine function in each polytope populated by the training data with a Gaussian kernel. Our experiments on both tabular and vision benchmarks show that the proposed approaches obtain well-calibrated posteriors while mostly preserving or improving the classification accuracy of the original algorithm for ID region, and extrapolate beyond the training data to handle OOD inputs appropriately.

## 1 Introduction

Machine learning methods, specially deep neural networks and random forests have shown excellent performance in many real-world tasks, including drug discovery, autonomous driving and clinical surgery . However, calibrating confidence over the whole feature space for these approaches remains a key challenge in the field . Calibrated confidence within the training or in-distribution (ID) region as well as in the out-of-distribution (OOD) region is crucial for safety critical applications like autonomous driving and computer-assisted surgery, where any aberrant reading should be detected and taken care of immediately .

The approaches to calibrate OOD confidence for learning algorithms described in the literature can be roughly divided into two groups: discriminative and generative. Intuitively, the easiest solution for OOD confidence calibration is to learn a function that gives higher scores for in-distribution samples and lower scores for OOD samples . The discriminative approaches try to either modify the loss function  or train the network exhaustively on OOD datasets to calibrate on OOD samples .

Recently, Hein et al.  showed ReLU networks produce arbitrarily high confidence as the inference point moves far away from the training data. Therefore, calibrating ReLU networks for the whole OOD region is not possible without fundamentally changing the network architecture. As a result, all of the aforementioned algorithms are unable to provide any guarantee about the performance of the network throughout the whole feature space. The other group tries to learn generative models for thein-distribution as well as the out-of-distribution samples. The general idea is to do likelihood ratio test for a particular sample using the generative models , or threshold the ID likelihoods to detect OOD samples. However, it is not obvious how to control likelihoods far away from the training data for powerful generative models like variational autoencoders (VAEs)  and generative adversarial networks (GAN) . Moreover, Nalisnick et al.  and Hendrycks et al.  showed VAEs and GANs can also yield overconfident likelihoods far away from the training data.

The algorithms described so far are concerned with OOD confidence calibration for deep-nets only. However, we show that other approaches which partition the feature space, for example random forest, can also suffer from poor confidence calibration both in the ID and the OOD regions. Moreover, the algorithms described above are concerned about the confidence in the OOD region only and do not address the confidence calibration within the ID region at all. This issue is addressed separately in a different group of literature [15; 16; 17; 18; 19; 20]. Instead, we consider both calibration problems jointly and propose an approach that achieves good calibration throughout the whole feature space.

In this paper, we conceptualize both random forest and ReLU networks as partitioning rules with an affine activation over each polytope. We consider replacing the affine functions learned over the polytopes with Gaussian kernels. We propose two novel kernel density estimation techniques named _Kernel Density Forest_ (KDF) and _Kernel Density Network_ (KDN). Our proposed approach completely excludes the need for training on OOD examples for the model (unsupervised OOD calibration). We conduct several simulation and real data studies that show both KDF and KDN are well-calibrated for OOD samples while they maintain good performance in the ID region.

## 2 Related Works and Our Contributions

There are a number of approaches in the literature which attempt to learn a generative model and control the likelihoods far away from the training data. For example, Ren et al.  employed likelihood ratio test for detecting OOD samples. Wan et al.  modified the training loss so that the downstream projected features follow a Gaussian distribution. However, there is no guarantee of performance for OOD detection for the above methods. To the best of our knowledge, apart from us, only Meinke et al.  has proposed an approach to guarantee asymptotic performance for OOD detection. Compared to the aforementioned methods, our approach differs in several ways:

* We address the confidence calibration problem for both ReLU-nets and random forests.
* We address ID and OOD calibration problem as a continuum.
* We provide an algorithm for OOD confidence calibration for both tabular and vision datatsets whereas most of the existing methods are tailor-made for vision problems.
* We propose an unsupervised post-hoc OOD calibration approach.

## 3 Technical Background

### Setting

Consider a supervised learning problem with independent and identically distributed training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\) such that \((,Y) P_{X,Y}\), where \( P_{X}\) is a \(^{D}\) valued input and \(Y P_{Y}\) is a \(=\{1,,K\}\) valued class label. Let \(\) be the high density region of the marginal, \(P_{X}\), thus \(\). Here the goal is to learn a confidence score, \(:^{D}^{K}\), \(()=[g_{1}(),g_{2}(),,g_{K}( )]\) such that,

\[g_{y}()=P_{Y|X}(y|),& \\ P_{Y}(y),&, y \] (1)

where \(P_{Y|X}(y|)\) is the posterior probability for class \(y\) given by the Bayes formula:

\[P_{Y|X}(y|)=(|y)P_{Y}(y)}{_{k=1}^{K}P_{X| Y}(|k)P_{Y}(k)}, y.\] (2)

Here \(P_{X|Y}(|y)\) is the class conditional density which we will refer as \(f_{y}()\) hereafter for brevity.

### Main Idea

Deep discriminative networks partition the feature space \(^{d}\) into a union of \(p\) affine polytopes \(Q_{r}\) such that \(_{r=1}^{p}Q_{r}=^{d}\), and learn an affine function over each polytope [4; 21]. Mathematically, the unnormalized class-conditional density for the label \(y\) estimated by these deep discriminative models at a particular point \(\) can be expressed as:

\[_{y}()=_{r=1}^{p}(_{r}^{}+b_{r}) ( Q_{r}).\] (3)

For example, in the case of a decision tree, \(_{r}=\), i.e., decision tree assumes uniform distribution for the class-conditional densities over the leaf nodes. Among these polytopes, the ones that lie on the boundary of the training data extend to the whole feature space and hence encompass all the OOD samples. Since the posterior probability for a class is determined by the affine activation over each of these polytopes, the algorithms tend to be overconfident when making predictions on the OOD inputs. Moreover, there exist some polytopes that are not populated with training data. These unpopulated polytopes serve to interpolate between the training sample points. If we replace the affine activation function of the populated polytopes with Gaussian kernels and prune the unpopulated ones, the tail of the kernel will help interpolate between the training sample points while assigning lower likelihood to the low density or unpopulated polytope regions of the feature space. This results in better confidence calibration for the proposed modified approach.

### Proposed Approach

We will call the above discriminative approaches as the 'parent approach' hereafter. Consider the collection of polytope indices \(\) from the parent approach which are populated by the training data. We replace the affine functions over the populated polytopes with Gaussian kernels \((;_{r},_{r})\). For a particular inference point \(\), we consider the Gaussian kernel with the minimum distance from the center of the kernel to the corresponding point:

\[r_{}^{*}=*{argmin}_{r}\|_{r}-\|,\] (4)

where \(\|\|\) denotes a distance. As we will show later, the type of distance metric considered in Equation 4 highly impacts the performance of the proposed model. In short, we modify Equation 3 from the parent ReLU-net or random forest to estimate the class-conditional density (unnormalized):

\[_{y}()=}_{r}n_{ry}(;_{r},_{r})(r=r_{}^{*}),\] (5)

where \(n_{y}\) is the total number of samples with label \(y\) and \(n_{ry}\) is the number of samples from class \(y\) that end up in polytope \(Q_{r}\). We add a small constant to the class conditional density \(_{y}\):

\[_{y}()=_{y}()+.\] (6)

Note that in Equation 6, \( 0\) as the total training points, \(n\). The intuition behind the added constant will be clarified further later in Proposition 2. The confidence score \(_{y}()\) for class \(y\) given a test point \(\) is estimated using the Bayes rule as:

\[_{y}()=_{y}()_{Y}(y)}{_{k= 1}^{K}_{k}()_{Y}(k)},\] (7)

where \(_{Y}(y)\) is the empirical prior probability of class \(y\) estimated from the training data. We estimate the class for a particular inference point \(\) as:

\[=*{argmax}_{y}_{y}().\] (8)Model Parameter Estimation

### Gaussian Kernel Parameter Estimation

We fit Gaussian kernel parameters to the samples that end up in the \(r\)-th polytope. We set the kernel center along the \(d\)-th dimension:

\[_{r}^{d}=}_{i=1}^{n}x_{i}^{d}(_{i } Q_{r}),\] (9)

where \(x_{i}^{d}\) is the value of \(_{i}\) along the \(d\)-th dimension. We set the kernel variance along the \(d\)-th dimension:

\[(_{r}^{d})^{2}=}\{_{i=1}^{n}(_{i} Q_{r})(x_{i}^{d}-_{r}^{d})^{2}+\},\] (10)

where \(\) is a small constant that prevents \(_{r}^{d}\) from being \(0\). We constrain our estimated Gaussian kernels to have diagonal covariance.

### Sample Size Ratio Estimation

For a high dimensional dataset with low training sample size, the polytopes are sparsely populated with training samples. For improving the estimate of the ratio \(}{n_{y}}\) in Equation 5, we incorporate the samples from other polytopes \(Q_{s}\) based on the similarity \(w_{rs}\) between \(Q_{r}\) and \(Q_{s}\) as:

\[_{ry}}{_{y}}=}_{i=1}^{n}w_{ rs}(_{i} Q_{s})(y_{i}=y)}{_{r} _{s}_{i=1}^{n}w_{rs}(_{i} Q_{s} )(y_{i}=y)}.\] (11)

As \(n\), the estimated weights \(w_{rs}\) should satisfy the condition:

\[w_{rs}0,&Q_{r} Q_{s}\\ 1,&Q_{r}=Q_{s}.\] (12)

For simplicity, we will describe the estimation procedure for \(w_{rs}\) in the next sections. Note that if we satisfy Condition 12, then we have \(_{rx}}{_{y}}}{n_{y}}\) as \(n\). Therefore, we modify Equation 5 as:

\[_{y}()=_{y}}_{r}_{ ry}(;_{r},_{r})(r=_ {}^{*}),\] (13)

where \(_{}^{*}=*{argmin}_{r}\|_{r}-\|\). Now we use \(_{y}()\) estimated using (13) in Equation (6), (7) and (8), respectively. Below, we describe how we estimate \(w_{rs}\) for KDF and KDN.

### Forest Kernel

Consider \(T\) number of decision trees in a random forest trained on \(n\)\(iid\) training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\). Each tree \(t\) partitions the feature space into \(p_{t}\) polytopes resulting in a set of polytopes: \(\{\{Q_{t,r}\}_{r=1}^{p_{t}}\}_{t=1}^{T}\). The intersection of these polytopes gives a new set of polytopes \(\{Q_{r}\}_{r=1}^{p}\) for the forest. For any two points \( Q_{r}\) and \(^{} Q_{s}\), we define the kernel \((r,s)\) as:

\[(r,s)=}{T},\] (14)

where \(t_{rs}\) is the total number of trees, \(\) and \(^{}\) end up in the same leaf node. Here, \(0(r,s) 1\).

_If the two samples end up in the same leaf in all the trees, i.e., \((r,s)=1\), they belong to the same polytope, i.e. \(r=s\)._ In short, \((r,s)\) is the fraction of total trees where the two samples follow the same path from the root to a leaf node. We exponentiate \((r,s)\) so that Condition 12 is satisfied:

\[w_{rs}=(r,s)^{k n}.\] (15)

We choose \(k\) using grid search on a hold-out dataset.

### Network Kernel

Consider a fully connected \(L\) layer ReLU-net trained on \(n\)\(iid\) training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\). We have the set of all nodes denoted by \(_{l}\) at a particular layer \(l\). We can randomly pick a node \(n_{l}_{l}\) at each layer \(l\), and construct a sequence of nodes starting at the input layer and ending at the output layer which we call an **activation path**: \(m=\{n_{l}_{l}\}_{l=1}^{L}\). Note that there are \(N=_{i=1}^{L}|_{l}|\) possible activation paths for a sample in the ReLU-net. We index each path by a unique identifier number \(z\) and construct a sequence of activation paths as: \(=\{m_{z}\}_{z=1,,N}\). Therefore, \(\) contains all possible activation pathways from the input to the output of the network.

While pushing a training sample \(_{i}\) through the network, we define the activation from a ReLU unit at any node as '\(1\)' when it has positive output and '\(0\)' otherwise. Therefore, the activation indicates on which side of the affine function at each node the sample falls. The activation for all nodes in an activation path \(m_{z}\) for a particular sample creates an **activation mode**\(a_{z}\{0,1\}^{L}\). If we evaluate the activation mode for all activation paths in \(\) while pushing a sample through the network, we get a sequence of activation modes: \(_{r}=\{a_{z}^{r}\}_{z=1}^{N}\). Here \(r\) is the index of the polytope where the sample falls in.

_If the two sequences of activation modes for two different training samples are identical, they belong to the same polytope._ In other words, if \(_{r}=_{s}\), then \(Q_{r}=Q_{s}\). This statement holds because the above samples will lie on the same side of the affine function at each node in different layers of the network. Now, we define the kernel \((r,s)\) as:

\[(r,s)=^{N}(a_{z}^{r}=a_{z}^{s})}{N}.\] (16)

Note that \(0(r,s) 1\). In short, \((r,s)\) is the fraction of total activation paths which are identically activated for two samples in two different polytopes \(r\) and \(s\). We exponentiate the kernel using Equation 15. Pseudocodes outlining the two algorithms are provided in Appendix D.

### Geodesic Distance

Consider \(_{n}=\{Q_{1},Q_{2},,Q_{p}\}\) as a partition of \(^{d}\) given by a random forest or a ReLU-net after being trained on \(n\) training samples. We measure distance between two points \( Q_{r},^{} Q_{s}\) using the kernel introduced in Equation 14 and Equation 16, and call it 'Geodesic' distance :

\[d(r,s)=-(r,s)+((r,r)+(s,s))=1- (r,s).\] (17)

**Proposition 1**.: \((_{n},d)\) _is a metric space._

Proof.: See Appendix A.1 for the proof. 

We use Geodesic distance to find the nearest polytope to the inference point. As Geodesic distance cannot distinguish between points within the same polytope, it has a resolution similar to the size of the polytope. For discriminating between two points within the same polytope, we fit a Gaussian kernel within the polytope (described above). As \(h_{n} 0\), the resolution for Geodesic distance improves. In Section 5, we will empirically show that using Geodesic distance scales better with higher dimension compared to that of Euclidean distance.

Given \(n\) training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\), we define the distance of an inference point \(\) from the training points as: \(d_{}=_{i=1,,n}\|-_{i}\|\), where \(\|\|\) denotes Euclidean distance.

**Proposition 2** (Asymptotic OOD Convergence).: _Given non-zero and bounded bandwidth of the Gaussians, then we have almost sure convergence for \(_{y}\) as:_

\[_{d_{}}_{y}()=_{Y}(y).\]

Proof.: See Appendix A.2 for the proof.

Empirical Results

We conduct several experiments on simulated, OpenML-CC18 1 and vision benchmark datasets to gain insights on the finite sample performance of KDF and KDN. The details of the simulation datasets and hyperparameters used for all the experiments are provided in Appendix C. For Trunk simulation dataset, we follow the simulation setup proposed by Trunk  which was designed to demonstrate 'curse of dimensionality'. In the Trunk simulation, a binary class dataset is used where each class is sampled from a Gaussian distribution with higher dimensions having increasingly less discriminative information. We use both Euclidean and Geodesic distance to detect the nearest polytope (see Equation (4)) on simulation datasets and use only Geodesic distance for benchmark datasets. For the simulation setups, we use classification error, Hellinger distance  from the true class conditional posteriors and mean max confidence  as performance statistics. While measuring in-distribution calibration for the datasets in OpenML-CC18 data suite, we used maximum calibration error as defined by Guo et al.  with a fixed bin number of \(R=15\) across all the datasets. Given \(n\) OOD samples, we define OOD calibration error (OCE) to measure OOD performance for the benchmark datasets as:

\[=_{i=1}^{n}|_{y}(_{Y|X }(y|_{i}))-_{y}(_{Y}(y))|.\] (18)

For the tabular and the vision datasets, we have used ID calibration approaches, such as Isotonic and Sigmoid regression, as baselines. Additionally, for the vision benchmark dataset, we provide results with OOD calibration approaches such as: ACET, ODIN, OE (outlier exposure) . For each approach, \(70\%\) of the training data was used to fit the model and the rest of the data was used to calibrate the model.

### Empirical Study on Tabular Data

#### 5.1.1 Simulation Study

Figure 1 leftmost column shows \(10000\) training samples with \(5000\) samples per class sampled within the region \([-1,1][-1,1]\) from the six simulation setups described in Appendix C. Therefore, the empty annular region between \([-1,1][-1,1]\) and \([-2,2][-2,2]\) is the low density or OOD region in Figure 1. Figure 1 quantifies the performance of the algorithms which are visually represented in Appendix Figure 4. KDF and KDN maintain similar classification accuracy to those of their parent algorithms. We measure hellinger distance from the true distribution for increasing training sample size within \([-1,1][-1,1]\) region as a statistics for in-distribution calibration. Column \(3\) and \(6\) in Figure 1 show KDF and KDN are better at estimating the ID region compared to their parent methods. In all of the simulations, using geodesic distance measure results in better performance compared to those while using Euclidean distance. For measuring OOD performance, we keep the training sample size fixed at \(1000\) and normalize the training data by the maximum of their \(l2\) norm so that the training data is confined within a unit circle. For inference, we sample \(1000\) inference points uniformly from a circle where the circles have increasing radius and plot mean max posterior for increasing distance from the origin. Therefore, for distance up to \(1\) we have in-distribution samples and distances farther than \(1\) can be considered as OOD region. As shown in Column \(4\) and \(7\) of Figure 1, mean max confidence for KDF and KDN converge to the maximum of the class priors, i.e., \(0.5\) as we go farther away from the training data origin.

Row \(6\) of Figure 1 shows KDF-Geodesic and KDN-Geodesic scale better with higher dimensions compared to their Euclidean counterpart algorithms respectively.

#### 5.1.2 OpenML-CC18 Data Study

We use OpenML-CC18 data suite for tabular benchmark dataset study. We exclude any dataset which contains categorical features or NaN values 2 and conduct our experiments on \(45\) datasets with varying dimensions and sample sizes. For the OOD experiments, we follow a similar setup as that of the simulation data. We normalize the training data by their maximum \(l_{2}\) norm and sample \(1000\)testing samples uniformly from hyperspheres where each hypersphere has increasing radius starting from \(1\) to \(5\). For each dataset, we measure improvement with respect to the parent algorithm:

\[_{p}-_{M}}{_{p}},\] (19)

Figure 1: **Simulation datasets, Classification error, Hellinger distance from true posteriors, mean max confidence or posterior for A. five two-dimensional and B. a high dimensional (Trunk) simulation experiments, visualized for the first two dimensions. The median performance is shown as a dark curve with shaded region as error bars.**

Figure 2: **Performance summary of KDF and KDN on OpenML-CC18 data suite. The dark curve in the middle shows the median of performance on \(45\) datasets with the shaded region as error bar.**

where \(_{p}=\)classification error, MCE or OCE for the parent algorithm and \(_{M}\) represents the performance of the approach in consideration. Note that positive improvement implies the corresponding approach performs better than the parent approach. We report the median of improvement on different datasets along with the error bar in Figure 2. The extended results for each dataset is shown separately in the appendix. Figure 2 left column shows on average KDF and KDN has nearly similar or better classification accuracy compared to their respective parent algorithm whereas Isotonic and Sigmoid regression have lower classification accuracy most of the cases. However, according to Figure 2 middle column, KDF and KDN have similar in-distribution calibration performance to the other baseline approaches. Most interestingly, Figure 2 right column shows that KDN and KDF improves OOD calibration of their respective parent algorithms by a huge margin while the baseline approaches completely fails to address the OOD calibration problem.

### Empirical Study on Vision Data

In vision data, each image pixel contains local information about the neighboring pixels. To extract the local information, we use convolutional or vision transformer encoders at the front-end. More precisely, we have a front-end encoder, \(h_{e}:^{D}^{m}\) and typically, \(m<<D\). After the encoder there is a few fully connected dense layers for discriminating among the \(K\) class labels, \(h_{f}:^{m}^{K}\). Note that the \(m\)-dimensional embedding outputs from the encoder are partitioned into polytopes by the dense layers (see Equation (3)) and we fit a KDN on the embedding outputs. The above approach results in extraction of better inductive bias by KDN from the parent model and makes KDN more scalable with larger parent models and training sample size.

#### 5.2.1 Simulation Study

For the simulation study, we use a simple CNN with one convolutional layer (\(3\) channels with \(3 3\) kernel) followed by two fully connected layers with \(10\) and \(2\) nodes in each. We train the CNN on \(2000\) circle (radius \(10\)) and \(2000\) rectangle (sides \(20,50\)) images with their RGB values being fixed at \(\) and their centers randomly sampled within a square with sides \(100\). The other pixels in the background where there is no object (circle, rectangle or ellipse) were set to \(0\).

Figure 3: KDN **filters out inference points with different kinds of semantic shifts from the training data.** Simulated images: (A) circle with radius \(10\), (B) rectangle with sides \((20,50)\) and out-of-distribution test points: (C) ellipse with minor and major axis \((10,30)\). Mean max confidence of KDN are plotted for semantic shift of the inference points created by (D) changing the color intensity, (E) taking convex combination of circle and rectangle, (F) changing one of the axes of the ellipse.

We perform three experiments while inducing semantic shifts in the inference points as shown in Figure 3. In the first experiment, we randomly sampled data similar to the training points. However, we added the same shift to all the RGB values of an inference point (shown as color intensity in Figure 3 D). Therefore, the inference point is ID for color intensity at \(127\) and otherwise OOD. In the second experiment, we kept the RGB values fixed at \(\) while taking convex combination of a circle and a rectangle. Let images of circles and rectangles be denoted by \(X_{c}\) and \(X_{r}\). We derive an interference point as \(X_{inf}\):

\[X_{inf}= X_{c}+(1-)X_{r}\] (20)

Therefore, \(X_{inf}\) is maximally distant from the training points for \(=0.5\) and closest to the ID points at \(=\{0,1\}\). In the third experiment, we sampled ellipse images with the same RGB values as the training points. However, this time we gradually change one of the ellipse axes from \(0.01\) to \(40\) while keeping the other axis fixed at \(10\). As a result, the inference point becomes ID for the axis length of \(10\). As shown in Figure 3 (D, E, F), in all the experiments KDN becomes less confident for the OOD points while the parent CNN remains overconfident throughout the semantic shifts of the test points.

#### 5.2.2 Vision Benchmark Datasets Study

In this study, we use a \(ViT\_B16\) (provided in keras-vit package) vision transformer encoder  pretrained on ImageNet  dataset and finetuned on CIFAR-10 . We use the same encoder for all the baseline algorithms and finetune it with the corresponding loss function without freezing any weight. As shown in Table 1, pretrained vision transformers are already well-calibrated for ID and the OOD approaches (ACET, ODIN, OE) degrade ID calibration of the parent model. On the contrary, ID calibration approaches (Isotonic, SIGMOID) perform poorly compared to that of KDN in the OOD region. KDN achieves a compromise between ID and OOD performance while having reduced confidence on wrongly classified ID samples. The number of populated polytopes (and Gaussians) for KDN is \(9323 353\). See Appendix F for the corresponding experiments using Resnet-50.

## 6 Limitations

Training time complexity for KDF and KDN is \(O(n^{2}l_{f})\) which is dominated by the Geodesic distance calculation. Here \(l_{f}=\) total number of leaves in the forest or total nodes in the dense layers of the network and \(n=\) total training samples. However, the distance calculation can be done in parallel using our provided code. Additionally, note that the number of Gaussian kernel used by KDN is upper bounded by number of training samples. Therefore, KDN may not scale for really big datasets like ImageNet . However, the scaling issue may be solved by selectively pruning neighboring polytopes which we will pursue in future.

## 7 Discussion

In this paper, we demonstrated a simple intuition that renders traditional deep discriminative models into a type of binning and kerneling approach. The bin boundaries are determined by the internal structure learned by the parent approach and Geodesic distance encodes the low dimensional structure learned by the model. Moreover, Geodesic distance introduced in this paper may have broader impact on understanding the internal structure of the deep discriminative models which we will pursue in future. Our code, including the package and the experiments in this manuscript, will be made publicly available upon acceptance of the paper.

   & **Dataset** & **Statistics** & Parent & KDN & Isotonic & Signord & ACET & ODIN & OE \\   &  &  & Accuracy\({}^{(3)}\) & \(98.06 0.00\) & \(97.45 0.00\) & \(98.10 0.00\) & \(98.10 0.00\) & \(98.28 0.00\) & \(97.97 0.00\) & \(97.94 0.00\) \\  & & MCE\({}_{}\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \(0.01 0.00\) & \(0.02 0.00\) & \(0.01 0.00\) \\  & _{}\)} & \(0.76 0.01\) & \( 0.08\) & \(0.74 0.02\) & \(0.90 0.01\) & \(0.86 0.02\) & \(0.97 0.01\) & \(0.69 0.01\) \\   & CIFAR-100 & OCE\({}_{}\) & \(0.47 0.01\) & \(0.12 0.01\) & \(0.47 0.01\) & \(0.69 0.01\) & \(0.57 0.01\) & \(0.79 0.00\) & \(0.29 0.01\) \\  &  & OCE\({}_{}\) & \(0.44 0.00\) & \( 0.02\) & \(0.34 0.12\) & \(0.64 0.16\) & \(0.47 0.04\) & \(0.75 0.03\) & \(0.11 0.02\) \\    & Noise & OCE\({}_{}\) & \(0.28 0.08\) & \(0.03 0.02\) & \(0.30 0.04\) & \(0.56 0.12\) & \( 0.00\) & \(0.53 0.09\) & \(0.07 0.02\) \\  

Table 1: **KDN achieves good calibration at both ID and OOD regions whereas other approaches which excel either in the ID or the OOD region. Notably, KDN has reduced confidence on wrongly classified ID points. ‘\(\)’ and ‘\(\)’ indicate whether higher and lower values are better, respectively. MMC\({}^{*}\) = Mean Max Confidence on wrongly classified ID points.**