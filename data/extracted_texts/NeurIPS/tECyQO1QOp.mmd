# Doubly Constrained Fair Clustering

John Dickerson

Seyed A. Esmaeili

Simons Laufer Mathematical Sciences Institute

Jamie Morgenstern

University of Washington

Claire Jie Zhang

University of Washington

###### Abstract

The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness (**GF**), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection (**DS**), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for one constraint (**GF** or **DS** only) we can obtain a constant approximation solution that satisfies both constraints simultaneously. Interestingly, we prove that any given solution that satisfies the **GF** constraint can always be post-processed at a bounded degradation to the clustering cost to additionally satisfy the **DS** constraint while the reverse is not true given a solution that satisfies **DS** instead. Furthermore, we show that both **GF** and **DS** are incompatible (having an empty feasibility set in the worst case) with a collection of other distance-based fairness notions. Finally, we carry experiments to validate our theoretical findings.

## 1 Introduction

Algorithms' deployment in consequential settings, from their use in selecting interview candidates during hiring, to criminal justice systems for risk assessment, to making decisions about where public resources should be allocated , have led to an explosion in interest of developing classification and regression algorithms designed to have equitable predictions. More recently, these questions have been extended to unsupervised settings, with the recognition that algorithms which behave as subroutines may have downstream impacts on the ability of a system to make equitable decisions. For example, while personalized advertising seems to be a much lower-risk application than those mentioned above, the advertisements in question might pertain to lending, employment, or housing. For this reason, understanding the impact of unsupervised data pre-processing, including dimensionality reduction and clustering, have been of recent interest, despite the fact that many mathematical operationalizations of fairness do not behave nicely under composition .

Clustering is a canonical problem in unsupervised learning and arguably the most fundamental. It is also among the classical problems in operations research and used heavily in facility location as well as customer segmentation. As a result, fairness in clustering has also been well-studiedin recent years . Much of this work has identified that existing clustering algorithms fail to satisfy various notions of fairness, and introduce special-purpose algorithms whose outcomes do conform to these definitions. As in supervised learning, a list of mathematical constraints have been introduced as notions of fairness for clustering (more than seven different constraints so far). For example, Chierichetti et al.  show algorithms that are guaranteed to produce clustering outputs that prevent the under-representation of a demographic group in a given cluster, hence being compliant to the disparate impact doctrine . Brubach et al.  show algorithms that bound the probability of assigning two nearby points to different clusters, therefore guaranteeing a measure of community cohesion. Other algorithms for well-motivated fairness notions have also been introduced, such as minimizing the maximum clustering cost per population among the groups [24; 1], assigning distance-to-center values that are equitable , and having proportional demographic representation in the chosen cluster centers .

These constraints and others may be well-justified in a variety of specific application domains, and _which_ are more appropriate will almost certainly depend on the particular application at hand. The dominant approach in the literature has imposed only one constraint in a given setting, though some applications of clustering (which are upstream of many possible tasks) might naturally force us to reconcile these different constraints with one another. Ideally, one would desire a single clustering of the data which satisfies a collection of fairness notions instead of having different clusterings for different fairness notions. A similar question was investigated in fair classification [31; 18] where it was shown that unless the given classification instance satisfies restrictive conditions, the two desired fairness objectives of calibration and balance cannot be simultaneously satisfied. One would expect that such a guarantee would also hold in fair clustering. For various constraints it can be shown that they are in fact at odds with one another. However, it is also worthwhile on the other hand to ask _if some fair clustering constraints are more compatible with one another, and how one can satisfy both simultaneously?_

Our Contributions:In this paper, we take a first step towards understanding this question. In particular, we consider two specific group fairness constraints (1) **GF**: The group fair clustering (**GF**) of Chierichetti et al.  which roughly states that clusters should have close to population level proportions of each group and (2) **DS**: The diversity in center selection (**DS**) constraint  which roughly states that the selected centers in the clustering should similarly include close to population level proportions of each group. We note that although these two definitions are both concerned with group memberships, the fact that they apply at different "levels" (clustered points vs selected centers) makes the algorithms and guarantees that are applicable for one problem not applicable to the other, certainly not in an obvious way. Further, both of these notions are motivated by disparate impact  which essentially states that different groups should receive the same treatment. Therefore, it is natural to consider the intersection of both definitions (**GF \(+\) Ds**). We show that by post-processing any solution satisfying one constraint then we can always satisfy the intersection of both constraints. At a more precise level, we show that an \(\)-approximation algorithm for one constraint results in an approximation algorithm for the intersection of the constraints with only a constant degradation to approximation ratio \(\). Additionally, we study the degradation in the clustering cost and show that imposing **DS** on a **GF** solution leads to a bounded degradation of the clustering cost while the reverse is not true. Moreover, we show that both **GF** and **DS** are incompatible (having an empty feasible set) with a set of distance-based fairness constraints that were introduced in the literature. Finally, we validate our finding experimentally. Due to the space limits we refer the reader to Appendix C for the proofs as well as further details.

## 2 Related Work

Fairness in clustering: GF and DS.In the line of works on group-level fairness, Chierichetti et al.  defined _balance_ in the case of two groups to require proportion of a group in any cluster to resemble its proportion in input data. They also proposed the method of creating fairlets and then run generic clustering algorithm on fairlets. Bera et al.  generalized the notion of balance to multiple groups, and considered when centers are already chosen how to assign points to centers so that each group has _bounded representation_ in each cluster. While only assuming probabilistic group assignment, Esmaeili et al.  presented algorithms that guarantee cluster outputs satisfy expected group ratio bounds. Also working with bounded representation, Esmaeili et al.  showed how to minimize additive violations of fairness constraint while ensuring clustering cost is within given upper bound. Besides center-based clustering, group fairness constraint is also applied to spectral clustering . Recent work due to Wang et al.  speeds up computation in this setting. Ahmadi et al.  applied group fairness to correlation clustering. As a recent follow-up, Ahmadian and Negahbani  generalized the setting and improved approximation guarantee. Ahmadian et al.  and Chhabra and Mohapatra  introduced group fairness in the context of hierarchical clustering, with work due to Knittel et al.  proposing an algorithm with improved approximation factor in optimizing cost with fairness setting. Diversity in center selection was first studied in Kleindessner et al.  for data summarization tasks. The authors presented an algorithms that solves the fair \(k\)-centers problem with an approximation factor that is exponential in the number of groups and with a running time that is linear in the number of input points. A follow-up work Jones et al.  improved the approximation factor to a constant while keeping the run time linear in number of input points. Concerned with both the diversity in selected center and its distortion of the summary, Angelidakis et al.  proposed an approximation algorithm for the \(k\)-centers problem that ensures number of points assigned to each center is lower bounded. Recent work due to Nguyen et al.  generalized the problem to requiring group representation in centers to fall in desired range, which is the setting this work is using.

Other fairness considerations in clustering.Besides fairness notions already mentioned, other individual fairness notions include requiring individual points to stay close to points that are similar to themselves in output clusters such as that of [34; 13]. The proportionally fair clustering based on points forming coalitions . A notion based on individual fairness that states that points should have centers within a distance \(R\) if there are \(n/k\) points around it within \(R\)[36; 30]. Newer fairness notions on clustering problems were introduced recently in Ahmadi et al.  and Gupta and Dukkipati . For the interested reader, we recommend a recent overview due to Awasthi et al.  for exhaustive references and an accessible overview of fair clustering research.

## 3 Preliminaries and Symbols

We are given a set of points \(\) along with a metric distance \(d(.,.)\). The set of chosen centers is denoted by \(S\) and the assignment function (assigning points to centers) is \(: S\). We are concerned with the \(k\)-center clustering which minimizes the maximum distance between a point and its assigned center. Formally, we have:

\[_{S:|S| k,}\,_{j}d(j,(j))\] (1)

In the absence of constraints, the assignment function \((.)\) is trivial since the optimal assignment is to have each point assigned to its nearest center. However, when the clustering problem has constraints this is generally not the case.

In fair clustering, each point \(j\) is assigned a color by a function \((j)=h\) to indicate its demographic group information, where \(\) is the set of all colors. For simplicity, we assume that each point has one group associated with it and that the total number of colors \(m=|\,\,|\) is a constant. Moreover, the set of points with color \(h\) are denoted by \(^{h}\). The total number of points is \(n=|\,\,|\) and the total number of points of color \(h\) is \(n_{h}=|\,^{h}\,|\). It follows that the proportion of color \(h\) is \(r_{h}=}{n}\). Finally, given a clustering \((S,)\), we denote the set of points in the \(i^{}\) cluster by \(C_{i}\) and the subset of color \(h\) by \(C_{i}^{h}=C_{i}^{h}\).

We now formally introduce the group fair clustering (**GF**) and the diverse center selection (**DS**) problems:

Group Fair Clustering [17; 10; 9; 21; 5]:Minimize objective (1) subject to proportional demographic representation in each cluster. Specifically, \( i S, h:_{h}|\,C_{i}\,||\,C_{i}^{h}\,| _{h}|\,C_{i}\,|\) where \(_{h}\) and \(_{h}\) are pre-set upper and lower bounds for the demographic representation of color \(h\) in a given cluster.

Diverse Center Selection [32; 29; 37]:Minimize objective (1) subject to the set of centers \(S\) satisfying demographic representation. Specifically, denoting the number of centers from demographic (color) \(h\) by \(k_{h}=|S^{h}\,|\), then as done in  it must satisfy \(k_{h}^{l} k_{h} k_{h}^{u}\) where \(k_{h}^{l}\) and \(k_{h}^{u}\) are lower and upper bounds set for the number of centers of color \(h\), respectvily.

Importantly, throughout we have \( h:_{h}>0\). Further, for **GF** we consider solutions that could have violations to the constraints as done in the literature [9; 10]. Specifically, a given a solution \((S,)\) has an additive violation of \(\)**GF** if \(\) is the smallest number such that the following holds: \( i S, h:_{h}|C_{i}|-|C_{i}^{h}| _{h}|C_{i}|+\). We denote the problem of minimizing the \(k\)-center objective while satisfying both the **GF** and **DS** constraints as **GF+DS**.

Why Consider **GF** and **DS** in Particular?There are two reasons to consider the **GF** and **DS** constraints in particular. First, from the point of view of the application both **GF** and **DS** are concerned with demographic (group) fairness. Further, they are both specifically focused on the representation of groups, i.e. the proportions of the groups (colors) in the clusters for **GF** and in the selected center for **DS**. Second, they are both "distance-agnostic", i.e. given a clustering solution one can decide if it satisfies the **GF** or **DS** constraints without having access to the distance between the points.

## 4 Algorithms for GF+DS

### Active Centers

We start by observing the fact that if we wanted to satisfy both **GF** and **DS** simultaneously, then we should make sure that all centers are _active_ (having non-empty clusters). More precisely, given a solution \((S,)\) then the **DS** constraints should be satisfied further \( i S:|C_{i}|>0\), i.e. every center in \(S\) should have some point assigned to it and therefore not forming an empty cluster. The following example clarifies this:

Example:Consider Figure 1. Suppose we have \(k=2\) and we wish to satisfy the **GF** and **DS** constraints with equal red to blue representation. **DS** requires one blue and one red center. Further, each cluster should have \(|C_{i}^{}|=|C_{i}^{}|=|C_{i}|\) to satisfy **GF**. Consider the following solution \(S_{1}=\{2,4\}\) and \(_{1}\) which assigns all points to point 2 including point 4. This satisfies **GF** and **DS**. Since we have one blue center and one red center. Further, the cluster of center 4 has no points and therefore \(0=|C_{i}^{}|=|C_{i}^{}|=|C_{i}|\). Another solution would have \(S_{2}=S_{1}=\{2,4\}\) but with \(_{2}\) assigning points 2 and 3 to center 2 and points 1 and 4 to center 4. This would also clearly satisfy the **GF** and **DS** constraints.

There is a clear issue in the first solution which is that although center 4 is included in the selection it has no points assigned to it (it is an empty cluster). This makes it functionally non-existent. This is why the definition should only count active centers.

This issue of active centers did not appear before in **DS**[32; 37], the reason behind this is that it is trivial to satisfy when considering only the **DS** constraint since each center is assigned all the points closest to it. This implies that the center will at least be assigned to itself, therefore all centers in a **DS** solution are _active_. However, we cannot simply assign each point to its closest center when the **GF** constraints are imposed additionally as the colors of the points have to satisfy the upper and lower proportion bounds of **GF**.

### The Divide Subroutine

Here we introduce the Divide subroutine (block \(1\)) which is used in subsections 4.3 and 4.4 in algorithms for converting solutions that only satisfy **DS** or **GF** into solutions that satisfy **GF+DS**. Divide takes a set of points \(C\) (which is supposed to be a single cluster) with center \(i\) along with a subset of chosen points \(Q\) (\(Q C\)). The entire set of points is then divided among the points \(Q\) forming \(|Q|\) many new non-empty (active) clusters. Importantly, the points of each color are divided among the new centers in \(Q\) so that the additive violation increases by at most \(2\). See Figure 2 for an intuitive illustration.

Here we use the symbol \(q\) to index a point in the set \(Q\). Importantly, the numbering starts with \(0\) and ends with \(|Q|-1\).

We prove the following about Divide:

Figure 1: In this graph the distance between the points is defined as the path distance.

**Lemma 1**.: _Given a non-empty cluster \(C\) with center \(i\) and radius \(R\) that satisfies the **GF** constraints at an additive violation of \(\) and a subset of points \(Q\) (\(Q C\)). Then the clustering \((Q,)\) where \(=(C,Q)\) has the following properties: (1) The **GF** constraints are satisfied at an additive violation of at most \(+2\). (2) Every center in \(Q\) is active. (3) The clustering cost is at most \(2R\). If \(|Q|=1\) then guarantee (1) is for the additive violation is at most \(\)._

### Solving GF+DS using a DS Algorithm

Here we show an algorithm that gives a bounded approximation for **GF+DS** using an approximation algorithm for **DS**. Algorithm 2 works by first calling an \(_{}}\)-approximation algorithm resulting in a solution \((,)\) that satisfies the **DS** constraints, then it solves an assignment problem using the AssignmentGF algorithm (shown in 3) where points are routed to the centers \(\) to satisfy the **GF** constraint. The issue is that some of the centers in \(\) may become closed and as a result the solution may no longer satisfy the **DS** constraints. Therefore, we have a final step where more centers are opened using the Divide subroutine to satisfy the **DS** constraints while still satisfying the **GF** constraints at an additive violation and having a bounded increase to the clustering cost.

```
1:Input: Set of points \(C\) with center \(i C\), Subset of points \(Q\) (\(Q C\)) of cardinality \(|Q|\).
2:Output: An assignment function \(:C Q\).
3:if\(|Q|=1\)then
4: Assign all points \(C\) to the single center in \(Q\).
5:else
6: Set firstIndex = \(0\).
7:for\(h\)do
8: Set: \(T_{h}=|}{|Q|},\ b_{h}=T_{h}-|Q|\,|T_{h}\), count = \(0\)
9: Set: \(q\) = firstIndex
10:while count \(|Q|-1\)do
11:if\(b_{h}>0\)then
12: Assign \( T_{h}\) many points of color \(h\) in \(C\) to center \(q\).
13: Update \(b_{h}=b_{h}-1\).
14: Update firstIndex = \((+1)|Q|\).
15:else
16: Assign \( T_{h}\) many points of color \(h\) in \(C\) to center \(q\).
17:endif
18: Update \(q=(q+1)|Q|\), count = count + 1.
19:endwhile
20:endfor
21:endif ```

**Algorithm 1**Divide

AssignmentGF works by solving a linear program (2) to find a clustering which ensures that (1) each cluster has at least a \(_{h}\) fraction and at most an \(_{h}\) fraction of its points belonging to color \(h\), and (2) the clustering assigns each point to a center that is within a minimum possible distance \(R\). While the resulting LP solution could be fractional, the last step of AssignmentGF uses MaxFlowGF which is an algorithm for rounding an LP solution to valid integral assignments at a bounded degradation to the **GF** guarantees and no increase to the clustering cost. See Appendix B for details on the MaxFlowGF and its guarantees.

Figure 2: Illustration of Divide subroutine.

```
1:Input: Points \(\), Solution \((,)\) with clusters \(\{C_{i},,C_{}\}\) satisfying the \(\) constraints with \(||= k\) of approximation ratio \(_{}\) for the \(\) clustering problem.
2:Output: Solution \((S,)\) satisfying the \(\) and \(\) constraints simultaneously.
3:\((S^{},^{})\) =AssignmentGF(\(,\))
4: Update the set of centers \(S^{}\) by deleting all non-active centers (which have no points assigned to them). Let \(\{C^{}_{1},,C^{}_{k^{}}\}\) be the (non-empty) clusters of the solution \((S^{},^{})\) with \(|S^{}|=k^{}\).
5: Set \( h:s_{h}=|S^{}^{h}\), Set \( i S:Q_{i}=\{i\}\)
6:while\( h\) such that \(s_{h}<k^{l}_{h_{0}}\)do
7: Pick a color \(h_{0}\) such that \(s_{h_{0}}<k^{l}_{h_{0}}\).
8: Pick a center \(i S^{}\) where there exists a point of color \(h_{0}\).
9: Pick a point \(j_{h_{0}}\) of color \(h_{0}\) in cluster \(C^{}_{i}\)
10: Set \(Q_{i}=Q_{i}\{j_{h_{0}}\}\).
11: Update \(s_{h_{0}}=s_{h_{0}}+1\).
12:endwhile
13:for\(i S^{}\)do
14:\(_{i}=_{i}$,$Q_{i}$)}\).
15:\( j C^{}_{i}\) : Set \((j)=_{i}(j)\).
16:endfor
17: Set \(S=S^{}_{i S^{}}Q_{i}\). ```

**Algorithm 2**DSToGF+\(\)

```
1:Input: Set of centers \(S\), Set of Points \(C\).
2:Output: An assignment function \(:CS\).
3: Using binary search over the distance matrix, find the smallest radius \(R\) such that \(LP(C,S,R)\) in (2) is feasible and call the solution \(^{*}\).
4: Solve MaxFlowGF(\(^{*},C,S\)) and call the solution \(}^{*}\). ```

**Algorithm 3**AssignmentGF

\[(C,S,R) :\] (2a) \[ j C, i S:x_{ij}=0d(i,j)>R\] (2b) \[ h, i S:_{h}_{j C}x_{ ij}_{j C^{h}}x_{ij}_{h}_{j C}x_{ij}\] (2c) \[ j C:_{i S}x_{ij}=1\] (2d)

To establish the guarantees we start with the following lemma:

**Lemma 2**.: _Solution \((S^{},^{})\) of line (3) in algorithm 2 has the following properties: (1) It satisfies the **GF** constraint at an additive violation of \(2\), (2) It has a clustering cost of at most \((1+_{})R^{*}_{}\) where \(R^{*}_{}\) is the optimal clustering cost (radius) of the optimal solution for **GF+DS**, (3) The set of centers \(S^{}\) is a subset (possibly proper subset) of the set of centers \(\), i.e. \(S^{} S\)._

**Theorem 4.1**.: _Given an \(_{}\)-approximation algorithm for the **DS** problem, then we can obtain an \(2(1+_{})\)-approximation algorithm that satisfies **GF** at an additive violation of 3 and satisfies **DS** simultaneously._

Remark:If in algorithm 2 no center is deleted in line (4) because it forms an empty cluster, then by Lemma 2 the approximation ratio is \(1+_{}\) which is an improvement by a factor of 2. Further, the additive violation for \(\) is reduced from \(3\) to \(2\).

### Solving GF+DS using a GF Solution

```
1:Input: Points \(\), Solution \((,)\) with clusters \(\{_{i},,_{}\}\) satisfying the GF constraints with \(||= k\).
2:Output: Solution \((S,)\) satisfying the GF and DS constraints simultaneously.
3:Initialize: \( h:s_{h}=0\), \( i:Q_{i}=\{\}\).
4:for\(i\)do
5:if\( h:s_{h}<k_{h}^{l}\)then
6: Let \(h_{0}\) be a color such that \(s_{h_{0}}<k_{h_{0}}^{l}\)
7:else
8: Pick \(h_{0}\) such that \(s_{h_{0}}+1 k_{h_{0}}^{u}\).
9:endif
10: Pick a point \(j_{h_{0}}\) of color \(h_{0}\) in cluster \(_{i}\)
11: Set \(Q_{i}=\{j_{h_{0}}\}\).
12: Update \(s_{h_{0}}=s_{h_{0}}+1\).
13:endfor
14:while\( h:s_{h}<k_{h}^{l}\)do
15: Pick a color \(h_{0}\) such that \(s_{h_{0}}<k_{h_{0}}^{l}\).
16: Pick a center \(i\) with cluster \(_{i}\) where there exists a point of color \(h_{0}\) not in \(Q_{i}\).
17: Pick a point \(j_{h_{0}}\) of color \(h_{0}\) in cluster \(_{i}\)
18: Set \(Q_{i}=Q_{i}\{j_{h_{0}}\}\).
19: Update \(s_{h_{0}}=s_{h_{0}}+1\).
20:endwhile
21: Set \(S=_{i}Q_{i}\).
22:for\(i\)do
23:\(_{i}=(_{i},Q_{i})\).
24:\( j_{i}\) : Set \((j)=_{i}(j)\). {Assignment to center is updated using Divide.}
25:endfor ```

**Algorithm 4**GFtoGF+DS

Here we start with a solution \((,)\) of cost \(\) that satisfies the GF constraints and we want to make it satisfy GF and DS simultaneously. More specifically, given any GF solution we show how it can be post-processed to satisfy **GF+DS** at a bounded increase to its clustering cost by a factor of 2 (see Theorem 4.2). This implies as a corollary that if we have an \(_{}\)-approximation algorithm for GF then we can obtain a \(2_{}\)-approximation algorithm for GF+DS (see Corollary 1).

The algorithm essentially first "covers" each given cluster \(_{i}\) of the given solution \((,)\) by picking a point of some color \(h\) to be a _future_ center given that picking a point of such a color would not violate the DS constraints (lines(4-13)). If there are still colors which do not have enough picked centers (below the lower bound \(k_{h}^{1}\)), then more points are picked from clusters where points of such colors exist (lines(14-20)). Once the algorithm has picked correct points for each color, then the Divide subroutine is called to divide the cluster among the picked points.

Now we state the main theorem:

**Theorem 4.2**.: _If we have a solution \((,)\) of cost \(\) that satisfies the GF constraints where the number of non-empty clusters is \(||= k\), then we can obtain a solution \((S,)\) that satisfies GF at an additive violation of 2 and DS simultaneously with cost \(R 2\)._

**Corollary 1**.: _Given an \(_{}\)-approximation algorithm for GF, then we can have a \(2_{}\)-approximation algorithm that satisfies GF at an additive violation of \(2\) and DS simultaneously._

Remark:If the given GF solution has the number of cluster \(=k\), then the output will have an additive violation of zero, i.e. satisfy the GF constraints exactly. This would happen Divide would always receive \(Q_{i}\) with \(|Q_{i}|=1\) and therefore we can use the guarantee of Divide for the special case of \(|Q|=1\).

## 5 Price of (Doubly) Fair Clustering

Here we study the degradation in the clustering cost (the price of fairness) that comes from imposing the fairness constraint on the clustering objective. The price of fairness \(_{c}\) is defined as \(_{c}=c}{}\)[20; 9]. Note that since we have two constrains here **GF** and **DS**, we also consider prices of fairness of the form \(_{c_{1}c_{2}}=c_{1}}{c_{1}}\) which equal the amount of degradation in the clustering cost if we were to impose constraint \(c_{2}\) in addition to constraint \(c_{1}\) which is already imposed. Note that we are concerned with the price of fairness in the _worst case_. Interestingly, we find that imposing the **DS** constraint over the **GF** constraint leads to a bounded \(\) if we allow an additive violation of \(2\) for **GF** while the reverse is not true even if we allow an additive violation of \(()\) for **GF**.

We find the following:

**Proposition 5.1**.: _For any value of \(k 2\), imposing **GF** can lead to an unbounded \(\) even if we allow an additive violation of \(()\)._

**Proposition 5.2**.: _For any value of \(k 3\), imposing **DS** can lead to an unbounded \(\)._

**Proposition 5.3**.: _For any value of \(k 2\), imposing **GF** on a solution that only satisfies **DS** can lead to an unbounded increase in the clustering cost even if we allow an additive violation of \(()\)._

**Proposition 5.4**.: _Imposing **DS** on a solution that only satisfies **GF** leads to a bounded increase in the clustering cost of at most 2 (\( 2\)) if we allow an additive violation of \(2\) in the **GF** constraints._

## 6 Incompatibility with Other Distance-Based Fairness Constraints

In this section, we study the incompatibility between the **DS** and **GF** constraints and a family of distance-based fairness constraints. We note that the results of this section do not take into account the clustering cost and are based only on the feasibility set. That is, we consider more than one constraints simultaneously and see if the feasibility set is empty or not. Two constraints are considered _incompatible_ if the intersection of their feasible sets is empty. In some cases we also consider solution that could have violations to the constraints. We present two main findings here and defer the proofs and further details to the Appendix section1.

**Theorem 6.1**.: _For any value \(k 2\), the **fairness in your neighborhood**, **socially fair** constraint [1; 24] are each incompatible with **GF** even if we allow an additive violation of \(()\) in the **GF** constraint. For any value \(k 5\), the **proportionally fair** constraints  is incompatible with **GF** even if we allow an additive violation of \(()\) in the **GF** constraint._

**Theorem 6.2**.: _For any value \(k 3\), the **fairness in your neighborhood**, **socially fair**[1; 24] and **proportionally fair** constraints are each incompatible with **DS**._

Figure 3: Figure showing the \(\) relation between Unconstrained, **GF**, **DS**, and **GF+DS** clustering.

Experiments

We use Python 3.9, the CPLEX package  for solving linear programs and NetworkX for max-flow rounding. Further, Scikit-learn is used for some standard ML related operations. We use commdity hardware, specifically a MacBook Pro with an Apple M2 chip.

We conduct experiments over datasets from the UCI repository  to validate our theoretical findings. Specifically, we use the **Adult** dataset sub-sampled to 20,000 records. Gender is used for group membership while the numeric entries are used to form a point (vector) for each record. We use the Euclidean distance. Further, for the **GF** constraints we set the lower and upper proportion bounds to \(_{h}=(1-)r_{h}\) and \(_{h}=(1+)r_{h}\) for each color \(h\) where \(r_{h}\) is color \(h^{}\)s proportion in the dataset and we set \(=0.2\). For the **DS** constraints, since we do not deal with a large number of centers we set \(k_{h}^{l}=0.8r_{h}k\) and \(k_{h}^{u}=r_{h}k\).

We compare the performance of 5 algorithms. Specifically, we have (1) **Color-Blind:** An implementation of the Gonzalez \(k\)-center algorithm  which achieves a 2-approximation for the unconstrained \(k\)-center problem. (2) **ALG-GF:** A **GF** algorithm which follows the sketch of , however the final rounding step is replaced by an implementation of the MaxFlowGF rounding subroutine. This algorithm has a 3-approximation for the **GF** constrained instance. (3) **ALG-DS:** An algorithm for the **DS** problem recently introduced by  for which also has an approximation of 3. (4) **GFtoGFDS:** An implementation of algorithm 4 where we simply use the **GF** algorithm just mentioned to obtain a **GF** solution. (5) **DstoGFDS:** Similarly an implementation of algorithm 2 where **DS** algorithm is used as a starting point instead.

Throughout we measure the performance of the algorithms in terms of (1) **PoF:** The price of fairness of the algorithm. Note that we always calculate the price of fairness by dividing by the Color-Blind clustering cost since it solves the unconstrained problem. (2) **GF-Violation:** Which is the maximum additive violation of the solution for the **GF** constraint as mentioned before. (3) **DS-Violation:** Which is simply the maximum value of the under-representation or over-representation across all groups in the selected centers.

Figure 4 shows the behaviour of all 5 algorithms. In terms of **PoF**, all algorithms have a significant degredation in the clustering cost compared to the Color-Blind baseline except for ALG-DS. However, ALG-DS has a very large **GF-Violation**. In fact, the **GF-Violation** of ALG-DS can be more than 5 times the **GF-Violation** of Color-Blind. This indicates that while ALG-DS has a small clustering cost, it can give very bad guarantees for the **GF** constraints. Finally, in terms of the **DS-Violation** we see that the ALG-GF and the Color-Blind solution can violate the **DS** constraint. Note that both coincide perfectly on each other. Further, although the violation is 1, it is very significant since unlike the **GF** constraints the number of centers can be very small. On the other hand, we see that both GFtoGFDS and DstoGFDS give the best of both worlds having small values for the **GF-Violation** and zero values for the **DS-Violation** and while their price of fairness can be significant, it is comparable to ALG-GF. Interestingly, the GFtoGFDS and DstoGFDS are in agreement in terms of measures. This could be because our implementations of the "**GF** part of DstoGFDS (its handling of the **GF** constraints) has similarities to the GFtoGFDS algorithm. We show further experiments in the appendix.

Figure 4: **Adult dataset results: (a) **PoF** comparison of 5 algorithms, with Color-Blind as baseline; (b) **GF-Violation** comparison; (c) **DS-Violation** comparison.