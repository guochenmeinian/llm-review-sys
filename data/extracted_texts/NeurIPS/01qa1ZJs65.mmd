# Bridge the Modality and Capability Gaps in Vision-Language Model Selection

Chao Yi, Yu-Hang He, De-Chuan Zhan, Han-Jia Ye

State Key Laboratory for Novel Software Technology, Nanjing University

{yic,heyh,zhandc,yehj}@lamda.nju.edu.cn

###### Abstract

Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. To better reuse the VLM resource and fully leverage its potential on different zero-shot image classification tasks, a promising strategy is selecting appropriate Pre-Trained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap"--the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap"-- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM **S**election **W**ith g**Ap **B**ridging (Swab) to mitigate the negative impact of two gaps. Swab first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging two gaps. By bridging two gaps to obtain better substitutes for test images, Swab can _accurately_ predict the performance ranking of different VLMs on the target task _without the need for the dataset's images_. Experiments across various VLMs and image classification datasets validate Swab's effectiveness. Code is available at: https://github.com/YCaigogogo/SWAB.

## 1 Introduction

Vision-Language Models (VLMs)  have demonstrated impressive image-text matching ability. One notable application of VLMs is zero-shot image classification , where VLMs are leveraged to generate image classifiers using only class names directly. This zero-shot approach has shown considerable success in scenarios with scarce or no training images .

Despite the success of VLM in image classification, the performance of a VLM may vary substantially according to the datasets and domains , making it challenging to use a single model to handle all tasks. Fortunately, many open-source VLMs are available , and these VLMs form a vast VLM Zoo. With different architectures, pre-training datasets, or training methods, these VLMs have different strengths. The diverse pre-trained VLMs increase the likelihood of pinpointing at least one VLM that excels in a given target dataset in most cases.1 To more effectively reuse the VLM Zoo across diverse target tasks and unlock its full potential, we need a model selection method to choose suitable VLMs from the VLM Zoo for the target task. However, in scenarios such as zero-shot imageclassification, many users might not have labeled images for their target tasks, especially those who are not Machine Learning researchers. They prefer to describe their needs in text and use a Model Search Engine to find the most suitable model. So one solution is _identifying the most suitable VLMs in the zoo for a target dataset without access to the dataset's images_. This VLM selection is termed as "Language-Only VLM Selection" (LOVM) , and the paradigm is illustrated in Figure 1.

Two key types of information are available for LOVM. One is the target dataset's text data, _i.e_., names of the target classes and class-related labeled texts generated by LLMs (Details described in Section B.1). The other is the open-source datasets, collected in the form of images with their corresponding class names. Based on these data, the goal is to estimate a VLM's zero-shot image classification capability ranking among the VLM zoo on the target dataset. LOVM encounters two challenges stemming from the inherent heterogeneity in models and datasets. The first challenge is the **Modality Gap** across different modal features extracted by a VLM. Since the visual and textual features extracted by VLMs tend to cluster into two distinct groups and have gap vectors between them , using text data as image proxies to rank VLMs is inaccurate. The second challenge is the **Capability Gap** between the VLM's overall ranking and its ranking in the specific dataset. Owing to the VLM's performance variation across different datasets, the VLM's average performance on open-source datasets is hard to reflect its performance on a specific target dataset. Thus, selecting a VLM based solely on its general strength may prove to be a less effective strategy.

In this paper, we propose VLM **S**election **W**ith g**A**p **B**ridging (Swab) to address both gaps. The key idea is to reuse VLMs' statistics from open-source datasets to estimate their statistics on the target dataset, which mitigates the negative impact of these two gaps. In particular, Swab first uses optimal transport to calculate the transport matrix based on textual similarity between class names of open-source and target datasets. After applying VLMs on open-source datasets to calculate VLMs' statistics, _i.e_., the class-specific modality gap vectors and performance rankings of different VLMs, Swab utilizes these statistics to estimate the same type of statistics on the target dataset. After that, Swab uses the estimated gap vectors to align the features of texts with the features of images from the corresponding category, which bridges the modality gap. Meanwhile, Swab's estimated VLMs' ranking also improves the prediction of their rankings on the target task, bridging the capability gap. The related work is in the appendix C. The main contributions are:

* We analyze two key challenges in LOVM -- the _modality gap_ across VLM's modal features and the _capability gap_ between the VLM's overall ranking and its ranking on the target dataset.
* We propose Swab, which utilizes optimal transport to transform useful statistics of VLMs on open-source datasets to the target dataset to bridge two gaps.
* Experimental results on a LOVM benchmark composed of a wide range of VLMs and image classification datasets demonstrate the effectiveness of Swab.

Figure 1: **Paradigm of Language-Only VLM Selection (LOVM). Users describe the details of their target tasks in text form, such as class names and image domains. Then, LOVM utilizes this information to generate class-related labeled texts through ChatGPT. These texts serve as substitutes for image samples in subsequent model selection algorithms. The model selection algorithm uses two types of data, including the open-source datasets (which have image and text data) and the text data from the target dataset, to predict the VLM’s absolute or relative performance on a target dataset. It then selects the most appropriate VLM based on the predicted performance.**

Preliminary

We formally introduce the LOVM setting, a baseline method for LOVM, and analyze the two kinds of gaps in LOVM. We use \(\|\|\) to represent the Euclidean norm of a vector unless otherwise defined.

### Selecting VLMs from a Model Zoo

**Zero-Shot Image Classification of VLM.** Assume there is a pre-trained VLM \(f=(f^{I},f^{T})\) consisting of an image encoder \(f^{I}\) and a text encoder \(f^{T}\). Given an image classification dataset \(\) with \(k_{}\) class names \(C_{}=\{c_{1}^{T},,c_{k_{}}^{T}\}\), we input the class names \(C_{}\) (probably with templates like "A photo of {class}") into the VLM's text encoder \(f^{T}\) to get the image classifiers \(\{}_{j}\}_{j=1}^{k_{}}\). Then, given a test image \(_{i}\), we use the image encoder \(f^{I}\) to extract its feature \(}_{i}\). Finally, we predict the label via the cosine similarity between the image feature \(}_{i}\) and image classifiers \(\{}_{j}\}_{j=1}^{k_{}}\). The class with the highest cosine similarity to the image is selected as the predicted class \(_{i}\). Given \(}_{i}=f^{I}(_{i}),\ }_{j}=f^{T}(c_{j}^{})\), Equation 1 describes this zero-shot image classification process:

\[_{i}=f(_{i},C_{})=*{argmax}_{c_{j}^{ }[C_{}]}}_{i}^{}}_{j} }{\|}_{i}\|\|}_{j}\|}.\] (1)

**VLM Zoo.** In recent years, a large number of (pre-trained) VLMs have emerged. Assume a collection of \(M\) VLMs constitute a VLM Zoo \(=\{f_{m}=(f_{m}^{I},f_{m}^{T})\}_{m=1}^{M}\). The capability of \(f_{m}\) is determined by three key factors: the model architecture (_e.g._, Transformer , ConvNeXt ), the pre-trained dataset (_e.g._, LAION-400M , MS-COCO ), and the training method (_e.g._, contrastive loss , caption loss ). Combinations of these factors result in "good and diverse" VLMs in \(\). Given a dataset \(\), it is probable to find a suitable VLM from the VLM zoo with high zero-shot image classification performance on \(\).

**Language-Only VLM Selection (LOVM).** Rather than using images from the target dataset, LOVM focuses on the zero-shot scenario where only the target dataset's text data, such as its class names \(C_{}\), are available for VLM selection. Besides, we can obtain some open-source image classification datasets \(\). The set of class names in \(\) is \(C_{}=\{c_{1}^{S},,c_{}^{}\}\), and the \(D_{}^{I}\) denote the labelled images in these classes. Given a target task \(\), the VLM selection method \(h\) estimates the zero-shot classification ability of \(f_{m}\) based on \(C_{}\), \(C_{}\), and \(D_{}^{I}\) via \(_{m}^{T}=h(f_{m} C_{},C_{},D_{}^{I })\), where \(m[1,,M]\). \(_{m}^{T}\) is the predicted ranking of the \(m\)-th VLM \(f_{m}\) on \(\). The higher the ranking, the more probable \(f_{m}\) achieves higher zero-shot image classification performance on \(\). Assuming we can obtain the test image set \(D_{}^{I}\) of the target dataset \(\) with \(|D_{}^{I}|\) images, then we can calculate the zero-shot image classification accuracy \(p_{m}^{}\) of \(f_{m}\) is calculated by \(p_{m}^{}=}^{I}|}_{(_{i}, y_{i}) D_{}^{I}}(y_{i}=f_{m}(_{i},C_{ }))\). \(f_{m}(_{i},C_{})\) represents the predicted class with the same manner as Equation 1. \(()\) is the indicator function, which outputs 1 if the condition is satisfied, and 0 otherwise. Based on \(\{p_{m}^{}\}_{m=1}^{M}\), we obtain the true ranking of \(M\) VLMs \(^{T}=[r_{1}^{T}\,,,r_{M}^{T}]\) by assigning higher ranking \(r\) to models with higher accuracy \(p\). However, in the zero-shot scenario, we can't obtain the test images set \(D_{}^{I}\) in advance. Therefore, the goal of LOVM is to make the predicted ranking \(}^{}=[_{1}^{},,_{M}^{ }]\) be an accurate estimation of the ground truth ranking \(^{}=[r_{1}^{},,r_{M}^{T}]\) so that the best VLM can be selected.

**Evaluation of LOVM Methods.** We measure the performance of the LOVM algorithm by comparing the ranking similarity between \(^{}\) and \(}^{}\). Specifically, we calculate the Top-5 Recall \(R_{5}\) (ranges from 0 to 1) and Kendall's Rank Correlation \(\) (ranges from -1 to 1). The larger the better.

### Possible Paradigms for LOVM

**Non-Learning-based LOVM**. There are three main paradigms for LOVM. _The first paradigm_ is to neglect the visual encoder and select VLM solely on texts. In detail, we can utilize ChatGPT  to generate auxiliary texts \(_{}\) based on class names \(C_{}\) of \(\). More details are described in Section B.1. These class-specific texts act as "image proxies". Then, whether a VLM \(f_{m}\) fits \(\) could be measured by transferability metrics, _e.g._, H-Score  and LogME , between the VLM's text encoder \(f_{m}^{T}\) and generated text dataset \(_{}\). _The second paradigm_ relies on the general performance of a certain VLM \(f_{m}\). We use open-source datasets to measure a VLM's general performance. If \(f_{m}\) achieves high zero-shot classification performance over open-source datasets, then it is expected to be competitive on \(\). These methods assume that a VLM's ranking is relatively consistent across tasks.

**Learning-based LOVM**. _The third paradigm_ is based on the learning process. In detail, the ability of a VLM could be predicted based on a ranker model \(f_{R}\). The input of \(f_{R}\) is a vector \(_{m}^{}\), depicting the dataset-specific representation of \(f_{m}\) on \(\), while the output of \(f_{R}\) is the relative/absolute performance \(_{m}^{}\) of \(f_{m}\) on \(\). The \(f_{R}\) could be _learned_ on open-source datasets \(\). Due to the availability of both class names \(C_{}\) and images \(D_{}^{I}\) in the open-source dataset \(\) such as ImageNet , we can calculate each VLM's representation \(\{_{m}^{}\}_{m=1,n=1}^{M,N}\) and true zero-shot image classification accuracy \(\{p_{m}^{n}\}_{m=1,n=1}^{M,N}\). Here \(N\) refers to the number of datasets in \(\). After constructing the train set, the ranker model \(f_{R}\) is learned based on the \(\{_{m}^{n},p_{m}^{n}\}_{m=1,n=1}^{M,N}\):

\[_{f_{R}}\ _{m=1}^{M}_{n=1}^{N}\ (f_{R}(_{m}^{n}),p_{m}^{n}).\] (2)

\(\) is a loss function that measures the discrepancy between the prediction and the ground truth, which can be Mean Squared Error Loss and Huber Loss, among others. Given \(\), the learned \(f_{R}\) is able to predict the performance \(\{_{m}^{}\}_{m=1}^{M}\) over \(\{_{m}^{}\}_{m=1}^{M}\) via \(_{m}^{}=f_{R}(_{m}^{})\). Finally, we can get the predicted VLMs' ranking \(}\) based on \(\{_{m}^{}\}_{m=1}^{M}\). This approach has similarities with meta-learning . Meta-learning attempts to use data from multiple datasets to learn a model adaptable to the target task, while Learning-based LOVM employs data from multiple datasets to learn a ranker model for selecting the suitable model from a VLM Zoo for the target task. The representation \(_{m}^{}\) is one of the keys in this paradigm, and ModelGPT  calculates values \(_{m}^{}\) via the capability of a VLM's text encoder \(f_{m}^{}\).

**ModelGPT** uses generated text data \(_{}\) as substitutes for images to calculate some metrics, which measures the zero-shot ability of \(f_{m}\) on unseen images by the classification ability of \(f_{m}\) on \(_{}\):

\[s_{m,i}^{}=_{i}(f_{m},_{} ).\] (3)

Here \(_{i}\) indicates the \(i\)-th metrics function such as Top-1 Accuracy and F1-Score. For example, the Top-1 Accuracy \(s_{m,1}^{}\) could be calculated in a similar manner as Equation 1, with the only difference being that the test samples were replaced with text samples \(t_{i}\) instead of image samples \(_{i}\):

\[s_{m,1}^{}=_{}|}_{(t_{i},y_{i}) _{}}(y_{i}=f_{m}(t_{i},C_{}) ).\] (4)

Besides, ModelGPT uses some metrics for assessing the features' quality extracted by the VLM's text encoder \(f_{m}^{}\). More details are in the Section B.2. Moreover, the zero-shot classification performance of \(f_{m}\) on ImageNet is also included in \(_{m}^{}\) as a general ability measure of \(f_{m}\). ModelGPT implements \(f_{R}\) as a simple linear model.

### Analysis of the Two Gaps in LOVM

There are two main challenges that limit the application of the aforementioned paradigms in LOVM. The first is the modality gap across different modalities' features in VLM's feature space, and the second is the capability gap between VLM's overall performance and dataset-specific performance.

**Modality Gap.** As described in Section 2.2, methods like H-Score, LogME, and ModelGPT utilize the ChatGPT generated auxiliary texts \(_{}\) as image proxies to calculate metrics that measure the zero-shot accuracy on the target dataset \(\). In other words, the zero-shot classification ability across text and image modalities is estimated by the intra-modality classification ability. The latent assumption is that the generated texts and their corresponding images are closely aligned in VLM's feature space. However, this assumption is difficult to meet , and instances' features are more likely to cluster according to their modalities. In particular, we define the modality gap vector \(\) between the features of an image-text pair \((_{i},t_{i})\) as \(_{m,i}:=f_{m}^{}(_{i})-f_{m}^{}(t_{i})\). Values in the gap vector are generally not close to zero. We name this phenomenon as _Modality Gap_ in LOVM, which makes the scores on \(_{}\) hard to reveal the true zero-shot image classification capability of a VLM on a given dataset.

We conduct a validation experiment on ImageNet with 43 VLMs. We first generate 50 auxiliary texts per class as \(_{}\) and then calculate the predicted Top-1 accuracy via Equation 4. Next, we use test images to calculate the VLM's true Top-1 accuracy. The consistency between the predicted Top-1 accuracy and true zero-shot image classification accuracy \(p_{m,}\) is measured by the Kendall Rank Correlation (\(\), higher is better) and Mean Absolute Error (MAE, lower is better). It can be observed from the left part of Figure 2 that the predicted accuracy derived from auxiliary texts \(_{}\) does not closely match the true accuracy, indicating that these generated auxiliary texts in \(_{}\) are not effective proxies for images.

To make the auxiliary texts act as better image proxies, one intuitive idea is to estimate the gap vector \(\) for each image-text pair. Then we can add it to the feature \(f_{m}^{T}(t_{i})\) of the text \(t_{i}\) to eliminate the modality gap, which may lead to more accurate scores \(s_{m,i}^{}\) in Equation 3. However, the gap vector cannot be calculated directly without the target dataset's images. Furthermore, gap vectors for different classes are diverse, so using a shared vector across all datasets may not be a good choice.

**Capability Gap.** To select one VLM from the model zoo given a target dataset, one direct approach is to select the VLM that performs the best on average across multiple datasets. For example, we may first estimate the VLM's zero-shot classification ability on open-source datasets and then select the VLM with the highest performance. The key question is whether a VLM's average ranking on the open-source datasets can reveal its true ranking on the target dataset. Our empirical analyses indicate that there exists a discrepancy between the VLM's overall ranking and its ranking on a specific dataset. We name the discrepancy between the VLM's average ability and its specific ability as the _Capability Gap_, which results from the fact that a VLM's performance fluctuates significantly across various datasets.

To verify the claim, we test 43 VLMs on 23 target datasets provided by  and obtain the rankings of each VLM across these datasets. Based on these ranking results, we calculate the average standard deviation and the mean value of the difference between each VLM's maximum and minimum ranking. The experiment process is illustrated in the right part of Figure 2. We find that the mean difference between one VLM's maximum and the minimum ranking is 38.86. Since the total number of VLMs is 43, such a difference demonstrates that the top-performing VLM in one dataset could likely be among the worst in another.

One solution to bridge such a capability gap is to consider the VLMs' ranking on a related dataset. In other words, the ranking of VLMs on datasets from open-source datasets collections that are relevant to the target task may provide more useful insights than a general performance ranking across all tasks. The main challenge is to figure out which open-source datasets are similar to the target dataset and transform the VLM's ranking on these datasets to the target dataset.

Figure 2: **Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs’ zero-shot image classification accuracy based on generated text data vs. VLM’s true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM’s maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets.**

**Summary.** We emphasize two kinds of gaps in LOVM, _i.e._, the _modality gap_ across features of different modalities generated by a VLM, and the _capability gap_ between a VLM's overall ranking and its ranking given a specific target dataset. Both two gaps pose obstacles to previous model selection methods, such as LogME and ModelGPT, and degrade their abilities in VLM selection. Moreover, those intuitive approaches to bridge the gaps still face challenges.

## 3 VLM Selection with Gap Bridging

To mitigate the impact of both gaps on LOVM and integrate non-learning-based and learning-based LOVM methods, we propose VLM **S**election **W**ith **gAp**B**ridging (Swab). The key idea of Swab is to bridge modality and capability gaps by utilizing class-level statistics of VLMs from open-source datasets. By measuring the textual similarity between the target dataset's class names and those in open-source datasets, we construct a bridge matrix. Based on it, we estimate the gap vectors between image and text modalities, which rectifies the text-derived scores in ModelGPT. In addition, we predict the VLM's performance ranking for the target dataset based on the bridge matrix and VLM's ranking on the open-source dataset. Both estimated statistics will be used to obtain a more accurate language-only VLM selection. The workflow of Swab is illustrated in Figure 3.

### Construct the Bridge Matrix Using Optimal Transport

Benefiting from the open-source datasets, some useful class-level statistics, such as modality gap vectors and zero-shot classification accuracy of a certain VLM, could be calculated, which can help the performance ranking estimation of a VLM on the target dataset. To better utilize these class-level statistics for predicting the corresponding statistics of a VLM on the target task, we introduce semantic relevance information between open-source datasets' classes and target dataset's classes into the statistics reusing process, which is automatically generated through Optimal Transport [7; 45].

Recall that the sets of class names of the open-source datasets and the target dataset are \(C_{}=\{c_{i}^{}\}_{i=1}^{k_{S}}\) and \(C_{}=\{c_{i}^{T}\}_{i=1}^{k_{T}}\), respectively. The semantic relevance between two classes could be measured by the textual similarity between their class names. In detail, we use a pre-trained text encoder \(\) (_e.g._, MPNet ), which extracts text features for these class names, _i.e._, \(\{(c_{1}^{}),,(c_{k_{S}}^{})\}\)and \(\{(c_{1}^{}),,(c_{k_{}}^{})\}\). Then, we can calculate the cosine similarity \(^{})^{}(c_{j}^{})}{\|(c_{i}^{ })\|\|(c_{j}^{})\|}\) between the text feature of the \(i\)-th class in the open-source datasets \((c_{i}^{})\) and that of the \(j\)-th class in the target dataset \((c_{j}^{})\). The larger the cosine similarity, the more similar the two classes are. After that, we construct the cost matrix in optimal transport via \(_{ij}=1-^{})^{}(c_{j}^{})}{\|(c_{i}^{})\|\|(c_{j}^{})\|}\). In practice, we exponentiate each element in \(_{ 0}^{k_{} k_{}}\) using the base \(e\) to amplify its differences. We then solve the optimal transport problem with the constructed cost matrix to get the transport matrix \(}\). Since optimal transport aims to obtain a transport matrix that minimizes the transmission cost, the transport matrix \(}\) will reuse more information between semantically similar classes:

\[^{*}=*{argmin}_{_{ 0}^{k_{ } k_{}}}_{i,j}_{i,j}\,\,_{i,j}\;,\;\;}=;\;^{T}=;\; _{i,j} 0.\] (5)

The cost matrix quantifies the expense of moving elements between all class pairs, and \(^{*}^{k_{} k_{}}\) is the transport matrix. OT minimizes the cost indicated by the matrix \(\) and moves elements from one distribution \(\) to another \(\). In Swab, we define \(\) and \(\) as uniformly distributed vector \(=/k_{}^{k_{}}\) and \(=/k_{}^{k_{}}\). This indicates that we treat all classes as equally important. We may also incorporate prior knowledge of class importance to define \(\) and \(\).

The solution \(^{*}\) of the OT problem in Equation 5 could be solved efficiently , and \(^{*}\) acts as a bridge matrix between open-source datasets' classes and target dataset's classes. Usually, the smaller \(_{i,j}\) is, the larger the corresponding element \(^{*}_{i,j}\) obtained by OT, indicating statistics of the \(i\)-th class of open-source datasets may help more when we estimate the statistics of the \(j\)-th target class.

### Bridge the Modality Gap and Capability Gap

**Bridge the Modality Gap.** Given the \(m\)-th VLM \(f_{m}\) in the model zoo, we want to estimate the modality gap \(_{m,j}^{}\) between the extracted image and text features for the \(j\)-th class in the target dataset \(\) to bridge the modality gap. However, in the zero-shot scenario, we can't get the target dataset's images in advance, so we can't directly calculate the gap vectors using image-text pairs. To solve this problem, Swab estimates the target dataset's gap vectors based on the open-source datasets' gap vectors with \(^{*}\). Given the \(k\)-th open-source class \(c_{k}^{}\), we can get the set of images \(D_{_{k}}^{I}=\{(_{i},y_{i})\,|\,(_{i},y_{i}) D _{}^{I},\;y_{i}=c_{k}^{}\}\) from the open-source datasets. \(|D_{_{k}}^{I}|\) is the number of images in \(D_{_{k}}^{I}\). Then, the modality gap vector \(_{m,k}^{}\) for class \(c_{k}^{}\) and model \(f_{m}\) can be calculated through \(_{m,k}^{}=_{k}}^{I}|}_{( _{i},y_{i}) D_{_{k}}^{I}}(^{I}(_{i}) }{\|f_{m}^{I}(_{i})\|}-^{I}(c_{k}^{})}{\|f_{m}^{I }(c_{k}^{})\|})_{m,k}^{}\) is the average difference between the normalized class text prototype embedding and all normalized image embeddings from the class \(c_{k}^{}\). In a similar manner, the gap vectors of all open-source classes \(\{_{m,1}^{},,_{m,k_{}}^{}\}\) can be obtained given \(f_{m}\). We use a matrix \(_{m}^{}^{k_{} d_{m}}\) to represent those \(k_{}\) gap vectors for the \(m\)-th VLM in the VLM Zoo, and \(d_{m}\) is the dimensionality of features extracted by \(f_{m}\).

The gap vectors \(\{_{m,1}^{},,_{m,k_{}}^{}\}\) for the target dataset could be estimated based on the \(_{m}^{}\) and the transport matrix \(^{*}\). If two classes are semantically similar, then we can reuse the gap vector from the similar class. We set the predicted gap vector for the \(j\)-th target class \(}_{m,j}^{}\) as a weighted sum of \(_{m}^{}\), and the weight comes from \(^{*}\), which is \(}_{m,j}^{}=|C_{}|(^{*}_{:,j})^{ }_{m}^{}^{*}_{:,j}\) is the \(j\)-th column of \(^{*}\). We use scaling factors \(|C_{}|\) to ensure that for each target class, the sum of \(^{*}_{:,j}\) equals 1. This scale operation has also been used in previous work [62; 63]. After that, we modify the step of ModelGPT in Equation 3, where the metrics over the generated auxiliary texts \(_{}\) are calculated. We add the gap vector \(}_{m,j}^{}\) to the embeddings of the auxiliary texts \(_{}^{j}\) from the \(j\)-th class in the target dataset:

\[}_{m,i}=f_{m}^{T}(t_{i})+}_{m,j}^{}\;,\;\;  t_{i}_{}^{j}\;.\] (6)

The modified text embedding \(}_{m,i}\) serves as better image proxies. In other words, classification metrics on \(f_{m}^{T}(t_{i})\) only reveal the discerning ability of the text encoder of \(f_{m}\), which is far from the (cross-modal) zero-shot classification ability due to the modality gap. By bridging such a gap with modified text embedding, classification metrics on \(}_{m,i}\)_are closer to the classification metrics on images with textual classifier_. Therefore, we use \(\{}_{m,1},\}\) in Equation 6 as better 

[MISSING_PAGE_FAIL:8]

CIFAR100  and so on. We obtain VLM's ground truth ranking based on VLM's Top-1 Accuracy calculated on the target dataset's test image set.

**Baseline.** We select representative methods for each of the three paradigms mentioned in Section 2.2 as our baselines. For the first paradigm, we use four classic model selection methods: H-Score , NCE , LEEP  and LogME . For the second paradigm, we use the VLM's ranking on ImageNet (INB) and VLM's average ranking (Avg Rank) on classes of the open-source datasets in the LOVM Benchmark. For the third paradigm, we compare our method with ModelGPT .

**Evaluations.** We use Top-5 Recall and Kendall's Rank Correlation to measure the similarity between the predicted and the ground truth model rankings to evaluate the LOVM method's performance. We also calculate the sum of these two metrics to consider the method's comprehensive capability.

**Implementation Details.** For a fair comparison, Swab follow ModelGPT  to sequentially extract a target dataset from each of the 23 datasets in the LOVM Benchmark and treat the remaining datasets as open-source datasets. Besides, Swab adopts ModelGPT's approach of adding Gaussian noise to corrupt the target dataset's generated text embeddings. Since LOVM does not provide the specific image data for the 23 datasets, we download these datasets ourselves and adopt their standard data splits. Using the templates provided by LOVM, we construct classifiers and recalculate each VLM's zero-shot image classification accuracy on these datasets. Additionally, we utilize the code provided by LOVM to generate class-related text data using ChatGPT. For H-Score, NCE, LEEP, LogME, INB, and Avg Rank, we follow the practices of previous work and do not add noise to the model's inputs. To ensure reliable results, we conduct ten repeated experiments using random seeds from 1 to 10 and report the mean value and standard deviation of ModelGPT's performance and Swab's performance in Table 1.

**Results Analysis.** From Table 1, we can draw the following conclusions: (1) Metric-based non-learning model selection methods such as LogME show poor performance on the LOVM Benchmark. This is primarily because such algorithms rely on the target dataset's images, thus the modality gap has a greater negative impact on them when using generated text data as a substitute for images. (2) Using open-source datasets is helpful for LOVM. We find that using open-source datasets in a non-learning way (_e.g._ INB, Avg Rank) or a learning way (_e.g._ ModelGPT) all helps LOVM, since their performance significantly surpasses that of methods not utilizing open-source datasets (_e.g._ LogME). (3) Despite leveraging more open-source datasets, the performance of Average Rank is worse than INB. This confirms our analysis of the Capability Gap, which suggests a discrepancy between the average ranking of a VLM and its ranking on a specific dataset. (4) Our Swab _achieves the best performance across all evaluation metrics_. Notably, our final performance of \(R_{5}+\) (0.822) represents a significant improvement of 14.8% over the original SoTA method ModelGPT (0.716).

### Ablation Study

We conduct an ablation study to demonstrate that bridging the Modality Gap and Capability Gap are both essential for Swab. Table 2 presents our experiment results, from which we can observe that Swab achieves the best performance across all metrics when both gaps are bridged simultaneously. The ablation study confirms our analysis.

   Methods & H-Score & NCE & LEEP & LogME & INB & Avg Rank & ModelGPT & Swab \\  \(R_{5}()\) & 0.174 & 0.235 & 0.161 & 0.191 & 0.443 & 0.443 & 0.446\(\)0.004 & **0.504\(\)**0.000 \\ \(()\) & 0.000 & -0.014 & 0.014 & -0.014 & 0.267 & 0.246 & 0.270\(\)0.009 & **0.318\(\)**0.002 \\  \(R_{5}\) + \(()\) & 0.174 & 0.221 & 0.175 & 0.177 & 0.710 & 0.689 & 0.716\(\)0.011 & **0.822\(\)**0.002 \\   

Table 1: **Results on LOVM Benchmark.** We evaluate our method across 23 datasets and 43 pre-trained VLMs. The results are averaged over all datasets. Our Swab achieves the best results across all metrics. For methods that involve adding random noise to data features, we report the standard deviation of metrics across 10 experiments to mitigate the impact of randomness on result reliability.

   Method & \(R_{5}()\) & \(()\) & \(R_{5}+()\) \\  Swab-C & 0.487\(\)0.012 & 0.296\(\)0.018 & 0.783\(\)0.019 \\ Swab-M & 0.474\(\)0.006 & 0.316\(\)0.019 & 0.790\(\)0.017 \\ Swab & **0.504\(\)**0.000 & **0.318\(\)**0.002 & **0.822\(\)**0.002 \\   

Table 2: **Ablation Study of Swab. Swab-C, Swab-M, and Swab indicates only bridging the Capability Gap, only bridging the Modality Gap, and bridging both gaps in Swab.**

### Influence of Key Components in Swab

**Will Bridge the Capability Gap Be Beneficial for VLM Selection?** We compare the LOVM performance directly using the VLM's average ranking on each class of open-source datasets and weighted-sum ranking based on transport matrix \(^{*}\). The results are shown in Table 3. We can find that _utilizing class relevance to bridge the Capability Gap is beneficial for VLM's Model Selection._

**Will Bridge the Modality Gap Be Beneficial for VLM Selection?** To eliminate the interference of other factors, we solely utilize the learning-based predicted rankings \(_{m}^{,(1)}\) in Swab, and the input to the ranker model \(f_{m}\) only consists of metrics calculated on the generated text data \(_{}\), which serves as substitutes for images. In this way, the method's performance depends solely on the quality of the generated text data \(_{}\). From the Table 4, we can find that the generated text data \(_{}\) become better substitutes for image data after bridging the Modality Gap.

**Which Kind of Gap Vectors Should We Use?** When utilizing the gap vectors from open-source datasets, we have two options: (1) Use the dataset-level mean gap vector calculated on the whole dataset's image-text pairs. (2) Use the class-level mean gap vector calculated on the corresponding class's image-text pairs. We hope that the gap vectors are as close to each other as possible so that their mean vector can substitute well for the whole set. Based on this idea, we calculate the statistics of the gap vectors within a dataset and within each class. We calculate three metrics which include: (1) the standard deviation of these gap vectors' magnitude; (2) the mean cosine similarity between these gap vectors and their corresponding mean gap vectors; and (3) the standard deviation of these cosine similarities. These metrics reflect the consistency of the gap vectors. Table 5 shows the results.

From the Table 5, we can find that the class-level gap vectors tend to be more consistent, which inspires us to use the class-level mean gap vectors. We also compare the results of Swab-M on the LOVM Benchmark using the dataset-level mean gap vectors and the class-level mean gap vectors, respectively. The implementation details are the same as Table 2. Table 6 shows the results, which verifies that using the class-level mean gap vectors is a better choice.

## 5 Conclusion

We analyze and address two key challenges in Language-Only VLM Selection (LOVM), which are VLM's modality gap across different modal features and VLM's Capability gap between its overall and dataset-specific rankings. Our key insight is that we can reuse the model's useful statistics on open-source datasets to help the model selection on the target dataset. Swab utilizes a transport matrix between classes of the target dataset and open-source datasets to transfer VLM's class-specific modality gap vectors and class-specific rank from open-source datasets to the target dataset, which mitigates the negative impacts of these two gaps. Experiment results on the LOVM benchmark show the superiority of our method.

   Gap Vector & \(R_{5}()\) & \(()\) & \(R_{5}\) + \(()\) \\  Dataset Mean & 0.443 & 0.304 & 0.747 \\ Class Mean & **0.474** & **0.316** & **0.790** \\   

Table 6: Results of Swab-M on the LOVM Benchmark using the dataset-level mean gap vectors and class-level mean gap vectors.

   Method & \(R_{5}()\) & \(()\) & \(R_{5}\) + \(()\) \\  Average Rank & 0.443 & **0.246** & 0.689 \\ OT Weighted Rank & **0.513** & 0.217 & **0.730** \\   

Table 3: Results of \(_{m}^{,(2)}\) on the LOVM before and after bridging the Capability Gap.

   Gap Vector & \(R_{5}()\) & \(()\) & \(R_{5}\) + \(()\) \\  Before Bridging MG & 0.216 & 0.061 & 0.277 \\ After Bridging MG & **0.371** & **0.080** & **0.451** \\   

Table 5: Results of metrics measuring gap vectors’ consistency belonging to the same dataset or the same class. M: Magnitude, D: Direction.