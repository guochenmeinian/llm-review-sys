# Non-asymptotic Analysis of Biased Adaptive

Stochastic Approximation

 Sobihan Surendran\({}^{1,2}\), Adeline Fermanian\({}^{2}\), Antoine Godichon-Baggioni\({}^{1}\), Sylvain Le Corff\({}^{1}\)

\({}^{1}\)Sorbonne Universite, CNRS,

Laboratoire de Probabilites, Statistique et Modelisation, Paris, France

\({}^{2}\)LOPF, Califiras' Machine Learning Lab, Paris, France

Corresponding author: sobihan.surendran@sorbonne-universite.fr

###### Abstract

Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models. Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator. In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with biased gradients, converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoencoders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.

## 1 Introduction

Stochastic Gradient Descent (SGD) algorithms are standard methods to train statistical models based on deep architectures. Consider a general optimization problem:

\[^{*}*{argmin}_{^{d}}V()\,\] (1)

where \(V\) is the objective function. Then, Gradient Descent methods produce a sequence of parameter estimates as follows: \(_{0}^{d}\) and for all \(n\),

\[_{n+1}=_{n}-_{n+1} V(_{n})\,\]

where \( V\) denotes the gradient of \(V\) and for all \(n 1\), \(_{n}>0\) is the learning rate. In many cases, it is not possible to compute the exact gradient of the objective function, hence the introduction of vanilla Stochastic Gradient Descent, defined for all \(n\) by:

\[_{n+1}=_{n}-_{n+1}(_{n})\,\]

where \((_{n})\) is an estimator of \( V(_{n})\). For example, in deep learning, stochasticity emerges with the use of mini-batches. While these algorithms have been extensively studied, both theoretically and practically, see, e.g., , many questions remain open. In particular, most results are based on the case where the estimator \(\) is unbiased. Although this assumption is valid in the case of vanilla SGD, it breaks down in many common applications. For example, zeroth-order methods usedto optimize black-box functions  in generative adversarial networks [58; 16] have access only to noisy biased realizations of the objective functions.

Furthermore, in reinforcement learning algorithms such as Q-learning , policy gradient , and temporal difference learning [8; 52; 18], gradient estimators are often obtained using a Markov chain with state-dependent transition probability. These estimators are then biased [69; 23]. Other examples of biased gradients can be found in the field of generative modeling with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) [34; 13]. In particular, the Importance Weighted Autoencoder (IWAE) proposed by , which is an extension of the standard Variational Autoencoder (VAE) , yields biased estimators. Finally, this is also the case in Bilevel Optimization [43; 36; 41] and Conditional Stochastic Optimization [40; 39].

Moreover, in practical applications, vanilla SGD shows difficulties in calibrating the step sequences. Therefore, modern variants of SGD employ adaptive steps that use past stochastic gradients or Hessians to avoid saddle points and deal with ill-conditioned problems. The idea of adaptive steps was first proposed in the online learning literature by  and later adopted in stochastic optimization, with the Adagrad algorithm of .

In this paper, we give non-asymptotic convergence guarantees for modern variants of SGD where both the estimators are biased and the steps are adaptive. To our knowledge, existing results consider either adaptive steps but unbiased estimators [27; 77; 67; 74; 19], or biased estimators with non-adaptive steps [70; 44; 2; 22; 21].

More precisely, our contributions are summarized as follows.

* We provide convergence guarantees for the Biased Adaptive Stochastic Approximation framework, under weak assumptions on the bias. To the best of our knowledge, these are the first convergence results to incorporate adaptive steps in biased Stochastic Approximation.
* In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with a biased gradient, converge to a critical point for non-convex smooth functions with a convergence rate of \(( n/+b_{n})\), where \(b_{n}\) is related to the bias at iteration \(n\). However, we achieve an improved linear convergence rate with the Polyak-Lojasiewicz (PL) condition.
* Finally, we show how our theoretical results apply to several applications with biased gradients. In particular, we show that our hypotheses hold for Stochastic Bilevel Optimization and Conditional Stochastic Optimization, but also for Self-Normalized Importance Sampling estimators or Coordinate Sampling. We also propose a first non-asymptotic bound on the bias of IWAE, which allows us to illustrate through several experiments the effect of bias on the convergence of the optimization, and to show how this effect can be reduced by an appropriate choice of hyperparameters.

**Organization of the paper.** In Section 2, we introduce the setting of the paper and relevant related works. In Section 3, we present the Adaptive Stochastic Approximation framework and the main assumptions. In Section 4, we present our principal results, i.e., convergence rates for the risk when the PL condition is assumed, and on the gradient norm without this hypothesis. We illustrate our results in Section 5. All proofs are postponed to the appendix.

## 2 Setting and Related Works

**Stochastic Approximation.** Stochastic Approximation (SA) methods go far beyond SGD. They consist of sequential algorithms designed to find the zeros of a function when only noisy observations are available. Indeed,  introduced the Stochastic Approximation algorithm as an iterative recursive algorithm to solve the following integration equation:

\[h()=_{}[H_{}(X)]=_{}H_{ }(x)(x)x=0\,\] (2)

where \(h\) is the mean field function, \(X\) is a random variable taking values in a measurable space \((,)\), and \(_{}\) is the expectation under the distribution \(\). In this context, \(H_{}\) can be any arbitrary function. If \(H_{}(X)\) is an unbiased estimator of the gradient of the objective function, then \(h()= V()\). As a result, the minimization problem (1) is then equivalent to solving problem (2), and we can note that SGD is a specific instance of SA. SA methods are then defined as follows:

\[_{n+1}=_{n}-_{n+1}H_{_{n}}(X_{n+1}),\]

where the term \(H_{_{n}}(X_{n+1})\) is the \(n\)-th stochastic update, also known as the drift term, and is a potentially biased estimator of \( V(_{n})\). It depends on a random variable \(X_{n+1}\) which takes its values in \((,)\). In machine learning, \(V\) typically represents the theoretical risk, \(\) the model parameters, and \(X_{n+1}\) the data.

**Adaptive Stochastic Gradient Descent.** SGD can be traced back to , and its averaged counterpart was proposed by . The non-asymptotic analysis of SGD in both convex and strong convex cases can be found in .  prove the convergence of a random iterate of SGD for nonconvex smooth functions, which was already suggested by the results of . They show that SGD with constant or decreasing stepsize \(_{n}=1/\) converges to a stationary point of a non-convex smooth function \(V\) at a rate of \((1/)\) where \(n\) is the number of iterations.

Most adaptive first-order methods, such as Adam , Adadelta , RMSProp , and NADA , are based on the blueprint provided by the Adagrad family of algorithms. The first known work on adaptive steps for non-convex stochastic optimization, in the asymptotic case, was presented by .  proved that Adagrad converges to a critical point for non-convex objectives at a rate of \(( n/)\) when using a scalar adaptive step. In addition,  extended this proof to multidimensional settings. More recently,  focused on the convergence rates for Adagrad and Adam. Furthermore, several modified versions of Adam have been proposed, such as AMSGRAD  and YOGI .

**Biased Stochastic Approximation.** The asymptotic results of Biased SA have been studied by . The non-asymptotic analysis can be found in the reinforcement learning literature, especially in the context of temporal difference (TD) learning, as explored by [8; 52; 18]. The case of non-convex smooth functions has been studied by . The authors establish convergence results for the mean field function at a rate of \(( n/+b)\), where \(b\) corresponds to the bias and \(n\) to the number of iterations. For strongly convex functions, the convergence of SGD with biased gradients can be found in , specifically addressing the case of Martingale noise with a constant step size.

[46; 21] introduce a novel assumption, known as "Expected Smoothness", which is the weakest assumption compared to the existing literature on biased SGD that we extend to cover the adaptive case. The authors provide convergence results in the case of non-convex smooth functions. Convergence results with assumptions on the control of bias and MSE can be found in [56; 22]. Applications of biased gradients can be found in Bilevel Optimization [43; 36; 41] and Conditional Stochastic Optimization [40; 39]. Moreover, biased gradients are also used in various other applications [38; 54; 6; 56]. Finally,  studied convergence results of biased gradients with Adagrad in the Markov chain case, focusing on the norm of the gradient of the Moreau envelope while assuming the boundedness of the objective function.

Our analysis provides non-asymptotic results in a more general setting, for a wide variety of objective functions and adaptive algorithms and treating both the Martingale and Markov chain cases.

## 3 Adaptive Stochastic Approximation

### Framework

Consider the optimization problem (1) where the objective function \(V\) is assumed to be differentiable. In this paper, we focus on the following SA algorithm with adaptive steps: \(_{0}^{d}\) and for all \(n\),

\[_{n+1}=_{n}-_{n+1}A_{n}H_{_{n}}(X_{n+1}),\] (3)

where \(_{n+1}>0\) and \(A_{n}\) is a sequence of symmetric and positive definite matrices. In a context of biased gradient estimates, choosing

\[A_{n}=[ I_{d}+\,_{k=0}^{n}H_{_{k}}(X_ {k+1})H_{_{k}}(X_{k+1})^{}]^{-1/2}\]

can be assimilated to the full Adagrad algorithm . However, computing the square root of the inverse becomes expensive in high dimensions, so in practice, Adagrad is often used with diagonal matrices. This approach has been shown to be particularly effective in sparse optimization settings. Denoting by \((A)\) the matrix formed with the diagonal terms of \(A\) and setting all other terms to 0, Adagrad with diagonal matrices is defined in our context as:

\[A_{n}=[ I_{d}+_{n}(X_{1:n+1},_{0:n}) ]^{-1/2}\,,\] (4)

where

\[_{n}(X_{1:n+1},_{0:n})=_{k=0}^{n}H_{_{k}}( X_{k+1})H_{_{k}}(X_{k+1})^{}.\]

In RMSProp , \(_{n}(X_{1:n+1},_{0:n})\) in (4) is an exponential moving average of the past squared gradients, defined by:

\[_{n}(X_{1:n+1},_{0:n})=(1-)_{k=0}^{n}^{n-k}H_{_{ k}}(X_{k+1})H_{_{k}}(X_{k+1})^{},\]

where \(\) is the moving average parameter. Furthermore, when \(A_{n}\) is a recursive estimate of the inverse Hessian, it corresponds to the Stochastic Newton algorithm .

### Assumptions

Consider the following assumptions.

* There exists a constant \(>0\) such that for all \(^{d}\), \[2V()-V(^{*})\| V( )\|^{2}\,.\]

H1 corresponds to the Polyak-Lojasiewicz condition, which is weaker than strong convexity and remains satisfied even when the function is non-convex. It ensures uniqueness of the minimizer \(^{*}\). The PL condition has been extensively studied theoretically  and has been verified empirically in many applications, such as over-parameterized deep networks  and Linear Quadratic Regulator models .

* The objective function \(V\) is \(L\)-smooth. For all \((,^{})^{d}^{d}\), \[\| V()- V(^{})\| L \|-^{}\|\,.\]

This assumption is crucial to obtain our convergence rate and is very common see, e.g., [59; 10]. Under this assumption, for all \((,^{})^{d}^{d}\),

\[V() V(^{})+ V (^{}),-^{}+ \|-^{}\|^{2}\,.\] (5)

* Biased Gradients: There exist two non-increasing positive sequences \((_{n})_{n 1}\) and \((r_{n})_{n 1}\) such that for all \(n\), \[ V(_{n}),A_{n}H_{_{ n}}(X_{n+1})_{n+1}( [\| V(_{n})\|^{2}]-r_{n+1})\,\,.\]
* Expected Smoothness: there exists a non-increasing non-negative sequence \((_{n}^{2})_{n 1}\), and positive constants \(_{1}\), \(_{2}\) such that for all \(n\), \[H_{_{n}}(X_{n+1})^{2}_ {n}^{2}+_{1}\| V(_{n})\|^{2}+ _{2}V(_{n})-V(^{*})\,.\]

In this assumption, \(r_{n+1}\) represents an additive bias term, generally of the order of the square of the bias, and \(_{n+1}\) may depend on the minimum eigenvalue of \(A_{n}\). In [21, Theorem 2], it has been demonstrated that this assumption is weaker than the alternatives used in the literature on biased SGD. We have extended these assumptions to the adaptive case. It is important to note that the first point of H3 depends on the application (objective function \(V\)) and on the adaptive algorithm (matrix \(A_{n}\)) that we want to use. The purpose of this assumption is to provide a more general framework that covers all possible applications and adaptive algorithms. In the biased SGD setting, if the bias term \(\|[H_{_{n}}(X_{n+1})_{n}]- V(_ {n})\|\) is bounded by \(_{n+1}\), we can easily verify the first point of H3 by considering \(_{n+1}=1/2\) and \(r_{n+1}=_{n+1}^{2}\). We show in Section 4.3 that this assumption is also verified in algorithms such as Adagrad and RMSProp. The second point of H3 is a weaker assumption compared to bounding the variance of the noise term. Applications where we can verify these assumptions are discussed in Appendix D.

We finally consider an additional assumption on \(A_{n}\). Let \(\|A\|\) be the spectral norm of a matrix \(A\).

**H4**: There exists \((_{n})_{n 1}\) such that for all \(n\), \(\|A_{n}\|:=_{}(A_{n})_{n+1}\).

In our setting, since \(A_{n}\) is assumed to be a symmetric matrix, the spectral norm is equal to the largest eigenvalue. H4 plays a crucial role, as the estimates may diverge when this assumption is not satisfied. Given a sequence \((_{n})_{n 1}\), one way to ensure that H4 is satisfied is to replace the random matrices \(A_{n}\) with

\[_{n}=\|,_{n+1}\}}{\|A_{n}\|}A_{n}\.\] (6)

It is then clear that \(\|_{n}\|_{n+1}\). Furthermore, in most cases, especially for Adagrad, RMSProp and Stochastic Newton, control of \(_{}(A_{n})\) in H4 is satisfied. For example, in Adagrad and RMSProp, in (4), we have \(_{}(A_{n})^{-1/2}\).

## 4 Convergence Results

### Convergence under the PL condition

In this section, we study the convergence rate of SGD with biased gradients and adaptive steps under the PL condition. We give below a simplified version of the bound we obtain on the risk and refer to Theorem A.2 in the appendix for a formal statement with explicit constants.

**Theorem 4.1**.: _Assume that H1 - H4 hold. Let \(_{n}^{d}\) be the \(n\)-th iterate of the recursion (3) and \(_{n}=C_{}n^{-},_{n}=C_{}n^{},_{n}=C_{ }n^{-}\) with \(C_{}>0,C_{}>0\), and \(C_{}>0\). Assume that \(,, 0\) and \(+<1\). Then,_

\[[V(_{n})-V(^{*})]= (n^{-+2+}+r_{n}).\] (7)

The rate obtained is classical and shows the tradeoff between a term coming from the adaptive steps (with a dependence on \(\), \(\), \(\)) and a term \(r_{n}\) which depends on the control of the bias. To minimize the right hand-side of (7), we would like to have \(==0\). For example, it is verified in the case of Adagrad and RMSProp if the gradients are bounded, as will be discussed later.

We stress that Theorem 4.1 applies to any adaptive algorithm of the form (3), with the only assumption being H4. Without any information on these eigenvalues, the choice that \(_{n} n^{}\) and \(_{n} n^{-}\) allows us to remain very general, which can even be seen as a worst-case scenario. Finally, note that non-adaptive SGD is a particular case of Theorem 4.1. Thus, our theorem gives new results also in the non-adaptive case with generic step sizes and biased gradients with decreasing bias.

### Convergence without the PL condition

In the non-convex smooth case, theoretical results are generally based on a randomized version of SA, as described in [60; 32; 44]. Instead of considering the final parameter \(_{n}\), we introduce a random variable \(R\), which takes its values in \(\{1,,n\}\), and the quantity of interest becomes \(_{R}\). Note that this procedure is a technical tool, in practical applications we use classical SA. The following theorem provides a bound in expectation on the gradient of the objective function \(V\), which is the best we can have given that no assumption is made about the existence of a global minimum of \(V\).

**Theorem 4.2**.: _Assume that H2 - H4 hold. Assume also that for all \(k\), we have \(_{k+1}_{k+1}/(_{1}L_{k+1}^{2})\). For any \(n 1\), let \(R\{0,,n\}\) be a discrete random variable such that:_

\[(R=k):=_{k+1}_{k+1}}{_{j=0}^{n}w_{j+1} _{j+1}_{j+1}}\,\]

_where \(w_{k+1}=_{j=1}^{k+1}(1+_{2}_{j})^{-1}\) with \(_{j}=L_{j}^{2}_{j}^{2}/2\). Then,_

\[[\| V(_{R})\|^{2}] 2 +_{1,n}+_{2,n}}{_{j=0}^{n}w_{j+1}_{j+1} _{j+1}}\,\]_where_

\[_{1,n}=_{k=0}^{n}w_{k+1}_{k+1}_{k+1}r_{k+1}\,\ _{2,n}= _{k=0}^{n}w_{k+1}_{k+1}_{k}^{2},\ \ \ \ V^{*}=[V(_{0})-V(^{*})]\.\]

If \(_{2}=0\), Theorem 4.2 recovers the asymptotic convergence rate obtained by  with respect to the hyperparameters \(\), \(\), and \(\), and to the bias. We can observe that if \(+2\), the condition on \((_{k})_{k 1}\) can be met simply by tuning \(C_{}\). In particular, if \(A_{n}=I_{d}\), the requirement on the step sizes can be expressed as \(_{k+1} 1/(_{1}L)\).

We give below the convergence rates obtained from Theorem 4.2 under the same assumptions on \(_{n}\), \(_{n}\), and \(_{n}\) as in Theorem 4.1.

**Corollary 4.3**.: _Assume that H2-H4 hold. Let \(_{n}=C_{}n^{-},_{n}=C_{}n^{},_{n}=C_{ }n^{-}\) with \(C_{}>0,C_{}>0\), and \(C_{}>0\). Assume that \(,, 0\) and \(+<1\). Then, if \(_{2}=0\), we have:_

\[\| V(_{R})\|^{2}= (n^{-++2}+b_{n})&-<1/2\,\\ (n^{+-1}+b_{n})&->1/2\,\\ (n^{+-1} n+b_{n})&-=1/2\, \]

_where the bias term \(b_{n}\) can be constant or decreasing. In the latter case, writing \(r_{n}=C_{r}n^{-r}\), we have:_

\[b_{n}=(n^{-r})&r++<1 \,\\ (n^{+-1})&r++>1\,\\ (n^{+-1} n)&r++=1\.\]

In practice, the value of \(r\) is known in advance while the other parameters can be tuned to achieve the optimal rate of convergence. In any scenario, we can never achieve a bound of \((1/+b_{n})\), and the best rate we can reach is \(( n/+b_{n})\) when \(=1/2,=0\), and \(=0\). In this case, all eigenvalues of \(A_{n}\) must be bounded from both below and above. Note that we could also have obtained such a rate by taking \(_{n}=n^{-1/2}\) and \(_{n}=n^{-1/2}\) while keeping \(_{n}\) constant. However, the assumption that \(_{n}=n^{-1/2}\) is too strong (fast decay of the eigenvalues of \(A_{n}\)), hence our choice of \(_{n}=C_{}n^{}\). Finally, for a decreasing bias, if \(r 1/2\), the bias term contributes to the convergence rate of the algorithm. Otherwise, the other term is the leading term of the upper bound. In both cases, the best achievable bound is \(( n/)\) if \(r 1/2\).

**Bounded Gradient Case.** Now, we analyze the convergence of Randomized Adaptive Stochastic Approximation when the stochastic updates are bounded, as given by the following assumption.

* There exists \(M 0\) such that for all \(n\), \(\|H_{_{n}}(X_{n+1})\| M\).

Boundedness of the stochastic gradient of the objective function is a classical assumption in adaptive stochastic optimization [67; 74; 19; 73].

**Corollary 4.4**.: _Assume that H2-H5 hold. Let \(_{n}=C_{}n^{-},_{n}=C_{}n^{},_{n}=C_{ }n^{-}\) with \(C_{}>0,C_{}>0\), and \(C_{}>0\). Assume that \(,, 0\) and \(+<1\). For any \(n 1\), let \(R\{0,,n\}\) be a uniformly distributed random variable. Then,_

\[[\| V(_{R})\|^{2}] +_{1,n}^{}+LM^{2}_{2,n}^{}/2}{}\,\]

_where \(_{1,n}^{}=_{k=0}^{n}_{k+1}_{k+1}r_{k+1}\), \(_{2,n}^{}=_{k=0}^{n}_{k+1}^{2}_{k+1}^{2}\), and \(V^{*}=[V(_{0})-V(^{*})]\)._

Importantly, in Corollary 4.4, there are no assumptions on the step sizes, and we obtain a better bound than in Theorem 4.2.

### Application to Adagrad and RMSProp

We give a convergence analysis of Adagrad and RMSProp with a biased gradient estimator. First, note that, under H5, for all eigenvalues \(\) of \(A_{n}\), the adaptive matrix in Adagrad or RMSProp, it holds that \((M^{2}+)^{-1/2}^{-1/2}\), i.e., H4 is satisfied with \(=0\) and \(=0\).

**Corollary 4.5**.: _Assume that H2 and H5 hold. Let \(_{n}=c_{}n^{-1/2}\) and \(A_{n}\) denote the adaptive matrix in Adagrad or RMSProp. For any \(n 1\), let \(R\{0,,n\}\) be a uniformly distributed random variable. Suppose that for any \(n 1\), there exist positive constants \(\) and \(C_{}\) such that:_

\[\|[H_{_{n}}(X_{n+1})|_{n} |- V(_{n})\|. C_{}n^{- }\.\] (8)

_Then,_

\[[\| V(_{R})\|^{2}]= (}+b_{n})\,\]

_where the bias \(b_{n}\) is explicitly given in Appendix A.5._

In the case of an unbiased gradient, we obtain the same bound of \(( n/)\) as in  under the same assumptions. If the bias is of the order \((n^{-1/4})\), the algorithm achieves the same convergence rate as in the case of an unbiased gradient.

### AMSGRAD with Biased Gradients

Finally, we show the convergence of AMSGRAD  with a biased gradient estimator. At each iteration, AMSGRAD uses an exponential moving average of past gradients instead of the current gradient as in Equation (3), which is detailed in Algorithm 1. The key difference between Adam and AMSGRAD lies in their handling of the second moment estimate. Specifically, AMSGRAD uses the updated term \(_{k}=(_{k-1},(V_{k}))\) instead of directly using \(V_{k}\), with the maximum taken coordinate-wise. This approach is crucial, as it ensures that the eigenvalues of \(A_{n}\) decrease at each iteration. The following theorem provides a bound in expectation on the gradient of the objective function \(V\) using randomized iterations with AMSGRAD.

**Theorem 4.6**.: _Assume that H2, H3 (i), and H5 hold. Let \(_{n}=c_{}n^{-1/2}\), \(A_{n}\) denote the adaptive matrix of AMSGRAD in Algorithm 1, and \(_{1},_{2}[0,1)\). For any \(n 1\), let \(R\{0,,n\}\) be a uniformly distributed random variable. Then,_

\[[\| V(_{R})\|^{2}]= (}+b_{n})\,\]

_where \(b_{n}\) corresponds to the bias which comes from \(r_{n}\) in H3(i). Choosing \(r_{n}=C_{r}n^{-r}\), we get:_

\[b_{n}=(n^{-r})&r<1/2\,\\ (n^{-1/2})&r>1/2\,\\ (n^{-1/2} n)&r=1/2\.\]

If the bias is of the order \((n^{-1/4})\), we achieve a convergence rate of \(( n/)\), which is the same as that of an unbiased gradient  and similar to that of Adagrad and RMSProp. It is worth noting that our results are also applicable to SGD momentum by taking \(A_{n}=I_{d}\) in Algorithm 1.

``` Input: Initial point \(_{0}\), maximum number of iterations \(n\), step sizes \(\{_{k}\}_{k 1}\), momentum parameters \(_{1},_{2}[0,1)\) and regularization parameter \( 0\).  Set \(m_{0}=0,V_{0}=0\) and \(_{0}=0\) for\(k=0\) to \(n-1\)do  Compute the stochastic update \(H_{_{k}}(X_{k+1})\) \(m_{k}=_{1}m_{k-1}+(1-_{1})H_{_{k}}(X_{k+1})\) \(V_{k}=_{2}V_{k-1}+(1-_{2})H_{_{k}}(X_{k+1})H_{_{k}}(X_{k+ 1})^{}\) \(_{k}=(_{k-1},(V_{k}))\) \(A_{k}=[ I_{d}+_{k}]^{-1/2}\) \(_{k+1}=_{k}-_{k+1}A_{k}m_{k}\) endfor Output:\((_{k})_{0 k n}\) ```

**Algorithm 1** AMSGRAD with Biased Gradients

### Convergence Results in i.i.d. and Markov Chain cases

For illustrative purposes, in this subsection we give the form of the bias of the gradient estimator, denoted by \(_{n}\), in two simple scenarios, i.e., when \(\{X_{n},n\}\) is either an i.i.d. sequence or a Markov chain. For Adagrad, RMSProp, and AMSGRAD, bounding the bias of the gradient estimator is a sufficient condition for verifying H3\((i)\), which in turn enables us to derive convergence results in each scenario.

I.i.d. case.Assume that \(\{X_{n},n\}\) are i.i.d. random variables. If the mean field function \(h(_{n})=[H_{_{n}}(X_{n+1}) _{n}]\) aligns with the true gradient, then the estimator is unbiased. Otherwise, the bias of the gradient estimator is

\[_{n+1}=\|h(_{n})- V(_{n})\|\.\]

Markov Chain case.Assume now that \(\{X_{n},n\}\) is a Markov Chain. The bias consists of two parts: the difference between the mean field function and the true gradient, and a term due to the Markov chain dynamics. For all \(T 0\), we define the stochastic update as follows:

\[H_{_{h}}(X_{k+1})=_{i=1}^{T}H_{_{h}} (X_{k+1}^{(i)}),\]

where \(X_{k+1}^{(i)}\) represents the i-th sample generated at iteration \(k+1\). This multi-sample estimator is commonly used in applications such as Reinforcement Learning, Markov Chain Monte Carlo, and Sequential Monte Carlo methods, effectively reducing the variance of the gradient estimator. The mixing time \(_{}\) of a Markov chain with stationary distribution \(\) and transition kernel \(P\) is characterized as:

\[_{}:=\{t\ ;\ _{x}D_{}(P^{t}(x,),) \},\]

where \(D_{}\) denotes the total variation distance. For an ergodic Markov chain with stationary distribution \(\), the bias of this gradient estimator when using \(T\) samples per step is

\[_{n+1}=\|h(_{n})- V(_{n})\|+M}/T}\,\]

where \(h()= H_{}(x)(dx)\). If the general optimization problem reduces to the following stochastic optimization problem with Markov noise, as considered in most of the literature [28; 24; 7]:

\[_{^{d}}V():=_{x}[f(;x)],\]

where \( f(;x)\) is a loss function, and \(\) is some stationary data distribution of the Markov Chain and \(H_{_{h}}(X_{k+1}^{(i)})= f(_{k};X_{k+1}^{(i)})\), then \(_{n+1}=M}/T}\), similar to SGD with Markov Noise .

## 5 Applications and Experiments

### Bilevel and Conditional Stochastic Optimization

We can now apply our theoretical results in various settings where biased gradients are involved. In particular, they apply to the fields of Stochastic Bilevel Optimization and Conditional Stochastic Optimization. Stochastic Bilevel Optimization consists of minimizing an objective function \(V\) with respect to \(\), where \(V\) is itself a function of \(^{*}()\) and \(^{*}()\) is obtained by solving another minimization problem. Conditional Stochastic Optimization focuses on optimizing the expected value of a function that contains a nested conditional expectation on a random variable \(\). We provide in Table 1 a summary of the assumptions satisfied in these settings, which allow to apply the results of Section 4 and to obtain a \(( n/+b_{n})\) convergence rate in both cases, and explicit forms for \(b_{n}\). To our knowledge, these are the first convergence rates obtained in these settings.

We refer to Appendix D for other examples in which the bias of the estimator can be controlled, in particular Self-Normalized Importance Sampling (Appendix D.1), Sequential Monte Carlo Methods (Appendix D.2), Policy Gradient (Appendix D.3), Zeroth-Order Gradient (Appendix D.4), and Coordinate Sampling (Appendix D.5).

### Experiments with IWAE and BR-IWAE

In this section, we illustrate our theoretical results in the context of deep VAE. The experiments were conducted using PyTorch , and the source code can be found here2. In generative models,

[MISSING_PAGE_FAIL:9]

Then, we illustrate empirically the convergence rates obtained in Corollary 4.5 and Theorem 4.6 for IWAE. Since the bias of the estimator of the gradient in IWAE is of the order \((1/k)\), choosing a bias of order \((n^{-})\) is equivalent to using \(n^{}\) samples at iteration \(n\) to estimate the gradient. We plot in Figure 2 the gradient squared norm \(\| V(_{n})\|^{2}\) and the Negative Log-Likelihood is given in Appendix E.2. Note that all figures are with respect to epochs, whereas here, \(n\) represents the number of updates of the gradient. The dashed curves correspond to the expected convergence rate \((n^{-1/4})\) for \(=1/8\) and \(( n/)\) for \(=1/4\) and \(=1/2\).

We observe that the algorithms converge at the expected theoretical rates, and even faster. In Appendix E.2, we have included an additional experiment on the FashionMNIST dataset , which shows similar behavior, but the convergence is closer to the expected rates, suggesting that our upper bounds may be tight. We see similar convergence rates for Adagrad, RMSProp, and Adam, although, as expected, Adam performs slightly better. Moreover, it is clear that convergence is faster with a larger \(\) but beyond a certain threshold for \(\) the rate of convergence does not change significantly. Since choosing a larger \(\) induces an additional computational cost, it is crucial to choose an appropriate value that achieves fast convergence without being too computationally expensive. Choosing an optimal number of samples at each iteration remains an open problem depending on the chosen generative model.

## 6 Discussion

This paper provides a non-asymptotic analysis of Biased Adaptive Stochastic Approximation with and without the PL condition in the non-convex smooth setting. We derive a convergence rate of \(( n/+b_{n})\) for non-convex smooth functions, where \(b_{n}\) corresponds to the time-dependent decreasing bias, and an improved linear convergence rate with the Polyak-Lojasiewicz (PL) condition. We also establish that Adagrad, RMSProp, and AMSGRAD with biased gradients converge to critical points for non-convex smooth functions. Our results provide insights on hyper-parameters tuning to achieve fast convergence and reduce computational time. A natural extension of this work is the analysis of the assumptions, the bias and convergence rates for specific deep learning architectures. A theoretical analysis of the Monte Carlo effort required at each iteration to obtain an optimal convergence rate is another interesting perspective.