# Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers

Siyu Chen

Department of Statistics and Data Science, Yale University

siyu.chen.sc3226@yale.edu

&Heejune Sheen

Department of Statistics and Data Science,

Yale University

heejune.sheen@yale.edu

&Tianhao Wang

Toyota Technological Institute at Chicago

tianhao.wang@ttic.edu

&Zhuoran Yang

Department of Statistics and Data Science,

Yale University

zhuoran.yang@yale.edu

equal contribution

###### Abstract

In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures. In particular, most existing work only theoretically explains how the attention mechanism facilitates ICL under certain data models. It remains unclear how the other building blocks of the transformer contribute to ICL. To address this question, we study how a two-attention-layer transformer is trained to perform ICL on \(n\)-gram Markov chain data, where each token in the Markov chain statistically depends on the previous n tokens. We analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention, and a feed-forward layer with normalization. We prove that the gradient flow with respect to a cross-entropy ICL loss converges to a limiting model that performs a generalized version of the "induction head" mechanism with a learned feature, resulting from the congruous contribution of all the building blocks. In the limiting model, the first attention layer acts as a _copier_, copying past tokens within a given window to each position, and the feed-forward network with normalization acts as a _selector_ that generates a feature vector by only looking at informationally relevant parents from the window. Finally, the second attention layer is a _classifier_ that compares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output. Our theory is further validated by simulation experiments.

## 1 Introduction

In-context learning (ICL) (Brown et al., 2020) has emerged as a crucial aspect of large language model (LLM) (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Anthropic, 2023; Team et al., 2023) functionality, enabling pre-trained LLMs to solve user-specified tasks during inference without updating model parameters. In ICL, a pre-trained LLM, typically a transformer, receives prompts containing a few demonstration examples sampled from a task-specific distribution and produces the desired output for that task. This capability is noteworthy because the tasks addressed during the ICL might not be part of the original training data set. The success of ICL requires the LLM to perform certain learning processes during inference.

Although many previous works aim to demystify ICL from either empirical or theoretical perspectives, the theoretical foundations of ICL remain elusive. This is primarily due to the complexity of transformer architectures, which integrate token and position embeddings, multiple layers of multi-head softmax attention, layer normalization, and feedforward neural networks. When it comes to understanding how the ICL ability emerges in transformers after training, existing works often focus on simplified models, such as linear attention mechanisms or single-layer transformers (Von Oswald et al., 2023), and ICL tasks are typically confined to linear regression (Akyurek et al., 2023). This leaves a gap in understanding how full-fledged transformer architectures facilitate ICL of more complex tasks, especially when latent causal structures exist among the tokens in a sequence.

In this paper, our aim is to narrow this gap by studying how a two-attention-layer transformer is trained to perform ICL of an \(n\)-gram Markov chain model, where each token in the Markov chain statistically depends on the \(n\) tokens before it, known as the parent set. Specifically, we consider a transformer model with relative positional embedding (RPE) (He et al., 2020), multi-head softmax attention, and a feed-forward network (FFN) layer with normalization. We employ such a transformer model to predict the \((L+1)\)-th token of an \(n\)-gram Markov chain, with the first \(L\) tokens given as the prompt, where \(L+1\) is the sequence length. Here the \(L\)-token sequence is sampled from a random Markov chain model, where a random transition kernel obeying the \(n\)-gram Markov property is used to generate sequences. The token sequence is fed into the transformer model, which outputs a probability distribution over the vocabulary set to predict the \((L+1)\)-th token. To train the transformer model, we sample token sequences from these random Markov chain models and minimize the cross-entropy loss between the predicted token distribution and the true token distribution.

Under this setting, we aim to answer the following three questions: (i) _Does the gradient flow with respect to the cross-entropy loss converge during training?_ (ii) _If yes, how does the limiting model perform ICL?_ (iii) _How do the building blocks of the transformer model contribute to ICL?_

Main Results.We provide an affirmative answer to the Question (i) by proving that the gradient flow converges during training. In particular, we identify three phases of training dynamics: in the first stage, FFN learns the potential parent set; in the second stage, each attention head of the first multi-head softmax attention layer learns to focus on a single parent token selected by FFN; and in the final stage, the parameter of the second attention layer increases, and the transformer approaches the limiting model. Moreover, for Questions (ii) and (iii), we show that the limiting model performs a specialized form of exponential kernel regression, dubbed "**generalized induction head**", which requires the congruous contribution of all the building blocks. Specifically, the first attention layer acts as a _copier_, copying past tokens within a given window to each position. The FFN layer acts as a _selector_ that generates a feature vector by only looking at informationally relevant parents from the window according to a modified \(^{2}\)-mutual information. Finally, the second attention layer is an _exponential kernel classifier_ that compares the features at each position with those created for the output position \(L+1\), and uses the resulting similarity scores to generate the desired output. When specialized to the case where \(n=1\), the limiting model selects the true parent token and implements the _induction head_ mechanism (Elhage et al., 2021). In this case, we recover the theory in Nichani et al. (2024). Our theory is complemented by numerical experiments, which validate the three-phase training dynamics and mechanism of generalized induction head.

To our best knowledge, our work is the first to provide a comprehensive understanding of how ICL is empowered by a collaboration of different building blocks in a transformer model. In particular, we identify the pivotal roles played by RPE in the copier component, the FFN layer with normalization in the selector component, and attention in the classifier component. We believe our work will shed light on the theoretical understanding of ICL for more complicated tasks.

Related Works.Our work adds to the rapidly growing literature on understanding in-context learning by transformers. We defer an in-depth discussion on related works in Appendix SSA due to the page limit.

Roadmap.The rest of the paper is organized as follows: We introduce the problem setup of ICL of Markov chains in SS2. Then in SS3, we present the main theoretical results and related discussions. A proof sketch is provided in SSD. Finally, we present corresponding experiment results in SSB, and the detailed proofs are deferred to the Appendix.

Notation.We denote by \(e_{1},,e_{d}\) the standard basis vectors in \(^{d}\) and by \(\) the all-one vector in \(^{d}\). We denote by \(()\) the softmax function such that the \(i\)-th coordinate of \((x)\) is \((x_{i})/_{l=1}^{L}(x_{l})\) for \(x^{L}\). By default, the softmax operation will always be applied row-wise. For any integer \(n>0\), we denote \([n]:=\{1,,n\}\). For a vector \(w^{M}\), we denote by \(w_{i}\) the \(i\)-th entry of \(w\) and \(w_{-i}\) the \((M+1-i)\)-th entry of \(w\) for positive integer \(i[M]\). For a matrix \(W\), we denote by \(W(i,j)\) the entry at the \(i\)-th row and \(j\)-th column of \(W\). For two vectors \(u\) and \(v\), we write \(u/v\) as the vector obtained by taking element-wise division between \(u\) and \(v\). We denote by \(a b\) and \(a b\) the maximum and minimum of \(a\) and \(b\), respectively. We denote by \(x_{s:t}\) the sequence \(\{x_{s},x_{s+1},,x_{t}\}\). For a class \(\), we denote by \(()\) the space of probability measures over \(\). We use the standard big O notation throughout the paper.

## 2 Problem Setup: In-Context Learning of Markov Chains

In this section, we present the details of the problem setting. In particular, we first introduce the statistical problem of ICL of \(n\)-gram Markov chains in SS2.1 and then lay out the details of the transformer model in SS2.2.

### In-Context Learning and \(n\)-Gram Markov Chains

We study how autoregressive transformers are trained to perform in-context learning (ICL). A pre-trained transformer can be viewed as a conditional distribution \(f_{}(\,|\,)\) over a finite vocabulary set \(\), where prompt is a sequence of tokens in \(\). We consider an in-context unsupervised learning problem where the pre-trained transformer \(f_{}\) is used to predict the \((L+1)\)-th token \(x_{L+1}\) with the first \(L\) tokens being the prompt. Here \(L\) is a fixed number and the joint distribution of the sequence \(x_{1:(L+1)}\) is sampled from a random \(n\)-gram Markov chain. In other words, with \(x_{1:(L+1)}\) sampled from some distribution, we evaluate how well \(f_{}(\,|\,x_{1:L})\) predicts the distribution of \(x_{L+1}\).

\(n\)-Gram Markov Chains.We assume the data comes from a mixture of \(n\)-gram Markov chain model, denoted by a tuple \((,,,_{0})\), where \(\) is the state space and \(=(-r_{1},,-r_{n})\) is the parent set with positive integers \(r_{1}<r_{2}<<r_{n}\). That is, for each \(l>r_{n}\), \(x_{l}\) only statistically depends on \((x_{l-r_{n}},,x_{l-r_{1}})\), which is denoted by \(X_{(l)}\) and referred to as the parent tokens of \(x_{l}\). We let \(d=||\) denote the vocabulary size. Moreover, \(\) is a probability distribution over the set of Markov transition kernels respecting the parent structure specified by \(\), and \(_{0}\) is the joint distribution of the first \(r_{n}\) tokens \(x_{1:r_{n}}\). Note that the size of the parent set \(n\) can be smaller than or equal to \(r_{n}\). Thus, the sequence \(x_{1:(L+1)}\) is generated as follows: (i) sample initial \(r_{n}\) tokens \((x_{1},,x_{r_{n}})_{0}\), (ii) sample a random transition kernel \(\), where \(^{n}()\), and (iii) sample token \(x_{l}(\,|\,X_{(l)})\) for \(l=r_{n}+1,,L+1\). See Figure 1 for an illustration of the generating model of \(x_{1:(L+1)}\).

Cross-Entropy Loss.When \(x_{1:(L+1)}\) is generated, \(x_{1:L}\) is fed into the transformer \(f_{}\) to predict \(x_{L+1}\). To assess the performance of ICL, we adopt the population cross-entropy (CE) loss

\[(f_{})=-_{,x_{1:(L+1)}} f_{}(x_{L+1}\,|\,x_{1:L})+,\] (2.1)

where \(>0\) is a small constant introduced for numerical stability and in the sequel we will take \(=O(L^{-1/2})\). Here, the expectation is taken with respect to the joint distribution of \(x_{1:(L+1)}\) (including the randomness of \(\)). When setting \(=0\), we note that minimizing this cross-entropy loss is equivalent to minimizing the KL divergence

\[_{,x_{1:L}}((\,|\,X_{ (L+1)})\,\|\,f_{}(\,|\,x_{1:L})).\]

As a remark, we also relax a condition in Nichani et al. (2024) where the last token \(x_{L}\) has to be resampled from a uniform distribution. In addition, our analysis can also be extended to sequential CE loss, which corresponds to predicting every token in the sequence given the past rather than just the last token \(x_{L+1}\). This is closer to the training paradigm used in practice (Brown et al., 2020). See SSC.4 for a further discussion on the sequential CE loss.

Figure 1: A two-gram Markov chain with parent set \(=\{-1,-2\}\).

### A Two-Layer Transformer Model

We consider a class of two-attention-layer transformer model, denoted by \((M,H,d,D)\), which incorporates Relative Positional Embedding (RPE) (He et al., 2020), Multi-Head Attention (MHA) (Vaswani et al., 2017), and a Feed-Forward network (FFN) with normalization. Here \(M\) is an integer that specifies the window size of RPE, \(H\) is the number of heads in the first attention layer, \(d\) is the vocabulary size, and \(D\) is an integer that controls the complexity of FFN. The details of \((M,H,d,D)\) are as follows.

Token Embedding, Input and Output.Note that each token takes values in \(\) with \(d=||\). We embed the tokens into one-hot vectors in \(^{d}\), and thus we can identify \(\) as the canonical basis in \(^{d}\), i.e., \(=\{e_{1},,e_{d}\}\). A transformer model can be viewed as a mapping from \(^{(L+1) d}\) to \(()\). In particular, given the input sequence \(x_{1:L}\), we denote \(X=(x_{1},,x_{L})^{}^{L d}\), and we append a zero vector \(^{d}\) to the sequence, and define \(=(x_{1},,x_{L},)^{}^{(L+1)  d}\). The transformer takes \(\) as input and outputs a probability distribution over \(\).

The First Attention Layer.The input sequence is processed by the first attention layer with \(H\) parallel heads. In all heads, we discard the token information and only use RPE to compute the attention score. Specifically, each attention head \(h\) maps \(\) into a sequence in \(^{d}\) with length \(L+1\), denoted by \(V^{(h)}=(v_{1}^{(h)},,v_{L+1}^{(h)})^{}^{(L+1) d}\). For any \(l[L+1]\), \(v_{l}^{(h)}\) is computed via

\[v_{l}^{(h)}=_{j=1}^{L}_{j}W_{P}^{(h)}(l,) x_ {j}=_{j=1}^{L}W_{P}^{(h)}(l,j) x_{j}}{_ {k=1}^{L}W_{P}^{(h)}(l,k)}.\] (2.2)

That is, we use the RPE parameter \(W_{P}^{(h)}\) to construct a weighted sum over the input sequence at each position \(l[L+1]\). Here \(W_{P}^{(h)}\) is the RPE matrix of the \(h\)-th head.

Feed-Forward Network with Normalization.Following the first attention layer, we concatenate the outputs of the \(H\) attention heads and define \(V=(V^{(1)},,V^{(H)})^{(L+1) Hd}\). Here we abuse the notation and write \(V=(v_{1},,v_{L+1})^{}\), i.e., each \(v_{l}\) is the \(l\)-th row of \(V\). For any vector \(v^{Hd}\), we can split it into \((v^{(1)},,v^{(H)})^{}\) where each block \(v^{(h)}^{d}\). For embedding dimension \(d_{e}\), each vector of \(V\) is passed through an FFN \(():^{Hd}^{d_{e}}\), which specifies a polynomial kernel such that for any \(v,v^{}^{Hd}\), we have

\[(v),(v^{})=_{[H]_{ D}}c_{ }^{2}_{h} v^{(h)},v^{(h)}.\] (2.3)

Here, the low-degree parent set \([H]_{ D}:=\{[H]:|| D\}\) contains all subsets of \([H]\) with cardinality at most \(D\), and \(\{c_{}:[H]_{ D}\}\) are the corresponding trainable parameters of \(()\). Therefore, the FFN \(()\) specifies a kernel on the output of the multihead attention which induces a special inner product structure. While (2.3) characterizes \(()\) implicitly, we provide an explicit construction of \(()\) in Lemma C.1 as a vector-valued mapping whose entries are monomials of the

Figure 2: Illustration of the relationship between RPE vector \(w^{(h)}\) and corresponding matrix \(W_{P}^{(h)}\).

input's entries. Moreover, the complexity of \(()\) is controlled by the maximum degree \(D\), which also influences the embedding dimension \(d_{e}\) as we show in the construction.

Furthermore, to control the magnitude of the FFN outputs, we normalize \(()\) by letting \(u_{l}=(v_{l})/}\) for all \(l[L+1]\), where we define \(C_{D}=_{[H]_{ D}}c_{}^{2}\). Such a normalization scheme is motivated by the standard layer normalization (Ba et al., 2016) in transformer architectures. To motivate the use of \(}\) as the normalization, consider a special case where the positional embeddings, after the softmax function, produce attention weights that are close to one-hot for each head. Then \(v_{l}^{(h)}\) in (2.2) is equal to some token in \(x_{1:L}\). As a result, each \(v_{l}\) consists of \(H\) tokens and

\[\|(v_{l})\|_{2}=[H]_{ D}}c_{}^{2} _{h} v_{l}^{(h)},v_{l}^{(h)}}=}.\]

Thus, \(u_{l}\) is roughly equivalent to the output of the layer normalization \((v_{l})/\|(v_{l})\|_{2}\) (without trainable parameters). Although our theoretical analysis and simulations focus on this simplified version of layer normalization, our additional experiments in SSB.2 demonstrate that it aligns well with the performance of the actual layer normalization.

The Second Attention Layer.The normalized vector sequence \(U=(u_{1},,u_{L+1})^{}\) and the original sequence \(\) are then fed into the second attention layer to generate the final output. In particular, \(u_{L+1}\) is used as the query to compare with the keys \(\{u_{M+1},,u_{L}\}\), and the resulting attention scores are used to aggregate the values \(x_{(M+1):L}\). This attention layer has a single head and a scalar trainable parameter \(a\). We let \(U_{1:L}=(u_{1},,u_{L})^{}^{L d_{e}}\) and denote by \(()\) the mask that sets every entry of the first \(M\) rows of a matrix to be \(-\). The final output is given by

\[y=_{j=M+1}^{L}_{j}a u_{L+1}^{}(U_{1:L} ^{}) x_{j}=_{j=M+1}^{L}a u_{L+1}^{ }u_{j} x_{j}}{_{k=M+1}^{L}\!a u_{L+1}^{ }u_{k}}.\] (2.4)

Note that the softmax function in (2.4) yields a probability distribution over \([L]\) and that \(x_{1:L}\) is a sequence of one-hot vectors. Thus \(y\) in (2.4) is a probability distribution over \(\). The mask operator is included here just to simplify our analysis while in the experiments we are not using the mask.

In summary, given the input \(^{(L+1) d}\), in the matrix form, our transformer model \((M,H,d,D)\) consecutively applies the following operations:

**First Attention:** \[V^{(h)}=(W_{P}^{(h)}) ^{(L+1) d},\; h[H];\] (2.5)
**Concatenate:** \[V=[V^{(1)},,V^{(H)}] ^{(L+1) Hd};\] (2.6)
**FFN & Normalize:** \[U=(V)/} ^{(L+1) d_{e}};\] (2.7)
**Second Attention:** \[y^{}=a u_{L+1}^{}(U_{1:L}^{}) X ^{1 d}.\]

The trainable parameters of the above transformer model are denoted by

\[=a,\{w_{-1}^{(h)},,w_{-M}^{(h)}\}_{h[H]},\{c_{}:[H]_{ D}\}}.\]

We remark that the transformer model in (2.6) is known as a disentangled transformer (Friedman et al., 2024), which is a version of the transformer model that is more amenable for theoretical analysis. One thing to be noted is that there is a residual connection that directly copies \(\) to the output of the FFN & Normalize block, which gives us \([U,]\), and the second attention layer will treat the copied \(\) as the value in the attention mechanism. We omit the residual connection in the above paradigm for notation simplicity. As shown in Nichani et al. (2024), any standard transformer model can be expressed as a disentangled transformer by specializing the attention weights to allow feature concatenation.

Our goal is to investigate whether the transformer model \((M,H,d,D)\) can perform ICL over \(n\)-gram Markov chains and further, whether such capability can be learned from data with common training algorithms like gradient descent.

Theoretical Results

In this section, we present the theoretical results. We first show in SS3.1 and SSC.1 that there exists a transformer in \((M,H,d,D)\) that implements a generalized "induction head" mechanism (Olsson et al., 2022) with a learned feature, which serves as a natural algorithm for learning \(n\)-gram Markov chains. Then in SS3.2 we prove that the gradient flow in (3.4) finds such a desired model asymptotically.

### Generalized Induction Head Mechanism for Learning \(n\)-Gram Markov Chains

Recall that we define the mixture of \(n\)-gram Markov chain model \((,,,_{0})\) in SS2.1, where \(\) is a distribution over the Markov transition kernels. For regularity, we assume existence of a unique stationary distribution for any \(()\), where a rigorous statement is deferred to Assumption 3.5. We also assume the window size \(M>r_{n}\). For any \(n\)-gram Markov chain with transition kernel \(\), we let \(^{}(^{M+1})\) denote the stationary distribution of the Markov chain over a window of size \(M+1\). Here we use \(\{z_{}\}_{l 1}\) to denote a random sequence of tokens generated by the Markov chain. Then \(^{}\) denotes the joint distribution of a block of \(M+1\) tokens \((z_{l-M},,z_{l-1},z_{l})\) under the stationary distribution of \(\), where \(l>M\) is an integer.

In the following, we introduce a generalized induction head (GH) estimator for the task of predicting \(x_{L+1}\) given \(x_{1:L}\), which is based on the following simple idea: \(x_{L+1}\)_should be similar to a previous token \(x_{l}\) if their parents are similar_. As the parent set pa is unknown, GIH adopts an information-theoretic criterion to select a subset of previous tokens as a proxy of the parents. Specifically, GIH uses a modified version of \(^{2}\)-mutual information, which is defined as follows.

**Definition 3.1** (Modified \(^{2}\)-Mutual Information).: _We take a length-\((M+1)\) windows \((z_{l-M},,z_{l-1},z_{l})\) for some \(l>M\) and suppose the sequence is sampled from stationary distribution \(^{}\) with \(\). Let \(Z=(z_{l-M},,z_{l-1})\). For any subset \([M]\), we use \(Z_{-}\) to denote the subvector of \(Z\) containing entries of the form \(z_{l-s}\), \( s\). For instance, suppose \(=\{2,5\}\), then \(Z_{-}=(z_{l-5},z_{l-2})\). The modified \(^{2}\)-mutual information for \(\) is defined as_

\[_{^{2}}()=_{,(z,Z) ^{}}_{e}(z=e\,| \,Z_{-})]^{2}}{^{}(z=e)}-1^{}(Z_{- }),\] (3.1)

_where \(^{}(z=\,|\,Z_{-})\) is the conditional distribution of \(z\) induced by \(^{}\) given the partial history \(Z_{-}\), and \(^{}(Z_{-}),^{}(z)\) are the marginal distributions of \(Z_{-}\) and \(z\) under \((z,Z)^{}\)._

Intuitively, \(_{^{2}}()\) is modified from the vanilla \(^{2}\)-mutual information (\(^{2}\)-MI) between two random variables (Polyanskiy and Wu, 2024) and quantifies how much information the partial history \(Z_{-}\) contains about \(z\). In particular, we incorporate an additional \(^{}(Z_{-})\) term that decreases with the growing size of \(\). To see the rationality, we first introduce a GIH estimator based on the modified \(^{2}\)-mutual information.

**Definition 3.2** (Generalized Induction Head).: _A GIH estimator with window size \(M\), feature size \(D\) is denoted by \((;M,D)\), which maps \(x_{1:L}\) to a distribution over \(\). We let \(^{}\) be the information-optimal subset (referred to as the "information set" in the sequel2) of \([M]\) with size no more than \(D\) that maximizes the modified \(^{2}\)-mutual information \(_{^{2}}()\) defined in (3.1). That is, we define the information set \(S^{}\) as_

\[^{}=_{[M]_{ 0}} _{^{2}}().\] (3.2)

_Then \((x_{1:L};M,D)\) outputs_

\[y^{}:=N^{-1}_{l=M+1}^{L}x_{l}(X_{l- ^{}}=X_{L+1-^{}}),N 1,\\ (L-M)^{-1}_{l=M+1}^{L}x_{l},.\] (3.3)

_Here, we define \(X_{l-^{}}\) as the set \(\{x_{l-s}:s^{}\}\) and \(N=_{l=M+1}^{L}(X_{l-^{}}=X_{L+1-^{ }})\)._

Note that \(^{}\) defined in (3.2) depends on the choices of \(M\) and \(D\) and serves as a proxy of the unknown parent set pa based on \(_{^{2}}()\) defined in (3.1). In a nutshell, the GIH estimator checkswhether the partial histories of \(X_{l-^{}}\) and \(X_{L+1-^{}}\) match and aggregate all the tokens \(x_{l}\) that have a matching partial history to predict \(x_{L+1}\). As a remark, using the modified \(^{2}\)-MI as the information criterion rules out redundancy in the information set \(^{}\) in the following sense:

\(\)\(^{}\) **cannot be a superset of the true parents.** Note that if \(\) is a superset of the true parent set, by the Markov property, \(z\) and \(Z_{-S}\) are conditionally independent given the true parents \(Z_{}\). Thus, maximizing the vanilla \(^{2}\)-mutual information yields multiple maximizers, i.e., all the supersets of the true parent set. However, with the modification in (3.1), any superset yields a strictly smaller \(_{^{2}}\) compared to the exact parent set, making them suboptimal.

\(\)**The modified \(^{2}\)-MI selects informative partial history.** Even a true parent may bear relatively little information about the target compared to other parents sometimes. Meanwhile, exact match of a larger set of partial history becomes much harder as it tends to appear less frequently in the context sequence, leading to poor estimation accuracy for the estimator in (3.3). The modified \(^{2}\)-MI reaches a balance by selecting the informative partial history while penalizing the size of the information set.

The term involving \(^{}(z=\,|\,Z_{-})\) can be viewed as the _signal_ part which helps us to find an informative subset \(\). The term \(^{}(Z_{-})\) can be viewed as _penalty on the model complexity_ which favors smaller subsets. Thus, the modified \(^{2}\)-MI strikes a balance between these two objectives and enables us to find a good proxy \(^{}\) of pa when \(L\) is finite. Moreover, when \(L\) is sufficiently large, we identify two scenarios in which maximizing \(_{^{2}}()\) yields the true parent set (see SSC.7 for details). Moreover, the GIH estimator is a generalization of the induction head mechanism (Elhage et al., 2021) to the stochastic setting with multiple parents, where we give the model more flexibility to learn based on a partial history that does not necessarily correspond to the true parent set. As we will show in SSC.1, the GIH mechanism can be implemented by the transformer model.

### Convergence Guarantee of Gradient Flow

In the following, we present the convergence guarantee for gradient flow. To simplify the discussion, we consider the case where \(H=M\), meaning there are enough heads to implement the GIH mechanism by having each head copy a unique parent token from a window of size \(M\). Let us first introduce the paradigm of training by gradient flow.

Training Paradigm.Consider training a transformer \((M,H,d,D)\) in (2.5) with \(M=H\) to perform ICL on the \(n\)-gram Markov chain model introduced in SS2.1. Specifically, we define \(()\) as the population cross-entropy loss in (2.1), where the transformer model \(f_{}\) is given by (2.5) with a parameter \(\). Ideally, when training the parameter \(\) with gradient flow, the dynamics with respect to the loss \(()\) is given by:

\[_{t}(t)=-(t).\] (3.4)

We consider a three-stage training paradigm where, in each stage, only a specific subset of the weights is trained by gradient flow. The three stages are outlined in Table 1. Specifically, in the first stage, we only train the FFN layer via gradient flow while keeping other weights fixed. We then only train the RPE weights in the first attention layer in the second stage. Finally, we only train the weight \(a\) in the second attention layer in the last stage, while fixing the rest of the parameters. This training approach is primarily used for analytical convenience; in practice, the entire model can be trained simultaneously, and similar convergence results are reported in SSB.2. From a theoretical standpoint, we will also justify the three-stage paradigm in the discussion following Theorem 3.6.

Initialization Conditions.Before presenting our main results about how training by gradient flow induces the GIH structure, let us introduce the following assumption on the initialization of the weights. We define the _information gap_ within the \(D\)-degree parent set \([H]_{ D}\) as

\[_{^{2}}=_{^{2}}(^{})- _{S[H]_{ D}\{^{}\}}_{^{2 }}(),\] (3.5)

where we recall that \(^{}\) defined in (3.2) maximizes the modified \(^{2}\) mutual information.

**Assumption 3.3** (Initialization).: _We assume that the following holds at initialization:_1. _For the first attention layer's RPE weights,_ \(w_{-h}^{(h)} w_{-j}^{(h)}+ w\) _for all_ \(h,j[H]\) _with_ \(j h\)_, where_ \( w>0\) _is a positive scalar satisfying_ \[ w(M-1)-1+_{^{} }/(14_{^{2}}(^{}))^{}-1 .\] (3.6)
2. _The scalar parameter_ \(a\) _in the second attention layer satisfies_ \(0<a O(L^{-3/2})\)_._

The first assumption on the RPE is used to induce the correspondence between parents and heads during the training by slightly breaking the symmetry between different attention heads. The second assumption on the scale of \(a\) ensures that the attention probability given by the second attention layer is close to the uniform distribution over \([L]\). These initialization conditions enable us to derive clean descriptions for the dynamics of the first attention layer and the FFN, shedding light on their respective roles in executing ICL.

We now outline our assumptions on the Markov chain used in the data generation process. Recall that \(r_{n}\) is the largest absolute integer in the parent set pa. For any position \(l\), we define the history \(Z=(z_{l-r_{n}},,z_{l-1})\) as the last state and \(Z^{}=(z_{l-r_{n}+1},,z_{l})\) as the current state. Since the parent of the new token \(z_{l}\) is already included in \(Z\), \(Z^{}\) is independent of all prior history given \(Z\), forming a Markov chain.

We define \(P_{}\) as the \(d^{r_{n}} d^{r_{n}}\) transition matrix for this Markov chain, where states are successive \(r_{n}\)-tokens. Each row of \(P_{}\) is indexed by \(Z^{}\) and each column by \(Z\). The matrix element \(P_{}(Z^{},Z)\) is thus given by

\[P_{}(Z^{},Z)=(z^{}_{l} Z_{(l)})(Z^{}_{l-r_{n}+1:-1}=Z_{l-r_{n}+1:-1}).\]

This means that to transition from \(Z\) to \(Z^{}\), all elements of \(Z^{}\) except for \(z^{}_{-1}\) must match the last \(r_{n}-1\) tokens of \(Z\). The token \(z^{}_{l}\) is then sampled according to the transition kernel \(\) and depends only on the parent \(Z_{(l)}\). The above definition is in fact independent of the position \(l\) as the transition kernel \(\) is the same across all positions. Note that \(P_{}\) is also a stochastic matrix but with zero entries due to the indicator. To proceed, we need the following notion of primitive matrix to state our assumption on \(P_{}\).

**Definition 3.4** (Primitive Matrix).: _A nonnegative and irreducible square matrix \(P\) is called primitive if there exists a positive integer \(k\) such that all entries of \(P^{k}\) are positive._

We defer more details about the above definition to SSC.3. By the celebrated Perron-Frobenius theorem, if a stochastic matrix \(P_{}\) is also primitive, then (i) there exists a unique stationary distribution for the Markov chain; (ii) \(P_{}\) has a unique leading eigenvalue equal to \(1\), and the corresponding eigenvector is the stationary distribution. Next, we state the assumptions on the mixture of Markov chains for data generation.

**Assumption 3.5** (Markov Chain).: _For any \(()\), we assume that:_

1. _The transition matrix_ \(P_{}\) _is primitive. In particular, we assume that there exists_ \(<1\) _such that the eigenvalue of_ \(P_{}\) _with the second largest magnitude satisfies_ \(|_{2}(P_{})|\)_. Note that_ \(_{2}(P_{})\) _can be complex-valued._

   Stage & Weights to Train & Description \\  I & \(\{c_{}\}_{[H]_{ D}}\) in the FFN & Ratio \(c_{^{}}(t)/c_{}(t)\) grows exponentially, \\  & layer & learning the low-degree features with \(^{}\), \\  II & \(\{w^{(h)}\}_{h[H]}\) in the RPE of & \(1-_{h^{}}(_{-h}^{(h)})^{2}\) decays polynomially \\  & the first attention layer, & training each head in \(^{}\) to be a copier, \\  III & \(a\) in the weight of the second & \(a(t)\) experiences a two-stage growth, \\  & attention layer & learning the softmax aggregator for GIH, \\   

Table 1: Three-stage training paradigm for gradient flow. Here, the “Weights to Train” column indicates the weights updated in each stage, and the “Description” column summarizes the corresponding results from Theorem3.6.

_._
2. _There exists_ \(>0\) _such that the transition kernel satisfies_ \((x X_{})\) _for any_ \((x,X_{})\)_._

In fact, the second condition \(( X_{})>\) already ensures that \(P_{}\) must be primitive, as is required by the first condition. See Corollary F.14 for details. On the high level, the first assumption guarantees a unique stationary distribution as well as a fast mixing rate of the Markov chain by ensuring a spectral gap for \(P_{}\). The second assumption implies a lower bound on the probability for any set \([M]\) under the stationary distribution, i.e., \(^{}(X_{l-})^{||}\) for any \(l>M\). See Corollary F.15 for details.

Now we are ready to present our main theoretical result on training transformers by gradient flow.

**Theorem 3.6** (Convergence of Gradient Flow).: _Suppose Assumption 3.3 and Assumption 3.5 hold. Consider \(H M\). We set \(=L^{-1/2}\) for the cross-entropy loss and assume \(L\) is sufficiently large. Then the following holds for the three-stage training of gradient flow:_

**Stage I: Parent Selection by FFN.**: _Let_ \(C_{D}(t)=_{[H]_{ D}}c_{}(t)^{2}\) _and_ \(p_{^{}}(t)=c_{^{}}^{2}(t)/C_{D}(t)\)_. Then in the first stage with duration_ \(t_{1} C_{D}(0) L/(a(0)_{^{2}})\)_, the ratio_ \(c_{^{}}/c_{}\) _grows exponentially fast for any_ \(^{}\)_, and_ \(^{}\) _dominates exponentially fast in the sense that,_

\[1-p_{^{}}(t)(1-p_{^{}}(0)) -(2C_{D})^{-1} a(0) p_{^{}}(0) _{^{2}} t, t[0,t_{1}).\]
**Stage II: Concentration of The First Attention.**: _Define_ \(^{(h)}(t)=(w^{(h)}(t))^{M}\)_, and let_ \(_{}(t):=_{h^{}}^{(h)}_{-h}(t)\)_. Then in the second stage with duration_ \(t_{2}-t_{1} L/(a(0)_{^{2}})\)_, the first layer's attention heads have attention probabilities concentrated on the optimal information set_ \(^{}\) _in the sense that for any_ \(t[t_{1},t_{1}+t_{2})\)_,_

\[1-_{h^{}}(^{(h)}_{-h}(t))^{2}^{}|(M-1)}{a(0)_{^{2}} _{}(0)(t-t_{1})/2+( w)+(M-1)} 1.\]
**Stage III: Growth of The Second Attention.**: _For some constants_ \(c_{1},c_{2}\) _depending on_ \((,^{})\) _with_ \(0<c_{1}<c_{2}\)_, there exists a small constant_ \(>0\) _such that the growth of_ \(a(t)\) _exhibits the following two sub-stages: (i) When_ \(a(t)(c_{1}/)\)_, it holds that_ \( a(t) e^{a(t)}\)_; (ii) After_ \(a(t)\) _has grown such that_ \(a(t)(c_{2}/)\)_, then_ \(_{t}a(t) 1/a(t)\) _until it reaches the value_ \( L/8\)_._

See SSD for a proof sketch and SSE for the detailed proof. We require that \(L\) is sufficiently large, and the specific conditions for \(L\) are deferred to SSE.1.

**Interpretation of Training Dynamics.** We empirically verify Theorem 3.6 by conducting a simulation experiment. In particular, we train a transformer with \(H=M=3\) and \(D=2\) based on Markov chain data with \(d=2\), \(L=100\) and \(=\{-1,-2\}\). We sample the transition kernel from a Dirichlet prior such that \(^{}=\{1,2\}\) also matches the parent set. For more details on this simulation, see SSD. The results are shown in Figure 3 and align perfectly with Theorem 3.6. From Theorem 3.6, we can interpret the three stages of training dynamics as follows.

* In the first stage, the training of FFN parameters learns a _selector_ that selects an informative set \(^{}\) by realizing the corresponding feature embedding through the polynomial kernel. That is, when \(t\) is sufficiently large, we have \(p_{^{}}(t) 1\) and \(p_{}(t) 0\) for all \(^{}\). In this case, for any input vectors \(v,v^{}^{Hd}\), the inner product in (2.3) reduces to \[(v),(v^{}) c_{^{}}^{2} _{h^{}} v^{(h)},v^{(h)}.\] That is, FFN only selects the blocks in \(^{}\) as the feature. We observe this phenomenon in the experiment, where we set \(^{}=\{1,2\}\). As shown in Figure 3-(a), it is clear that \(c_{^{}}\) immediately dominates the rest of \(c_{}\)'s within only a few gradient epochs.
* In the second stage, we update the parameters of the RPE. This stage turns the first attention layer into a _copier_ by establishing the correspondence between the attention heads and the parents in the selected \(^{}\). That is, each attention head copies a particular parent in \(^{}\). Specifically, when \(t\) is sufficiently large, for any \(h^{}\), \(^{(h)}(t)=(w^{(h)}(t)) 1\). Recalling the construction of RPE, this implies that \(v^{(h)}_{l}\) in (2.2) becomes \(x_{l-h}\) for all \(h^{}\). As shown in Figure 3-(b), in the experiment, the first two heads initialized towards the first two parents will deterministically copy parents \(-1\) and \(-2\) eventually. The third head stays close to its initial value. This head has a negligible effect on the output because \(3^{}\) and \(p_{^{}} 1\).
* After the first two stages are completed, we know that the features constructed approximately satisfy (C.1) up to a proportionality factor. Then, in the final training stage, the scalar weight \(a\) in the second attention layer keeps increasing. Thus, this stage learns an exponential kernel _classifier_ as specified in (C.2). When \(a(t)\) is sufficiently large, the learned transformer is close to a classifier that uses covariate-label pairs of the form \((X_{l-^{}},x_{l})\) to predict \(x_{L+1}\). In particular, when \(a(t)\) goes to infinity, the transformer exactly becomes the GIH mechanism given in Definition 3.2. Moreover, we theoretically prove that the increasing trajectory of \(a(t)\) has two stages, where \(a(t)/t\) is initially large and gradually decays, this is also clearly observed in the experiment. See Figure 3-s(c) for details.

In summary, we theoretically show that the limiting model obtained by three-stage training approximately implements the GIH mechanism. We will prove that the difference between these two estimators is at most \(O(L^{-1/8})\). We defer the formal statement and proof to SSE.5. Moreover, as an answer to the Question (iii) raised in SS1, the different components of the transformer architecture are all critical for achieving this: FFN with normalization realizes the _selector_, the multi-head design of attention supports the _copier_, and finally, the softmax operation facilitates the exponential kernel _classifier_. These components work organically as a whole system, yielding the trained transformer's capability of ICL of \(n\)-gram Markov chains.

Another takeaway from Theorem 3.6 is a strict separation in the growth rate of these three stages. In particular, the convergence rates of the corresponding components of the transformer model in these three stages range from exponentially fast (Stage I), polynomially fast (Stage II), to logarithmically slow (Stage III). With such two exponential separations of convergence rates, we expect that these three stages naturally arise when we simultaneously train the whole model via gradient descent/flow. We empirically verify this argument and the details are deferred to SSB.2.

In SSC.7, we provide more intuitive interpretation of the modified \(^{2}\)-mutual information, which demonstrates a balance of model complexity and information richness.

## 4 Conclusion and Future Work

In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning \(n\)-gram Markov chains in an in-context way. Our work opens new directions for developing a rigorous understanding of the transformer models, which includes understanding the induction head mechanism with standard FFN layer and investigating the training dynamics beyond a single loop of this induction head mechanism. We defer readers to SSC.8 for more discussions.

Figure 3: An illustration of the transformer parameters during the three-stage training. We train a transformer in \((M=3,H=3,d=3,D=2)\) with \(L=100\), \(=\{-1,-2\}\). See §B and Figure 4 for more details of the simulation.

Acknowledgement

We acknowledge Shaobo Wang for his help with the experiments. We also thank Jason D. Lee, Alex Damian, and Eshaan Nichani for their helpful discussions. Zhuoran Yang acknowledges the support of NSF under the award DMS-2413243.