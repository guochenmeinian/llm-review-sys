# Inserting Anybody in Diffusion Models via Celeb Basis

Ge Yuan\({}^{1,2,3,4}\)1 Xiaodong Cun\({}^{4}\) Yong Zhang\({}^{4}\) Maomao Li\({}^{4}\)2 Chenyang Qi\({}^{5}\)

**Xintao Wang\({}^{4}\) Ying Shan\({}^{4}\) Huicheng Zheng\({}^{1,2,3}\)3\({}^{}\)**

\({}^{4}\) Tencent AI Lab \({}^{5}\) The Hong Kong University of Science and Technology

###### Abstract

Exquisite demand exists for customizing the pretrained large text-to-image model, _e.g._ Stable Diffusion, to generate innovative concepts, such as the users themselves. However, the newly-added concept from previous customization methods often shows weaker combination abilities than the original ones even given several images during training. We thus propose a new personalization method that allows for the seamless integration of a unique individual into the pre-trained diffusion model using just _one facial photograph_ and only _1024 learnable parameters_ under _3 minutes_. So we can effortlessly generate stunning images of this person in any pose or position, interacting with anyone and doing anything imaginable from text prompts. To achieve this, we first analyze and build a well-defined celeb basis from the embedding space of the pre-trained large text encoder. Then, given one facial photo as the target identity, we generate its own embedding by optimizing the

Figure 1: Given a single facial photo (\(v1\) or \(v2\)) as a tunable sample, the proposed method can insert this identity into the trained text-to-image model, _e.g._, Stable Diffusion , where the new person (v1) can act like the original concept in the trained model and interact with another newly trained concept (\(v2\)). Note that the input images are randomly generated from StyleGAN .

weight of this basis and locking all other parameters. Empowered by the proposed celeb basis, the new identity in our customized model showcases a better concept combination ability than previous personalization methods. Besides, our model can also learn several new identities at once and interact with each other where the previous customization model fails to. Project page is at: http://celeb-basis.github.io. Code is at: https://github.com/ygtxr1997/CelebBasis.

## 1 Introduction

Vast image-text pairs  during training and the powerful language encoder  enable the text-to-image models [1; 5; 6] to generate diverse and fantastic images from simple text prompts. Though the generated images are exquisite, they may still fail to satisfy the users' demands since some concepts are not easy to be described by the text prompt . For instance, individuals frequently post self-portraits on social media platforms, where the pre-trained text-to-image models struggle to produce satisfactory pictures of them despite receiving comprehensive instructions. This shortcoming makes these models less attractive to general users.

Recent works [8; 9; 10; 7] solve this problem via efficiently tuning the parameters of the model for personalization usage. For example, these techniques insert the new concepts (_e.g_. a specific bag, a dog, or a person) into the model by representing them as rarely-used pseudo-words  (or text-embeddings [9; 7; 8; 11]) and finetuning the text-to-image model with a few of these samples [10; 7]. After training, these pseudo-words or embeddings can be used to represent the desired concept and can also perform some combination abilities. However, these methods often struggle to generate the text description-aligned image with the same concept class (_e.g_. the person identities) [8; 10]. For instance, the original Stable Diffusion  model can successfully generate the image of the text prompt:" _Barack Obama and Anne Hathaway are shaking hands._" Nevertheless, in terms of generating two newly-learned individuals, the previous personalized methods [8; 10; 7] fall short of producing the desired identities as depicted in our experiments (Fig. 5).

In this work, we focus on injecting the most specific and widely-existing concept, _i.e_. the human being, into the diffusion model seamlessly. Drawing inspiration from the remarkable 3DMM , which ingeniously represents novel faces through a combination of mean and weight values derived from a clearly defined basis, we build a similar basis to the embeddings of the celebrity names in pretrained Stable Diffusion. In this way, are capable of representing any new person in the trained diffusion model via the basis coefficients.

We first collect a bunch of celebrity names from the Internet and filter them by the pre-trained text-to-image models . By doing so, we obtain 691 well-known names and extract the text embedding by the tokenizer of the CLIP . These celeb names corresponds to over \(700\) samples in LAION datasets  (see our appendix for detailed statistics), containing powerful priors. Then, we construct a _celeb basis_ via Principal Component Analysis (PCA ). To represent a new person with PCA coefficients, we use a pre-trained face recognition network  as the feature extractor of the given photo and learn a series of coefficients to re-weight the celeb basis, so that the new face can be recognized by the pre-trained CLIP transformer encoder. During the process, we only use a single facial photo and fix the denoising UNet and the text encoder of Stable Diffusion to avoid overfitting. After training, we only need to store the 1024 coefficients of the celeb basis to represent the newly-added identity since the basis is shared across the model. Though simple, the concept composition abilities  of the trained new individual is well-preserved, as we only reweight the text embeddings of the trained CLIP model and freeze the weights in the diffusion process. Remarkably, the proposed method has the ability to produce a strikingly realistic photo of the injected face in any given location and pose. Moreover, it opens up some new possibilities such as learning multiple new individuals simultaneously and facilitating seamless interaction between these newly generated identities as shown in Fig. 1. The contributions of the paper are listed as follows:

* We propose celeb basis, a basis built from the text embedding space of the celebrities' names in the text-to-image model and verify its abilities, such as interpolation.
* Based on the proposed celeb basis, we design a new personalization method for the text-to-image model, which can remember any new person from a single facial photo using only 1024 learnable coefficients.

* Extensive experiments show our personalized method has more stable concept composition abilities than previous works, including generating better identity-preserved images and interacting with new concepts.

## 2 Related Work

**Image Generation and Editing.** Given a huge number of images as the training set, deep generative models target to model the distribution of training data and synthesize new realistic images through sampling. Various techniques have been widely explored, including GAN [15; 2], VAE [16; 17], Autoregressive [6; 18; 19; 20], flow [21; 22]. Recently, diffusion models [23; 24] gain increasing popularity for their stronger abilities of text-to-image generation [5; 6; 1]. Conditioned on the text embedding of pre-trained large language models , these diffusion models are iterative optimized using a simple denoising loss. During inference, a new image can be generated from sampled Gaussian noise and a text prompt. Although these diffusion models can synthesize high-fidelity images, they have difficulties in generating less common concepts  or controlling the identity of generated objects . Current editing methods are still hard to solve this problem, _e.g_. directly blending the latent of objects [26; 27] to the generated background will show the obvious artifacts and bring difficulties in understanding the scenes correctly . On the other hand, attention-based editing works [29; 30; 31; 32] only change the appearance or motion of local objects, which can not generate diverse new images with the same concept (_e.g_. human and animal identity) from the referred image.

**Model Personalization.** Different from text-driven image editing, tuning the model for the specific unseen concept, _i.e_. personalized model, remembers the new concepts of the reference images and can synthesize totally unseen images of them, _e.g_. appearance in a new environment, interaction with other concepts in the original stable diffusion. For generative adversarial networks [2; 33], personalization through GAN inversion has been extensively studied. This progress typically involves finetuning of the generator [34; 35], test-time optimization of the latents , or a pre-trained encoder . Given the recent diffusion generative model [1; 5], it is straightforward to adopt previous GAN inversion techniques for the personalization of diffusion models. Dreambooth  finetunes all weight of the diffusion model on a set of images with the same identity and marks it as the specific token. Meanwhile, another line of works [7; 9; 38] optimizes the text embedding of special tokens (_e.g_. \(V^{*}\)) to map the input image while freezing the diffusion model. Later on, several works [8; 39; 40] combine these two strategies for multi-concept interaction and efficient finetuning taking less storage and time. These methods focus on general concepts in the open domain while struggling to generate interactions between fine-grained concepts, _i.e_.human beings with specific identities. Since most of the previous works require the tuning in the test time, training inversion encoders are also proposed to generate textual embedding from a single image in the open domain (_e.g_. UMM , ELITE , and SuTI ), or in human and animal domain (_e.g_. Taming-Encoder , Instant-Booth , E4T ). However, a general human identity-oriented embedding is difficult to be obtained from a naively optimized encoder, and tuning the Stable Diffusion on larger-scale images often causes the concept forget. In contrast, our method focuses on a better representation of identity embedding in the diffusion model (celeb basis in Sec. 3.1), which significantly eases the process of optimization such that we only need 1024 parameters to represent an identity more correctly as in Sec. 3.2 and stronger concept combination abilities, which can converge in only 3 minutes.

**Identity Basis.** Representing the human identity via basis is not new in traditional computer vision tasks. _e.g_. in human face modeling, 3D Morphable Models  and its following models [46; 47] scans several humans and represent the shape, expression, identity, and pose as the PCA coefficients , so that the new person can be modeled or optimized via the coefficients of the face basis. Similar ideas are also used for face recognition , where the faces in the dataset are collected and built on the basis of the decision bound. Inspired by these methods, our approach takes advantage of the learned celebrity names in the pre-trained text-to-image diffusion model, where we build a basis on this celebrity space and generate the new person via a series of learned coefficients.

## 3 Method

Our method aims to introduce a new identity to the pre-trained text-to-image model, _i.e_. Stable Diffusion , from a single photo via the optimized coefficients of our self-built celeb basis. So that it can memorize this identity and generate new images of this person in any new pose and interact with other identities via text prompts. To achieve this, we first analyze and build a celeb basis on the embedding space of the text encoder through the names of the celebrities (Sec. 3.1). Then, we design a face encoder-based method to optimize the coefficients of the celeb basis for text-to-image diffusion model customization (Sec. 3.2).

### Celeb Basis

**Preliminary: Text Embeddings in Text-to-Image Diffusion Models.** In the text-to-image model, given any text prompts \(u\), the tokenizer of typical text encoder model \(e_{}\), _e.g_. BERT  and CLIP , divides and encodes \(u\) into \(l\) integer tokens by order. Correspondingly, by looking up the dictionary, an embedding group \(g=[v_{1},...,v_{l}]\) consisting of \(l\) word embeddings can be obtained, where each embedding \(v_{i}^{d}\). Then the text transformer \(_{}\) in \(e_{}\) encodes \(g\) and generates text condition \(_{}(g)\). The condition \(_{}(g)\) is fed to the conditional denoising diffusion model \(_{}(z_{t},t,_{}(g))\) and synthesize the output image following an iterative denoising process , where \(t\) is the timestamp, \(z_{t}\) is a soil image or latent at \(t\). Previous text-to-image model personalization methods [7; 10; 9] have shown the importance of text embedding \(g\) in personalizing semantic concepts. However, in text-to-image models' personalization, they only consider it as an optimization goal [9; 7; 8], instead of improving its representation.

**Interpolating Abilities of Text Embeddings.** Previous works have shown that text embedding mixups  benefit text classification. To verify the interpolation abilities in text-to-image generation, we randomly pick two celebrity names embeddings \(v_{1}\) and \(v_{2}\), and linearly combine them as \(= v_{1}+(1-)v_{2}\), where \(0<<1\). Interestingly, the generated image of the interpolated embedding \(\) also contains a human face as shown in Fig. 3, and all the generated images perform well in acting and interacting with other celebrities. Motivated by the above finding, we build a celeb basis so that each new identity can lie in the space formed by celebrity embeddings.

**Build Celeb Basis from Embeddings of the Collected Celebrities.** As shown in Fig. 2, we first crawl about 1,500 celebrity names from Wikipedia as the initial collection. Then, we build a manual filter based on the trained text-to-image diffusion model  by constructing the prompts of each name and synthesizing images. A satisfied celeb name should have the ability to generate human images

Figure 3: The interpolated text-embedding of two celebrities is still a human (top row) and it also can perform strong concept combination abilities in the pretrained Stable Diffusion  (bottom row).

Figure 2: The building process of the proposed Celeb Basis. First, we collect about 1,500 celebrity names as the initial collection. Then, we manually filter the initial one to \(m=691\) names, based on the synthesis quality of text-to-image diffusion model  with corresponding name prompt. Later, each filtered name is tokenized and encoded into a celeb embedding group \(g_{i}\). Finally, we conduct Principle Component Analysis to build a compact orthogonal basis, which is visualized on the right.

with prompt-consistent identity and interact with other celebs in synthesized results. Overall, we get \(m=691\) satisfied celeb names where each name \(u_{i},i\{1,...,m\}\) can be tokenized and encoded into a celeb embedding group \(g_{i}=[v_{1}^{i},...,v_{k_{i}}^{i}]\), notice that the length \(k_{i}\) of each celeb embedding group \(g_{i}\) might not the same since each name may contain multiple words (or the tokenizer will divide the word into sub-words). To simplify the formula, we compose the nonrepetitive embeddings so that each \(g_{i}\) only contains the first two embeddings (_i.e_. \(k_{i}=2\) for all \(m\) celebrities). Using \(_{1}\) and \(C_{2}\), _i.e_. \(_{k}=[v_{k}^{1},...,v_{k}^{m}]\), to denote the first and second embeddings of each \(g_{i}\) respectively, we can roughly understand them as the _first name and last name embedding sets_.

To further build a compact search space, inspired by 3DMM  which uses PCA  to map high-dimensional scanned 3D face coordinates into a compact lower-dimensional space, for each embedding set \(C_{k}\), we calculate its mean \(_{k}=_{i=1}^{m}v_{k}^{i}\) and PCA mapping matrix \(B_{k}=(C_{k},p)\), where \(_{k}^{d}\) and \((X,p)\) indicates the PCA operation that reduces the second dimension of matrix \(X^{m d}\) into \(p\)\((p<d)\) principal components, _i.e_. \(B_{k}=[b_{k}^{1},...,b_{k}^{p}]\). As shown in Fig. 2, the mean embedding \(_{k}\) still represents a face and we can get the new face via some coefficients applied to \(B_{k}\).

Overall, our celeb basis is defined on two basis \([_{1},B_{1}]\) and \([_{2},B_{2}]\) working like the first and last name. We use the corresponding principle components \(A_{1}\) and \(A_{2}\) (where \(A_{k}=[_{k}^{1},...,_{k}^{p}]\)) to represent new identities. Formally, for each new person \(\), we use two \(p\)-dimensional coefficients of the celeb basis and can be written by:

\[=[_{1},_{2}],_{k}=_{k}+_{x= 1}^{p}_{k}^{x}b_{k}^{x},\] (1)

In practice, \(p\) equals 512 as discussed in the ablation experiments.

To control the generated identities, we optimize the coefficients with the help of a face encoder as the personalization method. We introduce it in the below section.

### Stable Diffusion Personalization via Celeb Basis

**Fast Coefficients Optimization for Specific Identity.** Given a single facial photo, we use the proposed celeb basis to embed the given face image \(x\) of the target identity into the pretrained text-to-image diffusion model as shown in Fig. 4. Since direct optimization is hard to find the optimized weight, we consider using the pre-trained state-of-the-art face recognition models \(F\), _i.e_. ArcFace , to capture the identity-discriminative patterns. In detail, we adopt the \(F\) to extract 512 dimension face embedding as priors. Then a single-layer MLP followed by an \(L_{2}\)-normalization is used to map the face priors into the modulating coefficients \(A_{1}\) and \(A_{2}\). Following the Eq. 1, we can obtain the embedding group \(\) of the \(x\) using the pre-defined basis. By representing the text prompt of \(\) as \(V^{*}\), we can involve \(V^{*}\) to build the training pairs between the text prompt of input face and "_A photo of \(V^{*}\)_", "_A depiction of \(V^{*}\)_", _etc_.. Similar to previous works [10; 7; 8], we only use simple diffusion denoising loss :

\[_{ N(0,1),x,t,g}[\|-_{}(z_{t},t, _{}(g))\|],\] (2)

where \(\) is the unscaled noise sample, \(g\) denotes the text embeddings containing \(\). During training, only the weights of MLP need to be optimized, while other modules, including the celeb basis, face

Figure 4: During training (left), we optimize the coefficients of the celeb basis with the help of a fixed face encoder. During inference (right), we combine the learned personalized weights and shared celeb basis to generate images with the input identity.

encoder \(F\), CLIP transformer \(_{}\), and UNet \(_{}\) are fixed. Thus, the original composition abilities of the trained text-to-image network are well-preserved, avoiding the forgetting problem. Since we only have a single photo, we use color jitter, random resize, and random shift as data augmentations on the supervision to avoid overfitting. Notice that, we find our augmentation method can work well even though there are no regularization datasets which is important in previous methods [10; 7; 8], showing the strong learning abilities of the proposed methods. Since the proposed method only involves a few parameters, it only takes almost _3 minutes_ for each individual on an NVIDIA A100 GPU, which is also much faster than previous methods.

**Testing.** After training, only two groups of coefficients \(A_{1}\) and \(A_{2}\) applied to the principal celeb basis components need to be saved. In practice, the number of principal components of each group is \(p=512\), coming to only _1024 parameters_ and _2-3KB_ storage consumption for half-precision floatings. Then, users can build the prompt with multiple action description prompts (_e.g._ "_A photo of \(V^{*}\) is playing guitar_") to synthesize the satisfied images as described in Fig. 4.

**Multiple Identities Joint Optimization.** Most previous methods only work on a single new concept [9; 10], Custom Diffusion  claim their method can generate the images of multiple new concepts (_e.g._ the sofa and cat). However, for similar concepts, _e.g._ the different person, their method might not work well as in our experiments. Besides, their method is still struggling to learn multiple (\(>3\)) concepts altogether as in their limitation. Differently, we can learn multiple identities (_e.g._ 10) at once using a shared MLP mapping layer as in Fig. 4. In detail, we simply extend our training images to 10 and jointly train these images using a similar process as single identities. Without a specific design, the proposed method can successfully generate the weight of each identity. After training, our method can perform interactions between each new identity while the previous methods fail to as shown in the experiments. More implementation details are in the appendix.

## 4 Experiments

### Datasets and Metrics

**Datasets.** We conduct experiments on the self-collected \(2K\) synthetic facial images generated by StyleGAN . Using synthetic faces as input for assessing the effectiveness of generative models, one can ease the reliance on the dataset foundation of the initial pre-trained text-to-image model. We also perform some experiments on the real photo of the individual, more results and comparisons are shown in the supplemental materials.

**Metrics.** First, we assess the performance of the generated images by utilizing objective metrics. For instance, we calculate the consistency between the prompt and generated image through CLIP score , which is denoted as "**Prompt**" in tables. Additionally, ensuring identity consistency and clarity of facial features are crucial aspects of our task. Therefore, we evaluate identity similarity using a pretrained face recognition encoder  and mark it as "**Identity**". Furthermore, to demonstrate the rationality behind generation, we also calculate the rate of successful face detection ("**Detect**") via a pretrained face detector . Lastly, user studies are conducted to evaluate text-image alignment along with identity and photo qualities. The images having the highest text-alignment scores from a single inference batch are used compare the peak qualitative performance of all the methods.

### Comparing with State-of-the-Art Methods

We compare the proposed method with several well-known state-of-the-art personalization methods for Stable Diffusion , including DreamBooth , Textual-Inversion  and Custom Diffusion . As shown in Fig. 5, given one single image as input, we evaluate the performance of several different types of generation, including the simple stylization, concept combination abilities, and two new concept interactions. Textual inversion tends to overfit the input image so most of the concepts are

    &  &  &  &  \\    & Prompt & Identity & Detect & & & Quality & Text & Identity \\  Textual Inversion  & 0.1635 & **0.2958** & **92.86\%** & 2.23 & 1.88 & 2.55 & 1,536 & 24 \\ Dreambooth  & 0.2002 & 0.0512 & 54.76\% & 3.32 & 3.70 & 2.75 & \(9.83 10^{8}\) & 16 \\ Custom Diffusion  & **0.2608** & 0.1385 & 80.39\% & 3.31 & 3.55 & 2.96 & \(5.71 10^{7}\) & 12 \\  Ours & 0.2545 & 0.2072 & 84.78\% & **3.47** & **4.01** & **3.37** & **1,024** & **3** \\   

Table 1: Quantitative comparisons.

Figure 5: We compare several different abilities between our method and baselines (Textural Inversion , Dreambooth , and Custom Diffusion ).

forgotten. Although dreambooth  and custom diffusion  can successfully generate the human and the concept, the generated identities are not the same as the target image.

Besides visual quality, we also perform the numerical comparison between the proposed method and baselines in Tab. 1. From the table, regardless of the over-fitted Textual-Inversion (highest identity and detect scores but lowest prompt score), our method shows a much better performance in terms of identity similarity and the face detection rate and achieves similar text-prompt alignment as Custom Diffusion . We also plot the generated results on four detailed types in Fig. 6, where the proposed method shows the best trade-off. Notice that, the proposed method only contains very few learning-able parameters and optimizes faster.

Moreover, since identity similarity is very subjective, we generate 200 images from different identities to form a user study. In detail, we invite 100 users to rank the generated images from one (worst) to five (best) in terms of visual quality, prompt alignment, and identities, getting 60k opinions in total. The results are also shown in Tab. 1, where the users favor our proposed method.

### Results on Age, Gender, Head Pose, and Expression

We qualitatively evaluate the robustness of our method to input age, gender, head pose, and light variants in Fig. 7. Besides, modifying the text prompts can edit the age, gender, and pose of the results. We further provide the expression controlling results in the appendix, along with the comparisons with Face Composer . Thanks to the identity disentangling ability of the face recognition  network, the learned representations are highly relevant to the input identities while irrelevant and insensitive to other attributes like age, pose, expression, etc. Furthermore, the learned representation as an interpolation of celeb basis inherits the good characteristic of those "recognized" celeb names, which corresponds to large amount of image-text paired samples  used for pretraining Stable Diffusion .

  Methods & Prompt\(\) & Identity\(\) & Detect\(\) \\  w/o celeb basis & 0.1386 & **0.2528** & 69.28\% \\ w/ 350 names & 0.2214 & 0.2023 & 69.28\% \\ w/o filter & 0.1939 & 0.2037 & 80.62\% \\ w flatten & 0.2026 & 0.1873 & 80.39\% \\ \(p=64\) & 0.2247 & 0.1061 & 76.47\% \\ \(p=256\) & 0.1812 & 0.0656 & 60.13\% \\ \(p=768\) & 0.1380 & 0.0836 & 47.06\% \\ w/o \(F\) & 0.1914 & 0.1896 & 55.56\% \\ w/o aug. & 0.2083 & 0.1931 & 75.16\% \\  Ours (single) & 0.2234 & 0.2186 & 81.05\% \\ Ours (joint) & **0.2545** & 0.2072 & **84.78\%** \\  

Table 2: Ablation studies.

Figure 6: Numerical analysis in terms of the prompt and identity similarity on four prompt types.

Figure 7: Our method is not only robust to large age, gender, head pose, and light variants in the inputs, but also has good ability of controlling age, gender, and head pose in the outputs.

### Ablation Studies

To further evaluate the sub-modules of our proposed method in both building celeb basis (Sec. 4.4.1) and the proposed new personalization method (Sec. 4.4.2), we start from the default settings ('Ours (single)' in Tab. 2) of our method, conducting the ablation study by separately removing each submodule or using a different setting as follows. Due to the space limitation, we give more visual results in the appendix.

#### 4.4.1 Ablation Studies on Celeb Basis

**# of names in celeb basis.** We evaluate the influence of the names to build a celeb basis. In extreme cases, if there is no name and we directly learn the embedding from the face encoder \(F\) (w/o celeb basis), the model is overfitted to the input image and can not perform the concept combination. With fewer celeb names (w/ 350 names), the generated quality is not good as ours baseline (single) as in Fig. 7(a). Besides, the quality of the celeb basis is also important, if we do not filter the names (w/o filter), the performance will also decrease as in Tab. 2.

**Flatten basis _v.s. first_ and last name basis.** In the main method, we introduce our celeb basis as the _first and last name basis_ since each name embedding does not have the same length. We thus involve a more naive way by flattening all the embeddings to build the basis (w/ flatten). As shown in Fig. 7(b) and Tab. 2, the generated images of our first and last name basis understand prompts better.

**Choice of reduction dimension \(p\).** We also evaluate the influence of the number of coefficients \(p\). Considering the \(768\) dimensions of the CLIP text embedding, we vary \(p\) ranging in \(\{64,256,512,768\}\). Note that \(p=768\) can be also considered as 'w/o PCA' since the dimension of text embedding is \(768\). As shown in Tab. 2, the best result is obtained from the baseline choice (\(p=512\)) and we show the differences in the generated images in the appendix.

#### 4.4.2 Ablation Studies on Coefficients Optimization

**W/o face recognition encoder \(F\).** Naively, we can optimize the coefficients \(A_{1},A_{2}\) of the celeb embeddings from back-propagation directly. However, we find the search space is still large to get satisfied results as shown in Fig. 8(a) and Tab. 2 (w/o \(F\)). So we seek help from the pretrained face encoder, which has more discriminative features on the face.

**W/o data augmentation.** Since there is only one single image as the tuning-able sample, we perform some data augmentations as introduced in Sec. 3.2. However, if we remove these augmentations, the generated face becomes vague and unnatural as shown in Fig. 8(b), and the identity of the generated samples is also decreased as shown in Tab. 2.

Figure 8: Ablation studies on building celeb basis.

Figure 9: Ablation studies on coefficient optimization.

**Single _v.s._ joint training.** Our method supports joint training of multiple identities using a single MLP, we evaluate the differences between single training/joint testing and joint training/joint testing. As shown in Tab. 2 and Fig. 8(c), although training individually can also perform some interactions between the two-person, training the images jointly improve the robustness and reduces the over-fitting risk compared with single training, resulting in slightly better results.

## 5 Conclusion & Discussion

We propose a new method to personalize the pre-trained text-to-image model on a specific kind of concept, _i.e_. the human being, with simply single-shot tuning. Our approach is enabled by defining a basis in the domain of the known celebrity names' embeddings. Then, we can map the facial features from the pre-trained face recognition encoder to reconstruct the coefficients of the new identity. Compared with the previous concept injection method, our method shows stronger concept combination abilities, _e.g_. better identity preservation, can be trained on various identities at once, and can produce results where the newly-added humans interact with each other. Besides, the proposed method only requires 1024 parameters for each person and can be optimized in under 3 minutes, which is also much more efficient than previous methods.

**Limitations.** Although our method can successfully generate the images of new identities, it still occurs some limitations. First, the human faces generated by original stable diffusion  intrinsically contain some artifacts, causing somewhat unnatural performance of the proposed method (see our appendix). It might be solved by a more powerful pre-trained text-to-image model (_e.g_. Imagen , IF ) since they can generate better facial details. Second, we only focus on human beings currently. It is also interesting to build the basis of other species, _e.g_. cars, and cats. To this end, we provide some rough experiments (whose implementation details are introduced in the appendix) of extending our method to cars and cats, as shown in Fig. 10. The results show that our method can also generate text-aligned images with good quality without bells and whistles. But the simple try could be improved through using a stronger encoder or crawling more text to construct basis. Third, although the text alignment scores evaluate the quantitative performance, for the editing ability of age, expression, and pose, we do not have a metric result yet. As supplement, we will use facial feature extractors to quantitatively compare the human identity personalized tasks in the future. Fourth, similar to the inference-only method FastComposer , we have also tried to train a model on 50k identities (20 faces per identity without text labels) as an encoder-based method. However, it is hard to converge in a short GPU time within a batch size of eight. We think it might be because we need much more data and computing resources to train a unified or general facial model and we will explore it in the future.

**Ethics Consideration and Broader Impacts.** We propose a new method for model personalization on the human face, which might be used as a tool for deepfake generation since we can generate not only the human face but also interactions between people. However, this prevalent issue is not limited to this approach alone, as it also exists in other generative models and content manipulation techniques. Besides, our personalization person generation method can also ablate the abilities of the erasing concept methods  and other deep fake detection methods .

Figure 10: Through a simple replacement on the encoder and text basis, our method can be extended to more general classes, _e.g_. car and cat.

Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 61976231 and in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2023A1515012853.