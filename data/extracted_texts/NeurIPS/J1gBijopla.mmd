# Quantifying & Modeling Multimodal Interactions:

An Information Decomposition Framework

Paul Pu Liang\({}^{1}\), Yun Cheng\({}^{1,2}\), Xiang Fan\({}^{1,3}\), Chun Kai Ling\({}^{1,8}\), Suzanne Nie\({}^{1}\), Richard J. Chen\({}^{4,5}\), Zihao Deng\({}^{6}\), Nicholas Allen\({}^{7}\), Randy Auerbach\({}^{8}\), Faisal Mahmood\({}^{4,5}\), Ruslan Salakhutdinov\({}^{1}\), Louis-Philippe Morency\({}^{1}\)

\({}^{1}\)CMU, \({}^{2}\)Princeton University, \({}^{3}\)UW, \({}^{4}\)Harvard Medical School, \({}^{5}\)Brigham and Women's Hospital,

\({}^{6}\)University of Pennsylvania, \({}^{7}\)University of Oregon, \({}^{8}\)Columbia University

pliang@cs.cmu.edu, yc6206@cs.princeton.edu

###### Abstract

The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: _How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions?_ To answer these questions, we propose an information-theoretic approach to quantify the degree of _redundancy_, _uniqueness_, and _synergy_ relating input modalities with an output task. We term these three measures as the PID _statistics of a multimodal distribution_ (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.

## 1 Introduction

A core challenge in machine learning lies in capturing the interactions between multiple input modalities. Learning different types of multimodal interactions is often quoted as motivation for many successful multimodal modeling paradigms, such as contrastive learning to capture redundancy [54; 82], modality-specific representations to retain unique information , as well as tensors and multiplicative interactions to learn higher-order interactions [52; 58; 115]. However, several fundamental research questions remain: _How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions?_ This paper aims to formalize these questions by proposing an approach to quantify the _nature_ (i.e., which type) and _degree_ (i.e., the amount) of modality interactions, a fundamental principle underpinning our understanding of multimodal datasets and models .

By bringing together two previously disjoint research fields of Partial Information Decomposition (PID) in information theory [13; 41; 108] and multimodal machine learning [9; 62], we provide precise definitions categorizing interactions into _redundancy_, _uniqueness_, and _synergy_. Redundancy quantifies information shared between modalities, uniqueness quantifies the information present in only one of the modalities, and synergy quantifies the emergence of new information not previously present in either modality. A key aspect of these four measures is that they not only quantify interactions between modalities, but also how they relate to a downstream task. Figure 1 shows a depiction of these four measures, which we refer to as PID statistics. Leveraging insights fromneural representation learning, we propose two new estimators for PID statistics that can scale to high-dimensional multimodal datasets and models. The first estimator is exact, based on convex optimization, and is able to scale to features with discrete support, while the second estimator is an approximation based on sampling, which enables us to handle features with large discrete or even continuous supports. We validate our estimation of PID in \(2\) ways: (1) on synthetic datasets where PID statistics are known due to exact computation and from the nature of data generation, and (2) on real-world data where PID is compared with human annotation. Finally, we demonstrate that these estimated PID statistics can help in multimodal applications involving:

1. **Dataset quantification**: We apply PID to quantify large-scale multimodal datasets, showing that these estimates match common intuition for interpretable modalities (e.g., language, vision, and audio) and yield new insights in other domains (e.g, healthcare, HCI, and robotics).
2. **Model quantification**: Across a suite of models, we apply PID to interpret model predictions and find consistent patterns of interactions that different models capture.
3. **Model selection**: Given our findings from dataset and model quantification, a new research question naturally arises: _given a new multimodal task, can we quantify its PID values to infer (a priori) what type of models are most suitable?_ Our experiments show success in model selection for both existing benchmarks and completely new case studies engaging with domain experts in computational pathology, mood prediction, and robotics to select the best multimodal model.

Finally, we make public a suite of trained models across \(10\) model families and \(30\) datasets to accelerate future analysis of multimodal interactions at https://github.com/pliang279/PID.

## 2 Background and Related Work

Let \(_{i}\) and \(\) be sample spaces for features and labels. Define \(\) to be the set of joint distributions over \((_{1},_{2},)\). We are concerned with features \(X_{1},X_{2}\) (with support \(_{i}\)) and labels \(Y\) (with support \(\)) drawn from some distribution \(p\). We denote the probability mass (or density) function by \(p(x_{1},x_{2},y)\), where omitted parameters imply marginalization. Key to our work is defining estimators that given \(p\) or samples \(\{(x_{1},x_{2},y):_{1}_{2}\}\) thereof (i.e., dataset or model predictions), returns estimates for the amount of redundant, unique, and synergistic interactions.

### Partial Information Decomposition

Information theory formalizes the amount of information that one variable provides about another . However, its extension to \(3\) variables is an open question [34; 70; 93; 106]. In particular, the natural three-way mutual information \(I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)\)[70; 93] can be both positive and negative, which makes it difficult to interpret. In response, Partial information decomposition (PID)  generalizes information theory to multiple variables by decomposing \(I_{p}(X_{1},X_{2};Y)\), the total information \(2\) variables \(X_{1},X_{2}\) provide about a task \(Y\) into 4 quantities (see Figure 1): redundancy \(R\) between \(X_{1}\) and \(X_{2}\), uniqueness \(U_{1}\) in \(X_{1}\) and \(U_{2}\) in \(X_{2}\), and synergy \(S\) that only emerges when both \(X_{1}\) and \(X_{2}\) are present. We adopt the PID definition proposed by Bertschinger et al. :

\[R=_{q_{p}}I_{q}(X_{1};X_{2};Y),\] (1) \[U_{1}=_{q_{p}}I_{q}(X_{1};Y|X_{2}), U_{2}= _{q_{p}}I_{q}(X_{2};Y|X_{1}),\] (2) \[S=I_{p}(X_{1},X_{2};Y)-_{q_{p}}I_{q}(X_{1},X_{2};Y),\] (3)

where \(_{p}=\{q:q(x_{i},y)=p(x_{i},y)\; y,x_{i} _{i},i\}\) and the notation \(I_{p}()\) and \(I_{q}()\) disambiguates mutual information under \(p\) and \(q\) respectively. The key lies in optimizing \(q_{p}\) to satisfy the marginals \(q(x_{i},y)=p(x_{i},y)\), but relaxing the coupling between \(x_{1}\) and \(x_{2}\): \(q(x_{1},x_{2})\) need not be equal to \(p(x_{1},x_{2})\). The intuition behind this is that one should be able to infer redundancy and uniqueness given only access to \(p(x_{1},y)\) and \(p(x_{2},y)\), and therefore they should only depend on \(q_{p}\). Synergy is the only term that should depend on the coupling \(p(x_{1},x_{2})\), and this is reflected

Figure 1: PID decomposes \(I(X_{1},X_{2};Y)\) into redundancy \(R\) between \(X_{1}\) and \(X_{2}\), uniqueness \(U_{1}\) in \(X_{1}\) and \(U_{2}\) in \(X_{2}\), and synergy \(S\) in both \(X_{1}\) and \(X_{2}\).

in (3) depending on the full \(p\) distribution. This definition enjoys several useful properties in line with intuition, as we will see in comparison with related frameworks for interactions below .

### Related Frameworks for Feature Interactions

**Information-theoretic definitions**: Perhaps the first measure of redundancy in machine learning is co-training [14; 8; 23], where 2 variables are redundant if they are conditionally independent given the task: \(I(X_{1};X_{2}|Y)=0\). As a result, redundancy can be measured by \(I(X_{1};X_{2};Y)\). The same definition of redundancy is used in multi-view learning [98; 94; 89] which further define \(I(X_{1};Y|X_{2})\) and \(I(X_{2};Y|X_{1})\) as unique information in \(X_{1},X_{2}\). However, \(I(X_{1};X_{2};Y)\) can be both positive and negative . PID resolves this by separating \(R\) and \(S\) such that \(R-S=I(X_{1};X_{2};Y)\), identifying that prior measures confound redundancy and synergy. This crucially provides an explanation for the distinction between _mediation_, where one feature conveys the information already in another (i.e., \(R>S\)), versus _moderation_, where one feature affects the relationship of other features (i.e., \(S>R\)) [10; 36]. Furthermore, if \(I(X_{1};X_{2};Y)=0\) then existing frameworks are unable to distinguish between positive \(R\) and \(S\) canceling each other out.

**Statistical measures**: Other approaches have studied interaction measures via statistical measures, such as redundancy via distance between prediction logits using either feature , statistical distribution tests on input features [114; 7], or via human annotations . However, it is unclear how to extend these definitions to uniqueness and synergy while remaining on the same standardized scale like PID provides. Also of interest are notions of redundant and synergistic interactions in human and animal communication [78; 79; 30; 85], which we aim to formalize.

**Model-based methods**: Prior research has formalized definitions of non-additive interactions  to quantify their presence [88; 101; 102; 46] in trained models, or used Shapley values on trained features to measure interactions . Parallel research has also focused on qualitative visualizations of real-world multimodal datasets and models, such as DIME , M2Lens , and MultiViz .

## 3 Scalable Estimators for PID

**PID as a framework for multimodality**: Our core insight is that PID provides a formal framework to understand both the _nature_ and _degree_ of interactions involved when two features \(X_{1}\) and \(X_{2}\) are used for task \(Y\). The nature of interactions is afforded by a precise decomposition into redundant, unique, and synergistic interactions, and the degree of interactions is afforded by a standardized unit of measure (bits). However, computing PID is a considerable challenge, since it involves optimization over \(_{p}\) and estimating information-theoretic measures. Up to now, analytic approximations of these quantities were only possible for discrete and small support [13; 41; 109] or continuous but low-dimensional variables [76; 81; 110]. Leveraging ideas in representation learning, Sections 3.1 and 3.2 are our first technical contributions enabling scalable estimation of PID for high-dimensional distributions. The first, CVX, is exact, based on convex optimization, and is able to scale to problems where \(|_{i}|\) and \(||\) are around \(100\). The second, Batch, is an approximation based on sampling, which enables us to handle large or even continuous supports for \(X_{i}\) and \(Y\). Applying these estimators in Section 4, we show that PID provides a path towards understanding the nature of interactions in datasets and those learned by different models, and principled approaches for model selection.

### CVX: Dataset-level Optimization

Our first estimator, CVX, directly compute PID from its definitions using convex programming. Crucially, Bertschinger et al.  show that the solution to the max-entropy optimization problem: \(q^{*}=_{q_{p}}H_{q}(Y|X_{1},X_{2})\) equivalently solves (1)-(3). When \(_{i}\) and \(\) are small and discrete, we can represent all valid distributions \(q(x_{1},x_{2},y)\) as a set of tensors \(Q\) of shape \(|_{1}||_{2}|||\) with each entry representing \(Q[i,j,k]=p(X_{1}=i,X_{2}=j,Y=k)\). The problem then boils down to optimizing over valid tensors \(Q_{p}\) that match the marginals \(p(x_{i},y)\).

Given a tensor \(Q\) representing \(q\), our objective is the concave function \(H_{q}(Y|X_{1},X_{2})\). While Bertschinger et al.  report that direct optimization is numerically difficult as routines such as Mathematica's FindMinimum do not exploit convexity, we overcome this by rewriting conditional entropy as a KL-divergence , \(H_{q}(Y|_{1},X_{2})=||-KL(q||)\), where \(\) is an auxiliary product density of \(q(x_{1},x_{2})|}\) enforced using linear constraints: \((x_{1},x_{2},y)=q(x_{1},x_{2})/||\). Finally, optimizing over \(Q_{p}\) that match the marginals can also be enforced through linear constraints: the 3D-tensor \(Q\) summed over the second dimension gives \(q(x_{1},y)\) and summed over the first dimension gives \(q(x_{2},y)\), yielding the final optimization problem:

\[*{arg\,max}_{Q,}KL(Q||),(x_{1},x_{2},y)=Q(x_{1},x_{2})/||,\] (4) \[_{x_{2}}Q=p(x_{1},y),_{x_{1}}Q=p(x_{2},y),Q 0,_{x_{ 1},x_{2},y}Q=1.\] (5)

The KL-divergence objective is recognized as convex, allowing the use of conic solvers such as SCS , ECOS , and MOSEK . Plugging \(q^{*}\) into (1)-(3) yields the desired PID.

**Pre-processing via feature binning**: In practice, \(X_{1}\) and \(X_{2}\) often take continuous rather than discrete values. We work around this by histogramming each \(X_{i}\), thereby estimating the continuous joint density by discrete distributions with finite support. We provide more details in Appendix B.1.

### Batch: Batch-level Amortization

We now present Batch, our next estimator that is suitable for large datasets where \(_{i}\) is high-dimensional and continuous (\(||\) remains finite). To estimate PID given a sampled dataset \(=\{(x_{1}^{(j)},x_{2}^{(j)},y^{(j)})\}\) of size \(n\), we propose an end-to-end model parameterizing marginal-matching joint distributions in \(_{p}\) and a training objective whose solution returns approximate PID values.

**Simplified algorithm sketch**: Our goal, loosely speaking, is to optimize \(_{p}\) for objective (1) through an approximation using neural networks instead of exact optimization. We show an overview in Figure 2. To explain our approach, we first describe (1) how we parameterize \(\) using neural networks such that it can be learned via gradient-based approaches, (2) how we ensure the marginal constraints \(_{p}\) through a variant of the Sinkhorn-Knopp algorithm, and finally (3) how to scale this up over small subsampled batches from large multimodal datasets.

**Parameterization using neural networks**: The space of joint distributions \(\) is often too large to explicitly specify. To tackle this, we implicitly parameterize each distribution \(\) using a neural network \(f_{}\) that takes in batches of modalities \(_{1}}_{1}^{n},_{2} }_{2}^{n}\) and the label \(^{n}\) before returning a matrix \(A^{n n||}\) representing an (unnormalized) joint distribution \(\), i.e., we want \(A[i][j][y]=(_{1}[i],_{2}[j],y)\) for each \(y\). In practice, \(f_{}\) is implemented via a pair of encoders \(f_{(1)}\) and \(f_{(2)}\) that learn modality representations, before an outer product to learn joint relationships \(A_{y}=(f_{(1)}(_{1},y)f_{(2)}(_{2},y)^{})\) for each \(y\), yielding the desired \(n n||\) joint distribution. As a result, optimizing over \(\) can be performed via optimizing over parameters \(\).

**Respecting the marginal constraints**: How do we make sure the \(\)'s learned by the network satisfies the marginal constraints (i.e., \(_{p}\))? We use an unrolled version of Sinkhorn's algorithm  which projects \(A\) onto \(_{p}\) by iteratively normalizing \(A\)'s rows and columns to sum to \(1\) and rescaling to satisfy the marginals \(p(x_{i},y)\). However, \(p(x_{i},y)\) is not easy to estimate for high-dimensional continuous \(x_{i}\)'s. In response, we first expand \(p(x_{i},y)\) into \(p(y|x_{i})\) and \(p(x_{i})\) using Bayes' rule. Since \(A\) was constructed by samples \(x_{i}\) from the dataset, the rows and columns of \(A\) are already distributed according to \(p(x_{1})\) and \(p(x_{2})\) respectively. This means that it suffices to approximate \(p(y|x_{i})\) with unimodal classifiers \((y|x_{i})\) parameterized by neural networks and trained separately, before using Sinkhorn's algorithm to normalize each row to \((y|x_{1})\) and each column to \((y|x_{2})\).

**Objective**: We choose the objective \(I_{q}(X_{1};X_{2};Y)\), which equivalently solves the optimization problems in the other PID terms . Given matrix \(A\) representing \((x_{1},x_{2},y)\), the objective can be computed in closed form through appropriate summation across dimensions in \(A\) to obtain \((x_{i})\), \((x_{1},x_{2})\), \((x_{i}|y)\), and \((x_{1},x_{2}|y)\) and plugging into \(I_{}(X_{1};X_{2};Y)=I_{}(X_{1};X_{2})-I_{}(X_{1}; X_{2}|Y)\). We maximize \(I_{}(X_{1};X_{2};Y)\) by updating parameters \(\) via gradient-based methods. Overall, each

Figure 2: We propose Batch, a scalable estimator for PID over high-dimensional continuous distributions. Batch parameterizes \(\) using a matrix \(A\) learned by neural networks such that mutual information objectives over \(\) can be optimized via gradient-based approaches over minibatches. Marginal constraints \(_{p}\) are enforced through a variant of the Sinkhorn-Knopp algorithm on \(A\).

gradient step involves computing \(=_{}(A)\), and updating \(\) to maximize (1) under \(\). Since Sinkhorn's algorithm is differentiable, gradients can be backpropagated end-to-end.

**Approximation with small subsampled batches**: Finally, to scale this up to large multimodal datasets where the full \(\) may be too large to store, we approximate \(\) with small subsampled batches: for each gradient iteration \(t\), the network \(f_{}\) now takes in a batch of \(m n\) datapoints sampled from \(\) and returns \(A^{m m||}\) for the subsampled points. We perform Sinkhorn's algorithm on \(A\) and a gradient step on \(\) as above, _as if_\(_{t}\) was the full dataset (i.e., mini-batch gradient descent). While it is challenging to obtain full-batch gradients since computing the full \(A\) is intractable, we found our approach to work in practice for large \(m\). Our approach can also be informally viewed as performing amortized optimization  by using \(\) to implicitly share information about the full batch using subsampled batches. Upon convergence of \(\), we extract PID by plugging \(\) into (1)-(3).

**Implementation details** such as the network architecture of \(f\), approximation of objective (1) via sampling from \(\), and estimation of \(I_{}(\{X_{1},X_{2}\};Y)\) from learned \(\) are in Appendix B.3.

## 4 Evaluation and Applications of PID in Multimodal Learning

We design experiments to (1) understand PID on synthetic data, (2) quantify real-world multimodal benchmarks, (3) understand the interactions captured by multimodal models, (4) perform model selection across different model families, and (5) applications on novel real-world tasks.

### Validating PID Estimates on Synthetic Data

Our first goal is to evaluate the accuracy of our proposed estimators with respect to the ground truth (if it can be computed) or human judgment (for cases where the ground truth cannot be readily obtained). We start with a suite of datasets spanning both synthetic and real-world distributions.

**Synthetic bitwise features**: We sample from a binary bitwise distribution: \(x_{1},x_{2}\{0,1\},y=x_{1} x_{2},y=x_{1} x_{2},y=x_{1} x_ {2}\),. Each bitwise operator's PID can be solved exactly when the \(x_{i}\)'s and labels are discrete and low-dimensional . Compared to the ground truth in Bertschinger et al. , both our estimators exactly recover the correct PID values (Table 1).

**Gaussian Mixture Models (GMM)**: Consider a GMM, where \(X_{1}\), \(X_{2}\) and the label \(Y\{-1,+1\}\), comprising two equally weighted standard multivariate Gaussians centered at \(\), where \(^{2}\), i.e., \(Y(1/2)\), \((X_{1},X_{2})|Y=y(y,I)\). PID was estimated by sampling \(1e6\) points, histogramming them into \(50\) bins spanning \([-5,+5]\) to give \(p\), and then applying the CVX estimator. We term this PID-Cartesian. We also compute PID-Polar, which are PID computed using _polar coordinates_, \((r,)\). We use a variant where the angle \(\) is given by the arctangent with principal values \([0,]\) and the length \(r\) could be negative. \(\) specifies a line (through the origin), and \(r\) tells us where along the line the datapoint lies on.

**Results:** We consider \(||||_{2}\{1.0,2.0\}\), where for each \(||||_{2}\), we vary the angle \(\) that \(\) makes with the horizontal axis. Our computed PID is presented in Figure 3. Overall, we find that the PID matches what we expect from intuition. For Cartesian, unique information dominates when

   Task &  &  &  \\  PID & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) \\  Exact & 0.31 & 0 & 0 & 0.5 & 0.31 & 0 & 0 & 0.5 & 0 & 0 & 0 & 1 \\ CVX & 0.31 & 0 & 0 & 0.5 & 0.31 & 0 & 0 & 0.5 & 0 & 0 & 0 & 1 \\ Batch & 0.31 & 0 & 0 & 0.5 & 0.31 & 0 & 0 & 0.5 & 0 & 0 & 0 & 1 \\   

Table 1: Results on estimating PID on synthetic bitwise datasets. Both our estimators exactly recover the correct PID values as reported in Bertschinger et al. .

Figure 3: Left to right: (a) Contour plots of the GMM’s density for \(||||_{2}=2.0\). Red line denotes the optimal linear classifier. (b) PID (Cartesian) computed for varying \(\) with respect to the \(x\) axis. (c) PID (Polar) for varying \(\), with \(U_{1}\) and \(U_{2}\) corresponding to unique information from \((r,)\). Plots (d)-(f) are similar to (a)-(c), but repeated for \(||||_{2}=1.0\). Legend: \((R)\), \((U_{1})\), \((U_{2})\), \((S)\), \(()\). Observe how PID changes with the change of variable from Cartesian (b and e) to Polar (c and f), as well as how a change in \(||||_{2}\) can lead to a disproportionate change across PID (b vs e).

the angle goes to \(0\) or \(/2\) -- if centroids share a coordinate, then observing that coordinate yields no information about \(y\). Conversely, synergy and redundancy peak at \(/4\). Interestingly, synergy seems to be independent of \(||||_{2}\). For Polar, redundancy is \(0\). Furthermore, \(\) contains no unique information, since \(\) shows nothing about \(y\) unless we know \(r\) (in particular, its sign). When the angle goes to \(/2\), almost all information is unique in \(r\). The distinctions between Cartesian and Polar highlight how different representations of data can exhibit wildly different PID values, even if total information is the same. More thorough results and visualizations of \(q^{*}\) are in Appendix C.2.

**Synthetic generative model**: We begin with a set of latent vectors \(z_{1},z_{2},z_{c}(0_{d},_{d}^{2}),d=50\) representing information unique to \(X_{1},X_{2}\) and common to both respectively. \([z_{1},z_{c}]\) is transformed into high-dimensional \(x_{1}\) using a fixed transformation \(T_{1}\) and likewise \([z_{2},z_{c}]\) to \(x_{2}\) via \(T_{2}\). The label \(y\) is generated as a function of (1) only \(z_{c}\), in which case we expect complete redundancy, (2) only \(z_{1}\) or \(z_{2}\) which represents complete uniqueness, (3) a combination of \(z_{1}\) and \(z_{2}\) representing complete synergy, or (4) arbitrary ratios of each of the above with \(z_{i}^{*}\) representing half of the dimensions from \(z_{i}\) and therefore half of each interaction. In total, Table 2 shows the \(10\) synthetic datasets we generated: \(4\) specialized datasets \(_{I}\), \(I\{R,U_{1},U_{2},S\}\) where \(y\) only depends on one interaction, and \(6\) mixed datasets with varying interaction ratios. We also report the ground-truth interactions as defined by the label-generating process and the total capturable information using the bound in Feder and Merhav , which relates the accuracy of the best model on these datasets with the mutual information between the inputs to the label. Since the test accuracies for Table 2 datasets range from \(67\)-\(75\%\), this corresponds to total MI of \(0.42-0.59\) bits.

**Results**: From Table 2, both CVX and Batch agree in relative PID values, correctly assigning the predominant interaction type and interactions with minimal presence consistent with the ground-truth based on data generation. For example, \(_{R}\) has the highest \(R\) value, and when the ratio of \(z_{1}\) increases, \(U_{1}\) increases from \(0.01\) on \(y=f(z_{1}^{*},z_{2}^{*},z_{c}^{*})\) to \(0.06\) on \(y=f(z_{1},z_{2}^{*},z_{c}^{*})\). We also note some interesting observations due to the random noise in label generation, such as the non-zero synergy measure of datasets such as \(_{R},_{U_{1}},_{U_{2}}\) whose labels do not depend on synergy.

### Quantifying Real-world Multimodal Benchmarks

We now apply these estimators to quantify the interactions in real-world multimodal datasets.

**Real-world multimodal data setup**: We use a large collection of real-world datasets in Multi-Bench  which test _multimodal fusion_ of different input signals (including images, video, audio, text, time-series, sets, and tables) for different tasks (predicting humor, sentiment, emotions, mortality rate, ICD-\(9\) codes, image-captions, human activities, digits, and design interfaces). We also include experiments on _question-answering_ (Visual Question Answering 2.0  and CLEVR ) which test grounding of language into the visual domain. For the \(4\) datasets (top row of Table 3) involving images and text where modality features are available and readily clustered, we apply the CVX estimator on top of discrete clusters. For the remaining \(4\) datasets (bottom row of Table 3) with video, audio, and medical time-series modalities, clustering is not easy, so we use the end-to-end Batch estimator (see Appendix C.4 for full dataset and estimator details).

**Human judgment of interactions**: Real-world multimodal datasets do not have reference PID values, and exact PID computation is impossible due to continuous data. We therefore use human judgment as a reference. We design a new annotation scheme where we show both modalities and the label and ask each annotator to annotate the degree of redundancy, uniqueness, and synergy on a scale of \(0\)-\(5\), alongside their confidence in their answers on a scale of \(0\)-\(5\). We show a sample

   Task & _{R}\)} & _{U_{1}}\)} & _{U_{2}}\)} & _{S}\)} & ^{*},z_{2}^{*},z_{c}^{*})\)} \\  PID & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) \\  CVX & \(\) & \(0\) & \(0.05\) & \(0\) & \(\) & \(0\) & \(0.05\) & \(0\) & \(\) & \(0.05\) & \(0.07\) & \(0\) & \(0.01\) & \(\) & \(\) & \(0.01\) & \(0\) & \(\) \\ Batch & \(\) & \(0.02\) & \(0.02\) & \(0\) & \(\) & \(0\) & \(0\) & \(0\) & \(\) & \(0\) & \(0\) & \(0.11\) & \(0.02\) & \(0.02\) & \(\) & \(\) & \(0.01\) & \(0.01\) & \(\) \\ Truth & \(0.58\) & \(0\) & \(0\) & \(0\) & \(0.56\) & \(0\) & \(0\) & \(0\) & \(0.54\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0.56\) & \(0.13\) & \(0\) & \(0\) & \(0.27\) \\    
   Task & ,z_{2}^{*},z_{c}^{*})\)} & ,z_{2},z_{c}^{*})\)} & ^{*},z_{2}^{*},z_{c})\)} & ^{*},z_{c}^{*})\)} & ^{*},z_{c}^{*})\)} \\  PIDuser interface and annotation procedures in Appendix C.5. We give \(50\) datapoints from each dataset (except MIMIC and ENRICO which require specialized knowledge) to \(3\) annotators each.

**Results on multimodal fusion**: From Table 3, we find that different datasets do require different interactions. Some interesting observations: (1) all pairs of modalities on MUSTARD sarcasm detection show high synergy values, which aligns with intuition since sarcasm is often due to a contradiction between what is expressed in language and speech, (2) uniqueness values are strongly correlated with unimodal performance (e.g., modality \(1\) in AV-MNIST and MIMIC), (3) datasets with high synergy do indeed benefit from interaction modeling as also seen in prior work (e.g., MUSTARD, UR-FUNNY) [17; 43], and (4) conversely datasets with low synergy are those where unimodal performance is relatively strong (e.g., MIMIC) .

**Results on QA**: We observe very high synergy values as shown in Table 3 consistent with prior work studying how these datasets were balanced (e.g., VQA 2.0 having different images for the same question such that the answer can only be obtained through synergy)  and that models trained on these datasets require non-additive interactions . CLEVR has a higher proportion of synergy than VQA 2.0 (\(83\%\) versus \(75\%\)): indeed, CLEVR is a more balanced dataset where the answer strictly depends on both the question and image with a lower likelihood of unimodal biases.

**Comparisons with human judgment**: For human judgment, we cannot ask humans to give a score in bits, so it is on a completely different scale (\(0\)-\(5\) scale). To put them on the same scale, we normalize the human ratings such that the sum of human interactions is equal to the sum of PID estimates. The resulting comparisons are in Table 3, and we find that the human-annotated interactions overall align with estimated PID: the highest values are the same for \(4\) datasets: both explain highest synergy on VQA and CLEVR, image (\(U_{1}\)) being the dominant modality in AV-MNIST, and language (\(U_{1}\)) being the dominant modality in MOSEI. Overall, the Kripendorff's alpha for inter-annotator agreement is high (\(0.72\) for \(R\), \(0.68\) for \(U_{1}\), \(0.70\) for \(U_{2}\), \(0.72\) for \(S\)) and the average confidence scores are also high (\(4.36/5\) for \(R\), \(4.35/5\) for \(U_{1}\), \(4.27/5\) for \(U_{2}\), \(4.46/5\) for \(S\)), indicating that the human-annotated results are reliable. For the remaining two datasets (UR-FUNNY and MUSTARD), estimated PID matches the second-highest human-annotated interaction. We believe this is because there is some annotator subjectivity in interpreting whether sentiment, humor, and sarcasm are present in language only (\(U_{1}\)) or when contextualizing both language and video (\(S\)), resulting in cases of low annotator agreement in \(U_{1}\) and \(S\): \(-0.14\), \(-0.03\) for UR-FUNNY and \(-0.08\), \(-0.04\) for MUSTARD.

**Comparisons with other interaction measures**: Our framework allows for easy generalization to other interaction definitions: we also implemented \(3\) information theoretic measures **I-min**, **WMS**, and **CI**. These results are in Table 10 in the Appendix, where we explain the limitations of these methods as compared to PID, such as over- and under-estimation, and potential negative estimation . These are critical problems with the application of information theory for shared \(I(X_{1};X_{2};Y)\) and unique information \(I(X_{1};Y|X_{2})\), \(I(X_{2};Y|X_{1})\) often quoted in the co-training [8; 14] and multi-view learning [89; 94; 98] literature. We also tried \(3\) non-info theory measures: Shapley values , Integrated gradients (IG) , and CCA , which are based on quantifying interactions captured by a multimodal model. Our work is fundamentally different in that interactions are properties of data before training any models (see Appendix C.6).

### Quantifying Multimodal Model Predictions

We now shift our focus to quantifying multimodal models. _Do different multimodal models learn different interactions?_ A better understanding of the types of interactions that our current models struggle to capture can provide new insights into improving these models.

   Task &  &  &  &  \\  PID & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) \\  CVX & \(0.10\) & **0.97** & \(0.03\) & \(0.08\) & **0.73** & \(0.38\) & \(0.53\) & \(0.34\) & \(0.79\) & \(0.87\) & \(0\) & **4.92** & \(0.55\) & \(0.48\) & \(0\) & **5.16** \\ Human & **0.57** & **0.61** & \(0\) & \(0\) & - & - & - & - & \(0\) & \(0\) & \(0\) & **6.58** & \(0\) & \(0\) & \(0\) & **6.19** \\    
   Task &  &  &  &  \\  PID & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) & \(R\) & \(U_{1}\) & \(U_{2}\) & \(S\) \\  Batch & **0.26** & **0.49** & \(0.03\) & \(0.04\) & \(0.03\) & \(0.04\) & \(0.01\) & **0.08** & \(0.14\) & \(0.01\) & \(0.01\) & **0.30** & \(0.05\) & **0.17** & \(0\) & \(0.01\) \\ Human & **0.32** & **0.20** & \(0.15\) & \(0.15\) & \(0.04\) & **0.05** & \(0.03\) & **0.04** & \(0.13\) & **0.17** & \(0.04\) & **0.16** & - & - & - & - \\   

Table 3: Estimating PID on real-world MultiBench  datasets. Many of the estimated interactions align well with human judgement as well as unimodal performance.

**Setup**: For each dataset, we train a suite of models on the train set \(_{}\) and apply it to the validation set \(_{}\), yielding a predicted dataset \(_{}=\{(x_{1},x_{2},)_{}\}\). Running PID on \(_{}\) summarizes the interactions that the model captures. We categorize and implement a comprehensive suite of models (spanning representation fusion at different feature levels, types of interaction inductive biases, and training objectives) that have been previously motivated to capture redundant, unique, and synergistic interactions (see Appendix C.7 for full model descriptions).

**Results**: We show results in Table 4 and highlight the following observations:

_General observations_: We first observe that model PID values are consistently higher than dataset PID. The sum of model PID is also a good indicator of test performance, which agrees with their formal definition since their sum is equal to \(I(\{X_{1},X_{2}\};Y)\), the total task-relevant information.

_On redundancy_: Several methods succeed in capturing redundancy, with an overall average of \(R=0.41 0.11\) and accuracy of \(73.0 2.0\%\) on redundancy-specialized datasets. Additive, agreement, and alignment-based methods are particularly strong, and we do expect them to capture redundant shared information [26; 82]. Methods based on tensor fusion (synergy-based), including lower-order interactions, and adding reconstruction objectives (unique-based) also capture redundancy.

_On uniqueness_: Uniqueness is harder to capture than redundancy, with an average of \(U=0.37 0.14\). Redundancy-based methods like additive and agreement do poorly on uniqueness, while those designed for uniqueness (lower-order interactions  and modality reconstruction objectives ) do well, with on average \(U=0.55\) and \(73.0\%\) accuracy on uniqueness datasets.

_On synergy_: Synergy is the hardest to capture, with an average score of only \(S=0.21 0.10\). Some of the strong methods are tensor fusion , tensors with lower-order interactions , modality reconstruction , and multimodal transformer , which achieve around \(S=0.30,=73.0\%\). Additive, agreement, and element-wise interactions do not seem to capture synergy well.

_On robustness_: Finally, we also show connections between PID and model performance in the presence of missing modalities. We find high correlation (\(=0.8\)) between the performance drop when \(X_{i}\) is missing and the model's \(U_{i}\) value. Inspecting Figure 4, we find that the implication only holds in one direction: high \(U_{i}\) coincides with large performance drops (in red), but low \(U_{i}\) can also lead to performance drops (in green). The latter can be further explained by the presence of large \(S\) values: when \(X_{i}\) is missing, synergy can no longer be learned which affects performance. For the subset of points when \(U_{i} 0.05\), the correlation between \(S\) and performance drop is \(=0.73\) (in contrast, the correlation for \(R\) is \(=0.01\)).

### PID Agreement and Model Selection

Now that we have quantified datasets and models individually, the natural next question unifies both: _what does the agreement between dataset and model PID measures tell us about model performance?_ We hypothesize that models able to capture the interactions necessary in a given dataset should also

   Model & EF & Additive & Agree & Align & Elem & Tensor & MI & Mult & Lower & Rec & Average \\  \(R\) & \(0.35\) & \(\) & \(\) & \(\) & \(0.27\) & \(\) & \(0.20\) & \(0.40\) & \(\) & \(\) & \(\) \\ Acc(\(_{R}\)) & \(0.71\) & \(\) & \(\) & \(\) & \(0.70\) & \(\) & \(0.67\) & \(0.73\) & \(\) & \(\) & \(0.73 0.02\) \\  \(U\) & \(0.29\) & \(0.31\) & \(0.19\) & \(0.44\) & \(0.20\) & \(0.52\) & \(0.18\) & \(0.45\) & \(\) & \(\) & \(\) \\ Acc(\(_{U}\)) & \(0.66\) & \(0.55\) & \(0.60\) & \(0.73\) & \(0.66\) & \(0.73\) & \(0.66\) & \(0.72\) & \(\) & \(\) & \(0.68 0.06\) \\  \(S\) & \(0.13\) & \(0.09\) & \(0.08\) & \(0.29\) & \(0.14\) & \(\) & \(0.12\) & \(\) & \(\) & \(\) & \(\) \\ Acc(\(_{S}\)) & \(0.56\) & \(0.66\) & \(0.63\) & \(0.72\) & \(0.66\) & \(\) & \(0.65\) & \(\) & \(\) & \(\) & \(0.68 0.06\) \\   

Table 4: Average interactions (\(R/U/S\)) learned by models alongside their average performance on interaction-specialized datasets (\(_{R}/_{U}/_{S}\)). Synergy is the hardest to capture and redundancy is relatively easier to capture by existing models.

Figure 4: We find high correlation (\(=0.8\)) between the performance drop when \(X_{i}\) is missing and the model’s \(U_{i}\) value: high \(U_{i}\) coincides with large performance drops (red), but low \(U_{i}\) can also lead to performance drops. The latter can be further explained by large \(S\) so \(X_{i}\) is necessary (green).

achieve high performance. Given estimated interactions on dataset \(\) and model \(f()\) trained on \(\), we define the agreement for each interaction \(I\{R,U_{1},U_{2},S\}\) as:

\[_{I}(f,)=_{}I_{f()},_ {}=}}{_{I^{}\{R,U_{1},U_{2},S\}}I ^{}_{}},\] (6)

which summarizes the quantity of an interaction captured by a model (\(I_{f()}\)) weighted by its normalized importance in the dataset (\(_{}\)). The total agreement sums over \((f,)=_{I}_{I}(f,)\).

**Results**: Our key finding is that PID agreement scores \((f,)\) correlate (\(=0.81\)) with model accuracy across all \(10\) synthetic datasets as illustrated in Figure 5. This shows that PID agreement can be a useful proxy for model performance. For the specialized datasets, we find that the correlation between \(_{I}\) and \(_{I}\) is \(0.96\) for \(R\), \(0.86\) for \(U\), and \(0.91\) for \(S\), and negatively correlated with other specialized datasets. For mixed datasets with roughly equal ratios of each interaction, the measures that correlate most with performance are \(_{R}\) (\(=0.82\)) and \(_{S}\) (\(=0.89\)); datasets with relatively higher redundancy see \(=0.89\) for \(_{R}\); those with higher uniqueness have \(_{U_{1}}\) and \(_{U_{2}}\) correlate \(=0.92\) and \(=0.85\); those with higher synergy increases the correlation of \(_{S}\) to \(=0.97\).

Using these observations, our final experiment is model selection: _can we choose the most appropriate model to tackle the interactions required for a dataset?_

**Setup**: Given a new dataset \(\), we first compute its difference in normalized PID values with respect to \(^{}\) among our suite of \(10\) synthetic datasets, \(s(,^{})=_{I\{R,U_{1},U_{2},S\}}|_{ }-_{^{}}|\), to rank the dataset \(^{*}\) with the most similar interactions, and return the top-\(3\) performing models on \(^{*}\). In other words, we select models that best capture interactions that are of similar nature and degree as those in \(\). We emphasize that even though we restrict dataset and model search to _synthetic datasets_, we evaluate model selection on real-world datasets and find that it _generalizes to the real world_.

**Results**: We test our selected models on \(5\) new synthetic datasets with different PID ratios and \(6\) real-world datasets, summarizing results in Table 5. We find that the top \(3\) chosen models achieve \(95\%-100\%\) of the best-performing model accuracy, and \(>98.5\%\) for all datasets except \(95.2\%\) on MUSTARD. For example, UR-FUNNY and MUSTARD have the highest synergy (\(S=0.13\), \(S=0.3\)) and indeed transformers and higher-order interactions are helpful (MulT: \(65\%\), MI: \(61\%\), Tensor: \(60\%\)). Enrico has the highest \(R=0.73\) and \(U_{2}=0.53\), and methods for redundant and unique interactions perform best (Lower: \(52\%\), Align: \(52\%\), Agree: \(51\%\)). MIMIC has the highest \(U_{1}=0.17\), and unimodal models are mostly sufficient .

### Real-world Applications

Finally, we apply PID to \(3\) real-world case studies: pathology, mental health, and robotic perception (see full details and results in Appendix C.9-C.11).

**Case Study 1: Computational pathology.** Cancer prognostication is a challenging task in anatomic pathology that requires integration of whole-slide imaging (WSI) and molecular features for patient stratification . We use The Cancer Genome Atlas (TCGA), a large public data consortium of paired WSI, molecular, and survival information , including modalities: (1) pre-extracted histology image features from diagnostic WSIs and (2) bulk gene mutation status, copy number variation, and RNA-Seq abundance values. We evaluate on two cancer datasets in TCGA, lower-grade glioma (LGG , \(n=479\)) and pancreatic adenocarcinoma (PAAD , \(n=209\)).

   Dataset & \(5\) Synthetic Datasets & MIMIC & ENRICO & UR-FUNNY & MOSEI & MUSTARD & MAPS \\  \(\%\) Performance & \(99.91\%\) & \(99.78\%\) & \(100\%\) & \(98.58\%\) & \(99.35\%\) & \(95.15\%\) & \(100\%\) \\   

Table 5: **Model selection** results on unseen synthetic and real-world datasets. Given a new dataset \(\), finding the closest synthetic dataset \(^{}\) with similar PID values and recommending the best models on \(^{}\) consistently achieves \(95\%-100\%\) of the best-performing model on \(\).

Figure 5: PID agreement \((f,)\) between datasets and models strongly correlate with model accuracy (\(=0.81\)).

**Results**: In TCGA-LGG, most PID measures were near-zero except \(U_{2}=0.06\) for genomic features, which indicates that genomics is the only modality containing task-relevant information. This conclusion corroborates with the high performance of unimodal-genomic and multimodal models in Chen et al. , while unimodal-pathology performance was low. In TCGA-PAAD, the uniqueness in pathology and genomic features was less than synergy (\(U_{1}=0.06\), and \(U_{2}=0.08\) and \(S=0.15\)), which also match the improvement of using multimodal models that capture synergy.

**Case Study 2: Mental health.** Suicide is the second leading cause of death among adolescents . Intensive monitoring of behaviors via adolescents' frequent use of smartphones may shed new light on the early risk of suicidal ideations [37; 72], since smartphones provide rich behavioral markers . We used a dataset, MAPS, of mobile behaviors from high-risk consenting adolescent populations (approved by IRB). Passive sensing data is collected from each participant's smartphone across 6 months. The modalities include (1) _text_ entered by the user represented as a bag of top \(1000\) words, (2) _keystrokes_ that record the exact timing and duration of each typed character, and (3) _mobile applications_ used per day as a bag of \(137\) apps. Every morning, users self-report their daily mood, which we discretized into \(-1,0,+1\). In total, MAPS has \(844\) samples from \(17\) participants.

**Results**: We first experiment with MAPS\({}_{T,K}\) using text and keystroke features. PID measures show that MAPS\({}_{T,K}\) has high synergy (\(0.40\)), some redundancy (\(0.12\)), and low uniqueness (\(0.04\)). We found the purely synergistic dataset \(_{S}\) has the most similar interactions and the suggested models Lower, Rec, and Tensor that work best on \(_{S}\) were indeed the top 3 best-performing models on MAPS\({}_{T,K}\), indicating that model selection is effective. Model selection also retrieves the best-performing model on MAPS\({}_{T,A}\) using text and app usage features.

**Case Study 3: Robotic Perception.** MuJoCo Push is a contact-rich planar pushing task in MuJoCo , where a \(7\)-DoF Panda Franka robot is pushing a circular puck with its end-effector in simulation. The dataset consists of \(1000\) trajectories with \(250\) steps sampled at \(10\)Hertz. The multimodal inputs are gray-scaled images from an RGB camera, force and binary contact information from a force/torque sensor, and the 3D position of the robot end-effector. We estimate the 2D position of the unknown object on a table surface while the robot intermittently interacts with it.

**Results**: We find that Batch predicts \(U_{1}=1.79\) as the highest PID value, which aligns with our observation that image is the best unimodal predictor. Comparing both estimators, CVX underestimates \(U_{1}\) and \(R\) since the high-dimensional time-series modality cannot be easily described by clusters without losing information. In addition, both estimators predict a low \(U_{2}\) value but attribute high \(R\), implying that a multimodal model with higher-order interactions would not be much better than unimodal models. Indeed, we observe no difference in performance between these two.

## 5 Conclusion

Our work aims to quantify the nature and degree of feature interactions by proposing scalable estimators for redundancy, uniqueness, and synergy suitable for high-dimensional heterogeneous datasets. Through comprehensive experiments and real-world applications, we demonstrate the utility of our proposed framework in dataset quantification, model quantification, and model selection. We are aware of some potential **limitations**:

1. These estimators only approximate real interactions due to cluster preprocessing or unimodal models, which naturally introduce optimization and generalization errors. We expect progress in density estimators, generative models, and unimodal classifiers to address these problems.
2. It is harder to quantify interactions for certain datasets, such as ENRICO which displays all interactions which makes it difficult to distinguish between \(R\) and \(S\) or \(U\) and \(S\).
3. Finally, there exist challenges in quantifying interactions since the data generation process is never known for real-world datasets, so we have to resort to human judgment, other automatic measures, and downstream tasks such as estimating model performance and model selection.

**Future work** can leverage PID for targeted dataset creation, representation learning optimized for PID values, and applications of information theory to higher-dimensional data. More broadly, there are several exciting directions in investigating more applications of multivariate information theory in modeling feature interactions, predicting multimodal performance, and other tasks involving feature interactions such as privacy-preserving and fair representation learning from high-dimensional data [28; 42]. Being able to provide guarantees for fairness and privacy-preserving learning can be particularly impactful.