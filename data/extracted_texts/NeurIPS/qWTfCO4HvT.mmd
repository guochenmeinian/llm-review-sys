# PowerGraph: A power grid benchmark dataset for graph neural networks

Anna Varbella

D-MAVT

ETHZ

Zurich, Switzerland

avarbella@ethz.ch &Kenza Amara

D-INFK

ETHZ

Zurich, Switzerland

kenza.amara@ai.ethz.ch &Blazhe Gjorgiev

D-MAVT

ETHZ

Zurich, Switzerland

gblazhe@ethz.ch &Mennatallah El-Assady

D-INFK

ETHZ

Zurich, Switzerland

menna.elassady@ai.ethz.ch &Giovanni Sansavini

D-MAVT

ETHZ

Zurich, Switzerland

sansavig@ethz.ch

Equal Contribution

###### Abstract

Power grids are critical infrastructures of paramount importance to modern society and, therefore, engineered to operate under diverse conditions and failures. The ongoing energy transition poses new challenges for the decision-makers and system operators. Therefore, developing grid analysis algorithms is important for supporting reliable operations. These key tools include power flow analysis and system security analysis, both needed for effective operational and strategic planning. The literature review shows a growing trend of machine learning (ML) models that perform these analyses effectively. In particular, Graph Neural Networks (GNNs) stand out in such applications because of the graph-based structure of power grids. However, there is a lack of publicly available graph datasets for training and benchmarking ML models in electrical power grid applications. First, we present PowerGraph, which comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. Second, we provide ground-truth explanations for the cascading failure analysis. Finally, we perform a complete benchmarking of GNN methods for node-level and graph-level tasks and explainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse tasks that includes power flow and fault scenarios with real-world explanations, providing a valuable resource for developing improved GNN models for node-level, graph-level tasks and explainability methods in power system modeling. The dataset is available at https://figshare.com/articles/dataset/PowerGraph/22820534 and the code at https://github.com/PowerGraph-Datasets

## 1 Introduction

Our modern society depends on a reliable power system , essential for daily life and economic activities. Power systems are engineered to operate under diverse conditions and failures. However, the increasing complexity of power systems, driven by electrification and the rise of intermittent energy sources, is posing new challenges. Transmission system operators (TSOs) require online tools for effective power systems monitoring, but the current methods for grid analyses, hindered by their computational speed, cannot fully meet this need. Additionally, the failure of criticalcomponents under specific conditions can trigger cascading outages, potentially leading to complete blackouts of the power grid . Due to the rarity of such events and the scarcity of historical data, computer models are used to simulate cascading failures. These models replicate the complex behavior of systemic responses and the propagation of successive failures within the grid. However, the computational intensity of these tools prevents their use by power grid operators for real-time detection of cascading failures.

Machine learning techniques, particularly GNNs, offer significant potential for providing real-time solutions in electrical power systems and are well-suited for modeling power grid phenomena . Works such as  address the power flow (PF) problem by employing machine learning to replace traditional solvers. Similarly, researchers in  explore replacing non-linear solvers for optimal power flow (OPF) problems. These endeavors often test methods on synthetic power systems or benchmark IEEE power grids. In the realm of fault scenario applications, the need for a real-time tool capable of estimating the potential of cascading failures under various power grid operating conditions is evident. Although recent methodologies employing machine learning algorithms for real-time prediction of cascading failures show promise, they often lack generalizability across diverse failure sets  or use less accurate linear approximations of the AC power flow. Addressing these limitations,  demonstrates the superiority of GNNs over FNNs for power grids, albeit without providing a complete dataset or model benchmarking across diverse power grids.

The lack of explainability often hinders the application of ML tools in specific industries such as the power systems sector. While synthetic graph data generators have made strides in providing benchmark datasets with ground-truth explanations , none offer real-world data with empirical explanations in the context of graph classification. Power systems practitioners could greatly benefit from explanations of black-box deep learning models. Firstly, interpretable models are inherently more trustworthy. Secondly, if GNN explainability models could identify the specific edges causing a cascading failure, TSOs would gain critical insights into the power system's most vulnerable components.

Publicly available power grid datasets, such as the Electricity Grid Simulated (EGS) datasets , the PSML , and the Simbench dataset , are not tailored for machine learning on graphs. Some efforts, like , focus on the dynamic stability of synthetic power grids. Furthermore, we identified a critical gap in accordance with the OGB taxonomy for graph datasets , where no GNN dataset in the society domain is available for graph-level tasks.

In this work, we present PowerGraph, aiming to address the following research gaps:

* A public dataset designed for various power systems to solve power flow and optimal power flow problems using GNN supervised learning techniques.
* A public dataset encompassing a wide range of cascading failure scenarios for different graph-level tasks.
* A real-world dataset for GNN graph-level tasks with clear ground-truth explanations for GNN explainability.

The PowerGraph dataset comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. To create this comprehensive dataset, we leverage MATPOWER  for power flow and optimal power flow simulations and Cascades , an alternate-current (AC) physics-based model for cascading failure analyses. This process ensures that our dataset encompasses many scenarios, enabling robust analysis and training of GNN models for power grid applications. As a result, PowerGraph is a large-scale graph dataset tailored for power flow analysis and the prediction of cascading failures in electric power grids, modeled as classification or regression problems at the node and graph level. The breakdown of the total number of graphs across node and graph-level tasks per power grid is detailed in Table 1.

The PowerGraph dataset encompasses the IEEE24 , IEEE39 , IEEE118 , and UK transmission system . These selected test power systems mirror real-world-based power grids, offering a diverse array of scales, topologies, and operational characteristics. They provide comprehensive data essential for conducting cascading failure analysis. With PowerGraph, we aim to democratize the use of Graph Neural Networks (GNNs) in critical infrastructures like power grids. Our contributions are as follows:

* Introducing a data-driven approach for analyzing power flow and cascading failure events in power grids in real-time.

* Assessing and benchmarking various GNN architectures and hyperparameters.
* Providing the first real-world GNN dataset with empirical explanations to benchmark GNN explainability methods.
* Making the dataset easily accessible in a user-friendly format, allowing the GNN community to experiment with different architectures for node and graph-level applications.
* Offering a range of tasks at both the node and graph levels, including regression, binary classification, and multi-class classification.

The paper is structured as follows: Section 2 outlines the dataset structure; Section 3 presents the benchmark of GNN for power flow analysis, optimal power flow and cascading failure analysis, Section 4 presents explainability methods for cascading failure analysis; and Section 6 concludes with final remarks and discussion.

## 2 The PowerGraph dataset

### Node-level task: power flow and optimal power flow

The PowerGraph dataset for node-level tasks is obtained by performing power flow (PF) and optimal power flow (OPF) simulations with MATPOWER , using the formulations in Sections A.2.1 and A.2.2. This dataset consists of \(N\) attributed graphs, denoted as \(=G_{1},G_{2},,G_{N}\), with each graph representing a different grid loading condition. An attributed graph is defined as \(G=(,,,)\), where \(\) is the set of nodes (buses, i.e., the power injection points in the grid), and \(\) is the set of edges (branches, i.e., the power transmission asset in the grid). The node feature matrix \(^{|| t}\) includes \(t\) features for each of the \(||\) nodes, and the edge feature matrix \(^{|| s}\) includes \(s\) features for each of the \(||\) edges. Figure 1 illustrates the dataset structure for both PF and OPF tasks. PF analysis aims to solve the power flow equations, given the power dispatched at each generator except for a balancing bus called the slack bus. The predicted node-level features depend on the type of electrical bus, as detailed in Section A.2.1. The OPF problem aims to determine the optimal generator (dispatch) to operate the grid at minimum cost while respecting physical constraints. Here, too, the predicted node-level features depend on the type of bus, as detailed in Section A.2.2.

### Graph-level task: cascading failure analysis

The PowerGraph dataset for graph-level tasks is obtained by processing the results of the Cascades model (details in Section A.3). This dataset consists of \(N\) attributed graphs, each representing a unique pre-outage operating condition and a set of outages involving single or multiple branches. The total number of graphs \(N\) per power grid equals \(n_{load\;cond}*n_{outlists}\), with specific values listed in Table 8. Outages result in the removal of corresponding branches from the adjacency matrix and edge features, altering the graph topology. Figure 2 shows an example of the PowerGraph dataset for a power grid. This structure is consistent across all grids in PowerGraph, including IEEE24, IEEE39, UK, and IEEE118. The total number of instances is in Table 1. The post-outage evolution is known for each initial state, including demand not served (DNS) and the number of tripped lines, i.e. the edges that have failed during the cascading failure (henceforth called cascading edges). A cascading failure occurs if any additional branch trips after the initial outage.

 
**Test system** & **No. Bus** & **No. Branch** &  Power flow analysis- \\ Node level tasks \\ **No. Graphs-N** \\  & 
 Cascading failure analysis- \\ Graph level tasks \\ **No. Graphs-N** \\  \\ 
**IEEE24** & 24 & 38 & 34944 & 21500 \\
**UK** & 29 & 99 & 34944 & 64000 \\
**IEEE39** & 39 & 46 & 34944 & 28000 \\
**IEEE118** & 118 & 186 & 34944 & 122500 \\  

Table 1: The PowerGraph dataset serves both node and graph-level tasks. Electrical buses symbolize nodes, while transmission lines and transformers represent edges. We detail parameters for power flow and cascading failure analyses on four chosen power grids. The loading condition data , for a period of one year, are provided at a 15-minute resolution, with each graph illustrating a single loading condition. The cascading failure analysis data is given as a set of graphs, each of which denotes the power grid loading condition linked to a triggering outage.

Each graph is assigned an output label depending on the task. For **binary classification**, the graphs are labeled as stable (DNS=0) or unstable (DNS>0). In **multi-class classification**, the graphs are categorized into four classes shown in Table 2. For **regression** tasks, the label corresponds to the DNS in MW. The choice among binary classification, multi-class classification, or regression depends on the use case of the GNN model trained with the PowerGraph dataset. Binary classification models serve as early warning systems to detect critical grid states. Multi-class models distinguish different scenarios, helping transmission system operators understand when a cascading failure might not lead to unserved demand. Regression models estimate the DNS for specific pre-outage states, acting as surrogates for physics-based models and enabling security evaluations with low computational costs.

Explainability maskWe assign ground-truth explanations as follows: when a system state undergoes a cascading failure, the cascading edges are considered to be explanations for the observed demand not served. Therefore, for the Category A instances, we record the branches that fail dur

 
**Category A** & **Category B** & **Category C** & **Category D** \\  DNS \(>0\) MW & DNS \(>0\) MW & DNS \(=0\) MW & DNS \(=0\) MW \\ c.f. occurs & no c.f. & c.f. occurs & no c.f. \\  

Table 2: Multi-class classification of datasets. c.f. stands for _cascading failure_ and describes a state resulting in cascading failure of components. DNS denotes demand not served.

Figure 1: Instance of the PowerGraph dataset for power flow and optimal power flow. The input node features are in red, and output node-level predictions are in green. The known input quantities are reported for different node types, and the unknown quantities are set to zero to maintain the dataset’s dimensionality structure (indicated by an empty cell in the picture). Similarly, the output quantities depend on the node type; if a variable is known, we mask it during training, and masked values are indicated with grey cells. The quantities are: active power generation \(P_{g}\), reactive power generation \(Q_{g}\), active power demand \(P_{d}\), reactive power demand \(Q_{d}\), voltage magnitude \(V\), and voltage angle \(\), the number of loads \(N_{loads}\), and number of generators \(N_{gen}\). The edge level features are: branch conductance \(G_{ij}\) and branch susceptance \(B_{ij}\).

Figure 2: Instance of the PowerGraph dataset for cascading failure analysis. We highlight the initial outage with the red-dotted line, which is removed from the graph connectivity matrix and from the edge feature matrix. The cascading edge is in bold and encoded in the **M** boolean vector (0 - the edge has not tripped during cascading development, 1 - otherwise). The input node features are: net active power \(P_{net}=P_{gen}-P_{load}\), net apparent power \(S_{net}=S_{gen}-S_{load}\), and voltage magnitude \(V_{i}\). Where \(P_{gen}\) and \(P_{load}\) are the active power generation and demand, respectively, and \(S_{gen}\) and \(S_{load}\) are the apparent power generation and demand, respectively. The input edge features are: active power flow \(P_{i,j}\), reactive power flow \(Q_{i,j}\), line reactance \(X_{i,j}\), and line rating \(lr_{i,j}\).

ing the development of the cascading event. We set the explainability mask as a Boolean vector \(^{|| 1}\), whose elements are equal to 1 for the edges belonging to the cascading stage and 0, otherwise (see Figure 2).

## 3 Benchmarking the PowerGraph dataset

In this section, we present the experimental setting to benchmark the PowerGraph dataset for node-level tasks (i.e. power flow and optimal power flow analysis) and graph level tasks (i.e. cascading failure analysis).

Experimental Setting and Evaluation MetricsFor each power grid dataset, both for graph and node-level tasks we utilize four baseline GNN architectures: GCNConv , GATConv , GINEConv , and TransformerConv , which are commonly used in the graph xAI community. We experiment with state-of-the-art graph transformer convolutional layers , forming the backbone of recent models such as GraphGPS , Transformer-M , and TokenGT , chosen for their ability to incorporate edge features relevant to power grids. The GNN architecture includes multiple message passing layers (MPLs), each followed by a PReLU activation function . When graph-level tasks are involved, a maximum global pooling operator for obtaining graph-level embeddings from node embeddings, and one fully connected layer. We perform a grid search over the number of MPLs (1, 2, 3) and the hidden dimensionality (8, 16, 32). The Adam optimizer is used with an initial learning rate of \(10^{-3}\), and each model is trained for 50 epochs, with the learning rate adjusted using a scheduler that automatically reduces it if the reference metric stops improving. The datasets are split into 85% train, 5% validation, and 10% test sets, with a batch size of 32 for node-level tasks and 16 for graph-level tasks. For classification models, balanced accuracy  is used due to class imbalance, (see Table 3). For regression models, mean squared error is used as the reference metric.

ObservationsTable 4 presents the best models for node-level tasks using GCN, GAT, GINe, and Transformer architectures across the four power systems. Similarly, Table 5 reports the top-performing models for graph-level tasks. In addition to the presented result in this paper and its appendices, we present the complete results, including all models and their performances, on our website: https://powergraph.ivia.ch. The website also includes Gradient Boosted Trees results as a baseline for comparison. Figure 3 illustrates the node-averaged MSE for each predicted physical quantity for the power flow tasks for the best performing models reported in Table 4.

 
**Power grid** & **Category A** & **Category B** & **Category C** & **Category D** \\  IEEE24 & 15.8\% & 4.3\% & 0.1\% & 79.7\% \\ IEEE39 & 0.55\% & 8.4\% & 0.45\% & 90.6\% \\ UK & 3.5\% & 0 & 3.8\% & 92.7\% \\ IEEE118 & >0.1\% & 5.0\% & 0.9\% & 93.9\% \\  

Table 3: Results of categorization in percentage.

Figure 3: Node-averaged Mean Absolute Errors on the predicted physical quantities for the power flow and optimal power flow problems on the best performing models reported in Table 4.

DiscussionWe train our models using data from current simulation methods (see A.2.1 and A.3), aiming for the highest accuracy in classification and regression to replicate those results, while the key advantage of our ML models lies in surpassing traditional solvers or simulations in computational efficiency. For node-level tasks, the OPF task presents a higher level of complexity, leading to a higher mean squared error (MSE) for the machine learning model trained on it compared to the model trained on the PF task, as shown in Figure 1 and Table 4. Although model performance varies slightly across different grids, we observe that GNNs are effective in predicting optimal power flow and power flow solutions across grids with varying topologies, achieving low prediction error for key physical quantities regardless of grid size.

For both graph-level and node-level tasks, the Transformer model consistently outperforms other approaches, achieving optimal results, particularly with three MPL layers in most cases. In contrast, the GCN model repeatedly demonstrates lower performance compared to other models, emphasizing its limitations for power flow tasks where edge features are critical for analyzing power flow and modeling potential cascades. This highlights the advantages of the Transformer and GINE models, which emerge as the top performers. The Transformer's attention mechanism allows it to dynamically assign importance to neighboring nodes, capturing intricate relationships that enhance its predictive accuracy. This attention-based approach significantly contributes to the effectiveness of both the Transformer and GAT models across node-level and graph-level tasks. Meanwhile, the GINE model also shows strong performance due to its powerful ability to capture the graph topology of power grids, further boosting its predictive accuracy .

While binary and multi-class classification models show good results, the regression model predicting the exact demand not served does not. For graph-level regression tasks, the R2  score, also known as the coefficient of determination, peaks at 0.43 for the best model on the IEEE24 dataset but falls below 0.26 across other power systems. Ideally, an R2 score closer to 1 is desired for a better model fit, underscoring the need for further research to enhance GNN performance on regression tasks. on improving GNN performance on regression tasks. Additionally, GNNs show limitations on node-level regression, where simpler models like Gradient Boosted Trees (GBT) sometimes outperform them on certain datasets (see https://powergraph.ivia.ch). However, this trend does not extend to graph-level tasks, where GNNs consistently prove to be the superior method. This calls for the development of GNN architectures specifically optimized for regression tasks, crucial for power systems analysis. At the same time, even minor improvements in classification tasks should not be underestimated, as they can significantly impact decision-making in critical infrastructures such as power grids.

## 4 Benchmarking explanations on the graph-classification models

Experimental setting and evaluation metricsThe PowerGraph benchmark with explanations tests and compares explainability methods. Explanations are subsets of the grids. We prefer generating graph-based explanations over textual ones, as graph structures are more intuitive and effective for conveying critical nodes and edges to experts in transmission grids. The role of explainers is to identify the edges that are necessary for the graphs to be classified as Category A. This choice is explained in Appendix A.4. Then, the resulting edges are evaluated on how well they match the explanation masks, which represent the cascading edges, using the accuracy score, and on how they contribute to the model's prediction, using the faithfulness metric. The accuracy score refers to the balanced accuracy metric, defined in Appendix A.6, between the ground truth target cascading failure edges and the estimated explanatory edges by the xAI method. The faithfulness score measures the changes

 
**Power system** & **Task** & **Best model** & **MSE** \\ 
**IEEE24** & _Powerflow_ & transformer 3h 32n & 4.35e-05\(\)7.46e-06 \\  & _Optimal powerflow_ & transformer 3h 32n & 8.99e-05\(\)3.43e-06 \\
**IEEE39** & _Powerflow_ & transformer 3h 32n & 4.35e-05\(\)1.27e-05 \\  & _Optimal powerflow_ & transformer 3h 32n & 7.38e-05\(\)7.87e-06 \\
**IEEE118** & _Powerflow_ & transformer 3h 32n & 2.39e-05\(\)3.74e-06 \\  & _Optimal powerflow_ & transformer 3h 32n & 5.74e-05\(\)4.24e-06 \\
**UK** & _Powerflow_ & transformer 3h 32n & 2.02e-05\(\)8.68e-06 \\  & _Optimal powerflow_ & transformer 3h 32n & 3.75e-05\(\)1.39e-05 \\  

Table 4: Power flow and optimal power flow model results on the test set, averaged over five random seeds. MSE error is used as a reference metric, and only the best model architecture results are reported.

in model outputs induced by retraining only with the important graph entities identified, and its definition is given in Appendix B.5. We compare the results obtained for each of the four PowerGraph datasets: IEEE24, IEEE39, IEEE118, and UK. For each dataset, we select the trained Transformer model with 3 layers and 16 hidden units, obtained with the configuration of Section 3. To benchmark explainability methods, having the best GNN model is not essential. By appropriately filtering predictions (correct or mixed) and focusing the explanations (on the phenomenon or model) , we can work around lower test accuracy. We compare non-generative methods, including the heuristic Occlusion , gradient-based methods Saliency , Integrated Gradient , and Grad-CAM , and perturbation-based methods GNNExplainer , PGMExplainer  and SubgraphX . We also consider generative methods: GSAT , D4Explainer  and RCExplainer . For more details on generative vs. non-generative explainers, see Appendix B.2 and B.3. We compare those explainability methods to the base estimators: Random, Truth, and Inverse. Random assigns random importance to edges following a uniform distribution. Truth estimates edge importance as the pre-defined ground-truth explanations of the datasets. i.e., the cascading edges. The Inverse estimator represents the worst-case scenario, assigning edges opposite to the ground truth. Appendix B provides additional experimental details on Transformer performance and the explainability methods.

Human-centric evaluationWe use the balanced accuracy metric to evaluate the generated explanations. It compares the generated edge mask to the ground-truth cascading edges and takes into account the class imbalance since the cascading edges are a small fraction of the total edges. It measures how convincing the explanations are to humans. Appendix A.6 gives more details about this metric. We report the performance of 14 explainability methods on finding ground-truth explanations. All results are averaged on five random seeds.

Figure 4 shows the results of the human-centric evaluation. The Truth explainer consistently achieves an accuracy score of \(1\), while the Inverse method performs significantly worse with an accuracy score of \(0.5\), as expected. Additionally, the Random method exhibits similarly low accuracy scores, which can be attributed to the small amount of cascading edges within the grids (see Table 6). Regarding the IEEE39 and IEEE118 grids, none of the explainability methods succeed in identifying the cascading edges within their \(topk^{*}\) explanatory edges. The low performance of xAI methods on balanced accuracy for IEEE39 and IEEE118 is likely due to Category A graphs representing less than 1% of the

 
**Power System** & **IEEE24** & **UK** & **IEEE39** & **IEEE118** \\ 
**No. Explained grids** & 3416 & 2236 & 154 & 11 \\
**Avg No. cascading edges** & 2.4 & 6.8 & 3.1 & 3.4 \\
**Max No. cascading edges** & 8 & 10 & 10 & 6 \\
**Ratio GT edges/Total edges** & 0.2 & 0.1 & 0.2 & 0.03 \\  

Table 6: Explainability results summary, using the number of explained instances and the number of ground-truth edges, i.e., the cascading failure edges.

 
**Power system** & **Task** & **Best model** & **Metric** & \\ 
**IEEE24** & _binary_ & transformer 3h 32n & Balanced accuracy & 0.9828\(\)0.0056 \\  & _multiclass_ & transformer 3h 32n & Balanced accuracy & 0.9828\(\)0.0056 \\  & _regression_ & gin 3h 32n & MSE & 7.82e-04\(\)1.62e-04 \\ 
**IEEE39** & _binary_ & transformer 2h 32n & Balanced accuracy & 0.9880\(\)0.0020 \\  & _multiclass_ & transformer 3h 32n & Balanced accuracy & 0.9765\(\)0.0064 \\  & _regression_ & transformer 2h 16n & MSE & 6.31e-05\(\)1.86e-05 \\ 
**IEEE118** & _binary_ & gin 3h 32n & Balanced accuracy & 0.9982\(\)0.0006 \\  & _multiclass_ & gin 3h 32n & Balanced accuracy & 0.9962\(\)0.0014 \\  & _regression_ & gin 3h 32n & MSE & 2.99e-06\(\)3.19e-06 \\ 
**UK** & _binary_ & gin 3h 32n & Balanced accuracy & 0.9951\(\)0.0022 \\  & _multiclass_ & gin 2h 32n & Balanced accuracy & 0.9845\(\)0.0019 \\  & _regression_ & transformer 3h 32n & MSE & 1.07e-03\(\)3.47e-04 \\  

Table 5: Cascading failure analysis results on the test set averaged over five random seeds. The reference metric for the classification tasks is balanced accuracy, and the regression task is MSE. Only the best model architecture results are reporteddatasets, see Table 3. Resulting in a low number of explainable instances, i.e., 154 and 11 explainable graph instances, respectively. For the IEEE24 and UK datasets, which have more instances of Category A, GSAT significantly outperforms other methods, achieving a balanced accuracy of \(0.9\) for IEEE24 and almost \(0.8\) for UK. GradCAM and Occlusion methods are also able to identify a few cascading edges in these cases.

Model-centric evaluationHuman evaluation is not always practical because it requires ground truth explanations and is subjective to human judgement, not necessarily accounting for the model's reasoning. Model-focus evaluation, however, measures the consistency of model predictions, removing or keeping the explanatory graph entities. We, therefore, evaluate the faithfulness of the explanations using the fidelity+ metric. The fidelity+ measures how necessary the explanatory edges are to the GNN predictions. For PowerGraph, edges with high fidelity+ are the ones necessary for the graph to belong to Category A. The \(fid^{prob}_{+}\) is computed as in and described in Appendix B.5. We follow the systematic evaluation framework GraphFramEx , more details found in B.4.

Figure 5 shows the results of the model-centric evaluation of the explainability methods. Gradient-based methods generate the most faithful explanations between the Powergraph datasets. For the three IEEE datasets, IntegratedGrad and Saliency are the most faithful explainers; for the UK dataset, Occlusion and GradCAM give faithful results. These observations are consistent with the conclusions in , i.e. gradient-based xAI methods return more faithful explanations. Nevertheless, we observe low faithfulness scores for IEEE39 and IEEE118. Particularly for IEEE118, all methods show an average faithfulness score close to zero. Again, this might originate from the low ratio of cascading edges compared to the total number of edges in IEEE118 (refer to Table 6 for details). In addition, IEEE118 is the largest power grid with 186 branches, containing complex interdependencies among its power grid elements during cascading failures. Consequently, node and edge-level features play a significant role in explaining the GNN predictions. Therefore, we believe that an accurate model explanation will be obtained only with methods that provide node and link-level feature masks, along with edge masks.

Figure 4: Balanced accuracy of the explanations with \(topk^{*}\) edges. The _top_ balanced accuracy is computed on explanatory edge masks that contain the \(topk^{*}\) edges that contribute the most to the model predictions, with \(topk^{*}\) being the number of edges in the corresponding ground-truth explanations, i.e. the maximum number of cascading edges for each dataset.

Figure 5: Faithfulness of the explanations with \(topk^{*}\) edges. The faithfulness score is measured with the \(fid+^{acc}\) metric as defined in Equation 7 in Appendix B.5. The optimal number \(topk^{*}\) of edges kept for the explanations corresponds to the maximum number of expected cascading edges (i.e., ground truth explanations) and depends on the dataset.

DiscussionWhen comparing the outcomes of the human-centric evaluation in Figure 4 and the model-centric evaluation in Figure 5, a trend emerges: most methods exhibit contrasting performances when evaluated on the human-centric accuracy or the model-centric faithfulness. This means that explainability methods that excel in accuracy may not necessarily produce faithful explanations, and vice versa. In the model's perspective, cascading edges are not always necessary for predicting Category A events, i.e., DNS>0 and cascading failures. The ground-truth cascading edges (represented by the Truth baseline) reveal that they are not faithful to the model for both IEEE39 and IEEE118, and they are only marginally faithful for the UK dataset. Furthermore, explanations generated by faithful methods like Saliency and IntegratedGrad do not include any cascading edges. These observations highlight significant differences between human explanations based on physics-based simulations and model-centric explanations. Only GSAT generates accurate and faithful explanations, bridging the gap between human and model perspectives.

## 5 Limitations

While this study provides a valuable benchmark for power grid analysis using GNN models, there are limitations that should be acknowledged. Using power grid models, such as the IEEE test case systems, is a standard practice in power engineering modeling and analyses. Although these power systems are not ideal, they are still based on real power grids. The main reason we utilize these grids in our study is because real power system data is safety-critical and often classified, making it inaccessible for research. Furthermore, cascading failures are rare events, and data on their progression and the conditions under which they occur are largely unavailable. As a result, synthetic data is the only viable option for cascading failure analysis, which could limit the generalizability of the findings to real-world scenarios. Nevertheless, we use the Cascades model to simulate cascading failures and generate GNN datasets, which have been validated with historical blackout data from the WECC grid . Furthermore, the small number of buses in the IEEE test cases may constrain the ability of the models to capture long-range dependencies, potentially affecting their performance on larger, more complex power grids. While this work demonstrates a method for modeling power systems with GNNs, other approaches could be explored in future research to enhance model accuracy and robustness. For instance, incorporating energy price variations to analyze the multi-period economic dispatch, multi-period OPF and unit commitment could provide a more comprehensive understanding of the economic aspects of power grid operations, which was not considered in this study

## 6 Conclusions

PowerGraph addresses critical challenges encountered in utilizing Graph Neural Networks (GNNs) for power systems analyses. PowerGraph effectively fills a significant gap in the GNN domain for power engineering applications by offering a dataset tailored for node and graph-level tasks and model explainability. Through rigorous benchmarking against a range of GNN and explainability models, PowerGraph exhibits high performance in graph classification, albeit indicating a need for further refinement in regression models. Regression models remain essential in power systems and can be solved by GNNs for tasks like power flow and system security analysis. However, GNN still encounters challenges that demand further research and development. Across power grid analysis tasks, we find that an architecture with three message-passing layers yields the most accurate predictions for both power flow and cascading failure analysis. This result suggests that incorporating information from up to three-hop neighbors is essential for precise modeling, effectively balancing local and global network information required for accurate predictions in both graph- and node-level tasks. Furthermore, PowerGraph is the first real-world dataset to offer ground-truth explanations for graph-level tasks in explainable AI, enabling thorough evaluation of various explainability methods. Despite this advancement, the performance of these methods remains suboptimal, highlighting the lack of a universally superior approach. Given the crucial role of explainability for power grid operators, this underscores the ongoing need for dedicated research and development in this field. Looking forward, we plan to enhance PowerGraph by adding a temporal graph dataset to facilitate in-depth analysis of the stages of cascading failures. Although this information is already present in the dataset, please refer to A.5, it was beyond the scope of this work. We also benchmark methods on larger synthetic power systems, such as the grid in , to assess whether deeper GNN architectures are necessary, as noted in . A raw dataset and the benchmarking results are available at https://powergraph.ivia.ch.