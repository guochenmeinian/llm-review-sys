# Optimus-1 : Hybrid Multimodal Memory

Empowered Agents Excel in Long-Horizon Tasks

Zaijing Li\({}^{1}\)\({}^{2}\), Yuquan Xie\({}^{1}\), Rui Shao\({}^{1}\), Gongwei Chen\({}^{1}\), Dongmei Jiang\({}^{2}\), Liqiang Nie\({}^{1}\)

\({}^{1}\)Harbin Institute of Technology, Shenzhen

\({}^{2}\)Peng Cheng Laboratory

{lzj14011,xieyuquan20016,rshaojimmy,nieliqiang}@gmail.com

Corresponding authors

###### Abstract

Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a **Hybrid Multimodal Memory** module to address the above challenges. It **1)** transforms knowledge into **Hierarchical Directed Knowledge Graph** that allows agents to explicitly represent and learn world knowledge, and **2)** summarises historical information into **Abstracted Multimodal Experience Pool** that provide agents

Figure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task “Craft stone sword”, Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed Knowledge Graph into planning, then Action Controller executes these planning sequences step-by-step. During the execution of the task, the Experience-Driven Reflector is periodically activated and retrieve experience from Abstracted Multimodal Experience Pool to make reflection.

with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated **Knowledge-guided Planner** and **Experience-Driven Reflector**, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on various tasks. Please see the project page at https://cybertronagent.github.io/Optimus-1.github.io/.

## 1 Introduction

Optimus Prime faces complex tasks alongside humans in Transformers to protect the peace of the planet. Creating an agent [44; 13] like Optimus that can perceive, plan, reflect, and complete long-horizon tasks in an open world has been a longstanding aspiration in the field of artificial intelligence [22; 36; 37; 27; 58]. Early research developed simple policy through reinforcement learning  or imitation learning [1; 25]. A lot of work [47; 50] have utilized Large Language Models (LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action controllers. Further, recent studies [52; 33] employed Multimodal Large Language Models (MLLMs) [4; 39; 56] as planner and reflector. Leveraging the powerful instruction-following and logical reasoning capabilities of (Multimodal) LLMs , LLM-based agents have achieved remarkable success across multiple domains [14; 9; 10; 55]. Nevertheless, the ability of these agents to complete long-horizon tasks still falls significantly short of human-level performance.

According to relevant studies [28; 42; 46], the human ability to complete long-horizon tasks in an open world relies on long-term memory storage, which is divided into knowledge and experience. The storage and utilization of knowledge and experience play a crucial role in guiding human behavior and enabling humans to adapt flexibly to their environments in order to accomplish long-horizon tasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:

**Insufficient Exploration of Structured Knowledge**: Structured knowledge, encompassing open world rules, object relationships, and interaction methods with the environment, is essential for agents to complete complex tasks [34; 44]. However, MLLMs such as GPT-4V 1 lack sufficient knowledge in Minecraft. Existing agents [1; 25; 7] only learn dispersed knowledge from video data and are unable to efficiently represent and learn this structured knowledge, rendering them incapable of performing complex tasks.

**Lack of Multimodal Experience**: Humans derive successful strategies and lessons from information on historical experience [8; 32], which assists them in tackling current complex tasks. In a similar manner, agents can benefit from in-context learning with experience demonstrations [43; 54]. However, existing agents [47; 51; 33] only consider unimodal information, which prevents them from learning from multimodal experience as humans do.

To address the aforementioned challenges, we propose **Hybrid Multimodal Memory** module that consists of **Hierarchical Directed Knowledge Graph** (HDKG) and **Abstracted Multimodal Experience Pool** (AMEP). For HDKG, we map the logical relationships between objects into a directed graph structure, thereby transforming knowledge into high-level semantic representations. HDKG efficiently provides the agent with the necessary knowledge for task execution, without requiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal information (_e.g.,_ environment, agent state, task plan, video frames, etc.) from the agent's task execution process, ensuring that historical information contains both a global overview and local details. Different from the method of directly storing successful cases as experience , AMEP considers both successful and failed cases as references. This innovative approach of incorporating failure cases into in-context learning significantly enhances the performance of the agent.

On top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent, **Optimus-1**. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. To enhance the ability of agents to cope with complex environments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation into the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to efficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the current observation as inputs and generates low-level actions, interacting with the game environment to update the agent's state. In open-world complex environments, agents are prone to be erroneous when performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which is periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages the agent to reflect on its current actions and refine the plan.

We validate the performance of Optimus-1 in Minecraft, a popular open-world game environment. Experimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks, representing up to 30\(\%\) improvement over existing agents. Moreover, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal Memory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we verified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally improve its performance in a self-evolution manner. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance. Main contributions of our paper:

* We propose **Hybrid Multimodal Memory** module which is composed of HDKG and AMEP. HDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined historical experience and guides the agent to reason about the current situation state effectively.
* On top of the Hybrid Multimodal Memory module, we construct **Optimus-1**, which consists of **Knowledge-Guided Planner**, **Experience-Driven Reflector**, and Action Controller. Optimus-1 outperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to the level of human players.
* Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement, demonstrating the generalization of Hybrid Multimodal Memory.

## 2 Optimus-1

In this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1. As a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks. Next, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal Memory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally, we introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3), thereby enhancing the success rate of task execution for Optimus-1.

### Hybrid Multimodal Memory

In order to endow agent with a long-term memory storage mechanism [28; 46], we propose the Hybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool (AMEP) and Hierarchical Directed Knowledge Graph (HDKG).

#### 2.1.1 Abstracted Multimodal Experience Pool

Relevant studies [23; 29; 17; 15] highlight the importance of historical information for agents completing long-horizon tasks. Minedojo  and Voyager  employed unimodal storage of historical information. Jarvis-1  used a multimodal experience mechanism that stores task planning and visual information without summarization, posing challenges to storage capacity and retrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all multimodal information during task execution. It preserves the integrity of long-horizon data while enhancing storage and retrieval efficiency.

Specifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further perform a dynamic visual information abstraction, these frames are then fed into an image buffer with a window size of 16, where the image similarity is dynamically computed and final abstracted frames are adaptively updated. To align such abstracted visual information with the corresponding textual sub-goal, we then utilize MineCLIP , a pre-trained video-text alignment model, to calculate their multimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer and textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate environment information, agent initial state, and plan generated by Knowledge-Guided Planner, into such a pool, which forms the AMEP. In this way, we consider the multimodal information of each sub-goal, and summarise it to finally compose the multimodal experience of the given task.

#### 2.1.2 Hierarchical Directed Knowledge Graph

In Minecraft, mining and crafting represent a complex knowledge network crucial for effective task planning. For instance, crafting a diamond sword requires two diamonds and one wooden stick, while mining diamonds requires an iron pickcase, which involving further materials and steps. Such knowledge is essential for an agent's ability to perform long-horizon complex tasks. Instead of implicit learning through fine-tuning , we propose HDKG, which transforms knowledge into a graph representation. It enables the agent to perform explicit learning by retrieving information from the knowledge graph.

As shown in the Figure 2, we transform knowledge into a graph, where nodes set represent objects, and directed edges set point to nodes that can be crafted by this object. An edge in the can be represented as, where. The directed graph efficiently stores and updates knowledge. For a given object, retrieving the corresponding node allows extraction of a sub-graph, where nodes set and edges set can be formulated as:

(1)

Then by topological sorting, we can get all the materials and their relationships needed to complete the task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more

Figure 2: **(a)** Extraction process of multimodal experience. The frames are filtered through video buffer and image buffer, then MineCLIP  is employed to compute the visual and sub-goal similarities and finally they are stored in Abstracted Multimodal Experience Pool. **(b)** Overview of Hierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes represent objects, and directed edges point to materials that can be crafted by this object.

reasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge of the agent in a train-free manner.

### Optimus-1: Framework

Relevant studies indicate that the human brain is essential for planning and reflection, while the cerebellum controls low-level actions, both crucial for complex tasks [40; 41]. Inspired by this, we divide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In a given game environment with a long-horizon task, the Knowledge-Guided Planner senses the environment, retrieves knowledge from HDKG, and decomposes the task into executable sub-goals. The action controller then sequentially executes these sub-goals. During execution, the Experience-Driven Reflector is activated periodically, leveraging historical experience from AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the Knowledge-Guided Planner to revise its plan. Through iterative interaction with the environment, Optimus-1 ultimately completes the task.

**Knowledge-Guided Planner**. Open-world environments vary greatly, affecting task execution. Previous approaches  using LLMs for task planning failed to consider the environment, leading to the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information to plan conditions on the current situation, such as "leave the cave and find a river". Therefore, we integrate environmental information into the planning stage. Unlike Jarvis-1  and MP5 , which convert observation to textual descriptions, Optimus-1 directly employs observation as visual conditions to generate environment-related plans, _i.e.,_ sub-goal sequences. This results in more comprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves the knowledge needed to complete the task from HDKG, allowing task planning to be done once, rather than generating the next step in each iteration. Given the task \(t\), observation \(o\), the sub-goals

Figure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given the task “craft stone sword”, Optimus-1 incorporates the knowledge from HDKG into Knowledge-Guided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is periodically activated to introduce multimodal experience from AMEP to determine if the current task can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan.

[MISSING_PAGE_FAIL:6]

## 3 Experiments

### Experiments Setting

**Environment**. To ensure realistic gameplay like human players, we employ MineRL  with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the **Appendix** B.

**Benchmark**. We constructed a benchmark of 67 tasks to evaluate the Optimus-1's ability to complete long-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft. Please refer to **Appendix** D for more details.

**Baseline**. We compare Optimus-1 with various agents, including GPT-3.5 2, GPT-4V, DEPS , and Jarvis-1  on the challenging long-horizon tasks benchmark. In addition, we employed 10 volunteers to perform the same task on the benchmark, and their average performance served as a human-level baseline. Please refer to **Appendix** D.2 for more details about human-level baseline. For a more comprehensive comparison, we also report Optimus-1's performances on the benchmark used by Voyager , MP5 , and DEPS  in the **Appendix** F.2. Note that we initialize Optimus-1 with an empty inventory, while DEPS  and Jarvis-1  have tools in their initial state. This makes it more challenging for Optimus-1 to perform the same tasks.

   Group & Metric & GPT-3.5 & GPT-4V & DEPS & Jarvis-1 & Optimus-1 & Human-level \\   & SR \(\) & 40.16 & 41.42 & 77.01 & 93.76 & **98.60** & 100.00 \\  & AT \(\) & 56.39 & 55.15 & 85.53 & 67.76 & **47.09** & 31.08 \\  & AS \(\) & 1127.78 & 1103.04 & 1710.61 & 1355.25 & **841.94** & 621.59 \\   & SR \(\) & 20.40 & 20.89 & 48.52 & 89.20 & **92.35** & 100.00 \\  & AT \(\) & 135.71 & 132.77 & 138.71 & 141.50 & **129.94** & 80.85 \\  & AS \(\) & 2714.21 & 2655.47 & 2574.30 & 2830.05 & **2518.88** & 1617.00 \\   & SR \(\) & 0.00 & 0.00 & 16.37 & 36.15 & **46.69** & 86.00 \\  & AT \(\) & \(+\) & \(+\) & 944.61 & 722.78 & **651.33** & 434.38 \\  & AS \(\) & \(+\) & \(+\) & 8892.24 & 8455.51 & **6017.85** & 5687.60 \\   & SR \(\) & 0.00 & 0.00 & 0.00 & 7.20 & **8.51** & 17.31 \\  & AT \(\) & \(+\) & \(+\) & \(+\) & 787.37 & **726.35** & 557.08 \\  & AS \(\) & \(+\) & \(+\) & \(+\) & 15747.13 & **15527.07** & 13141.60 \\   & SR \(\) & 0.00 & 0.00 & 0.60 & 8.98 & **11.61** & 16.98 \\  & AT \(\) & \(+\) & \(+\) & 1296.96 & 1255.06 & **1150.98** & 744.82 \\  & AS \(\) & \(+\) & \(+\) & 23939.30 & 25101.25 & **23019.64** & 16237.54 \\   & SR \(\) & 0.00 & 0.00 & 0.00 & 16.31 & **25.02** & 33.27 \\  & AT \(\) & \(+\) & \(+\) & \(+\) & 1070.42 & **932.50** & 617.89 \\  & AS \(\) & \(+\) & \(+\) & \(+\) & 17408.40 & **12709.99** & 12357.00 \\   & SR \(\) & 0.00 & 0.00 & 9.98 & 15.82 & **19.47** & 28.48 \\  & AT \(\) & \(+\) & \(+\) & 997.59 & 924.60 & **824.53** & 551.30 \\   & AS \(\) & \(+\) & \(+\) & 17951.95 & 16492.96 & **16350.56** & 11026.00 \\  Overall & SR \(\) & 0.00 & 0.00 & 5.39 & 16.89 & **22.26** & 36.41 \\   

Table 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success rate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each task can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient at completing the task, while \(+\) indicates that the agent is unable to complete the task. Overall represents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor.

**Evaluation Metrics**. The agent always starts in survival mode, with an empty inventory. We conducted at least 30 times for each task using different world seeds and reported the average success rate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time of completing the task as evaluation metrics.

### Experimental Results

The overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in **Appendix** F. Optimus-1 has a success rate near 100\(\%\) on the Wood Group. Compared with Jarvis-1, Optimus-1 has 29.28\(\%\) and 53.40\(\%\) improvement on the Diamond Group  and Redstone Group, respectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task groups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover, compared with all baselines, Optimus-1 performance was closer (average 5.37\(\%\) improvement) to human levels on long-horizon task groups.

### Ablation Study

We conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6. As shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector, the performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of Knowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon tasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help of world knowledge, the performance of Optimus-1 decreased by an average of 20\(\%\) across all task groups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an

    &  \\  P. & R. & K. & E. & Wood & Stone & Iron & Gold & Diamond \\   & & & 14.29 & 0.00 & 0.00 & 0.00 & 0.00 \\ ✓ & & & 42.95 & 25.67 & 0.00 & 0.00 & 0.00 \\ ✓ & ✓ & & 55.00 & 47.37 & 18.11 & 2.08 & 1.11 \\ ✓ & ✓ & ✓ & & 73.53 & 64.20 & 24.19 & 3.08 & 1.86 \\ ✓ & ✓ & ✓ & & 92.37 & 69.63 & 38.33 & 3.49 & 2.42 \\ ✓ & ✓ & ✓ & ✓ & **97.49** & **94.26** & **53.33** & **11.54** & **9.59** \\   

Table 2: Ablation study results. We report average Table 3: Ablation study on AMEP. We report success rate (SR) on each task group. P., R., K., the average success rate (SR) on each task group. E. represent Planning, Reflection, Knowledge, and Zero, Suc., and Fail. represent retrieving from AMEP without getting the case, getting the success case, and getting the failure case, respectively.

Figure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms, STEVE-1  often gets into trouble and fails to complete the task. While Optimus-1, with the help of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect current situation and correct errors. This improves Optimus-1’s success rate on long-horizon tasks.

average of 12\(\%\). Finally, we performed ablation experiments on the way of retrieving cases from AMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average of 10\(\%\) decrease across all groups. It reveals that this reflection mechanism, which considers both success and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the role of the reflection mechanism, we have shown some cases in Figure 4.

### Generalization Ability

In this section, we explore an interesting issue: whether generic MLLMs can effectively perform various long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in Figure 5, We employ Deepseek-VL  and InternLM-XComposer2-VL  as Knowledge-Guided Planner and Experience-Driven Reflector. The experimental results show that the original MLLM has low performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft. With the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2 to 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result demonstrates the generalization of the proposed Hybrid Multimodal Memory.

### Self-Evolution via Hybrid Multimodal Memory

As shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then update it multiple times by using the "free exploration-teacher guidance" learning method. We set the epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free exploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate Optimus-1's learning ability on the task groups same as ablation study. Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM with Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner .

## 4 Related Work

### Agents in Minecraft

We summarise the differences of existing Minecraft agents in the **Appendix**D.3. Earlier work [30; 57; 2; 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP  used text-video data to train a contrastive video-language model as a reward model for policy, while VPT  pre-trained on unlabelled videos but lacked instruction as input. Building on VPT and MineCLIP, STEVE-1  added text input to generate low-level action sequences from human instructions and images. However, these agents struggle with complex tasks due to limitations in instruction comprehension and planning. Recent work [50; 47; 61] incorporated LLMs as planning and reflection modules, but lacked visual information integration for adaptive planning. MP5

Figure 5: **(a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1 success rate on the unseen task over 4 epochs.**

, MineDreamer , and Jarvis-1  enhanced situation-aware planning by obtaining textual descriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues by directly using observation as situation-aware conditions in the planning phase, enabling more rational, visually informed planning. Additionally, unlike other agents requiring multiple queries for task refinement, Optimus-1 generates a complete and effective plan in one step with the help of HDKG. This makes Optimus-1 planning more efficient.

### Memory in Agents

In the agent-environment interaction process, memory is key to achieving experience accumulation , environment exploration , and knowledge abstraction . There are two forms to represent memory content in LLM-based agents: textual form [17; 15; 31] and parametric form [5; 29; 48; 20]. In textual form, the information is explicitly retained and recalled by natural languages. In parametric form, the memory information  is encoded into parameters and implicitly influences the agent's actions. Recent work [49; 53; 12] has explored the long-term visual information storage [18; 19] and summarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and can provide world knowledge and multimodal experience for Optimus-1 efficiently.

## 5 Conclusion

In this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG and AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent, and AMEP provides the refined historical experience for the reflection phase of the agent. On top of the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1, in Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents on long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid Multimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V baseline. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance.

## 6 Limitation and Future Work

In the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent's ability to plan and reflect. For Action Controller, we directly introduce STEVE-1  as a generator of low-level actions. However, limited by STEVE-1's ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as "beat ender dragon" and "build a house". Therefore, a potential future research direction is to enhance the instruction following and action generation capabilities of action controller.

In addition, most of the work, including Optimus-1, utilize a multimodal large language model for planning and reflection, which then drives an action controller to perform the task. Building an end-to-end vision-language-action agent will be future work.

## 7 Acknowledgement

This study is supported by National Natural Science Foundation of China (Grant No. 62236003 and 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005), Natural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and Major Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08).