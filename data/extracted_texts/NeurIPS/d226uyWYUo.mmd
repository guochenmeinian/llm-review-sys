# Knowledge Graph Completion by Intermediate Variables Regularization

Changyi Xiao, Yixin Cao

School of Computer Science, Fudan University

changyi_xiao@fudan.edu.cn, caoyixin2011@gmail.com

Corresponding author.

###### Abstract

Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis. The code is available at https://github.com/changyi7231/IVR.

## 1 Introduction

A knowledge graph (KG) can be represented as a 3rd-order binary tensor, in which each entry corresponds to a triplet of the form \((head\ entity,relation,tail\ entity)\). A value of 1 denotes a known true triplet, while 0 denotes a false triplet. Despite containing a large number of known triplets, KGs are often incomplete, with many triplets missing. Consequently, the 3rd-order tensors representing the KGs are incomplete. The objective of knowledge graph completion (KGC) is to infer the true or false values of the missing triplets based on the known ones, i.e., to predict which of the missing entries in the tensor are 1 or 0.

A number of models have been proposed for KGC, which can be classified into translation-based models, tensor decomposition-based (TDB) models and neural networks models . We only focus on TDB models in this paper due to their wide applicability and great performance . TDB models can be broadly categorized into two groups: CANDECOMP/PARAFAC (CP) decomposition-based models, including CP , DistMult  and ComplEx , and Tucker decomposition-based models, including SimplE , ANALOGY , QuatE  and TuckER . To provide a thorough understanding of TDB models, we present a summary of existing models and derive a general form that unifies them, which provides a fundamental basis for further exploration of TDB models.

TDB models have been proven to be theoretically fully expressive (Trouillon et al., 2017; Kazemi and Poole, 2018; Balazevic et al., 2019), implying they can represent any real-valued tensor. However, in practice, TDB models frequently fall prey to severe overfitting. To counteract this issue, various regularization techniques have been employed in KGC. One commonly used technique is the squared Frobenius norm regularization (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2017). Lacroix et al. (2018) proposed another regularization, N3 norm regularization, based on the tensor nuclear \(p\)-norm, which outperforms the squared Frobenius norm in terms of performance. Additionally, Zhang et al. (2020) introduced DURA, a regularization technique based on the duality of TDB models and distance-based models that results in significant improvements on benchmark datasets. Nevertheless, N3 and DURA rely on CP decomposition, limiting their applicability to CP (Lacroix et al., 2018) and ComplEx (Trouillon et al., 2017). As a result, there is a pressing need for a regularization technique that is widely applicable and can effectively alleviate the overfitting issue.

In this paper, we introduce a novel regularization method for KGC to improve the performance of TDB models. Our regularization focuses on preventing overfitting while maintaining the expressiveness of TDB models as much as possible. It is applicable to most TDB models, while also ensuring tractable computation. Existing regularization methods for KGC rely on minimizing the norms of embeddings to regularize the model (Yang et al., 2014; Lacroix et al., 2018), leading to suboptimal performance. To achieve superior performance, we present an intermediate variables regularization (IVR) approach that minimizes the norms of intermediate variables involved in the processes of computing the predicted tensor of TDB models. Additionally, our approach fully considers the computing ways because different ways of computing the predicted tensor may generate different intermediate variables.

To support the efficacy of our regularization approach, we further provide a theoretical analysis. We prove that our regularization is an upper bound of the overlapped trace norm (Tomioka et al., 2011). The overlapped trace norm is the sum of the trace norms of the unfolding matrices along each mode of a tensor (Kolda and Bader, 2009), which can be considered as a surrogate measure of the rank of a tensor. Thus, the overlapped trace norm reflects the correlation among entities and relations, which can pose a constraint to jointly entities and relations embeddings learning. In specific, entities and relations in KGs are usually highly correlated. For example, some relations are mutual inverse relations, or a relation may be a composition of another two relations (Zhang et al., 2021). Through minimizing the upper bound of the overlapped trace norm, we encourage a high correlation among entities and relations, which brings strong regularization and alleviates the overfitting problem.

The main contributions of this paper are listed below:

1. We present a detailed overview of a wide range of TDB models and establish a general form to serve as a foundation for further TDB model analysis.
2. We introduce a new regularization approach for TDB models based on the general form to mitigate the overfitting issue, which is notable for its generality and effectiveness.
3. We provide a theoretical proof of the efficacy of our regularization and validate its practical utility through experiments.

## 2 Related Work

Tensor Decomposition Based ModelsResearch in KGC has been vast, with TDB models garnering attention due to their superior performance. The two primary TDB approaches that have been extensively studied are CP decomposition (Hitchcock, 1927) and Tucker decomposition (Tucker, 1966). CP decomposition represents a tensor as a sum of \(n\) rank-one tensors, while Tucker decomposition decomposes a tensor into a core tensor and a set of matrices. Kolda and Bader (2009) shows more details about CP decomposition and Tucker decomposition.

Several techniques have been developed for applying CP decomposition and Tucker decomposition in KGC. For instance, Lacroix et al. (2018) employed the original CP decomposition, whereas DistMult (Yang et al., 2014), a variant of CP decomposition, made the embedding matrices of head entities and tail entities identical to simplify the model. SimplE (Kazemi and Poole, 2018) tackled the problem of independence among the embeddings of head and tail entities within CP decomposition. ComplEx (Trouillon et al., 2017) extended DistMult to the complex space to handle asymmetric relations. QuatE (Zhang et al., 2019) explored hypercomplex space to further enhance KGC. Other techniquesinclude HolE (Nickel et al., 2016) proposed by Nickel et al. (2016), which operates on circular correlation, and ANALOGY (Liu et al., 2017), which explicitly utilizes the analogical structures of KGs. Notably, Liu et al. (2017) confirmed that HolE is equivalent to ComplEx. Additionally, Balazevic et al. (2019) introduced TuckER, which is based on the Tucker decomposition and has achieved state-of-the-art performance across various benchmark datasets for KGC. Nickel et al. (2011) proposed a three-way decomposition RESCAL over each relational slice of the tensor. You can refer to (Zhang et al., 2021) or (Ji et al., 2021) for more detailed discussion about KGC models or TDB models.

RegularizationAlthough TDB models are highly expressive (Trouillon et al., 2017; Kazemi and Poole, 2018; Balazevic et al., 2019), they can suffer severely from overfitting in practice. Consequently, several regularization approaches have been proposed. A common regularization approach is to apply the squared Frobenius norm to the model parameters (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2017). However, this approach does not correspond to a proper tensor norm, as shown by Lacroix et al. (2018). Therefore, they proposed a novel regularization method, N3, based on the tensor nuclear 3-norm, which is an upper bound of the tensor nuclear norm. Likewise, Zhang et al. (2020) introduced DURA, a regularization method that exploits the duality of TDB models and distance-based models, and serves as an upper bound of the tensor nuclear 2-norm. However, both N3 and DURA are derived from the CP decomposition, and thus are only applicable to CP and ComplEx models.

## 3 Methods

In Section 3.1, we begin by providing an overview of existing TDB models and derive a general form for them. Thereafter, we present our intermediate variables regularization (IVR) approach in Section 3.2. Finally, in Section 3.3, we provide theoretical analysis to support the efficacy of our proposed regularization technique.

### General Form

To facilitate the subsequent theoretical analysis, we initially provide a summary of existing TDB models and derive a general form for them. Given a set of entities \(\) and a set of relations \(\), a KG contains a set of triplets \(=\{(i,j,k)\}\). Let \(\{0,1\}^{||||||}\) represent the KG tensor, with \(_{ijk}=1\) iff \((i,j,k)\), where \(||\) and \(||\) denote the number of entities and relations, respectively. Let \(^{|| D},^{| | D}\) and \(^{|| D}\) be the embedding matrices of head entities, relations and tail entities, respectively, where \(D\) is the embedding dimension.

Various TDB models can be attained by partitioning the embedding matrices into \(P\) parts. We reshape \(^{|| D},^{| | D}\) and \(^{|| D}\) into \(^{||(D/P) P},^{| |(D/P) P}\) and \(^{||(D/P) P}\), respectively, where \(P\) is the number of parts we partition. For different \(P\), we can get different TDB models.

CP/DistMultLet \(P=1\), CP (Lacroix et al., 2018) can be represented as

\[_{ijk}=_{i:1},_{j:1},_{k:1}:=_{d=1}^ {D/P}_{id1}_{jd1}_{kd1}\]

where \(,,\) is the dot product of three vectors. DistMult (Yang et al., 2014), a particular case of CP, which shares the embedding matrices of head entities and tail entities, i.e., \(=\).

ComplEx/HolELet \(P=2\), ComplEx (Trouillon et al., 2017) can be represented as

\[_{ijk}=_{i:1},_{j:1},_{k:1}+ {H}_{i:2},_{j:1},_{k:2}+_{i:1},_{j:2}, _{k:2}-_{i:2},_{j:2},_{k:1}\]

Liu et al. (2017) proved that HolE (Nickel et al., 2011) is equivalent to ComplEx.

SimplELet \(P=2\), SimplE (Kazemi and Poole, 2018) can be represented as

\[_{ijk}=_{i:1},_{j:1},_{k:2}+ {H}_{i:2},_{j:2},_{k:1}\]ANALOGYLet \(P=4\), ANALOGY (Liu et al., 2017) can be represented as

\[_{ijk} =_{i:1},_{j:1},_{k:1}+ _{i:2},_{j:2},_{k:2}+_{i:3},_{j:3},_ {k:3}+_{i:3},_{j:4},_{k:4}\] \[+_{i:4},_{j:3},_{k:4}-_{i:4},_{j:4},_{k:3}\]

QuatELet \(P=4\), QuatE (Zhang et al., 2019) can be represented as

\[_{ijk} =_{i:1},_{j:1},_{k:1}-_{i:2},_{j:2},_{k:1}-_{i:3},_{j:3}, _{k:1}-_{i:4},_{j:4},_{k:1}\] \[+_{i:1},_{j:2},_{k:2}+ {H}_{i:2},_{j:1},_{k:2}+_{i:3},_{j:4}, _{k:2}-_{i:4},_{j:3},_{k:2}\] \[+_{i:1},_{j:3},_{k:3}-_{i:2},_{j:4},_{k:3}+_{i:3},_{j:1}, _{k:3}+_{i:4},_{j:2},_{k:3}\] \[+_{i:1},_{j:4},_{k:4}+ {H}_{i:2},_{j:3},_{k:4}-_{i:3},_{j:2}, _{k:4}+_{i:4},_{j:1},_{k:4}\]

TuckERLet \(P=D\), TuckER (Balazevic et al., 2019) can be represented as

\[_{ijk}=_{l=1}^{P}_{m=1}^{P}_{n=1}^{P}_{ lmn}_{i1l}_{j1m}_{k1n}\]

where \(^{P P P}\) is the core tensor.

General FormThrough our analysis, we observe that all TDB models can be expressed as a linear combination of several dot product. The key distinguishing factors among these models are the choice of the number of parts \(P\) and the core tensor \(\). The number of parts \(P\) determines the dimensions of the dot products of the embeddings, while the core tensor \(\) determines the strength of the dot products. It is important to note that TuckER uses a parameter tensor as its core tensor, whereas the core tensors of other models are predetermined constant tensors. Therefore, we can derive a general form of these models as

\[_{ijk} =_{l=1}^{P}_{m=1}^{P}_{n=1}^{P}_{lmn} _{i:l},_{j:m},_{k:n}=_{l=1}^{P}_{m=1}^{P}_ {n=1}^{P}_{lmn}(_{d=1}^{D/P}_{idl}_{jdm}_{kdn})\] \[=_{d=1}^{D/P}(_{l=1}^{P}_{m=1}^{P}_{n=1}^{P}_{lmn}_{idl}_{jdm}_{kdn})\] (1)

or

\[=_{d=1}^{D/P}_{1}_{:d:}_{2}_{d:} _{3}_{:d:}\] (2)

where \(_{n}\) is the mode-\(n\) product (Kolda and Bader, 2009), and \(^{P P P}\) is the core tensor, which can be a parameter tensor or a predetermined constant tensor. The general form Eq.(2) can also be considered as a sum of \(D/P\) TuckER decompositions, which is also called block-term decomposition (De Lathauwer, 2008). Eq.(2) is a block-term decomposition with a shared core tensor \(\). This general form is easy to understand, facilitates better understanding of TDB models and paves the way for further exploration of TDB models. The general form presents a unified view of TDB models and helps the researchers understand the relationship between different TDB models. Moreover, the general form motivates the researchers to propose new methods and establish unified theoretical frameworks that are applicable to most TDB models. Our proposed regularization in Section 3.2 and the theoretical analysis Section 3.3 are examples of such contributions.

The Number of Parameters and Computational ComplexityThe parameters of Eq.(2) come from two parts, the core tensor \(\) and the embedding matrices \(,\) and \(\). The number of parameters of the core tensor \(\) is equal to \(P^{3}\) if \(\) is a parameter tensor and otherwise equal to 0. The number of parameters of the embedding matrices is equal to \(||D+||D\) if \(=\) and otherwise equal to \(2||D+||D\). The computational complexity of Eq.(2) is equal to \((DP^{2}||^{2}||)\). The larger the number of parts \(P\), the more expressive the model and the more the computation. Therefore, the choice of \(P\) is a trade-off between expressiveness and computation.

TuckER and Eq.(2)TuckER (Balazevic et al., 2019) also demonstrated that TDB models can be represented as a Tucker decomposition by setting specific core tensors \(\). Nevertheless, we must stress that TuckER does not explicitly consider the number of parts \(P\) and the core tensor \(\), which are pertinent to the number of parameters and computational complexity of TDB models. Moreover, in Appendix A, we demonstrate that the conditions for a TDB model to learn logical rules are also dependent on \(P\) and \(\). By selecting appropriate \(P\) and \(\), TDB models can be able to learn symmetry rules, antisymmetry rules, and inverse rules.

### Intermediate Variables Regularization

TDB models are theoretically fully expressive (Trouillon et al., 2017; Kazemi and Poole, 2018; Balazevic et al., 2019), which can represent any real-valued tensor. However, TDB models suffer from the overfitting problem in practice (Lacroix et al., 2018). Therefore, several regularization methods have been proposed, such as squared Frobenius norm method (Yang et al., 2014) and nuclear 3-norm method (Lacroix et al., 2018), which minimize the norms of the embeddings \(\{,,\}\) to regularize the model. Nonetheless, merely minimizing the embeddings tends to have suboptimal impacts on the model performance. To enhance the model performance, we introduce a new regularization method that minimizes the norms of the intermediate variables involved in the processes of computing \(\). To ensure the broad applicability of our method, our regularization is rooted in the general form of TDB models Eq.(2).

To compute \(\) in Eq.(2), we can first compute the intermediate variable \(_{1}_{:d:}_{2}_{:d:}\), and then combine \(_{:d:}\) to compute \(\). Thus, in addition to minimizing the norm of \(_{:d:}\), we also need to minimize the norm of \(_{1}_{:d:}_{2}_{:d:}\). Since different ways of computing \(\) can result in different intermediate variables, we fully consider the computing ways of \(\). Eq.(2) can also be written as:

\[ =_{d=1}^{D/P}(_{2}_{:d:}_{3}_{: d:})_{1}_{:d:},\] \[ =_{d=1}^{D/P}(_{3}_{:d:}_{1}_{: d:})_{2}_{:d:}\]

Thus, we also need to minimize the norms of intermediate variables \(\{_{2}_{:d:}_{3}_{:d:},_{3}_{ :d:}_{1}_{:d:}\}\) and \(\{_{:d:},_{:d:}\}\). In summary, we should minimize the (power of Frobenius) norms \(\{\|_{:d:}\|_{F}^{2},\|_{:d:}\|_{F}^{2},\|_{:d:}\|_{F}^{2}\}\) and \(\{\|_{1}_{:d:}_{2}_{:d:}\|_{F}^{2},\| _{2}_{:d:}_{3}_{:d:}\|_{F}^{2},\|_{3}_{:d :}_{1}_{:d:}\|_{F}^{2}\}\), where \(\) is the power of the norms.

Since computing \(\) is equivalent to computing \(_{(1)}\) or \(_{(2)}\) or \(_{(3)}\), we can also minimize the norms of intermediate variables involved in the processes of computing \(_{(1)}\), \(_{(2)}\) and \(_{(3)}\), where \(_{(n)}\) is the mode-\(n\) unfolding of a tensor \(\)(Kolda and Bader, 2009). See Figure 1 for an example

Figure 1: Left shows a 3rd order tensor. Middle describes the corresponding mode-\(i\) fibers of the tensor. Fibers are the higher-order analogue of matrix rows and columns. A fiber is defined by fixing every index but one. Right describes the corresponding mode-\(i\) unfolding of the tensor. The mode-\(i\) unfolding of a tensor arranges the mode-\(i\) fibers to be the columns of the resulting matrix.

of the notation \(_{(n)}\). We can represent \(_{(1)}\), \(_{(2)}\) and \(_{(3)}\) as (Kolda and Bader, 2009):

\[_{(1)} =_{d=1}^{D/P}(_{1}_{:d:})_{(1)}(_{:d: }_{:d:})^{T},\] \[_{(2)} =_{d=1}^{D/P}(_{2}_{:d:})_{(2)}(_{:d: }_{:d:})^{T},\] \[_{(3)} =_{d=1}^{D/P}(_{3}_{:d:})_{(3)}(_{:d :}_{:d:})^{T}\]

where \(\) is the Kronecker product. Thus, the intermediate variables include \(\{_{1}_{:d:},_{2}_{:d:},_{3} _{:d:}\}\) and \(\{_{:d:}_{:d:},_{:d:}_{:d:},_{:d :}_{:d:}\}\). Therefore, we should minimize the (power of Frobenius) norms \(\{\|_{1}_{:d:}\|_{F}^{},\|_{2}_{:d:} \|_{F}^{},\|_{3}_{:d:}\|_{F}^{}\}\) and \(\{\|_{:d:}_{:d:}\|_{F}^{}=\|_{:d:}\|_{F}^{ }\|_{:d:}\|_{F}^{},\|_{:d:}\|_{F}^{},\|_{: d:}_{:d:}\|_{F}^{}=\|_{:d:}\|_{F}^{}\|_{:d:}\|_{F}^{ }\}\).

Our Intermediate Variables Regularization (IVR) is defined as a combination of all these norms:

\[()=_{d=1}^{D/P}_{1}(\|_{:d:}\|_ {F}^{}+\|_{:d:}\|_{F}^{}+\|_{:d:}\|_{F}^{})\] \[+_{2}(\|_{:d:}\|_{F}^{}\|_{:d:}\|_{F}^{ }+\|_{:d:}\|_{F}^{}\|_{:d:}\|_{F}^{}+\|_{: d:}\|_{F}^{}\|_{:d:}\|_{F}^{})\] \[+_{3}(\|_{1}_{:d:}\|_{F}^{}+\|_{2}_{:d:}\|_{F}^{}+\|_{3}_{:d:}\|_{F}^{ })\] \[+_{4}(\|_{2}_{:d:}_{3}_{:d: }\|_{F}^{}+\|_{3}_{:d:}_{1}_{:d:}\|_{F}^{ }+\|_{1}_{:d:}_{2}_{:d:}\|_{F}^{})\] (3)

where \(\{_{i}>0|i=1,2,3,4\}\) are the regularization coefficients.

In conclusion, our proposed regularization term is the sum of the norms of variables involved in the different ways of computing the tensor \(\).

We can easily get the weighted version of Eq.(3), in which the regularization term corresponding to the sampled training triplets only (Lacroix et al., 2018; Zhang et al., 2020). For a training triplet \((i,j,k)\), the weighted version of Eq.(3) is as follows:

\[(_{ijk})=_{d=1}^{D/P}_{1}(\|_{:d :}\|_{F}^{}+\|_{j:d:}\|_{F}^{}+\|_{kd:}\|_{F}^{})\] \[+_{2}(\|_{kd:}\|_{F}^{}\|_{j:d:}\|_{F}^{ }+\|_{kd:}\|_{F}^{}\|_{id:}\|_{F}^{}+\|_{ j:d:}\|_{F}^{}\|_{id:}\|_{F}^{})\] \[+_{3}(\|_{1}_{:d:}\|_{F}^{}+\|_{2}_{j:d:}\|_{F}^{}+\|_{3}_{kd:}\|_{F}^{ })\] \[+_{4}(\|_{2}_{j:d:}_{3}_{kd: }\|_{F}^{}+\|_{3}_{kd:}_{1}_{:d:}\|_{F}^{ }+\|_{1}_{id:}_{2}_{j:d:}\|_{F}^{})\] (4)

The computational complexity of Eq.(4) is the same as that of Eq.(1), i.e., \((DP^{2})\), which ensures that our regularization is computationally tractable.

The hyper-parameters \(_{i}\) make IVR scalable. We can easily reduce the number of hyper-parameters by setting some of them zero or equal. The hyper-parameters make us able to achieve a balance between performance and efficiency as shown in Section 4.3. We set \(_{1}=_{3}\) and \(_{2}=_{4}\) for all models to reduce the number of hyper-parameters. You can refer to Appendix C for more details about the setting of hyper-parameters.

We use the same loss function, multiclass log-loss function, as in (Lacroix et al., 2018). For a training triplet \((i,j,k)\), our loss function is

\[(_{ijk})=-_{ijk}+(_{k^{{}^{}}=1}^{||} (_{ijk^{{}^{}}}))+(_{ijk})\]

At test time, we use \(_{i,j,:}\) to rank tail entities for a query \((i,j,?)\).

### Theoretical Analysis

To support the effectiveness of our regularization IVR, we provide a deeper theoretical analysis of its properties. The establishment of the theoretical framework of IVR is inspired by Lemma 1 in Appendix B, which relates the Frobenius norm and the trace norm of a matrix. Lemma 1 shows that the trace norm of a matrix is an upper bound of a function of several Frobenius norms of intermediate variables, which prompts us to establish the relationship between IVR and trace norm. Based on Lemma 1, we prove that IVR serves as an upper bound for the overlapped trace norm of the predicted tensor, which promotes the low nuclear norm of the predicted tensor to regularize the model.

The overlapped trace norm (Kolda and Bader, 2009) for a 3rd-order tensor is defined as:

\[L(;):=\|_{(1)}\|_{*}^{/2}+\|_{(2)}\|_{*}^{ /2}+\|_{(3)}\|_{*}^{/2}\]

where \(\) is the power coefficient in Eq.(3). \(\|_{(1)}\|_{*},\|_{(2)}\|_{*}\) and \(\|_{(3)}\|_{*}\) are the matrix trace norms of \(_{(1)},_{(2)}\) and \(_{(3)}\), respectively, which are the sums of singular values of the respective matrices. The matrix trace norm is widely used as a convex surrogate for matrix rank due to the non-differentiability of matrix rank (Goldfarb and Qin, 2014; Lu et al., 2016; Mu et al., 2014). Thus, \(L(;)\) serves as a surrogate for \((_{(1)})^{/2}+(_{(2)})^{/2} +(_{(3)})^{/2}\), where \((_{(1)}),(_{(2)})\) and \((_{(3)})\) are the matrix ranks of \(_{(1)},_{(2)}\) and \(_{(3)}\), respectively. In KGs, each head entity, each relation and each tail entity uniquely corresponds to a row of \(_{(1)},_{(2)}\) and \(_{(3)}\), respectively. Therefore, \((_{(1)}),(_{(2)})\) and \((_{(3)})\) measure the correlation among the head entities, relations and tail entities, respectively. Entities or relations in KGs are highly correlated. For instance, some relations are mutual inverse relations or one relation may be a composition of another two relations (Zhang et al., 2021). Thus, the overlapped trace norm \(L(;)\) can pose a constraint to the embeddings of entities and relations. Minimizing \(L(;)\) encourage a high correlation among entities and relations, which brings strong regularization and reduces overfitting. We next establish the relationship between our regularization term Eq.(3) and \(L(;)\) by Proposition 1 and Proposition 2. We will prove that Eq.(3) is an upper bound of \(L(;)\).

**Proposition 1**.: _For any \(\), and for any decomposition of \(\), \(=_{d=1}^{D/P}_{1}_{:d:}_{2}_{:d:} _{3}_{:d:}\), we have_

\[2_{4}}L(;)_{d=1}^{D/P}_{1}( \|_{:d:}\|_{F}^{}+\|_{:d:}\|_{F}^{}+\|_{:d:}\|_{ F}^{})\]

\[+_{4}(\|_{2}_{:d:}_{3}_{:d:}\|_{F}^{ }+\|_{3}_{:d:}_{1}_{:d:}\|_{F}^{}+\| _{1}_{:d:}_{2}_{:d:}\|_{F}^{})\] (5)

_If \(}=}_{1}_{1}^{T},}=} {}_{2}_{2}^{T},}=}_{3}_{3}^{T}\) are compact singular value decompositions of \(},},}\)(Bai et al., 2000), respectively, then there exists a decomposition of \(\), \(=_{d=1}^{D/P}_{1}_{:d:}_{2}_{:d:} _{3}_{:d:}\), such that the two sides of Eq.(5) equal._

**Proposition 2**.: _For any \(\), and for any decomposition of \(\), \(=_{d=1}^{D/P}_{1}_{:d:}_{2}_{:d:} _{3}_{:d:}\), we have_

\[2_{3}}L()_{d=1}^{D/P}_{2}(\|_{:d:}\|_{F}^{}\|_{:d:}\|_{F}^{}+\|_{:d:}\|_{F}^{ }\|_{:d:}\|_{F}^{}+\|_{:d:}\|_{F}^{}\|_{:d :}\|_{F}^{})\]

\[+_{3}(\|_{1}_{:d:}\|_{F}^{}+\|_{2} _{:d:}\|_{F}^{}+\|_{3}_{:d:}\|_{F}^{})\] (6)

_And there exists some \(^{{}^{}}\), and for any decomposition of \(^{{}^{}}\), such that the two sides of Eq.(6) can not achieve equality._

Please refer to Appendix B for the proofs. Proposition 1 establishes that the r.h.s. of Eq.(5) provides a tight upper bound for \(2_{4}}L(;)\), while Proposition 2 demonstrates that the r.h.s. of Eq.(4) is an upper bound of \(2_{3}}L(;)\), but this bound is not always tight. Our proposed regularization term, Eq.(3), combines these two upper limits by adding the r.h.s. of Eq.(5) and the r.h.s. of Eq.(6). As a result, minimizing Eq.(3) can effectively minimize \(L(;)\) to regularize the model.

The two sides of Eq.(5) can achieve equality if \(P=D\), meaning that the TDB model is TuckER model (Balazevic et al., 2019). Although \(L(;)\) may not always serve as a tight lower bound of the r.h.s. of Eq.(5) for TDB models other than TuckER model, it remains a common lower bound for all TDB models. To obtain a more tight lower bound, the exact values of \(P\) and \(\) are required. For example, in the case of CP model (Lacroix et al., 2018) (\(P=1\) and \(=1\)), the nuclear 2-norm \(\|\|_{*}\) is a more tight lower bound. The nuclear 2-norm is defined as follows:

\[\|\|_{*}:=\{_{d=1}^{D}\|_{:d1}\|_{F}\|_{:d1}\|_{F}\| _{:d1}\|_{F}|=_{d=1}^{D}_{1}_{:d1}_{2} _{:d1}_{3}_{:d1}\}\]where \(=1\). The following proposition establishes the relationship between \(\|\|_{*}\) and \(L(;2)\):

**Proposition 3**.: _For any \(\), and for any decomposition of \(\), \(=_{d=1}^{D}_{1}_{:d1}_{2}_{:d1}_{ 3}_{:d1}\), and \(=1\), we have_

\[2_{4}}L(;2) 6 _{4}}\|\|_{*}_{d=1}^{D}_{1}(\|_{:d1}\|_{F}^{ 2}+\|_{:d1}\|_{F}^{2}+\|_{:d1}\|_{F}^{2a})\] \[+_{4}(\|_{2}_{:d1}_{3}_{:d1} \|_{F}^{2}+\|_{3}_{:d1}_{1}_{:d1}\|_{F}^{2}+\| _{1}_{:d1}_{2}_{:d1}\|_{F}^{2})\]

Although the r.h.s. of Eq.(6) is not always a tight upper bound for \(2_{3}}L(;)\) like the r.h.s. of Eq.(5), we observe that minimizing the combination of these two bounds, Eq.(3), can lead to better performance. The reason behind this is that the r.h.s. of Eq.(5) is neither an upper bound nor a lower bound of the r.h.s. of Eq.(6) for all \(\). We present Proposition 4 in Appendix B to prove this claim.

## 4 Experiments

We first introduce the experimental settings in Section 4.1 and show the results in Section 4.2. We next conduct ablation studies in Section 4.3. Finally, we verify the reliability of our proposed upper bounds in Section 4.4. Please refer to Appendix C for more experimental details.

### Experimental Settings

DatasetsWe evaluate the models on three KGC datasets, WN18RR (Dettmers et al., 2018), FB15k-237 (Toutanova et al., 2015) and YAGO3-10 (Dettmers et al., 2018).

ModelsWe use CP, ComplEx, SimplE, ANALOGY, QuatE and TuckER as baselines. We denote CP with squared Frobenius norm method (Yang et al., 2014) as CP-F2, CP with N3 method (Lacroix

    &  &  &  \\   & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 \\  CP & 0.438 & 0.416 & 0.485 & 0.332 & 0.244 & 0.507 & 0.567 & 0.495 & 0.696 \\ CP-F2 & 0.449 & 0.420 & 0.506 & 0.331 & 0.243 & 0.507 & 0.570 & 0.499 & 0.699 \\ CP-N3 & 0.469 & 0.432 & 0.541 & 0.355 & 0.261 & 0.542 & 0.575 & 0.504 & 0.703 \\ CP-DURA & 0.471 & 0.433 & 0.545 & 0.364 & 0.269 & 0.554 & 0.577 & 0.504 & 0.704 \\ CP-IVR & **0.478** & **0.437** & **0.554** & **0.365** & **0.270** & **0.555** & **0.579** & **0.507** & **0.708** \\  ComplEx & 0.464 & 0.431 & 0.526 & 0.348 & 0.256 & 0.531 & 0.574 & 0.501 & 0.704 \\ ComplEx-F2 & 0.467 & 0.431 & 0.538 & 0.349 & 0.260 & 0.529 & 0.576 & 0.502 & 0.709 \\ ComplEx-N3 & 0.491 & 0.445 & 0.578 & 0.367 & 0.272 & 0.559 & 0.577 & 0.504 & 0.707 \\ ComplEx-DURA & 0.484 & 0.440 & 0.571 & **0.372** & **0.277** & **0.563** & 0.585 & 0.512 & **0.714** \\ ComplEx-IVR & **0.494** & **0.449** & **0.581** & 0.370 & 0.275 & 0.561 & **0.586** & **0.515** & **0.714** \\  SimplE & 0.443 & 0.421 & 0.488 & 0.337 & 0.248 & 0.514 & 0.565 & 0.491 & 0.696 \\ SimplE-F2 & 0.451 & 0.422 & 0.506 & 0.338 & 0.249 & 0.514 & 0.566 & 0.494 & 0.699 \\ SimplE-IVR & **0.470** & **0.436** & **0.537** & **0.357** & **0.264** & **0.544** & **0.578** & **0.504** & **0.707** \\  ANALOGY & 0.458 & 0.424 & 0.526 & 0.348 & 0.255 & 0.530 & 0.573 & 0.502 & 0.703 \\ ANALOGY-F2 & 0.467 & 0.434 & 0.533 & 0.349 & 0.258 & 0.529 & 0.573 & 0.501 & 0.705 \\ ANALOGY-IVR & **0.482** & **0.439** & **0.568** & **0.367** & **0.272** & **0.558** & **0.582** & **0.509** & **0.713** \\  QuatE & 0.460 & 0.430 & 0.518 & 0.349 & 0.258 & 0.530 & 0.566 & 0.489 & 0.702 \\ QuatE-F2 & 0.468 & 0.435 & 0.534 & 0.349 & 0.259 & 0.529 & 0.566 & 0.489 & 0.705 \\ QuatE-IVR & **0.493** & **0.447** & **0.580** & **0.369** & **0.274** & **0.561** & **0.582** & **0.509** & **0.712** \\  TuckER & 0.446 & 0.423 & 0.490 & 0.321 & 0.233 & 0.498 & 0.551 & 0.476 & 0.689 \\ TuckER-F2 & 0.449 & 0.423 & 0.496 & 0.327 & 0.239 & 0.503 & 0.566 & 0.492 & 0.700 \\ TuckER-IVR & **0.501** & **0.460** & **0.579** & **0.368** & **0.274** & **0.555** & **0.581** & **0.508** & **0.712** \\   

Table 1: Knowledge graph completion results on WN18RR, FB15k-237 and YGAO3-10 datasets.

et al., 2018) as CP-N3, CP with DURA method (Zhang et al., 2020) as CP-DURA and CP with IVR method as CP-IVR. The notations for other models are similar to the notations for CP.

Evaluation MetricsWe use the filtered MRR and Hits@N (H@N) (Bordes et al., 2013) as evaluation metrics and choose the hyper-parameters with the best filtered MRR on the validation set. We run each model three times with different random seeds and report the mean results.

### Results

See Table 1 for the results. For CP and ComplEx, the models that N3 and DURA are suitable, the results show that N3 enhances the models more than F2, and DURA outperforms both F2 and N3, leading to substantial improvements. IVR achieves better performance than DURA on WN18RR dataset and achieves similar performance to DURA on FB15k-237 and YAGO3-10 dataset. For SimplE, ANALOGY, QuatE, and TuckER, the improvement offered by F2 is minimal, while IVR significantly boosts model performance. In summary, these results demonstrate the effectiveness and generality of IVR.

### Ablation Studies

We conduct ablation studies to examine the effectiveness of the upper bounds. Our notations for the models are as follows: the model with upper bound Eq.(5) is denoted as IVR-1, model with upper bound Eq.(6) as IVR-2, and model with upper bound Eq.(3) as IVR.

See Table 2 for the results. We use TuckER as the baseline. IVR with only 1 regularization coefficient, IVR-1, achieves comparable results with vanilla IVR, which shows that IVR can still perform well with fewer hyper-parameters. IVR-1 outperforms IVR-2 due to the tightness of Eq.(5).

### Upper Bounds

We verify that minimizing the upper bounds can effectively minimize \(L(;)\). \(L(;)\) can measure the correlation of \(\). Lower values of \(L(X;)\) encourage higher correlations among entities and relations, and thus bring a strong constraint for regularization. Upon training the models, we compute \(L(;)\). As computing \(L(;)\) for large KGs is impractical, we conduct experiments on a small KG dataset, Kinship (Kok and Domingos, 2007), which consists of 104 entities and 25 relations. We use TuckER as the baseline and compare it against IVR-1 (Eq.(5)), IVR-2 (Eq.(6)), and IVR (Eq.(3)).

See Table 3 for the results. Our results demonstrate that the upper bounds are effective in minimizing \(L(;)\). All three upper bounds can lead to a decrease of \(L(;)\), achieving better performance (Table 2) by more effective regularization. The \(L(;)\) of IVR-1 is smaller than that of IVR-2

    &  &  &  \\   & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 \\  TuckER & 0.446 & 0.423 & 0.490 & 0.321 & 0.233 & 0.498 & 0.551 & 0.476 & 0.689 \\ TuckER-IVR-1 & 0.497 & 0.455 & 0.578 & 0.366 & 0.272 & 0.553 & 0.576 & 0.501 & 0.709 \\ TuckER-IVR-2 & 0.459 & 0.423 & 0.518 & 0.336 & 0.241 & 0.518 & 0.568 & 0.493 & 0.700 \\ TuckER-IVR & **0.501** & **0.460** & **0.579** & **0.368** & **0.274** & **0.555** & **0.581** & **0.508** & **0.712** \\   

Table 2: The results on WN18RR and FB15k-237 datasets with different upper bounds.

    & \(\|_{(1)}\|_{*}\) & \(\|_{(2)}\|_{*}\) & \(\|_{(3)}\|_{*}\) & \(L()\) \\  TuckER & 10,719 & 8,713 & 11,354 & 30,786 \\ TuckER-IVR-1 & 5,711 & 5,021 & 6,271 & 17,003 \\ TuckER-IVR-2 & 6,145 & 5,441 & 6,744 & 18,330 \\ TuckER-IVR & **3,538** & **3,511** & **3,988** & **11,037** \\   

Table 3: The results on Kinship dataset with different upper bounds.

because the upper bound in Eq.(5) is tight. IVR, which combines IVR-1 and IVR-2, produces the most reduction of \(L(;)\). This finding suggests that combining the two upper bounds can be more effective. Overall, our experimental results confirm the reliability of our theoretical analysis.

## 5 Conclusion

In this paper, we undertake an analysis of TDB models in KGC. We first offer a summary of TDB models and derive a general form that facilitates further analysis. TDB models often suffer from overfitting, and thus, we propose a regularization based on our derived general form. It is applicable to most TDB models and improve the model performance. We further propose a theoretical analysis to support our regularization and experimentally validate our theoretical analysis. Our regularization is limited to TDB models, hoping that more regularization will be proposed in other types of models, such as translation-based models and neural networks models. We also intend to explore how to apply our regularization to other fields, such as tensor completion (Song et al., 2019).