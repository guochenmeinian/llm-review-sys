# Auto-Enhance: Towards a Meta-Benchmark to Evaluate AI Agents' Ability to Improve Other Agents

Samuel F. Brown

Independent

&Basil Labib

Impact Academy

&Codruta Lugoj

Independent

&Sai Sasank Y.

Impact Academy

Correspondence to: autoenhance@sambrown.eu

###### Abstract

LLM agents are rapidly improving at benchmarks designed to measure software development and reasoning ability. As the creation and improvement of such systems is itself a software development task, we are interested in this specific subset of ability. We describe the first steps towards a "meta-benchmark", in which a "top-level" agent aims to increase the performance of "reference agents" at tasks taken from a range of other benchmarks in the literature. We show that agents are able to alter other systems to a) better withstand prompt injection, b) fix non-trivial bugs in scaffolds, c) compare the performance of various LLMs at "operating" reference agents' scaffolds, and d) make progress towards model-editing tasks such as unlearning. We consider this a first step towards a broad and extensible meta-benchmark, which would allow us to measure AI self-improvement abilities.

## 1 Introduction

AI agents built using large language models (LLMs) are able to tackle a variety of problems with growing autonomy and success, from resolving real Github issues  to writing machine learning research papers end-to-end . In line with these trends towards more agency and capability, there are growing concerns over the risks these systems pose at technical and societal levels , and calls for evaluations that can keep us informed as these risks change . In particular, as AI systems are used to improve other AI systems , we may expect to see ever faster progress, associated risks changing ever faster, and visibility of capabilities relevant to autonomous replication and adaptation (ARA)  becoming ever more important for AI safety policy.

Making good proxies to measure a property of interest is challenging [3; 11]. Current attempts to measure AI performance often do this in abstracted ways  or in artificial environments [24; 18]. Benchmarks which focus on programming ability [4; 10] do not focus on self-improvement behaviour, and work which focuses on self-improvement [9; 15; 8] does not focus on rigorous, extensible benchmarking.

In this work we take the first steps towards creating an extensible meta-benchmark that integrates 'component' benchmarks from the literature to evaluate agents at real-world, consequential tasks. Our tasks evaluate the capabilities of "top-level" agents (TLAs) to improve other "reference" agents (RAs) at: increasing prompt injection resiliency on the CyberSecEval2 dataset , implementing unlearning of dangerous knowledge (WMDP) , fixing or improving a state-of-the-art agent for ML experimentation (MLAgentBench)  and solving real Github issues (SWE-bench) .

Our contributions are as follows: we create families of 'agent-enhancement' tasks spanning a range of difficulties, using four component benchmarks, and we report the performance of ReAct-based agents  on these tasks. 1Background and Related Work

LLM-based agents [11, SS1.1] use LLMs as their primary reasoning component: an LLM is 'wrapped' in a scaffold to give it access to various tools (for example, web search, terminal access, etc.) and also to structure thought, guide reflection and self-critique, allow context-management, and other abilities which the LLM may leverage to achieve long-term goals in complex environments[23; 22].

Many benchmarks aim to evaluate the abilities of AIs and agents. Some are general-purpose, e.g. for reasoning  or for general agentic tasks . Others are more focused: on single-completion generation of code, on agentic resolution of GitHub issues , on ML engineering tasks , and many others. While the field is rich (leading to its share of criticism [11; 20]) we do not know of any evaluations focused specifically on the ability of agents to improve other agents.

Using AI agents to perform scientific discovery is also a live area of research, with agents autonomously designing and performing experiments , and automatedly designing other autonomous systems . However, neither of these papers focus on creating general, extensible and standardised measurements of self-improvement capabilities, which is what we aim to work towards here.

## 3 Problem Setting

Top-Level Agent (TLA) and Reference Agent (RA)A TLA is tasked with improving the performance of an RA on a reference task. We can measure how the performance of the RA changes as it is modified by the TLA. The RA may be stronger, weaker, or as capable as the TLA. In this work, we operationalise the concept of 'agent' as an LLM combined with scaffolding code (e.g. ReAct ).

Reference Tasks and Enhancement TasksA reference task is one on which we evaluate the capability of the RA, e.g. "Close an open issue on a Github repository". An enhancement task given to the TLA requires it to improve the performance of the RA on the reference task, e.g. "Improve the RA to make it more capable of closing Github issues". We measure the ability of the TLA to improve the RA by measuring the change in performance of the RA on the reference task.

Component BenchmarkTo accurately gauge the ability of agents to improve other agents, we include reference tasks from several consequential domains: software engineering , cybersecurity knowledge , automated ML experimentation , and proxies for WMDP knowledge . We aim to extend our meta-benchmark to more fully incorporate component benchmarks.

## 4 Method

### Tasks and Agents

We use the METR Task Standard  to define our Tasks and agents in such a way that the work to integrate a new task or agent is linear, rather than scaling unfavourably with each task/agent pairing. Details on containerisation and auxiliary VMs, and on QA testing, are discussed in Appendix B.

All our top-level agents were based on the LangChain  implementation of the ReAct  LLM agent, originally taken from METR's example agents . We used GPT-4o to operate our TLA throughout. Minor edits were made to the tools available to the agents, and to descriptions which the agent received about its own tools, see Appendix C for details. The reference agent varies per task.

### SWE-bench Task: Comparing different core models' ability to operate a scaffold

One straightforward way of improving a scaffolded-agent's performance is to select or improve the "core" LLM model which "operates" the scaffold.

In SWE-bench , an agent is given a snapshot of a popular Github repository which fails certain tests, and is tasked with fixing the code such that the tests pass.

We build Tasks in which the TLA is tasked with assessing which of two candidate LLMs (GPT-3.5 and GPT-4o) performs better on SWE-bench when operating the SWE-agent scaffold . In the easy variant, the instructions provide guidance around how to run SWE-agent and SWE-bench. Inthe hard variant, the agent must read documentation to learn how to proceed. Instruction transcripts are provided in Appendix H.2.

Since the entire SWE-bench dataset is large and expensive to run, we restrict this experiment to 8 trajectories, 4 of which we expect to be solvable by SWE-agent/GPT-4o. The flags to ensure this filtering are the only "hand-holding" provided to the agent in the hard variant.

Milestones were manually defined, such as "successfully ran SWE-agent using model A" and "successfully analysed the output of the SWE-bench evaluation for model B's attempted bugfixes". This is particularly important for this task, since an outcome-based task would only measure which of two models was judged to be superior, and so even a coin-flip would expect to see some success at pass@5. Milestones instead assess the process, rather than merely the output.

### WDMP Task: Unlearning WMDP-cyber

Weapons of Mass Destruction Proxy (WMDP) benchmark  evaluates models on proxy hazardous knowledge. The reference task involves the Llama-3-8B model  (RA) answering the questions from the WMDP and MMLU  datasets. In the enhancement tasks, the TLA performs unlearning of cybersecurity knowledge on the RA using the RMU algorithm . We design two variants of this task: Unlearning-Easy and Unlearning-Hard. We use lm-eval-harness  to evaluate on datasets.

In the Easy variant, the enhancement task is to fix the bug in a given unlearn.py script (specifically in the run_rmu method). The bug involves an incorrect loss function, where wrong activations are used to calculate forget_loss.

In the Hard variant, the TLA must perform unlearning on the Llama-3-8B model to reduce cybersecurity knowledge while retaining general knowledge like MMLU.  The TLA is instructed to implement unlearn.py from scratch, perform evaluation, and iterate if necessary.

In both cases, we assign a numerical score to the TLA based on 1) reduction in WMDP-cyber accuracy, and 2) retention in MMLU accuracy. We also manually measure progress using a set of milestones. See appendix H.1 for complete task instructions.

### CyberSecEvals2 Task: Increasing resiliency against prompt-injection attacks

The TLA is tasked with reducing the success rate of prompt injection (PI) attacks on the RA based on the CyberSecEvals2  prompt injection dataset. Good performance on this task indicates that the TLA is capable of long-term planning, experimentation, and iterative improvement of the reference agent. The dataset is split into training and holdout sets. The training set allows the TLA to assess the impact of its changes on the RA. After completing the task, the holdout set evaluates the modified RA, and the improvement in the percentage of unsuccessful prompt injection attacks, compared to the baseline, measures the TLA's performance.

Prompt Injection datasetThe evaluation works by recording the response of the RA on each prompt from the training set (or holdout set, during testing) and using a secondary judge LLM to decide whether the attack succeeded. The dataset contains PI attacks from 15 different categories. To minimize overfitting, the split is performed to include at least one random example attack from each category, and then the remaining set is constructed by randomly sampling from the rest of the dataset.

VariantsWe defined four variants of the prompt injection defence task - easy. medium, hard, and very hard. We made these variants based on the information included in the instruction prompt given to the TLA. Harder variants have fewer hints included in the instruction prompt. Easy task have a cutoff of 30 steps and increase gradually to 100 steps for very hard task.

We used GPT-3.5-turbo as the RA LLM and the judge LLM. The ReAct agent had access to three tools: RunPython, RunBash, and ReadFile. Each request to OpenAI was restricted to 512 tokens of output and the temperature was kept at 0 in all experiments.

### MLAgentBench Tasks: scaffold improvements

MLAgentBench  is a set of tasks that evaluate LLM agents on ML experimentation tasks. We build tasks based on the CIFAR10 subtask, where the MLAgentBench results show that their "researchagent" achieves small, non-zero improvements over the baseline. See Appendix H.3 for instruction transcripts.

The MLAB-Easy variant provides "hand-holding", to gauge an agent's capability to fix bugs in another agent's scaffolding. We introduce two scaffolding errors in the research agent: i) the main loop exits prematurely, and ii) 'Action' is missing from the list of valid responses. The errors are at runtime, meaning the RA appears to run properly to the TLA and valid logs are still produced, requiring the TLA to understand the underlying structure of the scaffold. The TLA has access to the RA's code, receives instructions about which script contains the two errors, and is given a plan to follow to complete the task. Agents are scored based on the research agent script being error-free, i.e. the performance returning to baseline.

In the MLAB-Hard variant, the TLA must improve the scaffold of the MLAgentBench research agent (introduced in the original work) to improve the accuracy on CIFAR10 from an average of 9.2% to at least 15% over the baseline. Agents are scored on the final test accuracy on CIFAR10, with improvements over 15% being considered a success. The MLAB-Hard task essentially requires the TLA to improve on state-of-the-art research and therefore we do not expect agents to score highly.

For both tasks we also track agents' progress on subjective milestones.

## 5 Results

In general, we measure pass@5 for each TLA (ReAct and its variants, operated by GPT-4o), sometimes stopping early in the case of success. The agents (see Appendix C) are stochastic and fragile; experimentally, we found that pass@5 captured most of the variance we saw, with larger k providing diminishing returns for increasing expense. Still, we saw agents succeeding at tasks before we began rigorously recording results, which we then could not replicate, and so expect that a much larger \(k\), while more expensive than we were able to run, would capture success not seen at pass@5.

### SWE-bench Task: Comparing different core models' ability to operate a scaffold

The stock ReAct agent sometimes successfully complete all steps (see Table 2), running SWE-agent on the subset of tasks, scoring the trajectories using SWE-bench, summarising the results and correctly interpreting the results to determine the better-performing model. The modified ReAct+SSH-tool agent also manages to find success on the easy variant, however performance is still very fragile.

Agents did not make significant progress on the hard variant, often hallucinating flags, commands, or files. See Appendix D.1 for more discussion on agent failure modes.

Figure 1: Progress against milestones reached by ReAct agents on variants of four enhancement tasks from our benchmark. Each point represents a separate run; WMDP-easy has three points due to early-stopping at pass@5. All TLA’s are baseline ReAct with tools for Python and shell. Additionally, the Prompt-Injection TLA had access to a file-reading tool. For SWE and WMDP, we also trialled an agent with an SSH tool, which had broadly similar performance.

### Unlearning WMDP

Unlearning-EasyThe stock ReAct agent correctly fixes the bug in the calculation of forget_loss function. The fix is verified by running the script and checking the accuracies on MMLU and WMDP-cyber. Refer to appendix E for details on how the agent fixes the bug and table 4 for milestones.

Unlearning-HardThe ReAct agent driven by GPT-4o writes a somewhat buggy unlearn.py script 3 out of 5 times. See table 3 for various milestones reached in this task. In each run, the agent starts with a plan and completes basic operations like activating virtual environment, checking installed packages, creating initial unlearn.py script (sometimes without the RMU algorithm).

The unlearn.py implementation contains code for loading datasets and the model, running RMU, saving the model, logging and basic error handling. However, the agent uses a non-existent column of the datasets and uses activations from the last layer, which won't work and needs "hyperparameterization". See appendix F for the complete implementation written by the agent.

Also, the agent encounters challenges in dealing with overly long command outputs and syntax errors when writing to the scripts using bash. These issues ultimately result in the agent exhausting its budget without successfully completing the unlearning-hard task. See D.2 for a list of failure modes.

### MLAgentBench Tasks

On MLAB-Easy, the ReAct agent never fixes both errors in the script, but manages to fix either one in 3/5 runs. When given a plan to follow, the ReAct agent is generally capable of following it. In all instances the agent navigates to the right directory, runs the research agent script to obtain a baseline and inspects the logs.

Upon inspecting the research agent script, three of the agents immediately observe the main loop exits prematurely or add the action to the valid entries list. In all partial success runs, after fixing one of the bugs, the agent runs the evaluation script and gets back an environment error about a missing submission file. Rather than reason why the submission file is never created (i.e. the RA does not take any actions and therefore does not write a submission file), all agents naively follow the error and try to make sure the training script is called correctly, exhausting their budget on this unproductive plan.

The two agents that fail to fix any error display a common failure mode: they fail to initially see any improvement either by checking the logs or the research agent script and exhaust their actions searching the codebase. Since some of the runs lead to partial success but then fail in avoidable ways, we check whether new agent instances are able to find both errors when given the logs of one of the partially successful agents. We find that an agent fixes both errors within 25 steps with this setup.

On MLAB-Hard, the agents make little or no progress. Agents run into issues concerning incorrectly formatted action inputs, get stuck in loops calling the same action or follow bad plans. On rare occasions the agents propose a plan for improvement, but these are rarely noteworthy improvements.

### Increasing resiliency against prompt-injection attacks

We instructed the ReAct agent to improve the RA to get more than 5% improvement on the holdout set. Despite high variance of the outcomes of our experiments, we observed consistent improvement on Easy and Medium variants (see Table 1) when choosing certain hyperparameters. For further discussion, see G.

 
**Variant** & **Baseline** & 
 **Enhanced** \\ **Agent** \\  & **Improvement** \\  Easy & 83\% (25/30) & 93\% (28/30) & +10\% (+3/30) \\ Medium & 73\% (22/30) & 90\% (27/30) & +17\% (+5/30) \\ Hard & 77\% (23/30) & 80\% (24/30) & +3\% (+1/30) \\ Very Hard & 77\% (23/30) & 80\% (24/30) & +3\% (+1/30) \\  

Table 1: Improved resilience of the enhanced RA on the Prompt Injection dataset of 30 prompts.

Discussion

### Limitations

Minimal elicitationOur minimal efforts to elicit peak performance from our agents, targeted at particularly common failure modes (see Appendix D), often met qualitative improvement. We expect that further elicitation work would generate significant gains. Many LLM agent scaffolds exist [21; 22; 23], covering orders of magnitude of capability . Future work testing such agents would give indications of relevant capabilities of leading agents, and also guide difficulty-setting of new tasks.

Manually-defined milestones are arbitraryWe score our tasks somewhat arbitrarily, based on manually-defined milestones. Success on one task family may not be as difficult as success on another. Future work to expand the library of included tasks, and ideally to incorporate benchmarks wholesale, would allow results to become more granular, less arbitrary, and more robust and likely to generalise. Incorporating component benchmarks wholesale would also allow this approach to surpass the current focus on scaffolded LLM agents, and to be used to investigate agents' self-improvement capabilities more generally.

### Future Work

Improved extensibilityAn ideal version of this benchmark has a clear onboarding process for additional Component Benchmarks, such that developers of other benchmarks can easily include their work as a novel component of this meta-benchmark.

More granular range of task difficultiesThis work presents only two levels of difficulty: easy and hard. The easiest task variants, at which agents reliably succeed, approximate general bug-fixing (much like tasks in existing benchmarks ) rather than significant ARA capabilities. A more mature meta-benchmark would smooth the transition between easy and hard task variants, presenting many more tasks in the pursuit of a smooth gradient of TLA scores to aid differentiation.

Red linesOf particular interest to Autonomous Replication and Adaptation (ARA)  is self-improvement. Tasks in which the TLA attempts to improve the performance of copies of itself (i.e. the RA begins as a copy of the TLA) are of particular interest here.

Rare event analysisLLM agents are highly stochastic, and it is very common for them to fail during any particular run. However, we often find to be making meaningful, if slow, progress even at hard variants. When these tasks are safety-relevant, it is worthwhile establishing the success-rate, to be able to quantify risk. Since LLM- agent runs can be quite expensive to run, it would be worthwhile finding efficient ways of restarting agent runs which are making progress, as in 5.3, for example to construct a total success rate from the product of partial milestone success rates .

## 7 Conclusion

In this paper, we present the first steps toward a meta-benchmark for evaluating how LLM agents can improve other LLM agents across a range of simple and complex real-world tasks. Our evaluation covers areas including software engineering, knowledge unlearning, resiliency against prompt injection attacks, and improvements in ML experimentation. We evaluated a ReAct agent on these tasks, observing notable progress in some areas while identifying key failure modes that hinder task performance. These insights provide a foundation for future work in addressing these limitations.

Future research could focus on making the meta-benchmark more extensible, allowing for easier integration of new benchmarks and tasks, while expanding the range and variety of evaluated tasks. Additionally, future efforts could evaluate improved agents that handle failure modes more effectively, ensuring task-relevant performance.

In summary, our work marks a significant step toward understanding and evaluating agents' self-improvement capabilities, laying the groundwork for more rigorous and practical model evaluations.

Social Impact Statement

In this paper, we take a step towards developing a novel meta-benchmark for measuring the ability of AI agents to improve other agents. A high score on the benchmark by an agent would signal that the agent is capable of strategising over the long-term and autonomously enhancing other agents to solve consequential tasks like software engineering, ML experimentation, etc.

The enhancement tasks should be run inside a sandboxed environment as they often involve the agent performing potentially destructive actions like running arbitrary programs or making internet requests. Even though humans monitored the agent's actions during the runs, humans were not 'in the loop'.

Researchers may use this work as a starting point towards realising an AutoEnhance meta-benchmark with a more diverse set of component benchmarks. This benchmark could be used by AI researchers, policy makers, and government agencies to measure the capabilities which may lead to autonomous replication and adaptation capabilities in LLM-based agents.

In the long-run, the AutoEnhance benchmark will be a strong candidate to be included in any evaluation suite used to benchmark any frontier agent during development or pre-deployment.

Future work could include adding more component benchmarks to this meta-benchmark, creating an interface for seamlessly running any agent on the benchmark, as well as creating a leaderboard of baseline performance of top agents on the benchmark. The ability of an agent to improve another agent may motivate research into the theory of mind and self-awareness of LLM agents.

## Contributions

All authors contributed equally to the writing of this paper.

Samuel designed and led the project, and provided mentorship and direction throughout.

Each author led the research into individual component benchmarks: Basil on CyberSecEval 2, Codruta on MLAgentBench, Sai on WMDP, and Samuel on SWE-bench.