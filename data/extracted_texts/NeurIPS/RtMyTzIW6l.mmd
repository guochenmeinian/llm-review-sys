# SymILO: A Symmetry-Aware Learning Framework

for Integer Linear Optimization

Qian Chen

Tianjian Zhang

Linzin Yang

Qingyu Han

Shenzhen Research Institute of Big Data, China

Akang Wang

Ruoyu Sun

Xiaodong Luo

Tsung-Hui Chang

###### Abstract

Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. Recently, machine learning techniques have been utilized to solve ILPs. A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function. We conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods---achieving \(50.3\%\), \(66.5\%\), and \(45.4\%\) average improvements, respectively.

## 1 Introduction

Integer linear programs (ILPs) are optimization problems with integer variables and a linear objective, and have a wide range of practical uses in various fields, such as production planning (Pochet & Wolsey, 2006; Chen, 2010), resource allocation (Liu & Fan, 2018; Watson & Woodruff, 2011), and transportation management (Luathep et al., 2011; Schobel, 2001). An important property that often arises in ILPs is _symmetry_(Margot, 2003), which refers to a situation where permuting variables does not change the structure of an ILP.

Recently, there emerges many approaches equipping machine learning methods, supervised learning in particular, to help efficient solution identification for ILPs (Zhang et al., 2023). Among these approaches, an important category derived from the idea of predicting the optimal solution has demonstrated significant improvements (Han et al., 2023; Ding et al., 2020; Khalil et al., 2022; Nair et al., 2020). In this paper, we consider a classic supervised learning task that aims to train an ML model to predict an optimal solution for an ILP. Specifically, given a training dataset \(=\{(s_{i},y_{i})\}_{i=1}^{N}\) with \(y_{i}\) denoting an optimal solution to instance \(s_{i}\), we hope to train a neural networkmodel \(f_{}()\) to approximate the mapping from ILP instances to their optimal solutions, via minimizing the empirical risk defined on \(f_{}(s_{i})\) and \(y_{i}\).

However, for an ILP \(s_{i}\) with symmetry, there exist multiple optimal solutions including \(y_{i}\) and its symmetric counterparts, any of which has an equal probability of being returned as a label. Training neural networks without taking symmetry into account is basically learning a model supervised by random outputs, leading to prediction models of inferior performance.

To address this issue, we propose to leverage the symmetry of ILPs to improve the model performance of predicting an optimal solution. Specifically, given input \(s_{i}\), we define a new empirical risk using \(f_{}(s_{i})\) and \(_{i}(y_{i})\), where \(_{i}()\) denotes the operation of permuting elements in \(y_{i}\) into its symmetric counterpart. Along with ML model parameters, the permutation operators will also be optimized during training. To achieve this, we further develop a computationally affordable algorithm that alternates between optimization of model parameters and optimization of permutation operation. The distinct contributions of our work can be summarized as follows.

* We propose a symmetry-aware framework (called SymILO) that introduces permutation operators as extra optimization variables to the classic training procedure.
* We devise an alternating algorithm to solve the newly proposed problem, with a specific focus on updating the permutation operator for different symmetries.
* We conduct comprehensive numerical studies on four typical benchmark datasets involving symmetries, and the results show that our proposed approach significantly outperforms existing methods.

## 2 Related works

Previous works on identifying high-quality solutions to ILPs via machine learning techniques mainly focus on reducing problem sizes. For example, Ding et al. (2020) propose to identify and predict a subset of decision variables that stay unchanged within the collected solutions. Li & Wu (2022) formulate MILPs as Markov decision processes and learn to reduce problem sizes via early-fixing.

It is noteworthy that the emergence of GNNs has had a significant impact on solving ILPs. Gasse et al. (2019) are the first to propose a bipartite-graph representation of ILPs and pass it to GNNs. Nair et al. (2020) adopt the same representation scheme and train GNNs to predict the conditional distribution of solutions, from which they further sample solutions. Rather than directly fixing variables, Han et al. (2023) conduct search algorithms in a neighborhood centered around an initial point generated from the predicted distribution. Other works based on GNNs (Sonnerat et al., 2022; Lin et al., 2019; Khalil et al., 2022; Wu et al., 2021) also illustrate great potential in improving the solving efficiency.

Limitations of the existing GNN-based approaches are also noticed. Nair et al. (2020); Han et al. (2023) try to address the multiple solution problem by learning the conditional distribution. Chen et al. (2022) introduce random features into the bipartite graph representation to differentiate variable nodes involving symmetries.

However, none of the existing learning-based approaches explicitly leverage the inherent symmetries in ILPs to achieve improvements. In contrast, works from mathematical optimization perspectives suggest that symmetry-handling algorithms exhibit great abilities in solving symmetry-involving ILPs (Pfetsch & Rehn, 2019). To name a few, such algorithms include orbital fixing (Ostrowski et al., 2011), tree pruning (Margot, 2002), and lexicographical ordering (Kaibel & Pfetsch, 2008).

## 3 Background and preliminaries

### ILPs

An _integer linear program_ (ILP) has a formulation as follows:

\[_{x}\ \{c^{}x|Ax b,x^{n}\}\] (1)

where \(x^{n}\) are integer decision variables, and \(c^{n},A^{m n},b^{m}\) are given coefficients.

### Symmetry Group

Symmetry of ILPs is typically represented by groups. We start with some basic notations and most of which follow Margot (2009). Denoting the index set by \(I^{n}=\{1,2,,n\}\), a _permutation_ on \(I^{n}\) is a bijective (one-to-one and onto) mapping \(:I^{n}I^{n}\). For example, an identity permutation maps the index set to itself as \(\{(i)=i\}_{i=1}^{n}\) and a cyclic permutation has rotational mapping rules \(\{(i)=i+1\}_{i=1}^{n-1}\) and \((n)=1\). Schematic diagrams of these two permutations and other ones are shown in Figure 1. For brevity, we abuse the notation \(\) a little bit and denote the permutation acting on a vector \(y^{n}\) by rearranging its coordinates, namely \((y)=y_{(1)},y_{(2)},,y_{(n)}^{}\). Let \(Q\) be the set of all feasible solutions of (1) and \(S_{n}\) the set of all permutations on \(I^{n}\). Note that \(S_{n}\) is referred to as the _symmetric group_, which should not be confused with the _symmetry group_ discussed as follows.

**Definition 3.1**.: A _symmetry group_ of (1) is defined as the set of all permutations \(\) that map \(Q\) onto itself, such that each feasible solution is mapped to another feasible solution with the same objective value, i.e.,

\[G=\{ S_{n}:c^{}=c^{}()()  Q,\; Q\}.\] (2)

Next, we will delve into three commonly encountered symmetries, accompanied by typical example problems. Since not all variables in an ILP involve symmetry, we use \(q n\) to indicate the size of the symmetry group.

Symmetric groupThe _symmetric group_, denoted by \(S_{q}\), is the group that consists of all permutations (\(q!\) in total) on \(I^{q}\). Problems with this kind of symmetry include bin packing (Johnson, 1974) and optimal job scheduling Graham et al. (1979), etc. An example is illustrated in Appendix B.0.1.

Cyclic and dihedral groupsAs its name suggests, cyclic symmetry allows elements to be permuted to their right neighbors, cycling the right-most variables back to the left, e.g., a cyclic (or rotational) permutation \(\) in Figure 1 (c). The elements of a _cyclic group_\(C_{q}\) are powers of \(\), and \(|C_{q}|=q\). Problems with cyclic group often have characteristics of rotations or cycles, e.g., periodic event scheduling problem (Serafini & Ukovich, 1989).

Compared to the cyclic group, a _dihedral group_ (denoted as \(D_{q}\)) additionally includes reflective permutations, which is illustrated in Figure 1 (d). Consequently, \(D_{q}\) comprises a total of \(2q\) distinct permutations. A typical problem with such symmetry is the circular (or modular) golomb ruler problem (see Appendix B.0.2).

### Classic supervised learning for solution prediction

A classic solution prediction task based on supervised learning is formulated as follows. Let \(\) be the space of ILP instances and \(\) be the label (i.e., optimal solution) space. A model function \(f_{}:\) parameterized by \(\) is used to learn a mapping from instances to optimal solutions. Let \((S,Y)\) be a distribution over \(\). The performance of the model function is measured by a criterion called _true risk_ : \(R(f_{}):=E_{(S,Y)}[(f_{}(s),y)]\), where \(:^{+}\) is a given loss function, e.g., mean squared error or cross-entropy loss. An intuitive way to improve the model performance is to minimize the _true risk_. However, one cannot access all data from distribution \((S,Y)\), which makes it impossible to calculate the true risk. Practically, one can obtain a set of (instance, solution) pairs called training data \(=\{(s_{i},y_{i})\}_{i=1}^{N}( )^{N}\) sampled from \((S,Y)\), based on which define the _empirical risk_ as

\[r(f_{};):=_{i=1}^{N}(f_{}(s_{i }),y_{i}).\] (3)

Figure 1: Permutation examples with directed edges denoting mapping rules.

By minimizing the _empirical risk_, i.e., \(_{}r\), one aims to approximate the minimization of the _true risk_, under the assumption that the training data is a representative sample of the overall data distribution.

## 4 Methodology

### Reformulation of the learning task

In Section 3.3, we introduce a classic supervised learning task for general ILPs, which aims at learning a mapping \(f_{}\) from instances to optimal solutions. In this task, a dataset \(=\{(s_{i},y_{i})\}\) is given, and the mapping \(f_{}\) is learned by minimizing (3) with \(\) as decisions. However, for ILPs with symmetry, an ILP instance has multiple solutions (let \(Y_{i}\) be the set of optimal solutions of \(i\)-th instance). As a consequence, the labels in this task have multiple choices, thus datasets choosing different optimal solutions as labels \(\{^{}=\{(s_{i},y_{i}^{})\}_{i=1}^{N},\;y_{i}^{ } Y_{i}\}\) are all valid for the learning task. Empirically, we observe that different \(D^{}\) can lead to distinct performance, which motivates us to consider the selection of labels for ILPs with symmetry.

We reformulate the learning task as follows. Firstly, we augment dataset \(\) to dataset \(_{s}=\{(s_{i},y_{i},G_{i})\}\), where \(G_{i}\) is the symmetry group of \(i\)-th instance and \(_{i} G_{i}\). Secondly, we define the _symmetry-aware empirical risk_ as

\[r_{s}(f_{},\{_{i}\}_{i=1}^{N};_{s}):=_{i=1 }^{N}(f_{}(s_{i}),_{i}(y_{i})).\] (4)

Then, the mapping \(f_{}\) is learned by minimizing the symmetry-aware risk as \(_{,}r_{s}\) (both \(\) and \(\) as decisions). In contrast to the original task, the symmetry-aware task uses symmetry information by introducing extra decisions \(\{_{i}\}_{i=1}^{N}\), so as to dynamically selecting proper optimal solutions as labels. There are important differences between the symmetry-aware empirical risk and the classic one:

**Proposition 4.1**.: _Let \(r^{*}\) and \(r_{s}^{*}\) be the global minimal values of \(_{}r\) and \(_{,}r_{s}\), respectively. Then, the following claims hold:_

1. \(r_{s}^{*} r^{*}\)_,_
2. \(r_{s}^{*}<r^{*}\)_, if there exist_ \(i,j\{1,,N\}\)_, such that_ \(s_{i}=s_{j}\) _and_ \(y_{i} y_{j}\)_._

Claim (i) always holds since \(_{}r\) is a special case of \(_{,}r_{s}\) when \(_{1},,_{N}\) are all _identity permutations_. Claim (ii) shows a significant advantage of \(r_{s}\) compared to \(r\). A non-rigorous proof is available in Appendix A.1.

### An alternating minimization algorithm

The minimization of (4) is challenging due to the discrete nature of \(\). Motivated by the well-known block coordinate minimization algorithms (Mangasarian, 1994), we update \(\) and \(\) alternately, i.e.,

\[\{_{i}^{k+1}\}_{i=1}^{N} _{_{i} G_{i}}r_{s}(f_{^{k}},\{_{ i}\}_{i=1}^{N};_{s}),\] (5) \[^{k+1} _{}r_{s}(f_{},\{_{i}^{k +1}\}_{i=1}^{N};_{s}).\] (6)

Such an alternating mechanism divides the minimization of (4) into two sub-problems: a discrete optimization in (5) over sets \(\{G_{i}\}_{i=1}^{N}\) and a classic empirical risk minimization in (6). Repeatedly solving (6) to optimal is unrealistic, thus it is more practical to update \(\) by several gradient steps instead.

The sub-problem in (5) is further specified as shown in Section 4.2.1, according to the symmetry structures in the ILP instances.

We summarize the proposed alternating minimization algorithm in Algorithm 1. In the main loop, \(\{_{i}\}_{i=1}^{N}\) are updated first (line 5), after which an inner loop (lines 6-10) is operated to update \(\) through a gradient-based method \(\), e.g., Adam (Kingma & Ba, 2014). These two updates alternate until a preset maximum number of epochs \(K\) is reached. We finally note that Algorithm 1 can be easily adapted to a mini-batch version, in which the data can be randomly sampled from \(_{s}\).

#### 4.2.1 Optimization over symmetry groups in (5)

In this section, we investigate the concrete formulations of the sub-problem in (5) for the symmetry groups mentioned in Section 3.2 (symmetric group, cyclic and dihedral groups), and devise algorithms to solve them.

Cyclic and dihedral groupsThe cardinality of a cyclic group \(C_{q}\) is \(q\), and it is \(2q\) for a dihedral group \(D_{q}\). For symmetry groups with such reasonably small size, a straightforward and effective way to solve \((5)\) is to evaluate all possible permutations and select the one that yields the minimum \(r_{s}\).

Symmetric groupThe cardinality of a symmetric group is factorially large, \(|S_{q}|=q!\), so it is impractical to traverse all permutations. Since \(_{1},,_{N}\) are not coupled, we can separate them and solve the \(N\) sub-problems individually:

\[_{_{i}}(f_{}(s_{i}),_{i}(y_{j})), i=1, ,N.\] (7)

Without loss of generality, consider an ILP whose variables have a matrix form (e.g., see Appendix B.0.1), denoted by \(X^{p q}(p q<n)\), and a symmetric group \(S_{q}\) acting on its column coordinates. In this case, (7) is equivalent to solve the following binary linear program (BLP),

\[_{P}\;(,XP) P\{0,1\}^{q  q},P^{}=,P=,\] (8)

where \(P\) is a permutation matrix, \(\) is the matrix form of \(f_{}(s_{i})\), and \(\) is an all-one vector. We relax \(P\) to take continuous values between 0 and 1, and get a linear program (LP),

\[_{P}\;(,XP) P^{q q }\,,P^{}=,P=\] (9)

According to Proposition 4.2, one can solve (9), to get the optimal permutations for the original problem in (8). It can be done quite efficiently with the aid of off-the-shelf LP solvers, such as Gurobi Optimization, LLC (2023), CPLEX IBM (2020), etc.

**Proposition 4.2**.: _When \(\) is the squared error or binary cross-entropy loss, the optimal solution to (9) is also an optimal solution to (8). (See the proof in A.2.)_

### An overview of the SymILO framework

In this section, we summarize a novel learning framework (SymILO) that utilizes symmetry for solving ILPs. An overview is depicted in Figure 2, which consists of two parts: the upper row connected by green arrows delineates a graph neural network (GNN)-based workflow, and the lower row connected by red arrows outlines the training process.

Figure 2: An overview of the SymILO framework.

For the GNN-based workflow, an ILP is first converted to a bipartite graph (see appendix C for details), which is then fed to a GNN model \(f_{}\) (see appendix D for details), producing a predicted solution. Notably, the predicted solution is finally used in downstream tasks for refinement. Due to the complexity of solving ILPs, existing methods, such as Nair et al. (2020); Ding et al. (2020); Khalil et al. (2022); Han et al. (2023), often include a post-processing module taking the predicted solution as an initial point to identify higher-quality solutions. Our approach follows this routine and integrates certain downstream techniques. Section 5.1 specifies three downstream tasks.

For the training process, the data used to minimize the symmetry-aware empirical risk \(r_{s}\) include the collected solution \(y_{i}\) and the symmetry group \(G_{i}\) of each instance \(s_{i}\). Both parameters \(\) of the GNN model and permutations \(\{_{i}\}_{i=1}^{N}\) of each solution are optimized via an alternating algorithm mentioned in Algorithm 1. Given a trained model \(f_{^{K}}\), the prediction \(f_{^{K}}(s^{})\) for an unseen instance \(s^{}\) is used to guide the downstream tasks in identifying feasible solutions. Note that \(\{_{i}\}_{i=1}^{N}\) are utilized only in the training phase but not in the inference phase.

## 5 Experimental settings

In this section, the experimental settings are presented. The corresponding source code is available at https://github.com/NetSysOpt/SymILO.

### Downstream tasks and baselines

In our experiments, we pass the predictions of GNN models to three downstream tasks, namely fix and optimize, local branching, and node selection, to identify feasible solutions. For each downstream task, we choose one existing method as a baseline. The downstream tasks and their corresponding baselines (in parentheses) are shown below.

Fix and optimize (ND):"Fix and optimize" refers to a strategy where one first "fix" or set some variables to specific values and then "optimize" the remaining variables to find better solutions. The baseline we choose is "Neural Diving" (**ND**) proposed by Nair et al. (2020), a technique using a graph neural network to generate partial assignments for ILPs, which creates smaller sub-ILPs with the unassigned variables.

Local branching (PS):Local branching is a heuristic method that constructs a linear constraint based on a given initial solution to the original ILP instance. This constraint restrains the search space in a region around the initial solution. It can help guide the optimization process toward better solutions while balancing computational efficiency. Approaches based on this idea include Ding et al. (2020); Han et al. (2023); Chen et al. (2023) and we select the "predict-and-search" (**PS**) framework proposed by Han et al. (2023) as a baseline.

Node selection (MIP-GNN):In branch and bound algorithms, node selection is a process of choosing the proper nodes to explore next. Effective node selection is crucial for the algorithm's success in solving optimization problems. "MIP-GNN" (**MIP-GNN**) proposed by Khalil et al. (2022) uses GNN prediction to guide node selection and warm-starting, and is selected as another baseline.

### Benchmark datasets

We evaluate the proposed framework on four ILP benchmarks with certain symmetry, which consists of (i) two problems with symmetric groups: the item placement problem (IP) and the steel mill slab problem (SMSP), (ii) the periodic event scheduling problem (PESP) with cyclic group, and (iii) a modified variant of PESP (PESPD) which has a dihedral group.

The first benchmark IP is from the NeurIPS ML4CO 2021 competition (Gasse et al., 2022). We use their source code to randomly generate instances with binary variables ranging from \(208\) to \(1050\). Each instance has a symmetric group \(S_{4} S_{10}\). We use \(500\) instances for our experiments, taking \(400\) as the training set and the remaining \(100\) for testing. The SMSP benchmark is from Schaus et al. (2011), and contains \(380\) problem instances. We randomly select \(304\) of them as training data and take the others as testing data. The instances of this benchmark have 22k\(\)24k binary variables and nearly 10k constraints, with each of them having a symmetric group \(S_{111}\). The last two benchmarks are from PESPlib Goerigk (2012), a collection of periodic timetabling problems inspired by real-world railway timetabling settings. Since PESPlib only provides a few instances, which are not sufficient to support neural network training, we randomly perturb the weights of the provided instances to generate more data (see Appendix G.3.1 for details). We respectively generate 500 instances for PESP and PESPD, taking 400 of them as training sets and 100 as testing sets. The symmetry groups of these two datasets are cyclic groups \(C_{5} C_{15}\) and dihedral groups \(D_{5} D_{15}\), respectively. For all training sets, 30% instances are used for validation. The average numbers of variables and constraints, as well as the symmetry groups of each benchmark problem, are summarized in Appendix F.1. Besides, more details about their ILP formulations and corresponding symmetries are supplemented in Appendix G.

These benchmarks only include problem instances. We collect the corresponding solutions using an ILP solver CPLEX (IBM, 2020). However, solving ILP instances even with moderate sizes to optimal is extremely expensive. It is more practical to use high-quality solutions as the labels. Therefore, we run single-thread CPLEX for a time limit of 3,600 seconds and record the best solutions.

### Training settings

All models are trained with a batch size \(16\) for \(50\) epochs. The Adam optimizer with a learning rate of \(0.001\) is used, and other hyperparameters of the optimizer are set to their default values. The model with the smallest loss on the validation set is used for subsequent evaluations. Other training settings, such as the loss function and neural architectures, follow the configurations in Han et al. (2023). More details about the hyper-parameter tuning for the downstream tasks and software resources are shown in Section E.

### Evaluation metrics

To compare the prediction performance of the model trained on \(r\) and \(r_{s}\), we define the _Top-\(m\)% error_ for evaluation. In addition, another criterion _relative primal gap_ is used to evaluate the final performance in identifying feasible solutions in different downstream tasks.

Top-\(m\)% error:We use the distance between a rounded prediction and its nearest equivalent solution as the error. Specifically, given a prediction \(\) and its label \(y\), we define the equivalent solution closest to \(\) as \(=^{}(y)\), where \(^{}=_{}\|-(y)\|\). Then, the Top-\(m\)% error is defined as

\[(m)=_{i M}|(_{i})-_{i}|,\] (10)

where \(M\) is the index set of \(m\)% variables with largest values of \(|(_{j})-_{j}|\). This error measures the minimum distance between the prediction and all solutions equivalent to the label. Compared to naive use of the distance \(_{i M}|(_{i})-y_{i}|\), (10) can more accurately represent how close a prediction is to a feasible solution. Since for the naive distance, when \(()\) equals any equivalent solution \((y) y\), the distance is greater than 0, while that of (10) is 0.

Relative primal gap:We also feed the outputs of the models trained through \(r_{s}\) to the downstream tasks mentioned in Section 5.1 to evaluate the quality of the predictions. All the three downstream approaches incorporate ILP solvers to search for solutions. We run these ILP solvers on a single thread for a maximum of 800 seconds. Since all the problems used in the experiments are NP-hard, identifying optimality is highly time-consuming. Thus the metric used in our experiments is _relative primal gap_

\[()=-c^{}y^{*}|}{|c^{}y^{* }|+},\] (11)

which measures the relative gap in the objective value of a feasible solution \(\) to that of the best-known solution \(y^{*}\), and \(\) is a small positive value to avoid the numerical issue. Additionally, let \(_{r}\) and \(_{r_{s}}\) respectively be the primal gaps of models trained through \(r\) and \(r_{s}\), then an improvement gain of our approach is calculated as \((_{r}-_{r_{s}})/_{r}\).

Numerical results

In this section, we present the comparison results on empirical risk \(r\) and symmetry-aware one \(r_{s}\). In addition, primal gaps of SymILO and baselines on three downstream tasks are reported.

### On empirical risks and Top-\(m\%\) error

We denote training and test risks by \(r^{tr}()=r(;^{tr})\) and \(r^{te}=r(;^{te})\), respectively, and similarly use \(r_{s}^{tr}\) and \(r_{s}^{te}\) for symmetry-aware risk. Let \(f^{(k)}\) and \(f_{s}^{(k)}\) be the best classic model and symmetry-aware model obtained at \(k\)-th epoch by training with \(r^{tr}\) and \(r_{s}^{tr}\), respectively. We plot both the training and test risks versus the number of epochs in Figure 3. As predicted in Proposition 4.1, when algorithms converge, the classic empirical risk \(r^{tr}\) is always greater than symmetry-aware risk \(r_{s}^{tr}\).

As shown in Table 1, the symmetry-aware model \(f_{s}^{(K)}\) always predicts smaller Top-\(m\)% errors in (10) compared to the classic model \(f^{(K)},\) demonstrating the usefulness of proposed empirical symmetry-aware risk in predicting solutions correctly.

Moreover, the time costs of minimizing different empirical risks \(r\) and \(r_{s}\) for a mini batch are shown in Table 2. Here, \(t\) denotes the average time of solving the permutation decisions per instance. The reported times for \(r_{s}\) include the optimization time \(t\). The table illustrates that the alternate training strategy does not significantly increase the training duration, and the optimization step over \(\) is executed efficiently.

### Downstream results

The relative primal gaps of different downstream tasks at different solving time are shown in Figure 4, and the final values at 800 seconds are listed in Table 3. As Figure 4 shows, our proposed empirical risk significantly improves the performance of different downstream tasks over the primal gap in 800 seconds.

Note that the node selection task exhibits modest performance in comparison to other tasks; a possible reason is that it requires runtime interaction to call the callback functions provided by the CPLEX

    & IP &  &  &  \\   & \(f^{(K)}\) & \(f^{(K)}_{s}\) & \(f^{(K)}\) & \(f^{(K)}\) & \(f^{(K)}\) & \(f^{(K)}_{S}\) & \(f^{(K)}\) \\  
10\% & 0.8 \(\)0.8 & **0.4**\(\)0.6 & 0.6\(\)0.7 & **0.0**\(\) 0.0 & 7.5\(\)17 & **0.1**\(\)0.2 & 87.4\(\)41 & **12.7**\(\)4.8 \\
30\% & 3.9 \(\)1.5 & **2.9**\(\)1.3 & 5.3 \(\) 2.6 & **0.1**\(\)0.1 & 4.4\(\)235 & **0.1**\(\)0.5 & 275\(\)73 & **81.3**\(\)24 \\
50\% & 17.0 \(\)2.4 & **5.1**\(\)1.7 & 19.5\(\)5.5 & **0.6**\(\)2.5 & 52.7\(\)35 & **0.3**\(\)0.8 & 422\(\)102 & **223\(\)**17 \\
70\% & 46.5 \(\)2.8 & **36.3**\(\)4.3 & 47.5\(\)9.8 & **17.8**\(\)6.6 & 122\(\)26 & **23**\(\)5.5 & 638\(\)93 & **486\(\)**114 \\
90\% & 82.9 \(\)1.5 & **76.1**\(\)3.0 & 103\(\)15 & **47.0**\(\)9.1 & 1.6\(\)30 & **1212\(\)**23 & 854\(\)69 & **848**\(\)99 \\   

Table 1: Top-\(m\)% errors (\(\)) of model \(f^{(K)}\) and \(f^{(K)}_{s}\) averaged over different datasets.

    & IP & SMSP & PESP & PESPD \\  \(r\) & 5.54 & 69.43 & 14.97 & 16.17 \\ \(r_{s}\) & 6.01 & 71.5 & 15.14 & 16.46 \\ \(t\) & 0.029 & 0.129 & 0.011 & 0.018 \\   

Table 2: Time cost for minimizing different empirical risks (in seconds).

Figure 3: The training and test risks v.s. the number of epochs on four benchmark problems.

Python APIs, which can slow down the whole solving process. However, such a flaw does not affect the demonstration of the effectiveness of our proposed method.

For the primal gap at 800 seconds shown in Table 3, the models trained through \(r_{s}\) significantly improve all downstream tasks. The performance gain of the model trained through \(r_{s}\) is calculated by computing the relative gaps between our approach's gap improvements and that of the baselines. Average gains over the three downstream tasks are 50.3%, 66.5% and 45.4%, respectively. The overall results demonstrate the effectiveness of the proposed empirical risk \(r_{s}\). We also provide the corresponding p-values for the significance of improvements in Appendix F.2.

## 7 Limitations and conclusions

In conclusion, we propose SymILO, a novel symmetry-aware learning framework for enhancing the prediction of solutions for integer linear programs by incorporating symmetry into the training process. Our approach shows significant performance improvements over symmetry-agnostic methods on benchmark datasets. Despite the significant advancements presented in our symmetry-aware learning framework, SymILO, several limitations must be acknowledged. Firstly, while we provide realizations for three commonly encountered symmetry groups--symmetric, cyclic, and dihedral--the framework requires specific formulations for optimizing permutations, which limits its immediate applicability to other symmetry groups not discussed in this work. Secondly, for large-scale problem instances with extensive and complex symmetry groups, the sub-problems involved in optimizing permutations can significantly slow down the training process. Enhancing the computational efficiency of our alternating optimization algorithm for these cases remains a challenge and an area for future research.

Figure 4: Relative primal gaps at different times. Three downstream tasks, i.e., fix-and-optimize, local branching, and node selection, are evaluated with a time limit of 800 seconds. The results of the same downstream task use the same color. In addition, the relative primal gap of the Tuned CPLEX running on a single thread is also reported as the blue dashed line.

    &  &  &  &  \\   & & ND & SymILO & gain(\(\)) & PS & SymILO & gain(\(\)) & MIP-GNN & SymILO & gain(\(\)) \\   IP & 0.188 & 0.201 & **0.124** & 38.4\% & 0.168 & **0.102** & 39.4\% & 0.312 & **0.190** & 39.2\% \\ SMSP & 0.190 & 0.300 & **0.180** & 40.0\% & 0.230 & **0.160** & 30.4\% & 1.180 & **0.740** & 37.3\% \\ PESP & 0.056 & 0.084 & **0.050** & 39.8\% & 0.306 & **0.000** & 100\% & 1.899 & **0.280** & 85.3\% \\ PESPD & 3.194 & 2.389 & **0.404** & 83.1\% & 3.442 & **0.127** & 96.3\% & 3.755 & **3.006** & 20\% \\  Avg. & & & & 50.3\% & & & 66.5\% & & 45.4\% \\   

Table 3: Average relative primal gaps (\(\)) of different downstream tasks at 800 second. The values in this table are averaged over primal gaps of all test data for each benchmark problem. “Tuned CPLEX” is the result of the tuned CPLEX running on a single thread.