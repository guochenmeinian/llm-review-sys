# GTA: A Benchmark for General Tool Agents

Jize Wang\({}^{1,2}\) Zerun Ma\({}^{2}\) Yining Li\({}^{2}\) Songyang Zhang\({}^{2}\)

**Cailian Chen\({}^{1}\) Kai Chen\({}^{2*}\) Xinyi Le\({}^{1*}\)**

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Shanghai AI Laboratory

{jizewang2000, caillianchen, lexinyi}@sjtu.edu.cn

{mazerun, liyining, zhangsongyang, chenkai}@pjlab.org.cn

Corresponding Authors.

###### Abstract

Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs' tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only interactions, failing to effectively reveal the agents' real-world problem-solving abilities. To address this, we propose GTA, a benchmark for **G**eneral **T**ool **A**gents, featuring three main aspects: (i) _Real user queries_: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) _Real deployed tools_: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) _Real multimodal inputs_: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents. Dataset and code are available at [https://github.com/open-compass/GTA](https://github.com/open-compass/GTA).

## 1 Introduction

Integrating tools with large language models (LLMs) has attracted broad research interest as a potential approach towards general AI assistants. Notable works include LangChain , AutoGPT , and ChatGPT Plugins . These systems decompose workflow into two interactive parts: planning and execution, respectively handled by LLM controllers and callable tools. Solving complex real-world tasks requires multiple types of tools, including perception, operation, logic, and creativity, posing great challenges to LLMs' tool-use proficiency. Consequently, evaluating the models' tool-use capabilities for real-world tasks is crucial for enhancing the effectiveness of agent systems.

Despite the progress on benchmarking the tool-use capability of LLMs made by recent works, especially on collecting massive APIs and AI-generated user queries to enable scalable testing, there remain noticeable gaps regarding real-world scenarios, as shown in Table 1. First, AI-generated user queries, limited by the generative model, often result in overly brief or monotonous solutions.

This is unsuitable for evaluating agent systems' reasoning and planning capability, as shown in Table 2. Second, existing tool-use benchmarks mainly focus on text-formed user-agent interaction, lacking assessment of multimodal capabilities, thus falling short of aligning with real-world scenarios effectively. Third, existing tool-use evaluation approaches build up virtual tools. They can only evaluate isolated steps in the tool invocation chains, thus unable to reflect the agents' capability to accomplish complex tasks end-to-end.

To ensure the evaluation closely reflects real-world scenarios, we consider the authenticity of user queries, tools, and interaction modalities. We propose a comprehensive tool-use evaluation with real-world user queries. The primary features of the evaluation are:

1. [leftmargin=*,noitemsep,topsep=0pt]
2. _Real user queries._ The user queries are designed by humans, rather than generated by AI, to reflect real-world tasks accurately. These queries describe tasks with clear objectives, but the tool-use steps are implicit. Thus, the LLM must use reasoning to deduce the suitable tools to address the given tasks. In this way, we avoid the drawbacks of using AI-generated queries in which the tool invocation steps are often explicitly hinted at. Moreover, each query requires multiple steps to resolve, necessitating the model to plan the sequence of tool invocations.
3. _Real deployed tools._ We provide an evaluation platform deployed with tools across various categories, such as perception, operation, logic, and creativity. All tools are executable rather than simulated by text description. A detailed and executable ground truth tool chain is provided for each task, including each tool-use step and the final answer. Each step includes the tool name,

   Method & Real-world* & Real deployed & Multimodal & Human annotated & Execution result \\  & user queries & tools & context inputs & tool chains & evaluation \\  APIBench  & & & & & \\ ToolBench  & & & & & \\ APIBank  & & & & & \\ GAIA  & & & & & \\ m\&m’s  & & & & & \\
**GTA (Ours)** & & & & & & \\   

Table 1: Comparison of benchmarks for the LLM-based agent system. *Real-world means solving the queries is helpful for humans in real life while step-implicit and tool-implicit for LLMs.

Figure 1: Some samples in the GTA benchmark. The user queries are human-designed, step-implicit, tool-implicit, and settled in real-world scenarios. Multimodal context inputs are provided. Solving these queries is helpful for users and complex for a LLM-based tool agent. The agent must use a combination of executable tools in perception, operation, logic, and creativity categories.

argument value, and return value. The detailed tool chains enable a fine-grained evaluation of the actual problem-solving abilities of tool agents.
3. _Real multimodal inputs._ Each query is accompanied by one or two authentic image files, including spatial scenes, webpage screenshots, tables, code snippets, printed/handwritten materials, etc., to serve as the context for the user queries. The LLM is required to solve the problem based on the multimodal context and user queries. This setting closely aligns with the multimodal real-world problem-solving scenarios.

We manually design 229 real-world tasks and corresponding executable tool chains to evaluate mainstream LLMs. We build a platform covering a total of 14 tools across perception, operation, logic, and creation categories. Tools and some data samples are illustrated in Figure 1. We design fine-grained tool evaluation metrics that cover the entire process of tool invocation. Our findings indicate that real-world scenario queries present challenges to existing LLMs, with GPT-4 completing fewer than 50% of the tasks and most LLMs managing less than 25%.

In summary, our contributions are as follows:

* A tool-use benchmark for general tool agents. The user queries are human-designed, step-implicit, and settled in real-world scenarios. Multimodal contextual inputs are provided. Each query has a corresponding executable tool chain to enable a fine-grained tool-use evaluation.
* An evaluation platform equipped with a wide variety of executable tools covering the categories of perception, operation, logic, and creativity. Fine-grained metrics are designed for tool use, unveiling tool-augmented LLMs' reasoning and planning capabilities in real-world scenarios.
* Evaluation and analysis of mainstream large language models. We evaluate the tool-use ability of 16 LLMs in multiple dimensions. Our findings reflect the tool-use bottleneck of existing LLMs in real-world scenarios, providing suggestions for the development path of general tool agents.

## 2 Related Work

**LLM-based agents.** In the pursuit of developing general-purpose agents, there has been considerable focus on integrating LLMs with external tools. These LLM-based agents enable powerful capabilities in environment interaction, decision-making, and task execution. Open-source platforms have been proposed, such as LangChain , AutoGPT , and BabyAGI . Moreover, several efforts have been made to achieve specialized capabilities by integrating specialized tools into LLMs. WebGPT , WebCPM , WebShop  are proposed to enhance the model's web search ability. RestGPT  combines LLM with RESTful APIs to enable web service development. In the visual domain, Visual ChatGPT , MM-ReAct , MLLMtool , and LLaVA-Plus  prompt or finetune LLMs to interact with visual models. In the data analysis domain, DataCopilot  manages and processes massive data autonomously by invoking data analysis tools. HuggingGPT , ModelScopeAgent  build agent systems using LLMs integrated with massive machine learning models. In the field of human-computer interaction, AppAgent  allows LLMs to mimic human stapping and swiping operations to operate smartphones. In these works, the LLM serves as a central

   Method & Queries \\  ToolBench & Need to create an ASCII art representation of a mathematical equation. The equation is... Help me \\ generate the ASCII art... & Also please generate an ASCII art representation of the text... (**Related tools**: figlet, list figlet styles, matheq) \\  APIBench & Our customer is a zoo and we want to help them detect movement of different animals. Write a \\ Python program in 1 to 2 lines to call API in TensorFlowHub. (**Related tools**: ObjectDetection) \\  m&m’s & I need an illustration for my children’s book. I’ve imagined a scene where there’s a large group of \\ little kids... After we have the image, we also need to identify **all the objects**, then add labels to \\  & them. (**Related tools**: ImageGeneration, ObjectDetection, Tagging) \\ 
**GTA (Ours)** & Convert the table into a statistical chart with the type of image shown in the example. (**Related tools**: ImageDescription, OCR, Plot) \\   

Table 2: Comparison of GTA queries with AI-generated queries. The steps and tool types for queries in ToolBench and m&m’s are explicitly stated, as marked in red and blue. The queries in APIBench are simple and only contain one step. Our GTA’s queries are both step-implicit and tool-implicit.

controller, invoking a certain class of tools to accomplish specialized tasks. In real-world scenarios, the environment is more complex. This requires LLMs to engage in planning and coordination among various types of tools, thereby posing a challenge to their tool-use capabilities.

**Tool-use evaluations.** With the rise of LLM-based agents, many studies have been conducted to evaluate the tool-use capabilities of LLMs. ToolBench  collects RESTful APIs and leverages ChatGPT  to design tool-use tasks and corresponding tool chains. Two metrics, Pass Rate and Win Rate, are devised to evaluate the efficacy of tool use. APIBench  is a comprehensive dataset that includes APIs from HuggingFace, TorchHub, and TensorFlow, with evaluation metrics focusing on Abstract Syntax Tree (AST) accuracy. API-Bank  comprises 53 commonly utilized APIs, such as SearchEngine, PlayMusic, BookHotel, and ImageCaption, along with a comprehensive tool-augmented LLM workflow to evaluate the API calling, retrieving, and planning abilities. m&m's  is a benchmark to evaluate tool use for multi-step multimodal tasks. It aims to evaluate different planning strategies for LLMs as planning agents. Most of the benchmarks above, however, rely on AI-generated queries. The tool-use steps are explicitly and rigidly included. Thus, these queries do not accurately represent real-world scenarios. Among many previous studies, GAIA  is renowned for its real-world scenario based benchmark aiming at evaluating general AI assistants, which is closer to our work. It designs conceptually simple questions for humans yet is challenging for most advanced AIs. However, GAIA focuses on artificial general intelligence (AGI). In contrast, GTA is designed to evaluate tool agents specifically, offering real-deployed tools and executable tool chains for a fine-grained evaluation in real-world scenarios. Osworld  is also a real-world benchmark featuring multi-step, complex tasks inspired by authentic user cases. Still, it is specifically tailored for computer environments, whereas GTA is devised for tool agents operating in more generalized real-world scenarios.

## 3 GTA Benchmark

In this section, we describe the design and content of GTA. The whole dataset construction pipeline is shown in Figure 2. We first present the composition of each sample in the dataset in Section 3.1. The construction method of queries and tool chains are depicted in Section 3.2 and Section 3.3, respectively. We then present the dataset's statistics in Section 3.4.

### Dataset Formulation

Given a set of tools \(_{c}=\{t_{k}\}_{k=1}^{N}\), a sample in GTA is composed of five parts \((,,,,)\). Among these parts, \(\) is a set of files containing one or two images. \(\) is a query based on \(\). It is a real-world scenario based problem of simple form but needs to be solved through multiple steps with tools in \(_{c}\). Which tools need to be used, and in what steps are not explicitly included in the query. They require reasoning and planning by the LLM, which serves as a central controller. This procedure is given in the reference tool chain \(=\{s_{i}\}_{i=1}^{m}\). The tool chain contains \(m\) steps. Each step is \(s_{i}=(t_{i},a_{i},r_{i})\), where \(t_{i}\) is the tool used in step \(i\). \(a_{i}\) and \(r_{i}\) indicate arguments and return values. \(=_{i=1}^{m}\{t_{j}\}_{c}\) notes the set of tools involved in this query. \(\) is the final answer yielded by the LLM after reasoning with tools.

In our setting, \(_{c}\) contains 14 tools across four categories, including perception, operation, logic, and creativity. The full list of tools is shown in Figure 1, and more detailed information can be found in Appendix B.1. The queries \(\) are classified into three types: subjective, objective, and image generation. Examples of the three types of queries are shown in Appendix B.2. For a subjective query \(_{s}\), the final answer \(\) is usually some descriptive text. It is not unique, but the general idea is the same. In this case, \(\) contains a list of three reference answers. For an objective query \(_{o}\), \(\) is a uniquely determined number or phrase. For an image generation query \(_{g}\), we do not measure the generated image directly. In this situation, \(=\).

### Query Construction

To construct \((,,)\), we first gather human-designed queries that meet three main principles: **i)** Given \(_{c}\), the task \((,)\) can be solved with the capabilities enabled by tools in \(\). **ii)** To evaluate LLMs' reasoning and planning abilities, the tool invocation steps should not be explicitly stated in the queries. **iii)** The queries are meaningful and based on real-world scenarios. Satisfying all the principles simultaneously is challenging. It requires \(,\), and \(\) to match each other sensibly and logically. We use a query construction pipeline based on exemplar expansion, as shown in the first part of Figure 2. We first give some initial exemplars with diverse scenarios and tool combinations. Then, we instruct annotators to create more queries based on the exemplars.

**Exemplar designed by experts.** We first design some initial questions as exemplars, which are provided in Appendix C.1. These example questions are of diverse scenarios and contain different tool combinations. Every sample should comprise six components: \(\) (image files), \(\) (queries), \(\) (involved tools), \(\) (solution steps), \(\) (answers), and \(\) (evidence). Image files \(\) could be obtained from the internet, and their URLs must be recorded. \(\) could also be a photo taken or a diagram drawn by the annotators. The query \(\) must avoid obvious references to a specific tool. For example, the query _please describe the image for me_ is unqualified since it obviously refers to the tool ImageDescription. The components \(\), \(\), and \(\) will not appear in the final dataset but are utilized to assist annotators in meeting the annotation requirements. \(\) represents the steps required to solve the problem. Annotators should note down the steps, ensuring their number exceeds two. The answer \(\) of objective queries should be given to guarantee a unique answer. To ensure the uniqueness, the answer should not depend on the images generated in previous steps. For example, the question _what kind of animal is in the picture_ should not be asked after _generate an image of an animal_, as the answer is uncertain. For queries utilizing the Google Search tool, \(\) should include the answer's URL and a screenshot pinpointing the answer's location to verify the query's searchability with the tool.

**Diversified expansion by annotators.** After the initial exemplars are given, we instruct annotators to create more samples based on each exemplar. We adopt a diversified expansion strategy for the annotators to expand the questions based on the exemplars. The general idea is to keep the tool set \(\) of the template unchanged or slightly modify it. Then, annotators brainstorm scenarios different from the template. Further information on the diversified expansion approach is detailed in Appendix C.2. For each sample, we have crafted a manual expansion example to serve as guidance for the annotators. After the expansion process, we perform a quality check and manually filter out the questions that do not satisfy the expansion requirements. The instruction documents for annotators are reported in Appendix C.3.

**Considerations for the search tool.** Web search tools like Google Search may return variable results over time, harming the accuracy and stability of evaluation. To address this problem, we perform two constraints. First, the question must be answered using web search rather than relying on an LLM's internal knowledge. This can be achieved by designing time-sensitive questions, such as _what is the 2023 QS ranking of Tsinghua University_ rather than general inquiries like _where is Tsinghua University located in China_. We may also direct the query to a specific information source, for instance, asking _what is the recipe for Ma Po Tofu according to the BBC Good Food website_ instead

Figure 2: Two steps are performed in the dataset construction pipeline. \(\) During _query construction_, initial exemplars and instruction documents are designed by experts and given to human annotators. Annotators brainstorm and design more samples based on the exemplars. \(\) During _tool chain construction_, annotators manually call the deployed tools to check the executability of each query in the query set. Then they annotate the ground truth tool chains for each query.

of a broad question like _what is the recipe for Ma Po Toftu_. Second, it is crucial to ensure that answers remain constant over time within an evaluation dataset. To fulfill this criterion, we can specify a time frame, web page, or organization within the question. An example would be _what is the 2024 QS ranking of Tsinghua University_, rather than _what is the QS ranking of Tsinghua University_.

### Tool Chain Construction

Based on the \((,,)\) samples constructed in Section 3.2, we instruct three annotators majoring in computer science to manually construct the corresponding tool chain \(\) and the final answer \(\). We design a JSON file structure containing the query-related tool list, image paths, and ReAct  style dialog sequences. The dialog sequences include the user query, the executable tool chain, and the final answer. Initially, \((,,)\) are put into the associated sections for tools, images, and user queries. Subsequently, we deploy all tools in \(_{}\). The annotators utilize the tools according to the reference steps \(\) and get the outcomes. They record this process in the tool chain section of the dialog sequences, alongside the final answer. Since we do not evaluate the tools' efficacy, when a tool fails to provide accurate recognition for a query (for instance, OCR inaccuracies in text recognition within diagrams), we discard the query. Through the above process, we ensure the feasibility of the questions, the executability of the tool chains, as well as the precision of the final answers. The structure of the tool chain is provided in Appendix C.4.

### Dataset Statistics

GTA comprises a total of 229 questions, with the basic dataset statistics presented in Table 3. The dataset involves 252 images and 14 distinct tools. It includes 156 objective, 16 subjective, and 57 image generation queries. The number of tools involved in each question varies from 1 to 4, with most questions using 2 or 3 tools. The steps to resolve the questions range from 2 to 8, with most questions requiring 2 to 4 steps, as depicted in Figure 3(a). The detailed frequency distribution of different tool combinations is listed in Figure 3(b). P, O, L, C are short for Perception, Operation, Logic, Creativity, respectively. Perception+Logic and Perception+Operation are the most frequently appearing tool combination types.

## 4 Evaluation and Analysis

### Experiment Settings

We evaluate 16 LLMs on GTA. For API-based models, we select GPT-3.5 , GPT-4 , GPT-4o, Claude-3 , and Mistral-Large . For open-source models, we select Llama-3  series, Qwen1.5  series, Mistral , Mistral , Yi  series, Deepseek  series. Experiments are conducted using NVIDIA A100 GPU within OpenCompass  evaluation platform. We adopt Lagent  as the agent framework. ReAct  is used as the tool invocation prompt schema. More experiment information can be found in Appendix D.1 and D.2.

We evaluate the models in two modes. **Step-by-step mode** is designed to evaluate the model's fine-grained tool-use capabilities. In this mode, the model is provided with the initial \(n\) steps of the reference tool chain as prompts, with the expectation to predict the action in step \(n+1\). This method does not involve the actual use of the tool, and the prediction of each step does not depend on the

   Item & Number \\  Total query & \(229\) \\ Query w/ pure text answers & \(172\) \\ Query w/ image answers & \(57\) \\  Total tool calls & \(557\) \\ Image files & \(252\) \\ Tools & \(14\) \\
1/2/3/4-tool examples & \(17/147/50/15\) \\   

Table 3: Basic statistics of GTA. Figure 3: Other statistics of GTA. (a) Step number per query. (b) Frequency of different tool combination.

model's preceding outputs. This enables an alignment comparison between the model's output with each step of the ground truth tool chain. **End-to-end mode** is designed to reflect the tool agent's actual task executing performance dynamically. In this mode, the model actually calls the tools and solves the problem by itself. Each step relies on the preceding step's output. We compare the tools selected and the execution result with the ground-truth tool set and result under this mode.

### Evaluation Metrics

We design fine-grained metrics spanning from the LLM's tool invocation process to execution results. To evaluate the tool invocation process, we devise four metrics under step-by-step mode: _InstAcc_, _ToolAcc_, _ArgAcc_, and _SummAcc_. InstAcc is instruction-following accuracy, which quantifies the percentage of steps executed without errors. ToolAcc measures the accuracy of tool selection. ArgAcc accesses the accuracy of argument name prediction. SummAcc reflects how accurately the model can summarize the final answers considering all previous tool-use steps. For end-to-end mode, we use _AnsAcc_ to measure the accuracy of the execution result. Besides, we calculate the _F1 scores of tool selection_ in perception, operation, logic, and creativity categories. The four F1 scores compare the model's tool selection with the ground truth tool set, measuring its tool selection ability.

In calculating the metric AnsAcc, we exclude image generation queries and focus solely on queries with pure text answers, including subjective and objective queries. For objective queries, the ground truth contains both a whitelist and a blacklist of phrases. An answer is considered correct if it includes all terms from the whitelist and excludes all terms from the blacklist. In the case of subjective queries, the ground truth contains three manually labeled responses from distinct annotators. We compute the cosine similarity (ranging from 0 to 1) between the model's prediction and each of the three ground truth answers, ultimately considering the highest score obtained. We also design a metric _AnsAcc w/ImgGen_, to take image generation queries into account indirectly. Given that the outcome of the image generation is determined solely by the input parameters, we evaluate the accuracy of these parameter predictions. If the predicted parameters are correct, the images produced should align with the specified task objectives. The specific score calculation formulas of subjective and image generation queries are shown in Appendix D.3.

    &  &  \\  & Inst. & Tool. & Arg. & Summ. & P. & O. & L. & C. & **Ans.** & **Ans.+I** \\   _API-based_ & & & & & & & & & \\  GPT-4 1106-Preview & 85.19 & 61.4 & **37.88** & 75 & 67.61 & 64.61 & 74.73 & 89.55 & **46.59** & **44.9** \\ GPT-40 & **86.42** & **70.38** & 35.19 & 72.77 & **75.56** & **80** & **78.75** & 82.35 & 41.52 & 40.05 \\ GPT-3.5-Turbo & 67.63 & 42.91 & 20.83 & 60.24 & 58.99 & 62.5 & 59.85 & **97.3** & 23.62 & 21.18 \\ Claude-3-Opus & 64.75 & 54.4 & 17.59 & **73.81** & 41.69 & 63.23 & 46.41 & 42.1 & 23.44 & 14.47 \\ Mistral-Large & 58.98 & 38.42 & 11.13 & 68.03 & 19.17 & 30.05 & 26.85 & 38.89 & 17.06 & 11.94 \\   _Open-source_ & & & & & & & & & & \\   Owen1.5-72B-Chat & 48.83 & 24.96 & 7.9 & 68.7 & 12.41 & 11.76 & 21.16 & 5.13 & 13.32 & 10.22 \\ Mistral-8x7B-Instruct & 28.67 & 12.03 & 0.36 & 54.21 & 2.19 & 34.69 & 37.68 & 42.55 & 9.77 & 9.33 \\ Deepseck-LLM-67B-Chat & 9.05 & 23.34 & 0.18 & 11.51 & 14.72 & 23.19 & 22.22 & 27.42 & 9.51 & 7.93 \\ Llama-3-70B-Instruct & 47.6 & 36.8 & 4.31 & 69.06 & 32.37 & 22.37 & 36.48 & 31.86 & 8.32 & 6.25 \\ Yi-34B-Chat & 23.73 & 10.77 & 0 & 34.99 & 11.6 & 11.76 & 12.97 & 5.13 & 3.21 & 2.41 \\   Qwen1.5-14B-Chat & 42.25 & 18.85 & 6.28 & 60.06 & 19.93 & 23.4 & 39.83 & 25.45 & 12.42 & 9.33 \\ Qwen1.5-7B-Chat & 29.77 & 7.36 & 0.18 & 49.38 & 0 & 13.95 & 16.22 & 36 & 10.56 & 7.93 \\ Mistral-7B-Instruct & 26.75 & 10.05 & 0 & 51.06 & 13.75 & 33.66 & 35.58 & 31.11 & 7.37 & 5.54 \\ Deepseck-LLM-7B-Chat & 10.56 & 16.16 & 0.18 & 18.27 & 20.81 & 15.22 & 31.3 & 37.29 & 4 & 3.01 \\ Llama-3-8B-Instruct & 45.95 & 11.31 & 0 & 36.88 & 19.07 & 23.23 & 29.83 & 42.86 & 3.1 & 2.74 \\ Yi-6B-Chat & 21.26 & 14.72 & 0 & 32.54 & 1.47 & 0 & 1.18 & 0 & 0.58 & 0.44 \\   

Table 4: **Main results of GTA.** Inst., Tool., Arg., Summ., Ans.+I denote InstAcc, ToolAcc, ArgAcc SummAcc, AnsAcc, and AnsAcc w/ImgGen respectively. P., O., L., C. denote the F1 score of tool selection in Perception, Operation, Logic, and Creativity categories. **Bold** denotes the best score among all models. Underline denotes the best score under the same model scale. **AnsAcc** reflects the overall performance.

### Main Results

**Real-world tool-use tasks are challenging for existing LLMs.** Current LLMs are struggling to accurately invoke tools to solve these real-world tasks. As shown in Table 4, the best-performing models, GPT-4 and GPT-4o can only correctly solve fewer than 50% of the problems, while the rest of the models solve less than 25%. This shows that real-world problems with implicit steps, real tool invocations, and multimodal contextual inputs impose high requirements on the tool-use capabilities of LLMs. Regarding model performance comparisons, API-based models outperform open-source ones. Among open-source models, Qwen1.5-72B-Chat has the highest result accuracy. Larger models within the same series perform better than their smaller counterparts, but larger models from different series do not necessarily outperform the smaller ones, as shown in Figure 4. For example, the AnsAcc of Llama-3-70B-Instruct is higher than that of Llama-3-8B-Instruct, but lower than Qwen1.5-7B-Chat.

**The current bottleneck mainly lies on argument prediction.** From the results, we observe that the overall performance of the system is affected by the lowest metric. We argue that _the four metrics in the step-by-step mode follow the buckets effect._ To verify this observation, we calculate the Pearson correlation coefficients between four metrics (InstAcc, ToolAcc, ArgAcc, SummAcc) and AnsAcc, the result is shown in Figure 6. We find that the correlation coefficient for ArgAcc with AnsAcc is the highest. ArgAcc is low for most models, indicating that the four metrics follow the buckets effect. For example, the scores of Llama-3-70B-Instruct in InstAcc, ToolAcc, and SummAcc are higher than those of Qwen1.5-14B-Chat, but its ArgAcc is lower than Qwen1.5-14B-Chat, resulting in a lower final answer accuracy. The scores of GPT-4o in InstAcc and ToolAcc are higher than GPT-4, but its weaker argument prediction capability leads to a lower accuracy rate in the final result. The reason for the buckets effect is that under our evaluation framework, the model needs to follow user instructions, invoke tools multiple times in the correct format, and summarize the answer based on the returned results. Any error in this process can lead to an incorrect conclusion. Currently, argument prediction is the weakest capability for most models, suggesting that to enhance their general tool-use capabilities, researchers can focus on argument prediction capabilities. This concerns both the value and the format correctness of an argument.

**Different series of LLMs exhibit distinct behavioral patterns.** We count the number of successful and failed tool calls, illustrated in Figure 6. Successful means there are not any errors in the tool call. GPT-4o has the highest number of successful tool calls, while GPT-4 has the highest successful tool call rate. We find that models from different series exhibit distinct behavioral tendencies. Yi and Deepseek series tend to be _aggressive_, invoking tools frequently but lacks sufficient instruction-following ability to invoke tools in a correct format. The Qwen series is _conservative_, preferring to invoke tools less often, yet it has stronger instruction-following capabilities, resulting in a higher success rate of tool calls. The GPT series is _neutral_, tending to invoke tools moderately and possessing robust instruction-following abilities, which leads to the highest final answer accuracy. This suggests that to improve the performance of Yi or Deepseek, focus should be given to enhancing their instruction-following ability. Conversely, to enhance the Qwen series, reducing its conservative behavior to tool calls could be beneficial.

**Models favor either format errors or argument format errors, not both equally.** We count the percentage of error types when calling tools, including format error, argument format error, and N/A (other errors, mainly containing the tools' internal error). Most models exhibit a clear tendency toward either format errors or argument format errors, rather than making both types of mistakes in nearly equal numbers. For example, Claude-3's errors are predominantly argument format-related, amounting to 82.86%, while format errors account for a mere 4.29%. This indicates that Claude-3 can follow the tool-call format well, but fails to pass the argument in a correct format.

### Further Analysis and Exploration

**Detailed Error Analysis.** In Section 4.3, we discuss the bottleneck in task performance arising from most models' inability to generate responses or predict arguments in the correct format. To understand the reason behind these model failures, we conduct a detailed analysis of the predictions generated by GPT-4-1106-Preview and Llama-3-8B-Instruct. We systematically categorize seven primary error types. The statistical outcomes are presented in Table 6. Detailed error cases of each type can be found in Appendix D.4.

Our analysis reveals distinct error distributions between GPT-4 and Llama-3. GPT-4 consistently adheres to the given prompts when executing actions, in contrast to Llama-3, which often fails to maintain the prescribed format. However, GPT-4 is prone to generating passive thought processes or attempting interaction with the user, rather than taking decisive action.

For the GPT-4 model, the predominant type of error is No Action, wherein the model neither utilizes tools nor produces a final answer. In 38.7% of erroneous responses, GPT-4 attempts to engage with

   Error Type & GPT-4-1106-Preview & Llama-3-8B-Instruct \\ 
**Planning error:** & & \\ No action, requesting more information from the user. & 24 (38.7\%) & 0 \\ No action, the whole response is model thought. & 31 (50\%) & 2 (0.1\%) \\ 
**Format error:** & & \\  The arguments do not follow the correct JSON format. & 5 (8.1\%) & 118 (45.4\%) \\ Trying to call multiple tools in one step. & 1 (1.6\%) & 43 (16.5\%) \\ Generating redundant information that leads to incorrect argument parsing. & 0 & 51 (19.6\%) \\ Repeating contents from the prompt. & 0 & 41 (15.8\%) \\ Generating the final answer but not following the correct format. & 1 (1.6\%) & 5 (1.9\%) \\ 
**Total number** & 62 (100\%) & 260 (100\%) \\   

Table 6: Detailed error distribution of GPT-4-1106-Preview and Llama-3-8B-Instruct on argument prediction.

   Model & Format Error (\%) & Arg. Format Error (\%) & N/A (\%) \\  GPT-3-5-Turbo & 8.1 & 60.32 & 20.24 \\ GPT-4-1106-Preview & 70.29 & 4.35 & 25.36 \\ GPT-4o & 78.69 & 19.13 & 13.39 \\ Claude-3-Opus & 4.29 & 82.86 & 4.29 \\ Mistral-Large & 4.47 & 72.07 & 3.07 \\ Llama-3-8B-Instruct & 20.47 & 65.15 & 14.38 \\ Llama-3-70B-Instruct & 29.51 & 69.7 & 0.8 \\ Mistral-7B-Instruct & 49.21 & 46.56 & 4.23 \\ Mistral-8V-B-Instruct & 53.74 & 40.82 & 5.44 \\ Mugenl 5.7B-Chat & 2.56 & 89.74 & 7.69 \\ Qwen1.5-14B-Chat & 2.35 & 71.76 & 25.88 \\ Qwen1.5-72B-Chat & 10.71 & 71.43 & 17.86 \\ Yi-6B-Chat & 98.22 & 0.18 & 1.61 \\ Yi-34B-Chat & 88.11 & 6.22 & 5.67 \\ Deepseck-LLM-7B-Chat & 52.49 & 19.65 & 27.86 \\ Deepseck-LLM-67B-Chat & 58.22 & 34.39 & 7.39 \\   

Table 5: The percentage of different error types.

the user, mistakenly assuming the query lacks clarity and requesting additional information, despite the query and input images supplying sufficient details for task resolution. Furthermore, 50% of the error responses consist solely of the model's internal thought without any corresponding action.

For the Llama-3 model, most errors are related to formatting during action sequences, such as invoking tools or generating the final answer. Specifically, 45.4% of errors originate from argument predictions not adhering to a valid JSON format. Additionally, in 16.5% of the flawed responses, the model attempts to invoke multiple tools simultaneously, which is not supported by the agent system. Moreover, 19.6% of the errors occur when the model disregards the prompt and generates redundant information after argument prediction, leading to incorrect argument parsing. Finally, in 15.8% of the cases, the model fails to perform the correct action, merely repeating content from the prompt.

**Further Exploration to Enhance Model Performance.** Since the LLM functions as the central controller of an agent system, producing responses that strictly comply with the agent protocol is important. Fine-tuning on ReAct and JSON format may mitigate format-related errors during action execution. To verify this, we further compare Llama-2-Chat-7B with Agent-Flan-7B on GTA benchmark. AgentFLAN is a popular instruction tuning method that fine-tunes LLM-based agents using ReAct and JSON instruction-following data. Agent-Flan-7B is fine-tuned from Llama-2-Chat-7B using AgentFLAN method. The results are shown in Table 7.

We discovered that Agent-Flan-7B's InstAcc and ToolAcc metrics were significantly higher than those of Llama-2-Chat-7B. The responses of Agent-Flan-7B follow the format of 'Thought-Action-Action Input' that specified in the prompt. But most responses of Llama-2-Chat-7B fail to follow the format. This further suggests the improved instruction following capability of Agent-Flan-7B.

However, we note that the ArgAcc of Agent-Flan-7B is still low. We compare the response of the two models, as shown in Appendix D.5. We find that although Agent-Flan-7B follows the format of 'Thought-Action-Action Input', it sometimes fails to generate the argument (Action Input) in a correct JSON format, or summarizes the final answer incorrectly. Thus, how to further enhance the model's capabilities on GTA through instruction fine-tuning is still an open problem.

## 5 Conclusion

We propose GTA, a real-world tool-use benchmark for general-purpose agents. The user queries are human-designed, step-implicit, and settled in real-world scenarios. Multimodal contextual inputs are provided. We build an evaluation platform equipped with executable tools in the categories of perception, operation, logic, and creation. Fine-grained metrics are designed for the tool-use capabilities of LLMs in real-world scenarios. We evaluate the tool-use capabilities of 16 LLMs. The evaluation results show that GTA is challenging for current LLMs, with advanced models like GPT-4 struggling with these real-world tasks, completing less than 50% of them. Based on our findings, we give takeaways and further suggestions on tool-use capability improvement. We believe that the GTA benchmark will advance further research in identifying the model's tool-use capabilities and contribute to realizing general-purpose tool agents.

## 6 Limitations

Our benchmark lacks language diversity since all queries are in English. Multilingual queries can be added in future work to assess the capability of tool agents in non-English environments. Moreover, to achieve high data quality, both the user queries and the tool chains are human-written. So the cost of a data piece is higher than that of AI-generated counterparts.

  Model & Inst. & Tool. & Arg. & Summ. & Ans. & Ans.+1 \\  Llama-2-Chat-7B & 30.86 & 16.34 & 0.36 & 47.69 & 3.96 & 2.98 \\ Agent-Flan-7B & **71.60** & **41.11** & **6.82** & **52.88** & **6.45** & **4.85** \\  

Table 7: Comparison of Llama-2-Chat-7B with Agent-Flan-7B (which is fine-tuned from Llama-2-Chat-7B on ReAct and JSON format data) on GTA.

Acknowledgements

This work is supported by the National Key R&D Program of China (No. 2022ZD0161600), and the National Natural Science Foundation of China under Grants 62422311 and 62176152.