# Exploring Foveation and Saccade

for Improved Weakly-Supervised Localization

Timur Ibrayev

Manish Nagaraj

Amitangshu Mukherjee

Kaushik Roy

Purdue University,

West Lafayette, IN 47906 USA

###### Abstract

Deep neural networks have become the de facto choice as feature extraction engines, ubiquitously used for computer vision tasks. The current approach is to process every input with uniform resolution in a one-shot manner and make all of the predictions at once. However, human vision is an "active" process that not only actively switches from one focus point to another within the visual field, but also applies spatially varying attention centered at such focus points. To bridge the gap, we propose incorporating the bio-plausible mechanisms of foveation and saccades to build an active object localization framework. While foveation enables it to process different regions of the input with variable degrees of detail, saccades allow it to change the focus point of such foveated regions. Our experiments show that these mechanisms improve the quality of predicted bounding boxes by capturing all the essential object parts while minimizing unnecessary background clutter. Additionally, they enable the resiliency of the method by allowing it to detect multiple objects while being trained only on data containing a single object per image. Finally, we explore the alignment of our method with human perception using the interesting "duck-rabbit" optical illusion. The code is available at: https://github.com/TimurIbrayev/FALcon.

**Keywords:** neuro-inspired algorithms, foveation, saccades, active vision, optical illusions, weakly supervised learning, object localization, object detection, deep learning

## 1 Introduction

Deep Neural Networks (DNNs) have brought a lot of advancements to the field of computer vision (Deng et al., 2009; Krizhevsky et al., 2012). They have become the de facto choice as feature extraction engines, ubiquitously used for downstream tasks of object classification, localization, and detection (Ren et al., 2015; Liu et al., 2016; Redmon et al., 2016; Redmon and Farhadi, 2016; Lin et al., 2017, 2020). However, their nature of processing inputs can be characterized as being "passive", meaning that the entire image is processed in a one-shot manner with uniform resolution (i.e. spatially invariant attention within the visual field), and all of the predictions are made at once. In contrast, empirical observations from neuroscience (Curcio et al., 1990; Eckstein, 2011; Land and Nilsson, 2012) suggest that human vision is an "active" process that not only actively switches from one focus point to another within the visual field, but also applies spatially varying attention centered at such focus points.

To that effect, we advocate incorporating foveation and saccades into machine perception. _Foveation_ is the property of the human eye to process different regions of the visual field with variable degrees of detail. _Saccades_ are quick eye movements that allow changingthe focus point from one location to another. Their combined implementation can grant computer vision methods the ability to select where to look, ignoring the unnecessary clutter from the input. Such adaptive functionality may not be possible when the entirety of an input is processed with uniform resolution.

Note, foveation and saccades allow different views from within the same sample and hence augments the set of features used for training. Figure 1 illustrates this by comparing the processing (_a_) without and (_b_) with the mechanisms of foveation and saccades (collectively denoted as "F&S"). Figure 1(_a_) shows the standard computer vision pipeline that processes the input with uniform resolution. Training for a downstream task, e.g. the classification of the object or the prediction of its location, depends on the set of features that DNN is able to extract from the set of samples. When the entirety of the inputs is processed with uniform resolution, DNNs have to rely on the views of different samples belonging to the same class (_intra-class inter-sample views_) in order to come up with a consensus on which features describe the object of this class. For example, 1000 different "dog" images allow DNN to learn parts/features of the "dog" class that are necessary to either classify an image as depicting the "dog" class or draw a _complete_ bounding box _capturing all of its parts_ to predict its precise location.

On the contrary, Figure 1(_b_) shows the pipeline incorporating "F&S". With different fixation points (due to saccades) and different degrees of details around these fixation points (due to foveation), "F&S" can result in a large number of views obtained from within a single sample (_intra-sample views_). As a result, DNNs have a larger set of features that it can rely on to come up with the consensus on which features describe the object of this class. Moreover, this expanded set is more diverse as the intra-sample views might contain

Figure 1: Comparison of the processing pipelines for (a) standard computer vision methods processing inputs with uniform resolution and (b) a method with foveation and saccades. “F&S” denotes the mechanisms of foveation and saccades. They expand the set of training samples by obtaining different views from within each sample at different fixation points (due to saccades) and different degrees of details at these fixation points (due to foveation). Best viewed in color.

the entire object, only some parts of the object, or none of the object parts. This might be beneficial for learning fine-grained features in the case of classification tasks and for learning the "complete" object in the case of bounding box prediction tasks. For example, "F&S" might produce 10 views of a single "dog" image, some of which might purposefully drop any of the "dog" class features otherwise observable from the whole image. Then, the DNN can learn that such negative examples are incomplete in comparison with the "dog" shown in the whole image. This forces the DNN not only to learn consistency in intra-class inter-sample features but also in intra-sample features.

Incorporating the mechanisms of foveation and saccades, we propose an (active) object localization framework. They change the task of predicting object locations from being a "passive" process to an "active" process. The core idea is to reformulate the prediction of bounding boxes from a one-shot manner to an iterative process. By emulating _foveation_, our proposed framework observes only the limited (_foveated_) region of the input at each iteration. By emulating _saccades_, the framework is able to predict whether the current foveated region has any relevant object or whether it is better to switch to the next portion of the input. The framework then iteratively predicts the correct sequence of actions to expand the _foveated region_ to eventually capture the entirety of the object of interest.

The proposed framework, which we refer to as **FALcon**, was verified within the settings of weakly supervised object localization (WSOL). WSOL is the task that requires prediction of both the object class (_image-level label_) and the object location in the form of a bounding box (_instance-level label_), **while being trained only on the image-level labels**. Since

Figure 2: FALcon improves the localization performance and enables the detection of multiple objects while only being trained on the data that contains only a single object. If we assume that the localization is performed in some unconstrained environment, e.g. images are fetched from the internet, FALcon can be thought of as an advanced WSOL method that is more resilient to images with multiple objects during the final deployment.

the degree of supervision is already limited due to the absence of the object locations during training, we show that the addition of foveation and saccades facilitates improved localization. In particular, we combine the proposed FALcon framework with the existing WSOL method (Figure 2) and show that the bio-plausible mechanisms provide the following benefits:

1. We show that replacing standard "passive" localization with the FALcon improves the localization performance on the WSOL task (compare predictions in (a) and (c) of Figure 2). This is achieved due to more complete (i.e. capturing all essential object parts) and/or more tight (i.e. avoiding all unnecessary background clutter) bounding boxes resulting from the proposed utilization of foveation and saccades.
2. We show that FALcon enables a resilient WSOL pipeline. If we assume that the end goal is to have an automated annotation system, where the images might be fetched directly from an unconstrained environment like the internet, it is desirable to have a method that is resilient to images with multiple objects (even if trained only on data containing a single object). Hence, we show that using foveation and saccades it is possible to obtain a model that is capable of detecting multiple objects while being trained on data that contains only a single object (shown in (b) and (d) of Figure 2).
3. Finally, we show that designing neuro-inspired algorithms with mechanisms like foveation and saccades might be a promising research direction. Specifically, we show that, similar to humans, FALcon is capable of providing two different results (based on the starting point of the foveated glimpse) on the well-known "duck-rabbit" optical illusion.

## 2 Background

### Foveation and Saccades

The human vision has spatially varying acuity due to a non-uniform topography of photoreceptor cells in the eye's retina layer (Curcio et al., 1990). We broadly refer to such visual processing with variable degrees of detail using the term "foveation". Being in contrast to computer vision that processes inputs with uniform resolution, there has been research interest to explore the benefit of foveation for machine perception. In particular, the works that model and incorporate foveation showed its effectiveness for computational efficiency (Bauer et al., 2023; Jaderberg et al., 2015; Thavamani et al., 2021), image classification (Jaderberg et al., 2015; Pramod et al., 2022; Yang et al., 2022), scene understanding (Wu et al., 2018; Thavamani et al., 2021), object discovery and saliency (Matzen and Snavely, 2015; Yang et al., 2022), image generation (Gregor et al., 2015), and robustness against adversarial inputs (Deza and Konkle, 2020; Gant et al., 2021). Because the extent of the visual field that is processed with high details is limited, the responsibility of guiding it is assigned to "saccades": rapid movements of the eye between multiple fixation points (Eckstein, 2011; Land and Nilsson, 2012). Various works were proposed either to predict eye movements (Jindal and Manduchi, 2023) or to use eye movements (or _gaze_ fixations) as the data for object counting (Thompson et al., 2023), maze solving (Li et al., 2023), facial composite generation (Strohm et al., 2023), automated/assisted driving (Nikan and Upadhyay, 2023), and robot guidance (Yifan et al., 2023). Finally, the combination of both foveation and saccades has been shown useful for the applications of visual search and guidance. Cheung et al. (2017) demonstrate the importance of retinal sampling lattice for visual search task of an object in a cluttered scene using a neuron attention model. Akbas and Eckstein (2017) aggregates observations across multiple fixation points via a Foveated Object Detector as an alternative to the sliding window approach of passive detectors. Zhang et al. (2018) searches for a target in a cluttered scene using biologically inspired computation model with zero-shot training. The works of Elsayed et al. (2019) and Huang et al. (2022) propose different combinations of hard attention and reinforcement learning to efficiently improve the classification performance. We propose a complementary approach of utilizing foveation with extreme cutoff as the method of hard attention and saccades as the method of estimating the relevance of foveated observations for the purpose of advancing the weakly supervised object localization methods.

### WSOL: Weakly Supervised Object Localization

In this work, we advocate the efficacy of the novel application of foveation and saccades in the task of WSOL. Here we briefly describe some selected works with which we compare in the Results section and later provide an extensive study of previous WSOL methods in the Appendix. Zhou et al. (2016) re-purpose the global average pooling layers of convolutional neural networks to generate class activation maps (CAM) to obtain localization maps. However, CAM highlights only the most discriminative parts of an image leading to the part dominance issue which hinders localization estimates of DNNs. SPG (Zhang et al., 2018) separates the foreground from the background by learning spatial correlation information among pixels. PSOL (Zhang et al., 2020) encourages that weakly supervised object localization should be divided into two parts: a localization network performing class-agnostic object localization and a separate classification network performing class-specific object classification. SLT-Net (Guo et al., 2021) employs a separate localizer to learn the localization function by matching the class activation map of the input image and the inverse activation map of a transformation of the same image. C\({}^{2}\)AM (Xie et al., 2022) learns a class-agnostic activation map via a novel cross-image foreground-background contrastive loss to disentangle foreground from background on the assumptions that similar foreground objects have similar semantic feature representations which differs from that of the backgrounds. Existing methods provide object locations by directly predicting bounding box dimensions from the single uniform processing of the input. Our work advocates that transforming such "passive" processing into "active" iterative processing further boosts their localization performance and enables their resiliency as an end system.

## 3 Methodology

### Active Localization with Foveation and Saccades

The idea behind FALcon is to train a DNN model to predict the location of an object by iteratively observing different parts of the input image and regulating the size of these observations. To be precise, FALcon predicts the location of the object by: (a) focusing on different locations on the image (which we refer to as _initial fixation points_) usingsaccades, and (b) iteratively observing only parts of the input image (which we refer to as _foveated/foveation regions_), which are initiated at the fixation points but are expanded as necessary using foveation.

Figure 3(c) illustrates the processing of a single image using different fixation points and different sequences of foveated regions originating from the corresponding fixation points. The top row shows two different initial fixation points indicated by the small circles placed on the full-scale inputs. The two columns of smaller patches under the first row show the two different sequences of foveated regions, which originated from the corresponding initial fixation points. Specifically, the location of an initial fixation point serves as the center for an initial foveated region. Then, the initial foveated region is obtained by cropping out a portion of the input image of a pre-determined size, the value of which is chosen as one of the hyperparameters. For example, the initial foveated region resulting from the first initial fixation point is shown as the first crop under the first image in the top row. FALcon operates by processing _only such foveated regions_, i.e. only the limited regions cropped out from within the input, but not the full-scale input.

Figure 3: High-level overview of FALcon for weakly supervised object localization (WSOL) task. (a) In the context of WSOL, during training, we first extract predicted bounding boxes using the underlying WSOL method, which are referred to as “pseudo” bounding boxes. (b) FALcon actions are trained based on the current foveation region (orange box) and “pseudo” bounding boxes (cyan box). All possible actions (yellow dashed box) are trained on the set of permitted actions (green dashed box) using binary cross entropy loss. (c) Example illustrating the working principle of active localization based on foveation and saccades mechanisms.

The process of expanding the foveated regions and changing the initial fixation points is learnt by FALcon using a single DNN model with 5 output nodes. By observing the current foveated region, FALcon produces 5 outputs, each corresponding to one of the five allowed actions for the localizer to choose from. Four actions emulate the foveation mechanism that controls the size of the foveation region. In particular, the four actions are: (1) expand in \((dy^{-})\) direction, (2) expand in \((dy^{+})\) direction, (3) expand in \((dx^{-})\) direction, (4) expand in \((dx^{+})\) direction, with \((0,0)\) coordinates being top left corner of the full-scale input image. The last action ((5) switch) emulates the mechanism of saccades that controls whether the current foveation region contains anything of interest or the region is irrelevant and the fixation point needs to be switched. The predicted actions at each iteration are then applied in an iterative manner to either change (expand) the current foveated region or to completely switch the fixation point. As a result, a successful localization is achieved when FALcon decides neither to expand the foveation region nor to switch the fixation point to investigate any other portion of the input.

Figure 3(c) illustrates one possible course of actions that FALcon can take to localize the bird in the image. From the first initial fixation point, FALcon expands the initial foveated region to observe a larger portion of the input (as shown by the left column of Figure 3(c)). However, due to the absence of any relevant features in the initial and the second foveated regions, FALcon switches to the next fixation point. From the second initial fixation point (shown in the right column), FALcon produces and observes a series of expanding foveation regions. Finally, the bird in the input image is successfully localized by FALcon capturing it within the last foveated region and predicting no further action.

### Training Approach

The proposed training approach simultaneously uses different views generated by foveation and saccades and trains FALcon to improve them for the purpose of object localization. This is achieved by allowing FALcon to generate a sequence of foveated regions and training it on each of the foveated regions with the goal of capturing the object of interest. For each image, the sequence of observing and expanding foveated regions lasts for a pre-determined number of iterations (a hyperparameter referred to as the _foveation iterations_). As FALcon becomes better at capturing relevant object parts, the relevance of the foveated regions to the task improves. Due to different initial fixation points, FALcon has to learn to capture objects using various trajectories of foveated regions, which ensures the diversity of foveation iterations. For example, compare the foveation iterations to localize a "dog" in sample 3 from its "head" versus from its "tail" shown in Figure 1(b). As a result, the training progression itself increases the number of diverse and task-relevant training samples.

Figure 3(b) shows the training approach of the FALcon model at one of the foveation iterations. The foveated region at the current iteration is shown as an orange bounding box. On the right part of Figure 3(b), we can observe FALcon expand the current foveated region (orange bounding box) into all four directions (dashed yellow bounding box). FALcon is designed to take each expansion with a pre-determined fixed step size, which is one of the hyperparameters. In other words, if one of the edges is (predicted) to be expanded, it is changed by the fixed number of pixels. Otherwise, the foveated region dimensions will remain unchanged. We represent the expansion of the right, the left, the bottom, and the

top edges of the foveated region as \((dx^{+})\), \((dx^{-})\), \((dy^{+})\), and \((dy^{-})\), respectively. The model predicts the output values for each expansion direction, each representing the confidence of the model (in the range \(\)) to expand the corresponding edge of the foveated region. The predictions for each edge are independent, meaning that at each iteration, the model can decide to expand in all four directions or keep the entire region unchanged.

Training of FALcon requires a reference location of the object of interest, shown as the solid cyan bounding box in Figure 3(b). The targets to train the FALcon actions are generated based on the current foveated region and the reference box. Specifically, the target value for each edge is true, if the edge of the new (expanded) foveated region does not exceed beyond the dimensions of the reference box. Otherwise, the target is assigned a false value which directs FALcon not to expand in that direction. For the example shown in Figure 3(b), we can see that out of the 4 possible expansion actions only two are allowed, which will make the foveated region at the next iteration (dashed green bounding box) remain within the boundaries of the reference bounding box (shown in solid cyan).

Along with the foveation actions, FALcon is also trained to learn the saccades mechanism by estimating the relevance of the current region and determining if a switch of the fixation point is necessary. During training, this is achieved by purposefully initiating fixation points outside the reference bounding box for half of the images and within the reference bounding box for the other half. For the first half of the samples, the switch target is true, until the model correctly predicts the switch action. If the model predicts the switch action correctly, it is provided with a new initial fixation point that is within the reference bounding box. For the samples that have the fixation point within the reference bounding box (i.e. for the second half of the samples as well as the ones that successfully predicted the required switch), the switch target is changed to false.

Note that the behavior of switch predictions by saccades is different from the prediction of "objectness" scores in the standard computer vision detectors. Standard detectors assign the objectness score based on the final overlap of the predicted bounding boxes with the ground truth bounding box. As a result, while standard detectors need to learn to differentiate foreground and background implicitly through multiple predicted boxes and their assigned objectness scores (similarly to multiple instance learning), our method trains the difference explicitly by directly sampling portions of the image pertaining to the background.

### FALcon Deployment

From the application perspective, we consider FALcon as a replacement for the standard "passive" localization (layers or an entire model) that produces the final coordinates of the predicted bounding box after a single observation with uniform resolution. As explained in the previous subsection, FALcon is a supervised technique and requires at least a rough estimate of the object location. In the context of weakly supervised object localization, FALcon hence builds upon an existing WSOL method. Specifically, in order to train FALcon, a pre-trained WSOL method is used with its standard localization model to estimate the object location for the given input training image (Figure 3(a)). We refer to these bounding boxes predicted by the underlying WSOL method as "pseudo" bounding boxes in order to highlight the difference between ground truth bounding boxes used for final evaluation. After training is complete, FALcon works on its own without a need for the localization part of the underlying WSOL method (as shown in Figure 2(d)). For a fair comparison, however, we still take the same classification model of the underlying WSOL method. Although we did not consider FALcon in the context of fully supervised object localization and detection tasks, the proposed method should still be applicable when the ground truth bounding boxes are available for training.

In order to capture all the possible objects, during inference FALcon explicitly considers multiple initial fixation points equally distributed over the input image. This is essentially similar to dividing the entire image into grid cells and considering each of the cells as a starting foveation region. However, all of the grid cells that were predicted to switch are dropped from consideration. All of the remaining final foveated regions (which essentially represent predicted bounding boxes) are then passed through a non-maximum suppression technique, similar to most object detectors. This approach allows the proposed framework to manifest the resiliency that it learns during training. In particular, the learned saccatic switch behavior allows FALcon to consider and explore any place in the input space that potentially has relevance to any of the objects of interest. Additionally, the learned foveation behavior allows FALcon to expand and stop foveated regions to capture such objects. Such ability is neglected in object localization methods that process inputs with a uniform resolution, due to the underlying assumptions that only a single object is present and the resiliency is not necessary.

## 4 Results

### Experimental Settings

ImplementationThe proposed method was implemented using the PyTorch deep learning framework. The training and evaluation experiments were performed on a single Nvidia A40 80GB GPU card. In the context of the WSOL task, we used the PSOL work (Zhang et al., 2020) as the underlying WSOL method to provide "pseudo" bounding boxes for training images. Then, the proposed framework is trained on "pseudo" bounding boxes extracted using the PSOL method on all training images. FALcon relies on the number of hyperparameters. A detailed description of hyperparameters is provided in the Appendix.

DatasetsWe trained the proposed method on two datasets in WSOL settings: Caltech-UCSD Birds-200-2011 (Wah et al., 2011) (also known as CUB-200-2011 or CUB) and ILSVRC 2012 (Russakovsky et al., 2015) (also known as ImageNet-1k). CUB-200-2011 is the dataset of 5,994 training and 5,794 testing images belonging to 200 different bird species, which are treated as image-level object classes. ImageNet-1k is a large dataset of 1,281,167 training and 50k validation images, with each assigned one image-level object class label. Note that ImageNet-1k images might contain multiple objects, and its annotations provide multiple bounding boxes. However, during training, it is always assumed that the images contain only a single object described by the provided image-level label. Correspondingly, FALcon is only trained on a single "pseudo" bounding box per image predicted by the PSOL baseline for every image in the training set. To verify the effect of active localization using foveation and saccades, trained FALcon models are evaluated on test and validation images of CUB and ImageNet-1k datasets, respectively. To verify the resiliency of the pipeline with foveation and saccades, we also test the capabilities of the baseline models and FALcon models on multi-object datasets. Specifically, the FALcon model that was trained on the CUB dataset was verified on Pascal VOC07 and Pascal VOC12 datasets (Everingham et al., 2010), and the FALcon model trained on the ImageNet-1k dataset was verified on the detection set of ILSVRC2013 (Russakovsky et al., 2015) (which we refer to as ImageNet13-Det).

EvaluationTo show the effect of foveation and saccades both on the localization performance as well as the resiliency, FALcon is evaluated on images containing only a single object (i.e. CUB and ImageNet-1k datasets) and on images containing multiple objects (i.e. Pascal VOC07, Pascal VOC12, and ImageNet13-Det). (_Note: for both cases, FALcon is trained only on images containing a single object._) Hence, the following evaluation metrics were used. In the context of the localization of a single object, our method is evaluated on "GT-Known Loc" and "Top-1 Loc" metrics. "GT-Known Loc" or "GT Loc" is the percentage of samples for which the intersection over union (IoU) between the ground truth bounding box and the predicted bounding box is greater than or equal to 0.5 (i.e. localization only). "Top-1 Loc" is the percentage of samples for which both the IoU is greater than or equal to 0.5 **and** the label of the object within the bounding box is correctly predicted (i.e. localization and classification). In the context of the detection of multiple objects, our method is evaluated on "Average Precision (AP)" and "mean Average Precision (mAP)". Both of the metrics measure the area under the curve in the precision versus recall curve, but with "AP" measuring it for a specific class and "mAP" measuring the mean of the AP values for different classes.

### FALcon Improving Localization

Table 1 shows the performance of FALcon on WSOL benchmarks in comparison with PSOL (Zhang et al., 2020) as the baseline and other WSOL methods. In contrast to the standard WSOL methods, FALcon is designed to automatically consider the possibility of multiple objects being present in the image. It is expected that the method will make a single prediction per image whenever there is a single object. Consequently, without explicit supervision or forced restrictions, for the CUB and ImageNet-1k datasets, FALcon produced only a single prediction per image in 94.8% and 71.0% of test samples, respectively. However, there are some test samples for both datasets in which FALcon chooses a safer approach of making more than 1 prediction. For example, for CUB and ImageNet-1k, FALcon made 2 predictions per image in 4.8% and 19.5% of test samples, respectively. More detailed statistics are provided in the Appendix.

Hence, we report GT Loc and Top-1 Loc values for three different scenarios in Table 1: when we consider up to 1, up to 3, or up to 5 predictions per image. For each scenario, we evaluate up to a specified number of predictions per image, ranked based on the confidence of the classification prediction. Then, the object is considered to be correctly localized if any of the predictions correctly predicts the location and the object label.

On the CUB dataset, FALcon achieves significant localization results (based on GT Loc), even when it is limited to making a single prediction per image (max 1). The accuracy of bounding boxes improves from 77.41% of the baseline (PSOL) method to 88.30% with the addition of foveation and saccades. This improvement is explained by qualitative results shown in Figure 4(a). Comparing the bounding boxes produced by PSOL alone and FALcon trained with PSOL, we can observe that our technique improves _completeness_ (by capturing more of essential object parts) and/or _tightness_ (by minimizing the amount of background clutter) of predicted bounding boxes. The capability of FALcon to predict bounding boxes is that the proposed method is able to predict bounding boxes.

   & &  &  \\ 
**Method** & **\# of predictions** &  &  &  &  \\ 
**per image** & GT Loc &  &  &  \\  & **per image** & GT Loc &  \\  VGG16 CAM (Zhou et al., 2016) & 1 & 57.96 & 36.13 & 59.00 & 42.80 \\ InceptionV3 SPG (Zhang et al., 2018c) & 1 & 60.50 & 46.64 & 64.49 & 48.60 \\ VGG16 SLT-Net (Guo et al., 2021) & 1 & 87.60 & 67.80 & 67.20 & 51.20 \\ DenseNet161 C\({}^{2}\)AM (Xie et al., 2022) & 1 & 94.46 & 83.28 & 68.20 & 59.28 \\ 
**PSOL (baseline) (Zhang et al., 2020a)** & **1** & **77.41** & **63.56** & **66.28** & **55.31** \\   & max 1 & 88.30 & 62.82 & 62.45 & 49.39 \\
**FALcon + PSOL** & max 3 & 89.35 & 63.50 & 67.38 & 53.31 \\
**(Ours)** & **max 5** & **89.35** & **63.50** & **67.51** & **53.50** \\  

* _Note: official PSOL repository does not contain pre-trained weights on CUB dataset. We report results achieved by training the models with hyper-parameters and methods described in the original work._

Table 1: Weakly supervised object localization (WSOL) results

Figure 4: Qualitative results obtained on the (a) CUB and (b) ImageNet-1k datasets.

more than one object per image is not essential for the CUB dataset. Indeed, allowing more predictions results in a small improvement of localization accuracy at the cost of having false positive bounding boxes.

Contrary, on the ImageNet-1k dataset, the capability of FALcon to preemptively make multiple predictions adds a significant benefit. Even when the image-level labels describe a single object class per image, ImageNet-1k is a large and diverse dataset that can contain multiple objects in the image. As a result, the baseline PSOL method fails when there are multiple objects by attempting to find the optimal location that will capture all of them, as indicated by qualitative results shown in Figure 4(b). However, FALcon is not only able to focus on one of the individual ground truth bounding boxes but is also able to make multiple predictions for images containing multiple objects. While this allows the improvement over the baseline method, making the combined approach competitive with other methods, it also motivates an important concern of resiliency to the possibility of multi-object images. When compared with C\({}^{2}\)AM method, we believe that their high quantitative performance is attributed to positive and negative pairs used in contrastive learning, which can also be thought of as train-time intra-sample sampling. However, unlike foveation and saccades, which are part of the entire inference pipeline in FALcon, their technique is only used during training, still leaving the final deployed model vulnerable to the presence of multiple objects.

Note that, in this work, we focused on the localization model and its training on the different views generated by foveation and saccades. We did not consider fine-tuning the classification model on the resulting foveated regions. Instead, we applied the same classification model used by the baseline PSOL method with the aim of consistency. As a result, there is a slight degradation in Top-1 Loc accuracy compared to the baseline PSOL method. The reason is that, in contrast to all WSOL methods which process an entire image to predict the label, FALcon makes a classification prediction by only processing the portion of the input within the last foveated region. We believe this is tied to one of the fundamental debates about DNNs (Xiao et al., 2021), whether the background affects a DNN's classification capabilities. In the future work, this can possibly be mitigated (and potentially even improved) by fine-tuning the classification model on the sequence of foveated regions.

### FALcon Enabling Resiliency

In addition to qualitative results on ImageNet-1k, we also explicitly verified if foveation and saccades offer resiliency. Specifically, the localization models trained on the CUB and ImageNet-1k datasets by the baseline PSOL method and FALcon approach were applied to multi-object datasets. Models trained on "birds" of the CUB dataset were applied to Pascal VOC07 and Pascal VOC12 datasets and models trained on the ImageNet-1k dataset were applied to the ImageNet13-Det dataset. Table 2 illustrates the performance of detecting multiple objects while being trained only on the data containing a single object per image. While only being trained on a single bounding box per image, FALcon offers higher performance in detecting multiple objects than the baseline PSOL method. Qualitative results shown in Figure 5 demonstrate that foveation and saccades enable the capability to correctly make more than one prediction per image. This is especially noticeable in the case of detecting "birds" in Pascal VOC datasets. The less significant quantitative improvement in detecting ImageNet13-Det objects can be explained by the vast diversity of classes.

### Towards Neuro-inspired Algorithms

The addition of bio-plausible mechanisms of foveation and saccades was motivated by the idea of bridging the gap between computer vision and human perception. Hence, in order to demonstrate the neuro-inspiration, we applied FALcon model trained on ImageNet-1k to one variation of the famous "duck-rabbit" optical illusion. The interesting aspect of that optical illusion is that human vision allows us to only see one object: either the duck or the rabbit. Due to the nature of our vision, not only are we unable to see both objects at the same time, but we also ignore the features of the other object when focusing on the features of the first. Figure 6 illustrates the result of applying our neuro-inspired approach to this optical illusion. In agreement with human perception, FALcon observes **and** focuses on objects of different classes based on the different fixation points. This exciting finding

   &  &  \\ 
**Method** & Training & Testset (VOC07) & Testset (VOC12) & Training & Testset (ImageNet13-Det) \\  & Dataset & AP\({}_{00.5}\) & AP\({}_{00.5}\) & Dataset & mAP\({}_{00.5}\) \\  PSOL (baseline) &  &  &  &  &  \\ (Zhang et al., 2020) & & & & & \\ 
**FALcon + PSOL** &  &  &  &  &  \\ (**Ours**) & & & & & \\  

Table 2: Results of applying localization models trained on images containing a single object to datasets containing multiple objects per image

Figure 5: Qualitative results obtained on the (a) VOC07 and (b) ImageNet13-Det datasets.

motivates the exploration of neuro-inspired algorithms as a plausible research direction for the next generation of machine perception.

## 5 Discussions

Why F&S improves "pseudo" bounding boxes?Figure 7 shows examples of foveated regions that can be observed by FALcon during training. Figure 7(a)-(c) are foveated regions generated on the sample with the accurate "pseudo" bounding box, meaning that the underlying WSOL method provides the bounding box that matches the foveated region shown in Figure 7(b). Based on such "pseudo" bounding box, FALcon weights are reinforced for the correct expansion from foveated region shown in (a) to foveated region shown in (b), but punished for the wrong excessive expansion from (b) to (c). As a result, this knowledge is then transferred to improve samples with "pseudo" bounding boxes matching foveated regions shown in Figure 7(d) and Figure 7(e), with FALcon completing foveated region shown in (d) and tightening foveated region shown in (e).

Figure 6: Results of applying our neuro-inspired approach on one variation of the famous Duck and Rabbit optical illusion. Similar to human vision, FALcon observes **and** focuses on objects of different classes based on different fixation points.

Figure 7: Examples of foveated regions for samples with (a)-(c) accurate, (d) partial/incomplete, and (e) loose/oveextended “pseudo” bounding boxes.

Why consider resiliency and how it is achieved?The major motivation for WSOL works is to facilitate automated annotation system. The end goal of such systems would be to process images fetched directly from an unconstrained environment, like the internet, without extra human supervision. As such, it is important to consider the resiliency of such systems, i.e. their capability to either flag unexpected input or to detect multiple objects (even if trained only on data containing a single object).

The processing of the input with uniform resolution extracts all the features from the image at once, making it up to DNN to differentiate multiple objects, including those belonging to the same class. In contrast, the resiliency of FALcon is based on the inference of individual foveated regions originating from different fixation points within the input image. Consequently, FALcon extracts explicitly disentangled features, which allows to account for multiple objects.

Why foveation with extreme cutoff?In the context of neuroscience and human vision, foveation is a spatially varying attention within the visual field (Larson and Loschky, 2009; Loschky et al., 2019; Rosenholtz, 2016). As a result, previous works on foveation generally implement it using spatially varying operators, similarly to adaptive blur (Larson and Loschky, 2009; Wang and Cottrell, 2017; Pramod et al., 2022) and texture-based modifications (Rosenholtz, 2016; Deza et al., 2018). The application of such operators transforms the importance of different input bits, but still leaves the information available when DNNs process the entire input. Contrary, we wanted foveation to function as explicit hard attention within the input. This allows the processing of individual input regions, while enabling the model to ignore the rest of the image. Hence, the design choice was to implement foveation as extreme cutoff using cropping of the input regions and resizing to a fixed size.

## 6 Conclusion

Motivated by the idea to bridge the gap between human perception and computer vision, we explored the potential of the mechanisms of foveation and saccades to the task of object localization. The resulting active object localization framework, referred to as FALcon, models foveation as the method of observing isolated image regions and saccades as the method of switching fixation points. By allowing these mechanisms to explicitly sample diverse views from within each input, the proposed framework enriches the set of features otherwise limited only to the features obtained from the uniform processing of training images. The capabilities of FALcon were verified within the settings of weakly supervised object localization, where foveation and saccades balance out the weak supervision provided only in the form of a class label for each training image. Our experiments show that FALcon improves upon the localization of the baseline by completing and tightening the bounding box predictions and enables the localization to become resilient to multi object images. Finally, with foveation and saccades, FALcon was able to detect different possibilities stemming from a single duck-rabbit optical illusion example. Similar to humans, based on the initial fixation points, FALcon detects either the duck or the rabbit, but not both at the same time.