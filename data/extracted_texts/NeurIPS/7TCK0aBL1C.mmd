# IaC-Eval: A Code Generation Benchmark for

Cloud Infrastructure-as-Code Programs

 Patrick Tser Jern Kon, Jiachen Liu, Yiming Qiu, Weijun Fan, Ting He

**Lei Lin, Haoran Zhang, Owen M. Park, George S. Elengikal, Yuxin Kang**

**Ang Chen, Mosharaf Chowdhury, Myungjin Lee**\({}^{}\), Xinyu Wang**

University of Michigan \({}^{}\)Cisco Research

###### Abstract

Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval's dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user's intent; by making explicit the problem's requirements that can encompass various cloud services, resources and internal infrastructure details. Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.

## 1 Introduction

Cloud computing has become a cornerstone of our digital infrastructure. According to recent reports , 94% of all enterprises use cloud services of some form. Building on this trend, "Infrastructure as Code" (IaC) has emerged as the standard for developing cloud infrastructure. IaC allows users to codify their desired infrastructures within high-level IaC programs, which can be deployed repeatably and consistently. The IaC frameworks in turn are responsible for provisioning the underlying resources (e.g., compute instances) specified in the program, by interacting with cloud-specific APIs. The most widely adopted tool leading this paradigm is Terraform , and is the focus of our paper. Terraform programs (otherwise known as configuration files) are written in the HCL language ; Fig. 1a illustrates a simplified example of such a program, where we construct an infrastructure consisting of a compute instance (VM) connected to a network. First, we have resource blocks which define instances of specific infrastructure components. Cloud services typically encompass multiple resources, each representing a distinct part of the service, and the selection of these resources depends on the specific use case; here, we need a SUBNET and NIC, both essential parts of the virtual private cloud service, a key networking component. Second, these resource blocks contain various attributes that dictate their instantiation. Attributes can have differentvalue types, including strings, enum types (e.g., "US-west"), lists (the value of nic_ids), and even nested structures (the cpu_options attribute). Third, these resources can be interconnected through specific attributes, forming a dependency graph. In our example, the three resources are interlinked: the VM is connected to the NIC via the nic_ids attribute, while the NIC and SUBNET are connected through the subnet_id attribute.

Developing IaC programs is a challenging task. For one, cloud infrastructures can be built using a vast array of services offered uniquely by various providers (e.g., AWS, GCP), each with complex domain-specific details/rules frequently underspecified at the IaC level , requiring deep cloud expertise to get right. Second, this could involve learning a new language, as is the case for Terrorm, whose programs are specified in the feature-rich HCL language , which is a domain-specific language unfamiliar to most developers (e.g., as evidenced by the relative scarcity of IaC-specific code in public repositories compared to general-purpose languages ). Third, this complexity is further compounded by the growing diversity of cloud workloads incorporated within enterprises, extending beyond a few broad categories of Software-as-a-Service (SaaS) products , which in turn demands highly customized cloud infrastructures. Consequently, it is no surprise that 92% of enterprises employ extensive cloud engineering teams to manage this highly customized and intricate infrastructure .

We believe LLMs are a natural next step to aid in the process of creating infrastructure code. Indeed, LLMs have shown promise for general code generation, as exemplified by models such as Codex, CodeLlama , and AlphaCode . As a result, a surge of datasets and benchmarks have emerged, such as the widely used hand-crafted HumanEval  (Python programming), to quantify how well these models perform. Unfortunately, while existing models and datasets/benchmarks target general-purpose languages, the ability of existing LLMs to effectively generate IaC code remains uncertain, since there exist no systematic studies nor datasets/benchmarks available to quantitatively evaluate LLM performance on IaC code generation.

**IaC-Eval** bridges this gap with the first dataset and benchmark for evaluating IaC code generation. As a first step in this domain, our dataset (Sec. 2.2) specifically targets AWS, the most popular cloud provider . Our dataset includes 458 human-curated scenarios (comparable in size to the HumanEval dataset  which contains 164 programming problems) covering a variety of popular services, compiled over 1720 hours, ranging from simple to highly challenging scenarios that involve multiple resources across various services which can contain hundreds of lines of code (LoC). Each scenario consists mainly of a natural language problem description (e.g., "Create an AWS database") fed as user input to the LLM, and an infrastructure intent specification (Sec. 2.3) to check against the LLM generated program. This intent specification fulfils two key objectives:

_Objective 1: Determining user intent fulfilment._ The specification ensures that the generated IaC program conforms with the user's intent by explicitly detailing the problem's requirements, which can include various cloud services, resources, and internal infrastructure details. Crafting specifications is challenging, because just like regular programs: (1) User requirements for cloud infrastructure can be ambiguously specified. For instance, there are nearly a dozen ways to create an "AWS Database"  (e.g., DynamoDB, RDS). (2) Cloud infrastructure can also be complex. There are over 4,000 providers within the Terrorform ecosystem alone , each offering unique services composed of resources utilized in various ways. For example, AWS alone provides more than 200 services, each with multiple configuration options and interdependencies. These services can sometimes involve dozens of interlinked resources, with specific instantiation details governed by provider-specific rules that may or may not be publicized . Furthermore, with new services and updates continuously being introduced [23; 36], the landscape is constantly evolving, adding to the complexity. Overall, this means that the resulting specification for each problem description will codify a range of possible intent-fulfilling IaC programs, requiring deep cloud and IaC expertise to get right.

_Objective 2: Scalable evaluation._ Checking for intent fulfilment in IaC-Eval does not require executing the IaC programs, which typically involves deployment directly onto the cloud. Notwithstanding the fact that successful deployments are not an indication of correct fulfilment of user intention, this is problematic because deployments can take a significant amount of time--even simple configurations may require minutes to hours to deploy , making it an impractical evaluation strategy. Instead of waiting for a full deployment cycle (e.g., relying on knowledge from a state file  which will only be produced after a successful deployment), our evaluation benchmark relies only on compile-time operations that can be completed quickly and do not necessitate deployment (Sec. 2.1).

**Comprehensive evaluation.** Finally, we conduct a detailed evaluation (Sec. 3) against a range of state-of-the-art models known for their strong performance on the EvalPlus  benchmark (an improved version of HumanEval). Our results show that these models perform poorly on IaC-Eval, even with various enhancement strategies.

## 2 The IaC-Eval benchmark and dataset

### IaC-Eval benchmark: a high-level overview

We first describe our evaluation benchmark's overall workflow, illustrated in Fig.1b. Notably, we combined existing components natively supported by IaC (to benefit from continuous updates and robust support) into a novel benchmark for evaluating IaC code generation. 1 First, a problem description is retrieved from the dataset (Sec. 2.2) and fed to the LLM under evaluation. 2 The resulting LLM-generated IaC program is then fed into a two-phase pipeline that determines if the program is correct (or incorrect) without requiring deployment. 3 In the first phase, the generated IaC program is transformed into a speculative deployment plan using the native terraform plan command, which produces a dependency graph. This phase includes basic validations to ensure syntactical accuracy and adherence to a limited set of cloud provider-specific requirements. Configurations failing this step are deemed incorrect by IaC-Eval. 4 Next, the problem's user infrastructure intent specification (Sec.2.3) written in the Rego language  is 5 matched against the dependency graph using OPA, the most widely used IaC policy engine; a tool typically used to define policies (e.g., security, conformance), that IaC-Eval repurposes to evaluate user intent fulfilment. If no errors are found, the IaC program is considered correct.

Figure 1: IaC-Eval evaluation benchmark overview, and example IaC program.

Figure 2: IaC-Eval dataset service and difficulty distribution.

### Dataset characteristics

IaC-Eval's dataset is structured with each row consisting of three key columns: (1) a natural language prompt describing the problem, (2) user intent specifications written in Rego (Sec. 2.3), and (3) one example of a correct configuration written in Terraform HCL , which we envision could be useful in the future for fine-tuning purposes. Creating this dataset requires significant domain knowledge and a deep understanding of cloud services and the intricacies of cloud infrastructure orchestration. It also demands proficiency in two languages markedly different from regular imperative languages: namely, HCL and Rego, which are complex declarative languages unfamiliar to regular developers.

Fig. 2 illustrates that our dataset addresses a comprehensive set of commonly used cloud infrastructure services [64; 56; 39], where each problem count is represented by some resource in a given service. This includes a full stack of infrastructure components typically required in cloud environments: (1) Compute services: EC2, Lambda, and Lightsail; (2) Relational and in-memory databases: RDS and ElastiCache; (3) Data warehouses: Redshift; (4) Networking elements: virtual private cloud (VPC), Route53 (DNS service), and API gateways; (5) Content delivery networks: CloudFront; (6) Key-value stores: DynamoDB; (7) Object storage: S3; (8) Security: identity and access management (IAM), and monitoring service (Cloudwatch); (9) Stream processing: Kinesis, and managed Kafka (MSK). We note that certain services (in particular, VPC and IAM) are often required across problems, and hence appear disproportionately in the dataset; in contrast, certain other services aren't composed of many distinct resources, and hence appear to have fewer counts in the dataset. Services are often interconnected in various configurations, requiring multiple services to address a single problem (e.g., an RDS instance deployed in a specific VPC). All of these services are also made up of a diverse set of lower-level resources that control more fine-grained functionality: for instance, as shown in Fig. 3(a), within the Aurora service, we could instantiate an Aurora DB cluster resource  (deployed across multiple regions), that utilizes a DB proxy  to pool/share database connections. Furthermore, each of these resources can be configured in unique ways through various default and optional attributes (e.g., setting an idle timeout duration for the DB proxy).

Finally, we introduce a system of difficulty levels for IaC problems. We recognize that determining these levels is inherently ambiguous and subjective, akin to the informal designations used by online programming platforms (e.g., LeetCode ) and in existing research . Nevertheless, we propose an approximation that can calculate a difficulty level automatically by parsing the configuration, that is based on LoC, the number of resources, and their interconnections in the desired configuration (Appendix. A.4). The inset figure within Fig. 2 shows a full spectrum of difficulty levels found within our dataset, with over half of our dataset consisting of configurations with more than 4 interconnections, 4 resources, and over 42 LOC, and the longest configurations having either 280 LOC, 24 resources or 33 interconnections.

### Infrastructure intent specifications

Just like regular coding problems, infrastructure problems can often be specified ambiguously, where the user's intention is not always clearly defined. To address this, we use OPA Rego to encode a range of possible correct configurations for a given problem, creating an infrastructure intent specification crafted by a human expert. This step is crucial because functional correctness processes alone, such as ensuring configurations can be compiled, cannot resolve ambiguities in the user's intent. Intent specs in our dataset vary in length, averaging 37.5 LOC, and the longest spec containing 205 LOC.

In general, the specification will contain three categories of intents (i.e., clarifying three sources of ambiguity): (1) valid resources, (2) optional attributes, and (3) required attributes. The valid resources specify their dependencies and the number of resources allowed. The optional attributes specify their existence, and/or the range of acceptable values whereas the required attributes mandate the range of acceptable values. Note that anything not included in the specification is considered incorrect. This is showcased via an example in Fig. 3(b): For example, for valid resources, the intent spec contains a validation block (is_valid_aws_db_proxy) that checks if an aws_db_proxy resource exists in the LLM-generated config. Further, specifying a connection timeout limit for an aws_db_proxy is optional according to cloud-provider guidelines; however, since it is explicitly specified in the prompt, the intent will enforce its value to be between 1800 and 3600 (while inferring that this is specified in seconds). As another example, the intent infers that daily DB backups are equivalent to simply including a preferred_backup_window attribute, with the example timing of morning backups being irrelevant. For required attributes, the engine_family attribute within the proxy isconstrained by the user prompt to only use a subset of valid cloud-provider-defined values. We do not need to verify the existence of required attributes, as these are automatically validated during the compilation phase.

We emphasize that crafting the initial batch of dataset intents is challenging, but there is significant reuse potential across multiple configurations. The intents we have created can serve as templates (e.g., a validation block for a given resource), allowing for easier adaptation with minor adjustments. Moreover, problems often imply the existence of many interlinked required or optional resources each with their own attributes. For instance, creating an EC2 instance not only requires an attached AWS network interface card but also depends on a VPC, which in turn requires subnets and route tables. This chain of dependencies can extend even further, including security groups, IAM roles, and policies. Since manually specifying all these details at this stage is impractical, we leave some aspects under-specified, similar to the approach seen in HumanEval . An example is provided in Fig. 3(a), where an IAM role, which governs the actions executable by the DB proxy, is attached to the proxy via the role_arn attribute. The actual IAM role resource block itself is omitted for brevity. The role_arn is a required attribute of the resource, and its presence in a correct configuration is therefore implied (not explicitly stated) in the prompt. To simplify crafting intents, we only verify the existence of the aws_iam_role resource block and ensure that its policy is not empty; we do not check its content. With this base dataset, we envision a move towards the automatic synthesis of intents (Sec. 4) to facilitate the expansion of our dataset with increased service/provider coverage.

## 3 Experiments

### Evaluating LLMs performance on IaC-Eval

We evaluate a range of code generation models used within the popular HumanEval  and EvalPlus  Python code benchmarks, against IaC-Eval. These include the top-ranked GPT4, WizardCoder  and Magicoder  models, and CodeLlama  variants. Inference was performed using OpenAI APIs for GPT-4 and GPT-3.5, and Replicate  endpoints for all the other models, except for Magicoder, which was deployed on a g5.2xlarge instance running an NVIDIA A10G GPU (24 GB memory) via AWS SageMaker. We use the unbiased version of pass@_k_, a standard metric also used in the aforementioned benchmarks, to assess correctness by generating 20 samples for each problem. This method gives us the probability that at least one out of \(k\) chosen samples among the 20 is correct. The IaC-Eval column of Table 1 showcases our results where scores are tabulated in terms of pass@1 accuracy: our top-ranked models are GPT-4, WizardCoder-33B-V1.1, and GPT3.5, achieving a score of 19.36%, 8.93%, and 7.99%, respectively. Our worst-performing model was CodeLlama instruct (7B) which scored 1.97%. This is in stark contrast to these models' performance on EvalPlus, where they scored 86.6%, 73.2%, 70.7%, and 35.4%, respectively; highlighting the

Figure 3: IaC-Eval dataset row simplified snippet.

difficulty of our dataset , and demonstrating that current models are ineffective at generating IaC code. Further, extended results containing pass@\(k\) scores for all \(k\) values are showcased for the Top-6 performing models in Fig. 3(a). We observe improvements across all models with increasing \(k\) values, except for Magicoder; upon closer inspection, we found that this is because Magicoder tends to get correct answers for a narrow set of problems consistently, where generating more samples does not yield significant accuracy improvements. Finally, we showcase an example incorrect generated IaC program in Fig. 6, demonstrating that even GPT-4 can exhibit issues such as hallucinating entire service components.

**Impact of intents and difficulty on evaluation scores.** To examine the utility of our benchmark's two-step pipeline, we compare the accuracy (Fig. 4(a)) when only Terraform compilation checks are used (blue hatched bars), and when both compilation and intent checks are used: across all models, intent specifications helped remove over \(\)50% of false positives, i.e., programs deemed correct by the Terraform compilation phase but are actually incorrect as they do not satisfy user intent. Separately, our results in Fig. 4(b) show that complex IaC-Eval problems are harder for models to solve, demonstrating the efficacy of our system of difficulty levels.

### Comparing IaC-Eval against baseline metrics

We compare IaC-Eval against three existing metrics with results tabulated in Table. 1:

**BLEU and CodeBERTScore.** Both are widely utilized to assess the quality of machine-produced translations [26; 73] by determining how similar some generated text is to a reference text; unlike BLEU  which is general purpose, CodeBERTScore  is specifically designed for code generation. Both produce scores ranging from 0 to 1, with higher values indicating better translation quality. Our results are displayed as percentages: though a downward trend in scores is observed for

   &  &  \\ Rank & Name & BLEU & CodeBERTScore & LLM-judge & IaC-Eval \\ 
1 & GPT-4 & 18.49 & 83.39 & 61.79 & 19.36 \\
2 & WizardCoder-33B-V1.1 & 15.22 & 80.50 & 28.72 & 8.93 \\
3 & GPT-3.5-turbo & 14.52 & 77.26 & 34.49 & 7.99 \\
4 & Magicoder-S-CL-7B & 14.22 & 79.49 & 23.14 & 7.62 \\
5 & Gemini 1.0 Pro & 11.96 & 78.90 & 19.72 & 3.43 \\
6 & CodeLlama Instruct (34B) & 11.47 & 78.64 & 11.97 & 2.99 \\
7 & CodeLlama Instruct (13B) & 11.18 & 76.46 & 9.83 & 2.01 \\
8 & CodeLlama Instruct (7B) & 9.31 & 70.22 & 7.18 & 1.97 \\  

Table 1: Average benchmark scores for various models when tested against various evaluation metrics. Popular LLMs perform poorly on IaC-Eval, showcasing its difficulty.

Figure 4: Multi-sample generation improves scores. LLM-judge decisions are unreliable.

both metrics, consistent with IaC-Eval, a deeper analysis reveals that these metrics cannot effectively differentiate between correct and incorrect solutions (as judged by IaC-Eval), as they frequently assign similar scores to both, with an average difference of only 6.9% for CodeBERTScore and 8.3% for BLEU. This finding is consistent with existing studies , and motivates the use of functional correctness metrics instead.

**LLM-as-a-judge.** We also assess our models using the well-known LLM-as-a-judge metric, as referenced in previous studies . For this evaluation, we employ GPT-4, the top-performing code model, as the judge. The downward trend in scores suggests that the LLM-judge has some intuition. However, Figure 3(b) reveals that its precision is low (hovering between 10% and 30%), indicating a high rate of misclassification where incorrect solutions are frequently judged as correct.

### Assessing common enhancement strategies

**Few-shot and chain-of-thought.** These prompting strategies are commonly used in code generation tasks . Few-shot prompting involves providing a few examples to guide the model. The specific prompt used in our experiments is detailed in Appendix. B.1, where we employ three-shot prompting . Chain-of-thought reasoning (CoT) guides the model through a step-by-step process to arrive at a solution, mimicking human logical progression. In our approach, we ask the model to reason through the overall structure of the desired infrastructure, covering services, resources, and individual attributes (Appendix. B.2). Our findings indicate that these methods yield unreliable improvements, sometimes even degrading performance. For instance, both strategies reduced GPT-4's score from 19.36% to 10.64% and 9.31%, respectively. This is likely due to the extensive service diversity within cloud infrastructure, limiting the effectiveness of these strategies.

**Multi-turn.** This is a prompting strategy widely used in existing work . Here we assume the model has access to an interpreter to evaluate the generation; errors within the compilation phase are passed back to the LLM as additional context for program repair. Our setup involves two turns of interaction between the model and the interpreter. We observe a noticeable improvement across our Top-4 models (5.5% on average), and a slight decrease in performance for our remaining models (0.92% on average). The specific prompt used is detailed in Appendix. B.3.

**Retrieval-augmented generation (RAG) .** This approach leverages a vector database  that stores relevant IaC documentation  for contextual retrieval. Given the problem input, the LLM first helps identify key terms and contextual cues to retrieve relevant documentation and code snippets from our database. The LLM then uses the retrieved documentation snippets as context to generate the final IaC code. This approach allows the LLM to access examples of similar code snippets, API documentation, or coding patterns pertinent to the problem, enabling more accurate and contextually appropriate generation. We observe a noticeable improvement across all models, with an average increase of 6.14%, making this the most effective strategy we've evaluated. Our prompt used is shown in Appendix. B.4.

Overall, these methods may provide modest improvements, but we hope the introduction of IaC-Eval will catalyze further research specifically tailored to improving IaC code generation.

Figure 5: Ablation: effectiveness of intent specifications and complexity levels.

## 4 Discussion

**Future directions.** One approach is to develop more intricate evaluation metrics to address specific issues like security breaches, policy adherence, and code quality aspects such as brevity, clarity, and style . The aim could be to demonstrate that existing models lack "security-awareness" and could lead to problems, such as: (1) vulnerability to adversarial information extraction attacks, where attackers can extract IaC-specific secrets from LLMs , or (2) failure to follow security best practices , potentially causing naive users to push unsafe code into production. Second, given our base dataset, we could expand it by implementing automated dataset creation for fine-tuning or evaluation purposes . Third, advanced compile-time checks are an existing line of work  that we could integrate to increase our coverage. Fourth, IaC-Eval currently focuses on Terraform, the most popular IaC tool, but integrating other frameworks such as AWS CloudFormation and Pulumi are planned for future work (e.g., since they share similar intermediate representations as Terraform ). Finally, in terms of improving IaC code generation, one potential avenue is utilizing two-level synthesis or context injection, which could tailor the code more accurately to specific contexts, enhancing personalization and precision.

**Limitations.** There are several avenues for expansion. IaC-Eval currently does not include function generation, although supported by HCL. Additionally, IaC-Eval focuses on AWS, but we plan to extend support to other cloud providers like Azure. Despite these limitations, the core concepts of

   &  &  \\ Rank & Name & Few-shot & CoT & Multi-turn & RAG \\ 
1 & GPT-4 & 10.64 & 9.31 & 31.12 & 36.70 \\
2 & GPT-3.5-turbo & 0.80 & 1.60 & 11.44 & 21.81 \\
3 & Magicoder-S-CL-7B & 2.93 & 0.53 & 12.50 & 12.77 \\
4 & WizardCoder-33B-V1.1 & 1.60 & 1.06 & 9.04 & 11.70 \\
5 & CodeLlama Instruct (34B) & 3.19 & 3.19 & 2.13 & 6.12 \\
6 & CodeLlama Instruct (7B) & 2.39 & 3.72 & 0.53 & 5.59 \\
7 & Gemini 1.0 Pro & 1.33 & 0.00 & 2.93 & 5.32 \\
8 & CodeLlama Instruct (13B) & 1.06 & 1.86 & 1.06 & 3.46 \\  

Table 2: Average benchmark scores of various models when enhanced with differing strategies, ranked by highest score across all strategies. Evaluation performed against IaC-Eval. Performance is slightly improved across the board, especially with multi-turn and RAG.

Figure 6: Example errors from a GPT-4 generated configuration.

our benchmark (e.g., intent specifications), are applicable across other cloud platforms such as Azure and Google Cloud because IaC is built to be cloud-provider agnostic. As this is the first benchmark and dataset focused on IaC, we chose AWS as it is the most popular cloud provider, and is broadly adopted by the community. Our dataset focuses on AWS services because creating these datasets is time-intensive and requires an in-depth understanding of each cloud service in concern, especially since we cover a wide range of services and include questions of varying difficulty levels.

**Data collection and annotation process.** Our team learned from official AWS and IaC documentation, and other learning materials (including a mixture of public IaC repositories within GitHub, and IaC-related StackOverflow/Reddit posts). All configurations were tested to ensure correctness. Additionally, our team includes experts who have been using IaC for years. To maintain high quality, we have implemented channels (GitHub/HuggingFace issue trackers) for the community to report any issues/errors, which we will promptly address-this is explicitly mentioned in our supplementary material and repositories. Finally, we also welcome contributions from the community. More details are available within Section C3 of our supplementary material.

**Potential societal impact.** We do not foresee any potential negative societal impacts arising from our work on IaC-Eval. While IaC-Eval does evaluate LLM-generated IaC programs, it's important to note that these evaluations are limited to compile-time operations and do not involve deploying the programs into the cloud. This approach ensures that users need not worry about any potential impact on their cloud infrastructure. Additionally, we have conducted thorough manual reviews of our dataset to confirm that it is safe for execution, posing no risk to either cloud providers or users. Moreover, the composition, collection process, and other relevant details of our dataset are provided in our supplementary material, which adheres to the "datasheets for datasets" format to ensure transparency and accountability.

## 5 Related work

**LLMs for code.** LLMs have shown impressive performance in code generation (and its subtasks including code translation [42; 61] and program repair ) for general-purpose programming languages, where a model is typically provided with natural language descriptions of the desired functionality and asked to generate the corresponding code snippet. Models include OpenAI's ChatGPT/GPT-4, Codex , CodeLlama , and WizardCoder . Our work aims to determine, for the first time, whether these models can also perform well in generating IaC programs (Sec. 3).

**Code evaluation metrics for LLMs.** Text similarity metrics (e.g., BLEU ) and human evaluation  do not work well for evaluating general-purpose programming languages for various reasons (e.g., minor dissimilarities can result in significant problems/errors , while human evaluation is not scalable). LLM code generation is instead commonly evaluated based on functional correctness using the pass@_k_ metric, which makes use of input-output test cases: a code segment is considered correct if it passes all such test cases. IaC-Eval focuses on functional correctness as well, but applies this to a cloud infrastructure scenario using intent specifications.

**Coding benchmarks for LLMs.** These include the widely studied HumanEval  benchmark consisting of 164 hand-crafted programming problems, and the MBPP  (1K programming problems) benchmark, built for evaluating Python code generation. Benchmarks targeting other languages include Spider  (SQL), CodeContests  (C++ and Java). CloudEval-YAML  is closely related to our work as it focuses on the cloud domain; it provides a dataset for specific cloud applications (i.e., Kubernetes, Envoy, and Istio) composed of simple YAML configurations. IaC-Eval instead focuses on cloud infrastructure, a vastly different and more complex domain, and introduces the first IaC dataset consisting of 458 hand-crafted problems, and a unique evaluation pipeline (Sec. 2).

**IaC frameworks.** IaC frameworks simplify cloud infrastructure development by codifying resources to ensure consistency and repeatability. Similar paradigms exist, such as Kubernetes, a container orchestration platform which abstracts away the complexities of managing containers, and serverless computing, which allows developers to run/deploy applications without server management. Apart from Terraform, other cloud-agnostic IaC frameworks include Pulumi , OpenTofu , and Crossplane , as well as cloud-specific tools such as Azure Bicep and AWS CloudFormation. Preceding these frameworks, we have Ansible , Chef , and Puppet , which are mainly used for in-system configuration and automation of software/application setups, on individual systemsthat may be part of a larger network/computing infrastructure. Cloud infrastructure can also be provisioned using non-IaC cloud-level APIs as well (e.g., through SDKs such as AWS Boto3), as is done in some existing works , though this is a much harder process to navigate .

**IaC tools.** There are a broad range of industry tools designed to enhance specific aspects of IaC management, such as security checkers (e.g., TFSec , Terrascan  and KICS ), and drift detection tools (e.g., driftctl ). Additionally, various policy engines such as OPA Rego , the HashiCorp Sentinel framework , and Cloud Custodian  help developers define and enforce compliance rules within IaC programs. Recently, we have also seen the gradual introduction of LLMs for IaC program generation, with tools like Pulumi AI , Firefly , and InfraCopilot . Finally, academic projects have focused on directions such as enhanced IaC program validation , the cloudless computing vision project , fault analysis , and vulnerability detection .

## 6 Conclusion

We introduce IaC-Eval, the first dataset and benchmark capable of evaluating IaC code generation by LLMs. IaC-Eval comprises 458 human-curated scenarios that cover a diverse range of popular AWS services and varying difficulty levels. Our evaluation reveals that current LLMs, including GPT-4, perform poorly on IaC-Eval, with a pass@1 accuracy of 19.36%, compared to 86.6% on the EvalPlus Python benchmark. This underscores the need for advancements in LLM-based IaC code generation. We open-source IaC-Eval to facilitate future research in this domain.

## 7 Acknowledgments

We thank the anonymous reviewers for their insightful feedback. This work was partially supported by a Cisco Research Grant, AWS Cloud Research Credits, a VMware Early Career Faculty Grant, as well as NSF grants CNS-1942219, CNS-2106751, CNS-2106388, and CNS-2214272.