# Proportional Fairness in

Non-Centroid Clustering

 Ioannis Caragiannis

Aarhus University

iannis@cs.au.dk

&Evi Micha

Harvard University

emicha@seas.harvard.edu

&Nisarg Shah

University of Toronto

nisarg@cs.toronto.edu

###### Abstract

We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to _non-centroid clustering_, where the loss of an agent is a function of the other agents in its cluster, by adapting two proportional fairness criteria -- the core and its relaxation, fully justified representation (FJR) -- to this setting.

We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering , is unappealing for a natural loss function. In contrast, we design a new (inefficient) algorithm, GreedyCohesiveClustering, which achieves the relaxation FJR exactly under arbitrary loss functions, and show that the efficient GreedyCapture algorithm achieves a constant approximation of FJR. We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor. Our experiments on real data suggest that traditional clustering algorithms are highly unfair, whereas GreedyCapture is considerably fairer and incurs only a modest loss in common clustering objectives.

## 1 Introduction

Clustering is a fundamental task in unsupervised learning, where the goal is to partition a set of \(n\) points into \(k\) clusters \(C=(C_{1},,C_{k})\) in such a way that points within the same cluster are close to each other (measured by a distance function \(d\)) and points in different clusters are far from each other. This goal is materialized through a variety of objective functions, the most popular of which is the \(k\)-means objective: \(_{i=1}^{k}|}_{x,y C_{i}}d(x,y)^{2}\).

When the points are in a Euclidean space, the \(k\)-means objective can be rewritten as \(_{i=1}^{k}_{x C_{i}}d(x,_{i})^{2}\), where \(_{i}=|}_{x C_{i}}x\) is the mean (also called the _centroid_) of cluster \(C_{i}\).1 This gives rise to centroid clustering, where deciding where to place the \(k\) cluster centers is viewed as the task and the clusters are implicitly formed when each point is assigned to its nearest cluster center.

In the literature on fairness in centroid clustering, the loss of each data point (hereinafter, agent) is defined as the distance to the nearest cluster center ; here, the cluster centers do not merely help rewrite the objective, but play an essential role. This is a reasonable model for applications such as facility location, where the loss of an agent indeed depends on how much they have to travel to get to the nearest facility.

But in other applications of clustering, we simply partition the agents and agents prefer to be close to other agents in their cluster--there are no "cluster centers" that they prefer to be close to. For example, in clustered federated learning , the goal is to cluster the agents and have agents in each cluster collaboratively learn a model; naturally, agents would want other agents in their cluster to have similar data distributions, so the model learned is accurate on their own data distribution.2 Other examples where we want to cluster nearby points together without defining cluster centers include document clustering , image segmentation for biomedical applications , and social network segmentation .

While there exist plenty of clustering objectives which do not require defining cluster centers (such as the first formulation of the \(k\)-means objective above), in order to reason about fairness we need to define the loss of each agent under a non-centroid clustering and explore the tradeoff between the losses of different agents. We initiate the study of proportional fairness in non-centroid clustering.

We follow the idea of proportional fairness outlined in a recent line of work , which ensures that no group of at least \(}{{k}}\) agents should "improve" (formalized later) by forming a cluster of its own.3 Our main research questions are:

_Can we obtain compelling proportional fairness guarantees for non-centroid clustering as with centroid clustering? Do the algorithms known to work well for centroid clustering also work well for non-centroid clustering? Can we audit the proportional fairness of a given algorithm?_

### Our Contributions

In non-centroid clustering, we are given a set \(N\) of \(n\) points (agents) and the desired number of clusters \(k\). The goal is to partition the agents into (at most) \(k\) clusters \(C=(C_{1},,C_{k})\). Each agent \(i\) has a loss function \(_{i}\), and her loss under clustering \(C\) is \(_{i}(C(i))\), where \(C(i)\) denotes the cluster containing her. We study both the general case where the loss functions of the agents can be arbitrary, and structured cases where the loss of an agent for a cluster is the average or maximum of her distances -- according to a given distance metric -- to the agents in the cluster. In the latter case, our theoretical results hold for general metric spaces, as they rely solely on the satisfaction of the triangle inequality.

We study two proportional fairness guarantees, formally defined in Section 2: _the core_ and its relaxation, _fully justified representation_ (FJR) . Both have been studied for centroid clustering , but we are the first to study them in non-centroid clustering.

A summary of a selection of our results is presented in Table 1, with the cell values indicating approximation ratios (lower is better, 1 is optimal).

 
**Loss Functions** & **Core UB** & **Core LB** & **FJR** \\   Arbitrary & \(\) & & 1 \\  Average & \(O(}{{k}})\) (polytime) & 1.3 & 1 (4 in polytime) \\  Maximum & 2 (polytime) & 1 & 1 (2 in polytime) \\  

Table 1: The feasible core and FJR approximation guarantees, both existentially and in polynomial time. In each case, we can obtain a better FJR approximation than the core approximation.

Our results show the promise of FJR: while it is a slight relaxation of the core, it is satisfiable even under arbitrary loss functions, whereas the core can be unsatisfiable even under more structured loss functions. The existential result for FJR is achieved using a simple (but inefficient) algorithm we design, GreedyCohesiveClustering, which is an adaptation of the Greedy Cohesive Rule from social choice theory . The core approximations as well as efficient FJR approximations are achieved using an efficient version of it, which turns out to be an adaptation of the GreedyCapture algorithm that has been introduced for centroid clustering . We show that the FJR approximation achieved by GreedyCapture stems from the fact that its key subroutine achieves a constant approximation of that of the GreedyCohesiveClustering algorithm.

Next, we turn to auditing the FJR approximation of a given clustering. Surprisingly, we show that the same technique that we use to algorithmically _achieve a constant approximation of FJR_ can be used to also _estimate the FJR approximation of any given clustering_, up to a constant factor (4 for the average loss and 2 for the maximum loss).

We compare GreedyCapture to popular clustering methods, \(k\)-means++ and \(k\)-medoids, on three real datasets. We observe that in terms of both average and maximum loss, GreedyCapture provides significantly better approximations to both FJR and the core, and this fairness advantage comes at only a modest cost in terms of traditional clustering objectives, including those that \(k\)-means++ and \(k\)-medoids are designed to optimize.

### Related Work

In recent years, there has been an active line of research related to fairness in clustering . With a few exceptions, most of the work focuses on centroid-based clustering, where each agent cares about their distance from the closest cluster center. Mostly related to ours is the work by Chen et al. , who introduced the idea of proportionality through the core in centroid clustering. Their work has been revisited by Micha and Shah  for specific metric spaces. More recently, Aziz et al.  also introduced the relaxation of the core, fully justified representation, in centroid-based clustering. While one of our main algorithms, GreedyCapture, is a natural adaptation of the main algorithm used in all these works, there are significant differences between the two settings.

First, in centroid-based clustering, GreedyCapture provides a constant approximation to the core, while in the non-centroid case this approximation is not better than \(O(n/k)\) for the average loss function. Second, in centroid-based clustering, GreedyCapture returns a solution that satisfies FJR exactly. Here, for the non-centroid case, even though we know that an exact FJR solution always exists, GreedyCapture is shown to just provide an approximation better than 4 for the average loss and 2 for the maximum loss. In more specific metric spaces, Micha and Shah  show that a solution in the core always exists in the line. Here, we demonstrate that while this remains true for the maximum loss, it is not the case for the average loss, where the core can be empty. Finally, Chen et al.  conducted experiments using real data in which \(k\)-means++ performs better than GreedyCapture. However, for the same datasets, we found that GreedyCapture significantly outperforms \(k\)-means++ in the non-centroid setting.

Fairness in non-centroid clustering has received significantly less attention. Ahmadi et al.  recently introduced a notion of individual stability which indicates that no agent should prefer another cluster over the one they have been assigned to. Micha and Shah  studied the core when the goal is to create a balanced clustering (i.e. all clusters have almost equal size) and the agents have positive utilities for other agents. More generally, the hedonic games literature (e.g., see  for an early survey on the topic and  for a recent model that is close to the current paper) is also relevant to non-centroid clustering as it examines coalition formation. While the core concept has been extensively studied in hedonic games, there are two main differences with our work. First, subsets of any size can deviate to form their own cluster, rather than only proportionally eligible ones, and second, no approximate guarantees to the core have been provided, to the best of our knowledge.

## 2 Model

For \(t\), let \([t]\{1,,t\}\). We are given a set \(N\) of \(n\) agents, and the desired number of clusters \(k\). Each agent \(i N\) has an associated _loss function_\(_{i}:2^{N} 2^{N\{i\}}_{ 0}\), where \(_{i}(S)\) is the cost to agent \(i\) for being part of group \(S\). A \(k\)-clustering4\(C=(C_{1},,C_{k})\) is a partition of \(N\) into \(k\) clusters,5 where \(C_{t} C_{t^{}}=\) for \(t t^{}\) and \(_{t=1}^{k}C_{t}=N\). With slight abuse of notation, denote by \(C(i)\) the cluster that contains agent \(i\). Then, the loss of agent \(i\) under this clustering is \(_{i}(C(i))\).

**Loss functions.** We study three classes of loss functions; for each class, we seek fairness guarantees that hold for any loss functions the agents may have from that class. A distance metric over \(N\) is given by \(d:N N_{ 0}\), which satisfies: (i) \(d(i,i)=0\) for all \(i N\), (ii) \(d(i,j)=d(j,i)\) for all \(i,j N\), and (iii) \(d(i,j) d(i,k)+d(k,j)\) for all \(i,j,k N\) (triangle inequality).

* _Arbitrary losses._ In this most general class, the loss \(_{i}(S)\) can be an arbitrary non-negative number for each agent \(i N\) and cluster \(S i\).
* _Average loss._ Here, we are given a distance metric \(d\) over \(N\), and \(_{i}(S)=_{j S}d(i,j)\) for each agent \(i N\) and cluster \(S i\). Informally, agent \(i\) prefers the agents in her cluster to be close to her on average.
* _Maximum loss._ Again, we are given a distance metric \(d\) over \(N\), and \(_{i}(S)=_{j S}d(i,j)\) for each agent \(i N\) and cluster \(S i\). Informally, agent \(i\) prefers that no agent in her cluster to be too far from her.

## 3 Core

Perhaps the most widely recognized proportional fairness guarantee is _the core_. Informally, an outcome is in the core if no group of agents \(S N\) can choose another (partial) outcome that (i) they are entitled to choose based on their proportion of the whole population (\({}^{|S|/|N|}\)), and (ii) makes every member of group \(S\) happier. The core was proposed and widely studied in the resource allocation literature from microeconomics , and it has been adapted recently to centroid clustering . When forming \(k\) clusters out of \(n\) agents, a group of agents \(S\) is deemed worthy of forming a cluster of its own if and only if \(|S|}{{k}}\). In centroid clustering, such a group can choose any location for its cluster center. In the following adaptation to non-centroid clustering, no such consideration is required.

**Definition 1** (\(\)-Core).: For \( 1\), a \(k\)-clustering \(C=(C_{1},,C_{k})\) is said to be in the \(\)-core if there is no group of agents \(S N\) with \(|S|}{{k}}\) such that \(_{i}(S)<_{i}(C(i))\) for all \(i S\). We refer to the \(1\)-core simply as the core.

Given a clustering \(C\), if there exists a group \(S\) that demonstrates a violation of the \(\)-core guarantee, i.e., \(S\) has size at least \(}{{k}}\) and the loss of each \(i S\) for \(S\) is lower than \(}{{}}\) of her own loss under \(C\), we say that \(S\)_deviates_ under \(C\) and refer to it as the _deviating coalition_. We begin by proving a simple result that no finite approximation of the core can be guaranteed for arbitrary losses.

**Theorem 1**.: _For arbitrary losses, there exists an instance in which no \(\)-core clustering exists for any finite \(\)._

Next, for the more structured average loss function, we prove that the core can still be empty, albeit there is now room for a finite approximation. The proof, with an intricate construction, is delegated to Appendix A.

**Theorem 2**.: _For the average loss, there exists an instance in which no \(\)-core clustering exists for \(<}{2} 1.366\)._

To complement Theorems 1 and 2, we show the existence of a clustering in the \(O(}{{k}})\)-core (resp., \(2\)-core) for the average (resp., maximum) loss. Despite significant effort, we are unableto determine whether the core is always non-empty for the maximum loss, or whether a constant approximation of the core can be guaranteed for the average loss, which we leave as tantalizing open questions.

**Open Question 1:** For the maximum loss, does there always exist a clustering in the core?

**Open Question 2:** For the average loss, does there always exist a clustering in the \(\)-core for some constant \(\)?

**Our algorithms.** For the positive result, we design a simple greedy algorithm, GreedyCohesiveClustering (Algorithm 1). It uses a subroutine \(\), which, given a subset of agents \(N^{} N\), metric \(d\), and threshold \(\), finds a "cohesive" cluster \(S\). Here, the term "cohesive" is informally used, but we will see a formalization in the next section. The threshold \(\) is meant to indicate the smallest size at which a group of agents deserve to form a cluster, but \(\) can return a cluster of size greater, equal, or less than \(\).

The algorithm we use as \(\) in this section is given as SmallAgentBall (Algorithm 2). It finds the smallest ball centered at agent that captures at least \(\) agents, and returns a set of \(\) agents from this ball. We call this algorithm with the natural choice of \(=}{{k}}\), so GreedyCohesiveClustering(SmallestAgentBall) iteratively finds the smallest agent-centered ball containing \(}{{k}}\) agents and removes \(}{{k}}\) in that ball, until fewer than \(}{{k}}\) agents remain, at which point all remaining agents are put into one cluster and any remaining clusters are left empty.

``` Input: Subset of agents \(N^{} N\), metric \(d\), threshold integer \(\) Output: Cluster \(S\) if\(|N^{}|\)thenreturn\(S N^{}\); for\(i N^{}\)do \(_{i}\)-th closest agent in \(N^{}\) to agent \(i\); // Ties are broken arbitrarily \(r_{i} d(i,_{i})\); // Smallest ball centered at agent \(i\) capturing at least \(\) agents end if \(i^{*}*{arg\,min}_{i N^{}}r_{i}\); return\(S\) the set of \(\) closest agents in \(N^{}\) to agent \(i^{*}\); ```

**ALGORITHM 1**GreedyCohesiveClustering(\(\))

Overall, GreedyCohesiveClustering(SmallestAgentBall) is an adaptation of the GreedyCapture algorithm proposed by Chen et al.  for centroid clustering with two key differences in our non-centroid case: (i) while they grow balls centered at feasible cluster center locations, we grow balls centered at the agents, and (ii) while they continue to grow a ball that already captured \(}{{k}}\) agents (and any agents captured by this ball in the future are placed in the same cluster), we stop a ball as soon as it captures \(}{{k}}\) agents, which is necessary in our non-centroid case.6 Nonetheless, due to its significant resemblance, we refer to the particular instantiation GreedyCohesiveClustering(SmallestAgentBall) as GreedyCapture hereinafter. The following result is one of our main results, with an intricate proof found in Appendix A.

**Theorem 3**.: _For the average (resp., maximum) loss, the GreedyCapture algorithm is guaranteed to return a clustering in the \((2}{{k}}-3)\)-core (resp., \(2\)-core) in \(O(kn)\) time complexity, and these bounds are (almost) tight._

In many applications of clustering, such as clustered federated learning, the average loss is realistic because the agent's loss depends on all the agents in her cluster, and not just on a single most distant agent. Hence, it is a little disappointing that the only approximation to the core that we are able to establish in this case is \(=O(}{{k}})\), which is rather unsatisfactory. We demonstrate two ways to circumvent this negative result. First, we consider demanding that any deviating coalitions be of size at least \(}{{k}}\) for some \(>1\). In Appendix B, we show that any _constant_\(>1\) reduces the approximation factor \(\) to a constant. In the next section, we explore a different approach: we relax the core to a slightly weaker fairness guarantee, which we show can be satisfied exactly, even under arbitrary losses.

## 4 Fully Justified Representation

Peters et al.  introduced _fully justified representation_ (FJR) as a relaxation of the core in the context of approval-based committee selection. The following definition is its adaptation to non-centroid clustering. Informally, for a deviating coalition \(S\), the core demands that the loss \(_{i}(S)\) of each member \(i\) after deviation be lower than _her own loss before deviation_, i.e., \(_{i}(C(i))\). FJR demands that it be lower than the _minimum loss of any member before deviation_, i.e., \(_{j S}_{j}(C(j))\).

**Definition 2** (\(\)-Fully Justified Representation (\(\)-Fjk)).: For \( 1\), a \(k\)-clustering \(C=(C_{1},,C_{k})\) satisfies \(\)-fully justified representation (\(\)-FJR) if there is no group of agents \(S N\) with \(|S|}{{k}}\) such that \(_{i}(S)<_{j S}_{j}(C(j))\) for each \(i S\), i.e., if \(_{i S}_{i}(S)<_{j S}_{j}(C(j))\). We refer to 1-FJR simply as FJR.

We easily see that \(\)-FJR is a relaxation of \(\)-core.

**Proposition 1**.: _For \( 1\), \(\)-core implies \(\)-FJR for arbitrary loss functions._

Proof.: Suppose that a clustering \(C\) is in the \(\)-core. Thus, for every \(S N\) with \(|S|}{{k}}\), there exists \(i S\) for which \(_{i}(S)_{i}(C(i))_{j S}_{j}(C(j))\), so the clustering is also \(\)-FJR. 

### Arbitrary Loss Functions

We prove that an (exactly) FJR clustering is guaranteed to exist, even for arbitrary losses. For this, we need to define the following computational problem.

**Definition 3** (Most Cohesive Cluster).: Given a set of agents \(N\) and a threshold \(\), the Most Cohesive Cluster problem asks to find a cluster \(S N\) of size at least \(\) such that the maximum loss of any \(i S\) for \(S\) is minimized, i.e., find \(_{S N^{}:|S|}_{i S}_{i}(S)\).

For \( 1\), a \(\)-approximate solution \(S\) satisfies \(_{i S}_{i}(S)_{i S^{}}_{i} (S^{})\) for all \(S^{} N\) with \(|S^{}|\), and a \(\)-approximation algorithm returns a \(\)-approximate solution on every instance.

We show that plugging in a \(\)-approximation algorithm \(\) to the Most Cohesive Cluster problem into the GreedyCohesiveClustering algorithm designed in the previous section yields a \(\)-FJR clustering. In order to work with arbitrary losses, we need to consider a slightly generalized GreedyCohesiveClustering algorithm, which takes the loss functions \(_{i}\) as input instead of a metric \(d\), and passes these loss functions to algorithm \(\).

**Theorem 4**.: _For arbitrary losses, \( 1\), and an \(\)-approximation algorithm \(\) for the Most Cohesive Cluster problem, GreedyCohesiveClustering\(()\) is guaranteed to return a \(\)-FJR clustering. Hence, an (exactly) FJR clustering is guaranteed to exist._

Proof.: Suppose for contradiction that the \(k\)-clustering \(C=\{C_{1}, C_{k}\}\) returned by GreedyCohesiveClustering\(()\) on an instance is not \(\)-FJR. Then, there exists a group \(S N\) with \(|S|}{{k}}\) such that \(_{i S}_{i}(S)<_{i S}_{i}(C(i))\). Let \(i^{*}\) be the first agent in \(S\) that was assigned to a cluster during the execution of GreedyCohesiveClustering, by calling \(\) on a subset of agents \(N^{}\). Note that \(S N^{}\). Then, we have that \(_{i C(i^{*})}_{i}(C(i^{*}))_{i^{*}}(C(i^{*}))> _{i S}_{i}(S)\), which contradicts \(\) being an \(\)-approximation algorithm for the Most Cohesive Cluster problem. Hence, GreedyCohesiveClustering\(()\) must return an \(\)-FJR clustering.

Using an exact algorithm \(\) for the Most Cohesive Cluster problem (e.g., the inefficient brute-force algorithm), we get that a \(1\)-FJR clustering is guaranteed to exist. 

### Average and Maximum Loss Functions

Let \(^{*}\) be an exact algorithm for the Most Cohesive Cluster problem for the average (resp., maximum) loss. First, we notice that we cannot expect it to run in polynomial time, even for these structured loss functions. This is because it can be used to detect whether a given undirected graph admits a clique of at least a given size,7 which is an NP-complete problem. Hence, GreedyCohesiveClustering\((^{*})\) is an inefficient algorithm.

One can easily check that the proof of Theorem 3 extends to show that it achieves not only \(1\)-FJR (Theorem 4), but also in the \(O(n/k)\)-core (resp., \(2\)-core) for the average (resp., maximum) loss. For the core, GreedyCapture is an obvious improvement as it achieves the same approximation ratio but in polynomial time. For FJR, we show that GreedyCapture still achieves a constant approximation in polynomial time. We prove this by showing that the SmallestAgentBall algorithm used by GreedyCapture achieves the desired approximation to the Most Cohesive Cluster problem, and utilizing Theorem 4.

**Lemma 1**.: _For the average (resp., maximum) loss, SmallestAgentBall is a \(4\)-approximation (resp., \(2\)-approximation) algorithm for the Most Cohesive Cluster problem, and this is tight._

Plugging in Lemma 1 into Theorem 4, we get the following.

**Corollary 1**.: _The (efficient) GreedyCapture algorithm is guaranteed to return a clustering that is \(4\)-FJR (resp., \(2\)-FJR) for the average (resp., maximum) loss._

Determining the best FJR approximation achievable in polynomial time remains an open question.

**Open Question 3:** For the average (or maximum) loss, what is the smallest \(\) for which an \(\)-FJR clustering can be computed in polynomial time, assuming P \(\) NP?

Also, while Theorem 4 shows that exact FJR is achievable for the average and maximum losses, a single clustering may not achieve FJR for both losses simultaneously (the algorithm used in Theorem 4 depends on the loss function). In contrast, GreedyCapture does not depend on whether we are using the average or the maximum loss. Thus, the clustering it produces is simultaneously \(4\)-FJR for the average loss and \(2\)-FJR for the maximum loss (Corollary 1); this is novel even as an existential result, ignoring the fact that it can be achieved using an efficient algorithm GreedyCapture. We do not know how much this existential result can be improved upon.

**Open Question 4:** What is the smallest \(\) such that there always exists a clustering that is simultaneously \(\)-FJR for both the average loss and the maximum loss?

### Auditing FJR

Next, we turn to the question of auditing the FJR approximation of a given clustering. In particular, the goal is to find the maximum FJR violation of a clustering \(C\), i.e., the largest \(\) for which there exists a group of agents of size at least \(}{{k}}\) such that, if they were to form their own cluster, the loss of each of them would be lower, by a factor of at least \(\), than the minimum loss of any of them under clustering \(C\). Because guarantees such as the core and FJR are defined with exponentially many constraints, it is difficult to determine the exact approximation ratio achieved by a given solution efficiently, which is why prior work has not studied auditing for proportional fairness guarantees. Nonetheless, we show that the same ideas that we used to _find_ an (approximately) FJR clustering can also be used to (approximately) audit the FJR approximation of any given clustering.

**Definition 4** (\(\)-Approximate FJR Auditing).: We say that algorithm \(\) is a \(\)-approximate FJR auditing algorithm if, given any clustering \(C\), it returns \(\) such that the exact FJR approximation of \(C\) (i.e., the smallest \(\) such that \(C\) is \(\)-FJR) is in \([,]\).

``` Input: Set of agents \(N\), metric \(d\), number of clusters \(k\), clustering \(C\) Output: Estimate \(\) of the FJR approximation of \(C\) \(N^{} N; 0\);//Remaining agents, current FJR apx estimate while\(|N^{}|}{{k}}\)do \(S(N^{},d,}{{k}})\);//Find acohesive group \(S\) \(\{,_{i}(C(i))}{_{i S} _{i}(S)}\}\);//Update \(\)using the FJR violation due to \(S\) \(i^{*}_{i S}_{i}(C(i))\); \(N^{} N^{}\{i^{*}\}\);//Remove the agent with the smallest current loss  end while return\(\); ```

**ALGORITHM 3:**AuditFJR(\(\))

We design another parametric algorithm, AuditFJR(\(\)), presented as Algorithm 3, which iteratively calls \(\) to find a 'cohesive' cluster \(S\), similarly to GreedyCohesiveClustering. But while GreedyCohesiveClustering removes all the agents in \(S\) from further consideration, AuditFJR removes only the agent in \(S\) with the smallest loss under the given clustering \(C\) from further consideration. Thus, instead of finding up to \(k\) cohesive clusters, it finds up to \(n\) cohesive clusters. It returns the maximum FJR violation of \(C\) demonstrated by any of these \(n\) possible deviating coalitions (recall that the exact FJR approximation of \(C\) is the maximum FJR violation across all the exponentially many possible deviating coalitions of size at least \( n/k\)).

We show that if \(\) was a \(\)-approximation algorithm for the Most Cohesive Cluster problem, then the resulting algorithm is a \(\)-approximate FJR auditing algorithm. In particular, if we were to solve the Most Cohesive Cluster problem exactly in each iteration (which would be inefficient), the maximum FJR violation across those \(n\) cohesive clusters found would indeed be the maximum FJR violation across all the exponentially many deviating coalitions, an apriori nontrivial insight. Fortunately, we can at least plug in the SmallAgentBall algorithm, which we know achieves constant approximation to the Most Cohesive Cluster problem (Lemma 1).

**Theorem 5**.: _For \( 1\), if \(\) is a \(\)-approximation algorithm to the Most Cohesive Cluster problem, then AuditFJR(\(\)) is a \(\)-approximate FJR auditing algorithm. Given Lemma 1, it follows that for the average (resp., maximum) loss, AuditFJR(SmallAgentBall) is an efficient \(4\)-approximate (resp., \(2\)-approximate) FJR auditing algorithm._Unfortunately, the technique from Theorem 5 does not extend to auditing the core. This is because it requires upper bounding \(_{i S}(C(i))}{_{i}(S)}\) (instead of \(_{i}(C(i))}{_{i S}_{i}(S)}\)); this can be upper bounded by \(}(C(i^{*}))}{_{i^{*}}(S)}\), but we cannot lower bound \(_{i^{*}}(S)\). The fact that \(\) approximates the Most Cohesive Cluster problem only lets us lower bound \(_{i S}_{i}(S)\). We leave it as an open question whether an efficient, constant-approximate core auditing algorithm can be devised.

**Open Question 5:** Does there exist a polynomial-time, \(\)-approximate core auditing algorithm for some constant \(\)?

For the maximum loss, we can show that our 2-approximate FJR auditing algorithm is the best one can hope for in polynomial time; the proof is in Appendix A. The case of average loss remains open.

**Theorem 6**.: _Assuming P \(\) NP, there does not exist a polynomial-time \(\)-approximate FJR auditing algorithm for the maximum loss, for any \(<2\)._

## 5 Experiments

In this section, we empirically compare GreedyCapture with the popular clustering algorithms \(k\)-means++ and \(k\)-medoids on real data. Our focus is on the tradeoff between fairness (measured by the core and FJR) and accuracy (measured by traditional clustering objectives) they achieve.

**Datasets.** We consider three different datasets from the UCI Machine Learning Repository , namely Census Income, Diabetes, and Iris. For the first two datasets, each data point corresponds to a human being, and it is reasonable to assume that each individual prefers to be clustered along with other similar individuals. We also consider the third dataset for an interesting comparison with the empirical work of Chen et al. , who compared the same algorithms but for centroid clustering.

The Census Income dataset contains demographic and economic characteristics of individuals, which are used to predict whether an individual's annual income exceeds a threshold. For our experiments, we keep all the numerical features (i.e. age, education-num, capital-gain,

Figure 1: Census Income Dataset

capital-loss, and hours-per-week) along with sex, encoded as binary values. There are in total 32,561 data points, each with a sample weight attribute (fnlwgt). The Diabetes dataset contains numerical features, such as age and blood pressure, for about 768 diabetes patients. The Iris dataset consists of 150 records of numerical features related to the petal dimensions of different types of iris flowers.

**Measures.** For fairness, we measure the true FJR and core approximations of each algorithm with respect to both the average and maximum losses. For accuracy, we use three traditional clustering objectives: the average within-cluster distance \(_{t[k]}|}_{i,j C_{t}}d(i,j)\), termed _cost_ by Ahmadi et al. , as well as the popular \(k\)-means and \(k\)-medoids objectives.

**Experimental setup.** We implement the standard \(k\)-means++ and \(k\)-medoids clustering algorithms from the Scikit-learn project8, averaging the values for each measure over 20 runs, as their outcomes depend on random initializations. The computation of GreedyCapture neither uses randomization nor depends on the loss function with respect to which the core or FJR approximation is measured. Since calculating core and FJR approximations requires considerable time, for both the Census Income and Diabetes datasets, we sample 100 data points and plot the means and standard deviations over 40 runs. For the former, we conduct weighted sampling according to the fnlwgt feature.

**Results.** In Figure 1, we see the results for the Census Income dataset; the results for \(k\)-means and \(k\)-medoids objectives for this dataset, along with results for the other two datasets, are relegated to Appendix D due to being qualitatively similar to the results presented here. According to all four fairness metrics, GreedyCapture is significantly fairer than both \(k\)-means++ and \(k\)-medoids, consistently across different values of \(k\). Notably, the FJR approximation of GreedyCapture empirically stays very close to 1 in all cases, in contrast to the higher worst-case bounds (Corollary 1). Remarkably, the significant fairness advantage of GreedyCapture comes at a modest cost in accuracy: all three objective values (average within-cluster distance, \(k\)-means, and \(k\)-medoids) achieved by GreedyCapture are less than _twice_ those of \(k\)-means++ and \(k\)-medoids, across all values of \(k\) and all three datasets!

Lastly, our results are in contrast to the experiments of Chen et al.  for centroid clustering, where GreedyCapture provides a worse core approximation than \(k\)-means++ on Iris and Diabetes datasets; as demonstrated in Appendix D, this is not the case in non-centroid clustering.

## 6 Discussion

We have initiated the study of proportional fairness in non-centroid clustering. Throughout the paper, we highlight several intriguing open questions. Probably the most important of these are whether we can achieve a better approximation than \(O(n/k)\) of the core for the average loss, and whether the core is always non-empty for the maximum loss. In an effort to answer the latter question, in Appendix C.1 we show that the core is always non-empty for the maximum loss when the metric space is 1-dimensional (i.e., a line). This contrasts with the average loss, for which the core remains empty even on the line (see Appendix C.2).

In our work, we have shown that there are remarkable differences between centroid and non-centroid clustering settings. One can consider a more general model, where the loss of an agent depends on both her cluster center and the other agents in her cluster. Investigating what proportional fairness guarantees can be achieved in this case is an exciting direction. Another intriguing question is whether we can choose the number of clusters \(k\) intrinsically; this seems challenging as proportional fairness guarantees seem to depend on fixing \(k\) in advance to define which coalitions can deviate. Lastly, while classical algorithms such as \(k\)-means and \(k\)-centers are incompatible with the core and FJR in the worst case (see Appendix E), it is interesting to explore conditions under which they may be more compatible, and whether a fair clustering can be computed efficiently in such cases.