# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

To address these challenges, several computational methods have been developed. Novel approaches based on causal representation learning provide better mechanistic interpretation of single-cell perturbation data (Lopez et al., 2023; Bereket and Karaletsos, 2023; Zhang et al., 2023). These methods belong to the family of identifiable deep generative models (Khemakhem et al., 2020; Lachapelle et al., 2022; Zheng et al., 2022) and therefore offer, to some extent, theoretical guarantees but remain limited to the analysis of data from a single cellular context. Another set of studies model cells across multiple contexts using latent linear additive models (Hetzel et al., 2022; Lotfollahi et al., 2023). However, due to their additive assumption, these models fail to characterize interactions between treatments and biological contexts.

To address these limitations, we introduce the Factorized Causal Representation (FCR) learning framework. This identifiable deep generative model learns representations of cells that take the form of three disentangled blocks, specific to treatments, biological contexts, and their interactions, respectively. We first present the proposed generative model and then provide a set of sufficient conditions for its identifiability, extending the work of Khemakhem et al. (2020) to the case of interacting covariates. We then present an implementation of our FCR method that builds upon the variational auto-encoder framework (Kingma and Welling, 2014) as well as adversarial regularization (Ganin et al., 2016). We demonstrate that FCR not only effectively identifies interactions but also surpasses state-of-the-art methods in various tasks across four single-cell datasets.

## 2 Related Work

Learning Representations of Cellular ResponsesLearning representations of single-cell data is a powerful framework, with demonstrated impact in tasks such as imputation (Lopez et al., 2018), clustering (Trapnell et al., 2014; Zhu et al., 2019; Alquicira-Hernandez et al., 2019), and integration across modalities (Gayoso et al., 2022). Recent advancements have largely improved our capacity to predict cellular responses to drug treatments (Lotfollahi et al., 2019; Rampasek et al., 2019; Lotfollahi et al., 2021; Lopez et al., 2023; Bunne et al., 2023; Zapatero et al., 2023). Lotfollahi et al. (2023) and Hetzel et al. (2022) proposed generative models that additively combine treatment embeddings and biological context embeddings within a latent space. Wu et al. (2023) cast the prediction problem as a counterfactual inference problem. Despite these advancements, existing methods fail to address how treatments may preferentially impact specific cell types, a critical point for understanding the effects of drugs on biological systems.

Identifiable non-linear Independent Component Analysis modelsA field where disentanglement is most important is non-linear Independent Component Analysis (non-linear ICA) (Hyvarinen and Pajunen, 1999), where information from a set of latent variables is mixed through a non-linear encoding function. It has long been known that such models (i.e., either the encoding function, or the sources) are non-identifiable without further assumption. However, some recent works (Hyvarinen et al., 2019; Lachapelle et al., 2022; Zheng et al., 2022) proved identifiability was possible in a non-stationary regime. Often, this assumption takes the form of conditional independence of the latent variables given some auxiliary (observed) variables. Notably, the iVAE framework from Khemakhem et al. (2020) offers disentanglement guarantees within this conditional VAE setup. Our work extends the theory of Khemakhem et al. (2020) to prove identifiability of the interaction terms between multiple such auxiliary variables.

This approach differs significantly from specific models in the Variational Autoencoder (VAE) literature (Kingma and Welling, 2014) such as the betaVAE (Higgins et al., 2016) and factorVAE (Kim and Mnih, 2018) that both address disentanglement learning. Indeed, these latter models lack theoretical foundation regarding the identifiability of their parameters or latent variables (Esmaeili et al., 2019; Chen and Grosse, 2018).

## 3 Preliminaries

Single-cell perturbation experiments characterize causes and effects at the cellular and molecular levels. Our objective is to disentangle the contributions of treatments, cellular covariates, and their interactions to model the effects of perturbations. Let \(^{d}\) be a vector of _covariates_ representing intrinsic attributes of single cells, such as cell type, tissue of origin, or patient information.

Let \(^{p}\) represent the _treatment_ or _intervention_ applied to single cells and let \(^{k}\) denote the gene expression levels (outcome).

### Generative Model

We introduce a low-dimensional latent vector \(^{n}\) that encodes cellular states after treatment \(\) and in biological context \(\). We assume a block structure for \(=[_{x},_{tx},_{t}]\) with dimension \(n=n_{x}+n_{tx}+n_{t}\). Here, \(_{x}\) captures the effects of contextual covariates \(\), \(_{t}\) represents the direct effects of the treatment \(\), and \(_{tx}\) encodes the interaction effects between both terms.

More precisely, the generative model is specified as follows. Latent representation \(_{x}\) is generated from \(\) according to distribution \(_{x} p_{_{t}|}(_{x})\), \(_{t}\) is generated from \(\) following distribution \(_{t} p_{_{t}|}(_{t})\), and \(_{tx}\) from both \(\) and \(\) following distribution \(_{tx} p_{_{tx}|,}(_{tx} ,)\). The gene expression outcome vector \(\) is then deterministically generated \(=g(_{x},_{tx},_{t})\), where \(g\) is a smooth mixing function. A graphical representation of this generative model appears in Figure 1. For the control group (no treatment), the outcome is noted as \(^{0}\), and the representation as \(^{0}=[_{x}^{0},_{tx}^{0},_{t}^{0}]\).

Learning each element in this triplet of latent variables is a sound approach for unravelling interaction effects. Indeed, \(_{x}\) captures covariate-specific patterns that are invariant with respect to the perturbations, while also capturing the essential biological attributes tied to the cellular covariates. \(_{t}\) captures the intrinsic effects of the treatments, irrespective of the covariates. \(_{tx}\) unravels the interactions that a treatment could have with specific covariates. This last representation captures the nuanced manner in which distinct cell types, tissues, or patient groups react to treatments, reflecting the diversity and specificity of biological responses.

We note that our model does not take into account observation noise, since \(g\) is a _deterministic_ function in our assumption. However, our theory may be readily extended to incorporate Gaussian observation noise (Khemakhem et al., 2020), as well as counting observation noise (Lopez et al., 2024).

### Model Identifiability

We now introduce the definitions for the different classes of disentangled models that will appear later in the manuscript. Analogous definitions appear in previous work from Von Kugelgen et al. (2021) and Kong et al. (2022). Throughout this section, \(\) denotes a (random) latent vector and \(\) denotes an observed vector. \(g:\) is an unknown mixing function such that \(=g()\).

**Definition 3.1** (_Component-wise Identifiability_).: We say that latent vector \(\) is _identifiable_ from data \(\) if for any other latent vector \(}\) such that \(g(})\) and \(g()\) are equal in distribution, \(\) and \(}\) are equal up to a permutation of the indices, and deformation of each component by invertible scalar functions. More precisely, there exists a permutation \(\) of \(\{1,,n\}\), and invertible scalar functions \(h_{j}\) such that for all \(j\{1,,n\}\):

\[_{j}=h_{j}(z_{(j)}),\] (1)

where \(z_{j}\) and \(_{(j)}\) are the \((j)\)-th components of \(\) and \(}\) respectively.

**Definition 3.2** (_Block-wise Identifiability_).: For \(1 n_{1}<n_{2} n\), we denote as \(_{[n_{1}:n_{2}]}^{n_{2}-n_{1}+1}\) the block of \(\) from index \(n_{1}\) to \(n_{2}\). We say that latent vector \(_{[n_{1}:n_{2}]}\) is _block-identifiable_ from data \(\) if for any other latent vector \(}\) such that \(g(})\) and \(g()\) are equal in distribution, \(_{[n_{1}:n_{2}]}\) and \(}_{[n_{1}:n_{2}]}\) are equal up to an invertible function \(h\):

\[}_{[n_{1}:n_{2}]}=h(_{[n_{1}:n_{2}]}),\] (2)

where \(}_{[n_{1}:n_{2}]}\) is the corresponding block in the estimated vector \(}\).

Figure 1: Data generating process: shaded nodes denote observed variables. Empty nodes denote latent variables.

Identifiability Results

One strong advantage of the FCR framework is that it comes with strong theoretical guarantees concerning the disentanglement of its factorized latent space. We first prove the _component-wise identifiability_ of the interaction variable \(_{tx}\) under the assumption of sufficient experimental variability (Khemakhem et al., 2020) (Section 4.1). Then, we demonstrate the block-identifiability of \(_{t}\) and \(_{x}\) by exploiting their invariance with respect to \(\) and \(\), respectively (Section 4.2). These guarantees are important, as they ensure that the obtained latent variables will have desirable semantics. For example, given our theoretical results, the obtained interaction embedding \(}_{tx}\) must be reflective of the ground-truth interactions \(_{tx}\) only, and not of any of the other latent variables.

### Component-wise identifiability of \(_{tx}\)

Our proof relies on three technical assumptions. Two are classical from the nonlinear ICA literature, and the last one relates to the observability of a complementary set of experiments for identifiability of interactions.

**Assumption 4.1**.: The probability density function of the prior distribution for the latent variables is smooth and positive, i.e. \(p_{|,}(,)>0\) for all \((,,) \).

**Assumption 4.2**.: The components of \(\) are conditionally independent given \(\) and \(\).

**Assumption 4.3**.: _(Experimental Sufficiency) There exist at least \(2n_{tx}+1\) distinct values for the vector \([,]\) in the experimental design. One such setting can be referred to as a control condition and is noted as \([_{0},_{0}]\). Additionally, for any non-control environment \((_{i},_{i})\) for \(i\{1,,2n_{tx}\}\), we assume there always exist corresponding switched experiments under the settings \((_{0},_{i})\) and \((_{i},_{0})\)._

Assumption 4.3 is novel and essentially mandates that we conduct a sufficient number of experiments with cross-referenced covariates and treatments. This ensures that we can observe the specific drug response related to each covariate, and is often how such experiments are designed in practice.

**Theorem 4.4**.: _Let us first define \((_{tx},,)\) as the vector:_

\[(_{tx},,) =+1}(z_{n_{x}+1},,)}{ z_{n_{x}+1}},,+n_{tx}}(z_{n_{x}+n_{tx}}, ,)}{ z_{n_{x}+n_{tx}}},\] \[q_{n_{x}+1}(z_{n_{x}+1},,) }{ z_{n_{x}+1}^{2}},,q_{n_{x}+n_{tx}}(z_{n_{x} +n_{tx}},,)}{ z_{n_{x}+n_{tx}}^{2}},\]

_where \(q_{i}\) denotes the logarithm of probability density \(p_{_{i}|,}\) for component \(_{i}\). If in addition to the assumptions 4.1, 4.2, 4.3, we assume that the \(2n_{tx}\) vectors_

\[\{(_{tx},_{i},_{i})+( _{tx},_{0},_{0})-(_{tx}, _{0},_{i})-(_{tx},_{i}, _{0})\}_{i=1}^{2n_{tx}},\] (3)

_are linearly independent then \(_{tx}\) is component-wise identifiable._

The proof appears in Appendix A. Theorem 4.4 extends the concept of linear independence found in nonlinear ICA (Khemakhem et al., 2020). Unlike the original theory, where auxiliary variables must induce sufficient variations of the latent variables, we are confronted with a case where treatments and contexts must have sufficient variability in combination. For example, when \(\) is linear with respect to both \(\) and \(\) the vector of interest becomes the null vector. In this trivial case, the theorem's assumption is never satisfied (\(z_{tx}\) indeed has no purpose in that particular model). However, under a rich class of model with complex interaction patterns, our model will be able to infer informative latent variables.

### Block-wise identifiability of \(_{x}\) and \(_{t}\)

To prove the block-wise identifiability of \(_{x}\) and \(_{t}\), we exploit their invariance properties: \(_{t}\) remains unchanged across different values of \(\), while \(_{x}\) is stable across variations in \(\). This invariance allows us to distinguish these blocks from the interaction terms \(_{tx}\). Additionally, because this invariance reflects latent features' stability despite perturbations, its utilization can enable deeper biological insights and interpretability. For instance, \(_{x}\) might represent stable cellular characteristicsthat persist across different treatments, while \(_{t}\) could capture consistent treatment effects across various cell types.

We now state our main identifiability result for \(_{x}\) and \(_{t}\).

**Theorem 4.5**.: _We follow Assumptions 4.1, 4.2, 4.3, and the one from Theorem 4.4. We note as \((Z)\) the set of subsets \(S\) of \(\) that satisfy the following two conditions:_

1. \(S\) _has nonzero probability measure, i.e._ \(( S=^{{}^{}}, =^{})>0\) _for any_ \(^{{}^{}}\) _and_ \(^{{}^{}}\)_._
2. \(S\) _cannot be expressed as_ \(A_{_{x}}_{tx}_{t}\) _for any_ \(A_{_{x}}_{x}\) _or as_ \(_{x}_{tx} A_{_{t}}\) _for any_ \(A_{_{t}}_{t}\)_._

_We have the following identifiability result. If for all \(S()\), there exists \((_{1},_{2})\) and \(\) such that_

\[_{ S}p_{,}( _{1},)d_{ S}p_{ ,}(_{2},)d,\] (4)

_and there also exists \((_{1},_{2})\) and \(\) such that_

\[_{ S}p_{,}( ,_{1})d_{ S}p_{ ,}(,_{2})d,\] (5)

_then \(_{t}\) and \(_{x}\) are block-wise identifiable._

The proof appears in Appendix A, and is adapted from Kong et al. (2022). Our main assumption is that the conditional distribution \(p_{,}(,)\) undergoes substantive changes when spanning different treatments \(\) and covariates \(\). When treatments differ markedly from each other in their mechanisms and effects, the probability distributions of the latent variables conditioned on these treatments are unlikely to be identical.

## 5 Methodology

We now propose a tangible implementation of our method, termed Factorized Causal Representation (FCR) learning. Our approach has three components:

1. A variational inference approach to estimate the representations from our FCR model. Our model and inference architecture is specifically designed to learn disentangled representations \(_{x}\), \(_{tx}\), \(_{t}\) (Section 5.1).
2. A regularization method that enforces independence between \(_{x}\) and \(\), and encourages variability of \(_{t}\) with respect to \(\) (Section 5.2).
3. A second regularization technique to ensure that the conditional independence properties \(_{x}\!\!\!_{tx}\) and \(_{t}\!\!\!_{tx}\) are satisfied (Section 5.3).

The main computational structure of the model is illustrated as a schematic in Figure 2.

### Model Specification and Variational Inference

Model SpecificationWe parameterize the probability distributions as follows:

\[p(_{x}) (f_{}^{x}(),f_{}^{x}( ))\] (6) \[p(_{t}) (f_{}^{t}(),f_{}^{t}( ))\] (7) \[p(_{tx},) (f_{}^{t,x}(,),f_{ }^{t,x}(,)),\] (8)

where all the above functions are parameterized by neural networks.

To prevent \(_{tx}\) from having trivial dependencies with respect to \(\) and \(\), we explicitly encourage its prior to capture interactions between \(\) and \(\) by designing the functions \(f_{}^{t,x}\) and \(f_{}^{t,x}\) to be of the form:

\[f_{}^{t,x} =f_{}(k_{}() k_{}())\] (9) \[f_{}^{t,x} =f_{}(k_{}() k_{}( )),\] (10)where \(k_{x}()\) and \(k_{t}()\) represent the embeddings for the cellular covariates and treatments, respectively, while \(\) denotes the Hadamard product.

Variational InferenceBecause the posterior distribution of the latent variables are intractable, we use the variational autoencoder framework (Kingma and Welling, 2014) to jointly learn the model's parameters and an approximation to the posterior, following the approach used in previous causal representation learning work (Khemakhem et al., 2020). We consider the following mean-field variational approximation to the posterior distribution:

\[q_{}(_{x},_{t},_{tx},,)=q_{}(_{x},)q_{}( _{t},)q_{}(_{tx},,).\] (11)

Following the graphical model from Figure 1, the Evidence Lower Bound (ELBO) is derived as:

\[_{}= _{q_{}(_{x},_{t},_{ tx}|,,)} p_{}(_{x}, _{t},_{tx})\] (12) \[-D_{KL}(q_{}(_{t},)\|p_{ }(_{x}))\] \[-D_{KL}(q_{}(_{t},)\|p_{ }(_{t}))\] \[-D_{KL}(q_{}(_{tx},, )\|p_{}(_{tx},)),\]

where \(\) and \(\) denote the parameters of the generative model and the inference networks, respectively. \(D_{KL}\) denotes the Kullback-Leibler divergence between two probability distributions. For simplicity, we omit \(\) and \(\) as well as script notations in the following sections, wherever appropriate. The derivation of the ELBO appears in Appendix B.

The variational distributions defined in Equation 11 are parameterized as follows:

\[q(_{x},) (g_{}^{x}(,),g_{ }^{x}(,))\] (13) \[q(_{t},) (g_{}^{t}(,),g_{ }^{t}(,))\] (14) \[q(_{tx},,) (g_{}^{t,x}(,, ),g_{}^{t,x}(,,)),\] (15)

where all the functions introduced above are parameterized by neural networks.

### Causal Structure Regularization

We exploit both the variability of \(_{t}\) and the invariance of \(_{x}\) when comparing control and treated groups that share the same covariates. Specifically, our goal is to enforce the resemblance between \(_{x}\) and \(_{x}^{0}\) while reducing the congruence of \(_{t}\) and \(_{t}^{0}\). Towards this end, we first add the following score as a regularizer,

\[_{}=_{q(_{x},_{t}|,)q(_{x}^{0},_{t}^{0}|_{0},_{0})} [(_{t},_{t}^{0})- (_{x},_{x}^{0})],\] (16)

Figure 2: The illustration of FCR models. (a) is the component for \(p(_{x}),p(_{t})\) and \(p(_{tx},)\) (b) is the component to estimate \(q(_{x},),q(_{t}, )\) and \(q(_{t,x},,)\). Note that **0** indicates \(=\) representing the control samples. (c) computational diagrams to estimate the Kullback-Leibler divergences, causal structure regularization and permutation discriminators.

where \(()\) denotes the cosine similarity. Second, we introduce a classifier \(f_{}\) to predict the treatments \(\) from \([_{t},_{tx}]\) and \([_{t}^{0},_{tx}^{0}]\) as follows,

\[}=f_{}([_{t},_{tx} ],[_{t}^{0},_{tx}^{0}]).\] (17)

The predicted treatment probability vector \(}\) is then used for the computation of a cross-entropy loss

\[_{}=-_{t,q(_{tx},_{t}| ,)q(_{tx}^{0},_{tx}^{0}|^{0 },^{0})}[(})].\] (18)

### Permutation Discriminators

We want to ensure the conditions of Assumption 4.2 and Theorem 4.4, specifically that \(_{tx}_{t}\) and \(_{tx}_{x}\). Towards this goal, we use the following proposition, establishing a connection between exchangeability and conditional independence.

**Proposition 5.1**.: _(_Bellot and van der Schaar_,_ 2019_)_ _Let \(X,Y\) and \(Z\) be three random variables. Under the assumption of \(X\!\!\! Y Z\), we have the samples \((X_{i},Y_{i},Z_{i})_{i=1}^{M}\) and permuted samples \((X_{(i)},Y_{i},Z_{i})_{i=1}^{M}\) with a permutation function \(\). The corresponding statistics \(_{i}\) of \((X_{i},Y_{i})_{i=1}^{M}\) and \(_{(i)}\) of \((X_{(i)},Y_{i})_{i=1}^{M}\) are exchangeable._

This proposition states that permutation will not change the independence between two conditionally independent random variables. We therefore propose to use permutation discriminators for \(_{x}\), \(_{tx}\) and \(_{t}\), \(_{tx}\). First, we initially permute \(_{tx}\) within the triplet \((_{x}^{(j)},_{tx}^{(j)},^{(j)}=_{i}) _{j=1}^{M}\) to yield \((_{x}^{(j)},_{tx}^{(j)},^{(j)}=_{i}) _{j=1}^{M}\). Then, we train a binary classifier (the discriminator) to predict whether samples have been permuted or not. We denote the permutation label as \(l\) and predict it as

\[=f_{_{}}(_{x},}_{tx}, ),\] (19)

where \(}_{tx}\) could be permuted or non-permuted \(_{tx}\) samples. If \(_{x}\) and \(_{tx}\) are indeed independent given \(\), the discriminator should be unable to distinguish between the permuted and the original samples. For each discriminator, we add a regularization term that consists of the cross-entropy loss

\[_{_{}}=-_{,q(_{tx},_{tx}|,)}[l()].\] (20)

We proceed similarly to make sure that \(_{t}\) and \(_{tx}\) are independent conditionally on \(\).

### Objective function

Finally, we specify the overall loss for our model as

\[_{}=_{}+_{1}_{ }+_{2}_{}-_{3}(_{_{}}+_{_{}}),\] (21)

where \(_{1},_{2},_{3}>0\) are hyperparameters. To concurrently train both the representations and the discriminators, we employ an adversarial training approach as follows,

\[_{f_{_{}},f_{_{}}}_{, ,\,f_{}}_{}.\] (22)

The training procedure and the procedure used for hyperparameters selection are detailed in Appendix C and D, respectively.

## 6 Experiments

In this section, we are pursuing three primary objectives. First, we seek to validate the FCR method's proficiency in capturing the designated causal structure within the latent space through both clustering analysis (Section 6.1) and statistical testing of independence (Section 6.2), respectively. Second, we evaluate the method's efficacy in predict single-cell level conditional cellular responses (Section 6.3). We note that the current implementation of FCR does not make use of general embeddings for \(\) or \(\), and for that reason we do not perform experiments to predict cellular responses to unseen treatments and cell types (covariates).

DatasetsTo evaluate the efficacy and robustness of the FCR method, we conducted our study on four real single-cell perturbation datasets (Appendix E). The first of these is the sciPlex dataset (Srivastava et al., 2020), which provides insights into the impact of several HDAC (Hestone Deacetylase) inhibitors on a total of 11,755 cells from three distinct cell lines: A549, K562, and MCF7. Each of these cell lines was subjected to treatment in two independent replicate experiments, using five different drug dosages: 0 nM (control), 10 nM, 100 nM, 1 \(\)M, and 10 \(\)M. The subsequent three datasets are sourced from (McFarland et al., 2020), which executed several large-scale experiments in varied settings. The multiPlex-Tram dataset contains 13,713 cells from 24 cell lines, treated with Trametinib and a DMSO control over durations of 3, 6, 12, 24, and 48 hours. The multiPlex-7 dataset spans 61,552 cells across 97 cell lines, subjected to seven different treatments. Finally, the multiPlex-9 dataset incorporates 19,524 cells from 24 cell lines, undergoing a series of nine treatments.

BaselinesWe benchmarked our method against several established representation learning methods: (1) scVI (Lopez et al., 2018), (2) iVAE (Khemakhem et al., 2020), (3) \(\)VAE (Higgins et al., 2016), (4) factorVAE (Kim and Mnih, 2018), (5) VCI (Wu et al., 2023), (6) CPA (Lotfollahi et al., 2023), (7) scGEN (Lotfollahi et al., 2019) (8) sVAE (Lopez et al., 2023), (9) CINEMA-OT (Dong et al., 2023). For the clustering analysis, we employed all the inferred latent variables from each baseline method. For the conditional independence test, we selected a random subset of latent variables for each baseline matching the number of latent variables of FCR. We then tested each subset and repeated the process a total of ten times for each baseline using different random subsets to yield the best results. Specifically, CINEMA-OT and scGEN address only binary treatments/perturbations, so they are only considered in the cellular response predictions tasks.

Results on additional dataset, simulation studies, ablation studies, data visualization, and biological interpretation of the latent variables appear in Appendix F.

### Clustering Analysis on Covariates, Treatments, and Combined Features

We evaluated the performance of the obtained latent representations in capturing three key aspects: the cellular covariates \(\), the treatments \(\), and their interaction \((,)\). To assess each latent representation, we applied clustering and compared the fidelity of the resulting cluster labels with the corresponding \(\), \(\), or \((,)\) from the original data. This approach allowed us to gauge how well the latent representations preserved the underlying structure of the cellular covariates, treatments, and their interactions. We performed clustering using the Leiden algorithm (Traag et al., 2019). To assess the fidelity between two sets of cluster labels, we employed the Normalized Mutual Information (NMI) metric (Kim et al., 2019). Higher NMI values indicate better alignment between the clustering results and the original data structure. We conducted this clustering and fidelity assessment on our model's latent variables \(_{x}\), \(_{tx}\), and \(_{t}\), as well as on the variables obtained from baseline methods.

Our results highlight that \(_{x}\) has superior performance in clustering on covariates \(\) compared to all other available latent representations (Figure 3). Similarly, \(_{t}\) performs best for clustering on treatments \(\). Finally, \(_{tx}\) outperforms all other methods when clustering jointly on \(\) and \(\), showing that it faithfully represents the combined features of both \(\) and \(\). Specifically, for the sciPlex datasets,

Figure 3: Clustering results for the sciPlex dataset. Normalized Mutual Information (NMI) values for clustering based on: (a) covariates \(\); (b) combined covariates and treatments \([;]\); (c) treatments \(\).

clustering on \(\) yields better results than clustering based on both \((,)\) or on solely \(\). This can be attributed to the HDAC inhibitors exhibiting minimal impact on distinct cell lines until a maximal concentration of 10 \(\)M was reached. We report similar results for the other datasets in Appendix F. Taken together, these results suggest that FCR effectively learns disentangled representations across different datasets.

### Statistical Conditional Independence Testing

We validated the disentanglement of our latent representations via conditional independence testing, implemented as the Kernel Conditional Independence (KCI) (Zhang et al., 2012) tests. Our investigations focused on the following relationships: (1) \(_{x}\!\!\!\); (2) \(_{t}\!\!\!\); (3) \(_{t}\!\!\!_{tx}\); (4) \(_{x}\!\!\!_{tx}\). In conjunction with these tests, we also employed the Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005) to evaluate the (marginal) independence between: (a) \(_{t}\) and \(\); (b) \(_{x}\) and \(\). Our aim was two-fold. First, we wanted to assess whether the factors in our latent space complied with the necessary conditional independence statements, corroborating a well-captured causal structure. Second, we wanted to determine the dependence between our latent representations and their respective observed variables.

We focus our presentation of the results on the sciPlex dataset in the main text (results for the other datasets imply similar conclusions and appear in Appendix F.4). We first examined the results of testing for conditional independence statements \(_{x}\!\!\!\) and \(_{t}\!\!\!\) (Figure 4ab). In this experiment, all the baseline methods produced p-values smaller than 0.05. This implies a rejection of the null hypothesis (conditional independence) for the baseline methods, and suggests that their representations failed to maintain the desired conditional independence statements. Second, we examined the results of testing for conditional independence statements \(_{x}\!\!\!_{tx}\) and \(_{t}\!\!\!_{tx}\) (Figure 4c). Interestingly, this assessment also quantifies the efficacy of our permutation discriminators. The observed p-values generally exceed 0.05, suggesting that the null hypothesis cannot be rejected. Finally, we use the HSIC to report estimates of mutual information (assessing for marginal independence). Low HSIC values suggest poor dependence between the pairs of random variables. We report the HSIC values for assessing \(_{x}\!\!\!\), \(_{x}\!\!\!\), \(_{x}\!\!\!\), as well as \(_{t}\!\!\!\), \(_{t}\!\!\!\), and \(_{t}\!\!\!\), where \(\) represents simulated random vectors (Figure 4d). Contrasting our representations with randomly simulated vectors, we observe that both \(_{x}\) and \(\), as well as \(_{t}\) and \(\), have HSIC values far from zero, indicating a high dependence. The contrast in results between our approach and the baseline methods highlights FCR's nuanced capability in capturing and preserving causal structures.

### Conditional Cellular Responses Prediction

Our analysis demonstrates that treatments often elicit covariate (cell line) specific responses. Consequently, accurately predicting outcomes for novel drugs or cell lines becomes challenging without a careful consideration of the similarity in \(_{tx}\) and how \(\) interacts with \(\). Unlike previous literature,we do not conduct experiments to predict cellular responses to unseen treatments and cell types (covariates). This decision is based on extensive biological research showing that responses to covariates are context-specific (McFarland et al., 2020; Srivatsan et al., 2020). Without thoroughly examining the similarity of unseen treatments or cell types in the latent space, we cannot confidently predict cellular responses.

Nonetheless, our approach enables the prediction of cellular responses at the single-cell level. This paper focuses on predicting cellular responses (expression of 2000 genes) in control cells subjected to drug treatments. It is important to note that our comparative analysis is confined to CPA, VCI, sVAE, scGEN and CINEMA-OT as they are uniquely tailored for this task. We utilize FCR to extract control's \([_{x}^{0},_{tx}^{0},_{t}^{0}]\) and corresponding experiments' \([_{x},_{tx},_{t}]\), then use the decoder \(g\) to predict the gene expression level as \(}=g(_{x}^{0},_{tx},_{t})\). We measure the \(R^{2}\) score. From our results (Table 1), we observe that FCR generally outperforms other baselines across the first three datasets. However, CPA performs the best on the multiplex-9 dataset. The primary reason for this is that the multiplex-9 dataset has fewer covariate-specific responses (McFarland et al., 2020). Additionally, scGEN and CINEMA-OT, which are designed for binary perturbations, tend to underperform in these tasks.

## 7 Discussion

This paper aimed to resolve a current challenge--how to disentangle single-cell level drug responses using latent variables into representations for covariates, treatments and contextual covariate-treatment interactions. To do so, we established a theoretically grounded framework for identifiability of such components \(_{tx}\), \(_{x}\) and \(_{t}\). Expanding upon these theoretical foundations, we have developed the FCR algorithm to factorize the interactions between treatments and covariates. Looking ahead, our aim is to incorporate interpretable components into this framework. This enhancement will aid in pinpointing genes affected by \(_{x}\), \(_{t}\) or \(_{tx}\). Such advancements are expected to significantly contribute to the progress of precision medicine.