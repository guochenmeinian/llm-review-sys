# Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness

Ambar Pal

ambar@jhu.edu

&Jeremias Sulam

jsulam1@jhu.edu

&Rene Vidal

vidalr@upenn.edu

###### Abstract

The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution - concentration on small-volume subsets of the input space - determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, utilizing structure in data naturally leads to classifiers that enjoy data-dependent polyhedral robustness guarantees, improving upon methods for provable certification in certain regimes.

## 1 Introduction, Motivation and Contributions

Research in adversarial learning has shown that traditional neural network based classification models are prone to anomalous behaviour when their inputs are modified by tiny, human-imperceptible perturbations. Such perturbations, called adversarial examples, lead to a large degradation in the accuracy of classifiers . This behavior is problematic when such classification models are deployed in security sensitive applications. Accordingly, researchers have and continue to come up with _defenses_ against such adversarial attacks for neural networks.

Such defenses [49; 60; 42; 22] modify the training algorithm, alter the network weights, or employ preprocessing to obtain classifiers that have improved empirical performance against adversarially corrupted inputs. However, many of these defenses have been later broken by new adaptive attacks [1; 8]. This motivated recent impossibility results for adversarial defenses, which aim to show that all defenses admit adversarial examples. While initially such results were shown for specially parameterized data distributions , they were subsequently expanded to cover general data distributions on the unit sphere and the unit cube , as well as for distributions over more general manifolds .

On the other hand, we humans are an example of a classifier capable of very good (albeit imperfect ) robust accuracy against \(_{2}\)-bounded attacks for natural image classification. Even more, a large body of recent work has constructed _certified_ defenses [11; 63; 10; 29; 19; 54] which obtain non-trivial performance guarantees under adversarially perturbed inputs for common datasets like MNIST, CIFAR-10 and ImageNet. This apparent contention between impossibility results and the existence of robust classifiers for natural datasets indicates that the bigger picture is more nuanced, and motivates a closer look at the impossibility results for adversarial examples.

Our first contribution is to show that these results can be circumvented by data distributions whose mass concentrates on small regions of the input space. This naturally leads to the question of whether such a construction is necessary for adversarial robustness. We answer this question in the affirmative, formally proving that a successful defense exists only when the data distribution concentrates on anexponentially small volume of the input space. At the same time, this suggests that exploiting the inherent structure in the data is critical for obtaining classifiers with broader robustness guarantees.

Surprisingly, almost1 all _certified_ defenses do not exploit any structural aspects of the data distribution like concentration or low-dimensionality. Motivated by our theoretical findings, we study the special case of data distributions concentrated near a union of low-dimensional linear subspaces, to create a certified defense for perturbations that go beyond traditional \(_{p}\)-norm bounds. We find that simply exploiting the low-dimensional data structure leads to a natural classification algorithm for which we can derive norm-independent polyhedral certificates. We show that our method can certify accurate predictions under adversarial examples with an \(_{p}\) norm larger than what can be certified by applying existing, off-the-shelf methods like randomized smoothing . Thus, we demonstrate the importance of structure in data for both the theory and practice of certified adversarial robustness.

More precisely, we make the following main contributions in this work:

1. We formalize a notion of \((,)\)-concentration of a probability distribution \(q\) in Section 2, which states that \(q\) assigns at least \(1-\) mass to a subset of the ambient space having volume \(O()\). We show that \((,)\)-concentration of \(q\) is a necessary condition for the existence of any classifier obtaining at most \(\) error over \(q\), under perturbations of size \(\).
2. We find that \((,)\)-concentration is too general to be a sufficient condition for the existence of a robust classifier, and we follow up with a stronger notion of concentration in Section 3 which is sufficient for the existence of robust classifiers. Following this stronger notion, we construct an example of a strongly-concentrated distribution, which circumvents existing impossibility results on the existence of robust classifiers.
3. We then consider a data distribution \(q\) concentrated on a union of low-dimensional linear subspaces in Section 4. We construct a classifier for \(q\) that is robust to perturbations following threat models more general than \(_{p}\). Our analysis results in polyhedral certified regions whose faces and extreme rays are described by selected points in the training data.
4. We perform empirical evaluations on MNIST in Section 5, demonstrating that our certificates are complementary to existing off-the-shelf approaches like Randomized Smoothing (RS), in the sense that both methods have different strengths. In particular, we demonstrate a region of adversarial perturbations where our method is certifiably robust, but RS is not. We then combine our method with RS to obtain certificates that enjoy the best of both worlds.

## 2 Existence of Robust Classifier Implies Concentration

We will consider a classification problem over \(\) defined by the data distribution \(p\) such that \(\) is bounded and \(=\{1,2,,K\}\). We let \(q_{k}\) denote the conditional distribution \(p_{X|Y=k}\) for class \(k\). We will assume that the data is normalized, i.e., \(=B_{_{2}}(0,1)\), and the boundary of the domain is far from the data, i.e., for any \(x q_{k}\), an adversarial perturbation of \(_{2}\) norm at most \(\) does not take \(x\) outside the domain \(\).2

We define the robust risk of a classifier \(f\) against an adversary making perturbations whose \(_{2}\) norm is bounded by \(\) as 3

\[R(f,)=_{(x,y) p}( B_{_{2}}(x,) f() y).\] (1)

We can now define a robust classifier in our setting.

**Definition 2.1** (Robust Classifier).: _A classifier \(g\) is defined to be \((,)\)-robust if the robust risk against perturbations with \(_{2}\) norm bounded by \(\) is at most \(\), i.e., if \(R(g,)\)._

The goal of this section is to show that if our data distribution \(p\) admits an \((,)\)-robust classifier, then \(p\) has to be _concentrated_. Intuitively, this means that \(p\) assigns a "large" measure to sets of "small" volume. We define this formally now.

**Definition 2.2** (Concentrated Distribution).: _A probability distribution \(q\) over a domain \(^{n}\) is said to be \((C,,)\)-concentrated, if there exists a subset \(S\) such that \(q(S) 1-\) but \((S) C(-n)\). Here, \(\) denotes the standard Lebesgue measure on \(^{n}\), and \(q(S)\) denotes the measure of \(S\) under \(q\)._

With the above definitions in place, we are ready to state our first main result.

**Theorem 2.1**.: _If there exists an \((,)\)-robust classifier \(f\) for a data distribution \(p\), then at least one of the class conditionals \(q_{1},q_{2},,q_{K}\), say \(q_{k}\), must be \((,,)\)-concentrated. Further, if the classes are balanced, then all the class conditionals are \((C_{},,K)\)-concentrated. Here, \(=\{x f(x)=\}\), and \(C_{}=_{k}\{x f(x)=k\}\) are constants dependent on \(f\)._

The proof is a natural application of the Brunn-Minkowski theorem from high-dimensional geometry, essentially using the fact that an \(\)-shrinkage of a high-dimensional set has very small volume. We provide a brief sketch here, deferring the full proof to Appendix A.

Proof Sketch.: Due to the existence of a robust classifier \(f\), _i.e._, \(R(f,)\), the first observation is that there must be at least one class which is classified with robust accuracy at least \(1-\). Say this class is \(k\), and the set of all points which do not admit an \(\)-adversarial example for class \(k\) is \(S\). Now, the second step is to show that \(S\) has the same measure (under \(q_{k}\)) as the \(\)-shrinkage (in the \(_{2}\) norm) of the set of all points classified as class \(k\). Finally, the third step involves using the Brunn-Minkowski theorem, to show that this \(\)-shrinkage has a volume \(O((-n))\), thus completing the argument. 

**Discussion on Theorem 2.1.** We pause here to understand some implications of this result.

* Firstly, recall the apparently conflicting conclusions from Section 1 between impossibility results (suggesting that robust classifiers do not exist) and the existence of robust classifiers in practice (such as that of human vision for natural data distributions). Theorem 2.1 shows that whenever a robust classifier exists, the underlying data distribution has to be concentrated. In particular, this suggests that natural distributions corresponding to MNIST, CIFAR and ImageNet might be concentrated. This indicates a resolution to the conflict: concentrated distributions must somehow circumvent existing impossibility results. Indeed, this is precisely what we will show in Section 3.
* Secondly, while our results are derived for the \(_{2}\) norm, it is not very hard to extend this reasoning to general \(_{p}\) norms. In other words, whenever a classifier robust to \(_{p}\)-norm perturbations exists, the underlying data distribution must be concentrated.
* Thirdly, Theorem 2.1 has a direct implication towards classifier design. Since we now know that natural image distributions are concentrated, one should design classifiers that are tuned for small-volume regions in the input space. This might be the deeper principle behind the recent success  of robust classifiers adapted to \(_{p}\)-ball like regions in the input space.
* Finally, the _extent_ of concentration implied by Theorem 2.1 depends on the classifier \(f\), via the parameters \(,\) and \(\). On one hand, we get _high_ concentration when \(\) is large, \(\) is small, and \(\) is small. On the other hand, if the distribution \(p\) admits an \((,)\)-robust classifier such that \(\) is large (e.g., a constant classifier), then we get _low_ concentration via Theorem 2.1. This is not a limitation of our proof technique, but a consequence of the fact that some simple data distributions might be very lowly concentrated, but still admit very robust classifiers, e.g., for a distribution having \(95\%\) dogs and \(5\%\) cats, the constant classifier which always predicts "dog" is quite robust.

We have thus seen that data concentration is a necessary condition for the existence of a robust classifier. A natural question is whether it is also sufficient. We address this question now.

## 3 Strong Concentration Implies Existence of Robust Classifier

Say our distribution \(p\) is such that all the class conditionals \(q_{1},q_{2},,q_{k}\) are \((C,,)\)-concentrated. Is this sufficient for the existence of a robust classifier? The answer is negative, as we have not precluded the case where all of the \(q_{k}\) are concentrated over the same subset \(S\) of the ambient space. In other words, it might be possible that there exists a small-volume set \(S\) such that \(q_{k}(S)\) is high for all \(k\). This means that whenever a data point lies in \(S\), it would be hard to distinguishwhich class it came from. In this case, even an accurate classifier might not exist, let alone a robust classifier4. To get around such issues, we define a stronger notion of concentration, as follows.

**Definition 3.1** (Strongly Concentrated Distributions).: _A distribution \(p\) is said to be \((,,)\)-strongly-concentrated if each class conditional distribution \(q_{k}\) is concentrated over the set \(S_{k}\) such that \(q_{k}(S_{k}) 1-\), and \(q_{k}(_{k^{} k}S_{k^{}}^{+2})\), where \(S^{+}\) denotes the \(\)-expansion of the set \(S\) in the \(_{2}\) norm, i.e., \(S^{+}=\{x S\|x-\|_{2}\}\)._

In essence, Definition 3.1 states that each of the class conditionals are concentrated on subsets of the ambient space, which do not intersect too much with one another5. Hence, it is natural to expect that we would be able to construct a robust classifier by exploiting these subsets. Building upon this idea, we are able to show Theorem 3.1:

**Theorem 3.1**.: _If the data distribution \(p\) is \((,,)\)-strongly-concentrated, then there exists an \((,+)\)-robust classifier for \(p\)._

The basic observation behind this result is that if the conditional distributions \(q_{k}\) had disjoint supports which were well-separated from each other, then one could obtain a robust classifier by predicting the class \(k\) on the entire \(\)-expansion of the set \(S_{k}\) where the conditional \(q_{k}\) concentrates, for all \(k\). To go beyond this idealized case, we can exploit the strong concentration condition to carefully remove the intersections at the cost of at most \(\) in robust accuracy. We make these arguments more precise in the full proof, deferred to Appendix B, and we pause here to note some implications for existing results.

**Implications for Existing Impossibility Results.** To understand how Theorem 3.1 circumvents the previous impossibility results, consider the setting from  where the data domain is the sphere \(^{n-1}=\{x^{n}\|x\|_{2}=1\}\), and we have a binary classification setting with class conditionals \(q_{1}\) and \(q_{2}\). The adversary is allowed to make bounded perturbations w.r.t. the geodesic distance. In this setting, it can be shown (see [48, Theorem 1]) that any classifier admits \(\)-adversarial examples for the minority class (say class \(1\)), with probability at least

\[1-_{q_{1}}(-^{2}),\] (2)

where \(_{q_{1}}=_{x^{n-1}}q_{1}(x)\) depends on the conditional distribution \(q_{1}\), and \(\) is a normalizing constant that depends on the dimension \(n\). Note that this result assumes little about the conditional \(q_{1}\). Now, by constructing a strongly-concentrated data distribution over the domain, we will show that the lower bound in (2) becomes vacuous.

**Example 3.1**.: _The data domain is the unit sphere \(^{n-1}\) equipped with the geodesic distance \(d\). The label domain is \(\{1,2\}\). \(P\) is an arbitrary, but fixed, point lying on \(^{n-1}\). The conditional density of class \(1\), i.e., \(q_{1}\) is now defined as_

\[q_{1}(x)=d(x,P)},&d(x,P) 0.1\\ 0,&,\]

_where \(C=0.1\) is a normalizing constant. The conditional density of class 2 is defined to be uniform over the complement of the support of \(q_{1}\), i.e. \(q_{2}=(\{x^{n-1} d(x,P)>0.1\})\). Finally, the classes are balanced, i.e., \(p_{Y}(1)=p_{Y}(2)=1/2\)._

The data distribution constructed in Example 3.1 makes Eq. (2) vacuous, as the supremum over the density \(q_{1}\) is unbounded. Additionally, the linear classifier defined by the half-space \(\{x x,P(0.1)\}\) is robust (Appendix C provides a derivation of the robust risk, and further comments on generalizing this example). Example 3.1 is plotted for \(n=3\) dimensions in Fig. 1.

Figure 1: A plot of \(q_{1}\). Redder colors denote a larger density, and the gray plane denotes the robust classifier.

Compatibility of Theorem 3.1 with existing Negative ResultsThus, we see that strongly concentrated distributions are able to circumvent existing impossibility results on the existence of robust classifiers. However, this does _not_ invalidate any existing results. Firstly, measure-concentration-based results [48; 18; 12] provide non-vacuous guarantees given a _sufficiently flat_ (not concentrated) data distribution, and hence do not contradict our results. Secondly, our results are existential and do not provide, in general, an algorithm to _construct_ a robust classifier given a strongly-concentrated distribution. Hence, we also do not contradict the existing stream of results on the computational hardness of finding robust classifiers [6; 56; 47]. Our positive results are complementary to all such negative results, demonstrating a general class of data distributions where robust classifiers do exist.

For the reminder of this paper, we will look at a specific member of the above class of strongly concentrated data distributions and show how we can practically construct robust classifiers.

## 4 Adversarially Robust Classification on Union of Linear Subspaces

The union of subspaces model has been shown to be very useful in classical computer vision for a wide variety of tasks, which include clustering faces under varying illumination, image segmentation, and video segmentation . Its concise mathematical description often enables the construction and theoretical analysis of algorithms that also perform well in practice. In this section, we will study robust classification on data distributions concentrated on a union of low-dimensional linear subspaces. This data structure will allow us to obtain a non-trivial, practically relevant case where we can show a provable improvement over existing methods for constructing robust classifiers in certain settings. Before delving further, we now provide a simple example (which is illustrated in Fig. 2) demonstrating how distributions concentrated about linear subspaces are concentrated precisely in the sense of Definition 3.1, and therefore allow for the existence of adversarially robust classifiers.

**Example 4.1**.: _The data domain is the ball \(B_{_{}}(0,1)\) equipped with the \(_{2}\) distance. The label domain is \(\{1,2\}\). Subspace \(S_{1}\) is given by \(S_{1}=\{x x^{}e_{1}=0\}\), and \(S_{2}\) is given by \(S_{2}=\{x x^{}e_{2}=0\}\), where \(e_{1},e_{2}\) are the standard unit vectors. The conditional densities are defined as_

\[q_{1} =(\{x\|x\|_{} 1,|x^{}e_{1}|  e^{-}/2\}),\] \[q_{2} =(\{x\|x\|_{} 1,|x^{}e_{2}|  e^{-}/2\}),\]

_where \(>0\) is a large constant. Finally, the classes are balanced, i.e., \(p_{Y}(1)=p_{Y}(2)=1/2\). With these parameters, \(q_{1},q_{2}\) are both \((0.5,/n-1,0)\)-concentrated over their respective supports. Additionally, \(p\) is \((,0,e^{-}/2+2)\)-strongly-concentrated. A robust classifier \(f\) can be constructed following the proof of Theorem 3.1, and it obtains a robust accuracy \(R(f,) e^{-}/2+2\). See Appendix D for more details._

We will now study a specific choice of \(p\) that generalizes Example 4.1 and will let us move beyond the above simple binary setting. Recall that we have a classification problem specified by a data distribution \(p\) over the data domain \(=B(0,1)\{1,2,,K\}\). Firstly, the classes are balanced, _i.e._, \(p_{Y}(k)=1/K\) for all \(k\). Secondly, the conditional density, i.e., \(q_{k}=p_{X|Y=k}\), is concentrated on the set \(S_{k}^{+}\), where \(S_{k}\) is a low-dimensional linear subspace, and the superscript denotes an \(\)-expansion, for a small \(>0\).

For the purpose of building our robust classifier, we will assume access to a training dataset of \(M\)_clean_ data points \((s_{1},y_{1}),(s_{2},y_{2}),,(s_{M},y_{M})\), such that, for all \(i\), the point \(s_{i}\) lies exactly on one of the \(K\) low-dimensional linear subspaces. We will use the notation \(=[s_{1},s_{2},,s_{M}]\) for the training data matrix and \(=(y_{1},y_{2},,y_{M})\) for the training labels. We will assume that \(M\) is large enough that every \(x_{k}S_{k}\) can be represented as a linear combination of the columns of \(\).

Now, the robust classification problem we aim to tackle is to obtain a predictor \(g\) which obtains a low robust risk, with respect to an additive adversary \(\) that we now define. For any data-point \(x p\), \(\) will be constrained to make an additive perturbation \(v\) such that \(_{_{2}}(x+v,_{i}S_{i})\). In other words, the attacked point can have \(_{2}\) distance at most \(\) from any of the linear

Figure 2: A plot of \(q_{1}\) (orange), \(q_{2}\) (violet) and the decision boundaries of \(f\) (dashed).

subspaces \(S_{1},,S_{k}\). Note that \(\) is more powerful than an \(_{2}\)-bounded adversary as the norm of the perturbation \(\|v\|_{2}\) might be large, as \(v\) might be parallel to a subspace.

Under such an adversary \(\), given a (possibly adversarially perturbed) input \(x\), it makes sense to try to recover the corresponding point \(s\) lying on the union of subspaces, such that \(x=s+n\), such that \(\|n\|_{2}\). One way to do this is to represent \(s\) as a linear combination of a small number of columns of \(\), _i.e._, \(x=c+n\). This can be formulated as an optimization problem that minimizes the cardinality of \(c\), given by \(\|c\|_{0}\), subject to an approximation error constraint. Since such a problem is hard because of the \(_{0}\) pseudo-norm, we relax this to the problem

\[_{c}\|c\|_{1}\ \ \ \ \|x-c\|_{2}.\] (3)

Under a suitable choice of \(\), this problem can be equivalently written as

\[_{c,e}\|c\|_{1}+\|e\|_{2}^{2}\ \ \ \ x=c+e,\] (4)

for which we can obtain the dual problem given by

\[_{d} x,d-\|d\|_{2}^{2}\ \ \ \ \|^{}d\|_{} 1.\] (5)

Our main observation is to leverage the stability of the set of active constraints of this dual to obtain a robust classifier. One can note that each constraint of Eq. (5) corresponds to one training data point \(s_{i}\) - when the \(i^{}\) constraint is active at optimality, \(s_{i}\) is being used to reconstruct \(x\). Intuitively, one should then somehow use the label \(y_{i}\) while predicting the label for \(x\). Indeed, we will show that predicting the majority label among the active \(y_{i}\) leads to a robust classifier.

We will firstly obtain a geometric characterization of the problem in Eq. (5) by viewing it as the evaluation of a projection operator onto a certain convex set, illustrated in Fig. 3. Observe that for \(>0\), the objective (5) is strongly concave in \(d\) and the problem has a unique solution, denoted by \(d_{}^{*}(x)\). It is not hard to show that this solution can be obtained by the projection operator

\[d_{}^{*}(x)=(*{arg\,min}_{d}\| x-d\|_{2}\ \ \ \ \| ^{}d\|_{} 1)=_{K^{}}( x),\] (6)

where \(K^{}\) is the polar of the convex hull of \(\). Denoting \(=[,-]\), we can rewrite Problem (6) as \(d_{}^{*}(x)=(*{arg\,min}_{d}\| x-d\|_{2}\) sub. to \(^{}d)\). We now define the set of active constraints as

\[A_{}(x)=\{t_{i} t_{i},d_{}^{*}(x)=1\}.\] (7)

**Geometry of the Dual (5).** It is illustrated in Fig. 3, where \(s_{1},s_{2}\) are two chosen data-points. The blue shaded polytope is \(K^{}\). At \(=_{1}\), the point \(_{1}x\) lies in the interior of \(K^{}\). Hence, \(A_{}(x)\) is empty and \((c^{*}(x))\) is also empty. As \(\) increases, a non-empty support is obtained for the first time at \(=1/_{K^{}}(x)\). For all \(_{2}x\) in the red shaded polyhedron, the projection \(d_{_{2}}^{*}(x)=_{K^{}}(_{2}x)\) lies on the face \(F\). As \(\) increases further we reach the green polyhedron. Further increases in \(\) do not change the dual solution, which will always remain at the vertex \(d_{_{3}}^{*}(x)\).

Geometrically, \(A_{}(x)\) identifies the face of \(K^{}\) which contains the projection of \( x\), if \(A_{}(x)\) is non-empty (otherwise, \( x\) lies inside the polyhedron \(K^{}\)). The support of the primal solution, \(c^{*}(x)\), is a subset of \(A_{}\), _i.e._\((c^{*}(x)) A_{}(x)\). Note that whenever two points, say \(x,x^{}\), both lie in the same shaded polyhedron (red or green), their projections would lie on the same face of \(K^{}\). We now show this formally, in the main theorem of this section.

**Theorem 4.1**.: _The set of active constraints \(A_{}\) defined in (7) is robust, i.e., \(A_{}(x^{})=A_{}(x)\) for all \( x^{} C(x)\), where \(C(x)\) is the polyhedron defined as_

\[C(x)=F(x)+V(x),\] (8)

Figure 3: Geometry of the dual problem (5). See description on the left.

_with \(F K^{}\) being a facet of the polyhedron \(K^{}\) that \(x\) projects to, defined as_

\[F(x)=\{d\ |t_{i}^{}d=1, t_{i} A_{ }(x)\\ t_{i}^{}d<1,.\},\] (9)

_and \(V\) being the cone generated by the constraints active at (i.e., normal to) \(F\), defined as_

\[V(x)=\{_{t_{i} A_{}(x)}_{i}t_{i}_{i} 0, t_{i} A_{}(x)\}.\] (10)

The proof of Theorem 4.1 utilizes the geometry of the problem and properties of the projection operator, and is presented in Appendix E. We can now use this result to construct a robust classifier:

**Lemma 4.2**.: _Define the dual classifier as_

\[g_{}(x)=(\{y_{i} t_{i} A_{}(x)\}),\] (11)

_where Aggregate is any deterministic mapping from a set of labels to \(\), e.g., Majority. Then, for all \(x^{} C(x)\) as defined in Theorem 4.1, \(g_{}\) is certified to be robust, i.e., \(g_{}(x^{})=g_{}(x)\)._

**Implications.** Having obtained a certifiably robust classifier \(g\), we pause to understand some implications of the theory developed so far. We observe that the certified regions in Theorem 4.1 are not spherical, _i.e._, the attacker can make additive perturbations having large \(_{2}\) norm but still be unable to change the label predicted by \(g\) (see Fig. 4). This is in contrast to the \(_{2}\) bounded certified regions that can be obtained by most existing work on certification schemes, and is a result of modelling data structure while constructing robust classifiers. Importantly, however, note that we do not assume that the attack is restricted to the subspace.

**Connections to Classical Results.** For \(=0\), Eq. (3) is known as the primal form of the Basis Pursuit problem, and has been studied under a variety of conditions on \(\) in the sparse representation and subspace clustering literature . Given an optimal solution \(c^{*}(x)\) of this basis pursuit problem, how can we accurately predict the label \(y\)? One ideal situation could be that all columns in the support predict the same label, _i.e._, \(y_{i}\) is identical for all \(i(c^{*}(x))\). Indeed, this ideal case is well studied, and is ensured by necessary  and sufficient  conditions on the geometry of the subspaces \(S_{1},,S_{K}\). Another situation could be that _majority_ of the columns in the support predict the correct label. In this case, we could predict \((\{y_{i} i(c^{*}(x))\})\) to ensure accurate prediction. Theorem 4.1 allows us to obtain robustness guarantees which work for _any_ such aggregation function which can determine a single label from the support. Hence, our results can guarantee robust prediction even when classical conditions are not satisfied. Lastly, note that our Theorem 4.1 shows that the entire active set remains unperturbed - In light of the recent results in , this could be relaxed for specific choices of maps acting on the estimated support.

## 5 Experiments

In this section, we will compare our certified defense derived in Section 4 to a popular defense technique called Randomized Smoothing (RS) , which can be used to obtain state-of-the-art certified robustness against \(_{2}\) perturbations. RS transforms any given classifier \(f\) to a certifiably robust classifier \(g_{}^{}\) by taking a majority vote over inputs perturbed by Gaussian6 noise \((0,^{2}I)\), _i.e._,

\[g_{}^{}(x)=_{}(f)=*{arg\, max}_{k}_{v(0,^{2}I)}(f(x+v)=k).\] (12)

Then, at any point \(x\), \(g_{}^{}\) can be shown to be certifiably robust to \(_{2}\) perturbations of size at least \(r^{}(x)=^{-1}(p)\) where \(p=_{k}_{v(0,^{2}I)}(f(x+v)=k)\) denotes the maximum probability of any class under Gaussian noise.

It is not immediately obvious how to compare the certificates provided by our method described above and that of RS, since the sets of the space they certify are different. The certified region obtained by RS, \(C^{}(x)=\{\|x-\|_{2} r(x)\}\), is a sphere (orange ball in Fig. 4). In contrast, our certificate \(C_{}(x)\) from Theorem 4.1 is a polyhedron (resp., blue trapezoid), which, in general, is neither contained in \(C^{}(x)\), nor a superset of \(C^{}(x)\). Additionally, our certificate has no standard notion of _size_, unlike other work on elliptical certificates , making a size comparison non-trivial. To overcome these difficulties, we will evaluate two notions of _attack size_: in the first, we will compare the \(_{2}\) norms of successful attacks projected onto our polyhedron, and in the second, we will compare the minimum \(_{2}\) norm required for a successful attack. We will then combine our method with RS to get the best of both worlds, _i.e._, the green shape in Fig. 4. In the following, we present both these evaluations on the MNIST  dataset, with each image normalized to unit \(_{2}\) norm.

**Comparison along Projection on \(C_{}(x)\).** For the first case, we investigate the question: _Are there perturbations for which our method is certifiably correct, but Randomized Smoothing fails?_ For an input point \(x\), we can answer this question in the affirmative by obtaining an adversarial example \(\) for \(g^{}\) such that \(\) lies inside our certified set \(C_{}(x)\). Then, this perturbation \(v=x-\) is certified by our method, but has \(_{2}\) norm larger than \(r^{}(x)\) (by definition of the RS certificate).

To obtain such adversarial examples, we first train a standard CNN classifier \(f\) for MNIST, and then use RS7 to obtain the classifier \(g^{}_{}\). Then, for any \((x,y)\), we perform projected gradient descent to obtain \(=x^{T}\) by performing the following steps \(T\) times, starting with \(x^{0} x\):

\[x^{t}_{B_{_{2}}(x,)}x^{t-1}+ _{x}(g^{}_{}(x^{t}),y) x^{t}_{C_{}(x)}(x^{t})\] (13)

Unlike the standard PGD attack (step I), the additional projection (step II) is not straightforward, and requires us to solve a quadratic optimization problem, which can be found in Appendix F. We can now evaluate \(g^{}_{}\) on these perturbations to empirically estimate the robust accuracy over \(C_{}\), _i.e._,

\[()=_{x,y p_{}}  B_{_{2}}(x,) C_{}(x)g^{}() y.\]

The results are plotted in Fig. 5, as the dotted curves. We also plot the certified accuracies8 for comparison, as the solid curves. We see that the accuracy certified by RS drops below random chance (0.1) around \(=0.06\) (solid red curve). Similar to other certified defenses, RS certifies only a subset of the true robust accuracy of a classifier in general. This true robust accuracy curve is pointwise upper-bounded by the empirical robust accuracy curve corresponding to any attack, obtained via the steps I, II described earlier (dotted red curve). We then see that even the upper-bound drops below random chance around \(=0.4\), suggesting that this might be a large enough attack strength so that an adversary only constrained in \(_{2}\) norm is able to fool a general classifier. However, we are evaluating attacks lying on our certified set and it is still possible to recover the true

Figure 4: Comparing polyhedral and spherical certificates. Details in text.

Figure 5: Comparing RS with Our method for adversarial perturbations computed by repeating Steps I, II (13).

class (blue solid curve), albeit by our specialized classifier \(g_{}\) suited to the data structure. Additionally, this suggests that our certified set contains useful class-specific information - this is indeed true, and we present some qualitative examples of images in our certified set in Appendix F. To summarize, we have numerically demonstrated that _exploiting data structure in classifier design leads to certified regions capturing class-relevant regions beyond \(_{p}\)-balls_.

**Comparison along \(_{2}\) balls.** For the second case, we ask the question: _Are there perturbations for which RS is certifiably correct, but our method is not?_ When an input point \(x\) has a large enough RS certificate \(r^{}(x) r_{0}\), some part of the sphere \(B_{_{2}}(x,r^{}(x))\) might lie outside our polyhedral certificate \(_{}(x)\) (blue region in Fig. 4). In theory, the minimum \(r_{0}\) required can be computed via an expensive optimization program that we specify in Appendix F. In practice, however, we use a black-box attack  to find such perturbations. We provide qualitative examples and additional experiments on CIFAR-10 in Appendix F.

**Combining Our Method with RS.** We now improve our certified regions using randomized smoothing. For this purpose, we treat our classifier \(g_{}\) as the base classifier \(f\) in (12), to obtain \(_{}(_{})\) (abbreviated as \(_{}^{}\)). We then plot the \(_{2}\) certified accuracy\({}^{}\)[11, Sec 3.2.2] in Fig. 6, where note that, as opposed to Fig. 5, the attacks are _not_ constrained to lie on our certificate anymore.

As a final remark, we note that our objective in Fig. 6 was simply to explore RS as a method for obtaining an \(_{2}\) certificate for our method, and we did not tune our method or RS for performance. In particular, we believe that a wide array of tricks developed in the literature for improving RS performance [41; 45; 63] could be employed to improve the curves in Fig. 6. We now discuss our work in the context of existing literature in adversarial robustness, and sparse representation learning.

## 6 Discussion and Related Work

Our proof techniques utilize tools from high-dimensional probability, and have the same flavor as recent impossibility results for adversarial robustness [12; 48; 47]. Our geometric treatment of the dual optimization problem is similar to the literature on sparse-representation [13; 20] and subspace clustering [50; 51; 64; 25], which is concerned with the question of representing a point \(x\) by the linear combination of columns of a dictionary \(\) using sparse coefficients \(c\). As mentioned in Section 4, there exist geometric conditions on \(\) such that all such candidate vectors \(c\) are _subspace-preserving_, i.e., for all the indices \(i\) in the support of \(c\), it can be guaranteed that \(s_{i}\) belongs to the correct subspace. On the other hand, the question of classification of a point \(x\) in a union of subspaces given by the columns of \(\), or subspace classification, has also been studied extensively in classical sparse-representation literature [62; 61; 7; 27; 65; 65; 65; 39; 23; 32]. The predominant approach is to solve an \(_{1}\) minimization problem to obtain coefficients \(c\) so that \(x=c+e\), and then predict the subspace that minimizes the representation error. Various _global_ conditions can be imposed on \(\) to guarantee the success of such an approach , and its generalizations [15; 16]. Our work differs from these approaches in that we aim to obtain conditions on perturbations to \(x\) that ensure accurate classification, and as such we base our robust classification decision upon properties of solutions of the dual of the \(_{1}\) problem.

Our results are complementary to [44; 5], who obtain a lower bound on the robust risk \(R(f,)\), in a binary classification setting, in terms of the Wasserstein distance \(D\) between the class conditionals \(q_{0}\) and \(q_{1}\), _i.e._, \(R(f,) 1-D(q_{0},q_{1})\). Using this result, [44; 5] roughly state that the robust risk increases as the class conditionals get closer, _i.e._, it becomes more likely to sample \(x q_{0},x^{} q_{1}\), such that \(\|x-x^{}\|\). In comparison, for two classes, our Theorem 3.1 roughly states that for low robust risk, \(q_{0}\) and \(q_{1}\) should be concentrated on small-volume subsets separated from one another. Thus, the message is similar, while our Theorem 3.1 is derived in a more general multi-class setting. Note that [44; 5] do not explicitly require small-volume subsets, but this is implicit, as \(1-D(q_{0},q_{1})\)

Figure 6: Comparing Certified Accuracy after combining our method with RS.

is large due to concentration of measure when \(q_{0},q_{1}\) do not concentrate on very small subsets. We provide an analogue of the empirical results of  in Appendix F.6.

Our notion of concentration is also related to concentration of a measure \(\), as considered in , which denotes how much the \(\)-measure of any set _blows up_ after expansion by \(\). Under this definition, the uniform measure has a high degree of concentration in high dimensions, and this is called the concentration of measure phenomenon. In contrast, our Definition 2.2 of concentrated data distributions can be seen as a _relative_ notion of concentration with respect to the uniform measure \(\), in that we call a class conditional \(q\) concentrated when it assigns a large \(q\)-measure to a set of very small volume, _(i.e._, \(q(S)\) is high whereas \((S)\) is very low). In essence, Definition 2.2 defines concentration relative to the uniform distribution, whereas  define concentration independently. Definition 2.2 is useful in the context of adversarial robustness as it allows us to separate the concentration of data distributions (which is unknown) from the concentration of the uniform measure in high dimensions (for which there is a good understanding). This allows us to derive results in the non-realizable setting, where errors are measured against a probabilistic ground truth label \(Y\), which is strictly more general than the realizable setting which requires a deterministic ground truth classifier \(f^{*}\). As a result of this realizable setting, the analysis in  needs to assume a non-empty error region \(A\) for the learnt classifier \(g\) with respect to \(f^{*}\), in order to reason about \(\)-expansions \(A^{+}\). The results in  indicate that the robust risk grows quickly as the volume of \(A\) increases. However, humans seem to be a case where the natural accuracy is not perfect (e.g., we might be confused between a 6 and a poorly written \(5\) in MNIST), yet we seem to be very robust against small \(_{2}\) perturbations. This points to a slack in the analysis in , and our work fills this gap by considering \(\) expansions of a different family of sets.

Finally, our work is also related to recent empirical work obtaining robust classifiers by _denoising_ a given input \(x\) of any adversarial corruptions, before passing it to a classifier . However, such approaches lack theoretical guarantees, and might be broken using specialized attacks . Similarly, work on improving the robustness of deep network-based classifiers by adversarial training off the data-manifold can be seen as an empirical generalization of our attack model . More generally, it has been studied how adversarial examples relate to the underlying data-manifold . Recent work also studies the robustness of classification using projections onto a single low-dimensional linear subspace . The work in  studies an the attack model of bounded \(_{2},_{}\) attacks, and they provide robustness certificates by obtaining guarantees on the distortion of a data-point \(x\) as it is projected onto a single linear subspace using a projection matrix \(\). In contrast, our work can be seen as projecting a perturbed point onto a union of multiple low-dimensional subspaces. The resultant richer geometry allows us to obtain more general certificates.

## 7 Conclusion and Future Work

To conclude, we studied conditions under which a robust classifier exists for a given classification task. We showed that concentration of the data distribution on small subsets of the input space is necessary for any classifier to be robust to small adversarial perturbations, and that a stronger notion of concentration is sufficient. We then studied a special concentrated data distribution, that of data distributed near low-dimensional linear subspaces. For this special case of our results, we constructed a provably robust classifier, and then experimentally evaluated its benefits w.r.t. known techniques.

For the above special case, we assume access to a _clean_ dataset \(\) lying perfectly on the union of low-dimensional linear subspaces, while in reality one might only have access to noisy samples. In light of existing results on noisy subspace clustering , an immediate future direction is to adapt our guarantees to support noise in the training data. Similarly, while the assumption of low-dimensional subspace structure in \(\) enables us to obtain novel unbounded robustness certificates, real world datasets might not satisfy this structure. We hope to mitigate this limitation by extending our formulation to handle data lying on a general image manifold in the future.