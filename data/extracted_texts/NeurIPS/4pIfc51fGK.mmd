# Tianrui Liu\({}^{1}\) En Zhu\({}^{1}\) Kunlun He\({}^{3}\) Xinwang Liu\({}^{1}\)

Alleviate Anchor-Shift: Explore Blind Spots with Cross-View Reconstruction for Incomplete Multi-View Clustering

 Suyuan Liu\({}^{1}\) Siwei Wang\({}^{2}\) Ke Liang\({}^{1}\) Junpu Zhang\({}^{1}\) Zhibin Dong\({}^{1}\)\({}^{1}\)National University of Defense Technology, Changsha, China

\({}^{2}\)Academy of Military Sciences, Beijing, China

\({}^{3}\) Chinese PLA General hospital, Beijing, China

{suyuanliu, enzhu, xinwangliu}@nudt.edu.cn

Corresponding Author

###### Abstract

Incomplete multi-view clustering aims to learn complete correlations among samples by leveraging complementary information across multiple views for clustering. Anchor-based methods further establish sample-level similarities for representative anchor generation, effectively addressing scalability issues in large-scale scenarios. Despite efficiency improvements, existing methods overlook the misguidance in anchors learning induced by partial missing samples, _i.e.,_ the absence of samples results in shift of learned anchors, further leading to sub-optimal clustering performance. To conquer the challenges, our solution involves a cross-view reconstruction strategy that not only alleviate the anchor shift problem through a carefully designed cross-view learning process, but also reconstructs missing samples in a way that transcends the limitations imposed by convex combinations. By employing affine combinations, our method explores areas beyond the convex hull defined by anchors, thereby illuminating blind spots in the reconstruction of missing samples. Experimental results on four benchmark datasets and three large-scale datasets validate the effectiveness of our proposed method.

## 1 Introduction

In multi-view learning, data collected from different sensors or media often suffer from missing values . For example, remote sensing images collected from various sensors may experience partial missing due to channel noise. Traditional multi-view learning methods cannot directly handle these missing values . To address this issue, incomplete multi-view learning methods have been developed to leverage the available data from all views to perform downstream tasks . Among these, incomplete multi-view clustering (IMC) relies on view consistency and complementarity, effectively enabling the partitioning of data with missing values .

Existing IMC methods can be categorized into three types based on their approach: similarity-based, imputation-based, and matrix decomposition-based methods. Similarity-based methods recover a relationship matrix among all samples using available data in each view . Imputation-based methods first fill in the missing parts, transforming the problem into a complete multi-view clustering problem . Matrix decomposition-based methods map samples from all views into a common latent space to construct a unified representation for clustering . However, these methods all require constructing an \(n n\) similarity matrix,resulting in an \((n^{2})\) space complexity, which limits their application to large-scale scenarios [26; 27; 28].

In contrast, anchor-based IMC methods reduce space complexity to \((n)\) by learning a small number of representative anchors from the data and replacing the full similarity matrix with relationships between anchors and samples [29; 30; 29; 31]. Despite addressing scalability, these methods overlook the anchor-shift problem caused by missing data. As shown in Fig. 1(a)(b), anchor learning can be misled by missing data, resulting in a discrepancy between learned anchors and those from complete data. This problem diminishes the representational capacity of anchors and causes the anchor graphs to be misaligned across views, thereby compromising the clustering performance.

In this work, we propose a novel Anchor-based Incomplete Multi-view Clustering with Cross-View Reconstruction strategy, termed AIMC-CVR. AIMC-CVR encompasses two key modules designed to resolve the anchor-shift problem in anchor-based IMC. The first module, the cross-view anchor learning module, is dedicated to mitigating the anchor-shift problem by learning a complete anchor graph. Specifically, we designed a symmetrized cross-view projection mechanism to ensure dimensional consistency across view pairs. By leveraging the relationships between anchors and samples across different view pairs, we constructed a complete anchor graph.

The second module, the affine combination-based reconstruction module, iteratively updates the anchors with available and reconstructed data. Existing anchor-based IMC methods often use convex constraints to build relationships between anchors and samples [32; 33; 34; 35], leading to blind spots in sample reconstruction. As shown in Fig. 1(c), samples reconstructed based on convex combinations are restricted to the convex hull of the anchors. Instead of relying on convex combinations, our innovative approach utilizes an affine combination-based reconstruction strategy. This strategy broadens the scope of sample reconstruction, as depicted in Fig. 1(d), allowing for a more comprehensive and accurate representation of missing data.

Our main contributions are as follows:

* By employing cross-view anchor learning and affine combination-based reconstruction, we effectively alleviate the anchor-shift problem in missing data scenarios.
* Unlike traditional sample-level imputation methods, we reconstruct missing samples with anchors and anchor graphs, significantly reducing reconstruction complexity. Affine combinations further explore blind spots in sample reconstruction, as demonstrated by theoretical analysis and experimental results.
* Comparative experiments with state-of-the-art IMC and anchor-based IMC methods validate the effectiveness and superiority of AIMC-CVR.

## 2 Related Work

**Cross-View Learning.** Cross-view learning is a specialized form of multi-view learning that leverages interactions between paired views to learn cross-view representations, enabling the exploration of finer-grained inter-view relationships [36; 37; 38]. For instance, Tang et al. ensure local structural

Figure 1: (a)Anchors learned in complete data. (b)Anchors initialized in incomplete data. (c)Data reconstructed with convex combination. (d)Data reconstructed with affine combination.

consistency across views by simultaneously constructing similarity graphs between pairs of views and within each view . Similarly, Liu et al. leverage the fact that samples are always present in at least one view, constructing a complete cross-view similarity matrix based on relationships within and between views . These methods demonstrate the potential of cross-view learning to maintain structural consistency and fully utilize the available data. In the next section, we will explore the application of cross-view learning strategies to multi-view anchor learning, focusing on view complementarity to construct a complete anchor graph and mitigate the anchor-shift problem in scenarios with missing data.

**Sample-Level Imputation.** Imputation-based multi-view clustering methods first impute the missing data before clustering [41; 42; 21]. Typically, existing methods perform sample-level imputation by leveraging multi-view information to learn complete similarity relationships between samples. For example, Yin et al. reconstruct missing data based on the decomposition matrix and sample similarity relationships . Liu et al. integrate data imputation and clustering within a unified optimization framework . However, these sample-level imputation methods typically rely on similarity matrices of size \((n^{2})\), which limits their scalability for large datasets.

## 3 Method

In this section, we introduce the Anchor-based Incomplete Multi-view Clustering with Cross-View Reconstruction (AIMC-CVR) strategy. We begin by introducing the two core modules: the cross-view anchor learning module and the affine combination-based reconstruction module. We provide a theoretical analysis that demonstrate the merits of our proposed affine combination-based reconstruction strategy. Finally, we present the overall objective function of the algorithm and propose a four-step alternating iterative algorithm to solve the corresponding optimization problem.

### Cross-View Anchor Learning Module

Given multi-view data \(^{(p)}^{d_{p} n}}_{p=1}^{v}\), where \(d_{p}\) and \(n\) denotes the dimension of the data and the total number of samples, respectively.

In the context of incomplete data scenarios, the data matrix \(^{(p)}\) for the \(p\)-th view can be partitioned into two distinct subsets, _i.e._, \([^{(p)}_{o},^{(p)}_{m}]\), where \(^{(p)}_{o}^{d_{p} n_{p}}\) representing the existing portion of the data, and \(^{(p)}_{m}^{d_{p}(n-n_{p})}\) is the missing portion. The observed part is obtained by applying an index matrix \(^{(p)}\{0,1\}^{n n_{p}}\) to the complete data matrix, such that \(^{(p)}_{o}=^{(p)}^{(p)}\). The index matrix \(^{(p)}\) encodes the presence of samples, where indicates that the \(i\)-th sample in the complete dataset corresponds to the \(j\)-th ranked existing sample in the observed subset \(^{(p)}_{o}\). Based on the samples present in each view, we can learn the anchors and their corresponding anchor graphs as follows:

\[_{^{(p)},^{(p)}}& _{p=1}^{v}\|^{(p)}_{o}-^{(p)} ^{(p)}^{(p)}\|_{}^{2},\\ &^{(p)}=,^{(p)} 0. \]

The anchor graph \(^{(p)}\) is incomplete, representing only the relationship between existing samples and anchors in the \(p\)-th view. While a complete anchor graph can be synthesized from all views through late-fusion, this process is compromised by the inherent misalignment of anchor graphs caused by view discrepancy. This discrepancy, known as the anchor-shift problem, arises because anchors learned from \(n_{p}\) samples differ from those learned from complete data. Consequently, varying missing samples across views lead to inconsistent anchor-shift, resulting in misaligned anchor graphs at the representation level.

To address this issue, we propose a cross-view anchor learning module. This module constructs a complete anchor graph under the assumption that each sample appears in at least one view in the incomplete multi-view scenario. Specifically, for the anchor graph \(^{(p)}\) in the \(p\)-th view, we update it with the following objective:\[_{^{(pq)},^{(p)},^{(p)}}_{q=1}^ {v}\|^{(pq)}_{o}^{(q)}-^{(qp)}^{(p )}^{(p)}^{(q)}\|_{}^{2},\] (2) s.t. \[^{(pq)}{}^{}^{(pq)}=,{ ^{(p)}}{}^{}=,^{(p)} 0,\]

where \(^{(pq)}\) is the projection matrix, which projects the dimension-reduced data into a higher-dimensional space. Unlike previous approaches that reduce all data to the same lower dimension, we aim to preserve high-dimensional features across views as much as possible to ensure the effectiveness of cross-view learning. A symmetric cross-view projection mechanism is designed to ensure dimensional consistency between different view pairs: if \(d_{p}>d_{q}\), \(^{(pq)}\) is the projection matrix that needs to be optimized; otherwise, \(^{(pq)}\) equals the identity matrix, as shown below:

\[^{(pq)}=\{^{(pq)}^{d _{p} d_{q}},&d_{p}>d_{q},\\ ^{d_{q} d_{q}},&d_{p} d_{q}.. \]

By sequentially learning the similarity between the \(n_{q}\) points present in the \(q\)-th view (where \(q=1,,v\)) and the \(m\) anchors in the \(p\)-th view, we can obtain an anchor graph with a size of \(m n\). Furthermore, the complete anchor graph can guide the learning of anchors, thereby implicitly alleviating the anchor-shift problem.

### Affine Combination-based Reconstruction Module

The cross-view anchor learning module fills the missing columns of the anchor graph by measuring the distance between the existing samples in other views and the anchors of the current view in the same space with a projection matrix, which avoids recovering the missing samples. However, the measurement of similarity between cross-view representations is overly dependent on the reliability of the projection matrix and the consistency between views. Additionally, simply relying on cross-view information to implicitly correct anchor shifts is insufficient. Therefore, we propose to recover the incomplete data in each view to directly correct the anchors affected by the missing parts. Traditional imputation methods, which reconstruct missing data based on the similarity among all samples, require an additional quadratic space complexity, making them impractical for large-scale problems. Thus, we propose a fast reconstruction module, as follows:

\[_{_{m}^{(p)},^{(p)},^{(p)}} _{p=1}^{v}\|[_{o}^{(p)},_{m}^{(p )}]-^{(p)}^{(p)}\|_{}^{2},\] (4) s.t. \[^{(p)}{}^{}=,^{(p)} 0,\]

where \(_{o}^{(p)}^{d_{p} n_{p}}\) represents the existing samples in the view, and \(_{m}^{(p)}^{d_{p} n-n_{p}}\) represents the missing samples to be reconstructed. \(_{m}^{(p)}\) is constructed from the anchors and their corresponding anchor graphs. The reconstructed \(_{m}^{(p)}\) then participates in the next iteration of anchor learning, with the reconstruction of missing samples and anchor learning iterating and mutually reinforcing each other. Accurately reconstructed \(_{m}^{(p)}\) enables the learned anchors in the next iteration to be closer to the true global anchors, whereas inaccurate reconstruction exacerbates the anchor-shift problem. However, the reconstructed missing samples are constrained within the convex hull of the anchor set in Eq. (4). According to Theorem 1, there always exist samples that cannot be reconstructed by the convex combination of anchors.

**Theorem 1**.: _Suppose \(=\{a_{1},,a_{m}\}\) is an anchor set composed of cluster centers from the dataset \(=\{x_{1},,x_{n}\}\). Then, there always exist a sample \(c\) belonging to \(\) that lies outside the convex hull of the anchor set \(\). Mathematically,_

\[_{}\|_{i=1}^{m}f_{i}a_{i}-c\|_{2}^{2 }>0, c,\] (5) _s.t._ \[^{}=1, 0.\]

When most missing samples are outside the convex hull of the anchors, the next iteration of anchor learning erroneously shifts inward, worsening anchor-shift. Therefore, we propose an affine combination based reconstruction strategy, which relaxes convex constraints on anchor graph rows to affine ones.

\[_{_{m}^{(p)},^{(p)},^{(p)}}_ {p=1}^{v}\|[_{o}^{(p)},_{m}^{(p)}]-^{(p)}^{(p)}\|_{}^{2}, \] \[^{(p)}{}^{}=.\]

**Theorem 2**.: _For a sample \(c\) lying outside the convex hull of the anchor set \(\), the representation constructed by the affine combination of set \(\) is always closer to \(c\) than that constructed by the convex combination of set \(\). Mathematically,_

\[_{}\|_{i=1}^{m}g_{i}a_{i}-c\|_{2}^{2} <_{}\|_{i=1}^{m}f_{i}a_{i}-c\|_{2}^{2}, \] \[^{}=1,^{} =1, 0.\]

According to Theorem 2, utilizing the affine combination of anchors facilitates the recovery of more accurate missing samples. Ultimately, by combining the cross-view anchor learning module with the affine-combination based reconstruction module, the objective of AIMC-CVR is as follows:

\[_{}_{p=1}^{v}\|[_ {o}^{(p)},_{m}^{(p)}]-^{(p)}^{(p)}\| _{}^{2}+_{p=1}^{v}\|^{(p)}\|_{ }^{2} \] \[+_{p=1}^{v}_{q=1}^{v}\|^{(pq)}_{o}^{(q)}-^{(qr)}^{(p)}^{(p)} ^{(q)}\|_{}^{2},\] \[^{(pq)}{}^{}^{(pq)}=, ^{(p)}{}^{}=,\]

where \(=\{_{m}^{(p)},^{(pq)},^{(p)}, ^{(p)}\}\). The hyperparameter \(\) helps to adjust the sparsity of the anchor graph, and \(\) is a hyperparameter balancing the influence of the two modules. Finally, we concatenate the anchor graph from each view to obtain the common one, \(=[^{(1)};;^{(v)}]\), which avoids the anchor alignment problem present in other fusion methods . Note that the columns of \(\) still consist of ones, ensuring that its recovered transition probability matrix \(\) is a doubly stochastic matrix, as proven in the appendix. Therefore, directly performing \(k\)-means clustering on the left singular vectors of \(\) yields the final clustering results .

### Optimization

To solve the optimization problem in Eq. (8), we propose a four step alternating iterative algorithm. When optimizing a variable, the other variables are fixed to their previous iteration values. Additionally, since each variable is independent across views, we update them sequentially by view.

**Step 1:** Update \(_{m}^{(p)}\). Fixing \(^{(pq)}\), \(^{(p)}\), \(^{(p)}\) and removing terms unrelated to \(_{m}^{(p)}\), we have the following optimization problem:

\[_{_{m}^{(p)}}(_{m}^{(p)}{}^{ }_{m}^{(p)}-2_{m}^{(p)}{}^{}^{(p)}^{(p)}^{(p)}), \]

where \(^{(p)}\{0,1\}^{n n_{p}}\) is the index matrix, \(_{ij}^{(p)}=1\) indicates that the \(i\)-th sample is ranked \(j\)-th among the missing samples. By taking the derivative of Eq. (9) with respect to \(_{m}^{(p)}\) and setting it to zero, we obtain the closed-form solution for \(_{m}^{(p)}\) as follows:

\[_{m}^{(p)}=^{(p)}^{(p)}^{(p)}. \]

The solution for \(_{m}^{(p)}\) shows that it incorporates the anchors and anchor graphs from the previous iteration, contributing to the update of anchors in the next iteration.

**Step 2:** Update \(^{(pq)}\). Fixing \(_{m}^{(p)}\), \(^{(p)}\), \(^{(p)}\) and removing terms unrelated to \(^{(pq)}\), we have the following optimization problem when \(d_{p}<d_{q}\):

\[_{^{(pq)}}\|_{o}^{(p)}- ^{(pq)}^{(q)}^{(p)}^{(p)}\|_{ }^{2}+\|^{(pq)}_{o}^{(q)}-^{(p)} ^{(p)}^{(q)}\|_{}^{2},\\ ^{(pq)}{}^{}^{(pq)}=. \]

By further simplification, Eq. (11) can transform into the following form:

\[_{^{(pq)}}( ^{(pq)}{}^{}^{(pq)}),\\ ^{(pq)}{}^{}^{(pq)}=. \]

where \(^{(pq)}=_{o}^{(p)}(^{(q)}^{(q)} ^{(p)}){}^{}+^{(p)}^{(p)}^{(q)} _{o}^{(p)}{}^{}\). According to reference , the optimal \(^{(pq)}\) is derived from the product of the left and right singular vectors of \(^{(pq)}\).

**Step 3:** Update \(^{(p)}\). Fixing \(_{m}^{(p)}\), \(^{(pq)}\), \(^{(p)}\) and removing terms unrelated to \(^{(p)}\), we have the following optimization problem:

\[_{^{(p)}}(^{(p)}^{(p)} ^{(p)}{}^{}-2^{(p)}{}^{}^{(p)}), \]

where \(^{(p)}=^{(p)}^{(p)}{}^{}+_{q=1}^ {v}^{(p)}^{(q)}^{(q)}{}^{}^{(p)}{ }^{}\), \(^{(p)}=^{(p)}^{(p)}{}^{}+_{q=1}^ {v}^{(qp)}{}^{}^{(pq)}_{o}^{(q)}{}^{} ^{(p)}{}^{}\). By taking the derivative of Eq. (13) with respect to \(^{(p)}\) and setting it to zero, we obtain the closed-form solution for \(^{(p)}\) as follows:

\[^{(p)}=^{(p)}{}^{^{(p)}}{}^{-1}. \]

**Step4:** Update \(^{(p)}\). Since each column of \(^{(p)}\) is independent of others, we optimize the \(i\)-th column \(_{i}^{(p)}\) of \(^{(p)}\) while fixing \(_{m}^{(p)}\), \(^{(pq)}\), \(^{(p)}\) as follows:

\[_{_{i}^{(p)}}{}^{} _{i}^{(p)}{}^{-2}_{i}^{}_{i}^{(p)},\\ _{i}^{(p)}{}^{}=1. \]

where \(=(_{q=1}^{v}_{i}^{(q)}+1)^{(p)}{}^{} ^{(p)}+\), \(_{i}=_{q=1}^{v}_{i}^{(q)}_{:,i}^{(q)}{}^{ }^{(pq)}{}^{}^{(qp)}^{(p)}+_{:, i}^{(p)}{}^{}^{(p)}\). When \(i\)-th sample exists in the \(p\)-th view \(_{i}^{(q)}=1\), else \(_{i}^{(q)}=0\). We employ the Lagrange multiplier method to tackle the above problem. Firstly, the Lagrangian function for Eq. (15) is as follows:

\[=_{i}^{(p)}{}^{}_{i}^{(p)}-2 _{i}^{}_{i}^{(p)}+_{i}(_{i}^{(p)}{}^{ }-1), \]

where \(_{i}\) is the Lagrangian multiplier. The corresponding KTT conditions is

\[\{_{i}^{(p)}-_{i}+ {_{i}}{2}=0,\\ {}^{_{i}^{(p)}}{}^{}=1.. \]

By substituting the first term into the second, we can get \(_{i}=2^{-1}_{i})^{}-1}{ ^{-1}}\).Then we have \(_{i}^{(p)}=^{-1}(_{i}-)\). The entire optimization procedure for AIMC-CVR is summarized in Algorithm 1.

```
0: Multi-view dataset \(\{^{(p)}\}_{p=1}^{v}\), anchors number \(m\), clusters number \(k\), parameters \(\), \(\).
1: Initialize \(\{^{(p)}\}_{p=1}^{v},\{^{(pq)}\}_{p,q= 1}^{v}\) and \(\{^{(p)}\}_{p=1}^{v}\)
2:while not converged do
3:for\(p=1 v\)do
4: Update \(_{m}^{(p)}\) with Eq. (10).
5:for\(q=1 v\)do
6: Update \(^{(pq)}\) by solving Eq. (12).
7:endfor
8: Update \(^{(p)}\) with Eq. (14).
9: Update \(^{(p)}\) by solving Eq. (15).
10:endfor
11:endwhile
12:Concatenate \(^{(p)}\) to obtain \(\).
13:Performing \(k\)-means on \(\) to get the final cluster results.
```

**Algorithm 1** The proposed AIMC-CVR

### Complexity Analysis

The computational complexity of AIMC-CVR primarily consists of solving four variables. When updating \(_{m}^{(p)}\), the complexity of matrix multiplication is \((nmd_{p})\). For updating \(^{(pq)}\), the complexity of matrix multiplication is \((nmd_{p}+{nd_{p}}^{2}+{d_{p}}^{3})\), and the SVD decomposition complexity is \(({d_{p}}^{3})\). When optimizing \(^{(p)}\), the complexity of matrix multiplication is \((nmd_{p}+m^{2}d_{p}+nd_{p}d_{q})\), and the complexity of inversion is \((m^{3})\). When optimizing \(^{(p)}\), the complexity of matrix multiplication is \((m^{2}d_{p}+nmd_{p}+nd_{p}d_{q})\). Therefore, in each iteration, AIMC-CVR consumes a time complexity of \((n_{p=1}^{v}(md_{p}+{d_{p}}^{2}+d_{p}_{q=1}^{v}d_{q})+_{ p=1}^{v}(m^{2}d_{p}+d_{p}^{3}))\), which is linear with respect to \(n\).

The space complexity of AIMC-CVR is \((n(m+_{p=1}^{v}d_{p})+m_{p=1}^{v}d_{p}+_{p=1}^{v}_{ q=1}^{v}d_{p}d_{q})\), primarily stemming from storing relevant matrix variables, also scales linearly with the number of samples.

### Convergence Analysis

In this section, we provide a theoretical analysis of the convergence of our proposed AIMC-CVR. The objective function in Eq. (8) is non-convex when considering all variables simultaneously. To address this, we employ a four-step iterative optimization algorithm, detailed in Algorithm 1, where each variable is optimized sequentially while keeping the others fixed. During each iteration, the variables being optimized have analytical solutions, ensuring that the objective function of AIMC-CVR decreases monotonically with successive iterations. Furthermore, since the objective function in Eq. (8) is bounded below by zero, our proposed AIMC-CVR is guaranteed to converge to a local minimum.

## 4 Experiments

### Experimental Setup

**Datasets description.** Seven widely used multi-view datasets are employed to evaluate the performance of AIMC-CVR, including: MSRCV  is composed of images from seven categories. WebKB2 contains text and citations collected from the website. Wiki  is a dual-view dataset with text-image pairs. Hdigit3 is a dataset composed of handwritten digit images. YTF10 and YTF20are two subsets extracted from the YouTubeFace4 dataset. MNIST5 is a subset extracted from a larger dataset supplied by NIST. Details of these datasets are list in the Table 1. Following , we randomly remove 10% to 90% of the samples in 10% intervals to create missing versions of the above datasets, which ensures each sample exists in at least one view.

**Compared methods.** We compared our proposed method with the following eight state-of-the-art incomplete multi-view clustering algorithms: DAIMC, UEAF, EEIMVC, FLSD, V\({}^{3}\)H, IMVC-CBG, SCBGL, DVSAI. The first five comparison methods are traditional similarity-based approaches, while the latter three are large-scale anchor-based methods.

**Implementation details.** For all comparison algorithms, we set the parameters according to their descriptions in the corresponding literature. In our method, the anchors number is searched in \([k,2k,3k]\), and the parameter \(\) and \(\) are both searched in \([0.001,0.01,0.1,1,10,100]\). To evaluate the clustering performance, we employ accuracy (ACC), normalized mutual information (NMI), Purity, and Fscore for comparison. Each compared algorithm was tested across datasets with different missing rates, and the results for each missing rate were averaged to obtain the clustering results. Additionally, we conduct 20 repetitions of the \(k\)-means step for all algorithms and calculate the mean and variance for the final experimental result. All experiments were conducted on a desktop computer equipped with an Intel Core i9-10900X CPU, 64GB of RAM.

### Clustering Performance Comparison

We compare AIMC-CVR with eight state-of-the-art algorithms across seven datasets, as shown in Table 2. Our algorithm demonstrates superior or competitive clustering performance across all datasets, highlighting its effectiveness. On smaller datasets such as MSRCV, WebKB, Wiki, and Hdigit, our method achieves the highest ACC in all cases, surpassing the second-best algorithms by 3.65%, 7.89%, 0.19% and 3.96%. This showcases our algorithm's robustness and efficacy in handling incomplete multi-view datasets. Furthermore, Fig. 2 shows the clustering accuracy (ACC) curves for all algorithms across different datasets as the missing rate varies. Our algorithm consistently outperforms all others at every missing rate, demonstrating its superiority.

Additionally, our method demonstrates excellent scalability, successfully processing three large-scale datasets that the traditional methods cannot handle due to memory constraints. By addressing the anchor-shift problem, which was overlooked by other three anchor-based methods (IMVC-CBG, SCBGL, DVSAI), we achieved a notable enhancement in clustering performance. Specifically, on YTF10, YTF20, and MNIST datasets, our algorithm achieves ACC improvements over the second-best algorithms by 3.18%, 0.42% and 7.47%.

### Ablation Study

To showcase the effectiveness of different modules and strategies, we constructed five variants of AIMC-CVR as follows: (1) AIMC-CVR-v1 removes the cross-view anchor learning module by setting \(=0\). (2) AIMC-CVR-v2 removes the affine combination-based reconstruction module. (3) AIMC-CVR-v3 removes the sparsity regularization term by setting \(=0\). (4) AIMC-CVR-v4 keeps the initialized \(^{(p)}\) fixed and does not update it during subsequent optimization. (5) AIMC-CVR-v5 replaces affine combination with convex combination by adding non-negative constraints to \(^{(p)}\).

   Datasets & \#Samples (n) & \#Views (v) & \#Clusters (k) & \#Dimensionality (d\_p) \\  MSRCV & 210 & 3 & 7 & 256 & 512 & 1302 & - \\ WebKB & 1051 & 2 & 2 & 334 & 2949 & - & - \\ Wiki & 2866 & 2 & 10 & 10 & 128 & - & - \\ Hdigit & 10000 & 2 & 10 & 256 & 784 & - & - \\ YTF10 & 38654 & 4 & 10 & 512 & 576 & 640 & 944 \\ YTF20 & 63896 & 4 & 20 & 512 & 576 & 640 & 944 \\ MNIST & 70000 & 4 & 9 & 512 & 576 & 640 & 944 \\   

Table 1: Employed datasets in experiments.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]