# Optimal Algorithms for Online Convex Optimization with Adversarial Constraints

Abhishek Sinha, Rahul Vaze

School of Technology and Computer Science

Tata Institute of Fundamental Research

Mumbai 400005, India

abhishek.sinha@tifr.res.in, rahul.vaze@gmail.com

###### Abstract

A well-studied generalization of the standard online convex optimization (OCO) framework is constrained online convex optimization (COCO). In COCO, on every round, a convex cost function and a convex constraint function are revealed to the learner after it chooses the action for that round. The objective is to design an online learning policy that simultaneously achieves a small regret while ensuring a small cumulative constraint violation (CCV) against an adaptive adversary interacting over a horizon of length \(T\). A long-standing open question in COCO is whether an online policy can simultaneously achieve \(O()\) regret and \(()\) CCV without any restrictive assumptions. For the first time, we answer this in the affirmative and show that a simple first-order policy can simultaneously achieve these bounds. Furthermore, in the case of strongly convex cost and convex constraint functions, the regret guarantee can be improved to \(O( T)\) while keeping the CCV bound the same as above. We establish these results by effectively combining adaptive OCO policies as a blackbox with Lyapunov optimization - a classic tool from control theory. Surprisingly, the analysis is short and elegant.

## 1 Introduction

Online convex optimization (OCO) is a standard framework for modelling and analyzing a broad family of online decision problems under uncertainty. In the OCO problem, on every round \(t\), an online policy first selects an action \(x_{t}\) from a closed and convex admissible set (_a.k.a._ decision set) \(\). Then the adversary reveals a convex cost function \(f_{t}\), resulting in a cost of \(f_{t}(x_{t})\). The goal of an online policy is to choose an admissible action sequence \(\{x_{t}\}_{t=1}^{T}\) so that its cumulative cost is not much larger than that of any fixed admissible action chosen in hindsight. In particular, the objective is to minimize the static regret defined below

\[_{T}_{\{f_{t}\}_{t=1}^{T}}_{x^{*}} _{T}(x^{*}),_{T}(x^{*})_{t=1}^{T}f_{t}(x_{t})- _{t=1}^{T}f_{t}(x^{*}).\] (1)

The term _static_ refers to using a fixed benchmark, specifically only one action \(x^{*}\) throughout the horizon of length \(T\).

In this paper, we consider a generalization of the standard OCO framework. In this problem, on every round \(t\), the online policy first chooses an admissible action \(x_{t}\), and then the adversary chooses a convex cost function \(f_{t}:\) and \(k\) constraints of the form \(g_{t,i}(x) 0,\ i[k]\), where \(g_{t,i}:\) is a convex function for each \(i[k]\)1. Since \(g_{t,i}\)'s are revealed after the action \(x_{t}\) is tchosen, an online policy need not necessarily take feasible actions on each round, and the obvious metric of interest in addition to (1) is the total cumulative constraint violation (CCV) \((T)\) defined as

\[_{T}(T)=_{i=1}^{k}_{i}(T)_{i}(T)=_{t=1}^{T}(g_{t,i}(x_{t}))^{+}.\] (2)

Let \(^{}\) be the feasible set consisting of all admissible actions that satisfy all constraints \(g_{t,i}(x) 0,\ i[k],t[T]\). Under the standard assumption that \(^{}\) is not empty, the goal is to design an online policy to simultaneously achieve a small regret (1) with respect to any admissible benchmark \(x^{}^{}\) and a small CCV (2). We refer to this problem as the constrained OCO (COCO). The assumption \(^{}\) will be relaxed in Section 3 for the Online Constraint Satisfaction (OCS) problem where the cost functions are set to zero, and the objective is to minimize just the CCV.

COCO arises in many applications, including online portfolio optimization with risk constraints, resource allocation in cloud computing with time-varying demands, pay-per-click online ad markets with budget constraints (Liakopoulos et al., 2019), online recommendation systems, dynamic pricing, revenue management, robotics and path planning problems, and multi-armed bandits with fairness constraints (Sinha, 2024). The necessity for revealing the constraints sequentially may also arise, _e.g.,_ in communication-limited settings, where it might be infeasible to reveal all constraints defining the feasible set at a time (_e.g.,_ combinatorial auctions). See Section 4 for an application of the COCO framework in fraud detection which involves binary classification with a highly-imbalanced dataset.

### Related Work

Unconstrained OCO:In a seminal paper, Zinkevich (2003) showed that for solving (1), the ubiquitous projected online gradient descent (OGD) policy achieves an \(O()\) regret for convex cost functions with uniformly bounded sub-gradients. A number of follow-up papers proposed adaptive and parameter-free versions of OGD (Hazan et al., 2007; Orabona and Pal, 2018). See Orabona (2019); Hazan (2022) for textbook treatments of the OCO framework and associated algorithms.

**Constrained OCO (COCO): (A) Time-invariant constraints:** A number of papers considered COCO with time-invariant constraints, _i.e.,_\(g_{t,i}=g_{i},\ t\)(Yuan and Lamperski, 2018; Jenatton et al., 2016; Mahdavi et al., 2012; Yi et al., 2021). These works assume that the functions \(g_{i}\)'s are known to the policy _a priori_. However, they allowed the policy to remain infeasible on any round to avoid the costly projection step of the vanilla projected OGD policy. Their main objective was to design an _efficient_ policy (avoiding the explicit projection step) with a small regret and CCV.

**(B) Time-varying constraints:** Solving the COCO problem when the constraint functions, _i.e.,_\(g_{t,i}\)'s, change arbitrarily with time \(t\) is more challenging. In this case, except for Neely and Yu (2017) and Liakopoulos et al. (2019), most of the prior works construct some Lagrangian function and then update the primal and dual variables (Yu et al., 2017; Sun et al., 2017; Yi et al., 2023). However, the performance bounds obtained with this approach remain suboptimal. Both Neely and Yu (2017) and Liakopoulos et al. (2019) use the drift-plus-penalty (DPP) framework introduced by Neely (2010) to solve the constrained problem under various assumptions. In particular, Neely and Yu (2017) proposed a DPP-based policy for COCO upon assuming the Slater's condition, _i.e.,_\(g_{t,i}(x^{})<-\), for some \(>0\  i,t\). Clearly, this condition precludes the important case of non-negative constraint functions (_e.g.,_ constraint functions of the form \((0,g_{t}(x))\)). Furthermore, the bounds obtained upon assuming Slater's condition depend inversely with the Slater's constant \(\) (usually hidden under the big-Oh notation). Since \(\) could be arbitrarily small, these bounds could be arbitrarily loose. Liakopoulos et al. (2019) extended Neely and Yu (2017)'s result by considering a weaker form of the feasibility assumption without assuming Slater's condition. Furthermore, although these DPP-based results are interesting, they have not been able to provide improved regret or CVV bounds when the cost functions \(f_{t}\)'s are strongly convex because of the linearization step inherent in this approach.

In a recent paper, Guo et al. (2022) considered COCO and obtained the best-known prior results without assuming Slater's condition. However, in addition to yielding sub-optimal bounds, their policy is quite computationally intensive since it requires solving a convex optimization problem on each round. Compared to this, all policies proposed in this paper take only a single gradient-descent step and perform only one Euclidean projection on each round. Please refer to Table 1 for a brief summary of the results and Section A.5 in the Appendix for a qualitative comparison. The COCO problem has been considered in the _dynamic_ setting as well (Chen and Giannakis, 2018; Cao and Liu, 2018; Vaze, 2022; Liu et al., 2022) where the benchmark \(x^{}\) in (1) is replaced by \(x^{}_{t}\) that is also allowed to change its actions over time. However, we focus our attention on achieving the optimal performance bounds for the static version. A special case of COCO is the Online Constraint Satisfaction (OCS) problem that does not involve any cost function, _i.e.,_\(f_{t}=0,\  t,\) and the only object of interest is the CCV. The OCS problem becomes especially interesting in the setting where the feasible set may be empty.

### Our Contributions

In this paper, we consider both COCO and OCS problems and make the following contributions.

1. We propose an efficient first-order policy that simultaneously achieves \(O()\) regret and \(O( T)\) CCV for the COCO problem. Our result breaks the long-standing \(O(T^{}{{4}}})\) barrier for the CCV and matches the lower bound (derived in Theorem 3, previously missing from the literature) up to a logarithmic term. For strongly convex cost functions, the regret guarantee is improved to \(O( T)\) while keeping the CCV bound the same as above. Under an additional assumption that the regret is non-negative, we obtain a further improved logarithmic CCV bound in the strongly convex setting (see Table 1).
2. We additionally consider a special case of the COCO problem, called Online Constraint Satisfaction (OCS), under relaxed feasibility assumptions and obtain sub-linear CCV bounds.
3. On the algorithmic side, our policy simply runs an adaptive first-order OCO algorithm as a blackbox on a specially constructed convex surrogate cost function sequence. On every round, the policy needs to compute only two gradients and an Euclidean projection. This is way more efficient compared to the policies proposed in the previous works (Guo et al., 2022; Neely and Yu, 2017), which need to solve expensive convex optimization problems on each round while yielding sub-optimal bounds. Furthermore, in the special case of time-invariant constraints, our results yield an efficient first-order OCO policy with competitive regret and CCV bounds (Mahdavi et al., 2012; Jenatton et al., 2016; Yi et al., 2021).
4. Our results are obtained by introducing a crisp and elegant potential function-based algorithmic technique for simultaneously controlling the regret and the CCV. In brief, the regret and CCV bounds are derived from a single inequality that arises from plugging in off-the-shelf adaptive regret bounds in a new regret decomposition result (Eqn. (6)). This new analytical technique might also be of independent interest.

   Reference & Regret & CCV & Complexity per round & Assumptions \\  Mahdavi et al. (2012) & \(O()\) & \(O(T^{}{{4}}})\) & Projection & Time-invariant constraints \\ Jenatton et al. (2016) & \(O(T^{(}{{1}},}{{2}})})\) & \(O(T^{}{{-}{{2}}}}})\) & Projection & Time-invariant constraints \\ Sun et al. (2017) & \(O()\) & \(O(T^{}{{4}}})\) & Bregman Projection & - \\ Neely and Yu (2017) & \(O()\) & \(O()\) & Conv-OPT & Slater condition \\ Yuan and Lamperski (2018) & \(O(T^{(}{{1}},}{{2}})})\) & \(O(T^{}{{-}{{2}}}}})\) & Projection & Time-invariant constraints \\ Yu and Neely (2020) & \(O()\) & \(O(1)\) & Conv-OPT & Slater \& Time-invariant constraints \\ Yi et al. (2021) & \(O(T^{(}{{1}},}{{2}})})\) & \(O(T^{}{{-}{{2}}}}})\) & Conv-OPT & Time-invariant constraints \\ Yi et al. (2022) & \(O(T^{})\) & \(O(T^{1-}{{2}}})\) & Projection & Strongly convex cost \\ Guo et al. (2022) & \(O()\) & \(O(T^{}{{4}}})\) & Conv-OPT & - \\ Guo et al. (2022) & \(O( T)\) & \(O()\) & Conv-OPT & Strongly convex cost \\ Yi et al. (2023) & \(O(T^{(}{{1}},}{{2}})})\) & \(O(T^{}{{-}{{2}}}}})\) & Conv-OPT & - \\ Yi et al. (2023) & \(O((T))\) & \(O()\) & Conv-OPT & Strongly convex cost \\
**This paper** & \(O()\) & \(O()\) & Projection & - \\
**This paper** & \(O( T)\) & \(O()\) & Projection & Strongly convex cost \\
**This paper** & \(O( T)\) & \(O()\) & Projection & Strongly convex cost, Regret\({}_{T} 0,\) \\   

Table 1: Summary of the results on COCO. Unless stated otherwise, we assume arbitrary time-varying convex constraints and convex cost functions. In the above table, \(0 1\) is an adjustable parameter, \(\) is the strong convexity parameter of the strongly convex cost functions. Conv-OPT refers to solving a constrained convex optimization problem on each round. Projection refers to the Euclidean projection operation on the convex set \(\). For typical convex sets (_e.g.,_ Euclidean box, probability simplex), projection operations are substantially more efficient than solving a constrained convex optimization problem.

5. Finally, in Section 4, we evaluate the practical performance of our algorithm in the online credit card fraud detection problem with a highly imbalanced dataset.

## 2 The Constrained OCO (COCO) Problem

### Assumptions

We now state the assumptions considered in this paper. These assumptions are standard in literature on the COCO problem (Guo et al., 2022; Yi et al., 2021; Neely and Yu, 2017).

**Assumption 1** (Convexity).: _The cost function \(f_{t}:\) and the constraint function \(g_{t,i}:\) are convex for all \(t 1,i[k]\). The admissible set (a.k.a. the decision set or the action set) \(^{d}\) is closed and convex and has a finite Euclidean diameter \(D\)._

**Assumption 2** (Lipschitzness).: _All cost functions \(\{f_{t}\}_{t 1}\) and the constraint functions \(\{g_{t,i}\}_{i[k],t 1}\)'s are \(G\)-Lipschitz. In other words, for any \(x,y\), we have_

\[|f_{t}(x)-f_{t}(y)| G\|x-y\|,\ |g_{t,i}(x)-g_{t,i}(y)| G\|x-y\|,\  t  1,i[k].\]

Unless specified otherwise, the norm \(\|\|\) will refer to the standard Euclidean norm and \( f\) will refer to an arbitrary subgradient of a convex function \(f\). Assumption 2 implies that the \(_{2}\)-norm of the (sub)gradients of the cost and constraint functions are uniformly upper-bounded by \(G\) over the admissible set \(\). Finally, we make the following feasibility assumption about the constraint functions.

**Assumption 3** (Feasibility).: _There exists a feasible action \(x^{}\) s.t. \(g_{t,i}(x^{}) 0, t,i\). The feasible set \(^{}\) is defined to be the set of all feasible actions. The feasibility assumption implies that \(^{}\)._

The feasibility assumption distinguishes the cost functions from the constraint functions and is commonly assumed in the literature (Guo et al., 2022; Neely and Yu, 2017; Yu and Neely, 2016; Yuan and Lamperski, 2018; Yi et al., 2023; Liakopoulos et al., 2019). In Section 3, we will consider a constraint-only variant of the problem where the feasibility assumption (Assumption 3) will be relaxed. See Appendix A.1 for a brief discussion on the assumptions.

Remarks:On each round, multiple constraints of the form \(g_{t,i}(x) 0,i[k]\) can be replaced by a single new constraint \(g_{t}(x) 0\) where the constraint function \(g_{t}\) is defined to be the pointwise maximum of the given constraints, _i.e.,_\(g_{t}(x)_{i=1}^{k}g_{t,i}(x),x\). It is easy to verify that if each of the constraint functions \(\{g_{t,i}\}_{i=1}^{k}\) satisfies the above assumptions, then the constraint function \(g_{t}\) defined above also satisfies the assumptions. Hence, throughout this section and without loss of generality, we will assume that only one constraint function is revealed on each round. That being said, under the relaxed feasibility assumption in Section 3, this trick does not work and there we will need to consider the full set of \(k\) constraint functions.

### Online Policy for COCO

Recall that compared to the standard OCO problem where the only objective is to minimize the Regret (Hazan, 2022), in COCO, our objective is twofold: to _simultaneously_ control the Regret _and_ the CCV. See Section A.2 in the Appendix for preliminaries on the OCO problem and some standard results which will be useful in our analysis. In the following, we propose a Lyapunov function-based policy that yields the optimal Regret and CCV bounds for the COCO problem. Although for simplicity, we assume that the horizon length \(T\) is known, we can use the standard doubling trick for an unknown \(T\).

### Design and Analysis of the Algorithm

To simplify the analysis, we pre-process the cost and constraint functions on each round as follows.

**Pre-processing:** On every round, we first clip the negative part of the constraint function to zero by passing it through the standard ReLU unit. Then, we scale both the cost and constraint functions by a positive factor \(,\) which will be determined later. In other words, we work with the pre-processed inputs \(_{t} f_{t},_{t}(g_{t})^{+}\). Hence, the pre-processed functions are \( G\)-Lipschitz and \(_{t} 0,\, t\).

In the following, we derive the Regret and CCV bounds for the pre-processed functions. The bounds for the original problem are obtained upon scaling the results back by \(^{-1}\) in the final step.

#### 2.3.1 Defining the Surrogate Cost Functions

Let \(Q(t)\) denote the CCV for the pre-processed constraints up to round \(t\). Clearly, \(Q(t)\) satisfies the simple recursion \(Q(t)=Q(t-1)+_{t}(x_{t}),t 1\), with \(Q(0)=0\). Recall that one of our objectives is to make \(Q(t)\) small. Towards this, let \(:_{+}_{+}\) be any non-decreasing differentiable convex potential (Lyapunov) function such that \((0)=0\). Using the convexity of \(()\), we have

\[(Q(t))  (Q(t-1))+^{}(Q(t))(Q(t)-Q(t-1))\] (3) \[= (Q(t-1))+^{}(Q(t))_{t}(x_{t}).\]

Hence, the change (_drift_) of the potential function \((Q(t))\) on round \(t\) can be upper bounded as

\[(Q(t))-(Q(t-1))^{}(Q(t))_{t}(x_{t}).\] (4)

Recall that, in addition to controlling the CCV, we also want to minimize the cumulative cost \(_{t=1}^{T}f_{t}(x_{t})\) (which is equivalent to the regret minimization). Inspired by the stochastic _drift-plus-penalty_ framework of Neely (2010), we combine these two objectives to a single objective of minimizing a sequence of surrogate cost functions \(\{_{t}\}_{t=1}^{T}\) which are obtained by taking a positive linear combination of the drift upper bound (4) and the cost function. More precisely, we define

\[_{t}(x):=V_{t}(x)+^{}(Q(t))_{t}(x),\ \ t 1.\] (5)

In the above, \(V\) is a suitably chosen non-negative parameter to be determined later. In brief, the proposed policy for COCO, described in Algorithm 1, simply runs an adaptive OCO policy on the surrogate cost function sequence \(\{_{t}\}_{t 1}\), with a specific choice of the potential function \(()\), the parameter \(V\), and step-size sequence \(\{_{t}\}_{t 1}\), as dictated by the following analysis.

```
1:Input: Sequence of convex cost functions \(\{f_{t}\}_{t=1}^{T}\) and constraint functions \(\{g_{t}\}_{t=1}^{T}\), \(G=\) a common Lipschitz constant, \(T=\) Horizon length, \(D=\) Euclidean diameter of the admissible set \(,\)\(_{}()=\) Euclidean projection operator on the set \(\)
2:Parameter settings:
3:Convex cost functions:\(=(2GD)^{-1},V=1,=},(x)=( x)-1.\)
4:\(\)-strongly convex cost functions:\(=1,V=(Te)}{},(x)=x^{2}.\)
5:Initialization: Set \(x_{1}\) arbitrarily, \(Q(0)=0\).
6:for each\(t=1:T\)do
7: Play \(x_{t}\), observe \(f_{t},g_{t},\) incur a cost of \(f_{t}(x_{t})\) and constraint violation of \((g_{t}(x_{t}))^{+}\)
8:\(_{t} f_{t},_{t}(0,g_{t}).\)
9:\(Q(t)=Q(t-1)+_{t}(x_{t}).\)
10: Compute (sub)gradient \(_{t}=_{t}(x_{t}),\) where the surrogate function \(_{t}\) is defined in Eqn. (5)
11:\(x_{t+1}=_{}(x_{t}-_{t}_{t}),\) where \(_{t}=D}{2^{T}\|_{t}\|^{2} _{2}}},&\\ ^{T}H_{s}},&$= strong convexity parameter of $f_{s},s 1$)}\)
12:endfor ```

**Algorithm 1** Online Policy for COCO

#### 2.3.2 The Regret Decomposition Inequality

Let \(x^{}^{}\) be any feasible action guaranteed by Assumption (3). Plugging in the definition of surrogate costs (5) into the drift inequality (4), and using the fact that \(g_{}(x^{}) 0, 1,\) we have

\[(Q())-(Q(-1))+V(_{}(x_{})-_{}(x^{ }))_{}(x_{})-_{}(x^{}),\  1.\]

Summing the above inequalities for rounds \(1 t\), and using the fact that \((0)=0,\) we obtain

\[(Q(t))+V_{t}(x^{})^{}_{t}(x^{ }),\  x^{}^{},\] (6)

where \(_{t}\) on the LHS and \(^{}_{t}\) on the RHS of (6) refer to the regret for learning the pre-processed cost functions \(\{_{t}\}_{t 1}\) and the surrogate cost functions \(\{_{t}\}_{t 1}\) respectively. We will usethe following upper bound on the \(_{2}\)-norm of the (sub)gradients \(G_{t}\) of the surrogate cost function \(_{t}\) defined in Eqn. (5):

\[G_{t}\|_{t}(x_{t})\|}{ {}}V\|_{t}(x_{t})\|+^{}(Q(t))\|_{t }(x_{t})\|}{{}} GV+^{}(Q (t),\] (7)

where in \((a)\), we have used the triangle inequality for \(_{2}\) norms and in \((b)\), we have used the fact that all pre-processed functions are \( G\)-Lipschitz.

#### 2.3.3 Convex Cost and Convex Constraint Functions

We now apply the regret decomposition inequality (6) to the case of convex cost and convex constraint functions. Let us choose the regret-minimizing OCO subroutine for the surrogate cost functions to be the OGD policy with adaptive step sizes (a.k.a. _AdaGrad_) described in part 1 of Theorem 6 in the Appendix (see Algorithm 1). Plugging in the adaptive regret bound (24) on the RHS of (6), setting \(=(2GD)^{-1},\) and using Eqn. (7), we arrive at the following inequality valid for any \(t 1:\)

\[(Q(t))+V_{t}(x^{*})^{t}^{ }(Q())^{2}}+V.\] (8)

In deriving the above result, we have utilized simple algebraic inequalities \((x+y)^{2} 2(x^{2}+y^{2})\) and \(+,a,b 0\). Now recall that the sequence \(\{Q(t)\}_{t 1}\) is non-negative and non-decreasing as \(_{t} 0.\) Furthermore, the derivative \(^{}()\) is non-decreasing as the function \(()\) is assumed to be convex. Hence, bounding all terms in the summation on the RHS of (8) from above by the last term, we arrive at the following inequality for any feasible \(x^{*}^{*}:\)

\[(Q(t))+V_{t}(x^{*})^{}Q(t)+V.\] (9)

The simplified regret decomposition inequality (9) constitutes the key step for the subsequent analysis.

\(\)

Performance Analysis

An exponential Lyapunov function:We now derive the Regret and CCV bounds for the proposed policy (Algorithm 1) by choosing \(()\) to be the exponential Lyapunov function: \((x)( x)-1,\) where the parameter \( 0\) will be fixed later. Clearly, the function \(()\) satisfies the required conditions for a Lyapunov function - it is a non-decreasing and convex function with \((0)=0.\)

Bounding the Regret:With the above choice for the Lyapunov function \(()\), Eqn. (9) implies that for any feasible \(x^{*}^{*}\) and for any \(t[T],\) we have

\[( Q(t))-1+V_{t}(x^{*})( Q(t)) +V.\]

Transposing the first term on the above inequality to the RHS and dividing throughout by \(V\), we have:

\[_{t}(x^{*})++ (-1).\] (10)

Choosing any \(},\) the last term in the above inequality becomes non-positive for any \(t[T].\) Hence, for any \(x^{*}^{*}\), we have the following regret bound

\[_{t}(x^{*})+.\ \  t[T].\] (11)

Bounding the CCV:Since all pre-processed cost functions are \( G=(2D)^{-1}\)-Lipschitz, we trivially have \(_{t}(x^{*})=_{s=1}^{t}(_{s}(x_{s})-_{s}(x^ {*}))--.\) Hence, from Eqn. (10), we have that for any \(<}\) and any \(t[T]:\)

\[(1-) 2t+ Q (t)}.\] (12)

Choosing \(=},V=1,\) and scaling the bounds back by \(^{-1} 2GD,\) we arrive at our main result.

**Theorem 1**.: _For the COCO problem with adversarially chosen \(G\)-Lipschitz cost and constraint functions, Algorithm 1, with \(=(2GD)^{-1},\)\(V=1,(x)=(})-1,\) yields the following Regret and CCV bounds for any horizon length \(T 1:\)_

\[_{t} 2GD(+1),\ \  t[T],\ \ CCV_{T} 4GD (21+2T).\]

_In the above, \(D\) denotes the Euclidean diameter of the closed and convex admissible set \(\)._

#### 2.3.4 Strongly Convex Cost and Convex Constraint Functions

We now consider the setting where each of the cost functions \(f_{t},t 1,\) is \(\)-strongly convex for some \(>0\). The constraint functions \(g_{t}\)'s are assumed to be convex as before and not necessarily strongly convex. In this case, we choose the regret-minimizing OCO subroutine for the surrogate cost functions to be the OGD algorithm with the step-size sequence as given in part 2 of Theorem 6 in the Appendix (see Algorithm 1). Since the cost functions are known to be \(\)-strongly convex, each of the surrogate cost functions (5) is \(V\)-strongly convex. Hence, using the bound from Eqn. (7), choosing the scaling parameter to be \(=1\), and simplifying the generic regret bound given by Eqn. (25), we obtain the following regret bound for learning the surrogate cost functions \(\{_{s}\}_{s 1}\):

\[^{}_{t}(x^{})}{}(1+(t))+ {G^{2}}{ V}_{=1}^{t}(Q()))^{2}}{},\;x ^{}.\] (13)

In the above, we have used the standard bound for the Harmonic sum: \(_{=1}^{t} 1+(t)\), as well as the fact that \((a+b)^{2} 2(a^{2}+b^{2}).\) Substituting the bound (13) into the regret decomposition inequality (6), and using the non-decreasing property of the sequence \(\{Q()\}_{ 1}\) and the derivative \(^{}()\), we obtain

\[(Q(t))+V_{t}(x^{})}{}(1+(t))+ }{ V}(1+(t))^{}(Q(t))^{2},\; x ^{}^{}, t.\] (14)

Finally, choosing \(()\) as the quadratic Lyapunov function, _i.e.,_\((x) x^{2},\) we arrive at the following result for strongly convex cost and convex constraint functions.

**Theorem 2**.: _For the COCO problem with adversarially chosen \(\)-strongly convex, \(G\)-Lipschitz cost functions and \(G\)-Lipschitz convex constraint functions, Algorithm 1, with \(=1,V=(Te)}{},(x)=x^{2},\) yields the following Regret and CCV bounds for any horizon length \(T 1:\)_

\[_{t}(x^{})}{}1+(t), \;CCV_{t}=O}, x^{} ^{},\; t[T].\]

_Furthermore, if the worst-case regret is non-negative in some round \(t\) (i.e., \(_{x^{}^{}}_{t}(x^{}) 0\)), then the CCV can be further improved to \(CCV_{T}=O()\) while keeping the regret bound the same._

Please refer to Appendix A.6 for the proof of Theorem 2.

**Remarks:** The second part of the theorem is surprising because it says that when the regret is non-negative, a stronger logarithmic CCV bound holds for not necessarily strongly convex constraints. In Appendix A.7, we give example of an interesting class of adversaries, called _convex adversary_, for which the non-negative regret assumption holds true in the OCO setting.

### Lower Bounds

We now show that under Assumptions 1, 2, and 3, the regret and the CCV of any online policy for the COCO problem for \(T\) rounds are both lower bounded by \(()\) provided the problem is high-dimensional. Recall that if the constraint function \(g_{t}=0,\)\( t,\) then the COCO problem reduces to the standard OCO problem, and \(()\) is a well-known regret lower bound for OCO (Hazan, 2022, Theorem 10). In this case, we trivially have CCV = 0. The main challenge in proving a lower bound for COCO is _simultaneously_ bounding both the regret and CCV. Prior work does not give any simultaneous lower bounds since the standard adversarial inputs used to derive the lower bound of Hazan (2022) do not satisfy the feasibility assumption (Assumption 3). We derive the lower bound by constructing a sequence of cost and constraint functions that satisfy Assumption 3 in a \(d\)-dimensional Euclidean box of unit diameter.

**Theorem 3**.: _Under Assumptions 1, 2, and 3, for any choice of the horizon length \(T\) and online policy, there exists a problem instance with dimension \(d T\) where \((_{T},_{T})=()\)._

In high-dimensional problems where \(d T,\) the above lower bound matches with the upper bound given in Theorem 1. The proof of Theorem 3 can be found in Appendix A.4.

The Online Constraint Satisfaction Problem (OCS)

In this section, we study a special case of the COCO problem, which involves only constraint functions and no cost functions. The OCS problem arises in many practical settings, including the multi-task learning problem (see Section A.3 in the Appendix for a brief discussion). In Section A.8 in the Appendix, we also establish a connection between the OCS problem and the well-studied Convex Body Chasing problem (Argue et al., 2019). The setup is similar to the COCO setting - on every round \(t 1\), an online policy selects an action \(x_{t}\) from a closed, bounded, and convex admissible set \(^{d}\). After observing the current action \(x_{t}\), the adversary chooses \(k\) constraints of the form \(g_{t,i}(x) 0,i[k]\), where each \(g_{t,i}:\) is a convex function. Let \(\) be any sub-interval of the horizon \([1,T]\). The cumulative constraint violation (CCV) \((T)\) for the OCS problem is defined as the maximum _signed_ cumulative constraint violation in any sub-interval:

\[(T)=_{i=1}^{k}_{i}(T),_{i}(T)= _{[1,T]}_{t}g_{t,i}(x_{t}),\;1 i  k.\] (15)

The objective is to design an online learning policy so that \((T)\) is as small as possible. It is worth noting that in the OCS problem, we consider a soft constraint violation metric \(_{}_{t}g_{t,i}(x_{t})\) instead of the hard violation metric \(_{t=1}^{T}(g_{t,i}(x_{t}))^{+}\) as in COCO. This allows for compensating the infeasibility on one round with strict feasibility on other rounds. In contrast with the COCO setting, without Assumption 3, running a no-regret policy on the pointwise maximum of the constraint functions no longer works as the CCV of any fixed benchmark could grow linearly with \(T\). In the OCS problem, we relax the feasibility assumption (Assumption 3), and consider the following two distinct alternatives instead.

1. \(S\)-feasibility:Here, we assume that there is an admissible action \(x^{}\) that satisfies the aggregate constraints over any interval of \(S\) rounds. However, unlike Liakopoulos et al. (2019), which also considers the same assumption, the value of the parameter \(S\) is not necessarily known to the policy _a priori_. Towards this end, we define the set of all \(S\)-feasible actions \(_{S}\) as below:

\[_{S}=\{x^{}:_{}g_{,i} (x^{}) 0,[1,T],\;||=S, i[k]\}.\] (16)

We now replace Assumption 3 with the following weaker version:

**Assumption 4** (\(S\)-feasibility).: \(_{S}\) _for some \(1 S T\)._

Clearly, Assumption 4 is weaker than Assumption 3 as \(^{}_{S}, S 1\). Note that even when the individual constraint functions satisfy \(S\)-feasibility, their pointwise maximum need not satisfy \(S\)-feasibility. Hence, unlike COCO under Assumption 3, this problem cannot be solved by simply running a no-regret policy on the pointwise maximum of the constraints.

2. \(P_{T}\)-constrained adversaryIn this case, we drop any feasibility assumption altogether. As a consequence, any static admissible benchmark \(x^{}\) also incurs a CCV.

**Definition 1**.: _An adversary is called \(P_{T}\)-constrained if its minimum static CCV is \(P_{T}F\), i.e., \(_{x^{}}_{[T],i}_{t }g_{t,i}(x^{})=P_{T}\), where \(F\) is a normalizing factor denoting the maximum absolute value of the constraint functions within the compact admissible set \(\)._

As before, the value of \(P_{T}\) is not necessarily known to the policy _a priori_.

### Designing an OCS Policy with a Quadratic Lyapunov function

We define a process \((t)=Q_{i}(t),i[k],t 1\), which tracks the CCV:

\[Q_{i}(t)=Q_{i}(t-1)+g_{t,i}(x_{t})^{+},\;Q_{i}(0)=0,\;t 1, \; i[k].\] (17)

Notably, in contrast to COCO, we _do not_ clip the constraint functions in the above recursion. Expanding Eqn. (17), which is also known as the queueing recursion or the _Lindley process_(Asmussen, 2003, pp. 92), and using the definition in Eqn. (15), we have the following relation for all \(i[k]\) :

\[_{i}(T)_{t=1}^{T}(0,_{=0}^{t-1}_{s=t- }^{t}g_{s,i}(x_{l}))=_{t=1}^{T}Q_{i}(t).\] (18)

Equation (18) indicates that to control the CCV (15), it is sufficient to control the \((t)\) process. Similar to the COCO problem, we combine the classic Lyapunov method with adaptive no-regret OCO policies to control the \((t)\) process.

A Quadratic Lyapunov function:We consider the quadratic potential function \(((t))_{i=1}^{k}Q_{i}^{2}(t),t 1\). Since \(((x)^{+})^{2}=xx^{+}, x\), from Eqn. (17), we have

\[Q_{i}^{2}(t) = Q_{i}(t-1)+g_{t,i}(x_{t})Q_{i}(t)=Q_{i}(t-1)Q_{i}(t) +Q_{i}(t)g_{t,i}(x_{t}),\] (19) \[ Q_{i}^{2}(t)+Q_{i}^{2}(t-1)+Q_{i}(t)g_{t,i}(x _{t}),\  i[k].\]

where in \((a)\), we have used the AM-GM inequality. Rearranging Eqn. (19), the change of the potential function \((t)\) on round \(t\) can be upper bounded as follows

\[((t))-((t-1))=_{i=1}^{k}Q_{i}^{2}(t)-Q_{i}^{2}( t-1) 2_{i=1}^{k}Q_{i}(t)g_{t,i}(x_{t}).\] (20)

Similar to (5), we now define a surrogate cost function \(_{t}:\) as a linear combination of the constraint functions with the coefficients given by the vector \((t)\), _i.e._,

\[_{t}(x) 2_{i=1}^{k}Q_{i}(t)g_{t,i}(x).\] (21)

Clearly, the surrogate cost function \(_{t}()\) is convex since the coefficients \(Q_{i}(t)\)'s are non-negative and the constraint functions are convex. Our OCS policy, described below, simply runs a regret-minimizing adaptive OCO subroutine on the surrogate cost function sequence (21).

**The OCS policy (Algorithm 2):** Pass the surrogate cost functions \(\{_{t}\}_{t 1}\) to the AdaGrad algorithm which enjoys a data-dependent regret as given in part 1 of Theorem 6 in the Appendix (Eqn. (24)).

```
1:Input: Sequence of convex constraint functions \(\{g_{t,i}\}_{i[k],t 1}\), a closed and convex admissible set \(\) with a finite Euclidean diameter \(D\), \(_{}()\) = Euclidean projection operator on the set \(\)
2:Output: Sequence of admissible actions \(\{x_{t}\}_{t 1}\)
3:Initialization: Set \(x_{1}\) arbitrarily, \(Q_{i}(0)=0,\  i[k]\).
4:for each each round \(t 1\)do
5: Play \(x_{t}\), observe the constraint functions \(\{g_{t,i}\}_{i[k]}\) revealed by the adversary.
6: [Update \((t)\)] \(Q_{i}(t)=(Q_{i}(t-1)+g_{t,i}(x_{t}))^{+},i[k]\).
7: [Compute a subgradient]\(_{t}_{t}(x_{t})=2_{i=1}^{k}Q_{i}(t) g_{t,i}(x_{t})\).
8: [AdaGrad step] Compute the next action \(x_{t+1}=_{}(x_{t}-_{t}_{t})\), where \(_{t}=D}{2^{t}\|_{t}\|_{2}^{2}}}\).
9:endfor ```

**Algorithm 2** Online Policy for OCS

### Performance Bounds

**Theorem 4**.: _Under Assumptions 1, 2, and 4, Algorithm 2 achieves the following CCV bound for the OCS problem: \((T)=O((,S))\)._

**Theorem 5**.: _Under Assumptions 1 and 2, Algorithm 2 achieves the following CCV bound for the OCS problem for any \(P_{T}\)-constrained adversary as given in Definition 1:_

\[(T)=O(P_{T}^{}{{3}}}T^{}{{3}}})+O().\]

Trivially, we have \(S T\) and \(P_{T} T\). In the non-trivial case where either \(S\) or \(P_{T}\) increases _sub-linearly_ with the horizon length \(T\), the above theorems yield sublinear CCV bounds.

## 4 Experiments: Credit Card Fraud Detection

Classification with a highly imbalanced dataset:We first formulate the credit card fraud detection problem in the COCO framework. Assume that we receive a sequence of \(d\)-dimensional feature vectors \(\{z_{t}\}_{t 1}\) and the corresponding binary labels \(\{y_{t}\}_{t 1}\) for a sequence of credit card transactions, where each transaction can either be legitimate (label = 0) or fraudulent (label = 1). The problem is to predict the label \(_{t}\) for each transaction \(z_{t}\) before its true label \(y_{t}\{0,1\}\) is revealed. Typically, legitimate transactions outnumber fraudulent transactions by orders of magnitude. Since the goal is to detect any fraudulent transactions (even at the cost of a few false alarms), maximizing the classification accuracy alone is insufficient due to the significant class imbalance. We propose the following reformulation for this problem within the COCO framework.

Formulation:Let \(_{t}(z_{t},x)\) be the likelihood of class \(1\) for the feature \(z_{t}\), given by a parameterized model with parameter \(x\). Hence, the log-likelihood \((t)\) of the data on round \(t\) can be expressed as:

\[(t)=y_{t}(_{t}(z_{t},x))+(1-y_{t})(1-_{t}(z_{ t},x)).\]

We train the model by maximizing the sum of log-likelihoods for legitimate transactions, subject to the constraint that all fraudulent transactions have a likelihood value close to \(1\) (_i.e.,_ the sum of the log-likelihoods of the fraudulent transactions remains close to zero):

\[_{x}_{t=1}^{T}(1-y_{t})(1-_{t}(z_{t},x)),\ \ \ \ _{t=1}^{T}y_{t}(_{t}(z_{t},x)) 0.\] (22)

The above problem (22) can be immediately recognized to be an instance of COCO with the following cost and constraint functions:

\[f_{t}(x)-(1-y_{t})(1-_{t}(z_{t},x)),\ \ g_{t}(x)-y_{t}( _{t}(z_{t},x)),\ t 1.\]

In our experiments, we consider the common scenario in which the likelihoods are modeled by the output of a feedforward neural network. Note that the feasibility assumption (Assumption 3) is naturally satisfied as the overparameterized neural network models are known to perfectly fit the data (Belkin et al., 2019). However, in this case, the functions \(f_{t}\) and \(g_{t}\) are generally non-convex.

Experiments:We experiment with a publicly available credit card transaction dataset (Dal Pozzolo et al., 2014). This highly imbalanced dataset contains only \(492\) frauds (\( 0.17\%\)) out of \(284,807\) reported transactions. Each data point has \(D_{}=30\) features and binary labels. We choose a simple network architecture with a single hidden layer containing \(H=10\) hidden nodes and sigmoid non-linearities. Unlike previous algorithms, our algorithm is especially suitable for training neural network models as it only needs to compute the gradients (via backward pass) and evaluate the functions (via forward pass). Initially, all weights are independently sampled from a standard normal distribution. The network is then trained using Algorithm 1 on a quad-core CPU with 8 GB RAM. The projection operation corresponds to \(L_{2}\)-normalization. The code has been publicly released (Sinha, 2024).

Results:Given the severe class imbalance, the area under the ROC curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR), is an appropriate metric to evaluate any prediction algorithm for this problem. By varying the hyperparameter \(\), we obtain the ROC curve shown in Figure 2. The area under the ROC curve is computed to be \( 0.92\), which is an excellent score (cf. ideal score \(=1.0\)), notwithstanding the fact that, unlike the standard resampling-based techniques, the algorithm learns in an entirely online fashion starting from random initialization. Figure 2 illustrates the expected sublinear variation of CCV during one of the algorithm runs.

## 5 Conclusion

In this paper, we proposed efficient online policies for the COCO problem with optimal performance bounds. We also derived sublinear CCV bounds for the OCS problem under a set of relaxed assumptions. Our analysis is streamlined, leveraging Lyapunov theory and adaptive regret bounds for the standard OCO problem. In the future, exploring dynamic regret bounds and a bandit extension of the COCO problem would be interesting.

Acknowledgement

This work was supported by the Department of Atomic Energy, Government of India, under project no. RTI4001 and by a Google India faculty research award. The first author was also partially supported by a US-India NSF-DST collaborative grant coordinated by IDEAS-Technology Innovation Hub (TIH) at the Indian Statistical Institute, Kolkata. The authors gratefully acknowledge comments from the anonymous reviewers, which substantially improved the quality of the presentation.