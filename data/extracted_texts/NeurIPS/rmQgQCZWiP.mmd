# Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off

Zichen Zhang, Johannes Kirschner, Junxi Zhang, Francesco Zanini, Alex Ayoub,

**Masood Dehghan, Dale Schuurmans**

University of Alberta

{zichen2,jkirschn,junxi3,fzanini,aayoub,masood1,daes}@ualberta.ca

All authors contributed equally

###### Abstract

A default assumption in reinforcement learning (RL) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. Yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. These findings show that managing the temporal resolution can provably improve policy evaluation efficiency in LQR systems with finite data. Empirically, we demonstrate the trade-off in numerical simulations of LQR instances and standard RL benchmarks for non-linear continuous control.

## 1 Introduction

In many real-world applications of control and reinforcement learning, the underlying system evolves continuously in time (Eliasmith and Furlong, 2022). For instance, a physical system like a robot is naturally modelled as a stochastic dynamical system. Nonetheless, sensor measurements are typically captured at discrete time intervals, which entails choosing the sampling frequency or measurement _step-size_. This step-size is usually treated as an immutable quantity based on prior measurement design, but it has a significant impact on data efficiency (Burns et al., 2023). We will see that, from a data-cost perspective, learning can be far more data efficient if it operates at a temporal resolution that is allowed to differ from a prior step-size choice.

In this work, we investigate episodic policy evaluation with a finite data budget to provide a key initial step to addressing broader research questions on the impact of temporal resolution in reinforcement learning. We show that data efficiency can be significantly improved by leveraging a precise understanding of the trade-off between approximation error and statistical estimation error in value estimation -- two factors that react differently to the level of temporal discretization. Intuitively, employing a finer temporal resolution leads to a better approximation of the continuous-time system from discrete measurements; however, under a fixed data budget, denser data within each trajectory results in fewer trajectories, leading to increased estimation variance due to system stochasticity. This implies that, for a given data cost, it can be beneficial to increase temporal spacing between recorded data points beyond a pre-set measurement step-size. This holds true for any system with stochastic dynamics, even if the learner has access to _exact_ (noiseless) state measurements.

The main contributions of this work are twofold. First, we conduct a theoretical analysis focusing on the canonical case of Monte-Carlo value estimation in a Langevin dynamical system (linear dynamics perturbed by a Wiener process) with quadratic instantaneous costs, which corresponds to policy evaluation in linear quadratic control (LQR). To formalize the impact of time discretization on policy evaluation, we present analytical expressions for the mean-squared error that _exactly characterize the approximation-estimation trade-off_ with respect to the step-size parameter. From this trade-off, we derive the optimal step-size for a given Langevin system and characterize its dependence on the data budget. Second, we carry out a numerical study that illustrates and confirms the trade-off in both linear and non-linear systems, including several MuJoCo control environments. The latter also highlights the practical impact of the choice of sampling frequency, which significantly affects the MSE, and we therefore provide recommendations to practitioners for properly choosing the step-size parameter.

### Related Work

There is a sizable literature on reinforcement learning for continuous-time systems (e.g. Doya, 2000; Lee and Sutton, 2021; Lewis et al., 2012; Bahl et al., 2020; Kim et al., 2021; Yildiz et al., 2021). These previous works largely focus on deterministic dynamics without investigating trade-offs in temporal discretization. A smaller body of work considers learning continuous-time control under stochastic (Baird, 1994; Bradtke and Duff, 1994; Munos and Bourgine, 1997; Munos, 2006), or bounded (Lutter et al., 2021) perturbations, but their objective is to make standard learning methods more robust to small time scales (Tallec et al., 2019), or develop continuous-time algorithms that unify classical methods in discrete-time (Jia and Zhou, 2022a,b), without explicitly addressing temporal discretization. However, we find that managing temporal resolution offers substantial improvements not captured by previous studies.

The LQR setting is a standard framework in control theory and it gives rise to a fundamental optimal control problem (Lindquist, 1990), which has proven to be a challenging scenario for reinforcement learning algorithms (Tu and Recht, 2019; Krauth et al., 2019). The stochastic LQR considers linear systems driven by additive Gaussian noise with a quadratic cost, minimised using a feedback controller. Although this is a well-understood scenario with a known optimal controller in closed form (Georgiou and Lindquist, 2013), the statistical properties of the long-term cost have only recently been investigated (Bijl et al., 2016). Our research closely relates to the now extensive literature on reinforcement learning in LQR systems (e.g. Bradtke, 1992; Krauth et al., 2019; Tu and Recht, 2018; Dean et al., 2020; Tu and Recht, 2019; Dean et al., 2018; Fazel et al., 2018; Gu et al., 2016). These works uniformly focus on the discrete time setting, although the benefits of managing spatial rather than temporal discretization have also been considered (Sinclair et al., 2019; Cao and Krishnamurthy, 2020). Wang et al. (2020) studies continuous-time LQR, focusing on the exploration problem. Basei et al. (2022) provides a regret bound depending on sampling frequency, for a specific algorithm based on least-squares estimation. Their analysis considers approximation and estimation errors independently, without identifying a trade-off.

There is compelling empirical evidence that managing temporal resolution can greatly improve learning performance (Lakshminarayanan et al., 2017; Sharma et al., 2017; Huang et al., 2019; Huang and Zhu, 2020; Dabney et al., 2021; Park et al., 2021), typically achieved through options (Sutton et al., 1999), a specific instance of which is action persistence, achieved by maintaining a fixed action over multiple time steps (also known as action repetition). Recently, these empirical findings have been supported by an initial theoretical analysis (Metelli et al., 2020), showing that temporal discretization plays a role in determining the effectiveness of fitted Q-iteration. Their analysis does not consider fully continuous systems, but rather remains anchored in a base-level discretization. Furthermore, it only provides worst-case upper bounds, without capturing detailed practical trade-offs. Lutter et al. (2022) discusses the practical trade-off on time discretization but do not provide theoretical support. Bayraktar and Kara (2023) analyzes a trade-off between sample complexity and approximation error, which however requires the state and action spaces of the diffusion process to be discretized to yield an MDP. The two components in the trade-off are analyzed independently, unlike the unified statistical analysis provided in our work, and no exact characterization is presented.

## 2 Policy Evaluation in Continuous Linear Quadratic Systems

In the classical continuous-time linear quadratic regulator (LQR), a state variable \(X(t)^{n}\) evolves over time \(t 0\) according to the following equation:

\[X(t)=X(t)\;t+U(t)\;t+ \;W(t).\] (1)The dynamical model is fully specified by the matrices \(^{n n}\), \(^{n p}\) and the diffusion coefficient \(\). The control input \(U(t)^{p}\) is given by a fixed policy, and \(W(t)\) is a Wiener process. The state variable \(X(t)\) is fully observed. For simplicity, we assume that the dynamics start at \(X(0)=^{n}\)(e.f. Abbasi-Yadkori and Szepesvari, 2011; Dean et al., 2020).

The quadratic cost \(J\) is defined for positive definite, symmetric matrices \(^{n n}\) and \(^{p p}\), a _system horizon_\(0<\) and a discount factor \((0,1]\):

\[J_{}=_{0}^{}^{t}[X(t)^{}X (t)+U(t)^{}U(t)]dt.\] (2)

In the following, we consider the class of controllers given by static feedback of the state, i.e.: \(U(t)=KX(t)\) where \(K^{p n}\) is the static control matrix yielding the control input. It is well known that in infinite horizon setting, the optimal control belongs to this class. Given such an input, the LQR in Equation (1) can be further reduced to a linear stochastic dynamical system described by a Langevin equation. Using the definitions \(A:=+K\) and \(Q:=}+K^{}RK\), we express both the state dynamics and the cost in a more compact form:

\[X(t)=AX(t)\ t+\ W(t )\,, J_{}=_{0}^{}^{t}X(t)^{}QX (t)\ t.\] (3)

The expected cost w.r.t. the Wiener process is \(V_{}=[J_{}]\). The policy plays a role in this work solely from its impact in the closed-loop dynamics \(A\). Equation (3) is what we analyze in the following. We explicitly distinguish the _finite-horizon setting_ where \(<\), \( 1\) and the cost is \(V_{}\), and the _infinite-horizon setting_ where \(=\), \(<1\) and the cost is \(V_{}\). In order not to incur infinite costs in either scenario, a stable closed-loop matrix \(A\) should be assumed. Note that the existence of a stabilizing controller is guaranteed under the standard controllability assumptions in LQR (Fazel et al., 2018; Abbasi-Yadkori et al., 2019; Dean et al., 2020). Thus the closed-loop stability can be safely assumed.

Monte-Carlo Policy EvaluationOur main objective in _policy evaluation_ is to estimate the expected cost from discrete-time observations. To this end, we choose a uniform discretization of the interval \([0,T]\) with increment \(h\), resulting in \(N=T/h\) time points \(t_{k}:=kh\) for \(k\{0,1,,N\}\). Here, the _estimation horizon_\(T\), such that \(T<\) and \(T\), is chosen by the practitioner (for simplicity assume that \(T/h\) is an integer). With the \(N\) points sampled from one trajectory, a standard way to approximate the integral in Equation (3) is through the _Riemann sum estimator_

\[(h)=_{k=0}^{N-1}^{t_{k}}hX(t_{k})^{ }QX(t_{k}).\] (4)

To estimate \(V_{}\), we average \(M\) independent trajectories with cost estimates \(_{1},_{M}\) to obtain the _Monte-Carlo estimator_:

\[_{M}(h)=_{i=1}^{M}_{i}(h)= _{i=1}^{M}_{k=0}^{N-1}^{t_{k}}hX(t_{k})^{ }QX(t_{k}).\]

Our primary goal is to understand the mean-squared error of the Monte-Carlo estimator for a fixed system (specified by \(A\), \(\) and \(Q\)), to inform an optimal choice of the step-size parameter \(h\) for a _predetermined data budget_\(B=M N\).

Note that one degree of freedom remains in choosing \(M\) and \(N\). For simplicity, we require that in the finite-horizon setting, the estimation grid is chosen to cover the full episode \([0,]\) which leads to the constraint \(T==N h\). We write the mean-squared error-surface as a function of \(h\) and \(B\):

\[_{T}(h,B)=(_{M}(h)-V_{T})^{2}.\] (5)

In the infinite horizon setting, i.e. \(=\), the _estimation horizon_\(T\) is a free variable chosen by the experimenter that determines the number of trajectories through \(M==\). The mean-squared error for the infinite horizon setting is given as a function of \(h\), \(B\), and \(T\):

\[_{}(h,B,T)=(_{M}(h)-V_{})^{2} \,.\] (6)Characterizing the Mean-Squared Error (MSE)

In the following our aim is to characterize the MSE of the Monte-Carlo estimator as a function of the step-size \(h\) and data budget \(B\) (and estimation horizon \(T\) in the infinite horizon setting). Our results uncover a fundamental trade-off for choosing an _optimal_ step-size that leads to a minimal MSE.

One-Dimensional Langevin ProcessTo simplify the exposition while preserving the main ideas, we will first present the results for the 1-dimensional case. The analysis for the vector case exhibits the same quantitative behavior but is significantly more involved. To distinguish the 1-dimensional from the \(n\)-dimensional setting described in Equation (3), we use lower-case symbols. Let \(x(t)\) be the scalar state variable that evolves according to the following Langevin equation:

\[x(t)=ax(t)\;t+\;w(t).\] (7)

Here, \(a\) is the drift coefficient and \(w(t)\) is a Wiener process with scale parameter \(>0\). We assume that \(a 0\), i.e. the system is stable (or marginally stable).

The realized sample path in episode \(i=1,,M\) is \(x_{i}(t)\) (with starting state \(x(0)=0\)) and \(t[0,T]\). The expected cost is

\[V_{}=_{0}^{}^{t}r_{i}(t)\;t =_{0}^{}^{t}qx_{i}^{2}(t)\;t,\] (8)

where \(r_{i}(t)=qx_{i}^{2}(t)\) is the quadratic cost function for a fixed \(q>0\). The Riemann sum that approximates the cost realized in episode \(i[M]\) becomes \(_{i}(h)=_{k=0}^{N-1}hqx_{i}^{2}(kh)\). Given data from \(M\) episodes, the Monte-Carlo estimator is \(_{M}(h)=_{i=1}^{M}_{i}(h)\). Since the square of the cost parameter \(q^{2}\) factors out of the mean-squared error, we set \(q=1\) in what follows.

### Finite-Horizon Setting

Recall that in the finite-horizon setting we set the system horizon \(\) and estimation horizon \(T\) to be the same. This implies that the estimation grid covers the full episode, i.e. \(hN=T=\). Perhaps surprisingly, the mean-squared error of the Riemann estimator for the Langevin system (7) can be computed in closed form. The result takes its simplest form in the finite-horizon, undiscounted setting where \(=1\) and \(<\). This result is summarized in the following theorem.

**Theorem 3.1** (Finite-horizon, undiscounted MSE).: _In the finite-horizon, undiscounted setting, the mean-squared error of the Monte-Carlo estimator is_

\[_{T}(h,B) =E_{1}(h,T,a)+(h,T,a)}{B},\] \[E_{1}(h,T,a) =(-2ah+e^{2ah}-1)^{2}(e^{2aT}-1 )^{2}}{16a^{4}(e^{2ah}-1)^{2}},\] \[E_{2}(h,T,a) =T[h(e^{2aT}-1)(4e^{2ah}+e^{ 2aT}+1)-(e^{2ah}-1)(e^{2ah}+4e^{2aT}+1)T]}{2 a^{2}(e^{2ah}-1)^{2}}.\]

While perhaps daunting at first sight, the result _exactly_ characterizes the error surface as a function of the step-size \(h\) and the budget \(B\) for any given Langevin system. The proof involves computing the closed-form expressions for the second and fourth moments of the random trajectories \(x_{i}(t)\) and is provided in Appendices A and B.1.

In the case of marginal stability (\(a=0\)), a simpler form of the MSE emerges that is easier to interpret. Taking the limit \(a 0\) of the previous expression yields the following result (refer to the discussion and proof in Appendix B.1):

**Corollary 3.2** (MSE for marginally stable system).: _Assume a marginally stable system, \(a=0\). Then the mean-squared error of the Monte-Carlo estimator is_

\[_{T}(h,B)=T^{2}}{4} h^{2}+T^{ 5}}{3}+T^{2}(-2T^{2}+2hT-h^{2})}{3B}.\]The first part of the expression can be understood as a Riemann sum _approximation error_ controlled by the \(h^{2}\) term. The second part corresponds to the _variance term_ that decreases with the number of episodes as \(=\). The remaining terms are of lower order terms for small \(h\) and large \(B\). For a fixed data budget \(B\), the step-size \(h\) can be chosen to balance these two terms:

\[h^{*}(B):=*{arg\,min}_{h>0}_{T}(h,B) T( )^{1/3}\,,\] (9)

where the approximation omits higher order terms in \(1/B\). From this, we can compute the optimal number of episodes \(M^{*}}{T}=()^{1/3}B^{2/3}\). We remark that under the assumption \(B 1\), we also obtain that \(M^{*} 1\). This is in agreement with the implicit requirement that \(h\) is big enough to consider at least one whole trajectory, i.e. \(h>T/B\).

Consequently, the mean-squared error for the optimal choice of \(h\) (up to lower order terms in \(1/B\)):

\[_{T}(h^{*},B) 3(3/2)^{1/3}^{4}T^{4}B^{-2/3}\,.\]

In other words, the optimal error rate as a function of the data budget is \((B^{-2/3})\). We can further obtain a similar form for \(h^{*}\) for the general case where \(a 0\).

**Corollary 3.3** (Approximate MSE).: _The MSE is_

\[_{T}(h,B)=c_{1}(,a,T)h^{2}+(,a,T)}{hB}+ (+h^{3})\]

_for \(h 0\) and \(B\), with system-dependent constants_

\[c_{1}(,a,T)=^{4}-1)^{2}}{16a^{2}}\,, c_{2}( ,a,T)=-^{4}+e^{2aT}(8aT-4)+5)}{8a^{ 4}}\,.\]

_Moreover, for any \(h>0\) and \(B>0\),_

\[c_{1}h^{2}+}{hB}_{T}(h,B) 4c_{1}h^{2}+ }{hB}\]

_with \(c_{1}=c_{1}(,a,T)\) and \(c_{2}=c_{2}(,a,T)\) and the inequalities holds true up to a finite, lower-order polynomial expressions \(h}{B}(h,a,T)\), given in Appendix B.2._

For the proof please see Appendix B.2. From the corollary, we can derive an optimal step-size, up to lower order terms in \(1/B\):

\[h^{*}(B)(-+e^{2aT}(8aT-4)+5)}{a^{2 }(e^{2aT}-1)^{2}})^{1/3}B^{-1/3}.\] (10)

Note that the same \(h^{*}(B)\) also minimizes the upper bound of the MSE up to a constant factor. The scaling \(_{T}(h^{*},B)(B^{-2/3})\) cannot be improved given the lower bound on the MSE. The derivation is provided in Appendix B.3 where we also include a more precise expression of \(h^{*}\).

Discounted CostAdding discounting (\(<1\)) in the finite-horizon setting does not fundamentally change the results; however, it makes the derivation more involved (Appendix B.4).

Vector CaseAddressing the general case (\(n\)-dimensional Langevin systems with \(n>1\)) for a stable matrix \(A\) requires forgoing the _exact_ form of the MSE. We derive tight bounds on the MSE, both of which are convex functions of \(h\), thereby narrowing down its behaviour with respect to the step size. The results are presented in Appendix C.3. Although the convex behaviour is proven only for Langevin systems, our experimental results in Section 4 exhibit a similar trade-off for general nonlinear stochastic systems.

Under the additional assumption that the matrix \(A\) is also diagonalisable, we are again able to exactly characterise the MSE with closed-form computations. Diagonalisability is a mild assumption since it can be achieved under a controllable system. Indeed, controllability allows for the free adjustment of the eigenvalues of the closed-loop matrix \(A\) through the choice of the controller \(K\). The eigenvalues can effectively be chosen to be distinct from each other to ensure a diagonalisable \(A\). While the explicit form of the MSE is computable, its lengthy formula is not easily interpretable and is thus deferred to Appendix C. The following theorem summarizes the result as a Taylor expansion for small \(h\) and large \(B\).

**Theorem 3.4** (Mean-squared error - vector case).: _Assume \(A\) is diagonalisable, with eigenvalues \(=\{_{1},,_{n}\}\). The mean-squared error of the Monte-Carlo estimator in the finite-horizon, undiscounted setting, is_

\[_{T}(h,B) =E_{1}(h,T,)+(h,T,) }{B},\] \[E_{1}(h,T,) =(_{1}+C_{1}() (T))^{4}T^{2}h^{2}+(h^{3})\] \[(h,T,)}{B} =(_{2}+C_{2}() (T))^{4}}{hB}+(1/B).\]

The proof, including the exact derivation of the constants \(_{1}\), \(C_{1}()\), \(_{2}\), \(C_{2}()\), can be found in Appendix C.1. Note that the terms composing the MSE closely resemble those obtained in the scalar analysis. In fact, when comparing them with the expressions in Equation (22) and Equation (23) (in Appendix B.3), the expression has the same order for \(h,B\) and \(T\). The only difference is that in the vector case, cumbersome eigenvalue-dependent constants are involved, whereas in the scalar case, the result can more easily be expressed in terms of the system parameter \(a\).

Since the optimal choice for \(h\) is determined by balancing the trade-off between the two terms above, \(E_{1}\) for the approximation error and \(E_{2}\) for the variance, its expression is analogous to the scalar case, as shown by the following corollary.

**Corollary 3.5** (Optimal step size - vector case).: _Under the assumption that \(B 1\), the optimal step-size for the vector case is given by_

\[h^{*}(B)=(_{1}+C_{1}() (T)}{_{2}()+C_{2}( )(T)})^{1/3}TB^{-1/3}+oB^{-1/3} .\]

The constants in Corollary 3.5 are the same as in Theorem 3.4.

### Infinite-Horizon Setting

The main characteristic of the finite-horizon setting is the trade-off between approximation and estimation error. Recall that in the infinite-horizon setting (\(=\)), the estimation horizon \(T<\) becomes a free variable that is chosen by the experimenter to define the measurement range \([0,T]\). Consequently the mean-squared error of the Monte-Carlo estimator suffers an additional _truncation error_ from using a finite Riemann sum with \(N=T/h\) terms as an approximation to the infinite integral that defines the cost \(V_{}\). More precisely, we decompose the expected cost \(V_{}=V_{T}+V_{T,}\), where \(V_{T}=_{0}^{T}^{t}[x^{2}(t)]dt\) as before, and

\[V_{T,}=_{T}^{}^{t}[x^{2}(t)]\ t=^{T}}{2a}(-}{()+2a})\,.\] (11)

It is a direct calculation based on Lemma A.1 in Appendix. Thus the mean-squared error becomes

\[_{}(h,B,T)=(_{M}(h)-V)^{2}= _{T}(h,B)-2[_{M}(h)-V_{T}]V_{T,}+ V_{T,}^{2},\] (12)

where \(_{T}(h,B)=(_{M}(h)-V_{T})^{2}\) is the mean-squared error of discounted finite-horizon setting. Note that the term \(V_{T,}^{2}\) is neither controlled by a small step-size \(h\) nor by a large data budget \(B\), hence results in the truncation error from finite estimation. Fortunately, geometric discounting ensures that \(V_{T,}^{2}=(^{2T})\), which is not unexpected given that the term constitutes the tail of the geometric integral. In particular, setting \(T=c(B)/(1/)\) for large enough \(c>1\) ensures the truncation error is below the estimation variance. We summarize the result in the next theorem.

**Theorem 3.6** (Infinite-horizon, discounted MSE).: _In the infinite-horizon, discounted setting, the mean-squared error of the Monte-Carlo estimator is_

\[_{}(h,B,T)=^{4}\,T\,C(a,)+}{144} h^{4}+(h^{5})+(B^{-1}),\] (13)

_where we let \(C(a,)=}\) and assume that \(^{T}=o(h^{4})\)._The proof is provided in Appendix B.5. It follows that the optimal choice for the step-size is \(h^{*}(B,T)(36\,T\,C(a,)/B)^{1/5}\). The minimal mean-squared error is \(_{}(h^{*},T,B)(T\,C(a,)/B)^{4/5}+ ^{2T}\). Lastly, we remark that if \(^{T}\) is treated as a constant, the cross term \(_{M}(h)-V_{T}V_{T,}\) in Equation (12) introduces a dependence of order \((h^{2T})\) to the mean-squared error. In this case, the overall trade-off becomes \(_{}(h,B,T)1/(hB)+^{2T}(1+h))\), and the optimal step-size is \(h^{*} B^{-1/2}\).

Vector CaseSimilar to the finite-horizon setting, we establish tight bounds for the MSE in the general case involving a stable matrix \(A\). The detailed results are presented in Appendix C.3. As before, the MSE for the vector case can be computed in closed-form assuming that \(A\) is both diagonalisable and stable. The result reflects the same behaviour as in the scalar case. Conveniently, the MSE in Theorem 3.6 has been expressed with sharp terms in \(h\) and \(B\), while confining the dependence on the system parameter \(a\) within the constant \(C\), and the impact of higher-order terms in \(T\) within \(V_{T,}\). This allows us to state the vector case result in a similar form, where the constant will now depend on the eigenvalues of the matrix \(A\), as well as the discount factor \(\). These are provided in full detail in Appendix C.2.

**Corollary 3.7**.: _For \(A\) diagonalisable, with eigenvalues given by \(\), the mean-squared error of the Monte-Carlo estimator in the infinite-horizon, discounted setting is_

\[_{}(h,B,T)=C_{3}^{4}+}{144}h^{4}+(h^{5}+)\]

_with a constant \(C_{3}=C_{3}(,)\) and under the assumption that \(^{T}=o(h^{4})\)._

The terms in Corollary 3.7 correspond to estimation error, approximation error and truncation error, mirroring the scalar scenario. The optimal step-size exhibits the same dependencies on \(T\) and \(B\) as in the scalar case, albeit with a different constant dependent on the eigenvalues.

## 4 From Linear to Non-Linear Systems: A Numerical Study

The trade-off identified in our analysis suggests that there exists an optimal choice for temporal resolution in policy evaluation. Our next goal is to verify the trade-off in several simulated dynamical systems. While our analysis assumes a linear transition and quadratic cost, we empirically demonstrate that such a trade-off also exists in nonlinear systems. For our experimental setup, we choose simple linear quadratic systems mirroring the setting of Section 2, as well as several standard benchmarks from Gym (Brockman et al., 2016) and MuJoCo (Todorov et al., 2012). Our findings confirm the theoretical results and highlight the importance of choosing an appropriate step-size for policy evaluation.

### Linear Quadratic Systems

We first run numerical experiments on the Langevin dynamical systems to examine the behaviour of the trade-off identified in our analysis. The results are shown in Fig. 1. For these experiments, we fix the noise \(^{2}=1\) and the cost \(Q=I\). The lines in the plot represent the sample mean \((_{M}(h)-V)^{2}\) and the shading represent the standard error of our sample means, computed over \(50\) independent runs. The plots in Fig. 1 exhibit a clear, U-shaped trade-off, as predicted by our theoretical results.

Fig. 1(a) shows the MSE in a one-dimensional system with \(T=8\) and \(a=-1\). The ground truth \(V\) is calculated analytically by using Eq. 21 in the Appendix. The figure illustrates how the error changes as we vary the data budget, \(B=\{2^{12},2^{13},2^{14},2^{15},2^{16}\}\), and also illustrates the improvement that can be obtained by increasing the budget. As we increase \(B\), both the error and the optimal step size \(h^{*}\), decrease. This result strongly aligns with the analysis shown in Theorem 3.1 and Corollary 3.3. The same analysis is performed with respect to the parameter \(a\), while fixing the data budget, in Fig. 1(b). By increasing the absolute value of the drift coefficient, the diffusion has a smaller impact, thus trajectories have less variability. This leads to a smaller \(h^{*}\) for the optimal point of the trade-off.

Fig. 1(c) and 1(d) present the experimental results for both undiscounted finite horizon and discounted infinite horizon multi-dimensional systems. For the finite-horizon setting, \(V\) is computed by numerically solving the Riccati Differential Equation; while in infinite-horizon, it is calculated through Lyapunov equation using a standard solver. In our multi-dimensional experiments, we set the dimension \(n=3\). We fix all parameters and run our experiments for 5 randomly sampled \(3 3\) dense, stable matrices in each setting. More details on the matrix structure can be found in Appendix D.1. Results in both plots suggest that the impact of the eigenvalues of \(A\) on \(h^{*}\) is mild and that the eigenvalue-dependent constant terms in Corollary 3.5 only marginally affect the optimal step-size \(h^{*}\), similar to the trend observed in the scalar case with parameter \(a\). In the infinite horizon system, the horizon needs to be large enough to manage truncation error while simultaneously being small enough to collect multiple trajectories. We choose \(\) large enough such that a good estimate of \(V\) can be obtained, and set \(T=1/(1-)\), which is referred to as the effective horizon in the RL literature.

### Nonlinear Systems

Many nonlinear behaviors can be approximated by a high-dimensional linear system, which would be bounded by our theoretical results on the general case of n-dimensional systems, hinting that similar trade-offs could characterize nonlinear systems as well. We empirically show that the trade-off identified in linear quadratic systems carries over to nonlinear systems, with more complex cost functions. We demonstrate it in several simulated nonlinear systems from OpenAI Gym (Brockman et al., 2016), including Pendulum, BipedalWalker and six MuJoCo (Todorov et al., 2012) environments: InvertedDoublePendulum, Pusher, Swimmer, Hopper, HalfCheetah and Ant. We note that the original environments all have a fixed temporal discretization \( t\), pre-chosen by the designer. To measure the effect of \(h\), we first modify all environments to run at a small discretization \( t=0.001\) as the proxy to the underlying continuous-time systems. We train a nonlinear policy parameterized by a neural network for each system, by the algorithm DAU (Tallec et al., 2019). This policy is used to gather episode data from the continuous-time system proxy at intervals of \( t=0.001\) which are then down-sampled for different \(h\) based on the ratio of \(h\) and \( t\). The policy is stable in the sense that it produces reasonable behavior (e.g., pendulum stays mostly upright; Ant walking forward; etc.) and not cause early termination of episodes (e.g., BipedalWalker does not fall), in the continuous-time system proxy. The results of the MSE of Monte-Carlo policy evaluation are shown in Fig. 2. Similar to the linear systems case, we vary the data budget \(B\) and see how the MSE changes with the step-size \(h\). The MSE shows a clear minimum for choosing the optimal step-size \(h^{*}\), which generally decreases as the data budget increases. We slightly abuse notations by using \(V,\) to refer to the true and estimated sum of rewards instead of the cost. The true value of \(V\) is approximated by averaging the sum of rewards observed

Figure 1: Mean-squared error trade-off in linear quadratic systems of different dimension \(n\). The first two plots show the dependence of the optimal step-size on the data budget \(B\) and drift coefficient \(a\), respectively. A{1,2,3,4,5} in the last two plots are random matrices and the two sets are not equal.

at \( t=0.001\) from \(150k\) episodes. These environments fall under the finite horizon undiscounted setting. The system (and estimation) horizon \(T\) of our experiments is chosen to be the physical time of \(1k\) steps under the default \( t\) in the original environments (with the exception of 200 steps for Pendulum and 500 steps for BipedalWalker). Please refer to Appendix D.4 for more implementation details, including the setup of \(B\), \(T\), \( t\), \(h\), training, and the compute resources. These systems are stochastic in the starting state, while having deterministic dynamics. Despite the different settings from our analysis, a clear trade-off is evident in all systems. This suggests that our findings may have broader applicability than the specific conditions under which our theoretical analysis was established.

### Guidelines for Setting Step-Size \(h\)

The precise characterization of the MSE in Section 3 can be exploited to set the step-size close to the optimal value without any prior knowledge of the system, provided experiments on a smaller data budget are performed beforehand. Although the optimal step-size \(h^{*}\) clearly depends on all quantities characterizing the dynamics and the policy, the technical analysis of the MSE accurately quantifies how \(h^{*}\) scales with respect to the data budget \(B\). Specifically, \(h^{*}(B) c_{F}B^{-1/3}\) for finite horizon and \(h^{*}(B) c_{I}B^{-1/5}\) for infinite horizon, where \(c_{F}\) and \(c_{I}\) hide the dependencies on the system parameters, exposing only the order in \(B\). This allows us to extrapolate the optimal step-size for the given data budget, using the constant estimated with a smaller one. By evaluating through numerical experiments the performances of different step-sizes on the reduced data budget, it is possible to identify an approximate \(h^{*}\), and subsequently determine \(c_{F}\) or \(c_{I}\), which then gives the whole range of optimal values of \(h\) with respect to \(B\) through the aforementioned relation. Note that this approach does not require prior knowledge of the dynamics, yet it provides a systematic way for setting the step size \(h\) for any given scenario.

Figure 3 plots the empirical \(h^{*}\) over \(B\) for all nonlinear environments, and fitted lines based on the relation from the analysis \(h^{*}=c_{F}B^{-1/3}\), where the constant \(c_{F}\) varies with the environment. The plot shows that the scaling of \(h^{*}\) w.r.t. \(B\) predicted by our analysis is overall a good approximation of the trend observed in the experiments with nonlinear systems, except for negative cases like the InvertedDoublePendulum. This suggests that setting the step-size according to the analysis can yield a value close to optimality.

Figure 3: Empirical \(h^{*}\) in nonlinear experiments (solid) compared with analysis in Corollary 3.5 (dashed): \(h^{*}=c_{F}B^{-1/3}\), \(c_{F}\) is estimated from data by least squares.

Figure 2: MSE of Monte-Carlo policy evaluation in nonlinear systems. The line and shaded region denote the sample mean and its standard error of \((_{M}(h)-V)^{2}\), from 30 random runs. \(T\) is the horizon in physical time (seconds). \(B_{0}\) denotes the environment-dependent base sample budget, chosen such that it gives a full episode for the smallest \(h\) (see Appendix D.4). The optimal step-size generally decreases as the data budget increases (with ‘InvertedDoublePendulum-v2’ being the only exception).

Conclusions

We provide a precise characterization of the approximation, estimation and truncation errors incurred by Monte-Carlo policy evaluation in continuous-time linear stochastic dynamical systems with quadratic cost. This analysis reveals a fundamental bias-variance trade-off, modulated by the level of temporal discretization \(h\). Moreover, we confirm in numerical simulations that the analysis accurately captures the trade-off in a precise, quantitative manner. We also demonstrate that the trade-off carries over to non-linear environments such as the popular MuJoCo physics simulation. These findings show that managing the temporal discretization level \(h\) can greatly improve the quality of Monte-Carlo policy evaluation under a fixed data budget \(B\). These results have direct implications for practice, as it remains common to adopt a pre-set step-size regardless of the data resources anticipated, which we have seen is a highly sub-optimal approach for a given budget.

The present work serves as a first step toward understanding the effects of temporal resolution on RL. There are several limitations that we would like to address in future work. Our analysis is restricted to Monte-Carlo estimation, while there are more advanced techniques such as temporal difference learning and direct system identification, which may exhibit different behaviours. Also, our focus is policy evaluation. It is worth studying policy optimization to understand the full control setting, which might require relaxing the stability assumption on the closed-loop system since the controllers being iterated might not always be stable. Additionally, it would be interesting to explore non-uniform discretization schemes, such as through an adaptive sampling scheme. From the system's perspective, we have currently analyzed stochastic linear quadratic systems with additive Gaussian noise and noiseless observations. It remains to be determined if the exact characterization is still attainable with other types of noise, in the partially observable case, or with noisy observations. Finally, extending the analysis to non-linear systems would be valuable.