# Mechanic: A Learning Rate Tuner

Ashok Cutkosky

Boston University

Boston, MA

ashok@cutkosky.com &Aaron Defazio

Meta, FAIR

New York, NY

adefazio@meta.com &Harsh Mehta

Google Research

Mountain View, CA

harshm@google.com

###### Abstract

We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call mechanic. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate mechanic on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, mechanic either comes very close to, matches or even improves upon manual tuning of learning rates.

## 1 Introduction

Modern deep learning is driven by first-order stochastic optimization algorithms. These are algorithms that are designed to solve the classical stochastic optimization problem:

\[ F()=*{}_{}[f(, )]\]

where \(\) is a minibatch of examples, \(^{d}\) is the model parameters, and \(f\) is the loss incurred by using weights \(\) on the minibatch \(\). A first-order algorithm follows the protocol:

1. Output a \(t\)th iterate \(_{t}\).
2. Sample a random minibatch \(_{t}\).
3. Compute \(_{t}= f(_{t},_{t})\) (the gradient is taken with respect to \(_{t}\) only).
4. Possibly update some internal algorithm state based upon \(_{t}\) in preparation for computing the next iterate \(_{t+1}\).

The prototypical such optimization algorithm is stochastic gradient descent (SGD), which exemplifies the attractive features of this approach: it is computationally cheap (running in \(O(d)\) time per update), and with proper tuning obtains minimax optimal convergence guarantees [1; 2]. Modern practice makes use of a wide range of variants of SGD, such SGD with momentum, AdaGrad , Adam , AdamW  or Lion . Interest in such improvements to SGD is driven by the increasing computational demands of training large neural networks: better optimization means cheaper training, which translates to significant savings in terms of time, cost, and environmental impact.

Most modern algorithms for training neural networks are equipped with a scalar "scale factor" or "learning rate" hyperparameter \(s\). Roughly speaking, these algorithms produce iterates of the form \(_{t+1}=_{t}+s_{t}\) where \(_{t}\) is some _update vector_ produced as a function of the observed gradients \(_{1},,_{t}\) (we will use bold font for vectors in \(^{d}\) like \(\) and normal font for all other quantities like \(s\)). As an example, the classical SGD algorithm sets \(_{t}=-_{t}_{t}\) for some sequence of scalars \(\{_{t}\}\) typically called the _schedule_. The formula for the SGD update is:

\[_{t+1}=_{1}-s_{i=1}^{t}_{i}_{i}.\] (1)The process of selecting the optimal \(s\) is called "tuning", and is a key resource sink in machine learning. The typical approach is simply to try many possibilities to find the empirically optimal \(s\), which requires multiple expensive training runs. This paper introduces a technique for choosing \(s\) automatically on-the-fly in order to avoid this expense.

Our procedure, which we call mechanic, is a generic wrapper around any base optimization algorithm (base) that produces a new optimizer which does not require tuning of the scalar \(s\). The base optimization algorithm is allowed to make any kind of update (for example, it may use any kind of schedule, preconditioner or weight decay). If \(_{t}^{}^{d}\) is the \(t\)th iterate of base, then the wrapper will produce a scalar \(s_{t}\) and set the \(t\)th iterate of the wrapped algorithm to be \(_{t}=_{1}^{}+s_{t}(_{t}^{}-_{1}^{})\). As an example, suppose that base is the classical SGD algorithm with update equation (1). Then, given \(s_{t}\), we would set \(_{t}=_{1}^{}-s_{t}_{i=1}^{t-1}_{i} _{i}\). Disregarding for now the fact that the gradients \(_{i}\) actually depend on the iterates \(_{i}\)1, we see that \(_{t}\) is what the \(t\)th iterate of SGD _would have been_ if the schedule were scaled by \(s_{t}\).

Removing tuning of learning rate scalars is already a well-studied problem. One of the main attractions of early work in "adaptive" optimization such as AdaGrad and Adam [3; 7; 4] is that these algorithms require less tuning than ordinary SGD. Over the last decade, a number of works have aimed to tackle this problem from both an empirical and theoretical perspective [8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19]. An intuitive approach might take the route of "hy-pergradient descent": that is, differentiating the update step of the optimization algorithm itself (e.g. [8; 9]). Strangely, it appears to be difficult to prove that such schemes behave well: theory-based approaches often adopt rather different strategies. Instead, we start from known theory and propose a few important modifications to produce a simple and effective practical implementation. We then rigorously evaluate our algorithm on a variety of datasets. We emphasize that our primary contribution is not new theoretical development, but instead the translation between theory and practice, which involves fusing several known analytical techniques as well as subtle departures from theory.

Previous works that investigate deep learning performance of "learning-rate free" optimization inspired by theory (e.g. [20; 21; 16; 15]) have already demonstrated impressive results. However, these works typically build "hand-crafted" algorithms that often blend theoretical analysis with specific empirically successful algorithms such as Adam. In contrast, our wrapper works well with any base algorithm and so can seamlessly integrate new empirical advances in optimization: one does not need intimate familiarity with the analysis of our approach to apply it to a new algorithm.

## 2 Background: Online Convex Optimization

We develop our formalism via _online convex optimization_ (OCO) [22; 23; 24]. OCO is a popular framework for design and analysis of stochastic optimization algorithms. In brief, for each of \(T\) rounds (corresponding to \(T\) iterations of optimization), the OCO algorithm must first output a \(t\)th iterate \(_{t}\), after which the algorithm is presented with a \(t\)th loss function \(_{t}\). Typically, one envisions the case \(_{t}()=(,_{t})\) for some fixed loss \(\) and new data point \(_{t}\). The goal of an algorithm alg is to minimize the _regret_\(R^{}(})\):

\[R^{}(})_{t=1}^{T}_{t}( _{t})-_{t}(}).\]

Many references focus primarily on the case \(}=_{t=1}^{T}_{t}()\) in order to consider the single scalar value \(_{}}R_{T}(})\)[25; 26], but we will employ the formulation of regret as a function above instead as it is strictly more general. When \(_{t}\) is convex, then with \(_{t}_{t}(_{t})\) (or, more generally when \(_{t}\) is a subgradient of \(_{t}\) at \(_{t}\)), we have:

\[R^{}(})_{t=1}^{T}_{t}, _{t}-} R^{}_{}(}).\]

As a result, the vast majority of OCO algorithms provide analysis that bounds only the linearized regret \(R^{}_{}(})\). Such algorithms do not need to observe the entire function \(_{t}\): instead, they only make use of the gradients \(_{t}\). That is, the \(t\)th output of alg (i.e. \(_{t}\)) is purely a function of the previous sequence of gradients \(_{1},,_{t-1}\) so that alg is a first-order algorithm.

### Learning the Scale in OCO

Just like stochastic optimization algorithms, most OCO algorithms also require a scale factor \(s\). In fact, many stochastic optimization algorithms (such as SGD and AdaGrad) are _also_ OCO algorithms. Setting \(_{t}=\) for all \(t\), SGD ensures the regret bound:

\[R^{}(}) R^{}_{}(}) O(}-_{1}\|^{2}}{s} +s_{t=1}^{T}\|_{t}\|^{2}).\] (2)

From this equation, one can deduce in hindsight that for any given \(}\), the optimal value for \(s\) is \(}-_{1}\|}{^{T}\|_{t}\|^{2}}}\), which would provide the bound:

\[R^{s}_{}(}) O(\| }-_{1}\|^{T}\|_{t}\|^{2}} ).\] (3)

This result is minimax optimal , but requires knowledge of the unknown optimal \(s\). Very recently,  have produced algorithms that estimate the value of \(\|_{1}^{}-}\|\) on-the-fly and use this estimate to quickly identify the optimal scaling value \(s\). These algorithms achieve impressive practical performance, but they require an understanding of the closed-form solution for the optimal \(s\) value above. Our goal is to learn the correct scaling regardless of the base algorithm.

To this end, we will leverage a scheme recently developed by  that allows one to automatically tune the scale of a base OCO algorithm using another "meta" OCO algorithm. We reproduce their result below (with notation altered to suit our application) along with the short proof:

**Theorem 1** ().: _Suppose base and tuner are both OCO algorithms. Let \((_{t}^{})^{d}\) indicate the iterates of base in response to an arbitrary sequence of gradients \(\{_{t}\}\), and let \(\{s_{t}\}\) indicate the iterates of tuner in response to the sequence of scalars \(\{h_{t}=_{t},_{t}^{}-_{1}\}\). Define a new online algorithm mechanic via:_

\[_{t}^{}=_{1}^{}+s_{t}( _{t}^{}-_{1}^{}).\]

_Then \(_{t}^{}\) ensures regret:_

\[R^{}_{}(})_{}R^{ }_{}()+R^{}_{} ((}-_{1}^{})/).\]

Proof.: By definition, for any \(\), we have:

\[R^{}_{}(}) =_{t=1}^{T}_{t},_{1}^{ }+s_{t}(_{t}^{}-_{1}^{})-}\] \[=_{t=1}^{T}_{t},_{t}^{ }-_{1}^{}(s_{t}-)+_{t=1}^{T} _{t},_{t}^{}-_{1}^{}-(}-_{1}^{})/\] \[=R^{}_{}()+R^{ }_{}(_{1}^{}+(}-_ {1}^{})/).\]

With this result, the job of finding the optimal \(s\) can usually be completely relegated to tuner. Although the value \(\) appears in both terms of the sum \(R^{}_{}()+R^{}_{} (_{1}^{}+(}-_{1}^{ })/)\), it turns out that for essentially all plausible base algorithms, there is a particular value \(\) that causes \(R^{}_{}(_{1}^{}+(}-_{1}^{})/)\) to obtain the optimal regret bound (3). Thus, by setting \(\) to be this value, which is unknown _a priori_, we need only ensure that \(R^{}_{}()\) is small enough to not significantly affect the overall regret bound. Note that this setting of \(\) is done entirely in the analysis. For example, if base is actually SGD with a learning rate \(\) and \(s=1\) as in (2), we have

\[R^{}(}) R^{}_{}( })_{}R^{}_{}()+O (}-_{1}\|^{2}}{}+ _{t=1}^{T}\|_{t}\|^{2}),\]setting \(=}-_{1}\|}{^{T}\| _{t}\|^{2}}}\):

\[ R_{}^{}(}- _{1}\|}{^{T}\|_{t}\|^{2}}})+O(\|}-_{1}\|^{T}\|_{t}\|^{2}}).\]

Thus, if tuner obtains low regret, then we will obtain the same regret bound as if we had optimally tuned the scaling factor for SGD. Intuitively, the gradient \(h_{t}\) provided to tuner approximates the gradient over the entire course of the base optimizer rather than just at the most recent iterate. That is, for SGD, \(h_{t}_{t},_{t})}{ds}\) where \(_{t}=_{1}-s_{k=1}^{t-1}_{k}_{k}\).

### Parameter-Free Online Optimization

The problem with the above result is that we seem to have simply pushed the problem off to tuner: what if tuner itself requires us to set a scale factor? Solving this problem has been the focus of a substantial effort in the online optimization community [29; 30; 10; 11; 28; 12]. The most advanced such algorithms are able to ensure for all \(\) simultaneously:

\[R_{}()=_{t=1}^{T}h_{t}(s_{t}-) (||^{T}h_{t}^{2}}).\] (4)

Thus, if we set \(h_{t}=_{t},_{t}^{}-_{1}^{ }\), we obtain:

\[R_{}^{}()(| {s}|^{T}_{t},_{t}^{}- _{1}^{}^{2}}).\]

In a theoretical development of this technique, it is necessary to prevent the terms \(_{t},_{t}^{}-_{1}^{}^{2}\) from becoming too large (as otherwise \(R_{}^{}\) is too large). Typically, this is accomplished by constraining the base algorithm to satisfy \(\|_{t}^{}-_{1}^{}\|\) for some user-specified arbitrary \(\). Enforcing such a constraint means that the regret bound (2) would only apply to \(\|}\|\), but ensures that \(_{t},_{t}^{}-_{1}^{}^{2}^{2}\|_{t}\|^{2}\). Thus, by setting \(=\|}-_{1}^{}\|/\), the combined algorithm obtains the optimal regret bound of \(O(\|}-_{1}^{}\|^{T}\| _{t}\|^{2}})\) (amazingly, the value of \(\) is irrelevant!). In practice however, we do not attempt to explicitly enforce any such constraints and simply rely on the intuition that any non-diverging algorithm is unlikely to produce excessively large iterates.

At no point in this process do we need access to the internal state of the base algorithm base. This means that improvements to base will automatically be reflected in improvements to the overall algorithm. In this paper, we investigate the performance of mechanic on deep learning tasks. We consider a variety of settings for the base algorithm base (i.e. AdamW, Lion, SGD, with various batch sizes and learning rate schedules of various shapes), and employ a parameter-free algorithm as the tuner to automatically find the best scale factor for the base algorithm.

## 3 The mechanic algorithm

Our mechanic algorithm is specified in Algorithm 1. The algorithm is built by applying Theorem 1 to a parameter-free tuner algorithm presented in Algorithm 2, which is described along with theoretical analysis in Appendix D. However, when building mechanic, we modify the "pure" theoretically tractable Algorithm 2 to simplify the implementation while still capturing the essential intuition and maintaining the same performance. In the remainder of this section we will provide some intuition behind the tuner update as used in mechanic as well as describing some potentially unfamiliar subtleties relating to our use of exponentially weighted moving averages.

mechanic takes as input a base algorithm that generates _update vectors_\(_{t}\) as described in the previous sections. We then set \(_{t+1}=_{k=1}^{t}_{k}=_{t+1}^{}-_{1}^{}\). The majority of the algorithm contains our tuner method, which is a variant of the analytically tractable Algorithm 2, with a few modifications. Note that the indexing on \(\) is very important and may be counterintuitive: the definition of \(h_{t}\) does _not_ include \(_{t+1}\), but rather \(_{t}\). \(h_{t}\) is the "gradient" that is supplied to tuner, as described by Theorem 1.

To gain some intuition behind the update, let us consider the case that \(n=1\) and \(=1.0\) (that is, without employing any exponentially-weighted moving averages). We keep track of the quantity \(W_{t}=s_{init} m_{t}-_{k=1}^{t}h_{k}s_{k}\), which is usually called the "wealth" of the algorithm (the quantity \(r_{t}=-_{k=1}^{t}h_{k}s_{k}\) is sometimes called the "reward"). \(s_{init}\) specifies the starting value for \(s_{t}\) and should be an under-estimate of the true optimal scaling. We then set \(s_{t+1}=}{}}\) (neglecting the \(\) included for numerical stability). To understand this update strategy, we can re-write the update as:

\[s_{t+1}=s_{t}}}{}}-h_{t}}{ {v_{t}}}(1-^{2}}{2v_{t}})s_{t}-h_{t}}{ }}.\]

Thus, the update looks like a combination of an AdaGrad-esque gradient descent step with learning rate scaled by \(s_{t}\) and a kind of "adaptive decay" (multiplication by \(1-^{2}}{2v_{t}}\)). The adaptive decay is very important for stabilizing the algorithm: without it the values for \(s_{t}\) are prone to unchecked exponential growth due to scaling by \(s_{t}\) in \(h_{t}}{}}\). Intuitively, this decay is the minimum amount required to prevent instabilities.

In Appendix D, we provide a formal Theorem bounding the regret of a variant of the procedure described above. Roughly speaking, for \(=1\) this result suggests:

\[_{t=1}^{T}h_{t}(s_{t}-) O((+_{t}s_{t}) m_{ T}+(T/s_{init})^{T}h_{t}^{2}}).\] (5)

In fact, the dependence of \(O((T))\) in equation (5) can be improved to \(O()\) via more complicated algorithms (e.g. ). However, we favor the simpler update and pleasing resemblance to familiar algorithms like AdaGrad via the Taylor expansion analysis above. Of note, the dependence on \(s_{init}\) is very mild: this suggests that we should be able to set \(s_{init}\) to a very small value without damaging performance. In practice, we choose \(s_{init}=10^{-8}\), which we expect to dramatically underestimate the optimal value in all scenarios.

We hypothesize that the simplified tuner we use in mechanic in fact possesses a rigorous theoretical analysis (although perhaps only with respect to simpler non-fully-worst-case adversaries), but demonstrating such a bound appears to involve difficult technical hurdles. In particular, our implementation is designed to be "scale-free": rescaling the values of \(_{t}\) by any constant scalar will have no effect on \(s_{t}\). This property was first achieved only recently in theoretical analysis of parameter-free algorithms , and as-yet requires significantly more involved algorithms [12; 33].

### The use of \(\)

We include \(\) to introduce some recency bias in the statistics recorded by mechanic, a common feature of practical optimizers. Mathematically, we accomplish this by up-weighting the _t_th feedback to tuner by \(^{-t}\): \(h_{t} h_{t}^{-t}\). Thus, for example, we have \(_{t}=_{k=1}^{t}h_{k}^{2}^{-2kt}\) and \(r_{t}=-_{k=1}^{t}h_{k}s_{k-1}_{s}^{-k}\). Using these weights directly results in numerical stability issues as the weights become exponentially large. Instead, since we only need to maintain the correct ratio \(}{}}\), we can cancel a factor of \(_{s}^{-t}\) from both sides, giving the update equations in Algorithm 2.

We found that tuning the value of \(\) can significantly improve performance on different tasks. Thus, we incorporated multiple \(\) values simultaneously in a way that obviates the need for such tuning.

Our approach is inspired by work on "combining" parameter free algorithms . The idea is simple: parameter-free algorithms typically ensure \(R_{}(0)\) for some _constant_\(\) set by the user. So, if \(s_{t,1},,s_{t,n}\) are the outputs of \(n\) parameter-free algorithms with regret bounds \(R_{}^{1}(),,R_{}^{n}()\), we have for any \(j\):

\[_{t=1}^{T}h_{t}(_{i=1}^{n}s_{t,i}-) =_{t=1}^{T}h_{t}(s_{t,j}-)+_{i j}_{t=1}^{T }h_{t}(s_{t,i}-0)\] \[=R_{}^{j}()+_{i j}R_{}^ {i}(0) R_{}^{j}()+(n-1).\]

So, with small constant additive overhead in the regret, the sum of all the outputs \(s_{t,1}++s_{t,n}\) achieves the same regret as the _best_ of all the outputs. Motivated by this observation, we instantiate \(n=6\) copies of tuner with different \(\) values and add their iterates to produce a final scaling.

### Weight decay

Finally, we found that an addition of a peculiar weight-decay-esque term helped significantly on certain tasks, including vision tasks with smaller datasets, multi-objective NLP tasks and especially with reducing the variance in final results for all tasks. Specifically, rather than providing \(h_{t}=_{t},_{t}\) as the input to the tuner algorithm, we instead provide \(h_{t}=_{t}+_{t}\|(_{i=1}^{ n}s_{t,i})_{t}}{\|_{t}\|},_{t}\). We conjucture that this term is helpful in the common case that the base algorithm itself is incorporating regularization or weight-decay.

This extra term is the derivative of the regularizer \(\|_{t}\|(_{i=1}^{n}s_{t,i})\| \|\). From a standard theoretical perspective, this regularization may seem overly large. However, it may not have as big an impact as one might imagine because the base algorithm does not see this regularization. Instead, the base algorithm may (or may not) perform weight decay using another method that mechanic has no insight into. That said, we do not propose an analytical explanation for this modification. We simply observed that in practice it performed well with a fixed \(=0.01\).

### Runtime and Memory Cost

mechanic incurs little additional cost over that of base. In Algorithm 1, we denote \(d\)-dimensional vectors with bold font, and \(n\)-dimensional vectors and scalars with normal font (note that typically \(n=6\)). We use 1 additional \(O(d)\) memory slot, and four \(O(d)\)-time steps in lines 8, 9, 10 and 17. All other steps are \(O(1)\) or \(O(n)\) time and so have negligible cost.

## 4 Experiments

In this section we describe our experiments using mechanic to tune various base optimizers on different tasks. Note that almost all base optimizer implementations require a user-specified scale factor which is is not directly visible to mechanic. We set this value to 1.0 before applying mechanic. Since mechanic multiplies the base update by \(s_{t}\), setting the base scale factor to \(1.0\) allows us to interpret \(s_{t}\) as the "correct" value for the base scale.

### Masked Language Modeling

We perform BERT pre-training on the Wikibooks dataset following the procedure from  with a few minor changes, most notably, we omit the Next Sentence Prediction (NSP) loss following . Masked language modeling (MLM) requires reconstructing randomly masked tokens given an input sequence of tokens. As shown in Table 1, using mechanic leads to a noticeable improvement in MLM accuracy.

**Varying batch size and model size:** Past works observe that the scale factor \(s\) should decrease as either batch size is decreased or model size is increased . To inspect the scale factor that mechanic learns, we vary the batch size and model size while pre-training BERT using mechanic. As shown in Figure 1, in both cases, mechanic learns to decrease the scale factor \(s\) when decreasing the batch size and when increasing the model size.

**Addition Ablations:** Ablation studies on the effects of \(n\), \(\), \(s_{init}\) can be found in Appendix B.

**Finetuning pre-trained models:** In addition to pre-training, we evaluate our models on the 5 largest datasets from the GLUE suite . One possible failure mode of mechanic tuned pre-trained models could have been that, even though they lead to high accuracy at pre-training time, transfer learning may fail at finetuning time.

To ensure that standard transfer learning pipelines still work with mechanic pre-trained models, we finetune them without a learning rate tuner using the AdamW optimizer and find that mechanic pre-trained models lead to higher accuracy at pre-training time, and they also outperform in finetuning more often than not. We finetune BERT-B (110M) and BERT-L (340M) models for at most 10 epochs on each of the GLUE datasets and report results on the GLUE dev set in Table 1.

**Using mechanic for finetuning:** We also investigated using mechanic for finetuning. Typically, to not erase the progress already made, a much lower base learning rate is employed at finetuning

   Model & Size & Pre Opt & MLM & Optimizer & MNLI-m/mm & QNLI & SST-2 & QQP \\   &  & AdamW & 71.5 &  AdamW \\ \(\)-AdamW \\  & 84.3/84.8 & 91.0 & 92.4 & 90.1 \\  & & \)-AdamW} & **71.7** &  AdamW \\ \(\)-AdamW \\  & 83.7/83.5 & 90.6 & 91.9 & 90.5 \\  & & & & & & \\   & & & & & & \\   &  & Lion & 71.8 &  Lion \\ \(\)-Lion \\  & 83.4/83.5 & 86.8 & 89.7 & 89.4 \\  & & & & & & \\   & & & & & & \\   & & & & & & \\   &  & AdamW & **75.4** &  AdamW \\ \(\)-AdamW \\  & 86.2/86.4 & 92.2 & 93.9 & 91.3 \\  & & & & & & & \\   & & & & & & \\   & & & & & & \\   &  & Lion & **75.7** & 
 Lion \\ \(\)-Lion \\  & 86.7/86.6 & 90.7 & 92.9 & 91.1 \\  & & & & & & \\   & & & & & & \\   & & & & & & \\   

Table 1: Comparing mechanic on BERT. 5 largest datasets from GLUE. Results reported are peak validation scores averaged over 3 runs, both for the baseline and mechanic tuned models.

time. This could easily have been a potential failure mode of any kind of automatic learning rate tuner as such strategies might "explore" a high learning rate at the beginning of the optimization procedure. Fortunately, we observed that this inductive bias typically baked at finetuning time is still maintained when using mechanic.

### Image Classification

In this Section, we present results on popular Image Classification tasks. Apart from training from scratch, we also perform transfer learning experiments where we pre-train on the JFT-300M  dataset and finetune on ImageNet, Cifar-10 and Cifar-100 datasets. We follow the exact setting employed in  for both pre-training and finetuning.

As shown in Table 2, mechanic is quite competitive across the board and produces results either very close to the baseline or better. Since mechanic optimizes for the train loss, in general, we observe that it results in better **test** performance on tasks with large amounts of data where the model is unable to overfit to the train set. For instance, we see that mechanic beats the baseline substantially when pre-training ViT models on JFT-300M, whereas it lags slightly behind on smaller datasets like ImageNet-1k or CIFAR-10/100. Even though we fix \(\) to 0.01 as default for all our reported experiments, we find that for small datasets like CIFAR-10, increasing it led to better test performance.

### Comparison with D-adaptation

Recently,  introduced the D-adaptation algorithm, with the same goal of learning the correct scale \(s\) for SGD and Adam base optimizers. D-adaptation showed impressive empirical results on a range of popular deep learning tasks, so we compare mechanic with D-adaptation on a selection of tasks that D-adaptation worked well on, using code provided by the authors. Hyper-parameter settings were kept the same to ensure a fair comparison. In contrast to D-adaptation, mechanic does not require modification for different base optimizers and, as shown in Figure 2, it remains quite

Figure 1: Scaling values \(s\) learned by mechanic while varying batch size and model size.

competitive on small datasets like CIFAR-10/100 while outperforming both a manually tuned baseline and D-adaptation on bigger tasks like IWSLT14 and language modeling on BookWiki dataset. We present additional results in Appendix C.3, including a comparison on a suite of 12 logistic regression problems.

## 5 Conclusion

mechanic is a new technique for scaling the updates of any base optimization algorithm. Our approach provides a practical implementation of recent developments in optimization theory, and is able to match the performance of tuned baselines on large-scale machine learning tasks. This work suggests several natural future directions. First, is there a theoretical motivation for our weight-decay term? Next, is it possible to leverage similar techniques to learn a _per-layer_ scale factor? Such a capacity would not significantly increase computation cost, but by allowing more degrees of freedom may yield a method that significantly outperforms baselines since it is infeasible to manually tune a scale factor for every layer.

#### Acknowledgments

Ashok Cutkosky acknowledges funding support from NSF grant CCF-2211718, an Amazon research award, and Google.