# Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets

Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets

Arthur da Cunha

Universite Cote d'Azur, Inria, CNRS, I3S

 Aarhus University

 Aarhus, Denmark

dac@cs.au.dk

Francesco d'Amore

Aalto University

 Bocconi University

 Espoo, Finland

francesco.damore@aalto.fi

&Emanuele Natale

Universite Cote d'Azur, Inria, CNRS, I3S

 Sophia Antipolis, France

emanuele.natale@inria.fr

###### Abstract

The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH. In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.

This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning.

## 1 Introduction

Much of the success of deep learning techniques relies on extreme over-parameterization (Zhang et al., 2017, 2021; Novak et al., 2018; Brutzkus et al., 2018; Du et al., 2019; Kaplan et al., 2020). While such excess of parameters has allowed neural networks to become the state of the art in many tasks, the associated computational cost limits both the progress of those techniques and their deployment in real-world applications. This limitation motivated the development of methods for reducing the number of parameters of neural networks; both in the past (Reed, 1993) and in the present (Blalock et al., 2020; Hoefler et al., 2021).

Although pruning methods have traditionally targeted reducing the size of networks for inference purposes, recent works have indicated that they can also be used to reduce parameter counts during training or even at initialisation without sacrificing model accuracy. In particular, Frankle and Carbin (2019) proposed the _Lottery Ticket Hypothesis (LTH)_, which conjectures that randomly-initialisednetworks contain sparse subnetworks that can be trained and reach the performance of the fully-trained original network. Empirical investigations on the LTH (Zhou et al., 2019; Ramanujan et al., 2020; Wang et al., 2020) pointed towards an even more striking phenomenon: the existence of subnetworks that perform well without any training. This conjecture was named the _Strong Lottery Ticket Hypothesis (SLTH)_ by Pensia et al. (2020).

While the SLTH has been proved for many different classes of neural networks (see Section 2), those works are restricted to unstructured pruning, where the subnetworks are obtained by freely removing individual parameters from the original network. However, this lack of structure can significantly reduce the gains that sparsity can bring, both in terms of memory and computational efficiency. The possibility of removing parameters at arbitrary points of the network implies the need to store the indices of the remaining non-zero parameters, which can become a significant overhead with its own research challenges (Pooch and Nieder, 1973). Moreover, the theoretical computational gains of unstructured sparsity can also be difficult to realise in standard hardware, which is optimised for dense operations. Most notably, the irregularity of the memory access patterns can lead to both data and instruction cache misses, significantly reducing the performance of the pruned network.

The limitations of parameter-level pruning have motivated extensive research on _structured pruning_, which constrain the sparsity patterns to reduce the complexity of parameter indexation and, more generally, to make the processing of the pruned network more efficient. A simple example of structured pruning is _neuron pruning_ of fully-connected layers: deletions in the weight matrix are constrained to the level of whole rows/columns. As illustrated by Figure 1, pruning under this constraint produces a smaller network that is still dense, directly reducing the computational costs without any need for extra memory to store indices. Similarly, deleting entire filters in Convolutional Neural Networks (CNNs) (Polyak and Wolf, 2015) or "heads" in attention-based architectures (Michel et al., 2019) also produces direct reductions in computational costs.

It is important to note that structured pruning is a restriction of unstructured pruning so, theoretically, the former is bound to perform at most as well as the latter. For example, by deleting whole neurons one can remove about 70% of the weights in dense networks without significantly affecting accuracy, while through unstructured pruning one can usually reach 95% sparsity without accuracy loss (Alvarez and Salzmann, 2016; Liu et al., 2019). In practice, however, the computational advantage of structured pruning can offset this difference: a suitably structured sparse network can be more efficient than an even sparser one that lacks structure. This trade-off between sparsity and actual efficiency has motivated the study of less coarse sparsity patterns since weaker structural constraints such as strided sparsity (Anwar et al., 2017) (Figure 2b) or block sparsity (Siswanto, 2021) (Figure 2c) are already sufficient to deliver the bulk of the computational gains that structured can offer.

Despite its benefits, structured pruning has received little attention in the context of the SLTH. In fact, the work that first proved a version of the SLTH, Malach et al. (2020), has remained the only one to study this scenario, to the best of our knowledge. Moreover, it brings a negative result: the authors prove that removing neurons from a randomly-initialised shallow neural network (a single hidden layer) is equivalent to the random features model1(e.g., Rahimi and

Figure 1: Illustration of neuron pruning. The left side shows the effect of pruning of neurons in the weight-matrix of a fully-connected layer. The rows in white correspond to neurons pruned in the associated layer while the columns in white represent the effect of removing neurons from the previous layers. On the right, we allude to the possibility of collapsing the pruned matrix into a smaller, dense one.

which cannot efficiently approximate even a single ReLU neuron (Yehudai and Shamir, 2019).

As most proofs around the SLTH involve pruning a shallow random network to build the desired approximations, those results show that the SLTH can be challenging to tackle when restricted to neuron pruning.

In addition, we believe that a factor hindering progress toward structured versions of the SLTH more generally is a limitation of a result underlying almost all of the theoretical works on the SLTH: a theorem by Lueker on the _Random Subset-Sum Problem (RSSP)_.

**Theorem 1** ((Lueker, 1998; Da Cunha et al., 2023)).: _Let \(X_{1},,X_{n}\) be independent uniform random variables over \([-1,1]\), and let \((0,1/3)\). There exists a universal constant \(C>0\) such that, if \(n C(1/)\), then, with probability at least \(1-\), for all \(z[-1,1]\) there exists \(S_{z}[n]\) for which_

\[z-_{i S_{z}}X_{i}.\]

In general terms, the theorem states that given a rather small number of random variables, there is a high probability that any target value within an interval of interest can be approximated by a sum of a subset of the random variables. An important remark is that even though Theorem 1 is stated in terms of uniform random variables, it is not hard to extend it to a wide class of distributions.2

While Theorem 1 closely matches the setup of the SLTH, it only concerns individual random variables and directly applying it to entire random structures, as needed when considering structured pruning, would require an exponential number of random variables. The recent works by Borst et al. (2022); Becchetti et al. (2022) reduced this gap by proving multidimensional versions of Theorem 1. Still, the intricate manipulation of the network parameters in proofs around the SLTH imposes restrictions that are not covered by those results.

#### Contributions

In this work, we overcome those obstacles and prove that random networks in a wide class of CNNs are likely to contain structured subnetworks that approximate any sufficiently smaller CNN. To the best of our knowledge, our results provide the first sub-exponential bounds around the SLTH for structured pruning of deep neural networks of any kind. More precisely,

* We prove a multidimensional version Theorem 1 that is robust to some dependencies between coordinates, which is crucial for structured pruning (Theorem 5);
* We leverage this result and, by combining two types of structured sparsity (block and neuron/filter sparsity), we show that, with high probability and for a wide class of architectures, polynomially over-parameterized random networks can be pruned in a structured manner to approximate any target network (Theorem 2);

Figure 2: Examples of different pruning patterns.

* Our results cover CNNs, which generalise fully-connected networks as well as many layer types commonly used in modern architectures, such as pooling and normalisation layers;
* Additionally, our pruning scheme focuses on filter pruning, which, like neuron pruning, allows for a direct reduction of the size and computational cost relative to the original CNN.

## 2 Related Work

SlthPut roughly, research on the SLTH revolves around the following question:

_Question._ Given an error margin \(>0\) and a target neural network \(f_{}\), how large must an architecture \(f_{}\) be to ensure that, with high probability on the sampling of its parameters, one can prune \(f_{}\) to obtain a subnetwork that approximates \(f_{}\) up to output error \(\)?

Malach et al. (2020) first proved that, for dense networks with ReLU activations, it was sufficient for \(f_{}\) to be twice as deep and polynomially wider than \(f_{}\). Orseau et al. (2020) showed that the width overhead could be greatly reduced by sampling parameters from a hyperbolic distribution. Pensia et al. (2020) improved the original result for a wide class of weight distribution, requiring only a logarithmic width overhead, which the authors proved to be asymptotically optimal. Da Cunha et al. (2022) generalised those results with optimal bounds to CNNs with non-negative inputs, which Burkholz (2022a) extended to general inputs and to residual architectures. Burkholz (2022a) also reduced the depth overhead to a single extra layer and provided results that include a whole class of activation functions. Burkholz (2022b) obtained similar improvements to dense architectures. Fischer and Burkholz (2021) modified many of the previous arguments to take into consideration networks with non-zero biases. Ferbach et al. (2022) further generalised previous results on CNNs to general equivariant networks. Diffenderfer and Kailkhura (2021) obtained similar SLTH results for binary dense neural networks within polynomial depth and width overhead, which Sreenivasan et al. (2022) improved to polylogarithmic overhead.

Structured pruningWorks on structured pruning date back to the early days of the field of neural network sparsification with works such as Mozer and Smolensky (1988) and Mozer and Smolensky (1989). Since then, a vast literature has been built around the topic, particularly for the pruning of CNNs. For a survey of structured pruning in general, we refer the reader to the associated sections of Hoefler et al. (2021), and to He and Xiao (2023) for a survey on structured pruning of CNNs.

RsspPensia et al. (2020) introduced the use of theoretical results on the RSSP in arguments around the SLTH, namely Lueker (1998, Corollary 3.3). The work by Da Cunha et al. (2023) provides an alternative, simpler proof of this result. Borst et al. (2022) and Becchetti et al. (2022) proved multidimensional versions of the theorem. Theorem 5 diverges from those results in that it supports some dependencies between the entries of random vectors.

## 3 Preliminaries and contribution

Given \(n\), we denote the set \(\{1,,n\}\) by \([n]\). The symbol \(*\) represents the convolution operation, \(\) represents the element-wise (Hadamard) product, and \(\) represents the ReLU activation function. The notation \(_{1}\) refers to the sum of the absolute values of each entry in a tensor. Similarly, \(_{2}\) refers to the square root of the sum of the squares of each entry in a tensor. \(_{}\) denotes the maximum norm: the maximum among the absolute value of each entry. Sometimes we represent a tensor \(X^{d_{1} d_{n}}\) by the notation \(X=(X_{i_{1},,i_{n}})_{i_{1}[d_{1}],,i_{n}[d_{n}]}\). We denote the normal probability distribution with mean \(\) and variance \(^{2}\) by \((,^{2})\). We write \(U^{d_{1} d_{n}}\) to denote that \(U\) is a random tensor of size \(d_{1} d_{n}\) with independent and identically distributed (i.i.d.) entries, each following \((0,1)\). We refer to such random tensors as _normal tensors_. Finally, we refer to the axis of a 4-D tensor as _rows_, _columns_, _channels_, and _kernels_ (a.k.a. filters), in this order.

For the sake of simplicity, we assume CNNs to be of the form \(N[-1,1]^{D D c_{0}}^{D D c_{}}\) given by

\[N(X)=K^{}*(K^{-1}*(K^{1}*X)),\]

where \(K^{i}^{d_{i} d_{i} c_{i-1} c_{i}}\) for \(i[]\), and the convolutions have no bias and are suitably padded with zeros. Moreover, when the kernels \(K^{(i)}\) are normal tensors, we say that \(N\) is a _random CNN_.

Our main result is the following.

**Theorem 2** (Structured SLTH).: _Let \(D,c_{0},\), and \(_{>0}\). For \(i[]\), let \(d_{i},c_{i},n_{i}\). Let \(\) be the class of functions from \([-1,1]^{D D c_{0}}\) to \(^{D D c_{}}\) such that, for each \(f\)_

\[f(X)=K^{()}*(K^{(-1)}*(K^{(1)}*X)),\] (1)

_where, for \(i[]\), \(K^{(i)}^{d_{i} d_{i} c_{i-1} c_{i}}\) and \(\|K^{(i)}\|_{1} 1\)._

_Let also \(N_{0}[-1,1]^{D D c_{0}}^{D D c_{ }}\) be a \(2\)-layered random CNN given by_

\[N_{0}(X)=L^{(2)}*(L^{(2-1)}*(L^{(1)}*X)),\] (2)

_where for \(i[]\) the kernels \(L^{(2i-1)}\) and \(L^{(2i)}\) are normal tensors of shape \(1 1 c_{i-1} 2n_{i}c_{i-1}\) and \(d_{i} d_{i} 2n_{i}c_{i-1} c_{i}\), respectively._

_Finally, let \(\) be the class of subnetworks that can be obtained by pruning contiguous blocks of parameters and removing entire filters from \(N_{0}\)._

_There exists a universal constant \(C>0\), such that if, for \(i[]\),_

\[n_{i} Cd_{i}^{13}c_{i}^{6}^{3}^{2}c_{i}c_{i-1}}{ },\]

_then, with probability at least \(1-\), we have that, for all \(f\),_

\[_{X[-1,1]^{D D c_{0}}}_{g}\|f(X)-g( X)\|_{}.\]

The filter removals ensured by Theorem 2 take place at layers \(1,3,,2-1\) and imply the removal of the corresponding channels in the next layer. The overall modification yields a CNN with kernels \(^{(1)},,^{(2)}\) such that, for \(i[]\), the kernels \(^{(2i-1)}\) and \(^{(2i)}\) have shape \(1 1 c_{i-1} 2c_{i-1}m_{i}\) and \(d_{i} d_{i} 2c_{i-1}m_{i} c_{i}\), respectively, where \(m_{i}=/(C_{1}d_{i}(1/))}\) for a universal constant \(C_{1}\). Moreover, our proof ensures that the kernels \(^{(2i-1)}\) can be required to have a specific type of block sparsity: they can be structured as if pruned by \(2m_{i}\)-channel-blocked masks, defined as follows.

**Definition 3** (\(n\)-channel-blocked mask).: Given a positive integer \(n\), a binary tensor \(S\{0,1\}^{d d c cn}\) is called \(n\)-channel-blocked if and only if

\[S_{i,j,k,l}=1&=k,\\ 0&,\]

for all \(i,j[d]\), \(k[c]\), and \(l[cn]\).

We remark that, from a broader perspective, the central aspect of Theorem 2 is that the lower bound on the size of the random CNN depends only on the kernel sizes of the CNNs being approximated.

In subsection 4.2 we discuss the proof of Theorem 2. It requires handling subset-sum problems on multiple random variables at once (random vectors). Furthermore, the inherent parameter-sharing of CNNs creates a specific type of stochastic dependency between coordinates of the random vectors, which we capture with the following definition.

**Definition 4** (NSN vector).: A \(d\)-dimensional random vector \(Y\) follows a _normally-scaled normal_ (NSN) distribution if, for each \(i[d]\), \(Y_{i}=Z Z_{i}\) where \(Z,Z_{1},,Z_{d}\) are i.i.d. random variables following a standard normal distribution.

A key technical contribution of ours is a Multidimensional Random Subset Sum (MRSS) result that supports NSN vectors. In subsection 4.1 we discuss the proof of the next theorem, which follows a strategy similar to that of [Borst et al., 2022, Lemmas 1, 15].

**Theorem 5** (Normally-scaled MRSS).: _Let \(0< 1/4\), and let \(d\), \(k\), and \(n\) be positive integers such that \(n k^{2}\) and \(k Cd^{3}\) for some universal constant \(C_{>0}\). Furthermore, let \(X_{1},,X_{n}\) be \(d\)-dimensional i.i.d. NSN random vectors. For any \(^{d}\) with \(\|\|_{1}\), there exists with constant probability a subset \(S[n]\) of size \(k\) such that \(\|(_{i S}X_{i})-\|_{}\)._

While it is possible to naively apply Theorem 1 to obtain a version of Theorem 2, doing so leads to an exponential lower bound on the required number of random vectors.

Analysis

In this section, after proving our MRSS result (Theorem 5), we discuss how to use it to obtain our main result on structured pruning (Theorem 2). Full proofs are deferred to Appendix B.

### Multidimensional Random Subset Sum for normally-scaled normal vectors

Notation.Given a set \(S\) and a positive integer \(n\), we denote by \(\) the family of all subsets of \(S\) containing exactly \(n\) elements of \(S\). Given \(_{>0}\), we define the interval \(I_{}(z_{i})=[z_{i}-,z_{i}+]\) and the multi-interval \(I_{}()=[-,+]\), where \(=(1,1,,1)^{d}\). Moreover, for any event \(\), we denote its complementary event by \(}\).

In this subsection, we estimate the probability that a set of \(n\) random vectors contains a subset that sums up to a value that is \(\)-close to a given target. The following definition formalises this notion.

**Definition 6** (Subset-sum number).: Given (possibly random) vectors \(X_{1},,X_{n}\) and a vector \(\), we define the _\(\)-subset-sum number_ of \(X_{1},,X_{n}\) for \(\) as

\[T_{X_{1},,X_{n}}^{k}()=_{S} _{_{S}^{()}},\]

where \(_{S}^{()}\) denotes the event \(\|(_{i S}X_{i})-\|_{}\). We write simply \(T_{n,k}\) when \(X_{1},,X_{n}\) and \(\) are clear from the context.

To prove Theorem 5 we use the second moment method to provide a lower bound on the probability that the subset-sum number \(T_{n,k}\) is strictly positive, which implies that at least one subset of the random vectors suitably approximates \(\). Hence, we seek a lower bound on \([T_{n,k}]^{2}/[T_{n,k}^{2}]\).

Our first lemma provides a lower bound on the probability that a sum of NSN vectors is \(\)-close to a target vector, through which one can infer a lower bound on \([T_{n,k}]\).

**Lemma 7** (Sum of NSN vectors).: _Let \(k\), \((0,)\), \(^{d}\) such that \(\|\|_{1}\) and \(k 16\). Furthermore, let \(X_{1},,X_{k}\) be \(d\)-dimensional i.i.d. NSN random vectors with \(d k\), and let \(c_{d}=\{},\}\). It holds that_

\[_{i=1}^{k}X_{i} I_{}() }+2 c_{d})k}}^{d}.\]

Overview of the proof.: The main technical difficulty lies in the fact that the random vectors \(X_{1},,X_{k}\) are NSN vectors. Bounds can be easily derived for the case where the \(X_{i}\) are i.i.d. normal random vectors by observing that the sum of normal random variables is also normal.

For \(i[k]\), each entry of \(X_{i}\) can be written as \(Z_{i} Z_{i,j}\) where \(Z_{i}\) and \(Z_{i,j}\) are i.i.d. normal random variables. Conditional on \(Z_{1},,Z_{k}\), the \(d\) entries of \(X=_{i=1}^{k}X_{i}\) are independent and distributed as \((0,_{i=1}^{k}Z_{i}^{2})\). By noticing that \(Z_{i}^{2}\) is a chi-squared random variable and employing standard concentration inequalities (Lemma 17 in Appendix A) combined with the law of total probability, we can proceed as if the entries of \(X\) were normal, up to some correction factors. 

Bounding \([T_{n,k}^{2}]\) requires handling stochastic dependencies. Thus, we estimate the joint probability that two subsets of \(k\) elements of \(X_{1},,X_{n}\) sum \(\)-close to the same target, taking into account that the intersection of the subsets might not be empty. The next lemma provides an upper bound on this joint probability that depends only on the size of the symmetric difference between the two subsets.

**Lemma 8** (Sum of NSN vectors).: _Let \(k,j_{0}\) with \(1 j k\) Furthermore, let \(X_{1},,X_{k+j}\) be i.i.d. \(d\)-dimensional NSN random vectors with \(k Cd^{3}\). Let \(c_{d}=\{},\}\), \(A=_{i=1}^{j}X_{i}\), \(B=_{i=j+1}^{k}X_{i}\), and \(C=_{i=k+1}^{k+j}X_{i}\).3 Then, it holds that_

\[(A+B I_{}(),B+C I_{} ()) 3}{(1-2 })j}^{d}.\]Overview of the proof.: Let \(n=k+j\). We exploit once more the fact that, for all \(i[n]\), each entry \(X_{i}\) can be written as \(Z_{i} Z_{i,j}\) where \(Z_{i}\) and \(Z_{i,j}\) are i.i.d. normal random variables. Conditional on \(Z_{1},,Z_{k+j}\), the \(d\) entries of \(A\), \(B\), and \(C\) are independent and distributed as \((0,_{i=1}^{j}Z_{i}^{2})\), \((0,_{i=j+1}^{k}Z_{i}^{2})\), and \((0,_{i=k+1}^{k+j}Z_{i}^{2})\), respectively. Hence, by the concentration inequalities for the sum of chi-squared random variables (Lemma 17 in Appendix A) and by the law of total probability, we can focus on the term

\[(A_{i}+B_{i} I_{}(z_{i}),B_{i}+C_{i} I_{ }(z_{i})|Z_{1},,Z_{n}),\]

where \(A_{i}\), \(B_{i}\), and \(C_{i}\) indicate the \(i\)-th entries of \(A\), \(B\), and \(C\), respectively.

Another concentration argument for normal random variables (Lemma 14 in Appendix A), allow us to show that

\[(A_{i}+B_{i} I_{}(z_{i}),B_{i}+ C_{i} I_{}(z_{i})|Z_{1},,Z_{n})\] \[=_{B_{i}}[(A_{i} I_{} (z_{i}-B_{i}),C_{i} I_{}(z_{i}-B_{i})|Z_{ 1},,Z_{n},B_{i})]\] \[=_{B_{i}}[(A_{i} I_{} (z_{i}-B_{i})|Z_{1},,Z_{n},B_{i})(C_{i} I_{ }(z_{i}-B_{i})|Z_{1},,Z_{n},B_{i})]\] \[_{B_{i}}[(A_{i} I_{ }(0)|Z_{1},,Z_{n},B_{i})(C_{i} I_{ }(0)|Z_{1},,Z_{n},B_{i})]\] \[=(A_{i} I_{}(0)|Z_{1}, ,Z_{n})(C_{i} I_{}(0)|Z_{1}, ,Z_{n}).\]

Thus, we have reduced our argument to the estimation of probabilities of independent normal random variables being close to zero. 

The following lemma provides an explicit expression for the variance of the \(\)-subset-sum number.

**Lemma 9** (Second moment of \(T_{n,k}\)).: _Let \(k,n\) be positive integers. Let \(S_{0},S_{1},,S_{k}\) be subsets of \([n]\) such that \(|S_{0} S_{j}|=k-j\) for \(j=0,1,,k\). Let \(,^{}\) be two random variables yielding two subsets of \([n]\) of size \(k\) drawn independently and uniformly at random. Let \(X_{1},,X_{n}\) be \(d\)-dimensional i.i.d. NSN random vectors. For any \(>0\) and \(^{d}\), the second moment of the \(\)-subset sum number is_

\[[T_{n,k}^{2}]=^{2}_{j=0}^{k}( |^{}|=k-j)(_{S_{0}}^{()}_{S_{j}}^{() }),\]

_where \(_{S}^{()}\) denotes the event \(\|(_{i S}X_{i})-\|_{}\)._

Proof.: Let \(,^{}\) two random variables yielding two elements of \(\) drawn independently and uniformly at random. By the definition of \(T_{n,k}\), we have that

\[[T_{n,k}^{2}] =_{S}_{ _{S}^{()}}_{S^{} }_{_{S^{}}^{()}} \] \[=_{S,S^{}}(_{S}^{ ()}_{S^{}}^{()})\] \[=_{S,S^{}}(_{S}^{ ()}_{S^{}}^{()} \,=S,^{}=S^{})(= S,^{}=S^{})\] \[=^{2}_{j=0}^{k}(_{S}^{( )}_{S^{}}^{()}\, ^{}|=k-j)(| ^{}|=k-j),\]

as \((_{S}^{()}_{S^{}}^{ ()})\) depends only on the size of \(^{}\).

### Overview of the proof of Theorem 5

We use the second moment method (Lemma 15 in Appendix A) on the \(\)-subset-sum number \(T_{n,k}\) of \(X_{1},,X_{n}\). Thus, we want to lower bound the right-hand side of

\[(T>0)[T_{n,k}]^{2}}{[T_{n,k}^{2}]}.\]

Equivalently, we can provide an upper bound on the inverse, \([T_{n,k}^{2}]}{[T_{n,k}]^{2}}\). By Lemma 9,

\[[T_{n,k}^{2}]=^{2}_{j=0}^{k}(| ^{}|=k-j)(_{S_{0}}^{ ()}_{S_{j}}^{()})\]

where \(,^{},S_{i}\) and \(_{S}^{()}\) are defined as in the statement of the lemma. Observe also that

\[[T_{n,k}]=_{S}[ _{_{S}^{()}}]=_{S }(_{S}^{()})= (_{S_{0}}^{()}).\]

By using the two above observations, we have

\[[T_{n,k}]^{2}}{[T_{n,k}^{2}]} =^{2}}{[T_{n,k}]^{2}} _{j=0}^{k}(|^{}|=k-j)( _{S_{0}}^{()}_{S_{j}}^{( )})\] \[=_{j=0}^{k}(|^{}|=k-j )_{S_{0}}^{()} _{S_{j}}^{()})}{(_{ S_{0}}^{()})^{2}}.\]

Lemma 7 provides a lower bound on the term \((_{S_{0}}^{()})\) while Lemma 8 gives an upper bound on the term \((_{S_{0}}^{()}_{S_{j} }^{()})\).

In the full proof, we then show that \((|^{}| k/d)\) can be bounded using the Chernoff bound (Lemma 16 in Appendix A) even if we do not deal directly with binomial random variables. This allows us to discard the indices \(j\) for which \((_{S_{0}}^{()}_{S_{j} }^{()})\) is large, which leads to the result after some technical manipulations.

### Proving SLTH for structured pruning

To prove Theorem 2, we first show how to obtain the same approximation result for a single-layer CNN. Then, we iteratively apply the same argument for all layers of a larger CNN and show that the approximation error stays small.

We define the _positive_ and _negative_ parts of a tensor.

**Definition 10**.: Given a tensor \(X^{d_{1} d_{n}}\), the _positive_ and _negative_ parts of \(X\) are respectively defined as \(X_{}^{+}=X_{}_{X_{}>0}\) and \(X_{}^{-}=-X_{}_{X_{}<0}\), where \([d_{1}][d_{n}]\) points at a generic entry of \(X\).

#### Approximating a single-layer CNN

We first present a preliminary lemma that shows how to prune a single-layer convolution \((V X)\) in a way that dispenses us from dealing with the ReLU \(\).

**Lemma 11**.: _Let \(D,d,c,n\) be positive integers, \(V^{1 1 c 2nc}\), and \(X^{D D c}\). Let \(S_{1}\{0,1\}^{(V)}\) be a \(2n\)-channel-blocked mask. There exists a mask \(S_{2}\{0,1\}^{(V)}\) which only removes filters such that, for each \((i,j,k)[D][D][2nc]\), if \(=S_{1} S_{2}\), then_

\[(V X)_{i,j,k}= V^{+} X^{+}+V ^{-} X^{-}_{i,j,k}.\]Overview of the proof.: \(S_{2}\{0,1\}^{(V)}\) is such that \(=V=(V S_{1}) S_{2}\) contains only non-negative edges going from each input channel \(t\) to the output channels \(2(t-1)n+1,,(2t-1)n\), and only non-positive edges going from each input channel \(t\) to the output channels \((2t-1)n+1,,2tn\). Notice that, after applying the \(2n\)-channel-blocked mask \(S_{1}\), the only possible non-zero entry of the \(k\)-th filter is located at its \(t=\)-th channel. Hence, for each \(k[2nc]\), we keep filter \(k\) if the entry of its \(t\)-th channel is non-negative and \(2t-1=\), or it is non-positive and \(2t=\). 

We approximate a single convolution \(K*X\) by pruning a polynomially larger neural network of the form \(U*(V*X)\) exploiting only a channel-blocked mask and filter removal: this is achieved using the MRSS result (Theorem 5).

**Lemma 12** (Kernel pruning).: _Let \(D,d,c_{0},c_{1},n\) be positive integers, \((0,),M_{>0}\), and \(C_{>0}\) be a universal constant with_

\[n Cd^{13}c_{1}^{6}^{3}c_{1}c_{0}}{}.\]

_Let \(U^{d d 2nc_{0} c_{1}}\), \(V^{1 1 c_{0} 2nc_{0}}\) and \(S\{0,1\}^{(V)}\), with \(S\) being a \(2n\)-channel-blocked mask. We define \(N_{0}(X)=U*(V*X)\) where \(X^{D D c_{0}}\), and its pruned version \(N_{0}^{(S)}(X)=U*((V S)*X)\). With probability \(1-\), for all \(K^{d d c_{0} c_{1}}\) with \(\|K\|_{1} 1\), it is possible to remove filters from \(N_{0}^{(S)}\) to obtain a CNN \(_{0}^{(S)}\) for which_

\[_{X:\|X\|_{} M}\|K*X-_{0}^{(S)} (X)\|_{}< M.\]

Overview of the proof.: Exploiting Lemma 11, for each \((r,s,t_{1})[d][d][c_{1}]\), one can show that

\[(U*((V)*X))_{ r,s,t_{1}} =_{i,j[d],t_{0}[c_{0}]}_{k[nc_{0}]}U_{i,j, k,t_{1}}_{1,1,t_{0},k}^{+} X_{r-i+1,s-j+1,t_{0}}^{+}\] \[+_{i,j[d],t_{0}[c_{0}]}_{k[nc_{0}]}U_{i,j,k,t_{1}}_{1,1,t_{0},k}^{-} X_{r-i+1,s-j+1,t_{0 }}^{-},\]

where \(=S_{1} S_{2}\), with \(S_{1}\) being a \(2n\)-channel-blocked mask and \(S_{2}\) being a mask that removes filters. Through a Chernoff bound, we show that \(_{1,1,t_{0},:}^{+}\) has at least \(n/3\) non-zero entries. Up to reshaping the tensor as a one-dimensional vector, we observe that \(U_{:,:,k,:}_{1,1,t_{0},k}^{+}\) is an NSN vector (Lemma 18 in Appendix A). Hence, we can apply a boosted version of the MRSS result (Corollary 19 in Appendix A) and show that, with high probability, for all target filters \(K\) with \(\|K\|_{1} 1\), we can prune all but roughly \(d)}\) positive entries of \(_{1,1,t_{0},k}^{+}\), with \(C_{1}\) being a universal constant, such that \(_{k[nc_{0}]}U_{:,:,k,:}_{1,1,t_{0},:}^{+}\) approximates the channels \(K_{:,:,t_{0},:}\) up to error \(/(2d^{2}c_{0}c_{1})\). The same holds for \(_{k[nc_{0}]}U_{:,:,k,:}_{1,1,t_{0},:}^{-}\). This pruning can be achieved by applying a third mask \(S_{3}\) that only removes filters. Through some non-trivial calculations and by applying the Tensor Convolution Inequality (Lemma 20 in Appendix A), one can combine the above results to get the thesis. Notice that the overall pruning can be represented by the mask \(S_{1}(S_{2} S_{3})\), where \(S_{2} S_{3}\) is a mask that only removes filters, and \(S=S_{1}\) is a \(2n\)-channel-blocked mask. 

_Remark 13_.: From the proofs of Lemmas 12 and 11, we can see that the overall modification yields a pruned CNN \(*(*X)\) with \(^{1 1 c_{0} 2mc_{0}}\) and \(^{d d 2mc_{0} c_{1}}\), where \(m=d)}\) for a universal constant \(C_{1}\). Moreover, the kernel \(\) is obtained through a 3-stage pruning process: First, we apply a \(2n\)-channel-blocked mask \(S_{1}\). Second, we remove filters based on entries' signs through a mask \(S_{2}\). Third, we remove filters according to the MRSS result through a mask \(S_{3}\). Masks \(S_{2}\) and \(S_{3}\) can be combined into a single mask \(S_{2} S_{3}\) that only removes filters: overall the pruning process consists of a \(2n\)-channel-blocked mask and filter removal, which justifies the statement of Theorem 2.

### Overview of the proof of Theorem 2

We iteratively apply Lemma 12 to each layer while carefully controlling the approximation error via tools such as the Lipschitz property of ReLU and the Tensor Convolution Inequality (Lemma 20). More precisely, we show that (i) the approximation error does not increase too much at each layer; and (ii) all layer approximations can be combined to approximate the entire target network. Notice that Lemma 12 guarantees that, with high probability, we can approximate all possible target filters \(K\) (with \(_{1} 1\)) from the same 2-layered network. Hence, we get the result for the supremum over all possible choices of target filters.

## 5 Limitations and future work

In previous works (da Cunha et al., 2022, Burkholz, 2022a) the assumption that the kernel of every second layer has shape \(1 1\) is only an artifact of the proof since one can readily prune entries of an arbitrarily shaped tensor to enforce the desired shape. In our case, however, the concept of structured pruning can be quite broad, and such reshaping via pruning might not fit some sparsity patterns, depending on the context. The hypothesis on the shape can be a relevant limitation for such use cases. The constructions proposed by Burkholz (2022a,b) appear as a promising direction to overcome this limitation, with the added benefit of reducing the depth overhead.

The convolution operation commonly employed in CNNs can be cumbersome at many points of our analysis. Exploring different concepts of convolution can be an interesting path for future work as it could lead to tidier proofs and more general results. For instance, employing a 3D convolution would spare a factor \(c\) in Theorem 2.

Another limitation of our results is the restriction to ReLU as the activation function. Many previous works on the SLTH exploit the fact that ReLU satisfies the identity \(x=(x)-(-x)\). Burkholz (2022a) leveraged that to obtain an SLTH result for CNNs with activation functions \(f\) for which \(f(x)-f(-x) x\) around the origin. Our analysis, on the other hand, does not rely on such property, so adapting the approach of (Burkholz, 2022a) to our setting is not straightforward.

Finally, we remark that the assumption of normally distributed weights might be relaxed. Borst et al. (2022) provided an MRSSP result for independent random variables whose distribution converges "fast enough" to a Gaussian one.4 We believe our arguments can serve well as baselines to generalise our results to support random weights distributed as such.