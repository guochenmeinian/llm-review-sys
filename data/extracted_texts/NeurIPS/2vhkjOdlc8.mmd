# Dinomaly: The _Less Is More_ Philosophy in Multi-Class Unsupervised Anomaly Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images, serving as an alternative to the conventional one-class-one-model setup. Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models. Our research aims to bridge this substantial performance gap. In this paper, we introduce a minimalistic reconstruction-based anomaly detection framework, namely Dinomaly, which leverages pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks. Given this powerful framework consisted of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) _Foundation Transformers_ that extracts universal and discriminative features, (2) _Noisy Bottleneck_ where pre-existing Dropouts do all the noise injection tricks, (3) _Linear Attention_ that naturally cannot focus, and (4) _Loose Reconstruction_ that does not force layer-to-layer and point-by-point reconstruction. Extensive experiments are conducted across three popular anomaly detection benchmarks including MVTec-AD, VisA, and the recently released Real-IAD. Our proposed Dinomaly achieves impressive image AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also surpasses the most advanced class-separated UAD records.

## 1 Introduction

Unsupervised anomaly detection (UAD) aims to detect abnormal patterns from normal images and further localize the anomalous regions. Because of the diversity of potential anomalies and their scarcity, this task is proposed to model the accessible training sets containing only normal samples as an unsupervised paradigm. UAD has a wide range of applications, e.g., industrial defect detection, medical disease screening, and video surveillance, addressing the difficulty of collecting and labeling all possible anomalies in these scenarios.

Efforts on UAD attempt to learn the distribution of available normal samples. Most advanced methods utilize networks pre-trained on large-scale datasets, e.g. ImageNet , for extracting discriminative and informative feature representations. Specifically, _Feature reconstruction_[2; 3; 4] and _feature distillation_ methods [5; 6] are proposed to reconstruct features of pre-trained encoders, based on the hypothesis that the networks trained on normal images can only construct normal regions, but fail for unseen anomalous regions. _Feature statistics_ methods [7; 8; 9] memorize and model all anomaly-free features extracted from pre-trained networks in training, and compare them with the test features during inference. _Pseudo-anomaly_ methods [10; 11] generate pseudo defects or noiseson normal images or features to imitate anomalies, converting UAD to supervised classification  or segmentation tasks [10; 12].

Conventional works on UAD build a separate model for each object category, as shown in Figure 1(a). However, this one-class-one-model setting entails substantial storage overhead for saving models , especially when the application scenario necessitates a large number of object classes. For UAD methods, a compact boundary of normal patterns is vital to distinguish anomalies. Once the intra-normal patterns become exceedingly complicated due to various classes, the corresponding distribution becomes challenging to measure, consequently harming the detection performance. Recently, UniAD  and successive studies have been proposed to train a unified model for multi-class anomaly detection (MUAD), as shown in Figure 1(b). Under this setting, the "identity mapping" that directly copies the input as the output regardless of normal or anomaly harms the performance of conventional methods . This phenomenon is caused by the diversity of multi-class normal patterns that drive the network to generalize on unseen patterns.

Within two years, a number of methods have been proposed to address MUAD, such as neighbor-masked attention , synthetic anomalies , feature jitter , vector quantization , diffusion model [25; 26], and state space model (Mamba) . However, there is still a non-negligible performance gap between the state-of-the-art (SoTA) MUAD methods and class-separated UAD methods, restricting the practicability of implementing unified models, as shown in Figure 1(c). In addition, previous methods employ modules and architectures delicately designed, which may not be straightforward, and consequently suffer from limited universality and usability.

In this work, we aim to catch up with the performance of class-separated anomaly detection models using a multi-class unified model, namely Dinomaly. To begin with, we build a reconstruction-based UAD framework that consists of only vanilla Transformer blocks , i.e. Self-Attentis and Multi-Layer Perceptrons (MLPs). Within this framework, we propose four simple but essential elements that boost Dinomaly to perform equal to or better than SoTA conventional class-separated models. First, we show that self-supervised pre-trained Vision Transformers (ViT) , especially the DINO family [29; 30], serve as powerful feature encoders to extract discriminative representations as

Figure 1: Setting and Performance of UAD and multi-class UAD (MUAD). (a) Task setting of class-separated UAD. (b) Task setting of MUAD. (c) Comparison of Dinomaly and previous SoTA methods [13; 14; 15; 16; 8; 17; 18; 19] on MVTec-AD , VisA , and Real-IAD .

reconstruction objects. Second, as an alternative to carefully designed pseudo anomaly and feature noise, we propose to activate the out-of-the-box Dropout in an MLP to prevent the network from restoring both normal and anomalous patterns, which is previously referred to as identity mapping. Third, we propose to utilize the "side effect" of Linear Attention (a computation-efficient counterpart of Softmax Attention) that makes it hard to focus on local regions, to further alleviate the issue of identity mapping. Fourth, previous methods adopt layer-to-layer and region-by-region reconstruction schemes, distilling a decoder that can well mimic the behavior of the encoder even for anomalous regions. Therefore, we propose to loosen the reconstruction constraints by grouping multiple layers as a whole and discarding well-reconstructed regions during optimization.

To validate the effectiveness of the proposed Dinomaly under MUAD setting, we conduct extensive experiments on three widely used industrial defect detection benchmarks, i.e., MVTec AD  (15 classes), VisA  (12 classes), and recently released Real-IAD (30 classes). Notably, we achieve unprecedented image-level AUROC of 99.6%, 98.7%, and 89.3% on MVTec AD, VisA, and Real-IAD, respectively, which surpasses previous SoTA methods by a large margin.

Related works are presented in Appendix A.1.

## 2 Method

### Dinomaly Framework

_"What I cannot create, I do not understand"_--Richer Feynman

The ability to recognize anomalies from what we know is an innate human capability, serving as a vital pathway for us to explore the world. Similarly, we construct a reconstruction-based framework that relies on the epistemic characteristic of artificial neural networks. Dinomaly consists of an encoder, a bottleneck, and a reconstruction decoder, as shown in Figure 2. Without loss of generality, a standard ViT-Base/14 network  with 12 Transformer layers is used as the encoder, extracting informative feature maps with different semantic scales. The bottleneck is a simple MLP (a.k.a. feed-forward network, FFN) that collects the feature representations of the encoder's 8 middle-level layers. The decoder is similar to the encoder, consisting of 8 Transformer layers. During training, the decoder learns to reconstruct the middle-level features of the encoder by maximizing the cosine similarity between feature maps. During inference, the decoder is expected to reconstruct normal regions of feature maps but fails for anomalous regions as it has never seen such samples.

**Foundation Transformers.** Foundation models, especially ViTs [28; 31] pre-trained on large-scale datasets, serve as a basis and starting point for specific computer vision tasks. Such networks employ self-supervised learning schemes such as contrastive learning (MoCov3 , DINO ), masked image modeling (MAE , SimMIM , BEiT ), and their combination (iBOT , DINOv2 ), producing universal features suitable for image-level visual tasks (image classification, instance retrieval) and pixel-level visual tasks (depth estimation, semantic segmentation).

Because of the lack of supervision in UAD, most advanced methods adopt pre-trained networks to extract discriminative features. Recent works [37; 17; 38] have discovered the advantage of robust and universal features of self-supervised models over domain-specific ImageNet features in anomaly detection tasks. In this work, we further utilize the up-to-date Transformer foundation, i.e., DINOv2 with registers , as the encoder of Dinomaly.

### Noisy Bottleneck.

_"Dropout is all you need."_

Generalization ability is a merit of neural networks, allowing them to perform equally well on unseen test sets. However, generalization is not so wanted in the context of unsupervised anomaly detection that leverages the epistemic nature of neural networks. With the increasing diversity of images and their patterns due to multi-class UAD settings, the decoder can generalize its reconstruction ability to unseen anomalous samples, resulting in the failure of anomaly detection using reconstruction error. This phenomenon is called "identity mapping" in previous works of literature [3; 23; 18].

A direct solution for identity mapping is to shift "reconstruction" to "restoration". Specifically, instead of directly reconstructing the normal images or features given normal inputs, previous works propose to add perturbations as pseudo anomalies on input images [10; 40; 12] or feature representations [3; 25] during network forward propagation; meanwhile, still let the decoder restore anomaly-free images or features, formulating a denoising-like framework. However, such methods employ heuristic and hand-crafted anomaly generation strategies, that are not universal across domains, datasets, and methods.

In this work, we propose to activate the pre-existing Dropout in an MLP layer. Dropout, a popular network element introduced by Hinton et al.  in 2014 to prevent overfitting, flourished in nearly all neural network architectures to the present day, including Transformers. In Dinomaly, Dropout is used to discard neural activations in the MLP bottleneck randomly. Instead of alleviating overfitting, the role of Dropout in Dinomaly can be explained as feature noise and pseudo feature anomaly. Although the decoder takes noisy features during training, it is encouraged to restore clean features from the encoder. Without introducing any novel modules, this paradigm forces the decoder to restore normal features given a test image with anomalies, in turn, mitigating identical mapping.

### Unfocused Linear Attention.

_"One man's poison is another man's meat"_

**Softmax Attention** is the key mechanism of Transformers, allowing the model to attend to different parts of its input token sequence. Formally, given an input sequence \(^{N d}\) with length \(N\), Attention first transforms it into three matrices: the query matrix \(^{N d}\), the key matrix \(^{N d}\), and the value matrix \(^{N d}\):

\[=^{Q}\:,=^{K}\:, =^{V}\:,\] (1)

where \(^{Q},^{K},^{V}^{d d}\) are learnable parameters. By computing the attention map by the query-key similarity, the output of Attention is given as: 1

\[(,,)=(^{T})\:.\] (2)

Figure 2: The framework of Dinomaly, built by pure Transformer building blocks.

Because the attention map is obtained by computing the similarity between all query-key pairs followed by row-wise Softmax, the computation complexity is \((N^{2}d)\).

**Linear Attention** was proposed as a promising alternative to reduce the computation complexity of vanilla Softmax Attention concerning the number of tokens . By substituting Softmax operation with a simple activation function \(()\) (usually \((x)=(x)+1\)), we can change the computation order from \((^{T})\) to \((^{T})\). Formally, Linear Attention is given as:

\[(,,)=(() (^{T}))=()((^{T}))\;,\] (3)

where the computation complexity is reduced to \((Nd^{2})\). The trade-off between complexity and expressiveness is a dilemma. Previous studies [43; 44] attribute Linear Attention's performance degradation on supervised tasks to its incompetence in focusing. Due to the absence of non-linear attention reweighting by Softmax operation, Linear Attention cannot concentrate on important regions related to the query, such as foreground and neighbors.

Back to MUAD, previous methods [3; 24] suggest adopting Attentions instead of Convolutions because Convolutions can easily learn identical mappings. Nevertheless, both operations are in danger of forming identity mapping by over-concentrating on corresponding input locations for producing the outputs:

\[=0&0&0\\ 0&1&0\\ 0&0&0\;,=1&0&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\;.\]

In Dinomaly, we turn to leverage the "unfocusing ability" of Linear Attention. In order to probe how Attentions propagate information, we train two variants of Dinomaly using vanilla Softmax Attention or Linear Attention as the spatial mixer in the decoder and visualize their attention maps. As shown in Figure 3, Softmax Attention tends to focus on the exact region of the query, while Linear Attention spreads its attention across the whole image. This implies that Linear Attention, forced by its incompetence to focus, utilizes more long-range information to restore features at each position, reducing the chance of passing identical information of unseen patterns to the next layer

Figure 3: The decoder attention map (min-max to 0-1 for visualization) of Dinomaly with vanilla Softmax Attention _vs._ Linear Attention.

during reconstruction. Of course, employing Linear Attention also benefits from less computation, free of performance drop.

### Loose Reconstruction

_"The tighter you squeeze, the less you have."_

**Loose Constraint.** Pioneers of feature-reconstruction/distillation UAD methods [5; 2] are inspired by knowledge distillation . Most reconstruction-based methods distill specific encoder layers (e.g. 3 last layers of 3 ResNet stages) by the corresponding decoder layers [2; 5; 17] (Figure 4(a)) or the last decoder layer [3; 4] (Figure 4(b)). Intuitively, with more encoder-decoder feature pairs (Figure 4(c)), UAD model can utilize more information in different layers to discriminate anomalies. However, according to the intuition of knowledge distillation, the student (decoder) can better mimic the behavior of the teacher (encoder) given more layer-to-layer supervision, which is harmful for UAD models that detect anomalies by encoder-decoder discrepancy. This phenomenon is also embodied as identity mapping. Thanks to the top-to-bottom consistency of columnar Transformer layers, we propose to loosen the layer-to-layer constraint by adding up all feature maps of interested layers as a whole group, as shown in Figure 4(d). This scheme can be seen as loosening the layer-to-layer correspondence, so that the decoder is allowed to act much more differently from the encoder when the input pattern is unseen. Because features of shallow layers contain low-level visual characters that are helpful for precise localization, we can further group the features into the low-semantic-level group and high-semantic-level group, as shown in Figure 4(e).

**Loose Loss.** Following the above analysis, we also loosen the point-by-point reconstruction loss function by discarding some points in the feature map. Here, we simply borrow the hard-mining global cosine loss  that detaches the gradients of well-restored feature points with low cosine distance during training. Let \(f_{E}\) and \(f_{D}\) denotes (grouped) feature maps of encoder and decoder:

\[_{global-hm}=_{cos}((f_{E}),(f_{ D}))=1-(f_{E})^{T}(f_{D})}{\|(f_{E}) \|\ \|(f_{D})\|},\] (4)

\[f_{D}(h,w)=\{sg(f_{D}(h,w))_{0.1},_{cos}(f_{D}(h,w),f_{E}(h,w))<90\%[ _{cos}(f_{D},f_{E})]_{batch}\\ f_{D}(h,w),.,\] (5)

Figure 4: Schemes of reconstruction constraint. (a) Layer-to-layer (sparse). (b) Layer-to-cat-layer. (c) Layer-to-layer (dense). (d) Loose group-to-group, 1-group (Ours). (e) Loose group-to-group, 2-group (Ours).

where \(()\) denotes flatten operation, \(f_{D}(h,w)\) represents the feature point at \((h,w)\), \(sg()_{0.1}\) denotes shrink the gradient to one-tenth of the original 2, \(_{cos}(f_{D}(h,w),f_{E}(h,w))<90\%[_{cos}(f_{D},f_{E })]_{batch}\) selects 90% feature points with smaller cosine distance within a batch. Total loss is the mean \(_{global-hm}\) of all encoder-decoder feature pairs.

## 3 Experiments

### Experimental Settings

**Datasets. MVTec-AD** contains 15 objects (5 texture classes and 10 object classes) with a total of 3,629 normal images as the training set and 1,725 images as the test set (467 normal, 1258 anomalous). **VisA** contains 12 objects. Training and test sets are split following the official splitting, resulting in 8,659 normal images in the training set and 2,162 images in the test set (962 normal, 1,200 anomalous). **Real-IAD** is a large UAD dataset recently released, containing 30 distinct objects. We follow the official splitting that includes all views, resulting in 36,465 normal images in the training set and 114,585 images in the test set (63,256 normal, 51,329 anomalous).

**Metrics.** Following prior works [19; 17], we adopt 7 evaluation metrics. Image-level anomaly detection performance is measured by the Area Under the Receiver Operator Curve (AUROC), Average Precision (AP), and \(F_{1}\) score under optimal threshold (\(F_{1}\)-max). Pixel-level anomaly localization is measured by AUROC, AP, \(F_{1}\)-max and the Area Under the Per-Region-Overlap (AUPRO). The results of a dataset is the average of all classes.

**Implementation Details.** ViT-Base/14 (patchsize=14) pre-trained by DINOV2-R  is used as the encoder by default. The drop rate of Noisy Bottleneck is 0.2 by default and increases to 0.4 on the diverse Real-IAD. Loose constraint with 2 groups is employed, and the anomaly map is given by the mean per-point cosine distance of the 2 groups. The input image is first resized to \(448^{2}\) and then center-cropped to \(392^{2}\), so the feature map (\(28^{2}\)) is large enough for anomaly localization. StableAdamW optimizer  with AMSGrad  (more stable than AdamW  in training) is utilized with \(lr\)=2e-3, \(\)=(0.9,0.999) and \(wd\)=1e-4. The network is trained for 10,000 iterations (steps) on MVTec-AD and VisA, and 50,000 iterations on Real-IAD. More details are available in Appendix A.2.

### Comparison to Multi-Class UAD SoTAs

We compare the proposed Dinomaly with the most advanced methods. Among them, RD4AD  based on feature reconstruction, SimpleNet  based on feature-level pseudo-anomaly, and DeST-Seg  based on feature reconstruction & pseudo anomaly are designed for conventional class-separated UAD settings. UniAD based on feature reconstruction, ReContrast  based on contrastive reconstruction, ViTAD  based on feature reconstruction & Transformer, DiAD  based on Diffusion reconstruction, and MamboAD  based on feature reconstruction & Mamboa are designed for MUAD settings. Notably, ViTAD and MamboAD are contemporary _arxiv_ preprints released within months. The intuitive comparison is already presented in Figure 1.

Experimental results are presented in Table 1, where Dinomaly surpasses compared methods by a large margin on all datasets and all metrics. On the most widely used MVTec-AD, Dinomaly produces image-level performance of **99.6/99.8/99.0** and pixel-level performance of **98.4/69.3/69.2/94.8**, outperforming previous SoTAs by _1.0/0.2/1.2_ and _0.7/9.1/7.7/1.6_. This result declares that the image-level performance on the MVTec-AD dataset is nearly saturated under the MUAD setting. On the popular VisA, Dinomaly achieves image-level performance of **98.7/98.9/96.2** and pixel-level performance of **98.7/53.2/55.7/94.5**, outperforming previous SoTAs by _3.2/2.5/4.2_ and _0.2/5.3/5.1/2.6_. On the Real-IAD that contains 30 classes, each with 5 camera views, we produce image-level and pixel-level performance of **89.3/86.8/80.2** and **98.8/42.8/47.1/93.9**, outperforming previous SoTAs by _3.0/2.2/3.2_ and _0.3/4.9/5.4/3.4_, indicating our scalability to extremely complex scenarios. Per-class performances and qualitative visualization are presented in Appendix A.5 and A.6. In addition, adopting a larger backbone further improves the above performances, as presented in Table A2.

### Comparison to Class-Separated UAD SoTAs

We also compare our Dinomaly with class-separated SoTAs, as shown in Table 2. On MVTec-AD and VisA, our Dinomaly under MUAD setting is comparable to conventional SoTAs that build individual models for each class [2; 13; 8; 15]. In addition, Dinomaly is subjected to nearly no performance drop compared to its class-separated counterpart on these datasets. On the complicated Real-IAD that involves more images, classes, and views, class-separated Dinomaly sets new SoTA records. Multi-class Dinomaly presents moderate performance drop but is still comparable to class-separated SoTAs.

### Ablation Study

**Overall Ablation.** We conduct experiments to verify the effectiveness of the proposed elements, i.e., Noisy Bottleneck (NB), Linear Attention (LA), Loose Constraint (LC), and Loose Loss (LL). The already-powerful baseline is Dinomaly with noiseless MLP bottleneck, Softmax Attention, dense layer-to-layer supervision, and global cosine loss . Results on MVTec-AD and VisA are shown in Table 3 and Table A1, respectively. NB and LL can directly contribute to the model performance. LA and LC boost the performance with the presence of NB. The use of LC is not solely beneficial because

    &  &  &  \\   & & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\   & RD4AD  & 94.6 & 96.5 & 95.2 & 96.1 & 48.6 & 53.8 & 91.1 \\  & SimpleNet  & 95.3 & 98.4 & 95.8 & 96.9 & 45.9 & 49.7 & 86.5 \\  & DeSTSeg  & 89.2 & 95.5 & 91.6 & 93.1 & 54.3 & 50.9 & 64.8 \\  & UniAD † & 96.5 & 98.8 & 96.2 & 96.8 & 43.4 & 49.5 & 90.7 \\  & ReContrast † & 98.3 & 99.4 & 97.6 & 97.1 & 60.2 & 61.5 & 93.2 \\  & DiAD † & 97.2 & 99.0 & 96.5 & 96.8 & 52.6 & 55.5 & 90.7 \\  & ViTAD † & 98.3 & 99.4 & 97.3 & 97.7 & 55.3 & 58.7 & 91.4 \\  & MambaAD † & 98.6 & 99.6 & 97.8 & 97.7 & 56.3 & 59.2 & 93.1 \\  & **Dinomaly** (Ours) & **99.6** & **99.8** & **99.0** & **98.4** & **69.3** & **69.2** & **94.8** \\   & RD4AD  & 92.4 & 92.4 & 89.6 & 98.1 & 38.0 & 42.6 & 91.8 \\  & SimpleNet  & 87.2 & 87.0 & 81.8 & 96.8 & 34.7 & 37.8 & 81.4 \\  & DeSTSeg  & 88.9 & 89.0 & 85.2 & 96.1 & 39.6 & 43.4 & 67.4 \\  & UniAD † & 88.8 & 90.8 & 85.8 & 98.3 & 33.7 & 39.0 & 85.5 \\  & ReContrast † & 95.5 & 96.4 & 92.0 & 98.5 & 47.9 & 50.6 & 91.9 \\  & DiAD † & 86.8 & 88.3 & 85.1 & 96.0 & 26.1 & 33.0 & 75.2 \\  & ViTAD † & 90.5 & 91.7 & 86.3 & 98.2 & 36.6 & 41.1 & 85.1 \\  & MambaAD † & 94.3 & 94.5 & 89.4 & 98.5 & 39.4 & 44.0 & 91.0 \\  & **Dinomaly** (Ours) & **98.7** & **98.9** & **96.2** & **98.7** & **53.2** & **55.7** & **94.5** \\   & RD4AD  & 82.4 & 79.0 & 73.9 & 97.3 & 25.0 & 32.7 & 89.6 \\  & SimpleNet  & 57.2 & 53.4 & 61.5 & 75.7 & 2.8 & 6.5 & 39.0 \\  & DeSTSeg  & 82.3 & 79.2 & 73.2 & 94.6 & 37.9 & 41.7 & 40.6 \\  & UniAD † & 83.0 & 80.9 & 74.3 & 97.3 & 21.1 & 29.2 & 86.7 \\  & ReContrast † & 86.4 & 84.2 & 77.4 & 97.8 & 31.6 & 38.2 & 91.8 \\  & DiAD † & 75.6 & 66.4 & 69.9 & 88.0 & 2.9 & 7.1 & 58.1 \\  & ViTAD † & 82.3 & 79.4 & 73.4 & 96.9 & 26.7 & 34.9 & 84.9 \\  & MambaAD † & 86.3 & 84.6 & 77.0 & 98.5 & 33.0 & 38.7 & 90.5 \\  & **Dinomaly** (Ours) & **89.3** & **86.8** & **80.2** & **98.8** & **42.8** & **47.1** & **93.9** \\   

Table 1: Performance under **multi-class** UAD setting (%). †: method designed for MUAD.

    &  &  &  \\   & I-AUROC & P-AP & P-AUPRO & I-AUROC & P-AP & P-AUPRO & I-AUROC & P-AP & P-AUPRO \\  _Dinomaly_ (_MUAD_) & _99.6_ & _69.3_ & _94.8_ & _98.7_ & _53.2_ & _94.5_ & _89.3_ & _42.8_ & _93.9_ \\ 
**Dinomaly** & **99.7** & **68.9** & **95.0** & **98.9** & **50.7** & **95.1** & **92.0** & **45.2** & **95.1** \\ RD4AD  & 98.5 & 58.0 & 93.9 & 96.0 & 27.7 & 70.9 & 87.1 & n/a & 93.8 \\ PatchCore  & 99.1 & 56.1 & 93.5 & 95.1 & 40.1 & 91.2 & 89.4 & n/a & 91.5 \\ SimpleNet  & 99.6 & 54.8 & 90.0 & 96.8 & 36.3 & 88.7 & 88.5 & n/a & 84.6 \\ EfficientAD  & 99.1 & 63.8 & 93.5 & 98.1 & 40.8 & 94.0 & n/a & n/a & n/a \\   

Table 2: Performance under conventional **class-separated** UAD setting (%). n/a: not available.

LC makes the reconstruction too easy without injected noise. Combining some of the proposed elements boosts the performance of the baseline, while employing them all produces the best results.
**Noisy Rates**. We conduct ablations on the discarding rate of the Dropouts in MLP bottleneck, as shown in Table 4. Experimental results demonstrate that Dinomaly is robust to different levels of dropout rate. **Reconstruction Constraint**. We quantitatively examine different reconstruction schemes presented in Figure 4. As shown in Table 5, group-to-group LC outperforms layer-to-layer supervision. On image-level metrics, 1-group LC with all layers added performs similarly to its 2-group counterpart that separates low-level and high-level layers; however, 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. More ablations on scalability, input size, pre-trained foundations, etc., are presented in Appendix A.3.

## 4 Conclusion

Dinomaly, a minimalistic UAD framework, is proposed to address the under-performed MUAD models in this paper. We present four key elements in Dinomaly, i.e., Foundation Transformer, Noisy MLP Bottleneck, Linear Attention, and Loose Reconstruction, that can boost the performance under the challenging MUAD setting without fancy modules and tricks. Extensive experiments on MVTec AD, VisA, and Real-IAD demonstrate our superiority over previous model-unified multi-class models and even recent class-separated models, indicating the feasibility of implementing a unified model in complicated scenarios free of severe performance degradation.

    &  &  \\   & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\ 
0 (noiseless) & 98.19 & 99.55 & 98.51 & 97.55 & 63.11 & 64.39 & 93.33 \\
0.1 & 99.54 & 99.75 & 98.90 & **98.35** & **69.46** & **69.19** & 94.53 \\
0.2 \(\) & 99.60 & 99.78 & 99.04 & **98.35** & 69.29 & 69.17 & **94.79** \\
0.3 & **99.65** & **99.83** & 99.16 & 98.34 & 68.46 & 68.81 & 94.63 \\
0.4 & 99.64 & 99.80 & **99.23** & 98.22 & 67.95 & 68.33 & 94.57 \\
0.5 & 99.56 & 99.81 & 99.14 & 98.15 & 67.43 & 67.82 & 94.64 \\   

Table 4: Ablations of Dropout rates in Noisy Bottleneck, conducted on MVTec-AD (%). \(\): default.

    &  &  &  &  &  & \)-max} &  \\   & & & & & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\   &  & & & 98.41 & 99.09 & 97.41 & 97.18 & 62.96 & 63.82 & 92.95 \\  & & & & 99.06 & 99.54 & 98.31 & 97.62 & 66.22 & 66.70 & 93.71 \\  & & \(\) & & 98.54 & 99.21 & 97.62 & 97.20 & 62.94 & 63.73 & 93.09 \\  & & \(\) & & 98.35 & 99.04 & 97.43 & 97.10 & 61.05 & 62.73 & 92.60 \\  & & \(\) & & 99.03 & 99.45 & 98.19 & 97.62 & 64.10 & 64.96 & 93.34 \\ \(\) & \(\) & & & 99.27 & 99.62 & 98.63 & 97.85 & 67.36 & 67.33 & 94.16 \\ \(\) & \(\) & & & 99.50 & 99.72 & 98.87 & 98.14 & 68.16 & 68.24 & 94.23 \\ \(\) & \(\) & \(\) & & 99.52 & 99.73 & 98.92 & 98.20 & 68.25 & 68.34 & 94.17 \\ \(\) & \(\) & \(\) & & 99.57 & **99.78** & 99.00 & 98.20 & 67.93 & 68.21 & 94.50 \\ \(\) & \(\) & \(\) & \(\) & **99.60** & **99.78** & **99.04** & **98.35** & **69.29** & **69.17** & **94.79** \\   

Table 3: Ablations of Dinomaly elements on MVTec-AD (%). NB: Noisy Bottleneck. LA: Linear Attention. LC: Loose Constraint (2 groups). LL: Loose Loss. Results on VisA see Table A1.

    &  &  \\   & AUROC & AP & \(F_{1}\)-max & AUROC & AP & \(F_{1}\)-max & AUPRO \\  layer-to-layer (dense, every 1) & 99.39 & 99.68 & 98.73 & 98.12 & 68.55 & 68.63 & 94.28 \\ layer-to-layer (sparse, every 2) & 99.52 & 99.73 & 98.95 & 98.16 & 68.89 & 68.57 & 94.40 \\ layer-to-layer (sparse, every 4) & 99.54 & 99.77 & 99.05 & 98.04 & 66.69 & 67.17 & 94.07 \\ layer-to-cat-layer (every 2) & 99.48 & 99.71 & 99.26 & 97.83 & 62.29 & 62.91 & 93.16 \\ group-to-group (1 group) & **99.64** & **99.80** & **99.36** & 98.18 & 64.79 & 65.40 & 93.96 \\ group-to-group (2 groups)\(\) & 99.60 & 99.78 & 99.04 & **98.35** & **69.29** & **69.17** & **94.79** \\   

Table 5: Ablations of reconstruction constraint, conducted on MVTec-AD (%). \(\): default.