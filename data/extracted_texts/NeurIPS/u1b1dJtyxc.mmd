# What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores

What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called "brain score". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward. Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.

## 1 Introduction

Recent developments in large language models (LLMs) have led many to wonder whether LLMs process language like humans do. Whereas LLMs acquire many abstract linguistic generalizations, it remains unclear to what extent their internal machinery bears resemblance to the human brain . A number of studies have attempted to answer this question through the framework of neural encoding [2; 3; 4]. Within this framework, an LLM's internal representations of some linguistic stimuli are used to predict brain activity during comprehension of the same stimuli. Results have been uniformly positive, showing that LLM representations are highly effective at predicting neural signals [5; 6].

In one impactful study, authors evaluated the brain scores of 43 models on three neural datasets . They found that GPT-XL  achieved the highest brain score and, in one neural dataset, accounted for 100% of the "explainable" neural variance (i.e., taking into account the noise inherent in the data). This result was interpreted as evidence that the brain may be optimizing for the same objective as GPT2, namely, next-word prediction. Surprisingly, the authors further found that untrained (i.e. randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer architecture biases computations to be more brain-like. The finding that untrained LLMs predict neural signals significantly above chance has been replicated in other studies [9; 4; 10].

More generally, many studies have compared models to brain activity and concluded that high prediction performance reveals correspondence between some interesting aspect of the model and biological linguistic processing [4; 11; 12; 13; 14]. One issue with this approach is that it assumes that the subset of neural activity predicted by a model reflects core processes of the human language system . However, this assumption is not necessarily true. For example, a recent paper found that, when participants listen to stories, the fMRI signal includes an initial ramping, positional artifact . It is likely that LLMs which contain absolute positional embeddings would be able to predict this ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe, ) would not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical interest. This issue relates to a more general trend in machine learning research: a complex algorithm solves a task, but it is later discovered that the key innovation was a very simple component of the algorithm . Analogous to Weinberger , without attempting to rigorously deconstruct the mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain's mechanisms for processing language.

We analyze the same three neural datasets used in . These include the Pereira fMRI dataset, where participants read short passages ; the Fedorenko electrocorticography (ECoG) dataset, where participants read isolated sentences ; and the Blank fMRI dataset, where participants listened to short stories . As in Schrimpf et al. , we focus our analyses on the Pereira dataset. In order to deconstruct the mapping between LLMs and the brain, we follow Reddy and Wehbe  and de Heer et al.  by building a set of predictors that describe simple features of the linguistic input, and gradually add features that increase in complexity. Our goal is to find the simplest set of features which account for the greatest portion of the mapping between LLMs and brains.

## 2 Methods

### Experimental data

For all three neural datasets, we used the same version as used by . For additional details, refer to A.1.

**Pereira (fMRI):** The Pereira dataset is composed of two experiments. Experiment 1 (EXP1) consists of \(96\) passages each containing \(4\) sentences, with \(n=9\) participants. Experiment 2 (EXP2) consists of \(72\) passages each consisting of \(3\) or \(4\) sentences, with \(n=6\) participants. Passages in each experiment were evenly divided into \(24\) semantic categories which were not related across experiments (\(4\) passages per category in EXP1, and \(3\) passages per category in EXP2). A single fMRI scan (TR) was taken after visual presentation of each sentence. Unless otherwise noted, we focus our results on voxels from within the "language network" in the main paper. EXP1 was a \(384 92450\) matrix (number of sentences \(\) number of voxels) and EXP2 was a \(243 60100\) matrix. All analyses were conducted separately for each experiment.

**Fedorenko (ECoG):** Participants (\(n=5\)) read \(52\) sentences of length \(8\) words. A total of \(97\) language-responsive electrodes were used across \(5\) participants: \(47,8,9,15,\) and \(18\), for participants \(1\) through \(5\), respectively. Neural activity was temporally averaged across the full presentation of each word after extracting high gamma, and the entire dataset was a \(416 97\) matrix.

**Blank (fMRI):** The dataset consisted of \(5\) participants listening to \(8\) stories from the publicly available Natural Stories Corpus . An fMRI scan was taken every \(2\) seconds, resulting in a total of \(1317\) TRs across the \(8\) stories. fMRI BOLD signals were averaged across voxels within each functional region of interest (fROI). There were 60 fROIs across all 5 participants, resulting in a \(1317 60\) matrix.

### Language models

We focus our analyses on GPT2-XL , as it was shown to be the best-performing model on the Pereira dataset [10; 24; 2]. GPT2 is an auto-regressive transformer model, meaning that it can only attend to current and past inputs, trained on next token prediction. The XL variant has \(\)1.5B parameters and 48 layers. We replicate some of our key findings on Pereira with RoBERTa-Large (A.6). RoBERTa is a transformer model with bidirectional attention trained on masked token prediction, meaning that it can attend to past and future tokens. The large variant contains \(335\)M parameters and \(24\) layers. Both GPT2 and RoBERTa use learned absolute positional embeddings, such that a unique vector corresponding to each token position is added to the input static embeddings.

### LLM feature pooling

**Pereira:** Each sentence was fed into an LLM, with previous sentences from the same passage also fed as input. Since each fMRI scan was taken at the end of the sentence, we converted LLM token-level embeddings to sentence-level embeddings by summing across all tokens within a sentence (sum pooling). We used the sum pooling method because it is consistent with other neural encoding studies [26; 27], and it performed better than taking the representation at the last token which was done in  A.5.

**Fedorenko:** The current and previous tokens from within the same sentence were fed into the LLM as context. We converted LLM token-level embeddings to word embeddings, since each word has a neural response, by summing across tokens in multi-token words, and leaving single token words unmodified.

**Blank:** For each story, we fed the current and all preceding tokens up to a maximum context size of 512 tokens. As in Schrimpf et al. , for each TR, we took the representation of the word that was closest to being 4 seconds before the TR. For multi-token words, we took the representation of the last token of that word.

### Banded ridge regression

We used ridge regression (linear regression with an L2 penalty) to predict activations for each voxel/electrode/fROI independently. We did not use "vanilla" ridge regression because it applies a single L2 penalty for all weights, whereas our analyses use multiple sets of distinct features. In such a case, a single penalty causes the regression will be biased against small feature spaces. Moreover, different L2 penalties are likely optimal for each feature space. To remedy this, we employed banded ridge regression which effectively allows a different L2 penalty to be applied to each feature space  (for further details, refer to A.2).

### Out of sample \(R^{2}\) metric

We define the brain score of a model as the out-of-sample \(R^{2}\) metric (\(R^{2}_{}\)) . \(R^{2}_{}\) quantifies how much better a set of features performs at predicting held-out data compared to a model which simply predicts the mean of the training data (i.e. a regression with only an intercept term). To be precise, given mean squared error (MSE) values from a model using features \(M\) and MSE values from an intercept only regression (\(I\)), then:

\[R^{2}_{}=1-}{MSE_{I}}.\] (1)

A positive (negative) value indicates that \(M\) was more (less) helpful than predicting the mean of training data. We elected to use \(R^{2}_{}\) over the standard \(R^{2}\) because of this clear interpretation and because it is a less biased estimate of test set performance . We use \(R^{2}_{}\) over Pearson's correlation coefficient (\(r\)) because \(R^{2}_{}\) can be interpreted as the fraction of variance explained, which lends more straightforwardly to estimating how much variance one feature space explains over others. Whenever averaging across voxels, we set \(R^{2}_{}\) values to be non-negative to prevent differences in performance on noisy voxels/electrodes/fROIs from significantly impacting the results. We refer to \(R^{2}_{}\) as \(R^{2}\) throughout the rest of the paper for brevity, and use the notation \(R^{2}_{M}\) to refer to the performance of features \(M\).

### Selection of best layer

We evaluate the \(R^{2}\) for each LLM layer, and select the layer that performs best across voxels/electrodes/fROIs. Due to the stochastic nature of untrained LLMs, we selected the best layer for \(10\) random seeds and computed the average \(R^{2}\) across seeds. When reporting the best layer, we refer to layer \(0\) as the input static layer, and layer \(1\) as the first intermediate layer.

### Train, validation, and test folds:

For each dataset, we construct contiguous train-test splits by ensuring neural data from the same passage/sentence/story is not included in both train and test data. Due to low sample sizes, we employed a nested cross-validation procedure for each dataset (A.3). When computing \(R^{2}\) across inner or outer folds, we pooled predictions across folds and computed a single \(R^{2}\) as recommended by Hawinkel et al. . The optimal parameters for banded regression were selected based on validation data.

We created shuffled train-test splits, as done in , of the same size as the contiguous train-test splits. Unless explicitly noted, all results are performed using contiguous train-test splits.

### Correcting for decreases in test-set performance due to addition of feature spaces

It is possible for a "full" encoding model to perform worse than a "sub-model" (which consists of only a subset of the predictors) because we are evaluating performance on a held-out test set . To address this problem, in some analyses we select the best performing sub-model for each voxel/electrode/fROI which includes a given feature of interest. For instance, to examine how much feature space \(C\) adds onto features spaces \(A\) and \(B\), we select the best sub-model which includes \(C\) and denote it as \(A+B+C\)*. More precisely, the \(R^{2}\) of \(A+B+C\)* is:

\[R^{2}_{A+B+C}*=(R^{2}_{C},R^{2}_{A+C},R^{2}_{B+C},R^{2}_{A+B+C}).\] (2)

### Orthogonal Auto-correlated Sequences Model (OASM)

To model temporal auto-correlation in neural activity, we construct a feature matrix for each dataset by (i) forming an \(n\)-dimensional identity matrix, where \(n\) is the total number of time points in the dataset (per voxel / electrode / TR), and (ii) applying a Gaussian filter within "chunks" along the diagonal that correspond to temporally contiguous time points (i.e., within each passage in Pereira, each sentence in Fedorenko, and each story in Blank). This generates an auto-correlated sequence for each passage/sentence/story that is orthogonal to that of each other passage/sentence/story (A.7).

## 3 Pereira dataset

### Shuffled train-test splits are severely affected by temporal auto-correlation

Prior LLM encoding studies using this dataset [24; 2; 10; 30; 11] used shuffled train-test splits. Here, we demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs. First, we replicated the pattern of neural predictivity across GPT2-XL's layers reported in  and  when using shuffled splits. Using this procedure, early and late layers perform best and intermediate layers perform worst. Strikingly, when using the alternative approach of contiguous train-test splits, the opposite pattern is observed: intermediate layers perform best. Across layers, neural predictivity using the shuffled method is highly anti-correlated with neural predictivity using the contiguous method (\(r=-.929\) in EXP1, \(r=-.764\) in EXP2) (Fig. 1a).

Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits could be accounted for by OASM, a model which only represents within passage auto-correlation and between passage orthogonality. OASM out-performed GPT2-XL on both EXP1 and EXP2 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption of multiple previous studies [2; 11; 10] that performance on this benchmark is an indication of a model's brain-likeness,.

Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small relative to what OASM explains alone. To calculate this, we combine OASM with GPT2-XL and observe how much neural variance they explain together. To prevent OASM from ever weakening the reported performance of GPT2-XL for any voxel, we correct the \(R^{2}\) value for each voxel with the OASM+GPT2-XL model to be at least as high as with GPT2-XL alone (denoted OASM+GPT2-XL*) (2.8). Even with these corrections, we find that \(R^{2}_{OASM+GPT2-XL}\)* was \(13.6\)% higher than \(R^{2}_{OASM}\) in EXP1, and \(31.5\)% higher than \(R^{2}_{OASM}\) in EXP2 (Fig. 1b) (% differences after averaging \(R^{2}\) across participants). To be clear, this means that any linguistically-driven neural variance that GPT2-XL uniquely explains over OASM is far smaller (\(13.6\%\) on EXP1 and \(31.5\%\) on EXP2) than what is predicted solely by OASM, a model with no linguistic features that completely lacks the ability to generalize to fully held out passages. Thus, it appears that the largest determinant of model predictivity on this dataset when using shuffled train-test splits is whether a model contains autocorrelated sequences within passages that are orthogonal between passages.

### Untrained LLM neural predictivity is fully accounted for by sentence length and position

We next sought to deconstruct what explains the neural predictivity of untrained GPT2-XL (GPT2-XLU) in the Pereira dataset. We hypothesized that \(R^{2}_{GPT2-XLU}\) could be explained by two simple features: sentence length (SL) and sentence position within the passage (SP). Sentence length is captured by GPT2-XLU because the GELU nonlinearity in the first layer's MLP transforms normally distributed inputs with zero mean into outputs with a non-zero mean. This introduces a non-zero mean component to each token's representation in the residual stream. When these representations are sum-pooled, this non-zero mean component accumulates in a way that reflects the sentence length, making the length decodable in the intermediate layers (see A.9 for a formal proof). Sentence position is encoded within GPT2-XLU due to absolute positional embeddings which, although untrained, still result in sentences at the same position having similar representations when tokens are sum-pooled. We represent sentence position as a 4-dimensional one-hot vector, where each element corresponds to a given position within a passage, and sentence length as the number of words in a passage.

To obtain representations from GPT2-XLU, we selected the best-performing layer for each of the \(10\) untrained seeds. For EXP1 the best performing layer was layer \(0\) for \(6\) seeds, layer \(1\) for \(3\) seeds (first intermediate layer), and layer \(19\) for one seed. For EXP2 the best layer was layer \(1\) for 5 seeds, layer \(2\) for 4 seeds, and layer \(5\) for \(1\) seed.

We fit a regression using all subsets of the following feature spaces, SL, SP, GPT2-XLU, resulting in 7 models. For both experiments, \(R^{2}_{SP+SL}\) was descriptively higher than all other models, including the best-performing model with GPT2-XLU (SP+SL+GPT2-XLU) (Fig. 2a). Sentence position was particularly important in EXP1, and sentence length was particularly important in EXP2. This may explain why the static layer often outperformed intermediate layer representations in EXP1 despite encoding sentence length more poorly. Overall, these results suggest that, when averaging across voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding performance over sentence length and position.

Figure 1: Comparing different approaches for creating train-test splits in the Pereira dataset. Within each panel, EXP1 results are on the left and EXP2 results are on the right (same formatting in Figure 2,3) **(a)**\(R^{2}\) values across layers for GPT2-XL on shuffled train-test splits (gray) and contiguous (unshuffled) splits (blue). **(b)** Each dot shows the mean \(R^{2}\) value across voxels within a participant, with bars indicating mean \(R^{2}\) across participants.

Although GPT2-XLU did not enhance encoding performance when averaging across voxels, there may be a subset of voxels where GPT2-XLU does explain significant additional neural variance. To examine this possibility, we plotted a 2D histogram of voxel-wise \(R^{2}_{SP+SL}\) values vs. \(R^{2}_{SP+SL+GPT2-XLU}\) values in the language network (Fig. 2b). Values were clustered around the identity line, and there was no cluster of voxels where \(R^{2}_{SP+SL+GPT2-XLU}\) appeared significantly higher. Next, for each voxel, we performed a one-sided paired \(t\)-test between the squared error values obtained over sentences (EXP1: \(N=384\), EXP2: \(N=243\)) between SP+SL+GPT-XLU and SP+SL. Across all functional networks, only 1.26% (EXP1) and 1.42% (EXP2) of voxels were significantly (\(=0.05\)) better explained by the GPT2-XLU model before false discovery rate (FDR) correction; these numbers dropped to 0.001% (EXP1) and 0.078% (EXP2) after performing FDR correction within each participant and network . None of the significant voxels after FDR correction were inside the language network. Taken together, these results suggest GPT2-XLU does not enhance neural prediction performance over sentence length and position even at the voxel level.

To control for voxels where the neural encoding performance of GPT2-XLU is weakened by the addition of SP+SL, we compared SP+SL* and SP+SL+GPT2-XLU*. When averaging across voxels, \(R^{2}_{SP+SL}\)* still exceeded \(R^{2}_{GPT2-XLU+SP+SL}\)* (Fig. 2c). Furthermore, the values for \(R^{2}_{SP+SL}\)* and \(R^{2}_{GPT2-XLU+SP+SL}\)* across brain areas were highly similar in both experiments (Fig. 2d). Only 1.00% (EXP1) and 1.18% (EXP2) of voxels were significantly better explained by the addition of GPT2-XLU before FDR correction; 0% (EXP1) and 0.05% (EXP2) of voxels were better explained

Figure 2: For all panels, EXP1 results are on the left and EXP2 results are on the right. **(a)** Brain score (\(R^{2}\)) for different combinations of features. Each dot represents \(R^{2}\) values averaged across voxels in a single participant, with bars showing mean across participants. **(b)** 2D histogram of \(R^{2}\) values for the best model without GPT2-XLU (SP+SL), and the best model with GPT2-XLU (GPT2-XLU+SP+SL). The dotted lines show \(y=x\), \(y=0\), and \(x=0\). Values below \(y=0\) or left of \(x=0\) were clipped when averaging, but are shown here to visualize the full distribution. **(c)** Same as **(a)**, but after voxel-wise correction; lines connect data-points from the same participant. **(d)** Glass brain plots showing \(R^{2}\) values of SP+SL (left) and GPT2-XLU+SP+SL (right) after voxel-wise correction. Conventions are the same as Figure 1.

after FDR correction (once again, no significant voxels were inside the language network ). Thus, our results hold even when controlling for decreases in performance due to the addition of feature spaces.

Sentence length, sentence position, and static word embeddings account for the majority of trained LLM encoding performance

We next turned to explaining the neural predictivity of the trained GPT2-XL. In addition to sentence position and sentence length, we added static word embeddings (WORD). Together, these features defined a baseline model which does not account for any form of linguistic processing of words in context. We next included three more complex features which involved contextual processing. First, we added sense-specific word embeddings from RoBERTa-Large using the LMMS package . Sense embeddings contain distinct representations for different senses of the same word (e.g., mouse: _computer device_, and mouse: _rodent_). LMMS generates sense embeddings by averaging over contextual embeddings corresponding to the same sense of a word (see A.10 for further details).

Whereas sense embeddings help disambiguate many content words, they do not disambiguate pronouns, i.e., do not encode the entities that they refer to. Therefore, our sense embeddings were generated for a version of the Pereira text where pronouns were dereferenced (i.e., replaced by the words that they referred to). To maintain consistency with these sense embeddings, our static word embeddings were created (1) by taking a frequency-weighted average of sense embeddings for the same word, where frequency values were obtained from WordNet ; and (2) based on the dereferenced Pereira texts. Importantly, this means the impact of pronoun dereferencing and word and sense embeddings are not decoupled in this study. Finally, we created an abstract representation of the syntax of each sentence (SYNT), using an approach highly similar to that of Caucheteux et al. : we collected sentences that are syntactically equivalent but semantically dissimilar to the original sentence, and averaged their representations from the best layer of GPT2-XL (A.11). We selected the best layer based on averaged \(R^{2}\) across language voxels on test data (EXP1: layer 21, EXP2: layer 16).

We fit a regression to the fMRI data using all subsets of the feature spaces SL+SP, WORD, SENSE, SYNT, GPT2-XL, resulting in 64 models. In this list, features are ranked from least to most complex. For each feature, we took the model that exhibited the best performance in the language network which included that feature but did not include features more complex than it. For instance, values reported for \(R^{2}_{SL+SP+WORD+SENSE}\) were taken from the best model which included SENSE, excluding models which included SYNT and GPT2-XL. By doing so, we were able to examine the impact of adding more complex features in explaining R2\({}_{GPT2-XL}\) while still accounting for decreases in test performance due to adding redundant features. We note that since this procedure is not performed at the voxel-level, we do not add a * to the \(R^{2}\) notation.

Table 1 displays the performance of each model, including GPT2-XL on its own (Fig. 2a, 2b). The baseline SP+SL+WORD model, which does not account for any form of contextual processing, performs 75% as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2. When adding contextual features, namely SENSE and SYNT, our model performs 84.4% as well as GPT2-XL and the full model in EXP1, and better than GPT2-XL and 95.5% as well as the full model in EXP2, indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond simple features in this dataset.

Similar to previous sections, we perform voxel-wise correction by selecting the best sub-model with GPT2-XL and the best sub-model without GPT2-XL for each voxel. We focus only on sentence

 
**Features** & **EXP1** & **EXP2** \\  GPT2-XL & \(0.032\) & \(0.036\) \\ SP+SL & \(0.013\) & \(0.031\) \\ SP+SL+WORD & \(0.024\) & \(0.039\) \\ SP+SL+WORD+SENSE & \(0.026\) & \(0.040\) \\ SP+SL+WORD+SENSE+SYNT & \(0.027\) & \(0.043\) \\ SP+SL+WORD+SENSE+SYNT+GPT2-XL & \(0.032\) & \(0.045\) \\ 

Table 1: Mean \(R^{2}\) values (across participants) for each model. For models composed of multiple features, the best sub-model is used which includes the last feature.

position, sentence length, and static word embeddings because sense and syntax had modest contributions beyond these features. \(R_{SP+SL+WORD}^{2}\)* was \(0.028\) in EXP1 and \(0.048\) in EXP2, and \(R_{SP+SL+WORD+GPT2-XL}^{2}\)* was \(0.036\) in EXP1 and \(0.056\) in EXP2 (mean across participants) (Fig. 3c). This indicates that even after controlling for a reduction in GPT2-XL performance from the addition of simple features, GPT2-XL only explains an additional 28.57% (EXP1) and 16.7% (EXP2) neural variance over a model composed of features that are all non-contextual.

## 4 Fedorenko dataset

### Shuffled train-test splits also impact ECoG datasets, but less than with fMRI

We first evaluated the impact of shuffled train-test splits on the Fedorenko dataset. Unlike in Pereira, the across-layer performance is well correlated between shuffled and contiguous splits (\(r=0.622\)) (Fig. 4a). The OASM model performs \(93.1\%\) as well as GPT2-XL when averaging \(R^{2}\) values across participants (Fig. 4b). \(R_{OASM+GPT2-XL}^{2}\)* was 45.3% better than OASM, meaning that the unique contribution of GPT2-XL is less than half the total contribution of a simple, auto-correlated model. Therefore, shuffled train-test splits also impact results on Fedorenko, albeit less than Pereira. This may be due to lower autocorrelation of ECoG compared to fMRI. We use contiguous splits for the remainder of the Fedorenko analyses.

### Word position explains all of untrained, and most of trained, GPT2-XL brain score

As noted in , there was a strong positional signal in the ECoG dataset during comprehension of sentences that is likely related to the construction of sentence meaning. We therefore hypothesized

Figure 3: For all panels, EXP1 results are on the left and EXP2 results are on the right. **(a)** For each model, we display the sub-model which includes the added feature. Dots represent participants and bars are mean across participants. Grey dashed line is the performance of GPT2-XL alone. **(b)** 2d histogram comparing full model and full model with GPT2-XL. **(c)** Same as **(a)** but after voxel-wise correction for SP+SL+WORD and SP+SL+WORD+GPT2-XL. **(d)** Glass brain plots showing \(R^{2}\) values of SP+SL+WORD (left) and SP+SL+WORD+GPT2-XLU (right) after voxel-wise correction.

that a feature space that accounted for word position (WP) would do well relative to untrained and trained GPT2-XL. We generated a simple feature space that encodes word position, such that words in nearby positions were given similar representations (A.12). When performing a one-sided paired \(t\)-test between the squared error predictions of WP+GPT2-XLU* and WP, three electrodes were significantly better explained by the addition of GPT2-XLU before FDR correction, and none were better explained after FDR correction within each participant. Moreover, WP performs 86.7% as well as GPT2-XL, and \(82.1\)% as well as WP+GPT2-XL*. Our results therefore suggest that the mapping between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals.

## 5 Blank dataset is predicted at near chance levels

Lastly, we address the Blank dataset. We find that OASM achieves an \(R^{2}\) that is \(103.6\) times larger than that of GPT2-XL when using shuffled splits A.13, demonstrating that such splits are massively contaminated by temporal autocorrelation. We next turn to using contiguous splits, and test whether GPT2-XL performs better than an intercept only model by applying a one-sided paired \(t\)-test between the squared error values obtained from GPT2-XL and the intercept only model (\(N=1317\) TRs). GPT2-XL predicts \(1\) ROI significantly better than an intercept only model, and \(0\) fROIs are significantly better after FDR correction. Our results therefore suggest that GPT2-XL performs at near chance levels on the version of the Blank dataset used by [2; 10; 11].

## 6 Limitations and Conclusions

Our study has three main limitations. First, our method of examining how much neural variance an LLM predicts over simple features scales poorly when the number of features is large. Second, although we attempted to correct for cases where adding features decreases test set performance and employed banded regression, fitting regressions with large feature spaces on noisy neural data with low sample sizes can lead to poor estimations of the neural variance explained. Finally, we did not analyze datasets with large amounts of neural data per participant, for instance , in which the gap between the neural predictivity of simple and complex features might be much larger.

In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and trained GPT2-XL brain score is largely explained by non-contextual features. We find that the majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and on the Blank dataset GPT2-XL predicts neural activity at near chance levels. These results suggest that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally, analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding performance, and an evaluation against simple and theoretically uninteresting features. Only after such deconstruction can we be somewhat confident that the neural predictivity of LLMs reflects core aspects of human linguistic processing.

Figure 4: **(a)** Across-layer \(R^{2}\), averaged across electrodes in the Fedorenko dataset, for GPT2-XL with and without shuffled splits. **(b)** Each dot is a participant, lines connect data-points from the same participant. Bars display mean across participants. **(c)** and **(d)** Same guidelines as **(b)**.