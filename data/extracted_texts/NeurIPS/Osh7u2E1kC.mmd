# Leveraging Separated World Model for Exploration in Visually Distracted Environments

Kaichen Huang\({}^{1,2}\), Shenghua Wan\({}^{1,2}\), Minghao Shao\({}^{1,2}\),

**Hai-Hang Sun\({}^{1,2}\)**, **Le Gan\({}^{1,2}\)**, **Shuai Feng\({}^{3}\)**, **De-Chuan Zhan\({}^{1,2}\)**

\({}^{1}\)School of Artificial Intelligence, Nanjing University, China

\({}^{2}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{3}\)School of Cyberspace Science and Technology, Beijing Institute of Technology, China

{huangxc,wansh,shaomh,sunhh}@lamda.nju.edu.cn,

{ganl,zhandc}@nju.edu.cn, fengshuai@bit.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Model-based unsupervised reinforcement learning (URL) has gained prominence for reducing environment interactions and learning general skills using intrinsic rewards. However, distractors in observations can severely affect intrinsic reward estimation, leading to a biased exploration process, especially in environments with visual inputs like images or videos. To address this challenge, we propose a bi-level optimization framework named **S**eparation-assisted **e**X**plorer (SeeX). In the inner optimization, SeeX trains a separated world model to extract exogenous and endogenous information, minimizing uncertainty to ensure task relevance. In the outer optimization, it learns a policy on imaginary trajectories generated within the endogenous state space to maximize task-relevant uncertainty. Evaluations on multiple locomotion and manipulation tasks demonstrate SeeX's effectiveness.

## 1 Introduction

Unsupervised learning has a rich history in computer vision and natural language processing, as demonstrated by methods such as . It leverages unlabeled, task-agnostic data to train models that can be quickly adapted to downstream tasks, addressing sample inefficiency. This approach has also gained traction in unsupervised reinforcement learning (URL) , where skills are developed through intrinsic motivation rather than external rewards. A key advantage of URL is that learned dynamics models can gather prior knowledge about the environment during exploration, minimizing the need for extensive interactions during adaptation to downstream tasks.

A major challenge in unsupervised reinforcement learning arises from distractors, which significantly hinder learning in complex real-world environments. For example, a book-finding robot in a library must identify relevant objects (like books) while ignoring irrelevant ones (such as posters or people) to effectively complete its task. These distractors exponentially expand the original state space, creating numerous redundant states that impede efficient exploration . As shown in Figure 1-Left, a single world model encodes both task-relevant and task-irrelevant information in the latent space. Even

Figure 1: Comparison of traditional URL methods using a single world model (left) versus our separated world model (right).

if the agent's state remains unchanged in \(_{i+1,*}\), varying background distractors inflate uncertainty estimates. In contrast, as illustrated in Figure 1-Right, our proposed separation world model distinctly separates task-relevant information \(s^{+}\) from task-irrelevant information \(s^{-}\), allowing for accurate uncertainty estimation unaffected by distractors.

Despite its prevalence in real-world environments, this challenge has received limited attention in unsupervised reinforcement learning. Current unsupervised RL methods: spanning data-driven, knowledge-based, and competence-based approaches (see Section 6), remain vulnerable to redundant states, which can lead agents down unproductive exploration paths .

To address visual inputs with complex distractors, we propose a bi-level optimization framework called Separation-Assisted Explorer (SeeX). We extend URLB-pixels to environments with distractors, demonstrating that performance declines vary with task difficulty. Our method, SeeX, separates latent information in the world model into task-relevant and task-irrelevant components. We assume the task-irrelevant part captures transitions, using it to predict rewards and actions.

Our contributions are summarized as follows: **(I)** We introduce Separation-Assisted Explorer (SeeX), a new approach for tackling unsupervised RL tasks with complex distractors in observations. **(II)** We provide a theoretical analysis that formalizes URL with distractors, where the policy maximizes task-relevant uncertainty while the world model minimizes environmental uncertainty. **(III)** We demonstrate the outstanding performance of policies learned by SeeX across various locomotion and manipulation tasks with diverse visual distractors.

## 2 Preliminary

World models are highly effective for reinforcement learning (RL) tasks with visual inputs, performing well in both simulated [20; 21; 56] and robotics [67; 66] environments. In unsupervised settings, agents explore using intrinsic rewards during pre-training. This task-agnostic data facilitates fine-tuning through a world model. URLB-pixels  demonstrates that unsupervised methods combined with world models outperform model-free agents. Inspired by these results, we adopted a model-based framework in our approach.

Several studies, including URLB  and URLB-pixels , have created a unified framework for various unsupervised reinforcement learning (URL) methods, advancing the field. The benchmark consists of two phases: pre-training (PT), where agents explore without task-specific rewards, and fine-tuning (FT), where they apply knowledge gained during PT with limited interactions. While URLB-pixels effectively evaluates pixel-based performance, it does not address complex distractors in visual inputs. To fill this gap, we tested baseline methods on noisy video backgrounds and developed a novel approach to tackle this challenge.

Our focus lies in scenarios where agents must acquire skills from noisy visual observations, often disrupted by task-irrelevant videos. To model the dynamics under these conditions, we adopt the EX-BMDP framework, an adaptation of Block MDP. A Block MDP can be represented as a tuple \(=(,,,,, )\), where \(\), \(\) and \(\) denotes the set of observations, latent states and actions respectively; the transition function \(:()\) maps latent states and actions to probability distributions over next latent states; the reward function \(:\) assigns reward to observation-action pairs; the emission function \(:()\) maps latent states to probability distributions over observations. The EX-BMDP is formulated as follows:

**Definition 2.1**.: (Exogenous Block Markov Decision Process). An EX-BMDP is a BMDP such that the latent state can be decoupled into two parts \(z=(s^{+},s^{-})\) where \(s^{+}^{+}\) is endogenous state and \(s^{-}^{-}\) is the exogenous state. For \(z\) the initial distribution and transition functions are decoupled, that is: \((z)=_{+}(s^{+})_{-}(s^{-})\), and \((z^{}|z,a)=_{+}(s^{+^{}}|s^{+},a)_ {-}(s^{-^{}}|s^{-})\).

## 3 Bi-level Optimization for Exploration in Distracted Environments

The objective of the pre-training phase in URL is to learn a world model capable of handling downstream tasks with diverse reward functions. Following previous work [13; 8; 52], we formalize the pre-training phase as a minimax regret problem.

**Problem 3.1**.: _(Regret Optimization for World Model) In the context of reward-free BMDP, consider a world model \(W\) that defines the latent dynamics \(}^{R}\), which simulates the real dynamics under a reward function \(R\). Let \(V_{^{R}}()\) denotes the expected value of policy \(\) under dynamics \(\) with reward function \(R\). The regret of policy \(\) is defined as \((,^{R})=V_{^{R}}(^{*})-V_{^{R }}(),^{*}=_{^{}}V_{^{R}}(^{})\), which measures the performance gap between \(\) and the optimal policy. The optimization goal is to find the world model that is robust to different possible reward functions, minimizing the regret of world model policy under real dynamics:_

\[_{W}_{R}(_{R}^{*},^{R}), {}_{R}^{*}=_{}V_{^{R}}}()\] (1)

In the absence of the reward function during pre-training, computing regret under specific reward functions becomes infeasible. To address this challenge, we propose transforming regret into a novel, reward-free objective using Simulation Lemma . Our proposition is outlined below.

**Proposition 3.2**.: _Denote the learned latent dynamics in the world model as \(}\) and the true latent dynamics as \(\). For any reward function \(R\), the regret of the optimal world model is bounded by:_

\[(_{R}^{*},^{R})& }_{z,a d( _{R}^{*},})}}(|z,a),(|z,a)\\ &+_{z,a d(_{R}^{*},})}}(|z,a),( |z,a)\] (2)

As outlined in Proposition 3.2, the optimal world model policy exhibits low regret if the latent state-action distribution of both \(_{R}^{*}\) and \(_{R}^{*}\) in \(}\) is accurately captured by \(}\). However, during unsupervised pre-training, the reward function remains inaccessible, preventing us from directly obtaining these distributions. This is because the state-action distribution induced by \(_{R}^{*}\), and \(_{R}^{*}\) is inherently dependent on the reward function. To address this challenge, we introduce an exploration policy \(^{}\), specifically designed to maximize the expected error (in terms of total variation distance) of the latent dynamics model:

\[^{}=_{}_{z,a d(,})}}(|z,a),( |z,a)\] (3)

This enables us to derive an upper bound on the regret that is independent of the reward function:

\[(_{R}^{*},^{R})}_{z,a d(^{},})} }(|z,a),(| z,a)R.\] (4)

In the context of EX-BMDPs, the transition dynamics can be decomposed into two components: \((z^{}|z,a)=_{+}(s^{+}|s^{+},a)_{-}(s^{- }|s^{-})\), where \(z=(s^{+},s^{-})\), \(s^{+}\) represents the endogenous state, and \(s^{-}\) represents the exogenous state. To apply EX-BMDP to visually distracted environments, we further assume that the reward depends solely on \(s^{+}\), implying that decisions are made based only on endogenous state.

To model this separation, we construct the world model as two distinct components: the endogenous part \(}_{+}\) and the exogenous one \(}_{-}\), corresponding to the task-relevant and task-irrelevant processes, respectively. Their associated latent dynamics are denoted by \(}_{+}\) and \(}_{-}\). Utilizing this decomposition, we can derive a lower bound for the policy optimization target:

\[&_{z,a d(,})} }(|z,a),(| z,a)\\ =&_{s^{+},s^{-},a d(,})}}_{+}(|s^{+},a )}_{-}(|s^{-}),_{+}(|s^{+},a) _{-}(|s^{-})\\ &_{s^{+},a d(,}_{+})}}_{+}(|s^{+},a), _{+}(|s^{+},a)\] (5)

The strategy's optimization objective is a form of model uncertainty. Estimating model uncertainty for the URL serves as a reward for the exploration policy, which is crucial for learning. However, under the EX-BMDP assumption, directly maximizing the reward of the overall model error introduces significant bias. This is because the objective models not only the endogenous part but also the exogenous part, while the strategy is solely based on \(s^{+}\), leading to inefficient exploration. For the model optimization target, using triangle inequality, we have:

\[_{z,a d(,}_{+})} {TV}}(|z,a),(|z,a)\] (6) \[ _{s^{+},a d(,}_{+})} }_{+}(|s^{+},a),_{+}( |s^{+},a)\] \[+_{s^{-} d(}_{-})} }_{-}(|s^{-}),_{-}(| s^{-})\]

With these bounds, we can reformulate the problem as a bi-level optimization problem:

**Problem 3.3**.: _(Bi-level Optimization of World Model Error) Consider a reward-free EX-BMDP with latent dynamics functions \(}_{+}\), \(}_{-}\) and an exploration policy \(\), assuming the reward depends only on the endogenous state. Let \(=_{+}_{-}\) denote the true latent dynamics. We use a bi-level optimization: the inner optimization finds the world model minimizing the error, while the outer optimization maximizes the endogenous error with respect to the exploration policy._

\[_{}_{+}}_{s^{ +},a d(,}_{+})}}_{+}(|s^{+},a),_{+}(|s^{+},a)\] (7) \[+_{}_{-}}_{s^{-} d( }_{-})}}_{- }(|s^{-}),_{-}(|s^{-})\] \[_{}_{s^{+},a d(, }_{+})}}_{+ }(|s^{+},a),_{+}(|s^{+},a)\]

Problem 3.3 optimizes the world model error as a surrogate for Problem 3.1. It seeks a world model with low prediction error for both endogenous and exogenous components, guided by an exploration policy that maximizes endogenous error. This ensures the optimal policy generalizes well across varying exogenous distributions, regardless of the future reward function. Next, we introduce a practical framework to solve Problem 3.3.

## 4 Practical Implementation of SeeX

### Control with Separation-assisted Latent Dynamics

Separated World Model.For high-dimensional inputs like images and videos, model-based frameworks encode past experiences into latent representations to predict future sequences [59; 65; 19]. Building on this, we propose a separated world model to disentangle task-relevant (endogenous) and task-irrelevant (exogenous) information. As shown in Figure 2, the endo-encoder \(h_{t}^{+}=E_{^{+}}(o_{t})\) and exo-encoder \(h_{t}^{-}=E_{^{-}}(o_{t})\) extract these components. Two transition models handle dynamics: \(q_{^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1})\) for task-relevant states and \(q_{^{-}}(s_{t}^{-}|s_{t-1}^{-})\) for task-irrelevant ones. Additionally, inference models \(p_{^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1},h_{t}^{+})\) and \(p_{^{-}}(s_{t}^{-}|s_{t-1}^{-},h_{t}^{-})\) provide posterior estimates.

Observation Model.Some model-based approaches [20; 49] use an auxiliary reconstruction loss to refine the observation encoder, ensuring \(z_{t}\) captures sufficient information from \(o_{t}\) by maximizing the mutual information \((o_{t};z_{t})\). The IM algorithm  provides the Barber-Agakov lower bound: \((o_{t};z_{t})_{p(o_{t},z_{t})}[ q_{}(o_{t}|z _{t})]+(p(o_{t}))\), where \((p(o_{t}))\) is the entropy of the observation distribution. In our separation setting, this bound becomes \((o_{t};s_{t}^{+},s_{t}^{-})_{p(o_{t},s_{t}^{+},s_{t}^ {-})}[ q_{}(o_{t}|s_{t}^{+},s_{t}^{-})]\). Inspired by [63; 18; 24], We design an observation model \(q_{}(o_{t}|s_{t}^{+},s_{t}^{-})\), an endo-decoder \((_{t}^{+},m_{t}^{-}) D_{^{+}}(s_{t}^{+})\), an exo-decoder \((_{t}^{-},m_{t}^{-}) D_{^{-}}(s_{t}^{-})\) and a mask-mixing model \(m_{t}=M_{}(m_{t}^{+},m_{t}^{-})\). \(_{t}^{+}\) and \(_{t}^{-}\) are expected to reconstruct the task-relevant and task-irrelevant parts of the original observation, while \(m_{t}^{+}\) and \(m_{t}^{-}\) are corresponding masks. So \(q_{}(o_{t}|s_{t}^{+},s_{t}^{-})\) can be implemented as \(_{t}=m_{t}_{t}^{+}+(1-m_{t})_{t}^{-}\). Considering insights from [18; 64] that task-relevant information occupies only a small portion of the observation, we design an exogenous reconstruction (Exo-Rec) model \(_{t}^{} q_{}(o_{t}|s_{t}^{-})\) to ensure that \(s^{-}\) contains the vast majority of information.

World Model Optimization.Similar to a variational autoencoder (VAE) [30; 51], all model components are trained jointly by maximizing the evidence lower bound (ELBO). Our optimization objective extends that of Dreamer : \(_{p}(_{t}(_{O}^{t}+_{R}^{t}+ _{KL}^{t}))\). The detailed formulations of each term are presented as follows, and the derivations are shown in Appendix B.1.

\[_{O}^{t}  q_{}(o_{t}|s_{t}^{+},s_{t}^{-})+ q_{ }(o_{t}|s_{t}^{-})\] (8) \[_{R}^{t}  q_{}(r_{t}|s_{t}^{+})\] (9) \[_{KL}^{t} (p_{^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1},h_{ t}^{+})||q_{^{+}}(s_{t}^{+}|s_{t-1}^{+},a_{t-1}))+(p_{ ^{-}}(s_{t}^{-}|s_{t-1}^{-},h_{t}^{-})||q_{^{-}}(s_{t}^{-}|s_{t-1} ^{-},a_{t-1}))\]

Policy Optimization.To enhance sample efficiency, as shown in Figure 2-(b), we apply the policy optimization strategy of  to learn a parametric policy with imaginary trajectories. More specifically, we design an actor \((a_{t}|s_{t}^{+})\) and a reward model \(q_{}(r_{t}|s_{t}^{+})\). The reward model fits the true reward function in the FT phase. The actor selects action based on the current endogenous state \(s_{t}^{+}\) under the guidance of intrinsic reward in the PT phase or the reward model in the FT phase.

### Intrinsic Reward: Compute Endogenous Uncertainty under Separation View

Crafting an effective intrinsic reward function is paramount during the pretraining (PT) phase. A well-designed intrinsic reward can steer the agent towards exploring states with the highest uncertainty, maximizing skill acquisition. A common practice [46; 47; 54] is to maximize the mutual information \((h_{1:T}^{+};|s_{0}^{+},_{})\), where \(\) represents the true unknown dynamics. This objective is motivated by the fact that mutual information quantifies how comprehensively the trajectory explores the environment.

**Proposition 4.1**.: _(Single step's mutual information) If only we find the policy \(_{}\) that maximize every single step's mutual information \(_{t=0}^{T-1}(h_{t+1}^{+};|s_{t}^{+},a_{t})\), then the whole trajectories's mutual information \((h_{1:T}^{+};|s_{0}^{+},_{})\) will be also maximized. Proof in Appendix B.2._

With Proposition 4.1, the optimization objective can be written as:

\[_{}^{*} _{_{}}_{t=0}^{T-1}(h_ {t+1}^{+};|s_{t}^{+},a_{t})\] (10) \[=_{_{}}_{t=0}^{T-1}(h_{t+1}^ {+}|s_{t}^{+},a_{t})-_{t=0}^{T-1}(h_{t+1}^{+}|=,s_{t}^{+ },a_{t})\] (11)

To approximate the unknown \(\), we design a set of predictive heads \(\{q_{k}(_{t+1,k}^{+}|_{k},s_{t}^{+},a_{t})\}_{k=1:K}\) which are implemented as conditional Gaussians \(((=_{k},s_{t}^{+},a_{t}),^{2})\). Given fixed variance, the conditional entropy does not depend on state or action . Suppose that \(p()\) is an uniform distribution on \(\{_{k}\}_{k=1:K}\), then we have \((h_{t+1}^{+}|s_{t}^{+},a_{t})=_{k=1}^{K}( h_{t+1}^{+}|=_{k},s_{t}^{+},a_{t})\). Since the

Figure 2: **(a)** Our separated world model comprises task-relevant and task-irrelevant branches, with ensemble predictive heads providing reward signals. **(b)** Imaginary trajectories within the endogenous branch enhance sample efficiency by reducing real-world interactions.

marginal entropy in Equation (11) lacks a closed-form expression amenable to optimization, we employ the empirical variance across ensemble means as a substitution. It is also an approximation to the world model TV error in Problem 3.3. We define the intrinsic reward as \(r_{t}^{}=(\{_{t+,k}^{+}\}_{k=1:K})\), and employ the exploratory policy \(_{}\) that maximizes \(r_{t}^{}\) as an approximation of \(_{}^{*}\) in Equation (10).

## 5 Experiments

All experiments are conducted with at least three seeds and evaluated for 10 episodes. We conduct experiments to answer the following questions: (1) Can SeeX outperform other counterparts in URLB? (2) How do pre-training steps affect final performance? (3) Can SeeX give a reasonable model uncertainty estimation? (4) How do different components affect the model training? (5) Can the pre-trained world model and policy generalize to OOD distractors? (6) Can data augmentation handle visual distractors?

Environments.Following the URLB evaluation, we select three domains: _Walker_, _Quadruped_, and _Jaco Arm_, spanning twelve downstream tasks: Walker (stand, walk, run, flip), Quadruped (stand, walk, run, jump), and Jaco (reach-top-right, reach-top-left, reach-bottom-right, reach-bottom-left). These tasks vary in difficulty, providing a well-rounded performance assessment. The Jaco Arm domain is particularly challenging due to its multi-joint structure and sparse rewards, only granted upon catching the red ball. Visualizations of the environments are provided in Appendix A. Agents receive only visual inputs (64, 64, 3), with episodes lasting 1000 frames and an action repeat of \(R=2\). Pre-training utilizes up to 2M frames, and fine-tuning employs 100K frames, consistent with URLB-pixels.

Datasets.Leveraging the large-scale and high-quality Kinetics dataset , we construct two sub-datasets for our experiments: the **Driving-car** dataset, commonly used in previous works [18; 63], and the **Random-video** dataset, composed of videos randomly selected from other Kinetics classes. To evaluate generalization capabilities, we conducted experiments on the Random-video dataset Section 5.5 (_Note that we only use the Random-video here_). This zero-shot transfer assessment gauges the effectiveness of the policy on unseen random video scenarios.

Baselines.We choose seven different unsupervised methods as baselines, which can be categorized into three types: _Knowledge-based_: ICM , RND , LBS  and Plan2Explore  maximize prediction error to better understand the world; _Data-based_: APT  encourages exploration by maximizing entropy; _Competence-based_: DAYN  and APS  maximize mutual information to achieve diverse discovery and generalization. The implementation detail and more concrete introduction of the above methods are shown in Appendix D.2.

Figure 3: We show fine-tuning (FT) performance curves of SeeX and baselines across two domains and eight tasks. Pre-training (PT) used 2M frames, FT 100K. Normalized returns are benchmarked against the expert baseline , with mean (solid line) and variance (shaded area).

### Evaluation on URLB with Distractors

The performance curves in Figure 3 show the normalized fine-tuning performance of SeeX and baseline methods pre-trained on 2 million frames. SeeX consistently outperforms or matches other methods across tasks, achieving expert-level performance in the walker-walk task. Notably, SeeX displays higher mean performance and greater variance, reflecting the positive correlation typical in reinforcement learning. In the quadruped domain, even SeeX's lowest performance exceeds the highest curves of other methods, a trend seen across quadruped tasks. Plan2Explore shows relatively weaker results, especially in the walker-stand and walker-walk tasks, likely due to relying solely on predictive heads based on the entire latent space. In contrast, our method demonstrates significant performance improvements. Other baselines like RND and APT perform reasonably well in simpler tasks but struggle with more complex tasks like walker-run and walker-flip. This indicates that using prediction errors, entropy maximization, or mutual information alone is inadequate. In contrast, our separated world model effectively extracts relevant information to enhance policy training.

### Can SeeX Give a Reasonable Model Uncertainty Estimation?

As stated in Section 4.2, model uncertainty is an approximation to the world model TV error in Problem 3.3. Thus we evaluate the accuracy of dynamics fitting for SeeX and Plan2Explore (both incorporating model uncertainty) with pre-trained models of 1M frames. To ensure a comprehensive assessment, we employ two policies: a random policy and SeeX's exploration policy (1M frames). As illustrated in Table 1, consistently across policies and domains, \(U(s^{+})\) values are lower than \(U(z)\) values. This indicates that SeeX achieves a more accurate estimation of true dynamics compared to Plan2Explore, potentially contributing to its superior performance relative to other baseline methods. Furthermore, a notable observation is that \(U(s^{-})\) values significantly exceed \(U(s^{+})\) values. This suggests that the total environmental uncertainty remains unchanged; rather, most uncertainty is concentrated in \(q_{^{-}}\) and \(p_{^{-}}\) due to SeeX's effective capture of task-irrelevant distractor transitions.

### How do pre-training steps affect final performance?

Building upon the promising results from the previous section, where SeeX outperformed seven baseline methods and achieved expert-level performance from  (trained in a distractor-free environment, representing the upper bound for our setting) in some tasks, we delve into an intriguing

  
**Policy** & **Method** & **Walker** & **Quadruped** & **Jaco** \\   & \(U(s^{+})\) & **0.04\(\)0.00** & **0.17\(\) 0.00** & **0.13\(\)0.00** \\  & \(U(s^{-})\) & 0.64\(\)0.03 & 33.0\(\)31.0 & 0.48\(\)0.03 \\  & \(U(z)\) & 0.31\(\)0.01 & 1.49\(\)0.09 & 0.40\(\)0.01 \\   & \(U(s^{+})\) & **0.05\(\)0.00** & **0.18\(\)0.00** & **0.14\(\)0.00** \\  & \(U(s^{-})\) & 0.65\(\)0.03 & 45.0\(\)50.0 & 0.52\(\)0.03 \\   & \(U(z)\) & 0.35\(\)0.01 & 1.78\(\)0.08 & 0.44\(\)0.02 \\   

Table 1: Uncertainty estimation of SeeX and Plan2Explore on the Driving-car dataset. Using two policies, we collected 1000 distinct states and reported the mean and standard deviation of uncertainty. \(U(s^{+})\), \(U(s^{-})\), and \(U(z)\) denote the uncertainty for SeeX (endogenous and exogenous states) and Plan2Explore (latent belief state), respectively.

Figure 4: **(a) Average performance of four Jaco arm tasks. Each row shows comparative probabilities with 95% confidence intervals, indicating Algorithm X outperforms Algorithm Y . Probabilities are based on 150 runs (50 per seed across 3 seeds) per task for robust evaluation. (b) Performance varies with pretraining steps. Given the small gap at 500k on Walker-Stand, a table highlights details, with red marking over 90% expert performance.**question: how does the fine-tuning performance of SeeX vary with different pre-training step sizes? More specifically, can we achieve fine-tuned performance with as few pre-training steps as possible to achieve more than \(90\%\) of expert performance? We conduct experiments on walker-stand and walker-walk. For all methods, we assessed the fine-tuning performance using pre-training frames of (100k, 500k, 1m, 2m). To evaluate SeeX on a more granular level, we add choices of (20k, 50k, 200k, 300k, 400k). As shown in Figure 4-(b), for the Walker-Walk task, SeeX reaches 90% expert performance (red dotted line) within 500k frames. In contrast, no baseline achieves this level, even with 2M pre-training frames. For the simpler Walker-Stand task, a table offers detailed analysis. SeeX requires only 100k frames of random exploration to reach 90% expert performance, while other baselines need significantly more pretraining.

### How do different components affect the model training?

Ablation of modules.We first examine the impact of various modules in SeeX, including the separate world model, Exo-Rec design, and the number of predictive heads (K). As shown in Figure 5-(a), removing either the separate world model or Exo-Rec significantly reduces performance, underscoring the effectiveness of these designs. A small value of \(K\) hinders accuracy, while performance improves with more heads, confirming our intuition. To balance estimation accuracy and computational cost, we choose \(K=5\).

Ablation of policy.In SeeX, we use only the endogenous state \(s^{+}\) to predict actions. To validate this design, we compare two policies: \((s^{+})\) (SeeX) and \((s^{+},s^{-})\) (SeeX-both-branch). As shown in Figure 5-(b), SeeX consistently outperforms SeeX-both-branch, particularly in the last three challenging tasks. This demonstrates that relying solely on endogenous information is beneficial in environments with moving distractors.

Ablation of \(\).We conducted experiments to assess the impact of Exo-Rec's weight \(\). As shown in Figure 5-(c), the optimal value of \(\) varies across domains. For Jaco and quadruped tasks, a smaller \(\) (e.g., 1) enhances performance, whereas walker tasks benefit from a larger weight (e.g., 2 or 3) to effectively extract exogenous information into \(s^{-}\).

### Can the Pre-trained World Model and Policy Generalize to OOD Distractors?

In this subsection, we evaluate the effectiveness of SeeX in addressing the "generalize to distractors from other distributions (OOD distractors)" challenge. To assess generalization ability, we compare SeeX with other baselines on four different walker tasks and report the average normalized return. We pre-trained and fine-tuned agents on the driving-car dataset and evaluated their performance on both driving-car and random-video datasets. The reported results represent the average normalized return across four walker tasks, with 50 runs conducted for each seed. As shown in Figure 6-(b), SeeX outperforms baselines on test distractor datasets and shows minimal performance drop under distribution shifts, highlighting its strong generalization.

Figure 5: Ablation results of SeeX with 500k fixed pretraining frames: **(a)** separation design, Exo-Rec term, and predictive head values; **(b)** different policy design: \((s^{+})\) (SeeX) and \((s^{+},s^{-})\) (SeeX both-branch); **(c)** impact of different \(\) (Exo-Rec weight) across three domains. Due to Jaco’s task complexity, we ran 50 trials per seed and reported the top 30 mean returns.

### Can data augmentation handle visual distractors?

Numerous studies [3; 73; 70; 42] explore data augmentation (DA) in RL as an effective strategy for visual generalization. This subsection highlights the differences between DA and our approach. Our bi-level separation framework extracts task-relevant information, similar to how DA captures task-relevant representations. However, since few approaches use DA for exploration, we integrated it into Plan2Explore and SeeX to assess its impact on performance in the moving distractor setting. Specifically, we applied the classic random shift augmentation from  (4-pixel padding) four times per image. To evaluate DA's effectiveness, we tested it during both pretraining (pDA) and finetuning (fDA) on the walker task. The results in Figure 6-(a) reveal: (1) fDA improves performance by mitigating distractors. (2) SeeX's separation design outperforms fDA on tasks with moving distractors. (3) pDA reduces performance, likely by disrupting world model learning, further investigation is planned.

## 6 Related Work

Model-based Control.Learning a dynamics model of the environment is a promising way to tackle the problem of low sample efficiency, and has achieved impressive results in various tasks including continuous control as well as discrete control. To ensure the fairness of comparison, we adopted the official version of Plan2Explore and the implementation of URLB-pixels, which combines all baseline methods into a unified Dreamer-like framework. Plan2Explore makes use of the dreamerv2 framework and takes the uncertainty of prediction of the next latent state as the intrinsic reward. In our work, we focus on the scenarios with complex visual distractors and design an intuitive separated world model to capture the exogenous and endogenous states respectively, and learn policy with imaginary trajectories in latent space.

Unsupervised RL.In recent years, there are many works that tried to promote the unsupervised representation learning manner in various fields including computer vision (CV) [12; 23; 25] and natural language processing (NLP) [9; 48; 14]. Additionally, there have been numerous works applying RL algorithms to real-world applications, such as [37; 68; 44]. These works encourage the RL community to explore the more efficient way of learning [34; 32; 53; 58; 72; 11; 26], however, these works still need to optimize an extrinsic reward. Recently, there have been works to adopt a pure unsupervised manner (reward-free pre-training followed by reward-specific fine-tuning), which can be categorized into three kinds. (i) _Data-based_: maximal entropy RL has enabled agent for diverse exploration and data [39; 55; 71]; (ii) _Knowledge-based_: increase knowledge about the world with self-supervised prediction [46; 47]; (iii) _Competence-based_: methods based on mutual information [16; 22; 38; 57] shows capability for diverse discovery and generalization. URLB-pixels offers a unified model-based framework to implement some methods referred to above, we make use of this benchmark to serve as the baseline. A shared limitation of existing unsupervised exploration methods lies in their exclusive reliance on state information for exploration, lacking the ability to explicitly extract endogenous information. This inherent limitation renders these methods susceptible to performance degradation when confronted with visually rich environments containing complex distractors. Different from the above methods, our method designs an intuitive separated model to separate exogenous and endogenous information and only collect imaginary trajectories in

Figure 6: **(a)** The impact of DA on performance in PT (pDA) and FT (fDA) stages. **(b)** Generalization ability to distractions from other distributions. The rightmost column indicates the percentage drop in performance caused by the distribution shift.

endogenous latent space to train policy. As a result, we largely improved performance compared to the other methods.

Learning with Noisy Observations.To address the issue of learning with noisy observations, diverse approaches have emerged, broadly categorized into four main strategies: (1) Use data augmentation methods to mitigate exogenous noises ; (2) Learning task-relevant representations with bisimulation metrics ; (3) Design auxiliary tasks to extract endogenous information ; (4) Take actions or rewards as discriminating factors to separate exogenous and endogenous information . Our proposed method, SeeX, falls into the intersection of the last two categories, combining the benefits of auxiliary task design and discriminating factors to effectively handle noisy observations in complex real-world environments.

## 7 Conclusion

In this paper, we consider the problem of learning a policy in the visual unsupervised reinforcement learning (URL) setting with moving distractors. To tackle this intricate scenario, we introduce SeeX, a bi-level optimization framework that leverages a separated world model and task-relevant uncertainty maximization to mitigate the impact of distractors and enhance exploration efficiency. Furthermore, Our theoretical analysis formalizes URL with distractors: the policy maximizes task-relevant uncertainty to drive exploration, while the world model minimizes environmental uncertainty to reduce distractor influence. SeeX utilizes a separated world representation to disentangle exogenous and endogenous factors from the original observation domain. Policy training is then conducted exclusively on imaginary trajectories generated within the endogenous latent representation. Extensive experiments on the DMC-suite benchmark demonstrate that SeeX outperforms other baseline methods. Ablation studies provide insights into the contributions of individual components to our final performance.

Limitations and Future Work.Our work presents several areas for improvement: **(I)** Our work focuses on DMC tasks, leaving real-world applications like self-driving and navigation challenges for future exploration. **(II)** Distractors fall into four types based on their impact on rewards and actions: task-irrelevant + action-independent (our focus), task-relevant + action-independent, task-irrelevant + action-dependent, and task-relevant + action-dependent. The latter three will be addressed in future work. **(III)** Using only \(s^{+}\) for policy learning is beneficial in our setting. However, for the task-relevant + action-independent case (e.g., multi-agent systems), we propose incorporating some \(s^{-}\) into policy optimization as a potential solution.