# Review Re-weighting in Imbalanced Learning by Density Ratio Estimation

Jiaan Luo\({}^{1,3}\) Feng Hong\({}^{1}\) Jiangchao Yao\({}^{1,3}\) Bo Han\({}^{4}\) Ya Zhang\({}^{2,3}\) Yanfeng Wang\({}^{2,3}\)

\({}^{1}\)Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{2}\)School of Artificial Intelligence, Shanghai Jiao Tong University

\({}^{3}\)Shanghai Artificial Intelligence Laboratory

\({}^{4}\)Hong Kong Baptist University

{luojiaan, feng.hong, Sunarker, ya_zhang, wangyanfeng}@sjtu.edu.cn

bhanm1@comp.hkbu.edu.hk

The first two authors contribute equally.The corresponding author is Jiangchao Yao (Sunarker@sjtu.edu.cn).

###### Abstract

In deep learning, model performance often deteriorates when trained on highly imbalanced datasets, especially when evaluation metrics require robust generalization across underrepresented classes. To address the challenges posed by imbalanced data distributions, this study introduces a novel method utilizing density ratio estimation for dynamic class weight adjustment, termed as Re-weighting with Density Ratio (RDR). Our method adaptively adjusts the importance of each class during training, mitigates overfitting on dominant classes and enhances model adaptability across diverse datasets. Extensive experiments conducted on various large scale benchmark datasets validate the effectiveness of our method. Results demonstrate substantial improvements in generalization capabilities, particularly under severely imbalanced conditions. The code is available here.

## 1 Introduction

In recent years, deep learning has made significant strides across various domains by utilizing complex architectures and large-scale datasets, setting new benchmarks for performance. However, these advancements often rely on well-curated datasets that ensure balanced class distributions . In contrast, real-world datasets typically exhibit a long-tailed distribution, where few classes dominate the majority of samples, while many others are underrepresented . This imbalance leads to model biases favoring frequent classes, thereby reducing performance on the less common ones. Yet, in many applications--such as medical diagnostics and financial analysis--greater emphasis is placed on ensuring strong generalization for underrepresented classes. Addressing this challenge not only reduces data collection costs but also improves the robustness and fairness of the models.

Many excellent methods, such as re-sampling , re-weighting , decoupled learning , margin-based learning , transfer learning  and contrastive learning , have been proposed to tackle the issue of imbalanced data. Despite the simplicity of re-weighting, it falls behind in performance significantly compared with other directions of methods due to the inappropriate weighting coefficients during training. Cui et al.  proposes a method for re-weighting by effective number, which accounts for potential overlaps among data samples and adjusts the weights for each category based on the actual effective number of samples. Chen et al.  leverages the effective area to re-weight, considering the actual spanned space of each class. However, such subsequent improvements can alleviate but still cannot effectively push that forward. Wang et al.

 obtains a fine-grained generalization bound for re-weighting in imbalanced learning through the data-dependent contraction technique. Limited research has focused on the intrinsic limitations of the commonly employed re-weighting-based loss functions and the corresponding balancing mechanisms designed to enhance parity in class representation.

This study rethinks the characteristics of re-weighted loss and explores the question _"Why is re-weighting necessary under conditions of sample imbalance?"_ Under conditions of sample imbalance, the variation in weights of samples arises due to discrepancies between the distribution of collected data and a balanced data distribution. In scenarios where class balance exists, such discrepancies are absent, thus obviating the need for re-weighting. Conversely, in imbalanced settings, re-weighting becomes essential to bridge the gap between these distributions. The weights must therefore represent a suitable compromise between balanced and imbalanced distributions and necessarily reflect accurately on each sample. Additionally, as model training progresses dynamically, optimizing the fit to feature distributions, the weights applied to each sample should be continuously updated to maintain robust performance.

This research introduces a novel method, Reweighting with Density Ratio (RDR), designed to mitigate learning disparities in imbalanced distributions. In this method, a feature extractor is employed to discern the features from the training data. A more balanced feature distribution is approximated by continuously updating the momentum on the feature level. This enables real-time density ratio estimation with features learned under imbalanced distributions, thereby obtaining the sample-wise weights. Notably, as the learned features evolve, our method dynamically adjusts weights in response to observed shifts in class density throughout the training cycle, ensuring that the model remains adaptive and effective. This method significantly enhances the robustness and adaptability of the training process. By integrating density ratio estimation to evaluate the difference between the balanced and real data distributions, our approach more accurately reflects the underlying class distribution and improves the model's generalization capabilities across diverse datasets. The contributions are summarized as follows:

* We explore the existing re-weighting techniques, and model the performance of various algorithms during training under different data distributions. This approach offers a novel perspective on understanding re-weighting methods in the scenarios of sample imbalance.
* We introduce a novel methodology, Re-weighting with Density Ratio (RDR), which leverages the method of density ratio estimation to dynamically adjust class weights during model training. This approach not only addresses the limitations of prior re-weighting methods but also introduces a mechanism to continuously adapt to the changing importance of classes as learning progresses, thereby enhancing model robustness and adaptability.
* We conduct extensive experiments to validate the effectiveness of our proposed RDR method. These experiments are conducted across various large-scale, long-tailed datasets, demonstrating substantial improvements in handling class imbalance. Our results illustrate significant enhancements in generalization capabilities, particularly under severely imbalanced scenarios.

## 2 Related Work

### Re-weighting Based Methods

Re-weighting methods for addressing class imbalance have evolved significantly over the years. Early techniques, such as [Zadrozny et al., 2003], employed inverse frequency techniques to address class imbalances but failed to consider deeper data distribution traits, leading to sub-optimal outcomes. Addressing these shortcomings, Huang et al.  introduced a cost-sensitive learning framework that, beyond simple frequency adjustments, incorporated misclassification costs to achieve a more nuanced balance. However, this approach still struggled with complexities like class overlap and label noise. To further refine this approach, Lin et al.  developed Focal Loss, which employs a modulation factor based on the prediction probability to adjust the loss function, thereby amplifying the impact of hard-to-classify samples while reducing the loss contribution of easy-to-classify samples. Cui et al.  introduced Class-Balanced Loss, which adjusts loss by data overlap, calculating the effective number of each class. Advancements continued with methods based on training gradients, such as [Ren et al., 2020b, Wang et al., 2021a]. Chen et al. [2023b] proposed Adaptive Re-weighting via effective area, which enhances model accuracy by considering the spatial distribution and density of data points within classes. Ma et al. (2023) introduced a re-weighting method that adjusts based on semantic richness and visual variability. However, no prior work has tackled the issue of sample imbalance by dynamically re-weighting based on the model's performance across training and test sets with differing distributions during the training process.

### Non-re-weighting Based Methods

In addition to re-weighting, many other methods are available to address the issue of sample imbalance. Re-sampling techniques (Kubat and Matwin, 1997; Wallace et al., 2011; Han et al., 2005; Hong et al., 2024) mitigate category imbalances by under-sampling dominant classes (Buda et al., 2018) or over-sampling minority classes (Bowyer et al., 2011). However, under-sampling may degrade feature representation by discarding valuable majority class data, whereas over-sampling could cause overfitting by duplicating minority class samples. Decoupled training approaches, such as (Kang et al., 2020), challenge the traditional joint training model by separating representation learning from classification. Margin-based methods such as, LADM (Cao et al., 2019), LA (Menon et al., 2020) and VS (Kini et al., 2021), adjust training processes to increase minority class margins, therefore to obtain a more balanced decision boundary. More flexible and robust methods are proposed, including Transfer Learning (Yin et al., 2019; Liu et al., 2019), Contrastive Learning (Li et al., 2022; Chen et al., 2023), Ensemble Learning (Wang et al., 2021; Cai et al., 2021) and Self-supervised Learning (Liu et al., 2022; Zhou et al., 2023). Please refer to Appendix B for more discussions.

## 3 Method

### Problem Setup

For a typical classification task in imbalanced learning, suppose given a training dataset \(=_{i=1}^{n}\{(_{i},_{i})\}\), where \(n\) is the total number of samples. Denote \(\{n_{1},n_{2},...,n_{d}\}\)as the sample number of each class. We assume, without loss of generality, that \(n_{i}<n_{j}\), when \(i<j\), with \(n_{d}\) typically much larger than \(n_{1}\), reflecting a pronounced imbalance in class distribution. We use a typical loss function like \(l_{+}\). Denote \(\{_{1},_{2},...,_{d}\}\) as the proportions of each class, such that \(_{i=1}^{d}_{i}=1\). Define a family of deep learning models parameterized by \(^{k}\). Typically, a model consists of a feature extractor \(f(x;)\) and a classifier \(h(z;)\), with \(=\{,\}\). The notations used in this paper are summarized in Appendix A.

### Motivation

Inspired by our review of prior methods, we observe a gap in the adaptation of dynamic class-weight adjustments during training phases. Building on the groundwork of static re-weighting strategies, we introduce a novel approach utilizing the method of density ratio estimation to dynamically recalibrate class weights. This innovation aims to provide a more refined adjustment by estimating real-time class density, thereby promoting an equitable influence of all classes throughout the training.

### Dynamic Re-weighting with Density Ratio

In a typical training optimization problem, our objective is to minimize the empirical risk of the loss function, _i.e._, \(=_{i=1}^{n}l(x_{i},y_{i};)\). However, in imbalanced datasets, where the frequency of samples across different classes varies, it is necessary to adjust for these gaps by applying different weights for the samples. We assume that the weight of each sample is denoted by \((x,y;)\), then the empirical risk can be formulated like \(=_{i=1}^{n}(x_{i},y_{i};)l(x_{i},y_{i}; )\).

In naive re-weighting approaches, the weight \(\) of class \(y\) is often set to \(}\). This setting is based on the assumption that the distribution of the training set \(P\) and the distribution balanced data set \(P_{bal}\) satisfy the equation \((x|y;)=P_{bal}(x|y;)\). However, in practical training scenarios, both training and test sets are subsets drawn from the actual distribution, leading to potential missing of feature patterns. Furthermore, classes with more complex features and lower sample frequencies tend to exhibit more pronounced missing of patterns. Therefore, in the training process, there exists a discrepancy between \((x|y;)\) and \(P_{bal}(x|y;)\). We measure the extent of this discrepancy using the ratio \(r(x|y;w)=(x|y;)/P_{bal}(x|y;)\), incorporating it as a correction term into our weighting scheme. Consequently, the empirical risk can be reformulated as follows

\[=_{i=1}^{n}|y;)}{_{y_{i}}}l(x_{i},y_{i};)\] (1)

We can explain the rationality of this formula as follows. Considering each class \(i\), where \(P_{bal}(y_{i};)_{i}^{-1}P(y_{i};)\) and \(P_{bal}(x_{i}|y_{i};)=r(x_{i}|y_{i};)P(x_{i}|y_{i};)\), with conditional probability formula, we can derive:

\[ R&=_{P}}(r (x_{i}|y_{i};)l(x_{i},y_{i};))=_{P}(y;) }{P(y;)}(x|y;)}{P(x|y;)}l(x,y;)\\ &=_{P}((x,y;)}{P(x,y;)}l( x,y;))=_{P_{bal}}l(x,y;)\] (2)

Eq. (2) demonstrates that our approach aligns with the balanced risk of the loss function. Consequently, minimizing Eq. (1) also serves to minimize the balanced risk.

Let's take a closer look at \(r(x|y;w)=(x|y;)/P_{bal}(x|y;)\). The variable \(r\) represents the ratio of two different distributions. We approximate this ratio using methods of density ratio estimation. This problem can be solved by first-order moment matching approach. Our goal is to minimize

\[*{argmin}_{r}\| xr(x|y;)P_{bal}(x|y;)x- xP(x|y;)x\|^{2}\] (3)

where \(\|\|\) denotes the Euclidean norm. Recall that we capture the features of the input samples by the feature extractor \(f(x;)\) in our model, and these features are a good reflection of what our model learned from the distribution of the input samples. Therefore, in order to capture more complex structures and patterns in raw data, we use \(f(x;)\) to obtain a variant of Eq. (3). Our goal can be achieved by obtaining \(*{argmin}_{r}^{{}^{}}(r)\), where \(^{{}^{}}(r)\) denotes

\[\| f(x;)r(x|y;)P_{bal}(x|y;)x- f(x; )P(x|y;)x\|^{2}\] (4)

where \(\) stands for'moment matching'. Let us ignore the irrelevant constant in \(^{{}^{}}(r)\), and define the rest as \((r)\):

\[\| f(x;)r(x|y;)P_{bal}(x|y;)x\|^{2}-2  f(x;)r(x|y;)P(x|y;)x, f(x; )P(x|y;)x\] (5)

where \(,\) denotes the inner product. In practice, as for the real-world imbalanced data distribution \(P\), we denote \(_{P}\) to dynamically reflect the knowledge learned from the distribution \(P\), that is \(_{P}=(f(x_{1};),,f(x_{n};))\). Remember that the output of feature extractor is \(z\), _i.e_., \(z=f(x;)\) is a \(Z\)-dimensional vector, then \(_{P}\) would be a \([Z,n]\)-dimensional vector. Similarly, we denote \(_{P}^{i}\) for class \(i\), which is a \([Z,n_{i}]\)-dimensional vector. As for the balanced data distribution \(P_{bal}\), we design a momentum mechanism to accumulatively estimate the expectation of features learned from balanced data distribution along with the training. Concretely, for each class, we maintain a prototype feature \(F\) for the entire training progress, using each batch's feature expectation for momentum updates. Therefore, we define \(F_{P_{bal}}\) as follows \(F_{P_{bal}}=(F_{1},,F_{d})\).

Since the total number of classes is \(d\), \(F_{P_{bal}}\) would be a \([Z,d]\)-dimensional vector. For each batch, we can obtain \(=(_{1},,_{d})\), where \(_{i}\) the mean of \(z\) of all samples in class \(i\). Then, the momentum updates works as follows

\[F_{P_{bal}} mF_{P_{bal}}+(1-m)\] (6)

where \(m[0,1)\) is a momentum coefficient. Back to Eq. (5), replace the expectations over \(P_{bal}\) and \(P\) by \(_{P}\) and \(F_{P_{bal}}\), respectively. Then, take the derivative of \((r)\) with respect to \(r\) and set it to zero. Detailed derivations are provided in Appendix C.1. For each class \(i\), we can obtain the estimation of density ratio in imbalanced learning as follows

\[}=n_{i}(_{P}^{i}\,{}^{}_{P}^ {i})^{-1}_{P}^{i}\,{}^{}F_{i}\] (7)Substitute Eq. (7) into Eq. (1), we can obtain our object to optimize

\[&=_{i=1}^{d}}_{y_{j}=i}n_{i}(_{P}^{i\ }_{P}^{i})^{-1}_{P}^{i\ }F_{i} l(x_{j},y_{j};)\\ &_{i=1}^{d}_{y_{j}=i}(_{P}^{i \ }_{P}^{i})^{-1}_{P}^{i\ }F_{i} l(x_{j},y_{j};)\] (8)

In our implementation, we introduced a warm-up phase to pre-adapt the feature distribution in \(F_{P_{bal}}\), thereby mitigating excessive oscillations during the initial stages of training. Additionally, we employed a temperature coefficient \(\) to modulate the influence of weights, which is typically set to 1. When integrating with logit adjustment (LA) (Menon et al., 2020), we adhere to the same procedures outlined in (Wang et al., 2023) to ensure the fisher consistency. The framework and pseudo-code of our method are shown in Appendix C.2 and Appendix C.3.

### Generalization Bound Analysis

Here, we use a formal generalization analysis to characterize the interesting point of our method.

**Theorem 1**.: _Given a model \(m\) and the loss function \(l\), for any \((0,1)\), with probability at least \(1-\) over the training set \(\), according to (Wang et al., 2023), the following generalization bound holds for the risk on the balanced distribution_

\[R_{}^{l}(m)(l,)+_{ }()}{d_{1}}_{y=1}^{d}w_{y}}[1- (B_{y}(m))]\] (9)

_where \((l,)\) is positively correlated with the empirical re-weighting risk of the training set. \(_{}()\) denotes the empirical complexity of the function set \(\). \(B_{y}(f)\) denotes the minimal prediction on the ground-truth class \(y\) in the training set. \(w_{y}\) refers to the weight of class \(y\) of the re-weighting loss._

Specifically, from the above generalization bound, we can find two inherent requirements for re-weighting methods. 1) _Why re-weighting is necessary_: \(w_{y}\) helps to re-balance the imbalanced term \(}[1-(B_{y}(m))]\) to get a sharper bound. 2) _Why dynamic re-weighting is necessary_: The term \(B_{y}(m)\) changes dynamically with model training. Therefore, we need a \(w_{y}\) that can adapt dynamically to the changes of \(B_{y}(m)\). 3) _Why RDR works_: From Fig. 1, we can observe that the dynamic trend of the RDR weight aligns well with \(}[1-(B_{y}(m))]\), denoted as \(B_{y}^{}\). This shows that our RDR can adapt to the dynamics in \(B_{y}^{}\), maintaining a sharp bound during training.

### Implementation and Complexity Analysis

At the end of each epoch, a global variable is maintained and updated using momentum, as described by Eq. (6). Within each minibatch, the weight of each sample is computed dynamically. Typical

Figure 1: Dynamic trend of the RDR weights well inversely aligns with \(B^{}\) throughout the training process in different categories. \(B_{y}^{}\) denotes \(}[1-(B_{y}(m))]\), where \(B_{y}(m)\) denotes the minimal prediction on the ground-truth class \(y\), i.e., \(_{_{y}}m()_{y}\). Experiments were conducted on CIFAR-10-LT dataset with an imbalance factor of 10.

optimization procedures for deep neural networks entail both forward and backward passes per mini-batch, characterized by a computational complexity of \((B)\), where \(B\) represents the batch size and \(\) denotes the overall parameter size. Within the RDR framework, suppose the feature dimension used as input to the classifier is \(K\), and the sample weights are computed according to Eq. (8). This computation for all \(d\) classes aggregates to a complexity of \((_{i=1}^{d}(n_{i} K^{2}+K^{3}))\) where \(n_{i}\) is the sample count of class \(i\) in a minibatch. Given that \(K\) generally exceeds \(n_{i}\), the complexity predominantly stems from the matrix inversion, approximating to \((dK^{3})\). The complexity for momentum updates is \((BK)\). Notice that \(K\) and \(B\) are considerably minor relative to the scale of the model parameters, rendering the time overhead of this method manageable.

On the storage front, the memory cost of the RDR primarily arises from the matrix inversion step in Eq. (8), resulting in a space complexity of \((K^{2})\). Given the scales of \(K\) is much lower than \(\), the extra memory usage is negligible when compared with the memory utilization of the model parameters. To this end, RDR imposes a relatively small computational or space cost, enabling its integration with existing approaches at a reduced cost. An empirical evaluation of the computational expense is presented in Fig. 3. For more discussions about limitations of RDR, please refer to Appendix E.

## 4 Experiments

### Experimental Setup

**Datasets.** We conduct experiments on four major long-tailed datasets, CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT (Liu et al., 2019) and Places-LT (Liu et al., 2019). CIFAR-10-LT and CIFAR-100-LT are two datasets sampled from the original CIFAR (Krizhevsky et al., 2009) dataset with a total of 10 and 100 classes, respectively. We conduct experiments with different imbalance factors \(IF=}{n_{min}}\), where \(n_{max}\) and \(n_{min}\) denotes the number of the most and least frequent classes (Kang et al., 2020; Hong et al., 2023; Zhou et al., 2024b). Following the mainstream protocol (Wang et al., 2023), we set the imbalance factor as 100 and 10 for evaluation. ImageNet-LT has 115.8K training images covering 1000 classes, with imbalance factor being 256. The number of samples per class ranges from 1280 to 5 images. Places-LT contains 62.5K training images covering 365 categories, with imbalance factor being 996. The number of samples per class ranges from 4980 to 5 images.

**Evaluation Protocol.** In the task of long-tailed classification, all classes are treated equally during testing. Following (Rangwani et al., 2022; Zhou et al., 2023c), we also report accuracy on three splits of classes according to the number of training data. Since the number of samples per class increases by its class index, for CIFAR-10-LT dataset, class[0, 3), class[3, 7) and class[7, 10) are reported as _Many_, _Medium_ and _Few_ classes, respectively. Similarly, CIFAR-100-LT is splited as class[0, 35), class[35, 69) and class[69, 100). ImageNet-LT is splited as class[0, 390), class[390, 835) and class[835, 1000), while Places-LT is splited as class[0, 131), class[131, 288) and class[288, 365).

**Baselines.** Our method is combined with existing long-tailed classification methods to demonstrate the efficacy, including the baseline trained by cross-entropy loss (CE), focal loss (Focal) (Lin et al., 2017), class-balanced loss (CB) (Cui et al., 2019) and logit adjustment (LA) (Menon et al., 2020). Recently, Sharpness-Aware minimization (SAM) (Foret et al., 2021) has been proved to be a powerful method in imbalanced learning, therefore we also adopt baseline including SAM (Foret et al., 2021),

### Comparison Results

Comparative analyses have been performed to evaluate the effectiveness of the proposed RDR. The results are presented in Table 1, Table 2 and Table 4. The metric employed to measure performance is the top-1 accuracy on the test sets.

**Results on CIFAR-10-LT and CIFAR-100-LT.** We first evaluate RDR on CIFAR-10-LT and CIFAR-100-LT. We report the final accuracy of different methods with imbalance factor ratio {10, 100} in Table 1 and Table 2. We can observe that RDR significantly outperforms all baselines under different imbalance factor ratios across the two datasets. Our observations highlight that RDR consistently outperforms baselines across various class distributions--_Many_, _Medium_, and _Few_--particularly under severe imbalance (_IF_=100).

In CIFAR-10-LT, combined with CE and LA, our method shows substantial improvement in the categories with fewer samples, increasing the accuracy by 19.8% and 9.5% respectively in the Few category under _IF_=100. This improvement is notable as it effectively addresses the challenge of learning from scarce data. With the inclusion of SAM, the performance of RDR is further enhanced. Under a less severe imbalance (_IF_=10), where the results show less performance drop-off between categories, RDR combined methods still maintain high performance across all categories, suggesting scalability and reliability of our approach in different imbalance contexts.

In CIFAR-100-LT, where the data distributions are more diverse and challenging, RDR also enhances the overall performance, particularly for the _Medium_ and _Few_ categories. Under the imbalance factor of 10 and 100, RDR increases the accuracy in _Few_ classes by 11.8% and 20.0% respectively, compared to the original CE loss. Furthermore, it is notable that techniques like ImbSAM and CCSAM, which especially focus on the _Few_ categories, may heavily sacrifice the performance on _Many_ classes. The results in both datasets show that RDR generally outperforms in the _Many_ classes compared to the other two variants of SAM, indicating that RDR can efficiently address the overfitting issues for _Few_ classes. For more experimental details, please refer to Appendix D.2 and Appendix D.1.

**Flat minima of loss landscape.** Key metrics associated with Eigen Spectral Density, such as the maximum and minimum eigenvalues (\(_{max}\) and \(_{min}\)) and the trace of the Hessian matrix (\(Tr(H)\)), effectively reflect the smoothness of the loss landscape. Lower values of \(_{max}\) and \(Tr(H)\) indicate a smoother loss landscape. Rangwani et al. (2022) have demonstrated that smoother loss landscapes correlate with stronger model generalization, which is particularly crucial when dealing

    &  &  &  &  \\    & IF=10 & IF=100 & IF=10 & & \\   CE & 88.9\({}_{ 0.4}\) & 75.6\({}_{ 0.8}\) & 59.3\({}_{ 0.7}\) & 42.7\({}_{ 0.3}\) & 43.2\({}_{ 0.1}\) & 29.3\({}_{ 0.2}\) \\ Focal & 89.0\({}_{ 0.3}\) & 76.0\({}_{ 0.1}\) & 59.7\({}_{ 0.5}\) & 43.0\({}_{ 0.6}\) & 43.8\({}_{ 0.8}\) & 29.5\({}_{ 0.2}\) \\ CB & 89.0\({}_{ 0.4}\) & 76.7\({}_{ 0.8}\) & 60.4\({}_{ 0.6}\) & 43.5\({}_{ 1.2}\) & 43.8\({}_{ 0.1}\) & 32.5\({}_{ 0.3}\) \\ LA & 89.2\({}_{ 0.3}\) & 82.2\({}_{ 0.7}\) & 62.3\({}_{ 0.5}\) & 48.2\({}_{ 0.4}\) & 47.9\({}_{ 0.4}\) & 37.5\({}_{ 0.2}\) \\  RDR+CE & 89.9\({}_{ 0.1}\) & 81.9\({}_{ 0.1}\) & 62.3\({}_{ 0.4}\) & 48.5\({}_{ 0.4}\) & 45.2\({}_{ 0.1}\) & 39.4\({}_{ 0.2}\) \\ RDR+LA & **90.2\({}_{ 0.4}\)** & **83.4\({}_{ 0.3}\)** & **62.9\({}_{ 0.2}\)** & **49.4\({}_{ 0.3}\)** & **48.1\({}_{ 0.3}\)** & **39.5\({}_{ 0.1}\)** \\   

Table 1: Top-1 accuracy (%) results for overall classes on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT and Places-LT, CIFAR-10-LT and CIFAR-100-LT are employed with imbalance factors of 10 and 100, respectively.

with imbalanced data. \(_{min}\) also serves as a significant indicator of the loss landscape characteristics. A preponderance of negative eigenvalues from the Hessian spectrum, resulting in smaller \(_{min}\) values, empirically suggests convergence to saddle points. Saddle points typically represent regions in the loss landscape characterized by a plateau with some negative curvature. In non-convex settings, it has been shown that an exponential number of saddle points exist, and convergence to these points is indicative of poor generalization.

Fig. 4 illustrates the Eigen Spectral Density under different loss function training regimes. It is evident that combining our method with the CE technique significantly improves the loss landscape. On one hand, \(_{max}\) is substantially reduced, indicating a flatter loss landscape. On the other hand, there is an increase in \(_{min}\), suggesting our method's effectiveness in escaping from saddle points.

Table 3 delves deeper into the changes in the loss landscape for all _Few_ classes. It reveals that combining our method with CE and SAM results in average reductions in \(_{max}\) by 58.5% and 66.1%, respectively, and increases in \(_{min}\) by 49.2% and 7.6%, respectively. Furthermore, our method significantly reduces \(Tr(H)\) for minority classes (class7, class8 and class9). The average value of \(Tr(H)\) decreases by 55.7% and 66.0% when combined with CE and SAM, respectively. These findings underscore the efficacy of our method in improving the loss landscape and enhancing the generalization capability of the model .

**Results on ImageNet-LT and Places-LT.** Our experiments conducted on ImageNet-LT and Places-LT, two large-scale datasets characterized by irregular and complex data distributions, demonstrate notable accuracy improvements through the application of RDR, as shown in Table 1 and Table 4.

    &  &  &  &  \\   & & & Many & Med. & Few & All & Many & Med. & Few & All \\    &  & SAM & 94.8 & 74.5 & 60.6 & 76.4 & 95.8 & 85.9 & 86.5 & 89.3 \\  & & ImbSAM & 94.6 & 73.9 & 67.7 & 78.3 & 94.2 & 84.1 & 90.1 & 89.0 \\  & & CCSAM & 85.6 & 79.2 & 80.3 & 81.4 & 90.6 & 85.3 & 90.8 & 88.5 \\  & & RDR+SAM & 91.8 & 78.6 & 81.9 & **83.6** & 94.2 & 86.3 & 92.5 & **90.5** \\   &  & SAM & 90.8 & 78.1 & 82.9 & 83.3 & 92.6 & 86.5 & 91.8 & 89.9 \\  & & ImbSAM & 85.2 & 75.1 & 89.6 & 82.5 & 90.8 & 83.6 & 94.8 & 89.1 \\  & & CCSAM & 85.8 & 77.8 & 80.5 & 81.0 & 90.7 & 82.4 & 90.8 & 87.4 \\  & & RDR+SAM & 88.5 & 80.0 & 85.0 & **84.1** & 92.5 & 86.5 & 93.6 & **90.4** \\   &  & SAM & 72.7 & 40.3 & 7.5 & 41.5 & 75.3 & 60.2 & 45.1 & 60.8 \\  & & ImbSAM & 71.5 & 40.6 & 17.7 & 44.3 & 72.5 & 57.9 & 53.5 & 61.7 \\  & & CCSAM & 61.5 & 50.8 & 29.7 & 48.0 & 62.8 & 59.3 & 54.5 & 59.0 \\  & & RDR+SAM & 63.4 & 51.9 & 30.5 & **49.3** & 68.0 & 61.6 & 58.3 & **62.8** \\    &  & SAM & 64.3 & 50.5 & 30.8 & 49.2 & 66.1 & 60.9 & 59.7 & 62.3 \\   & & ImbSAM & 58.8 & 45.4 & 40.1 & 48.4 & 62.9 & 56.6 & 64.2 & 61.2 \\   & & CCSAM & 57.7 & 48.9 & 29.0 & 45.8 & 61.0 & 57.8 & 51.9 & 57.1 \\   & & RDR+SAM & 63.9 & 52.4 & 30.5 & **49.6** & 67.6 & 61.7 & 60.1 & **63.2** \\   

Table 2: Top-1 accuracy (%) (\(\)) results for _Many_, _Medium_, _Few_ and overall classes on CIFAR-10-LT, categorized by imbalance factors (_IF_) of 100 and 10. The experiments are employed with the integration of Sharpness-Aware-Minimization-based methods.

Figure 4: Eigen spectral density for the class with the fewest samples across different methods. Experiments conduct on the CIFAR-10-LT, under an imbalance factor of 100. Maximum eigenvalue \(_{max}\) (\(\)) minimum eigenvalue \(_{min}\) (\(\)) in the top right corner of each panel. A lower \(_{max}\) indicates a smoother loss landscape, while a higher \(_{min}\) suggests conditions more favorable for escaping from saddle points, thereby enhancing the modelâ€™s generalization capabilities.

Specifically, when combined with CE and LA on ImageNet-LT, our method achieves accuracy enhancements of 18.7% and 2% in _Few_ classes, respectively. While the SAM technique combined with CE and LA offers limited accuracy improvements, its integration with our approach still results in an overall accuracy increase of approximately 2.3% compared to other methods. Notably, the LA method tends to suppress accuracy in _Many_ classes more than the CE method; however, this side effect is effectively mitigated when LA is combined with RDR.

In the Places-LT dataset, the performance of RDR is even more pronounced. Combinations of our method with CE and LA result in accuracy gains of 10.1% and 2% in overall classes, respectively. Additionally, integrating SAM with our method also yields incremental improvements of 10.3% and 0.5% under CE and LA conditions, respectively. Our approach not only enhances accuracy in _Few_ classes but also surpasses other methods in _Medium_ classes, indicating its comprehensive efficacy across different categories. This broad applicability is particularly crucial for addressing the challenges of imbalanced learning.

**Results on data with label noise.** We further investigate the performance of our approach on datasets with label noise. Specifically, we evaluate two datasets: CIFAR-10-LT-NL and CIFAR-100-LT-NL, both of which exhibit class imbalance and label noise. Experiments are conducted with a noise ratio of 5%, and the results are presented in Fig. 5. As shown, our method demonstrates consistent and significant improvements on more datasets with label noise.

### Ablation Study

In our study, we perform an ablation experiment to validate the efficacy of the multiple components that comprise our method. The outcomes from experiments across four datasets are delineated in Table 5. It is crucial to note that our weighting definition for each category \(i\) follows the formula \(w=r/n_{i}\). When \(r=1\), our method simplifies to the traditional inverse frequency weighting \(w=1/n_{i}\). We explored the differences between our approach with and without the integration of SAM compared to this conventional weighting method.

    &  &  &  \\   & & Many & Med. & Few & All & Many & Med. & Few & All \\    & SAM & 64.6 & 35.8 & 10.1 & 42.8 & 45.2 & 27.6 & 12.1 & 30.6 \\  & ImbSAM & 62.5 & 37.3 & 13.9 & 43.3 & 43.1 & 26.2 & 16.1 & 30.1 \\  & CCSAM & 54.1 & 44.1 & 30.8 & 45.8 & 40.4 & 39.7 & 31.7 & 38.2 \\  & RDR+SAM & 59.1 & 46.5 & 26.2 & **48.1** & 41.5 & 41.3 & 39.0 & **40.9** \\   & SAM & 39.6 & 45.9 & 39.1 & 42.3 & 42.5 & 41.9 & 35.5 & 40.8 \\  & ImbSAM & 56.1 & 45.1 & 37.9 & 48.2 & 38.6 & 35.8 & 40.4 & 37.8 \\   & CCSAM & 52.2 & 43.9 & 32.8 & 45.3 & 32.1 & 41.4 & 40.9 & 37.9 \\   & RDR+SAM & 57.5 & 49.7 & 35.9 & **50.5** & 41.5 & 42.9 & 37.8 & **41.3** \\   

Table 4: Top-1 accuracy (%) (\(\)) results for _Many_, _Medium_, _Few_ and overall classes on ImageNet-LT and Places-LT. The experiments are employed with the integration of Sharpness-Aware-Minimization-based methods.

   Method & \(}}\) & \(}}\) & \(Tr_{6}\) & \(Tr_{7}\) & \(Tr_{8}\) & \(Tr_{9}\) & \(_{Few}\) \\   SGD & -76.24 & 110.78 & 335.64 & 301.67 & 234.97 & 323.87 & 286.84 \\ RDR & -38.73 & 45.95 & 290.06 & 293.04 & 46.15 & **41.71** & 126.96 \\  SAM & -14.42 & 74.82 & 300.13 & 314.48 & 123.10 & 291.00 & 242.86 \\ RDR+SAM & **-13.33** & **25.36** & **179.50** & **147.11** & **36.28** & 64.32 & **82.57** \\   

Table 3: Loss landscape metrics across different methods on CIFAR-10-LT, with imbalance factor 100. Average minimum eigenvalues \(}}\) (\(\)), average maximum eigenvalues \(}}\) (\(\)), and the trace \(Tr\) (\(\)) of the Hessian matrix for classes with few samples. \(Tr_{6}\), \(Tr_{7}\), \(Tr_{8}\), and \(Tr_{9}\) represent the traces of the Hessian matrix for class 6, 7, 8 and 9, respectively, with descending sample quantities. \(_{Few}\) denotes average trace of Hessian matrix over _Few_ classes. Lower \(_{}\) and \(Tr\) values indicate a flatter loss landscape, while a higher \(_{}\) suggests a landscape more conducive to escaping from saddle points, thereby potentially enhancing model generalization.

The results depicted in Table 5 reveal that our dynamic weighting approach consistently outperforms the classic method under various scenarios. Without SAM, when combined with CE and LA, our method achieves accuracy improvements ranging from 0.4% to 4.3% and 0.3% to 2.3%, respectively. When integrated with SAM, the improvement in accuracy is particularly notable on large datasets. Specifically, the accuracy enhancements on ImageNet-LT and Places-LT reach 7.1% and 1.9% respectively when combined with LA. These results underscore the tangible benefits of our dynamic weighting strategy in enhancing model performance. The impact of momentum coefficient \(m\) in RDR is shown in Fig. 3. For more experimental details, please refer to Appendix D.3.

## 5 Conclusion

In this work, we have introduced RDR, a novel approach for mitigating model degradation in imbalanced learning scenarios by dynamically adjusting class weights using density ratio estimation. Our method dynamically adjusts class weights during training based on density ratio estimation, enhancing both model robustness and adaptability. Extensive experiments on diverse large-scale datasets demonstrate the effectiveness of RDR, particularly in severely imbalanced settings. Future work will focus on refining the dynamic adjustment mechanisms and exploring broader applicability across various domains and dataset complexities.