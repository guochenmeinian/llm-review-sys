# Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds

Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds

 Jiayi Huang\({}^{1}\)  Han Zhong\({}^{1}\)  Liwei Wang\({}^{1,2}\)  Lin F. Yang\({}^{3}\)

\({}^{1}\)Center for Data Science, Peking University

\({}^{2}\)National Key Laboratory of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{3}\)University of California, Los Angles

{jjyhuang, hanzhong}@stu.pku.edu.cn,

wanglu@cis.pku.edu.cn, linyang@ee.ucla.edu

###### Abstract

While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are _heavy-tailed_, i.e., with only finite \((1+)\)-th moments for some \((0,1]\). In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, Heavy-OFUL, for heavy-tailed linear bandits, achieving an _instance-dependent_\(T\)-round regret of \(dT^{}^{T} _{t}^{2}}+dT^{}\), the _first_ of this kind. Here, \(d\) is the feature dimension, and \(_{t}^{1+}\) is the \((1+)\)-th central moment of the reward at the \(t\)-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in stochastic and deterministic linear bandits. We then extend this algorithm to the RL settings with linear function approximation. Our algorithm, termed as Heavy-LSVI-UCB, achieves the _first_ computationally efficient _instance-dependent_\(K\)-episode regret of \((d^{*}}K^{}+d^{*}}K)\). Here, \(H\) is length of the episode, and \(^{*}\), \(^{*}\) are instance-dependent quantities scaling with the central moment of reward and value functions, respectively. We also provide a matching minimax lower bound \((dHK^{}+dK})\) to demonstrate the optimality of our algorithm in the worst case. Our result is achieved via a novel robust self-normalized concentration inequality that may be of independent interest in handling heavy-tailed noise in general online regression problems.

## 1 Introduction

Designing efficient reinforcement learning (RL) algorithms for large state-action space is a significant challenge within the RL community. A crucial aspect of RL is understanding the reward functions, which directly impacts the quality of the agent's policy. In certain real-world situations, reward distributions may exhibit heavy-tailed behavior, characterized by the occurrence of extremely large values at a higher frequency than expected in a normal distribution. Examples include image noise in signal processing , stock price fluctuations in financial markets [12; 18], and value functions in online advertising [11; 19]. However, much of the existing RL literature assumes rewards to be either uniformly bounded or light-tailed (e.g., sub-Gaussian). In such light-tailed settings, the primary challenge lies in learning the transition probabilities, leading most studies to assume deterministic rewards for ease of analysis [5; 22; 15]. As we will demonstrate, the complexity of learning reward functions may dominate in heavy-tailed settings. Consequently, the performance of traditional algorithms may decline, emphasizing the need for the development of new, efficient algorithms specifically designed to handle heavy-tailed rewards.

Heavy-tailed distributions have been extensively studied in the field of statistics  and in more specific online learning scenarios, such as bandits . However, there is a dearth of theoretical research in RL concerning heavy-tailed rewards, whose distributions only admit finite \((1+)\)-th moment for some \((0,1]\). One notable exception is Zhuang and Sui , which made a pioneering effort in establishing worst-case regret guarantees in _tabular_ Markov Decision Processes (MDPs) with heavy-tailed rewards. However, their algorithm cannot handle RL settings with large state-action space. Moreover, their reliance on truncation-based methods is sub-optimal as these methods heavily depend on raw moments, which do not vanish in deterministic cases. Therefore, a natural question arises:

_Can we derive sample and time-efficient algorithms for RL with large state-action space that achieve instance-dependent regret in the presence of heavy-tailed rewards?_

In this work, we focus on linear MDPs  with heavy-tailed rewards and answer the above question affirmatively. We say a distribution is _heavy-tailed_ if it only admits finite \((1+)\)-th moment for some \((0,1]\). Our contributions are summarized as follows.

* We first propose a computationally efficient algorithm Heavy-OFUL for heavy-tailed linear bandits. Such a setting can be regarded as a special case of linear MDPs. Heavy-OFUL achieves an _instance-dependent_\(T\)-round regret of \(dT^{}^{T }_{t}^{2}+dT^{}}\), the _first_ of this kind. Here \(d\) is the feature dimension and \(_{t}^{1+}\) is the \((1+)\)-th central moment of the reward at the \(t\)-th round. The instance-dependent regret bound has a main term that only depends on the summation of central moments, and therefore does not have a \(\) term. Our regret bound is shown to be minimax optimal in both stochastic and deterministic linear bandits (See Remark 4.2 for details).
* We then extend this algorithm to time-inhomogeneous linear MDPs with heavy-tailed rewards, resulting in a new computationally efficient algorithm Heavy-LSVI-UCB, which achieves a \(K\)-episode regret scaling as \((d^{s}}K^{}+d^{s}}K)\) for the _first_ time. Here, \(H\) is the length of the episode and \(^{},^{}\) are quantities measuring the central moment of the reward functions and transition probabilities, respectively (See Theorem 5.2 for details). Our regret bound is _instance-dependent_ since the main term only relies on the instance-dependent quantities, which vanishes when the dynamics and rewards are deterministic. When specialized to special cases, our instance-dependent regret recovers the variance-aware regret in Li and Sun  (See Remark 5.3 for details) and improves existing first-order regret bounds  (See Corollary 5.6 for details).
* We provide a minimax regret lower bound \((dHK^{}+dK})\) for linear MDPs with heavy-tailed rewards, which matches the worst-case regret bound implied by our instance-dependent regret, thereby demonstrating the minimax optimality of Heavy-LSVI-UCB in the worst case.

For better comparisons between our algorithms and state-of-the-art results, we summarize the regrets in Table 1 and 2 for linear bandits and linear MDPs, respectively. More related works are deferred to Appendix A. Remarkably, our results demonstrate that \(=1\) (i.e. finite variance) is sufficient to obtain variance-aware regret bounds of the same order as the case where rewards are uniformly bounded for both linear bandits and linear MDPs. The main technique contribution behind our results is a novel robust self-normalized concentration inequality inspired by Sun et al. . To be more specific, it is a non-trivial generalization of adaptive Huber regression from independent and identically distributed (i.i.d.) case to heavy-tailed online regression settings and gives a _self-normalized_ bound instead of the \(_{2}\)-norm bound in Sun et al. . Our result is computationally efficient and only scales with the feature dimension, \(d\), \((1+)\)-th _central_ moment of the noise, \(\), and does not depend on the absolute magnitude as in other self-normalized concentration inequalities .

Road MapThe rest of the paper is organized as follows. Section 2 introduces heavy-tailed linear bandits and linear MDPs. Section 3 presents the robust self-normalized concentration inequality for general online regression problems with heavy-tailed noise. Section 4 and 5 give the main results for heavy-tailed linear bandits and linear MDPs, respectively. We then conclude in Section 6. Related work, experiments and all proofs can be found in Appendix.

NotationsLet \(\|\|:=\|\|_{2}\). Let \([t]:=\{1,2,,t\}\). Let \(_{d}(r):=\{^{d}|\|\| r\}\). Let \(x_{[a,b]}:=\{a,\{x,b\}\}\) denote the projection of \(x\) onto the close interval \([a,b]\). Let \((\{X_{s}\}_{s[t]})\) be the \(\)-field generated by random vectors \(\{X_{s}\}_{s[t]}\).

## 2 Preliminaries

### Heavy-Tailed Linear Bandits

**Definition 2.1** (Heterogeneous linear bandits with heavy-tailed rewards).: Let \(\{_{t}\}_{t 1}\) denote a series of fixed decision sets, where all \(_{t}_{t}\) satisfy \(\|_{t}\| L\) for some known upper bound \(L\). At each round \(t\), the agent chooses \(_{t}_{t}\), then receives a reward \(R_{t}\) from the environment. We define the filtration \(\{_{t}\}_{t 1}\) as \(_{t}=(\{_{s},R_{s}\}_{s[t]}\{_{t+1}\})\) for any \(t 1\). We assume \(R_{t}=_{t},^{*}+_{t}\) with the unknown coefficient \(^{*}_{d}(B)\) for some known upper bound \(B\). The random variable \(_{t}\) is \(_{t}\)-measurable and satisfies \([_{t}|_{t-1}]=0,[|_{t}| ^{1+}|_{t-1}]=_{t}^{1+}\) for some \((0,1]\) with \(_{t}\) being \(_{t-1}\)-measurable.

The agent aims to minimize the \(T\)-round _pseudo-regret_ defined as \((T)=_{t=1}^{T}[_{t}^{*},^{*} -_{t},^{*}]\), where \(_{t}^{*}=_{_{t}},^{*}\).

### Linear MDPs with Heavy-Tailed Rewards

We use a tuple \(M=M(,,H,\{R_{h}\}_{h[H]},\{_{h}\}_{h[H]})\) to describe the _time-inhomogeneous finite-horizon MDP_, where \(\) and \(\) are state space and action space, respectively, \(H\) is the length of the episode, \(R_{h}:\) is the random reward function with expectation \(r_{h}:\), and \(_{h}:()\) is the transition probability function. More details can be found in Puterman . A time-dependent _policy_\(=\{_{h}\}_{h H}\) satisfies \(_{h}:()\) for any \(h[H]\). When the policy is deterministic, we use \(_{h}(s_{h})\) to denote the action chosen at the \(h\)-th step given \(s_{h}\) by policy \(\). For any state-action pair \((s,a)\), we define the _state-action value function_\(Q_{h}^{}(s,a)\) and _state value function_\(V_{h}^{}(s)\) as follows: \(Q_{h}^{}(s,a)=[_{h^{}=h}^{H}r(s_{h^{}},a_{h^{ }})|s_{h}=s,a_{h}=a]\), \(V_{h}^{}(s)=Q_{h}^{}(s,_{h}(s))\), where the expectation is taken with respect to the transition probability of

 
**Algorithm** & **Regret** &  **Dataset** \\ **Dependent** \\  &  **First** \\ **Order** \\  &  **Multiway** \\ **Optimal** \\  &  **Computa** \\ **Combat** \\ **Efficient** \\  &  **Heavy- \\ **Tailed** \\ **Reward** \\  \\   LSVI-UCB \\  & \((H^{4}K})\) & No & No & No & **Yes** & No \\   Force  \\  & \((H^{3}V_{1}^{*}K})\) & No & **Yes** & No & No & No \\   VQOL  \\  & \((dK})\) & No & No & **Yes** & **Yes** & No \\   VARA  \\  & \((d^{*}K})\) & **Yes** & **Yes** & **Yes** & **Yes** & \(=1\) \\  
 Heavy-LSVI-UCB (**Ours**) \\  & \((d^{*}K}+d^{*}K})\) & **Yes** & **Yes** & **Yes** & **Yes** \\  

Table 2: Comparisons with previous works on time-inhomogeneous linear MDPs. \(d\), \(H\), \(K\), \(V_{1}^{*}\), \(^{*}\) are feature dimension, the length of the episode, the number of episodes, optimal value function, variance-dependent quantity defined in Li and Sun . \(^{*}\), \(^{*}\) are defined in Theorem 5.2.

\(M\) and the agent's policy \(\). If \(\) is randomized, then the definition of \(V\) should have an expectation. Denote the optimal value functions as \(V_{h}^{*}(s)=_{}V_{h}^{n}(s)\) and \(Q_{h}^{*}(s,a)=_{}Q_{h}^{n}(s,a)\).

We introduce the following shorthands for simplicity. At the \(h\)-th step, for any value function \(V:\), let \([_{h}V](s,a)=_{s^{} p_{h}(|s,a)}V(s^{})\), \([_{h}V](s,a)=[_{h}V^{2}](s,a)-[_{h}V]^{2}(s,a)\) denote the expectation and the variance of the next-state value function at the \(h\)-th step given \((s,a)\).

We aim to minimize the \(K\)-episode _regret_ defined as \((K)=_{k=1}^{K}[V_{1}^{*}(s_{1}^{k})-V_{1}^{^{k}}(s_{1}^{ k})]\).

In the rest of this section, we introduce linear MDPs with heavy-tailed rewards. We first give the definition of linear MDPs studied in Yang and Wang , Jin et al. , with emphasis that the rewards in their settings are deterministic or uniformly bounded. Then we focus on the heavy-tailed random rewards.

**Definition 2.2**.: An MDP \(M=M(,,H,\{R_{h}\}_{h[H]},\{_{h}\}_{h[H]})\) is a _time-inhomogeneous finite-horizon linear MDP_, if there exist known feature maps \((s,a):_{d}(1)\), unknown \(d\)-dimensional signed measures \(\{_{h}^{}\}_{h[H]}\) over \(\) with \(\|_{h}^{*}()\|:=_{s}|(s)|s\) and unknown coefficients \(\{_{h}^{*}\}_{h[H]}_{d}(B)\) for some known upper bound \(B\) such that

\[r_{h}(s,a)=(s,a),_{h}^{*},_{ h}(|s,a)=(s,a),_{h}^{*}()\]

for any state-action pair \((s,a)\) and timestep \(h[H]\).

**Assumption 2.3** (Realizable rewards).: For all \((s,a,h)[H]\), the random reward \(R_{h}(s,a)\) is independent of next state \(s^{}_{h}(|s,a)\) and admits the linear structure

\[R_{h}(s,a)=(s,a),_{h}^{*}+_{h}(s,a),\]

where \(_{h}(s,a)\) is a mean-zero heavy-tailed random variable specified below.

We introduce the notation \(_{n}[X]=[|X-X|^{n}]\) for the \(n\)-th central moment of any random variable \(X\). And for any random reward function at the \(h\)-th step \(R_{h}:\), let

\[[_{h}R_{h}](s,a)=[R_{h}(s_{h},a_{h})|(s_{h},a_{h})=(s,a)],\]

\[[_{1+}R_{h}](s,a)=[[|R_{h}-_{h}R_{h}](s_ {h},a_{h})|^{1+}|(s_{h},a_{h})=(s,a)]\]

denote its expectation and the \((1+)\)-th central moment given \((s_{h},a_{h})=(s,a)\) for short.

**Assumption 2.4** (Heavy-tailedness of rewards).: Random variable \(_{h}(s,a)\) satisfies \([_{h}_{h}](s,a)=0\). And for some known \(,^{}(0,1]\) and constants \(_{R},_{R^{}} 0\), the following unknown moments of \(_{h}(s,a)\) satisfy

\[[_{h}|_{h}|^{1+}](s,a)_{R}^{1+}, [_{1+^{}}|_{h}|^{1+}](s,a) _{R^{}}^{1+^{}}\]

for all \((s,a,h)[H]\).

Assumption 2.4 generalizes Assumption 2.2 of Li and Sun , which is the weakest moment condition on random rewards in the current literature of RL with function approximation. Setting \(=1\) and \(^{}=1\) immediately recovers their settings.

**Assumption 2.5** (Realizable central moments).: There are some unknown coefficients \(\{_{h}^{*}\}_{h[H]}_{d}(W)\) for some known upper bound \(W\) such that

\[[_{h}|_{h}|^{1+}](s,a)=(s,a), {}_{h}^{*}\]

for all \((s,a,h)[H]\).

**Remark 2.6**.: When \(=1\), that is the rewards have finite variance, Li and Sun  use the fact that \([_{2}R_{h}](s,a)=[_{h}R_{h}](s,a)=[_{h}R_{h}^{2}](s,a) -[_{h}R_{h}]^{2}(s,a)\), assume the linear realizability of the second moment \([_{h}R_{h}^{2}](s,a)\), and estimate it instead. However, when \(<1\), there is no such relationship between the \((1+)\)-th central moment \([_{1+}R_{h}](s,a)\) and the \((1+)\)-th raw moment \([_{h}R_{h}^{1+}](s,a)\). Thus, we adopt a new approach to estimate \([_{1+}R_{h}](s,a)\) directly, and bound the error by a novel perturbation analysis of adaptive Huber regression in Appendix C.3.

**Assumption 2.7** (Bounded cumulative rewards).: For any policy \(\), let \(\{s_{h},a_{h},R_{h}\}_{h[H]}\) be a random trajectory following policy \(\). And define \(r_{}=_{h=1}^{H}[_{h}R_{h}](s_{h},a_{h})=_{h=1}^{H}r_{h}(s_{h },a_{h})\). We assume (1) \(0 r_{}\). (2) \(_{h=1}^{H}[_{1+}R_{h}]^{}(s_{h},a_{h}) \). (3) \((r_{})\).

Here, (1) gives an upper bound of cumulative expected rewards \(r_{}\). (2) assumes the summation of \((1+)\)-th central moment of rewards \([_{1+}R_{h}](s_{h},a_{h})\) is bounded since \([_{h=1}^{H}[_{1+}R_{h}](s_{h},a_{h})]^{} _{h=1}^{H}[_{1+}R_{h}]^{}(s_{h},a_{h}) \) due to Jensen's inequality. And (3) is to bound the variance of \(r_{}\) along the trajectory following policy \(\).

```
0: Number of total rounds \(T\), confidence level \(\), regularization parameter \(\), \(_{}\), parameters for adaptive Huber regression \(c_{0},c_{1},_{0}\), estimated central moment \(_{t}\) and moment parameter \(b\) that satisfy \(_{t}/_{t} b\) for all \(t T\).
0: The estimated coefficient \(_{t}\).
1:\(=d(1+TL^{2}/(d_{}^{2}))\).
2: Set \(_{t-1}=+_{s=1}^{t-1}_{s}^{-2} _{s}_{s}^{}\).
3: Set \(_{t}=\{_{t},_{},_{t}\|_{_{t-1}^{-1}}}{c_{0}},}{c_{1}^{4}( 2 b^{2})^{}}\|_{t}\|_{_ {t-1}^{-1}}^{}\}\).
4: Set \(_{t}=_{0}^{2}}}{w_{t}}t^{}\) with \(w_{t}=\|_{t}/_{t}\|_{_{t-1}^{-1}}\).
5: Define the loss function \(L_{t}():=\|\|^{2}+_{ s=1}^{t}_{_{s}}(-_{s}, }{_{s}})\).
6: Compute \(_{t}=*{argmin}_{ _{d}(B)}L_{t}()\). ```

**Algorithm 1** Adaptive Huber Regression

## 3 Adaptive Huber Regression

At the core of our algorithms for both heavy-tailed linear bandits and linear MDPs is a new approach - adaptive Huber regression - to handle heavy-tailed noise. Sun et al.  imposed adaptive Huber regression to handle i.i.d. heavy-tailed noise by utilizing Huber loss  as a surrogate of squared loss. Li and Sun  modified adaptive Huber regression for heterogeneous online settings, where the variances in each round are different. However, it is not readily applicable to deal with heavy-tailed noise. Our contribution in this section is to construct a new self-normalized concentration inequality for general online regression problems with heavy-tailed noise.

We first give a brief introduction to Huber loss function and its properties.

**Definition 3.1** (Huber loss).: _Huber loss_ is defined as

\[_{}(x)=}{2}&|x|,\\ |x|-}{2}&|x|>,\] (3.1)

where \(>0\) is referred as a robustness parameter.

Huber loss is first proposed by Huber  as a robust version of squared loss while preserving the convex property. Specifically, Huber loss is a quadratic function of \(x\) when \(|x|\) is less than the threshold \(\), while becomes linearly dependent on \(|x|\) when \(|x|\) grows larger than \(\). It has the property of strongly convex near zero point and is not sensitive to outliers. See Appendix C.1 for more properties of Huber loss.

Next, we define general online regression problems with heavy-tailed noise, which include heavy-tailed linear bandits as a special case. Then we utilize Huber loss to estimate \(^{*}\). Below we give the main theorem to bound the deviation of the estimated \(_{t}\) in Algorithm 1 from the ground truth \(^{*}\).

**Definition 3.2**.: Let \(\{_{t}\}_{t 1}\) be a filtration. For all \(t>0\), let random variables \(y_{t},_{t}\) be \(_{t}\)-measurable and random vector \(_{t}_{d}(L)\) be \(_{t-1}\)-measurable. Suppose \(y_{t}=_{t},^{*}+_ {t}\), where \(^{*}_{d}(B)\) is an unknown coefficient and

\[[_{t}|_{t-1}]=0,[|_ {t}|^{1+}|_{t-1}]=_{t}^{1+}\]

for some \((0,1]\). The goal is to estimate \(^{*}\) at any round \(t\) given the realizations of \(\{_{s},y_{s}\}_{s[t]}\).

**Theorem 3.3**.: For the online regression problems in Definition 3.2, we solve for \(_{t}\) by adaptive Huber regression in Algorithm 1 with \(c_{0},c_{1},_{0}\) in Appendix C.2. Then for any \((0,1)\), with probability at least \(1-3\), for all \(t T\), we have \(\|_{t}-^{*}\|_{_{t}} _{t}\), where \(_{t}\) is defined in Algorithm 1 and

\[_{t}=3B+24t^{}b ( 3T)^{}((2T^{2}/))^{}.\] (3.2)

Proof.: To derive a tight high-probability bound, we take the most advantage of the properties of Huber loss. A Chernoff bounding technique is used to bound the main error term, which requires a careful analysis of the moment generating function. See Appendix C.2 for a detailed proof.

We refer to the regression process in Line 6 of Algorithm 1 as _adaptive Huber regression_ in line with Sun et al.  to emphasize that the value of robustness parameter \(_{t}\) is chosen to adapt to data for a better trade-off between bias and robustness. Specifically, since we are in the online setting, \(_{t}\) are dependent on \(\{_{s}\}_{s<t}\), which is the key difference from the i.i.d. case in Sun et al.  where they set \(_{t}=_{0}\), for all \(t T\). Thus, as shown in Line 4 of Algorithm 1, inspired by Li and Sun , we adjust \(_{t}\) according to the importance of observations \(w_{t}=\|_{t}/_{t}\|_{_{t-1}^{-1}}\), where \(_{t}\) is specified below. In the case where \(<1\), different from Li and Sun , we first choose \(_{t}\) to be small for robust purposes, then gradually increase it with \(t\) to reduce the bias.

Next, we illustrate the reason for setting \(_{t}\) via Line 3 of Algorithm 1. We use \(_{t}_{t-1}\) to estimate the central moment \(_{t}\) and use moment parameter \(b\) to measure the closeness between \(_{t}\) and \(_{t}\). When we choose \(_{t}\) as an upper bound of \(_{t}\), \(b\) becomes a constant that equals to \(1\). And \(_{}\) is a small positive constant to avoid singularity. The last two terms with respect to \(c_{0}\) and \(c_{1}\) are set according to the uncertainty \(\|_{t}\|_{_{t-1}^{-1}}\). In addition, setting the parameter \(c_{0} 1\) yields \(w_{t} 1\), which is essential to meet the condition of elliptical potential lemma .

**Remark 3.4**.: The error bound \(_{t}\) in (3.2) is only related to the feature dimension \(d\) and moment parameter \(b\). While the Bernstein-style self-normalized concentration bounds [42; 41] depend on the magnitude of \(_{t}\), thus cannot handle heavy-tailed errors.

## 4 Linear Bandits

In this section, we show the algorithm Heavy-OFUL in Algorithm 2 for heavy-tailed linear bandits in Definition 2.1. We first give a brief algorithm description, and then provide a theoretical regret analysis.

### Algorithm Description

Heavy-OFUL follows the principle of Optimism in the Face of Uncertainty (OFU) , and uses adaptive Huber regression in Section 3 to maintain a set \(_{t}\) that contains the unknown coefficient \(^{*}\) with high probability. Specifically, at the \(t\)-th round, Heavy-OFUL estimates the expected reward of any arm \(\) as \(_{_{t-1}},}\), and selects the arm that maximizes the estimated reward. The agent then receives the reward \(R_{t}\) and updates the confidence set \(_{t}\) based on the information up to round \(t\) with its center \(_{t}\) computed by adaptive Huber regression as in Line 9 of Algorithm 2.

```
0: Number of total rounds \(T\), confidence level \(\), regularization parameter \(\), \(_{}\), parameters for adptive Huber regression \(c_{0},c_{1},_{0}\), confidence radius \(_{t}\).
1:\(=d(1+}}{d_{}^{2}})\), \(_{0}=_{d}(B)\), \(_{0}=\).
2:for\(t=1,,T\)do
3: Observe \(_{t}\).
4: Set \((_{t},)=*{argmax}_{_{t},_{t-1}},\).
5: Play \(_{t}\) and observe \(R_{t},_{t}\).
6: Set \(_{t}=_{t},_{},_{t}\| _{_{t-1}^{-1}}}{c_{0}},}{c_{1}^{}(2) }\|_{t}\|_{_{t-1}^{-1}}^{}}\).
7: Set \(_{t}=_{0}^{2}}}{w_{t}}t^{}\) with \(w_{t}=\|_{t}/_{t}\|_{_{t-1}^{-1}}\).
8: Update \(_{t}=_{t-1}+_{t}^{-2}_{t}_{t}^{}\).
9: Solve for \(_{t}\) by Algorithm 1 and set \(_{t}=\{^{d}\|-_{t}\|_{ _{t}}_{t}\}\).
10:endfor ```

**Algorithm 2** Heavy-OFUL

We next give the instance-dependent regret upper bound of Heavy-OFUL in Theorem 4.1.

**Theorem 4.1**.: For the heavy-tailed linear bandits in Definition 2.1, we set \(c_{0},c_{1},_{0},_{t}\) in Algorithm 2 according to Theorem 3.3 with \(b=1\). Besides, let \(=d/B^{2}\), and \(_{}=1/\). Then with probability at least \(1-3\), the regret of Heavy-OFUL is bounded by

\[(T)=(dT^{} ^{T}_{t}^{2}}+dT^{ }).\]

Proof.: The proof uses the self-normalized concentration inequality of adaptive Huber regression and a careful analysis to bound the summation of bonuses. See Appendix D.1 for a detailed proof. 

**Remark 4.2**.: Theorem 4.1 shows Heavy-OFUL achieves an instance-dependent regret bound. When we assume \(_{t}, t 1\) have uniform upper bound \(\) (which can be treated as a constant), then the bound is reduced to \((dT^{})\). It matches the lower bound \((dT^{})\) by Shao et al.  up to logarithmic factors. In the deterministic scenario, where \(=1\) and \(_{t}=0\), for all \(t 1\), the bound is reduced to \((d)\). It matches the lower bound \((d)\)1 up to logarithmic factors.

## 5 Linear MDPs

In this section, we show the algorithm Heavy-LSVI-UCB in Algorithm 3 for linear MDP with heavy-tailed rewards defined in Section 2.2. Let \(_{k,h}:=(s_{k,h},a_{k,h})\) for short. We first give the algorithm description intuitively, then provide the computational complexity and regret bound.

### Algorithm Description

Heavy-LSVI-UCB features a novel combination of adaptive Huber regression in Section 3 and existing algorithmic frameworks for linear MDPs with bounded rewards [22; 15]. At a high level, Heavy-LSVI-UCB employs separate estimation techniques to handle heavy-tailed rewards and transition kernels. Specifically, we utilize adaptive Huber regression proposed in Section 3 to estimate heavy-tailed rewards and weighted ridge regression [42; 15] to estimate the expected next-state value functions. Then, it follows the value iteration scheme to update the optimistic and pessimistic estimation of the optimal value function \(Q_{h}^{k}\), \(V_{h}^{k}\) and \(_{h}^{k}\), \(_{h}^{k}\), respectively, via a rare-switching policy as in Line 7 to 15 of Algorithm 3. We highlight the key steps of Heavy-LSVI-UCB as follows.

Estimation for expected heavy-tailed rewardsSince the expected rewards have linear structure in linear MDPs, i.e., \(r_{h}(s,a)=(s,a),_{h}^{*}\), we use adaptive Huber regression to estimate \(_{h}^{*}\):

\[_{k,h}=*{argmin}_{_{d}(B)} }{2}\|\|^{2}+_{i=1}^{k}_{_{i,h}} (-_{i,h},}{_{i,h}} ),\] (5.1)

where \(_{i,h}\) will be specified later.

Estimation for central moment of rewardsBy Assumption 2.5, the \((1+)\)-th central moment of rewards is linear in \(\), i.e., \([_{1+}R_{h}](s,a)=(s,a),_{h}^{*}\). Motivated by this, we estimate \(_{h}^{*}\) by adaptive Huber regression as

\[_{k,h}=*{argmin}_{_{d}(W)}}{2}\|\|^{2}+_{i=1}^{k}_{_{i,h}} (|^{1+}-_{i,h},}{_{i,h}}),\] (5.2)

where \(W\) is the upper bound of \(\|_{h}^{*}\|\) defined in Assumption 2.5. Since \(_{i,h}\) is intractable, we estimate it by \(_{i,h}=R_{i,h}-_{i,h},_{i,h}\), which gives \(}_{k,h}\) as

\[}_{k,h}=*{argmin}_{_{d}( W)}}{2}\|\|^{2}+_{i=1}^{k}_{_{i,h }}(_{i,h}|^{1+}-_{i,h},}{_{i,h}}).\] (5.3)

The inevitable error between \(_{k,h}\) and \(}_{k,h}\) can be quantified by a novel perturbation analysis of adaptive Huber regression in Appendix C.3.

[MISSING_PAGE_FAIL:8]

In addition, for any \(f,g:\), it holds that \(_{h}[f+g]=_{h}[f]+_{h}[g]\) and \(}_{k,h}[f+g]=}_{k,h}[f]+}_{k,h}[g]\) due to the linear property of integration and ridge regression.

We remark \(}_{k,h}[f]\) is the estimation of \(_{h}[f]\) by weighted ridge regression on \(\{_{i,h},f(s_{i,h+1})\}_{i[k]}\). And we estimate the coefficients \(}_{k,h},}_{k,h},}_{k,h}\)

\[}_{k,h}=}_{k,h}[V_{h+1}^{k}], }_{k,h}=}_{k,h}[_{h+1}^{k}],}_{k,h}=}_{k,h}[(V_{h+1}^{k})^{2}],\] (5.8)

where \(V_{h}^{k}\) and \(_{h}^{k}\) are optimistic and pessimistic estimation of the optimal value functions.

Estimation for variance of next-state value functionsInspired by He et al. , we set the weight \(_{k,h}\) for weighted ridge regression in (5.7) as

\[_{k,h}=\{_{k,h},HD_{k,h}},_{ },\|_{k,h}\|_{^{-1}_{k-1,h}},}H}\|_{k,h}\|_{^{-1}_{k-1,h}} ^{}\},\] (5.9)

where \(_{}\) is a small constant to avoid singularity, \(_{k,h}^{2}=[}_{h}V_{h+1}^{k}](s_{k,h},a_{k,h})+E_{k,h}\) with

\[[}_{h}V_{h+1}^{k}](s_{k,h},a_{k,h})=_{k,h},}_{k-1,h}_{[0,^{2}]}- {}_{k,h},}_{k-1,h}_{[0,]}^{2},\] (5.10)

\[E_{k,h}=\{4_{k,h},}_{k-1,h }-}_{k-1,h}+11_{0}\|_{k,h} \|_{^{-1}_{k-1,h}},^{2}\}.\] (5.11)

\[D_{k,h}=\{2_{k,h},}_{k-1,h }-}_{k-1,h}+4_{0}\|_{k,h} \|_{^{-1}_{k-1,h}},^{2}\},\] (5.12)

where \(_{0}=(H^{2}}/_{})\). Here \(_{k,h}^{2}\) and \(D_{k,h}\) are upper bounds of \([_{h}V_{h+1}^{*}](s_{k,h},a_{k,h})\) and \(\{[_{h}(V_{h+1}^{k}-V_{h+1}^{*})](s_{k,h},a_{k,h}) \},[_{h}(V_{h+1}^{*}-_{h+1}^{k})](s_{k,h},a_{k,h })\}\), respectively.

### Computational Complexity

**Theorem 5.1**.: For the linear MDPs with heavy-tailed rewards defined in Section 2.2, the computational complexity of Heavy-LSVI-UCB is \((d^{4}||H^{3}K+HK)\). Here \(\) is the cost of the optimization algorithm for solving adaptive Huber regression in (5.1). Furthermore, we can specialize \(\) by adopting the Nesterov accelerated method, which gives \(=(d+d^{-}H^{}K^{})\).

Proof.: See Appendix E for a detailed proof. 

Such a complexity allows us to focus on the complexity introduced by the RL algorithm rather than the optimization subroutine for solving adaptive Huber regression. Compared to that of LSVI-UCB++ , \((d^{4}||H^{3}K)\), the extra term \((HK)\) causes a slightly worse computational time in terms of \(K\). This is due to the absence of a closed-form solution of adaptive Huber regression in (5.1). Thus extra optimization steps are unavoidable. Nevertheless, Nesterov accelerated method gives \(=(K^{})\) with respect to \(K\), which implies the computational complexity of Heavy-LSVI-UCB is better than that of LSVI-UCB , \((d^{2}||HK^{2})\) in terms of \(K\), thanks to the rare-switching updating policy. We conduct numerical experiments in Appendix B to further corroborate the computational efficiency of adaptive Huber regression.

### Regret Bound

**Theorem 5.2** (Informal).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2, we set parameters in Algorithm 3 as follows: \(_{R}=d/\{B^{2},W^{2}\}\), \(_{V}=1/^{2}\), \(_{}\), \(_{}\), \(c_{0}\), \(c_{1}\), \(_{0}\), \(_{0}\), \(_{R}\), \(_{0}\), \(_{R}\), \(_{V}\) in Appendix F.1. Then for any \((0,1)\), with probability at least \(1-16\), the regret of Heavy-LSVI-UCB is bounded by

\[(K)=(d^{*}}K^{}+d^{*}K}),\]

where \((0,1]\), \(^{*}=\{_{0}^{*},\}\), \(^{*}=\{_{0}^{*},\}\) with \(_{0}^{*}\), \(_{0}^{*}\) defined in Appendix F.2 and \(,,\) defined in Assumption 2.7.

Proof.: See Appendix F.2 for a formal version of Theorem 5.2 and its detailed proof.

Quantities \(^{*}\), \(^{*}\)We make a few explanations for the quantities \(^{*}\), \(^{*}\). On one hand, \(^{*}\) is upper bounded by \(\), which is the upper bound of the sum of the \((1+)\)-th central moments of reward functions along a single trajectory. On the other hand, \(^{*}\) is no more than \(^{*}_{0}\), which is the sum of the \((1+)\)-th central moments with respect to the averaged occupancy measure of the first \(K\) episodes. \(^{*}\) is defined similar to \(^{*}\), but measures the randomness of transition probabilities.

**Remark 5.3**.: When \(=1\), we can show that this regret is bounded by \((d^{*}K})\), where \(^{*}\) is an variance-dependent quantity defined by Li and Sun . Thus, our result recovers their variance-aware regret bound. See Remark F.15 in Appendix F.2 for a detailed proof.

To demonstrate the optimality of our results and establish connections with existing literature, we can specialize Theorem 5.2 to obtain the worst-case regret  and first-order regret .

**Corollary 5.4** (Worst-case regret).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2 and for any \((0,1)\), with probability at least \(1-16\), the regret of Heavy-LSVI-UCB is bounded by

\[(dHK^{}+dK}).\]

Proof.: Notice \(^{*}\) and \(^{*}\) are upper bounded by \(H_{R}^{2}\) and \(^{2}\) (total variance lemma in Jin et al. ) respectively. When \(=H\), and we treat \(_{R}\) as a constant, the result follows. 

Next, we give the regret lower bound of linear MDPs with heavy-tailed rewards in Theorem 5.5, which shows our proposed Heavy-LSVI-UCB is minimax optimal in the worst case.

**Theorem 5.5**.: For any algorithm, there exists an \(H\)-episodic, \(d\)-dimensional linear MDP with heavy-tailed rewards such that for any \(K\), the algorithm's regret is

\[(dHK^{}+dK}).\]

Proof.: Intuitively, the proof of Theorem 5.5 follows from a combination of the lower bound constructions for heavy-tailed linear bandits in Shao et al.  and linear MDPs in Zhou et al. . See Appendix G for a detailed proof. 

Theorem 5.5 shows that for sufficiently large \(K\), the reward term dominates in the regret bound. Thus, in heavy-tailed settings, the main difficulty is learning the reward functions.

**Corollary 5.6** (First-order regret).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2 and for any \((0,1)\), with probability at least \(1-16\), the regret of Heavy-LSVI-UCB is bounded by

\[(d^{*}}K^{}+dV_{1}^{*}K}).\]

And when the rewards are uniformly bounded in \(\), the result is reduced to the first-order regret bound of \((dV_{1}^{*}K})\).

Proof.: See Section F.3 for a detailed proof. 

Our first-order regret \((dV_{1}^{*}K})\) is minimax optimal in the worst case since \(V_{1}^{*} H\). And it improves the state-of-the-art result \((dV_{1}^{*}K})\) by a factor of \(\).

## 6 Conclusion

In this work, we propose two computationally efficient algorithms for heavy-tailed linear bandits and linear MDPs, respectively. Our proposed algorithms, termed as Heavy-DFUL and Heavy-LSVI-UCB, are based on a novel self-normalized concentration inequality for adaptive Huber regression, which may be of independent interest. Heavy-DFUL and Heavy-LSVI-UCB achieve minimax optimal and instance-dependent regret bounds scaling with the central moments. We also provide a lower bound for linear MDPs with heavy-tailed rewards to demonstrate the optimality of Heavy-LSVI-UCB. To the best of our knowledge, we are the first to study heavy-tailed rewards in RL with function approximation and provide a new algorithm for this setting which is both statistically and computationally efficient.