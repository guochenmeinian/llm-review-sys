# Newton Losses: Using Curvature Information

for Learning with Differentiable Algorithms

 Felix Petersen

Stanford University

mail@felix-petersen.de &Christian Borgelt

University of Salzburg

christian@borgelt.net &Tobias Sutter

University of Konstanz

tobias.sutter@uni.kn &Hilde Kuehne

Tuebingen AI Center

MIT-IBM Watson AI Lab

h.kuehne@uni-tuebingen.de &Oliver Deussen

University of Konstanz

oliver.deussen@uni.kn &Stefano Ermon

Stanford University

ermon@cs.stanford.edu

###### Abstract

When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function's second-order information to replace it by a Newton Loss, while training the network with gradient descent. This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms.

## 1 Introduction

Traditionally, fully-supervised classification and regression learning relies on convex loss functions such as MSE or cross-entropy, which are easy-to-optimize in isolation. However, the need for large amounts of ground truth annotations is a limitation of fully-supervised learning; thus, weakly-supervised learning with non-trivial objectives  has gained popularity. Rather than using fully annotated data, these approaches utilize problem-specific algorithmic knowledge incorporated into the loss function via a continuous relaxation. For example, instead of supervising ground truth values, supervision can be given in the form of ordering information (ranks), e.g., based on human preferences . However, incorporating such knowledge into the loss can make it difficult to optimize, e.g., by making the loss non-convex in the model output, introducing bad local minima, and importantly leading to vanishing as well as exploding gradients, slowing down training .

Loss functions that integrate problem-specific knowledge can range from rather simple contrastive losses  to rather complex losses that require the integration of differentiable algorithms , . In this work, we primarily focus on the (harder) latter category, which allows for solving specialized tasks such as inverse rendering , learning-to-rank , self-supervised learning , differentiation of optimizers , and top-k supervision . In this paper, we summarize these loss functions under the umbrella of algorithmic losses  as they introduce algorithmic knowledge via continuous relaxations into the training objective.

While the success of neural network training is primarily due to the backpropagation algorithm and stochastic gradient descent (SGD), there is also a promising line of work on second-order optimization for neural network training . Compared to first-order methods like SGD, second-order optimization methods exhibit improved convergence rates and therefore require fewer training steps; however, they have two major limitations , namely (i) computing the inverse of the curvature matrix for a large and deep neural network is computationally substantially more expensive than simply computing the gradient with backpropagation, which makes second-order methods practically inapplicable in many cases ; (ii) networks trained with second-order information have been shown to exhibit reduced generalization capabilities .

Inspired by ideas from second-order optimization, in this work, we propose a novel method for incorporating second-order information into training with non-convex and hard to optimize algorithmic losses. Loss functions are usually cheaper to evaluate than a neural network. Further, loss functions operate on lower dimensional spaces than those spanned by the parameters of neural networks. If the loss function becomes the bottleneck in the optimization process because it is difficult to optimize, it suggests to use a stronger optimization method that requires fewer steps like second-order optimization. However, as applying second-order methods to neural networks is expensive and limits generalization, we want to train the neural network with first-order SGD. Therefore, we propose Newton Losses, a method for locally approximating loss functions with a quadratic with second-order Taylor expansion. Thereby, Newton Losses provides a (locally) convex loss leading to better optimization behavior, while training the actual neural network with gradient descent.

For the quadratic approximation of the algorithmic losses, we propose two variants of Newton Losses: (i) _Hessian-based Newton Losses_, which comprises a generally stronger method but requires an estimate of the Hessian . Depending on the choice of differentiable algorithm, choice of relaxation, or its implementation, the Hessian may, however, not be available. Thus, we further relax the method to (ii) _empirical Fisher matrix-based Newton Losses_, which derive the curvature information from the empirical Fisher matrix , which depends only on the gradients. The empirical Fisher variant can be easily implemented on top of existing algorithmic losses because it does not require to compute their second derivatives, while the Hessian variant requires computation of second derivatives and leads to greater improvements when available.

We evaluate Newton Losses for an array of eight families of algorithmic losses on two popular algorithmic benchmarks: the four-digit MNIST sorting benchmark  and the Warcraft shortest-path benchmark . We find that Newton Losses leads to consistent performance improvements for each of the algorithms--for some of the algorithms (those which suffer the most from vanishing and exploding gradients) more than doubling the accuracy.

## 2 Background & Related Work

The related work comprises algorithmic supervision losses and second-order optimization methods. To the best of our knowledge, this is the first work combining second-order optimization of loss functions with first-order optimization of neural networks, especially for algorithmic losses.

Algorithmic Losses.Algorithmic losses, i.e., losses that contain some kind of algorithmic component, have become quite popular in recent machine learning research. In the domain of recommender systems, early learning-to-rank works already appeared in the 2000s , , , but more recently Lee _et al._ proposed differentiable ranking metrics, and Swezey _et al._ proposed PiRank, which relies on differentiable sorting. For differentiable sorting, an array of methods has been proposed in recent years, which includes NeuralSort , SoftSort , Optimal Transport Sort , differentiable sorting networks (DSN) , and the relaxed Bubble Sort algorithm . Other works explore differentiable sorting-based top-k for applications such as differentiable image patch selection , differentiable k-nearest-neighbor , , top-k attention for machine translation , differentiable beam search methods , , survival analysis , and self-supervised learning . But algorithmic losses are not limited to sorting: other works have considered learning shortest-paths , , , , learning 3D shapes from images and silhouettes , learning with combinatorial solvers for NP-hard problems , learning to classify handwritten characters based on editing distances between strings , learning with differentiable physics simulations , and learning protein structure with a differentiable simulator , among many others.

Second-Order Optimization.Second-order methods have gained popularity in machine learning due to their fast convergence properties when compared to first-order methods . One alternative to the vanilla Newton's method are quasi-Newton methods, which, instead of computing an inverse Hessian in the Newton step (which is expensive), approximate this curvature from the change in gradients . In addition, a number of new approximations to the pre-conditioning matrix have been proposed in the literature, i.a., . While the vanilla Newton method relies on the Hessian, there are variants which use the empirical Fisher matrix, which can coincide in specific cases with the Hessian, but generally exhibits somewhat different behavior. For an overview and discussion of Fisher-based methods (including natural gradient descent), see .

## 3 Newton Losses

### Preliminaries

We consider the training of a neural network \(f(x;)\), where \(x^{n}\) is the vector of inputs, \(^{d}\) is the vector of trainable parameters and \(y=f(x;)^{m}\) is the vector of outputs. As per vectorization, \(=[x_{1},,x_{N}]^{}^{N n}\) denotes a set of \(N\) input data points, and \(=f(;)^{N m}\) denotes the neural network outputs corresponding to the inputs. Further, let \(:^{N m}\) denote the loss function, and let the "label" information be implicitly encoded in \(\). The reason for this choice of implicit notation is that, for many algorithmic losses, it is not just a label, e.g., it can be ordinal information between multiple data points or a set of encoded constraints. We assume the loss function to be twice differentiable, but also present an extension for only once differentiable losses, as well as non-differentiable losses via stochastic smoothing in the remainder of the paper.

Conventionally, the parameters \(\) are optimized using an iterative algorithm (e.g., SGD , Adam , or Newton's method ) that updates them repeatedly according to:

\[_{t} \;(f(;))\;=_{t-1}\,.\] (1)

However, in this work, we consider splitting this optimization update step into two alternating steps:

\[_{t}^{} \;()\;=f(;_{t-1})\,,\] (2a) \[_{t} \|_{t}^{}-f( ;)\|_{2}^{2}=_{t-1}\,.\] (2b)

More formally, this can also be expressed via a function \((\,\,,\,\,)\) that describes one update step (its first argument is the objective to be minimized, its second argument is the variable to be optimized, and its third argument is the starting value for the variable) as follows:

\[_{t} \;(\;(f(;)),, _{t-1}\;)\] (3)

And, for two update step functions \(_{1}\) and \(_{2}\), we can formalize (2) to

\[_{t}^{} _{1}(\,(),, f( ;_{t-1})\,)\,,\] (4a) \[_{t} _{2}(\|_{t}^{}-f( ;)\|_{2}^{2},\;,\;_{t-1}\,)\,.\] (4b)

The purpose of the split is to enable us to use two different iterative optimization algorithms \(_{1}\) and \(_{2}\). This is particularly interesting for optimization problems where the optimization of the loss function \(\) is a difficult optimization problem. For standard convex losses like MSE or CE, gradient descent is a perfectly sufficient choice for \(_{1}\) (MSE will recover the goal, and CE leads to outputs \(^{}\) that achieve a perfect argmax classification result). However, if there is the asymmetry of \(\) being harder to optimize (requiring more steps), while (4a) being much cheaper per step compared to (4b), then the optimization of the loss (4a) comprises a bottleneck compared to the optimization of the neural network (4b). Such conditions are prevalent in the space of algorithmic supervision losses.

A similar split (for the case of splitting between the layers of a neural network, and using gradient descent for both (2a) and (2b), i.e., the requirement of \(_{1}=_{2}\)) is also utilized in the fields of biologically plausible backpropagation  and proximal backpropagation , leading to reparameterizations of backpropagation. For SGD, we show that (3) is exactly equivalent to (4) in Lemma 2, and for a special case of Newton's method, we show the equivalence in Lemma 3 in the SM. Motivated by the equivalences under the split, in the following, we consider the case of \(_{1}_{2}\)

### Method

Equipped with the two-step optimization (2) / (4), we can introduce the idea behind Newton Losses:

_We propose \(_{1}\) to be Newton's method, while \(_{2}\) remains stochastic gradient descent._

In the following, we formulate how we can solve optimizing (2a) with Newton's method, or, whenever we do not have access to the Hessian of \(\), using a step pre-conditioned via the empirical Fisher matrix. This allows us to transform an original loss function \(\) into a Newton loss \(^{*}\), which allows optimizing \(^{*}\) with gradient descent only while maintaining equivalence to the two-step idea, and thereby making it suitable for common machine learning frameworks.

Newton's method relies on a quadratic approximation of the loss function at location \(}=f(;)\)

\[_{}}()\ =\ (})+( -})^{}_{}}(})+(-})^{}\,_{}}^{2} (})\,(-})\,,\] (5)

and sets its derivative to \(0\) to find the location \(^{}\) of the stationary point of \(_{}}()\):

\[_{^{}}_{}}(^{ })=0\ \ \ \ _{}}(})+_{}}^{2} (})(^{}-})=0\ \ \ \ ^{}=}-(_{}}^{2}(}))^{-1}_{}(}).\] (6)

However, when \(\) is non-convex or the smallest eigenvalues of \(_{}}^{2}(})\) either become negative or zero, this \(^{}\) may not be a good proxy for a minimum of \(\), but may instead be any other stationary point or lie far away from \(}\), leading to exploding gradients downstream. To resolve this issue, we introduce Tikhonov regularization  with a strength of \(\), which leads to a well-conditioned curvature matrix:

\[^{}=}-(_{}}^{2}(})+)^{-1}\,_{}}( })\,.\] (7)

Using \(^{}\), we can plug the solution into (2b) to find the Newton loss \(^{*}\) and compute its derivative as

\[^{*}_{^{}}()=(^{}- )^{}(^{}-)= ^{}-_{2}^{2} _{}^{*}_{^{}}()=-^ {}\,.\] (8)

Here, as in Section 3.1, \(=f(,)\). Via this construction, we obtain the Newton loss \(^{*}_{^{}}\), a new convex loss, which itself has a gradient that corresponds to one Newton step of the original loss. In particular, on \(\), one gradient descent step on the Newton loss (8) reduces to

\[\ \ \ \ -_{}^{*}_{ ^{}}()=-(-^{ })\ \ =\ \ -(_{}^{2}()+ )^{-1}\,_{}()\,,\] (9)

which is exactly one step of Newton's method on \(\). Thus, we can optimize the Newton loss \(^{*}_{^{}}(f(;))\) with gradient descent, and obtain equivalence to the proposed concept.

In the following definition, we summarize the resulting equations that define the Newton loss \(^{*}_{^{}}\).

**Definition 1** (Newton Losses (Hessian)).: _For a loss function \(\) and a given current parameter vector \(\), we define the Hessian-based Newton loss via the empirical Hessian as_

\[^{*}_{^{}}()=\|^{}- \|_{2}^{2} z^{}_{i}=_{i}-( _{j=1}^{N}_{_{j}}^{2}(})+ )^{-1}_{_{i}}(})\] (10)

_for all \(\ i\{1,...,N\}\) and \(\ }=f(;)\)._

We remark that computing and inverting the Hessian of the loss function is usually computationally efficient. (We remind the reader that the Hessian of the loss function is the second derivative wrt. the inputs of the loss function and we further remind that the inputs to the loss are **not** the neural network parameters / weights.) Whenever the Hessian matrix of the loss function is not available, whether it may be due to limitations of a differentiable algorithm, large computational cost, lack of a respective implementation of the second derivative, etc., we may resort to using the empirical Fisher matrix (i.e., the second uncentered moments of the gradients) as a source for curvature information. We remark that the empirical Fisher matrix is not the same as the Fisher information matrix , and that the Fisher information matrix is generally not available for algorithmic losses. While the empirical Fisher matrix, as a source for curvature information, may be of lower quality than the Hessian matrix, it has the advantage that it can be computed from the gradients, i.e.,

\[=_{x}[_{f(x,)}\,(f(x,)) _{f(x,)}\,(f(x,))^{}].\] (11)

This means that, assuming a moderate dimension of the prediction space \(m\), computing the empirical Fisher comes at no significant overhead and may, conveniently, be performed in-place as we discuss later. Again, we regularize the matrix via Tikhonov regularization with strength \(\) and can, accordingly, define the empirical Fisher-based Newton loss as follows.

```
#Pythonstylepseudo-code model=...#neuralnetwork loss=...#originallossfn optimizer=...#optim.ofmodel tik_l=...#hyperparameter fordata,labelindata_loader:#applyaneuralnetworkmodel y=model(data) #computegradientoforig.loss grad=gradient(loss(y,label),y) #computeHessian(oralt.Fisher) hess=hessian(loss(y,label),y)
#computetheprojectedoptimum z_star=(y-grad@inverse(hess +tik_l*eye(g.shape))).detach() #computetheNewtonloss l=MSELoss()(y,z_star) #backpropagateandoptim.step l.backward() optimizer.step() ```

**Algorithm 1** Training with a Newton Loss

**Definition 2** (Newton Loss (Fisher)).: _For a loss function \(\), and a given current parameter vector \(\), we define the empirical Fisher-based Newton loss as_

\[_{^{*}}^{*}()=\|^{*}- \|_{2}^{2} z_{i}^{*}=_{i}-( _{j=1}^{N}_{_{j}}(})\,_{_{j}} (})^{}+)^{-1}\!_{_{i }}(})\]

_for all \(i\{1,...,N\}\) and \(}=f(;)\)._

Before continuing with the implementation, integration, and further computational considerations, we can make an interesting observation. In the case of using the trivial MSE loss, i.e., \((y)=\|y-y^{*}\|_{2}^{2}\) where \(y^{*}\) denotes a ground truth, the Newton loss collapses to the original MSE loss. This illustrates that Newton Losses requires non-trivial original losses. Another interesting aspect is the arising fixpoint--the Newton loss of a Newton loss is equivalent to a simple Newton loss.

### Implementation

After introducing Newton Losses, in this section, we discuss aspects of implementation and illustrate its implementations in Algorithms 1 and 2. Whenever we have access to the Hessian matrix of the algorithmic loss function, it is generally favorable to utilize the Hessian-based approach (Algo. 1 / Def. 1), whereas we can utilize the empirical Fisher-based approach (Algo. 2 / Def. 2) in any case.

```
#implementstheFisher-basedNewton
#lossviaaninjectedmodification
#ofthebackwardpass: classInjectFisher(AutoGradFunction): defforward(ctx,x,tik_l): assertlen(x.shape)==2 ctx.tik_l=tik_l returnx defbackward(ctx,g): fisher=g.T@g*g.shape input_grad=g@inverse(fisher +ctx.tik_l*eye(g.shape)) returninput_grad,None fordata,labelindata_loader:#applyaneuralnetworkmodel y=model(data) #injecttheFisherbackwardmod. y=InjectFisher.apply(y,tik_l) #computetheoriginalloss l=loss(y,label) #backpropagateandoptim.step l.backward() optimizer.step() ```

**Algorithm 2** Training with InjectFisher

In Algorithm 1, the difference to regular training is that we use the original loss only for the computation of the gradient (grad) and the Hessian matrix (hess) of the original loss. Then we compute \(^{*}\)(z_star). Here, depending on the automatic differentiation framework, we need to ensure not to backpropagate through the target z_star, which may be achieved, e.g., via ".detach()" or ".stop_gradient()", depending on the choice of library. Finally, the Newton loss l may be computed as the squared / MSE loss between the model output y and z_star and an optimization step on l may be performed. We note that, while we use a label from our data_loader, this label may be empty or an abstract piece of information for the differentiable algorithm; in our experiments, we use ordinal relationships between data points as well as shortest-paths on graphs.

In Algorithm 2, we show how to apply the empirical Fisher-based Newton Losses. In particular, due to the empirical Fisher matrix depending only on the gradient, we can compute it in-place during the backward pass / backpropagation, which makes this variant particularly simple and efficient to apply. This can be achieved via an injection of a custom gradient right before applying the original loss,which replaces the gradient in-place by a gradient that corresponds to Definition 2. The injection is performed by the InjectFisher function, which corresponds to an identity during the forward pass but replaces the gradient by the gradient of the respective empirical Fisher-based Newton loss.

In both cases, the only additional hyperparameter to specify is the Tikonov regularization strength \(\) (tik_l). \(\) heavily depends on the algorithmic loss function, particularly, on the magnitude of gradients provided by the algorithmic loss, which may vary drastically between different methods and implementations. Other factors may be the choice of Hessian / Fisher, the dimension of outputs \(m\), the batch size \(N\). Notably, for very large \(\), the direction of the gradient becomes more similar to regular gradient descent, and for smaller \(\), the effect of Newton Losses increases. We provide an ablation study for \(\) in Section 4.3.

## 4 Experiments1

For the experiments, we apply Newton Losses to eight methods for differentiable algorithms and evaluate them on two established benchmarks for algorithmic supervision, i.e., problems where an algorithm is applied to the predictions of a model and only the outputs of the algorithm are supervised. Specifically, we focus on the tasks of ranking supervision and shortest-path supervision because they each have a range of established methods for evaluating our approach. In ranking supervision, only the relative order of a set of samples is known, while their absolute values remain unsupervised. The established benchmark for differentiable sorting and ranking algorithms is the multi-digit MNIST sorting benchmark [2; 5; 11; 37; 40]. In shortest-path supervision, only the shortest-path of a graph is supervised, while the underlying cost matrix remains unsupervised. The established benchmark for differentiable shortest-path algorithms is the Warcraft shortest-path benchmark [21; 22; 24]. As these tasks require backpropagating through conventionally non-differentiable algorithms, the respective approaches make the ranking or shortest-path algorithms differentiable such that they can be used as part of the loss.

### Ranking Supervision

In this section, we explore ranking supervision  with an array of differentiable sorting-based losses. Here, we use the four-digit MNIST sorting benchmark , where sets of \(n\) four-digit MNIST images are given, and the supervision is the relative order of these images corresponding to the displayed value, while the absolute values remain unsupervised. The goal is to learn a CNN that maps each image to a scalar value in an order preserving fashion. As losses, we use sorting supervision losses based on the NeuralSort , the SoftSort , the logistic Differentiable Sorting Network , and the monotonic Cauchy DSN . _NeuralSort_ and _SoftSort_ work by mapping an input list (or vector) of values to a differentiable permutation matrix that is row-stochastic and indicates the order / ranking of the inputs. _Differentiable Sorting Networks_ offer an alternative to NeuralSort and SoftSort. DSNs are based on sorting networks, a classic family of sorting algorithms that operate by conditionally swapping elements. By introducing perturbations, DSNs relax the conditional swap operator to a differentiable conditional swap and thereby continuously relax the sorting and ranking operators. We discuss the background of each of these diff. sorting and ranking algorithms in greater detail in Supplementary Material B.

Setups.The sorting supervision losses are cross-entropy losses defined between the differentiable permutation matrix produced by a respective differentiable sorting operator and the ground truth permutation matrix corresponding to a ground truth ranking. The Cauchy DSN may be an exception to the hard to optimize classification as it is quasi-convex . We evaluate the sorting benchmark for numbers of elements to be ranked \(n\{5,10\}\) and use the percentage of rankings correctly identified

Figure 1: Overview over ranking supervision with a differentiable sorting / ranking algorithm. A set of input images is (elementwise) processed by a CNN, producing a scalar for each image. The scalars are sorted / ranked by the differentiable ranking algorithm, which returns the differentiable permutation matrix, which is compared to the ground truth permutation matrix.

as well as percentage of individual element ranks correctly identified as evaluation metrics. For each of the four original baseline methods, we compare it to two variants of their Newton losses: the empirical Hessian and the empirical Fisher variant. For each setting, we train the CNN on 10 seeds using the Adam optimizer  at a learning rate of \(10^{-3}\) for \(10^{5}\) steps and batch size of \(100\).

Results.As displayed in Table 1, we can see that--for each original loss--Newton Losses improve over their baselines. For NeuralSort, SoftSort, and Logistic DSNs, we find that using the Newton losses substantially improves performance. Here, the reason is that these methods suffer from vanishing and exploding gradients, especially for the more challenging case of \(n=10\). As expected, we find that the Hessian Newton Loss leads to better results than the Fisher variant, except for NeuralSort and SoftSort in the easy setting of \(n=5\), where the results are nevertheless quite close. Monotonic differentiable sorting networks, i.e., the Cauchy DSNs, provide an improved variant of DSNs, which have the property of quasi-convexity and have been shown to exhibit much better training behavior out-of-the-box, which makes it very hard to improve upon the existing results. Nevertheless, Hessian Newton Losses are on-par for the easy case of \(n=5\) and, notably, improve the performance by more than \(1\%\) on the more challenging case of \(n=10\). To explore this further, we additionally evaluate the Cauchy DSN for \(n=15\) (not displayed in the table): here, the baseline achieves \(30.84 2.74\)\((82.30 1.08)\), whereas, using NL (Fisher), we improve it to \(32.30 1.22\)\((82.78 0.53)\), showing that the trend of increasing improvements with more challenging settings (compared to smaller \(n\)) continues. Summarizing, we obtain strong improvements on losses that are hard to optimize, while in already well-behaving cases the improvements are smaller. This perfectly aligns with our goal of improving performance on losses that are hard to optimize.

### Shortest-Path Supervision

In this section, we apply Newton Losses to the shortest-path supervision task of the \(12 12\) Warcraft shortest-path benchmark . Here, \(12 12\) Warcraft terrain maps are given as \(96 96\) RGB images (e.g., Figure 2 left) and the supervision is the shortest path from the top left to the bottom right (Figure 2 right) according to a hidden cost embedding (Figure 2 center). The hidden cost embedding is not available for training. The goal is to predict \(12 12\) cost embeddings of the terrain maps such that the shortest path according to the predicted embedding corresponds to the ground truth shortest path. Vlastelica et al.  have shown that integrating an algorithm in the training pipeline substantially improves performance compared to only using a neural network with an easy-to-optimize loss function, which has been confirmed by subsequent work . For this task, we explore a set of families of algorithmic supervision approaches: _Relaxed Bellman-Ford_ is a shortest-path algorithm relaxed via the AlgoVision framework, which continuously relaxes algorithms by perturbing all accessed variables with logistic distributions and approximating the expectation value in closed form. _Stochastic Smoothing_ is a sampling-based differentiation method that can be used to relax, e.g., a shortest-path algorithm by perturbing the input with probability distribution. _Perturbed Optimizers with Fenchel-Young Losses_ build on stochastic smoothing and Fenchel-Young losses  and identify the argmax to be the differential of max, which allows a simplification of stochastic smoothing, again applied, e.g., to shortest-path

    &  &  &  &  \\  Baseline & \(71.33 2.05\) & \((87.10 0.96)\) & \(70.70 6.20\) & \((86.75 1.26)\) & \(53.56 1.80\) & \((77.04 1.03)\) & \(85.09 0.77\) & \((93.31 0.39)\) \\ NL (Hessian) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \((93.31 0.34)\) \\ NL (Fisher) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \((93.25 0.37)\) \\    &  &  &  &  \\  Baseline & \(24.26 0.152\) & \((74.47 0.83)\) & \(27.46 3.58\) & \((76.02 1.92)\) & \(12.31 10.22\) & \((58.81 16.79)\) & \(55.29 2.46\) & \((87.06 0.85)\) \\ NL (Hessian) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ NL (Fisher) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Ranking supervision with differentiable sorting. The metric is the percentage of rankings correctly identified (and individual element ranks correctly identified, in parentheses) avg. over \(10\) seeds. Statistically significant improvements (sig. level \(0.05\)) are indicated bold black; improved means are indicated in bold grey.

Figure 2: \(12 12\) Warcraft shortest-path problem. An input terrain map (left), unsupervised ground truth cost embedding (center) and ground truth supervised shortest path (right).

learning problems. We use the same hyperparameters as shared by previous works . In particular, it is notable that, throughout the literature, the benchmark assumes a training duration of \(50\) epochs and a learning rate decay by a factor of \(10\) after \(30\) and \(40\) epochs each. Thus, we do not deviate from these constraints.

#### 4.2.1 Relaxed Bellman-Ford

The relaxed Bellman-Ford algorithm  is a continuous relaxation of the Bellman-Ford algorithm via the AlgoVision library. To increase the number of settings considered, we explore four sub-variants of the algorithm: For+\(L_{1}\), For+\(L_{2}^{2}\), While+\(L_{1}\), and While+\(L_{2}^{2}\). Here, For / While refers to the distinction between using a While and For loop in Bellman-Ford, while \(L_{1}\) vs. \(L_{2}^{2}\) refer to the choice of metric between shortest paths. As computing the Hessian of the AlgoVision Bellman-Ford algorithm is too expensive with the PyTorch implementation, for this evaluation, we restrict it to the empirical Fisher-based Newton loss. The results displayed in Table 2. While the differences are rather small, as the baseline here is already strong, we can observe improvements in all of the four settings and in one case achieve a significant improvement. This can be attributed to (i) the high performance of the baseline algorithm on this benchmark, and (ii) that only the empirical Fisher-based Newton loss is available, which is not as strong as the Hessian variant.

#### 4.2.2 Stochastic Smoothing

After discussing the analytical relaxation, we continue with stochastic smoothing approaches. First, we consider stochastic smoothing , which allows perturbing the input of a function with an exponential family distribution to estimate the gradient of the smoothed function. For a reference on stochastic smoothing with a focus on differentiable algorithms, we refer to the author's recent work . For the baseline, we apply stochastic smoothing to a hard non-differentiable Dijkstra algorithm based loss function to relax it via Gaussian noise ("SS of loss"). We utilize variance reduction via the method of covariates. As we detail in Supplementary Material B.4, stochastic smoothing can also be used to estimate the Hessian of the smoothed function. Based on this result, we can construct the Hessian-variant Newton loss. As an extension to stochastic smoothing, we apply stochastic smoothing only to the non-differentiable Dijkstra algorithm (thereby computing its Jacobian matrix) but use a differentiable loss to compare the predicted relaxed shortest-path to the ground truth shortest-path ("SS of algorithm"). In this case, the Hessian Newton loss is not applicable because the output of the smoothed algorithm is high dimensional and the Hessian of the loss becomes intractable. An extended discussion of the "SS of algorithm" formulation can be found in SM B.4.1. Nevertheless, we can apply the Fisher-based Newton loss. We evaluate both approaches for \(3\), \(10\), and \(30\) samples.

In Table 3, we can observe that Newton Losses improves the results for stochastic smoothing in each case with more than \(3\) samples. The reason for the poor performance on \(3\) samples is that the Hessian or empirical Fisher, respectively, is estimated using only \(3\) samples, which makes the estimate unstable. For \(10\) and \(30\) samples, the performance improves compared to the original method. In Figure 3, we display a respective accuracy plot. When comparing "SS of loss" and "SS of algorithm", we can observe that the extension to smoothing only the algorithm improves performance for at least \(10\) samples. Here, the reason, again, is that smoothing the algorithm itself requires estimating the Jacobian instead of only the gradient; thus, a larger number of samples is necessary; however starting at \(10\) samples, smoothing the algorithm performs better, which means that the approach is better at utilizing a given sample budget.

   Variant & For+\(L_{1}\) & For+\(L_{2}^{2}\) & While+\(L_{1}\) & While+\(L_{2}^{2}\) \\  Baseline & \(94.19 0.33\) & \(95.90 0.21\) & \(94.30 0.20\) & \(95.77 0.41\) \\ NL (Fisher) & **94.52\(\)0.34** & \(96.08 0.46\) & \(94.47 0.34\) & \(95.94 0.27\) \\   

Table 2: Shortest-path benchmark results for different variants of the AlgoVision-relaxed Bellman-Ford algorithm . The metric is the percentage of perfect matches averaged over \(10\) seeds. Significant improvements are bold black, and improved means are bold grey.

Figure 3: Test accuracy (perfect matches) plot for ‘SS of loss’ with \(10\) samples on the Warcraft shortest-path benchmark. Lines show the mean and shaded areas show the 95% conf. intervals.

#### 4.2.3 Perturbed Optimizers with Fenchel-Young Losses

Perturbed optimizers with a Fenchel-Young loss  is a formulation of solving the shortest path problem as an \(\) problem, and differentiating this problem using stochastic smoothing-based perturbations and a Fenchel-Young loss. By extending their formulation to computing the Hessian of the Fenchel-Young loss, we can compute the Newton loss, and find that we can achieve improvements of more than \(2\%\). However, for Fenchel-Young losses, which are defined via their derivative, the empirical Fisher is not particularly meaningful, leading to equivalent performance between the baseline and the Fisher Newton loss. Berthet _et al_.  mention that their approach works well for small numbers of samples, which we can confirm as seen in Table 3 where the accuracy is similar for each number of samples. An interesting observation is that perturbed optimizers with Fenchel-Young losses perform better than stochastic smoothing in the few-sample regime, whereas stochastic smoothing performs better with larger numbers of samples.

### Ablation Study

In this section, we present our ablation study for the (only) hyperparameter \(\). \(\) is the strength of the Tikhonov regularization (see, e.g., Equation 7, or tik_l in the algorithms). This parameter is important for controlling the degree to which second-order information is used as well as regularizing the curvature. For the ablation study, we use the experimental setting from Section 4.1 for NeuralSort and SoftSort and \(n=5\). In particular, we consider \(13\) values for \(\), exploring the range from \(0.001\) to \(1000\) and plot the element-wise ranking accuracy (individual element ranks correctly identified) in Figure 4. We display the average over \(10\) seeds as well as each seed's result individually with low opacity. We can observe that Newton Losses are robust over many orders of magnitude for the hyperparameter \(\). Note the logarithmic axis for \(\) in Figure 4. In general, we observe that choices within a few orders of magnitude around \(1\) are generally favorable. Further, we observe that NeuralSort is more sensitive to drastic changes in \(\) compared to SoftSort.

### Runtime Analysis

We provide tables with runtimes for the experiments in Supplementary Material D. We can observe that the runtimes between the baseline and empirical Fisher-based Newton Losses are indistinguishable for all cases. For the analytical relaxations of differentiable sorting algorithms, where the

Figure 4: Ablation study wrt. the Tikhonov regularization strength hyperparameter \(\). Displayed is the element-wise ranking accuracy (individual element ranks correctly identified), averaged over \(10\) seeds, and additionally each seed with low opacity in the background. **Left**: NeuralSort. **Right**: SoftSort. Each for \(n=5\). Newton Losses, and for both the Hessian and the Fisher variant, significantly improve over the baseline for up to (or beyond) 6 orders of magnitude in variation of its hyperparameter \(\). Note the logarithmic horizontal axis.

   Method &  &  &  \\  \# Samples & 3 & 10 & 30 & 3 & 10 & 30 & 3 & 10 & 30 \\  Baseline & \(\) & \(77.01 2.18\) & \(85.48 1.23\) & \(\) & \(78.70 1.90\) & \(87.26 1.50\) & \(80.64 0.75\) & \(80.39 0.57\) & \(80.71 2.28\) \\ NL (Hessian) & \(62.40 5.48\) & \(\) & \(85.94 1.33\) & & & & & \(\) & \(\) & \(\) \\ NL (Fisher) & \(58.80 1.0\) & \(\) & \(\) & \(53.82 8.45\) & \(\) & \(\) & \(\) & \(80.37 0.98\) & \(80.45 0.78\) \\   

Table 3: Shortest-path benchmark results for the stochastic smoothing of the loss (including the algorithm), stochastic smoothing of the algorithm (excluding the loss), and perturbed optimizers with the Fenchel-Young loss. The metric is the percentage of perfect matches averaged over \(10\) seeds. Significant improvements are bold black, and improved means are bold grey.

computation of the Hessian can become expensive with automatic differentiation (i.e., without a custom derivation of the Hessian and without vectorized Hessian computation), we observed overheads between \(10\%\) and \(2.6\). For all stochastic approaches, we observe indistinguishable runtimes for Hessian-based Newton Losses. In summary, applying the Fisher variant of Newton Losses has a minimal computational overhead, whereas, for the Hessian variant, any overhead depends merely on the computation of the Hessian of the algorithmic loss function. While, for differentiable algorithms, the neural network's output dimensionality or algorithm's input dimensionality \(m\) is typically moderately small to make the inversion of the Hessian or empirical Fisher cheap, when the output dimensionality \(m\) becomes very large such that inversion of the empirical Fisher becomes expensive, we refer to the Woodbury matrix identity , which allows simplifying the computation via its low-rank decomposition. A corresponding deviation is included in SM F. Additionally, solver-based inversion implementations can be used to make the inversion more efficient.

## 5 Conclusion

In this work, we focused on weakly-supervised learning problems that require integration of differentiable algorithmic procedures in the loss function. This leads to non-convex loss functions that exhibit vanishing and exploding gradients, making them hard to optimize. We proposed a novel approach for improving performance of algorithmic losses building upon the curvature information of the loss. For this, we split the optimization procedure into two steps: optimizing on the loss itself using Newton's method to mitigate vanishing and exploding gradients, and then optimizing the neural network with gradient descent. We simplified this procedure via a transformation of an original loss function into a Newton loss, which comes in two flavors: a Hessian variant for cases where the Hessian is available and an empirical Fisher variant as an alternative. We evaluated Newton Losses on a set of algorithmic supervision settings, demonstrating that the method can drastically improve performance for weakly-performing differentiable algorithms. We hope that the community adapts Newton Losses for learning with differentiable algorithms and see great potential for combining it with future differentiable algorithms in unexplored territories of the space of differentiable relaxations, algorithms, operators, and simulators.