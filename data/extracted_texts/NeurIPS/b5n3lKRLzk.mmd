# emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation

Sasha Salter1, Richard Warren1, Collin Schlager1, Adrian Spurr, Shangchen Han, Rohin Bhasin2, Yujun Cai, Peter Walkington, Anuoluwapo Bolarinwa, Robert Wang, Nathan Danielson, Josh Merel2, Effychios Pnevmatikakis, and Jesse Marshall

###### Abstract

Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement; existing sEMG models have thus required hundreds of users and device placements to effectively generalize for tasks other than pose inference. To facilitate progress on sEMG pose inference, we introduce the _emg2pose benchmark_, which is to our knowledge the first publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. _emg2pose_ contains 2kHz, \(16\) channel sEMG and pose labels from a 26-camera motion capture rig for \(193\) users, \(370\) hours, and \(29\)_stages_ with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: _held-out users, sensor placements_, and _stages_. This benchmark provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.

Reality Labs, Meta

Figure 1: We introduce the _emg2pose_ dataset and benchmark to facilitate the development of pose estimation models from sEMG. Our _vemg2pose_ model is capable of estimating in real-time hand pose (lower) from held-out users wearing an sEMG wristband (top). See text for further details.

Introduction

Despite rapid progress in computing hardware and software, current input devices can be inefficient and non-intuitive for new and emerging computing platforms. This is particularly evident for spatial interactions, such as those encountered in virtual and augmented reality, where conventional input devices like controllers, keyboards, and mice do not offer the necessary level of intuitive use across the population (requiring extensive practice for proficiency) nor sufficient bandwidth to enable precise control (e.g., object manipulation). Interactions based on hand movements offer a high-dimensional continuous input that is instinctive, universal, and particularly well suited to spatial interactions. Furthermore, existing inputs can be viewed as low dimensional summaries of hand movements (e.g. a mouse click tells you that a finger has pressed a button). As such, hand kinematics is a potentially holistic and encompassing modality, covering existing inputs and extending them in a natural manner. High fidelity hand tracking enables various AR/VR applications including gaming (Han et al., 2020), virtual teaching (Shrestha et al., 2022), teleoperations (Santos Carreras, 2012; Darvish et al., 2023), haptics (Scheggi et al., 2015), embodied realism (Wang et al., 2020), sports analytics (Gatt et al., 2020), and healthcare and rehabilitation (Krasoulis et al., 2017).

Given the high utility and broad appeal of effective hand pose estimation, there have been diverse approaches developed across many sensing modalities from: optical approaches (e.g. monocular, multi-view, depth-based, motion capture, infrared) using fixed (Cai et al., 2018; Mueller et al., 2018; Ge et al., 2016; Supancic et al., 2018; Park et al., 2020) or head-mounted cameras (Han et al., 2018); wearable data gloves using magnetic (Parizi et al., 2019), inertial (Yang et al., 2021), capacitative (Truong et al., 2018), and stretch sensors (Shen et al., 2016; Tashakori et al., 2024; Luo et al., 2021); smart rings (Parizi et al., 2019); wrist and forearm wearables that use impedance tomography (Zhang and Harrison, 2015), inertial measurement units (Laput and Harrison, 2019), acoustics (Laput et al., 2016) or ultrasound (McIntosh et al., 2017). Each modality comes with its own hardware constraints and limitations. Optical approaches can struggle with occlusions, poor lighting conditions, and limited field of view, and often require multiple cameras for effective inference, which places constraints on the overall size of the device. On the other hand, glove wearables can hinder dexterous manipulation (Roda-Sales et al., 2020) and forearm wearables typically only support discrete gesture classification.

Surface electromyography (sEMG) sensing on the wrist or forearm provides an appealing alternative that does not struggle with occlusion, field of view, poor lighting, or physical encumberance. sEMG uses electrodes on the skin to measure electrical potentials generated by muscles during movement (Stashuk, 2001). Specifically, sEMG detects the electrical activity that occurs when spinal motor neurons activate the muscle fibers that drive motion (Merletti and Farina, 2016). As such, sEMG is particularly well suited for kinematic inference and numerous approaches have been developed (Liu et al., 2021; Quivira et al., 2018; Sosin et al., 2018; Simpetru et al., 2022). Nevertheless, learning a universal sEMG-to-pose model that _generalizes_ to new participants and kinematics is particularly challenging. This is due to sEMG sensing containing many axes of variation, primarily: _user anatomy_, _sensor placement_, and _hand kinematics_(CTRL-labs at Reality Labs et al., 2024; Liu et al., 2021). User anatomy and sensor placement both influence the locations of the sensors relative to the muscles. Hand kinematics influence what combination of muscle activities are sensed. Given the number of generative dimensions, sEMG models are particularly data-hungry (CTRL-labs at Reality Labs et al., 2024), necessitating many samples across these axes to effectively learn universal models that generalize (see Section 4.4 experiments). Existing datasets are not open sourced and are relatively small (<20 participant) and brief (<20 minutes per participant), thus hindering the development of generic models (Liu et al., 2021; Simpetru et al., 2022).

Another complication of sEMG is that it encodes muscle activity, which relates more closely to motion than the pose that we would like to recover. As such, direct pose inference from sEMG is particularly challenging (see Section 4), potentially requiring reasoning over long historical sEMG sequences to disambiguate pose from sequences of indirect motion measurements. Extracting relevant information from long sequences, or contexts, in the presence of ambiguity has been extensively explored in fields such as CV (Brunetti et al., 2018; Kirillov et al., 2023; Pan et al., 2018), natural language (Achiam et al., 2023; Kojima et al., 2022; Gu et al., 2021), and robotics (Lauri et al., 2022; Dunion et al., 2024; Jang et al., 2022). Despite this, prior sEMG works have shown promising results for personalized or single-user pose inference settings (Liu et al., 2021; Simpetru et al., 2022).

To facilitate progress toward developing universal sEMG-to-pose models, we introduce the _emg2pose benchmark_ dataset, a large-scale dataset of simultaneously recorded high-fidelity wrist sEMG record ings and hand pose labels. High resolution sEMG recordings are obtained with the sEMG-RD wrist band [CTRL-labs at Reality Labs et al., 2024](see Section 3.1) and high precision pose labels are obtained from a 26-camera motion capture rig that offers benefits compared to multi-view computer vision [Liu et al., 2021, Sosin et al., 2018]. To our knowledge, this is the only publicly-available wrist-based sEMG hand pose dataset, spanning \(193\) users, \(370\) hours, and \(29\) diverse kinematic categories, called _stages_, each containing diverse low-level behaviors, called _gestures_. In addition, the \(80\)M labelled frames that our dataset contains compares favourably with even the newest and largest CV equivalents [Sener et al., 2022, Yu et al., 2020] in both number of frames as well as subjects (see Table 2). We additionally provide three competitive baselines and challenging hand pose inference benchmarks, investigating generalization to unseen _users_, _stages_, and _user-stage_ combinations. Instructions regarding accessing and using the _emg2pose benchmark_ is provided in https://github.com/facebookresearch/emg2pose. Given the high potential impact of sEMG input devices, and the similar research challenges to existing fields, we believe this benchmark will be of great value to the machine learning community.

## 2 Related Work

sEMG Datasets:There are several publicly available sEMG datasets for tasks other than pose regression, specifically pose (sequence) classification. Data have been collected with either clinical-grade high-density electrode arrays and amplifiers [Amma et al., 2015, Du et al., 2017, Malesevic et al., 2021, Jiang et al., 2021] or the MyoBand, a consumer-grade hardware that has fewer channels and lower temporal resolution [Atzori et al., 2012, Pizzolato et al., 2017, Lobov et al., 2018]. Clinical-grade hardware offers hundreds of recording channels and acquisition rates >1 kHz but are impractical due to lengthy donning procedures that include shaving the skin before applying conductive gel and the electrode arrays. In contrast, existing consumer-grade hardware is easier to deploy, but is limited by low bandwidth (200 Hz) and channel counts (8) and thus may not provide the level of fidelity required for pose estimation. In contrast, our dataset uses the sEMG-RD band [CTRL-labs at Reality Labs et al., 2024], that can be quickly donned, can record 16 channels at >2 kHz and has proven performant for generalized pose classification modelling.

Of the aforementioned datasets, most only include single recording sessions per subject [Atzori et al., 2012, Pizzolato et al., 2017, Lobov et al., 2018, Malesevic et al., 2020, Malesevic et al., 2021], limiting the ability to develop models that generalize across device placements. Palermo et al. , Amma et al. , Du et al. , Jiang et al.  include \(10,5,23,20\) subjects and up to \(10,5,3,2\) sessions per subject, respectively. Our dataset includes \(193\) users and \(751\) sessions, allowing us train models that generalize favourably across these axes (see Table 1, reporting upper bound statistics for Du et al. ). In Table 1, _# gestures_ represents the number of pose classification categories. Category definitions may vary significantly across datasets and thus comparisons should be taken with a pinch-of-salt. Our dataset contains _gesture_ categories as well as joint angles.

Pose Regression from sEMG:Several papers have studied pose regression from sEMG, although without open sourcing datasets. Liu et al.  use the MyoBand to estimate hand pose across diverse movements in an 11 participant dataset. They test sEMG decoding models of hand pose across users and sessions with both convolutional (NeuroPose; see Section 3.5) and LSTM architectures. Simpetru et al. [2022a] (SensingDynamics; see Section 3.5) use a clinic-grade system to collect several dozen minute datasets in a set of 13 participants. They use a custom 3D convolutional architecture to predict hand joint angles, landmark positions, and grip force, reporting tracking with low error in a held-out test set within each participant. Existing datasets have been limited in scale, with only 11 or 13 participants, and 15 or 20 minutes of data per participant for Liu et al. , Simpetru et al. [2022a], respectively, likely limiting generalization across users. In contrast, our dataset includes \(193\) users and \(370\) hours, aiding the development of generic models that generalize across users (see Section 4.4).

Pose from Computer Vision:Computer vision (CV) based hand pose estimation has received considerable attention in recent years, usually taking depth, RGB, or both as input, and leveraging large open-sourced datasets [Mueller et al., 2017, 2018, Spurr et al., 2018, 2020, 2021, Wan et al.,

   Dataset & \# Sess. & \# Subj. & \# Secs. / subj. & \# Get. & Inc. Pose \\  Palermo et al.  & 100 & 10 & 10 & 7 & No \\ Amma et al.  & 25 & 5 & 5 & 27 & No \\ Du et al.  & 69 & 23 & 3 & 22 & No \\ Jiang et al.  & 40 & 20 & 2 & 34 & No \\  Ours & 751 & 193 & 4 & 50 & Yes \\   

Table 1: The largest publicly available sEMG datasets2019; Boukhayma et al., 2019). Labels are either obtained using marker-based motion capture (Fan et al., 2023) - whose markers create an input distributional shift due to lack of markers during deployment - or using alternate approaches with lower quality labels or inputs, such as multi-view cameras (Zimmermann et al., 2019; Moon et al., 2024), synthetic data (Zimmermann and Brox, 2017), and magnetic sensors (Yuan et al., 2017). In contrast, motion capture markers afford high quality labels for sEMG, but they do not affect the data from which predictions are generated.

The gestural diversity of CV-based datasets mostly focuses on exploring the full static pose space of the hand (Yuan et al., 2017; Zimmermann and Brox, 2017; Zimmermann et al., 2019), interaction with objects (Fan et al., 2023; Samarth et al., 2020; Hampali et al., 2020) or hand-hand interactions (Moon et al., 2020, 2024). Conversely, our dataset focuses on movements of the hand because sEMG, unlike CV, is more closely related to motion than pose. Furthermore, our dataset has 80M frames and 193 subjects, comparing favorably to CV datasets (see Table 2, reporting million frames, subjects and fps).

**Pose from Other Modalities:** In addition to vision and sEMG, there exists a diverse range of additional wearable approaches to pose inference (see Section 1), which typically focus on pose (sequence) classification. For example, Achenbach et al. (2023) released a dataset for pose classification using commercially available sensor gloves. Other datasets typically use bespoke hardware and are small in scale, with the exception of a large (50 participant, 25 class) dataset available for classification using commercially available smartwatches (Laput and Harrison, 2019).

## 3 emg2pose Benchmark

### sEMG Device

Data are collected using the 16 channel bipolar sEMG-RD wrist band from CTRL-labs at Reality Labs et al. (2024). They demonstrate the effectiveness of this device for generalized pose sequence classification across \(6400\) participants, the largest study to date. This high performance is achieved without the need for high-density sEMG platforms (Amma et al., 2015), with a similar form factor and ease of use to other low-density platforms (Rawat et al., 2016) (see Figs. 1 and 2 for a visual depiction of the device). In contrast to the previously used low-density Thalmic Labs Myo band (Liu et al., 2021) that streams data at \(200\)Hz, across \(8\) channels and with \(8\)-bits, sEMG-RD senses at \(2\)kHz, across \(16\) channels and with \(12\)-bits. For more details see Appendix B.1.

### Dataset

Consenting participants (see Appendix A) stood in a \(26\) camera motion capture array (Appendix B.2). A research assistant placed \(19\) motion capture markers on each of the participants' hands (Han et al. (2018)) and an sEMG-RD band on each wrist (CTRL-labs at Reality Labs et al., 2024). All sEMG and motion capture data were streamed to a real-time data acquisition system at \(2\)kHz and \(60\) Hz, respectively. We time-aligned device streams using software timestamps, which we found to show less than \(10\)ms relative latency between devices. Motion capture data were post-processed using

  Dataset & \# Frames & \# Subjects & \# FPS \\  Yuan et al. (2017) & 2.2M & 10 & 60 \\ Moon et al. (2020) & 2.6M & 27 & 5-30 \\ Moon et al. (2024) & 1.5M & 10 & 5-30 \\ Samarth et al. (2020) & 2.9M & 50 & n/a \\ Fan et al. (2023) & 2.1M & 10 & 30 \\ Liu et al. (2022) & 2.4M & 4 & n/a \\ Sener et al. (2022) & 111M & 53 & n/a \\ Yu et al. (2020) & 24M & 453 & 60 \\  Ours (per hand) & 80M & 193 & 60 \\ Ours (across hands) & 40M & 193 & 60 \\  

Table 2: Largest CV hand datasets. The _per hand_ row counts the data from the left and right hands independently, whereas the _across hands_ row pools those data.

   & Train &  &  & Overall \\  & & _User_ & _User_ & _Stage_ & _User_ & _User_ & _Stage_ & \\  Subjects & 158 & 15 & 15 & 20 & 158 & 20 & 193 \\ Unique stages & 23 & 23 & 6 & 23 & 6 & 6 & 29 \\ Hours & 250.9 & 21.7 & 4.6 & 31.9 & 54.2 & 7.0 & 370.3 \\ Hours / subject & 1.6 \(\) 0.4 & 1.4 \(\) 0.5 & 0.3 \(\) 0.1 & 1.6 \(\) 0.3 & 0.3 \(\) 0.1 & 0.3 \(\) 0.0 & 1.9 \(\) 0.5 \\ Sessions / subject & 3.9 \(\) 0.6 & 3.8 \(\) 0.6 & 3.7 \(\) 0.6 & 3.9 \(\) 0.3 & 3.8 \(\) 0.7 & 3.8 \(\) 0.5 & 3.9 \(\) 0.6 \\  

Table 3: _emg2pose_ dataset statistics, reporting mean and standard deviation. Three separate test sets measure generalization to new users, types of behaviors (stages), and user-behavior combinations (user, stage). Note that the overall hours is the sum of the hours across all splits. The number of hours counts the right-handed and left-handed data separately for each participant.

an offline inverse kinematics (IK) solver to reconstruct the joint angles of the hand (Appendices A and B.2). The IK solver failed for \(12.7\%\) of frames, typically due to simultaneously occluded markers. Finally, joints angles were linearly interpolated to 2 kHz to match the sample rate of sEMG.

Participants followed a standardized data collection protocol across a diverse set of 45-120 s _stages_ in which participants were prompted to perform either a mix of 3-5 similar _gestures_ in random orderings (e.g. specific _finger counting_ orderings such as _ascending_ or _descending_) or unconstrained freeform movements (see Appendices A and B.3 for further details). _Stages_ can be viewed as a categorization of _gestures_. For example, the _Counting_ stage categorizes _Counting_ Up and _Counting Down_ gestures (see Fig. 2). During data collection, the majority of users donned on-and-off the device 4 times, with a small fraction only thrice. Each group of stages with a single band placement is referred to as a _session_. We report the prompted movements for each stage in detail in Appendix B.3. During each stage, we prompted participants using videos and verbal instructions by the research assistant. Participants were instructed to move their hands across their body and between their waist and shoulders to ensure a range of different postures were sampled. See Fig. 2 for a visualization of the data collection.

The full dataset is organized hierarchically by participant, session, and stage. In total, we collected data from \(193\) participants, spanning \(370\) hours, \(751\) sessions, \(29\) diverse stages (see Appendix B.3 for further details and Table 3 for statistics). Note that the number of hours counts the right-handed and left-handed data separately for each participant, although they were collected simultaneously. To our knowledge, this is the only open-sourced sEMG and motion capture dataset and is of similar scale to those in the CV literature (Yuan et al., 2017; Brahmbhatt et al., 2020; Moon et al., 2020, 2024). The entire dataset consists of \(25,253\) HDF5 files, each consisting of time-aligned sEMG and joint angles for a single hand in a single stage.

### Tasks

The _emg2pose benchmark_ includes two benchmark tasks: _pose regression_ and _pose tracking_.

**Regression:** For this task, previously explored in Liu et al. (2021); Simpetru et al. (2022), one must regress from sEMG to hand joint angle sequences. Without knowledge of the initial hand pose and

Figure 2: _Dataset composition_: a) sEMG-RD wrist-band and motion capture marker (white dots) setup. b) Dataset breakdown. i) Users are prompted to perform a sequence of movement types (_gestures_), such as counting up and down. sEMG and poses are recorded simultaneously. ii) Groups of specific gesture types comprise a _stage_, such as counting. Stages are partitioned into train/val/test splits (see Section 3.4). Our dataset consists of \(29\) diverse stages. iii) Each of the \(193\) users perform various stages, donning on-and-off the wrist band. In total we record \(370\) hours of data.

velocity, this is a partially observable task (Spaan, 2012), and thus particularly challenging for the reasons mentioned in Section 1. Pose regression is the most challenging task and is meant to promote continued research with applications including unimodal pose prediction in settings where computer vision is infeasible or unreliable.

**Tracking:** For this simpler task, one must regress from sEMG to hand joint angle sequences whilst being provided with the initial hand pose in the sequence. Providing the initial pose addresses the partial observability dilemma. Nevertheless, this task still poses the generalization challenges discussed in Section 1. The tracking task is meant to promote initial research and progress, and has several real-world applications. An effective tracker would provide great value in settings where: the user is prompted to match a given pose before tracking commences; visual pose prediction feedback is provided, allowing the user to adjust their pose to correct for erroneous initial predictions, and when ground truth pose estimates are intermittently available, such as from computer vision settings whenever partial or full occlusions occur.

**Evaluation:** We evaluate on \(5\) second trajectories and report test set mean absolute _joint angular error_ (\({}^{}\)) and mean (Euclidean) _landmark distance_ (mm). Landmarks correspond to joint and fingertip Cartesians. We do not regress to wrist angles, which were not recorded for this dataset. Landmarks corresponding to the most proximal joint for fingers other than the thumb always have zero error because the wrist does not move. These landmarks are therefore excluded from our metrics. We obtain landmark locations by passing joint angles through a default hand model. This introduces bias, as it will not perfectly align with each user's anatomy. We leave addressing this limitation for future work. In real world applications, it will be important to not only improve mean performance for these metrics, but also lower percentile scores across the population.

### Held-Out Settings

Effective pose inference requires models that generalize across _device placements_, _users_, and _hand kinematics_. Prior works have only investigated generalization across a subset of these axes, such as user (Liu et al., 2021, CTRL-labs at Reality Labs et al., 2024) or device placement (Liu et al., 2021, Palermo et al., 2017), but generalization to new types of kinematics has not been explicitly explored. In contrast, we provide three separate test sets intended to measure these axes independently. The statistics of each held-out scenario are reported in Table 3. In short, _users_ corresponds to unseen users, but in-distribution kinematics (_stages_). _Stages_ represents unseen kinematic categories, but in-distribution users. Finally, _users, stages_ constitute held-out users and stages, and is of greatest value as the most encompassing real-world deployment setting. Both held out user scenarios all constitute new device placements, which vary across all sessions. We break down train, validation and test splits roughly using \(0.7:0.1:0.2\) ratio with exact splits shown in Table 3. Held-out users are randomly sampled and held-out stages are chosen to be visually out-of-distribution with respect to the training stages. See Fig. 3 for a breakdown of which stages are in the training and held-out sets, Table 6 for details regarding each stage, and Appendix B.2.1 for further dataset details.

### Baselines

We provide three baselines: open-source re-implementations of the _NeuroPose_ and _SensingDynamics_ network architectures (Liu et al., 2021, Simpetru et al., 2022), and a new _vemg2pose_ model. Algorithm details can be found in Appendix C.

**vemg2pose:** sEMG meaures underlying muscle activity, and therefore relates more strongly to hand movements than the static pose of the hand. Therefore, _vemg2pose_ ("Velocity-based emg2pose") predicts joint angular velocities, which are then integrated to produce joint angle predictions. sEMG is first embedded via a causal strided convolutional _featurizer_, which temporally down-samples sEMG from 2 kHz to 50 Hz. A _Time-Depth Separable Convolution_ (TDS) network is used for the featurizer, as it has been shown to be effective and parameter-efficient in the automatic speech recognition literature (Hannun et al., 2019) (see Appendix C for implementation details). The features at each time-step are then concatenated to the joint angle predictions at the previous time step and fed to an LSTM _decoder_, which produces the next velocity prediction. Those velocities are added to the previous joint angles to produce the next prediction. vemg2pose is therefore auto-regressive with respect to its own predictions. Finally, predictions are linearly up-sampled to match the sample rate of the joint angles targets. For the tracking task, the initial joint angles are set to the ground truth,according to the motion capture labels. For the regression task, the initial state is also predicted by the decoder (see Appendix C for further details).

**NeuroPose:** NeuroPose and vemg2pose differ in their prediction spaces and network architectures. Whereas vemg2pose predicts angular velocities, NeuroPose predicts joint angles directly. NeuroPose uses a U-Net architecture with residual bottleneck layers. Briefly, a convolutional encoder spatially and temporally down-samples sEMG while extracting features which are then refined via a stack of residual blocks. Finally, a decoder generates pose predictions at the original sample rate via convolutions and up-sampling layers. Because our sEMG device measures at 10x the temporal frequency and 2x the spatial frequency of the MyoBand used in Liu et al. (2021), we increase the temporal and spatial down and up-sampling of NeuroPose's featurizer and decoder (by 8x and 2x, respectively), such that the receptive field remains comparable to the original model. See Liu et al. (2021) for full model details and Appendix C for further details.

**SensingDynamics:** SensingDynamics and NeuroPose primarily differ in their architectures. Instead of a U-Net, SensingDynamics' featurizer comprises of 2d convolutions over sEMG channels and time, with learnable SMU activations (Biswas et al., 2021), batch normalisation, circular padding across channels, and dropout layers. The decoder comprises of a 3-layered MLP. Uniquely, SensingDynamics additionally passes 20Hz low-passed filtered sEMG as input to the featurizer. See Simpetru et al. (2022) for full model details and Appendix C for further details.

**Training Setup:** All algorithms are trained to minimize the L1 error between predicted and ground truth joint angles as well as the Euclidean error between between predicted and ground truth fingertip locations. The joint angle loss term has a weight of \(1\) and the fingertip loss term has a weight of \(.01\). We train on 1-6 seconds of non-overlapping trajectories. The training trajectory length - in addition to other hyperparameters - is optimized independently for each algorithm (see Table 7). We train for 500 epochs with a 50 epoch early stopping criterion. Time-points for which motion capture data are not available are skipped during training and evaluation. We use a batch size of \(64\) per GPU. We train on Amazon EC2 g5.48xlarge instances which have \(8\)x NVIDIA T4 GPUs for less than a day.

## 4 Experiments

### Benchmark Results

We report _regression_ results in Table 4 and _tracking_ results in Table 5. We do not report standard deviation across model seeds, as we observed these to be negligible. Results are further broken down by stage, finger, and joint in Figs. 3, 10 and 11, respectively. For the regression task, vemg2pose outperforms both NeuroPose and SensingDynamics with respect to both angular errors and landmark distances. In general, accuracy degrades most for the held-out _user, stage_ combination, which is the hardest of all transfer scenarios. For the tracking task - in which the initial ground truth pose is provided - errors are lower overall, as expected (see Section 3.3). For this task, we do not report

    &  \\  Test Set & Baseline & Angular Error (\({}^{}\)) & Landmark Distance (\(mm\)) \\   & SensingDynamics & 15.5 \(\) 1.4 & 21.8 \(\) 2.1 \\  & NeuroPose & 13.2 \(\) 1.1 & 17.5 \(\) 1.3 \\  & vemg2pose & **12.2 \(\) 1.3** & **15.8 \(\) 1.9** \\  Stage & SensingDynamics & 18.8 \(\) 1.6 & 26.6 \(\) 2.0 \\  & NeuroPose & 17.2 \(\) 1.7 & 24.0 \(\) 2.1 \\  & vemg2pose & **15.2 \(\) 1.6** & **20.4 \(\) 2.2** \\  User, Stage & SensingDynamics & 18.7 \(\) 1.6 & 27.2 \(\) 2.0 \\  & NeuroPose & 17.5 \(\) 1.5 & 24.9 \(\) 1.7 \\  & vemg2pose & **15.8 \(\) 1.4** & **21.6 \(\) 2.0** \\  

Table 4: _Regression_ test set results. Mean and standard deviation are reported across users. Bold indicates the significance of a Wilcoxon signed-rank test comparing vemg2pose to NeuroPose and Sensing Dynamics for each metric and condition (.01 threshold adjusted to.0008 via Bonferroni correction, see Appendix C.7 for details).

scores for NeuroPose and SensingDynamics, as these models were not originally designed to leverage knowledge of the initial ground truth pose during inference.

Performance varies considerably across users for all models and tasks, potentially due to anatomical differences (Tables 4 and 5). Performance varies significantly across stages (Fig. 3), which is likely a result of the amount and type of movements in each stage. Stages with limited movement (StaticHands, WristFlex) may be easier for the model track because they involve very limited postural transitions. Stages with complex hand poses and dynamic articulation of individual fingers (Gesture2, Pointing) are more challenging and have higher errors. Moreover, Fig. 10 shows that performance varies significantly across fingers and finger joints, with the thumb the most reliably predicted, followed by the index, middle, ring, and pinky fingers. We also find that proximal joint angles of the fingers are easier to track than distal joint angles (Fig. 11). Together, this suggests that stages with high amounts of thumb movements (e.g. ThumbRotations) may be easier to track than those with more general finger movements (e.g. Freestyle1).

### Analysis on Challenging Stages for Vision-Based Systems

Some stages were specifically designed to test behaviors that are known to be challenging for vision-based hand pose estimation (see Appendix D.1 for details). We found that stages with hand-hand

Figure 4: vmg2pose tracking results _with/without occlusion (left) and physical interactions (right)_. Distributions are over users. See Appendix D.1 for more details.

   Test Set & Baseline & Angular Error (\({}^{}\)) & Landmark Distance (\(mm\)) \\  User & vmg2pose & 7.7 \(\) 1.0 & 10.3 \(\) 1.5 \\  Stage & vmg2pose & 11.2 \(\) 1.4 & 15.2 \(\) 1.9 \\  User, Stage & vmg2pose & 11.0 \(\) 1.0 & 15.4 \(\) 1.4 \\   

Table 5: _Tracking_ test set results. Mean and standard deviation are reported across users.

Figure 3: _veng2pose tracking performance break down_ by stage and generalization condition. Distributions are over users. Note the variability in performance across stages. Each box shows the median and interquartile range (IQR), and whiskers show the minimum and maximum values that are within 1.5 times the IQR of the lower and upper quartiles.

interactions or hand-object interactions have similar model performance compared to stages without such interactions (Fig. 4, right), although differences in behavioral distribution across these stages makes direct comparison challenging. Furthermore, we find that visual occlusion does not impact sEMG based pose reconstruction, as expected. Stages in which the hand is occluded from a CV based headset tracking system have similar accuracy compared to stages without occlusion in which the same behaviors are performed (Fig. 4, left).

### Qualitative Analysis

We plot _vemg2pose, tracking_ real-time online and offline kinematic predictions for _held-out users and stages_ in Figs. 1 and 5 (see Appendix C.5 for online setup details). This is the most challenging scenario, representing generalization to held-out kinematics, user anatomy, and device placement. For Fig. 5, we plot a median-performance representative held-out stage (Counting2, see Table 6) and user. As seen, individual finger movements are mostly tracked, but not always. We visualize top and bottom percentile (15% and 85%) offline kinematics for the held-out users and stages generalization setting in Figs. 12 to 15. In general, we observed three challenges specific to sEMG pose inference: angular drift due to sensing that strongly relates to pose derivatives (see the ring finger in Fig. 15); movements related to harder-to-sense intrinsic hand muscles, such as the finger adduction/abduction present in the "vulcan" gesture (Table 6); movements related to smaller and fewer muscles, such as pinky (see Fig. 10) and distal joint motion (see Fig. 1).

### Dataset Scale Analysis

We ran experiments to _demonstrate the importance of the scale_ of our dataset for effective generalization. In Fig. 6, we show that increasing the number of training users considerably reduces the error for held-out users, perhaps because models are exposed to sEMG from users with a variety of wrist anatomies. We also show that increasing the number of stages per-user improves performance across all modes of generalization, demonstrating the importance of behavioural diversity.

### Quantifying Generalization Difficulty across Users and Stages

To directly quantify the _difficulty of generalizing_ across held-out _stages_ and _users_, we performed experiments in which a subset of the data from the held-out users and stages were either folded into the training set or excluded entirely. Fig. 7 shows that excluding specific users and stages from the training set markedly degrades performance, demonstrating the difficulty of generalizing across these dimensions. Refer to Fig. 7 for a detailed description of the experimental setup.

Figure 5: _Median percentile held-out user and stage_ (Counting2). Top: motion capture; bottom: vemg2pose, tracking predictions. Clips unroll evenly left-to-right over a \(2\) second segment.

Figure 6: _Generalization vs. number of training users (left two) or stages (right three) for vemg2pose tracking_. We subsampled the training users/stages but evaluated on the same held-out users/stages. As seen, performance improves with the number of training users/stages, demonstrating the importance of our dataset scale for effective generalization. Box plots take the same format as Fig. 3.

## 5 Limitations and Future Work

Modelling:We provide an initial investigation into generalized sEMG-to-pose modelling and open-source our baselines to the community. Nevertheless, there remains a plethora of unexplored, potentially fruitful sequence modelling directions, such as state space and diffusion-based methods. Pose estimation in the presence of uncertainty introduced by sensor noise and anatomical variability could also be addressed with probabilistic methods . Model personalization has also been shown to be beneficial [CTRL-labs at Reality Labs et al., 2024, Liu et al., 2021], yet we do not explore this avenue here. In addition, our models obtain mean landmark distance errors that are higher than reported in the CV literature [19, Mueller et al., 2017], despite having the advantage of not having to infer the wrist position or user's anatomy. Addressing this performance gap will be of great importance. Finally, the lack of broader access to the sEMG-RD wrist-band [CTRL-labs at Reality Labs et al., 2024] might be limiting, as this precludes human-in-the-loop testing of models.

Metrics:Our landmark distance metrics use a default hand model to convert joint angles to joint positions. The mismatch between the hand model and user anatomy will bias this metric. In general, our metrics do not capture the physical plausibility of model predictions. For example, we have observed that vemg2pose sometimes predicts unfeasible kinematics, such as intra-finger penetration. Providing metrics that capture these failure modes will be of value, especially for embodied applications . Simulators of the hand  could be leveraged in a manner similar to Yuan et al.  to ensure physical constraints are adhered to. Finally, our held-out _user, stage_ test scenario is meant to best represent real-world in the wild performance. Nevertheless, it does not cover a potential range of signal aggressors such as: electrode-skin contact artifacts; impedance changes from sweat; electrical interference from external devices; and non-stationarity due to muscle fatigue. While these aggressors likely play a minor role in sEMG variability, they may be important to include in future datasets and test sets.

Dataset:We discuss dataset limitations in Appendix B.4.

Ethical and Societal Implications:We discuss ethical and societal implications in Appendix B.5.

## 6 Conclusion

We introduce the _emg2pose benchmark_, the first large, diverse, and open-source dataset of high-fidelity sEMG recordings and hand pose labels. We introduce competitive benchmark models that can track or regress to hand pose for held-out users, stages and sessions, although there remains significant room to improve these models in future research. Due to the myriad sources of variability in sEMG signals, deciphering the relationship between sEMG and movement in a manner that generalizes across people and kinematics will likely require new algorithmic advances, taking inspiration from related machine learning fields. Large datasets like _emg2pose_ should thus facilitate progress in both sEMG decoding and machine learning applied to biosignals more broadly. Progress will enable intuitive, high-dimensional human-computer interfaces that we perceive as extensions of ourselves.

Figure 7: Excluding stages (left) or users (right) from the training set _markedly decreases performance for these stages/users_. For the _include stages/users_ condition, we include \(70\)% of the data from the held-out stages/users in the training set. For the _exclude stages/users_ condition we exclude that \(70\)% entirely. Both test sets are identical allowing us to isolate the influence of holding out stages/users. Data are from a tracking task with a vemg2pose model. Distributions are over users.

Acknowledgements

We thank Patrick Kaifosh and TR Reardon for their sponsorship and vision and the entire CTRL-labs team for their collaboration and support. We thank Carl Hewitt and Migmar Tsering for help with data collection, Steve Olsen and Mark Hogan for assistance setting up motion capture recordings, John Choi and Diogo Peixoto for technical assistance and advice, and Dano Morrison and Sunaina Rajani for assistance with visualizations.