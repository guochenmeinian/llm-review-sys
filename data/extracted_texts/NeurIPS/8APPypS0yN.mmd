# On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks

Paolo Pellizzoni

Max Planck Institute of Biochemistry

Martinsried, Germany

pellizzoni@biochem.mpg.de

&Till Hendrik Schulz

Max Planck Institute of Biochemistry

Martinsried, Germany

tschulz@biochem.mpg.de

&Dexiong Chen

Max Planck Institute of Biochemistry

Martinsried, Germany

dchen@biochem.mpg.de

&Karsten Borgwardt

Max Planck Institute of Biochemistry

Martinsried, Germany

borgwardt@biochem.mpg.de

These authors jointly supervised this work.

###### Abstract

Graph neural networks (GNNs) employing message passing for graph classification are inherently limited by the expressive power of the Weisfeiler-Leman (WL) test for graph isomorphism. Node individualization schemes, which assign unique identifiers to nodes (e.g., by adding random noise to features), are a common approach for achieving universal expressiveness. However, the ability of GNNs endowed with individualization schemes to generalize beyond the training data is still an open question. To address this question, this paper presents a theoretical analysis of the sample complexity of such GNNs from a statistical learning perspective, employing Vapnik-Chervonenkis (VC) dimension and covering number bounds. We demonstrate that node individualization schemes that are permutation-equivariant result in lower sample complexity, and design novel individualization schemes that exploit these results. As an application of this analysis, we also develop a novel architecture that can perform substructure identification (i.e., subgraph isomorphism) while having a lower VC dimension compared to competing methods. Finally, our theoretical findings are validated experimentally on both synthetic and real-world datasets.

## 1 Introduction

Graph Neural Networks (GNNs) have become a dominant approach in graph learning, leveraging the inherent structural information of graphs through multiple iterations of message passing. This iterative exchange of information between nodes allows GNNs to learn rich representations for both nodes and entire graphs. However, the expressivity of these models in the context of graph classification, i.e., their ability to distinguish non-isomorphic graphs, is intrinsically limited by the capabilities of the Weisfeiler-Leman (1-WL) algorithm , a heuristic used for the graph isomorphism problem . 1-WL is known to fail to distinguish certain classes of graphs, such as regular graphs . Notably, GNNs are in general unable to solve the substructure identification task , which is concerned with identifying subgraph patterns in the data at hand.

To address these expressivity limitations, higher-order GNNs  have been proposed. These models consider interactions among larger subgraphs or sets of nodes but often suffer from highcomputational complexity. An alternative approach, which we call _node individualization schemes_, entails introducing perturbations, such as random noise, into node features to break graph symmetries artificially. This technique enables message passing GNNs to become universal function approximators  without the computational overhead associated with higher-order GNNs. Several node individualization schemes have been proposed in the literature. Random Node Initializations (RNI)  add random noise to node labels to make them distinct with high probability, while Relational Pooling (RP)  and Colored Local Iterative Procedure (CLIP)  partition nodes based on their labels and assign unique identifiers within partitions. A recent paper  proposes a general framework based on individualization-refinement algorithms. Other approaches to increase the expressivity of GNNs relabel the graphs with structural or positional node encodings, such as node centralities, graphlet counts , Laplacian [16; 62; 28], and random walk  encodings. These encodings give information about the graph topology, complementing the information obtained from message passing, but cannot guarantee universal expressivity in general.

Beyond expressivity, the sample complexity of GNNs [36; 54; 23], i.e., the amount of training data required for generalization on unseen data, is a critical consideration to obtain effective graph learning models, and it has been recently linked to the 1-WL expressivity . Nonetheless, the sample complexity of GNNs with node individualization schemes and with structural or positional node encodings, which is of paramount importance in the understanding of the generalization power of maximally expressive GNNs, remains an unanswered question.

This paper presents a novel theoretical analysis of both the expressivity and sample complexity of GNNs endowed with node individualization and structural or positional node encodings, bridging the gap between the expressivity analysis of GNNs and statistical learning theory. Specifically, we examine the sample complexity of various node individualization schemes in the binary graph classification setting, offering insights that can guide the design of more effective graph learning models. We make the following contributions:

* We demonstrate the utility of node individualization schemes even on graphs recognizable by the 1-WL test, thereby enhancing the expressivity of shallow GNNs (Section 3).
* We analyze the sample complexity of GNNs with various node individualization schemes using VC-dimension-based bounds (Theorem 1). Based on our analysis, we develop a model architecture EGGNN that is universally expressive for substructure identification and has low VC dimension (Section 4.2).
* We provide a sharper analysis based on covering-number bounds, leading to the design of novel node individualization schemes (Theorem 4). Our bounds rely on a novel covering bound for Lipschitz functions , a result that is potentially of independent interest.
* We substantiate our theoretical findings with experimental results on both synthetic and real-world datasets (Section 5).

By offering a comprehensive analysis and practical guidance, this paper aims to enhance the understanding and development of GNNs through the lens of node individualization, pushing the boundaries of what these models can achieve in terms of expressivity and sample efficiency.

### Related work

Expressivity of GNNsSince the seminal works of [41; 64] revealed the limitations of MPNNs due to their expressivity being bounded by the 1-WL test, research has surged toward developing more powerful GNNs. One prominent approach has been to design GNNs that emulate higher-order WL  or Folklore-WL  tests, exemplified by k-GNNs  and k-FGNNs . However, their computational and memory demands often hinder real-world applicability. To mitigate this, researchers have explored more efficient alternatives, leveraging graph locality and sparsity [40; 67; 22]. Another line of research involves subgraph GNNs, which break symmetry by transforming the original graph into modified subgraphs for GNN processing [14; 48; 8]. For a more comprehensive overview, readers are encouraged to consult [51; 39].

Node individualizationsIndividualization schemes have been proposed by several works [52; 42; 15; 20; 11] to enhance the expressivity of GNNs. GNNs with such schemes have been shown to be universal function approximators for invariant functions [1; 5]. Individualization schemes are also crucial on edge-level tasks [34; 53], where they break symmetry among different edge-orbits.

Sample complexityThe generalization capabilities of restricted families of GNNs has been addressed by means of VC dimension , Rademacher complexity  and PAC-bayesian approaches [29; 33]. Recently, [37; 21] proposed VC-dimension bounds based on the number of 1-WL color classes. The sample complexity of frequent subgraph identification has been addressed in . Moreover,  and address the sample complexity of equivariant models using a covering-number-based approach [6; 25], which is exploited also in  in an empirical setting. For a general overview on statistical learning theory, consult .

## 2 Preliminaries

In what follows, we define a graph as a tuple \(G=(V_{G},E_{G},L_{G})\), with \(V_{G}=\{1,,|V_{G}|\}\) a finite set of nodes, and \(E_{G}\{\{u,v\}:\:u v V_{G}\}\) a set of undirected edges. We define the vertex-label function as \(L_{G}:V_{G}\), with a finite set of labels \(\). For the sake of simplicity, we consider edges to be unlabeled. We define the neighborhood of a node as \((v)=\{w V_{G}:\{v,w\} E_{G}\}\). We say that two graphs \(G\) and \(H\) are isomorphic, denoted as \(G H\), if there exists a bijective mapping \(:V_{G} V_{H}\), called isomorphism, such that \(L_{G}(v)=L_{H}((v)),\: v V_{G}\) and \(\{(u),(v)\} E_{H}\) if and only if \(\{u,v\} E_{G}\). The isomorphism relation induces equivalence classes, which we call _unordered_ graphs . The group of isomorphisms from \(G\) to itself is called the automorphism group \(Aut(G)\). We denote a set of graphs with \(\), and if necessary, as \(_{}\) to explicate the label set. We denote sets of unordered graphs as quotient sets \(/\). Moreover, we consider sets of graphs of bounded size, that is \(|V_{G}| n_{}, G\). A function defined on graphs \(f:\) is called invariant if for any \(G H\), \(f(G)=f(H)\). A function from graphs to graphs \(f:^{}\) is here called equivariant if for any \(G H\), \(f(G) f(H)\).

### The Weisfeiler-Leman and Tinhofer algorithms

The color refinement algorithm, also known as 1-Weisfeiler-Leman (denoted as WL) algorithm, is a heuristic algorithm for the graph isomorphism problem. Let \(C_{0}(v)=L_{G}(v)\) be the initial color of node \(v V_{G}\). Then the algorithm updates vertex colors at iteration \(k>0\) as

\[C_{k}(v)=(C_{k-1}(v),\{\{C_{k-1}(w):w( v)\}\}),\]

with \(\) an injective map. Let \(^{(k)}(G)\) be the partitioning of \(V_{G}\) based on colors after the \(k\)-th iteration of color refinement. Then, after at most \(|V_{G}|\) iterations, the color partition stabilizes, i.e. \(^{(k)}(G)=^{(k+1)}(G)\), \( k|V_{G}|\). Two graphs are deemed \(k\)-hop WL-isomorphic if \(\{\{C_{k}(v):v V_{G}\}\}=\{\{C_{k}(v):\:v V_{H}\}\}\), and WL-isomorphic if it holds for \(k=|V_{G}|\). We denote this as \(G_{}H\). Note that \(G H G_{}H\), but the converse is not true. We call a graph \(G\)_WL-amenable_ if \( H\) such that \(G H\), \(G_{}H\).

Tinhofer  developed a color-refinement-based algorithm, described in Section A.2, that returns an ordering of the nodes of a graph. This is a canonical ordering on a large class of graphs, called Tinhofer, which is a strict superset of WL-amenable class . The Tinhofer algorithm is an instance of the individualization-refinement paradigm  on which several isomorphism solvers are based .

### Graph neural networks

Message passing graph neural networks (GNNs), given a graph \(G\), iteratively produce for each node \(v V_{G}\), at each level \(k=1,,K\), the embeddings \(h_{v}^{k}^{d_{h}}\) by taking into account _messages_ coming from its neighbors \((v)\). More formally, the embedding of node \(v\) is updated as \(h_{v}^{k}=f_{}(h_{v}^{k-1},f_{}(\{\{h_{v}^{ k-1}:u(v)\}\})),\) where \(f_{}\) and \(f_{}\) are the aggregate and the update operations, respectively. The first layer of the GNN is fed with the initial node embeddings \(h_{v}^{0}\), e.g. one-hot encodings of the node labels. Finally, one can get a graph-level readout \(h_{G}\) by aggregating the output node embeddings via a function \(f_{}\). In [64; 41] it was shown that there exist injective functions \(f_{}\), \(f_{}\) and \(f_{}\) yielding GNNs that are provably as expressive as color refinement. We provide examples for such architectures in Section A.1.

We denote as \(_{K}=\{_{K,},\}\) the class of parametric functions formed by such a model with \(K\) message passing layers and parameter space \(\). Moreover, we define \(_{K,}^{}(G)=[_{K,}(G)>0.5]\) and \(_{K}^{}=\{_{K,}^{ },\}\) the class of predictors.

Node individualization schemes

In this section, we present a general framework for node individualization schemes.

**Definition 1**.: _A graph \(G=(V,E,L_{G})\) is called individualized if all node labels are pairwise distinct, that is \(|\{L_{G}(v):v V\}|=|V|\). Moreover, we define a graph as \(k\)-weakly individualized if all nodes have different colors after \(k\) color refinement iterations, that is \(|\{C_{k}(v):v V\}|=|V|\)._

Graph individualization is effectively achieved by assigning unique identifiers to nodes through a relabeling process. We formally define a relabeling function \(:_{}_{ ^{}}\) as a transformation \(((V,E,L),)(V,E,L^{})\) that maps injectively the node labels from \(L:V\) to a new set of labels \(L^{}:V^{}\), e.g. with \(^{}=\) and \(L^{}(v)=(L(v),c),\ c\). This function preserves the graph's underlying structure (the nodes and edges). For the sake of generality, the relabeling function incorporates an additional argument \(\). This integer can be interpreted as a source of pseudo-randomness involved in the relabeling process (i.e., a random seed). For example, the relabeling function used in RNI , due to its pseudo-random nature, can generate non-isomorphic relabeled graphs even from two identical copies of the same graph if it is given two different random seeds. Moreover, we note that node encodings such as the random walk  or the Laplacian  positional encodings can be cast as relabeling functions.

A relabeling function is an _individualization scheme_ if, for each \((G,)\), it holds that \((G,)\) is a (possibly \(k\)-weakly) individualized graph. Moreover, for \(G\), let \((G):=\{(G,),\}\) and let \(():=\{(G):G\}\).

Expressive GNNs on individualized graphs are well known to be universal approximators for functions over graphs . In fact, even a GNN with a single message passing layer and a MLP head is enough to provide a universal approximator . We can generalize this result to \(k\)-weakly individualized graphs by noticing that \(k\) message passing layers, each simulating a WL iteration, provide a graph individualized by its node embeddings. The following statement formalizes this intuition.

**Proposition 1**.: _Let \(f:\{0,1\}\) be an invariant function and \(k 1\). Then there exists \(\) such that \(_{k,}^{}(G)=f(G)\) for every \((k-1)\)-weakly individualized graph \(G\)._

The above proposition highlights the importance of node individualization schemes, not only for enhancing model expressivity, but also for managing model complexity. While GNNs are theoretically capable of representing arbitrary functions on WL-amenable graphs (which compose the majority of graphs ), they might require a large number of message-passing layers to achieve this. This phenomenon, known as under-reaching , can be problematic. GNNs with many layers can suffer from oversmoothing  or oversquashing , hindering their ability to effectively distinguish graphs in practice. However, as Proposition 1 demonstrates, endowing GNNs with a (possibly \(k\)-weak) individualization scheme can offer sufficient signals for graph discrimination with fewer message-passing iterations, thus mitigating these issues. In Section 5.1, we experimentally investigate this assessment and show that it has implications in real-world scenarios.

## 4 Sample complexity bounds

In this section, we show a general framework to derive sample complexity bounds for GNNs endowed with relabeling functions, for graph-level binary classification tasks.

Let \(_{}\) be a joint distribution over a set of graphs \(\) and binary labels \(Y=\{0,1\}\). Moreover, let \(\) be a joint distribution over the domain of \(_{}\) and integers \(\). The integers \(\) model any (pseudo-)randomness within the classification model \(f(G,)\). In the following, we denote by \(x=(G,)\). Given a loss function \((f(x),y)\), our goal, in accordance with standard learning theory , is to bound the difference between the true risk \(R(f)\) and the empirical risk \((f)\):

\[R_{}(f)=_{(x,y)}[(f(x),y)] _{D,}(f)=_{i=1}^{m}(f(x_{i}),y_{i}),\]

where \(D=\{(x_{i},y_{i})\}_{i}^{m}\) represents a training dataset of size \(m\) sampled i.i.d. from \(\).

In Sections 4.1 and 4.2 we derive bounds to \(|R_{}(f)-_{D,}(f)|\) based on the VC dimension, while in Section 4.3 we derive more refined bounds based on covering numbers.

### VC dimension

Let \((f(x),y)=1[f(x) y]\) be the 0-1 loss. Then if \(d\) is the VC dimension of the hypothesis class, we have, with probability \(1-\) over the dataset \(D^{m}\), that \(_{f}|R_{}(f)-_{D,}(f)|+O( {(1/)/m})\)[36, Corollary 3.4]. We define formally the VC dimension of a class of functions on a set.

**Definition 2**.: _Let \(X\) be a set and \(\{f:X\{0,1\}\}\) an hypothesis class. We say that \(\) shatters \(S=(x_{1}, x_{|S|})\) if \(|f(x_{1}),,f(x_{|S|}):f}|=2^{|S|}\). Then, we say that \((X,)\) has VC dimension \((X,)=d\) if the largest set \(S\) shattered by \(\) has size \(d\), and \(+\) if the size of sets that can be shattered is unbounded._

In particular, we consider a set \(X=\) consisting of pairs of graphs and integers. Our hypothesis class \(\) comprises binary classification functions formed by the composition of graph relabeling functions (such as individualization schemes) and GNNs that are as expressive as color refinement: \(=_{K}^{}:=\{_{K,}^{} :_{K,}^{} _{K}^{}\}\), setting \(K=n_{}\) to ensure expressivity. GNNs with color refinement expressivity may require a number of parameters exponential in the maximum graph size \(n_{}=_{G}|V_{G}|\), which is a strong assumption we have to make. In the cases where the parameter count constraints the expressive power, the VC bounds from [37, Theorem 6] can be applied instead. The following theorem, which follows from [37, Proposition 2], relates the VC dimension to the WL-isomorphism classes of the graphs in \(()\).

**Theorem 1**.: _Let \(:^{}\) be a relabeling function. Then \(,_{K}^{ }=|( )/_{}|\). If \(\) is an individualization scheme, \(|()/_{}|=|()/|\)._

We provide specific bounds, proven in Section C.2, for relevant examples of relabeling schemes.

Random Node Initializations The \(\) scheme perturbs with some random noise the labels of the nodes. In line with the analysis in the original paper, we suppose that the random noise, selected via \(\), takes values in a finite set \(\). We then have that a graph \(G\) with \(n\) nodes is mapped to \(O(||^{n}/|Aut(G)|)\) relabeled unordered graphs. In the case of unlabeled graphs with \(n\) nodes, we then have that \(|()/|=(^{}||^{n})=(||^{n}||)\).

Relational Pooling  and CLIP Let \(G\) be a graph and let \(v_{1},,v_{n}\) be an arbitrary enumeration of its nodes selected via \(\). The simplest version of the \(\) scheme updates node labels as \(^{}(v_{i})=((v_{i}),i)\). A more refined version of the scheme partitions the nodes as \(\{V_{1},,V_{C}\}\) based on their labels and, letting \(V_{c}=(v_{i_{1}},,v_{i_{|V_{c}|}})\), it updates node labels as \(^{}(v_{i_{j}})=((v_{i_{j}}),j)\). Then, each graph can be mapped to \(_{c}|V_{c}|!/|Aut(G)|\) relabeled unordered graphs. For unlabeled graphs with \(n\) nodes, this amounts to having \(|()/|=(2^{})=(n! ||)\). The 1-CLIP node individualization scheme is equivalent to \(\) on the node partition based on initial labels.

Tinhofer algorithmLet \(G\) be a graph and let \(v_{1},,v_{n}\) be an ordering given by the Tinhofer  algorithm (Section A.2). Then updating node labels as \(^{}(v_{i})=((v_{i}),i)\) yields a valid individualization scheme \(\), that is an instance of the general individualization-refinement framework given by . Let the algorithm perform at most (i.e, for any choice of \(\)) \(I\) individualization iterations on graph \(G\), let \(\{V_{1},,V_{C}\}\) be a partitioning of the nodes of \(G\) based on their labels and let \(R=_{c}|V_{c}|\). Then we have that \(|(G)/| R^{I}/|Aut(G)| n^{I}/|Aut(G)|\). In particular, as shown in the following lemma, for data distributions restricted to WL-amenable graphs, endowing a GNN with the Tinhofer individualization scheme does not increase its VC dimension.

**Lemma 1**.: _Let \(\) be a set of WL-amenable graphs. Then \(,_{K}^{ }=|/|\)._

In fact, the class of graphs for which the Tinhofer individualization scheme does not increase the VC dimension is the class of Tinhofer graphs  and is strictly larger than the WL-amenable class.

Positional and structural encodingsDeterministic and equivariant encodings such as the random walk structural encoding  or adding graphlet counts  yield \(|()|=||\), and therefore the increase in VC dimension is only due in the increase in the number of WL-isomorphism classes, if any. The Laplacian positional encoding  is known not to be equivariant in general ,i.e., there are graphs \(G_{1} G_{2}\) for which \((G_{1},)(G_{2},)\). Therefore we have that \(|()/_{WL}|>|/_{WL}|\), which leads to an increase in the VC dimension and further motivates the attempts to obtain equivariant Laplacian-based relabeling functions [62; 28].

DiscussionNote that, in general, the number of relabeled graphs given by \(\) is lower if the relabeling scheme is equivariant and if it does not depend on the second argument \(\), i.e., if it is deterministic. For example, expressive GNNs endowed with the Tinhofer relabeling scheme, when applied to WL-amenable graphs with at most \(n\) nodes, have VC dimension that is lower by a factor \(O(n!)\) compared to equally expressive GNNs endowed with the RP scheme.

Finally, relabeling schemes that rely on the randomness introduced by the second argument \(\) (e.g. randomized schemes) are often enhanced by resampling different \(\) values at each epoch [42; 51], effectively performing data augmentation. As discussed in , when the augmented dataset includes all possible transformations of each data point (in this case, all possible graph relabelings), the VC dimension of the model reduces to that of a transformation-invariant model. However, the number of relabeled graphs in our setting often grows super-exponentially with the maximum graph size, e.g., \(n_{}!\) for RP, making it difficult to obtain enough samples for good generalization, as shown experimentally in Section 5.2. The theoretical analysis of sample complexity for randomized individualization schemes remains an open question we leave for future work.

### Improved VC bounds for substructure identification

In this section, we showcase that developing sample complexity bounds and relabeling schemes with low VC dimension can guide the design of architectures tailored to specific tasks. In particular, a relevant task that has been studied in past works is substructure identification [12; 28; 45], where the task is to decide if a graph pattern \(P\) appears as a subgraph or induced subgraph of a query graph \(G\), denoted hereafter as \(P G\). Notably, message passing GNNs are unable to solve the task in general . We provide a message-passing GNN architecture, similar to the one proposed in [12; Section 5] that, paired with node individualization schemes, is provably expressive for substructure identification and with lower VC dimension compared to the ones presented in Section 2.2. The idea, as in many subgraph GNNs [14; 48; 8; 50], is to transform the original graph into a set of smaller subgraphs and feeding them to a GNN.

In particular, let the task at hand concern a subgraph pattern \(P\) of radius \(_{P}=_{v V_{P}}_{w V_{P}}_{vw}\), where \(_{vw}\) is the length of the shortest path from \(v\) to \(w\). Let a \(k\)-ego-net of node \(v\) in graph \(G\) be the induced subgraph \(_{v,G,k}\) of nodes \(\{u V_{G}:_{uv} k\}\) at distance at most \(k\) from \(v\). Then, a model that is able to recognize if \(P\) appears in any of the \(_{P}\)-ego-nets of the query graph will be able to solve the task. Note that there could be ego-nets that are not recognizable by a GNN alone (Lemma 6), so we individualize each ego-net with a function \(\) to guarantee expressivity. Let then

\[h^{}_{v}=^{}_{1, }((_{v,G,_{P}},)) \{0,1\}, h_{G}=_{v V_{G}}h^{}_{v}\{0,1\},\]

that is, we run a 1-layer GNN on each individualized (potentially using some randomness from \(\)) ego-net and we aggregate the results using max-pooling. We denote such parametric functions as \(^{}_{_{P},}\). We have the following results on the expressivity and the VC dimension of the model.

**Theorem 2**.: _Let \(f:\{0,1\}\) be \(f(G)=[P G]\). Let \(\) be an individualization scheme. Then there exists \(\) such that \(^{}_{_{P},}(G)=f(G)\) for every \(G\)._

**Theorem 3**.: _Let \(_{}\) be the set of ego-nets of radius \(\) of the graphs of \(\). Then \((,^{}_{}){}|(_{})/|\)._

In general, \(_{}\) is much smaller compared to \(\), especially for small \(\). This leads to the fact that, in general, \((_{})\) will be much smaller than \(()\). Thanks to Theorem 1, we then have that the VC dimension of \(^{}_{}\) is in general lower compared to the one of \(GNN^{}_{K}\). In particular, relabeling the ego-nets using a scheme with lower \(|(_{})/|\), such as the Tinhofer scheme, leads to lower VC dimension compared to schemes that would produce more relabeled ego-nets, such as RP or RNI. This theoretical result is also experimentally validated in Section 5.3.

This approach clearly increases the space complexity compared to the standard GNN model. Indeed, if a graph \(G\) has \(n\) nodes, and each ego net has at most \(n_{E}\) nodes and \(m_{E}\) edges, the disjoint union of the ego-nets of \(G\) has \(nn_{E}\) nodes and \(nm_{E}\) edges.

### Covering numbers

The generalization bounds based on the VC dimension theory provide results that are useful only in restricted cases, e.g., when the task is substructure identification. Indeed, for a maximally expressive GNN model the VC-based bounds state that one should observe all the possible individualized graphs from the data distribution before the model can generalize. We provide an alternative analysis, following a Rademacher-complexity-based approach introduced in  and used in  for graph classification, which can lead to tighter results.

Let the margin function be \(M(f(x),y)=(2y-1)(2f(x)-1)[-1,1]\). Then the margin loss, given a \(>0\), is defined as

\[(f(x),y)=[M(f(x),y)<0]+(1-M(f(x),y)/)[0 M (f(x),y)]\ .\]

Let \(_{}=\{(x,y)(f(x),y):f\}\) be the set of margin losses, for each predictor \(f:=_{K}\). We then have , with probability \(1-\) over the dataset \(D=\{(x_{i},y_{i})\}_{i}^{m}\), that \(_{f}|R_{}(f)-_{D,}(f)|_{D}( _{})+O()\), with \(_{D}(_{})\) the empirical Rademacher complexity of \(_{}\) on \(D\) (see Definition 3 in the Appendix). In particular, given that \(_{(x,y)}[[f(x)>0.5] y] R(f)\), one can then bound the probability of making an error at inference time from the empirical loss. We then bound the empirical Rademacher complexity \(_{D}(_{})\) via a covering-number-based approach. Let \(D\) be the training dataset and let \(_{}|_{D}=\{f|_{D}:\ f_{}\}\) be the set of margin losses restricted to the dataset.

Given a pseudometric space \((X,)\) and a subset \(S X\), we call \(C X\) an \(\)-cover of \(S\) if \(_{x S}_{c C}(x,c)\) and define the \(\)-covering number of \(S\) as \((X,,)=\{|C|:\ C\}\), i.e., the minimum number of balls of radius \(\) required to cover \(S\). Similarly, for a finite \(S\), we call \(C X\) a \(p\)-norm \(\)-cover of \(S\) if \(_{x X}_{c C}(x,c)^{p}\)\({}^{}{{p}}}\) and define the \(p\)-norm \(\)-covering number of \(S\) as \(^{(p)}(X,,)=\{|C|:\ C\}\). With slight abuse of notation, we let \(^{()}(X,,)=(X,, )\). In particular, we have that the \(p\)-norm \(\)-covering number of a metric space is monotonically increasing in \(p\{\}\).

Moreover, for a finite set \(X\) and \(\{f:X\}\), we define the pseudometrics \(\|f_{1}-f_{2}\|_{p}=_{x X}|f_{1}(x)-f_{2}(x)|^{p}\) for \(p 1\) and \(\|f_{1}-f_{2}\|_{}=_{x X}|f_{1}(x)-f_{2}(x)|\). By Pollard's bound  we then have that

\[_{D}(_{})_{>0}(+(_{}|_{D},,\|\|_{1})/|D|}).\]

We bound the covering number of the class of functions \(_{}\) by covering the space of graphs and assuming the Lipschitzness of GNNs, similarly to the approaches in  and . In fact, we provide a generalization of a covering bound for Lipschitz functions [24, Lemma 5.2], that relaxes the Kolmogorov-Tikhomirov bound [58, Eq. 238], tailored to the aforementioned Rademacher bound.

**Lemma 2**.: _Let \((X,)\) be a pseudometric space and \(S X\) a finite subset. Let \(\{f:S\}\) be a set of \(C\)-Lipschitz functions. Let \(p,q\{+\},q p 1\). Then_

\[(,,\|\|_{p})(1/+1) ^{(q)}(S,,).\]

**Theorem 4**.: _Let \(_{}\) be the set of margin losses for predictors \(:=_{K}\). Let \((D)=\{G^{}=(G,):\ (G,,y) D\}\) be the graphs of the dataset after the relabeling, endowed with a pseudometric \(\). Let functions in \(_{K}\) be \(C\)-Lipschitz continuous with respect to \(\). Then_

\[_{D}(_{})_{>0}(+|D|^{- }{{2}}}^{(1)}((D),,)}).\]

Theorem 4 relies on the assumption that the GNN models are Lipschitz with respect to some pseudometric \(\) on graphs, such as the edit distance , the Tree Mover distance  or the WWL distance . This assumption, albeit strong, is often taken when considering the generalization of GNNs . Note that, for small \(\) and arbitrarily complex GNNs, i.e. \(C>\!>1\), we have that the covering number is proportional to the number of WL-isomorphic graphs in the relabeled dataset \((D)\), effectively retrieving the bounds obtained by the VC dimension. When \(_{K}\) is instead simpler, i.e. it has smaller Lipschitz constant \(C\), the space of graphs can be covered with fewer balls, and we obtain better sample complexity.

DiscussionLet the graphs of the dataset \(D_{G}=\{G:(G,,y) D\}\) and the relabeled graphs \((D)=\{(G,):(G,,y) D\}\) be subsets of the same metric space \((,)\). The results of Theorem 4 suggest that if a labeling function does not increase much the covering number of the dataset, i.e. \(^{(1)}((D),,) ^{(1)}(D_{G},,)\), then the model \(_{K}\) would have roughly the same sample complexity as \(_{K}\) while being more expressive.

In particular, this can be achieved if the relabeling scheme maps pairs of graphs with low distance (e.g. \((G_{1},G_{2})\)) to pairs of relabeled graphs with low distance. An example of such graphs at low distance could be a pair of graphs that differ only in the label of a node belonging to a singleton color class, like graphs \(G_{1}\) and \(G_{2}\) in Figure 1. These two graphs could be, for example, two molecules where an atom is substituted with a similar one, or two proteins where an amino-acid is substituted with a functionally-similar one .

Driven by this intuition, we design a (\(k\)-weak) individualization scheme, which we call _weak Tinhofer_ scheme \(_{W}\), that relabels only few nodes. The scheme obtains an ordering \(v_{1},,v_{n}\) of the nodes via the Tinhofer algorithm. Let \(\{V_{1},,V_{C}\}\) be a partition of the nodes of graph \(G\) into color classes after \(k\) color refinement steps and let \(V_{c}=(v_{i_{1}},,v_{i_{|V_{c}|}})\). The scheme then relabels nodes by appending their position in the ordering within their partition as \(^{}(v_{i_{j}})=((v_{i_{j}}),j)\). For nodes that belong to singleton classes, which is common in graphs such as molecules or random graphs, no relabeling is necessary.

See Figure 1 for a visual example of a comparison between \(\) and \(_{}\) on the pair of graphs \(G_{1}\) and \(G_{2}\). We have, for a \(u V_{G}\), that \(:V_{G_{1}} V_{G_{2}}\) such that \(\{(v),(w)\} E_{G_{2}}\) iff \(\{v,w\} E_{G_{1}}\), \(L_{G_{2}}((v))=L_{G_{1}}(v), v u V_{G_{1}}\) and \(L_{G_{2}}((u)) L_{G_{1}}(u)\). Then \((G_{1},G_{2})\) would be given by the cost of substituting \(L_{G_{2}}((u))\) with \(L_{G_{1}}(u)\). Consider the graphs produced by the \(_{W}\) procedure. Here we have that the labels of nodes that matched in the original graphs still match in the relabeled graphs. By contrast, with the graphs produced by the \(\) procedure, the number of node labels that do not match could be arbitrarily large.

This scheme ensures that the relabeled graphs are \(k\)-weakly individualized, while yielding no increase in the VC dimension for WL-amenable graphs (Lemma 1) and increasing the covering numbers only marginally. This in general yields better generalization, as shown experimentally in Section 5.4.

## 5 Experimental evaluation

In this section, we provide proof-of-concept experimental evidence about our theoretical results. In particular, Section 5.1 investigates the empirical expressivity of GNNs with and without individualization schemes, Section 5.2 and Section 5.3 validate respectively the general VC dimension bounds and the substructure identification ones, and Section 5.4 explores the results on covering numbers. Details about the datasets, experimental setup, tasks and infrastructure are reported in Section E. Code and datasets are available at https://github.com/BorgwardtLab/NodeIndividualizedGNNs.

### Expressivity

To highlight the efficacy of individualization schemes in practical applications, we compare a GNN endowed with \(\) to ordinary GNNs on challenging datasets. In particular, we consider subsets of the real-world datasets MCF-7 and Peptites-func, for which at least six message-passing iterations are necessary to distinguish all pairs of graphs, and synthetic datasets Cycles-pin and CSL-pin, for which nine, resp. five, iterations are necessary. To compare the performance between

Figure 1: Comparison between \(\) and \(_{}\). _Panel (a):_ Two graphs, where letters indicate initial node labels. The Tinhofer algorithm finds a canonical ordering on the two graphs. _Panel (b):_ The \(\) scheme concatenates the position of the node in the ordering to the node label. The relabeled graphs have edit distance 3. _Panel (c):_ The \(_{}\) scheme concatenates the position of the node within its WL color class. The edit distance remains 1, as in the original graphs.

the two methods, we measure the models' capabilities to distinguish non-isomorphic graphs. Figure 2 shows the accuracy obtained by ordinary GNNs with a theoretically sufficient number of layers as well as a 1-layer GNN endowed with \(\). While the ordinary GNNs can achieve a perfect score for both synthetic datasets, their learning process is slow and unstable. For the real-world datasets, it can be observed that ordinary GNNs do not achieve a score close to \(100\%\) within the tested 1000 epochs at all. These observations may be explained by the fact that the model has to learn to propagate structural information for several layers, possibly encountering oversquashing issues. In contrast, the GNN with \(\) converges faster and more stably with only a single GNN layer. The results particularly emphasize that the Tinhofer individualization scheme can improve the expressive power in shallow GNNs, demonstrating the value of our approach in practical scenarios.

### VC dimension

We empirically evaluate the generalization bounds based on the VC dimension derived in Section 4.1. Recall that for an individualization scheme Rel and a set of graphs \(\), the generalization gap is bounded by the VC dimension \(d=|()/|\). We evaluate RNI, RP, and Tinhofer using datasets of circular skip link (CSL)  graphs of sizes \(n=17,41,83\) and 3-regular graphs of sizes \(n=16,32,64\). The task is to distinguish non-isomorphic graphs. Figure 3 shows the generalization gap with increasing numbers of permuted copies of the graphs in the training set. RNI and RP, which generate highly randomized individualizations, require several samples to generalize effectively. As graph size \(n\) increases, the VC dimension increases, resulting in a greater generalization gap. Interestingly, \(\) perfectly learns the target function on CSL graphs with only a single copy of the graphs, as the individualization is in fact canonical.

### Substructure identification

To assess the expressivity and generalization of GNNs for substructure identification, we created four challenging datasets, each comprising 1000 unlabeled 3-regular graphs. The task is to detect whether

Figure 3: Difference between test and training accuracy for GNNs with the \(\), \(\) and \(\) individualization schemes, on datasets of CSL and 3-regular graphs of various sizes.

Figure 2: Accuracy for \(_{K}\) and \(_{1}\) over 1000 epochs on synthetic datasets Cycles-pin and CSL-pin and real-world datasets MCF–7 [65; 38] and Peptides-func [57; 18].

graphs contain the induced subgraphs: 3-cycle \(C_{3}\), 4-cycle \(C_{4}\), 5-cycle \(C_{5}\), or complete bipartite graph \(K_{2,3}\). We tested GNNs with and without individualization schemes, along with our novel architecture \(_{_{P}}\). As reported in Table 1, GNNs lacking individualization fail to distinguish between graphs. Those with RP or Tinhofer individualizations tend to overfit on training data but struggle to generalize. In contrast, \(_{_{P}}\) models generally exhibit a smaller generalization gap. Not providing individualizations to ego-nets results in a small generalization gap, but often fails to fit the training data. RP individualization enables the model to fit training data but may lead to overfitting. The Tinhofer individualization yields, as predicted by the theory, the best performance.

### Covering numbers

To validate the theoretical results on covering numbers from Section 4.3, we use two molecular (NCI1 and Mutagenicity) and two social network (IMDB-b and COLLAB-b) datasets. As suggested by the theory, we use as a proxy for the sample complexity the ratio between the covering number (for simplicity fixing \(=0.05\), see Section F for complete results) of the relabeled dataset and the dataset size \(}=^{(1)}((D),\,,) /|D|\). As a (pseudo-)metric, we use the distance used by the WWL kernel . We report \(}\), together with the test accuracy and the difference between test and train accuracy, in Table 2 for GNNs with no individualizations, with RP, RNI, Tinhofer and weak Tinhofer individualizations, and with a Laplacian-encoding-based relabeling. We observe that across datasets \(}\) strongly correlates with the generalization gap, confirming empirically the intuition provided by the theory. Moreover, we observe that on molecular datasets, that are easily recognizable by the WL test, individualizations don't help expressivity. On the other hand, on unlabeled datasets such as COLLAB-b, ordinary GNNs might struggle to converge to good local minima, and the additional information given by relabeling schemes usually helps. In fact, it is common in the literature [64; 41] to relabel such unlabled graphs with, e.g., degree centralities.

## 6 Conclusion

In this paper, we developed novel sample complexity bounds for graph neural networks endowed with individualization schemes. Several research directions are left open for future work, including the analysis of data augmentation for individualization schemes via resampling, and the development of tighter covering bounds for specific model architectures. We envision that this work will inspire the development of new, practical graph learning models that are both theoretically sound and empirically effective, ultimately advancing the field of graph representation learning.

    & &  &  \\  GNN & Rel & Train & Test & Train & Test & Train & Test & Diff. & \(}\) & Test & Diff. \\  _{_{P}}\)} & None & 0.090 & 83.8\(\) & 16.1\(\) & 0.156 & 83.0\(\) & 17.0\(\) & 0.025 & 71.8\(\) & 3.6\(\) & 0.241 & 56.1\(\) & 1.1\(\) \\  & RP & 0.719 & 67.8\(\) & 32.2\(\) & 0.746 & 70.9\(\) & 29.1\(\) & 0.443 & 65.4\(\) & 33.0\(\) & 0.248 & 67.5\(\) & 3.1\(\) \\  & RNI & 0.705 & 67.1\(\) & 32.9\(\) & 0.717 & 73.1\(\) & 26.9\(\) & 0.580 & 62.4\(\) & 37.6\(\) & 0.547 & 56.8\(\) & 1.2\(\) \\  & Tinhofer & 0.657 & 71.5\(\) & 28.4\(\) & 0.697 & 49.4\(\) & 25.1\(\) & 0.266 & 70.0\(\) & 19.1\(\) & 0.471 & 68.3\(\) & 20.4\(\) \\  & Tinhofer & 0.203 & 81.9\(\) & 180.0\(\) & 0.232 & 82.6\(\) & 17.4\(\) & 0.133 & 67.8\(\) & 21.3\(\) & 0.310 & 79.5\(\) & 136.6\(\) \\  & LPE & 0.589 & 77.3\(\) & 22.6\(\) & 0.485 & 76.2\(\) & 23.8\(\) & 0.271 & 68.0\(\) & 21.1\(\) & 0.409 & 74.6\(\) & 21.6\(\) \\  _{_{P}}\)} & None & 0.090 & 81.8\(\) & 18.1\(\) & 0.156 & 81.5\(\) & 18.4\(\) & 0.025 & 71.4\(\) & 22.2\(\) & 0.241 & 75.3\(\) & 0.0\(\) \\  & RP & 0.719 & 67.4\(\) & 32.6\(\) & 0.746 & 71.2\(\) & 28.8\(\) & 0.433 & 63.8\(\) & 36.4\(\) & 0.485 & 71.5\(\) & 18.1\(\) \\  & RNI & 0.705 & 68.0\(\) & 32.0\(\) & 0.717 & 72.7\(\) & 27.3\(\) & 0.580 & 63.6\(\) & 36.4\(\) & 0.547 & 72.1\(\) & 19.6\(\) \\  & Tinhofer & 0.657 & 72.9\(\) & 26.9\(\) & 0.697 & 73.1\(\) & 26.9\(\) & 0.266 & 68.6\(\) & 30.5\(\) & 0.471 & 75.2\(\) & 19.5\(\) \\  & Tinhofer & 0.657 & 81.8\(\) & 18.1\(\) & 0.282 & 81.2\(\) & 18.8\(\) & 0.133 & 69.4\(\) & 19.6\(\) & 0.310 & 80.8\(\) & 12.2\(\) \\  & LPE & 0.589 & 76.4\(\) & 23.5\(\) & 0.485 & 75.4\(\) & 24.6\(\) & 0.271 & 68.4\(\) & 20.7\(\) & 20.7\(\) & 0.409 & 75.8\(\) & 19.9\(\) \\  

Table 2: Covering numbers and classification accuracies on real-world datasets.

    & &  &  \\   & & &  &  &  \\  GNN & Rel & \(}\) & Test & Diff. & \(}\) & Test & Diff. & \(}\) & Test & Diff. & \(}\) & Test & Diff. \\  _{}\)} & None & 0.090 & 83.8\(\) & 161.1\(\) & 0.156 & 83.0\(\) & 17.0\(\) & 0.025 & 71.8\(\) & 3.6\(\) & 0.241 & 56.1\(\) & 1.1\(\) \\  & RP & 0.719 & 67.8\(\) & 32.1\(\) & 0.746 & 70.9\(\) & 29.