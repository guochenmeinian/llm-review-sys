# Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow

Chen-Hao Chao\({}^{*1,2}\)

Chien Feng\({}^{*1}\)

Wei-Fang Sun\({}^{2}\)

Cheng-Kuang Lee\({}^{2}\)

Simon See\({}^{2}\)

Chun-Yi Lee\({}^{1}\)

\({}^{1}\) Elsa Lab, National Tsing Hua University, Hsinchu City, Taiwan

\({}^{2}\) NVIDIA AI Technology Center, NVIDIA Corporation, Santa Clara, CA, USA

Equal contribution.Corresponding author. Email: cylee@cs.nthu.edu.tw

###### Abstract

Existing Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) methods for continuous action spaces are typically formulated based on actor-critic frameworks and optimized through alternating steps of policy evaluation and policy improvement. In the policy evaluation steps, the critic is updated to capture the soft Q-function. In the policy improvement steps, the actor is adjusted in accordance with the updated soft Q-function. In this paper, we introduce a new MaxEnt RL framework modeled using Energy-Based Normalizing Flows (EBFlow). This framework integrates the policy evaluation steps and the policy improvement steps, resulting in a single objective training process. Our method enables the calculation of the soft value function used in the policy evaluation target without Monte Carlo approximation. Moreover, this design supports the modeling of multi-modal action distributions while facilitating efficient action sampling. To evaluate the performance of our method, we conducted experiments on the MuJoCo benchmark suite and a number of high-dimensional robotic tasks simulated by Omniverse Isaac Gym. The evaluation results demonstrate that our method achieves superior performance compared to widely-adopted representative baselines.

## 1 Introduction

Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17] has emerged as a prominent method for modeling stochastic policies. Different from standard RL, MaxEnt RL integrates the entropy of policies as rewards, which leads to a balanced exploration-exploitation trade-off during training. This approach has demonstrated improved robustness both theoretically and empirically [17; 18; 19]. Building on this foundation, many studies leveraging MaxEnt RL have shown superior performance on continuous-control benchmark environments [8; 9] and real-world applications [20; 21; 22].

An active research domain in MaxEnt RL concentrates on the learning of the soft Q-function [8; 9; 10; 11; 12; 13; 14; 15]. These methods follow the paradigm introduced in soft Q-learning (SQL) . They parameterize the soft Q-function as an energy-based model  and optimize it based on the soft Bellman error  calculated from rewards and the soft value function. However, this approach presents two challenges. First, sampling from an energy-based model requires a costly Monte Carlo Markov Chain (MCMC) [24; 25] or variational inference  process, which can result in inefficient interactions with environments. Second, the calculation of the soft value function can involve computationallyinfeasible integration, which requires an effective approximation method. To tackle these issues, various methods [8; 9; 10; 11; 12; 13; 14; 15] were proposed, all grounded in a common design philosophy. To address the first challenge, these methods suggest operating on an actor-critic framework and optimizing it through alternating steps of policy evaluation and policy improvement. For the second challenge, they resort to Monte Carlo methods to approximate the soft value function using sets of random samples. Although these two issues can be circumvented, these methods still have their drawbacks. The actor-critic design introduces an additional optimization process for training the actor, which may lead to optimization errors in practice . Moreover, the results of Monte Carlo approximation may be susceptible to estimation errors and variances when there are an insufficient number of samples [28; 29; 30].

Instead of using energy-based models to represent MaxEnt RL frameworks, this paper investigates an alternative method employing normalizing flows (i.e., flow-based models), which offer solutions to the aforementioned challenges. Our framework is inspired by the recently introduced Energy-Based Normalizing Flows (EBFlow) . This design facilitates the derivation of an energy function from a flow-based model while supporting efficient sampling, which enables a unified representation of both the soft Q-function and its corresponding action sampling process. This feature allows the integration of the policy evaluation and policy improvement steps into a single objective training process. In addition, the probability density functions (pdf) of flow-based models can be calculated efficiently without approximation. This characteristic permits the derivation of an exact representation for the soft value function. Our experimental results demonstrate that the proposed framework exhibits superior performance on the commonly adopted MuJoCo benchmark [32; 33]. Furthermore, the evaluation results on the Omniverse Isaac Gym environments  indicate that our framework excels in performing challenging robotic tasks that simulate real-world scenarios.

## 2 Background and Related Works

In this section, we walk through the background material and the related works. We introduce the objective of MaxEnt RL in Section 2.1, describe existing actor-critic frameworks and soft value estimation methods in Section 2.2, and elaborate on the formulation of Energy-Based Normalizing Flow (EBFlow) in Section 2.3.

### Maximum Entropy Reinforcement Learning

In this paper, we consider a Markov Decision Process (MDP) defined as a tuple \((,,p_{T},,,p_{0})\), where \(\) is a continuous state space, \(\) is a continuous action space, \(p_{T}:_{  0}\) is the pdf of a next state \(_{t+1}\) given a current state \(_{t}\) and a current action \(_{t}\) at timestep \(t\), \(:\) is the reward function, \(0<<1\) is the discount factor, and \(p_{0}\) is the pdf of the initial state \(_{0}\). We adopt \(r_{t}\) to denote \((_{t},_{t})\), and use \(_{}(_{t},_{t})\) to represent the state-action marginals of the trajectory distribution induced by a policy \((_{t}|_{t})\).

Standard RL defines the objective as \(^{*}=*{argmax}_{}\,_{t}_{(_{t}, _{t})_{}}[r_{t}]\) and has at least one deterministic optimal policy [35; 36]. In contrast, MaxEnt RL  augments the standard RL objective with the entropy of a policy at each visited state \(_{t}\). The objective of MaxEnt RL is written as follows:

\[^{*}_{}=*{argmax}_{}\,_{t}_{ (_{t},_{t})_{}}[r_{t}+( (|_{t}))],\] (1)

where \(((|_{t}))_{ (|_{t})}[-(|_{t})]\) and \(_{>0}\) is a temperature parameter for determining the relative importance of the entropy term against the reward. An extension of Eq. (1) defined with \(\) is discussed in . To obtain \(^{*}_{}\) described in Eq. (1), the authors in  proposed to minimize the soft Bellman error for all states and actions. The solution can be expressed using the optimal soft Q-function \(Q^{*}_{}:\) and soft value function \(V^{*}_{}:\) as follows:

\[^{*}_{}(_{t}|_{t})=((Q^{*}_{}(_{t},_{t})-V^{*}_{}(_{t}))),\] (2)

\[Q^{*}_{}(_{t},_{t})=r_{t}+_{ _{t+1} p_{T}}[V^{*}_{}(_{t+1})],V^{*}_{}(_{t})=(Q^{*}_{}(_{t},))d.\] (3)

In practice, a policy can be modeled as \(_{}(_{t}|_{t})=((Q_{}( _{t},_{t})-V_{}(_{t}))\) with parameter \(\), where the soft Q-function and the soft value function are expressed as \(Q_{}(_{t},_{t})\) and \((Q_{}(_{t},_{t}) )d_{t}\), respectively. Given an experience reply buffer \(\) that stores transition tuples \((_{t},_{t},r_{t},_{t+1})\), the training objective of \(Q_{}\) (which can then be used to derive \(V_{}\) and \(_{}\)) can be written as the following equation according to the soft Bellman errors:

\[()=_{(_{t},_{t},r_{t},_{t+1})}[(Q_{}(_{t}, _{t})-(r_{t}+ V_{}(_{t+1})))^{2}].\] (4)

Nonetheless, directly using the objective in Eq. (4) presents challenges for two reasons. First, drawing samples from an energy-based model (i.e., \(_{}(_{t}|_{t})(Q_{}(_ {t},_{t})/)\)) requires a costly MCMC or variational inference process [26; 37], which makes the interaction with the environment inefficient. Second, the calculation of the soft value function involves integration, which require stochastic approximation methods [28; 29; 30] to accomplish. To address these issues, the previous MaxEnt RL methods [8; 9; 10; 11; 12; 13; 14] adopted actor-critic frameworks and introduced a number of techniques to estimate the soft value function. These methods are discussed in the next subsection.

### Actor-Critic Frameworks and Soft Value Estimation in MaxEnt RL

Previous MaxEnt RL methods [8; 9; 10; 11; 12; 13; 14; 15] employed actor-critic frameworks, in which the critic aims to capture the soft Q-function, while the actor learns to sample actions based on this soft Q-function. Available choices for modeling the actor include Gaussian models , Gaussian mixture models , variational autoencoders (VAE) [15; 13; 39], normalizing flows [10; 11], and amortized SVGD (A-SVGD) [8; 40], all of which support efficient sampling. The separation of the actor and the critic prevents the need for costly MCMC processes during sampling. However, this design induces additional training steps aimed to minimize the discrepancy between them. Let \(_{}(_{t}|_{t})(Q_{ }(_{t},_{t}))\) and \(_{}(_{t}|_{t})\) denote the pdfs defined through the critic and the actor, respectively. The objective of this additional training process is formulated according to the reverse KL divergence \(_{}[_{}(\,|_{t})||_{}( \,|_{t})]\) between \(_{}\) and \(_{}\), and is typically reduced as follows :

\[()=_{_{t}}[- _{_{t}_{}}[Q_{}(_{t},_{t})- _{}(_{t}|_{t})]].\] (5)

The optimization processes defined by the objective functions \(()\) and \(()\) in Eqs. (4) and (5) are known as the policy evaluation steps and policy improvement steps , respectively. Through alternating updates according to \(_{}()\) and \(_{}()\), the critic learns directly from the reward signals to estimate the soft Q-function, while the actor learns to draw samples based on the distribution defined by the critic.

Although the introduction of the actor enhances sampling efficiency, calculating the soft value function in Eq. (3) still requires Monte Carlo approximations for the computationally infeasible integration operation. Prior soft value estimation methods can be categorized into two groups: soft value estimation in Soft Q-Learning (SQL) and that in Soft Actor-Critic (SAC), with the former yielding a larger estimate than the latter, derived from Jensen's inequality (i.e., Proposition A.1 in Appendix A.1). These two soft value estimation methods are discussed in the following paragraphs.

Soft Value Estimation in SQL.Soft Q-Learning  leverages importance sampling to convert the integration in Eq. (3) into an expectation, which can be estimated using a set of independent and identically distributed (i.i.d.) samples. To ensure the estimation variance is small, the authors in  proposed to utilize samples drawn from \(_{}\). Let \(\{^{(i)}\}_{i=1}^{M}\) be a set of \(M\) samples drawn from \(_{}\). The soft value function is approximated based on the following formula:

\[ V_{}(_{t})&= (Q_{}(_{t},)/)d =_{}(|_{t})(_{t},)/)}{_{}( |_{t})}d\\ &=_{_{}}[(_{t},)/)}{_{}( |_{t})}](_{i=1 }^{M}(_{t},^{(i)})/ )}{_{}(^{(i)}|_{t})}).\] (6)

Eq. (6) has the least variance when \(_{}(\,|_{t})(Q_{}(_{t},)/)\). In addition, as \(M\), the law of large numbers ensures that this estimation converge to \(V_{}(_{t})\).

Soft Value Estimation in SAC.Soft Actor-Critic  and its variants [10; 11; 13; 14; 12] reformulated the soft value function \(V_{}(_{t})=(Q_{}(_{t}, )/)d\) as its equivalent form \(_{_{}}[Q_{}(_{t},) -_{}(|_{t})]\) based on the relationship that \(_{}(|_{t})=((Q_{}( _{t},))-_{}(|_{t})\). In addition, as \(M\), the law of large numbers ensures that this estimation converge to \(V_{}(_{t})\).

Soft Value Estimation in SAC.Soft Actor-Critic  and its variants [10; 11; 13; 14; 12] reformulated the soft value function \(V_{}(_{t})=(Q_{}(_{t}, )/)d\) as its equivalent form \(_{_{}}[Q_{}(_{t},)- _{}(|_{t})]\) based on the relationship that \(_{}(|_{t})=((Q_{}( _{t},))-_{}(|_\(V_{}(_{t})\)). By assuming that the policy improvement loss \(()\) is small (i.e., \(_{}_{}\)), the soft value function \(V_{}\) can be estimated as follows:

\[ V_{}(_{t})&=_ {_{}}[Q_{}(_{t},)- _{}(|_{t})]\\ &_{_{}}[Q_{}(_{t},)-_{}(|_{t})] _{i=1}^{M}(Q_{}(_{t},^{(i)})-_ {}(^{(i)}|_{t})).\] (7)

An inherent drawback of the estimation in Eq. (7) is its reliance on the assumption \(_{}_{}\). In addition, the second approximation involves Monte Carlo estimation with \(M\) samples \(\{^{(i)}\}_{i=1}^{M}\), where the computational cost increases with the number of samples \(M\).

### Energy-Based Normalizing Flows

Normalizing flows (i.e., flow-based models) are universal representations for pdf . Given input data \(^{D}\), a latent variable \(^{D}\) with prior pdf \(p_{}\), and an invertible function \(g_{}=g_{}^{L} g_{}^{1}\) modeled as a neural network with \(L\) layers, where \(g_{}^{i}:^{D}^{D},\, i\{1, ,L\}\). According to the change of variable theorem and the distributive property of the determinant operation, a parameterized pdf \(p_{}\) can be described as follows:

\[p_{}()=p_{}(g_{}()) _{i=1}^{L}|(_{g_{}^{i}}(^{i-1})) |,\] (8)

where \(^{0}\) is the input, \(^{i}=g_{}^{i} g_{}^{1}()\) is the output of the \(i\)-th layer, and \(_{g_{}^{i}}(^{i-1})^{i-1}}g_{}^{i}(^{i-1})\) represents the Jacobian of the \(i\)-th layer of \(g_{}\) with respect to \(^{i-1}\). To draw samples from \(p_{}\), one can first sample \(\) from \(p_{}\) and then derive \(g_{}^{-1}()\). To facilitate efficient computation of the pdf and the inverse of \(g_{}\), one can adopt existing architectural designs  for \(g_{}\). Popular examples involve autoregressive layers  and coupling layers , which utilizes specially designed architectures to speed up the calculation.

Energy-Based Normalizing Flows (EBFlow)  were recently introduced to reinterpret flow-based models as energy-based models. In contrast to traditional normalizing flow research  that focuses on the use of effective non-linearities, EBFlow emphasizes the use of both linear and non-linear transformations in the invertible transformation \(g_{}\). Such a concept was inspired by the development of normalizing flows with convolution layers  or fully-connected layers , linear independent component analysis (ICA) models , as well as energy-based training techniques . Let \(_{l}=\{i\,|\,g_{}^{i}\}\) and \(_{n}=\{i\,|\,g_{}^{i}\}\) represent the sets of indices of the linear and non-linear transformations in \(g_{}\), respectively. As shown in , the Jacobian determinant product in Eq. (8) can be decomposed according to \(_{n}\) and \(_{l}\). This decomposition allows a flow-based model to be reinterpreted as an energy-based model, as illustrated in the following equation:

\[p_{}()=}(g_{}() )_{i_{n}}|(_{g_{}^{i}} (^{i-1}))|}_{}_{l}}|(_{g_{}^{i}})|} _{}()) }_{}^{-1}}_{}.\] (9)

In EBFlow, the energy function \(E_{}()\) is defined as \(-(p_{}(g_{}())_{i_ {n}}|(_{g_{}^{i}}(^{i-1}))|)\) and the normalizing constant \(Z_{}=(-E_{}())d=_{i _{l}}|_{g_{}^{i}}|^{-1}\) is independent of \(\). The input-independence of \(Z_{}\) holds since \(g_{}^{i}\) is either a first-degree or zero-degree polynomial for any \(i_{l}\), and thus its Jacobian is a constant to \(^{i-1}\). This technique was originally proposed to reduce the training cost of maximum likelihood estimation for normalizing flows. However, we discovered that EBFlow is ideal for MaxEnt RL. Its unique capability to represent a parametric energy function with an associated sampler \(g_{}^{-1}\), and to calculate a normalizing constant \(Z_{}\) without integration are able to address the challenges mentioned in Section 2.2. We discuss our insights in the next section.

## 3 Methodology

In this section, we introduce our proposed MaxEnt RL framework modeled using EBFlow. In Section 3.1, we describe the formulation and discuss its training and inference processes. In Section 3.2,we present two techniques for improving the training of our framework. Ultimately, in Section 3.3, we offer an algorithm summary.

### MaxEnt RL via EBFlow

We propose a new framework for modeling **Max**Ent RL using EBFlow, which we call **MEow**. This framework possesses several unique features. First, as EBFlow enables simultaneous modeling of an unnormalized density and its sampler, MEow can unify the actor and the critic previously separated in MaxEnt RL frameworks. This feature facilitates the integration of policy improvement steps with policy evaluation steps, and results in a single objective training process. Second, the normalizing constant of EBFlow is expressed in closed form, which enables the calculation of the soft value function without resorting to the approximation methods mentioned in Eqs. (6) and (7). Third, given that normalizing flow is a universal approximator for probability density functions, our policy's expressiveness is not constrained, and can model multi-modal action distributions.

In MEow, the policy is described as a state-conditioned EBFlow, with its pdf presented as follows:

\[_{}(_{t}|_{t})& =}(g_{}(_{t}|_{t}) )_{i_{n}}|(_{g^{i}_{ }}(_{t}^{i-1}|_{t}))|}_{}_{i}}|( _{g^{i}_{}}(_{t}))|}_{}\\ &Q_{}( _{t},_{t}))}_{}V_{}(_{t}))}_{}, \] (10)

where the soft Q-function and the soft value function are selected as follows:

\[Q_{}(_{t},_{t}) p_{} (g_{}(_{t}|_{t}))_{i _{n}}|(_{g^{i}_{}}(_{t}^{i-1}|_{t}))|,V_{}(_{t})-_{i _{i}}|(_{g^{i}_{}}(_{t}))|.\] (11)

Such a selection satisfies \(V_{}(_{t})=(Q_{}(_{t}, )/)d\) based on the property of EBFlow. In addition, both \(Q_{}\) and \(V_{}\) have a common output codomain \(\), which enables them to learn to output arbitrary real values. These properties are validated in Proposition 3.1, with the proof provided in Appendix A.2. The training and inference processes of MEow are summarized as follows.

**Proposition 3.1**.: _Eq. (11) satisfies the following statements: (1) Given that the Jacobian of \(g_{}\) is non-singular, \(V_{}(_{t})\) and \(Q_{}(_{t},_{t})\), \(_{t},_{t}\). (2) \(V_{}(_{t})=(Q_{}(_{t}, )/)d\)._

Training.With \(Q_{}\) and \(V_{}\) defined in Eq. (11), the loss \(()\) in Eq. (4) can be calculated without using Monte Carlo approximation of the soft value function target. Compared to the previous MaxEnt RL frameworks that rely on Monte Carlo estimation (i.e., Eqs. (6) and (7)), our framework offers the advantage of avoiding the errors induced by the approximation. In addition, MEow employs a unified policy rather than two separate roles (i.e., the actor and the critic), which eliminates the need for minimizing an additional policy improvement loss \(()\) to bridge the gap between \(_{}\) and \(_{}\). This simplifies the training process of MaxEnt RL, and obviates the requirement of balancing between the two optimization loops.

Inference.The sampling process of \(_{}\) can be efficiently performed by deriving the inverse of \(g_{}\), as supported by several normalizing flow architectures [43; 44; 45; 46; 47; 48]. In addition, unlike previous actor-critic frameworks susceptible to discrepancies between \(_{}\) and \(_{}\), the distribution established via \(g_{}^{-1}(|_{t})\), where \( p_{}\), is consistently aligned with the pdf defined by \(Q_{}\). As a result, the actions taken by MEow can precisely reflect the learned soft Q-function.

### Techniques for Improving the Training and Inference Processes of MEow

In this subsection, we introduce a number of training and inference techniques aimed at improving MEow while preserving its key features discussed in the previous subsection. For clarity, we refer to the MEow framework introduced in the last section as 'MEow (Vanilla)'.

Learnable Reward Shifting (LRS). Reward shifting  is a technique for shaping the reward function. This technique enhances the learning process by incorporating a shifting term in the reward function, which leads to a shifted optimal soft Q-function in MaxEnt RL. Inspired by this, this work proposes modeling a reward shifting function \(b_{}:\) with a neural network to enable the automatic learning of a reward shifting term. For notational simplicity, the parameters are denoted using \(\), and the details of the architecture are presented in Appendix A.5.1. The soft Q-function \(Q_{}^{b}\) augmented by \(b_{}\) is defined as follows:

\[Q_{}^{b}(_{t},_{t})=Q_{}(_{t}, _{t})+b_{}(_{t}).\] (12)

The introduction of \(Q_{}^{b}\) results in a corresponding shifted soft value function \(V_{}^{b}(_{t})(Q_{}^{b}( _{t},)/)d=V_{}(_{t})+b_{ }(_{t})\) (i.e., Proposition A.3 in Appendix A.2), which can be calculated without Monte Carlo estimation. Moreover, with the incorporation of \(b_{}\), the policy \(_{}\) remains invariant since \(((Q_{}^{b}(_{t},_{t})-V_{ }^{b}(_{t})))=(((Q_{}(_{t },_{t})+b_{}(_{t}))-(V_{}(_{t})+b_{ }(_{t}))))=((Q_{}(_{t}, _{t})-V_{}(_{t})))\), which allows the use of \(g_{}^{-1}\) for efficiently sampling actions. As evidenced in Fig. 1, this method effectively addresses the issues of the significant growth and decay of Jacobian determinants of \(g_{}\) (discussed in Appendix A.3). In Section 4.4, we further demonstrate that the performance of MEow can be significantly improved through this technique.

Shifting-Based Clipped Double Q-Learning (SCDQ).As observed in , the overestimation of value functions often occurs in training. To address this issue, the authors in  propose clipped double Q-learning, which employs two separate Q-functions and uses the one with the smaller output to estimate the value function during training. This technique is also used in MaxEnt RL frameworks . Inspired by this and our proposed learnable reward shifting, we further propose a shifting-based method that adopts two learnable reward shifting functions, \(b_{}^{(1)}\) and \(b_{}^{(2)}\), without duplicating the soft Q-function \(Q_{}\) and soft value function \(V_{}\) defined by \(g_{}\). The soft Q-functions \(Q_{}^{(1)}\) and \(Q_{}^{(2)}\) with corresponding learnable reward shifting functions \(b_{}^{(1)}\) and \(b_{}^{(2)}\) can be obtained using Eq. (12), while the soft value function \(V_{}^{}\) is written as the following formula:

\[V_{}^{}(_{t})=(V_{}(_{t}) +b_{}^{(1)}(_{t}),V_{}(_{t})+b_{}^{(2)} (_{t}))=V_{}(_{t})+(b_{}^{(1)}( _{t}),b_{}^{(2)}(_{t})).\] (13)

This design also prevents the production of two policies in MEow, as having two policies can complicate the inference procedure. In our ablation analysis presented in Section 4.4, we demonstrate that this technique can effectively improve the training process of MEow.

Deterministic Policy for Inference.As observed in , deterministic actors typically performed better as compared to its stochastic variant during the inference time. Such a problem can be formalized as finding an action \(\) that maximizes \(Q(_{t},)\) for a given \(_{t}\). Since \(\) is a continuous space, finding such a value would require extensive calculation. In the MEow framework, this value can be derived by making assumptions on the model architecture construction. Our key observation is that if the Jacobian determinants of the non-linearities (i.e., \(g_{}^{i}_{n}\)) are constants with respect to its inputs, and that \(*{argmax}_{}p_{}()\) can be directly obtained, then the action \(\) that maximizes \(Q(_{t},)\) can be efficiently derived according to the following proposition.

**Proposition 3.2**.: _Given that \(|(_{g_{}^{i}}(^{i-1}|_{t}))|\) is a constant with respect to \(^{i-1}\), then \(g_{}^{-1}(*{argmax}_{}p_{}( )|_{t})=*{argmax}_{}Q_{}(_{t}, )\)._

The proof is provided in Appendix A.2. It is important to note that \(g_{}^{i}\) can still be a non-linear transformation, given that \(|(_{g_{}^{i}}(^{i-1}|_{t}))|\) is a constant. To construct such a model, a Gaussian prior with the additive coupling transformations  can be used as non-linearities. Under such a design, an action can be derived by calculating \(g_{}^{-1}(|_{t})\), where \(\) represents the mean of the

Figure 1: The Jacobian determinant products for (a) the non-linear and (b) the linear transformations, evaluated during training in the Hopper-v4 environment. Subfigure (b) is presented on a log scale for better visualization. This experiment adopt the affine coupling layers  as the nonlinear transformations.

Gaussian distribution. We elaborate on our model architecture design in Appendix A.5.1, and provide a performance comparison between MEow evaluated using a stochastic policy (i.e., \(_{t}_{}(\,|_{t})\)) and a deterministic policy (i.e., \(_{t}=*{argmax}_{}Q_{}(_{t}, )\)) in Section 4.4.

### Algorithm Summary

We summarize the training process of MEow in Algorithm 1. The algorithm integrates the policy evaluation steps with the policy improvement steps, resulting in a single loss training process. This design differs from previous actor-critic frameworks, which typically perform two consecutive updates in each training step. In Algorithm 1, the learning rate is denoted as \(\). A set of shadow parameters \(^{}\) is maintained for calculating the delayed target values , and is updated according to the Polyak averaging  of \(\), i.e., \(^{}(1-)^{}+\), where \(\) is the target smoothing factor.

```
0: Learnable parameters \(\) and shadow parameters \(^{}\). Target smoothing factor \(\). Learning rate \(\).  Neural networks \(g_{}(\,|)\), \(b_{}^{(1)}()\), and \(b_{}^{(2)}()\). Temperature parameter \(\). Discount factor \(\).
1:for each training step do
2:\(\) Extend the Replay Buffer.
3:\(_{t}=g_{}^{-1}(|_{t})\), \( p_{}()\).
4:\(_{t+1} p_{T}(|_{t},_{t})\).
5:\(\{(_{t},_{t},r_{t}, _{t+1})\}\).
6:\(\) Update Policy.
7:\((_{t},_{t},r_{t},_{t+1})\).
8:\(Q_{}(_{t},_{t})=(p_{}(g_{ }(_{t}|_{t}))_{i_{n}}| (_{g_{}^{i}}(_{t}^{i-1}|_{t}))|)\). \(\) Eq. (11)
9:\(V_{^{}}(_{t+1})=-_{i_{t}}| (_{g_{^{}}^{i}}(_{t+1}))|\). \(\) Eq. (11)
10:\(Q_{}^{(1)}(_{t},_{t})=Q_{}(_{t}, _{t})+b_{}^{(1)}(_{t})\) and \(Q_{}^{(2)}(_{t},_{t})=Q_{}(_{t}, _{t})+b_{}^{(2)}(_{t})\). \(\) Eq. (12)
11:\(V_{^{}}^{}(_{t+1})=V_{^{}}(_{t+1})+(b_{^{}}^{(1)}(_{t+1}),b_{^{ }}^{(2)}(_{t+1}))\). \(\) Eq. (13)
12:\(()=(Q_{}^{(1)}(_{t},_{t })-(r_{t}+ V_{^{}}^{}(_{t+1})))^{2}+ (Q_{}^{(2)}(_{t},_{t})-(r_{t}+ V_{ ^{}}^{}(_{t+1})))^{2}\). \(\) Eq. (4)
13:\(+_{}()\).
14:\(^{}(1-)^{}+\).
15:endfor ```

**Algorithm 1** Pseudo Code of the Training Process of MEow

## 4 Experiments

In the following sections, we first present an intuitive example of MEow trained in a two-dimensional multi-goal environment  in Section 4.1. We then compare MEow's performance against several continuous-action RL baselines in five MuJoCo environments [32; 33] in Section 4.2. Next, in Section 4.3, we evaluate MEow's performance on a number of Omniverse Isaac Gym environments  simulated based on real-world robotic application scenarios. Lastly, in Section 4.4, we provide an ablation analysis to inspect the effectiveness of each proposed technique. Among all experiments, we maintain the same model architecture, while adjusting inputs and outputs according to the state space and action space for each environment. We construct \(g_{}\) using the additive coupling layers  with element-wise linear transformations, utilize a unit Gaussian as \(p_{}\), and model the learnable adaptive reward shifting functions \(b_{}\) as multi-layer perceptrons (MLPs). For detailed descriptions of the experimental setups, please refer to Appendix A.5.

### Evaluation on a Multi-Goal Environment

In this subsection, we present an example of MEow trained in a two-dimensional multi-goal environment . The environment involves four goals, indicated by the red dots in Fig. 2 (a). The reward function is defined by the negative Euclidean distance from each state to the nearest goal, and the corresponding reward landscape is depicted using contours in Fig. 2 (a). The gradient map in Fig. 2 (a) represents the soft value function predicted by our model. The blue lines extending from the center represent the trajectories produced using our policy.

As illustrated in Fig. 2 (a), our model's soft value function predicts higher values around the goals, suggesting successful learning of the goal positions through rewards. In addition, the trajectories demonstrate our agent's correct transitions towards the goals, which validates the effectiveness of our learned policy. To illustrate the potential impact of approximation errors that might emerge when employing previous soft value estimation methods, we compare three calculation methods for the soft value function: (I) Our approach (i.e., Eq. (11)): \(V_{}(_{t})\), (II) SQL-like (i.e., Eq. (6)): \((_{i=1}^{M}(_{t}, ^{(i)})/)}{_{}(^{(i)}|_{t})})\), and (III) SAC-like (i.e., Eq. (7)): \(_{i=1}^{M}(Q_{}(_{t},^{(i)})- _{}(^{(i)}|_{t}))\), where \(\{^{(i)}\}_{i=1}^{M}\) is sampled from \(_{}\). The approximation errors of the soft value functions at the initial state are calculated using the Euclidean distances between (I) and (II), and between (I) and (III), for various values of \(M\). As depicted in Fig. 2 (b), the blue line and the orange line decreases slowly with respect to \(M\). These results suggest that Monte Carlo estimation converges slowly, making approximation methods such as Eqs. (6) and (7) challenging to achieve accurate predictions.

### Performance Comparison on the MuJoCo Environments

In this experiment, we compare MEow with several commonly-used continuous control algorithms on five MuJoCo environments  from Gymnasium . The baseline algorithms include SQL , SAC , deep deterministic policy gradient (DDPG) , twin delayed deep deterministic policy gradient (TD3) , and proximal policy optimization (PPO) . The results for SAC, DDPG, TD3, and PPO were reproduced using Stable Baseline 3 (SB3) , utilizing SB3's refined hyperparameters. The results for SQL were reproduced using our own implementation, as SB3 does not support SQL and the official code is not reproducible. Our implementation adheres to SQL's original paper. Each method is trained independently under five different random seeds, and the evaluation curves for each environment are presented in the form of the means and the corresponding confidence intervals.

As depicted in Fig. 3, MEow performs comparably to SAC and outperforms the other baseline algorithms in most of the environments. Furthermore, in environments with larger action and state dimensionalities, such as 'Ant-v4' and 'Humanoid-v4', MEow offers performance improvements over SAC and exhibits fewer spikes in the evaluation curves. These results suggest that MEow is capable of performing high-dimensional tasks with stability. To further investigate the performance difference between MEow and SAC, we provide a thorough comparison between MEow, SAC , Flow-SAC [10; 11], and their variants in Appendix A.4.2. The results indicate that the training process involving policy evaluation and policy improvement steps may be inferior to our proposed training process with a single objective. In the next subsection, we provide a performance examination using the simulation environments from the Omniverse Isaac Gym .

Figure 3: The results in terms of total returns versus the number of training steps evaluated on five MuJoCo environments. Each curve represents the mean performance, with shaded areas indicating the 95% confidence intervals, derived from five independent runs with different seeds.

Figure 2: (a) The soft value function and the trajectories generated using our method on the multi-goal environment. (b) The estimation error evaluated at the initial state under different choices of \(M\).

### Performance Comparison on the Omniverse Issac Gym Environments

In this subsection, we examine the performance of MEow on a variety of robotic tasks simulated by Omniverse Isaac Gym , a GPU-based physics simulation platform. In addition to 'Ant' and 'Humanoid', we employ four additional tasks: 'Ingenuity', 'ANYmal', 'AllegroHand', and 'Franka-Cabinet'. All of them are designed based on real-world robotic application scenarios. 'Ingenuity' and 'ANYmal' are locomotion environments inspired by NASA's Ingenuity helicopter and ANYbodies' industrial maintenance robots, respectively. On the other hand, 'AllegroHand' and 'FrankaCabinet' focus on executing specialized manipulative tasks with robotic hands and arms, respectively. A demonstration of these tasks is illustrated in Fig. 5.

In this experimental comparison, we adopt SAC as a baseline due to its excellent performance in the MuJoCo environments. The evaluation results are presented in Fig. 5. The results demonstrate that MEow exhibits superior performance on 'Ant (Isaac)' and 'Humanoid (Isaac)'. In addition, MEow consistently outperforms SAC across the four robotic environments (i.e., 'Ingenuity', 'ANYmal', 'AllegroHand', and 'FrankaCabinet'), indicating that our algorithm possesses the ability to perform challenging robotic tasks simulated based on real-world application scenarios.

### Ablation Analysis

In this subsection, we provide an ablation analysis to examine the effectiveness of each technique introduced in Section 3.2.

**Training Techniques.** Fig. 6 compares the performance of three variants of MEow: 'MEow (Vanilla)', 'MEow (+LRS)', and 'MEow (+LRS & SCDQ)', across five MuJoCo environments. The results show that 'MEow (Vanilla)' consistently underperforms, with its total returns demonstrating negligible or no growth throughout the training period. In contrast, the variants incorporating translation functions demonstrate significant performance enhancements. This observation highlights the importance of including \(b_{}\) in the model design. In addition, the comparison between 'MEow (+LRS)' and 'MEow (+LRS & SCDQ)' suggests that our reformulated approach to clipped double Q-learning  improves the final performance by a noticeable margin.

**Inference Technique.** Fig. 7 compares the performance of two variants of MEow: 'MEow (Stochastic)' and 'MEow (Deterministic)'. The former samples action based on \(_{t}_{}(|_{t})\) while the latter derive action according to \(_{t}=*{argmax}_{}Q_{}(_{t}, )=g_{}^{-1}(|_{t})\). As shown in the figure, MEow with a deterministic policy outperforms its stochastic variant, suggesting that a deterministic policy may be more effective for MEow's inference.

## 5 Conclusion

In this paper, we introduce MEow, a unified MaxEnt RL framework that facilitates exact soft value calculations without the need for Monte Carlo estimation. We demonstrate that MEow can be optimized using a single objective function, which streamlines the training process. To further enhance MEow's performance, we incorporate two techniques, learnable reward shifting and shifting-based clipped double Q-learning, into the design. We examine the effectiveness of MEow via experiments conducted in five MoJoCo environments and six robotic tasks simulated by Omniverse Isaac Gym. The results validate the superior performance of MEow compared to existing approaches.

## Limitations and Discussions

As discussed in Section 3.2, deterministic policies typically offer better performance compared to their stochastic counterparts. Although our implementation of MEow supports deterministic inference, this capability is based on the assumptions that the Jacobian determinants of the non-linear transformations are constants with respect to their inputs, and that \(*{argmax}_{}p_{}()\) can be efficiently derived. These assumptions may not hold for certain types of flow-based models. Therefore, exploring effective architectural choices for MEow represents a promising direction for further investigation.

On the other hand, the training speed of MEow is around \(2.3\) slower than that of SAC, even though updates according to \(()\) are bypassed in MEow. According to our experimental observations, the computational bottleneck of MEow may lie in the inference speed of the flow-based model during interactions with environments. While this speed is significantly faster than many iterative methods, such as MCMC or variational inference, it is still slower compared to the inference speed of Gaussian models. As a result, enhancing the inference speed of flow-based models represents a potential avenue for further improving the training efficiency of MEow.

Finally, our hyperparameter sensitivity analysis, as presented in A.4.5, indicates that our current approach requires different values of \(\) to achieve optimal performance. Since hyperparameter tuning often demands significant computational resources, establishing a more generalized parameter setting or developing an automatic tuning mechanism for \(\) presents an important direction for future exploration.

Figure 6: The performance comparison of MEow’s variants (i.e., ‘MEow (Vanilla)’, ‘MEow (+LRS)’, and ‘MEow (+LRS & SCDQ’)’ on five MuJoCo environments. Each curve represents the mean performance of five runs, with shaded areas indicating the 95% confidence intervals.

Figure 7: Performance comparison between MEow with a deterministic policy and MEow with a stochastic policy on five MuJoCo environments. Each curve represents the mean performance of five runs, with shaded areas indicating the 95% confidence intervals.