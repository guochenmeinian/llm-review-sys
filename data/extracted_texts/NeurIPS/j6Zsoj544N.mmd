# Does Worst-Performing Agent Lead the Pack?

Analyzing Agent Dynamics in Unified Distributed SGD

Jie Hu   Yi-Ting Ma   Do Young Eun

Department of Electrical and Computer Engineering

North Carolina State University

{jhu29, yma42, dyeun}@ncsu.edu

###### Abstract

Distributed learning is essential to train machine learning algorithms across _heterogeneous_ agents while maintaining data privacy. We conduct an asymptotic analysis of Unified Distributed SGD (UD-SGD), exploring a variety of communication patterns, including decentralized SGD and local SGD within Federated Learning (FL), as well as the increasing communication interval in the FL setting. In this study, we assess how different sampling strategies, such as _i.i.d._ sampling, shuffling, and Markovian sampling, affect the convergence speed of UD-SGD by considering the impact of agent dynamics on the limiting covariance matrix as described in the Central Limit Theorem (CLT). Our findings not only support existing theories on linear speedup and asymptotic network independence, but also theoretically and empirically show how efficient sampling strategies employed by individual agents contribute to overall convergence in UD-SGD. Simulations reveal that a few agents using highly efficient sampling can achieve or surpass the performance of the majority employing moderately improved strategies, providing new insights beyond traditional analyses focusing on the worst-performing agent.

## 1 Introduction

Distributed learning deals with the training of models across multiple agents over a communication network in a distributed manner, while addressing the challenges of privacy, scalability, and high-dimensional data . Each agent \(i[N]\) holds a private dataset \(_{i}\) and an agent-specified loss function \(F_{i}:^{d}_{i}\) that depends on the model parameter \(^{d}\) and a data point \(X_{i}\). The goal is then to find a local minima \(^{*}\) of the objective function \(f()_{i=1}^{N}f_{i}()\), where agent \(i\)'s loss function \(f_{i}()_{X_{i}}[F_{i}(,X)]\) and \(_{i}\) represents the target distribution of data for agent \(i\).1 Each agent \(i\) can locally compute the gradient \( F_{i}(,X)^{d}\) w.r.t. \(\) for every sampled data point \(X_{i}\). Due to the distributed nature, \(\{_{i}\}_{i[N]}\) and \(\{_{i}\}_{i[N]}\) are not necessarily identically distributed over \([N]\) so that the minima of each local function \(f_{i}()\) can be far away from \(\). This is particularly relevant in decentralized training data, e.g., Federated Learning (FL) with _heterogeneous_ data across data centers or devices .

In this paper, we focus on Unified Distributed SGD (UD-SGD), where each agent \(i[N]\) updates its model parameter \(^{i}_{n+1}\) in a two-step process:

\[\ \ ^{i}_{n+1/2}=^{i}_{n}-_{n+1}  F_{i}(^{i}_{n},X^{i}_{n}),\] (1a)

#### Aggregation:

\[^{i}_{n+1}=_{j=1}^{N}w_{n}(i,j)^{j}_{n+1/2},\] (1b)

where \(_{n}\) denotes the step size, \(X^{i}_{n}\) is the data sampled by agent \(i\) at time \(n\) (i.e., agent dynamics), and \(_{n}\!=\![w_{n}(i,j)]_{i,j[N]}\) represents the doubly-stochastic communication matrix satisfying \(w_{n}(i,j) 0\) and \(^{T}_{n}\!=\!^{T}\), \(_{n}\!\!=\!\). In the special case of \(N=1\), (1) simplifies to the vanilla SGD where \(_{n}=1\) for all \(n\). UD-SGD covers a wide range of distributed algorithms, e.g., decentralized SGD (DSGD) , distributed SGD with changing topology (DSGD-CT) , local SGD (LSGD) in FL , and its variant aimed at reducing communication costs (LSGD-RC) .

**Versatile Communication Patterns \(\{_{n}\}\):** For visualization, we depict the scenarios of UD-SGD (1) in Figure 1. In DSGD, each agent (node) in the graph communicates with its neighbors after each SGD computation via \(_{n}\), representing the underlying network topology. As a special case, central server-based aggregation, forming a fully connected network, translates \(_{n}\) into a rank-1 matrix \(_{n}\!=\!^{T}/N\). To minimize communication expenses, FL variants allow each agent to perform multiple SGD steps before aggregation , resulting in a communication interval of length \(K\) and a consistent pattern \(_{n}=\) for \(n=mK, m\), and \(_{n}=_{N}\) otherwise. In particular, i) \(\!=\!^{T}/N\) corresponds to LSGD with full agent participation (LSGD-FP) ; ii) \(\) is a random matrix generated by partial agent participation (LSGD-PP) ; iii) \(\) is generated by Metropolis-Hasting algorithm in decentralized setting, e.g., hybrid LSGD (HLSGD)  and decentralized FL (DFL) . We defer further discussion of \(\) to Appendix F.1.

**Markovian vs _i.i.d._ Sampling:** Agents typically employ _i.i.d._ or Markovian sampling, as illustrated in the bottom brown box of Figure 1. In cases where agents have full access to their data, DSGD with _i.i.d_ sampling has been extensively studied . In FL, many application-oriented LSGD variants have been investigated . However, these works solely focus on _i.i.d._ sampling, restricting their applicability to Markovian sampling scenarios.

Markovian sampling, which has received increased attention in limited settings (see Table 1), is vital where agents lack independent data access. For instance, in statistical applications, agents with an unknown a priori distribution often use Markovian sampling over _i.i.d._ sampling . In HLSGD across device-to-device (D2D) networks , random walks reduce communication costs compared to the frequent aggregations required by Gossip algorithms . For single-agent scenarios, vanilla SGD with Markovian noise, as applied in a D2D network, has shown improved communication efficiency and privacy . In contrast, for agents with full data access, Markov Chain Monte Carlo (MCMC) methods can be more efficient than _i.i.d._ sampling, especially in high-dimensional spaces with constraints , where acceptance-rejection methods  lead to computational inefficiency (e.g., wasted samples) due to multiple rejections before obtaining a sample that satisfies constraints . In addition, shuffling methods can be considered as high-order Markov chains , which achieves faster convergence than _i.i.d._ sampling .

**Limitations of Non-Asymptotic Analysis on Agent's Sampling Strategy:** Recent studies on the non-asymptotic behavior of DSGD and LSGD variants under Markovian sampling, as summarized in Table 1, have made significant strides. However, these works often fall short in accurately revealing the statistical influence of _each_ agent dynamics \(\{X^{i}_{n}\}\) on the performance of UD-SGD. For instance,  proposed the error bound \(O((1/)}{n^{-8}})\), where \(a(0.5,1]\) and \(\) denotes the identical mixing rate for all agents, overlooking agent heterogeneity in sampling strategy. A similar assumption to \(\) is also evident in . More recent contributions from  have attempted to relax these constraints by considering a finite-time bound of \(O(_{mix}^{2}/(n+1))\), where \(_{mix}\) is the mixing time of the slowest agent. This approach, however, inherently focuses on _the worst-performing agent_, neglecting how other agents with faster mixing rates might positively influence the system.2 Such an analysis fails to capture the collective impact of _other_ agents on the overall system performance,

Figure 1: GD-SGD algorithm with a communication network of \(N=5\) agents, each holding potentially distinct datasets; e.g., agent \(j\) (in blue) samples \(_{j}\)_i.i.d._ and agent \(i\) (in red) samples \(_{i}\) via Markovian trajectory.

[MISSING_PAGE_FAIL:3]

intervals (Assumption 2.3-ii) and in proving the scaled consensus error's boundedness (Lemma B.1). Furthermore, we reformulate UD-SGD as a stochastic approximation-like iteration and tackle the Markovian noise term using the Poisson equation, a technique previously confined only to vanilla SGD with Markovian sampling [17; 38; 52]. The key here is to devise the noise decomposition that separates the consensus error among all agents from the error caused by the bias from the Markov chain, which aligns with the target distribution only asymptotically at infinity, not at finite times.

\(\) In analyzing (3), we derive the exact form of \(\) as \(}_{i=1}^{N}_{i}\). Here, \(_{i}\) is the limiting covariance matrix of agent \(i\), which depends mainly on its sampling strategy \(\{X_{n}^{i}\}\). This allows us to show that improving _individual_ agents' sampling strategy can reduce the covariance in CLT, which in turn implies a smaller mean-square error (MSE) for large time \(n\). This is a significant advancement over previous finite-sample bounds that only account for the worst-performing agent and do not fully capture the effect of individual agent dynamics on overall system performance. Our CLT result (3) also treats recent findings in  as a very special case with \(N=1\), where the relationship therein between the sampling efficiency of the Markov chain and the limiting covariance matrix in the CLT of vanilla SGD, can carry over to our UD-SGD.

\(\) We demonstrate that our analysis supports recent findings from studies such as , which exhibited linear speedup scaling with the number of agents under LSGD-FP with Markovian sampling; and [62; 61], which examined the notion of 'asymptotic network independence' for DSGD with _i.i.d._ sampling, where the convergence of the algorithm (1) at large time \(n\) depends solely on the left eigenvector of \(_{n}\) (\(\) considered in this paper) rather than the specific communication network topology encoded in \(_{n}\), but now under Markovian sampling. We extend these findings in view of CLT to a broader range of communication patterns \(\{_{n}\}\) and general sampling strategies \(\{X_{n}^{i}\}\).

\(\) We conduct numerical experiments using logistic regression and neural network training with several choices of agents' sampling strategies, including a recently proposed one via nonlinear Markov chain . Our results uncover a key phenomenon: a handful of compliant agents adopting highly efficient sampling strategies can match or exceed the performance of the majority using moderately improved strategies. This finding is crucial for practical optimization in large-scale learning systems, moving beyond the current literature that only considers the worst-performing agent in more restrictive settings.

## 2 Preliminaries

**Basic Notations:** We use \(\|\|\) to indicate the Euclidean norm of a vector \(^{d}\) and \(\|\|\) to indicate the spectral norm of a matrix \(^{d d}\). The identity matrix of dimension \(d\) is denoted by \(_{d}\), and the all-one (resp. all-zero) vector of dimension \(N\) is denoted by \(\) (resp. \(\)). Let \(^{T}/N\). The diagonal matrix with the entries of \(\) on the main diagonal is written as \(()\). We also use '\(\)' for Loewner ordering such that \(\) is equivalent to \(^{T}(-) 0\) for any \(^{d}\).

**Asymptotic Covariance Matrix:** Asymptotic variance is a widely used metric for evaluating the second-order properties of Markov chains associated with a scalar-valued test function in the MCMC literature, e.g., Chapter 6.3 , and asymptotic covariance matrix is its multivariate version for a vector-valued function. Specifically, we consider a finite, irreducible, aperiodic and positive recurrent (ergodic) Markov chain \(\{X_{n}\}_{n 0}\) with transition matrix \(\) and stationary distribution \(\), and the estimator \(_{n}()_{s=0}^{n-1}(X_{s})\) for any vector-valued function \(:[N]^{d}\). According to the ergodic theorem [12; 13], we have \(_{n}_{n}()=_{}( )\) as. As defined in [13; 38], the asymptotic covariance matrix \(_{X}()\) for a vector-valued function \(()\) is given by

\[_{X}()\!\!_{n}\!n (_{n}())\!=\!_{n} \{_{n}_{n}^{T}\},\] (4)

where \(_{n}_{s=0}^{n-1}((X_{s})-_{ {}}())\). By following the algebraic manipulations in [12, Theorem 6.3.7] for asymptotic variance (univariate version), we can rewrite (4) in a matrix form such that

\[_{X}()=^{T}()(-_{N}+^{T}),\] (5)

where \([(1),,(N)]^{T}^{N  d}\) and \([_{N}-+^{T} ]^{-1}\). This matrix form explicitly shows the dependence on the transition matrix \(\) and its stationary distribution \(\), and will be utilized in our Theorem 3.3.

**Model Description:** The UD-SGD in (1) can be expressed in a compact iterative form, i.e., we have

\[_{n+1}^{i}=_{j=1}^{N}w_{n}(i,j)(_{n}^{j}-_{n+1} F_{j} (_{n}^{j},X_{n}^{j})),\] (6)

at each time \(n\), where each agent \(i\) samples according to its own Markovian trajectory \(\{X_{n}^{i}\}_{n 0}\) with stationary distribution \(_{i}\) such that \(_{X_{i}}[F_{i}(,X)]\!=\!f_{i}()\). Let \(K_{l}\) denote the communication interval between the \((l-1)\)-th and \(l\)-th aggregation among \(N\) agents, and \(n_{l}_{m=1}^{l}K_{m}\) be the time instance for the \(l\)-th aggregation. We also define \(_{n}_{l}\{l:n_{l} n\}\) as the index of the upcoming aggregation at time \(n\) such that \(K_{_{n}}\) indicates the communication interval for the \(_{n}\)-th aggregation, or more precisely, the length of the communication interval that includes the time index \(n\). The communication pattern follows that \(_{n}=_{n}\) if \(n n_{l}\) and \(_{n}=\) otherwise for \(l 1\), where the examples of \(\) will be discussed in Appendix F.1. Note that i) when \(K_{l}=1\), (6) reduces to DSGD; ii) when \(K_{l}=K>1\), (6) becomes the local SGD in FL. iii) When \(K_{l}\) increases with \(l\), we recover some choices of \(K_{l}\) studied in  beyond LSGD-RC with _i.i.d._ sampling. This increasing communication interval aims to further reduce the frequency of aggregation among agents for lower communication costs, but now under a Markovian sampling setting and a wider range of communication patterns. We below state the assumptions needed for the main theoretical results.

**Assumption 2.1** (Regularity of the gradient).: _For each \(i[N]\) and \(X^{i}\), the function \(F_{i}(,X)\) is \(L\)-smooth in terms of \(\), i.e., for any \(_{1},_{2}^{d}\),_

\[\| F_{i}(_{1},X)- F_{i}(_{2},X)\| L\|_{1}- _{2}\|.\] (7)

_In addition, we assume that the objective function \(f\) is twice continuously differentiable and \(\)-strongly convex only around the local minima \(^{*}\), i.e.,_

\[^{2}f(^{*})_{d}.\] (8)

Assumption 2.1 imposes the regularity conditions on the gradient \( F_{i}(,X)\) and Hessian matrix of the objective function \(f()\), as is commonly assumed in . Note that (7) requires per-sample Lipschitzness of \( F_{i}\) and is stronger than the Lipschitzness of its expected version \( f_{i}\), which is commonly assumed under _i.i.d_ sampling setting . However, we remark that this is in line with previous work on DSGD and LSGD-FP under Markovian sampling as well , because \( F_{i}(,X)\) is no longer the unbiased stochastic version of \( f_{i}()\) and the effect of \(\{X_{n}^{j}\}\) has to be taken into account in the analysis. The local strong convexity at the minimizer is commonly assumed to analyze the convergence of the algorithm under both asymptotic and non-asymptotic analysis .

**Assumption 2.2** (Ergodicity of Markovian sampling).: \(\{X_{n}^{i}\}_{n 0}\) _is an ergodic Markov chain with stationary distribution \(_{i}\) such that \(_{X_{i}}[F_{i}(,X)]=f_{i}()\), and is independent from \(\{X_{n}^{j}\}_{n 0},j i\)._

The ergodicity of the underlying Markov chains, as stated in Assumption 2.2, is commonly assumed in the literature . This assumption ensures the asymptotic unbiasedness of the loss function \(F_{i}(,)\), which takes _i.i.d._ sampling as a special case.

**Assumption 2.3** (Decreasing step size and slowly increasing communication interval).: _i) For bounded communication interval \(K_{_{n}} K, n\), we assume the polynomial step size \(_{n}=1/n^{a}\) and \(a(0.5,1]\); Or ii) If \(K_{_{n}}\) as \(n\), we assume \(_{n}=1/n\) and define \(_{n}=_{n}K_{_{n}}^{L+1}\), where the sequence \(\{K_{l}\}_{l 0}\) satisfies \(_{n}_{n}^{2}<\), \(K_{_{n}}=o(_{n}^{-1/2(L+1)})\), and \(_{l}_{n_{l}+1}/_{n_{l+1}+1}=1\)._

In Assumption 2.3, the polynomial step size \(_{n}\) is standard in the literature and it has the property \(_{n}_{n}=\), \(_{n}_{n}^{2}<\). Inspired by , we introduce \(_{n}\) to control the step size within each \(l\)-th communication interval with length \(K_{l}\) to restrict the growth of \(K_{l}\). Specifically, \(_{n}_{n}^{2}<\) and \(K_{_{n}}=o(_{n}^{-1/2(L+1)})\) ensure that \(_{n} 0\) and \(K_{_{n}}\) does not increase too fast in \(n\). \(_{l}_{n_{l}+1}/_{n_{l+1}+1}=1\) sets the restriction on the increment from \(n_{l}\) to \(n_{l+1}\). Several practical forms of \(K_{l}\) suggested by , including \(K_{l}(l)\) and \(K_{l}(l)\), also satisfy Assumption 2.3-ii). We defer to Appendix A the mathematical verification of these two types of \(K_{l}\), together with the practical implications of increasing communication interval \(K_{l}\).

**Remark 1**.: _In Assumption 2.3, we incorporate an increasing communication interval along with a step size \(_{n}=1/n\). This complements the choice of step size \(_{n}\) in [51, Assumption \(3.3\)], where \(_{n}=1/n^{a}\) for \(a(0.5,1)\). It is important to note, however, that the increasing communication interval specified in [51, Assumption \(3.2\)] is applicable only in i.i.d sampling. Under the Markoviansampling framework, the expression \( F_{i}(,X)- f_{i}()\) loses its unbiased and Martingale difference properties. Consequently, the Martingale CLT application as utilized by  does not directly extend to Markovian sampling. To address this, we adapted techniques from  to accommodate the increasing communication interval within the Markovian sampling setting and various communication patterns. This adaptation necessitates \(_{n}=1/n\), a specification not covered in . Exploring more general forms of \(K_{l}\) that could relax this assumption is outside the scope of our current study._

**Assumption 2.4** (Stability on model parameter).: _We assume \(_{n}\|_{n}^{i}\|<\) almost surely \( i[N]\)._

Assumption 2.4 claims that the sequence of \(\{_{n}^{i}\}\) always remains in a path-dependent compact set. It is to ensure the stability of the algorithm that serves the purpose of analyzing the convergence, which is often assumed under the asymptotic analysis of vanilla SGD with Markovian noise . As mentioned in , checking Assumption 2.4 is challenging and requires case-by-case analysis, even under _i.i.d._ sampling. Only recently the stability of SGD under Markovian sampling has been studied in , but the result for UD-SGD remains unknown in the literature. Thus, we analyze each agent's sampling strategy in the asymptotic regime under this stability condition.

**Assumption 2.5** (Contraction property of communication matrix).: _i). \(\{_{n}\}_{n 0}\) is independent of the sampling strategy \(\{X_{n}^{i}\}_{n 0}\) for all \(i[N]\) and is assumed to be doubly-stochastic for all \(n\); ii). At each aggregation step \(n_{l}\), \(_{n_{l}}\) is independently generated from some distribution \(_{n_{l}}\) such that \(\|_{_{n_{l}}}[^{T}]- \| C_{1}\!<\!1\) for some constant \(C_{1}\)._

The doubly-stochasticity of \(_{n}\) in Assumption 2.5-i) is widely assumed in the literature . Assumption 2.5-ii) is a contraction property to ensure that agents employing UD-SGD will asymptotically achieve the consensus, which is also common in . Examples of \(\) that satisfy Assumption 2.5-ii), e.g., Metropolis-Hasting matrix, partial agent participation in FL, are deferred to Appendix F.1 due to space constraint.

## 3 Asymptotic Analysis of UD-SGD

**Almost Sure Convergence:** Let \(_{n}_{i=1}^{N}_{n}^{i}\) represent the consensus among all the agents at time \(n\), we establish the asymptotic consensus of the local parameters \(_{n}^{i}\), as stated in Lemma 3.1.

**Lemma 3.1**.: _Under Assumptions 2.1, 2.3, 2.4 and 2.5, the consensus error \(_{n}^{i}-_{n}\) diminishes to zero at the rate specified below: Almost surely, for every agent \(i[N]\),_

\[\|_{n}^{i}-_{n}\|=O(_{n})&,\\ O(_{n})&.\] (9)

Lemma 3.1 indicates that all agents asymptotically reach consensus at a rate of \(O(_{n})\) (or \(O(_{n})\)). This finding extends the scope of [58, Proposition 1], incorporating considerations for Markovian sampling, FL settings, and increasing communication interval \(K_{l}\). The proof, detailed in Appendix B, primarily tackles the challenge of establishing the boundedness of the sequences \(\{_{n}^{-1}(_{n}^{i}-_{n})\}\) (or \(\{_{n}^{-1}(_{n}^{i}-_{n})\}\)) almost surely for all \(i[N]\). This is specifically analyzed in Lemma B.1. Next, with additional Assumption 2.2, we are able to obtain the almost sure convergence of \(_{n}\) to \(^{*}\).

**Theorem 3.2**.: _Under Assumptions 2.1 - 2.5, the consensus \(_{n}\) converges to \(\) almost surely, i.e.,_

\[_{n}_{^{*}}\|_{n}-^{*}\| =0\] (10)

Theorem 3.2 is achieved by decomposing the Markovian noise term \( F_{i}(_{n}^{i},X_{n}^{i})- f_{i}(_{n}^{i})\), using the Poisson equation technique as discussed in , into a Martingale difference noise term, along with additional noise terms. We then reformulate (6) into an iteration akin to stochastic approximation, as depicted in (56). The subsequent step involves verifying the conditions on these noise terms under our stated assumptions. Crucially, this theorem also establishes that UD-SGD ensures an almost sure convergence of each agent to a local minimum \(^{*}\), even in scenarios where the communication interval \(K_{l}\) gradually increases, in accordance with Assumption 2.3-ii). The detailed proof of this theorem is provided in Appendix C.

**Central Limit Theorem:** Let \(_{i}\!\!_{X^{i}}( F_{i}(^{*}, ))\) represent the asymptotic covariance matrix (defined in (5)) associated with each agent \(i[N]\), given their sampling strategy \(\{X_{n}^{i}\}\) and function \( F_{i}(^{*},)\). Define \(}_{i=1}^{N}_{i}\). We assume the polynomial step-size \(_{n}\!\!_{*}/n^{a}\), \(a\!\!(0.5,1]\) and \(_{}>0\). In the case of \(a=1\), we further assume \(_{}>1/2\), where \(\) is defined in (8). For notational simplicity, and without loss of generality, our remaining CLT result is stated while conditioning on the event that \(\{_{n}^{*}\}\) for some \(^{*}\).

**Theorem 3.3**.: _Let Assumptions 2.1 - 2.5 hold. Then,_

\[_{n}^{-1/2}(_{n}-^{*})[n]{ dist.}(,),\] (11)

_where the limiting covariance matrix \(\) is in the form of_

\[=_{0}^{}e^{t}e^{t}dt.\] (12)

_Here, we have \(=-\) if \(a(0.5,1)\), or \(=_{d}/2_{*}-\) if \(a=1\), where \(\) is defined in (8)._

_Moreover, let \(_{n}=_{s=0}^{n-1}_{s}\) and \(^{}=^{-1}^{-1}\). For \(a(0.5,1)\), we have_

\[(_{n}-^{*})[n]{ dist.}(,^{}),\] (13)

The proof, presented in Appendix D, addresses the technical challenges in deriving the CLT for UD-SGD, specifically the second-order conditions in decomposing the Markovian noise term, which is not present in the _i.i.d._ sampling case . We decompose \( F_{i}(_{n},X_{n}^{i})- f_{i}(_{n})\) into three parts in (48) using Poisson equation: \(e_{n+1}^{i},_{n+1}^{i},_{n+1}^{i}\). The consensus error \(_{n}^{i}-_{n}\) embedded in noise terms \(e_{n+1}^{i}\) and \(_{n+1}^{i}\) is a new factor, whose characteristics have been quantified in our Lemma 3.1 but are not present in the single-agent scenario analyzed as an application of stochastic approximation in . The specifics of this analysis are expanded upon in Appendices D.1 to D.3. We require \(_{*}\!>\!1/2\) for \(a\!=\!1\) to ensure that the largest eigenvalue of \(\) is negative, as this is a necessary condition for the existence of \(\) in (12) (otherwise integration diverges). In the case where there is only one agent (\(N\!=\!1\)), \(\) and \(^{}\) reduce to the matrices specified in the CLT result of vanilla SGD . In addition, for a special case of constant communication interval in Assumption 2.3-i) and _i.i.d._ sampling as shown in Table 1, we recover the CLT of LSGD-RC in . See Appendix E for detailed discussions.

Theorem 3.3 has significant implications for the MSE of \(\{_{n}\}\) for large time \(n\), i.e., \([\|_{n}-^{*}\|^{2}]\!=\!_{i=1}^{d}_{i}^{T} [(_{n}-^{*})(_{n}-^{*})^{T}]_{i} \!\!_{n}_{i=1}^{d}_{i}^{T}_{i} =_{n}()\), where \(_{i}\) is the \(d\)-dimensional vector of all zeros except \(1\) at the \(i\)-th entry. This indicates that a smaller limiting covariance matrix \(\), according to the Loewner order, results in a smaller trace of \(\) and consequently in a reduced MSE for large \(n\). Consideration for smaller \(\) will be presented in the next section, where agents have the opportunity to improve their sampling strategies.

**Remark 2**.: _Studies by  have shown that in DSGD with a fixed doubly-stochastic matrix \(\), the influence of communication topology diminishes after a transient period. Our Theorem 3.3 extends these findings to Markovian sampling and a broader spectrum of communication patterns as in Table 1. This extension is based on the fact that the consensus error, impacted primarily by the communication pattern, decreases faster than the CLT scale \(O(})\) and is thus not the dominant factor in the asymptotic regime, as suggested by Lemma 3.1._

**Remark 3**.: _Recent studies have highlighted linear speedup with increasing number of agents \(N\) in the dominant term of their finite-sample error bounds under DSGD-CT with i.i.d. sampling  and LSGD-FC with Markovian sampling . However, our Theorem 3.3 demonstrates this phenomenon under more diverse communication patterns and Markovian sampling in Table 1 via the leading term \(\) in our CLT. Specifically, it scales with \(1/N\), i.e. \(\!=\!}/N\), where \(}\!=\!_{i=1}^{N}_{i}\) denotes the average limiting covariance matrices across all \(N\) agents and \(_{i}\!=\!_{0}^{}e^{t}_{i}e^{ t}dt\), suggesting that the MSE \([\|_{n}-^{*}\|^{2}]\) will be improved by \(1/N\). A similar argument also applies to \(^{}\) in (13), i.e., \(^{}=}^{}/N\), where \(}^{}=_{i=1}^{N}_{i}^{}\) and \(_{i}^{}=^{-1}_{i}^{-1}\)._

**Impact of Agent's Sampling Strategy:** In the literature, the mixing time-based technique has been widely used in the non-asymptotic analysis in SGD, DSGD and various LSGD variants in FL , i.e., for each agent \(i[N]\) and some constant \(C\),

\[\| F_{i}(,X_{n}^{i})- f_{i}()\| C\|\|_{i}^{ n},\] (14)

where \(_{i}\) is the mixing rate of the underlying Markov chain. However, typical non-asymptotic analyses often rely on \(_{i}_{i}\) among \(N\) agents, i.e., the worst-performing agent in their finite-time bounds , or assume an identical mixing rate across all \(N\) agents .

In contrast, Remark 3 highlights that each agent holds its own limiting covariance matrices \(_{i}\) and \(_{i}^{}\), which are predominantly governed by the matrix \(_{i}\), capturing the agent's sampling strategy\(\{X^{i}_{n}\}\) and _contributing equally_ to the overall performance of UD-SGD. For each agent \(i\), denote by \(^{X}_{i}\) and \(^{Y}_{i}\) the asymptotic covariance matrices associated with two candidate sampling strategies \(\{X^{i}_{n}\}\) and \(\{Y^{i}_{n}\}\), respectively. Let \(^{X}\) and \(^{Y}\) be the limiting covariance matrices of the distributed system in (12), where agent \(i\) employs \(\{X^{i}_{n}\}\) and \(\{Y^{i}_{n}\}\), respectively, while keeping other agents' sampling strategies unchanged. Then, we have the following result.

**Corollary 3.4**.: _For agent \(i\), if there exist two sampling strategies \(\{X^{i}_{n}\}_{n 0}\) and \(\{Y^{i}_{n}\}_{n 0}\) such that \(^{X}_{i}^{Y}_{i}\), we have \(^{X}^{Y}\)._

Corollary 3.4 directly follows from the definition of Loewner ordering, and Loewner ordering being closed under addition (i.e., \(\) implies \(++\)). It demonstrates that even a single agent improves its sampling strategy from \(\{X^{i}_{n}\}\) to \(\{Y^{i}_{n}\}\), it leads to an overall reduction in \(\) (in terms of Loewner ordering), thereby decreasing the MSE and benefiting the entire group of \(N\) agents. The subsequent question arises: _How do we identify an improved sampling strategy \(\{Y^{i}_{n}\}\) over the baseline \(\{X^{i}_{n}\}\)?_

This question has been partially addressed by [57; 48; 38], which qualitatively investigates the 'efficiency ordering' of two sampling strategies. In particular, [38; Theorem \(3.6\) (i)] shows that sampling strategy \(\{Y_{n}\}\) is more efficient than \(\{X_{n}\}\) if and only if \(_{X}()_{Y}()\) for any vector-valued function \(()^{d}\). Consequently, in the UD-SGD framework, employing a more efficient sampling strategy \(\{Y^{i}_{n}\}\) over the baseline \(\{X^{i}_{n}\}\) by agent \(i\) leads to \(_{X^{i}}( F_{i}(^{*},)) _{Y^{i}}( F_{i}(^{*},))\), thus satisfying \(^{X}_{i}^{Y}_{i}\). This finding, as per Corollary 3.4, implies an overall improvement in UD-SGD.

For illustration purposes, we list a few examples where two competing sampling strategies follow efficiency ordering: i) When an agent has complete access to the entire dataset (e.g., deep learning), shuffling techniques like single shuffling and random reshuffling are more efficient than _i.i.d._ sampling [38; 78]; ii) When an agent works with a graph-like data structure and employs a random walk, e.g., agent \(i\) in Figure 1, using non-backtracking random walk (NBRW) is more efficient than simple random walk (SRW) . iii) A recently proposed self-repellent random walk (SRRW) is shown to achieve _near-zero_ sampling variance, indicating even higher sampling efficiency than NBRW and SRW .3 This random-walk-based sampling finds a particular application in large-scale FL within D2D networks (e.g., mobile networks, wireless sensor networks), where each agent acts as an edge server or access point, gathering information from the local D2D network [37; 32]. Employing a random walk over local D2D network for each agent constitutes the sampling strategy.

Theorem 3.3 and Corollary 3.4 not only qualitatively compare these sampling strategies but also allow for a quantitative assessment of the overall system enhancement. Since every agent contributes equally to the limiting covariance matrix \(\) of the distributed system as in Remark 3, a key application scenario is to encourage a subset of compliant agents to adopt highly efficient strategies like SRRW, potentially yielding better performance than universally upgrading to slightly improved strategies like NBRW. This approach, more feasible and impactful in large-scale machine learning scenarios where some agents cannot freely modify their sampling strategies, is a unique aspect of our framework not addressed in previous works focusing on the worst-performing agent [80; 42; 68; 72].

## 4 Experiments

In this section, we empirically evaluate the effect of agents' sampling strategies under various communication patterns in UD-SGD. We consider the \(L_{2}\)-regularized binary classification problem

\[_{}f()_{i=1}^{N}f_{i}(),f_{i}()\!=\!\!_{j=1}^{B}\!\!(\!1\!+\!e^{^{T} _{i,j}}\!)\!-\!y_{i,j}(^{T}_{i,j}) \!+\!\|\|^{2},\] (15)

where the feature vector \(_{i,j}\) and its corresponding label \(y_{i,j}\) are held by agent \(i\), with a penalty parameter \(\) set to \(1\). We use the _ijcnn1_ dataset  with \(22\) features in each data point and \(50\)k data points in total, which is evenly distributed to two groups with \(50\) agents each (\(N=100\) agentsin total) and each agent holds \(B=500\)_distinct_ data points. Each agent in the first group has full access to its entire dataset, and thus can employ _i.i.d._ sampling (baseline) or single shuffling. On the other hand, each agent in the other group has a graph-like structure and uses SRW (baseline), NBRW or SRRW with reweighting to sample its local dataset with uniform weight. In this simulation, we assume that agents can only communicate through a communication network using the DSGD algorithm. This scenario with heterogeneous agents, as depicted in Figure 1, is of great interest in large-scale machine learning . In addition, we employ a decreasing step size \(_{n}=1/n\) in our UD-SGD framework (1) because it is typically used for the strongly convex objective function and is tested to have the fastest convergence in this simulation setup. Due to space constraints, we defer detailed simulation setup, including the introduction of SRW, NBRW, and SRRW, to Appendix G.1.

The simulation results are obtained through \(120\) independent trials. In Figure 2(a), we assume that the first group of agents perform either _i.i.d._ sampling or shuffling method, while the other group of agents all change their sampling strategies from baseline SRW to NBRW and SRRW, as shown in the legend. This plot shows that improved sampling strategy leads to overall convergence speedup since NBRW and SRRW are more efficient than SRW . Furthermore, it illustrates that SRRW is significantly more efficient than NBRW in this simulation setup, i.e., \(>\) in terms of sampling efficiency. While keeping the second group of agents unchanged, we can see that shuffling method outperforms _i.i.d._ sampling with smaller asymptotic MSE. However, shuffling method may not perform perfectly for small time \(n\) due to slow mixing behavior in the initial period, which is also observed in the single-agent scenario in . The error bar therein also indicates that the random-walk sampling strategy has a significant impact on the overall system performance and SRRW has smaller variance than NBRW and SRW.

In Figure 2(b), we let the first group of agents perform _i.i.d._ sampling while only changing a portion of agents in the second group to upgrade from SRW to SRRW, e.g., \(30\) SRW \(20\) SRRW in the legend means that there are \(30\) agents using SRW while the rest \(20\) agents in the second group upgrade to SRRW. We observe that more agents willing to upgrade from SRW to SRRW lead to smaller asymptotic MSE, as predicted by Theorem 3.3 and Remark 3. This improvement in MSE reduction doesn't scale linearly with more agents adopting SRRW because each agent holds its own dataset that are not necessarily identical, resulting in different individual limiting covariance matrices \(_{i}\!\!_{j}\).

While maintaining _i.i.d._ sampling for the first group of agents, we compare the performance when the second group of agents in Figure 2(c) employ NBRW or SRRW. Remarkably, the case with only \(10\) agents out of \(50\) agents in the second group adopting far more efficient sampling strategy (\(40\) SRW, \(10\) SRRW) through incentives or compliance already produces a smaller MSE than all \(50\) agents using slightly better strategy (\(50\) NBRW). The performance gap becomes even more pronounced

Figure 2: Binary classification problem. From left to right: (a) Impact of efficient sampling strategies on convergence. (b) Performance gains from partial adoption of efficient sampling. (c) Comparative advantage of SRRW over NBRW in a small subset of agents. (d) Asymptotic network independence of four algorithms under UD-SGD framework with fixed sampling strategy (shuffling, SRRW). (e) Different sampling strategies in the DSGD algorithm with time-varying topology (DSGD-VT). (f) Different sampling strategies in the DFL algorithm with increasing communication interval.

when \(20\) agents upgrade from SRW to SRRW (\(30\) SRW, \(20\) SRRW). We show that the performance of a distributed system can be improved significantly when a small proportion of agents adopt highly efficient sampling strategies.

Figure 2(d) empirically illustrates the asymptotic network independence property via four algorithms under our UD-SGD framework: Centralized SGD (communication interval \(K=1\), communication matrix \(=^{T}/N\)); LSGD-FP (FL with full client participation, \(K=5\), \(=^{T}/N\)); DSGD-VT (DSGD with time-varying topologies, randomly chosen from \(5\) doubly stochastic matrices); DFL (decentralized FL with fixed MH-generated \(\) and increasing communication interval \(K_{l}=\{1,(l)\}\) after \(l\)-th aggregation). We fix the sampling strategy (shuffling, SRRW) throughout this plot. All four algorithms overlap around \(1000\) steps, implying that they have entered the asymptotic regime with similar performance where the CLT result dominates, implying the asymptotic network independence in the long run.

Figure 2(e) and 2(f) show the performance of different sampling strategies in DSGD-VT and DFL algorithms in terms of MSE. Both plots consistently demonstrate that improving agent's sampling strategies (e.g., shuffling > iid sampling, and SRRW > NBRW > SRW) leads to faster convergence with smaller MSE, supporting our theory.

Furthermore, in Appendix G.2, we simulate an image classification task with CIFAR-10 dataset  by training a \(5\)-layer CNN and ResNet-18 model collaboratively through a \(10\)-agent network. The result is illustrated in Figure 3, where SRRW outperforms NBRW and SRW as expected. In summary, we find that upgrading even a small portion of agents to efficient sampling strategies (e.g., shuffling method, NBRW, SRRW under different dataset structures) improves system performance in UD-SGD. These results are consistent in binary and image classification tasks, underscoring that _every agent matters_ in distributed learning.

## 5 Conclusion

In this work, we develop an UD-SGD framework that establishes the CLT of various distributed algorithms with Markovian sampling. We overcome technical challenges such as quantifying consensus error under very general communication patterns and decomposing Markovian noise through the Poisson equation, which extends the analysis beyond the single-agent scenario. We demonstrate that even if only a few agents optimize their sampling strategies, the entire distributed system will benefit with a smaller limiting covariance in the CLT, suggesting a reduced MSE. This finding challenges the current established upper bounds where the worst-performing agent leads the pack. Future studies could pivot towards developing fine-grained finite-time bounds to individually characterize each agent's behavior, and theoretically analyze the effect of SRRW in UD-SGD.

## 6 Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their constructive comments. This work was supported in part by National Science Foundation under Grant Nos. CNS-2007423, IIS-1910749, and IIS-2421484.