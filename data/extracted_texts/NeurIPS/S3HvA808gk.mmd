# A Closer Look at AUROC and AUPRC

under Class Imbalance

 Matthew B. McDermott

Harvard Medical School

matthew_mcdermott@hms.harvard.edu

&Haoran Zhang

Massachusetts Institute of Technology

haoranz@mit.edu

&Lasse Hyldig Hansen

Aarhus University

201908623@post.au.dk

&Giovanni Angelotti

IRCCS Humanitas Research Hospital

giovanni.angelotti@humanitas.it

&Jack Gallifant

Massachusetts Institute of Technology

jgally@mit.edu

###### Abstract

In machine learning (ML), a widespread claim is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a _harmful metric_ as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we reviewed over 1.5 million scientific papers to understand the origin of this invalid claim-finding it is often made without citation, misattributed to papers that do not argue this point, and aggressively overgeneralized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.

## 1 Introduction

Machine learning (ML), especially in critical domains like healthcare, necessitates careful selection and application of evaluation metrics to guide appropriate model choices and understand performance nuances . Model evaluation can happen in one of two settings: (1) a _methodological/model comparison_ setting, which occurs outside of a specific deployment setting and in which target model usage workflows, optimal decision thresholds, or specific false-positive (FP) and false-negative (FN) costs are typically not known, or (2) an _application/deployment_ setting, where reasonably specific estimates of model usage workflows and FP/FN costs can be made. In both settings, appropriate metric choice is critical, as inappropriate selection can hinder innovation when used for model comparison and lead to significant real-world costs (e.g., misdiagnosis in a medical setting) in deployment settings.

This study focuses on two widely used metrics for binary classification tasks across both evaluation contexts: Area Under the Precision-Recall Curve (AUPRC) and Area Under the Receiver Operating Characteristic (AUROC). Central to this paper is the following key claim:

**Claim 1.** Let \(f\) be a model which outputs continuous probabilistic predictions trained to solve a binary classification task for which the prevalence of negative labels is significantly higher than the prevalence of positive labels. For this problem, the AUPRC will yield a "better" or "more accurate" or "fairer" evaluation of \(f\) than the AUROC.

Claim 1 is made widely in both the scientific literature [399; 71; 159; 124], in ML educational content [119; 141], and in popular press sources [80; 254]. It is so widespread that even basic search results for queries relating to AUROC and AUPRC1 and large language model assistants like ChatGPT or Github Co-pilot will profess its veracity.2 Throughout these sources, it has been justified on numerous, often imprecise grounds (see Section 5), but despite this extensive attention, _we show in this work that this claim is, in fact, wrong, and may be dangerous from a model fairness perspective_; further, many of its justifications are _invalid_ or _misapplied_ in common ML settings. More specifically, we show the following:

**1) AUROC and AUPRC only differ with respect to model-dependent parameters in that AUROC weighs all false positives equally, whereas AUPRC weighs false positives at a threshold \(\) with the inverse of the model's likelihood of outputting any scores greater than \(\)** (Theorem 1). This result shows that we can reason about the suitability of AUROC vs. AUPRC based on whether we care more about reducing false positives above low thresholds or high thresholds. In particular,

**2) AUROC favors model improvements uniformly over all positive samples, whereas AUPRC favors improvements for samples assigned higher scores over those assigned lower scores** (Theorem 2). This indicates that _the key factor differentiating the utility of AUROC or AUPRC as an evaluation metric is not class imbalance at all, but it is rather based on the target use case of the model in question_. See Figure 1 for a visual explanation. It also reveals that _AUPRC can amplify algorithmic biases_. In particular,

**3) AUPRC can unduly prioritize improvements to higher-prevalence subpopulations at the expense of lower-prevalence subpopulations**, raising serious fairness concerns in any multi-population use cases (Theorem 3).

In this work, we establish these three claims both theoretically and empirically via synthetic experiments and real-world validation on popular public fairness datasets. In addition, we demonstrate through an extensive, large-language model aided literature review of over 1.5 million scientific papers that Claim 1 has been used to motivate numerous improper uses of AUPRC relative to AUROC across high-stakes domains like healthcare and in several established venues, including AAAI, NeurIPS, ICML, ICLR, Cancer Cell, Nature Journals, PNAS, and more. Through this paper, we hope to shed light on the nuances of appropriate evaluation and provide key guidance to limit future misuse of evaluation metrics in the scientific and machine learning communities.

## 2 Theoretical Analyses

Please note that all notation used is defined in Appendix Section C.

### Relationship between AUROC and AUPRC

In this section, we introduce Theorem 1, which is as follows:

_Theorem 1_.: Let \(,=\{0,1\}\) represent a paired feature and binary classification label space from which i.i.d. samples \((x,y)\) are drawn via the joint distribution over the random variables \(,\). Let \(f:(0,1)\) be a binary classification model outputting continuous probability scores over this space. Then,

\[(f) =1-_{t f()|y=1}[(f,t)]\] \[(f) =1-P_{}(y=0)_{t f()|y=1}[ (f,t)}{P_{}(f(x)>t)}]\]Figure 1: **a)** Consider a model \(f\) yielding continuous output scores for a binary classification task applied to a dataset consisting of two distinct subpopulations, \(\{0,1\}\). If we order samples in ascending order of output score, each misordered pair of samples (e.g., mistake 1-4) represents an opportunity for model improvement. Theorem 2 shows that a model’s AUROC will improve by the same amount no matter which mistake you fix, while the model’s AUPRC will improve by an amount correlated with the score of the sample. **b)** When comparing models absent a specific deployment scenario, we have no reason to value improving one mistake over another, and model evaluation metrics should therefore improve equally regardless of which mistake is corrected. **c)** When false negatives have a high cost relative to false positives, evaluation metrics should favor mistakes that have _lower scores_, regardless of any class imbalance. **d)** When limited resources will be distributed among a population according to model score, _in a manner that requires certain subpopulations to all be offered commensurate possible benefit from the intervention for ethical reasons_, evaluation metrics should prioritize the importance of within-group, high-score mistakes such that the highest risk members of all subgroups receive interventions. **e)** When false positives are expensive relative to false negatives and there are no fairness concerns, evaluation metrics should favor model improvements in decreasing order with score.

We provide the proof in Appendix Section D. The two key intuitions are that integrating over the TPR is equivalent to taking the expectation over the induced distribution of positive sample scores, and that via Bayes rule, \((f,)=1-P_{}(y=0)(f,)}{P_{}(f(x)>)}\).

Despite its simplicity, Theorem 1 has far-reaching implications. Namely, it reveals that the only difference between AUROC and AUPRC with respect to model dependent parameters (i.e., omitting the dependence of AUPRC on the fixed prevalence of the dataset, which is not model varying) is that optimizing AUROC equates to minimizing the expected false positive rate over all positive samples in an unweighted manner (equivalently, in expectation over the distribution of positive sample scores) whereas optimizing AUPRC equates to minimizing the expected false positive rate over all positive samples weighted by the inverse of the model's "firing rate" (\(P_{}(f(x)>)\)) at the given positive sample score. This preference can be crystallized when we examine how AUROC vs. AUPRC would prioritize correcting indivisible units of model improvements, termed "mistakes" which we will discuss next.

### AUPRC prioritizes high-score mistakes, AUROC treats all mistakes equally

Understanding how a given evaluation metric prioritizes correcting various kinds of model mistakes or errors offers significant insight into when that metric should be used for optimization or model selection. To examine this topic for AUROC and AUPRC, consider the following definition of an "incorrectly ranked adjacent pair", which we will colloquially refer to as a "model mistake":

**Definition 2.1**.: Let \(f,,,,\) be defined as in Theorem 1. Further, let us suppose we have sampled a static dataset from \(,\) for evaluation which will be denoted \(,=\{(x_{1},y_{1}),,(x_{N},y_{N})\}\), for \(x_{i},y_{i}\{0,1\}\), and \(\). We assume for convenience that \(f\) is an injective map and all \(x_{i}\) are distinct (i.e., \((i,j)|i j:x_{i} x_{j}\) which, by injectivity of \(f\), implies that \(f(x_{i}) f(x_{j})\)).

We say that \((x_{i},x_{j})\) are an _incorrectly ranked adjacent pair_ and thus that the model makes a "_mistake_" at samples \((x_{i},x_{j})\) if:

1. \(y_{i}=1\) and \(y_{j}=0\)
2. \(f(x_{i})<f(x_{j})\)
3. \( x_{k}\) such that \(f(x_{i})<f(x_{k})<f(x_{j})\).

Essentially, Definition 2.1 states that a _mistake_ occurs when a model assigns adjacent probability scores to a pair of samples with discordant labels, as shown in Figure 1. With this in mind, we can then introduce Theorem 2 which states that AUROC improves by a constant amount regardless of which mistake is corrected for a given model and dataset whereas AUPRC improves more when the mistake corrected occurs at a higher score than when it occurs at a lower score:

_Theorem 2_.: Define \(f,,,\) and \(N\) as in Definition 2.1. Further, suppose without loss of generality that the dataset \(\) is ordered such that \(f(x_{i})<f(x_{i+1})\) for all \(i\). Then, let us define \(M=\{i|(x_{i},x_{i+1})\) is an _incorrectly ranked adjacent pair_ for model \(f\}\). Define \(f^{}_{i}\) to be a model that is identical to \(f\) except that the probabilities assigned to \(x_{i}\) and \(x_{i+1}\) are swapped:

\[f^{}_{i}:xf(x)&x\{x_{i},x_{i+1}\}\\ f(x_{i+1})&x=x_{i}\\ f(x_{i})&x=x_{i+1}.\]

Then, \((f^{}_{i})=(f^{}_{j})\) for all \(i,j M\), and \((f^{}_{i})<(f^{}_{j})\) for all \(i,j M\) such that \(i<j\).

The proof for Theorem 2 can be found in Appendix E. This proof simply stems from the fact that correcting a single mistake \((x_{i},x_{j})\) (as defined in Definition 2.1) always changes the false positive rate by the same amount, and only changes it at the threshold \(f(x_{i})\). This, combined with the formalization of AUROC and AUPRC in Theorem 1, establishes the proof. Note that this Theorem can be trivially extended to include a case where ties are possible simply by noting that "swapping" two samples \(x_{i}\) and \(x_{j}\) where \(f(x_{i})=f(x_{j})\) in the manner of the theorem results in no change to either AUROC or AUPRC, and similarly by the same reasoning separating any tie in the appropriate direction will improve AUROC uniformly over samples and will improve AUPRC in a manner monotonic with model score.

### AUPRC is explicitly discriminatory in favor of high-scoring subpopulations

The reliance on a model's firing rate revealed in Theorem 1 and the optimization behavior in Theorem 2 reveals significant issues with the fairness of AUPRC. In particular, in this section we introduce Theorem 3:

_Theorem 3_.: Let \(f,,,,N,M,\) and \(f^{}_{j}\) all be defined as in Theorem 2. Further, suppose that in this setting the domain \(\) now contains an attribute defining two subgroups, \(=\{0,1\}\), such that for any sample \((x_{i},y_{i})\), \(a_{i}\) denotes the subgroup to which that sample belongs. Let \(f\) be perfectly calibrated for samples in subgroup \(a=0\), such that \(P_{|,}(y=1|a=0,f(x)=t)=t\). Then,

\[_{P_{|}(y=1|a=0) 0}P(a_{i}=a_{i+1}=1i= *{arg\,max}_{j M}((f^{}_{j})) )=1.\]

Essentially, Theorem 3 (proof provided in Appendix F) shows the following. Suppose we are training a model \(f\) over a dataset with two subpopulations: Population \(a=0\) and \(a=1\). If the model \(f\) is calibrated and the rate at which \(y=1\) for population \(a=0\) is sufficiently low relative to the rate at which \(y=1\) for population \(a=1\), then the mistake that, were it fixed, would maximally improve the AUPRC of \(f\) will be a mistake purely in population \(a=1\). This demonstrates that AUPRC provably favors higher prevalence subpopulations (those with a higher base rate at which \(y=1\)) under sufficiently severe prevalence imbalance between subpopulations.

Note that this property is, generally speaking, not desirable. _In particular, this property establishes that in settings where model fairness among a set of subpopulations in the data is important, AUPRC should not be used as an evaluation metric due to the risk that it will introduce biases in favor of the highest prevalence subpopulations._ We validate this result empirically over both synthetic and real-world data in Section 3, demonstrating that the import of Theorem 3 is not merely limited to an analytical curiosity but can have real-world impact on algorithmic disparities in practice. Furthermore, note that this theorem does not indicate that AUPRC will be superior to AUROC for _differentiating_ a low prevalence (or low risk) subpopulation relative to a high-risk subpopulation, a property that is sometimes attributed to AUPRC in the literature. Rather, Theorem 3 shows that maximizing AUPRC will be more likely to optimize solely within the high-risk subgroup, rather than optimizing to differentiate across subgroups, as low-risk subgroup samples will predominantly occur in lower-score regions under severe class imbalance.

## 3 Experimental Validation

In this section, we establish via synthetic and real-world experiments that Theorem 3 is not merely an analytical effect but has real world consequences on the implications of optimizing or performing model selection via AUPRC.

### Synthetic optimization experiments demonstrate AUPRC-induced disparities

In this section, we use a carefully constructed synthetic optimization procedure to demonstrate that, when all other factors are equal, optimizing by or performing model selection on the basis of AUPRC vs. AUROC risks excacerbating algorithmic disparities in the manner predicted by Theorem 3. For analyses under more realistic conditions with more standard models, see our real-world experiments in Section 3.2.

Experimental Setup.Let \(y\{0,1\}\) be the binary label, \(s\) be the predicted score, and \(a\{1,2\}\) be the subpopulation. We fix \(P_{|}(y=1|a=1)=0.05\) and \(P_{|}(y=1|a=2)=0.01\). We sample a dataset for each group \(_{a}=\{(s_{1},y_{1}),...,(s_{n_{a}},y_{n_{a}})\}\), such that \((_{1})(_{2}) (_{1}_{2})=0.85\) (See Appendix G.1; A target AUROC of 0.65 was also profiled in Appendix Figure 5).

Our main experimental challenge is to determine how to simulate "optimizing" or "selecting" a model by AUROC or AUPRC. Simulating optimizing by these metrics allows us to explicitly assess how the use of either AUPRC or AUROC as an evaluation metric in model selection processes such as hyperparameter tuning or architecture search, can translate into model-induced inequities in dangerous ways. We explore two approaches here. First, we can simply correct the atomic mistakethat maximally improves AUROC or AUPRC in each optimization iteration. In our experiments, we use \(n_{1}=n_{2}=200\) and optimize for \(50\) steps for this experiment. This is the most straightforward optimization procedure to analyze, but it is unrealistic. In real optimization scenarios, larger model changes will be made at once, and a model will have an opportunity to _degrade_ performance in some regions in order to improve it in others.

Next, we profile an optimization procedure that randomly permutes all the (sorted) model scores up to 3 positions (See Appendix G.3 for details). This has the effect of randomly adjusting all model scores, and can worsen model performance under some random permutations, but offers precisely the same "optimization capacity" to the low and high prevalence subgroups. To ensure the model is under some optimization constraint (and therefore does not always find the "perfect" permutation to maximize both metrics identically), we allow the model to sample only 15 possible permutations before choosing the best option. This means the system will be forced to navigate optimization trade-offs between which permutations improve the right regions of the score most effectively among its limited set. We use \(n_{1}=n_{2}=100\) for these experiments and optimize for \(25\) total steps.

Across both settings, we run these experiments across 20 randomly sampled datasets and show the mean and an empirical 90% confidence interval around the mean in Figure 2. We present a formal mathematical formulation of these perturbations, as well as profile a third random perturbation method, in Appendix G.3.

Results.Our results demonstrate the impact of the optimization metric on subpopulation disparity. In particular, in Figure 2, we observe a notable disparity introduced when optimizing under

Figure 2: Synthetic experiment per-group AUROC, showing a confidence interval spanning the 5th to 95th percentile of results observed across all seeds, after successively either fixing individual mistakes, as defined in Definition 2.1, (**a**) and b)) or successively choosing the optimal score permutation (**c**) and **d)**) in order to optimize either AUROC (**a**) and **c**)) or AUPRC (**b**) and **d)**). It is clear across both forms of optimization that AUPRC definitively favors the higher prevalence subpopulation, whereas AUROC treats subgroups approximately equally. Similar patterns were observed when comparing per-group AUPRCs over the same experimental procedures, as shown in Appendix Figure 4.

the AUPRC metric regardless of the optimization procedure. This is evident in the performance metrics across the high and low prevalence subpopulations, which exhibit significant divergence as the optimization process favors the group with higher prevalence. In the more realistic, random-permutation optimization procedure (Figure 1(d)), this even results in a decrease in the AUROC for the low prevalence subgroup. In comparison, when optimizing for overall AUROC, the AUROCs of both groups increase together. Note that we show the effect of this optimization on the AUPRC metric, which shows very similar trends, in Appendix Figure 4. These results demonstrate explicitly that not only does optimizing for AUPRC differ greatly than for AUROC, as has been noted historically by researchers developing explicit AUPRC optimization schemes , but it in fact does so in an explicitly discriminatory way in realistic scenarios.

### Real-world experimental validation

To demonstrate the generalizability of our findings to the real world, we evaluate fairness gaps induced by AUROC and AUPRC selection on four common datasets in the fairness literature .

Datasets.We use the following four tabular binary classification datasets: adult, compas, lsac, and mimic. In each dataset, we consider both sex and race as sensitive attributes. To mimic the setting of our theorems, we balance each dataset by the sensitive attribute during both training and test, by randomly subsampling the majority group. Further details about each dataset, as well as preprocessing steps, can be found in Appendix H.

Experimental setup.We train XGBoost models  on each dataset. For each task, we iterate over a grid of per-group weights in order to create a diverse set of models that favor different groups. For each setting of task and per-group weight, we conduct a random hyperparameter search  with 50 runs. Though more complex hyperparameter search methods such as BOHB  or TPE  might lead to better performance, random searches are far more popular and practical, and have been used in popular benchmarking libraries .

We evaluate the validation set overall AUROC and AUPRC. We also evaluate the test set AUROC gap and AUPRC gap between groups, where gaps are defined as the value of the metric for the higher prevalence group minus the value for the lower prevalence group. Based on our theorems, our hypothesis is that overall AUPRC should be more positively correlated with the signed AUROC gap than overall AUROC, indicating that it better favors the higher prevalence group, especially when the prevalence ratio between groups is high. To test this hypothesis, we evaluate the Spearman correlation coefficient between these quantities. We repeat this experiment 20 times, with different random data splits, to obtain a 95% confidence interval.

Results.In Figure 3, we plot the difference in the Spearman correlation coefficient of the AUROC gap versus the overall AUPRC, and AUROC gap versus overall AUROC. We observe mixed results in datasets with low prevalence ratio. In dataset with higher prevalence ratio, we find that overall AUPRC

Figure 3: Difference in the Spearman’s \(\) between the test-set signed AUROC gap versus the validation set overall AUPRC, and the AUROC gap versus the overall AUROC. Numbers in parentheses are the prevalence ratios between the two groups for the particular attribute, and datasets are sorted by this quantity. Error bars are 95% confidence intervals from 20 different random data splits.

is more positively correlated with the AUROC gap than overall AUROC, indicating that AUPRC more aggressively favors the higher prevalence group. We emphasize that the prevalence ratios observed in these real-world datasets is much lower than the ratio of 5 used in our synthetic experiments, which may account for the mild effect observed. To see raw results from these experiments, see Appendix Figure 7. Similar results for neural network classifiers can be found in Appendix Section H.3.

Next, in Appendix Figure 8, we plot the difference in the Spearman's \(\) from Figure 3, versus the prevalence gap. We find that there is a statistically significant correlation between the two (Spearman's \(=0.905\), p = 0.002). Thus, while our power to detect a prevalence mediated AUPRC bias amplification effect is limited due to the limited prevalence disparities in these datasets, we nonetheless observe a strong positive correlation between the extent of the prevalence mismatch between the low and high prevalence group and the amount that AUPRC favors the high prevalence group over AUROC. _In other words, our results show that across these fairness datasets and attributes, as the prevalence disparity grows more extreme, we observe a statistically significant corresponding increase in the extent to which AUPRC introduces algorithmic bias, exactly in accordance with what Theorem 3 suggests._

## 4 When _Should_ One Use AUPRC vs. AUROC?

In Sections 2 and 3, we have shown that AUPRC is not universally superior in cases of class imbalance (and that instead, it merely preferentially optimizes high-score regions over low-score regions) and that it also poses serious risks to the model fairness in settings where subgroup prevalences differ. In light of this, how should we revise Claim 1 to reflect when we _actually_ should use AUPRC instead of AUROC or vice versa? Below, we explore this question and provide practical guidance on metric selection for binary classification, building on our theoretical results and highlight specific contexts in which one may be favorable (Figure 1). Note that while we provide guidance below on situations in which AUROC vs. AUPRC is more or less favorable, this is not to suggest that authors should not report both metrics, or even larger sets of metrics or more nuanced analyses such as ROC or PR curves; rather this section is intended to offer guidance on what metrics should be seen as more or less appropriate for use in things like model selection, hyperparameter tuning, or being highlighted as the 'critical' metric in a given problem scenario.

For context-independent model evaluation, use AUROC:For model evaluations conducted outside of specific deployment contexts, where the differential costs of errors are undefined, the necessity for a metric that impartially values improvements across the entire model output space becomes paramount (Figure 0(a)). As it is not known in advance where samples of interest will live in the output space, nor are particular cost ratios known, there should be no preference for mistake correction. Therefore, in this setting, AUROC is favorable as it uniformly accounts for every correction, offering a fair assessment irrespective of decision thresholds.

For deployment scenarios with elevated false negative costs, use AUROC:In applications where the consequences of false negatives are especially high, such as in the early screening for critical illnesses like cancer (Figure 0(c)), a primary goal of the model will be to achieve the fewest missed cancer cases; equating to prioritizing model recall. In such a scenario, the most important mistakes to correct occur at lower score thresholds, as high-score mistakes will not change which positive samples are missed in deployment settings (as chosen thresholds are likely to be low). This behavior is the _inverse_ of what AUPRC prioritizes, demonstrating that in such situations, AUROC should be preferred over AUPRC.

For ethical resource distribution among diverse populations, use AUROC:When faced with the challenge of ethically allocating scarce resources across a broad population, necessitating equitable benefit distribution among subgroups (Figure 0(d)), one must avoid prioritizing model improvements that selectively favor one subpopulation. As AUPRC will target high-score regions selectively, it risks unduely favoring high-prevalence subpopulations, as shown in Theorem 3 and Figures 2 and 3. Even though in this resource distribution problem, high-score regions are selectively important compared to low-score regions, the fact that in this problem, we must prioritize across all subpopulations equally means that AUPRC's global preference is untenable as it could induce bias.

For reducing false positives in high-cost, single-group intervention prioritization or information retrieval settings, use AUPRC:In scenarios where the cost associated with false positives significantly outweighs that of false negatives, absent of equity concerns--such as in selecting candidate molecules from a fragment library for drug development trials, where only the most promising molecules will proceed to costly experimental validation (Figure 1e)--the metric of choice should facilitate a reduction in high-score false positives. This necessitates a focus on correcting high-score errors, for which AUROC might not be ideal due to its uniform treatment of errors across the score spectrum, potentially obscuring improvements in critical high-stake decisions.

## 5 Literature Review: Examining how Claim 1 Became so Widespread

Claim 1 states that "AUPRC is better than AUROC in cases of class imbalance" and is widespread in the literature. Via both a manual literature search and an automated search of over 1.5M arXiv papers (see Appendix I for methodology), we observed 424 publications making this claim.3 This widespread dissemination of Claim 1 _has a significant impact on the validity of scientific discourse_. We observed examples of high-profile papers operating in medically critical settings where high-recall is a key priority evaluating ML systems via AUPRC due to their task's underlying class imbalance ; papers focusing on fairness critical applications relying on AUPRC due to this claim, even while our results demonstrate AUPRC has major _problems_ in the fairness regime [366; 306], and numerous other papers perpetuating this source of scientific misinformation.

Among the 424 papers we discovered referencing this claim, 167 did so with no associated citation. These papers were published in a wide range of venues, including high profile venues such as NeurIPS, ICML, and ICLR. This reflects not only the widespread belief in this claim, but also that we may be too comfortable making seemingly "correct" assertions without appropriate attribution in ML today. Further, Among the 257 that reference this claim and cite a source for this assertion, 135 _do not cite any papers that actually make this claim in the first place_. Most often, papers erroneously attribute this claim to , which was cited as a source for this claim 144 times. While  makes many interesting, meaningful claims about the ROC and PR curves, and _does argue that the precision-recall curve may be more informative than the ROC in cases of class imbalance_ it never asserts that the _area under_ the PR curve should be preferred over the _area under_ the ROC in cases of class imbalance. This distinction is important, because while curves can be used to simultaneously communicate many different performance statistics to their viewers across different FPR/TPR or Precision/Recall criteria, and therefore should be assessed primarily as communication tools to deployment experts, _areas under these curves_ are single-point summarizations which are primarily used for deployment-agnostic model comparison, which is a very different use case.

Even when appropriate papers are cited, the valid settings in which AUPRC should be preferred (see Section 4 for examples) are often over-shadowed by significant over-generalizations to preferring AUPRC in _all_ settings featuring class imbalance. For example, claims such as that "precision-recall curves are more informative of deployment metrics" are often used to justify why AUPRC should be used in all cases of class imbalance, rather than just in cases where the relevant deployment metrics are most directly associated with the PR curve. Another class of arguments made in favor of Claim 1 is rooted in claims that AUROC is poor in cases of class imbalance because its scores are misleadingly high. While this argument can reflect a meaningful limitation of the communication value of the ROC or the AUROC, comments about singleton metric results (rather than model comparison through metric values) are inherently orthogonal to the goal of model evaluation. _In other words, what matters for model evaluation is not how high a given metric is, but rather the extent to which the metric meaningfully captures the right improvements in the model in the right ways_. The widespread nature of Claim 1 has also led researchers astray when exploring new optimization procedures for AUPRC, by advocating for the importance of AUPRC when processing skewed data, even in domains such as medical diagnoses that often have high false negative costs relative to false positive costs . For a more extensive breakdown of the arguments we observed in the literature and the sources making them, see Appendix Tables 2 and 3.

## 6 Limitations and Future Works

There are still a number of areas for further improvement and future work. Firstly, our theoretical findings can be refined and generalized to, e.g., take into account the difficulty of the target task (which may differ between subgroups), not require models to be calibrated (in the case of Theorem 3), or specifically take into account more than 2 subpopulations for more nuanced comparisons beyond what can be inferred through pairwise comparisons between subpopulations, where our results would naturally apply. Further, extending our real-world experiments to more fairness datasets and identifying more nuanced ways to probe the impact of metric choice on disparity measures would significantly strengthen this work. These analyses can also be extended to consider other metrics, such as the area under the precision-recall-gain curve , the area under the net benefit curve [384; 307], and single-threshold, deployment centric metrics as well. In addition, one of the largest limitations of Theorem 3 is its restrictive assumptions, in particular the requirement of perfect calibration. A ripe area of future work is thus to investigate how we can soften our analyses for models with imperfect calibration or to determine whether or not our results imply anything about the viability or safety of post-hoc calibration of models optimized either through AUPRC or AUROC.

## 7 Conclusion

This study interrogates the pervasive assumption within the ML community that AUPRC is a better evaluation metric than AUROC in class-imbalanced settings. Our empirical analyses and literature review reveal several concrete findings that challenge this notion. In particular, we show that while optimizing for AUROC equates to minimizing the model's FPR in an unbiased manner over positive sample scores, optimizing for AUPRC equates to minimizing the FPR specifically for regions where the model outputs higher scores relative to lower scores. _Further, we show both theoretically and empirically over synthetic and real-world fairness datasets that AUPRC can be an explicitly discriminatory metric through favoring higher-prevalence subgroups._

In summary, our research advocates for a more thoughtful and context-aware approach to selecting evaluation metrics in machine learning. This paradigm shift, favoring a balanced and conscientious approach to metric selection, is essential in advancing the field towards developing not only technically sound, but also equitable and just models.