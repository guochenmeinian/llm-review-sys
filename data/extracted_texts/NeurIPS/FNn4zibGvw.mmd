# Hyperbolic VAE via Latent Gaussian Distributions

Seunghyuk Cho

POSTECH GSAI

shhj1998@postech.ac.kr &Juyong Lee

KAIST AI

agi.is@kaist.ac.kr &Dongwoo Kim

POSTECH GSAI & CSED

dongwoo.kim@postech.ac.kr

###### Abstract

We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. We demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and state representation learning for model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolic- and Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported in previous hyperbolic-VAEs. The implementation is available at https://github.com/ml-postech/GM-VAE.

## 1 Introduction

The geometry of latent space in generative models, such as variational auto-encoders (VAE) (Kingma & Welling, 2013), reflects the structure of the data representations. Mathieu et al. (2019); Nagano et al. (2019); Cho et al. (2022) show that employing hyperbolic space as the latent space improves the preservation of the hierarchical structure within the data. The theoretical background for adopting hyperbolic space lies in the analysis of Sarkar (2011); the tree-structured data can be embedded with arbitrary low distortion in hyperbolic space, while Euclidean space requires extensive dimensions.

Previously proposed hyperbolic VAEs rely on Poincare normal distribution (Mathieu et al., 2019) or hyperbolic wrapped normal distribution Nagano et al. (2019) for the prior and variational distributions. Unlike the Gaussian distribution in Euclidean space, however, these distributions suffer from several shortcomings, including the absence of closed-form Kullback-Leibler (KL) divergence, numerical instability (Mathieu et al., 2019; Skopek et al., 2019), and high computational cost in sampling (Mathieu et al., 2019).

Meanwhile, we can form a Riemannian manifold from the set of univariate Gaussian distributions by equipping the Fisher information metric (FIM). It is known that the FIM of univariate Gaussian distributions is akin to that of the metric tensor of the Poincare half-plane model (Costa et al., 2015), providing a perspective of viewing the points in hyperbolic space as univariate Gaussian distributions. In other words, a Gaussian distribution can be mapped to a single point in the open half-plane manifold as shown in Figure 1, where the FIM forms the shortest geodesic distance between two Gaussian distributions. Noting that the numerical issue of Poincare normal arises from the geodesic distance of hyperbolic space, we question whether this perspective can lead us to define a new distribution with better analytic properties.

In this work, inspired by the fact that KL divergence itself is a statistical distance that locally approximates the geodesic distance (Tifrea et al., 2018), we propose a hyperbolic distribution bysubstituting the geodesic distance of Poincare normal with the KL divergence between the univariate Gaussian distributions. We then verify that this simple yet powerful alteration results in several practical analytic properties; the proposed distribution reduces into the product of two well-known distributions, i.e., the Gaussian and gamma distributions, which are easy to sample, with a closed-form KL divergence between the proposed distributions. By adopting the proposed hyperbolic distribution, we introduce a new variant of hyperbolic VAE, named Gaussian manifold VAE (GM-VAE), whose latent space is a set of Gaussian distributions.

During the experiments, we observe that the proposed distribution is robust in terms of sampling and KL divergence computation compared to the commonly-used hyperbolic distributions; we briefly explain the reason why others are numerically unstable. Experimental results on the density estimation task with image datasets show that GM-VAE can achieve outperforming generalization performances to unseen data against baselines of Euclidean and hyperbolic VAEs. Application of GM-VAE on model-based reinforcement learning (RL) verifies the feasibility of using hyperbolic space on another domain of task.

We summarize our contributions as follows:

* We introduce a variant of VAE whose latent space is defined on a statistical manifold formed by univariate Gaussian distributions, namely Gaussian manifold.
* We propose a new distribution, called a pseudo Gaussian manifold normal distribution, which is easy to sample and has closed-form KL divergence, to train VAE on the Gaussian manifold.
* We empirically verify that the newly proposed VAE performs stable training without numerical issues on the density estimation task with several image datasets. The proposed model outperforms the baseline Euclidean VAE and other hyperbolic variants.
* We show that our method can be used for model-based RL. Specifically, we replace the latent space of the world model with hyperbolic space for learning environments, showing competitive results with a state-of-the-art baseline.

## 2 Preliminaries

In this section, we first review the fundamental concepts of hyperbolic space and commonly used hyperbolic models. We then explain the Riemannian geometry between statistical objects, showing the connection between the statistical manifold and hyperbolic space.

### Review on hyperbolic space

Riemannian manifold.A \(n\)-dimensional Riemannian manifold consists of a manifold \(\) and a metric tensor \(g:^{n n}\), which is a smooth map from each point \(\) to a symmetric

Figure 1: (a) The visualization of the Gaussian manifold consisting of a set of Gaussian distributions. Each point of the Gaussian manifold is a pair of two parameters of a univariate Gaussian distribution: (\(\), \(\)) \(_{>0}\). The dashed lines are the geodesics, which are either the ellipses with eccentricity \(1/\) with the origin placed on the \(\)-axis or straight lines parallel to the \(\)-axis. (b) Three univariate Gaussian distributions correspond to three points in the Gaussian manifold in (a).

positive definite matrix. The metric tensor \(g()\) defines the inner product of two tangent vectors for each point of the manifold \(,_{}:_{} _{}\), where \(_{}\) is the tangent space of \(\).

The metric tensor induces basic Riemannian operations, such as a geodesic, exponential map, log map, and parallel transport. Given two points \(,\), geodesic \(_{}:\) is a unit speed curve on \(\) being the shortest path between \((0)=\) and \((1)=\). This curve can be interpreted as a generalized path of a straight line in Euclidean space. The exponential map \(_{}:_{}\) is defined as \(_{}()=(1)=\) given \(\) is a geodesic starting from \((0)=\) and \(^{}(0)=_{}\). The log map \(_{}:_{}\) is the inverse of the exponential map, i.e., \(_{}(_{}())=\). The parallel transport \(_{}:_{}_{}\) moves the tangent vector \(\) along the geodesic between \(\) and \(\). The geodesic distance \(d_{}(,)\) can be induced by the metric tensor as follows:

\[d_{}(,)=_{0}^{1}(t ),(t)_{(t)}}dt.\]

Hyperbolic space.One method of classifying Riemannian manifolds, a basic question of differential geometry, is based on curvature. Among different types of curvatures, one popular curvature is the sectional curvature \(_{g}\), which is a generalization of the Gaussian curvature in classical surface geometry. Given two linearly independent vector fields \(X,Y()\), the sectional curvature \(_{g}\) can be computed with Riemannian curvature tensor \(R()() ()()\) as below:

\[_{g}=},\]

where \(R(X,Y)Y\) returns a tensor field assigning a tensor to each point of the Riemannian manifold \(\). The hyperbolic space is a Riemannian manifold that has the sectional curvature value of constant negative (Nickel and Kiela, 2018). The hyperbolic space is known to be able to embed tree-structured data with arbitrarily low distortion (Sarkar, 2011).

Hyperbolic models.We utilize three famous models of hyperbolic space: the Poincare disk model, the Lorentz model, and the Poincare half-plane model.

The Poincare disk model is a hyperbolic space with an open disk manifold. Earlier hyperbolic machine learning work uses the Poincare disk model because it has a simple closed-form of the operations, such as exponential and log maps (Mathieu et al., 2019). However, the Poincare disk model suffers from numerical stability issues when the points exist near the boundary of the manifold.

The Lorentz model is often used as an alteration of the Poincare disk model (Nickel and Kiela, 2018; Nagano et al., 2019; Bose et al., 2020; Cho et al., 2022). The Lorentz model uses a half hyperboloid manifold, where the closed form of the Riemannian operations exists, so the numerical stability issue of employing the Poincare disk model is relieved.

The Poincare half-plane model is another well-known model of hyperbolic space with an open half-plane manifold. The metric tensor of a point of the two-dimensional Poincare half-plane model \((x,y)\) is \(y^{-2}(1,1)\).

Numerical stability issues of the hyperbolic models.Hyperbolic space suffers from numerical stability when applied to machine learning algorithms (Yu and Sa, 2021; Skopek et al., 2019; Mathieu et al., 2019). The numerical stability mainly occurs for two reasons: machine precision error and unstable Riemannian operations.

First, due to the machine precision error, the hyperbolic points represented with floating point differ from the real value (Yu and Sa, 2021, 2019). In contrast to Euclidean space, the points of hyperbolic space need to satisfy manifold constraints, e.g., the Poincare disk model allows points whose Euclidean norm is less than one. A point can be placed near the boundary during the optimization or inference processes. Although the point does not violate the manifold constraint in theory, it can be located on or out of the boundary when represented with a floating point due to machine precision error. We empirically observe that the manifold constraint violation occurs frequently when we need to embed many data points in hyperbolic space. Figure 3 demonstrates the machine precision error of each hyperbolic model.

Second, the Riemannian operations of hyperbolic space can result in a not-a-number (NaN) value when the input value is not in the manifold. For example, the geodesic distance from the Poincaredisk model and the log mapping of the Lorentz model are unstable Riemannian operations, which are written as:

\[d_{}(,)=^{-1}(1+2- \|^{2}}{(1-\|\|^{2})(1-\|\|^{2})}),\,_{ }()=()}{-1}}(-),\]

where \(=_{0}_{0}-_{i=1}^{n}_{i}_{i}\) is the Lorentzian inner product of \(,\). For the Poincare disk model, when the points \(,\) are near the boundary of the unit disk, the floating point representation of the norm values \(\|\|^{2},\|\|^{2}\) becomes one. The denominator of the geodesic distance then becomes zero. For the Lorentz model, if \(=\) and \(\) contains large values in the coordinates, \(\) becomes less than one which results in NaN because the domain of \(^{-1}(x)\) is \(x 1\).

### Statistical manifold of univariate Gaussians

A particular case of the Riemannian manifold is a statistical manifold, where each point in the manifold corresponds to a probability distribution. Specifically, the parameter manifold \(\) of the probability distributions \(p_{}:\), where \(\), equipped with the Fisher information metric (FIM) forms a Riemannian manifold (Rao, 1992). The FIM is defined as:

\[g_{ij}()=_{}(x )}{_{j}}(x)}{_{j}}p _{}(x)\,dx.\]

In the parameter space of univariate Gaussian distributions \(\{(,),_{>0}\}\), the FIM can be simplified as two-dimensional diagonal matrix \(^{-2}(1,2)\)(Costa et al., 2015).

Connection to the Poincare half-plane model.The diagonal form of the FIM implies that the Riemannian manifold with \(\{(,),_{>0}\}\) has the same set of points as the manifold of the Poincare half-plane, but with different curvature value of \(-0.5\).

The parameter space of the \(n\)-dimensional diagonal Gaussian distributions becomes the product of \(n\) manifolds of the parameter space of univariate Gaussian distributions. In turn, the statistical manifold of \(n\)-dimensional diagonal Gaussian distributions can be viewed as the product of \(n\) hyperbolic spaces. The operations on the product of the Riemannian manifolds \(_{i=1}^{n}_{i}\) are defined manifold-wise. For example, an exponential map applied on a point \((p_{i})_{i=1}^{n}_{i=1}^{n}_{i}\), with tangent vector \(v_{i}_{p_{i}}_{i}\) for each \(i\{1,,n\}\), can be represented as \((_{p_{i}}(v_{i}))_{i=1}^{n}\).

Distance in the statistical manifold.In the statistical manifold, distance functions measure the difference between two distributions on the statistical manifold. One example is the geodesic distance derived from the FIM, which is called the Fisher-Rao distance. The Fisher-Rao distance of the statistical manifold of univariate Gaussian distributions is the same as the geodesic distance of the Poincare half-plane model with constant negative curvature \(-0.5\).

KL divergence is another widely-used statistical distance for distributions, defined as \(D_{}(p q):=_{x}p(x)\,dx\) for two distributions \(p,q\) in the same statistical manifold. One notable property of KL divergence is that it can locally approximate the squared Fisher-Rao distance (Tifrea et al., 2018):

\[D_{}(p(;+d) p( ;))=_{ij}g_{ij}()d _{i}d_{j}+(\|d\|^{3}).\]

## 3 Method

In this section, we first present the concept of the Gaussian manifold, which can have an arbitrary curvature by reparameterizing univariate Gaussian distribution. We then propose a pseudo Gaussian manifold normal distribution. Finally, we suggest a new variant of the VAE defined over the Gaussian manifold with PGM normal as prior. We denote the density function of the Gaussian distribution as \((x;,^{2})=1/(})(-(-x)^{2}/( 2^{2}))\).

### Gaussian manifold with arbitrary curvature

Previous studies on hyperbolic space emphasize the importance of having an arbitrary curvature (Skopek et al., 2019; Mathieu et al., 2019). These works empirically show that the generalization performances of hyperbolic VAEs can be improved with varying curvatures. However, as shown in Section 2.2, the univariate Gaussian distributions form a manifold with curvature value of \(-0.5\), limiting the flexibility of the manifold.

We show that the statistical manifold of univariate Gaussian distributions can have an arbitrary curvature by reparameterizing the univariate Gaussian distribution properly. Let \((,^{2})\) be the reparameterized univariate Gaussian distribution with additional parameter \(c>0\). The reparameterization leads to the FIM of \(^{-2}(1,1/c)\) showing that the curvature of the statistical manifold is \(-c\). The computation of the sectional curvature of the extended FIM is described in Appendix B.1.

We call the statistical manifold with the reparameterized univariate Gaussian distributions and the extended FIM as the Gaussian manifold and denote it as \(_{c}\), where \(-c\) is the curvature of the Gaussian manifold.

We then verify that the KL divergence between the distributions of the Gaussian manifold approximates the geodesic distance, even in the presence of arbitrary curvature in the Gaussian manifold. Let \((,)_{c}\) be an arbitrary point of the Gaussian manifold. The KL divergence between \((,)\) and its neighbor \((+d,+d)\) can be computed as:

\[}(((+d),(+d )^{2})||(,^{2}))}{2c}= d\\ d\!\!(}&0\\ 0&})d\\ d+((d)^{3}),\] (1)

where the first term is the squared Riemannian norm of the tangent vector \((d,d)\) approximating the squared Fisher-Rao distance. The detailed derivation of the KL divergence of the Gaussian manifold is described in Appendix B.2.

### Pseudo Gaussian manifold normal distribution

We propose a pseudo Gaussian manifold (PGM) normal distribution defined over the Gaussian manifold. Let \((,)_{c}\) be a point in the Gaussian manifold. Inspired by the Riemannian normal distribution (Pennec, 2006), we define the probability density function of PGM normal with the KL divergence as:

\[_{c}(,;,,)=}{Z(c,, )}(-}((, ^{2})(,^{2}))}{2c ^{2}}),\] (2)

where \((,)_{c}\), and \(_{>0}\) are the parameters of the distribution. The distribution is centered at \((,)\) with additional scale parameter \(\). We verify the convergence of the PGM normal and compute the normalizing constant \(Z(c,,)\) over the probability measure of the Gaussian manifold at Appendix C.1. As shown in Equation 1, the KL divergence of the Gaussian manifold approximates the Fisher-Rao distance between \((,^{2})\) and \((,^{2})\). Therefore, the PGM normal accounts for the geometric structure of the univariate Gaussian distributions.

The factorization of the probability density function in Equation 2 multiplied with the square root of the determinant of the FIM shows the advantages of the PGM normal, which can be written as:

\[_{c}(,;,,)=(;,^{2}^{2}) 2( ^{2};}+1,^{2}}),\] (3)

where \((z;a,b)=}{(a)}z^{a-1}(-bz)\) and \(g\) is the FIM of the Gaussian manifold. The \(\) term enables us to sample and compute the KL divergence in an Euclidean manner. Thanks to the properties of Gaussian and gamma distributions, the PGM normal is easy to sample and has a closed-form KL divergence. The detailed derivation is available in Appendix C.2 and Appendix C.3. The factorization has the same form as the well-known conjugate prior to the Gaussian distribution. In that sense, the PGM normal explicitly incorporates the geometric structure between Gaussians into the known prior distribution.

We note that the PGM normal can be easily extended for the diagonal Gaussian manifold, a manifold formed by diagonal Gaussian distributions since the diagonal Gaussian manifold is the product of the Gaussian manifolds.

### Gaussian manifold VAE

We introduce a Gaussian manifold VAE (GM-VAE) whose latent space is defined over the diagonal Gaussian manifold with the help of the PGM normal. We use the PGM normal for variational and prior distributions. To be specific, with the PGM normal, the evidence lower bound (ELBO) of the GM-VAE can be formalized with the diagonal Gaussian manifold \(\{(,)^{n}, ^{n}_{>0}\}\) as:

\[_{q_{}(,|)} [ p_{}(,)]-D_{ KL }(q_{}(,) p (,)),\]

where \(p_{}(,)\) is the decoder network, \(q_{}(,)\) is the encoder network and \(p(,)\) is the prior. The variational distribution is set to \(q_{}(,)=_{c}(_{ }(),_{}(),_{}())\), where \(_{}()^{n}\) and \(_{}(),_{}()^{n}_{>0}\), and the prior is set to \(p(,)=_{c}(,I,I)\) in our experiments given curvature \(-c\). The pseudo-algorithm for the decoder of GM-VAE is present at Algorithm 1.

## 4 Related Work

Information geometry on VAE.Focusing on the bridge between probability theory and differential geometry, the adaptation of information geometry to the deep learning framework has been investigated in various aspects (Karakida et al., 2019; Bay and Sengupta, 2017; Gomes et al., 2022). Having said that, Han et al. (2020) show that the training process of VAE can be seen as minimizing the distance between the two statistical manifolds: manifolds with the parameters of the decoder and the encoder. Not only can the parameters but the outputs from the VAE decoder be modeled as probability distributions. Arvanitidis et al. (2021) suggest a method of using the pull-back metric defined with arbitrary decoders on the latent space. Our work focuses more on the statistical manifolds lying on the outputs of the encoder with the benefits from the information geometry.

VAE with Riemannian manifold latent space.The latent space of VAE reflects the geometrical property of the representations of the data. The efficacy of setting the latent space to be hyperbolic space (Mathieu et al., 2019; Nagano et al., 2019; Cho et al., 2022) or elliptic space (Xu and Durrett, 2018; Davidson et al., 2018) has been verified for various datasets. Skopek et al. (2019) further extend the approach to enable the latent space to be the product of Riemannian manifolds with different learnable curvatures. On top of these, we explore the method of setting the latent space to be the diagonal Gaussian manifold, which can be viewed as the product of hyperbolic spaces, and provide a novel viewpoint on prior work with information geometry.

Distributions over hyperbolic space.Defining a tractable distribution over hyperbolic space is challenging. Nagano et al. (2019) suggest hyperbolic wrapped normal distribution (HWN) from the observation that the tangent space is Euclidean space. Leveraging operations defined on the tangent spaces, e.g., parallel transport, enables an easy sampling algorithm. Mathieu et al. (2019) propose a rejection sampling method for the Riemannian normal distribution defined on the Poincare disk model, namely Poincare normal distribution. This method rejects the pathological samples and enables accurate sampling from the distribution in exchange for high computational complexity.

Although these distributions are widely adopted in many applications (Skopek et al., 2019; Mathieu and Nickel, 2020; Cho et al., 2022), one can barely adopt the full covariance matrix due to the difficulties

```
0: Parameter \((,)_{c},\), Decoding layers \(()\)
0: Reconstruction \(^{}\)
1: Sample \((,^{2}^{2})\)
2: Sample \(^{2}(}+1,^{2}})\)
3:\(^{}=([,^{2}])\)
4:return\(^{}\) ```

**Algorithm 1** Decoderin Monte-Carlo based KL approximation. The number of samples to approximate the KL divergence increases exponentially when the full covariance matrix is used (Cho et al., 2022), so it is common to use isotropic or diagonal covariance instead. Especially in the Poincare normal, the computation of KL divergence is slow due to the expensive rejection sampling.

RL with hyperbolic space.The hierarchical relationship between the states lying on the trajectories earned from RL agents has been gaining attention recently. Nagano et al. (2019) have studied that the hierarchical structure of Atari2600 Breakout game states can be well-captured with hyperbolic VAEs. We compare the same task, where GM-VAE outperforms the previous work. Cetin et al. (2022) suggest using hyperbolic space as the geometric prior for representation learning in model-free RL agent, showing improvements in generalization performances. Here, we focus on model-based RL, especially the method of using the world model (Ha and Schmidhuber, 2018; Hafner et al., 2020), and open the possibility of applying hyperbolic space to broader domains of RL by solving the bottleneck of the numerical stability.

## 5 Experiments

In this section, we demonstrate the performances of GM-VAE on two tasks: density estimation of image datasets and model-based RL. We remark on the practical properties of GM-VAE shown in the experiments with additional analyses.

### Density estimation on image datasets

We conduct density estimation on image datasets to measure the effectiveness of hyperbolic latent space against Euclidean space with the proposed GM-VAE. We use three datasets: the images from Atari2600 Breakout with binarization (Breakout) (Nagano et al., 2019), Oxford 102 Flower (Oxford102) (Nilsback and Zisserman, 2008), Food101 (Bossard et al., 2014), and Caltech-UCSD Birds-200-2011 (CUB) (Wah et al., 2011). The datasets are chosen with the four lowest \(\)-hyperbolicity (\(\)-H), a metric that measures how the given images are well-embed in hyperbolic space. Low \(\)-H implies that the dataset is likely to embed in hyperbolic space. The details about \(\)-H are available in Appendix D. The values of \(\)-H for the four datasets and other candidate datasets are in Appendix D. Several studies show that the images from the chosen datasets have an implicit hierarchical structure (Nagano et al., 2019; Li et al., 2019; Bossard et al., 2014; Kerdels and Peters, 2015).

We compare GM-VAE with the three baseline models: VAE with Euclidean latent space (\(\)-VAE), and hyperbolic VAE equipped with HWN (\(\)-VAE) and Poincare normal (\(\)-VAE). We use the product

    & \(d\) & \(\)-VAE & \(\)-VAE & \(\)-VAE &  GM-VAE \\ (\(c=1\)) \\  &  GM-VAE \\ (\(c=3/2\)) \\  & 
 GM-VAE \\ (\(c=3/2\)) \\  \\   & 2 & \(124.74_{ 0.86}\) & \(122.58_{}\) & \(270.05_{ 2.84}\) & \(}\) & \(122.64_{ 1.13}\) & \(122.47_{ 1.98}\) \\  & 4 & \(66.39_{ 0.76}\) & \(66.70_{ 0.32}\) & \(271.73_{ 42.95}\) & \(65.83_{ 0.49}\) & \(66.39_{ 0.50}\) & \(}\) \\  & 8 & \(}\) & \(45.25_{ 0.27}\) & \(81.55_{ 64.61}\) & \(45.14_{ 0.30}\) & \(45.31_{ 0.36}\) & \(45.36_{ 0.49}\) \\   & 50 & \(992.05_{ 1.38}\) & \(993.03_{ 1.64}\) & \(990.49_{ 2.26}\) & \(985.46_{ 3.82}\) & \(986.27_{ 3.81}\) & \(}\) \\  & 60 & \(969.99_{ 3.13}\) & \(968.79_{ 3.70}\) & \(964.02_{ 3.55}\) & \(958.00_{ 3.25}\) & \(960.88_{ 3.46}\) & \(}\) \\  & 70 & \(949.13_{ 2.72}\) & \(948.88_{ 3.19}\) & \(942.44_{ 4.40}\) & \(939.08_{ 3.13}\) & \(942.43_{ 3.44}\) & \(}\) \\   & 50 & \(1297.81_{ 4.51}\) & \(1298.45_{ 6.32}\) & \(1293.26_{ 7.14}\) & \(}\) & \(1299.58_{ 7.02}\) & \(1290.57_{ 8.23}\) \\  & 60 & \(1224.03_{ 8.31}\) & \(1227.16_{ 5.18}\) & \(1218.09_{ 3.88}\) & \(1213.31_{ 3.88}\) & \(1216.63_{ 4.56}\) & \(}\) \\  & 70 & \(1164.95_{ 3.80}\) & \(1165.39_{ 5.54}\) & \(1165.91_{ 4.91}\) & \(1152.80_{ 3.35}\) & \(1160.97_{ 4.18}\) & \(}\) \\   & 50 & \(1297.41_{ 6.69}\) & \(1296.41_{ 1.56}\) & \(1294.12_{ 1.80}\) & \(1292.90_{ 3.43}\) & \(}\) & \(1289.09_{ 1.72}\) \\  & 60 & \(1253.80_{ 2.57}\) & \(1256.52_{ 2.99}\) & \(1251.77_{ 1.82}\) & \(}\) & \(1248.72_{ 1.62}\) & \(1247.47_{ 2.51}\) \\   & 70 & \(1231.52_{ 3.18}\) & \(1229.38_{ 3.44}\) & \(1219.75_{ 1.72}\) & \(1215.07_{ 2.52}\) & \(1218.54_{ 3.85}\) & \(}\) \\   

Table 1: Density estimation on real-world datasets. \(d\) denotes the latent dimension. We report the negative test log-likelihoods of average 10 runs for Breakout, CUB, Food101, and Oxford102 with 95% confidence interval. N/A in the log-likelihood indicates that the results are not available due to the failure of all runs, and N/A in the standard deviation indicates the results are not available due to failures of some runs. The best results are bolded.

latent space for both \(\)-VAE and \(\)-VAE, and set the curvature value to \(-1\). The other details on the implementation and experimental setups are described in Appendix E.1.

The results are reported at Table 1. GM-VAE outperforms the baselines in all the settings, except one case of Breakout. Especially in CUB and Oxford102, GM-VAE outperforms the baselines regardless of the curvature value. In Breakout, \(\)-VAE shows inferior performance due to unstable training, and \(\)-VAE fails in some of the runs with small latent dimension. The results of \(\)-VAE and \(\)-VAE with non-product latent space, a common choice in previous work, are also present in Appendix F.

### State representation learning in model-based RL

We focus on the model-based RL task to verify the utility of GM-VAE on various tasks. Specifically, we apply GM-VAE to a world model, which aims to learn the representation of the environments (Ha and Schmidhuber, 2018; Hafner et al., 2019, 2020). We use DreamerV2 (Hafner et al., 2020) as the baseline model to evaluate the performance of GM-VAE in modeling environments. DreamerV2 is composed of a recurrent state space model (RSSM) (Hafner et al., 2019) and three predictors for the image \(p_{}(x_{t}|h_{t},z_{t})\), the reward \(p_{}(r_{t}|h_{t},z_{t})\), and the discount factor \(p_{}(_{t}|h_{t},z_{t})\), where \(x_{t}\) is the observation which the format is the image, \(r_{t}\) is the reward, \(_{t}\) is the discounting factor, \(h_{t}\) is the deterministic recurrent state, and \(z_{t}\) is the stochastic state. The model is trained by maximizing the likelihood of \(p(,,)\) given observations \(\), rewards \(\), and discount factors \(\) earned from the sequence of actions \(\) of an agent. By deriving the evidence lower bound of \(p(,,)\), the world model is learned to optimize the likelihood with the variational distribution \(q_{}(z_{t} h_{t},x_{t})\) with the following objective \((,)\) as:

\[(,)=_{t=1}^{T}- p_{ }(x_{t},r_{t},_{t}|h_{t},z_{t})+}[q_{ }(z_{t}|h_{t},x_{t})\  p_{}(z_{t}|h_{t})],\]

where \(\) is KL loss scaling factor, \(T\) is the length of input sequence, \(p_{}\) is the prior, and \(q_{}\) is the approximated posterior. GM-VAE is employed by replacing the space of \(z_{t}\) with the Gaussian manifold and two components in RSSM, the representation model \(q_{}(z_{t}|h_{t},x_{t})\) and transition predictor \(p_{}(z_{t}|h_{t})\), with PGM normal.

We compare evaluation scores between different types of latent space on world model learning over the Atari2600 environments. The agents are trained with 100M environment steps. We select games having the \(\)-H values of the four lowest and the two highest among 60 popular Atari2600 games. The other details on the implementation and experimental setups are described in Appendix E.2 with the \(\)-H for all 60 games in Appendix D.3.

With a commonly-used hyperbolic distribution, i.e., HWN, we observe that training the world model fails due to the numerical stability issue. On the other hand, GM-VAE shows competitive results with the baselines in Euclidean and discrete latent space in all the games we test. The results are reported in Figure 1(b). We note that the reproduced Euclidean baseline results by using the official code are better than those reported in Hafner et al. (2020).

### Remark on GM-VAE

Numerical stability.One notable property of GM-VAE is the numerical stability during training compared to \(\)-VAE and \(\)-VAE. During the experiments, \(\)-VAE and \(\)-VAE fail to run in some of the Breakout image density estimations and all the seeds of model-based RL due to the numerical instability. Similar observations are also reported in several previous works (Mathieu et al., 2019; Chen et al., 2021; Skopek et al., 2019). The sampling from a hyperbolic distribution is a major cause of the numerical instability. Consequently, the sampling-based KL divergence computation can be unstable.

We first show that sampling from PGM normal can be stabilized via a simple reparameterization trick. To train GM-VAE, one needs to obtain sample \(\) and \(\) from PGN normal \(_{c}(,,)\). Sampling \(\) can be done from \((,^{2}^{2})\) without numerical issues. Sampling \(\) can be done from \((a,b)\) where \(a=1/4c^{2}+1\), \(b=1/4c^{2}^{2}\) as shown in Appendix C.2. However, due to machine precision error, often, \(\) violates the manifold constraints, i.e., \(=0\). Eventually, direct sampling of \(\) can cause the numerical instability. To avoid \(\) being zero, we use the output of the VAE encoder as \(^{2}\) whose value ranges over the entire real numbers and is more stable even when \(\) is close to zero. With \(^{2}\), instead of sampling \(^{2}\), we sample \(^{2}=+ b\), where \(\) is sampled from \((a,1)\), through the reparameterization of the Gamma distribution, where \( b\) can be directly computed from \(^{2}\).

We can show that the KL divergence between an arbitrary PGM normal and prior distribution \(_{c}(,I,I)\) has a closed-form solution without any sampling. The KL divergence of PGM normal is the sum of the KL divergences between two Gaussian distributions and between two Gamma distributions, as shown in Appendix C.3. First, the KL divergence between a univariate Gaussian distribution \((,^{2})\) and the prior distribution can be obtained with \(^{2}\) as shown in Equation 4. Second, the KL divergence between the two Gamma distributions, \((a_{1},b_{1})\) and \((a_{2},b_{2})\), written as:

\[D_{}((a_{1},b_{1})(a_{2},b_{2})) =a_{2}}{b_{2}}-)}{(a_{2})}+(a_{1}-a_ {2})(a_{1})-(1-}{b_{1}})a_{1},\]

where \(\) is the digamma function, can be computed using \( b\).

Training time comparison.Common bottlenecks of the mode hyperbolic VAEs arise from the complex manifold constraints and the difficulty of sampling from the hyperbolic distributions. For example, the Poincare disk model of \(\)-VAE and the Lorentz model of \(\)-VAE requires the samples to be inside of a unit disk and to be on a hyperboloid with constraint \(\{^{n+1}-x_{0}^{2}+_{i=1}^{n}x_{i}^{2}\}\), respectively. Such manifolds need complex transformations, e.g., clipping, projection, or geometric transformations using the Riemannian operations, to match the manifold constraint so making the training of the hyperbolic VAEs slower. The Gaussian manifold, on the other hand, has a much simple manifold constraint and even does not require any transformations if we utilize the log space of \(\).

We report the time consumptions of the VAEs with the latent dimension of 8 per epoch in the density estimation of Breakout at Table 2. The results demonstrate that the algorithmic distinctions enable GM-VAE to be trained much faster than the baseline hyperbolic VAEs and even similar to \(\)-VAE.

Latent space analysis.We present a plot of the learned representation in the hyperbolic space at Figure 1(a) for qualitative analysis. We take the world model with GM-VAE trained for Breakout and illustrate the geodesic starting from the origin in the figure with four generated samples along with the geodesic. We also provide the scatter plot of game states with their cumulative rewards represented in different colors. The brighter the color, the higher the cumulative reward. The scatter

   \(\)-VAE & \(\)-VAE & \(\)-VAE & GM-VAE \\ 
24.5 & 35.9 & 49.2 & 25.5 \\   

Table 2: The training time of the VAEs in density estimation of the Breakout image dataset. We report the training time of the VAEs in seconds per epoch. GM-VAE is 1.93x faster than \(\)-VAE and 1.41x faster than \(\)-VAE in the experiments held on a single A100 40GB PCI GPU.

Figure 2: The results of model-based RL experiment. (a) The dots from yellow to purple represent the latent states from the world model in the Atari2600 Breakout with decreasing rewards. Along the red geodesic dashed line passing, we sample for images to visualize the learned representations. As the sample shows, we can observe a hierarchical structure at different stages of the game along the geodesic. (b) We compare the methods of using Euclidean, discrete, and hyperbolic latent space. We report averaged rewards over four runs and bold the best reward.

plot reveals that the states with high cumulative rewards are distributed near the origin. Together with the samples from the geodesic, we can observe that the hierarchical structure of Breakout is well captured in the latent space.

Note that in the Poincare disk model, the depth of the hierarchy is expected to be shown as the distance from the origin (Nickel and Kiela, 2017). When the root node is placed near the origin, the leaf nodes are likely to be placed near the boundary of the open disk. The geodesic lines starting from the origin to the boundary of the Poincare disk model are identical to the geodesics of the Gaussian manifold starting from \((0,1)\), i.e., the origin of the Gaussian manifold. The connection implies that the data hierarchy should be aligned along the geodesic curves if the hierarchy is well captured.

To quantitatively measure the correlation between the cumulative rewards and the states, we measure the Pearson correlation between the cumulative reward and the norm of the states. We obtain a correlation coefficient of 0.46 from the hyperbolic latent space, whereas the correlation coefficient of the Euclidean latent space is 0.40, showing the hyperbolic space better captures the hierarchy along the increasing norm. More experimental details are explained in Appendix G.2.

## 6 Conclusion & Future Work

In this work, we propose a novel method of representation learning with GM-VAE, utilizing the Gaussian manifold for the latent space. With the newly-proposed PGM normal defined over the Gaussian manifold, which shows better stability and ease of sampling compared to the commonly-used ones, we verify the efficacy of our method on several tasks. Our method achieves outperforming results on density estimation with image datasets and competitive results on model-based RL compared to the baselines. We explain the behavior of GM-VAE in terms of solving the frequent numerical issue of commonly-used hyperbolic VAEs. The analysis of latent space exhibits that the hierarchy lying in the dataset can be preserved by using GM-VAE.

We suggest that the numerical stability of our method can be helpful for scaling the generative models, e.g., very deep VAE (Child, 2021), endowed with hyperbolic geometrical priors. As GM-VAE is beneficial for capturing hierarchy with promising results in modeling RL environment, another potential future work can be extending the use of hyperbolic space, such as learning a skill tree for solving complex long-horizon tasks (Shi et al., 2022). We believe that the connection between the statistical manifold and hyperbolic space provides new insight to the research community and hope to see more interesting connections and analyses in the future.