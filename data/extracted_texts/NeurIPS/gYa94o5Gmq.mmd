# Towards Unsupervised Model Selection for

Domain Adaptive Object Detection

Hengfu Yu Jinhong Deng Wen Li Lixin Duan

University of Electronic Science and Technology of China

hfyu@std.uestc.edu.cn, {jhdengvision, liwenbnu, lxduan}@gmail.com

Equal contribution.The corresponding author.

###### Abstract

Evaluating the performance of deep models in new scenarios has drawn increasing attention in recent years due to the wide application of deep learning techniques in various fields. However, while it is possible to collect data from new scenarios, the annotations are not always available. Existing Domain Adaptive Object Detection (DAOD) works usually report their performance by selecting the best model on the validation set or even the test set of the target domain, which is highly impractical in real-world applications. In this paper, we propose a novel unsupervised model selection approach for domain adaptive object detection, which is able to select almost the optimal model for the target domain without using any target labels. Our approach is based on the flat minima principle, _i.e._, models located in the flat minima region in the parameter space usually exhibit excellent generalization ability. However, traditional methods require labeled data to evaluate how well a model is located in the flat minima region, which is unrealistic for the DAOD task. Therefore, we design a Detection Adaptation Score (DAS) approach to approximately measure the flat minima without using target labels. We show via a generalization bound that the flatness can be deemed as model variance, while the minima depend on the domain distribution distance for the DAOD task. Accordingly, we propose a Flatness Index Score (FIS) to assess the flatness by measuring the classification and localization fluctuation before and after perturbations of model parameters and a Prototypical Distance Ratio (PDR) score to seek the minima by measuring the transferability and discriminability of the models. In this way, the proposed DAS approach can effectively represent the degree of flat minima and evaluate the model generalization ability on the target domain. We have conducted extensive experiments on various DAOD benchmarks and approaches, and the experimental results show that the proposed DAS correlates well with the performance of DAOD models and can be used as an effective tool for model selection after training. The code will be released at [https://github.com/HenryYu23/DAS](https://github.com/HenryYu23/DAS).

## 1 Introduction

With the explosion of deep neural networks , object detection  has achieved promising results and shown great potential in many downstream tasks such as autonomous driving , video understanding , robot navigation , _etc._ However, the well-trained object detection models frequently face previously unseen domains in real-world scenarios and often suffer from dramatic performance degradation when being deployed in a novel domain . This is because the training set (_i.e._, source domain) and the test set (_i.e._, target domain) have distinct domain distributions. To address this problem, Domain Adaptive Object Detection (DAOD) has been proposed to transfer the knowledge from the labeled source domain to an unlabeled target domain by leveraging adversarial training or pseudo-labeled approaches.

Although effective, these DAOD methods  evaluate the detector performance and conduct model selection relying on the labeled target data, which is usually unavailable and impractical for real-world domain adaptation scenarios. Due to the natural complexity of object detection, DAOD methods that leverage adversarial training and self-training techniques are often unstable and prone to overfitting to the target domains. As shown in Fig. 1 (a), DAOD methods usually suffer from a performance drop during training (marked as purple in Fig. 1 (a)). These issues limit the application of the DAOD model in real-world scenarios. Therefore, it is urgent and desirable to develop an unsupervised model selection method for DAOD, as shown in Fig. 1 (b).

Regarding the unsupervised model selection, several seminal works  attempt to evaluate the models from different aspects. For example, TS  examines the spatial uniformity of the unsupervised domain adaptive classifier, as well as the transferability and discriminability of deep representation. ATC  learns a threshold on the confidence of the model and predicts accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. In object detection, however, the evaluation involves not only classification but also precise localization of objects within an image. This crucial distinction renders these methods ineffective in fully assessing an object detection model. A recent work  proposes a BoS metric to evaluate the generalization of the detection model by measuring the stability of the box under feature dropout. However, it does not consider the domain discrepancy between the source and the target domain, making the metric unreliable for DAOD model selection (see Fig. 1 (a) and our experiments in Sec. 4.2).

In this paper, we propose a novel Detection Adaptation Score (DAS) to evaluate the DAOD models without accessing any target labels, enabling select almost the optimal model for the target domain in an unsupervised way. The proposed DAS is based on the flat minima principles, _i.e._, models that are located in the flat minima region in the parameter space exhibit better generalization ability than that in sharp minima region . However, the traditional flat minima search method requires labeled data to evaluate how well a model is located in the flat minima region, which is unrealistic for DAOD tasks. Therefore, we investigate how to measure the flat minima approximately without using target labels. We derive a generalization error bound that shows the flatness can be deemed as model variance, while the minima depend on the domain distribution distance for the DAOD tasks. Therefore, we first propose a Flatness Index Score (FIS) to assess the flatness by observing the classification and localization fluctuation before and after perturbations of model parameters. Then, we propose a Prototypical Distance Ratio (PDR) score to seek the minima models by measuring the transferability and discriminability of the models in the target domain. To this end, the proposed DAS can effectively represent the degree of flat minima for DAOD models and evaluate the model performance on the target domain. To evaluate the effectiveness of the proposed DAS, we conduct extensive experiments on public DAOD benchmarks, including weather adaptation, real-to-art, and synthetic-to-real adaptation. The experimental results show that the proposed metric can effectively evaluate the performance of DAOD models without annotating the target domain.

Figure 1: (a) The performance of classic DAOD method AT  on Real-to-Art (P2C) adaptation task during training. It suffers from performance degradation as the training goes on. The proposed DAS outperforms previous unsupervised model evaluation methods and selects desirable checkpoints without accessing any labels in the target domain. (b) The motivation of the work. We propose a Detection Adaptation Score including a Prototypical Distance Ratio (PDR) score and Flatness Index Score (FIS) to evaluate the model performance in an unsupervised way. It can be a good substitute metric for using annotations for DAOD model evaluation.

The contributions of our work can be summarized as follows:

* With the pressing need for the application of DAOD in real-world scenarios, to our best knowledge, we are the first to evaluate the DAOD models without using target labels.
* We propose a novel Detection Adaptation Score (DAS) by seeking the flat minima without using any target labels to evaluate the performance of DAOD models on the target domain. A Flatness Index Score (FIS) and a Prototypical Distance Ratio (PDR) score are proposed to meet the requirements of flatness and minima, respectively.
* We have conducted extensive experiments on several DAOD benchmarks and approaches. Our DAS benefits from selecting the optimal checkpoints of model parameters to avoid negative transfer or pseudo-label error accumulation. The experimental results validate the effectiveness of the proposed DAS.

## 2 Related Work

**Object Detection.** Object detection [23; 47; 54; 46; 39] is a fundamental task in computer vision that involves identifying and locating multiple objects within an image or video. The object detection approaches can be roughly divided into two categories: one-stage and two-stage. The one-stage methods [54; 46; 39; 19] directly estimate the categories and the location of the objects, such as FCOS , CenterNet , and YOLO series . The two-stage methods [23; 47; 3] first generate region proposals for objects and then classify the category and regress the bounding box coordinates based on the proposal features, _e.g._, Faster RCNN  and Cascade RCNN . Recently, an end-to-end object detection model based on Transformer (_i.e._, DEtection TRansformer, DETR)  has been proposed to eliminate the complex anchor generation and post-processing operations such as non-maximum suppression (NMS). Many successive works [67; 63] further improve the training efficiency and accuracy of DETR. With the strong representation of deep neural networks, the object detection model has achieved promising results in many object detection benchmarks. However, these models usually suffer from performance degradation  because of the domain discrepancy between the training and testing domains.

**Domain Adaptive Object Detection.** Domain Adaptive Object Detection (DAOD) [11; 50; 4; 38; 15; 8; 66; 16; 30; 65] aims to transfer knowledge from the labeled source domain to an unlabeled target domain. Previous works can be roughly categorized into two aspects: domain alignment and self-training. The domain alignment [11; 50; 8; 66] directly minimizes the domain discrepancy between the source and target domains. They minimize the feature distribution mismatch via adversarial training [11; 50; 66; 16], prototype alignment , graph matching [35; 36; 37], _etc._ The self-training approaches [14; 38; 4; 15; 10] follow a Mean Teacher (MT) framework and generate pseudo-labels from the teacher network to supervise the training of the student network. UMT  leverages the style transfer algorithm to eliminate the MT model bias towards the source domain. Adaptive Teacher (AT)  combines adversarial training and self-training to improve the accuracy on the target domain. CMT  introduces contrastive learning to improve the instance feature representation. HT  reveals that the consistency between classification and localization is crucial for pseudo-label generation and proposes a reweight strategy based on the harmony measure between the classification and localization. While effective, these approaches [11; 50; 4; 38; 15; 16; 35; 36] evaluate the model performance relying on labeled target data, which is not always available in real-world scenarios. Therefore, we propose a Detection Adaptation Score (DAS) to evaluate the model performance in an unsupervised manner, enabling the application of DAOD models in real-world scenarios.

**Unsupervised Model Selection (UMS) for UDA.** Unsupervised Model Selection for UDA evaluates model performance in the target domain without involving annotations. Some previous works [22; 45; 40; 44; 62; 9; 26; 59] seek to predict model performance in OOD scenarios. PS , ATC  leverage the prediction confidence, and [49; 48] estimate from the perspective of entropy. DEV  estimates and decreases the target risk by embedding adapted feature representation while validation. TS  examines the spatial uniformity of the classifier, as well as the transferability and discriminability of deep representation. However, they mainly focus on classification tasks. For object detection, the BoS  estimates the detection performance via the stability of bounding boxes with feature dropout but does not consider the domain discrepancy, which is vital for DAOD methods. To this end, we introduce the Detection Adaptation Score (DAS), which consists of a Flatness Index Score (FIS) and a Prototypical Distance Ratio (PDR) score by seeking a flat minima model in the target domain.

## 3 Method

In the DAOD task, we are given a labeled source domain, which includes images annotated with bounding boxes and class labels, and an unlabeled target domain containing only unlabeled images. Let us denote \(_{s}=\{(x^{s}_{i},y^{s}_{i})\}_{i=1}^{N_{s}}\) drawn from a data distribution \(_{s}\) as the labeled source domain and \(_{t}=\{x^{t}_{j}\}_{j=1}^{N_{t}}\) drawn from a data distribution \(_{t}\) as the unlabeled target domain. Distributions \(_{s}\) and \(_{t}\) are related but different domains, _i.e._, (\(_{s}_{t}\)). In other words, they have distinct domain shifts. And \(^{s}_{i}=\{(^{s}_{i},c^{s}_{ij})|_{j=1}^{m_{i}}\}\), where \(^{s}_{ij}^{4}\) and \(c^{s}_{ij}\{1,,K\}\) are the bounding box and corresponding category for each object, and \(m_{i}\) is the total number of objects in an image \(x^{s}_{i}\). The goal of the DAOD approach is to learn an object detection model that performs well on the target domain from both the labeled source and unlabeled target domains. In this work, we consider the model selection for DAOD approaches. In particular, there are \(M\) models \(=\{f^{t}|_{l=1}^{M}\}\) from different epochs or iterations. Our goal is to propose a proper metric for model evaluation in an unsupervised manner that can reflect the detection performance (_i.e._, mAP) of the DAOD models.

In the following, we first introduce the domain adaptation generalization bound with flat minima in Sec. 3.1. Then, we will introduce our Detection Adaptation Score (DAS) in detail, which consists of a Flatness Index Score (FIS) in Sec. 3.2 and a Prototypical Distance Ratio (PDR) score in Sec. 3.3.

### Domain Adaptation Generalization Bound with Flat Minima

In this paper, we propose a novel unsupervised model selection approach for the DAOD task, which can almost select the optimal checkpoint for the target domain. Our approach is based on the assumption that flat minima exhibit better model generalization than sharp minima, which has been evidenced by many literatures [21; 7]. The model parameters at flat minima will have smaller changes of loss values within its neighborhoods than sharp minima. To find flat minima, traditional methods require labeled data, which is unrealistic for DAOD. As the target labels are unavailable for model evaluation in DAOD scenarios, we cannot directly find the flat minima. To this end, we derive a generalization error bound as follows:

**Theorem 1**.: _Given any \( 0\), exist hypothesis \(h\) where \(\) is the hypothesis set, \(_{h}\) denotes the parameters of \(h\). Given any hypothesis \(h^{}\{h^{}|h^{},\|_{h^{}}- _{h}\|_{2}\}\), which is located in the neighborhood of \(h\) with radius \(>0\), the following generalization error bound holds with at least a probability of \(1-\),_

\[_{}(h^{})|_{}(h^{ })-_{}(h)|+_{}(h)+dis(,)+, \]

_where \(dis(,)\) is the distribution mismatch between the source domain \(\) and target domain \(\). \(_{}(h)\) is the risk of \(h\) on the source domain. \(\) is a constant term. Proof is provided in the appendix._

The generalization bound shows that in addition to the constant term \(\), the risk for the target hypothesis \(h^{}\) around by \(h\) with \(\)-ball radius can be bounded by three terms: the flatness, _i.e._, the difference between the original \(h\) and neighborhood \(h^{}\) of \(h\), the error on the source domain \(_{s}\), and the distribution distance between the source and target domains. Usually, the error on source \(_{}(h)\) can be minimized with the labeled source samples. To this end, one can minimize the first and third terms to find the flat minima.

From the analysis in Theorem 1, the flatness can be deemed as model variance, while the minima depends on the domain distribution distance for DAOD task. Accordingly, we propose a Flatness Index Score (FIS) to assess the flatness by measuring the classification and localization fluctuation before and after perturbations of model parameters, and a Prototypical Distance Ratio (PDR) score to seek the minima by measuring the transferability and discriminability of the models. In this way, the proposed DAS approach can effectively represent the degree of flat minima and evaluate the model generalization ability on the target domain.

### Flatness Index Score

In this subsection, we introduce the Flatness Index Score (FIS), which assesses the flatness of the model parameters by measuring the variance in outputs before and after parameter perturbations. For object detection, we calculate the variance of both classification and localization predictions. For the property of minima on the target domain, we introduce it later in Sec. 3.3.

Specifically, let \(\) denote the radius of the parameter space. We can then obtain the neighbor model \(f(;^{})\) by adding a perturbation \(\) to the original parameter \(\), _i.e._, \(^{}+\). We control \(\) of the perturbation as a constant (_i.e._, \(\|\|=\)), thus the neighbor model \(f(;^{})\) lies on a fixed radius sphere of the original model \(f(;)\).

We measure the prediction correspondence between the original and neighbor models. Given an input target domain image \(x_{i}\), the original model predicts \(\{(_{j},_{j})|_{j=1}^{n_{i}}\}\) as the neighbor model predicts \(\{(}_{j},}_{j})|_{j=1}^{n_{j}}\}\), where \(_{j}\) is the bounding box and \(_{j}\) is the probability vector of the \(j\)-th instance. We use \(d_{jj^{}}(f_{}(x_{i};))\) to represent the matching cost of the \(j\)-th and \(j^{}\)-th object in two model's predictions on image \(x_{i}\). As the model predictions has results from both regression and classification branches, the matching cost \(d_{jj^{}}\) contains the divergence of bounding boxes and classification probability vectors as follows:

\[d_{jj^{}}=(_{j},}_{j^{}})- (_{j},}_{j^{}}), \]

where \(\) is the intersection over union of two boxes, \(\) is the KL-divergence over two probability vectors. The smaller the \(d_{jj^{}}\) is, the better the two object predictions match. Then the Flatness Index Score (FIS) on the original model parameter \(\) with radius \(\) is defined as:

\[=-_{^{}+}[_{ x_{i}_{t}}[_{}^{}}_{j=1} ^{n_{i}^{}}d_{j(j)}]], \]

where \(n_{i}^{}=\{n_{i},n_{i}^{}\}\) is the smaller number of the predicted objects from the original and neighbor models. \((j)\) is the \(j\)-th object's one-to-one corresponding index leading to the minimized matching cost when \(n_{i} n_{i}^{}\), otherwise \(=-_{^{}+}[_{ x_{i}_{t}}[_{}^{}}_{j^{ }=1}^{n_{i}^{}}d_{(j^{})j^{}}]]\). Hungarian Algorithm is applied to select the best-matched pairs.

### Prototypical Distance Ratio

To facilitate the search for flat minima, we explore methods to identify minima regions in the target domain. In DAOD, the model with better transferability and discriminability would perform well on the target domain. There are many methods to evaluate the domain distance between the source and target domains, for example, Maximum Mean Discrepancy (MMD)  and Proxy A-distance (PAD) . However, simply measuring the image feature distribution is not feasible to reflect the transferability of the detection model. Meanwhile, the traditional discriminability metrics such as entropy , and mutual information  also fail to effectively correlate the model performance. In this paper, we consider the class prototype distance of instances in images across domains to evaluate the transferability and discriminability of the DAOD models. The class prototype of instances is the center of a specific class and aggregates the instance information from the samples. In unsupervised domain adaptation, prototype-based domain alignment is comprehensively studied in the literature [58; 57; 43] and attempts to narrow the distance between prototypes of the same categories of two domains, showcasing the effectiveness of prototype alignment.

Now, we show how to leverage prototype distance to effectively evaluate the transferability and discriminability of the DAOD models. Our prototype is calculated based on the instance feature, _e.g._, the proposal feature in Faster RCNN. In particular, we denote the instance feature as \(F_{ij}^{d}\) for the \(j\)-th instance in the \(i\)-th image and the final classification probability vector as \(_{ij}\). The prototype of the \(k\)-th class \(P_{k}^{d}\) for the target domain can be calculated softly as follows:

\[P_{k}=_{x_{i}_{t}}[^{}}_{ j=1}^{n_{i}^{}}F_{ij}_{ij}^{k}], \]

where \(n_{i}^{}\) is the number of proposals in the \(i\)-th image. The source prototypes can be obtained similarly. Finally, we have the source and target domain prototypes \(P^{}^{K d}\) and \(P^{}^{K d}\).

An ideal DAOD network aligns the features of the same category between domains while increasing the feature distances between different categories. We propose a Prototypical Distance Ratio (PDR) score to evaluate this ability with the category-wise prototypes. The prototype distance of the same category across domains can be defined as follows:

\[d_{ intra}=(M(P^{},P^{})), \]

where \(M(P^{},P^{})^{K K}\) is the category-wise distance matrix between \(P^{}\) and \(P^{}\). Denote \(M_{kk^{}}(P,P^{})\) as the distance between \(P_{k}\) and \(P^{}_{k^{}}\).

Similarly, we could obtain the inter-category prototype distances as follows:

\[d_{ inter}=(P^{},P^{})(P^{},P^{})(P^{},P^{}), \]

where \((P,P^{})=-K}_{k k^{}}^{K}M_{kk^{ }}(P,P^{})\) is a function to calculate distance among different categories. Combine the intra-category distance and inter-category distance into our Prototypical Distance Ratio (PDR) score as follows:

\[ PDR=d_{ inter}/d_{ intra}. \]

The proposed PDR score can be an effective metric to evaluate the transferability and discriminability of the DAOD models. The higher PDR score indicates that the instance features of detection models have better properties with large inter-category distances and small intra-category distances.

### Detection Adaptation Score

For different checkpoints in a training session, we simply normalize the FIS and PDR separately by min-max normalization. The DAS is a combination of FIS and PDR as follows.

\[ DAS_{}=_{}+_{ }, \]

where \(t\) is the index of the checkpoint. \(_{}\) and \(_{}\) are the \(t\)-th FIS and PDR score normalized among all checkpoints. \(\) is a trade-off parameter that controls the contribution of \(_{}\) for \( DAS_{}\).

## 4 Experiments

### Experiment Setup

**Benchmarks.** We follow previous works [11; 50; 38] and evaluate the effectiveness of the proposed DAS on the following adaptation scenarios:

* **Real-to-Art Adaptation (P2C)**: In this scenario, we test our proposed method with domain shift between the real image domain and the artistic image domain. Following , we choose the PASCAL VOC 2007/2012 and Clipart1k as the source and target domains, respectively. **PASCAL VOC** is a widely used benchmark in the object detection community. We use the 2007 and 2012 versions of PASCAL VOC that contain about 15k images with instance annotations for 20 object classes. **Clipart1k** contains 500 train images and 500 testing images in clipart style, annotated with bounding boxes for the same 20 object categories as in the PASCAL VOC.
* **Weather Adaptation (C2F)**: In real-world scenarios, such as autonomous driving systems, object detectors may encounter various weather conditions. We study the adaptation from normal to foggy weather. In particular, we use the Cityscapes and Foggy Cityscapes as the source and target domains, respectively. **Cityscapes** contains a diverse set of urban street scenes captured from \(50\) cities and \(2,975\) training images and \(500\) validation images, annotated for \(8\) object classes. **Foggy Cityscapes** is a variant of the Cityscapes dataset where synthetic fog is added to the images to simulate adverse weather conditions. The annotations remain consistent with the original Cityscapes dataset. Note that we choose the worst foggy level (_i.e._, \(0.02\)) of Foggy Cityscapes in the experiment following .
* **Synthetic-to-Real Adaptation (S2C)**: Synthetic images offer an alternative solution for addressing the data collection and annotation issues. However, there is a distinct distribution mismatch between synthetic images to real images. To adapt the synthetic to the real scenes, we utilize Sim10k as the source domain and Cityscapes as the target domain. **Sim10k** consists of 10,000 synthetic images generated from a simulation environment, with annotations for car bounding boxes. Because only the car is annotated in both domains, we report the AP of "car" in the validation set of Cityscapes.

**DAOD Frameworks.** In this work, we test our method on four classical DAOD models in the two main DAOD streams (_i.e._, adversarial training and self-training): DA Faster RCNN (**DAF**) , Mean-Teacher (**MT**) , Adaptive Teacher (**AT**) , Contrastive Mean-Teacher (**CMT**) . DAF  is a typical DAOD framework extending the Faster RCNN architecture by incorporating domain adaptation techniques. It employs adversarial training to align the feature distribution across domains at both image and instance levels. MT  leverages a teacher-student paradigm to generate pseudo labels to provide extra supervision for the student model on the target domain. The teacher model is an exponential moving average (EMA) of the student model and thus can provide more accurate pseudo-labels for the student model. This approach helps in leveraging unlabeled data by enforcing consistency between the predictions of the teacher and student models. AT  improves upon the Mean-Teacher framework by employing an adversarial learning module to align the feature distributions across two domains, reducing domain bias and improving pseudo-label quality. CMT  enhances the Mean-Teacher framework with contrastive learning techniques by encouraging similar instances to be closer in the feature space while pushing the features of dissimilar instances apart.

**Unsupervised Model Selection Baselines.** Existing unsupervised model selection methods for UDA tasks are mainly based on classification tasks, such as Prediction Score (PS) , Entropy Score (ES) , Average Threshold Confidence (ATC) , Transfer Score (TS) . We reproduce these approaches on the classification branch of object detection models. Besides, we also use the Frechet Distance (FD)  to measure the domain distance on backbone features as a compared method. Bounding Box Stability (BoS) is introduced to tackle unsupervised model evaluation problems specifically for object detection networks. It evaluates the model generalization of the detection model by probing the bounding box stability under feature dropout.

**Implement Details.** Following previous works , we choose one of the representative detection frameworks Faster RCNN as our base detector. We followed the instructions from the released code of the DAOD methods and reproduced their results. The hyperparameters, learning rates, and optimizers are set according to the default configurations provided in the original papers. We set our hyperparameters \(=1\) and \(=1\) in all experiments, which perform well on all the benchmarks. Our implementation is built upon the Detectron2 detection framework. We have added a detailed implementation in the appendix.

  Benchmark &  &  &  \\ DAOD &  &  &  &  \\  DAF & 15.72 & 17.16 & +1.44 & 17.49 & 28.27 & 29.01 & +0.74 & 29.17 & 43.05 & 45.09 & +2.04 & 45.09 \\ MT & 34.25 & 35.27 & +1.02 & 35.75 & 45.74 & +0.00 & 47.51 & 54.85 & 55.41 & +0.56 & 55.41 \\ AT & 41.98 &

### Main Results

**Checkpoint Selection after Training.** Checkpoint selection after training is pivotal for the application of DAOD approaches in real-world scenarios since the DAOD models usually suffer from negative transfer and overfitting the target domain during training. For example, on the real-to-art adaptation (P2C), AT  drops \(5.85\%\) from the highest mAP (\(47.83\%\)) to the last checkpoint (\(41.98\%\)). The detailed results on checkpoint selection, with the highest DAS, after training are listed in Table 1. Compared with the last checkpoint, the proposed method works well on almost all the DAOD methods. For example, the proposed method for AT  achieves \(5.85\%\), \(1.1\%\), and \(19.09\%\) improvements in terms of mAP on real-to-art, weather, and synthetic-to-real adaptations, respectively. These experimental results demonstrate that the proposed method can choose a reliable checkpoint and thus avoid the negative transfer and overfitting on the target domain during training.

Moreover, we compare our method with the recent unsupervised model evaluation methods in Table 2, showing that our method consistently outperforms all the compared baselines. For instance, our methods outperform recent works TS  and BoS  by \(2.42\%\) and \(3.60\%\) mAP on average, respectively. These results demonstrate the effectiveness of the proposed method in choosing the optimal checkpoints. We also present the correlation between unsupervised model evaluation metrics and the ground truth mAP (_i.e._, using the annotations in the target domain to evaluate the model) in Fig. 2. It is clearly shown that the previous methods give higher scores as the training goes on and fail to correlate with the performance of DAOD checkpoints. In contrast, the proposed DAS score correlates well with the performance of DAOD checkpoints (_i.e._, \(0.86\) PCC).

Figure 3: Hyperprameter tuning on AT  using our DAS. (a) \(_{ dis}\) that controls the weight of adversarial loss from domain discriminator. (b)\(_{ unsup}\) that controls the weight of unsupervised loss.

Figure 2: The comparison of different unsupervised model evaluation methods for DAOD. The experiments are conducted on real-to-art adaptation (P2C) using AT. Note that the directions of all scores are unified.

**Hyperparameter Tuning for DAOD Methods.** Domain adaptation methods can be highly sensitive to the hyperparameters. Inappropriate hyperparameter selection would limit the transfer performance and lead to negative transfer, which even leads models perform below the source-only ones. Therefore, the validation of hyperparameters is an important problem in DAOD, yet researchers have unfortunately overlooked it. We evaluate our DAS in hyperparameters tuning by leveraging the weights of adversarial loss (denoted as \(_{}\)) for the domain discriminator and of unsupervised loss (denoted as \(_{}\)) for self-training in AT . For a model with a specific hyperparameter setting, we obtain the last three checkpoints of its training process and calculate their average performance to represent the model. The final DAS of the specific model is defined as the average DAS of these obtained checkpoints. The experimental results are conducted on the weather adaptation and summarized in Fig. 3. From Fig. 3, we can observe that the model with higher DAS presents better improvement after adaptation. The proposed DAS can correlate the detection performance without any labels on the target domain. We further compare our DAS with the unsupervised model evaluation methods and summarize the results in Table 3. The proposed DAS consistently outperforms the previous baselines, which demonstrates the effectiveness of the proposed method in hyperparameter tuning. In a nutshell, our DAS can be a good indicator to guide the hyperparameter tuning for DAOD methods, thus avoiding the negative model optimization due to the inappropriate hyperparameter selection.

### Further Analysis

**Ablation Study.** We conduct the ablation study of the proposed method by isolating DAS into separate metrics. In particular, we conduct experiments on three benchmarks and use four DAOD methods including DAF , MT , AT , and CMT . We summarize the results in Table 4. The experimental results indicate that the proposed PDR score and FIS evaluate the model performance without labels effectively. In particular, the PDR score and FIS select checkpoints with mAP \(42.14\%\) and \(41.74\%\), PCC \(0.64\) and \(0.48\), respectively. DAS combines them and further improves their performance to \(42.52\%\) mAP and \(0.67\) PCC, demonstrating the synergy effect among them.

**Hyperparameter Sensitivity.** We investigate the sensitivity of the hyperparameter \(\) controlling the weight of PDR for DAS on real-to-art adaptation (P2C). The results are summarized in Table 5. \(=1.0\) achieves the best average results. From a wide range of \(\), DAS has relatively stable results.

## 5 Conclusion

In this work, we propose a novel metric named Detection Adaptation Score (DAS) by seeking flat minima to evaluate the domain adaptive object detection models without involving any target labels. The proposed DAS consists of a Flatness Index Score (FIS) and a Prototypical Distance Ratio (PDR) score and can find flat minima of DAOD models in an unsupervised way. Extensive experiments have been conducted on public DAOD benchmarks for several classical DAOD methods. Experimental results indicate that the proposed DAS correlates well with the performance of the detection model and thus can be used for checkpoint selection after training. The proposed method fosters the application of DAOD methods in the real-world scenario. We hope that our work will inspire researchers and contribute to advancing research in DAOD.

    & PDR & FIS & mAP & PCC \\  Last Checkpoint & & & 39.27 & - \\   & & ✓ & 41.74 & 0.48 \\  & ✓ & & 42.14 & 0.64 \\  & ✓ & ✓ & 42.52 & 0.67 \\        & & \(\) & mAP & PCC \\  Last Checkpoint & - & 33.02 & - \\   & 0.1 & 34.76 & 0.748 \\  & 0.5 & 35.08 & 0.842 \\  & 1.0 & **35.09** & **0.854** \\  & 2.0 & 35.05 & 0.821 \\  & 10.0 & 35.05 & 0.729 \\    
    & & \(\) & mAP & PCC \\  Last Checkpoint & - & 33.02 & - \\   & 0.1 & 34.76 & 0.748 \\  & 0.5 & 35.08 & 0.842 \\  & 1.0 & **35.09** & **0.854** \\  & 2.0 & 35.05 & 0.821 \\  & 10.0 & 35.05 & 0.729 \\   

Table 4: Ablation study of the proposed DAS Table 5: The hyperparameter sensitivity of the method. The results are averaged from DAOD proposed method on real-to-art adaptation.