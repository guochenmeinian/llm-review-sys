# Parts of Speech-Grounded Subspaces in

Vision-Language Models

 James Oldfield\({}^{1}\) Christos Tzelepis\({}^{1}\) Yannis Panagakis\({}^{2,3}\)

Mihalis A. Nicolaou\({}^{4}\) Ioannis Patras\({}^{1}\)

\({}^{1}\)Queen Mary University of London \({}^{2}\)National and Kapodistrian University of Athens

\({}^{3}\)Archimedes/Athena RC \({}^{4}\)The Cyprus Institute

Corresponding author: j.a.oldfield@qmul.ac.uk

###### Abstract

Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased towards specific visual properties (such as _objects_ or _actions_) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP's joint vision-language space by leveraging the association between _parts of speech_ and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What's more, we show the proposed model additionally facilitates learning subspaces corresponding to _specific_ visual appearances (e.g. artists' painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists' styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification.

## 1 Introduction

Many recent advances in machine learning have been driven by vision-language (VL) models' ability to learn powerful, generalisable image representations from natural language supervision . The image features from VL models well-capture representations of many visual attributes as evidenced by the broad applicability they have found for use in downstream tasks. The image or text encoders of CLIP in particular  have been used for controllable image synthesis , image captioning , and multiple other discriminative tasks . However, modelling the many different visual modalities in a single vector representation is not without its drawbacks-recent work shows that CLIP's visual representations are often _entangled_. For example, Goh et al.  find that specific neurons fire in response to both images containing a visual concept and _images of text_ relating to the same concept. This leaves CLIP open to vulnerabilities in the form of 'Typographic attacks'-writing another class name as text on the image can often cause CLIP to predict this class with a higher probability than that of the original image's true category. Other recent works show that CLIP's visual representations encode task-specific attributes (such as of the object or action depicted in an image) inan unpredictable manner, and often the embedding is biased in the prominence with which it encodes different modalities . We show in Figure 0(a) a motivating example of this problem identified by Menon et al. -the 'goldfish' noun embedding dominates the image's representation despite there being multiple additional labels which accurately describe important information about the image's contents. As a visual example, we find that CLIP encodings of text prompts containing 'visually polysemous'  phrases of artists' names lead CLIP-based text-to-image models  to synthesize an unpredictable combination of _both_ images of the artists and of artworks in their signature styles (as shown in Figure 0(b)). Multiple visual associations of the text prompt, including both the appearance of the artist themselves and the style of their artwork, are entangled in the same CLIP embedding. For VL representations to make for useful image features, it's vital that the particular modalities of interest are indeed well-represented in the embedding. One popular means to this end is fine-tuning the representations for specific downstream tasks . However, this not only requires additional computation but makes the restrictive assumption of the existence of labelled data for each task.

In this paper, we address the problem of better disentangling the modes of visual variation in CLIP's shared vision-language space. In particular, we ask the question: do there exist subspaces in CLIP's joint VL space that capture the representations of the 'content' of an image or text, that are invariant to its 'appearance'? To take the first steps towards achieving this we leverage the association between _parts of speech_ in natural language and specific modes of visual variation: our learnt noun subspace isolates representations of the 'object' of an image or text prompt (e.g. an animal in an image, or the noun described in a sentence), and the adjective space its appearance (e.g. whether an object is shiny, or a scene is snowy). This image-text semantic similarity is installed in the representations through CLIP's contrastive learning training objective, through which it is encouraged to learn image encodings that have large cosine similarity in the shared vision-language space with text encodings of captions describing its visual content. We show how by using example words in the parts of speech categories in the WordNet  database, one can extract representations of _both_ image and text embeddings that better isolate the individual modes of visual variation. This is in contrast to the method of VL model 'prompting' [17; 18; 13], which often steers the embeddings of either only the text or image modality. We take inspiration from the related line of work that finds so-called 'interpretable directions' in latent space [19; 20; 21; 22; 23] that capture high-level semantic attributes-however, we learn subspaces that capture variation _uniquely_ present amongst representations of words of particular parts of speech. We achieve this by formulating an appropriate objective function which we show can be manipulated into a well-known trace maximisation problem with a fast solution given in closed form. Since the CLIP representations live on the hypersphere , we further propose a manifold generalisation of the subspaces [25; 26] (illustrated in Figure 2) that share the property of capturing the variance of only the desired visual attributes, yet better respect the manifold on which the data lie. Concretely, we compute the proposed component analysis in the tangent space to the sphere's intrinsic mean, which can be seen as a local approximation of the manifold .

Figure 1: CLIP represents multiple visual modes of variation in an embedding (e.g. the ‘object’ and its ‘appearance’). The learnt PoS subspaces more reliably separate the constituent visual components.

The method's ability to disentangle visual modes of variation is measured both qualitatively and quantitatively. Using a popular CLIP-based text-to-image model we demonstrate _visually_ how the learnt PoS subspaces can better separate the content from the style associated with a text prompt. We show, for example, how simply projecting onto the orthogonal complements of the noun and adjective subspaces respectively can more reliably produce images of either the artists' work, or of the artists themselves, as shown in rows 2 and 3 of Figure 0(b). We find removing a CLIP representations' adjective space component to be remarkably effective for preventing the visual imitation of artists' styles in this new class of CLIP-based text-to-image models , addressing societal concerns of the technology. Further, we show our objective additionally facilitates learning subspaces corresponding to more specific visual appearances (e.g. 'gory'). Projections onto the orthogonal complements of such subspaces consequently remove entire visual themes from text-to-image models, whilst preserving existing concepts through the PoS guidance.

Finally, we validate the subspaces' ability to isolate visual modes of variation quantitatively through a measure of class invariance (comparing to two existing baselines), and by showing how the baseline zero-shot classification protocol _in the learnt subspaces_ leads to higher accuracies on the ViT-B-32 CLIP model on \(14/15\) datasets considered. Our contributions can be summarised as follows:

* We present a method for learning geometry-aware subspaces in CLIP's shared vision-language space that disentangle representations of the content in an image or text prompt from the way it looks, using parts of speech as supervision. To the best of our knowledge, we are the first to use the semantic relationship between parts of speech and specific visual modes of variation to disentangle CLIP's shared vision-language space.
* We formulate and solve an appropriate trace maximisation problem that admits a fast closed-form solution respecting the manifold on which the data lie.
* We validate our method's success at disentangling the visual modes of variation both quantitatively and qualitatively: by visually separating a text prompt's 'content' from its 'appearance' with text-to-image models for the former and through improved zero-shot classification in the submanifolds for the latter.
* We show the model's ability to further learn subspaces of more specific appearance-based variation (e.g. artists' styles), providing a way of erasing entire visual themes from CLIP-based text-to-image models.

## 2 Method

We first recall the basics of CLIP. We then motivate and introduce our objective function for learning parts of speech-specific linear subspaces in Section 2.1, and detail its closed-form solution in Section 2.1.1. Finally, in Section 2.2 we show how to learn _subspaces of the tangent space_ to the CLIP VL hypersphere's intrinsic mean, which better respects the geometry of the manifold on which the CLIP representations lie.

CLIP preliminariesPre-trained image and text encoders map images and text respectively to latent representations \(_{I},_{T}^{d}\) in a shared 'vision-language' (VL) space . For test-time zero-shot classification, candidate text prompts are first encoded with the text encoder. Then, the cosine similarity between an encoded image representation of interest and the text candidates is computed with \(S(_{T},_{I})=_{T}}}^{}_{I}\ /\ (\| _{T}\|\|_{I}\|)\), after which the softmax function is applied to determine the most likely text label for the image. We now address the task of extracting representations of solely the target modes of variation in the section that follows.

### Objective

We seek a lower-dimensional subspace on which to project either a text or image CLIP representation \(^{d}\) to predictably isolate the desired visual mode of variation. We achieve this through natural language supervision in the form of words from the different parts of speech. Let the elements of a set \(\) index into the relevant word class of interest (i.e. \(=\{N,A,V,R\}\) for nouns, adjectives, verbs, and adverbs respectively), and \(_{i}^{d n}, i\) contain in their columns the CLIP encodings of \(n\) words belonging to word class \(i\). Then, for a word class \(i\) of interest, we seek a \(k\)-dimensional subspace of \(^{d}\) spanned by the columns of a learnt \(_{i}^{d k}\) in which the CLIPrepresentations of the words in the class of interest \(i\) have a large norm and the remaining categories' representations in \(\{i\}\) are close to the zero vector. Intuitively, a hyperplane with this property models factors of variation that are uniquely present in representations of text that belong to a particular part of speech. We quantify this by formulating the following objective function:

\[_{i}=*{arg\,max}_{_{i}^{}_{i}= _{k}}(1-)||_{i}^{}_{i}||_{F }^{2}-\{i\}}{}||_{i}^{ }_{j}||_{F}^{2}},\] (1)

where \(\) is a hyperparameter that controls the importance of killing the variation in the non-target categories relative to preserving the variation in the target class.

#### 2.1.1 Closed-form solution

Imposing column orthonormality on \(_{i}\) not only ensures its columns span a full \(k\)-dimensional subspace, but also allows Equation (1) to be solved in closed form. Concretely, we first manipulate the objective as follows

\[(1-)||_{i}^{}_{i}||_{F}^{2}- \{i\}}{}|| _{i}^{}_{j}||_{F}^{2}\] \[=(1-)((_{i}^{}_{i}) ^{}(_{i}^{}_{i}))- \{i\}}{}((_{i}^{}_{ j})^{}(_{i}^{}_{j}))\] (2) \[=_{i}^{}((1-)_{i} _{i}^{}-\{i\}}{} _{j}_{j}^{})_{i}\] (3) \[=(_{i}^{}_{i}_{i} ),\] (4)

where \(_{i}=((1-)_{i}_{i}^{}- {j\{i\}}{}_{j}_{j}^{ })\). Having now reformulated our original objective in Equation (1) as a (constrained) trace maximisation problem, its solution \(_{i}\) is given in closed form2 as the leading \(k\) eigenvectors of \(_{i}\).

For ease of presentation, we have assumed the number of data points in each class to be equal. One can account for class imbalance straightforwardly whilst retaining a solution in closed form however: multiplying each Frobenius norm term in the original objective of Equation (1) by \(}\) (where \(n_{p}\) is the number of columns of \(_{p}, p\)) leads to \(_{i}=(}_{i}_{i}^{ }-\{i\}}{}}_{j}_{j}^{})\) in Equation (4).

Figure 2: Our proposed method, shown on toy data, for learning ‘geodesic submanifolds’  that capture the variance of visual attributes uniquely associated with specific parts of speech. After mapping to the tangent space (1), a linear subspace of the tangent space is learnt that captures the variation in only the target class \(i\) (leading eigenvectors of \(}_{i}\)). Test samples can then be projected onto this subspace (2), and mapped back (3) to the VL sphere.

Special casesTo contrast the proposed objective with related decompositions, it's instructive to consider the resulting subspaces for extreme values of \(\) (for zero-mean data). In the limit case of \(:=0\) for example, \(_{i}\) is given by the top principal components of the data points for one particular class \(i\). Conversely, a value of \(:=1\) gives the _bottom_ principal components of datapoints in all other classes \(j\{i\}\). Thus, \(\) can be seen as providing a trade-off between hyperplanes well-capturing the variance of the attributes in the target class and lying near-orthogonal to the points of the remaining classes. Finally, one recovers regular PCA when \(_{i}:=_{j}}_{j}_ {j}^{}\).

### From subspaces to submanifolds

Through CLIP's choice of the cosine similarity as an objective function during training, the unit-norm VL representations live on the _hypersphere_\(}_{I},}_{T}^{d-1}^{d}\). However, subspace learning in the ambient Euclidean space \(^{d}\) does not respect the underlying geometry of the manifold on which the data lie. An orthogonal projection of a vector onto the learnt Euclidean subspaces is not guaranteed to result in a vector that remains on the sphere, even if all the input data do. Motivated by this, we extend the component analysis for linear subspaces developed in Equation (1) to _geodesic submanifolds_. As demonstrated in Fletcher et al.  for PCA, an analogous approximate projection onto the geodesic submanifolds capturing the desired variances can be made by applying the exact same component analysis of Equation (1) in the tangent space \(_{}}^{d-1}^{d}\) to the VL sphere data's intrinsic mean \(}^{d-1}\) instead. To this end, we use the so-called _Logarithmic Map_\(_{}}:^{d-1}_{}} ^{d-1}\), which maps points on the sphere to the tangent space at a reference point \(}^{d-1}\) and its inverse, the _Exponential Map_\(_{}}:_{}}^{d-1} ^{d-1}\) to map points back onto the hypersphere (whose well-known definitions are provided in the supplementary material).

We can then compute the subspace _of the tangent space to the intrinsic mean_ spanned by the columns of a \(}_{i}^{d k}\) for word class \(i\) as the leading \(k\) eigenvectors of \(}_{i}=_{n}(1-)_{}}(_{in})_{}}(_{in})^{}-_{j \{i\}}_{}}(_{jn})_{}}(_{jn})^{}\). We have again assumed an equal number of class data points purely for ease of presentation. This whole process is visualised in Figure 2.

By projecting onto the learnt subspaces of \(_{}}^{d-1}\), one can better _isolate_ or _remove_ the visual attributes associated with part of speech \(i\). To isolate or kill the attributes we compute the projection onto the column space (i) or orthogonal complements (ii) respectively with

(5)

Projection onto the Euclidean subspaces can be computed using the projection matrices \(_{i}=_{i}_{i}^{}\) and \(_{i}^{}=_{d}-_{i}\). The way in which one can extract representations relating to the visual modes of variation for the parts of speech is explored in detail in Section 3.

## 3 Experiments

Here we present both qualitative (Section 3.1) and quantitative (Section 3.2) experiments to validate the model's ability to disentangle the object in a CLIP text or image representation from its appearance. We henceforth use'subspace' to refer throughout to the manifold generalisation from Section 2.2.

Implementation detailsFor all experiments, we use the following 4 parts of speech: _nouns_, _adjectives_, _verbs_, and _adverbs_. Our labelled data points (with which we compute the closed-form solution of Equation (4)) for these parts of speech are given by the WordNet  database. There are a total of \(112219\), \(18021\), \(7295\), and \(3910\) text-string data points from each of the categories respectively, after filtering out any word that appears in two or more parts of speech. We set \(:=1/2\) for all experiments. For all quantitative results, we use the base CLIP ViT-B-32 model. Please see the supplementary material for ablation studies on \(\) and experiments on additional CLIP architectures.

### Qualitative results

#### 3.1.1 Visual disentanglement

Recent CLIP-based text-to-image models (TTIMs) learn a mapping from the CLIP embeddings to synthetic images depicting visually a text prompt [15; 29; 30]-a process that Menon et al. show is prone to also inheriting task bias. We begin in this section by following the experimental protocol of Materzynska et al. , using a recent popular CLIP-based TTIM  from LAION to demonstrate _visually_ how our PoS subspaces can succeed in predictably isolating visual variation in the CLIP representations pertaining to either the object described in a text prompt or its appearance.

As one motivating example, we first study a curious instance of 'visually polysemous'  natural language phrases-terms that have multiple visual associations. In particular, we find that when prompted with artists' names, TTIMs produce _both_ artworks in their style and images of the person themselves (e.g. the top row of Figure 3). That is to say, CLIP entangles in its representation these two dual meanings of the artists-their works' style and their physical appearance. We find that the PoS subspaces offer an intuitive way of reliably separating factors of visual variation in the representations for not just visually polysemous phrases, but also for natural language prompts more generally.

Concretely, projecting onto the orthogonal complement of the _noun_ subspace \(_{N}^{}(_{T})\) successfully removes from the embedding visual information about the object/content described implicitly in a text prompt \(T\). For example, as visualised in row 2 of Figure 3 by reliably synthesising just artwork in an artist's style, or by leaving just the snowy or multicoloured appearance-based textures. On the other hand, projecting onto the adjective subspace's orthogonal complement \(_{A}^{}(_{T})\) removes the visual appearances and styles associated with a text description (row 3 of Figure 3). For the visually polysemous artists' names, this isolates the representations of the artists themselves as humans-removing representations of their artwork styles. For the more complex prompts, this produces images of just the basic object described in the text prompt instead, such as the penguin or New York City. Many more examples of this visual disentanglement, details on the experimental setup, and ablation studies can be found in the supplementary material.

#### 3.1.2 Style-blocking subspace projections

One societal concern with free-form TTIMs is their ability to produce imitation artworks copying the style of artists. Here, we show how the learnt adjective subspace can be used as is as a step towards mitigating this. To achieve this, one simply modifies the TTIM forward pass to first project the CLIP text representations onto the orthogonal complement of the adjective subspace with \(_{A}^{}(_{T})\) before feeding it into the image generator. We see from the results in Figure 4 that this modification indeed prevents the imitation of the visual styles of a range of artists (even with multiple forms of sentence structure in the prompt), whilst still enabling a diverse set of images to be generated nonetheless.

Visual theme subspacesWhilst successfully preventing the visual imitation of many famous artists, applying the adjective subspace projection to _every_ text prompt's CLIP representation can restrict the ability to use adjectives to specify visual appearance. We find a further effective strategy is to build additional subspaces for more specific visual appearances (e.g. artists' painting styles). Concretely,

Figure 3: The synthetic images from the original CLIP representations (top row) and when first projecting them onto the orthogonal complements of the noun and adjective subspaces to remove the ‘content’ and ‘appearance’ component, respectively.

we embed \(830\) artists' names and surnames in a new matrix \(_{i^{}}\) and solve Equation (1) using all PoS classes in the negative summation to prevent the destruction of existing concepts. In contrast to the adjective space, projection onto the orthogonal complement of this 'artist subspace' _preserves_ adjective-based visual descriptions whilst also successfully preventing style imitation (Figure 4(a)). Crucially, we highlight that for the example shown in Figure 4(a), **the artist's name Qi Baishi is not present in the 'training' list of example artists**, suggesting the subspace has learnt a more general notion of an artist rather than simply the variation for only those artists whose names are provided as supervision.

We suggest more generally that subspaces learnt from a collection of words describing specific themes (whilst retaining variation in the PoS data through the main objective) could be useful for erasing entire visual concepts (such as NSFW imagery) from the CLIP representations. An additional example of another such custom subspace is shown in Figure 4(b) for removing only gory/bloody visual appearances. Not only does this offer a way to erase specific appearances in CLIP-based text-to-image models, but might also offer application in discriminative tasks, such as being able to block CLIP-based retrieval of images of sensitive nature. Please see the supplementary material for additional details, results, and visualisation of failure cases.

### Quantitative results

Here we show quantitatively how the PoS-grounded subspaces can isolate the variation in the CLIP representation of either an image _or_ text prompt. We validate this in two ways: through a class invariance metric in Section 3.2.1 and through zero-shot classification in Section 3.2.2.

Figure 4: Killing the CLIP representations’ component in the adjective subspace provides a way to block the imitation of artists’ styles in CLIP-based text-to-image models.

[MISSING_PAGE_FAIL:8]

classification3, given this is a very common application of CLIP in practice . Importantly, these experiments serve as an additional way of measuring the subspaces' ability to isolate task-relevant modes of variation in the CLIP representations. With this in mind, we consider the baseline zero-shot setting  which computes the cosine similarity between all images in the training sets' and names of the class labels' CLIP representations. In our case, rather than computing the cosine similarity of the CLIP representations with \(S(_{I},_{T})\) we measure instead the similarity in the _noun_ subspace with \(S_{N}(_{I}),_{T}\). We hypothesise that if the noun subspace indeed better isolates the object of an image-invariant to the particular style of the image-we should expect to see improvement to the baseline image classification.

We show the Top-1 accuracies for a large variety of datasets in Table 1. We find the noun submanifold projection to lead to improved zero-shot classification on \(14/15\) of the datasets considered with CLIP ViT-B-32, and include a comparison to the Euclidean subspaces in Figure 7 to further illustrate the benefit of the geometry-aware variant. This is without needing any prompt engineering or domain knowledge about each dataset separately, confirming that the subspaces serve the intended role of being able to isolate the visual modalities of interest automatically. For all results in this subsection, we project onto a relatively large \(k:=500\) dimensional subspace for PCA, PGA, and the proposed method. Please see the supplementary material for results on 2 more alternative CLIP architectures.

## 4 Related work

Vision-language representationsThere has been much interest in studying the properties of the VL representations in large-scale models such as CLIP. In particular, Goh et al.  show that there exist neurons that fire in response to both visual representations of a concept as well as to text relating to the concept. Viewing this as a form of _entanglement_, Materzynska et al.  address this by learning a linear subspace in which written text is disentangled from its visual component. Whilst we also wish to disentangle visual concepts, we do not focus on disentangling written words from visual representations, but rather visual modalities more generally. Further, our solution is instead given in closed form and has far fewer hyperparameters to tune. In addition to disentangling written/visual concepts, a number of works explore other forms of disentanglement in multimodal models. For example, to better perform image captioning [35; 36], generation [37; 38; 39; 40; 41], or discovering compositional representations or structure [37; 38; 39; 42].

Another popular approach to learning more useful VL representations is fine-tuning; Ilharco et al.  use fine-tuning in combination with weight interpolation to improve results on specific tasks without affecting existing performance, whilst many other works fine-tune CLIP for specific domains such as action recognition , video recognition , image captioning , and more . Despite the success of fine-tuning, this comes with the restrictive requirement of task-specific labels. The method of 'prompting' [44; 17; 18; 13] is one alternative approach. At a high level, this technique learns a set of parameters (either in the form of 'words' in a prompt [17; 44] or as pixels appended to an input image [18; 13]) to steer the input to produce more useful representations for specific downstream tasks. In contrast, our method disentangles the modes of variation directly in the joint

Figure 7: Ablation study on the zero-shot accuracy for the subspaces vs the submanifolds with CLIP ViT-B-32; the submanifold outperforms the subspaces on almost all datasets considered.

vision-language representation space-this facilitates the ability to separate semantic information in _both_ image and text representations. Consequently, the proposed method has direct application for both discriminative tasks and CLIP text-driven generative tasks.

Component analysisLet the matrices \(_{1},_{2}^{d n}\) contain \(n\) data points of two classes in their columns. The _Principal Component Analysis_ (PCA)  computes the low-dimensional subspace of maximum variance as the leading eigenvectors of the empirical covariance matrix of all data points. One pertinent drawback of PCA is its unsupervised nature in disregarding the labels of the data points. Whilst one can learn class-specific subspaces via PCA on the relevant subset of data (i.e. the eigenvectors of zero-mean \(_{1}_{1}^{}\)), there is nothing to prevent the remaining 'nuisance' class(es) from also having large variance in this learnt subspace. One method that provides a way to jointly maximise the variance in one class' projected embeddings whilst minimizing that of another class is the _Fukunaga-Koontz transform_ (FKT). FKT learns a class \(1\)-specific subspace spanned by the columns of \(_{1}^{d k}\) as the leading eigenvectors of \((^{-})^{}_{1}_{1}^{}(^{-})\), where \(,\) are the eigenvectors (stacked column-wise) and eigenvalues (along the diagonal) respectively of the matrix \((_{1}_{1}^{}+_{2}_{2}^{})\). Whilst FKT also learns a subspace in which solely the target class' points have a large coefficient, the FKT operates on just two classes of interest and is more computationally expensive than the proposed method in requiring two eigendecompositions (rather than one). Another related supervised decomposition is the _Fisher Discriminant Analysis_ (FDA)  which learns a subspace in which classes can be easily discriminated. Despite FDA's objective involving a similar trace maximisation form to Equation (4), the goals of FDA and the proposed objective are very different. FDA aims to _minimise_ intra-class variation, whilst the proposed objective _maximises_ the norm of vectors of the target class in the learnt subspaces, whilst minimising those of the remaining classes. Additionally, the proposed objective does not suffer from the same restrictively small upper bound on the dimensionality of the learnt subspace as FDA does through the low-rank of the between-scatter matrix. A comparison to the subspaces learnt with FDA, FKT, and the proposed objective is shown in the supplementary material to illustrate the differences visually.

## 5 Conclusion

In this paper, we have proposed a method for learning geometry-aware subspaces in CLIP's vision-language space that disentangle the modes of visual variation in representations of both images and text. To achieve this, we used the semantic relationship between parts of speech in natural language and specific visual modes of variation. We demonstrated the disentanglement qualitatively with a text-to-image model, showcasing the model's ability to remove visual imitation of artists' styles from synthetic images. Class invariance metrics and zero-shot classification results further validated the disentanglement quantitatively.

LimitationsA common drawback of subspace learning approaches is choosing a good number of dimensions \(k\). Our method inherits this limitation, and one must choose the appropriate value for the specific task. Despite this, the closed-form eigensolution means only a single fast computation is needed, and any desired number of eigenvectors can be used at test-time. Although the recovered subspaces show wide applicability in downstream tasks, they are not able to perfectly separate the modes of variation for every possible image and text prompt. Additionally, the PoS supervision can not help address polysemy _within_ a particular part of speech (such as disambiguating between the bird/machine meanings of the word 'crane'). This further alludes to the promise of the custom subspaces in being able to target more specific visual concepts. Finally, often even minor changes to text-to-image models' text input or its representation produces very different output images . As can be seen from our results, the subspace projection of the entire text representation similarly does not guarantee the preservation of the structure of the images generated from the 'original' prompts.

## 6 Acknowledgements

This research was partially supported by the EU's Horizon 2020 programme H2020-951911 AI4Media project, a grant from The Cyprus Institute on Cyclone, and by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.