# Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models

Anant Raj

Coordinated Science Laboraotry

University of Illinois Urbana-Champaign.

Inria, Ecole Normale Superieure

PSL Research University, Paris, France.

anant.raj@inria.fr

&Umut Simsekli

Inria, CNRS, Ecole Normale Superieure

PSL Research University, Paris, France.

umut.simsekli@inria.fr

&Alessandro Rudi

Inria, Ecole Normale Superieure

PSL Research University, Paris, France.

alessandro.rudi@inria.fr

###### Abstract

This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities  (the positive semi-definite - PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision \(\) with a cost that is \(m^{2}d(1/)\) where \(m\) is the dimension of the model, \(d\) the dimension of the space. The proposed approach consists of: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error \(\), and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. \(\)-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance \(\) from the solution of the equation, with a model of dimension \(m=^{-(d+1)/(-2s)}((1/))^{d+1}\) where \(1/2 s 1\) is the fractional power to the Laplacian, and total computational complexity of \(O(m^{3.5}(1/))\) and then (b) for Fokker-Planck equation, it is able to produce i.i.d. samples with error \(\) in Wasserstein-1 distance, with a cost that is \(O(d^{-2(d+1)/-2}(1/)^{2d+3})\) per sample. This means that, if the probability associated with the SDE is somewhat regular, i.e. \( 4d+2\), then the algorithm requires \(O(^{-0.88}(1/)^{4.5d})\) in the preparatory phase, and \(O(^{-1/2}(1/)^{2d+2})\) for each sample. Our results suggest that as the true solution gets smoother, we can circumvent the curse of dimensionality without requiring any sort of convexity.

## 1 Introduction

High dimensional stochastic differential equations (SDE) and their associated partial differential equations (PDEs) arise often in various scientific and engineering applications such as control systems, aviation, fluid dynamics, etc . Sampling from an SDE is an active area of research. Generally, a direct approach like Langevin sampling  is commonly employed to sample from SDEs. However, an alternative approach involves approximating the solution of the associated PDE in a form that enables easy sampling. In this paper, we focus on the latter approach.

The exact solutions of most of the high-dimensional PDEs are not computable in closed form. Hence, for this reason, finding approximate numerical solutions to high-dimensional SDEs and related PDEs has been an important area of research in numerical methods . With the advent of new machine learning methods, the question naturally arises as to whether we can leverage the expressive power of these models to effectively capture the solution of high-dimensional partial differential equations (PDEs). Recent research efforts have explored this direction by employing various machine learning models for different classes of PDEs . For instance, works such as  have harnessed the capabilities of reproducing kernel Hilbert spaces (RKHS) to model PDE solutions. With the recent developments in neural network models and architecture,  consider neural networks to model and approximate the solution of high dimensional PDEs. Despite impressive algorithmic innovation, the theoretical understanding of these approaches is still limited, leaving a significant class of PDEs beyond the reach of existing machine learning-based modeling frameworks or with inadequate theoretical insights into the obtained solutions.

In this paper, our goal is to sample from the stochastic differential equations (SDE) that are driven by a Brownian motion or an \(\)-stable process. The time evolution of probability density functions (PDFs) of stochastic differential equations (SDE) that are driven by a Brownian motion or an \(\)-stable process is given by Fokker-Planck Equation (FPE) and the fractional Fokker-Planck Equation (fractional FPE) respectively (see e.g, . Therefore, our focus in this study lies in approximating the solution of the Fokker-Planck Equation (FPE), along with its fractional counterpart, as discussed in , and subsequently sampling from the approximated solution. The modeling of solutions for FPEs (FPEs) and fractional FPEs poses significant challenges, encompassing the following key aspects: (i) Solutions of FPEs and fractional FPEs are probability densities that are always non-negative and vanish at infinity, (ii) Modeling a probability density is hard because of the normalization property, and (iii) Fractional FPEs involve a fractional Laplacian operator that is non-local in nature. In order for an approximate solution of FPEs or fractional FPEs to be useful for sampling from the corresponding stochastic differential equations (SDEs), it is necessary that sampling from the approximate solution of the corresponding partial differential equation (PDE) be straightforward. Unfortunately, a comprehensive algorithmic approach that addresses the above-mentioned issues and includes a formal approximation analysis is currently lacking in the literature.

This paper adopts a positive semi-definite (PSD) model-based approach to effectively tackle the problem at hand. The recently proposed PSD model  offers promising solutions to the challenges encountered in probability density modeling, making it an excellent choice for representing the solution of FPEs and their fractional counterparts. In a recent work by , it was demonstrated that employing a Gaussian kernel in the PSD model allows for the exact determination of the normalization constant and enables the characterization of a broad range of probability densities using such PSD models. Moreover, an algorithm for sampling from the PSD model is also proposed recently with statistical guarantees .

Driven by these insights, we explore the utilization of a PSD model for approximating the solution of FPEs and fractional FPEs. Consequently, we employ the sampling algorithm presented in to generate samples from the approximate solution. Our primary objective in this study is to obtain rigorous guarantees on the approximation error of PSD models when applied to the approximation of solutions of FPEs and fractional FPEs. Under regularity conditions on the solution of these PDEs, we make the following contributions.

Contributions:We make the following contributions in this paper:

* We show that the PSD-based representation of the solution of FPE has good approximation properties under the regularity assumptions on the true solution of the PDE. In particular, we show that we can achieve an approximation error of \(O()\) while using \(m=O(^{-(d+1)/(-2)}()^{(d+1)/2})\) number of samples for PSD representation where \(d\) is the dimension of space variables and the true solution is \(\) times differentiable (**Theorem 1 and 2**). In addition, we are able to provide Wasserstein guarantees for the sampling error of a PSD model-driven solution when sampling from an SDE driven by a brownian motion (**Corollary 1**). An important contribution of our approach is that it relies on much weaker assumptions on the solution of associated PDE (\(\) times differentiable) compared to existing methods that employ dissipativity, log-Sobolev, and Poincare like conditions to provide sampling error guarantees .

* We demonstrate that utilizing a positive semi-definite (PSD) based representation is a natural and effective method to approximate the fractional Laplacian operator when applied to a probability density. In particular, we provide results that illustrate the effectiveness of PSD-based representations for Gaussian and shift-invariant kernels, which can be represented using Bochner's theorem (**Theorem 3**).
* As a final contribution, we also establish an approximation guarantee for the solution of the fractional FPE under certain regularity assumptions on the true solution. This extends our earlier result for the FPE and demonstrates the effectiveness of the PSD-based approach in approximating the solution of both FPE and fractional FPE We achieve an approximation error of \(O()\) while using \(m=O(^{-(d+1)/(-2s)}()^{(d+1)/2})\) number of samples for PSD representation where \(d\) is the dimension of space variables and the true solution is \(\) times differentiable (**Theorem 4 and 5**). This result directly implies a Wasserstein guarantee on the sampling error when sampling from an SDE driven by an \(\)-stable levy process using the PSD model. To the best of our knowledge, our algorithm stands as the first of its kind, providing a provable guarantee for sampling from an SDE driven by an \(\)-stable levy process as well as for approximating the solution of fractional FPEs under regularity assumption on the solution.

Our results clearly suggest that as the true solution gets smoother, we require less number of samples to approximate the true solution using PSD-based representation, hence circumventing the curse of dimensionality. All the proofs are provided in the appendix.

## 2 Related Work

 provides a good introduction on PDEs.  have discussed numerical methods to solve a PDE at length.

**Machine learning-based solution to PDEs:** Modeling the solution of a PDE using machine learning approaches and in particular kernel methods is a well-established technique leading to optimal sample complexity and very efficient algorithms (see for example ). With the recent advancement of neural networks in machine learning, new approaches have emerged. A deep learning-based approach that can handle general high-dimensional parabolic PDEs was introduced in . Two popular approaches that were proposed in this context are (i) _Physics inspired neural networks (PINN)_ and (ii) _Deep Ritz method_. However, no statistical optimality was provided in the original papers. Recently,  studied the statistical limits of deep learning and Fourier basis techniques for solving elliptic PDEs. The sample complexity to obtain a solution via a PINN and Deep Ritz model are shown to be \(O(n^{-} n)\) and \(O(n^{-} n)\) respectively where \(\) is the order of smoothness of the solution. Subsequently, in a series of papers , the theoretical properties of solving a different class of PDEs are discussed under restrictive assumptions on the function class (barron space). For an overview, we refer to . Solutions of PDEs that involve a fractional operator have also been modeled by function in RKHS . However, these works do not provide finite sample approximating error guarantees for the solution obtained. A review of kernel-based numerical methods for PDEs can be found in .

**The (fractional) Fokker-Planck equation:** Various efforts have been made to approximate the solution of FPE and its fractional counterpart by numerical methods .  utilized a radial basis kernel-based approach to numerically compute the solution of an FPE. RKHS-based methods have also been used to obtain an approximate solution . However, the above mentioned works are numerical in nature and do not discuss the approximation limits of the solution. On the more applied side, various works have been proposed by harnessing the expressive power of deep networks to model the solution of FPEs and fractional FPEs . However, a systematic theoretical study is missing from the literature.

**Langevin Monte Carlo:** An important line of research considers the probabilistic counterpart of FPE that is directly simulating the SDEs associated with the FPEs by time-discretization. Under convexity assumptions  proved guarantees for obtaining a _single_ (approximate) sample from the SDE for a given time. These frameworks were later extended in , so that the convexity assumptions were weakened to more general notions such as uniform dissipativity.  introduced a more general SDE framework, which covers a larger class of PDEs. In the case of sampling SDEs with domain constraints proposed Langevin-based methods under convexity assumption, which were later weakened in .  considered SDEs which corresponded to the fractional FPE in dimension one and this approach was later extended in . An overview of these methods is provided in . Contrary to this line of research, which typically requires convexity or dissipativity, our framework solely requires smoothness conditions.

PSD Models:PSD models are an effective way to model non-negative functions and enjoy the nice properties of linear models and more generally RKHS. It was introduced in  and their effectiveness for modeling probability distributions was shown in . It has been also effectively used in optimal transport estimation , in finding global minima of a non-convex function , in optimal control . In , an algorithm to draw samples from a PSD model was proposed.

## 3 Notations and Background

Notation:Space input domain is denoted as \(\) and time input domain is denoted as \(\). Combined space \(\) is denoted as \(}\). \(L^{}(})\), \(L^{1}(})\) and \(L^{2}(})\) denote respectively the space of essentially bounded, absolutely integrable and square-integrable functions with respect to Lebesgue measure over \(}\). \(W_{2}^{}(})\) denotes the Sobolev space of functions whose weak derivatives up to order \(\) are square-integrable on \(}\). We denote by \(^{d}_{++}\) the space vectors in \(^{d}\) with positive entries, \(^{n d}\) the space of \(n d\) matrices, \(^{n}_{+}=_{+}(^{n})\) the space of positive semi-definite \(n n\) matrices. Given a vector \(^{d}\), we denote \(()^{d d}\) the diagonal matrix associated to \(\).

### (Fractional) Fokker-Planck Equation

The FPE describes the time evaluation of particle density if the particles are moving in a vector field and are also influenced by random noise like Brownian motion. Let us consider the following SDE driven by the standard Brownian motion (Wiener process) \(W_{t}\):

\[dX_{t}=(X_{t},t)dt+ dW_{t}\] (1)

where \(X_{t}^{d}\) is a random variable, \((X_{t},t)^{d}\) is drift, \(^{d k}\) and \(W_{t}\) is a \(k\)-dimensional Wiener process. \(_{i}(X_{t},t)\) denotes the \(i\)th element of vector \((X_{t},t)\). The diffusion tensor1 is defined by \(D=^{}\).

The probability density function \(p(x,t)\) of the random variable \(X_{t}\) solving the SDE (1) is given by the FPE, given as follows :

\[=-_{i=1}^{d}}(_{i}(x,t)p(x,t))+_{i=1}^{d}_{j=1}^{d}D_{ij} }{ x_{i} x_{j}}p(x,t),p(0,x)=p_{0}(x).\] (2)

It is known that when \((X_{t},t)\) is the gradient of a potential or objective function \(f:\), i.e., \(= f\), where \(f\) satisfies certain smoothness and growth conditions, the stationary distribution of the stochastic process in equation (1) exists and is described by the so-called Gibbs distribution .

Despite of having nice theoretical properties, Brownian-driven SDEs are not suitable in certain models in engineering, physics and biology . A popular non-Gaussian SDE used in such applications is given as follows:

\[dX_{t}=(X_{t},t)dt+dL_{t}^{},\] (3)

where \(L_{t}^{}\) denotes the (rotationally invariant) \(\)-stable Levy process in \(^{d}\) and is defined as follows for \((0,2]\):

1. \(L_{0}^{}=0\) almost surely;
2. For any \(t_{0}<t_{1}<<t_{N}\), the increments \(L_{t_{n}}^{}-L_{t_{n-1}}^{}\) are independent;
3. The difference \(L_{t}^{}-L_{s}^{}\) and \(L_{t-s}^{}\) have the same distribution, with the characteristic function \([e^{j(u,L_{t-s}^{})}]=(-(t-s)^{}\|u\|_{ 2}^{2})\) for \(t>s\) and \(j=\);
4. \(L_{t}^{}\) is continuous in probability.

In the SDE described in equation (3), we choose \(=1\) for the simplicity of the expressions. When \(=2\), the process recovers the Brownian motion, i.e., \(L_{t}^{}=W_{t}\). The fundamental difference between \(L^{}_{t}\) and \(W_{t}\) when \(<2\) is that the increments of this process are heavy-tailed, with the following property: \([L^{}_{t}]^{p}|=+\) for all \(p\) and \(t 0\). This heavy-tailed structure makes the solution paths of (3) discontinuous: \((X_{t})_{t 0}\) can exhibit at most countable number of discontinuities. SDEs based on \(\)-stable processes have also received some attention in machine learning as well . The resulting governing equation for stable-driven SDEs is similar to the traditional Fokker-Planck equation except that the order \(\) of the highest derivative is fractional when \(<2\). The following generalization of the FPE is known in the literature as fractional FPE because of the presence of fractional Laplacian operator :

\[=-_{i=1}^{d}}(_{i}(x,t)p(x,t))-(-)^{/2}p(x,t),p(0,x)=p_{0}(x),\] (4)

where \((-)^{s}\) is the non-local Laplacian operator which is also known as the fractional Laplacian operator and is induced by the stable process. For \(0<s<1\), the fractional Laplacian of order \(s\), \((-)^{s}\) can be defined on functions \(f:^{d}\) as a singular integral as follows: \((-)^{s}f(x)=c_{d,s}_{^{d}}^{d+ 2s}}\ dy,\) where \(c_{d,s}=(d/2+s)}{ d^{2}|(-s)|}.\) A more useful representation of the fractional Laplacian operator is given as a Fourier multiplier, \([(-)^{s}f](u)=\|u\|^{2s}[f](u),\) where \([f](u)=_{^{d}}f(x)e^{j^{}u}\ dx\). We will utilize the Fourier multiplier-based representation of the fractional Laplacian operator to prove our approximation result for the fractional FPE.

### PSD Models for Probability Representation

Consider a feature map representation \(:\) from input space \(\) to a Hilbert space \(\), and a linear operator \(M_{+}()\), then PSD models are represented as in ,

\[f(x;M,)=(x)^{}M(x).\] (5)

It is clear that \(f(x;M,)\) is a non-negative function and hence PSD models offer a general way to parameterize non-negative functions. We consider \(\) to be the RKHS corresponding to the kernel \(k\) such that \((x)=k(x,)\). In particular, we will consider the case where: \(=_{}:^{d}_{}\) is the feature map associated with the Gaussian kernel \(k_{}(x,y)=_{}(x)^{}_{}(y)=e^{-\|x-y\|^{2}}\) with \(>0\). Let us assume that \(M\) lies in the span of feature maps corresponding to \(n\) data points, \(X=\{x_{1},,x_{n}\}\) then \(M=_{i,j}A_{ij}(x_{i})(x_{j})^{}\) for some \(A_{+}^{n}\). Hence, a Gaussian PSD model can be defined as, \(f(x;A,X,)=_{ij}A_{ij}k_{}(x_{i},x)k_{}(x_{j},x)\). Given two base point matrices \(X^{n d}\) and \(X^{}^{m d}\), then \(K_{X,X^{},}\) denotes the kernel matrix with entries \((K_{X,X^{},})_{ij}=k_{}(x_{i},x_{j}^{})\) where \(x_{i}\) and \(x_{j}^{}\) are the \(i\)-th and \(j\)-th rows of \(X\) and \(X^{}\) respectively. Gaussian PSD model has a lot of interesting properties and the details can be found in . But, we would like to mention below an important aspect of PSD models which enable them to be used as an effective representation for probability density:

**Sum Rule (Marginalization and Integration)**: The integral of a PSD model can be computed as,

\[ f(x;A,X,)\ dx=c_{2}(AK_{X,X,}),\ \ c_{}= k(0,x)\ dx.\]

Similarly, only one variable of a PSD model can also be integrated to result in the sum rule.

**Proposition 1** (Marginalization in one variable ).: _Let \(X^{n d}\), \(Y^{^{}}\), \(A_{+}(^{n})\) and \(,^{}+\), then the following integral is a PSD model,_

\[ f(x,y;A,[X,Y],(,^{}))\ dx=f(y;B,Y,^{})\ \ B=c_{2}A K_{X,X,}.\]

In a recent work,  gave an efficient algorithm to sample from a PSD model, with the following result

**Proposition 2** (Efficient sampling from PSD models ).: _Let \(>0\). Given a PSD model \(f(x;A,)\) with \(A^{m m}\) for \(x^{d}\). There exists an algorithm (presented in ) that samples i.i.d. points from a probability \(_{}\) such that the 1-Wasserstein distance between \(f(;A,)\) and \(_{}\) satisfies \(_{1}(f,_{})\). Moreover, the cost of the algorithm is \(O(m^{2}d(d/))\)._

## 4 Approximation of the (Fractional) Fokker-Planck Equation

In this section, we investigate the approximating properties of PSD models for solving FPEs and fractional FPEs. Our focus is on the approximation guarantee achieved by fitting a single PSD model in the joint space-time domain. Let's denote the solution of the PDE as \(p^{}(x,t)\), where we consider the FPE in section 4.1 and the fractional FPE in section 4.2. We introduce mild assumptions on the domain and the solution probability density. Throughout the paper, we assume the spatial domain \(\) to be \((-R,R)^{d}\) and the time domain \(\) to be \((0,R)\). We have the following assumption from  on the solution \(p^{}\) of the PDE,

**Assumption 1**.: _Let \(>2\), \(q\). There exists \(f_{1},f_{2},,f_{q} W^{}_{2}(}) L^{ }(})\), such that the density \(p^{}:}\) satisfies, \(p^{}(x,t)=_{i=j}^{q}f_{j}^{2}(x,t)\)._

The assumption above is quite general and satisfied by a wide family of probabilities, as discussed in the Proposition 5 of . We reiterate the result here below.

**Proposition 3** (Proposition 5, ).: _The assumption above is satisfied by_

* _any_ \(p^{}(x,t)\) _that is_ \(\)_-times differentiable and strictly positive on_ \([-R,R]^{d}[0,R]\)_,_
* _any exponential model_ \(p(x,t)=e^{-v(x,t)}\) _such that_ \(v W^{}_{2}(}) L^{}(})\)_,_
* _any mixture model of Gaussians or, more generally, of exponential models from (ii),_
* _any_ \(p\) _that is_ \(+2\)_-times differentiable on_ \([-R,R]^{d}[0,R]\) _with a finite set of zeros in_ \(}\) _and with positive definite Hessian in each zero._

_Moreover when \(p^{}\) is \(\)-times differentiable over \([-R,R]^{d}[0,R]\), then it belongs to \(W^{}_{2}(}) L^{}(})\)._

We will need to impose some extra mild assumptions on \(f_{j}\) for \(j\{1,,q\}\) to approximate the solution of a fractional Fokker-Planck equation because of involvement of the fractional laplacian operator (non-local operator). We will cite these extra assumptions when we describe the approximation results in section 4.2. Apart from that, we also have a boundedness assumption for the coefficients of the PDE in the domain we consider.

**Assumption 2**.: _We have for all \((x,t)}\), \(_{i}(x,t) R_{}\), \((x,t)}{ x_{i}} R_{_{p}}\), and \(D_{ij} R_{d}\) for all \((i,j)\{1,,d\}\)._

It is also obvious to see that the PSD-based representation of probability density vanishes at infinity.

### Approximating Solution of Fokker-Planck Equation

Let us consider the Fokker-Planck equation given in equation (2) and \(p^{}(x,t)\) is the solution of Fokker-Planck equation in equation (2) which satisfies the assumption 1. Hence, \(p^{}(x,t)\) satisfies the following,

\[(x,t)}{ t}=-_{i=1}^{d}}(_{i}(x,t)p^{}(x,t))+_{i=1}^{d}_{j=1} ^{d}D_{ij}}{ x_{i} x_{j}}p^{}(x,t).\]

For now, we are ignoring the initial value as we only are interested in approximation error and we will have a uniform approximation of the solution density \(p^{}\) and its derivatives up to the order 2 in the domain.

Given \(m\) base points pair \((_{i},_{i})}\) for \(i\{1,2,,m\}\) of space and time, let us consider the following approximation model for the solution of FPE,

\[(x,t)=_{i,j}A_{ij}k_{X,T}((x,t),(_{i},_{i}))  k_{X,T}((x,t),(_{j},_{j}))\] (6)

where \(k_{X,T}\) is the joint kernel across space and time and \(A\) is a positive semi-definite matrix of dimension \(^{m m}\). \(k_{X,T}\) can be defined as the product of two Gaussian kernels \(k_{X}\) and \(k_{T}\) which are defined on space and time domain respectively i.e \(k_{X,T}((x,t),(x^{},t^{}))=k_{X}(x,x^{}) k_{T}(t,t^{ })\). We represent \(_{X}(x)=k_{X}(x,)_{X}\), \(_{T}(t)=k_{T}(t,)_{T}\) and \(_{X,T}(x,t)=_{X}(x)_{T}(t)\). Here, we assume that \(k_{X}\) and \(k_{T}\) are both Gaussian kernels and hence the RKHS \(_{X}\) and \(_{T}\) correspond to the RKHS of the Gaussian kernel. For simplicity, we keep the kernel bandwidth parameter \(\) the same for \(k_{X}\) and \(k_{T}\). It is clear that the joint kernel \(k_{X,T}\) is also Gaussian with the kernel bandwidth parameter equal to \(\). Given \(M=_{i,j}A_{ij}_{X,T}(x_{i},t_{i})_{X,T}(x_{j},t_{j})^{},p(x,t)\) can also be represented with function \(f((x,t),A,X,)\) as,

\[(x,t)=(x,t)^{}M_{X,T}(x,t)}_{:=f((x,t) ;M,_{X,T})}=A(x,t)}_{:=f((x,t),A,, )},\] (7)where \((x,t)^{n}\), \([(x,t)]_{i}=k_{X}(x,_{i}) k_{T}(t,_{i})\) and \(^{m d+1}\) is the concatenated data matrix of space and time for \(m\) pair of samples \((x_{i},t_{i})\) for \(i\{1,,m\}\).

We can now easily optimize an associate loss function with the FPE to obtain the unknown PSD matrix \(A\). Let us assume that there exists a positive semi-definite matrix \(A\) for which the PSD model \((x,t)\) approximates the true solution \(p^{}(x,t)\) well. In that case, our goal is to obtain an upper bound on the following error objective:

\[\|-)(x,t)}{ t}+_{i=1}^{d} }(_{i}(x,t)(p^{}-)(x,t))-_ {i=1}^{d}_{j=1}^{d}D_{ij}}{ x_{i} x_{j}} (p^{}-)(x,t)\|_{L^{2}()}.\] (8)

Our approximation guarantee works in two steps. In the first step, we find an infinite dimensional positive operator \(M_{}:_{X}_{T}_{X} _{T}\) such that if we denote \((x,t)=_{X,T}(x,t)^{}M_{}_{X,T}(x,t)\) then for some \(M_{}\), \(\) is \(\)-approximation of the true density \(p^{}\). Clearly, for a set of functions \(_{1},,_{q}_{X}_{T}\), \(M_{}=_{j=1}^{q}_{j}_{j}^{}\) is a positive operator from \(_{X}_{T}_{X}_{T}\). Now, we carefully construct \(_{j}\) for \(j\{1,,q\}\) in RKHS \(_{X}_{T}\) such that approximation guarantee in theorem 1 is satisfied. Details of the construction is given in Appendix. We represent \((x,t)\) as a \(d+1\) dimensional vector \(\) and a mollifier function \(g\) (details are in the appendix) such that \(g_{v}(x)=v^{-(d+1)}g(/v)\). Then, we construct our \(_{j}=f_{j} g_{v}\) for \(j\{1,2,,q\}\). We prove the result in Theorem 1 with this construction. To prove our approximation error guarantee for the solution of FPE using PSD model, we assume the derivatives of \(f_{1},,f_{q}\) up to order 2 are bounded which is already covered in Assumption 1.

**Theorem 1**.: _Let \(>2,q\). Let \(f_{1},,f_{q} W_{2}^{}(^{d}) L^{}( ^{d})\) and the function \(p^{}=_{i=1}^{q}f_{i}^{2}\). Let \((0,1]\) and let \(^{d}_{++}\). Let \(_{X,T}\) be the feature map of the Gaussian kernel with bandwidth \(\) and let \(_{X}_{T}\) be the associated RKHS. Then there exists \(_{}_{+}(_{X}_ {T})\) with \((_{}) q\), such that for the representation \((x,t)=_{X,T}^{}_{}_{X,T}\), following holds under assumption 2 on the coefficients of the FPE,_

\[\|(x,t)}{ t}+_{i=1}^{d} }(_{i}(x,t)(x,t))-_{i=1}^{d}_{j =1}^{d}D_{ij}}{ x_{i} x_{j}}(x,t) \|_{L^{2}()}=O(),\] (9) \[(_{})| |^{1/2}(1+^{}(}{_{0}}e^{-})),\]

_where \(||=(())\), and \(^{}\)and\(^{}\) depend only on \(,d,\|f_{i}\|_{W_{2}^{}(^{d})},\|f_{i}\|_{L^{}(^{d})},\|}{ x_{j}}\|_{L^{2}(^{d})}\), and \(\|f_{i}}{ x_{j}x_{k}}\|_{L^{2}(^{d})}\)._

Even though, from theorem 1, we know that there exists a positive operator \(_{}:_{X}_{T}_{X} _{T}\) such that \((x,t)=_{X,T}(x,t)^{}_{}_{X,T}(x,t)\) approximate the solution \(p^{}(x,t)\) of FPE from Theorem 1, it is not clear how to compute \(_{}\) as we do not have access of \(f_{j}\) for \(j\{1,2,,q\}\) beforehand. Hence, to get the final approximation error bound we need to further approximate \((x,t)\) with a PSD model of finite dimension \((x,t)\) which can be computed if we have the access to a finite number of base points \((x_{i},t_{i})\) for \(i\{1,,m\}\) in the domain \(}\). Let us now construct a matrix \(A_{m}\) as follows. Consider \(A_{m}=K_{(X,T)(X,T)}^{-1}M^{}K_{(X,T)(X,T)}^{-1}\), where \(K_{(X,T)(X,T)}\) is a kernel matrix in \(^{m m}\) such that \([K_{(X,T)(X,T)}]_{ij}=_{X,T}(x_{i},t_{i})^{}_{X,T}(x_{j},t_{j})\). Let us define the operator \(:_{X}_{T}^{m}\) such that \(u=((x_{1},t_{1})^{}u,,(x_{m},t_{m})^{}u)\), and \(^{}=_{i=1}^{m}(x_{i},t_{i})_{i}\). We further define a projection operator \(=^{}K_{(X,T)(X,T)}^{-1}\). Hence, we now define the approximation density \((x,t)\) as

\[(x,t)=A_{m}(x,t)}_{:=f((x,t),A_{m}, ,)}=(x,t)^{}M_{X,T }(x,t)}_{:=f((x,t),M,_{X,T})}.\]

In the main result of this section below, we show that \((x,t)\) is a good approximation for \((x,t)\) and hence for the true solution \(p^{}(x,t)\).

**Theorem 2**.: _Let \(p^{}\) satisfy Assumptions 1 and 2. Let \(>0\) and \(>2\). There exists a Gaussian PSD model of dimension \(m\), i.e., \((x,t)=f((x,t),A_{m},X,)\), with \(A_{m}_{+}^{m}\) and \(X^{m d+1}\)_and \(_{m}^{d}_{++}\), such that with \(m=O(^{-(d+1)/(-2)}()^{(d+1)/2})\), we have,_

\[\|(x,t)}{ t}+_{i=1}^{d}}(_{i}(x,t)(x,t))-_{i=1}^{d}_{j=1}^{d}D_{ij} }{ x_{i} x_{j}}(x,t)\|_{L^{2} ()}=O().\] (10)

Theorem 2 indicates that there is a positive semi-definite matrix \(A_{m}^{m m}\) such that \((x,t)=(x,t)^{}A_{m}(x,t)\) is a good approximation of \(p^{}(x,t)\). Next, we would see the usefulness of the above result in Theorem 2 in sampling from the solution of an FPE given the drift term \(\) satisfies the regularity condition as in [1, Theorem 1.1] and \( W^{}_{2}(})\). In this case, the drift function \(\) can be approximated by a function \(\) in a Gaussian RKHS, and the approximation guarantees are due to interpolation result in Lemma 4. The following result is the direct consequence of results in the Theorem 2 and Proposition 2.

Efficient sampling methodLet \(_{}\) be the approximation of \(\) such that \(\|-\|_{L^{2}(})}=O()\), by Gaussian kernel approximation. Let us apply the construction of Thm.2 to \(\).

**Step 1:** Find the best model \(_{m}\) by minimizing the l.h.s. of equation (12) over the set of \(m\)-dimensional Gaussian PSD models. Note that in that case, since everything is a linear combination of Gaussians (or products of Gaussians), we can compute the \(L^{2}\) distance in closed form and minimize it over the positive semidefinite cone of \(m\)-dimensional matrices, obtaining \(_{}\) exactly.

**Step 2:** Denote the resulting PSD model by \(_{}(x,t)=f(x,t;_{},)\). At any time \(t\) of interest we sample from the probability \(_{}(,t)\) via the sampling algorithm in .

**Corollary 1** (Error of sampling).: _Assume that \( W^{}_{2}(})\) satisfies the regularity condition as in [1, Theorem 1.1]. For any time \(t\), the algorithm described above provides i.i.d. samples from a probability \(_{t}\) that satisfies in expectation \(_{t}\ _{1}(p^{}_{t},_{t})^{2}=O( ^{2}),\) where \(p^{}_{t}:=p^{}(,t)\) is the probability at time \(t\) of a stochastic process with drift \(\) and diffusion \(D\). Moreover, the cost of the algorithm is \(O(m^{2}d(d/))\) where \(m=O(^{-d/-2}((1/))^{d})\)._

This result is proven in the appendix after theorem 2. It turns out that the parameter of the PSD model can be learned by a simple semi-definite program  which has a computational complexity of \(O(m^{3.5}(1/))\) where \(m\) is the dimension of the model.

**Remark**.: _It is evident from the aforementioned result that our method excels in sampling from the Stochastic Differential Equation (SDE) while ensuring the particle density adheres to Assumption 1 within a bounded domain. In contrast, traditional sampling methods such as Langevin struggle to sample from this extensive class due to the absence of dissipativity, log-sobolev, and Poincare-like conditions._

### Approximating Solution of Fractional Fokker-Planck Equation

In this section, our aim is to approximate the solution \(p^{}(x,t)\) of a fractional Fokker-Planck equation, similar to the previous section. The equation replaces the Laplacian operator with a fractional Laplacian operator while keeping other terms unchanged. The fractional Laplacian operator, being non-local, is generally challenging to compute analytically. Therefore, approximating the fractional operator on a function has become a separate research direction. Below, we demonstrate that utilizing a PSD model-based representation is a natural choice for approximating the fractional operator acting on probability density. We present results for two cases: (i) a Gaussian kernel with the bandwidth parameter \(\), and (ii) a shift-invariant kernel represented using Bochner's theorem .

**Theorem 3**.: _Consider probability density represented by a PSD model as \(p(x)=f(x;A,X,)=_{i,j=1}^{m}A_{ij}k(x_{i},x)k(x_{j},x)\) for some PSD matrix \(A\), where \(X^{m d}\) is the data matrix whose \(i\)-th row represents sample \(x_{i}^{d}\) for \(i\{1,,m\}\), then_

_1. if the kernel \(k\) is Gaussian with bandwidth parameter \(\), we have_

\[(-)^{s}p(x)=C_{i,j=1}^{n}A_{ij}e^{-(\|x_{i}\|^{2}+\|x_{j}\|^{2}) }_{ N(,)}\|\|^{2s}e^{-j^{}x}]\]

_where \(=2j(x_{i}+x_{j})\), \(=4 I\) and \(C\) can be computed in the closed form._

_2. if the kernel \(k\) have the bochner's theorem representation, i.e. \(k(x-y)=_{^{d}}q()e^{j^{}(x-y)}\ d\) where \(q\) is a probability density, then_

\[(-)^{s}p(x)=_{i,j=1}^{n}A_{ij}\ [\|_{}+_{k}\|^{ 2s}e^{j_{}^{}(x_{i}+x)}e^{j_{k}^{}(x_{j}+x)}],\]

_where expectations are over \(_{},_{k} q()\)._We utilize the Fourier multiplier-based definition of the fractional Laplacian operator in Theorem 3. In both cases discussed in Theorem 3, empirical estimation of \((-)^{s}p(x)\), obtained from finite samples (empirical average), is sufficient for practical purposes. Shifting our focus from the representation of a non-local operator acting on a probability density, we now turn to the approximation result for the solution of a fractional Fokker-Planck equation in this section. We assume that the optimal solution \(p^{}(x,t)\) can be expressed as a sum of squares of \(q\) functions, as stated in assumption 1. To establish the approximation results for the fractional Fokker-Planck equation, we need to impose an additional mild assumption on \(f_{j}\) for \(j 1,,q\), in addition to assumption 1, which we state next.

**Assumption 3**.: _Let \([f]()\) denotes the Fourier transform of a function \(f\), then \([f_{i}]() L^{1}(}) L^{}( {})\), and \([(-)^{s}f_{i}]() L^{1}(})\) for \(i\{1, q\}\)._

Similar to the previous section, it requires two steps to obtain the approximation error bound. By similar construction as in section 4.1, we define an infinite dimensional positive operator \(M_{}:_{X}_{T}_{X} _{T}\) and based on this positive operator, we have a probability density \(\) as \((x,t)=_{X,T}(x,t)^{}M_{}_{X,T}(x,t)\) which is \(O()\) approximation of \(p^{}(x,t)\). The result is stated below in Theorem 4 and proven in Appendix C.3.

**Theorem 4**.: _Let \(>2,q\). Let \(f_{1},,f_{q}\) satisfy assumptions 1 and 3 and the function \(p^{}=_{i=1}^{q}f_{i}^{2}\). Let \((0,1]\) and let \(^{d}_{++}\). Let \(_{X,T}\) be the feature map of the Gaussian kernel with bandwidth \(\) and let \(_{X}_{T}\) be the associated RKHS. Then there exists \(_{}_{+}(_{X}_ {T})\) with \((_{}) q\), such that for the representation \((x,t)=_{X,T}^{}_{}_{X,T}\), following holds under assumption 2 on the coefficients of the fractional FPE,_

\[\|(x,t)}{ t}+_{i=1}^{d} }(_{i}(x,t)(x,t))+(-)^{s} (x,t)\|_{L^{2}(})}=O(),\] (11) \[\ \ (_{}) ||^{1/2}(1+^{}( ^{}}{_{0}}^{-}) ),\]

_where \(||=(())\), and \(\) and \(\) depend only on \(,d\) and properties of \(f_{1},,f_{q}\)._

Major difficulty in proving the result in Theorem 4 arises from the challenge of effectively managing the fractional Laplacian term. To tackle this issue, we utilize the Fourier-based representation to gain control over the error associated with the fractional Laplacian. The details can be found in Lemma7, presented in Appendix C.1. As a next step, we would follow a similar procedure as in section 4.1 to obtain the final approximation bound. In the second part of the proof, we utilize Gagliardo-Nirenberg inequality  for fractional Laplacian to obtain the final approximation guarantee. In the next result, we show that for a particular choice of a PSD matrix \(A_{m}\), \((x,t)=(x,t)^{}A_{m}(x,t)=_{X,T}(x,t)^{}M _{X,T}(x,t)\) is a good approximation for the true solution \(p^{}(x,t)\) of the fractional Fokker-Planck equation.

**Theorem 5**.: _Let \(p^{}\) satisfy Assumptions 1, 2, and 3. Let \(>0\) and \(>2\). There exists a Gaussian PSD model of dimension \(m\), i.e., \((x,t)=f((x,t),A_{m},X,_{X,T})\), with \(A_{m}_{+}^{m}\) and \(X^{m d+1}\) and \(_{m}^{d}_{++}\), such that with \(m=O(^{-(d+1)/(-2s)}()^{(d+1)/2})\), we have_

\[\|(x,t)}{ t}+_{i=1}^{d} }(_{i}(x,t)(x,t))+(-)^{s} {p}(x,t)\|_{L^{2}(})} O().\] (12)

Similar results as in Corollary 1 can be obtained for the case fractional Fokker-Planck equations as well. We can directly utilize results from  to bound the Wasserstein metric between the solution of two fractional FPE whose drift terms are close in \(L^{}\) metric. However, to find a PSD matrix \(A_{m}\) from finite samples does require two separate sampling procedures which would contribute to the estimation error, (i) sampling to estimate the mean in the estimation of the fractional Laplacian and (ii) sampling of a finite number of data points from the data generating distribution.

**Remark**.: _As far as we know, Theorem 5 represents the first proof of approximation error bounds for the solution of the Fractional Fokker-Planck Equation when the approximated solution is a density function. This significant result allows for the utilization of the algorithm presented in  to sample from the approximate solution._Conclusion

In this paper, we study the approximation properties of PSD models in modeling the solutions of FPEs and fractional FPEs. For the FPE, we show that a PSD model of size \(m=(^{-(d+1)/(-2)})\) can approximate the true solution density up to order \(\) in \(L^{2}\) metric where \(\) is the order of smoothness of the true solution. Furthermore, we extend our result to the fractional FPEs to show that the required model size to achieve \(\)-approximation error to the solution density is \(m=(^{-(d+1)/(-2s)})\) where \(s\) is the fractional order of the fractional Laplacian. In the process, we also show that PSD model-based representations for probability densities allow an easy way to approximate the fractional Laplacian operator (non-local operator) acting on probability density. As a future research direction, we would like to investigate and obtain a finite sample bound on the estimation error under regularity conditions on the drift term \(\).