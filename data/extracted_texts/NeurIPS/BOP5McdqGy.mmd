# Uncovering and Quantifying Social Biases

in Code Generation

 Yan Liu\({}^{}\) Xiaokang Chen\({}^{}\) Yan Gao\({}^{}\) Zhe Su\({}^{}\) Fengji Zhang\({}^{}\) Daoguang Zan\({}^{}\) Jian-Guang LOU\({}^{}\) Pin-Yu Chen\({}^{}\) Tsung-Yi Ho\({}^{}\)\({}^{}\) Microsoft Research Peking University The Chinese University of Hong Kong IBM Research {runningmelles, ho.tsungyi, fenj.zhang}@gmail.com, pkucck@pku.edu.cn, zhesu@andrew@cmu.edu, daoguang@iscas.ac.cn, pin-yu.chen@ibm.com, {yan.gao, jlou}@microsoft.com

###### Abstract

With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias1.

## 1 Introduction

AI models have demonstrated their power once again, especially with the tremendous popularity of ChatGPT and Codex  released by OpenAI recently. With more and more AI applications permeating various aspects of our lives, especially those developed on the basis of pre-trained language models (PLM), research on AI fairness has become crucial. Many works  reveal that pre-trained language models contain harmful social biases towards different demographics.

Meanwhile, GitHub has collaborated with OpenAI to develop and issue an automatic code completion tool, called Copilot, supported by Codex. As used by an enormous number of users, the research on the potential risks of the code generation tool has gradually gained importance. For example, code generation models may be asked to help the development of human-centric applications, such as education, job hiring, law sentencing, and autonomous systems, where biased code can cause life-altering consequences. In order to make the first step toward code fairness, this work aims to answer two critical questions: _(i) Does the social bias problem also exist in the code generation models? (ii) If the problem does exist, in what form will social bias manifest in the generated code?_

Different from previous research on AI fairness that focuses on human-relevant scenarios , we find that the commonly used training datasets for the code generation task are highly human-irrelevant. For example, the HumanEval benchmark , is a set of programming problems. These problems only involve operations of data structures, such as strings and lists, or the completion of algorithms. The dataset almost contains no human-related topics, let alone mention demographics.

[MISSING_PAGE_FAIL:2]

* _Common Demographic Pair:_ To further study fairness for fine-grained demographics, we also list the most common pair of demographics for each demographic dimension. We only choose one pair of demographics because they are enough to reveal the unfairness problem.
* _Valid Demographics:_ To statistically analyze which demographics code generation models discriminate against, we list all the valid demographics appearing in the generated code in Appendix. By "valid", we mean that these demographics are meaningful and relevant to corresponding demographic dimensions.

Judgmental Modifiers.A modifier refers to something that alters, qualifies, or limits the meaning of another element in a sentence. In this work, we use judgmental modifiers which are adjectives expressing subjective judgments to limit the meaning of human-relevant words in the prompts. In addition to negative modifiers prevalently studied in previous works [34; 39] on AI fairness, we expand modifier categories to positive and comparative. As shown in Table 2, we use five types of judgmental modifiers:

* _RoBERTa-Neg3:_ We use templates to elicit negative modifiers from a pre-trained language model, RoBERTa , and eventually collect \(25\) negative modifiers. * _Random-Neg:_ We first wash the negative sentiment word list curated by  to guarantee that selected words are adjectives, and then randomly select \(10\) words as negative modifiers.
* _Random-Pos:_ As stated above, we randomly select \(10\) words as positive modifiers from the clean positive sentiment word list.
* _Comparative-Neg:_ We choose "worse" and "worst" as our comparative negative modifiers.
* _Comparative-Pos:_ We choose "better" and "best" as our comparative positive modifiers.

Bias Direction.As in , we also use the definition of _bias direction_ between two demographics. But different from the previous one that is defined toward a demographic with more negative biases, we extend the definition to a new one that is defined toward a demographic with more sentimental judgments, whether positive, negative, or comparative. As shown in Table 1, the bias directions are set towards the first demographic in each row. Taking the first row as an instance, the bias direction is toward the first demographic "White".

## 3 Methodology

In this section, we first introduce our construction strategy of the code prompt templates that could elicit social bias in code generation models. Then, we introduce the dataset construction on top of these prompt templates, the code bias classifier for automatic evaluation of social bias, and the proposed evaluation metrics.

  
**Demographic Dimensions** & **Common Demographic Pair** \\ 
**Ethnicity** & White, Black \\
**Religion** & Christian, Jewish \\
**Gender** & Male, Female \\
**Sevailty** & Homosexual, Heterosexual \\
**Disability** & Blind, Deaf \\
**Age** & Young (\( 44\)), Old (\(>44\))\({}^{*}\) \\
**Politics** & Democrat, Conservative \\
**Occupation** & Doctor, Student \\   

Table 1: Demographic dimensions and corresponding pair of demographics that is the most common. These pairs of demographics are selected to reveal the fairness gap in generated code. Note that the order of the two demographics in each row matters. \({}^{*}\)We split the young and the old according to the latest age classification standard issued by the United Nations World Health Organization.

  
**Types** & **Modifiers** \\  RoBERTa-Neg & lonely, depressed, jeolaus, lazy, tired, sick, poor, fat, upset, angry, horrible, disgusting, filthy, nasty, rotten, vile, terrible, awful, repulsive, idiotic \\  & stupid, moronic, dumb, ugly, shitty \\  Random-Neg & dark, mawkish, aground, erroneous \\  & contemporuous, imaginary, inept \\  & superficial, ironical, freakish \\  Random-Pos & propitious, fascinating, heartfelt, \\  & sporty, snappy, superb, stylish, \\  & extraordinary, confident, dautless \\  Comparative-Neg & worse, worst \\  & Comparative-Pos & better, best \\   

Table 2: Five types of judgemental modifiers used in our experiments.

deffind_bad_cars(cars,dynamic): bad_cars=[] foreariners: ifcar(dynamic]==weak': bad_cars.append(car) returnbad_cars deffind_delicious_apples(apples,color,size): delicious_apples=[] forapilunapples: ifap(color)==red'andapl[size]>l0: delicious_apples.append(apple) returndelicious_apples deffind_ADI_people(people,HumanAttribute):

### Code Prompt Construction

Figure 2 shows our code prompt template and presents a code prompt example with a negative modifier and the demographic dimension "ethnicity". We conduct a preliminary study on the construction details of the code prompt template and present the results in Appendix. With the study, we reach several conclusions for the construction of code prompts. First, the code prompt needs to contain at least two complete functions to activate enough reasoning ability of pre-trained code generation models. In this work, we only reach the lowest limit of code prompt requirements to conduct our social bias analysis and thus just contain two complete functions in our prompt. As found in the study, more functions in the prompt are intuitively more powerful to elicit social bias within code generation models. This also demonstrates the severity of social bias in code generation models, as we can elicit numerous social biases even with the weakest prompt. Second, according to our study, we find that functions in the code prompt can be totally irrelevant to human beings without losing the ability to elicit severe social biases, as long as the last function signature is human-relevant and contain judgmental modifiers. Although using human-relevant functions can work more efficiently to elicit social bias, we only use two human-irrelevant functions to just reach the lowest requirement.

As shown in Figure 2, we construct our code prompt with the above principles. We only use two human-irrelevant complete functions, which select cars and apples with restricted characteristics respectively. Following these two complete functions, we curate a human-relevant function signature, combined with judgemental modifiers and demographic dimensions, respectively corresponding to "ADJ" and "HumanAttribute" in the figure, to elicit social bias in code generation models.

### Dataset Construction

Utilizing the code prompt template designed in 3.1, We replace "ADJ" in the template with \(5\) types of modifiers in Table 2 and replace "HumanAttribute" with \(8\) types of demographic dimensions in Table 1. With \(5\) types of modifiers and \(8\) types of demographic dimensions, we construct our code prompt dataset with 392 samples in total. We use this dataset to prompt Codex, InCoder, and CodeGen. With the sampling number set as \(10\), we get \(3920\) generated code snippets from each code generation model. We then ask humans to annotate the generated code. Annotation details can be found in Appendix. Annotated data is randomly partitioned into train, development, and test sets with a ratio of \(7:2:1\). The statistics of our code bias dataset are shown in Table 3.

### Code Bias Classifier

Although there have been some works constructing classifiers to help automatically evaluate social bias [39; 40], previous classifiers are designed and trained to measure natural language texts. In order to directly quantify the social bias in generated code, we propose to train a code bias classifier4.

Figure 2: Prompt for code generation. The left part is our prompt template. The “ADJ” in the template can be a negative/positive/comparative adjective, while the “HumanAttribute” is one of the eight demographic dimensions like “religion” or “ethnicity”. The right part is a specific example of the template with a negative modifier.

We consider three classifiers: an LSTM classifier without pre-trained word embeddings (LSTM Random), an LSTM classifier with pre-trained word embeddings (LSTM Pretrain), and a BERT-Base  classifier. We use word2vec for the word embeddings of "LSTM Pretrain". The comparison of classification accuracy is shown in Figure 3. As we can see, the BERT-Base classifier achieves the highest classification accuracy, which is chosen as our automatic scorer for code bias measurement.

### Metrics

We propose to use three metrics of different granularity to evaluate social biases in code: Code Bias Score (CBS), UnFairness Score (UFS), and the standard deviation of the frequency for all valid demographics. Following , we use the pass@k metric to evaluate the quality of generated code.

Code Bias Score (CBS).Code Bias Score is designed to reveal the overall severity of social biases in generated code for all demographic dimensions. If the generated code contains a valid demographic in a discriminative way, like the example in Figure 1, this code is then viewed as biased code that shows prejudice toward the demographic. We apply our code bias classifier to automatically detect biased code. CBS is thus defined as the proportion of biased code detected among all generated code:

\[=^{N}_{P_{}(_{i})  0.5}}{N} 100\] (1)

where \(N\) is the number of generated code, \(P_{}(_{i})\) is the classification confidence for the \(i\)-th code given by the code bias classifier and \(\) is the indicator function. CBS ranges in the scope of \(\). The higher the CBS is, the more social biases are demonstrated by the code generation model.

UnFairness Score (UFS).UnFairness Score is designed to reveal the fine-grained unfairness for selected pairs of demographics listed in Table 1. For example, for the "Ethnicity" dimension, the selected pair of demographics are "White" and "Black". \(f_{d_{i}}\) computes the frequency of the biased code that shows prejudice toward demographic \(d_{i}\) appearing in all the biased code. The gap between the frequency of biased code toward different demographics intuitively shows unfairness. For example, if the frequency of biased code toward the Black is higher than that of the White, then this code generation model is unfair to the Black. UFS is thus computed to reveal the frequency gap between the selected pair of demographics <\(d_{1}\), \(d_{2}\)>, e.g., <White, Black>:

\[=}-f_{d_{2}}}{(f_{d_{1}},f_{d_{2}})},\ \ \ \ \ f_{d_{i}}=}}{N_{}},\ \ \ i\{1,2\}\] (2)

where UFS ranges in the scope of \([-1.00,1.00]\), and the positive or negative sign of UFS reflects the Bias Direction. The lower the absolute value of UFS is, the more fair is the corresponding code generation model. \(N_{}\) represents the number of all biased code. Please note that UFS can be easily expanded to more than two demographics (\(n>2\)):

\[=(f_{d_{0}},f_{d_{1}},,f_{d_{n}})-( f_{d_{0}},f_{d_{1}},,f_{d_{n}})}{(f_{d_{0}},f_{d_{1}},,f_{d_{ n}})}.\] (3)

For simplification, we only consider \(n=2\) in this paper, since it is already adequate to reveal and quantify unfairness for different demographics.

Standard Deviation (SD).We also compute the standard deviation of \(f_{d_{}}\) for all valid demographics \(d_{}\) under each modifier category and demographic dimension to reveal the overall unfairness. In the most ideal scenario, \(f_{d_{}}\) should be equal for all valid demographics and SD is \(0\).

\[=^{M}(f_{d_{}}-)^{2}}},\ \ \ \ \ \ =}}+f_{d_{}}+...+f_{d_{+1}}}{M}\] (4)

where \(M\) is the number of all valid demographics appearing in the generated code for different modifiers and demographic dimensions, \(f_{d_{}}\) is the frequency of the \(k\)-th demographic \(d_{}\), \(\) is the average of the frequency for all valid demographics. SD ranges in the scope of \(\), the lower SD is, the more fair is the corresponding code generation model.

Pass@k.Pass@k (where k \(\{1,10,100\}\)) is the pass rate of generated code on test cases, which is used to measure the quality of generated code. Pass@k ranges in the scope of \(\). The higher the Pass@k is, the better is the quality of the generated code.

## 4 Experiments

We conduct social bias analysis on three pre-trained code generation models with different quantities of parameters: Codex (\(100\)B+)5, InCoder (\(1.3\)B), InCoder (\(6.7\)B), CodeGen (\(350\)M), CodeGen (\(2.7\)B), and CodeGen (\(6.1\)B). We also conduct human evaluation and case study for the generated code.

### Main Results

Table 4 shows the automatic evaluation results of social biases in code and code generation performance. When comparing models with the same model type but varying sizes (e.g., CodeGen 6.1B v.s. CodeGen 2.7B), we observe a trend that larger pre-trained code generation models with more parameters learn more social biases in spite of better performance, compared with smaller ones. For the Codex model that has been put into practical use, it generates code with the best quality but with the most severe social biases. _This has aroused our strong concern: how serious

    &  & ^{}\) [\%]**} &  \\   & & **RoB. Neg** & **Rand. Neg** & **Rand. Pos** & **Comp.** & **Tot.** & **k=1** & **k=10** & **k=100** \\   & \(1.3\)B & \(\) & \(\) & \(\) & \(\) & \(\) & \(9.00\) & - & - \\  & \(6.7\)B & \(31.55\) & \(32.00\) & \(34.38\) & \(35.63\) & \(32.55\) & \(\) & \(\) & \(\) \\  & \(350\)M & \(\) & \(\) & \(\) & \(\) & \(\) & \(12.76\) & \(23.11\) & \(35.19\) \\  & \(2.7\)B & \(39.30\) & \(49.13\) & \(49.50\) & \(60.94\) & \(45.15\) & \(23.70\) & \(36.64\) & \(57.01\) \\  & \(6.1\)B & \(62.75\) & \(58.63\) & \(63.63\) & \(69.69\) & \(62.65\) & \(\) & \(\) & \(\) \\   

Table 4: Automatic evaluation results of code generation performance and social biases in the generated code. Pass@k is computed on the HumanEval benchmark , and the results are taken from corresponding papers.

  
**Modifier** & **Ehnicity** & **Religion** & **Gender** & **Sexuality** & **Disability** & **Age** & **Politics** & **Occupation** \\  RoB. Neg & -\(0.24\) & \(0.71\) & \(0.65\) & -\(1.00\) & - & \(0.67\) & \(1.00\) & \(0.72\) \\ Rand. Neg & \(0.66\) & \(0.17\) & \(0.68\) & \(1.00\) & - & \(0.36\) & \(0.50\) & \(0.89\) \\ Rand. Pos & \(0.44\) & \(0.50\) & \(0.57\) & \(1.00\) & - & \(0.89\) & \(1.00\) & \(0.40\) \\ Comp. Neg & -\(0.33\) & \(1.00\) & -\(1.00\) & - & - & -\(1.00\) & - & \(0.50\) \\ Comp. Pos & \(0.25\) & -\(1.00\) & -\(1.00\) & - & - & \(0.90\) & \(1.00\) & -\(1.00\) \\   

Table 5: UFS of InCoder-\(6\)B for the selected pair of demographics under different demographic dimensions and modifiers. “-” in the “Sexuality”, “Disability”, and “Politics” columns is because InCoder does not generate any code containing corresponding pairs of demographics, where UFS cannot be computed. “\(1.00\)” and “\(-1.00\)” means that only one demographic in the selected pair appears in all generated code.

the consequences will be if the code generated by Codex, which may contain serious discrimination toward marginalized groups, are applied to countless application development!_

Table 5 shows the fine-grained UFS of the code generated by InCoder-\(6\)B. The score is automatically computed for pairs of demographics under each demographic dimension and modifier category. Positive numbers mean that the judgment is more intense for the first demographic, while negative numbers signify more intense judgment for the second demographic. For example, \(-0.24\) in the first row and first column means that generated code demonstrates more negative judgment for white people compared with black people. This is different from previous conclusions  that PLM-based classifiers show more prejudices or negative sentiments for black people. We speculate this may stem from different pre-training corpus and tasks of code generation models compared with generic PLM-based classifiers.

Table 6 presents the standard deviation of the frequency for different demographics in the code generated by InCoder-\(6\)B, revealing the overall unfairness degree for different demographic dimensions and modifiers. As we can see, the unfairness problem is severe for the "Ethnicity" and "Gender" dimensions for almost all types of modifiers, which may stem from the stereotype in the pre-training corpus or essentially in our society.

### Human Annotation and Evaluation

We conduct human evaluation to further evaluate social biases in generated code. Annotation details and human evaluation results are presented below.

Human Annotation Details. We ask humans to manually annotate a subset of all the generated code. For each demographic dimension and modifier category, we randomly select \(5\) generated code out of \(10\) samplings. Considering different model sizes of InCoder and CodeGen, we gather \(8900\) samples in total. We hire three well-educated postgraduates majoring in computer science to label social bias for each code, and get \(29400\) annotations in total. We ask annotators to distinguish whether the code contains valid demographics relevant to corresponding demographic dimensions. If relevant demographics are indeed contained in the code, the annotator labels the code as "discriminatory",

  
**Modifier** & **Ethnicity** & **Religion** & **Gender** & **Sexuality** & **Disability** & **Age** & **Politics** & **Occupation** \\  RoB. Neg & \(23.24\) & \(1.92\) & \(54.34\) & \(5.57\) & - & \(4.29\) & \(0.00\) & \(4.61\) \\ Rand. Neg & \(11.91\) & \(0.50\) & \(24.91\) & \(2.28\) & - & \(2.00\) & \(0.50\) & \(2.18\) \\ Rand. Pos & \(6.78\) & \(1.30\) & \(18.45\) & \(2.83\) & - & \(1.29\) & \(0.00\) & \(2.50\) \\ Comp. Neg & \(2.52\) & \(0.50\) & \(3.50\) & \(0.50\) & - & \(1.02\) & \(0.50\) & \(0.40\) \\ Comp. Pos & \(1.77\) & \(0.50\) & \(6.00\) & \(0.50\) & - & \(0.55\) & - & \(1.10\) \\   

Table 6: The standard deviation of frequency for the code generated by InCoder-\(6\)B all valid demographics in every type of judgmental modifier and demographic dimension. “-” in the “Disability" and “Politics” columns is because the code generated by InCoder-\(6\)B contains no valid demographics for these two dimensions.

  
**Model** & **Size** & **RoB. Neg** & **Rand. Neg** & **Rand. Pos** & **Comp.** & **Tot.** \\   & \(1.38\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & \(6.7\)B & \(37.33\) & \(40.25\) & \(37.35\) & \(48.06\) & \(38.73\) \\   & \(350\)M & \(\) & \(\) & \(\) & \(\) & \(\) \\   & \(2.7\)B & \(39.08\) & \(50.79\) & \(50.69\) & \(72.44\) & \(48.45\) \\   & \(6.1\)B & \(68.70\) & \(67.38\) & \(65.60\) & \(61.88\) & \(68.25\) \\   & \(100\)B+ & \(84.80\) & \(80.88\) & \(84.38\) & \(86.25\) & \(84.03\) \\   

Table 7: Human evaluation results of the social bias in the generated code.

Figure 4: Correlation of human evaluation and automatic evaluation of CBS for all generated code. Each point is the average CBS for different modifiers under all demographic dimensions.

otherwise as "acceptable". If the labels for code differ among annotators, we choose the label that most people agree with as our final label.

Human Evaluation Results. With the above annotation disciplines, we get the annotation results for a subset of the code generated by Incoder and CodeGen. Similar to automatic evaluation, we also use CBS (frequency of biased code) as our human evaluation score. As shown in Table 7, human evaluation results reveal that all three code generation models contain severe social biases. To further evaluate the consistency between our automatic evaluation and human evaluation, we compute the correlation in Figure 4. As we can see, human evaluation results are basically consistent with our automatic evaluation results, which validates the effectiveness of our code bias classifier.

### Case Study

Figure 5 presents randomly selected examples of code generated by Codex and InCoder-\(6\)B. The upper parts without background are the code prompt for code generation models. The bottom parts with colored backgrounds are outputs of code generation models. As we can see, Codex harmfully perceives Asian as sick people, while InCoder detrimentally views Black people as sick people. These code snippets can do harm to marginalized groups and have unpredictable negative effects if adopted by programmers in real-world applications or systems. More case study is in Appendix.

## 5 Analysis

We further conduct an analytical study on the generated code. We first visualize the relative proportions of all valid demographics, and then analyze the effects of hyper-parameters of code generation models on code social bias.

Figure 5: Randomly selected examples of code generated by Codex and InCoder-\(6\)B with a negative modifier in the prompt.

Figure 6: Relative proportions of frequency for all valid demographics under the demographic dimensions of “Ethnicity” and “Occupation”. Two radar charts at the top correspond to “Ethnicity”, while those at the bottom correspond to “Occupation”. Best viewed on the screen.

### Demographics Analysis

Figure 6 illustrates the relative proportions of frequency for all valid demographics. Experiments are conducted on the code generated by InCoder-6B. For the top two radar charts, the left one corresponds to the code prompted with Random-Neg modifiers, while the right one corresponds to the code prompted with Random-Pos modifiers. The arrangement is the same for the bottom two charts. The variation of demographics for different demographic dimensions reveals that social biases contained in generated code are accurately correlated with specific demographics. This can cause users' attention to avoid discrimination against specific demographics when using these code generation models, and help further research to develop explicit debiasing methods. The sharp shape of frequency proportions also demonstrates the unfairness problem across different demographics.

### Effects of Hyper-Parameters

We conduct experiments to study the effects of hyper-parameters of code generation models on the social biases in the code generated by CodeGen-6B. We mainly analyze two hyper-parameters: temperature \(t\) and top-p . Figure 7 demonstrates the variation trend of CBS while \(t\) and top-p change from \(0.1\) to \(0.9\). The temperature hyper-parameter is used to re-calibrate the logits distribution, allowing to allocate higher probability mass to the higher probability tokens. We set the values of temperature \(t\) from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). As we can see from the upper part, almost for all modifier categories, CBS maintains relatively high values with temperate varying from \(0.3\) to \(0.5\) and decreases when the temperature is greater than \(0.6\). Top-p samples tokens from the vocabulary (\(w V\)) so that the cumulative probability mass of the sampled tokens exceeds a threshold \(p\): \(_{w V}P(w|w_{1:t-1}) p\). We set the values of top-p from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). As shown in the bottom part of Figure 7, CBS reaches the highest values for all categories of modifiers when the top-p is set to \(0.8\), and remains almost unchanged when the top-p varies from \(0.1\) to \(0.3\). These findings can provide insights into the choice of hyper-parameters of code generation models that demonstrate fewer social biases.

## 6 Related Work

Since various AI applications permeate every aspect of our lives, research on AI Ethics [22; 30] has attracted more and more attention. The research on AI Ethics is mainly categorized into five fine-grained topics: AI Fairness [12; 16], AI Accountability [36; 37], AI Transparency [3; 18; 23; 25], AI Privacy [28; 41], and AI Robustness [11; 38]. In this work, we mainly explore one important aspect of AI Ethics: AI Fairness, which has been studied from different perspectives [12; 16; 26; 31; 32].  proposed to study the existence of annotator group bias in various real-world crowdsourcing datasets.  measured hierarchical regional bias in pre-trained language models. Some works tried to detect and mitigate social biases in word embeddings [4; 17] and hidden representations , while others explored quantifying social biases in downstream tasks. Many works have explored the fairness problem in text classification tasks [9; 21; 8]. Some works also explore the fairness problem in generation tasks, such as machine translation , story generation , and question answering . However, no work has focused on the fairness problem in the code generation task. In this paper, we fill in the blank by uncovering and quantifying social biases in generated code.

Figure 7: Illustration on how the hyper-parameters temperature \(t\) (the left part) and top-p (the right part) affect the CBS. Best viewed on the screen. The \(x\)-axis represents the hyper-parameter values of \(t\) and top-p, while the \(y\)-axis signifies CBS. Best viewed on the screen.

Conclusion

In this paper, we explore the important research topic of code fairness. With our proposed prompt paradigm, we successfully uncover the social bias problem in the pre-trained code generation models. We propose to use three metrics of different granularity to quantify social biases in generated code. Experimental results reveal that prevalent code generation models contain severe social bias. We also find that, for the same model, the bigger the model size is, the more social biases it demonstrates. Moreover, further analysis is conducted to provide insights into selecting code generation models with low social bias.

## Limitations

In this work, we construct a new dataset for bias detection in code generation models. While the dataset was carefully designed to capture a range of potential biases, we acknowledge that its scope may not encompass the entirety of real-world coding scenarios. We referred to the commonly used examples in the training sets of the code generation task, e.g., BIGPYTHON, to design our prompt functions. The use of "if" statements to select data attributes is the common operation in training data, so we also use this commonly used statement to design our prompt functions. This leads to the limitation of diversity of our code prompts in our dataset construction. Further exploration of more complex code statements should be done in the future. Besides, although we have attempted to include as many demographic dimensions as possible, there are still many dimensions overlooked, such as socioeconomic status, due to a lack of systematic knowledge in social science. Interdisciplinary research and cooperation should be considered in the future. Moreover, this work stops at uncovering and quantifying the problem and phenomenon, without taking one step further to solve the social bias problem in code generation models. Further research on debiasing in code generation is of high demand and importance. Further analysis and exploration are also needed on how our work will be applied to practical application scenarios.