# Efficient Modeling of Irregular Time-Series with Stochastic Optimal Control

Byoungwoo Park, Hyungi Lee, Juho Lee

KAIST

{bw.park, lhk2708, juholee}@kaist.ac.kr

Equal contribution

###### Abstract

Many real-world datasets, such as healthcare, climate, and economic data, are often collected as irregular time series, which pose significant challenges for modeling. Previous research has approached this problem in two main directions: 1) Transformer-based models and 2) dynamics-based models. Transformer-based models efficiently handle irregular time series with simple architectures and time encoding but struggle with long sequences and require many parameters due to the lack of inductive biases. Continuous dynamics-based models offer accurate Bayesian inference of dynamic states but suffer from the complexity of sequential computation, leading to increased computational costs scaling with the length of time intervals. To address these limitations, we propose Parallel Bayesian Diffusion Filtering (PBDF), a variational inference algorithm based on parallelizable stochastic differential equations and stochastic optimal control theory. PBDF combines the parallel inference capabilities of Transformer-based models with the Bayesian inference of continuous-discrete state space models. Through empirical evaluations on the USHCN and Physionet datasets for both interpolation and extrapolation tasks, we demonstrate PBDF's superior performance and computational efficiency.

## 1 Efficient Modeling with Stochastic Optimal Control

In this section, we explain our proposed model for irregular time series data, called PBDF. We start by introducing the Continuous-Discrete State Space Model (CD-SSM)  and formulate the variational inference problem for the state variables in Sec 1.1. Next, we discuss amortized inference for the auxiliary variables and detail the efficient learning and inference algorithm for PBDF in Sec 1.2.

### Controlled Continuous-Discrete State Space Model.

Let us consider for a set of time steps (regular or irregular) \(\{t_{i}\}_{i=0}^{K}\) over an interval \(=[0,T]\), \(i.e.,0 t_{1} t_{K} T\). The CD-SSM assumes a continuous-time Markov state trajectory \(_{0:T}\) in a latent space \(\) as a solution of the stochastic differential equation (SDE):

\[d_{t}=b(t,_{t})dt+d_{t}, _{0}_{0}\] (1)

and \(\{_{t}\}_{t[0,T]}\) is a \(\)-valued Wiener process that is independent of the \(_{0}\). Since \(_{t}\) is a Markov process, we can describe the time-evolution of \(_{t}\) by a transition kernel \(_{t}\), and for any measurable event \(A\) and \(_{t_{i-1}}\), transition kernel \(_{t_{i}}\) for the time \(t_{i}\) can be computed as \(_{t_{i}}(_{t_{i-1}},A)=_{A}_{t_{i}}(_{t_{i-1}},_{t_{i}})d_{t_{i}}\), where the transition density \(_{t}\) is obtained by a solution of the Fokker-Planck equation associated with \(_{t}\). By utilizing transition kernel \(_{t_{i}}\)s for all \(i[1:K]\), we can define aproduct law of \(\{_{t_{i}}\}_{i[0:K]}\) as follows:

\[d(_{0:t_{k}})=d_{0}(_{0})_{i=1}^{K} _{t_{i}}(_{t_{i-1}},d_{t_{i}}).\] (2)

In practice, we can only access the observations \(\{_{t_{i}}\}_{i=1}^{K}\) recorded for each discrete time steps \(\{t_{i}\}_{i=1}^{K}\). Each \(y_{t_{i}}\) is assumed to be generated from a measurement model \(_{t_{i}}(_{t_{i}}|_{t_{i}})\). Our goal is to infer the _filtering_ distribution, the conditional distribution of \(_{0:t_{k}}\) given a set of observations up to time \(t_{k}\), \(_{t_{k}}=\{_{t_{i}}|i k\}\),

\[d^{}(_{0:t_{k}})=(_{t_{ k}})}_{i=0}^{K}g_{t_{i}}(_{t_{i}}|_{t_{i}})d (_{0:t_{k}}),\] (3)

where \((_{t_{k}})=_{}[_{i=1}^{K}g _{t_{i}}(_{t_{i}}|_{t_{i}})]\) is a marginal likelihood of \(_{t_{k}}\). Sampling trajectories from \(^{}\) is generally infeasible except for highly restrictive cases, such as when the drift function \(b\) in (1) is linear and the measurement model \(g\) in (3) is Gaussian.

SOC and VI.In this study, we propose a variational inference (VI) algorithm for filtering, based on the theory of stochastic optimal control (SOC) . To do so, we introduce a path measure \(^{}\) which is induced by the solutions of the following _affine-control_ SDE:

\[ d_{t}=[b(t,_{t})+(t,_{t};_{t})]dt+d}_{t},_{0}_{0}.\] (4)

Here, \(^{d}\) denotes a _control function_, which is chosen by a user to achieve a specific objective. In our case, we design \(\) to approximate the posterior path measure \(^{}\). We propose an objective function \(()\) which is related the SOC problem with VI for the posterior path measure in (3).

**Proposition 1.1** (Variational Bound).: _If we choose the prior path measure as \(\) and variational path measure as \(^{}\), then the ELBO coincides with negative cost function \(()\):_

\[-_{T})}_{} (|_{T}):=_{^{}}[_{0}^{T} \|_{s}\|^{2}ds-_{i=1}^{K} g_{i}( _{t_{i}}|_{t_{i}}^{})].\] (5)

Proposition 1.1 states that the minimization problem involving \(()\) in equation (5) can be interpreted as VI for the path measure. In other words, it is possible to construct an approximate posterior, by parameterizing the control function with a suitably expressive family of functions such as DNN (_i.e._, \(:=(;)\)). This enables a close approximation of the true posterior for the VI .

### Efficient Modeling of the Latent System

Utilizing gradient-descent based optimization [12; 23] for (5) requires computing gradients through the simulated diffusion process over an interval \([0,T]\), which can be slow, unstable, and memory-intensive as time sequence length \(T\) increases. It contrasts with the core philosophy of many recent generative models, which focus on splitting the generative problem and solving them jointly.

Locally Linear Dynamics.Motivated by the simulation-free property of linear dynamical models , we explore linear drift functions that allow for the parallel computation of the approximate posterior distribution. This approach enables us to estimate a closed-form expression for the state at any time \([t,T]\) given the information \(_{t}\) up to time \(t 0\). To enable the simulation-free state estimation for \(_{t}\), we study a linear drift \(b(t,_{t})=_{t}\) and non-markov control \((t,_{t},_{t})(_{t})\). This linear formulation enable us to estimate the conditional distribution for any \(t[0,T]\) for a given initial Gaussian distribution.

Recognizing the limitations of linear dynamics in real-world scenarios, we propose using locally linear dynamics as a more flexible approximation of the nonlinear drift function. This method leverages neural networks to fully utilize available information while maintaining a linear structure. To achieve this, we introduce a parameterization strategy inspired by [1; 11], where the transition matrix is defined as \(_{i}=_{l=1}^{L}w^{(l)}^{(l)}\). Here, the weights \(=\{w^{(l)}\}_{l=1}^{L}=_{}(_{t_{i}})\) are obtainedfrom a neural network \(_{}\) with a softmax output and \(\{^{(l)}\}_{l=1}^{L}\) represents a set of \(L\) parameterized diagonal matrices. Additionally, the control is parameterized by \(_{i}=_{}(_{t_{i}})\). Since \(_{t}\) changes discretely at each observation time \(\{t_{i}\}_{i=1}^{K}\), the drift function remains piecewise constant within each interval. This structure enables us to derive a closed-form solution for the intermediate latent states.

**Theorem 1.2** (Simulation-free estimation).: _Let us consider the confrol-affine SDEs:_

\[d_{t}=[_{i}_{t}+_{i}]dt+ d _{t}, t[t_{i-1},t_{i}).\] (6)

_Then, for an interval \([t_{i-1},t_{i})\) for all \(i[K]\), the solution to (6) condition to initial distribution \(_{0}^{}=(_{0},_{0})\) is a Gaussian process \((_{t},_{t})\) with the first two moments is given by_

\[_{t}=e^{(t-t_{0})_{i}}_{0}+_{j= 1}^{i-1}(e^{(t-t_{k})_{j}}_{j}^{-1}(e^{(t_{j}-t_{k-1} )_{j}}-)_{j})+_{i}^{-1}(e^{(t-t_{i- 1})_{i}}-)_{i},\] (7) \[_{t}=e^{2(t-t_{0})_{i}}_{0}+_ {j=1}^{i-1}(e^{2(t-t_{j})_{j}}_{j}^{-1}-t_{j-1})_{j}}-)}{2})+_{i}^{-1})_{i}}-)}{2}.\] (8)

Moreover, we use parallel scans to efficiently compute the the marginal distributions \(_{t_{i}}^{}=(_{t_{i}},_{t_{i}})\) at each time stamp \(\{t_{i}\}_{i=1}^{K}\). Given an associative operator \(\) and a sequence of elements \([s_{t_{1}}, s_{t_{K}}]\), the parallel scan algorithm (_Scan_) computes the all-prefix-sum which returns the sequence \([s_{t_{1}},(s_{t_{1}} s_{t_{2}}),,(s_{t_{1}} s_{t_{2}}  s_{t_{K}})]\) in \(( K)\) time. Since the Gaussian distribution can be chareacterized by the first two moments, applying the _Scan_ to the \(\) and \(\) yields the desired computation. See Appendix B for more details.

Amortization.To enhance flexibility and efficiency, we treat \(\{_{t_{i}}\}_{i=1}^{K}\) as an auxiliary variable in the latent space which is produced by a encoder \(q_{}(_{0:T}|_{0:T})=_{i=1}^{k}q_{}( _{t_{i}}|_{t_{i}})=_{i=1}^{k}(_{t_{i}}| _{}(_{t_{i}}),_{i})\) with neural network \(_{}\) applied to the time series \(\{_{t_{i}}\}_{i=1}^{K}\). This approach allows us to decouple the representation learning of \(_{t}\) from the dynamics of \(_{t}\), resulting in a more efficient parameterization. Additionally, it enables the modeling of nonlinear conditional distributions through a neural network decoder \(p_{}(_{t}_{t})\).

Training and Inference.We jointly train the encoder-decoder \(\{,\}\) and the control \(\{\}\) by maximizing the evidence lower bound (ELBO) of the observation log-likelihood for time series \(_{0:T}\):

\[ p_{}(_{0:T}) _{_{T} q_{}(_{0:T}| _{0:T})}[^{K}p_{}(_{t_{i}}| _{t_{i}})p_{}(_{T})}{_{i=0}^{K}q_{}( _{t_{i}}|_{t_{i}})}]\] (9) \[\] (10)

The variational parameters \(\{,\}\) can be optimized separately for each sequence, and the prior over the auxiliary variable \(p_{}(_{T})\) can be computed using the ELBO proposed in (5) as part of our variational inference procedure for the latent posterior \(^{}\) in proposed Sec 1.1. The overall training and inference processes are summarized in the Algorithm 1 and the Algorithm 2, respectively.

## 2 Experiment

In this section, we present empirical results that validate the effectiveness of PBDF on irregular time-series modeling. Here, we conduct experiments on two tasks: interpolation and extrapolation, using two different real-world datasets, USHCN  and Physionet . We compare our approach against various baselines including RNN architecture (RKN-\(_{t}\), GRU-\(_{t}\), GRU-D ) as well as dynamics-based models (Latent ODE [5; 15], ODE-RNN , GRU-ODE-B , CRU ) and attention-based models (mTAND ), which have been developed for modeling irregular time series data. In addition to reporting the test MSE loss for each task and dataset, we also provide the actual training time per epoch for each model to validate PBDF's computational efficiency. For all experiments, we followed the same experimental setup as in . Additional experimental details can be found in Appendix E.

InterpolationWe begin by evaluating the effectiveness of PBDF on the interpolation task. Following the approach of  and , each model is required to infer all time points \(t\) based on the complete set of observations \(_{}\). The interpolation results presented in Table 1 clearly indicate that PBDF outperforms other baselines in terms of test MSE loss for the Physionet dataset and comparable performance for the USHCN dataset.

ExtrapolationWe evaluated PBDF's performance on the extrapolation task following the experimental setup of . Each model infer values for all time stamps \(t^{}\), where \(^{}\) denotes the union of observed time stamps \(=\{t_{i}\}_{i=1}^{k}\) and unseen time stamps \(_{u}=\{t_{i}\}_{i=k+1}^{N}\), \(^{}=_{u}\). For the Physionet dataset, input time stamps \(\) covered the first 24 hours, while target time stamps \(^{}\) spanned the first 48 hours. In the USHCN dataset, the timeline was split evenly, with \(t_{k}=\). We report the mean squared error (MSE) for unseen time stamps \(_{u}=^{}-\). As shown in Table 1, PBDF consistently outperformed all baselines in terms of MSE on both datasets. Particularly for USHCN, PBDF achieved a substantial performance margin over the second-best model.

Computational EfficiencyTo assess the training costs relative to dynamics-based models that rely on numerical simulation, we re-ran CRU on the same hardware used for training our model (highlited by \({}^{*}\) in Table 1. Specifically, we utilized a single NVIDIA RTX A6000 GPU. As shown in Table 1, PBDF substantially reduces training costs when compared to dynamics-based models. Notably, PBDF exhibited a faster runtime than CRU while achieving superior performance. These findings demonstrate PBDF's capability to accurately approximate the posterior distribution for unseen time points, all while maintaining computational efficiency.

## 3 Conclusion

In this work, we introduce a novel variational inference method for approximating the filtering distribution of CD-SSM by leveraging SOC. Linear approximation of controlled drift function enable us parallel computation of the latent dynamics, thereby improving the efficiency of inference algorithm of filtering distribution. Furthermore, our approach incorporates amortization, which successfully models complex real-world time-series data such as USHCN and Physionet.

    &  &  &  \\   & USHCN & Physionet & USHCN & Physionet & USHCN & Physionet \\  mTAND\({}^{}\) & \(1.766 0.009\) & \(0.208 0.025\) & \(2.360 0.038\) & \(\) & 7 & 10 \\ RKN-\(^{}\) & \(0.009 0.002\) & \(0.186 0.030\) & \(1.491 0.272\) & \(0.703 0.050\) & 94 & 39 \\ GRU-\(^{}\) & \(0.090 0.059\) & \(0.271 0.057\) & \(2.081 0.054\) & \(0.870 0.077\) & 3 & 5 \\ GRU-D\({}^{}\) & \(0.944 0.011\) & \(0.338 0.027\) & \(1.718 0.015\) & \(0.873 0.071\) & 292 & 5736 \\ Latent ODE\({}^{}\) & \(1.798 0.009\) & \(0.212 0.027\) & \(2.034 0.005\) & \(0.725 0.072\) & 110 & 791 \\ ODE-RNN\({}^{}\) & \(0.831 0.008\) & \(0.236 0.009\) & \(1.955 0.466\) & \(0.467 0.006\) & 81 & 299 \\ GRU-ODE-B\({}^{}\) & \(0.841 0.142\) & \(0.521 0.038\) & \(5.437 1.020\) & \(0.798 0.071\) & 389 & 90 \\ CRU\({}^{}\) & \(0.016 0.006\) & \(0.182 0.091\) & \(1.273 0.066\) & \(0.629 0.093\) & 122 (57.8)\({}^{*}\) & 114 (63.5)\({}^{*}\) \\ 
**PBDF** (Ours) & \(\) & \(\) & \(\) & \(0.627 0.019\) & 2.3 & 3.8 \\   

Table 1: Test MSE (\( 10^{-2}\)) for inter/extra-polation on USHCN and Physionet. The best results are highlighted in **bold**, while the second-best results are shown in blue. \(\) indicates result from .