# Code Repair with LLMs gives

an Exploration-Exploitation Tradeoff

 Hao Tang

Cornell University

haotang@cs.cornell.edu

&Keya Hu

Shanghai Jiao Tong University

hu_keya@sjtu.edu.cn

&Jin Peng Zhou

Cornell University

jpzhou@cs.cornell.edu

&Sicheng Zhong

University of Toronto

sicheng.zhong@mail.utoronto.ca

&Wei-Long Zheng

Shanghai Jiao Tong University

weilong@sjtu.edu.cn

&Xujie Si

University of Toronto

CIFAR AI Chair, Mila

six@cs.toronto.edu

&Kevin Ellis

Cornell University

kellis@cornell.edu

###### Abstract

Iteratively improving and repairing source code with large language models (LLMs), known as _refinement_, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.

## 1 Introduction

An emerging paradigm for problem-solving with large language models (LLMs) is to have the language model _correct, repair, or debug_ its initial outputs [1; 2; 3; 4; 5], which we refer to here as _refinement_. For example, when generating the source code of a program, refinement prompts the LLM with a buggy program it previously generated, potentially with diagnostic information such as a stack trace, then asks it to fix its code. This strategy alleviates the need for the LLM to predict perfect code on the first try, and approximates the iterative code-writing process of software development.

Complex programming problems often require several rounds of refinement, with each round requiring more LLM calls, each of which is stochastic, and so yields multiple possible outcomes. Therefore, this process generates a tree of possible programs (Fig. 1 left). This tree is infinitely deep, because every refinement can itself be further refined. It is also infinitely wide, because the LLM can return infinitely many possible refinements. Success should, in principle, depend on exactly what policy is used to expand this tree. Recent work [1; 6; 3] adopts simple expansion policies (e.g., breadth-first),with mixed success: On close examination, the gains from refinement, at least with these basic policies, are marginal compared to repeatedly sampling i.i.d. programs from the initial prompt .

Our work here reanalyzes refinement in terms of a tradeoff between exploiting the refinement of programs that are closer to being correct (i.e., pass more testcases), and exploring programs that have been refined fewer times. Fig. 1 (right) diagrams this tradeoff for a tree after the first few node expansions. This exploration-exploitation tradeoff is made more challenging by the fact that every time we refine a program, we create another brand-new program, giving an ever-increasing set of possible actions (possible next refinements). Our problem is however not solvable by standard approaches to Monte Carlo Tree Search (MCTS)--a bandit-based node expansion policy--because our branching factor is infinite, our transition function is stochastic, and "rollouts" would demand a prohibitively expensive series of LLM calls to refine down to some maximum depth.

Our primary contribution is an algorithm for efficiently performing refinement, which we call _REx_ (REfine, Explore, Exploit). _REx_ should be seen as a strategy for conducting LLM-guided search that constructs and navigates a tree of refinements. We derive _REx_ via a multiarmed bandit framing: different actions correspond to refining different programs; reward corresponds to the quality of a newly generated program; and maximizing discounted future reward corresponds to solving the programming problem in the minimum number of LLM calls.

The resulting algorithm is broadly applicable to LLM-based code generation tasks. We describe applications to competition programming problems, challenging software verification problems involving generating nonlinear loop invariants, and visual reasoning puzzles from the Abstraction and Reasoning corpus (ARC: ). Across every domain, _REx_ solves more problems in fewer LLM calls, typically reducing the amount of API calls by a factor of 1.5x-5x. _REx_ is also able to consistently solve a modest number of difficult problems that prove out-of-reach for other approaches.

## 2 Background: Bandits and Thompson Sampling

**Bandit problems** concern maximizing total reward given a set of actions, \(\), each with an unknown reward distribution, \(P(r|a)\), for each action \(a\). At each time step \(t\), an action \(a_{t}\) is chosen and a reward \(r_{t}\) is received. Bandit problems are challenging because they involve balancing exploration and exploitation: On the one hand, each action must be tried enough times to estimate its average reward (exploration), but asymptotically, only the actions with the highest expected reward should be chosen (exploitation). Actions are called _arms_, and taking an action is called _pulling_ that arm.

**Thompson Sampling** is a framework for designing bandit strategies that performs probability matching: pulling an arm with probability equal to the odds that that arm is optimal (has the highest expected reward). It maintains probabilistic beliefs about the reward distribution of each arm, and updates these beliefs following Bayes rule. Writing \(_{a}\) for the parameters of the reward distribution for arm \(a\), this means that \(P(r|a)=P(r|_{a})\) (by definition), and that \(_{a}\) is treated as a random variable with a prior \(P(_{a})\). Beliefs are updated after each reward, and the next arm can be selected by sampling \(_{a}\) from the posteriors, and pulling the arm with the highest (expected) reward. Concretely,

Figure 1: Left: The tree of possible refinements is infinitely deep and has infinite branching factor. Each node is a program and each edge is an LLM sample. Right: Explore-Exploit tradeoff for a search state after performing 3 node expansions. Exploit by sampling another child of a program that is nearly correct, or Explore by sampling a child of a program that has been expanded fewer times.

[MISSING_PAGE_EMPTY:3]

framing as "arm-acquiring." Our approach derives from Thompson Sampling, a stochastic algorithm that pulls an arm with probability equal to the probability that it is the best arm. Therefore we need to calculate probabilistic beliefs of the optimality of a given arm, which will be biased by the heuristic function \(h\) to prioritize programs with high heuristic value. The derivation of our Thompson Sampler will also automatically penalize programs that have been refined many times.

Concretely, we receive a reward of 1 if pulling an arm (refining a program) yields a new program that satisfies \(\), and otherwise receive reward zero. Therefore the reward \(r\) follows a Bernoulli distribution, whose parameter we will write \(_{}\):

\[_{}=P(r=1|,)=*{}_{^{}  P_{}(|,)}[[^{} ]]\] (6)

As we solve this bandit problem using Thompson Sampling , we need to maintain probabilistic beliefs about each arm's expected reward, \(_{}\), which get updated with each refinement. This means we need a prior over \(_{}\), for which we choose the Beta distribution, which is conjugate to the Bernoulli, yielding simple Bayesian updates . We choose a prior that places more mass on \(_{}\) whenever \(h()\) is also large, effectively injecting heuristic knowledge into the prior:

\[P(_{}) =(_{}^{},_{}^{})\] (7) \[_{}^{} =1+C h()\] (8) \[_{}^{} =1+C(1-h())\] (9)

where \(C\) is a hyperparameter. Large \(C\) gives greedier behavior by increasing the importance of \(h\).

The posterior beliefs over \(\) get updated whenever \(\) is refined. Assume the program (arm) has been refined (pulled) \(N_{}\) times, all with no success (no reward). Then the posterior is

\[P(_{}|N_{})  P(N_{}|_{})P(_{})=(1-_{} )^{N_{}}(_{}^{},_{}^{ })\] \[(_{}^{},_{}^{ }+N_{})\] \[=(1+C h(),1+C(1-h())+N_{})\] (10)

The above equation essentially defines _REx_. For each program we track its heuristic value and how many times it has been refined, and to select the next program to refine, we sample from the Beta distributions in Eq. 10 and refine the program \(\) with highest sampled \(_{}\). An implementation is about ten lines of Python (see below and Appendix Alg 1). Refining an empty program means generating a new program from scratch. The heuristic value of an empty program is unknown so we set \(C h()=1-C h()=:=0\).

```
1defREx(problem,C):  programs= (problem.empty_solution())  failed_cnt=default(lambda:\(\))#\(N_{}\)inpaper  whileTrue:  program=max(programs,key=lambda p:np.beta(  1 + C*p.heuristic_value,  1 + C*(1-p.heuristic_value)+failed_cnt[p] #\(1+C h())+N_{}  ))  new_program=program.refine(problem)#\(^{} P_{}(|,)\)  ifis_solved(problem,new_program):#\(^{}\)inpaper  returnnew_program  failed_cnt[program]+=1  programs.add(new_program) ```

Understanding the behavior of _REx_.Intuitively, we begin with optimistic beliefs about the utility of refining a program. The strength of that optimism is controlled by \(h\), with higher heuristic-value programs having higher initial estimates of \(_{}\). With each unsuccessful refinement, we update our beliefs to be less optimistic, with the expected reward decaying toward zero with increasing number of failures:

\[*{}[_{} N_{}]=}\] (11)However, the system actually works with the full posterior distribution over \(_{}\), not merely its expectation. Fig. 2 (middle-right) illustrate how the posterior evolves depending on the number of times that a program has been refined, showing that it initially concentrates its mass around \(h()\), and shifts downward with each refinement, making it progressively less likely to be refined further, but maintaining the property that every program always has a nonzero chance of being the next refinement. The variance also decays following \([_{} N_{}]=(N_{}^{- 3})\), meaning that a program that has already been heavily refined is not only lower in expectation, but also less likely to be randomly 'bumped up' and promoted as the next arm to pull by Thompson Sampling.

## 5 Experimental Results

We study three different domains that each involve code generation. See Fig. 3. (Followup work  also applies _REx_ to learning world models, where its scales to synthesizing 200+ line programs):

1. **Competition Programming.** We benchmark on APPS, one of the most challenging LLM programming problem benchmarks . APPS problems involved generating self-contained Python code that solves an algorithmic puzzle, given a natural-language problem description. APPS is split into three difficulty levels: Competition, Interview, and Introductory. Competition-level problems are very challenging, with landmark LLMs such as AlphaCode only solving 8% of APPS Competition , making APPS substantially more difficult than basic benchmarks such as HumanEval  and MBPP .
2. **Visual Reasoning.** We take visual reasoning puzzles from the Abstraction and Reasoning Corpus (ARC [7; 17]). In ARC the goal is to infer an image-manipulating program from input-output examples, effectively combining inductive reasoning and visual/geometric puzzle solving. We work with a 40-problem subset of ARC introduced in  that has been annotated with natural-language descriptions of the target function.
3. **Loop Invariant Synthesis for Software Verification.** Discovering loop invariants is an important formal verification task; see Appendix A.3 for a primer on this problem statement. We collect 38 _non-linear_ loop invariant synthesis tasks  from [20; 21]. These previous works rely on manually supplied templates/features and on dynamic program analyses (i.e., running the code). In contrast, we solely use source code without any dynamic execution traces or feature engineering. We check all three criteria of being a sufficiently strong inductive invariant (i.e., precondition, induction, and postcondition) with the Z3 solver . The Z3 solver timeout is set to 2 minutes.

We use these datasets to study several research questions: (1) For a given compute budget, which approaches to refinement solve the most problems? (2) Can _REx_ solve hard problems other methods cannot? (3) How much of a speedup/cost-reduction does _REx_ grant? (4) How sensitive are different methods to hyperparameter choices, and what are reasonable default hyperparameter settings for this new method? To investigate these questions, we study a range of baselines:

Figure 2: How the model’s beliefs about the benefit of refining a program, \(\), change as we vary (1) \(N\), the number of times it was previously refined, and (2) \(h\), the heuristic estimate of how close we are to satisfying the specification (larger \(h\) is better). Left: Expected benefit of refining decreases the more we refine, and asymptotically decays to zero (Eq. 11). Middle/Right: Posterior beliefs initially center around \(h\) and shift toward zero with each additional refinement. Same colored curves show same values of \(h\) for different values of \(N\). The hyperparameter \(C\) modulates the rate of decay with each additional refinement, and also affects the initial concentration of the density around \(h\).

Figure 3: Evaluation domains. For visual reasoning, the goal is to synthesize an image-manipulating program that translates input images to the correct outputs. For software verification, the goal is to synthesize logical conditions as a valid loop invariant, in order to formally verify the functionality of the code. For competition programming, the goal is to generate an algorithm in Python.

1. **Greedy baseline** refines the program with the highest heuristic value. It has a hyperparameter corresponding to the heuristic value of the initial (blank) program.
2. **Breadth-First Search** expands the refinement tree breadth-first. Its single hyperparameter is the width (branching factor), searching deeper and wider the larger the compute budget.
3. **Fixed-Width** generates a fixed number of initial programs, and then refines them round-robin, while never refining the same program more than once . This generates a refinement tree of fixed width, searching deeper (but not wider) the larger the compute budget. There is a single hyperparameter corresponding to the initial width.

Problems solved as a function of compute budget.Fig. 4 and Fig. 5 show programming problems solved as the number of refinements increases, varying method and hyperparameters. We consider both the number of problems solved at each level of compute budget, as well as the total Area Under Curve (AUC). While the exact rank order of methods depends on the dataset and the LLM, we see consistently that _REx_ solves the most problems. However, the final number of problems solved is typically only modestly larger for _REx_. What method counts as second-best varies by domain. For example, Fixed-Width is nearly as good as _REx_ on APPS-Competition and ARC, but on Loop Invariants and APPS-Introductory, Fixed-Width is the worst-performing method.

Figure 4: Comparing _REx_ with alternatives using GPT-4 (temp=1). BFS and FW are Breadth First Search and Fixed Width, respectively. AUC denotes Area Under the Curve, and Final denotes the success rate at the maximum # LLM calls (64 for ARC and 300 for others due to domain conventions). Dark lines show performance with the best hyper-parameter setting for each method. Light lines show each run on each hyperparameter. The inset box plots show the distribution while varying the hyper-parameters. APPS baselines: Parsel , AlphaCode , and Olausson et al. . Nonlinear Loop Invariant baselines: Z3/GSpacer  and Yao et al. . ARC baseline: Hypo. Search . More results on APPS Interview-Level and ARC in Figure 11 and Figure 13

We hypothesize that our robustness across datasets may come from the ability of _REx_ to adaptively choose whether to search wide or deep. The other methods commit to a particular depth/width tradeoff, and how to optimally balance depth/width can vary by dataset. Indeed, examining the trees produced by _REx_ shows that the algorithm searches quite broadly but refines deeply along more promising branches (Fig. 7), in ways that are reminiscent of Monte Carlo Tree Search.

Speedups.Given the high monetary and environmental cost of frontier LLMs, it is valuable to understand whether different methods demand different levels of compute when solving the same problems. Fig. 6 analyzes these speedups, finding that _REx_ requires less compute than alternatives: roughly 2\(\) less on APPS-Competition, 2-5\(\) less on loop invariants, and 1.1-1.6\(\) less on ARC. We also see that there is little advantage to our method on easier benchmarks: for example, there is no speedup on APPS Introductory. By definition, harder problems take more time, therefore results on the hardest problems are the most relevant for saving time and compute.

Solving hard problems.In the large compute limit _REx_ asymptotically solves modestly more problems than the baselines, particularly on harder datasets (Fig. 4, APPS Competition vs Intro). How do these performance levels compare to other recent works on the same problem domains? On ARC, the state-of-the-art is Hypothesis Search , which we replicate, finding _REx_ solves more problems.1 On Loop Invariants, we solve more problems than general purpose solvers such as Z3-GSpacer  as well as the state-of-the-art solver that is specially designed for this dataset . The specially designed solver individually tuned the feature space and the hyper-parameters for each benchmark problem. It also assumes access to runtime traces of the code, and the ability to run the code thousands of times. Yet _REx_ outperforms it by a large margin (73.7% v.s. 60.5%2), with no special tuning. _REx_ is also, to the best of our knowledge, state-of-the-art on APPS-Competition with GPT4 (Fig. 4). _REx_ therefore not only solves problems that our baselines cannot, but can set new state of the arts on the specific datasets we apply it to.

Figure 5: Comparing _REx_ with alternatives with other LLMs on competition programming (APPS Competition-Level). More results on ARC are available in Appendix in Figure 12.

Figure 6: Number of LLM Requests needed to get the best LLM-based state-of-the-art methods (for ARC, APPS) and the best LLM-based baseline (for Nonlinear Loop Invariant). The lower the better. Box plot shows range of results across hyperparameters. ”x” denotes the factor by which our method is faster than the compared baseline on this benchmark (ratio of median #LLM calls).

Hyperparameter sensitivity.We consider a range of hyperparameters for each method. Box-and-whisker plots in Fig. 4, Fig. 5, and Fig. 6 illustrate the distribution of results for different methods, while varying hyperparameters. For _REx_, large \(C\) values work well on all datasets (\(C=20\)). For the other methods, the optimal hyperparameter varies more widely: For example, even though Olausson et al.  suggests larger width for the fixed-width approach, we find smaller width can be better, depending on the dataset: Fixed-Width benefits from larger widths on ARC and Nonlinear Loop Inv, but performs better on APPS at smaller widths. Appendix Tbl. 1 shows these and other results in more detail, finding that our method is significantly more robust to variation in hyperparameters, while the other methods degrade more when their hyperparameters are not tuned to each specific dataset. We speculate that this may be due to the same reason that _REx_ is more robust across domains: that it can adaptively decide when and where to search wide or deep.

## 6 Related Work

Code refinement.Prompting an LLM to fix problems with code that it generates has been explored in recent work [1; 4; 3]. To the best of our knowledge, our work is the first to systematically explore different policies for guiding refinement, and to introduce new bandit-based policies. Other works have explored fine-tuning specialized models to perform refinement, which allows using smaller models [6; 27], and which could be synergistic with our work here. More broadly within computer science, there is a long history of research on automated program repair using non-neural methods such as genetic search and constraint solving[28; 29; 30; 31], i.a.], which generally considers a different problem setting: assisting the fixing of human-written code, often within the context of a large code base. It is also possible to combine these classical methods with contemporary machine learning [32; 33].

Bandits and Tree Search.That tree search algorithms introduce an explore-exploit tradeoff has been long appreciated. Monte Carlo Tree Search (MCTS) is the canonical tree search algorithm that relies on this insight . Although _REx_ derives from a similar insight, its structure is very different from MCTS. Unlike MCTS, we do not perform rollouts, backups, or tree traversals. Instead, _REx_ exploits unique properties of the refinement process: Any node can be further expanded (infinitely),

Figure 7: Example search tree generated by _REx_. Blue\(\)Yellow gradient indicates heuristic value (blue: low, yellow: high). The order of children from left to right indicates the sequence of generations (left: older, right: newer). See also appendix Fig. 14-17

every node can be treated as a terminal node, and node expansions are very expensive (so rollouts would not be practical). To the best of our knowledge it is not possible to apply MCTS or any of its standard variants [35; 36; 37] to our problem statement out-of-the-box because of these unique properties. Bandits-based algorithms have also been applied for fuzz testing [38; 39] where code coverage is more important and code mutation is performed instead of code refinement.

## 7 Limitations and Future Directions

We see that our method is only modestly more effective at actually solving more problems overall in the large-compute limit: Its advantages are largest when viewed through the lens of minimizing cost, and of being robust across hyperparameters and datasets. Progress on cracking the hardest problems, such as solving all of ARC or all of APPS, is more likely to come from improvements to the base model, and less likely to come from improved ways of organizing the refinement search process.

We merely use the pass-rates of the programs to guide the search. There are richer information and more advanced bandits-based algorithms to integrate for better performance. A natural extension of our REx is to use contextualized bandits algorithms conditioned on the problem statement, the program, and the refinement history.

Acknowledgements.This work was supported by NSF grant #2310350, a gift from Cisco, and in part, by Individual Discovery Grants from the Natural Sciences and Engineering Research Council of Canada, and the Canada CIFAR AI Chair Program. We thank the Microsoft Accelerating Foundation Models Research (AFMR) program for providing generous support of Azure credits.