# Entropic Neural Optimal Transport

via Diffusion Processes

 Nikita Gushchin

Sokoltech

_Moscow, Russia_

n.gushchin@skoltech.ru

&Alexander Kolesov

Skoltech1

_Moscow, Russia_

a.kolesov@skoltech.ru

&Alexander Korotin

Skoltech1

_Moscow, Russia_

a.korotin@skoltech.ru

&Dmitry Vetrov

HSE University,1 AIRI2

_Moscow, Russia_

vetrovd@yandex.ru

&Evgeny Burnaev

Skoltech1

_Moscow, Russia_

e.burnaev@skoltech.ru

###### Abstract

We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between continuous probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schrodinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks. The code for the ENOT solver can be found at https://github.com/ngushchin/EntropicNeuralOptimalTransport.

## 1 Introduction

Optimal transport (OT) plans are a fundamental family of alignments between probability distributions. The majority of scalable neural algorithms to compute OT are based on the dual formulations of OT, see  for a survey. Despite the success of such formulations in generative modeling , these dual form approaches can hardly be generalized to the popular **entropic OT**. This is due to the numerical instability of the dual entropic OT problem  which appears for small entropy

Figure 1: Trajectories of samples learned by our Algorithm 1 for Celeba deblurring with \(=0,1,10\).

regularization values which are suitable for downstream generative modeling tasks. At the same time, entropic OT is useful as it allows to learn one-to-many stochastic mappings with tunable level of sample diversity. This is particularly important for ill-posed problems such as super-resolution .

**Contributions**. We propose a saddle-point reformulation of the Entropic OT problem via using its dynamic counterpart known as the Schrodinger Bridge problem (SS4.1). Based on our new reformulation, we propose a novel end-to-end neural algorithm to solve the related entropic OT problem for a pair of continuous distributions accessible by samples (SS4.2). Unlike many predecessors, our method allows handling small entropy coefficients. This enables practical applications to \(\) mapping tasks that require slight variability in the learned maps. Furthermore, we provide an error analysis for solving the suggested saddle point optimization problem through duality gaps which are the errors of solving the inner and outer optimization problems (SS4.3). Empirically, we illustrate the performance of the method on several toy and large-scale EOT tasks (SS5).

## 2 Background

Optimal Transport (OT) and Schrodinger Bridge (SB) problems imply finding an efficient way to transform some initial distribution \(_{0}\) to target distribution \(_{1}\). While the solution of OT only gives the information about which part of \(_{0}\) is transformed to which part of \(_{1}\), SB implies finding a stochastic process that describes the entire evolution from \(_{0}\) to \(_{1}\). Below we give an introduction to OT and SB problems and show how they are related. For a detailed overview of OT, we refer to [49; 41] and of SB - to [32; 11].

### Optimal Transport (OT)

**Kantorovich's OT formulation** (with the quadratic cost). We consider \(D\)-dimensional Euclidean spaces \(\), \(\) and use \(_{2}()=_{2}()\) to denote the respective sets of Borel probability distributions on them which have finite second moment. For two distributions \(_{0}_{2}()\), \(_{1}_{2}()\), consider the following minimization problem:

\[_{(_{0},_{1})}_{ }}{2}d(x,y),\] (1)

where \((_{0},_{1})_{2}( )\) is the set of probability distributions on \(\) with marginals \(_{0}\) and \(_{1}\). Such distributions \((_{0},_{1})\) are called the transport plans between \(_{0}\) and \(_{1}\). The set \((_{0},_{1})\) is non-empty as it always contains the trivial plan \(_{0}_{1}\). A minimizer \(^{*}\) of (1) always exists and is called an OT plan. If \(_{0}\) is absolutely continuous, then \(^{*}\) is deterministic: its conditional distributions are degenerate, i.e., \(^{*}(|x)=_{T^{*}(x)}\) for some \(T^{*}:\) (OT map).

**Entropic OT formulation.** We use \(H()\) to denote the differential entropy of distribution \(\) and \((||^{})\) to denote the Kullback-Leibler divergence between distributions \(\) and \(^{}\). Two most popular entropic OT formulations regularize (1) with the entropy \(H()\) or KL-divergence between plan \(\) and the trivial plan \(_{0}_{1}\), respectively (\(>0\)):

\[_{(_{0},_{1})}_{ }}{2}d(x,y)- H(),\] (2)

\[_{(_{0},_{1})}_{ }}{2}d(x,y)+(||_{ 0}_{1}).\] (3)

Since \((_{0},_{1})\), it holds that \((||_{0}_{1})\!=\!-H()\!+\!H( _{0})\!+\!H(_{1})\), i.e., both formulations are equal up to an additive constant when \(_{0}\) and \(_{1}\) are absolutely continuous. The minimizer of (2) is **unique** since the functional is strictly convex in \(\) thanks to the strict convexity of \(H()\). This unique minimizer \(^{*}\) is called the entropic OT plan.

### Schrodinger Bridge (SB)

**SB with the Wiener prior.** Let \(\) be the space of \(^{D}\) valued functions of time \(t\) describing some trajectories in \(^{D}\), which start at time \(t=0\) and end at time \(t=1\). We use \(()\) to denote the set of probability distributions on \(\). We use \(dW_{t}\) to denote the differential of the standard Wiener process. Let \(W^{}()\) be the Wiener process with the variance \(\) which starts at \(_{0}\). This diffusion process can be represented via the following stochastic differential equation (SDE):

\[W^{}:dX_{t}=dW_{t}, X_{0}_{0}.\] (4)We use \((T||Q)\) to denote the Kullback-Leibler divergence between stochastic processes \(T\) and \(Q\). The Schrodinger Bridge problem was initially proposed in 1931/1932 by Erwin Schrodinger . It can be formulated as follows [11, Problem 4.1]:

\[_{T(_{0},_{1})}(T||W^{}),\] (5)

where \((_{0},_{1})()\) is the set of probability distributions on \(\) having marginal distributions \(_{0}\) and \(_{1}\) at \(t=0\) and \(t=1\), respectively. Thus, the Schrodinger Bridge problem implies finding a stochastic process \(T\) with marginal distributions \(_{0}\) and \(_{1}\) at times \(t=0\) and \(t=1\), respectively, which has minimal KL divergence with the prior process \(W^{}\).

**Link to OT problem.** Here we recall how Scrodinger Bridge problem (5) relates to entropic OT problem (2). _This relation is well known (see, e.g.,  or [11, Problem 4.2]), and we discuss it in detail because our proposed approach (S4) is based on it._ Let \(^{T}\) denote the joint distribution of a stochastic process \(T\) at time moments \(t=0,1\) and \(_{0}^{T}\), \(_{1}^{T}\) denote its marginal distributions at time moments \(t=0,1\), respectively. Let \(T_{|x,y}\) denote the stochastic processes \(T\) conditioned on values \(x,y\) at times \(t=0,1\), respectively. One may decompose \((T||W^{})\) as [48, Appendix C]:

\[(T||W^{})=(^{T}||^{W^{}})+_{ }(T_{|x,y}||W^{}_{|x,y})d^{T} (x,y),\] (6)

i.e., KL divergence between \(T\) and \(W^{}\) is a sum of two terms: the first represents the similarity of the processes at start and finish times \(t=0\) and \(t=1\), while the second term represents the similarity of the processes for intermediate times \(t(0,1)\) conditioned on the values at \(t=0,1\). For the first term, it holds (see Appendix A or [11, Eqs 4.7-4.9]):

\[(^{T}||^{W^{}})=_{} }{2}d^{T}(x,y)-H(^{T})+C,\] (7)

where \(C\) is a constant which depends only on \(_{0}\) and \(\). In [32, Proposition 2.3], the authors show that if \(T^{*}\) is the solution to (5), then \(T^{*}_{|x,y}=W^{}_{|x,y}\). Hence, one may optimize (5) over processes \(T\) for which \(T_{|x,y}=W^{}_{|x,y}\) for every \(x,y\) and set the last term in (6) to zero. In this case:

\[_{T(_{0},_{1})}(T||W^{ })=_{T(_{0},_{1})}(^{T}|| ^{W^{}})=_{^{T}(_{0},_{1})}(^{T}||^{W^{}}).\] (8)

i.e., it suffices to optimize only over joint distributions \(^{T}\) at time moments \(t=0,1\). _Hence, minimizing (8) is equivalent (up to an additive constant C) to solving EOT (2) divided by \(\), and their respective solutions \(^{T^{*}}\) and \(^{*}\) coincide._ Thus, SB problem (5) can be simplified to the entropic OT problem (2) with entropy coefficient \(\).

**Dual form of EOT**. Entropic OT problem (8) has several dual formulations. Here we recall the one which is particularly useful to derive our algorithm. The dual formulation follows from the weak OT theory [7, Theorem 1.3] and we explain it in detail in the proof of Lemma B.3:

\[_{(_{0},_{1})}(||^{W^{ }})=_{}_{}^{C}(x)d_{0}( x)+_{}(y)d_{1}(y)},\]

where \(^{C}(x)}}{{=}}_{_{ 2}()}||^{W^{}}(|x) -_{}(y)d(y)}\). The \(\) here is taken over \(\) belonging to the set of functions

\[_{b,2}()}}{{=}}\{ : u,v,w:\,u\|\|^{2}+v() w\},\]

i.e., \(\) should be continuous with mild boundness assumptions.

**Dynamic SB problem (DSB)**. It is known that the solution to the SB problem (5) belongs to the class \((_{0})\) of finite-energy diffusions \(T_{f}\)[32, Proposition 4.1] which are given by:

\[T_{f}:dX_{t}=f(X_{t},t)dt+dW_{t}, X_{0}_{0}, _{T_{f}}[_{0}^{1}||f(X_{t},t)||^{2}dt]<,\] (9)

where \(f:^{D}^{D}\) is the drift function. The last inequality in (9) means that \(T_{f}\) is a finite-energy diffusion. Hence, optimizing only over finite-energy diffusions rather than all possible processes is enough to solve SB problem (5). For finite-energy diffusions, it is possible to rewrite the optimization objective. One can show that \((T_{f}||W^{})\) between processes \(T_{f}\) and \(W^{}\) is :

\[(T_{f}||W^{})=_{T_{f}}[_{0}^ {1}||f(X_{t},t)||^{2}dt].\] (10)

By substituting (10) in (5), SB problem reduces to the following problem [11, Problem 4.3]:

\[_{T_{f}(_{0},_{1})}(T_{f}||W^{ })=_{T_{f}(_{0},_{1})}_{T_{f}}[_{0}^{1}||f(X_{t},t)||^{2}dt],\] (11)

where \((_{0},_{1})()\) is the set of finite-energy diffusion on \(\) having marginal distributions \(_{0}\) and \(_{1}\) at \(t=0\) and \(t=1\), respectively. Note that \((_{0},_{1})(_{0}, _{1})\). Since the problem (11) is the equivalent reformulation of (5), its optimal value also equals (8). However, solving this problem, as well as (8) is challenging as it is still hard to satisfy the boundary constraints.

## 3 Related Work

In this section, we overview the existing methods to compute the OT plan or map. To avoid any confusion, we emphasize that popular Wasserstein GANs  compute only the OT cost but not the OT plan and, consequently, are out of scope of the discussion.

**Discrete OT.** The majority of algorithms in computational OT are designed for the discrete setting where the inputs \(_{0},_{1}\) have finite supports. In particular, the usage of entropic regularization (2) allows to establish efficient methods  to compute the entropic OT plan between discrete distributions with the support size up to \(10^{5}\)-\(10^{6}\) points, see  for a survey. For larger support sizes, such methods are typically computationally intractable.

**Continuous OT.** Continuous methods imply computing the OT plan between distributions \(_{0}\), \(_{1}\) which are accessible by empirical samples. Discrete methods "as-is" are not applicable to the continuous setup in high dimensions because they only do a stochastic _matching_ between the train samples and do not provide out-of-sample estimation. In contrast, continuous methods employ neural networks to explicitly or implicitly _learn_ the OT plan or map. As a result, these methods can map unseen samples from \(_{0}\) to \(_{1}\) according to the learned OT plan or map.

There exists many methods to compute OT plans [51; 36; 38; 26; 28; 27; 42; 16; 19; 18] but they consider only unregularized OT (1) rather than the entropic one (2). In particular, they mostly focus on computing the deterministic OT plan (map) which may not exist. Recent works [31; 30] design algorithms to compute OT plans for weak OT [20; 7]. Although weak OT technically subsumes entropic OT, these works do not cover the entropic OT (2) because there is no simple way to estimate the entropy from samples. Below we discuss methods specifically for **EOT** (2) and **DSB** (11).

### Continuous Entropic OT

In LSOT , the authors solve the dual problem to entropic OT (2). The dual potentials are then used to compute the _barycentric projection_\(x_{}y\,d^{*}(y|x)\), i.e., the first conditional moment of the entropic OT plan. This strategy may yield a deterministic approximation of \(^{*}\) for small \(\) but does not recover the entire plan itself.

In [14, Figure 3], the authors show that the barycentric projection leads to the averaging artifacts which make it impractical in downstream tasks such as the unpaired image super-resolution. To solve these issues, the authors propose a method called SCONES. It recovers the entire conditional distribution \(^{*}(y|x)\) of the OT plan \(^{*}\) from the dual potentials. Unfortunately, this is costly. During the training phase, the method requires learning a score-based model for the distribution \(_{1}\). More importantly, during the inference phase, one has to run the Langevin dynamic to sample from \(^{*}(y|x)\).

The optimization of the above-mentioned entropic approaches requires evaluating the exponent of large values which are proportional to \(^{-1}\)[45, Eq. 7]. Due to this fact, those methods are **unstable** for small values \(\) in (2). In contrast, our proposed method (SS4) resolves this issue: technically, it works even for \(=0\).

### Approaches to Compute Schrodinger Bridges

Existing approaches to solve DSB mainly focus on generative modeling applications (_noise \(\) data_). For example, FB-SDE  utilizes data likelihood maximization to optimize the parameters of 

[MISSING_PAGE_FAIL:5]

in EOT. This term is replaced by the energy of the process \(_{T_{f}}_{0}^{1}||f(X_{t},t)||^{2}dt\) which can be straightforwardly estimated from the samples of \(T_{f}\), allowing to establish a computational algorithm.

### Practical Optimization Procedure

To solve (12), we parametrize drift function \(f(x,t)\) of the process \(T_{f}\) and potential \((y)\)4 by neural nets \(f_{}:^{D}^{D}\) and \(_{}:^{D}\). We consider the following maximin problem:

\[_{}_{T_{f_{}}}_{T_{f_ {}}}[_{0}^{1}||f_{}(X_{t},t)||^{2}dt]+_{}_ {}(y)d_{1}(y)-_{}_{}(y)d_{1}^{T_{f_{ }}}(y)}.\] (14)

We use standard Euler-Maruyama (Eul-Mar) simulation (Algorithm 2 in Appendix C) for sampling from the stochastic process \(T_{f}\) by solving its SDE (9). To estimate the value of \(_{T_{f}}[_{0}^{1}||f(X_{t},t)||^{2}dt]\) in (14), we utilize the mean value of \(||f(x,t)||^{2}\) over time \(t\) of trajectory \(X_{t}\) that is obtained during the simulation by Euler-Maruyama algorithm (Appendix C). We train \(f_{}\) and \(_{}\) by optimizing (12) with the stochastic gradient ascent-descent by sampling random batches from \(_{0}\), \(_{1}\). The optimization procedure is detailed in Algorithm 1. We use \(f_{n,m}\) to denote the drift at time step \(n\) for the \(m\)-th object of the input sample batch. We use the averaged of the drifts as an estimate of \(_{0}^{1}||f(X_{t},t)||^{2}dt\) in the training objective.

**Remark.** For the image tasks (SS5.3, SS5.4), we find out that using a slightly different parametrization of \(T_{f}\) considerably improves the quality of our Algorithm, see Appendix F.

It should be noted that the term \(_{T_{f}}[_{0}^{1}||f(X_{t},t)||^{2}dt]\) is not multiplied by \(\) in the algorithm since this does not affect the optimal \(T_{f^{*}}\) solving the inner optimization problem, see Appendix D.

**Relation to GANs.** At the first glance, our method might look like a typical GAN as it solves a maximin problem with the "discriminator" \(_{}\) and SDE "generator" with the drift \(f_{}\). Unlike GANs, in our saddle point objective (12), optimization of "generator" \(T_{f}\) and "discriminator" \(\) are swapped, i.e., "generator" is adversarial to "discriminator", not vise versa, as in GANs. For further discussion of differences between saddle point objectives of neural OT/GANs, see [31, SS4.3], [42, SS4.3], .

### Error Bounds via Duality Gaps

Our algorithm solves a maximin optimization problem and recovers some _approximate_ solution \((,T_{f})\). Given such a pair, it is natural to wonder how close is the recovered \(T_{f}\) to the optimal \(T_{f^{*}}\). Our next result sheds light on this question via bounding the error with the _duality gaps_.

**Theorem 4.3** (Error analysis via duality gaps).: _Consider a pair (\(\), \(T_{f}\)). Define the duality gaps, i.e., errors of solving inner and outer optimization problems by:_

\[_{1}}}}{{=}}(,T_{f})-_{T_{f}}(,T_{f}), _{2}}}}{{=}}_{}_{T _{f}(_{0})}(,T_{f})-_{T_{f}} (,T_{f}).\] (15)

_Then it holds that_

\[_{}(T_{f},T_{f^{*}})+_{2}}, _{}(^{T_{f}},^{T_{f^{*}}})+_{2}},\] (16)

_where we use \(_{}(,)\) to denote the total variation norm (between the processes or plans)._

**Relation to prior works**. There exist deceptively similar results, see [38, Theorem 3.6], [42, Theorem 4.3], [16, Theorem 4], [6, Theorem 3]. _None of them are relevant to our EOT/DSB case._

In , , , the authors consider maximin reformulations of unregularized OT (1), i.e., non-entropic. Their result requires the potential \(\) (\(f,\) in their notation) to be a convex function which in practice means that one has to employ ICNNs  which have poor expressiveness . Our result is free from such assumptions on \(\). In , the authors consider general OT problem  and require the general cost functional to be strongly convex (in some norm). Their results also do not apply to our case as the (negative) entropy which we consider is not strongly convex.

## 5 Experimental Illustrations

In this section, we qualitatively and quantitatively illustrate the performance of our algorithm in several entropic OT tasks. Our proofs apply only to EOT (\(>0\)), but for completeness, we also present results \(=0\), i.e., unregularized case (1). Furthermore, we test our algorithm with \(=0\) on the Wasserstein-2 Benchmark , see Appendix J. We also demonstrate the extension of our algorithm to costs other than the squared Euclidean distance in Appendix I. The implementation details are given in Appendices E, F and G. The code is written in PyTorch and is publicly available at

https://github.com/ngushchin/EntropicNeuralOptimalTransport

### Toy 2D experiments

Here we give qualitative examples of our algorithm's performance on toy 2D pairs of distributions. We consider two pairs \(_{0},_{1}\): _Gaussian \(\) Swiss Roll, Gaussian \(\) Mixture of 8 Gaussians_. We provide qualitative results in Figure 2 and Figure 5 (Appendix E), respectively. In both cases, we provide solutions of the problem for \(=0,0.01,0.1\) and sample trajectories. For \(=0\), all the trajectories are straight lines as they represent solutions for non-regularized OT (1), see [43, SS5.4]. For bigger \(\), trajectories, as expected, become more noisy and less straight.

### High-dimensional Gaussians

For general continuous distributions \(_{0},_{1}\), the ground truth solution of entropic OT (2) and DSB (11) is unknown. This makes it challenging to assess how well does our algorithm solve these problems. Fortunately, when \(_{0}\) and \(_{1}\) are Gaussians, there exist _closed form solutions_ of these related problems, see  and . Thus, to quantify the performance of our algorithm, we consider entropic OT problems in dimensions \(D\{2,16,64,128\}\) with \(=1\) for Gaussian \(_{0}=(0,_{0})\), \(_{1}=(0,_{1})\). We pick \(_{0}\), \(_{1}\) at random: their eigenvectors are uniformly distributed on the unit sphere and eigenvalues are sampled from the loguniform distribution on \([- 2, 2]\).

Figure 2: _Gaussian \(\)Mix of 8 Gaussians_. The process learned with ENOT **(ours)** for \(\!=\!0,0.01,0.1\).

**Metrics**. We evaluate **(a)** how precisely our algorithm fits the target distribution \(_{1}\) on \(^{D}\); **(b)** how well it recovers the entropic OT plan \(^{*}\) which is a Gaussian distribution on \(^{D}^{D}\); **(c)** how accurate are the learned marginal distributions at intermediate times \(t=0,,,1\).

In each of the above-mentioned cases, we compute the BW\({}_{2}^{2}\)-UVP [29, SS5] between the learned and the ground truth distributions. For two distributions \(\) and \(\), it is the Wasserstein-2 distance between their Gaussian approximations which is further normalized by the variance of the distribution \(\):

\[_{2}^{2},=()}_{2}^{2}((_{ },_{}),(_{},_{}) ).\] (17)

We estimate the metric by using \(10^{5}\) samples.

**Baselines**. We compare our method ENOT with LSOT , SCONES , MLE-SB , DiffSB, FB-SDE  (two algorithms, A and J). Results are given in Tables 1, 2 and 3. LSOT and SCONES solve EOT without solving SB, hence there are no results in Table 3 for these methods. Our method achieves low BW\({}_{2}^{2}\)-UVP values indicating that it recovers the ground truth process and entropic plan fairly well. The competitive methods have good results in low dimensions, however they perform worse in high dimensions. Importantly, our methods scores the best results in recovering the OT plan (Table 2) and the marginal distributions of the Schrodinger bridge (Table 3). To illustrate the stable convergence of ENOT, we provide the plot of BW\({}_{2}^{2}\)-UVP between the learned plan and the ground truth plan for ENOT during training in Figure 6 (Appendix E).

### Colored MNIST

In this section, we test how the entropy parameter \(\) affects the stochasticity of the learned plan in higher dimensions. For this, we consider the entropic OT problem between colorized MNIST digits of classes "2" (\(_{0}\)) and "3" (\(_{1}\)).

**Effect of parameter \(\).** For \(=0,1,10\), we learn our Algorithm 1 on the _train_ sets of digits "2" and "3". We show the translated _test_ images in Figures 2(a), 2(b) and 2(c), respectively. When \(=0\), there is no diversity in generated "3" samples (Figure 2(a)), the color remains since the map tried to minimally change the image in the RGB pixel space. When \(=1\), some slight diversity in the shape of "3" appears but the color of the input "2" is still roughly preserved (Figure 2(c)). For higher \(\), the diversity of generated samples becomes clear (Figure 2(c)). In particular, the color of "3" starts to slightly deviate from the input "2". That is, increasing the value \(\) of the entropy term in (2) expectedly leads to bigger stochasticity in the plan. We add the conditional LPIPS variance [24, Table 1] of generated samples for test datasets by ENOT to show how diversity changes for different \(\) (Table 4). We provide examples of trajectories learned by ENOT in Figure 7 (Appendix F).

**Metrics and baselines.** We compare our method ENOT with SCONES , and DiffSB  as these are the only methods which the respective authors applied for _data\(\)data_ tasks. To evaluate the results, we use the FID metric  which is the Bures-Wasserstein (Freschet) distance between thedistributions after extracting features using the InceptionV3 model . We measure test FID for every method and present the results and qualitative examples in Figure 3. There are no results for SCONES with \(=0,1,10\), since it **is not applicable** for such reasonably small \(\) due to computational instabilities, see [14, SS5.1]. DiffSB  can be applied for small regularization \(\), so we test \(=1,10\). By the construction, this algorithm is not suitable for \(=0\).

Our ENOT method outperforms the baselines in FID. DiffSB  yield very high FID. This is presumably due to instabilities of DiffSB which the authors report in their sequel paper . SCONES yields reasonable quality but due to high \(=25,100\) the shape and color of the generated images "3" starts to deviate from those of their respective inputs "2".

For completeness, we provide the results of the stochastic _matching_ of the **test parts** of the datasets by the discrete OT for \(=0\) and EOT  for \(=1,10\) (Figures 2(f), 2(g), 2(h)). This is not the out-of-sample estimation, obtained samples "3" are just **test** samples of "3" (this setup is _unfair_). Discrete OT is _not a competitor_ here as it does not generate new samples and uses _target_ test samples. Still it gives a rough estimate what to expect from the learned plans for increasing \(\).

### Unpaired Super-resolution of Celeba Faces

For the large-scale evaluation, we adopt the experimental setup of SCONES . We consider the problem of unpaired image super-resolution for the \(64 64\) aligned faces of CelebA dataset .

We do the _unpaired_ train-test split as follows: we split the dataset into 3 parts: 90k (train A1), 90k (train B1), 20k (test C1) samples. For each part we do \(2\) bilinear downsample and then \(2\) bilinear upsample to degrade images but keep the original size. As a result, we obtain degraded parts A0, B0, C0. For training in the unpaired setup, we use parts A0 (degraded faces, \(_{0}\)) and B1 (clean faces, \(_{1}\)). For testing, we use the hold-out part C0 (unseen samples) with C1 considered as the reference.

We train our model with \(0,1,10\) to and test how it restores C1 (Figure 3(a)) from C0 images (Figure 3(b)) and present the qualitative results in Figures 3(f), 3(g), 3(h). We provide examples of trajectories learned by ENOT in Figure 1.

**Metrics and baselines.** To quantify the results, as in , we compute the FID score  between the sets of mapped C0 images and C1 images (Table 5). The FID of ENOT is better than FID values of the other methods, but increases with \(\) probably due to the increasing variance of gradients during training. As in SS5.1 and SS5.3, the diversity of samples increases with \(\). Our method works with small values of \(\) and provides **reasonable** amount of diversity in the mapped samples which grows with \(\) (Table 4). As the baseline among other methods for EOT we consider only SCONES, as it is the only EOT/DSB algorithm which has been applied to _data\(\)data_ task at \(64 64\) resolution. At the same time, we emphasize that SCONES **is not applicable** for small \(\) due to instabilities, see [14, SS5.1]. This makes it **impractical**, as due to high \(\), its produces up-scaled images (Figures 3(c)) are

  \(\) & **0** & **1** & **10** \\ 
**Colored MNIST** & \(0\) & \(5.3 10^{-3}\) & \(2.0 10^{-2}\) \\ 
**Celeba** & \(0\) & \(3.4 10^{-2}\) & \(5.1 10^{-2}\) \\  

Table 4: LPIPS variability of ENOT samples.

Figure 3: Samples of colored MNIST obtained by ENOT **(ours)** and DOT for different \(\).

nearly random and do not reflect the attributes of the input images (Figure 3(a)). We do not provide results for DiffSB  since it already performs bad on Colored MNIST (SS5.3) and the authors also did not consider any image-to-image apart of grayscale 28x28 images.

For completeness, we present results on this setup for other methods, which do not solve EOT: ICNN-based OT  and AugCycleGAN . ICNN (4d) learns a deterministic map. AugCycleGAN (4e) learns a stochastic map, but the generated samples differ only by brightness.

## 6 Discussion

**Potential impact.** There is a lack of scalable algorithms for learning continuous entropic OT plans which may be used in _data\(\)data_ practical tasks requiring control of the diversity of generated samples. We hope that our results provide a new direction for research towards establishing scalable and efficient methods for entropic OT by using its connection with SB.

**Potential social impact.** Like other popular methods for generating images, our method can be used to simplify the work of designers with digital images and create new products based on it. At the same time, our method may be used for creating fake images just like the other generative models.

**Limitations.** To simulate the trajectories following SDE (9), we use the Euler-Maruyama scheme. It is straightforward but may be imprecise when the number of steps is small or the noise variance \(\) is high. As a result, for large \(\), our Algorithm 1 may be computationally heavy due to the necessity to backpropagate through a large computational graph obtained via the simulation. Employing time and memory efficient SDE integration schemes is a promising avenue for the future work.

Acknowledgements. This work was partially supported by Skoltech NGP program (SkoltechMIT joint project).

Figure 3: Faces produced by ENOT **(ours)** and SCONEs for various \(\).

Figure 3(a) shows test degraded images (C0), 4b â€“ their original high-resolution counterparts (C1).

 
**Method** & **ENOT**\(=0\) & **ENOT**\(=1\) & **ENOT**\(=10\) & SCONES , \(=100\) & AugCycleGAN  & ICNN  \\ 
**FID** & **3.78** & **7.63** & **14.8** & 18.88 & 15.2 & 22.2 \\  

Table 5: Test FID values of various methods in unpaired super-resolution of faces experiment.