# Generalized Belief Transport

Junqi Wang

Department of Math & CS

Rutgers University

Newark, NJ, 07102

junqi.wang@rutgers.edu

&Pei Wang

Department of Math & CS

Rutgers University

Newark, NJ, 07102

peiwang@rutgers.edu

&Patrick Shafto

Department of Math & CS

Rutgers University

Newark, NJ, 07102

shafto@rutgers.edu

###### Abstract

Human learners have ability to adopt appropriate learning approaches depending on constraints such as prior on the hypothesis, urgency of decision, and drift of the environment. However, existing learning models are typically considered individually rather than in relation to one and other. To build agents that have the ability to move between different modes of learning over time, it is important to understand how learning models are related as points in a broader space of possibilities. We introduce a mathematical framework, Generalized Belief Transport (GBT), that unifies and generalizes prior models, including Bayesian inference, cooperative communication and classification, as parameterizations of three learning constraints within Unbalanced Optimal Transport (UOT). We visualize the space of learning models encoded by GBT as a cube which includes classic learning models as special points. We derive critical properties of this parameterized space including proving continuity and differentiability which is the basis for model interpolation, and study limiting behavior of the parameters, which allows attaching learning models on the boundaries. Moreover, we investigate the long-run behavior of GBT, explore convergence properties of models in GBT mathematical and computationally, document the ability to learn in the presence of distribution drift, and formulate conjectures about general behavior. We conclude with open questions and implications for more unified models of learning.

Learning and inference are subject to internal and external constraints. Internal constraints include the availability of relevant prior knowledge. External constraints include the availability of time to accumulate evidence versus the need make the best decision now or environmental non-stationarity. Standard models of machine learning tend to view different constraints as different problems, which impedes development of unified learning agents.

These internal and external constraints map onto classic dichotomies in machine learning. Availability of prior knowledge maps onto the Frequentist-Bayesian dichotomy in which the latter uses prior knowledge as a constraint on posterior beliefs, while the former does not. Within Bayesian theory, a classic debate pertains to uninformative, or minimally informative, settings of priors (Jeffreys, 1946; Robert et al., 2009). Availability of time to accumulate evidence informs the use of generative versus discriminative approaches (Ng and Jordan, 2001), and static or drift/dynamic models (Dagum et al., 1992; Murphy, 2002). Combining constraints on probability of beliefs and costs of data models cooperative communication (Wang et al., 2020).

Learning agents must interpolate between modes of reasoning as necessary given the constraints of the moment. Imagine observing an agent behaving in an environment. As an observer, one may wish to learn about the environment from the agent's actions. However, any inferences depend on one's model of the agent and their constraints. How is the agent updating their beliefs? Do they have stable goals, or are they changing over time? Perhaps the agent is selecting actions to communicate what they know? In order to draw inferences over these possibilities, one must parameterize the space,ideally in such a way one could optimize over the possibilities. Indeed, in order to implement these possibilities, the agent _themself_ must parameterize the space in order to interpolate between classic dichotomies such as Bayesian and frequentist, static and dynamic environments, and helpful versus neutral agent, given constraints.

We introduce Generalized Belief Transport (GBT), based on Unbalanced Optimal Transport (Sec. 1), which paramterizes and interpolates between known reasoning modes (Sec. 2.2), with four major contributions. First, we prove continuity in the parameterization and differentiability on the interior of the parameter space (Sec. 2.1). Second, we analyze the behavior under variations in the parameter space (Sec. 2.3). Third, we study sequential learning, where learners may (not) track the empirically observed data frequencies in (Sec. 3). Fourth, we investigate predictive performance under environmental drift (Sec. 4).

**Notations.**\(_{ 0}\) denotes the non-negative reals. Vector \(=(1,,1)\). The \(i\)-th component of vector \(v\) is \(v(i)\). \((A)\) is the set of probability distributions over \(A\). For a matrix \(M\), \(M_{ij}\) represents its \((i,j)\)-th entry, \(M_{(i,-)}\) denotes its \(i\)-th row, and \(M_{(-,j)}\) denotes its \(j\)-th column. Probability is \((\ \ )\).

## 1 Learning as a problem of unbalanced optimal transport

Consider a general learning setting: an agent, which we call a **learner**, updates their belief about the world based on observed data subject to constraints. There is a finite set \(=\{d^{1},,d^{n}\}\) of all possible data, that defines the interface between the learner and the world. The world is defined by a true hypothesis \(h^{*}\), whose meaning is captured by a probability mapping \((d|h^{*})\) onto observable data. For instance, the world can either be the environment in classic Bayesian inference (Murphy, 2012) or a **teacher** in cooperative communication (Wang et al., 2020).

A learner is equipped with a set of hypotheses \(=\{h^{1},,h^{m}\}\) which may _NOT_ contain \(h^{*}\); an initial belief on the hypotheses set, denoted by \(_{0}()\); and a non-negative cost matrix \(C=(C_{ij})_{m n}\), where \(C_{ij}\) measures the underlying cost of mapping \(d^{i}\) into \(h^{j}\)1. The cost matrix can be derived from other matrices that record the relation between \(\) and \(\), such as likelihood matrices in classic Bayesian inference or consistency matrices in cooperative communication (see details in Section 2.2).This setting reflects an agent's learning constraints: pre-selected hypotheses, and the relations between them and the communication interface (data set).

A learner observes data in sequence. At round \(k\), the learner observes a data \(d_{k}\) that is sampled from \(\) by the world according to \((d|h^{*})\). Then the learner updates their beliefs over \(\) from \(_{k-1}\) to \(_{k}\) through a _learning scheme_, where \(_{k-1},_{k}()\). For instance, in Bayesian inference, the learning scheme is defined by Bayes rule; while in discriminative models, the learning scheme is prescribed by a code book.

The learner transforms the observed data into a belief on hypotheses \(h\) with a minimal cost, subject to appropriate constraints, with the goal of learning the exact map \((d|h^{*})\). We can naturally cast this learning problem as Unbalanced Optimal Transport.

### Unbalanced Optimal Transport

Unbalanced Optimal Transport (UOT), introduced by Liero et al. (2018), is a generalization of (entropic) Optimal Transport (Villani, 2008; Cuturi, 2013; Peyre and Cuturi, 2019), that relaxes the marginal constraints. Formally, for non-negative scalar parameters \(=(_{P},_{p},_{})\), the _UOT plan_ is,

\[P^{}(C,,)=*{arg\,min}_{P(_{ 0 })^{n m}}\{ C,P-_{P}H(P)+_{} (P|)+_{}(P^{T}|)\}. \]

Here, \( C,P=_{i,j}C_{ij}P_{ij}\) is the inner product between \(C\) and \(P\), \(H(P)=-_{ij}P_{ij}( P_{ij}-1)\) is the _entropy_ of \(P\), and \((|):=_{i}(a_{i}(a_{i}/b_{i})-a_{i}+b_{i})\) is the Kullback-Leibler divergence between vectors. It is shown in Chizat et al. (2018) that UOT plans can be solved efficiently via Algorithm 1 : Given a cost \(C\), \(P^{}\) can be obtained by applying \((,,)\)-unbalanced Sinkhorn scaling on \(K^{}:=e^{-}C}=(e^{-}C_{ij}}) _{m n}\), with convergence rate \(}(})\)(Pham et al., 2020).

**Proposition 1**.: _The UOT problem with cost matrix \(C\), marginals \(,\) and parameters \(=(_{P},_{},_{})\) generates the same UOT plan as the UOT problem with \(tC\), \(\), \(\), \(t=(t_{P},t_{},t_{})\) for any \(t(0,)\). Therefore, the analysis on \(\) and \(t\) are the same for general cost \(C\)._

Thus a positive common factor on \(C,_{P},_{},_{}\) does not affect the solution of Eq. (1). Therefore, for the later analysis, we fix \(_{P}=1\) unless otherwise stated.

**Framework: Generalized Belief Transport (GBT).** Learning, efficiently transport one's belief with constraints, is naturally a UOT problem, i.e. a _Generalized Belief Transport_. Each round, a learner, defined by a choice of \(=(_{P},_{},_{})\), updates their beliefs as follows. Let \(_{k-1},_{k-1}\) be the learner's estimations of the data distribution and the belief over hypotheses \(\) after round \(k-1\), respectively. At round \(k\), the learner first improves their estimation of the mapping between \(\) and \(\), denoted by \(M_{k}\), through solving the UOT plan Eq. (1) with \((C,_{k-1},_{k-1})\), i.e. \(M_{k}=P^{}(C,_{k-1},_{k-1})\). Then with data observation \(d_{k}\), the learner updates their beliefs over \(\) using corresponding row of \(M_{k}\), i.e. suppose \(d_{k}=d^{i}\) for some \(d^{i}\), the learner's belief \(_{k}\) is defined to be the row normalization of the \(i\)-th row of \(M_{k}\). Finally, the learner updates their data distribution to \(_{k}\) by increment of the \(i\)-th element of \(_{k-1}\), see Algorithm 2.

```
input:\(C\), \(\), \(\), \(=(_{P},_{},_{})\), \(N\) stopping condition \(\) output:\(P^{}(C,,)\) initialize:\(=(-_{P}C)\), \(^{(0)}=_{m}\) while\(k<N\) and not\(\)do \(^{(k)}^{(k-1)}} ^{}{_{}+_{P}}}\), \(^{(k)}^{T}^{(k) }}^{}{_{}+_{P}}}\) endwhile \(P^{}(C,,)=(u)(v)\)
```

**Algorithm 1** Unbalanced Sinkhorn Scaling

## 2 Generalized Belief Transport

Many learning models--including Bayesian inference, Frequentist inference, Cooperative learning, and Discriminative learning--are unified under our GBT framework under choice of \(\). In this section, we focus on the single-round behavior of the GBT model, i.e., given a pair of marginals \((,)\), how different learners update beliefs with a single data observation. We first visualize the entire learner set as a cube (in terms of parameters), see Figure 1. Then, we study the topological properties of the learner set through continuous deformations of parameters \(\). In particular, we show that existing models including Bayesian inference, cooperative inference and discriminative learning are learners with parameters \((1,0,)\), \((1,,)\) and \((0,,)\) respectively in our UOT framework.

### The parameter space of GBT model

The space of learners in GBT are parameterized by three regularizers for the underlying UOT problem (1): \(_{P}\), \(_{}\) and \(_{}\), each ranges in \([0,)\). Therefore, the constraint space for GBT is \(^{3}_{ 0}\), with the standard topology. When \(C\), \(\) and \(\) are fixed (assume \(^{m}_{>0}\)), the map \(=(_{P},_{},_{})(P^{})\) bears continuous properties:

**Proposition 2**.: 2 _The UOT plan \(P\) in Equation (1), as a function of \(\), is continuous in \((0,)[0,)\)2. Furthermore, \(P\) is differentiable with respect to \(\) in the interior of its domain \(^{3}_{ 0}\)._

Continuity on \(\) provides the basis for interpolation between different learning agents. The proof of Proposition 2 also implies the continuity on \(\) and \(\). Further, towards the boundaries of the parameter space (where theories like Bayesian, Cooperative Communication live in), we show:

**Proposition 3**.: _Let \(s_{P}\), \(s_{}\), \(s_{} 0\) be arbitrary finite numbers, the following holds: (1) The limit of \(P^{}\) exists as \(\) approaches \((,s_{},s_{})\). In fact, \(_{(,s_{},s_{})}P^{}_{ij}=1\) for all \(i,j\). (2) As \((s_{P},,s_{})\), \(P^{}\) converges to the solution to_

\[ C,P-s_{P}H(P)+s_{}(P^{T}|), P=,\]

_(3) Similarly, as \((s_{P},s_{},)\), \(P^{}\) converges to the solution to_

\[ C,P-s_{P}H(P)+s_{}(P|),P^{T}=.\]

_(4) And when \((s_{P},,)\), the matrix \(P^{}\) converges to the EOT solution:_

\[ C,P-s_{P}H(P),P^{T}= P=.\]

_(5) When \((,,s_{}),(,s_{},)\) or \((,,)\), the limit does not exist, but the directional limits can be calculated._

The parameter space for GBT with its boundaries can be visualized in Fig. 1. Proposition 3 implies that the parameter space is \(=[0,]^{3}(\{(,,x):x[0,]\} \{(,x,):x[0,]\})\). In Fig. 1, segment \([0,)\) is mapped to \([0,1)\) by \(((x))\). Then boundaries are added to the image cube \([0,1)^{3}\). The dashed lines on top of the cube indicates limits that do not exist.

### Special points in the parameter space

**Bayesian Inference.** Given observed data, a Bayesian learner (BI)  derives posterior belief \((h|d)\) based on prior belief \((h)\) and likelihood matrix \((d|h)\), according to the Bayes rule. Intuitively, due to soft time constraint (\(_{P}=1\)), a Bayesian learner is a generative agent who puts a hard internal constraint on their prior belief (\(_{}=\)), and omits the estimated data distribution \(\) in the learning process, (\(_{}=0\)).

**Corollary 4**.: _Consider a UOT problem with \(=(h)\), \(()\). The optimal UOT plan \(P^{(1,_{},_{})}\) converges to \(\) as \(_{P}\). Frequentist Inference is a special case of GBT with \(=(1,0,)\)._

**Frequentist Inference.** A frequentist updates their belief from data observations by increasing the corresponding frequencies of datum. Intuitively, a frequentist is an agent who puts a hard constraint on the data distribution \(\) (\(_{}=\)), and omits prior knowledge \(\) (\(_{}=0\)) in a learning process without time constraint (\(_{P}=\)). Formally we show:

**Corollary 5**.: _Consider a UOT problem with \(()\), \(=(d)\). The optimal UOT plan \(P^{(_{P},,0)}\) converges to \(\) as \(_{P}\). Frequentist Inference is a special case of GBT with \(=(,,0)\)._

**Cooperative Communication.** Two cooperative agents, a teacher and a learner, are considered in , , . Cooperative learners (CI) draw inferences about hypotheses based on which data would be most effective for the teacher to choose Given a data observation, a cooperative learner derives an optimal plan \(L=(,)\) based on a prior belief \((h)\), a shared data distribution \((d)\) and a matrix \(M\) specifies the consistency between data and hypotheses (such as \(M_{ij}\) records the co-occurrence of \(d^{i}\) and \(h^{j}\)). Intuitively, a cooperative learner is also a generative agent who puts hard constraints on both data and hypotheses (\(_{}=,_{}=\)), and aims to align with the true belief asymptotically, (\(_{P}=1\)). Thus we show:

Figure 1: The parameter space \(\) of GBT. Parameters \(=(_{P},_{},_{})\) can take the value \(\), rendering the corresponding regularization to a strict constraint. The two dashed edges with \(_{P}=\) are not generally well-defined since the limits do not exist. The vertices corresponding to \(\), Frequentist (\(\)) and \(\) are the limits taken along the vertical edges. Given \((C,,)\) as shown in the left corner, each colored map plots each GBT learner (differ by constraints)’s estimation of the mapping between hypotheses and data (UOT plan).

**Corollary 6**.: _Let cost \(C=- M\), marginals \(=(h)\) and \(=(d)\). The optimal UOT plan \(P^{(1,_{},_{})}\) converges to the optimal plan \(L\) as \(_{}\) and \(_{}\). Cooperative Inference is a special case of GBT with \(=(1,,)\), which is exactly entropic Optimal Transport (Cuturi, 2013)._

**Discriminative learning.** A discriminative learner decodes an uncertain, possibly noise corrupted, encoded message, which is a natural bridge to information theory (Cover, 1999; Wang et al., 2020b). A discriminative learner builds an optimal map to hypotheses \(\) conditioned on observed data \(\). The map is perfect when, for all messages, encodings are uniquely and correctly decoded. Intuitively, a discriminative learner aims to quickly build a deterministic code book (implies \(_{P}=0\)) that matches the marginals on \(\) and \(\). We show that discriminative learner is GBT with \(=(0,,)\):

**Corollary 7**.: _Consider a UOT problem with cost \(C=-(d,h)\), \(m=n\), and marginals \(=\) are uniform. The optimal UOT plan \(P^{(_{P},_{},_{})}\) approaches to a diagonal matrix as \(_{},_{}\) and \(_{P} 0\). In particular, discriminative learner is a special case of GBT with \(=(0,,)\), which is exactly classical Optimal Transport (Villani, 2008)._

Many other interesting models are unified under GBT framework as well. GBT with \(=(0,,0)\) denotes Row Greedy learner which is widely used in Reinforcement learning community (Sutton and Barto, 2018); \(=(,,)\) yields \(\) which is independent coupling used in \(^{2}\)(Fienberg et al., 1970); \(=(_{P},_{},)\) is used for adaptive color transfer studied in (Rabin et al., 2014); and \(=(0,_{},_{})\) is UOT without entropy regularizer developed in (Chapel et al., 2021). Other points in the GBT parameter space are also of likely interest, past or future.

### General properties on the transportation plans

The general GBT framework builds a connection between the above theories, and the behavior of theory varies according to the change of parameters. In particular, each factor of \(=(_{P},_{},_{})\) expresses different constraints of the learner. Given \((C,,)\) as shown in the top-left corner of Fig. 1, we plot each learner's UOT plan with darker color representing larger elements.

\(_{P}\) controls a learner's learning horizon. When \(_{P} 0\), a learner's UOT plan is concentrated on a clear leading diagonal which allows them to make fast decisions. This corresponding to agents who are under the time pressure of making immediate decision, i.e. discriminative learner, or row greedy learner on the bottom of the cube (Fig. 1). Most of the time, one datum is enough to identify the true hypothesis and convergence is achieved within every data observation. When \(_{P}\), GBT converges to a reticent learner, such as learners on the top of the cube. Data do not constrain the true hypothesis, and learners draw their conclusions independent of the data. In between, GBT provides a generative (probabilistic) learner. When \(_{P}=1\), we have Bayesian learner and Cooperative learner, for whom data accumulate to identify the true hypothesis in a manner broadly consistent with probabilistic inference, and consistency is asymptotic.

\(_{}\) controls a learner's knowledge on the data distribution \(\). When \(_{}\), GBT converges to a learner who is aware of the data distribution and reasons about the observed data according to the probabilities/costs of possible outcomes. Examples include the Discriminative and Cooperative learners on the front of the cube. When \(_{} 0\), GBT converges to a learner who updates their belief without taking \(\) into consideration, such as Bayesian learners on the back of the cube, and the Tyrant who does not care about data nor cost and is impossible to be changed by anybody.

\(_{}\) controls the strength of the learner's prior knowledge. When \(_{} 0\), GBT converges to learners who utilizes no prior knowledge. Hence, they do NOT maintain beliefs over \(\), and draws their conclusions purely on the data distribution, such as a Frequentist learner on the left of the cube. When \(_{}\), GBT converges to a learner who enforces a strict prior such as Bayesian, Cooperative and Discriminative learners on the right of the cube. In particular, we show that:

**Proposition 8**.: _In GBT with \(_{}=\), cost \(C\) and current belief \(\). The learner updates \(\) with UOT plan in the same way as applying Bayes rule with likelihood from \(P^{}(C,,)\), and prior \(\)._

## 3 Sequential GBT - Static

The sequential GBT captures the asymptotic behavior of a learning problem \((C,_{0},h^{*})\). Static world where there exists a fixed true hypotheses \(h^{*}\) is considered in this section. Data is sampled from\(=(d|h^{*})\) (not necessarily related to some \(h\)). Then the learner follows GBT with cost \(C\), and parameter \(\), starts with a prior \(_{0}\), then in each round \(k\), applies GBT with \(_{k-1}\) and \(_{k-1}\) to generate \(_{k}\).

We investigate two cases. In the Preliminary sequential model (**PS**), we assume \(_{k}=\) for all \(k\). In practice, often a learner does not have access \(\). Instead, in each round the learner may choose to use the current observed data distribution \(_{k}(d)\) as an estimation of \(\), Thus we study the Real sequential model (**RS**) where \(_{k}[k]{a.s.}\).

In statistics, a model is said to be consistent when, for every fixed hypothesis \(h\), the model's belief \(\) over the hypotheses set \(\) converges to \(_{h}\) in probability as more data are sampled from \(=(d|h)\). Such consistency has been well studied for Bayesian Inference since Bernstein and von Mises and Doob (Doob, 1949), and recently demonstrated for Cooperative Communication (Wang et al., 2020). However, the challenge arises when one tries to learn a \(h^{*}\) that is not contained in the pre-selected hypothesis space \(\). It is not clear which \(h\) is the 'correct' target to converge to.

In this section, we demonstrate GBT's ability of learning new hypothesis. Analogize to consistency, the properties are stated directly in the language of posterior sequence \((_{k})_{k=1}^{}\) as random variables, focusing on whether the sequence converges (and in which sense), and how conclusive (how likely to a stable new hypothesis is learned) the sequence is.

For a learning problem \((C,_{0},h^{*})\), results in this section are organized based on different \(_{}\) values.

**Conclusive and Bayesian-style:**\(_{}=\). These learners are located on the right side of Cube Fig. 1. Many well-studied learners are in this class: Bayesian, Cooperative, Discriminative, Row Greedy etc. According to Prop 8, learners in this class perform "Bayesian" style learning.

When \(_{}=0\), i.e. the learners who update their belief without considering data distribution, (**PS**) and (**RS**) are essentially the same. The following holds:

**Theorem 9** ((Doob, 1949),(Wang et al., 2020)).: _In GBT sequential model (both (**PS**) and (**RS**)) with \(=(_{P},0,)\) where \(_{P}(0,)\), the sequence \(_{k}\) converges to some \(_{h}\) almost surely, \(h\) is the closest column of \(e^{-C/_{P}}\) to \(\) in the sense of KL-divergence._

When \(_{}=\), the models (**PS**) and (**RS**) present slightly different behaviors.

**Theorem 10** (**Ps**).: _When \(_{}=_{}=\), for the **PS** problem belief random variables of a GBT learner \((_{k})_{k}\) converge to the random variable \(Y\) in probability, where \(Y=_{h}_{0}(h)_{h}\) and \(Y\) is supported on \(\{_{h}\}_{h}\) with \((Y=_{h})=_{0}(h)\) for \(_{}=_{}=\) and \(_{P}(0,)\)._

**Corollary 11**.: _Given a fixed data sequence \(d_{i}\) sampled from \(\), if \(_{k}\) converges to \(_{h^{j}}\), then the \(j\)-th column of \(M_{k}\) converges to \(\)._

Thus a GBT learner, with access to the data distribution and using strict marginal constraints, converges to the true hypothesis mapping \(\) with probability \(1\). Moreover, the probability of which \(h\) is shaped into \(\) is determined by their prior \(_{0}\). That is, GBT learners converge to the truth by reforming one of their original hypotheses into the true hypothesis.

**Proposition 12**.: _When \(_{}=_{}=\), for the (**RS**) problem, the belief random variables of a GBT learner \((_{k})_{k}\) satisfies that for any \(s>0\), \(_{k}_{h}( (h)>1-s)=1\). As a consequence, \(M_{k}\) as the transport plan has a dominant column (\(h^{j}\)) with total weights \(>1-s\), and \(|(M_{k})_{ij}-_{k}(i)|<s\)._

In fact, as long as the sequence of \(_{k}\) as random variables converges to \(\) in probability, the above proposition holds. The limit \(_{k}_{h}( (h)>1-s)\) measures how conclusive the model is.

In contrast with standard Bayesian or other inductive learners, Proposition 12 shows that a GBT learner is able to learn _any_ hypothesis mapping \(=(d|h^{*})\) up to a given threshold \(s\) with probability \(1\). In addition to unifying disparate models of learning, GBT enables a fundamentally more powerful approach to learning by empirically monitoring the data marginal.

Fig. 2 illustrates convergence over learning problems and episodes. In each bar, we sample \(100\) learning problems \((C,_{0},h^{*})\) from Dirichlet distribution with hyperparameters the vector \(\). Then we sample \(1000\) data sequences (episodes) of maximal length \(N=10000\). The learner learns with Algo. 2 where the stopping condition \(\) is set to be \(_{h}(h)>1-s\) with \(s=0.001\). The \(y\)-axis in the plots represents the percentage of total episode converged. The color indicates in how many rounds the episode converges. For instance, in the bar corresponding to '\(10 10\_\)update_uot', with \(10\) data points (yellow portion), about \(50\%\) episodes satisfy the stopping condition.

The first plot shows results for \(10 10\) and \(5 3\) matrices. The second plot shows results for rectangular matrices of dimension \(m 10\) with \(m\) ranges in \(\). The third plot shows results for square matrices of dimension \(m m\) with \(m\) ranges in \(\). Here 'exact' and 'update' indicate the problem is (**PS**) or (**RS**), respectively. For parameters, \(uot\) represents the parameter choice \((_{P}=1,_{}=_{}=40)\) vs. \(ot\) represents the parameter choice \((_{P}=1,_{}=_{}=)\).

The first plots demonstrates that learners that do not have access to the true hypothesis (empirically builds estimation of \(\)) learn faster than learners who have full access. The second plot indicates with a fixed number of hypotheses, learning is faster when the dimension of \(\) increases. The third plot shows that the GBT learner scales well with the dimension of the problem.

Then we study the learners that interpolate between Bayesian and Cooperative learners (located on the line connecting CI and BI in Fig 1). Consider a fixed learning problem \((C,_{0},h^{*})\). Consistency of Bayesian inference states that asymptotically, the learner Bayesian converges to a particular hypothesis \(h_{b}\) almost surely where \(h_{b}\) is the hypothesis closest to \(h^{*}\) under KL divergence. Theorem 10 indicates that a GBT cooperative learner modifies one of the hypotheses into \(h^{*}\) in probability \(1\). The probability of \(h^{j}\) converges to \(h^{*}\) is determined by \(_{0}(h^{j})\).

In Fig. 3, we study the asymptotic behavior of the learners corresponding to \(=(1,_{},)\), with \(_{}\{0,0.02,0.2,0.5,1,2,5,50,\}\). We sample a learning problem with a dimension \(5 5\) from Dirichlet distribution with hyperparameters the vector \(\). Each learner \(=(1,_{},)\) is equipped with a fixed \(C\), \(_{0}\) and \(_{k}=\) for all \(k\). We run \(400,000\) learning episodes per learner, and plot their convergence summary in the bar graph. A continuous transition from a Bayesian learner to a cooperative learner can be empirically observed: the coefficients \(a(h)\) of the limit in terms of \(_{h}a(h)_{h}\) changes from \((h^{3})\) by Bernstein-von Mises to \(_{0}(h)\) by Theorem 10.

From the previous empirical results, we conclude the following conjecture:

Figure 3: Left: Behavior of models spanning the line segment between BI and CI. With \(_{P}=1\) and \(_{}=\), when \(_{}\) varies from 0 to \(\), the theory changes from BI to CI. Each bar graphs the Monte-Carlo result of 400,000 teaching sequences, we empirically observe that the coefficients \(a(h)\) of the limit in terms of \(_{h}a(h)_{h}\) changes from BI to CI continuously from \((h^{3})\) by Bernstein-von Mises to \(_{0}(h)\) by Theorem 10. Right: the Euclidean distances of each coefficient \(a(h)\) to BI result (blue crosses), and to CI result (orange dots).

Figure 2: Evidence of general consistency: we plot the percentage of episodes that reaches a threshold (\(0.999\)) by round number (in colors of the bars). Each bar represents a size of matrix, for each bar 100 matrices were randomly sampled, and 1000 rounds were simulated per matrix. “exact” means learner uses \(_{k}=\), (**PS**), “update” means learner uses statistics on current data in the episode (**RS**). “uot” takes \(=(1,40,40)\) and “ot” comes with exact and \(=(1,,)\).

**Conjecture 13**.: _When \(=(_{P},_{},)\), where \(_{P}(0,)\), the sequence of posteriors \(_{k}\) from generic \(C\), \(\), \(\) and \(\) as random variables satisfy \(_{k}_{h}(|_{k}(h) -1|\!<\!e)=1\) for any \(e>0\)._

Further, we pick out those episodes with \(_{N}(h)>0.95\), plot the values \(_{_{N}(h)>0.95}[_{k}(h)-(1-_{k}(h))]\) for each \(h\) against \(k\) in Fig. 4. Near linear relations are observed away from the first several rounds and before the values reaches the precision threshold. These are empirical estimates of the rate of convergence.

**Inconclusive and independent: \(_{}=0\).** The following holds for both (**PS**) and (**RS**):

**Proposition 14**.: _For \(=(_{P},_{},0)\) with \(_{P}(0,)\), as \(_{k}\) almost surely, the sequence \(_{k}\) of posteriors as a sequence of random variables converges in probability to variable \(\), where \((=^{i})=(i)\) and \(^{i}=P_{(i,\_)}/(_{j=1}^{m}P_{ij})\) and \(P=P^{}(C,,)\). Therefore, for any \(s>0\), \(_{k}_{h}(|_{k}(h)-1|<s)=0\) for generic (for all but in a closed subset) cost \(C\) and \(\), \(\)._

With \(_{}=0\), the constraint on column-sum (\(_{}\)-term) fails to affect the transport plan, thus the \(_{k}\)'s in the sequence are independent from each other, in contrast that in all other cases the adjacent ones are correlated via a nondegenerate transition distribution. The independence makes the sequence of posterior-samples in one episode behave totally random, thus rarely converge as points in \(()\). Furthermore, when consider the natural coupling \((_{k-1},_{k})\) from Markov transition measure for \(_{}=0\) (which is independent), \((|_{k-1}-_{k}|^{2})\) converges to the variance \(Var()\). In contrast, for \(_{}=\), \((|_{k-1}-_{k}|^{2})\) converges to 0 if Conj. 13 holds.

\(_{}(0,)\)**: partially conclusive.** From Conj. 13 and Prop. 14, together with the continuity of the transition distribution on \(\), we conjecture the following continuity on conclusiveness:

**Conjecture 15**.: _For both (**PS**) and (**RS**) models, when \(=(_{P},_{},_{})\) with \(_{P},_{}(0,)\), the posterior sequence \(_{k}\) from generated from generic \(C\), \(\), \(\) and \(\) satisfy that \(_{k}_{h}(|_{k}(h)-1|<s)=L\) exists, and \(L(0,1)\), for any \(s>0\)._

**An Exploration on Interpolation** It is popular in the state of art machine learning models that an agent learns probabilistically, but makes decisions greedily. This heuristic represents a path where a big leap on the cube was taken at the last step. An interesting question is under what circumstances this is optimal, what are the trade-offs, and under what conditions smoother trajectories are preferable. Instead of the giant leap, small steps along two paths are explored in the cube.

Our exploration takes a slightly different situation where the last step is replaced by a discriminative learning strategy \(=(0,,)\). We choose a matrix randomly of shape \(4 4\) (see Supplementary for detail), set total steps or total data points taught \(N=10\), and uniform prior \(()\) on hypotheses. Three learners are postulated: Blue performs Bayesian in first 9 steps and Discriminative in the last. Orange and Red follows two different interpolation curves with the same endpoints on the cube drawn in Fig. 5a. The curves are line and parabola segments on the cube sides.

Results of the three learners are shown in Fig. 5b. We sample \(h^{*}\) uniformly from the 4 columns of (the column-normalized) \(M\), 40000 repeats for each learner. Conclusiveness (minimal \(l^{1}\) distance between posterior and a 1-hot vector) and posterior entropy are plotted as histograms. The results show that the smoother path may lead to a more conclusive posterior. Numerical results: Conclusiveness of Blue: mean 0.9406, standard deviation 0.1300. Conclusiveness of Orange: mean 0.9964, standard deviation

Figure 4: Top: For a learning problem \(C\), behaviors of 9 different learners with \(_{P}=1\), \(_{}=\) and various \(_{}\) (denoted in figure) on conclusion distributions, \(a(h)\) in bar graph, plots below bars are estimated convergence rates \((_{k}(h)/(1-_{k}(h)))\) averaged on episodes converging to \(h\), one curve per hypothesis.

0.0327. Conclusiveness of Red: mean 0.9834, standard deviation 0.0676. Furthermore, compared with a sudden jump, gradual interpolations have lower entropy. Numerical results: entropy of Blue: mean 0.1261, standard deviation 0.2435, entropy of Orange: mean 0.0079, standard deviation 0.0629; entropy of Red: mean 0.0388, standard deviation 0.1336.

From this experiment, in the 10-sample learning, two smoother learners behave more conclusive in their posterior with smaller posterior entropy. Meanwhile, we still know very little about the interpolation behavior, such as which path works better, how to distribute vertices on the curve, etc.

## 4 Sequential GBT - Dynamic

While static models are frequently studied, in many cases world changes dynamically. In this section, we take a first step in this direction by exploring the sequential behaviors of GBT learners assuming the world changes periodically. Demonstrate that unlike existing learners, GBT learner is capable of detecting the non-static property of a given problem.

Let integer \(p>0\) be the period, given a set of true hypotheses distributions \(=\{_{0},_{1},,_{p-1}\}( )\), datum \(d_{t}\) is sampled from \(d_{k}\%_{p}\) where \(k\%_{p}\) represents the remainder of \(k\) under division by \(p\).

**Proposition 16**.: _For a Bayesian learner, the posterior sequence \(\{_{i}\}\) converges almost surely to the average of true hypotheses \(=_{k=0}^{p-1}_{k}\)._

For random variables of learner's posterior sequence \((_{k})_{k=1}^{}\), group them by period, we denote \(_{t}:=(_{tp},_{tp+1},,_{(t+1)p-1})\). Here \(k\) represents the time step, \(t\) denotes the period index.

**Proposition 17**.: _For \(\) in the interior of the cube, for (**PS**) problem, the sequence \(\{_{t}\}\) (random variables over \(()^{p}\)) form a time-homogeneous Markov chain. For (**RS**) problem, \(\{(_{t},_{k=0}^{p-1}t_{k})\}\), the random variable sequence producing samples \(\{(_{t},_{k=0}^{pt-1}_{d_{k}})\}\), forms a Markov chain._

Next we compare different learners' behavior empirically for (**RS**) problem. For visualization, \(M\) is taken of shape \(3 3\), thus \(()\) and \(()\) are both of dimension 2. If the Markov chain defined in Prop. 17 stabilizes, \([_{k}]\) will be periodic, matching the pattern of \(_{t}\). In fact, the period could be \(p\), or a factor of \(p\), or stabilizes where the period can be considered as \(1\). Thus we analyze \(\) in \(()\) and \([_{k}]\) in \(()\), obtained from Monte-Carlo sampling along certain amount of episodes.

Fig. 6 (a) assume the true hypothesis travel along the triangular path connecting the 3 columns of \(M\) (shown in blue crosses). We found that GBT learners with \(\) in the interior of the cube (general GBT) produce a posterior path of period \(p\), while the posteriors of Bayesian and SCBI learners tend to converge (Fig. 6 (b-e)). Thus a general GBT learner can naturally detect the periodicity of the world. We simulate up to \(k=400\) steps and \(10240\) repeats for each learner.

Moreover, we discovered that a general GBT learner's posteriors converge to a curve whose area is proportional to the area of the path of true hypotheses. In Fig. 7, as the path of true hypotheses vary by radius and shapes, the ratio (shown as slope) between both areas tends to be the same, it is 0.1620 in (a) and 0.1616 in (d), which suggests that this ratio is independent from the path of \(\).

Figure 5: (a): Baseline Blue and two learners Orange and Red following corresponding interpolation paths. (b): Results of the three learners over 40000 repeats. Top: conclusiveness, the frequency distribution of maximal posterior component. Bottom: entropy distribution.

Related Work.Prior work defines and outlines basic properties of Unbalanced Optimal Transport (Liero et al., 2018; Chizat et al., 2018; Pham et al., 2020). Bayesian approaches are prominent in machine learning (Murphy, 2012) and beyond (Jaynes, 2003; Gelman et al., 1995). There is also research on cooperative learning (Wang et al., 2019, 2020; Wang et al., 2019, 2020; Le et al., 2017) see also (Liu et al., 2021; Yuan et al., 2021; Zhu, 2015; Liu et al., 2017; Shafto and Goodman, 2008; Shafto et al., 2014; Frank and Goodman, 2012; Goodman and Frank, 2016; Fisac et al., 2017; Ho et al., 2018; Laskey et al., 2017). Discriminative learning is the reciprocal problem in which one sees data and asks which hypothesis best explains it (Ng and Jordan, 2001; Mandler, 1980). We are unaware of any work that attempts to unify and analyze the general problem of learning in which each of these are instances.

## 5 Conclusions

We have introduced Generalized Belief Transport (GBT), which unifies and parameterizes classic instances of learning including Bayesian inference, Cooperative Inference, and Discrimination, as Unbalanced Optimal Transport (UOT). We show that each instance is a point in a continuous, differentiable on the interior, 3-dimensional space defined by the regularization parameters of UOT. Moreover, to demonstrate general GBT's capacity of supporting generalized learning, we prove and illustrate asymptotic consistency and estimate rates of convergence, including convergence to hypotheses with zero prior support, and ability of gripping dynamic of the world. In summary, GBT unifies very different modes of learning, yielding a powerful, general framework for modeling learning agents.

Figure 6: **(a)**. Setup: each dot represents an \(_{k}\); data are sampled along the dots from red to yellow of period 18, \(M\) has 3 columns represented by the blue crosses. **(b-d)**. Bayesian, general GBT, SCBI learners, resp., blue curve shows \([_{k}]\) and red crosses are \([_{t}]\) (mean of 18 consecutive \([_{k}]\)’s). **(e)**. for 6 different learners (shown in colors), plot (1) the averaged distance between \([_{k}]\) and its center v.s. number of periods, in solid lines and left y-ticks, and (2) the step-length of \([_{t}]\) between consecutive periods in dash-dots and right y-ticks.

Figure 7: Behavior of a GBT learner with \(=(1,10,10)\) on two different paths of \(\) with \(p=20\), tested in 300 steps and 10240 episodes. \(M\) is fixed and represented by the red dots. Learner’s posteriors form roughly periodic paths, small panels on corners of (a, d), plot path of \(\) and posterior paths, ratio between their enclosed areas are shown in yellow to blue dots. (b, c) shows the 20 concentric similar paths that \(\) follow. Colors are matched between paths and corresponding area ratios.