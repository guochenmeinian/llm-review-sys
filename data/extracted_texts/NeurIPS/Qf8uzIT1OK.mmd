# Ethical Considerations for Responsible Data Curation

Jerone T. A. Andrews

Sony AI, Tokyo

&Dora Zhao

Sony AI, New York

&William Thong

Sony AI, Zurich

&Apostolos Modas

Sony AI, Zurich

Correspondence to jerone.andrews@sony.com. Equal contribution; authors are listed in random order.

###### Abstract

Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.

## 1 Introduction

Contemporary human-centric computer vision (HCCV) data curation practices, which prioritize dataset size and utility, have pushed issues related to privacy and bias to the periphery, resulting in dataset retractions and modifications , as well as models that are unfair or rely on spurious correlations . HCCV datasets primarily rely on nonconsensual web scraping . These datasets not only regard image subjects as free raw material , but also lack the ground-truth metadata required for fairness and robustness evaluations . This makes it challenging to obtain a comprehensive understanding of model blindspots and cascading harms  across dimensions, such as data subjects, instruments, and environments, which are known to influence performance . While, for example, image subject attributes can be inferred , this is controversial for social constructs, notably race and gender . Inference introduces further biases  and can induce psychological harm when incorrect .

Recent efforts in machine learning (ML) to address these issues often rely on post hoc reflective processes. Dataset documentation focuses on interrogating and describing datasets after data collection . Similarly, initiatives by NeurIPS and ICML ask authors to consider the ethical and societal implications of their research after completion . Further, dataset audits  and bias detection tools  expose dataset management issues and representational biases without offering guidance on responsible data collection. Although there are existing proposals for artificial intelligence (AI) and data design guidelines , as well as calls to adopt methodologies from more established fields , general-purpose guidelines lack domain specificity and task-oriented guidance . For example, remedies may prioritize privacy and governance  but overlook data composition and image content. Other recommended practices lack persuasive justification for adoption  or fail to provide propercontextualization for appropriate application [252; 323; 367]. For instance, the _People + AI Guidebook_ suggests creating dataset specifications without explaining the rationale, and privacy methodologies are advocated without cognizant of privacy and data protection laws [252; 367; 323]. These efforts, which hold significance in promoting responsible practices, would benefit from being supplemented by proactive, domain-specific recommendations aimed at tackling privacy and bias concerns starting from the inception of a dataset.

Our research directly addresses these critical concerns by examining purpose (Section 3), consent and privacy (Section 4), and diversity (Section 5). Compared to recent scholarship, we adopt an ante hoc reflective perspective, offering considerations and recommendations for curating HCCV datasets for fairness and robustness evaluations. Our work, therefore, resonates with the call for domain-specific resources to operationalize fairness [68; 150; 305]. We draw insights from current practices [42; 170; 375], guidelines [31; 222; 231], dataset withdrawals [78; 126; 216], and audits [35; 36; 247], to motivate each recommendation, focusing on HCCV evaluation datasets that present unique challenges (e.g., visual leakage of personally identifiable information) and opportunities (e.g., leveraging image metadata for analysis). To guide curators towards more ethical yet resource-intensive curation, we provide a checklist in Appendix A.3 This translates our considerations and recommendations into pre-curation questions, functioning as a catalyst for discussion and reflection.

While several of our recommendations can also be applied retroactively such measures cannot undo incurred harm, e.g., resulting from inappropriate uses, privacy violations, and unfair representation . It is important to make clear that our proposals are not intended for the evaluation of HCCV systems that detect, predict, or label sensitive or objectionable attributes such as race, gender, sexual orientation, or disability.

## 2 Development Process

HCCV should adhere to the most stringent ethical standards to address privacy and bias concerns. As stated in the NeurIPS Code of Ethics , it is essential to abide by established institutional research protocols, ensuring the safeguarding of human subjects. These protocols, initially designed for biomedical research, have, however, been met with confusion, resulting in inconsistencies when applied in the context of data-centric research . For example, HCCV research often amasses millions of "public" images without obtaining informed consent or participation, disregarding serious privacy and ethical concerns [3; 35; 245; 301; 313]. This exemption from research ethics regulation is grounded in the limited definition of human-subjects research, which categorizes extant, publicly available data as minimal risk [218; 256]. Thus, numerous ethically-dubious HCCV datasets would not fall under Institutional Review Board (IRB) oversight . What's more, the NeurIPS Code of Ethics only mandates following existing protocols when research involves "direct" interaction between human participants and researchers or technical systems. Even when research is subjected to supervision, IRBs are restricted from considering broader societal consequences beyond the immediate study context . Compounding matters, CV-centric conferences are still to adopt ethics review practices .

These limitations are concerning, especially considering the potential for predictive privacy harms when seemingly non-identifiable data is combined [35; 70; 218] or when data is used for harmful downstream applications such as predicting sexual orientation [192; 344], crime propensity [358; 365], or emotion [10; 224]. Acknowledging this, our research study employed the same principles underpinning established guidelines [24; 331] for protecting human subjects in research to identify ethical issues in HCCV dataset design, namely autonomy, justice, benefence, and non-maleficence. _Autonomy_ respects individuals' self-determination--e.g., through informed consent and assent for HCCV datasets. _Justice_ promotes the fair distribution of risks, costs, and benefits, guiding decisions on compensation, data accessibility, and diversity. _Beneference_ entails the proactive promotion of positive outcomes and well-being, e.g., by soliciting individuals' to self-identify, while _non-maleficence_ centers on minimizing harm and risks during dataset design, e.g., by redacting privacy-leaking image regions and metadata.

To ensure comprehensive consideration, we harnessed diverse expertise, following contemporary, interdisciplinary practices [261; 270; 307]. Our team comprises researchers, practitioners, and lawpers with backgrounds in ML, CV, algorithmic fairness, philosophy, and social science. With a range of ethnic, cultural, and gender backgrounds, we bring extensive experience in designing CV datasets, training models, and developing ethical guidelines. To align our expertise with the principles, we collectively discussed them, considering each author's background. After identifying key ethical issues in HCCV data curation practices, we iteratively refined them into an initial draft of ethical considerations. We extensively collected, analyzed, and discussed papers spanning a range of themes such as HCAI, HCCV datasets, data and model documentation, bias detection and mitigation, AI and data design, fairness, and critical AI. Our comprehensive literature review incorporated pertinent studies and datasets, resulting in refined considerations with detailed explanations and recommendations for responsible data curation. Additional details are provided in Appendix B.

## 3 Purpose

In ML, significant emphasis has been placed on the acquisition and utilization of "general-purpose" datasets . Nevertheless, without a clearly defined task pre-data collection, it becomes challenging to effectively handle issues related to data composition, labeling, data collection methodologies, informed consent, and assessments related to data protection. This section addresses conflicting dataset motivations and provides recommendations.

### Ethical Considerations

**Fairness-unaware datasets are inadequate for measuring fairness.** Datasets lacking explicit fairness considerations are inadequate for mitigating or studying bias, as they often lack the necessary labels for assessing fairness. For instance, the COCO dataset , focused on scene understanding, lacks subject information, making fairness assessments challenging. Researchers, consequently, resort to human annotators to infer, e.g., subject characteristics, limiting bias measurement to visually "inferable" attributes. This introduces annotation bias  and the potential for harmful inferences [275; 47].

**Fairness-aware datasets are incompatible with common HCCV tasks.** Industry practitioners stress the importance of carefully designed and collected "fairness-aware" datasets to detect bias issues . Fabris et al.  found that out of 28 CV datasets used in fairness research between 2014 and 2021, only eight were specifically created with fairness in mind. Among these, seven were HCCV datasets (scraped from the web) [342; 343; 170; 216; 308; 319; 344], including five focused on facial analysis. Due to the limited availability and delimited task focus of fairness-aware datasets, researchers repurpose "fairness-unaware" datasets [120; 139; 196; 198; 208; 346; 373]. Fairness-aware datasets fall short in addressing the original tasks associated with well-known HCCV datasets, which encompass a range of tasks, such as segmentation [64; 209], pose estimation [196; 13], localization and detection [73; 91; 110], identity verification , action recognition , as well as reconstruction, synthesis and manipulation [114; 171]. The absence of fairness-aware datasets with task-specific labels hampers the practical evaluation of HCCV systems, despite their importance in domains such as healthcare [155; 220], autonomous vehicles , and sports . Additionally, fairness-aware datasets lack self-identified annotations from image subjects, relying on inferred attributes, e.g., from online resources [319; 308; 43].

### Practical Recommendations

**Refrain from repurposing datasets.** Existing datasets, repurposable but optimized for specific functions, can inadvertently perpetuate biases and undermine fairness . Repurposing fairness-unaware data for fairness evaluations can result in _dirty data_, characterized by missing or incorrect information and distorted by individual and societal biases [181; 265]. Dirty data, including inferred data, can have significant downstream consequences, compromising the validity of research, policy, and decision-making [14; 63; 265; 341]. ML practitioners widely agree that a proactive approach to fairness is preferable, involving the direct collection of demographic information from the outset . To mitigate epistemic risk, curated datasets should capture key dimensions influencing fairness and robustness evaluation of HCCV models, i.e., data subjects, instruments, and environments. Model Cards explicitly highlight the significance of these dimensions in fairness and robustness assessments .

**Create purpose statements.** Pre-data collection, dataset creators should establish _purpose statements_, focusing on motivation rather than cause . Purpose statements address, e.g., data collection motivation, desired composition, permissible uses, and intended consumers. While dataset documentation [108; 258] covers similar questions, it is a _reflective_ process and can be manipulated to fit the narrative of the collected data, as opposed to directing the narrative of the data to be collected. Purpose statements can play a crucial role in preventing both _hindsight bias_[51; 97; 176] and _purpose creep_, ensuring alignment with stakeholders' consent and intentions . To enhance transparency and accountability, as recommended by Peng et al. , purpose statements can undergo peer review, similar to _registered reports_. Registered reports, recognized by the UK 2021 Research Excellence Framework, incentivize rigorous research practices and can lead to increased institutional funding .

## 4 Consent and Privacy

Informed consent is crucial in research ethics involving humans [230; 235], ensuring participant safety, protection, and research integrity [59; 253]. Shaping data collection practices in various fields [35; 235], informed consent consists of three elements: _information_ (i.e., the participant should have sufficient knowledge about the study to make their decision), _comprehension_ (i.e., the information about the study should be conveyed in an understandable manner), and _volutariness_ (i.e., consent must be given free of coercion or undue influence). While consent is not the only legal basis for data processing, it is globally preferred for its legitimacy and ability to foster trust [253; 82]. We address concerns related to consent and privacy, and provide recommendations.

### Ethical Considerations

**Human-subjects research.** As aforementioned in Section 2, HCCV datasets are frequently collected without informed consent or participation, primarily due to the classification of publicly available data as "minimal risk" within human-subjects research. However, beyond possible predictive privacy harms and unethical downstream uses, collecting data without informed consent hinders researchers and practitioners from fully understanding and addressing potential harms to data subjects [218; 333]. Some argue that consent is pivotal as it provides individuals with a last line of defense against the misuse of their personal information, particularly when it contradicts their interests or well-being [253; 254; 77; 223].

**Creative Commons loophole.** Some datasets have been created based on the misconception that the "unlocking [of] restrictive copyright"  through Creative Commons licenses implies data subject consent. However, the Illinois Biometric Information Privacy Act (BIPA)  mandates data subject consent, even for publicly available images . In the UK and EU General Data Protection Regulation (GDPR)  Article 4(11), images containing faces are considered biometric data, requiring "freely given, specific, informed, and unambiguous" consent from data subjects for data processing. Similarly, in China, the Personal Information Protection Law (PIPL)  Article 29 mandates obtaining individual consent for processing sensitive personal information, including biometric data (Article 28). While a Creative Commons license may release copyright restrictions on specific artistic expressions within images , it does not apply to image regions containing biometric data such as faces, which are protected by privacy and data protection laws .

**Vulnerable persons.** Nonconsensual data collection methods can result in the inclusion of vulnerable individuals unable to consent or oppose data processing due to power imbalances, limited capacity, or increased risks of harm [89; 207]. While scraping vulnerable individuals' biometric data may be incidental, some researchers actively target them, jeopardizing their sensitive information without guardian consent [260; 128].

Paradoxically, attempts to address racial bias in data have involved soliciting homeless persons of color, further compromising their vulnerability . When participation is due to economic or situational vulnerability, as opposed to one's best interests, monetary offerings may be perceived as inducement . Further ethical concerns manifest when it is unclear whether participants were adequately _informed_ about a research study. For instance, in ethnicity recognition research , despite obtaining informed consent, criticism arose due to training a model that discriminates between Chinese Uyghur, Korean, and Tibetan faces. Although the study's focus is on the technologyitself , its potential use in enhancing surveillance on Chinese Uyghurs raises ethical questions due to the human rights violations against them .

**Consent revocation.** Dataset creators sometimes view autonomy as a challenge to collecting biometric data for HCCV, especially when data subjects prioritize privacy [214; 287; 297]. Nonetheless, informed consent emphasizes _voluntariness_, encompassing both the ability to give consent and the right to withdraw it at any time . GDPR grants explicit revocation rights (Article 7) and the right to request erasure of personal data (Article 17) . However, image subjects whose data is collected without consent are denied these rights. The nonconsensual FFHQ face dataset  offers an opt-out mechanism, but since inclusion was _involuntary_, subjects may be unaware of their inclusion, rendering the revocation option hollow. Moreover, this burdens data subjects with tracking the usage of their data in datasets, primarily accessible by approved researchers .

**Image- and metadata-level privacy attributes.** Researchers have focused on obfuscation techniques, e.g., blurring, inpainting, and overlaying, to reduce private information leakage of nonconsensual individuals [46; 101; 194; 195; 213; 252; 311; 323; 362; 367]. Nonetheless, face detection algorithms used in obfuscation may raise legal concerns, particularly if they involve predicting facial landmarks, potentially violating BIPA [61; 370]. BIPA focuses on collecting and using face geometry scans regardless of identification capability, while GDPR protects any identifiable person, requiring data holders to safeguard the privacy of nonconsenting individuals. Moreover, reliance on automated face detection methods raises ethical concerns, as demonstrated by the higher precision of pedestrian detection models on lighter skin types compared to darker skin types . This predictive inequity leads to allocative harm, denying certain groups opportunities and resources, including the rights to safety  and privacy .

It is important to note that face obfuscation may not guarantee privacy [145; 367]. The Visual Redactions dataset  includes 68 image-level privacy attributes, covering biometrics, sensitive attributes, tattoos, national identifiers, signatures, and contact information. Training faceless person recognition systems using full-body cues reveals higher than chance re-identification rates for face blurring and overlaying , indicating that solely obfuscating face regions might be insufficient under GDPR. Furthermore, image metadata can also disclose sensitive details, e.g., date, time, and location, as well as copyright information that may include names [11; 239]. This is worrisome for users of commonly targeted platforms like Flickr, which retain metadata by default.

### Practical Recommendations

**Obtain voluntary informed consent.** Similar to recent consent-driven HCCV datasets [136; 254; 268], explicit informed consent should be obtained from each person depicted in, or otherwise identifiable, in a dataset, allowing the sharing of their facial, body, and biometric information for evaluating the fairness and robustness of HCCV technologies. Datasets collected with consent _reduce_ the risk of being fractured, however, data subjects may later revoke their consent over, e.g., privacy concerns they may not have been aware of at the time of providing consent or language nuances [65; 379]. Following GDPR (Article 7), plain language consent and notice forms are recommended to address the lack of public understanding of AI technologies .

When collecting images of individuals under the age of majority or those whose ability to protect themselves is significantly impaired on account of disability, illness, or otherwise, guardian consent is necessary . However, relying solely on guardian consent overlooks the views and dignity of the vulnerable person . To address this, in addition to guardian consent, voluntary informed _assent_ can be sought from a vulnerable person, in accordance with UNICEF's principlism-guided data collection procedures [31; 327]. When employing appropriate language and tools, assent establishes the vulnerable person understands the use of their data and willingly participates . If a vulnerable person expresses dissent or unwillingness to participate, their data should not be collected, regardless of guardian wishes.

Informed by the U.S. National Bioethics Advisory Commission's _contextual vulnerability framework_, dataset creators should assess vulnerability on a continuous scale. That is, the circumstances of participation should be considered, which may require, e.g., a participatory design approach, assurances over compensation, supplementary educational materials, and insulation from hierarchical or authoritative systems .

**Adopt techniques for consent revocation.** To permit consent revocation, dataset creators should implement an appropriate mechanism. One option is _dynamic consent_, where personalized communication interfaces enable participants to engage more actively in research activities [174; 348]. This approach has been implemented successfully through online platforms, offering options for blanket consent, case-by-case selection, or opt-in depending on the data's use [174; 211; 314]. Alternatively, another recommended approach is to establish a steering board or charitable trust composed of representative dataset participants to make decisions regarding data use . The feasibility of these proposals may vary based on a dataset's scale. Nonetheless, at a minimum, data subjects should be provided a simple and easily accessible method to revoke consent [136; 254; 268]. This aligns with guidance provided by the UK Information Commission's Office (ICO), emphasizing the need to provide alternatives to online-based revocation processes to accommodate varying levels of technology competency and internet access among data subjects .

**Collect country of residence information.** Anonymizing nonconsensual human subjects through face obfuscation, as done in datasets such as ImageNet , may not respect the privacy laws specific to the subjects' country of residence. To comply with relevant data protection laws, dataset curators should collect the country of residence from each data subject to determine their legal obligations, helping to ensure that data subjects' rights are protected and future legislative changes are addressed [249; 268]. For instance, GDPR Article 7(3) grants data subjects the right to withdraw consent at any time, which was not explicitly addressed in its predecessor .

**Redact privacy leaking image regions and metadata.** The European Data Protection Board emphasizes that anonymization of personal data must guard against re-identification risks such as singling out, linkability, and inference . Re-identification remains possible even when nonconsensual subjects' faces are obfuscated, through other body parts or contextual information . One solution is to redact all privacy-leaking regions related to nonconsensual subjects (including their entire bodies, clothing, and accessors) and text (excluding copyright owner information). However, anonymization approaches should be validated empirically, especially when using methods without formal privacy guarantees. Moreover, to mitigate algorithmic failures or biases, human annotators should be involved in creating region proposals, as well as verifying automatically generated proposals, for image regions with identifying or private information . For nonconsensual individuals residing in certain jurisdictions (e.g., Illinois, California, Washington, Texas), automated region proposals requiring biometric identifiers should be avoided. Instead, human annotators should take the responsibility of generating these proposals.

Notwithstanding, to further protect privacy, dataset creators should take steps to ensure that image metadata does not reveal identifying information that data subjects did not consent to sharing. This may involve replacing exact geolocation data with a more general representation, such as city and country, and excluding user-contributed details from metatags containing personally identifiable information, except when this action would violate copyright. However, we do not advise blanket redaction of all metadata, as it contains valuable image capture information that can be useful for assessing model bias and robustness related to instrument factors.

## 5 Diversity

HCCV dataset creators widely acknowledge the significance of dataset diversity [173; 171; 170; 171; 173; 196; 283; 361; 368], realism [173; 174; 175; 164; 368; 13, 64], and difficulty [173; 176; 170; 173; 178; 91; 179; 180; 361; 368] to enhance fairness and robustness in real-world applications. Previous research has emphasized diversity across image subjects, environments, and instruments [43; 139; 222; 287], but there are many ethical complexities involved in specifying diversity criteria [14; 15]. This section examines taxonomy challenges and offers recommendations.

### Ethical Considerations

**Representational and historical biases.** The Council of Europe have expressed concerns about the threat posed by AI systems to equality and non-discrimination principles . Many dataset creators often prioritize protected attributes, i.e., gender, race, and age, as key factors of dataset diversity . Nevertheless, most HCCV datasets exhibit historical and representational biases [35; 166; 172; 312; 366]. These biases can be pernicious, particularly when models learn and _amplify_ them. For instance, image captioning models may rely on contextual cues related to activities like shopping and laundry  to generate gendered captions. Spurious correlations are detrimental, as they are not causally related and perpetuate harmful associations . In addition, prominent examples in HCCV research demonstrate disparate algorithmic performance based on race and skin color . Most recently, autonomous robots have displayed racist, sexist, and physionomic stereotypes . Furthermore, face detection models have shown lower accuracy when processing images of older individuals compared to younger individuals . While not endorsing these applications, discrepancies have also been observed in facial emotion recognition services for children in both commercial and research systems , as well as age estimation .

Despite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments . GDPR guidance from the UK ICO confirms that sensitive attributes can be collected for fairness purposes . However, obtaining this information presents challenges, such as historical mistrust in clinical research among African-Americans  or the social stigma of being photographed that some women face . Nonetheless, marginalized communities may require explicit explanations and assurances about data usage to address concerns related to service provision, security, allocation, and representation . This is particularly important as remaining _unseen_ does not protect against being _mis-seen_.

**The digital divide and accessibility.** Healthcare datasets often lack representation of minority populations, compromising the reliability of automated decisions . The World Health Organization (WHO) emphasizes the need for data accuracy, completeness, and diversity, particularly regarding age, in order to address ageism in AI . ML systems may prioritize younger populations for resource allocation, assuming they would benefit the most in terms of life expectancy . The digital divide further exacerbates the underrepresentation of vulnerable groups, including older generations, low-income school-aged children, and children in East Asia and the South Pacific who lack access to digital technology . Insufficient access to digital technology hampers the representation of vulnerable persons in datasets , leading to _outcome homogenization_--i.e., the systematic failing of the same individuals or groups .

**Confused taxonomies.** Sex and gender are often used interchangeably, treating gender as a consequence of one's assigned sex at birth . However, this approach erases intersex individuals who possess non-binary physiological sex characteristics . Treating sex and gender as interchangeable perpetuates normative views by casting gender as binary, immutable, and solely based on biological sex . This perspective disregards transgender and gender nonconforming individuals. Moreover, sex, like gender, is a social construct, as sexed bodies do not exist outside of their _social context_.

Similar to sex and gender, race and ethnicity are often used synonymously . Nations employ diverse census questions to ascertain ethnic group composition, encompassing factors such as nationality, race, color, language, religion, customs, and tribe . However, these categories and their definitions lack consistency over time and geography, often influenced by political agendas and socio-cultural shifts . This variability makes it challenging to collect globally representative and meaningful data on ethnic groups. Consequently, several HCCV datasets have incorporated inconsistent and arbitrary racial categorization systems . For instance, the FairFace dataset  creators reference the US Census Bureau's racial categories without considering the social definition of race they represent . The US Census Bureau explicitly states that their categories reflect a social definition rather than a biological, anthropological, or genetic one. Consequently, labeling the "physical race" of image subjects based on nonphysiological categories is contradictory. Furthermore, the FairFace creators do not disclose the demographics or cultural compatibility of their annotators.

**Own-anchor bias.** HCCV approaches for encoding age in datasets vary, using either integer labels  or group labels . Age groupings are often preferred when collecting unconstrained images from the web, as human annotators must infer subjects' ages, which is challenging . This is evident in crowdsourced annotations, where 40.2% of individuals in the OpenImages MIAP dataset  could not be categorized into an age group. Factors unrelated to age, such as facial expression  and makeup , influence age perception. Furthermore, annotators have exhibited lower accuracy when labeling people outside of their own demographic group .

**Post hoc rationalization of the use of physiological markers.** Gender information about data subjects is obtained through inference or self-identification [136; 190; 203; 204; 375]. Inference raises concerns as it assumes that gender can be determined solely from imagery without consent or consultation with the subject, which is noninclusive and harmful [87; 127; 179]. Even when combined with non-image-based information, inferred gender fails to account for the fluidity of identity, potentially mislabeling subjects at the time of image capture [277; 278]. Moreover, physical traits are just one of many dimensions, including posture, clothing, and vocal cues, used to infer not only gender but also race [177; 100; 179].

**Erasure of nonstereotypical individuals.** HCCV datasets frequently adopt a US-based racial schema [170; 190; 203; 204; 343], which may oversimplify and essentialize groups . This approach may not align with other more nuanced models, e.g., the continuum-based color system used in Brazil, which considers a wide range of physical characteristics. Nonconsensual image datasets rely on annotators to assign semantic categories, perpetuating stereotypes and disseminating them beyond their cultural context . Notably, images without label consensus are often discarded [170; 267; 343], potentially excluding individuals who defy stereotypes, such as multi-ethnic individuals .

**Phenotypic attributes.** Protected attributes may not be the most appropriate criteria for evaluating HCCV models . Social constructs like race and gender lack clear delineations for subgroup membership based on visible or invisible characteristics. These labels capture invisible aspects of identity that are not solely determined by visible appearance. Moreover, the phenotypic characteristics within and across subgroups exhibit significant variability [25; 48; 96; 138; 180; 347].

**Environment and instrument.** The image capture device and environmental conditions significantly influence model performance, and their impact should be considered . Factors such as camera software, hardware, and environmental conditions affect HCCV model robustness in various settings [4; 197; 221; 229; 353; 360; 364; 371]. Understanding performance differences is crucial from ethical and scientific perspectives. For example, sensitivity to illumination or white balance may be linked to sensitive attributes, e.g., skin tone [62; 184; 185; 378], while available instruments or environmental co-occurrences may correlate with demographic attributes [139; 295; 376].

**Annotator positionality.** Psychological research highlights the influence of annotators' sociocultural background on their visual perception [19; 107; 263; 275; 291]. However, recent empirical studies have evidenced a lack of regard for the impact an annotator's social identity has on data [79; 111]. Only a handful of HCCV datasets provide annotator demographic details [12; 56; 287; 375].

**Recruitment and compensation.** Data collected without consent pactently lacks compensation. Balancing between excessive and deficient payment is crucial to avoid coercion and exploitation [231; 268]. An additional concern is the employment of remote workers from disadvantaged regions , often with low wages and fast-paced work conditions [71; 135; 162; 206]. This can lead to arbitrary denial of payment based on opaque quality criteria  and prevents union formation , creating a sense of invisibility and uncertainty for workers .

### Practical Recommendations

**Obtain self-reported annotations.** Practitioners are cautious about inferring labels about people to avoid biases . Moreover, data access request rights, e.g., as offered by GDPR, California Consumer Privacy Act, and PIPL, may require data holders to disclose inferred information. To avoid stereotypical annotations and minimize harm from misclassification , labels should be collected directly from image subjects, who inherently possess contextual knowledge of their environment and awareness of their own attributes.

**Provide open-ended response options.** Closed-ended questions, such as those on census forms, may lead to incongruuous responses and inadequate options for self-identification [156; 179; 274]. Open-ended questions provide more accurate answers but can be taxing, require extensive coding, and are harder to analyze [109; 178; 298; 40]. To balance this, closed-ended questions should be augmented with an open-ended response option, avoiding the term "other", which implies _othering_ norms . This gives subjects a _voice_[234; 296] and allows for future question design improvement.

**Acknowledge the mutability and multiplicity of identity.**_Identity shift_--the intentional self-transformation in mediated contexts --is often overlooked. To address this, we propose collecting self-identified information on a per-image basis, acknowledging that identity is temporal and nonstatic. Specifically, for sensitive attributes, allowing the selection of multiple identity categories without limitations is preferable [304; 309]. This prevents oversimplification and marginalization. While we acknowledge the potential burden of self-identification on fluid and dynamic identities, an image captures a single moment. Thus, evolving identity may not require metadata updates; however, we recommend providing subjects with mechanisms for updates when needed.

**Collect age, pronouns, and ancestry.** First, to capture accurate age information, dataset curators should collect the exact biological age in years from image subjects, corresponding to their age at the time of image capture. This approach offers flexibility, insofar as permitting the appropriate aggregation of the collected data. This is particularly important given the lack of consistent age groupings in the literature.

Second, dataset curators should consider opting to collect self-identified pronouns. This promotes mutual respect and common courtesy, reducing the likelihood of causing harm through misgendering . Self-identified pronouns are particularly important for sexual and gender minority communities as they "convey and affirm gender identity" . Significantly, pronoun use is increasingly prevalent in social media platforms , workplaces , and education settings , fostering gender inclusivity . However, subjects should always have the option of not disclosing this information.

Finally, to address issues with ethnic and racial classification systems , dataset creators should consider collecting ancestry information instead. Ancestry is defined by historically shaped borders and has been shown to offer a more stable and less confusing concept . The United Nations' M49 geoscheme can be used to operationalize ancestry , where subjects select regions that best describe their ancestry. To situate responses, subjects could be asked, e.g., "Where do your ancestors (e.g., great-grandparents) come from?". This avoids reliance on proxies, e.g., skin tone, that risk normalizing their inadequacies without reflecting their limitations .

**Collect aggregate data for commonly ignored groups.** Additional sensitive attributes should also be collected, such as disability and pregnancy status, when voluntarily disclosed by subjects. These attributes should be reported in aggregate data to reduce the safety concerns of subjects . Given that definitions of these attributes may be inconsistent and tied to culture, identity, and histories of oppression , navigating tensions between benefits and risks is necessary. Despite potential reluctance, sourcing data from underrepresented communities contributes to dataset inclusivity . Regarding disability, the American Community Survey  covers categories related to hearing, vision, cognitive, ambulatory, self-care, and independent living difficulties.

**Collect phenotypic and neutral performance features.** Collecting phenotypic characteristics can serve as _objective_ measures of diversity, i.e., attributes which, in evolutionary terms, contribute to individual-level recognition , e.g., skin color, eye color, hair type, hair color, height, and weight . These attributes have enabled finer-grained analysis of model performance and biases . Additionally, considering a multiplicity of _neutral performative features_, e.g., facial hair, hairstyle, cosmetics, clothing, and accessories, is important to surface the perpetuation of social stereotypes and spurious relationships in trained models .

**Record environment and instrument information.** Data should capture variations in environmental conditions and imaging devices, including factors such as image capture time, season, weather, ambient lighting, scene, geography, camera position, distance, lens, sensor, stabilization, use of flash, and post-processing software. Instrument-related factors may be easily captured, by restricting data collection to images with exchangeable image file format (Exif) metadata. The remaining factors, e.g., season and weather can be self-reported or coarsely estimated utilizing information such as image capture time and location.

**Recontextualize annotators as contributors.** Dataset creators should document the identities of annotators and their contributions to the dataset , rather than treating them as anonymous entities responsible for data labeling alone . While many datasets  neglect to report annotator demographics, assuming objectivity in annotation for visual categories is flawed . Furthermore, using majority voting to reach the assumed ground truth, disregards minority opinions, treating them as noise . Annotator characteristics, including pronouns, age, and ancestry, should be recorded and reported to quantify and address annotator perspectives and bias in datasets . Additionally, allowing annotators freedom in labeling helps to avoid replicating socially dominant viewpoints .

**Fair treatment and compensation for contributors.** In accordance with Australia's National Health and Medical Research Council  and the WHO , dataset contributors should not only be guaranteed compensation above the minimum hourly wage of their country of residence , but also according to the complexity of tasks to be performed. However, alternative payment models, for example, based on the average hourly wage, may offer benefits in terms of promoting diversity by increasing the likelihood of higher socio-economic status contributors .

Besides payment, the implementation of direct communication channels and feedback mechanisms, such as anonymized feedback forms , can help to address issues faced by annotators while providing a level of protection from retribution. Furthermore, the creation of plain language guides can ease task completion and reduce quality control overheads. Ideally, recruitment and compensation processes should be well-documented and undergo ethics review, which can help to further reduce the number of "glaring ethical lapses" .

## 6 Discussion and Conclusion

Supplementary to established ethical review protocols, we have provided proactive, domain-specific recommendations for curating HCCV evaluation datasets for fairness and robustness evaluations. However, encouraging change in ethical practice could encounter resistance or slow adoption due to established norms , inertia , diffusion of responsibility , and liability concerns . To garner greater acceptance, platforms such as NeurIPS could adopt a model similar to the registered reports format, embraced by over 300 journals . This entails pre-acceptance of dataset proposals before curation, alleviating financial uncertainties associated with more ethical practices.

Nevertheless, seeking consent from all depicted individuals might give rise to logistical challenges. Resource requirements tied to the implementation and maintenance of consent management systems could emerge, potentially necessitating significant investment in technical infrastructure and dedicated personnel. Particularly for smaller organizations and academic research groups, these limitations could present considerable hurdles. A potential solution is forming _data consortia_, which helps address operational challenges by pooling resources and knowledge.

Extending our recommendations to the curation of "democratizing" foundation model-sized training datasets  poses an economic challenge. To put this into perspective, the GeoDE dataset of 62K crowdsourced object-centric images , without personally identifiable information, incurred a cost of $1.08 per image. While our recommendations may not seamlessly _scale_ to the curation of fairness-aware, billion-sized image datasets, it is worth considering that "solutions which resist scale thinking are necessary to undo the social structures which lie at the heart of social inequality" . Large-scale, nonconsensual datasets driven by scale thinking have included harmful and distressing content, including rape , racist stereotypes , vulnerable persons , and derogatory taxonomies . Such content may further generate legal concerns . We contend that these issues can be mitigated through the implementation of our recommendations.

Nonetheless, balancing resources between model development and data curation is value-laden, shaped by "social, political, and ethical values" . While organizations readily invest significantly in model training , compensation for data contributors often appears neglected , disregarding that "most data represent or impact people" . Remedial actions could be envisioned to bridge the gap between models developed with ethically curated data and those benefiting from expansive, nonconsensually crawled data. Reallocating research funds away from dominant data-hungry methods  would help to strike a balance between technological advancement and ethical imperatives.

However, the granularity and comprehensiveness of our diversity recommendations could be adapted beyond evaluation contexts, particularly when employing "fairness without demographics"  training approaches, reducing financial costs. Nevertheless, the applicability of any proposed recommendation is intrinsically linked to the specific context . Decisions should be guided by the social framework of a given application to ensure ethical and equitable data curation.

Just as the concepts of identity evolve, our recommendations must also evolve to ensure their ongoing relevance and sensitivity. Thus, we encourage dataset creators to tailor our recommendations to their _context_, fostering further discussions on responsible data curation.