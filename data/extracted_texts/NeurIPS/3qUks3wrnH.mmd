# Efficient Multi-Task Reinforcement Learning

with Cross-Task Policy Guidance

 Jinmin He\({}^{1,2}\), Kai Li\({}^{1,2}\), Yifan Zang\({}^{1,2}\), Haobo Fu\({}^{5}\), Qiang Fu\({}^{5}\), Junliang Xing\({}^{4}\), Jian Cheng\({}^{1,3}\)

\({}^{1}\)Institute of Automation, Chinese Academy of Sciences

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)AiRiA \({}^{4}\)Tsinghua University \({}^{5}\)Tencent AI Lab

{hejinmin2021,kai.li,zangyifan2019,jian.cheng}@ia.ac.cn,

{haobofu,leonfu}@tencent.com, jlxing@tsinghua.edu.cn

Corresponding authors.

###### Abstract

Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks. Existing approaches primarily focus on parameter sharing with carefully designed network structures or tailored optimization procedures. However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition. To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies, generating better training trajectories. In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance. CTPG is a general framework adaptable to existing parameter sharing approaches. Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks.

## 1 Introduction

Deep reinforcement learning (RL) has undergone remarkable progress over the past decades, show-casing its efficacy across various domains, such as game playing [16; 28] and robotic control [12; 13]. However, most of these deep RL methods primarily focus on learning different tasks in isolation, making it challenging to utilize shared information between tasks to develop a generalized policy. Multi-task reinforcement learning (MTRL) aims to master a set of RL tasks effectively. By leveraging the potential information sharing among different tasks, joint multi-task learning typically exhibits higher sample efficiency than training each task individually .

A significant challenge in MTRL lies in determining what information should be shared and how to share it effectively. Recent studies have proposed various approaches to tackle this challenge via network parameter sharing with carefully designed network structures [10; 22; 27] or tailored optimization procedures [4; 14; 30]. We summarize such methods as _implicit knowledge sharing_ in Section 2. Despite these unremitting efforts, another largely overlooked way exists to exploit cross-task similarities to improve the learning efficiency of multiple tasks. Intuitively, humans can effortlessly discern which skills can be shared from other tasks while learning a specific task. For instance, someone who can ride a bicycle can quickly learn to ride a motorcycle by referring torelated skills, such as operating controls, maintaining balance, and executing turns. Likewise, a motorcyclist adept in these skills can also quickly learn to ride a bicycle. This ability allows humans to efficiently master multiple tasks by selectively referring to skills previously learned. As shown in Figure 1, similar full or partial policy sharing is also evident in robotic arm manipulation tasks. These cross-task similarities enable _policy guidance_, _i.e_., control policies of tasks already proficient in specific skills can generate valuable training data for unmastered tasks. Compared to the common practice, which blindly generates training trajectories for each task solely with its own control policy, generating training trajectories using a control policy from other tasks that perform better in the current situation can better facilitate the learning procedure. Moreover, this _explicit policy sharing_ approach significantly reduces unnecessary exploration of similar contexts in different tasks.

The key challenge encountered in this approach to MTRL is discerning beneficial sharing control policies for each task adaptively. To address this challenge,  uses a Q-filter to identify single-step shareable behaviors without ensuring optimality for long-term policy sharing. In contrast, we propose a simple yet effective framework called Cross-Task Policy Guidance (CTPG) for more robust long-term policy guidance. Initially, we group the control policies of all tasks into a candidate set. Subsequently, for each task, we train a guide policy to identify useful sharing control policies, and then the chosen control policy generates better training trajectories to achieve _policy guidance_. Furthermore, we design two gating mechanisms to avoid unfavorable policy guidance interfering with learning. The first, policy-filter gate, leverages the value function to refine the candidate set by masking out control policies that are not beneficial for guidance. The second, guide-block gate, withholds extra guidance for the mastered easy tasks, allowing the focus to be on further solidifying the skills already acquired. With the incorporation of the above two gates, CTPG greatly improves the quality of the policy guidance, thereby fostering enhanced exploration and learning efficiency.

CTPG is a generalized MTRL framework that can be combined with various existing parameter sharing methods. Among these, we choose several classical approaches and integrate them with CTPG, achieving significant improvement in sample efficiency and final performance on both manipulation  and locomotion  MTRL benchmarks. Furthermore, we conduct detailed ablation studies to gain insights into how each component of CTPG contributes to its final performance.

## 2 Related Work

Multi-task learning is a training paradigm that enhances generalization by leveraging the information inherent in potentially related tasks [3; 19; 34]. Multi-task reinforcement learning extends this concept to reinforcement learning, expecting that information shared across tasks will be uncovered by simultaneously learning multiple RL tasks . In this study, we distinguish between information sharing as implicit knowledge sharing and explicit policy sharing.

Implicit Knowledge Sharing.Implicit knowledge sharing primarily focuses on sharing parameters or representations, but it encounters the challenge of negative knowledge transfer due to simultaneous updates within the same network. [14; 30] regard MTRL as a multi-objective optimization problem aimed at managing conflicting gradients resulting from different task losses during training. [22; 27]

Figure 1: Full or partial policy sharing in the manipulation environment. (a): Task _Button-Press_ and _Drawer-Open_ share almost the same policy, where the robotic arm needs to reach a specified position (button or handle) and then push the target object. (b): Task _Door-Open_ and _Drawer-Open_ share the policy of grabbing the handle in the first phase, but they are required to open the target object by different movements (rotation or translation).

partition the network into distinct modules and combine these modules to form different sub-policies for different tasks. [5; 21] endeavor to choose or learn better representations as more effective task-conditioned information for policy training. [7; 23] employ distillation and regularization to fuse separate task-specific policies into a unified policy for diverse tasks.

Explicit Policy Sharing.Explicit policy sharing is expressed as the direct sharing of behaviors or policies between different tasks.  employs a hierarchical policy that decides when to directly use a previously learned policy and when to acquire a new one. Nonetheless, instead of learning multiple tasks simultaneously, it adopts a sequential task-learning approach, necessitating a manually well-defined curriculum of tasks.  uses a Q-filter to identify shareable behaviors. During exploration, each control policy proposes a candidate action, and the policy that suggests the maximum Q-value action on the source task is executed for the following timesteps. However, maximizing Q-value in a single timestep does not guarantee the optimality of this policy across continuous timesteps. CTPG is a new explicit policy sharing method that learns a guide policy for long-term policy guidance.

## 3 Preliminaries

Multi-Task Reinforcement Learning.We aim to simultaneously learn \(N\) tasks, where each task \(i\) is represented as a Markov decision process (MDP) [1; 18]. Each MDP is defined by the tuple \( S,A,P_{i},R_{i},\), where \(S\) denotes the state space, \(A\) the action space, \(P_{i}:S A S\) the environment transition function, \(R_{i}:S A\) the reward function, and \([0,1)\) the discount factor. In the scope of this work, different tasks share the same state and action spaces, distinguished by different transition and reward functions. The goal of the MTRL agent is to maximize the average expected return across all tasks, which are uniformly sampled during training.

Soft Actor-Critic.In this work, we use the Soft Actor-Critic (SAC)  algorithm, an off-policy actor-critic method under the maximum entropy framework. The critic network \(Q_{}(s_{t},a_{t})\) parameterized by \(\), representing a soft Q-function , aims to minimize the soft Bellman residual:

\[J_{Q}()=_{(s_{t},a_{t},r_{t})} [(Q_{}(s_{t},a_{t})-(r_{t}+_{s _{t+1} P}[V_{}(s_{t+1})]))^{2}],\] (1) \[V_{}(s_{t})=_{a_{t}_{}}[Q_ {}(s_{t},a_{t})-_{}(a_{t}|s_{t})],\] (2)

where \(\) represents the data in the replay buffer, and \(\) is the target critic network parameter. The actor network \(_{}(a_{t}|s_{t})\) is parameterized by \(\), and the objective of policy optimization is:

\[J_{}()=_{s_{t}}[_{a_{t} _{}}[_{}(a_{t}|s_{t})-Q_{}(s_{t},a_{t})] ],\] (3)

where \(\) is a learnable temperature parameter to penalize entropy as follows:

\[J()=_{a_{t}_{}}[-_{}(a_{t}|s _{t})-}],\] (4)

where \(}\) is a desired minimum expected entropy. If the optimization leads to an increase in \(_{}(a_{t}|s_{t})\) with a decrease in the entropy, the temperature \(\) will accordingly increase. In the following sections, we use subscripts to signify the networks specific to each task. Specifically, the control policy of task \(i\) is represented as \(_{i}\), and the corresponding Q-value function is denoted as \(Q_{i}\).

## 4 Cross-Task Policy Guidance

Explicit policy sharing offers a direct and efficient way to master multiple tasks. If a task is already mastered, its control policy can be fully or partially shared with other tasks to guide tasks requiring similar skills to be quickly learned. Instead of each task generating trajectories constantly by its corresponding control policy, as in most existing MTRL algorithms, we consider using control policies of other tasks to generate training data for the current task when appropriate. To achieve this goal, we propose a novel framework called Cross-Task Policy Guidance (CTPG), which extra learns a guide policy for each task to identify beneficial policies for guidance. We illustrate the trajectory generation process of Task 1 in Figure 2. For this task, its guide policy \(_{i}^{q}\) selects a policy \(^{}\) from the candidate set of all control policies \(\{_{i}\}_{i=1}^{N}\) every fixed \(K\) timesteps. It then uses \(^{}\) as the behavior policy to interact with the environment and collect data for the next \(K\) timesteps.

The CTPG framework alters only the data collection process, guiding the control policy training through better exploration trajectories. In Section 4.1, we introduce the guide policy in detail and propose a hindsight off-policy correction mechanism for its training. In addition, we propose two gating mechanisms to enhance the efficiency of CTPG: the policy-filter gate discussed in Section 4.2 and the guide-block gate detailed in Section 4.3.

### Guide Policy

The control policy \(_{i}(a_{t}|s_{t})\) of task \(i\) maps the state \(s_{t}\) to the environment action \(a_{t}\), and its corresponding Q-value function \(Q_{i}(s_{t},a_{i})\) estimates the expected return. The guide policy \(^{g}_{i}(j_{t}|s_{t})\) of task \(i\) outputs a _task index_\(j_{t}\), and the corresponding control policy \(_{j_{t}}\) of task \(j_{t}\) serves as the behavior policy. The guide Q-value function \(Q^{g}_{i}(s_{t},j_{t})\) estimates the expected return of using \(^{g}_{i}\) to select different control policies for every \(K\) timesteps, with its Bellman equation defined as follows:

\[^{^{g}_{i}}Q^{g}_{i}(s_{t},j_{t}) R^{g}_{i}(s_{t},j_{ t})+^{K}_{j_{t+K}^{g}_{i},s_{t+K} P_{i}}[Q^{g}_{i} (s_{t+K},j_{t+K})],\] (5)

where \(^{^{g}_{i}}\) is the Bellman operator of the guide policy \(^{g}_{i}\), and the corresponding reward function \(R^{g}_{i}\) is defined as the expected cumulative discount rewards for behavior policy \(_{j_{t}}\) over \(K\) timesteps:

\[R^{g}_{i}(s_{t},j_{t})=_{a_{t^{}}_{j_{t}},s_{t^{} +1} P_{i}}[_{t^{}=t}^{t+K-1}^{t^{}-t}R_{i}(s_{t^ {}},a_{t^{}})].\] (6)

For each task \(i\), during trajectory generation, CTPG first utilizes its guide policy \(^{g}_{i}\) to sample a behavior policy \(_{j_{t}}\) from the candidate set of all tasks' control policies:

\[j_{t}^{g}_{i}(|s_{t}),\] (7)

and then samples actions using the behavior policy \(_{j_{t}}\) for the next \(K\) timesteps:

\[a_{t^{}}_{j_{t}}(|s_{t^{}}),\] (8)

where \(t^{}\{t,t+1,,t+K-1\}\). After each timestep, we obtain the reward \(r_{t^{}}=R_{i}(s_{t^{}},a_{t^{}})\) and the next state \(s_{t^{}+1}\). The transition \( i,s_{t^{}},a_{t^{}},j_{t},r_{t^{}},s_{t^{}+1}\) is stored in the replay buffer.

The guide policy is trained to maximize the expected return of the current task by choosing appropriate control policies in certain states. If the control policies of some tasks already proficient in specific skills can be shared with the current task in similar states, the guide policy can quickly learn to use these control policies in those states to generate better training data for the current task. The guide policy and the control policy are trained simultaneously. We can use any off-policy RL algorithms for control policy training and any on/off-policy RL algorithms for guide policy training. In this work, we use SAC  for the control policy training and the discrete action space variant of SAC  for the guide policy training. Given that the guide policy acts every \(K\) timesteps, its training frequency is \(1/K\) that of the control policy. The detailed pseudo-codes for the control policy and guide policy training are provided in Appendix A.1 (Algorithms 1 and 2).

Hindsight Off-Policy Correction.During off-policy training, the guide policy faces a non-stationary challenge. Since the control policies are continually updated during the training of the guide policies, the actions chosen by the behavior policies during data collection may no longer align with the improved corresponding control policies, thereby compromising the validity of the training experience. We address this concern by implementing a hindsight off-policy correction mechanism that reassigns the action \(j_{t}\) sampled by the past guide policy to a new one \(j^{}_{t}\), whose control policy \(_{j^{}_{t}}\) is more likely to output the historical action sequence \(\{a_{t^{}}\}_{t^{}=t}^{t+K-1}\). Specifically, we utilize maximum likelihood estimation following:

\[j^{}_{t}=_{j}_{t^{}=t}^{t+K-1}_{j}(a_{t^{}}| s_{t^{}})=_{j}_{t^{}=t}^{t+K-1}_{j}(a_{t^{}}| s_{t^{}}).\] (9)

In this way, we can leverage past experiences effectively to train the guide policy. The workings of the hindsight off-policy correction mechanism in SAC are detailed in Appendix C.

Figure 2: Overview of the CTPG framework.

### Not All Policies Are Beneficial for Guidance

In Section 4.1, we set the guide policy's action space as the set of all control policies \(\{_{i}\}_{i=1}^{N}\). However, not all control policies within the action space of \(_{i}^{g}\) are beneficial for task \(i\) in state \(s_{t}\). Some control policies perform even worse than the current task's own control policy \(_{i}\), rendering them ineffective for guidance. To address this issue, we design a policy-filter gate to refine the action space of the guide policy by adaptively filtering out unfavorable control policies in state \(s_{t}\). The trajectory generation process solely using the current task's control policy \(_{i}\) can be regarded as equipped with a special guide policy \(_{i}^{}\) that exclusively selects \(_{i}\) as the behavior policy, _i.e._, \(_{i}^{}(i|s_{t})=1\) for any \(s_{t}\). The guide Q-value \(Q_{i}^{}\) of \(_{i}^{}\), defined by Equation 5, is:

\[ Q_{i}^{}(s_{t},i)&=R_{i}^{g} (s_{t},i)+^{K}_{s_{t+K} P_{i}}[Q_{i}^{}(s_{t +K},i)]\\ &=_{a_{t^{}}_{i},s_{t^{}+1} P_{ i}}[_{t^{}=t}^{t+K-1}^{t^{}-t}R_{i}(s_{t^{}},a_{ t^{}})+^{K}Q_{i}^{}(s_{t+K},i)]\\ &=\\ &=_{a_{t^{}}_{i},s_{t^{}+1} P_{ i}}[_{t^{}=t}^{}^{t^{}-t}R_{i}(s_{t^{}},a_{ t^{}})]\\ &=V_{i}(s_{t}),\] (10)

where we repeatedly expand \(Q_{i}^{}\) to find that \(Q_{i}^{}(s_{t},i)\) is equal to \(V_{i}(s_{t})\), the state value function of task \(i\)'s control policy. Because the value function can serve as a filter for high-quality training data [17; 29; 33], it becomes intuitive to judge the quality of the behavior policy \(_{j}\), following guide policy and the current task's control policy \(_{i}\) by directly comparing \(Q_{i}^{g}(s_{t},j_{t})\) and \(V_{i}(s_{t})\). In our implementation, we estimate \(V_{i}(s_{t})\) via Monte Carlo sampling of \(Q_{i}(s_{t},a_{t})\) with \(a_{t}_{i}(a_{t}|s_{t})\). Relying on this mechanism, the policy-filter gate serves as a mask vector \(m(s_{t})\) to indicate whether each control policy is beneficial for guidance in state \(s_{t}\). Specifically, each element of \(m(s_{t})\) is:

\[m_{j}(s_{t})=1,&Q_{i}^{g}(s_{t},j) V_{i}(s_{t}),\\ 0,&Q_{i}^{g}(s_{t},j)<V_{i}(s_{t}),j\{1,2,,N\},\] (11)

where \(j\) indicates the element index and task index. Then, the behavior policy \(_{j_{t}}\) is sampled by:

\[j_{t}(_{i}^{g}(|s_{t}) m(s_{t})).\] (12)

If none of the control policies are beneficial for guidance, _i.e._, \(m(s_{t})=\), it indicates that the current task's control policy \(_{i}\) is the most proficient within the current state, rendering other control policies unnecessary for enhancing trajectory generation.

Comparable Guide Q-Value.Typically, RL algorithms estimate Q-values to approximate the expected return of the current state-action pair, allowing for the calculation of the policy-filter gate in Equation 11 through a direct comparison of \(V_{i}(s_{t})\) and \(Q_{i}^{g}(s_{t},j_{t})\). However, in maximum entropy RL algorithms such as SAC, Q-value estimation incorporates the maximum entropy objective, leading to the incomparability of two policies with different entropy objectives. Therefore, we learn another comparable guide Q-value \(_{i}^{g}\) with discounted entropy of \(_{i}\) following:

\[_{i}^{g}(s_{t},j_{t})&=_{a_{t^{}}_{j_{t}},s_{t^{}+1} P_{i}}[_{t^{ }=t}^{t+K-1}^{t^{}-t}(R_{i}(s_{t^{}},a_{t^{ }})+_{i}(_{i}(|s_{t^{}})))]\\ &+^{K}_{j_{t+K}_{i}^{g},s_{t+K} P_{i}} [_{i}^{g}(s_{t+K},j_{t+K}^{g})].\] (13)

Since both \(_{i}^{g}(s_{t},j_{t})\) and \(V_{i}(s_{t})\) estimate the return with the entropy of the current task's control policy, they can be directly compared to assess whether control policies are beneficial for guidance. A detailed comparability analysis of this comparable guide Q-value in SAC is provided in Appendix B.

### Not All Tasks Need Guidance

When simultaneously learning multiple tasks, easy tasks converge faster than difficult ones. The control policies of easy tasks allow for the quick acquisition of some effective skills, which maybe helpful in exploring other tasks. However, these mastered or easy tasks do not need additional guidance from other policies; instead, they focus on further solidifying their already acquired skills. Therefore, not all tasks require assistance from the guide policy.

Based on the above analysis, we design another guide-block gate to prevent the guide policy from engaging in tasks that do not necessitate guidance. This mechanism is directly related to SAC's temperature coefficient \(_{i}\). For difficult tasks \(i_{}\), their control policy entropies \((_{i_{}}(|s_{t}))\) tend to be high, and the corresponding temperature parameters \(_{i_{}}\) decrease according to Equation 4. Conversely, the temperature parameters \(_{i_{}}\) increase for easy tasks \(i_{}\). Therefore, \(_{i}\) is a metric reflecting the relative difficulty and mastery of different tasks. We form the tasks that require guidance into a subset \(^{g}\) following:

\[^{g}=\{i|_{i}_{j=1}^{N} _{j}\},\] (14)

which selects the tasks by comparing the difficulty of the current task versus the average of all tasks. For tasks \(i^{g}\), the guide-block gate restricts them from using the guide policy, so they only use their own control policies to interact with the environment. Consequently, the guide policy can stop training with samples from the tasks \(i^{g}\) and focus on learning the guidance for unmastered tasks.

The comprehensive CTPG framework, with two special gating mechanisms, is summarized in Figure 3. The complete pseudo-code for CTPG is described in Appendix A.2 (Algorithm 3).

## 5 Experiments

The experiments are designed to answer the following research questions: **Q1:** Can implicit knowledge sharing approaches be combined with CTPG to further improve performance? **Q2:** Does the guide policy in CTPG learn useful sharing policies? **Q3:** How does each component within CTPG contribute to the final performance? **Q4:** How does CTPG perform without implicit knowledge sharing approaches? **Q5:** Can CTPG expedite the exploration of new tasks effectively?

### Environments

We conduct experiments on MetaWorld manipulation and HalfCheetah locomotion MTRL benchmarks, selecting two setups for evaluation within each benchmark.

MetaWorld Manipulation Benchmark.The MetaWorld benchmark  consists of 50 robotics manipulation tasks employing a sawyer arm in the MuJoCo environment . It provides two setups: _MetaWorld-MT10_, comprising a suite of 10 tasks, and _MetaWorld-MT50_, comprising a suite of 50 tasks. Following the settings in , the goal position is randomly reset at the start of every episode. We use the mean success rate as our evaluation metric, which is clearly defined in the environment.

HalfCheetah Locomotion Benchmark.The HalfCheetah is a 6-DoF walking robot consisting of 9 links and 8 joints connecting them in the MuJoCo environment . The multi-task benchmark HalfCheetah Task Group  contains different HalfCheetah robots. _HalfCheetah-MT5_ includes 5

Figure 3: Illustration of the comprehensive CTPG framework. Initially, the guide-block gate selectively provides guidance on tasks \(i^{g}\). Subsequently, the policy-filter gate generates a mask \(m\) to sift through the beneficial policies. Finally, the policy chosen by the guide policy or the control policy of the current task itself interacts with the environment over \(K\) timesteps to collect training data.

tasks under various scales of simulated earth-like gravity, ranging from one-half to one-and-a-half of the normal gravity level. _HalfCheetah-MT8_ includes 8 tasks with various morphology of a specific robot body part. We use the episode return as our evaluation metric.

Further information regarding environmental setups is provided in Appendix D.

### Performance Improvement on Implicit Knowledge Sharing Approaches

CTPG is a generalized MTRL framework adaptable to various implicit knowledge sharing approaches, wherein both the control policy and the guide policy use a unified network. Specifically, in our implementation, the unified control policy employs the same network structure and update procedure as the implicit knowledge sharing approaches, and the unified guide policy utilizes a straightforward multi-head structure for parameter sharing.

To answer **Q1**, we choose five classical implicit knowledge sharing approaches: (1) **MTSAC** extends SAC for MTRL by employing one-hot encoding for task representation. (2) **MHSAC** utilizes a shared network backbone apart from independent heads for each task. (3) **PCGrad** resolves issues arising from conflicting gradients among tasks through gradient manipulation. (4) **SM** trains a routing network to propose weights for soft module combinations. (5) **PaCo** learns a compositional policy where task-shared parameters combine with task-specific parameters to form task policies. Combined with these implicit knowledge sharing approaches, we compare CTPG against: (i) **Base** represents the source version of these approaches. (ii) **QMP** employs a one-step Q-value filter to identify shareable behaviors.

We train all combinations with 0.8 million samples per task in the HalfCheetah locomotion benchmark and 1.5 million samples per task in the MetaWorld manipulation benchmark. Each combination is trained 5 times with different seeds. We evaluate the final policy over 100 episodes per task and report the mean performance and standard deviation across different seeds in Table 1.

Based on the experimental findings, it becomes evident that, except for the PaCo algorithm in HalfCheetah-MT8, the combination with CTPG leads to a notable enhancement in performance across all scenarios. QMP does not perform as well as CTPG because QMP only offers single-step behavior guidance, whereas CTPG's guide policy learns long-term policy guidance. Notably, as the number of tasks increases, CTPG exhibits superior performance due to the higher probability of explicit policy sharing between tasks, facilitating more effective policy exploration. Besides the final performance evaluation, comprehensive training curves are presented in Appendix E.1.

   Environment & Method & MTSAC & MHSAC & PCGrad & SM & PaCo \\   & Base & 9.16 \(\) 0.42 & 8.68 \(\) 0.55 & 9.57 \(\) 0.73 & 9.57 \(\) 0.21 & 7.18 \(\) 0.44 \\  & _w_/ QMP & 8.81 \(\) 0.22 & 9.09 \(\) 0.64 & 9.46 \(\) 0.57 & 10.09 \(\) 0.53 & 7.83 \(\) 0.28 \\  & _w_/ CTPG & **9.59 \(\) 0.40** & **9.25 \(\) 0.12** & **10.27 \(\) 0.40** & **10.47 \(\) 0.34** & **7.95 \(\) 0.47** \\    & Base & 9.00 \(\) 0.88 & 8.90 \(\) 0.60 & 10.17 \(\) 1.06 & 10.05 \(\) 0.55 & 8.44 \(\) 0.56 \\  & _w_/ QMP & 10.00 \(\) 0.47 & 9.61 \(\) 0.54 & 10.65 \(\) 0.43 & 10.41 \(\) 0.61 & **9.28 \(\) 0.48** \\  & _w_/ CTPG & **10.17 \(\) 0.31** & **9.82 \(\) 0.40** & **[11.09 \(\) 0.50]** & **10.81 \(\) 0.51** & 9.02 \(\) 0.48 \\    & Base & 62.72 \(\) 6.19 & 63.51 \(\) 2.97 & 69.62 \(\) 4.04 & 74.52 \(\) 2.29 & 69.77 \(\) 7.28 \\  & _w_/ QMP & 64.91 \(\) 8.82 & 65.87 \(\) 3.05 & 67.53 \(\) 2.93 & 69.78 \(\) 7.50 & 69.84 \(\) 3.49 \\  & _w_/ CTPG & **75.76 \(\) 3.82** & **74.94 \(\) 2.97** & **73.31 \(\) 3.66** & **[78.97 \(\) 2.41]** & **70.40 \(\) 3.62** \\    & Base & 47.51 \(\) 1.95 & 52.04 \(\) 2.78 & 52.85 \(\) 4.12 & 55.04 \(\) 2.84 & 59.46 \(\) 5.14 \\  & _w_/ QMP & 47.82 \(\) 1.62 & 51.79 \(\) 4.83 & 54.05 \(\) 1.39 & 55.91 \(\) 5.08 & 53.81 \(\) 2.00 \\  & _w_/ CTPG & **55.97 \(\) 2.56** & **56.91 \(\) 2.57** & **58.91 \(\) 2.10** & **66.24 \(\) 3.37** & **[68.10 \(\) 3.44]** \\   

Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.

### Guidance Learned by Guide Policy

To answer **Q2**, we visualize the task _Pick-Place_ with guidance on MetaWorld-MT10 to show the specific role of the guide policy. Since the guide policy is only used in trajectory generation during training, we visualize one of the sampled trajectories in Figure 4. We only present the initial 50 timesteps of this trajectory, in which the task has essentially been completed.

To better understand policy sharing between tasks, we first explain the relevant tasks. _Pick-Place_: Pick and place a puck to a goal. _Peg-Insert-Side_: Insert a peg sideways. _Push_: Push the puck to a goal. _Button-Press-Topdown_: Press a button from the top. _Drawer-Close_: Push and close a drawer. _Reach_: Reach a goal position. The visualizations of these tasks are provided in Figure 8 (Appendix D.1).

The initial and final 20 timesteps showcase that the guide policy learns useful guidance to successfully complete the source task. In the initial 20 timesteps, _Pick-Place_ and _Peg-Insert-Side_ employ a shared policy directing the robotic arm toward the target object. In the final 20 timesteps, the task is executed using _Button-Press-Topdown_ to raise the gripper and then _Drawer-Close_ to move forward. Interestingly, although the task in the final 20 timesteps intuitively aligns with _Reach_, the guide policy opts not to use it because the learned _Reach_'s control policy always opens the gripper, causing the puck to fall. In the middle 10 timesteps, the probability of _Pick-Place_ is notably high due to the absence of alternative shared policies at this stage. Specifically, the most similar _Peg-Insert-Side_ grabs the end of the peg for insertion, while _Pick-Place_ requires gripping the puck centrally.

### Ablation Studies

To answer **Q3**, we conduct detailed ablation studies to analyze the impact of each component in CTPG on performance enhancement. CTPG contains three key components: (1) the policy-filter gate, (2) the guide-block gate, and (3) the hindsight off-policy correction mechanism. We study the influence of each component using the MHSAC implicit knowledge sharing approach on MetaWorld-MT10.

(1) Figure 5(a) shows that the policy-filter gate plays a significant role within CTPG, leveraging the value function to constrain the direction of guided exploration. (2) Given the precise definition of binary-valued success signal in the MetaWorld benchmark, we compare the SAC temperature metric, described in Section 4.3, with another success rate metric to determine if tasks need guidance.

Figure 4: We display the state of task _Pick-Place_ at every 10 timesteps, along with the corresponding output probability of the guide policy and the actual sampled behavior policy. Except for employing the _Pick-Place_ task’s control policy during timesteps 20 to 30, the guide policy selects control policies of other tasks for the remaining timesteps, successfully accomplishing the task.

Figure 5: Three distinct ablation studies of MHSAC w/ CTPG on MetaWorld-MT10.

Specifically, we block guidance for tasks with success rates exceeding \(80\%\) during evaluation. Figure 5(b) illustrates that both metrics showcase competitive performance, exhibiting clear performance gains over no guide-block gate. However, since the success rate is a human-defined metric and difficult to define for some tasks (_e.g._, HalfCheetah), the SAC temperature metric is more general across most environments. (3) We ablate the hindsight off-policy correction mechanism used in guide policy training in Figure 5(c), elucidating its improvement in training efficiency and stability. Further ablation studies of SM with CTPG on MetaWorld-MT50 are provided in Appendix E.2.

### CTPG without Implicit Knowledge Sharing

To answer **Q4**, we train \(N\) control and guide policies independently on \(N\) tasks without implicit knowledge sharing. We perform experiments on HalfCheetah-MT8 and MetaWorld-MT10, employing the same environment configuration as in Section 5.2. As illustrated in Figure 6, CTPG significantly improves the performance of Single-Task SAC. Notably, the impact of CTPG on MetaWorld-MT10 is more pronounced. The potential rationale is that the tasks within HalfCheetah-MT8 exhibit minimal variance in difficulty, resulting in a narrow gap in learning progress. Conversely, MetaWorld-MT10 presents a broader disparity in task difficulty, where CTPG facilitates the guidance from simpler to more challenging tasks, thus appearing more efficient.

### Exploration of New Tasks with CTPG

To answer **Q5**, we split the original task set in half, pre-training expert policies on the one half \(^{e}\). For the other half, we compare direct learning with CTPG, where the agent leverages guidance from both the control policies being learned and the expert policies. Specifically, we use MT-SAC on HalfCheetah-MT8, where \(^{e}\) includes all tasks that enlarge the size of body parts. On the other hand, we use SM on MetaWorld-MT10, with \(^{e}\) containing tasks indexed from 0 to 4. Figure 7 indicates that, compared to learning the new set of tasks from scratch, CTPG can reasonably utilize experts to explore new tasks rapidly. Notably, in environments where task similarity is high, such as HalfCheetah-MT8, CTPG can quickly transfer the expert's abilities to the control policy being learned with the guide policy.

## 6 Conclusion and Discussion

This paper proposes the Cross-Task Policy Guidance (CTPG) framework for the MTRL explicit policy sharing. CTPG contains a guide policy and two special gates to identify beneficial sharing policies from the set of all task control policies and choose the most proficient one to generate high-quality trajectories for the current task as policy guidance. In addition, CTPG is a generalized framework adaptable to diverse implicit knowledge sharing approaches. Empirical evidence showcases that these approaches combined with CTPG further improve sample efficiency and final performance.

Limitations and Future Works.One limitation of CTPG is its reliance on a predetermined guide step \(K\), necessitating hyperparameter tuning for \(K\) across different environments. Moreover, the fixed guide step setting lacks flexibility, as the duration of shared skills execution timesteps varies inconsistently among different tasks. Consequently, exploring methods to automate the selection of the guide step \(K\) presents an intriguing avenue for future research. Additionally, irrespective of the human-defined win rate and the unique temperature parameter of SAC, investigating alternative metrics for the guide-block gate emerges as another important direction for future endeavors.

Figure 6: CTPG also improves performance in the absence of implicit knowledge sharing approaches.

Figure 7: CTPG with expert policies can expedite the exploration of new tasks effectively.

Acknowledgements

This work is supported in part by the National Science and Technology Major Project (2022ZD0116401), the Natural Science Foundation of China (Grant Nos. 62076238, 62222606, and 61902402), the Key Research and Development Program of Jiangsu Province (Grant No. BE2023016), and the China Computer Federation (CCF)-Tencent Open Fund.