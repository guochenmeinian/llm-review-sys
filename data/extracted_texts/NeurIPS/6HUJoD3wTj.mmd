# Separations in the Representational Capabilities of Transformers and Recurrent Architectures

Satwik Bhattamishra\({}^{1}\)\({}^{}\) Michael Hahn\({}^{2}\) Phil Blunsom\({}^{1,3}\) Varun Kanade\({}^{1}\)\({}^{1}\)

\({}^{1}\)University of Oxford \({}^{2}\)Saarland University \({}^{3}\)Cohere

Corresponding Authors: satwik.bmishra, varun.kanade@cs.ox.ac.uk

###### Abstract

Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality. For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size. Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of \(N\) nearly orthogonal vectors in \(O( N)\) dimensional space and our lower bounds are based on reductions from communication complexity problems. We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.

## 1 Introduction

Transformers  are the go-to architecture for building LLMs , but recently, there has been significant interest in reviving recurrent architectures to build LLMs for practical tasks  to circumvent the quadratic complexity of inference for Transformers. While Transformers process all tokens in parallel, maintaining \(N\) vectors, and are in some sense stateless, recurrent models primarily store information in a fixed-size hidden state that they can update during the course of the computation. This raises a fundamental question that we explore: Are there tasks that are computationally easier for one architecture to represent but significantly more difficult for the other one?

Ever since Transformers supplanted LSTMs, there has been significant interest in understanding the differences in the computational capabilities of the models both theoretically and empirically. On the one hand, even as Transformers have been widely adopted for building LLMs, several studies found that they struggled at modeling various formal languages, particularly those that required modular counting or state-tracking . At the same time, despite substantial effort, it has proven hard to match the performance of Transformer-based LLMs at scale with recurrent architectures ; in particular, it has been observed that LLMs based on recurrent models struggle on associative recall or extraction-related tasks . More recently, Arora et al.  and Bhattamishraet al.  demonstrated that Transformers are better than attention-free models at synthetic tasks based on associative recall and more general forms such as implementing nearest neighbors. Based on these observations, it is natural to wonder if such tasks are theoretically easier for Transformers to represent in comparison to recurrent models.

**Our Contributions.** We show differences in the representational capabilities of Transformers and recurrent models across several natural tasks of real-world relevance. In this paper, we show strong _separation_ results: when a task is easy we show that it can be expressed by one architecture with size poly-logarithmic in the input length \(N\); on the other hand we show the other type of architecture requires the size to be linear in \(N\). We describe the tasks studied and our key results below.

**(i) Index Lookup.** Given a sequence of symbols \(s_{1},,s_{N}\) followed by a position \(p[N]\), a model has to output the symbol in the \(p\)-th position \(s_{p}\) (Figure 0(a)). Our first result shows that one-layer Transformers with size poly-logarithmic in \(N\) can express this task (Theorem 1) whereas any recurrent model must have width \((N)\) to perform this task (Theorem 3).

**(ii) Bounded Dycks.** Dyck languages with bounded depth require a model to recognize whether a string of parentheses is well-balanced (Figure 0(b)). These formal languages have received a great deal of attention in the study of neural sequence models because they capture hierarchical dependencies that occur in natural languages [17; 25; 23; 6]. They are also central to formal language theory as all context-free languages can be expressed in terms of Dyck languages . In contrast to the results for index lookup, we show that one-layer Transformers must have a size that grows linearly in input length to represent this task (Theorem 5), whereas prior works found that constant-size recurrent models can express this task [25; 6].

**(iii) String Equality.** This task is formalized by the Equality function \((,)=[=]\) where \(,\) are two strings of length \(N\); it models the natural task of matching two documents. We show that log-sized two-layer Transformers can represent the function \(\) (Theorem 6), whereas both one-layer Transformers (Theorem 12) and recurrent models (Theorem 11) must have a size that grows linearly with the sequence length. These results extend to a broader class of Boolean functions.

**(iv) Nearest Neighbor and Associative Recall.** In this task, a model is provided with a sequence of inputs and labels \(_{1},y_{1},,_{k-1},y_{k-1}\) followed by a query input \(_{k}\). The model has to determine the label \(y_{k}\) of the query input by applying the nearest neighbor algorithm in its forward pass (Figure 0(c)). This task subsumes various associative recall tasks considered in earlier works (cf. Appendix G) and was introduced to understand in-context learning. Bhattamishra et al.  found empirical differences in the performance of Transformers and attention-free architectures, though theoretical understanding is lacking. We show that a two-layer Transformer with size logarithmic in the input length can perform this task (Theorem 7), whereas recurrent models require a size linear in the length (Theorem 8).

We also empirically investigated the performance of Transformers and standard recurrent models , including recently proposed state-space models [22; 21] on these tasks. The observed behavior is along the lines indicated by our theoretical results.

**Our Techniques.** For constructing Transformers that perform the tasks, one of the key requirements is the ability to precisely attend to specific positions. In order to do this with low-dimensional embeddings, our constructions make use of nearly orthogonal vectors obtained using the Johnson-Lindenstrauss lemma ; furthermore these can be generated efficiently in logarithmic space which allows the size of the models to be poly-logarithmic in the input length (cf. Appendix B.3).

For our lower bounds, we appeal to results from communication complexity. We show how to obtain protocols with communication complexity bounded by the size of the models. Together with established (and new) communication complexity lower bounds, we obtain lower bounds on the model size. For our lower bound for one-layer Transformers, we derive a lower bound on the communication complexity for bounded Dyck languages (Lemma 1).

### Related Work

**Expressivity of Sequence Models.** The expressive power of neural sequence models has been an active area of research, targeting both RNNs and Transformers [e.g. 23; 52; 40; 55; 37]. With parameters of infinite precision, Transformers are universal function approximators  and Turing complete [47; 7]. In bounded precision settings, they relate to Boolean circuits [39; 24] and logical formalisms . Importantly, such studies usually consider asymptotic expressivity of a single model in the limit of unboundedly long inputs. Our study provides a more fine-grained and realistic picture by accounting for the size of the network, and its scaling with the input. The closest to our work is Sanford et al. , who first used communication complexity to prove lower bounds for Transformers and RNNs for abstract tasks such as a sparse averaging task and pair/triple detection tasks. Our work extends that line of work to show separations on natural tasks of practical and real-world relevance.

**Formal languages and Algorithmic tasks.** A strand of work has sought to understand sequence models via empirical analysis on formal languages and algorithmic tasks . Numerous works have examined the ability of recurrent models  and Transformers  to model Dyck languages. More recently, significant effort  has been devoted to investigating how Transformers can learn to implement learning algorithms in their forward pass to understand the in-context learning phenomenon. Bhattamishra et al.  empirically observed that attention-free architectures struggle to implement the nearest neighbor algorithm in comparison to Transformers. Our result takes a step toward understanding this phenomenon by showing that nearest neighbors can be implemented by small-sized Transformers but not by recurrent architectures.

## 2 Definitions

We consider two types of models: Transformers  and recurrent models. We use recurrent models to refer more generally to nonlinear RNNs such as LSTMs , state-space models  as well as variants of linear Transformer  which can process inputs in a recurrent manner .

For some finite alphabet \(\), a sequence model is given a sequence in \(^{N}\) and depending on the task outputs either \(\{0,1\}\) or a sequence of outputs. Each \(s_{i}\) from a sequence \(s_{1} s_{N}^{N}\) is mapped to a vector in \(^{d}\) via an embedding map \(:^{d}\). For Transformers, the input embedding function further takes the position \(i\) as input, along with \(s_{i}\). Each layer for Transformers and recurrent models maps inputs from \(^{N d}^{N d}\). A model \(M\) has _fixed precision_\(p\) if all the parameters of the model as well as the values in the intermediate vectors can be implemented with \(p\)-bit precision numbers (cf. Appendix B.2).

**Transformers.** Each layer of a Transformer has an attention block followed by an MLP block. The attention block takes as input \(^{N d}\) and applies the operation \(()=(_{Q}^{} _{K}^{})_{V}^{}\) where \(_{Q},_{K},_{V}^{m d}\). For simplicity, we will use \(Q(_{i})\) (and likewise \((_{i})\) and \(V(_{i})\)) to denote \(_{Q}_{i}\). The _width_ of the Transformer is \((m,d)\), where \(m d\) is the shape of the projection matrices \(_{Q},_{K}\). For any matrix \(^{N M}\), the \(\) operator is applied row-wise as follows \(()_{i,j}=_{i,j})}{_{k=1}^{M }(_{i,k})}\). Multi-head attention with \(H\) heads is defined as \(_{H}()=[_{1}(),,_{H}()]_{O}\) where each \(_{i}()\) has its own set of parameters. The matrix \(_{O}^{mH d}\) projects the concatenated vector to a vector of dimension \(d\). For an input \(^{N d}\), the output of a layer of Transformer will be \((())^{N d}\) where \(:^{d}^{d}\) is a feedforward network. We use \(^{L}_{m,p,H}\) to denote the class of all Transformers operating over \(p\)-bit precision numbers with width \(m\), \(H\) heads, and at most \(L\) layers.

**Recurrent Models.** A general recurrent neural network (RNN) takes as input the sequence \(_{1},,_{N}\) where \(_{i}^{d}\) and produces an output sequence \(y_{1},,y_{N}\); in this paper we will mostly consider the case when \(y_{i}\{0,1\}\). An RNN with a hidden state of size \(m\) over \(p\)-bit numbers can be defined as follows. The hidden state is an \(mp\)-bit memory \(_{i}\{0,1\}^{mp}\) and for some \(_{0}\{0,1\}^{mp}\), the RNN computes \(_{t}=g_{(t)}(_{t},_{t-1})\) and \(y_{t}=f_{(t)}(_{t})\) for \(t=1,,N\), and \(g_{(t)}\) and \(f_{(t)}\) are arbitrary functions. Since the transition function is allowed to be arbitrary, this definition captures the general family of _multi-layer_ recurrent architectures including LSTMs, state-space models, and linear Transformers, each of which differs in the way the transition function is defined. For a recurrent model, we say that the _representation size_ of a hidden state is \(mp\), and its _width_ is \(m\), i.e. the hidden state consists of \(m\) units each of which uses \(p\)-bits. Throughout the paper, we will use recurrent models or RNNs to refer to the general family of recurrent architectures mentioned above.

By the _representation size_ of a model, we will refer to the total number of bits required to represent the model including all the parameters and embeddings. For Transformers, this is \((mdpH)\). For a recurrent model, the representation size is at least \(mp\).

## 3 Index Lookup Task

**Task Description.** We introduce a simple task called Index Lookup (IdxL). In this task a model receives a sequence of tokens \(s_{1},,s_{N}\) (possibly with repetitions) followed by an index \(p\) where \(p[N]\) and the goal of the model is to output the token \(s_{p}\). Here the symbols \(s_{i}\) belong to a finite alphabet \(\).

This simple and natural task helps illustrate the key tools and building blocks we use to obtain other more general results in this paper: On the one hand, we show how a one-layer Transformer with width \(O( N)\) can perform this task; on the other hand, we use communication complexity arguments to show that any type of recurrent or state-space model performing this task needs a hidden state with representation size \((N)\).

Our first result shows that, for any length \(N\), there is a \(1\)-layer Transformer with width \(O( N)\) that performs the Index Lookup task for all input sequences of length at most \(N\). Naively one could construct such a transformer by using one-hot encodings as positional embeddings, as they are orthogonal and would allow to attend to the desired index. However, this would require the embedding dimension, and hence the width of the model, to be \((N)\). Key to our constructions of a width \(O( N)\) Transformer, both here and in other sections, is a result (Lemma 2 in the Appendix) which states that, in \(k=O( N/^{2})\) dimensional space we can find \(N\) nearly orthogonal vectors. We use such vectors in our construction of the Transformers to allow it to attend almost exactly over desired positions.

**Theorem 1**.: _For all \(N\), there is a \(1\)-layer Transformer with width \(m=O( N)\) and precision \(p=O( N)\) which performs the index lookup task for all input sequences of lengths up to \(N\)._

Proof Sketch.: For an input sequence \((s_{1},,s_{N},p)\), the Transformer uses the embeddings of the position token \(p\) and the positional embeddings of the first \(N\) inputs to attend over \(s_{p}\), so that the feedforward network can extract the label from the output of the attention block. Our key idea is to use the \(N\) almost orthogonal vectors provided by Lemma 2, both as positional embeddings and also as a way to embed the numbers \(\{1,,N\}\), any of which can be used as the index \(p\). Formally, let \((1),,(N)\) be \(N\) vectors of dimension \(k=O( N)\) such that \((i),(j) 1/4\) for \(i j\) and \((i),(j) 3/4\) for \(i=j\).

Formal details of the construction are in Appendix C.2; we provide a sketch. The embedding of each input token is of size \(||+2k\) where \(||+k\) entries are used for the token embeddings and the last \(k\) entries are used for the positional embeddings. The query and key matrices are designed so that the query vector \(Q()=[(p)]\), the key vectors \(K(_{i})=[(i)]\) and \(K()=[_{k}]\). The value vectors simply contain the token embeddings \(V(_{i})=[(s_{i})]\), where \(:\{0,1\}^{||}\) is some binary encoding of \(\). With such query and key vectors, the dot products in attention, \( Q(),K(_{i})\), are \( 3/4\) if \(i=p\), and \(/4\) otherwise. The dot product of the query vector with itself will be \( Q(),K()=0\). We choose \(>0\) to scale the dot products to amplify the difference between the _high_ and _low_ dot products. Thus we have,

\[(Q()^{}K())=)^{}K(_{p}))}{(Q()^{}K(_{ p}))+_{j p}(Q()^{}K(_{j}))})}{()+N()}\]

which is at least \(\) for some \(=( N)\); the total attention weight over the remaining tokens is at most \(<\). Recall that the value vectors contain the binary encodings of the input symbols, \(V(_{i})=[(s_{i})]\). The attention-weighted average value vector aligns closely with \((s_{p})\) as \(3/4\)

Figure 1: Illustration of a few key tasks considered in our work.

weight is on it. It is then straightforward to design a \(\)-FFN that can act as a threshold function to retrieve \((s_{p})\) from it, which leads to the desired output. 

We use results from communication complexity to show that RNNs require essentially \((N)\) width to solve this and several other problems. In communication complexity, there are two parties, typically called Alice and Bob, each of whom has part of the (discrete) input, and their goal is to compute a function of the combined input using as little communication as possible. Our first key insight here is that the output of an RNN can be computed with a bounded amount of communication when Alice has a prefix of the input and Bob has the remaining part. The resulting protocol will be one-way (Alice to Bob) and one-round. We first state a more general result and then discuss the implications for the Index Lookup problem.

**Theorem 2**.: _If an RNN with a hidden state of representation size \(mp\) computes any function \(f:^{N}\{0,1\}\), then for any \(K<N\), if Alice has access to \(s_{1},,s_{K}\) and Bob has access to \(s_{K+1},,s_{N}\), then there exists a one-way communication protocol with \(mp\) bits from Alice to Bob, by which Bob can compute the output of the function \(f(s_{1} s_{N})\)._

Proof.: Assume that both Alice and Bob have access to the RNN that represents the function \(f\). Alice can provide the sequence \(s_{1},,s_{K}\) to the recurrent model and iteratively update the hidden state from the initial state \(_{0}\) to obtain the \(K\)th hidden state \(_{K}\). Alice can then send the hidden state to Bob which requires \(mp\) bits. Bob can then update the hidden state using \(s_{K+1},,s_{N}\) to obtain \(_{N}\), from which he can obtain the output of the RNN. Note that Alice and Bob can compute the output using one-way communication of \(mp\) bits. 

Problems similar to Index Lookup are well-studied in communication complexity; specifically, the Index problem (See Appendix B.1) has a one-way communication complexity of \((N)\) (Fact 3). We deduce a lower bound on the size of the hidden state of RNNs by showing that any RNN that can represent the Index Lookup task can also compute the Index problem and since that implies the existence of a one-way communication protocol with \(mp\) bits (Theorem 2), it follows that the width of the hidden state \(m\) must be \((N/p)\) (cf. Appendix C.1).

**Theorem 3**.: _Any recurrent model with a hidden state of width \(m\) using \(p\)-bits of precision that computes the Index Lookup task for all sequences of length \(N\) must have \(m N/p\)._

**Discussion.** The above results theoretically formalize intuitive differences between the way Transformers and recurrent models process sequences. Since Transformers have access to \(N\) input vectors during their computation, a small-sized attention block can attend over the desired input vector to make the correct prediction. On the other hand, any recurrent model--even with arbitrary positional embeddings--must store all the required information in its hidden state, which lower bounds the size of such models to compute the right output. These intuitions are made rigorous by showing (i) how soft-attention can do lookup using the almost orthogonal vectors, and (ii) small-width RNNs yield a short one-way communication protocol. These lower bounds also apply to causal forms of linear attention architectures where softmax is removed and attention weights become dot products .

At first glance, it might seem unfair to compare Transformers and RNNs with the same number of parameters: Transformers have access to \(N\) input vectors, whereas RNNs have a fixed-size hidden state. But note that, in practice, empirical research on language models typically compares models of the same size e.g., a 7B Transformer vs a 7B state-space model. Hence, it is natural to ask if Transformers of a particular size can express something that recurrent models cannot.

## 4 Lower Bounds for RNNs and 1-layer Transformers

Whereas Section 3 established a case where one-layer Transformers can be more powerful than RNNs, we next exhibit an example of the opposite phenomenon. Here, the key tool will again be a communication complexity argument, but this time it applies to one-layer Transformers: We establish a communication protocol by which Alice and Bob can compute the output of a one-layer Transformer by exchanging a number of bits that is bounded by the representation size of the Transformer and an overhead that is logarithmic in the input length. The key property here is that this protocol works not just when Alice and Bob have access to a prefix and suffix of a string, but instead works for an _arbitrary_ partitioning of the input string (proof is in Appendix D):

**Theorem 4**.: _Consider a one-layer Transformer \(f^{1}_{m,p,H}\) operating over inputs of length \(N\). Consider any disjoint subsets \(S_{A} S_{B}=\{1,,N\}\), \(S_{A} S_{B}=\). Assume Alice has access to \(s_{i}\) for \(i S_{A}\), and Bob has access to \(s_{i}\) for \(i S_{B}\). Then Alice and Bob can communicate \(3m(p+ N)H\) bits to compute the output \(f(s_{1} s_{N})\)._

The proof idea is that Alice and Bob first compute their parts of the numerator and denominator of the softmax and exchange these to compute the overall attention output. A naive implementation of this idea runs into the issue that the exponentiation of logits may exceed the bounds of \(p\)-bit precision; we circumvent this by first communicating the maximum logit and subtracting it from each logit, keeping the exponentials bounded without altering the resulting attention weights. Theorem 4 is a slightly more general and formal version of a result in Sanford et al. [51, Theorem. 7].

### Separation on Bounded Hierarchical Languages

We now use the communication protocol for one-layer Transformers to establish a separation between these and RNNs on bounded Dyck languages. Dyck languages are of central importance in formal language theory, as any context-free language can be expressed in terms of Dyck languages . Due to the boundedness of human memory , natural language tends to have more bounded levels of embedding . This has motivated the study of bounded-depth Dyck languages as plausible simple models of the hierarchical structure underlying language .

**Task.** Formally, Dyck-(n, k) (cf. Appendix E) is the language of well-matched strings over \(n\) types of parenthesis pairs \((_{1},_{1}),(_{2},,(_{n},)_{n}\), where any prefix has at most \(k\) opening parentheses not yet closed. For instance, the string '([ ] ) ()' has a maximum depth 2 corresponding to the prefix '([ '. Dyck-(n, k) can be recognized with access to a bounded stack that never holds more than \(k\) elements. In fact, each Dyck-(n, k) is a regular language and is accepted by a finite automaton.

We show that there is a linear communication complexity lower bound for Dyck-(n, k), already at \(n=k=2\). However, unlike the communication bound we used in Theorem 3, Alice and Bob now have access not to two halves of the input; rather, Alice and Bob have access to the even and odd positions in the string, respectively. Intuitively, in such a situation, Alice needs to know almost all of the bits available to Bob in order to decide whether a given string is well-bracketed-and vice versa. More formally, they need to exchange at least \(N-1\) bits to decide membership in Dyck-(2, 2):

**Lemma 1**.: _Suppose Alice and Bob have the symbols in the odd and even indices of a string \(s^{N}\) respectively. To each compute whether \(s\) Dyck-(2, 2), they must exchange at least \(N-1\) bits._

The proof of Lemma 1 is in Appendix E and is based on fooling sets which is a standard technique to prove lower bounds on communication complexity. Combining Lemmas 4 and 1 entails a lower bound on the width of a Transformer for computing Dyck-(2, 2):

**Theorem 5**.: _Consider a one-layer Transformer \(f^{1}_{m,p,H}\) deciding membership in Dyck-(2, 2). Then \(mH\)._

This result establishes a second separation between one-layer Transformers and RNNs, but now in the other direction: Bounded-depth Dyck languages are regular, and previous works have shown that constant width RNNs can recognize them with standard activation functions such as Sigmoid  and ReLU . We further note that two-layer Transformers of sublinear size can model bounded-depth Dyck languages .

### Lower Bounds on Boolean Functions

There are some notable differences between the types of communication complexity lower bounds for one-layer Transformers (Theorem 4) and for RNNs (Theorem 2). RNNs computing \(f\) yield a one-way protocol for contiguous partitions of the input; thus showing a _one way_ communication lower bound for such partitions is sufficient to obtain lower bounds on the size of RNNs. Transformers computing \(f\) yield a multi-way protocol that work for _arbitrary partitions_ of the input; thus showing a lower bound for _any_ partition is sufficient to establish lower bounds on the size of the Transformer. For Dyck-(2, 2), contiguous partitions are not a hard case, and in fact, communicating \( 2\) open brackets from the first half is sufficient. This is why the lower bound of Lemma 1 does not apply to RNNs.

**Lower Bounds.** Despite the differences discussed above, there are several Boolean functions, for which we can establish that when widths are bounded, neither one-layer Transformers nor RNNscan compute them. The Equality function \(:\{0,1\}^{N}\{0,1\}\) is a Boolean function defined as \(()=[(_{1},,_{N/2})=( _{N/2+1},,_{N})]\). A related problem is _Disjointness_: given two vectors \(,\{0,1\}^{N/2}\), the function \((,)=_{i}_{i}=[^{T}>0]\). Both the functions Equality and Disjointness are known to have communication complexity \((N)\) (see Appendix B.1) and Theorems 4 and 2 imply that both one-layer Transformers and RNNs must have width \((N)\) to represent them. In the next section, we show that these lower bounds do not apply to two-layer Transformers. Additionally, it is worth noting that the functions \(\) and Disj can also be expressed in the form of \(2\)-CNFs with \(O(N)\) terms. Hence, a more general consequence of the limitations of RNNs (and one-layer Transformers) is that with \(o(N)\), they cannot compute certain functions in the class of uniform \(^{0}\).3 It is interesting since the class of uniform \(^{0}\) is considered one of the simplest classes of Boolean circuits and even the expressive power of hard-attention Transformers has been shown to be within this class . In Appendix F.1, we provide an alternate proof of the lower bound for RNNs computing Equality based on their relation to DFAs.

## 5 Representational Capabilities of 2-layer Transformers

In Section 4, we showed that single-layer Transformers and recurrent models must have size _linear_ in the input length to express natural Boolean functions such as \(\). In this section, we show that two-layer transformers overcome these limitations by efficiently expressing such Boolean functions and more general forms of associative recall tasks, such as simulating the nearest neighbor algorithm.

### Representing Boolean Functions

We start by showing that two-layer Transformers of poly-logarithmic size can express the Equality function (proof is in Appendix F.2). The input domain need not necessarily be the Boolean vectors \(\{0,1\}^{N}\); rather, the construction works for sequences over any finite alphabet \(\).

**Theorem 6**.: _For any \(N\), there exists a 2-layer Transformer \(f^{2}_{m,p,2}\) where width \(m=O( N)\) and precision \(p=O( N)\) such that \(f()=()\) for all \(\{0,1\}^{N}\)._

The construction is based on tools developed in Section 3. The broad idea is as follows. In the first layer, at each position \(i>N/2\), an attention head attends to position \(i-N/2\) and copies the input \(x_{i-N/2}\). A feedforward network then checks whether the retrieved value is equal to \(x_{i}\). The second layer simply uses uniform attention over the outputs of the previous layer to check if there is a mismatch at any position. Importantly, we show that the above strategy can be implemented with a representation size \(O(( N)^{3})\).

Generalizing this result, we find that two-layer Transformers with logarithmic width can express a more general class of Boolean functions: thresholds of \(k\)-sparse features, a class including functions such as Equality and Disjointness. Since such functions cannot be expressed by one-layer Transformers and recurrent models with width \(o(N)\), these results imply a _separation_ on Equality and Disjointness: these functions can be expressed by small-sized two-layer Transformers whereas one-layer Transformers and recurrent models must grow linearly with input length to represent them.

### Implementing the Nearest Neighbors Algorithm

The goal of the nearest neighbor task (NstNb) is to analyze whether a sequence modeling architecture can implement the well-known nearest neighbor algorithm to make predictions. Our description follows closely to the one used by Bhattacharya et al.  for their experiments.

**Nearest Neighbors.** In the NstNb task, a model is provided with a sequence of vectors and labels \((_{1},y_{1},,_{k-1},y_{k-1},_{k})\) where \(N/2<k N\), the input _unit_ vectors \(_{i}^{d}\) and labels \(y_{i}\{0,1\}\). For each \(_{k}\) where \(k>N/2\), the output is the label corresponding to the nearest neighbor in \((_{1},,_{k-1})\), that is, if \(j=*{arg\,max}_{i[k-1]}_{k}^{}_{i}\) or \(j=*{arg\,min}_{i[k-1]}\|_{k}-_{i}\|_{2}\), then the output for \(_{k}\) is the label \(y_{j}\). Since we are working with unit vectors, maximizing the inner product is equivalent to minimizing the \(_{2}\) distance. If the second half of the sequence \(_{+1},,_{N}\) is a permutation of the first half \(_{1},,_{}\) then the task reduces to the Multi-Query Associative Recall (Mqar) task  (cf. Appendix G).

**Assumptions.** We will make two assumptions about the problem. The first assumption is that all input vectors are of unit norm, i.e., \( x_{2}=1\) and the second is the existence of a margin between the dot product with the nearest neighbor and the dot product with other input vectors, i.e. there exists \( N^{-c}\) for some universal constant \(c\), such that for any \(N/2<k N\), if \(j^{*}=_{i[k-1]}_{k}^{}_{i}\), then \(_{k}^{}_{j^{*}}_{k}^{}_{i}+\) for any \(i j^{*}\).

The following is one of our main results which states that two-layer Transformers of logarithmic size can implement the nearest neighbor algorithm in their forward pass and as a corollary can also perform associative recall tasks like Mqar (Proofs in Appendix G.2).

**Theorem 7**.: _For any \(N\), there exists a 2-layer Transformer \(f_{}^{2}_{m,p,2}\) with width \(m=O( N)\) and precision \(p=O( N)\) such that \(f_{}\) computes the nearest-neighbor task all sequences of length at most \(N\) satisfying the assumptions above._

The broad idea of the construction is to identify the nearest neighbor input \(_{j^{*}}\) and retrieve the position of the corresponding label \(y_{j^{*}}\) in the first layer. The second layer then uses this positional information to retrieve the desired label. There are a few challenges to implementing this strategy which we address in our construction. First, note that for input vectors \(_{1},,_{k}\), naively using them with dot-product attention will result in the query input \(_{k}\) having maximum dot product and hence maximum attention weight over itself. Second, the dot product with some label vectors \(y_{i}\)s could be higher than the dot product with the nearest neighbor \(_{j^{*}}\). Third, the positional information must be retrieved using soft-attention in a way that it can be used in the next layer to obtain the desired label. Our intuitive, though somewhat involved, construction deals with these issues to ensure that a two-layer Transformer with \(O(( N)^{3})\) total size implements the nearest neighbor algorithm.

**Theorem 8**.: _Any recurrent model with a hidden state of width \(m\) with \(p\)-bits of precision that can perform the nearest neighbor task for all inputs of length \(N\) must have \(m N/2p\)._

The lower bound for recurrent models follows via a reduction from the Disjointness problem.

**Discussion.** Prior works [8; 2] have empirically demonstrated that Transformer-based LLMs can exhibit mechanisms such as nearest neighbors and Mqar. Further, on synthetic setups, they have observed that recurrent models struggle to perform these tasks compared to Transformers. Our results take a step towards understanding the differences in the performance between the two architectures.

## 6 Empirical Analysis

While we focus on the differences in the representational capabilities of Transformers and recurrent models, it is natural to examine if differences of a similar nature arise in their empirical performance. One thing to keep in mind is that positive results regarding expressiveness presented in earlier sections do not imply that models can _learn_ such tasks. With regard to negative results, they do imply that when the sequence length is much larger than the size of the hidden state or width of the model, then the model will be incapable of representing the task and consequently fail to learn the task. However, even for one-layer recurrent models with hidden states of size \(128\) with 64 bits of precision, our lower bound applies at lengths over 8k.

In this section, we investigate the performance of Transformers and recurrent models on tasks such as Index Lookup and recognizing bounded Dyck languages on sequences of small lengths (\(<1000\)). Our experiments are designed to answer the following questions: (1) Are one-layer Transformers better than larger recurrent models on the Index Lookup task? (2) Are recurrent models and two-layer Transformers better than one-layer Transformers at recognizing the Dyck-(2, 2) language? Importantly, as our results concern the scaling of the model size with the input length, we are specifically interested in the behavior of different models across input lengths.

We also explore the performance of models on string equality in Appendix H.2. Tasks like NstNb and Mqar have already been analyzed empirically in prior works [8; 2] so we do not include them.

**Setup and Training details.** We train the models with cross-entropy loss using the Adam optimizer . The models are trained for up to 250k steps where at each step we sample a fresh batch of 64 training examples - resulting in \( 16\) million examples over 250k steps. The models are evaluated on 5000 examples for each task. For each model, we tune the various hyperparameters, notably across learning rates \(\) {1e-2, 5e-3,..., 1e-6} to find the best-performing model. The details of the data generation method, hyperparameters, and implementation details can be found in Appendix H.

**Index Lookup Task.** We compare the performance of one-layer Transformers with five different recurrent models - LSTMs , state space models such as DSS  and Mamba , linear Transformers , and its variant RetNet . We explore the performance across various lengths \(\{20,50,100,200,400\}\). We evaluate relatively small-sized Transformers with widths \(64\) against recurrent models with up to \(6\) layers and widths or hidden state size of \(256\). The size of the alphabet in the experiments is \(||=64\). Figure 2 (left) depicts the performance of all models across various lengths and Figure 2 (middle) depicts the validation curves during training on examples of length 200. As depicted by the figures, while one-layer Transformers with width \(64\) achieve near-perfect accuracy within a few thousand steps, the performance of relatively larger recurrent or state-space models degrades on lengths over \(100\) and they fail to learn even with \(10\) training iterations. We explore the influence of width on the performance of Mamba in Appendix H.1.

**Bounded Dycks.** For Dyck-2 with depth at most 2, our separation results apply to one-layer Transformers and nonlinear recurrent models such as LSTMs but not to linear RNNs such as state-space models and linear Transformers. Hence, we are primarily interested in the difference in performance between one-layer Transformers and LSTMs. In our experiments, we compare the performance of one-layer Transformers with relatively smaller recurrent models such as LSTMs and two-layer Transformers. We also include Mamba for reference. We consider LSTMs and Mamba with hidden state sizes of 64 and similarly, two-layer Transformers with width 64. We evaluate a one-layer Transformer with a width of 256 across lengths \(\{20,,400\}\) most of which are smaller than the width of the model. We observe that one-layer Transformers achieve near-perfect accuracy up to lengths \(100\) but struggle on higher lengths. In contrast, small-sized recurrent models as well as two-layer Transformers can achieve near-perfect accuracy for lengths up to \(400\). Figure 2 (right) depicts the validation curve of the models on examples of length 400.

## 7 Discussion and Final Remarks

Based on prior theoretical results, it is known that, while recurrent models can express any regular language , Transformers with logarithmic precision can only express languages in the class of uniform constant depth threshold circuits (TC\({}^{}\)) . These results indicate that--under standard conjectures--Transformers are unable to represent certain state-tracking tasks that recurrent models can represent. With such results, it might appear that Transformers are less expressive than recurrent models-potentially at odds with the persistent practical success of Transformer-based LLMs. Our findings, however, show that when the model size is constrained relative to the sequence length, a variety of tasks relevant to practice can be represented by small-sized Transformers but not by recurrent models. Our results suggest that the attention mechanism does lead to expressiveness that cannot be replicated by recurrent architectures even with arbitrary transition functions.

**Limitations.** A general limitation of this line of work is that positive expressivity results do not imply that the problems under consideration are learnable. Additionally, while lower bounds for an architecture imply difficulty in learning, when using double precision these results only apply to very long sequences in practice. Our results (and probably techniques) do not imply any limitations on two-layer Transformers; this is left as an open question. We note that communication complexity-based techniques akin to Theorem 4 cannot exist for two-layer Transformers (cf. Appendix F.4). Hence, we believe that other tools will be needed to prove lower bounds for two-layer Transformers.

Figure 2: Performance of models on the Index Lookup and bounded Dyck task. Labels such as TF-(1, 64) denote Transformers with 1 layer and 64 widths. See Section 6 for more details.