# Theoretical Analysis of Weak-to-Strong Generalization

Hunter Lang

MIT CSAIL

&David Sontag

MIT CSAIL

&Aravindan Vijayaraghavan

Northwestern University

###### Abstract

Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call _pseudolabel correction_ and _coverage expansion_, respectively. We give a new bound based on _expansion_ properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.

## 1 Introduction

Weakly-supervised learning allows practitioners to train models with possibly-incorrect, easy-to-obtain _pseudol_s instead of accurate and expensive ground-truth labels. For example, suppose the goal is to classify documents based on whether they have positive or negative sentiment. Instead of employing humans to label examples \(_{i}\) with positive/negative sentiment labels \(y_{i}\), weak supervision enables models to learn from simple rules, such as: if 'incredible' \(_{i}\), sentiment = positive. Called _programmatic weak supervision_, a wealth of literature has shown how to aggregate rules, often called "labeling functions", into individual pseudolabels that can be used to train a model [e.g., 51, 49, 63, 21]. In typical pipelines, this consists of fine-tuning a pre-trained neural network . This method has met with a huge amount of empirical success in natural language processing  computer vision , and verticals such as healthcare .

Another emerging trend in weak supervision is to use the zero-shot or few-shot outputs of a large language model (LLM) as pseudolabels for training another language model--the student model often outperforms its noisy "teacher", and this technique even works when the teacher is a less powerful model than the student, distinguishing it from classical knowledge distillation . Good data selection can be critical, and several approaches carefully choose a "confident" subset of the pseudolabels [e.g. 35, 18, 39, 66]. The increasing prevalence of LLM use in crowdwork  highlights the importance of better understanding this learning method.

In both cases, the pseudolabels \(\) are given by the _pseudolabeler_ or _teacher model_, which is some function of the input example \(\). The pseudolabeler may make errors (\(() y()\)), and it may not _cover_ every point--there are some points where the teacher abstains from providing a weak label. In the examples above, these are points not covered by the rules and points where the teacher LLM is not confident, respectively. _A priori_, it seems that a powerful enough classifier should exactly fit the pseudolabeler on the covered data and have trivial performance on the uncovered data. However, this is not what happens in practice--the empirical success of weak supervision is due to two surprising, related phenomena that together comprise _weak-to-strong generalization:_ (a) _Pseudolabel correction:_The performance of the model exceeds the performance of the pseudolabels used to train it; and (b) _Coverage expansion:_ The model performs well even on the portion of example space \(\) that is not covered by pseudolabels. These empirical outcomes are key to the success of weak supervision.

Surprisingly, the existing theoretical literature on programmatic weak supervision does not address either phenomenon--the majority of weak supervision theory literature either focuses on how to adeptly combine the outputs of multiple "weak rules" into a single pseudolabel \(_{i}\), , or treats learning from weak supervision as learning from noisy labels [e.g. 46, 59, 15], but as we discuss in Section 2, this framing does not capture the setting we consider, where the pseudolabels used to train one model are the outputs of another model.

In this work, we give a theoretical analysis of weakly-supervised learning that provably accounts for the effects of pseudolabel correction and coverage expansion. Our results use a natural _expansion_ condition on the population data distribution. Informally, expansion implies that "bad" points (points with incorrect pseudolabels, or points with no pseudolabel at all) have many "good" neighbors (points with correct pseudolabels). If the learned student model is relatively robust on the neighborhoods of interest, then making a mistake on a bad point means making many mistakes on good points as well. This allows us to prove a relationship between the student model's error on the weak labels (the training objective) and the student model's error on the true labels (the desired objective). Our assumptions and bounds in Section 4 formalize this intuition. Section 5 details a procedure for checking our expansion conditions from finite data, and in Section 6 and Appendix E, we give empirical evidence that these conditions hold on real data.

To the best of our knowledge, our results provide the first error bounds for programmatic weak supervision with realistic assumptions. We show that our bounds generalize and connect several existing results from the co-training, self-training, and distribution shift literature [e.g., 6, 3, 12] and adapt them to the weak supervision setting. For example, we show in Appendix C.1 that Theorem 4.2 generalizes the co-training results of Blum and Mitchell . We discuss these generalizations in detail in Sections 2 and 4 and Appendix C. Our result in Section 5 is the first among these works to prove that the expansion assumptions can, in principle, be checked using finite data. Unlike most prior work in this space, our experiments in Section 6 attempt to check whether the expansion assumptions hold in practice. While our experiments are limited in scope, our attempt to systematically check expansion in a practical scenario is a major departure from previous work.

Finally, prior work with expansion assumptions similar to ours (in the co-training , self-training , and distribution shift  literature) requires that the classifiers are either perfectly robust  or adversarially robust  for their bounds to apply. Empirical results suggest that adversarial training has fairly limited value for improving coverage expansion and pseudolabel correction , and that these two effects still occur for student models that are not adversarially trained . To close this gap, we make a connection to the literature on _robust expansion_ and prove error bounds for student models that are merely "robust on average." Unlike prior work, these bounds allow for the presence of adversarial examples for every input point.

## 2 Related Work

Ratner et al. , Fu et al.  focus on how to combine the outputs of multiple "weak rules" into a single pseudolabel \(()\) for each covered example, a problem with a long history in the crowdsourcing literature [e.g. 16, 30, 29]. However, empirical results indicate that this is not the important aspect of weak supervision: most methods for combining weak rules fail to significantly outperform majority vote once the final classifier is trained . Works in this literature that _do_ provide error bounds for the student (e.g., Fu et al. , Ratner et al. ) either fail to capture weak-to-strong generalization effects, as shown in Section 3, or make difficult-to-justify assumptions--for example, Ratner et al.  assumes that \(y()\) and \(\) are conditionally independent given \(()\). If this were true, there would be no gain from training a classifier, since \(()\) already captures all the information that \(\) contains about \(y()\). Work that treats learning from weak supervision as a noisy label learning problem [e.g. 45, 46, 59, 15] does not capture the types of weak supervision we consider. When the supervision comes from weaker model (be it rule-based or a weaker LLM), there is no exogenous noise process that corrupts the training labels. There are simply some points that deterministically get the wrong labels and some points with no label. This rules out common noise models like class-conditional noise  and Tsybakov noise , and is arguably not appropriate to model as instance-dependent noise , since for each \(\) the noise is deterministically 0 or 1.

Burns et al.  conducted a large empirical study showing widespread weak-to-strong generalization effects when training a strong language model on the generations of a weaker model. Our results are a step toward a theoretical understanding of these effects. Several works have used expansion to give provable guarantees in other settings where models are learning from each other. Balcan et al.  use expansion to analyze co-trained  classifiers. Our expansion assumption is similar to their "left-right" expansion, but we generalize beyond the multi-view setup of co-training and account for error propagation. Wei et al.  give provable guarantees for self-training under expansion assumptions similar to ours. We provide a different pseudolabel correction bound, tighter guarantees for coverage expansion under weaker assumptions, and generalize both results to classifiers that are not adversarially robust. Cai et al.  use expansion to prove general guarantees for pseudolabel correction, semi-supervised learning, and unsupervised domain adaptation, but their results require the student to be very adversarially robust. Compared to all these expansion works, we also outline a rigorous theoretical framework for checking expansion on finite data and provide more empirical evidence for our assumptions. We discuss more related work in Appendix A.

## 3 Setup and Shortcomings of Existing Bounds

**Notation.**\(\) refers to a random variable with distribution \(\) and italicized letters \(\) refer to realizations of \(\). We will assume for ease of exposition that the input space \(\) is a discrete1 (but possibly very large) set, such as all vectors in \(^{d}\) up to a fixed numerical precision. For \(A\) we use \(= A\). We assume there is a ground-truth function of interest, \(y:=\{1,,k\}\), and a _pseudolabeler_\(:\{\}\), which assigns to each point \(\) either a label in \(\) or the special "abstention" symbol \(\). The function \(\) can also be thought of as the _teacher model_, but we are primarily concerned with instances where the teacher is much less capable than the "student" it will be used to train.

Define \(S=\{|()\}\) to be the _covered_ subset of \(\), i.e., the subset of \(\) that has a pseudolabel, and let \(T=\{|()=\}= S\) be the uncovered set. This notation serves to emphasize that training occurs on a (pseudolabeled) _source_ subset \(S\), and then evaluation occurs on the union of \(S\) and the (uncovered) _target_\(T\). Let \(\{_{i}\}\) be a partition of \(\) such that within each \(_{i}\), the ground-truth label is constant. For example, we could set \(_{i}=\{|y()=i\}\) to be the set of points with ground-truth label \(i\). We will use this definition of \(_{i}\) for convenience, but all our results hold for more general partitions. Each of \(S\) and \(T\) can further be partitioned as \(S_{i}=S_{i}\), \(T_{i}=T_{i}\). Finally, each \(S_{i}\) can be further partitioned into the correctly-pseudolabeled examples \(S_{i}^{good}=\{ S_{i}|()=y()\}\) and the incorrectly-pseudolabeled examples \(S_{i}^{bad}=S_{i} S_{i}^{good}\). Let \(_{i}:=(S_{i}^{bad}|S_{i})\) be the error rate of \(\) on \(S_{i}\). We assume \(0<_{i}<\) for all \(i\).

**Problem Setup.** For two classifiers \(f,g:\) and a set \(U\), we use \((f,g|U)\) to represent \((f() g()| U)\), their probability of disagreement conditioned on \(\) falling in the set \(U\). Here the probability is over \(\); this will often be omitted for notational convenience. We will be particularly interested in classifiers obtained by minimizing the error on the non-abstaining weak labels over the strong model hypothesis class \(\), i.e., (approximate) solutions to \(_{f}(f,|S)\). The ultimate goal is to obtain upper bounds on the error \((f,y|)\) for such classifiers. That is, we want to upper bound the error of a classifier \(f\) on the _true labels_ over the _entire_ input space \(\).

There are two key challenges. First, the classifier is trained using \(\), not \(y\), and \(\) may have arbitrary errors that are not captured by any well-studied noise model such as class-conditional or Tsybakov noise --we've assumed \(\) and the true labels \(y\) are both deterministic functions of the input, so there is no exogenous noise process that corrupts the training labels.

Second, we care about the performance of \(f\) on the entire space \(\), but we only train on the _covered_ samples from \(S\). Again, since \(\) is an arbitrary deterministic function, our samples from \(S\) are not distributed according to \(\), and \(S\) and \(T\) have no overlap, ruling out approaches like importance-weighting. The following example elaborates on the issues at play and illustrates the shortcomings of existing bounds in the weak supervision literature.

### Shortcomings of Existing Bounds: Illustrative Example

A special case of weak-to-strong generalization is training a strong pretrained model on the outputs of very coarse rules. Following the example from Section 1, suppose our goal is to obtain a sentiment classifier, so \(\) is the space of text documents, \(=\{-1,1\}\) and \(\) is given by the following rules: if 'incredible' \(\), \(()=+1\). If 'horrible' \(\), \(()=-1\). Otherwise, \(()=\).

Assume for simplicity that "incredible" and "horrible" never co-occur, so \(\) is well-defined. This example shows that the student model hypothesis class \(\) and the training procedure both play a vital role in weak-to-strong generalization. Suppose \(\) is the class of bag-of-words classifiers, and we obtain a student \(f\) by minimizing \((f,|S)\). Without modifications to the training procedure (such as L2 regularization), \(f\) may place a large positive weight on "incredible", a large negative weight on "horrible", and zero weight on all other tokens. This model has zero error on the weak labels, so it exactly minimizes the training objective. It reproduces the pseudolabels on the covered set and has trivial performance on the uncovered set, so there is no weak-to-strong generalization. On the other hand, if we were to instead train a linear probe on top of a SentenceBERT  representation, we would obtain a model that improves over \(\) on \(S\) (the _covered_ set of documents containing either "horrible" or "incredible") and has reasonable performance on \(T\) (the uncovered set). Section 6 contains precise results for this example, but the critical (seemingly obvious) aspect is that the student representation and the training details matter for achieving weak-to-strong generalization.

The following proposition (proven in Appendix B.3) illustrates how existing error bounds in the programmatic weak supervision literature, which do not account for training details and the student hypothesis class, are unable to capture pseudolabel correction and coverage expansion.

**Proposition 3.1**.: _Suppose the label marginals for the above example satisfy \((=y)=\) for \(y\{-1,1\}\), and assume that the weak label error rates \(_{-1}=_{1}=\), and that the weak labels cover each class equally often: \((=|=y)=(=)\). Let \(=_{f}(f,|S)\) be the classifier minimizing the weak label error on the covered set. Then the bound from Fu et al. [21, Theorem 3] simplifies (in our notation) to: \((,y)(S) 4(1-)+(T)\)._

The first term accounts for the error of \(\) on the covered set \(S\). The weak labels themselves have error \(\) on \(S\), but the bound for \(\) is \(4(1-)>\) whenever \(<\), so Fu et al. 's bound does not allow for pseudolabel correction in this example. The second term accounts for the error of \(\) on the uncovered set \(T\). A random guess achieves error \(\) on \(T\), but the bound charges every point in \(T\) as an error, so it also does not account for coverage expansion or even the performance of random guessing on \(T\).

Expansion assumptions similar to ours have also been studied in the context of self-training  and domain adaptation . The following proposition shows that while the results of Wei et al.  can be adapted to weakly-supervised learning, our bounds capture the full weak supervision setup better, since Wei et al. [69, Theorem 4.3] was not designed to deal with partial coverage \(( S)<1\), so applying it to weakly-supervised learning still requires fairly large coverage.

**Proposition 3.2** (informal).: _Suppose the coverage \(( S)\) in the example above is less than \(2/3\). Then the bound from Wei et al. [69, Theorem 4.3] does not apply since directly adapting it to the weak supervision setting requires \(( S) 2/3\)._

Empirically, coverage expansion and pseudolabel correction can both occur in the low-coverage regime . Finally, as mentioned in Section 1, Wei et al.  assumes the classifier is adversarially robust. In contrast, we provide bounds that directly account for coverage expansion, place no restrictions on the amount of weak label coverage, and allow for the presence of many adversarial examples. As the example in this section suggests is necessary, the model hypothesis class and the training details (in particular, the _robustness_ of the model) play a central role in our bounds. The following definitions attempt to capture these properties.

### Definitions

**Definition 1** (Neighborhood).: _Let \(\) be a neighborhood function that maps each point \(\) to a set of points \(()\) that we call the neighborhood of \(\). We will assume that \(\) satisfies \((^{})^{}()\), i.e., that the neighborhoods are symmetric. We can extend \(\) to a function of sets as \((A)=_{ A}()\). Examples to keep in mind are \(()=\{^{}:||()-(^{ })|| r\}\) for some representation \(:^{d}\), or, in the case of text inputs \(\), the set of fluent paraphrases of \(\). However, our results work with any definition of \(\)._

**Definition 2** (\(\)-robust).: _For an arbitrary classifier \(f\) and point \(\), define \(r(f,)=(f(^{}) f()|^{} ())\) as the probability \(f\) gives different labels to \(\) and a random neighbor \(^{}\) of \(\).__A classifier \(f:\) is said to be \(\)-robust at a point \(\) if \(r(f,)\). Define \(R_{}(f)=\{:r(f,)\}\) as the set of \(\)-robust points for \(f\)._

If \(=0\), \(f\) is \(\)-robust at \(\) if and only if \(f\) is adversarially robust over \(()\), so this definition generalizes adversarial robustness. By Markov's inequality, any classifier \(f\) with:

\[_{},}^{} |()}[f() f(}^{})] \]

is \(\)-robust on a set of probability at least \(1-/\) (see Lemma B.1). The requirement (1) is significantly more natural than adversarial robustness: a classifier satisfies (1) whenever it gives most points the same labels as most of their neighbors. We refer to (1) as "average-case robustness".

**Definition 3** (Expansion).: _Fix sets \(A,B\). We say the distribution \(_{}}\) satisfies \((c,q)\)-expansion on \((A,B)\) if for all sets \(U B\) with \((U|B)>q\), \(((U)|A)>c(U|B)\)._

Figure 1 shows examples of Definition 3 graphically. Intuitively, a pair of sets \(A\) and \(B\) satisfy this definition if large subsets of \(B\) correspond/_expand_ (via the neighborhood \(\)) to large subsets of \(A\). Definition 3 requires _all sets_\(U\) with large enough probability in \(B\) to expand to \(A\). However, this is unnecessarily strong. Our theorems will only need certain _structured_ sets, corresponding to elements of the student hypothesis class, to expand. This is captured by Definition 4 and is the key to our results in Section 5 on checking the expansion property.

**Definition 4** (Expansion of a set collection).: _Fix sets \(A,B\) and suppose \(\) is a collection of subsets of \(B\). Then we say \(\) satisfies \((c,q)\)-expansion on \((A,B)\) if all sets \(U\) with \((U|B)>q\) satisfy \(((U)|A)>c(U|B)\)._

## 4 Error Bounds for Weakly-Supervised Classifiers

In this section, we upper bound the _gold_ error of \(f\) on the covered and uncovered sets--\((f,y|S)\) and \((f,y|T)\)--with expressions involving the _weak_ error \((f,|S)\) of \(f\) on the covered set, expansion parameters, and robustness parameters. These bounds give a theoretical justification for why empirical risk minimization using the weak labels leads to weak-to-strong generalization. We condition on subsets \(S_{i} S\) (for pseudolabel correction) or pairs of subsets \(S_{i} S\), \(T_{i} T\) (for coverage expansion). This subset-wise conditioning allows for different parts of the distribution to expand in different amounts, yielding tighter bounds, an approach also followed by Cai et al. .

### Adversarially Robust Models

We begin by stating our theorems for the case when the classifier of interest is \(\)-robust with \(=0\) on most points \(\). That is, we assume that for most points \(\), the classifier is adversarially robust over \(()\). This follows the assumptions in related work from other domains . We describe our results for the significantly more general average-case-robust classifiers in Section 4.2.

**Expanding Families.** We first define the families \(\) and \(^{}\) of sets that must expand according to Definition 4. Let \(\) be the hypothesis class of the strong model and for each \(f\), define \(R(f)=R_{0}(f)=\{:r(f,)=0\}\) to be the set of adversarially robust points for \(f\). For \(B\) and \(f\), define \(U(B,f)=\{ B:f() y()\}\) as the set of points in \(B\) where \(f\) makes a mistake on the true label \(y\). Now define \((B,)\) to be the class of _robust_ mistakes sets: \((B,)=\{U(B,f) R(f):f\}\). Similarly, define \(^{}(B,)=\{(B U(B,f)) R(f):f \}\) as the family of robust _non_-mistakes on \(B\).

Figure 1: Relative expansion (Definition 3) on the sets \((A,B)\). Expansion requires that certain subsets \(U B\) have neighborhoods \((U)\) such that \(((U)|A) c(U|B)\). These probabilities are represented graphically on the right-hand-side as the fractions \(|(U) A|/|A|\) and \(|U|/|B|\).

**Pseudolabel Correction.** Here we relate the gold error of the student model \(f\) on a covered subset, \((f,y|S_{i})\), to the weak error of \(f\) on that set, \((f,|S_{i})\). The goal is to allow for the _correction_ of some of the incorrect weak labels (\(S_{i}^{bad}\)): we want our bounds for \((f,y|S_{i})\) to be less than \(_{i}\), the error rate of the weak labels. Expansion between the points with correct pseudolabels, \(S_{i}^{good}\), and points with incorrect pseudolabels, \(S_{i}^{bad}\), implies that there are many bad points with good neighbors. If the classifier is suitably robust on the neighborhoods, pseudolabel correction can occur. The bound in this section makes this intuition quantitative.

**Theorem 4.1** (Pseudolabel correction).: _Suppose \(^{}(S_{i}^{good},)\) satisfies \((c,q)\)-expansion on the sets \((S_{i}^{bad},S_{i}^{good})\) for \(q<(1-2)\). Consider an arbitrary classifier \(f\) such that \((f()()\) or \(f\) not robust at \(|S_{i})\). Then the true error of \(f\) on \(S_{i}\) satisfies:_

\[(f,y|S_{i})}{1-2_{i}}( {R(f)}|S_{i})+(f,|S_{i})+_{i}(1-c ).\]

The expansion condition intuitively states that "good" sets (elements of \(M^{}(S_{i}^{good},)\)) must have suitably many neighbors with the wrong pseudolabel (elements of \(S_{i}^{bad}\)). The trivial error bound obtained via the triangle inequality is \((f,y|S_{i})(f,|S_{i})+_{i}\). The bound in Theorem 4.1 has almost the same form, but the multiplicative term on \(_{i}\) allows it to be much tighter than the trivial bound. Theorem 4.1 allows for pseudolabel correction because the right-hand-side can be much less than \(_{i}\) (the error of the weak teacher) when \(c\) is large and \((f,|S_{i})\) and \((|S_{i})\) are small. While Wei et al. [69, Theorem 4.3] also gives pseudolabel correction guarantees for adversarially robust classifiers, Theorem 4.1 is a different bound with several desirable properties that Wei et al. [69, Theorem 4.3] lacks--we compare the two in detail in Appendix C.2 and also show how to generalize Wei et al. 's results to average-case-robustness.

The following proposition shows that the expansion conditions in Theorem 4.1 rule out strong models that can exactly fit the weak model.

**Proposition 4.1**.: _Suppose there exists \(f\) such that for all \( S\), \(f()=()\). Then the \((c,q)\)-expansion conditions of Theorem 4.1 are not satisfied._

Proof.: The family of sets that must satisfy \((c,q)\)-expansion between \(S_{i}^{bad}\) and \(S_{i}^{good}\) is \(^{}=\{R(f)(S_{i}^{good}(f)):f \}\). Choose \(f\) such that \(f=\) when restricted to \(S\), since we assumed this is a valid choice. \(S_{i}^{good}\) is defined as the set where \(\) makes no mistakes (and the true label is \(i\)), so \(S_{i}^{good}(f)=S_{i}^{good}\), and thus \(R(f) S_{i}^{good}^{}\). Let \(N=\{ S_{i}^{good}:() S_{i}^{bad}\}\) be the subset of \(S_{i}^{good}\) with neighbors in \(S_{i}^{bad}\). Consider an arbitrary point \( N\), so there exists \(^{}() S_{i}^{bad}\). Since \(\) and \(^{}\) are both in \(S_{i}\), \(y()=y(^{})\). Then we have \(f()=()=y()=y(^{})(^{ })=f(^{})\), so \(f() f(^{})\). Thus \( R(f)\). This shows \(N\). Hence

\[R(f) S_{i}^{good} =R(f)((S_{i}^{good} N)(S_{i}^{good}))\] \[ R(f)((S_{i}^{good})(S_{i} ^{good}))\] \[=R(f)(S_{i}^{good}).\]

The latter set is made up entirely of points with no neighbors in \(S_{i}^{bad}\), so

\[((R(f) S_{i}^{good})|S_{i}^{bad})( (R(f)(S_{i}^{good}))|S_{i}^{bad})=0.\]

But for expansion to hold, we needed \(((R(f) S_{i}^{good})|S_{i}^{bad})>c(R(f)  S_{i}^{good}|S_{i}^{good})\). 

Proposition 4.1 shows the strong model hypothesis class enters our error bounds indirectly via the (data-dependent) expansion parameter. Using a richer class for the strong model may decrease the amount of expansion, since it may make it easier to exactly fit the weak labels. At the same time, the error of the strong model on the weak labels also appears as a term in the bounds (\((f,|S_{i})\)), and a richer class might decrease this term, so these two terms capture a tradeoff. This makes our boundsflexible enough to capture empirical results showing that whether a richer class or more restricted class works best for the strong model depends on the problem .

**Coverage Expansion.** In this section, we relate the error of \(f\) on an uncovered subset, \((f,y|T_{i})\), to the weak error of \(f\) on the corresponding covered subset, \((f,|S_{i})\). The goal is to give a nontrivial error bound on these points even though we see none of them during training. Expansion from \(T_{i}\) to \(S_{i}^{good}\) implies that subsets of \(T_{i}\) have enough correctly-pseudolabeled neighbors. If the student model is robust on \(\), this is already enough to prove an error bound for \(T_{i}\). However, we _also_ assume that subsets of \(T_{i}\) have enough _incorrectly_-pseudolabeled neighbors. Intuitively, this means that subsets of \(T_{i}\) have the "correct" number of neighbors in \(S_{i}^{good}\)_and_ the "correct" number of neighbors in \(S_{i}^{bad}\). This implies a regular structure in the \(S_{i}\)-\(T_{i}\) neighborhood connections that allows us to prove a much tighter error bound. Our empirical results suggest that this structure is present in real-world examples. We prove a weaker bound that only assumes expansion from \(T_{i}\) to \(S_{i}^{good}\) in Appendix B.

**Theorem 4.2** (Error bound for uncovered points).: _Suppose \((T_{i},)\) satisfies \((c,q)\)-expansion on \((S_{i}^{good},T_{i})\), and \(^{}(T_{i},)\) satisfies \((c,q)\)-expansion on \((S_{i}^{bad},T_{i})\). Consider an arbitrary classifier \(f\) that fits the weak labels well on \(S_{i}\) and is fairly robust on \(T_{i}\): \((f,|S_{i})+(|T_{i})<c(1-q-_ {i})\) Then the true error of \(f\) on \(T_{i}\) satisfies:_

\[(f,y|T_{i})(1+}{1-2_{i}}) (|T_{i})+(q,(f,|S_ {i})-c_{i}}{c(1-2_{i})}).\]

To qualify for the bound, \(f\) must fit the weak labels well on \(S_{i}\), so \((f,|S_{i})\) is small, and be adversarially robust at most points on \(T_{i}\), so \((|T_{i})\) is small. We show in Appendix C that the original co-training setup of Blum and Mitchell  satisfies the assumptions of Theorem 4.2 with \(c=1\), \(q=0\), and \((|T_{i})=0\). Theorem 4.2 exactly recovers the bounds of Blum and Mitchell , Lang et al.  in this case, so Theorem 4.2 is a direct generalization of Blum and Mitchell  but without the restrictive assumptions regarding multi-view data and conditional independence. In Appendix B, we prove a generalization of Theorem 4.2 that allows \(T_{i}\) to expand to \(S_{i}^{good}\) and \(S_{i}^{bad}\) at different rates (i.e., different expansion parameters).

It is not immediately clear from Theorem 4.2 how the _coverage_\((S)\)--the probability that the weak labeler does not abstain--affects the bounds. As with the role of strong model hypothesis class, this enters the picture implicitly through the parameter measuring expansion between \(S_{i}\) and \(T_{i}\). Informally, for a fixed neighborhood \(\), it may be easier to have expansion from \(T_{i}\) to \(S_{i}\) when \(S_{i}\) is larger, since for each uncovered point in \(T_{i}\), there are more possible covered neighbors in \(S_{i}\). On the other hand, increasing the coverage by including more points in \(S_{i}\) might also affect the weak label accuracy parameters \(_{i}\), which also appear in the bounds. In practice, there is not a consistent tradeoff between coverage and performance, as explored recently in, e.g., Lang et al. , so our bounds do not prescribe a functional dependence of the error on the amount of coverage and instead allow that dependence to enter through data-dependent parameters.

### Relaxing Robustness Requirements

The previous bounds in this section assumed the student is adversarially robust at most points. Here, we considerably generalize this requirement so our results apply to any classifier \(f\) that satisfies (1), i.e., any classifier that gives _most_ points the same label as _most_ of their neighbors. This requires several additional definitions and goes beyond the assumptions made in other work with expansion-based error bounds, which assume _adversarial_ robustness at most  or (effectively) all  points.

To allow for this relaxed assumption on the classifier, we assume a more robust version of expansion, aptly called _robust expansion_. To define robust expansion, we start by defining a graph over examples with edges induced by the neighborhood \(\) and weights given by the underlying probability measure. HaoChen et al.  study this graph in the context of contrastive pretraining.

**Definition 5** (Example graph).: _Let \(G=(,E)\) be a graph with one node for each element of \(\) (we assumed \(\) is a possibly very large, but finite, set), and connect two nodes \((,^{})\) if \((^{})\) or, equivalently, if \(^{}()\), with an edge weight of \(w(,^{}):=()(^{}) }[(^{})]\)._

Definition 3 (regular, non-robust expansion) is very sensitive to removal of a few edges from the example graph. A set \(U B\) may have \(((U)|A)\) large, but only because a small fraction of the edges (by probability mass) are connected to many \( A\) with \((|A)\) large. If we ignored these small-probability edges, the neighborhood would be much smaller. Figure 2 (appendix) shows an example. The _robust neighborhood_ tries to address this issue:

**Definition 6** (\(\)-robust neighborhood size).: _Let \(A,U\). The size of the \(\)-robust neighborhood of \(U\) in \(A\) is: \(P_{1-}(U,A):=_{V}\{(V|A):w(V,U)(1- )w((U),U)\}\)._

\(P_{1-}(U,A)\) is the probability of the "smallest" subset of \(A\) that still captures at least a \(1-\) fraction of the edge weight incident on \(U\). When \(=0\), we have \(P_{1}(U,A)=((U)|A)\), so this recovers the size of the non-robust neighborhood. In the pathological example described above, we would have \(((U)|A)\) large, but \(P_{1-}(U,A)\) small for some \(>0\). We are now ready to give a "robustified" definition of expansion, which is identical to Definition 4 except that it requires the _robust_ neighborhood, rather than the regular neighborhood, to be large.

**Definition 7** (Robust expansion).: _Fix sets \(A,B\) and suppose \(\) is a collection of subsets of \(B\). \(\) satisfies \((c,q,)\)-robust expansion on \((A,B)\) if for all \(U\) with \((U|B)>q\), \(P_{1-}(U,A)>c(U|B)\). This exactly recovers Definition 3 when \(=0\)._

The following (informal) theorem shows that Theorems 4.1 and 4.2 hold for average-case-robust classifiers when we replace expansion with robust expansion and \(R(f)\) with \(R_{}(f)\). We state and prove formal versions of Theorems 4.1 and 4.2 for average-case-robust classifiers in Appendix B.

**Theorem 4.3** (Informal).: _Theorems 4.1 and 4.2 hold exactly with \((c,q,)\)-expansion instead of \((c,q)\)-expansion and \(R_{}(f)\) instead of \(R(f)\)._

By Markov's inequality, for any \(>0\) a classifier \(f\) with \(_{|A,^{}| ()}[f() f(^{})]\) has \(((f)}|A)\). Theorem 4.3 shows that by assuming the data distribution follows a slightly more "regular" structure (robust expansion), we can give guarantees for average-case-robust classifiers. This generalization is important since it matches with empirical results: adversarial training and adversarial robustness are _not_ required for weak-to-strong generalization to occur [73; 38; 10], and most empirical work on weak supervision does not include adversarial training in the pipeline .

## 5 Checking Expansion

We now outline a statistical theory for checking the expansion properties of the population distribution from finite data. This is possible because, as described in Section 4, our results do not actually require _all sets_ to expand--rather, they only require expansion for a class of sets that is generated by the student hypothesis class. This means we can check expansion on a finite dataset and control the generalization of our estimate using the complexity of the hypothesis class. The purpose of checking expansion is not (currently) algorithmic--the goal of the procedures described in this section is to give empirical evidence that our assumptions hold in the real world and that our bounds correlate with actual occurrences of pseudolabel correction and coverage expansion. Exactly checking the expansion of _all subsets_ is coNP-complete ; whether our notion of expansion with respect to a certain family of sets \(\) can be checked efficiently is an interesting direction for future research. We show that expansion can at least be checked _statistically_ (i.e., from finite data), if not efficiently.

For a fixed choice of \(q\), the (non-robust) expansion of a set family \(\) between sets \(A\) and \(B\) is: \(c=_{U:\,(U|B)>q}((U)|A) }{(U|B)}\). Suppose we have two samples \(_{A}=\{(_{i},y(_{i}))\}_{i=1}^{n_{A}}\) with \((|A)\), and \(_{B}=\{(_{i},y(_{i}))\}_{i=1}^{n_{B}}\) with \((|B)\). For a fixed \(U\), the denominator is straightforward to estimate using \(_{B}\) as: \((U|B)}_{_{i}_{B}} [_{i} U]\). Estimating the numerator is less straightforward: due to finite sampling, \(_{A}_{B}\) may contain no pairs \((,^{})\) with \((^{})\). That is, the empirical neighbor graph may be empty even when the population distribution expands (see Wei et al.  for a more thorough discussion). This is a major difference between our assumptions and similar work that uses expansion-like assumptions to analyze the performance of label-propagation algorithms that use the empirical graph, such as Pukdee et al. . To overcome this, we assume we have access to a neighborhood oracle \(n:A B\) that for each \( A\) returns a point \(n() B\) such that \(n()()\). We assume nothing about the distribution of \(n()\) values (i.e., we do not assume that they are drawn from \((|B)\), merely that \((n()|B)>0\)). We describe how to construct \(n\) in a practical scenario in Section 6.

The neighborhood oracle makes estimating the expansion numerator more straightforward, since if \(n() U\), then by construction, \((U)\). Formally, \(((U)|A)(n() U|A)\), where the quality of \(n()\) determines the tightness of this bound. This inequality is valid for any \(n:A B\) as long as \((n())\). Now we can estimate: \((n() U|A)}_{_{i} _{A}}[n(_{i}) U]\). Putting it all together, we can form our empirical estimate of the expansion by solving \(=_{U}_{i}}}_{_{i} _{A}}[n(_{i}) U]}{n_{B}_{_{i} _{B}}[_{i} U]}\) subject to: \(}_{_{i}_{B}}[_{i} U]  q-\), where \(\) is chosen appropriately to account for empirical error in estimating the probability \((U|B)\). The following theorem, proven in Appendix D, shows that the expansion on the population distribution can't be too much smaller than the expansion on the empirical distribution.

**Theorem 5.1** (Expansion generalization, informal).: _For arbitrary \(U\), define the population and empirical expansion estimates as \(c(U):=(n() U| A)/( U | B)\) and \((U):=}_{i=1}^{n_{A}}[n(_{i}) U]/ }_{j=1}^{n_{B}}[_{i} U]\). Then for any \((0,1]\), with probability at least \(1-\), \(_{U}(U)-c(U)}(()/n_{A}}),\) where \(}\) hides constants and log factors in \(()\), \(n_{A}+n_{B}\), and \(1/\)._

Heuristic approximation.While Theorem 5.1 gives a rigorous statistical theory for checking expansion from finite data, it is unfortunately still intractable to compute the set with the worst expansion on the empirical data, i.e., to solve \(=_{U}(U)\). Instead, our experiments in Section 6 use a simple randomized heuristic for approximating this minimization. If the learning algorithm \(:S^{m}\) is deterministic conditioned on the observed training data, we can simplify our hypothesis class \(\) of interest to those \(f\) such that there exists a training sample \( S\) with \(f=()\). Since each \(f\) generates a set \(U(f)\), given a training sample \(\), we can compute \(f=()\), then use our "test" sample \(_{A},_{B}\) to compute \((U(f))\). Repeating this procedure for many samples \(\) and choosing the smallest value approximates \(_{U}(U)\). Table 1 shows the expansion measurements for different hypothesis classes on a weakly-supervised classification task inspired by the example in Section 3. Appendix 6 contains more results and a much more detailed description of the setup and process of checking expansion, but these results indicate that expansion is present and correlates with performance.

## 6 Experiments

**Setup.** We explore training linear classifiers on top of the contrastively-fine-tuned SentenceBERT embeddings2. As shown in Muennighoff et al. , training simple classifiers on top of these complex pretrained representations leads to very competitive performance. We study binary sentiment prediction for movie reviews on the IMDb dataset , continuing with the example from Section 3. For the teacher model, we use a very coarse rule  based on the presence of the unigrams "incredible" and "horrible". Let \(C(w,)\) be the number of times word \(w\) appears in input \(\). The weak label \(()\) is 1 when \(C(,)>C(,)\), 0 when \(C(,)>C(,)\), and \(\) otherwise. This counts the occurrences of "horrible" and "incredible" and assigns the binary label corresponding to the word that occurs strictly more often, and abstains otherwise.

  
**Model** & \(i\) & \((S_{i}^{bad},S_{i}^{good})\) exp. & \(_{i}\) & \((f,|S_{i})\) & Bound val & \((f,y|S_{i})\) \\   & 0 & 0.848 & 0.11 & 0.12 & 0.05 & 0.04 \\  & 1 & 0.497 & 0.33 & 0.29 & 0.37 & 0.35 \\   

Table 1: Measured expansion and error bounds for the covered sets \(S_{i}\). Expansion values for the family of sets \(^{}(S_{i}^{good},)\) are measured using the heuristic described in Section 5 and shown in the \((S_{i}^{bad},S_{i}^{good})\) exp. column. This column shows our heuristic finds expansion in practice. Pseudolabel error \(_{i}=( y|S_{i})\). Worst-case error of trained classifier \(f\) on the weak labels \(\), \((f,|S_{i})\), across 5 independent training runs. This column shows the student can’t exactly fit the teacher labels using this representation. Value of the error upper bound in Theorem 4.1 (specifically, the tighter version, B.1), computed using the numbers from the other columns (details in Appendix E). For label \(i=0\), the bound being strictly less than the error \(_{i}\) of the teacher \(\) suggests pseudolabel correction may occur. Finally, the actual worst-case error of trained classifier \(f\) on the _true_ labels \(y\), \((f,y|S_{i})\), across 5 independent training runs, shows pseudolabel correction _does_ occur for label \(i=0\).

**Neighborhood function and oracle.** We set \(()\) to be the examples obtainable from \(\) by sampling from a pretrained paraphrase model. As described in Section 5, to measure expansion between sets \(A\) and \(B\), we need a neighborhood oracle \(n:A B\) that, given \( A\), returns a point \(^{}() B\). Our results require us to measure expansion between \((S_{i}^{good},T_{i})\), \((S_{i}^{bad},T_{i})\), and \((S_{i}^{bad},S_{i}^{good})\). For \( S_{i}^{good}\) (resp. \( S_{i}^{bad}\)), we generate a target point \(^{}() T\) by rejection sampling from a pretrained paraphrase model. Because \(\) takes a simple form, we can efficiently approximate this step by setting the logits of tokens "horrible" and "incredible" to \(-\) during decoding so they are never generated. For \( S_{i}^{bad}\), to generate a neighbor \(^{}() S_{i}^{good}\), we prompt GPT-4 to paraphrase a randomly chosen sentence from \(^{}() T_{i}\) and include the _correct_ word in its paraphrase, so that \(^{} S_{i}^{good}\). We rejection sample until this constraint is satisfied. Figure 3 (appendix) shows examples of these procedures.

**Expansion results.** Table 1 measures the expansion of the set family \(^{}(S_{i}^{good},)\) on the sets \((S_{i}^{bad},S_{i}^{good})\) using the procedure from Section 5. Theorem 4.1 shows this is related to pseudolabel correction. For the SentenceBERT model, the measured expansion is high and the student fits the weak labels well, but doesn't overfit to the teacher labels (i.e., \((f,|S_{i})>0\)). For label 0, our bound indicates that pseudolabel correction should be present, since the value for our error bound is less than \(_{i}\). There is indeed pseudolabel correction on this label: \((f,y|S_{0})<_{0}\). For label 1, our bound indicates that pseudolabel correction may not occur--the bound value is greater than \(_{i}\) since the the measured expansion is lower for this label and the error of the student on the weak labels is higher. As suggested by the bound, there is no pseudolabel correction: \((f,y|S_{1})>_{1}\). Our expansion-based theory can therefore differentiate between cases where pseudolabel correction does and does not occur. Table 4 (appendix) shows the measured expansion values between the set pairs \((S_{i}^{good},T_{i})\) and \((S_{i}^{bad},T_{i})\), which Theorem 4.2 shows are related to the amount of coverage expansion. For example, for label 1, \(T_{i}\) expands to both \(S_{i}^{good}\) (\(c=0.75\)) _and_ to \(S_{i}^{bad}\) (\(c=0.55\)). The fact that both expansions are nonzero gives evidence for the structure assumed Theorem 4.2. In this case, our coverage expansion bounds show that the student model has nontrivial performance on the uncovered sets \(T_{i}\)--for example, for label 1, the worst-case value of \((f,y|T_{1})\) in all the training runs is 0.29, and the value of our bound is 0.33. Appendix E contains more details on how the models are trained and the bounds are computed.

## 7 Limitations & Conclusion

In this work, we proved error bounds based on _expansion_ properties of the data distribution and student hypothesis class that directly allow for weak-to-strong generalization, gave a statistical theory for checking these expansion properties, and gave empirical evidence that they hold in practice. Our empirical procedure for finding the worst-expanding set generated by our hypothesis class is ultimately still a heuristic, and our experiments are limited in scope. However, Sections 5 and 6 go beyond prior work by testing our assumptions more carefully. Finally, this work contains no new weak supervision _algorithms_ (e.g., new training methods) for improving weak-to-strong generalization. While our work does not propose new weak supervision algorithms, we believe our theory suggests a framework for encouraging weak-to-strong generalization effects: find a neighborhood structure and student hypothesis class pair that expands, then find the student model \(f\) that minimizes the error on the weak labels while staying as robust as possible on the neighborhoods. Both expansion and contrastive pre-training are related to spectral properties of the underlying neighborhood graph . Can we improve the performance of weakly-supervised learning by imbuing the contrastive pretraining objective with knowledge of the pseudolabeler \(\)? Burns et al.  show that weak-to-strong generalization effects are much stronger in classification problems than in other settings such as reward modeling. Adapting our analysis techniques to reward modeling and using our results to design algorithms for weak-to-strong reward model training are interesting directions for future work.

Thanks to Hussein Mozannar and Ilker Demirel for feedback on drafts of this paper. DS and HL were partially supported by NSF AitF award CCF-1723344, AV was supported by NSF grants EECS2216970 and CCF-2154100, and HL was supported by the NDSEG fellowship. Finally, this project was partially supported by an OpenAI "Superalignment Fast" grant.