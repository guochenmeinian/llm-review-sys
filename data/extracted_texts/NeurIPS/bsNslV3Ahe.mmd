# Learning Dynamic Attribute-factored World Models

for Efficient Multi-object Reinforcement Learning

 Fan Feng

City University of Hong Kong

ffengl017@gmail.com &Sara Magliacane

University of Amsterdam

MIT-IBM Watson AI Lab

sara.magliacane@gmail.com

###### Abstract

In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of _compositional generalization_, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them into classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.

## 1 Introduction

Model-based reinforcement learning (MBRL) and world models  have demonstrated improved performance in many RL tasks by providing better sample efficiency. However, most world models focus only on modeling a single object or holistic modeling over the environment, while in real-world tasks, we often have environments with multiple objects that interact, and we are interested in generalizing to unseen combinations and numbers of objects. In recent years, there have been several studies exploring and learning object-oriented environment models or policy models  and tackling the problem of _compositional_ or combinatorial generalization, in which we consider combining the modeling of multiple objects or tasks to solve a new task.

Although these methods have effectively leveraged object-centric and object-factored representations in RL and thus improved the sample efficiency in multi-object settings, they have not fully exploited the benefits of factorization in terms of object attributes. Often an object's transition and reward functions are influenced only by a sparse subset of attributes, e.g. an object's position and reward are affected by its previous position, but not by its appearance or activation state once the object of interest is identified (e.g., "the red block"). In these environments, interactions between objects areoften sparse, both in time and in terms of which attributes are affected, e.g. the position of a box is affected by the position of another box at the timestep in which they collide, but not directly by the other object's friction coefficient. Additionally, objects of the same type share similar factored dynamics, modulated by an object-specific latent parameter, while objects of different types might instead have different attributes, dynamics, and rewards.

In this paper, we propose Dynamic Attribute FacTored RL (DAFT-RL), a framework that learns a fine-grained attribute-factored representation across objects, including a dynamic graph for modeling interactions between objects. As part of this framework, we propose a model, DAFT-MDP, that builds on Factored (PO)MDPs [18; 19; 20; 21; 22; 23], Relational MDPs [24; 25; 26] and especially Object-Oriented (PO)MDPs [27; 28; 29], but focuses on a more fine-grained factorization at the attribute level and dynamic graphs. We implement our framework as a structured and sequential generative model by combining it with state-of-the-art object-centric representation learning [30; 31] for extracting objects and attributes from visual inputs, factored adaptation approaches inspired by the causality literature [32; 33] for estimating the factored dynamics and reward, soft attention networks  for action binding  and (dynamic) Neural Relational Inference [36; 37] for modeling interactions.

Our framework allows us to learn a policy on a set of source environments that can successfully generalize to new environments with unseen combinations of objects with different latent parameters (possibly with unseen values) and types, as well as to combinations of previously learned tasks on different objects, without any further policy learning. We show the benefits of DAFT-RL in three benchmark datasets for compositional generalization, where it outperforms the baselines.

## 2 Dynamic Attribute-FacTored MDPs (DAFT-MDP)

We formalize our assumptions by introducing our DAFT-MDP model, which is an extension with class template graphs, interaction patterns, and interaction graphs of factored (PO)MDPs [18; 19; 20; 21; 22; 23]. This extension takes inspiration from Relational MDPs [24; 25; 26] and their literature, especially Object-Oriented (PO)MDPs [27; 28; 29], but we propose a more fine-grained factorization of the transition and reward at the object attribute level, based on estimating template and dynamic interaction graphs.

Intuitively, we will consider learning a policy that can be generalized across different environments that vary in the number and characteristics of their objects. We will assume that each environment is composed of multiple _objects_, each of a specific type, or _class_. Each object has several observable _attributes_ (e.g. position, velocity) and some latent constant _parameters_ (e.g. an object-specific friction coefficient). Objects of the same class will have the same set of attributes, the same transition and reward functions, but can differ in the values of the attributes (e.g. they are at different positions) and in the value of the latent parameters (e.g. they have different friction coefficients). We will assume that the transition and reward functions can be _factored_ in terms of attributes and that for a given attribute only a sparse subset of other attributes influences these functions.

In our setting, objects can interact with each other, which might influence their dynamics. We will assume that these _interactions_ are _sparse_, both in time and in terms of the effect on the attributes of each object, and that all objects in a class have the same _interaction pattern_ in terms of how the attributes interact with objects of another class. In each environment, we will assume that an action has only an effect on one object at a time.

We formalize these assumptions in the following. We start by defining our class system, and then we describe three types of graphs (class template graphs, interaction patterns, and dynamic interaction graphs in Fig. 1) that describe how dynamics and reward factorize based on the classes, attributes, and interactions, and finally provide a formal definition of a Dynamic Attribute-FacTored MDP.

Class system, attributes, and objects.We assume a known set of classes \(=\{C_{1},,C_{k}\}\) of objects, where each class \(C_{j}\) describes a set of observable attributes \(\{C_{j}.s_{1},,C_{j}.s_{n}\}\), which we assume for simplicity are the same number in each class. We assume that each class has a set of latent constant parameters \(C_{j}.\), that represent physical properties of the object that can vary across different objects of the same type. For example, in one of the benchmarks , we consider two types of objects \(=\{,\}\), boxes and switches, with class attributes \(\{.,.,.,.\}\) representing the object color, position, velocity, and friction coefficient of a box, and \(\{.,.,.\}\), representing the color, position, and activation state of a switch. The class system specifies a template for a set of environments that can vary in the number and characteristics of the objects. Each environment has a fixed set of objects \(=\{o_{1},,o_{m}\}\), where each object is an instance of a class in \(\), which we denote as \(o_{i} C_{j}\) for \(i\{1,,m\},j\{1,,k\}\). We denote the class of an object \(o_{i}\) with \(C(i)\). For example, using the class system from the previous example, we can represent a source environment as \(^{}=\{o_{1},o_{2}\}\) where \(o_{1}\) and \(o_{2}\), and a target environment as \(^{}=\{o_{1},o_{2},o_{3}\}\), where we add \(o_{3}\) to the original objects. For each object \(o_{i} C_{j}\) and timestep \(t=\{1,,T\}\), we denote its attributes at time \(t\) as \(_{i}^{t}=\{o_{i}.s_{1}^{t},,o_{i}.s_{t}^{t}\}\), which are instantiations of the class attributes \(\{C_{j}.s_{1},,C_{j}.s_{n}\}\), and its constant parameters as \(o_{i}.\). In our example, for box \(o_{1}\) the attributes \(o_{1}.s_{1}^{t}\), \(o_{1}.s_{2}^{t}\) and \(o_{1}.\) are its position and velocity at time \(t\) and its friction coefficient, while for the switch \(o_{2}\) the attributes \(o_{2}.s_{1}^{t}\) and \(o_{2}.s_{2}^{t}\) are its position and activation at time \(t\).

States, actions, transitions, and rewards.We define the state at time \(t\) as the collection of all the object states, i.e., \(^{t}=\{_{1}^{t},,_{m}^{t}\}\) with domain \(\). We collect all object-specific latent parameters in a global parameter \(=\{o_{1}.,,o_{m}.\}\) with domain \(\). We define the space of actions \(\) and use \(^{t}\) to denote the action at time \(t\). We denote the transition probability as \(p(^{t+1}|^{t},^{t},)\). The reward at time \(t\) is denoted as \(r^{t}\) and the reward probability is denoted as \(p(r^{t}|^{t},^{t},)\). The transition and reward probabilities are factorized according to the _class template graphs_, _interaction pattern graphs_, and _interaction graphs_ that we can learn from data, and that we will introduce below.

Class template graphs (Fig. 1A).We assume that all objects within the same class share the same factorization in terms of how their own attributes, latent parameters, and actions influence their dynamics and rewards. For example, all boxes will have a similar relationship between their position, velocity, and latent friction parameters, since they follow the same physical laws. Similarly, all switches will share similar dynamics, which will be different from the boxes. We describe these relationships as Dynamic Bayesian Networks (DBNs) , which are graphical models that describe the template for the relations between two contiguous timesteps, \(t\) and \(t+1\), and assume this structure is time-invariant. In particular, for each class \(C_{j}\), we learn a DBN \(_{C_{j}}\) over the nodes \(\{C_{j}.s_{1}^{t},,C_{j}.s_{n}^{t},C_{j}.s_{1}^{t+1},,C_{j}.s_{n}^ {t+1},C_{j}.,^{t},r^{t}\}\) that represents a template for the instance graph between the attributes for each object of this class \(_{o_{i}}^{t}\) for \(o_{i} C_{j}\). In particular, the edges between the action \(a^{t}\) and the attributes at timestep \(t+1\) can be switched on or switched off at different timesteps, since as we will see in the description of the interaction graphs, they represent the interaction pattern of the agent with any specific object.

We show an example of two DBNs for two classes of objects (boxes and switches) in Fig. 1a. In this example, \(C_{1}.s_{2}^{t}\) influences \(C_{1}.s_{2}^{t+1}\) and \(C_{1}.s_{3}^{t+1}\), but it's not influenced by \(C_{1}.s_{3}^{t-1}\). The reward \(r^{t}\) is only influenced by \(C_{1}.s_{3}^{t}\) and the action will only have an effect on \(C_{1}.s_{2}^{t+1}\) and \(C_{1}.s_{3}^{t+1}\). Moreover, \(C_{2}.s_{2}^{t}\) influences only \(C_{2}.s_{2}^{t+1}\) and \(r^{t}\).

Interaction pattern graphs (Fig. 1B).When two objects interact, it often happens that only some of the attributes of each object affect the other. For example, when two boxes collide, their positions and velocities might change, but their masses will not. Therefore, we will assume that from an attribute perspective, the interplay between objects during interaction is also factored

Figure 1: The graphical representation of DAFT-MDP. The colors denote the attributes for an object or a class, the red dashed lines denote edges that can be switched on or off at different timesteps.

and sparse. Additionally, we assume that the interactions between two objects follow patterns based on the classes of the objects. For example, the attribute of a box will always interact in the same way with the attributes of a switch, regardless of the specific object. In particular, for any pair of classes \(C_{i}\) and \(C_{j}\) (possibly also with \(i=j\)), we learn a DBN \(_{C_{i},C_{j}}\) over the nodes \(\{\{C_{i}.s_{l}^{t}\}_{l=1}^{n},\{C_{i}.s_{l}^{t+1}\}_{l=1}^{n},C_{i}.,\{C_{j}.s_{l}^{t}\}_{l=1}^{n},\{C_{j}.\}_{l=1}^{n},C_{j}. {}\}\). We show an example of a DBN describing how boxes interact with other boxes in Fig. 1b. In this case, the interaction is limited to \(C_{1}.s_{2}^{t}\) from one object influencing the \(C_{1}.s_{1}^{t+1}\) and \(C_{1}.s_{2}^{t+1}\) from the other object, and the latent parameters \(C_{1}.\) from each object influencing \(C_{1}.s_{3}^{t+1}\) from the other object. While the interaction patterns are time-invariant, we assume that for each pair of objects, the interactions are only switched on at some points in time, as we will now describe through the interaction graphs.

Dynamic interaction graph (Fig. 1C).The class template and interaction pattern graphs that we just described can model the general behavior of the classes of objects in a static, time-invariant way. On the other hand, in many multi-object environments object interactions occur sparsely, the pairs of interacted objects are not fixed, and the action has an effect only on a limited number of objects at any given time (e.g. we will assume only one for simplicity). We, therefore, propose to model these interactions between objects as a dynamic graph \(_{}=\{_{}^{t}\}_{l=1}^{T}\) at the object level, which is a sequence of graphs \(_{}^{t}\) with edges from a subset of \(\{o_{1}^{t},,o_{m}^{t},^{t}\}\) to a subset of \(\{o_{1}^{t+1},,o_{m}^{t+1}\}\). Each edge \(o_{i}^{t} o_{j}^{t+1}\) represents an interaction between an object \(o_{i}\) and an object \(o_{j}\). This interaction is instantiated in an instance interaction graph \(_{o_{i},o_{j}}^{t}\), following the interaction pattern graph \(_{C_{i},C_{j}}\) for each pair of objects \(o_{i} C_{i}\) and \(o_{j} C_{j}\).

We also learn an _object selector_\(^{t}\{1,,m\}\) that selects the single object on which the action \(^{t}\) has an effect at timestep \(t\), which we represent with an edge from \(^{t}\) to the selected object \(o_{^{t}}^{t}\) in \(_{}^{t}\). In particular, this selection (or _action binding_) activates the edges from \(^{t}\) to the object attributes described in the class template graph \(_{C_{j}}\) in the instantiated version for object \(o_{i} C_{j}\), which we denote \(_{o_{i}}^{t}\). The graph \(_{}\) is dynamic because the edges of each graph in the sequence can change at each timestep \(t=1,,T\).

We show an example of interactions between three objects at different timesteps in Fig. 1C. In this example, at timestep \(t\) there is an interaction between \(o_{1}\) and \(o_{3}\), which will follow the interaction pattern presented in Fig. 1B, i.e. \(o_{1}.s_{2}^{t}\{o_{3}.s_{1}^{t+1},o_{3}.s_{2}^{t+1}\}\) and \(o_{1}.^{t} o_{3}.s_{3}^{t+1}\), and viceversa for \(o_{3}\) towards \(o_{1}\). Moreover, \(^{t}\) has an effect on \(o_{2}\), and specifically \(o_{2}.s_{3}^{t+1}\), following the class template graph presented in Fig. 1A. Instead, at timestep \(t+1\), there are no interactions between the objects and \(^{t+1}\) has an effect on \(o_{1}\), specifically \(o_{1}.s_{2}^{t+2}\) and \(o_{1}.s_{3}^{t+2}\). For completeness, in App. B we provide an example of this graph combined with the instantiated class template graphs for each object \(^{o_{1}},^{o_{2}}\) and \(^{o_{3}}\) for three timesteps \(t,t+1,t+2\), as well as the instantiated interaction pattern graph \(^{o_{1},o_{3}}\) that is switched on at timestep \(t\).

Our modeling assumptions.Now that we have introduced all of the graphical structures that we will need, we can describe our assumptions as a Dynamic Attribute-FacTored Markov Decision Process (DAFT-MDP). We will assume that the class system is fixed and that the objects can vary in each environment, as can their interaction graphs. Under this assumption, a DAFT-MDP defines a family of MDPs, that is parametrized in the objects \(\) and their dynamic interaction graph \(_{}\).

**Definition 1** (Daft-Mdp).: _A Dynamic Attribute-FacTored Markov Decision Process (DAFT-MDP) is a tuple \((,,,,,_{s},,)\), where \(\) is the set of classes, \(\) is the set of objects, \(\) is the space of the constant latent parameters, \(\) the action space, \(_{s}\) is the transition distribution, \(\) is the reward function and \(\) is a set of graphs that contains the collection of class template graphs for each class \(\{_{C_{j}}\}_{C_{j}}\), the collection of interaction pattern graphs for each pair of classes \(\{_{C_{i},C_{j}}\}_{C_{i},C_{j}}\) and the dynamic interaction graph \(_{}\), as defined previously. These graphs define the factorization of the transition distribution per object and per attribute, as follows:_

\[_{s}(^{t+1}|^{t},,^{t})=_{i=1}^{m} _{l=1}^{n}_{s}(o_{i}.s_{l}^{t+1}|_{_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1}),\{_{_{o_{i}}^{t},o_{k }}(o_{i}.s_{l}^{t+1})\}_{o_{k}_{i}_{} ^{t}})\]

_where \(^{t}\) is the collection of all attributes of all objects at time \(t\), \(\) is the collection of all latent constant parameters for all objects, \(^{t}\) is the action. In the right-hand term, \(o_{i}.s_{l}^{t+1}\) is attribute \(s_{l}\) of object \(o_{i}\) at time \(t+1\), while \(_{_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1})\) are the parents of the attribute \(l\) for object \(o_{i}\) based on the class template graph \(_{C(i)}\), where \(C(i)\) is the class of \(o_{i}\), and where the action binding \(^{t}\) activates any potential connections from \(^{t}\). In the second term of the conditioning, we iterate over the objects \(o_{k}\) that are interacting with \(o_{i}\) at time \(t\) in the dynamic interaction graph \(o_{k} o_{i}_{}^{t}\). For each of these objects \(o_{k}\) we collect the attributes that interact with \(o_{i}.s_{l}\) in the instance interaction pattern \(_{o_{i},o_{k}}^{t}\) based on interaction pattern graph \(_{C(i),C(k)}\) for the respective classes \(C(i)\) and \(C(k)\). Similarly, we define the factorization of the reward function per object and per attribute as \((^{t},^{t},)=(\{ _{_{o_{i}}}(r^{t})\}_{o_{i}})\), where for each object \(o_{i}\) we collect all the attributes that have an edge to the reward in the instantiation of the class template graph._

In the following, we assume that the classes \(\) are known and fixed across environments, while the objects \(\) can vary, as can the latent parameters \(\). In the training phase, we will learn how to classify objects, the transition and reward functions based on the class template graphs \(\{_{C_{j}}\}_{C_{j}}\) and the interaction patterns \(\{_{C_{i},C_{j}}\}_{C_{i},C_{j}}\). In the testing phase, we will infer the class and latent parameters of each object, as well as the interactions between the objects in the dynamic interaction graph \(_{}^{t}\), which specify the transition and reward functions in the new environment.

## 3 The DAFT-RL Framework

In the previous section, we introduced our model, DAFT-MDP (Fig. 2). In this section we provide a framework for estimating DAFT-MDPs, leveraging them for policy learning in a set of source environments, and adapting the policy to a new target environment with different objects, without any additional policy learning. Our framework is divided in four steps: (i) offline class learning in single-object environments, (ii) offline interaction learning and latent parameter inference in multi-object environments, (iii) policy learning and imagination in multi-object environments, and finally (iv) adaptation to a new multi-object environment. We present each step and its implementation in the following.

In all steps and all environments, if the input is an image, we extract the objects and their attributes \(^{t}=\{_{1}^{t},_{2}^{t},,_{m}^{ t}\}\) from sequences of images \(^{t}\) with pre-trained object-centric methods, e.g. SA  and AIR . For symbolic inputs, we directly access the attributes of all objects \(^{t}=\{_{1}^{t},_{2}^{t},,_{m}^{ t}\}\). For each set of objects \(\), we learn to classify the objects into their classes \(\) with supervised learning, which we describe in detail in App. C.

Figure 2: The learning pipelines of the DAFT-RL framework.

### Step 1: Class Learning in Single-Object Environments

In this step we consider data from \(m\) single-object environments with different objects and no agent interaction. In particular, for each class \(C_{j}\) we collect the transitions for several objects \(o_{i} C_{j}\) as \(\{_{i}^{t},_{i}^{t},r_{i}^{t}\}_{t=1}^{T}\), in environments in which there is only object \(o_{i}\), and a random policy is used to generate actions. We denote these data as \(^{}=\{\{_{i}^{t},_{i}^{t},r_{i}^ {t}\}_{t=1}^{T}\}_{i C_{j}, j=1,,k,i=1,,m}\).

We initiate the class template \(_{C_{j}}\) for each class \(C_{j}\) randomly, and then use \(^{}\) to learn it, except the contribution of the latent parameters, which is learned in the next phase. In particular, we learn the class template graph by maximizing the log-likelihood of \( p_{}(_{i}^{t+1},r_{i}^{t}_{i}^{t}, _{i}^{t},^{C_{j}})\), where \(=\{_{s},_{r}\}\) are the parameters of dynamics and reward models. For the implementation, we use Gated Recurrent Units (GRU)  to learn the dynamics and reward models jointly with the class template graphs. At time step \(t\) for each object \(i\) with class \(C_{j}\), the inputs to the GRU are \(\{_{_{C_{j}}}(_{i}^{t+1})\}_{i=1}^{m}\) and \(\{_{_{C_{j}}}(r_{i}^{t})\}_{i=1}^{m}\), and the GRU outputs \(_{i}^{t+1}\) and \(r_{i}^{t}\). The learning objective of this step is given below, where we maximize the log-likelihoods of dynamics and reward models and regularize the graph to be sparse:

\[_{,\{^{C_{j}}\}_{j=1}^{k}}_{t=1}^{T}_ {i=1}^{m}_{l=1}^{n}( p_{}(o_{i}.s_{l}^{t+1},r_{i}^{t} _{_{C(i)}}(o_{i}.s_{l}^{t+1}),_{_{ C(i)}}(r_{i}^{t}))-_{j=1}^{k}\|^{C_{j}}\|_{1}\]

where \(m\) and \(k\) indicate the number of single-object environments and object type classes, respectively, and \(_{_{C(i)}}\) denotes the parents of a variable in the template class graph for the class \(C(i)\) of object \(o_{i}\). After this step, we fix the learned \(\{^{C_{1}},^{C_{2}},,^{C_{k}}\}\) with the exception of the edges from the latent parameters \(\), which here we assume are disabled and we will learn in the next step. In later stages, we will reuse the learned reward model \(_{r}\) and the class template graphs.

### Step 2: Interaction Learning and Latent Parameter Inference in Multi-object Environments

In this step, we consider data \(^{}\) from \(N\) multi-object environments with different object configurations and in which the objects can have varying latent parameters. Formally, we define \(^{}=\{\{_{1}^{t},_{1}^{t},r_{1}^ {t},_{2}^{t},_{2}^{t},r_{2}^{t},,_{m}^{t}, _{m}^{t},r_{m}^{t}\}_{t=1}^{T}\}_{m=1}^{N}\). In each of these environments we assume the agent can interact only with one object at a time. On these data, we again extract the objects and their attributes from a sequence of images with pretrained object-centric methods and classify the objects using the object classifier. We use these data to learn the interaction pattern graphs \(_{C_{i},C_{j}}\) for each pair of classes \(C_{i}\) and \(C_{j}\) and the dynamic interaction graph \(_{}\) by exploiting the previously learned class template graphs. In particular, we first learn the action binding \(^{t}\), and at a second stage, we jointly learn the rest of the dynamic interaction graph \(_{}\), the interaction patterns \(_{C_{i},C_{j}}\) for each pair of classes, the object-specific latent parameters \(o_{i}.\) and their edges to the other attributes in \(_{C_{j}}\). We describe these two stages in detail in the following.

#### 3.2.1 Step 2.1: Learning the Action Binding

Motivated by , we learn the dynamic action binding \(=\{^{1},^{2},,^{T}\}\) using soft attention networks, which are modified by the single-head self-attention module in the Transformer model . Specifically, we perform non-linear transformations on the states and actions using multi-layer perceptrons (MLPs) to derive the key \(^{t}= f_{k}(_{1}^{t}),f_{k}(_{2}^{t}), ,f_{k}(_{m}^{t})\), query \(^{t}=f_{q}(^{t})\), and value \(^{t}= f_{v}(_{1}^{t}),f_{v}(_{2}^{t}), ,f_{v}(_{m}^{t})\), respectively. We then compute the attention weights \(^{t}=((_{1}^{t})^{}^{t },(_{2}^{t})^{}^{t},,(_{m}^{t})^{ }^{t})\). We use the learned attention weights \(\) as the action binding selector, as it provides an estimation of the binding affinity from the action to the objects at each time step. The soft attention mechanism assigns weights from the action to the objects by multiplying the value vector \(^{t}\) with the attention weights \(^{t}\), and then embeds the weighted actions into the dynamics of each object. We maintain a fixed structure for the class template graphs and focus on learning the action binding selector by updating \(f_{k}\), \(f_{q}\), and \(f_{v}\).

#### 3.2.2 Step 2.2: Learning Dynamic Interaction Graphs

As noted in Section 2, the interaction among objects may change over time and usually occurs in a sparse manner. To learn this dynamic graph, we leverage a sequential latent variable model to inferthe object interaction graph. Following the neural relational inference (NRI) works [36; 37], we use an encoder to generate the latent variables and subsequently sample the interaction graph based on these variables. Specifically, we use graph neural networks (GNN) as the encoder module, where the nodes represent the states of each object, and the predicted edges denote the temporal interactions between the objects. In line with the dynamic NRI (dNRI) , we use recurrent units to model the temporal relations of the interaction graph. We outline the key components of the model in the following, and provide the detailed description in App. C.

**Encoder and prior** During training, at each time step \(t\), we use GNN layers to generate hidden embeddings \(^{t}=(_{1}^{t},_{2}^{t},, _{m}^{t})\). For each object pair \(o_{i}\) and \(o_{j}\), we then obtain \(_{(i,j)}^{t}\). For the encoder, we use a Gated Recurrent Unit (GRU)  to model the temporal dependency of the interaction graphs. The inputs to the GRU include the future embeddings of the states, which then generate the updated embeddings: \(_{(i,j),}^{t}=_{}(_{(i,j)}^{t},_{(i,j)}^{t+1})\). For the prior, we also use a GRU, but in this context, we append \(_{(i,j)}^{t}\) with the output of the GRU from the previous step \(_{(i,j),}^{t-1}\) as the input. In specific terms, we have \(_{(i,j),}^{t}=_{}( _{(i,j)}^{t},_{(i,j),}^{t-1})\). For both the encoder and the prior, we feed the output embedding from the GRUs to an MLP layer to derive the distribution of the latent variables \(q_{}(^{t}^{1:T})\), where \(^{1:T}=\{_{1}^{t},,_{m}^{t}\}_{t=1}^{T}\), and prior distribution \(p_{}(^{t}^{1:t},^{1:t-1})\), respectively. We assume that the encoder outputs a Bernoulli distribution for each edge and the graph \(_{}\) is sampled using the Gumbel-Softmax trick .

**Decoder** We perform one-step prediction to generate \(}_{i}^{t+1}\) for each object \(o_{i}\), predicting the state dynamics with the learned graphs, including \(_{}\), as well as the interaction pattern graph \(_{C_{i},C_{j}}\) for two objects with class \(C_{i}\) and \(C_{j}\). At the same time, our goal is also to learn the interaction pattern graph with sparsity regularization. We also learn the latent parameters, \(\) at this stage. Specifically, we also incorporate \(C_{j}.\) and the graph from \(_{C_{j},,_{i}^{t}}\) for each object \(o_{i}\) with class \(C_{j}\) into the dynamics model and use the sparsity regularization for \(_{C_{j},,_{i}}\).

Therefore, the learning objectives include maximizing the likelihood of the dynamics model, minimizing the KL divergence between \(p_{}\) and \(q_{}\) to estimate \(\), and encouraging the sparsity of the interaction pattern graph \(_{C_{i},C_{j}}\) and the subgraph from latent parameters to states \(_{C_{i},,_{i}}\):

\[_{_{s},,v,k,q,,} _{t=1}^{T}_{i=1}^{m}_{l=1}^{n} p_{_{s}}o_{i}.s_{ l}^{t+1}_{_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1}),\{ _{_{o_{i},o_{k}}^{t}}(o_{i}.s_{l}^{t+1})\}_{o_{k } o_{i}_{}^{t}}\] \[-_{j=1}^{k}_{i=1}^{k}_{C_{i},C_{j}} _{1}-_{i=1}^{m}_{j=1}^{k}_{C_{j}, ,_{i}}_{1}-_{t=2}^{T} q_{}(^{t}^{1:T})\|p_{}( ^{t}^{1:t},^{1:t-1})\]

where \(\) is the dynamic interaction graph \(_{}\), interaction pattern graphs \(\{_{C_{i},C_{j}} i\{1,2,,k\},j\{1,2,,k\}\}\) and the subgraph from latent parameters to states \(\{_{C_{j},,_{i}} i\{1,2,,m \},j\{1,2,,k\}\}\). Similarly, to Definition 1, \(_{_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1})\) indicates the parents of the attribute \(s_{l}\) for object \(o_{i}\) based on the class template graph \(_{C(i)}\), where \(C(i)\) is the class of \(o_{i}\), and where the action binding \(^{t}\) activates or deactivates any potential connections from \(^{t}\). In the second term of the conditioning, we iterate over the objects \(o_{k}\) that are interacting with \(o_{i}\) at time \(t\) in the dynamic interaction graph \(o_{k} o_{i}_{}^{t}\). For each of these objects \(o_{k}\) we collect the attributes that interact with \(o_{i}.s_{l}\) in the instance interaction pattern \(_{o_{i},o_{k}}^{t}\) based on interaction pattern graph \(_{C(i),C(k)}\) for the respective classes \(C(i)\) and \(C(k)\). \(_{s}\) and \(\) indicate the dynamics model and encoder parameters, while \(v,k,q\) are the parameters of MLPs for learning the attention models. After this, we have learned all the graphs, dynamics, and reward models that estimate DAFT-MDP.

### Step 3: Policy Learning and Imagination in Multi-object Environments

In the first two phases, we have learned the template for our world model, which we can now finetune to new multi-object domains by inferring the environment-specific latent parameters \(\) and the interaction graph \(_{}\). We again consider several multi-object environments with different object configurations and in which the objects can have varying latent parameters. For each environment, we then use the finetuned environment-specific world model to create a set of imagined trajectories. Finally, we can learn a policy \(^{*}(^{t}|^{t},,_{})\) across different environments, based on the real and imagined trajectories. We can apply policy learning or planning methods using any RL algorithms. Totake full advantage of the estimated models, we use RL or planning methods such as model predictive control (MPC)  and proximal policy optimization (PPO)  to learn \(^{*}\). Detailed information about the domain parameters are provided in App. C.

### Step 4: Adaptation to the New multi-Object Environment

In a new environment, we apply the policy \(^{*}(^{t}|^{t},,_{})\) by inferring latent parameters \(\) and dynamic interaction graphs \(_{}\) based on a few trajectories, without any policy learning.

## 4 Related Work

We shortly summarize the related work in this section and provide a more detailed discussion of related work, including the discussion of each method and a comparison based on the method features in App. A. Recent work in object-oriented and relational RL has incorporated various inductive biases for modeling object relations into both model-based and model-free RL frameworks. Zhou et al.  investigate the benefits of deep learning architectures, such as MLP, self-attention, and deep-sets in goal-conditioned RL with factorized object states. Likewise, Yoon et al.  provide a comprehensive empirical evaluation of pre-trained object-oriented models, such as SA  and SLATE , for model and policy learning in multi-object RL. Mambelli et al.  use linear relational networks to model object interactions and learn the policy. Another line of research focuses on learning structured representations among objects or their interactions [9; 5; 6; 8; 12; 17; 16; 44; 45; 46; 47; 48; 49]. Most of these approaches aim to learn an object-wise factorization model, either with structured symbolic input or high-dimensional raw pixels as input. NCS  and STEDIE  go further by disentangling action/control-relevant or irrelevant features for each object. Unlike these works, we propose a more fine-grained factored world model that considers the structure among all attributes of the objects as well as the dynamic interaction among all objects.

## 5 Experimental Evaluation

We consider a diverse set of RL benchmarks and setups, including modified OpenAI Fetch environments [50; 15; 51] (symbolic inputs), Spriteworld  (visual inputs), and the Block-stacking benchmark (visual inputs). We compare our approach with several baseline models. These include methods using Deep sets, GNN, and Self-Attention as the inductive bias [15; 4], such as SRICS , STOVE , SMORL , NCS , LRN , and COBRA . To ensure a fair comparison, we modify these baselines, so that methods that originally only support symbolic input can also be adapted to handle image-based input by using visual encoders to obtain the symbolic states, and add an imagination component. We provide the details of these modifications in App. D.1. We provide descriptions of all setups, training and testing domains for each benchmark in App. D.2. Here we describe the most representative results, but we provide the complete results in App. D.3.

Symbolic benchmark: OpenAI Fetch - Push and Switch.Following , we modify the OpenAI Gym Fetch environment  to create the \(N\)-push and \(N\)-switch benchmarks, where \(N\) denotes the number of objects. In the \(N\)-push task, the agent is trained to push all cubes to their corresponding target positions. Similarly, for the \(N\)-switch task, the agent is required to flip all switches in the environment. In this benchmark, all inputs are symbolic. As a sanity check, we show in App. D.3 that our method has comparable s with the baselines in the _single-task mode_, in which we train the model estimation and policy learning individually on the \(2\)-Push, \(3\)-Push, \(2\)-Switch, and \(3\)-Switch tasks. To evaluate _compositional generalization_, we train on the set of tasks {\(1\)-Push, \(1\)-Switch, \(2\)-Push, \(2\)-Switch}; while during the test phase, we test the model in different settings as shown in Table 1. We consider combinations of skills (denoted by \(S\)), e.g. \(2\)-Push+\(2\)-Switch (\(S\)), which combines the training tasks \(2\)-Push, \(2\)-Switch. We also consider changes in the number of objects (denoted by \(O\)) and skills, e.g. \(3\)-Push+\(3\)-Switch (\(S\)+\(O\)), which combines the training tasks but also varies the number of objects. We also test whether our model can achieve efficient transfer during testing when the objects' latent parameters differ from the training samples (denoted by \(L\)). Generally, during training, we consider the objects with masses and friction coefficients uniformly sampled from a set of values, and during testing, we test the model with two different sets of values. For example \(2\)-Switch (\(L\)) considers this case. Finally, we consider the challenging setting in which we combine all of these changes. As seen in Table 1, in most cases DAFT-RL outperforms the baselines, with a bigger gain 

[MISSING_PAGE_FAIL:9]

the learned graphs in App. D.3, which show that our model is capable of learning the true causal of a single object in symbolic cases. For pixel inputs, we do not have ground-truth causal graphs.

## 6 Conclusions, Limitations, and Future Work

We proposed Dynamic Attribute FacTored RL (DAFT-RL), a framework that leverages learned attribute-factored representations with dynamic graphs. For each class of object, we learned a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes, as well as an interaction pattern graph that describes how it interacts with objects of different classes at the attribute level. We also learned about interactions between objects and with the agent with a dynamic graph. Through this template world model, we learned a policy that can then be directly applied in a new environment by estimating the interactions and latent parameters. We showed that DAFT-RL outperforms the state-of-the-art in three compositional generalization benchmarks.

One limitation of our method is that during the adaptation phase, we only consider object classes that the agent has encountered before. This means that our model might encounter challenges when introduced to unseen object classes. One potential area for future exploration is the integration of open-set recognition  to identify and adapt to unseen object classes. Furthermore, we could integrate the causal representations  in multi-entity environments that could benefit the downstream compositional generalization tasks. Another exciting future direction is to explore the real-world object-centric manipulation or interactive tasks  or even complex embodied AI tasks .

#### Acknowledgments

FF would like to acknowledge the CityU High-Performance Computing (HPC) resources in Hong Kong SAR and LISA HPC from the SURF.nl. SM was supported by the MIT-IBM Watson AI Lab and the Air Force Office of Scientific Research under award number FA8655-22-1-7155. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force.

Figure 3: A. The smoothed learning curve for \(2\)-Push + \(2\)-Switch (L+S) with different friction coefficients for each object (for clarity, we show only the top three methods in terms of the success rate); B. The smoothed learning curve for the object comparison task in Spriteworld with unseen object numbers, combinations of colors and shapes (for clarity, we show only the top three methods in terms of the success rate); C. Success rate versus number of blocks in the stacking task, where each block has distinct mass.

**Experiment** &  \\
**Setting** & DAF-RL (SA) & DAF-RL (AIR) & SMORL & SRICS & GNN & STOVE & NCS & LRN \\ 
2 Blocks & \(0.809 0.019\) & \(\) & \(0.658 0.028\) & \(0.704 0.016\) & \(0.549 0.016\) & \(0.728 0.044\) & \(0.797 0.035\) & \(0.649 0.026\) \\
4 Blocks & \(\) & \(0.698 0.022\) & \(0.605 0.020\) & \(0.591 0.049\) & \(0.526 0.041\) & \(0.498 0.013\) & \(0.571 0.026\) & \(0.461 0.028\) \\
6 Blocks & \(0.591 0.025\) & \(\) & \(0.536 0.040\) & \(0.509 0.043\) & \(0.461 0.088\) & \(0.475 0.023\) & \(0.521 0.049\) & \(0.602 0.097\) \\
8 Blocks & \(0.506 0.083\) & \(\) & \(0.386 0.062\) & \(0.420 0.061\) & \(0.334 0.047\) & \(0.278 0.086\) & \(0.397 0.052\) & \(0.463 0.077\