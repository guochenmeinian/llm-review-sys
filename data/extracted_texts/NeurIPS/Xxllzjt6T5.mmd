# ReSync: Riemannian Subgradient-based

Robust Rotation Synchronization

 Huikang Liu

School of Information Management and Engineering

Shanghai University of Finance and Economics

liuhuikang@shufe.edu.cn

Xiao Li

School of Data Science

The Chinese University of Hong Kong, Shenzhen

lixiao@cuhk.edu.cn

Corresponding Author

&Anthony Man-Cho So

Department of Systems Engineering and Engineering Management

The Chinese University of Hong Kong

manchoso@se.cuhk.edu.hk

###### Abstract

This work presents ReSync, a Riemannian subgradient-based algorithm for solving the robust rotation synchronization problem, which arises in various engineering applications. ReSync solves a least-unsquared minimization formulation over the rotation group, which is nonsmooth and nonconvex, and aims at recovering the underlying rotations directly. We provide strong theoretical guarantees for ReSync under the random corruption setting. Specifically, we first show that the initialization procedure of ReSync yields a proper initial point that lies in a local region around the ground-truth rotations. We next establish the weak sharpness property of the aforementioned formulation and then utilize this property to derive the local linear convergence of ReSync to the ground-truth rotations. By combining these guarantees, we conclude that ReSync converges linearly to the ground-truth rotations under appropriate conditions. Experiment results demonstrate the effectiveness of ReSync.

## 1 Introduction

Rotation synchronization (RS) is a fundamental problem in many engineering applications. For instance, RS (also known as "rotation averaging") is an important subproblem of structure from motion (SfM) and simultaneous localization and mapping (SLAM) in computer vision , where the goal is to compute the absolute orientations of objects from relative rotations between pairs of objects. RS has also been applied to sensor network localization , signal recovery from phaseless observations , digital communications , and cryo-EM imaging .

Practical measurements of relative rotations are often _incomplete_ and _corrupted_, leading to the problem of robust rotation synchronization (RRS) . The goal of RRS is to reconstruct a set of ground-truth rotations \(_{1}^{},,_{i}^{},,_{n}^{} (d)\) from measurements ofrelative rotations represented as

\[_{ij}=_{i}^{}_{j}^{},&(i, j),\\ _{ij},&(i,j),\\ ,&(i,j)^{c},(i,j) ,&pq,\\ ,&(1-p)q,\\ ^{c},&.\] (1)

Here, \((d):=^{d d}:^{}= ,()=1}\) denotes the rotation group (also known as the special orthogonal group), \(\) represents the indices of all available observations, \(\) denotes the indices of true observations, \(^{c}:=\) is the indices of _outliers_, \(_{ij}(d)\) is an outlying observation, and the missing observations are set to be \(\) by convention; see, e.g., [23, section 2.1]. We use \(q(0,1)\) to denote the observation ratio and \(p(0,1)\) to denote the ratio of true observations.

**Related works.** Due to the vast amount of research in this field, our overview will necessarily focus on theoretical investigations of \(\). In the case where no outliers exist in the measurement model (1), i.e., \(p=1\), a natural formulation is to minimize a smooth least-squares function \(_{(j,j)}\|_{i}_{j}^{}-_{ij}\|_{F}^{2}\) over \(_{i}(d),\;1 i n\). Spectral relaxation and semidefinite relaxation (SDR) are typical approaches for addressing this problem , where they provide strong recovery guarantees. However, these results cannot be directly applied to the corrupted model (1) due to the existence of outliers (i.e., \(p<1\)) and the sensitivity of the least-squares solution to outlying observations.

Theoretical understanding of \(\) is still rather limited. One typical setting for theoretical analysis of \(\) is the random corruption model (\(\)); see Section 2.2. The work  introduces a least-unsquared formulation and applies the SDR method to tackle it. Under the \(\) and in the full observation case where \(q=1\), it is shown that the minimizer of the SDR reformulation exactly recovers the underlying Gram matrix (hence the ground-truth rotations) under the conditions that the true observation ratio \(p 0.46\) for \((2)\) (and \(p 0.49\) for \((3)\)) and \(n\). In , the authors established the relationship between cycle-consistency and exact recovery and introduced a message-passing algorithm. Their method is tailored to find the corruption level in the graph, rather than recovering the ground-truth rotations directly. They provided linear convergence guarantees for their algorithm once the ratios satisfy \(p^{8}q^{2}=( n/n)\) under the \(\). However, it is unclear how this message-passing algorithm is related to other optimization procedures for solving the problem. Let us mention that they also provided guarantees for other compact groups and corruption settings. Following partly the framework established in , the work  presents an interesting nonconvex quadratic programming formulation of \(\). It is shown that the global minimizer of the nonconvex formulation recovers the true corruption level (still not the ground-true rotations directly) when \(p^{2}q^{2}=( n/n)\) under the \(\). Unfortunately, the work does not provide a concrete algorithm that provably finds a global minimizer of the nonconvex formulation. In , the authors introduced and analyzed a depth descent algorithm for recovering the underlying rotation matrices. In the context of the \(\), they showed asymptotic convergence of their algorithm to the underlying rotations without providing a specific rate. The result is achieved under the conditions that the algorithm is initialized near \(^{}\), \(q( n/n)\), and \(p 1-1/(d(d-1)+2)\). The latter requirement translates to \(p 3/4\) for \((2)\) and \(p 7/8\) for \((3)\). It is important to note, however, that the primary focus of their research lies in the adversarial corruption setup rather than the \(\).

**Main contributions.** Towards tackling the \(\) problem under the measurement model (1), we consider the following least-unsquared formulation, which was introduced in  as the initial step for applying the SDR method:

\[&^{nd d}}{}\;f():=_{(i,j)}\|_{i}_{j}^{}-_{ij} \|_{F}\\ &\;_{i}(d),\;1 i n.\] (2)

Note that this problem is _nonsmooth_ and _nonconvex_ due to the unsquared Frobenius-norm loss and the rotation group constraint, respectively. We design a _Riemannian **S**ubgradient **s**ynchronization algorithm (\(\)) for addressing problem (2); see Algorithm 1. \(\) will first call an initialization procedure named \(\) (see Algorithm 2), which is a spectral relaxation method. Then, it implements an iterative Riemannian subgradient procedure. \(\) targets at directly recovering the ground-truth rotations \(^{}(d)^{n}\) rather than the Gram matrix or the corruption level. Under the \(\) (see Section 2.2), we provide the following strong theoretical guarantees for \(\):1. _Initialization_. The first step of \(\) is to call \(\) for computing the initial point \(^{0}\). Theoretically, we establish that \(^{0}\) can be relatively close to \(^{}\) depending on \(p\) and \(q\); see Theorem 2.
2. _Weak sharpness_. We then establish a problem-intrinsic property of the formulation (2) called weak sharpness; see Theorem 3. This property characterizes the geometry of problem (2) and is of independent interest.
3. _Convergence analysis_. Finally, we derive the local linear rate of convergence for \(\) based on the established weak sharpness property; see Theorem 4.

The main idea is that the weak sharpness property in (S.2) helps to show _linear convergence_ of \(\) to \(^{}\) in (S.3). However, this result only holds _locally_. Thus, we need the initialization guarantee in (S.1) to initialize our algorithm in this local region and then argue that it will not leave this region once initialized. We refer to Sections 3.1 to 3.3 for more technical challenges and our proof ideas. Combining the above theoretical results yields our overall guarantee: \(\)_converges linearly to the ground-truth rotations \(^{}\) when \(p^{7}q^{2}=( n/n)\)_; see Theorem 1.

**Notation**. Our notation is mostly standard. We use \(^{nd d}=(_{1};;_{n})(d) ^{n}\) to represent the Cartesian product of all the variables \(_{i}(d),1 i n\). The same applies to the ground-truth rotations \(^{}=(_{1}^{};;_{n}^{})\). Let \(_{i}=\{j(i,j)\}\), \(_{i}=\{j(i,j)\}\), and \(_{i}^{c}=_{i}_{i}\). We also define \(_{ij}=_{i}_{j}\) for simplicity. For a set \(S\), we use \(|S|\) to denote its cardinality. For any matrix \(,^{nd d}\), we define the following distance up to a global rotation:

\[(,)=\|-^{}\|_{F}, ^{}=*{arg\,min}_{(d)}\| -\|_{F}^{2}=_{(d)}(^{}).\]

Besides, we introduce the following distances up to the global rotation \(^{}\) defined above:

\[_{1}(,)=_{i=1}^{n}\|_{i}- _{i}^{}\|_{F},_{}(,) =_{1 i n}\|_{i}-_{i}^{}\|_{F}.\]

## 2 Algorithm and Setup

### ReSync: Algorithm Development

In this subsection, we present \(\) for tackling the nonsmooth nonconvex formulation (2); see Algorithm 1. Our algorithm has two main parts, i.e., initialization and an iterative Riemannian subgradient procedure.

**Initialization.**\(\) first calls a procedure \(\) (see Algorithm 2) for initialization. \(\) is a spectral relaxation-based initialization technique. \(\) computes the first \(d\) leading unit eigenvectors of the data matrix to form \(^{nd d}\). We multiply \(\) to those eigenvectors to ensure that its norm matches that of \((d)^{n}\). We also construct \(\), which reverses the sign of the last column of \(\) so that the determinants of \(\) and \(\) differ by a sign. Then, we compute the projection of \(\) and \(\) onto \((d)^{n}\). The projection is computed in a block-wise manner, namely

\[}_{i}=_{(d)}(_{i}), 1  i n,\]

where \(_{i},}_{i}^{d d}\) are the \(i\)-th block of \(\) and \(}\), respectively.

The projection can be explicitly evaluated as

\[}_{i}=_{i}_{i}^{},& (_{i})>0,\\ }_{i}_{i}^{},&, 1 i n.\]

Here, \(_{i},_{i}^{d d}\) are the left and right singular vectors of \(_{i}\) (with descending order of singular values), respectively, and \(}_{i}\) is obtained by reversing the sign of the last column of \(_{i}\). The initial point \(^{0}\) is chosen as \(}\) or \(}\), depending on which is closer to \((d)^{n}\).

Let us mention that the computation of \(}\) and Steps 5 - 9 in \(\) can practically improve the approximation error \((^{0},^{})\). We demonstrate such a phenomenon in Figure 1, in which "Naive \(\)" refers to outputing \(^{0}=}\) directly in Algorithm 2.

**Riemannian subgradient update.** ReSync then implements an iterative Riemannian subgradient procedure after obtaining the initial point \(^{0}\). The key is to compute the search direction (Riemannian subgradient) \(_{}f(_{i}^{k})\) and the retraction \(_{_{i}^{k}}()\) onto \((d)\) for \(1 i n\). Towards providing concrete formulas for the Riemannian subgradient update, let us impose the Euclidean inner product \(,=(^{})\) as the inherent Riemannian metric. Consequently, the tangent space to \((d)\) at \((d)\) is given by \(_{}:=\{:^{d d},+ {S}^{}=0\}\). The Riemannian subgradient \(_{}f(_{i})\) can be computed as [40, Theorem 5.1]

\[_{}f(_{i})=_{_{ _{i}}}(f(_{i})), 1 i n,\] (3)

where the projection can be computed as \(_{_{_{i}}}()=_{i}_{i}^{ }-^{}_{i}/2\) for any \(^{d d}\) and \(f(_{i})\) is the Euclidean subgradient of \(f\) with respect to the \(i\)-th block variable \(_{i}\). Let us define \(f_{i,j}():=\|_{i}_{j}^{}-_{ij}\|_{F}\). The Euclidean subdifferential \( f(_{i})\) with respect to the block variable \(_{i}\) is given by

\[ f(_{i})=2_{j:(i,j)} f_{i,j}(_{ i}), f_{i,j}(_{i})=_{i}- _{ij}_{j}}{\|_{i}_{j}^{}-_{ij}\|_{F}},\ \ \|_{i}_{j}^{}-_{ij}\|_{F}  0,\\ ^{d d},\ \|\|_{F} 1,\ \]

```
0: Initialize \(^{0}=()\) (Algorithm 2), where \(^{nd nd}\) and its \((i,j)\)-th block is \(_{i,j}^{d d}\);
1: Set iteration count \(k=0\);
2:while stopping criterion not met do
3: Update the step size \(_{k}\);
4: Riemannian subgradient update: \[_{i}^{k+1}=_{_{i}^{k}}(-_{k}_{}f(_{i}^{k}))\] for \(1 i n\);
5: Update iteration count \(k=k+1\);
6:endwhile ```

**Algorithm 1** ReSync: Riemannian Subgradient Synchronization

```
0: Initialize \(^{0}=()\) (Algorithm 2), where \(^{nd nd}\) and its \((i,j)\)-th block is \(_{i,j}^{d d}\);
1: Set iteration count \(k=0\);
2:while stopping criterion not met do
3: Update the step size \(_{k}\);
4: Riemannian subgradient update: \[_{i}^{k+1}=_{_{i}^{k}}(-_{k}_{}f(_{i}^{k}))\] for \(1 i n\);
5: Update iteration count \(k=k+1\);
6:endwhile ```

**Algorithm 2** SpectrIn: Spectral Initialization

Any element \(f(_{i}) f(_{i})\) is called a Euclidean subgradient. In \(\), one can choose an arbitrary subgradient \(f(_{i}) f(_{i})\) at \(_{i}\).

Mimicking the gradient method to update along the search direction \(_{}f(_{i})\) provides a point \(_{i}^{+}=_{i}-_{}f(_{i})\) on the tangent space \(_{_{i}}\) at \(_{i}\), which may violate the manifold constraint "\(_{i}^{+}(d)\)". One common approach in Riemannian optimization is to employ a retraction operator to address the feasibility issue. For \((d)\), we can use a QR decomposition-based retraction and implement the Riemannian subgradient step as

\[_{i}^{+}=_{_{i}}(-_{ }f(_{i}))=(_{i}-_{}f(_{i})), 1 i n.\] (4)

Here, \(()\) returns the Q-factor in the thin QR decomposition of \(\), while the diagonal entries of the R-factor are restricted to be positive .

Finally, setting \(_{i}=_{i}^{k}\), \(_{j}=_{j}^{k}\) for all \(j\) such that \((i,j)\), \(=_{k}\) in (3) and (4) yields a concrete implementation of Step 4 in \(\) and leads to \((d)_{i}^{k+1}=_{i}^{+}\) for \(1 i n\). This completes the description of one full iteration of \(\). Note that the per-iteration complexity of the Riemannian subgradient procedure is \((n^{2}q)\), and Algorithm 2 has computational cost \((n^{3})\).

### RCM Setup for Theoretical Analysis

We develop our theoretical analysis of \(\) by adopting the random corruption model (\(\)). The \(\) was previously used in many works to analyze the performance of various synchronization algorithms; see, e.g., . Specifically, we can represent our measurement model (1) on a graph \((,)\), where \(\) is a set of \(n\) nodes representing \(\{_{1}^{},,_{n}^{}\}\) and \(\) is a set of edgescontaining all the available measurements \(\{_{i,j},(i,j)\}\). We assume that the graph \(\) follows the well-known Erdos-Renyi model \((n,q)\), which implies that each edge \((i,j)\) is observed with probability \(q\), independently from every other edge. Each edge \((i,j)\) is a true observation (i.e., \((i,j)\)) with probability \(p\) and an outlier (i.e., \((i,j)^{}\)) with probability \(1-p\). Furthermore, the outliers \(\{_{i,j}\}_{(i,j)^{}}\) are assumed to be independently and uniformly distributed on \((d)\).

## 3 Main Results

In this section, we present our theoretical results for \(\). Our main results are summarized in the following theorem, which states that our proposed algorithm can converge at a linear rate to the underlying rotations \(^{}\). Our standing assumption in this section is stated below.

All our theoretical results in this section are based on the RCM; see Section 2.2.

**Theorem 1** (overall).: _Suppose that the ratios \(p\) and \(q\) satisfy_

\[p^{7}q^{2}=().\]

_With probability at least \(1-(1/n)\), \(\) with \(_{k}=_{0}^{k}\), where \(_{0}=(p^{2}/n)\) and \(=1-\), converges linearly to the ground-truth rotations \(^{}\) (up to a global rotation), i.e.,_

\[(^{k},^{})_{0} ^{k},_{}(^{k},^{}) _{0}^{k}, k 0.\]

_Here, \(_{0}=(q})\) and \(_{0}=(p^{2})\)._

The basic idea of the proof is to establish the problem-intrinsic property of weak sharpness and then use it to derive a linear convergence result. However, the result only holds locally. Thus, we develop a procedure to initialize the algorithm in this local region and argue that \(\) will not leave this region afterwards. In the remaining parts of this section, we implement the above ideas and highlight the challenges and approaches to overcoming them.

### Analysis of SpectrIn with Leave-One-Out Technique

**Theorem 2** (initialization).: _Let \(^{0}\) be generated by \(\) (see Algorithm 2). Suppose that the ratios \(p\) and \(q\) satisfy_

\[p^{2}q=().\]

_Then, with probability at least \(1-(1/n)\), we have_

\[(^{0},^{})=(}{p})_{}( ^{0},^{})=(}{p} ).\] (5)

The works  and  show that exact reconstruction of \(^{}\) is information-theoretically possible if the condition \(p^{2}q=( n/n)\) holds for the cases \(d=2\) and \(d=3\), respectively. Though Theorem 2 does not provide exact recovery, it achieves an optimal sample complexity for reconstructing an approximate solution in the infinity norm. Specifically, Theorem 2 shows that, as long as \(p^{2}q C n/n\) for some constant \(C>0\) large enough, the \(_{}\)-distance \(_{}(^{0},^{})\) (i.e., \(_{1 i n}(_{i},_{i}^{})\)) can be made relatively small. However, the \(_{2}\)-distance \((^{0},^{})\) is of the order \(()\) under such a sample complexity.

The work  considers orthogonal and permutation group synchronization and shows that spectral relaxation-based methods achieve near-optimal performance bounds. Our result differs from that of  in twofold: 1) Our approach follows the standard leave-one-out analysis based on the standard "\(\)" (up to \((d)\) invariance) defined above Lemma 3 in the Appendix. Nonetheless, we have to transfer the results to "\(\)" due to the structure of \((d)\) in Lemma 5, which is a nontrivial step due to the specific structure of \((d)\). 2) Our result can handle incomplete observations (i.e., \(q<1\)). In the case of incomplete observations, the construction in (17) in the Appendix becomes more intricate; it has the additional third column, rendering the analysis of our Lemma 2 more involved.

We prove Theorem 2 with some matrix concentration bounds and the leave-one-out technique. We provide the proof sketch below and refer to Appendix A for the full derivations.

**Proof outline of Theorem 2**. According to (1) and the fact \((_{ij})=\) since outliers are assumed to be independently and uniformly distributed on \((d)\) in the RCM (see Appendix A), we know that \((_{ij})=pq_{i}^{}_{j}^{}\) for all \((i,j)[n][n]\). This motivates us to introduce the noise matrix \(_{ij}=_{ij}-pq_{i}^{}_{j}^{}\), i.e.,

\[=pq^{}^{}+.\] (6)

The condition \(p^{2}q=( n/n)\) in Theorem 2 ensures that the expectation \(pq^{}^{}\) will dominate the noise matrix \(\) in the decomposition (6).

We first discuss how to bound \((^{0},^{})\). Notice that \(^{0}\) and \(^{}\) are the \(d\) leading eigenvectors of \(\) (after projection onto \((d)^{n}\)) and \(pq^{}^{}\), respectively. We can then use the matrix perturbation theory (see Lemma 3) to bound \((^{0},^{})\). Towards this end, we need to estimate the operator norm \(\|\|_{2}\), which could be done by applying the standard matrix Bernstein concentration inequality  since the blocks \(\{_{ij}\}\) are i.i.d. white noise with bounded operator norms and variances; see Lemma 2.

We next turn to bound the initialization error in the infinity norm, i.e., \(_{}(^{0},^{})\). Let us use \((^{0})_{m}^{d d}\) to denote the \(m\)-th block of \(^{0}^{nd d}\) for \(1 m n\). The main technical challenge lies in deriving a sharp bound for the term \(_{1 m n}\|(^{0})_{m}\|_{F}\), as it involves two _dependent_ random quantities, i.e., the noise matrix \(\) and the initial \(^{0}\) that is obtained by projecting the first \(d\) leading eigenvectors of \(\) onto \((d)^{n}\). To overcome such a statistical dependence, we utilize the leave-one-out technique. This technique was utilized in  to analyze the phase synchronization problem and was later applied to many other synchronization problems . Let us define

\[^{(m)}=pq^{}^{}+^{(m)} _{kl}^{(m)}=_{kl}_{\{k m\}}_{\{l m \}}.\] (7)

That is, we construct \(^{(m)}^{nd nd}\) by setting the \(m\)-th block-wise row and column of \(\) to be \(\). Then, it is easy to see that \(^{(m)}\) is statistically independent of \(_{m}^{}^{d nd}\), where the latter denotes the \(m\)-th block-wise row of \(\). Let \(^{(m)}\) be the \(d\) leading eigenvectors of \(^{(m)}\). Consequently, \(^{(m)}\) is also independent of \(_{m}^{}\). Based on the above discussions, we can bound each \(\|(^{0})_{m}\|_{F}\) in the following way:

\[\|(^{0})_{m}\|_{F}=\|_{m}^{}^{0}\|_{F}\| _{m}^{}^{(m)}\|_{F}+\|_{m}^{}(^{0}-^{(m)})\|_{ F}.\] (8)

The first term \(\|_{m}^{}^{(m)}\|_{F}\) can be bounded using an appropriate concentration inequality due to the statistical independence between \(_{m}^{}\) and \(^{(m)}\). The second term can be bounded as

\[\|_{m}^{}(^{0}-^{(m)})\|_{F}\|_{m}\|_{2} \|^{0}-^{(m)}\|_{F},\]

in which \(\|_{m}\|_{2}\) can be further bounded by matrix concentration inequality (see Lemma 2) and \(\|^{0}-^{(m)}\|_{F}\) can be bounded using standard matrix perturbation theory (see Lemma 4).

### Weak Sharpness and Exact Recovery

We next present a property that is intrinsic to problem (2) in the following theorem.

**Theorem 3** (weak sharpness).: _Suppose that the ratios \(p\) and \(q\) satisfy_

\[p^{2}q^{2}=().\]

_Then, with probability at least \(1-(1/n)\), for any \((d)^{n}\) satisfying \(_{}(,^{})=(p)\), we have_

\[f()-f(^{})\,_{1}(,^{ }).\]

Some remarks on Theorem 3 are in order. This theorem shows that problem (2) possesses the _weak sharpness_ property , which is intrinsic to the problem and independent of the algorithm used to solve it. It is known that with this property, various subgradient-type methods can achieve linear convergence . We will establish a similar linear convergence result for \(\) in the next subsection based on Theorem 3.

The weak sharpness property shown in Theorem 3 is of independent interest, as it could be helpful when analyzing other optimization algorithms (not just \(\)) for solving problem (2). Currently,only a few applications are known to produce sharp optimization problems, such as robust low-rank matrix recovery , robust phase retrieval , and robust subspace recovery . Furthermore, sharp instances of manifold optimization problems are especially scarce. Hence, Theorem 3 extends the list of optimization problems that possess the weak sharpness property and contributes to the growing literature on the geometry of structured nonsmooth nonconvex optimization problems.

It is worth noting that Theorem 3 also establishes the _exact recovery_ property of the formulation (2). Specifically, up to a global rotation, the ground-truth \(^{}\) is guaranteed to be the unique global minimizer of \(f\) over the region \((d)^{n}\{:_{}(,^{})= (p)\}\). Consequently, recovering the underlying \(^{}\) reduces to finding the global minimizer of \(f\) over the aforementioned region. As we will show in the next subsection, ReSync will converge linearly to the global minimizer \(^{}\) when initialized in this region. However, the initialization requirement is subject to the stronger condition \(p^{4}q=( n/n)\) on the ratios \(p\) and \(q\), which is ensured by Theorem 2.

We list our main ideas for proving Theorem 3 below. The full proof can be found in Appendix B.

**Proof outline of Theorem 3**. Note that the objective function \(f\) can be decomposed into two parts:

\[f()=}_{i}^{}_{ j}-_{i}^{}_{j}^{}_{F}}+^{c}}_{i}^{}_{j}-_{ij}_{F}}\,.\] (9)

It is easy to see that \(g(^{})=0\) and \(g() 0\). Based on the fact that the true observation is uniformly distributed in all the indices, we have \((g())=pq_{1 i,j n}_{i}^{ }_{j}-_{i}^{}_{j}^{}_{F}_{1}(,^{})\); see Appendix B.2 for the last inequality. A traditional way to lower bounding \(g()\) using \((g())\) for all \((d)\) is to apply concentration inequality and an epsilon-net covering argument. Unfortunately, the sample complexity condition \(p^{2}q^{2}=( n/n)\) does not lead to a high probability result in this way. Instead, our approach is to apply the concentration theory on the cardinalities of index sets rather than on \(\) directly; see the following lemma.

**Lemma 1** (concentration of cardinalities of index sets).: _Given any \(=(}{q})\), with probability at least \(1-(1/n)\), we have_

\[(1-)nq _{i}(1+)nq, (1-)npq_{i}(1+)npq,\] \[(1-)npq^{2} _{i}_{j}(1+ )npq^{2}, (1-)np^{2}q^{2}_{ij}(1+)np^{2 }q^{2}\]

_for any \(1 i,j n\). See Section 1 for the notation._

We then provide a sharp lower bound on \(g()\) based on Lemma 1.

**Proposition 1**.: _Under the conditions of Theorem 3, with probability at least \(1-(1/n)\), we have_

\[g()_{1}(,^{ }),(d)^{n}.\] (10)

Next, to lower bound \(h()-h(^{})=_{(i,j)^{c}}(_ {i}^{}_{j}-_{ij}_{F}-_{i}^{}_ {j}^{}-_{ij}_{F})\) we first bound

\[h()-h(^{})_{(i,j)^{c}} {_{i}^{}_{j}^{}-_{ij}}{_{i}^{} _{j}^{}-_{ij}_{F}},_{i}^{}_{j}-_ {i}^{}_{j}^{},\]

where the inequality comes from the convexity of the norm function \(_{F}\) whenever \(_{i}^{}_{j}^{}-_{ij}\). Then, using the orthogonality of each block of \(^{}\), we further have

\[h()-h(^{})_{(i,j)^{c}} {-_{i}^{}_{ij}_{j}^{}}{- _{i}^{}_{ij}_{j}^{}_{F}},_{i}^{ }_{i}^{}_{j}_{j}^{}-.\] (11)

Recall that since the outliers \(\{_{i,j}\}_{(i,j)^{c}}\) are independently and uniformly distributed on \((d)\), so are \(\{_{i}^{}_{ij}_{j}^{}\}_{(i,j)^{c}}\). This observation indicates that \(\{-_{i}^{}_{ij}_{j}^{}/ -_{i}^{}_{ij}_{j}^{}_{F}\}_{(i,j) ^{c}}\) are i.i.d. random matrices. Hence, by invoking concentration results that utilize the randomness of the outliers \(\{_{i,j}\}_{(i,j)^{c}}\) and the cardinalities \((i,j)^{c}\), we obtain the following result.

**Proposition 2**.: _Under the conditions of Theorem 3, with probability at least \(1-(1/n)\), we have_

\[h()-h(^{})-_{1}(,^{})\] (12)

_for all \((d)^{n}\) satisfying \(_{}(,^{})=(p)\)._

Combining Proposition 1 and Proposition 2 gives Theorem 3.

### Convergence Analysis and Proof of Theorem 1

Let us now turn to utilize the weak sharpness property shown in Theorem 3 to establish the local linear convergence of \(\). As a quick corollary of Theorem 3, we have the following result.

**Corollary 1**.: _Under the conditions of Theorem 3, with probability at least \(1-(1/n)\), for any \((d)^{n}\) satisfying \(_{}(,^{})=(p)\), we have_

\[_{}f(),^{}- -_{1}(,^{ }),\ _{}f()_{}f().\] (13)

This condition indicates that any Riemannian subgradient \(_{}f()\) provides a descent direction pointing towards \(^{}\). However, it only holds for \((d)^{n}\) satisfying \(_{}(,^{})=(p)\). Our key idea for establishing local convergence is to show that the Riemannian subgradient update in \(\) is a contraction operator _in both the Euclidean and infinity norm-induced distances_ using Corollary 1, i.e., if \(^{k}\) lies in the local region, then \(^{k+1}\) also lies in the region. This idea motivates us to define two sequences of neighborhoods as follows:

\[_{F}^{k}=\{(,^{}) _{k}\}_{}^{k}=\{ _{}(,^{})_{k}\}\,.\] (14)

Here, \(_{k}=_{0}^{k},_{k}=_{0}^{k}\), where \(_{0}\), \(_{0}\), and \((0,1)\) will be specified later. Thus, these two sequences of sets \(\{_{F}^{k}\}\) and \(\{_{}^{k}\}\) will linearly shrink to the ground-truth. It remains to show that if \(^{k}_{F}^{k}_{}^{k}\), then \(^{k+1}_{F}^{k+1}_{}^{k+1}\), which is summarized in the following theorem.

**Theorem 4** (convergence analysis).: _Suppose that \(_{0}=(p^{2})\) and \(_{0}=(_{0})\). Set \(=1-\) and \(_{k}=_{k}/n\) in \(\). If \(^{k}_{F}^{k}_{}^{k}\) for any \(k 0\), then with probability at least \(1-(1/n)\), we have_

\[^{k+1}_{F}^{k+1}_{}^{k+1}.\]

**Proof outline of Theorem 4.** The proof consisted of two parts. On the one hand, we need to show that \(^{k+1}_{F}^{k+1}\), which can be achieved by applying Corollary 1. On the other hand, in order to show that \(^{k+1}_{}^{k+1}\), we need a good estimate of each block of \(_{}f()\). See Appendix C.

Having developed the necessary tools, we are now ready to prove Theorem 1.

**Proof of Theorem 1.** Based on Theorem 2, we know that \(^{0}_{F}^{0}_{}^{0}\) if \(_{0}\) and \(_{0}\) satisfy

\[_{0}=(}{p})_{0}=(}{p}).\] (15)

According to Theorem 4, by choosing \(_{0}=(p^{2})\) and \(_{0}=(q})\), condition (15) holds when \(p^{7}q^{2}=( n/n)\). This completes the proof of Theorem 1.

## 4 Experiments

In this section, we conduct experiments on \(\) for solving the \(\) problem on both synthetic and real data, providing empirical support for our theoretical findings. Our experiments are conducted on a personal computer with a 2.90GHz 8-core CPU and 32GB memory. All our experiment results are averaged over 20 independent trials. Our code is available at https://github.com/Huikang2019/\(\).

### Synthetic Data

We consider the rotation group \((3)\) in all our experiments. We generate \(_{1}^{},,_{n}^{}\) by first generating matrices of the same dimension with i.i.d. standard Gaussian entries and then projectingeach of them onto \((3)\). The underlying graph, outliers, and relative rotations in the measurement model (1) are generated according to the \(\) as described in Section 2.2. In our experiments, we also consider the case where the true observations are contaminated by additive noise, namely, \(\{_{i,j}\}_{(i,j)}\) in (1) is generated using the formula

\[_{i,j}=_{(3)}(_{i}^{}_{j}^ {}+_{i,j})(i,j),\] (16)

where \(_{i,j}\) consists of i.i.d. entries following the standard Gaussian distribution and \( 0\) controls the variance level of the noise.

**Convergence verification of ReSync.** We evaluate the convergence performance of \(\) with the noise level \(=0\) in (16). We set \(p=q=( n/n)^{1/3}\) in the measurement model (1), which satisfies \(p^{2}q= n/n\). We use the initial step size \(_{0}=1/npq\) and the decaying factor \(\{0.7,0.8,0.85,0.90,0.95,0.98\}\) in \(\). We test the performance for various \(n\) selected from \(\{400,600,800,1000\}\). Figure 2 displays the experiment results. It can be observed that (i) \(\) converges linearly to ground-truth rotations for a wide range of \(\) and (ii) a smaller \(\) often leads to faster convergence speed. These corroborate our theoretical findings. However, it is worth noting that excessively small \(\) values may result in an early stopping phenomenon (e.g., \( 0.8\) when \(n=400\)). In addition, \(\) performs better with a larger \(n\), as it allows for a smaller \(\) (e.g., \(=0.7\) when \(n=1000\)) and hence converges to the ground-truth rotations faster.

**Comparison with the state-of-the-arts.** We next compare \(\) with state-of-the-art synchronization algorithms, including IRLS_L12 , MPLS , CEMP_GCW , DESC , and LUD . We obtain the implementation of the first four algorithms from https://github.com/ColeWyeth/DESC, while LUD's implementation is obtained through private communication with its authors. In our comparisons, we use their default parameter settings. For \(\), we set the initial step size to \(_{0}=1/npq\) and the decaying factor to \(=0.95\), as suggested by the previous experiment. We fix \(n=200\) and vary the true observation ratio \(p\) (or the observation ratio \(q\)) while keeping \(q=0.2\) (or \(p=0.2\)) fixed. We display the experiment results for \(=0\) and \(=1\) in Figures 2(a) and 2(b), respectively, where \(p\) is selected from \(\{0.2,0.3,0.4,,1\}\). When \(=0\), \(\) achieves competitive performance compared to other robust synchronization algorithms. When the additive noise level is \(=1\), \(\) outperforms other algorithms. In Figures 2(c) and 2(d), we present the results with varying \(q\) chosen from \(\{0.2,0.3,0.4,,1\}\) for noise-free (\(=0\)) and noisy (\(=1\)) cases, respectively. In the noise-free case, DESC performs best when \(q<0.5\), while \(\) slightly outperforms others when \(q 0.5\). In the noisy case, it is clear that \(\) achieves the best performance for a large range of \(q\).

Figure 3: Comparison with state-of-the-art synchronization algorithms.

### Real Data

We consider the global alignment problem of three-dimensional scans from the Lucy dataset, which is a down-sampled version of the dataset containing 368 scans with a total number of 3.5 million triangles. We refer to  for more details about the experiment setting. We apply three algorithms LUD , DESC  and our ReSync on this dataset since they have the best performance on noisy synthetic data. As Figure 4 shows, ReSync outperforms the other two methods.

## 5 Conclusion and Discussions on Limitations

In this work, we introduced ReSync, a Riemannian subgradient-based algorithm with spectral initialization for solving \(\). We established strong theoretical results for \(\) under the \(\). In particular, we first presented an initialization guarantee for \(\), which is a procedure embedded in \(\) for initialization. Then, we established a problem-intrinsic property called weak sharpness for our nonsmooth nonconvex formulation, which is of independent interest. Based on the established weak sharpness property, we derived linear convergence of \(\) to the underlying rotations once it is initialized in a local region. Combining these theoretical results demonstrates that \(\) converges linearly to the ground-truth rotations under the \(\).

**Limitations.** Our overall guarantee in Theorem 1 requires the sample complexity of \(p^{7}q^{2}=( n/n)\), which does not match the currently best known lower bound \(p^{2}q=( n/n)\) for exact recovery . We showed in Theorem 2 that approximate recovery with an optimal sample complexity is possible. Moreover, we showed in Theorem 3 that exact recovery with \(p^{2}q^{2}=( n/n)\) is possible if we have a global minimizer of the objective function of problem (2) within a certain local region. However, due to the nonconvexity of problem (2), it is non-trivial to obtain the said minimizer. We circumvented this difficulty by establishing the linear convergence of \(\) to a desired minimizer in Theorem 4. Nevertheless, a strong requirement on initialization is needed, which translates to the weaker final complexity result of \(p^{7}q^{2}=( n/n)\).

Although our theory allows for \(p 0\) as \(n\), our argument relies heavily on the randomness of the outliers \(\{_{i,j}\}\) and the absence of additive noise. In practice, adversarial outliers that arbitrarily corrupt a measurement and additive noise contamination are prevalent. It remains unknown how well \(\) performs in such scenarios.

The above challenges are significant areas for future research and improvements.

Figure 4: Histogram of the unsquared residuals of LUD, DESC, and ReSync for the Lucy dataset.