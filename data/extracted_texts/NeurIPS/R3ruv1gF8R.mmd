# The Reliability of OKRidge Method in Solving Sparse Ridge Regression Problems

Xiyuan Li Youjun Wang Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Lee_xiyuan@outlook.com, youjunw1208@gmail.com, liuweiwei863@gmail.com

Corresponding author: Weiwei Liu (liuweiwei863@gmail.com).

###### Abstract

Sparse ridge regression problems play a significant role across various domains. To solve sparse ridge regression,  recently proposes an advanced algorithm, Scalable Optimal \(K\)-Sparse Ridge Regression (OKRidge), which is both faster and more accurate than existing approaches. However, the absence of theoretical analysis on the error of OKRidge impedes its large-scale applications. In this paper, we reframe the estimation error of OKRidge as a Primary Optimization (**PO**) problem and employ the Convex Gaussian min-max theorem (CGMT) to simplify the **PO** problem into an Auxiliary Optimization (**AO**) problem. Subsequently, we provide a theoretical error analysis for OKRidge based on the **AO** problem. This error analysis improves the theoretical reliability of OKRidge. We also conduct experiments to verify our theorems and the results are in excellent agreement with our theoretical findings.

## 1 Introduction

Sparse Ridge Regression (SRR) has achieved notable success across various machine learning applications, including statistics , signal processing , dynamical systems , and others. In this paper, we are interested in addressing the following \(k\)-sparse linear regression problem with additive noise:

\[=^{*}+\;\;\;\;\|^{*} \|_{0} k,\] (1)

where \(^{*}^{d}\) represents the "true" weight parameter, \(=(_{1},_{2},,_{n})^{}^{n d}\) is the input measurement matrix, \(=(y_{1},y_{2},,y_{n})^{}^{n}\) is the real output responses, \(=(_{1},_{2},,_{n})^{} ^{n}\) is the noise vector, \(k^{+}\) specifies the maximum number of nonzero elements for the model, \(\|\|_{0}\) denotes the number of nonzero elements of the given vector. Moreover, the entries of \(\) are drawn i.i.d. from \((0,1)\); the entries of \(\) are drawn i.i.d. from \((0,^{2})\); and we assume \(\) is a constant and \(_{d}=(0,1)\).

The formulation (1) represents a black box model where \(^{*}\) is fixed. Given \(\) and \(\), to determine the target vector \(^{*}\), the most basic method is solving the following \(k\)-Sparse Ridge Regression Optimization (\(k\)-SRO), as outlined by :

\[_{}\|-\|_{2}^{2}+\|\|_{ 2}^{2}\|\|_{0} k,\] (2)

where \(>0\) is a regularizer parameter, and \(\|\|_{2}\) denotes the Euclidean norm. Our paper focuses on the worst-case scenario \(\|^{*}\|_{0}=k\). This \(k\)-SRO is different from the traditional ridge regressiondue to the constraint of \(k\)-sparse structure for \(\). The \(k\)-SRO problem (2) is NP-hard, and is more challenging in the presence of highly correlated features .

Two main types of algorithms are commonly employed for solving \(k\)-SRO problem (2): heuristic algorithms [9; 10] and optimal algorithms . However, heuristic algorithms lack the ability to assess the solution quality, while the optimal algorithms are slow. In order to rapidly solve \(k\)-SRO problem (2) while ensuring solution optimality,  introduces a highly efficient method called OKRidge. Therefore, a complete algorithm of OKRidge, including how to choose hyper-parameters can be seen in the original paper . OKRidge substitutes \(k\)-SRO problem (2) with an unconstrained optimization on a novel tight lower bound. The experiment results in  show that OKRidge is superior to heuristic algorithms, optimal algorithms, and existing mixed-integer programming (MIP) formulations solved by the commercial solver Gurobi. Nevertheless, the absence of theoretical error analysis for OKRidge impedes its scalability in practical applications.

In this paper, we provide theoretical error analysis for OKRidge utilizing the framework of the CGMT . Specifically, we propose another novel tight lower bound \(_{}()\) to replace \(k\)-SRO problem (2):

\[_{}():=\|-\|_{2}^{2}+ _{k}(),\] (3)

where \(\) denotes Hadamard product, and \(_{k}()\) represents the summation of the largest \(k\) elements of a given vector. The tight lower bound (3) is equivalent to that proposed by . Thus, \(_{}()\) can replace the objective function of OKRidge. It is noteworthy that our proposed regularizer, defined as \(()=_{k}()\), differs from any previously proposed instances by . Then, the optimal solution obtained by OKRidge is

\[}=*{arg\,min}_{}_{}().\] (4)

 utilizes \(}\) as the estimate of \(^{*}\) in problem (1). By combining formulations (1) and (3), the estimation error of OKRidge can be obtained through the following normalized optimization problem:

\[_{}}\|-\|_{2}^{2}+ _{k}(+^{*})(+ ^{*}),\] (5)

where \(:=-^{*}\) is a random variable with randomness from the random variables \(\) and \(\), and the estimation error can be measured by \(\|\|_{2}\). Subsequently, we transform the optimization (5) into a **PO** problem about the error of OKRidge, using the Fenchel-Moreau theorem . Then, we employ the CGMT framework to substitute the complex **PO** problem with a simplified **AO** problem. Finally, we present the theoretical error analysis of OKRidge based on the **AO** problem. Our theoretical results focus on the Normalized Squared Error (NSE) of OKRidge and can be summarized as:

\[_{d}_{ 0} (),\] (6)

where \(:=\|}-^{*}\|_{2}^{2}/^{2}\), and \(()\) is a function of \(\). These theoretical results indicate that if the regularizer parameter \(\) used in OKRidge is constant, the NSE limit of OKRidge is also fixed. Moreover, \(}\) learned by OKRidge is reliable to estimate \(^{*}\), due to \(_{d}_{ 0}\|}-^{*}\|_{2} 0\). The comprehensive experiments of OKRidge on real-world examples were conducted by the NeurIPS 2023 paper  (see Figure 3 and Appendix H in ), which demonstrates that the error of OKRidge tends to zero. Our analysis explains the experimental phenomenon observed in , strengthens the theoretical underpinnings of OKRidge, and provides theoretical reliability for its broad application.

We also conduct numerical experiments to validate our theorems. The findings demonstrate that the NSE converges to a fixed constant determined by \(\), aligning excellently with our theoretical predictions.

### Outline

The structure of the remaining sections in this paper is as follows. Section 2 provides a review of related work. Section 3 offers background information on OKRidge, CGMT, and basic concepts. Section 4 introduces an alternative tight lower bound for the objective function of OKRidge. In Section 5, we convert the estimation error of OKRidge into a **PO** problem and simplify it into an **AO** problem using CGMT. Subsequently, an estimation error analysis of OKRidge based on the **AO** problem is conducted. Section 6 presents the experimental results. Finally, we conclude with a summary in Section 7. Additionally, the limitaion and impact of our work are detailed in Appendix A Related work

### Heuristic and Optimal Methods

Heuristic methods approximate solutions to optimization problems based on practical experience , including ensemble methods , swapping features , greedy methods , etc. While heuristic methods are fast, they often become trapped in local minima, and their solution quality cannot be assessed due to the absence of a lower bound on performance. Optimal methods aim to precisely solve sparse regression problems, such as the big-M method , the conditional-value-at-risk (CVaR) approach , big-M free mixed integer second order conic (MISOC) method , and so on. However, exact optimal methods are slow, particularly for large instances, to achieve near-optimality [20; 21]. To address the limitations of heuristic and optimal methods,  proposes an efficient approximation algorithm, OKRidge. Experimental results in  demonstrate that OKRidge outperforms heuristic algorithms, optimal algorithms, and existing MIP formulations solved by the commercial solver Gurobi.

### Lower Bound Methods

Lower bound methods are capable of solving the NP-hard \(k\)-SRO problems. Several algorithms utilize the lower bound method, such as SOS1 formulation , big-M formulation , Subset Selection CIO method , and others. However, the SOS1 formulation lacks scalability in high dimensions, the big-M formulation is sensitive to hyperparameters, and the Subset Selection CIO method runs slowly. Recently, the perspective formulation [6; 23; 24] has been employed to induce a convex relaxed lower bound that is easier to solve. Building upon the perspective formulation,  proposes a novel lower bound used as the objective function for the OKRidge method.

### Normalized Squared Error

NSE, defined as \(\|}-^{*}\|_{2}^{2}/^{2}\), serves as a natural measure of the estimation error. NSE is an important indicator in signal-to-noise ratio scenes [25; 26]. Bounds on NSE have been derived by [27; 28]. Additionally,  is the first to precisely formulate the limiting behavior of NSE. These studies primarily consider a Gaussian sensing matrix \(\) and utilize the Approximate Message Passing (AMP) framework for analysis [30; 31]. These achievements motivate us to utilize NSE for evaluating the estimation error of the OKRidge method.

## 3 Preliminary

### Relaxed Transformation of \(k\)-Sro

According to , the \(k\)-SRO problem (2) can be reformulated as the following optimization problem:

\[_{}_{}(), \ \{(1-z_{j})_{j}=0,\ j=1,2,,d,\\ _{j=1}^{d}z_{j} k,\ z_{j}\{0,1\},.\] (7)

where \(=(z_{1},z_{2},,z_{n})^{}^{d}\), and \(_{}():=\|-\|_{2}^{2}+_{j=1}^{d}_{j}^{2}\). This problem (7) remains NP-hard under the sparsity constraint . Existing methods such as SOS1, big-M, or the perspective formulation do not leverage the \(k\)-sparse structure of the problem.  develops a novel method, OKRidge, to preserve the special structure through the following relaxed transformation.

By employing the perspective formulation [33; 34] and the Fenchel conjugate ,  transforms the problem (7) to a new perspective optimization problem:

\[_{,}_{}_{ }^{}(,,),\ _{j=1}^{d}z_{j} k,\ z_{j}\{0,1\}.\] (8)

where \(=(c_{1},c_{2},,c_{n})^{d}\), and \(_{}^{}(,, ):=\|-\|_{2}^{2}+ _{j=1}^{d}(_{j}c_{j}-^{2}}{4}z_{j})\). This transformation does not change the optimal solution of the problem (7) [1; 35], indicating that problem (7) can be replaced by problem (8). To efficiently solve problem (8),  further relaxsthe binary constraint \(\{0,1\}\) to the interval \(\), ultimately yielding the following relaxed convex optimization problem:

\[_{,}_{}_{}^{}(,,),\ _{j=1}^{d}z_{j} k,\ z_{j}.\] (9)

According to problem (7), to preserve the special sparse structure of \(\), we always have \(_{j}=0\) if \(z_{j}=0\). Directly solving the min-max problem (9) is computationally challenging.  utilizes the relaxed problem (9) to obtain a tight lower bound for the problem (8), where the lower bound corresponds to the objective function of OKRidge.

### The Convex Gaussian Min-max Theorem

The CGMT framework, introduced by , has been utilized to analyze the performance of solutions to non-smooth regularized convex optimization problems. It has achieved significant success in various practical applications, including regularized logistic regression , max-margin classifiers , adversarial training [38; 39] and others. These achievements inspire us to apply the CGMT framework to analyze the NSE of the OKRidge method.

CGMT originates from Gordon's Gaussian Min-max Theorem (GMT) , which provides probabilistic bounds on the optimal cost of **PO** problem via a simpler **AO** problem. CGMT further tightens the bounds under convexity assumptions. According to GMT,  introduces the following asymptotic sequence and notation.

**Definition 3.1** (GMT admissible sequence).: The sequence \(\{^{(d)},^{(d)},^{(d)},_{}^{(d)}, _{}^{(d)},^{(d)}\}_{d}\) indexed by \(d\), with \(^{(d)}^{n d}\), \(^{(d)}^{n}\), \(^{(d)}^{d}\), \(_{}^{(d)}^{d}\), \(_{}^{(d)}^{n}\), \(^{(d)}:_{}^{(d)}_{}^{(d)} \) and \(n=n(d)\), is said to be admissible if, for each \(d\), \(_{}^{(d)}\) and \(_{}^{(d)}\) are compact sets and \(^{(d)}\) is continuous on its domain. Onwards, we will drop the superscript \((d)\) from \(^{(d)}\), \(^{(d)}\), \(^{(d)}\).

A sequence \(\{^{(d)},^{(d)},^{(d)},_{}^{(d)}, _{}^{(d)},^{(d)}\}_{d}\) defines a sequence of min-max problems

\[^{(d)}():=_{_{}^{(d)}}_{ _{}^{(d)}}^{}+^{(d)}(,),\] (10)

\[^{(d)}(,):=_{_{}^{(d)}}_{ {u}_{}^{(d)}}\|\|_{2}^{}+\|\|_ {2}^{}+^{(d)}(,).\] (11)

Importantly, the formulation (10) is called Primary Optimization (**PO**) and the formulation (11) is called Auxiliary Optimization (**AO**). Additionally, let \(_{}^{(d)}()\) denote the optimal minimizer of **PO** problem (10), and \(_{}^{(d)}(,)\) denote the optimal minimizer of **AO** problem (11). Define \(^{(d)}:_{}^{(d)}\) as follows,

\[^{(d)}(;,):=_{_{}^{(d )}}\|\|_{2}^{}+\|\|_{2}^{}+^{( d)}(,).\] (12)

Clearly, \(^{(d)}(,):=_{_{}^{(d)}} ^{(d)}(;,)\). For a sequence of random variables \(\{^{(d)}\}_{d}\) and a constant \(c\), \(^{(d)}\) denotes convergence in probability, i.e. \(>0,_{d}| ^{(d)}-c|>=0\). Based on the GMT admissible sequence and the notation introduced above, we present the CGMT below.

**Theorem 3.2** (CGMT ).: _Let \(\{^{(d)},^{(d)},^{(d)},_{}^{(d)}, _{}^{(d)},^{(d)}\}_{d}\) be a GMT admissible sequence as in Definition 3.1, for which additionally the entries of \(\), \(\), \(\) are drawn i.i.d. from \((0,1)\). Let \(^{(d)}()\), \(^{(d)}(,)\) be the optimal costs, and, \(_{}^{(d)}()\), \(_{}^{(d)}(,)\) the corresponding optimal minimizers of the **PO** and **AO** problems in (10) and (11). The following three statements hold_

1. _For any_ \(d\) _and_ \(c\)_,_ \[^{(d)}()<c 2^{(d)}( ,) c.\]_._
2. _For any_ \(d\)_. If_ \(_{}^{(d)}\)_,_ \(_{}^{(d)}\) _are convex, and,_ \(^{(d)}(,)\) _is convex-concave on_ \(_{}^{(d)}_{}^{(d)}\)_, then, for any_ \(\) _and_ \(t>0\)_,_
3. _Assume the conditions of (ii) hold for all_ \(d\)_. Let_ \(\|\|\) _denote some norm in_ \(^{d}\) _and recall (_12_). If, there exist constants (independent of_ \(d\)_)_ \(^{*}\)_,_ \(^{*}\) _and_ \(>0\) _such that_ 1. \(^{(d)}(,)}{{}}^ {*}\)_,_ 2. \(\|_{}^{(d)}(,)\|}{{ }}^{*}\)_,_ 3. _with probability one in the limit_ \(d\)__ \[^{(d)}(;,)^{(d)}(,)+ \|\|-_{}^{(d)}(,)^{2}, _{}^{(d)}},\] _then,_ \[\|_{}^{(d)}()\|}{{ }}^{*}.\] (13)

Theorem 3.2 indicates that, if the optimal cost \((,)\) of (11) concentrates to some value \(\), the same holds true for \(()\) of (10). Furthermore, under appropriate additional assumptions, the optimal solutions of the **AO** and **PO** problems are also closely related by \(\|_{}()\|=\|_{}(,)\|\), as \(n\). This suggests that, within the CGMT framework, a challenging **PO** problem can be replaced with a simplified **AO** problem, from which the optimal solution of the **PO** problem can be accurately inferred . Subsequently, we rewrite the lower bound of problem (9) in the form of **PO** problem (10) and analyze the minimizer of the simplified **AO** problem instead.

### Basic Concept

Suppose \(f:^{d}\) and \(,^{d}\), the Fenchel conjugate of \(f\) is defined as \(f^{*}()=_{}^{}-f()\). Additionally, \(f^{*}\) is always convex and lower semi-continuous. According to the Fenchel-Moreau theorem , if \(f\) is convex and continuous, we have \(f()=_{}^{}-f^{*}()\). In this paper, we utilize the following conjugate pairs

\[f()=\|\|_{2}^{2} f^{*}()=\|_{2 }^{2}}{4}.\] (14)

If \(():^{d}\) is a convex function of \(\), the subdifferential of \(()\) at \(^{*}\) is the set of vectors: \((^{*})=\{^{d}(^{*}+)(^{*})+^{}\}\). According to , \((^{*})\) is nonempty, convex and compact. Given \(^{d}\), we define \((,(^{*}))=_{ (^{*})}\|-\|_{2}\). Then, the Gaussian squared distance corresponding to the scaled subdifferential is defined as \(D():=D_{(^{*})}():=_{} ^{2}(,(^{*})),\) where \(>0\). Suppose \(C()=-\), \(_{d}(0,1)\), \(_{d}()(0,1)\), \(_{d}()\). Based on the Gaussian squared distance, we define a \(map\) function:

\[():=()-()}{()}}, >0.\] (15)

We denote \(_{map}\) as the solution of \(()-/2=0\). Since \(map()\) depends on \(()\) and \(^{*}\), when the form of \(()\) and the value of \(^{*}\) are determined, the \(_{map}\) is fixed.

## 4 Tight Lower Bound in OKRidge

In this section, we utilize problem (9) to derive another novel lower bound for problem (8), serving as used as the objective function of OKRidge. Our lower bound is equivalent to the tight lower bound provided by . Specifically,  eliminates the parameter \(\) in problem (9) by setting the gradient of \(\) to \(\), while we eliminate the parameter \(\) by setting the gradient of \(\) to \(\). These two methodsare equivalent due to the independence and convexity of \(\) and \(\). Given any \(\) and \(\), the optimality condition for \(\) in problem (9) is taking \(_{}^{}(,,)/ =\). Therefore, we have

\[_{}^{}(,,)}{}=-() {c}}{2}=,\] (16) \[ c_{j}=\{&, z_{j}=0,\\ }{z_{j}}&,z_{j} 0,.\] (17)

where \(()\) is a diagonal matrix with \(\) on the diagonal. Inspired by this optimality condition, we present the following theorem.

**Theorem 4.1**.: _If we define the parameter \(\) as (16), the problem (9) is equivalent to the following optimization problem:_

\[_{,}_{}^{}(,),_{j=1}^{d}z_{j} k,\ z_{j},\] (18)

_where_

\[_{}^{}(,):=\|- {X}\|_{2}^{2}+_{j=1,z_{j} 0}^{d}^{2}}{z_{j}}.\] (19)

The proof of Theorem 4.1 follows Theorem 3.1 of  and is included in Appendix B for completeness. Following the approach by , we can approximately solve the problem (18) while still obtaining a feasible lower bound. We define a new function \(()\) as:

\[()=_{}_{}^{}(,),_{j=1}^{d}z_{j} k,\ z_{j}.\] (20)

For any \(\), \(()\) serves as a valid lower bound for problem (7). We should choose \(\) such that this lower bound \(()\) is tight.

**Theorem 4.2**.: _The function \(()\) defined in Equation (20) is lower bounded by_

\[()\|-\|_{2}^{2}+_{k}().\] (21)

_where \(\) is Hadamard product, and \(_{k}()\) denotes the summation of the largest \(k\) elements of a given vector._

The proof of Theorem 4.2 can be seen in Appendix C. Based on (9), (18) and (20), the tight lower bound (21) is equivalent to the one provided by , as both are derived through equivalent processes. OKRidge solves the original \(k\)-sparse problem (7) using this tight lower bound (21) as its objective function. If we define

\[_{}():=\|-\|_{2}^{2} +_{k}(),\]

OKRidge solves \(k\)-SRO problem (2) with

\[_{}_{}(),\] (22)

where we obtain \(_{}\) of formulation (3). So far, we transform the constrained \(k\)-SRO problem (2) into the unconstrained optimization problem (22). Let

\[}=_{}\ _{}(),\]

OKRidge regards \(}\) as the estimation of \(^{*}\) in problem (1). Next, we apply CGMT to analyze the error \(\|}-^{*}\|_{2}^{2}\) for OKRidge.

The Error Analysis for OKRidge

### From PO to AO

As discussed in Section 4, the estimation error of OKRidge is characterized by \(\|}-^{*}\|_{2}^{2}\). Taking formulation (1) into the properly normalized objective (22), OKRidge (22) can be equivalently transformed to the following optimization:

\[_{}}\|(-^{*})+ \|_{2}^{2}+_{k}() .\] (23)

The crucial step is to convert (23) into a **PO** problem within the framework of CGMT. We introduce the new variable \(:=-^{*}\) and apply the Fenchel-Moreau theorem (14) to formulation (23),

\[}\|-\|_{2}^{2}+ _{k}(+^{*})(+ ^{*})\] \[= _{}}^{}- ^{}-\|_{2}^{2}}{4}+_ {k}(+^{*})(+^{*}),\] (24)

where \(^{d},^{n}\). Based on (10) and (24), the **PO** problem corresponding to the estimation error of OKRidge is

\[_{}()=_{}_{}} ^{}+(,),\] (25)

where

\[(,):=-^{}-\|_{2}^{2}}{4} +_{k}(+^{*})(+^{*}).\] (26)

Since the entries of \(\) are drawn i.i.d. from \((0,1)\), to replace the challenging **PO** problem (25) with a simplified **AO** problem through CGMT, \((,)\) should be a convex-concave function. The following Lemma illustrates that the \((,)\) satisfies the conditions of Theorem 3.2.

**Lemma 5.1**.: _Suppose \((,)\) is defined as in formulation (26). Then, \((,)\) is convex-concave function._

The proof of Lemma 5.1 can be seen in Appendix D. Define

\[():=_{k}().\]

Because the **PO** problem (25) satisfies the assumptions of CGMT, we transform it to the following **AO** problem:

\[_{}(,) =_{}_{}}\|\|_{ 2}^{}+\|\|_{2}^{}-^{} -\|_{2}^{2}}{4}+(^{*}+)\] \[=_{}_{}}(\|\|_{ 2}-)^{}+\|\|_{2}^{}-\|_{2}^{2}}{4}+(^{*}+),\] (27)

where the entries \(\), \(\) are drawn i.i.d. from \((0,1)\), due to the property of \(\). Suppose \(_{_{}}\) is the of optimal solutions of the **PO** problem (25), and \(_{_{}}\) is the optimal solutions of the **AO** problem (27). According to Theorem 3.2, if \(\|_{_{}}\|_{2}^{*}\), we have \(\|_{_{}}\|_{2}^{*}\). Thus, we can analyze the minimizer of **AO** problem (27) instead of **PO** problem (25).

### Simplification for AO

In this chapter, we simplify the **AO** problem (27) into ones involving only scalar quantities. Since \(()\) is a convex (see Lemma 5.1), \((^{*})\) is nonempty, convex and compact. According to [13, Theorem 23.4], we have \((^{*}+)=(^{*})+_{ (^{*})}^{}+O(\|\|_{2}^{2})\). The first-order approximation of \(()\) around the vector of interest \(^{*}\) is

\[(^{*}+):=(^{*})+_{ (^{*})}^{}.\] (28)where \(=^{*}+\). Then, following the approach from , the **AO** problem (27) can be simplified by the first-order approximation (28):

\[_{}(,) =_{}_{}}(\|\|_{ 2}-)^{}+\|\|_{2}^{}+ (^{*})+_{(^{* })}^{}-\|_{2}^{2}}{4}\] \[=_{}_{\|\|_{2} 0\\ (^{*})} }(\|\|_{2}-)^{}+(\|\|_{2}+)^{}+(^{*})-\|_{2}^ {2}}{4}.\] (29)

Suppose \(f()\) and \(()\) denote the objective functions of the original and the approximated **AO** problems (27) and (29), respectively,

\[f()= (\|\|_{2}-)^{}+\|\|_{2} ^{}-\|_{2}^{2}}{4}+(^{*} +),\] \[()= (\|\|_{2}-)^{}+\|\|_{2} ^{}-\|_{2}^{2}}{4}+(^{*})+_{(^{*})}^{}.\]

Then, based on (28), we have

\[_{\|-^{*}\|_{2} 0}()=f().\] (30)

Compared with **AO** problem (27), the approximated **AO** problem (29) is tight when \(\|-^{*}\|_{2} 0\), and we later demonstrate that this condition is satisfied as \(^{2} 0\), independent of the original **AO** problem (27). This fact allows us to translate the analysis on the optimal solution \(_{_{}}\) of the approximated **AO** problem (29) to the analysis on the optimal solution \(_{_{}}\) of the corresponding original **AO** problem (27). Because \((^{*})\) is a constant, the approximated **AO** problem (29) is equivalent to the following optimization problem:

\[_{}_{\|\|_{2} 0\\ (^{*})}}(\|\|_{2}-)^{}+(\|\|_{2} +)^{}-\|_{2}^{2}}{4},\] (31)

where we have approximated \(\) in the first order. Since \((0,^{2})\), the term \(\|\|_{2}-\) above is statistically identical to a random vector with entries drawn i.i.d. from \((0,\|\|_{2}^{2}+^{2})\), where \(\) is the unit matrix. Following the method used by , we substitute the first term in the objective (31) with \(\|_{2}^{2}+^{2}^{}}\). Then, we obtain:

\[_{}_{\|\|_{2} 0\\ (^{*})} }\|_{2}^{2}+^{2}}^{}+(\|\|_{2}+)^{}-\|_{2}^{2}}{4}.\] (32)

Let \(=\|\|_{2}\). Since \(_{\|\|_{2}=}^{}=\|\|_{2}\|\|_{2}\) and \((0,)\), in term of mathematical expectation, the optimization (32) can be equivalently expressed as:

\[_{}_{ 0\\ (^{*})} }\|_{2}^{2}+^{2}}\|\|_{2}- {h}-^{}-}{4}.\] (33)

The objective (33) is strongly convex in \(\) and (jointly) concave in \(\), \(\), and the constraint sets are bounded. Therefore, we can reverse the order of min-max in problem (33) based on [13, Corollary 37.3.2]. Let \(=\|\|_{2}\). Since \(_{\|\|_{2}=}(-+)^{}=- \|-\|_{2},\) the optimization (33) can be equivalently reformulated as:

\[_{ 0\\ (^{*})}_{ 0} }+^{2}}\|\|_{2}- \|-\|_{2}-}{4}.\] (34)

Next, we further reverse the order of min-max, as the objective (34) exhibits the desired concave-convex structure. Then, we proceed to maximize over \((^{*})\). Since \(_{(^{*})}\|- \|_{2}=,(^{*}) ,\) the optimization problem (34) can alternatively be formulated as:

\[_{ 0}_{ 0}}+ ^{2}}\|\|_{2}-,-}{4}.\] (35)Because both the random components \(\|\|_{2}\) and dist(\(,\)) are Lipschitz, \(\|\|_{2}\) concentrates around \(\) and dist(\(,\)) around \()}\)[43, Lemma B.2]. Suppose, as \(d,()(0,1)\), \(()\), and \(():=_{n}}{4}\). Then, the optimal minimizer of (35) converges to the optimal minimizer of the following deterministic optimization in probability :

\[_{ 0}_{ 0}+^{2}}- ()}-().\] (36)

Here, we complete the simplifications by reducing the **AO** problem (27) to an equivalent optimization (36) that now only involves two scalar variables: \(\) and \(\).

### Error Analysis

Based on the analysis above, if the optimal solution of optimization (36) is \(=^{*}\), we have \(\|_{}\|_{2}^{*}\) for approximated **AO** problem (29). If \(^{*}\) further tends to \(0\), according to formulation (30) and CGMT, \(\|_{}\|_{2}^{*}\) holds for **PO** problem (25). Then, for the estimation error of OKRidge produced by (22), we have \(\|}-^{*}\|_{2}^{*}\). Therefore, it only remains to obtain the optimal value of \(\) in optimization (36) that plays the role of \(\|\|_{2}\). Following , we conclude the estimation error of OKRidge with Theorem 5.2 below.

**Theorem 5.2**.: _Suppose \(^{*}\) is the true weight parameter of the problem (1), \(}\) is the optimal solution to the objective function (22) of OKRidge, \(()(0,1)\), aNSE \(:=_{^{2} 0}=_{^{2} 0}\|}-^{*}\|_{2}^{2}/^{2}\). Define \(_{map}\) is the solution of map\(()=0\) for \(>0\), then, the estimation error of OKRidge is given by the following probability limit:_

\[_{d 0}(),\] (37)

_where \(()=()}{1-D()}\), and \(=_{map}\)._

The proof of Theorem 5.2 can be seen in Appendix E.

_Remark 5.3_.: In the objective (24) concerning estimation error of OKRidge, \(()=_{k}()\) and the value of \(^{*}\) is assumed to be known. Then, the analysis on \(map()\) in Section 3.3 reveals that the form of \(map()\) and the value of \(_{map}\) are fixed. Thus, \(()\) is a function of \(\). In other words, if the regularizer parameter \(\) of OKRidge is fixed, the NSE limit of OKRidge \(()\) is also fixed. Additionally, Theorem 5.2 also indicates that \(_{d}_{ 0}\|}-^{*}\|_{2} 0\), which guarantees the effectiveness of \(}\) learned by OKRidge in accurately estimating \(^{*}\). These results substantiate the theoretical reliability of OKRidge and promote its broad application in the real world.

## 6 Numerical Experiments

In this section, we conduct experiments to verify Theorem 5.2. The experiments contain two aspects: (i) When \(\) is fixed, NSE tends to a fixed constant as \( 0\). (ii) When \( 0\), NSE is determined by the weight \(\) of the regularizer. In other words, NSE is a function of \(\).

In our experiments, \(^{*}\) is randomly generated with \(\|^{*}\|_{0} k\). For \(i\{1,2,,n\}\), \(_{i}\) is drawn i.i.d. from \((0,)\), and \(_{i}\) is drawn i.i.d. from \((0,^{2})\). According to the \(k\)-sparse linear regression (1), \(y_{i}=_{i}^{}^{*}+_{i}\), we get dataset \((_{i},y_{i})\) with \(i=1,2,,n\). Then, we apply OKRidge to get the estimator \(}\) and calculate the NSE by \(\|}-^{*}\|_{2}^{2}/^{2}\). The NSE is averaged over \(10\) trials to evaluate the effectiveness of the OKRidge algorithm. In the main paper, we set \(=0.5\), \(=0.1\), \(d=100\). The computer resources are detailed in Appendix F.1. More experiments with various settings about \(\) and \(\) can be seen in Appendix F.2.

### The Change of NSE with \(\)

We investigate the change of NSE with \(1/\) under \(=1,5,10,_{best},\). The results are illustrated in Figure 2. As depicted in Figure 2, when \(\) is constant, NSE converges to a fixed value as \( 0\). This observation validates aspect (i) of Theorem 5.2.

### The Change of NSE with \(\)

We analyze the change of NSE with \(\) under \(^{2}=1,0.1,0.01,0.001\). The outcomes are depicted in Figure 2. As shown in Figure 2, the curves converge towards the real blue curve as \( 0\), where the blue curve relies on \(\). This observation confirms aspect (ii) of Theorem 5.2.

## 7 Conclusion

In this paper, we present a theoretical high-dimensional error analysis of the OKRidge algorithm in idealized settings using the CGMT framework. Specifically, when OKRidge tackles a \(k\)-sparse linear model with \((0,)\), \((0,^{2})\), and \(_{d}=(0,1)\), we have

\[_{d}_{^{2} 0}}- ^{}\|_{2}^{2}}{^{2}} (),_{d}_{ 0}\|}-^{}\|_{2}0.\]

where \(()\) depends on \(\). This indicates that \((i)\) the NSE limit of OKRidge remains constant when \(\) is fixed; \((ii)\)\(}\) learned by OKRidge is effective in estimating \(^{}\). Our experimental findings support these theoretical assertions. This theoretical error analysis substantiates the reliability of OKRidge and provides guidelines on the error analysis of other algorithms.