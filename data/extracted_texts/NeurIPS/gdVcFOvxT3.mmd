# Finding Safe Zones of Markov Decision Processes

Policies

 Lee Cohen

TTI-Chicago

&Yishay Mansour

Tel-Aviv University

Google Research

&Michal Moshkovitz

Bosch Center for AI

###### Abstract

Given a policy of a Markov Decision Process, we define a SafeZone as a subset of states, such that most of the policy's trajectories are confined to this subset. The quality of a SafeZone is parameterized by the number of states and the escape probability, i.e., the probability that a random trajectory will leave the subset. SafeZones are especially interesting when they have a small number of states and low escape probability. We study the complexity of finding optimal SafeZones, and show that in general, the problem is computationally hard. Our main result is a bi-criteria approximation learning algorithm with a factor of almost \(2\) approximation for both the escape probability and SafeZone size, using a polynomial size sample complexity.

## 1 Introduction

Most research in reinforcement learning (RL) deals with learning an optimal policy for some Markov Decision Process (MDP). One notable exception to that is _Safe RL_ which addresses the concept of safety. Traditional Safe RL focuses on finding the best policy that meets safety requirements, typically by either adjusting the objective to include the safety requirements and then optimizing for it, or incorporating additional safety constraints to the exploration. In both of these cases, the safety requirements should be pre-specified. _Anomaly Detection_ is the problem of identifying patterns in data that are unexpected, i.e., anomalies (see, e.g., Chandola et al. (2009) for survey). This paper introduces the SafeZone problem, which addresses the safety of a specific Markov decision process (MDP) policy by detecting anomalous events rather than finding a policy that satisfies some pre-defined safety constraints.

Consider a finite horizon MDP and a policy (a mapping from states to actions). The policy induces a Markov Chain (MC) on the MDP. Given a subset of states, a trajectory _escapes it_ if at least one of its states is not in the subset. The _escape probability_ of a subset is the probability that a randomly sampled trajectory will escape it. A SafeZone is a subset of states whose quality is measured by (1) its escape probability and (2) its size. If a SafeZone has low escape probability, we consider it _safe_ (hence escaping is the anomaly). We emphasize that safety is policy-dependent and that different policies could have different SafeZones.

Trivial solutions for SafeZone include the entire states set (minimal escape probability of \(0\), maximal size), and the empty set (minimal size, maximal escape probability of \(1\)). The goal is to find a SafeZone with a good balance: a relatively small size but still safe enough (small escape probability). More precisely, given an upper bound over the escape probability, \(>0\), the goal of the learner is to find the smallest SafeZone with escape probability at most \(\) using trajectory sampling. We address an unknown environment, by which we mean no prior knowledge of the transition function or the policy used. The learner is only given access to random trajectories generated by the induced Markov chain. For many applications, if a small SafeZone exists, it is useful to find it.

One such example is designing a policy for a smaller state space that performs well in most cases but is undefined for some states, or formally, imitation learning with compact policy representation Abel et al. (2018); Dong et al. (2019). Suppose a company would like to automatically generate a 'lite' edition of a software or an app (e.g., Microsoft Office Lite, Facebook Lite) that contains only part of the system's states, finding a SafeZone makes a lot of sense-- capturing popular users' trajectories. If instead of finding SafeZone, one were to simply take the \(90\%\) most popular states of users using Office, they might not include the state that allows for the precious option of saving, which emphasizes the importance of the problem.

Another motivation for the problem is autonomous vehicles and specifically infrastructure design for them. Even though a lot of the progress in the field of autonomous driving is credited to sensors installed on the vehicles, relying solely on the vehicles' sensors has its limitations (e.g., Yang et al. (2020)). In extreme weather, a vehicle might unintentionally deviate from the current lane and the vehicle sensors might not trigger a response in time. Vehicular-to-Infrastructure (V2I) is a type of communication network between vehicles and road infrastructures that are designed to fill the need for an extra layer of safety.1 An important part of the V2I communication is based on Road Side Units (RSUs), sensors that are installed alongside roads. Together with the sensors that are installed on the vehicles, they span the V2I communication. As the resources for RSUs distribution are limited and their enhanced safety is key for V2I and the autonomous vehicle adaptation, distributing RSUs in states of a (good) SafeZone could enhance the safety of popular commutes efficiently. Namely, given data regarding commutes (trajectories) in an area, installing RSUs in its 'SafeZone' states will ensure increased safety measures of a major part of the commutes, from starting point to destination, and potentially increase the trust in the system. In addition, if regulation were to prevent people from making autonomous commutes outside of the SafeZone, having most of the autonomous commutes confined to the SafeZone implies that most commutes can still be driverless.

Another useful application is automatic robotic arms that assemble products. If something unusual happened during the assembly of a product, it might result in a malfunctioning product, and in that case, the operator should be notified (anomaly detection). On the other hand, it is not really autonomic if the operator is notified too frequently. If we find a'safe enough' SafeZone, we can make sure that we notify the operator only in the rare event the production process (trajectory) escapes it, which means that something went wrong with the product assembly. Furthermore, if the SafeZone is small, the manufacturer can potentially test the SafeZone states and verify their compliance, ensuring that the majority of products are well constructed for a significantly lower testing budget.

Finally, the SafeZone problem can be viewed through the lens of explainable RL, where the goal is to explain a specific policy. SafeZone is a new post-hoc explanation of the summarization type Alharin et al. (2020). For example, for the autonomous vehicle infrastructure design, governments could explain to their citizens the design that was chosen.

Our results include approximation algorithms for the SafeZone problem, which we show is NP-hard. We are interested in a good trade-off between the escape probability of the SafeZone and its size. Our algorithms are evaluated based on two criteria: their approximation factors (w.r.t. the escape probability bound and the optimal set size for this bound), and their trajectory sample complexity bounds (e.g., Even-Dar et al. (2006)).

**Contribution**: In Section 2 we formalize the SafeZone problem. In Section 3, we explore naive approaches, namely greedy algorithms that select SafeZones based on state distributions and trajectory sampling. In addition, we show particular cases in which their solutions are far from optimal, either in terms of high escape probability or significantly larger set size. In Section 4 we design Finding SafeZone, an efficient approximation algorithm with provable guarantees. The algorithm returns a SafeZone which is slightly more than twice the size and twice the escape probability compared to the optimal. While the main focus of this work is the introduction of the problem and the aforementioned theoretical guarantees, we do demonstrate the problem empirically, to provide additional intuition to the readers. In Section 5, we compare the performance of the naive approaches to Finding SafeZone and show that different policies might lead to completely different SafeZones. In Appendix A, we show that the problem is hard, even for known environment setting, namely even when the induced Markov chain is given, finding a SafeZone is NP-hard, even for horizon \(H=2\).

For brevity, some algorithms and (full) proofs are relegated to the appendix.

### Related Work

MDPs have been studied extensively in the context of decision making in particular by the Reinforcement Learning (RL) community (see Puterman (1994) for a broad background on MDPs, and Sutton & Barto (2018) for background on reinforcement learning).

**Safe RL** A related line of research is safe RL, where the goal of the learner is to find the best policy that satisfies safety guarantees. The two main methodologies to handle such problems are: (1) altering the objective to include the safety requirement and optimizing over it, and (2) adding additional safety constraints to the exploration part. See Pfrommer et al. (2021); Emam et al. (2021); Xu et al. (2021); Hendrycks et al. (2021); HasanzadeZonuzy et al. (2021); Bennett et al. (2023); Prajapat et al. (2022); Liu et al. (2022) for recent works and Garcia & Fernandez (2015); Amodei et al. (2016) for surveys. Recent work by Sootla et al. (2022) augments the environment to accommodate some pre-specified safety constraints and thus satisfies them almost surely. In our work, the goal is not to find the optimal policy, but rather, given a policy, finding its SafeZone. The SafeZone is not characterized by specific requirements, and might not be unique. Moreover, beyond the MDP, the solution very much depends on the policy.

**Imitation Learning.** In imitation learning, the learner observes a policy behavior and wants to imitate it (see Hussein et al. (2017) for a survey). Similar to imitation learning, we are given access to samples of a given policy. In contrast, rather than imitating the policy we find the policy's SafeZone, which is an important property of the policy.

**Approximate MDP equivalence.** Another related research line is that of finding an (almost) equivalent minimal model for a given MDP, where the goal is that the optimal policy on the (almost) equivalent model induces an (approximately) optimal policy in the original MDP, e.g., Givan et al. (2003); Even-Dar & Mansour (2003). This line of works and ours differ in that we do not try to modify the MDP (e.g., cluster similar states), but rather to find a SafeZone, a property that is defined for the existing MDP and a specific policy.

**Explainability.** In explainability, the goal is to provide a post-hoc explanation to a specific (given) model Molnar (2019), e.g., using decision trees Blanc et al. (2021); Moshkovitz et al. (2021), influential examples Koh & Liang (2017), or local approximation explanations Li et al. (2020). We focus on explainability for reinforcement learning, and specifically, we suggest a new summarization explanation through our SafeZone(Amir & Amir, 2018).

## 2 The Safe Zone Problem

We model the problem using a Markov model with a finite horizon \(H>1\). Formally, there is a Markov chain (MC) \(,P,s_{0}\) where \(\) is the set of states, \(s_{0}\) is the initial state, and \(P:\) is the transition function that maps a pair of states into probability by \(P(s,s^{})=[s_{t+1}=s^{}|s_{t}=s]\). We assume the transition function \(P\) is induced by a policy \(:^{}\) on an MDP \(,s_{0},P^{},\) with transition function \(P^{}:\) such that \(P(s,s^{})=_{a}P^{}(s,a,s^{})(a|s)\) for all \(s,s^{}\) (though any MC can be generated this way, thus our theoretical guarantees apply for general MCs).

A _trajectory_\(=(s_{0},,s_{H})\) starts in the initial state \(s_{0}\) and followed by a sequence of \(H\) states generated by \(P\), i.e., \([s_{i+1}=s^{}|s_{i}=s]=P(s,s^{})\) for all \(i[H]\), where \([H]:=\{1,,H\}\). For example, in the context of autonomous vehicles, the trajectory is a commute. We abuse the notation and regard a trajectory \(\) both as a sequence and a set.

Given a subset of states \(F\), a trajectory \(\)_escapes_\(F\) if it contains at least one state \(s\) such that \(s F\), i.e., \( F\). For example, if a commute passes through at least one area (state) that does not have an RSU sensor, it escapes the SafeZone. We refer to the probability that a random trajectory escapes \(F\) as _escape probability_ and denote it by \((F)=_{}[ F]\). We call \(F\) a \(-\)_safe_ (w.r.t. the model \(,s_{0},P\)) if its escape probability, \((F)\), is at most \(\). Formally,

**Definition 2.1**.: _A set \(F\) is \(-\)safe if \((F):=_{}[ F],\) where \(\) is a random trajectory._A set \(F\) is called \((,k)-\)SafeZone if \(F\) is \(-\)safe and \(|F| k\). Given a safety parameter \((0,1)\), we denote the smallest size \(-\)safe set by \(k^{*}()\):

\[k^{*}()=_{F-}|F|.\]

Whenever the discussed parameter \(\) is clear from the context we use \(k^{*}\) instead of \(k^{*}()\). We remark that there might be multiple different \((,k)-\)SafeZone sets. The learner knows the set of states, \(\), the initial state, \(s_{0}\), and the horizon \(H\). However, the transition function \(P\) and the minimal size of the \(-\)safe set, \(k^{*}\), are unknown to the learner. Instead, the learner receives information about the model from sampling trajectories from the distribution induced by \(P\).

Given \(>0\), the ultimate goal of the learner would have been to find a \((,k^{*}())-\)SafeZone. However, as we show in Appendix A, finding a \((,k^{*}())-\)SafeZone is NP-hard, even when the transition function \(P\) is known. This is why we loosen the objective to find a bi-criteria approximation \((^{},k^{})-\)SafeZone. (Bi-criteria approximations are widely studied in approximation and online algorithms Vazirani (2001); Williamson & Shmoys (2011).) In our setting, given \(\) the objective is to find a set \(F\) which is \((^{},k^{})-\)SafeZone with minimal size \(k^{} k^{*}\) and minimal escape probability \(^{}\). In addition, we are interested in minimizing the sample complexity.

Notice that the learner can efficiently verify, with high probability, whether a set \(F\) is approximately \(-\)safe or not, as we formalize in the next proposition. The following proposition follows directly from Lemma C.2.

**Proposition 2.2**.: _There exists an efficient algorithm such that for every set \(F\) and parameters \(,>0\), the algorithm samples \(O(})\) random trajectories and returns \((F)\), such that with probability at most \(\) we have \(|(F)-(F)|\)._

### A Note on Trajectory Escaping

The SafeZone problem deals with escaping trajectories. In particular, given a SafeZone, a trajectory escapes it, no matter if only one of its states is outside the SafeZone or all of them. A related, yet very different problem, is that of minimizing a subset size, such that the expected number of states outside the set is minimized. This related problem, while significantly easier (as it is solved by returning the most visited states), does not apply to the applications we described earlier. For example, consider the infrastructure design for autonomous vehicles. We want passengers to have a safe experience end-to-end. Hence the entire route must have that extra security layer provided by the RSUs. In Section 3, we show that the solution for the SafeZone does not necessarily overlap with the most visited states. Furthermore, simply returning states that appeared in trajectory samples could result in a set size far from optimal.

### A Note on Multiple Policies

Our framework can accommodate an arbitrary number of policies, representing multiple agents. This is made feasible through the implementation of a single mixed policy. This mixed policy is designed to stochastically select a policy from this ensemble of policies, uniformly at random or according to any specified distribution over the participating agents and their respective policies.

### Summary of Contributions

We summarize the results of all the algorithms that appear in the paper in Table 1. The bounds of Greedy by Threshold and Greedy At Each Step require the Markov Chain model as input, and a pre-processing step that takes \(O(|S|^{2}H)\) time. Additionally, the bounds for the first three algorithms (the naive approaches) require additional knowledge of \(k^{*}()\). The sample complexities of Simulation is bounded by \(poly(k^{*},)\), and of Finding SafeZone Algorithm is bounded by \(poly(k^{*},H,,)\) for some parameters \(,(0,1)\).

Beyond the upper bounds, we provide each of the first three algorithms (the naive approaches) instances that show that they are tight up to a constant.

The following theorem is an informal statement of our main theorem, Theorem 4.2.

**Theorem 2.3**.: _For every \(,,>0\), with probability \( 0.99\) there exists an algorithm that returns a set which is \((2+2,(2+)k^{*})-\)SafeZone._In addition to the sample complexity, the running time of the algorithm is also bounded by \(poly(k^{*},H,,)\).

We empirically evaluate the suggested algorithms on a grid-world instance (where the goal is to reach an absorbing state), showing that Finding SafeZone outperforms the naive approaches. Moreover, we show that different policies have qualitatively different SafeZones. Finally, an informal statement of Theorem A.2 which appears in Appendix A due to space limitations.

**Theorem 2.4**.: SafeZone _is NP-hard._

## 3 Gentle Start

This section explains and analyzes various naive algorithms to the SafeZone problem. We show that even if the transition function is known in advance, these naive algorithms result in outputs that are far from optimal. To describe the algorithms, we define for each state \(s\) the probability to appear in a random trajectory and denote it by \(p(s)=_{}[s]\). Note that \(_{s}p(s)\) is a number between \(1\) and \(H\) (e.g., \(p(s_{0})=1\)), and can be estimated efficiently using dynamic programming if the environment and policy are known and sampling otherwise. To be precise, some of the algorithms assume the probabilities \(\{p(s)\}_{s}\) are received as input.

Greedy by Threshold Algorithm.The algorithm gets, in addition to \(\), the distribution \(p\) and a parameter \(>0\) as input. It returns a set \(F\) that contains all states \(s\) with probability at least \(\), i.e., \(p(s)\). We formalize this idea as Algorithm 3 in Appendix B. For \(=}\), the output of the algorithm is \((2,H}{})-\). More generally, we prove the following lemma.

**Lemma 3.1**.: _For any \(,(0,1)\), the Greedy by Threshold Algorithm returns a set that is \((+k^{*},)-\). In particular, for \(=}\), this set is \((2,H}{})-\)._

While it is clear why there are instances for which the safety is tight, Lemma B.1 in Appendix B shows that the set size is tight as well.

Simulation Algorithm.The algorithm samples \(O(}{})\) random trajectories and returns a set \(F\) with all the states in these trajectories. It is formalized in Appendix B as Algorithm 4.

**Lemma 3.2**.: _Fix \(,(0,1)\). With probability at least \(0.99\), Simulation Algorithm returns a set that is \((+k^{*},O(k^{*}+}{}))-\). In particular, for \(=}\), this set is \((2,O(k^{*}H k^{*}))-\)._

While the algorithm achieves a low escape probability, only \(2\), in Lemma B.2 in the appendix we prove that the size of \(F\) is tight up to a constant, i.e., we show an MDP instance where \(|F|=(k^{*}H k^{*})\). The algorithms presented so far were approximately safe (i.e., low escape probability), but the returned set size was large. Without any further assumptions, the following algorithm provides a \(( H,Hk^{*})-\), thus not improving the previous algorithms. However, when considering MDPs with a special structure it provides an optimal sized SafeZone, at the price of large escape probability.

Greedy at Each Step Algorithm.For the analysis of the next algorithm, we assume the MDP is _layered_, i.e., there are no states that appear in more than a single time step and denote \(=_{i=1}^{H}_{i}\). I.e., the transitions \(P(s,s^{})\) are nonzero only for \(s^{}_{i+1}\) and \(s_{i}\). The Greedy at Each Step Algorithm, sometimes simply called greedy, takes at each time step \(i\) the minimal number of states such that the sum of their probabilities is at least \(1-\). It is formalized in Appendix B as Algorithm 5.

  
**Algorithm** & **Safety** & **Set Size** \\  Greedy by Threshold & \(2\) & \(k^{*}H/\) \\  Simulation & \(2\) & \(O(k^{*}H k^{*})\) \\  Greedy at Each Step* & \( H\) & \(k^{*}\) \\  Finding SafeZone & \(2+2\) & \((2+)k^{*}\) \\   

Table 1: Upper bounds for safety and set size. * Only for layered MDPs.

**Lemma 3.3**.: _For any \((0,1)\), if the MDP is layered, Greedy at Each Step Algorithm returns a set that is \(( H,k^{*})-\)._

In Lemma B.3 in the appendix we provide a lower bound on the escape probability, matching up to a constant.

**Weaknesses of the naive algorithms.** We showed algorithms that identify SafeZone with escape probability much greater than \(\) or size much greater than \(k^{*}\), and instances with tight lower bounds for each of them. This holds even when providing extra information about the model or the optimal size of the \(-\)safe set, i.e., \(k^{*}\).

## 4 Algorithm for Detecting Safe Zones

In this section, we suggest a new algorithm that builds upon and improves the added trajectory selection of the Simulation Algorithm. One reason for why Simulation returns a large set is that it treats every sampled trajectory identically, regardless of how many states are being added, which could be as large as \(H\). More precisely, fix any \((,k^{*})-\) set, \(F^{*}\), and consider a trajectory \(\) that escapes it, i.e., \( F^{*}\). If \(\) was sampled, its states are added to the constructed set \(F\), which might increase the size of \(F\) by up to \(H\) states that are not in \(F^{*}\), without significantly improving the safety. In contrast, when selecting which trajectory to add to \(F\), we would consider the number of states it adds to the current set. For the sake of readability, we refer to any state which is not in the current set \(F\) as \(\), and denote by \(_{F}()\) the number of new states in \(\) w.r.t. \(F\), i.e.,

\[_{F}():=| F|.\]

Note that for every \(F\), \(_{}[_{F}() 0]=(F)\).

The new algorithm does not sample each trajectory uniformly at random, but samples from a new distribution, which will be denoted by \(Q_{F}\).

While favoring trajectories with higher probabilities, which we already get by the sampling process, another key idea would guide this new distribution: To prefer trajectories that _gradually_ increase the size of \(F\). To implement this idea, we will ensure that the probability of adding a trajectory \(\) to \(F\) should be _inversely proportional_ to \(_{F}()\).

Formally, the support of \(Q_{F}\) is the trajectories with new states, i.e., \(X=\{|_{F}() 0\}\). For every \( X\)

\[Q_{F}()_{F}()},\]

where \([]\) is the probability of trajectory \(\) under the Markov Chain with dynamics \(P\). Note that the new distribution depends on the current set \(F\), and changes as we modify it. Intuitively, adding trajectories to \(F\) according to \(Q_{F}\) instead of adding trajectories sampled directly from the dynamics (as we do in Simulation) would increase the expected ratio between the added safety and the number of new states we add to \(F\), thus improving the set size guarantee of the output set. We elaborate on this in Section 4.2.

Our main algorithm is Finding SafeZone, Algorithm 1. The algorithm receives, in addition to the safety parameter \(\), parameters \(\), \((0,1)\), and maintains a set \(F\) that is initiated to \(\{s_{0}\}\). On a high level, to implement the idea of adding trajectories to \(F\) according to \(Q_{F}\), we use _rejection sampling_. Namely, in each iteration of the while-loop we first sample a trajectory \(\), and if \(_{F}() 0\), we _accept_ it with probability \(1/_{F}()\). If the trajectory is accepted, it is added to \(F\). More precisely, if \(_{F}() 0\), we sample a Bernoulli random variable, \((1/_{F}())\). If \(=1\), we add \(\) to \(F\). This process of adding trajectories to \(F\) generates the desired distribution, \(Q_{F}\). Whenever a trajectory is added to \(F\), we estimate the escape probability \((F)\) (w.r.t. the updated set, \(F\)).

The algorithm stops adding states to \(F\) and returns it as output when it becomes "safe enough". To be precise, let \((F)\) denote the result of the escape probability estimation (by sampling trajectories as suggested in Proposition 2.2). If \((F) 2+\), it means that \(F\) is \((2+2)-\)safe with probability \( 1-_{j}>1-\), in which case the algorithm terminates and returns \(F\) as output.

To implement the estimation \((F)\), the algorithm calls _EstimateSafety_ Subroutine. The subroutine samples \(N_{j}=(}})\) trajectories, and returns the fraction of trajectories that escaped \(F\)For cases in which the transition function \(P\) is known to the learner, we provide an alternative implementation for _EstimateSafety_ which computes the exact probability \((F)\) (see Lemma E.1 in Appendix E).

```
Input: \((0,1)\)  Parameters: \(,(0,1)\) \(F\{s_{0}\},j 1,(F) 1\) while\((F)>2+\)do \(\) sample a random trajectory  Compute \(new_{F}()\) if\(new_{F}() 0\)then  sample \(accept Br(1/new_{F}())\) if\(accept=1\)then \(F F\) \(_{j}}\), \(j j+1\) \((F) EstSafety(,_{j},F)\) endif endif endwhile return\(F\)
```

**Algorithm 1** Finding SafeZone

### Algorithm Analysis

We define the event \(=\{ i\ |(F_{i-1})-(F_{i-1})| \}\), which states that all our _EstimateSafety_ Subroutine estimations are accurate. We show that \(\) holds with high probability using Hoeffding's inequality. In most of the analysis, we condition on \(\) to hold.

The following theorem is the central component in the proof of the main theorem that follows it.

**Theorem 4.1**.: _Given \(,,(0,1)\), Finding SafeZone Algorithm returns a subset \(F\) such that:_

1. _The escape probability is bounded from above by_ \((F) 2+2\)_, with probability_ \(1-\)_._
2. _The expected size of_ \(F\) _given_ \(\) _is bounded by_ \([|F|\ |\ ] 2k^{*}\)_._
3. _The sample complexity of the algorithm is bounded by_ \(O(}{^{2}}}{}+} {})\)_, and the running time is bounded by_ \(O(}{^{2}}}{}+k^ {*}}{})\)_, with probability_ \(1-\)_._

To obtain the main theorem, we run Finding SafeZone Algorithm several times and return the smallest output set, \(F\), see the next section for more details.

**Theorem 4.2**.: _(main theorem) Given \(,,>0\), if we run Finding SafeZone for \(()\) times and return the smallest output set, \(F\), then with probability \( 0.99\)_

1. _The escape probability is bounded by_ \((F) 2+2\)_._
2. _The size of_ \(F\) _is bounded from above by_ \(|F|(2+)k^{*}\)_._
3. _The total sample complexity and running time are bounded by_ \(O(}{^{2}^{2}}}{}+}{ ^{2}})\)_, and_ \(O(}{^{2}^{2}}}{}+k^{* }}{^{2}})\)_, respectively._

Finding an almost \(2\)-safe SafeZone, nearly \(2k^{*}\) in size can be valuable. For example, it enhances the safety of popular commuting routes, promotes trust in autonomous vehicles, and aligns with potential regulatory restrictions, making most commutes driverless within this secure zone. Regarding sample complexity, we expect the dependency on \(k^{*}\) to be nearly optimal. The reason why is the following. Consider an induced MC with \(k^{*}-1\) trajectories of size \(2\), each starting from an initial 

[MISSING_PAGE_FAIL:8]

**Sample Complexity.** To discuss the sample complexity, we drop the assumption that the MC is known to the learner and use _EstimateSafety_ Subroutine to approximate \((F)\). The number of calls to _EstimateSafety_ is bounded by the size of the output set, \(F\). Hence, this part of the sample complexity is bounded by \(|F| N_{|F|}\) and we show that is \(O(}{e^{2}} k^{*})\). Another source of sampling is trajectories sampled for purposes of potentially adding them to \(F\). Observe that at any iteration the set \(F\) has an escape probability of at least \(2\), and each trajectory that escapes \(F\) is accepted with a probability of at least \(1/H\). This implies a lower bound for the probability that a random trajectory is accepted is \(2/H\). This gives an upper bound of \(\) for the expected sample complexity.

**Amplification.**Theorem 4.1 shows that if \(\) holds, then the set size, \(|F|\), is bounded _in expectation_ by \(2k^{*}\). As \([] 1-\) implies, from Markov's inequality, that the size \((2+)k^{*}\) with small probability of about \(+=O()\). If we want to make sure that the actual size is at most \((2+)k^{*}\) with high probability, we can repeat the process about \(()\) times and take the smallest size set.

## 5 Empirical Demonstration

This section demonstrates the qualitative and quantitative performance of the described algorithms in the paper. For additional figures, we refer the reader to Appendix D.

**The MDP.** We focus on a grid of size \(N N\), for some parameter \(N\). The agent starts off at mid-left state, \((0,)\) and wishes to reach the (absorbing) goal state at \((N-1,)\) with a minimal number of steps. At each step, it can take one of four actions: up, down, right, and left by \(1\) grid square. With probability \(0.9\), the intended action is performed and with probability \(0.1\) there is a drift down. The agent stops either way after \(H=300\) steps.

### Finding SafeZone vs. naive approaches

To compare the Finding SafeZone Algorithm to the naive approaches presented in Section 3 we focus on the policy that first goes to the right and when it reaches the rightmost column, it goes up. The policy is described in the appendix, Figure 6(d). We take \(N=30\) and \(2000\) episodes.

We run the Finding SafeZone, Greedy, and Simulation algorithms, and estimate their coverage based on a test set containing \(2000\) random trajectories. Figure 1 depicts the trajectories coverage of each algorithm minus the coverage of the Greedy algorithm. For a figure with absolute values, we refer the reader to Figure 6(b) in the appendix. We see that the new algorithm exhibits better performance compared to its competitors. We also see that taking less than \(30\%\) of the states (\(k=250\) out of \(900\) states) is enough to get coverage of more than \(80\%\) of the trajectories.

Figures 5(a),5(b) show the sets found for \(k=60\) both by the _Finding SafeZone_ Algorithm and Greedy. We see that Greedy chooses an unconnected set for this small \(k\), leading to a coverage of \(0\). While the new algorithm chooses a few states which consist of several trajectories, thus leading to a coverage larger than \(0\).

Figure 1: Coverage percentage: difference from Greedy Algorithm.

## 6 Discussion and Open Problems

In this paper, we have introduced the SafeZone problem. We have shown that it is NP-hard, even when the model is known, and designed a nearly \((2,2k^{*})\) approximation algorithm for the case where the model and policy are unknown to the algorithm. Beyond improving the approximation factors (or showing that it cannot be done unless \(P=NP\)), a natural direction for future work is the following. Given a small \(>0\) and a (known or unknown to the learner) MDP, find a policy with a small \(-\)safe subset. If the value of the policy, when restricted to the SafeZone states, is close to the optimal value of the original MDP, restricting the policy to the SafeZone states generates a compact policy representation with a value close to optimal, and most trajectories are completed in the SafeZone.