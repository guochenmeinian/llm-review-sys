# Variational Inference in Similarity Spaces: A Bayesian Approach to Personalized Federated Learning

Pedro H. Barros

Universidade Federal de Minas Gerais

Belo Horizonte, Brazil

pedro.barros@dcc.ufmg.br &Fabricio Murai

Worcester Polytechnic Institute

Worcester, USA

fmurai@wpi.edu &Amir Houmansadr

University of Massachusetts

Amherst, USA

amir@cs.umass.edu &Alejandro C. Frery

Victoria University of Wellington

Wellington, New Zealand

alejandro.frery@vuw.ac.nz &Heitor S. Ramos

Universidade Federal de Minas Gerais

Belo Horizonte, Brazil

ramosh@dcc.ufmg.br

###### Abstract

Similarity space (or S-space) employs an encoder function, fed by labeled original pairwise data, to find a latent pairwise space with markers (prototypical) vector. It divides the space into regions where pairs of objects are either similar or dissimilar. This paper enhances S-space, equipping variational inference from personalized federated learning. The S-space representation aligns local representation spaces across clients, while variational inference improves generalization and reduces overfitting caused by data scarcity and client heterogeneity. Our theoretical analysis improved upper bounds on KL divergence between optimal local and optimal global variational models compared to traditional distributed Bayesian neural networks.

## 1 Introduction

Federated Learning (FL) has emerged as a methodology that enables learning from decentralized datasets without compromising data privacy . Real-world FL applications often involve non-IID data, which decrease individual client performance due to heterogeneous local data distributions . Personalized Federated Learning (PFL) addresses this by tailoring models to better align with local data characteristics . Various methods have been proposed to enhance PFL, including global model personalization , federated meta-learning , and parameter decoupling .

Traditional Neural Networks (NN) are typically used in PFL, although they usually show poor calibration and overconfidence in predictions, particularly when faced with varying data distributions . In contrast, Bayesian Neural Networks (BNNs) is a probabilistic approach that has been used in other contexts for modeling uncertainty and enabling models to learn continually by capturing past information , suggesting its potential use for PFL.

This paper proposes a novel framework for PFL with BNNs to address challenges in model overfitting due to limited local data (FL privacy constraints). Our approach leverages variational inference (VI) within an auxiliary representation space to enhance PFL model performance by quantifyingweight uncertainty in NNs at client and server models. To achieve personalization, each client updates its local VI parameters by reusing the global distribution from the server and balancing the KL divergence between the local posterior distribution and the server variational parameters. This strategy improves the upper bounds on this KL divergence compared to traditional distributed BNNs [13; 14]. Finally, our experiments show promising results in five datasets in the literature.

## 2 Methodology

**Federated Learning:** Consider a client \(u_{i}\) wishing to train a machine learning model with their respective datasets \(_{i}\). Traditional (centralized) machine learning training methods group all the data in the set \(=_{1}_{N}\). In contrast, a FL system enables clients to collaboratively train a model with the same architecture across all clients without sharing their local datasets \(_{i}\) with one another. However, FL introduces challenges due to the heterogeneity of local datasets.

**(Neural Network-induced) Similarity Space:** Let \(\) be the set of \(_{i}\) elements in an \(m\)-dimensional feature space, each associated with a label \(y_{i}\) and the function \(\), which maps the data into their respective labels. The original feature space \(\) is transformed into a _latent feature space_\(\) by a representation function \(f_{}\), where \(_{i}=f_{}(_{i})^{d}\) and \(\) is the set of weights of a given NN.

We estimate an _auxiliary space_, referred to as _S-Space_, to refine the latent feature space with a mapping function \(f^{S}\), where if \((_{i})=(_{j})\), then (\(_{i}\), \(_{j}\)) is a similar pair. On the other hand, we consider (\(_{i}\), \(_{j}\)) to be a dissimilar pair if \((_{i})(_{j})\). Given a pair of elements \((_{i},_{j})\) from the original feature space, \(f^{S}\) computes a similarity vector \(_{ij}\) using the absolute element-wise difference between their latent space representations

\[_{ij} =f^{S}(_{i},_{j})\] \[=|f_{}(_{i})-f_{}(_{j})|\] \[=|_{i}-_{j}|\] \[=(|z_{i,1}-z_{j,1}|,,|z_{i,d}-z_{j,d}|),\]

where \(_{ij}^{d}\) has the same dimension as \(_{i}\), \(z_{n,k}\) is the \(k\)-th feature of the \(n\)-th sample in the latent space representation \(\), and the absolute operation ensures symmetry \(_{ij}=_{ji}\).

We define two disjoint sets in the S-Space to quantify the similarity between input pairs: similarity markers (\(^{+}\)) and dissimilarity markers (\(^{-}\)). Their union \(=^{+}^{-}\) is the set of all markers (or prototypes) and \(^{+}^{-}=\). The (dis)similarity between input pairs is determined by the distance of the vector \(_{ij}\) to the markers \(_{e}\). We use a Cauchy kernel  to measure the similarity \(q^{e}_{ij}\) between the point \(_{ij}\) and the marker \(_{e}\) as

\[q^{e}_{ij}=_{ij}-_{e}\|_{2}^{2})^{-1}}{ _{m_{e^{}}}(1+\|_{ij}-_{e^{}}\|_{2 }^{2})^{-1}}.\]

Consider the pair \((_{i},_{j})\), we define the probability of \(s_{ij}\) vector being "similar" as \(q^{+}_{ij}=_{p}q^{p}_{ij}\) for all \(_{p}^{+}\). Analogously, the probability of \(s_{ij}\) vector being "dissimilar" is \(q^{-}_{ij}=_{n}q^{n}_{ij}\) for all \(_{n}^{-}\). Notice that \(q^{+}_{ij}+q^{-}_{ij}=1\) because the sets \(^{+}\) and \(^{-}\) are disjoint.

The binary pair cross-entropy loss function \(J()\) is

\[J()=-_{i}_{J}[u_{ij} q ^{+}_{ij}+(1-u_{ij}) q^{-}_{ij}],\] (1)

where \(u_{ij}=[(_{i})=(_{j})]\) is the indicator function, i.e., \([]=1\) if \([]\) is true and \(0\) otherwise; We note that similar definitions exist for this auxiliary space . However, as demonstrated in the following section, this is the first instance of applying VI within FL settings.

**Variational inference:** We exploit the S-Space by introducing VI techniques to estimate the markers \(\). Our challenge is to make statistical inferences from the posterior distribution \(p_{}(_{1},,_{N})\) based on the NN parameters \(\) without compromising data privacy. To tackle this, each client minimizes the Kullback-Leibler (KL) divergence between each client's variational distribution \(q_{_{k}}\) and the true posterior, formulated as

\[*{arg\,min}_{q_{_{k}}()} q_{_{k}}() p_{}( _{1},,_{N})},\] (2)

where \(\) is a variational family of distributions.

We assume that each client's marker distribution (variational parameters) in the S-Space follows a Gaussian distribution (\((,^{2})\)), representing the variational distribution \(q_{_{k}}\) associated with the client \(k\) as a product of normal distributions (mean-field approximation) . The likelihood function \(p_{}(_{k})(-J(_{k})/)\) is defined using an exponential loss function (Boltzmann distribution) , where \(J()\) is the S-Space loss function (or energy function - Eq. 1) and \(>0\) is a (temperature) scaling parameter .

Denote as \(_{ k}=\{_{1},,_{k-1},_{k+1},,_{N}\}\) the local datasets excluding the data from client \(u_{k}\). Note that client \(u_{k}\) does not have access to \(_{ k}\) due to FL privacy constraints. We approximate the posterior distribution using Bayes' theorem and a server variational model as \(p_{}(_{ k}) s()\). The KL divergence (Eq. 2) can be approximated as1

\[q_{_{k}}() s()(_{k})}{Z_{k}}=q_{ _{k}}() s()+ Z_{k}+_{q_{_{k}}}[J(_{k})],\]

where \(Z_{k}\) is a normalization constant.

Following , we have adjusted the scale \(\) to enhance numerical stability. We omit the normalization constant \( Z_{k}\) from the optimization problem (Evidence Lower Bound) . Our approach captures model performance on specific tasks and ensures regularization by minimizing divergence from the server model; cf. Appendix B for details. Our VI approach in FL is a dual optimization framework that enhances client-level personalization:

Client: \[*{arg\,min}_{q_{_{k}}()} F_{k}(s())=_{q_{_{k}}()}[J (_{k})]+q_{_{k}}()  s()},\] (3) Server: \[*{arg\,min}_{s()}\{_{ k=1}^{N}F_{k}(s())\}.\] (4)

**Theoretical Analysis:** In this section, we present a theoretical discussion about the S-space. Moreover, we state the necessary Assumptions 1, 2, 3 and analyze equal-width BNNs as in .

**Definition 2.1**.: _(Optimal Variational Latent Space - OVLS) Consider all \(_{i},_{j}\) and a latent representation function \(f_{}\). The transformation \(f_{}\) generates an OVLS \(\) if (i) \(\|_{ij}\|_{2}(0)(_{i})=(_{j})\), and if (ii) \((_{i})(_{j})[\|_{ij}\|_{2} ]>0\), where \(()\) is the Dirac delta function, and \(\) maps an unlabeled example \(_{i}\) into its true label \(y_{i}\)._

Definition 2.1 emphasizes the relationship between geometric proximity in the latent space \(\) and label identity. Unlike traditional deep metric learning methods , our approach estimates a more flexible representation and captures more sophisticated data patterns (see Appendix A). Furthermore, a 1-nearest neighbor classifier in OVLS is optimal, emphasizing its capacity for perfect classification in theoretical applications.

Our algorithm induces a representation space as Definition 2.1. The loss function (Eq. 1) clusters samples by minimizing intra-group local distances (\(\|_{ij}\|_{2}(0)\)) and aligns a positive marker \(^{+}^{+}\) near the origin \((\|^{+}\|_{2}(0))\), see Appendix D.2.

**Corollary 1**.: _(based on Lemma C.1) Let \(f_{i,}\) be a latent representation function that generates an OVLS, and \(q^{*}_{_{i}}()\) denote the optimal variational distribution for the \(i\)-th user, estimated by a NN with weights \(\). If the markers are permuted based on the norm, i.e., the variational parameters \((^{*}_{i,j},^{*}_{i,j}) q^{*}_{_{i}}()\) are organized into an ordered set according to the norm \(\|^{*}_{i,j}\|_{2}\), then the optimal server variational distribution \(s^{*}()\) admits the existence of an \(i\) such that \(\|^{*,i}_{i}\|_{2}(0)\)._Based on Corollary 1 (see Appendix C), we adapt the aggregation function proposed in  by ordering the set \(\) using the norm \(\|\|_{2}\) before transmitting the weights to the server. This permutation optimizes the aggregation function's performance, ensuring alignment with the criteria for OVLS.

**Theorem 1**.: _If Assumptions 1, 2, 3 are true, then the following inequality holds_

\[q_{_{k}}^{*}() s^{*}() (C^{}nr_{n})<C^{}(n-1)r_{n},\]

_where \(D\) is the number of markers in \(\), \(r_{n}\) and \(C^{}\) are constants._

Theorem 1 provides an upper limit for the KL divergence between the local (client) optimal variational solution \(q_{_{k}}^{*}\) and the global (server) optimal variational solution \(s^{*}\). Our approach, thus, improves this FL theoretical results by using the variational S-space to estimate an optimal global VI distribution, with a tighter upper bound on divergence compared to traditional BNN approaches \(((q_{_{k}}^{*}() s^{*}()) C^{ }nr_{n})\), as documented in [13; 14]. We present the proof in Appendix C.2.

## 3 Experimentation

FL settings: We performed our experiments using the Flower framework  in a FL setting characterized by non-IID clients (quantity-based label imbalance) [29; 30]. For each dataset, we sorted the data by labels and divided it into \(N\) clients. Each client was assigned \(\#S\) random non-overlapping subsets (shard), each containing an equal number of samples [29; 30].

Thus, we used a server and 100/200 clients in our experiment to evaluate our model, and we trained our method with two NVIDIA RTX 6000 Ada Generation (\(48\,\)) for 1000 FL epochs. For each training round, the server selects \(5\,\%\) of clients to train for five local epochs of the user model. We use the _F1-Score_ commonly used in classification tasks, which can be directly computed from the confusion matrix.

Dataset description: We use five datasets for our evaluation: The MNIST/FMNIST datasets consist of 28x28 pixel grayscale images, with 60,000 training and 10,000 testing examples. The Malimg dataset comprises 9,339 samples from 25 malware families, with sample counts ranging from 80 to 2,949 per family. MaleVis features byte images of 25 malware types and one goodware class, totaling 14,226 images. CIFAR-10 includes 60,000 color images across ten different classes. Each malware binary code is visualized as a 64x64 grayscale image.

Results: We compared our proposal to six PFL techniques from the literature [3; 5; 13; 17; 31; 32], as well as the standard FedAvg (Global Model - GM) and Local model without client communication (baseline). The results, summarized in Table 1, show that our approach outperformed FedAvg across all five datasets. For example, on the CIFAR10 dataset, our method achieved an F1-score improvement of \(27.43\,\%\) with four shards per client.

Our method also shows improvements across other datasets. For instance, on CIFAR10, our approach achieved the highest F1 scores of 0.7015 and 0.7202 with four and five shards per client, respectively. On the Malimg dataset, it scored 0.9324 with five shards per client, surpassing FedRep, which achieved the second-best result of 0.9250. However, on the MNIST dataset, our proposal achieved the third-best results for both shard values.

Figure 1 compares the F1-scores between our proposal and the best methods from the literature across five datasets with four shards per client over 1000 FL epochs. In summary, our approach achieved the best performance in six out of ten FL settings and the second-best in two additional settings. These findings are consistent with results obtained using 200 clients, as detailed in Table 2 (Appendix D.3).

## 4 Conclusion

In this work, we introduced the variational auxiliary similarity space within an FL environment, designed to enhance latent feature representation. The variational S-Space adapts to the unique data distributions of individual clients, mitigating the effects of data heterogeneity. Our evaluations, conducted across five datasets and ten different FL settings, demonstrate that our approach outperforms existing PFL methods. In addition, we have provided theoretical results showing that our approach achieves improved upper bounds compared to traditional distributed BNNs by applyingan aggregation function projected onto BNNs within an FL framework. A particularly promising direction is designing a specialized aggregation function optimized for the variational S-space.