# Computation-Aware Gaussian Processes:

Model Selection And Linear-Time Inference

 Jonathan Wenger\({}^{1}\) Kaiwen Wu\({}^{2}\) Philipp Hennig\({}^{3}\) Jacob R. Gardner\({}^{2}\) Geoff Pleiss\({}^{4}\) John P. Cunningham\({}^{1}\)

\({}^{1}\) Columbia University

\({}^{2}\) University of Pennsylvania

\({}^{3}\) University of Tubingen, Tubingen AI Center

\({}^{4}\) University of British Columbia, Vector Institute

###### Abstract

Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory. While many approximations exist, all incur inevitable approximation error. Recent work accounts for this error in the form of computational uncertainty, which enables--at the cost of quadratic complexity--an _explicit_ tradeoff between computational efficiency and precision. Here we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset. We propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets. Our experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU. As a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty--a fundamental prerequisite for optimal decision-making.

## 1 Introduction

Gaussian Processes (GPs) remain a popular probabilistic model class, despite the challenges in scaling them to large datasets. Since both computational and memory resources are limited in practice, approximations are necessary for both inference and model selection. Among the many approximation methods, perhaps the most common approach is to map the data to a lower-dimensional representation. The resulting posterior approximations typically have a functional form similar to the exact GP posterior, except where posterior mean and covariance feature _low-rank updates_. This strategy can be explicit--by either defining feature functions (e.g. Nystrom , RFF )--or a lower-dimensional latent inducing point space (e.g. SoR, DTC, FITC , SGPR , SVGP ), or implicit--by using an iterative numerical method (e.g. CGGP ). All of these methods then compute coefficients for this lower-dimensional representation from the full set of observations by direct projection (e.g. CGGP) or via an optimization objective (e.g. SGPR, SVGP).

While effective and widely used in practice, the inevitable approximation error adversely impacts predictions, uncertainty quantification, and ultimately downstream decision-making. Many proposed methods come with theoretical error bounds [e.g. 11, 12, 13, 14], offering insights into the scaling and asymptotic properties of each method. However, theoretical bounds often require too many assumptions about the data-generating process to offer "real-world" guarantees , and in practice, the fidelity of the approximation is ultimately determined by the available computational resources.

One central pathology is overconfidence, which has been shown to be detrimental in key applications of GPs such as Bayesian optimization [e.g. variance starvation of RFF, 16], and manifests itself even in state-of-the-art variational methods like SVGP. SVGP, because it treats inducing variables as "virtual observations", can be overconfident at the locations of the inducing points if they are not in close proximity to training data, which becomes increasingly likely in higher dimensions. This phenomenon can be seen in a toy example in Figure 1, where SVGP has near zero posterior variance at the inducing point away from the data. See also Section S5.1 for a more detailed analysis.

These approximation errors are a central issue in inference, but they are exacerbated in model selection, where errors compound and result in biased selections of hyperparameters . Continuing the example, SVGP has been observed to overestimate the observation noise , which can lead to oversmoothing. This issue can also be seen in Figure 1, where the SVGP model produces a smoother posterior mean than the exact (Cholesky)GP and attributes most variation from the posterior mean to observational noise (see also Figure S3(b)). There have been efforts to understand these biases  and to mitigate the impact of approximation error on model selection for certain approximations [e.g. CGGP, 12], but overcoming these issues for SVGP remains a challenge.

Recently, Wenger et al.  introduced computation-aware Gaussian processes (CaGP), a class of GP approximation methods which--for a fixed set of hyperparameters--provably does not suffer from overconfidence. Like SVGP and the other approximations mentioned above, CaGP also relies on low-rank posterior updates. Unlike these other methods, however, CaGP's posterior updates are constructed to guarantee that its posterior variance is always larger than the exact GP variance. This conservative estimate can be interpreted as additional uncertainty quantifying the approximation error due to limited computation; i.e. _computational uncertainty_. However, so far CaGP has fallen short in demonstrating wallclock time improvements for posterior inference over variational methods and model selection has so far remained an open problem.

ContributionsIn this work, we extend computation-aware Gaussian processes by demonstrating how to perform inference in linear time in the number of training data, while maintaining its theoretical guarantees. Second, we propose a novel objective that allows model selection without a significant bias that would arise from naively conducting model selection on the projected GP. In detail, we enforce a sparsity constraint on the "actions" of the method, which unlocks linear-time inference, in a way that is amenable to hardware acceleration. We optimize these actions end-to-end alongside the hyperparameters with respect to a custom training loss, to optimally retain as much information from the data as possible given a limited computational budget. The resulting hyperparameters are less prone to oversmoothing and attributing variation to observational noise, as can be seen in Figure 1, when compared to SVGP. We demonstrate that our approach is strongly competitive on large-scale data with state-of-the-art variational methods, such as SVGP, without inheriting their pathologies. As a consequence of our work, one can train GPs on up to \(1.8\) million data points in a few hours on a single GPU without adversely impacting uncertainty quantification.

Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data.

## 2 Background

We aim to learn a latent function mapping from \(^{d}\) to \(\) given a training dataset \(=(_{1},,_{n})^{n d}\) of \(n\) inputs \(_{j}^{d}\) and corresponding targets \(=(y_{1},,y_{n})^{}^{n}\).

Gaussian ProcessesA _Gaussian process_\(f(,K_{})\) is a stochastic process with mean function \(\) and kernel \(K_{}\) such that \(=f()=(f(_{1}),,f(_{n}))^{}(,}})\) is jointly Gaussian with mean \(_{i}=(_{i})\) and covariance \(_{ij}=K_{}(_{i},_{j})\). The kernel \(K_{}\) depends on hyperparameters \(^{p}\), which we omit in our notation. Assuming \( f()f(),^{2}\), the posterior is a Gaussian process \((_{},K_{})\) where the mean and covariance functions evaluated at a test input \(_{}^{d}\) are given by

\[_{}(f(_{})) =(_{})+K(_{},)_{},\] (1) \[K_{}(f(_{}),f(_{})) =K(_{},_{})-K(_{},)}^{-1}K(,_{}),\]

where \(}=+^{2}\) and the _representer weights_ are defined as \(_{}=}^{-1}(-)\).

In model selection, the computational bottleneck when optimizing kernel hyperparameters \(\) is the repeated evaluation of the _negative_\(\)-_marginal likelihood_

\[^{}()=- p()= -)^{}}^{-1 }(-)}_{}+})}_{}+n(2)\] (2)

and its gradient. Computing (2) and its gradient via a Cholesky decomposition has time complexity \((n^{3})\) and requires \((n^{2})\) memory, which is prohibitive for large \(n\).

Sparse Gaussian Process Regression (SGPR) Given a set of \(m n\)_inducing points_\(=(_{1},,_{m})^{}\) and defining \(:=f()=(f(_{1}),,f(_{m}))^{}\), SGPR defines a variational approximation to the posterior through the factorization \(p_{}(f()) q(f())= p(f())\ q( )du\), where \(q()\) is an \(m\)-dimensional multivariate Gaussian. The mean and covariance of \(q()\) (denoted as \(:=_{q}()\), \(:=_{q}()\)) are jointly optimized alongside the kernel hyperparameters \(\) using the evidence lower bound (ELBO) as an objective:

\[,,,=_{,,,}^{},\] (3) \[^{}(,,,):= ^{}()+(q() p_{ }(,))\] (4) \[= -_{q()}( p())+(q( ) p())- p().\]

The inducing point locations \(\) can be either optimized as additional parameters during training or chosen a-priori, typically in a data-dependent way (see e.g. Sec. 7.2 of ). Following Titsias , ELBO optimization and posterior inference both require \((nm^{2})\) computation and \((nm)\) memory, a significant improvement over the costs of exact GPs.

Stochastic Variational Gaussian Processes (SVGP) SVGP extends SGPR to reduce complexity further to \((m^{3})\) computation and \((m^{2})\) memory. It accomplishes this reduction by replacing the first term in Equation (4) with an unbiased approximation

\[_{q()}( p())=_{q()}(_{i= 1}^{n} p(y_{i} f(_{i}))) n\,_{q(f(_{i}))}(  p(y_{i} f(_{i})))\,.\]

Following Hensman et al. , we optimize \(,\) alongside \(,\) through joint gradient updates. Because the asymptotic complexities no longer depend on \(n\), SVGP can scale to extremely large datasets that would not be able to fit into computer/GPU memory.

Computation-aware Gaussian Process Inference (CaGP) CaGP1 maps the data \(\) into a lower dimensional subspace defined by its _actions_\(_{i}^{n i}\) on the data, which defines an approximate posterior \((_{i},K_{i})\) with

\[_{i}(_{}) =(_{})+K(_{},)_{i}\] (5) \[K_{i}(_{},_{}) =K(_{},_{})-K(_{},) _{i}K(,_{}),\]

[MISSING_PAGE_FAIL:4]

ELBO Training LossMotivated by this observation, we desire a tractable training loss that encourages maximizing the evidence for the _entire set of targets_\(\). Importantly though, we need to accomplish this evidence maximization without incurring prohibitive \((n^{3})\) computational cost. We define a variational objective, using the computation-aware posterior \(q_{i}(,)\) in Equation (5) to define a variational family \(\{q_{i}()=(;_{i}(), K_{i}(,))^{n i}\}\) parametrized by the action matrix \(\). We can then specify a (negative) evidence lower bound (ELBO) as follows:

\[^{}_{}()=}{^{}()}+q_{i} p_{}}{(q_{i} p_{})}- p( ).\] (10)

This loss promotes learning the same hyperparameters as if we were to maximize the computationally intractable evidence \( p()\) while minimizing the error due to posterior approximation \(q_{i}() p_{}(,)\). In the computation-aware setting, this translates to minimizing computational uncertainty, which captures this inevitable error.

Although both the evidence and KL terms of the ELBO involve computationally intractable terms, these problematic terms cancel out when combined. This results in an objective that costs the same as evaluating CaGP's predictive distribution, i.e.

\[^{}_{}() =}\|-_{i}( )\|_{2}^{2}+_{j=1}^{n}K_{i}(_{j},_{j})+(n-i)( ^{2})+n(2)\] (11) \[+}_{i}^{}^{} }_{i}-((^{}})^{-1} ^{})+(^{}} )-(^{})\]

where \(}_{i}=(^{}})^{-1}^{}(-)\). For a derivation of this expression of the loss see Lemma S3. If we compare the training loss \(^{}_{}\) in Equation (10) with the projected-data NLL in Equation (9), there is an explicit squared loss penalty on _the entire data_\(\), rather than just for the projected data \(}\), resulting in better generalization as Figures S1 and S2 show on synthetic data. In our experiments, this objective was critical to achieving state-of-the-art performance.

## 4 Choice of Actions

So far we have not yet specified the actions \(^{n i}\) mapping the data to a lower-dimensional space. Ideally, we would want to optimally compress the data both for inference and model selection.

Posterior Entropy MinimizationWe can interpret choosing actions \(\) as a form of active learning, where instead of just observing individual datapoints, we allow ourselves to observe linear combinations of the data \(\). Taking an information-theoretic perspective , we would then aim to choose actions such that _uncertainty about the latent function is minimized_. In fact, we show in Lemma S5 that given a prior \(f(,K)\) for the latent function and a budget of \(i\) actions \(\), the actions that minimize the entropy of the posterior at the training data

\[(_{1},,_{i})=^{n i}}{ *{arg\,min}}*{arg\,min}_{^{n i }}*{arg\,min}_{p(f()|^{})}(f())\] (12)

are the top-\(i\) eigenvectors \(_{1},,_{i}\) of \(}\) in descending order of the eigenvalue magnitude (see also Zhu et al. ). Unfortunately, computing these actions is just as prohibitive computationally as computing the intractable GP posterior.

(Conjugate) Gradient / Residual ActionsDue to the intractability of choosing actions to minimize posterior entropy, we could try to do so approximately. The Lanczos algorithm  is an iterative method to approximate the eigenvalues and eigenvectors of symmetric positive-definite matrices. Given an appropriate seed vector, the space spanned by its approximate eigenvectors is equivalent to the span of the gradients / residuals \(_{i}=(-)-}_{i-1}\) of the method of Conjugate Gradients (CG)  when used to iteratively compute an approximation \(_{i}_{}=}^{-1}(-)\) to the representer weights \(_{}\). We show in Lemma S4 that the CaGP posterior only depends on the span of its actions. Therefore choosing approximate eigenvectors computed via the Lanczos process as actions is equivalent to using CG residuals. This allows us to reinterpret CaGP-CG, as introduced by Wenger et al. , as approximately minimizing posterior entropy.4 See Section S3.1 for details.

As Figure 2 illustrates, CG actions are similar to the top-\(i\) eigenspace all throughout hyperparameter optimization. However, this choice of actions focuses exclusively on posterior inference and incurs quadratic time complexity \((n^{2}i)\).

Learned Sparse ActionsSo far in our action choices we have entirely ignored model selection and tried to choose optimal actions assuming _fixed_ kernel hyperparameters. The second contribution of this work, aside from demonstrating how to perform model selection, is recognizing that the actions should be informed by the outer optimization loop for the hyperparameters. We thus optimize the actions alongside the hyperparameters _end-to-end_, meaning the training loss for model selection defines what data projections are informative. This way the actions are adaptive to the hyperparameters without spending unnecessary budget on computing approximately optimal actions for the current choice of hyperparameters. Specifically, the actions are chosen by optimizing \(_{}^{}\) as a function of the hyperparameters \(\) and the actions \(\), s.t.

\[(_{},_{i})=_{(,)} _{}^{}(,).\] (13)

Naively this approach introduces an \(n i\) dimensional optimization problem, which in general is computationally prohibitive. To keep the computational cost low and to optimally leverage hardware acceleration via GPUs, we impose a sparse block structure on the actions (see Eq. 14) where each block is a column vector \(^{}_{j}^{k 1}\) with \(k=n/i\) entries such that the total number of trainable action parameters, i.e. non-zero entries \(()=k i=n\), equals the number of training data. Due to the sparsity, these actions cannot perfectly match the maximum eigenvector actions. Nevertheless, we see in Figure 2 that optimizing these sparse actions in conjunction with hyperparameter optimization produces a nontrivial alignment with optimal action choice minimizing posterior entropy. Importantly, the sparsity constraint not only reduces the dimensionality of the optimization problem, but crucially also reduces the time complexity of posterior inference and model selection to _linear_ in the number of training data points.

### Algorithms and Computational Complexity

We give algorithms both for iteratively constructed dense actions (Algorithm S1), as used in CaGP-CG, and for sparse batch actions (Algorithm S2), as used for CaGP-Opt, in the supplementary

Figure 2: _Visualization of action vectors defining the data projection. We perform model selection using two CaGP variants, with CG and learned sparse actions—denoted as CaGP-CG, and CaGP-Opt—on a toy 2-dimensional dataset. Left: For each \(_{j}\{_{1},,_{n}\}\), we plot the magnitude of the entries of the top-5 eigenvectors of \(\) and of the first five action vectors. Yellow denotes larger magnitudes; blue denotes smaller magnitudes. Right: We compare the span of the actions \(\) against the top-\(i\) eigenspace throughout training by measuring the Grassman distance between the two subspaces (see also Section S5.2). CaGP-CG actions are closer to the kernel eigenvectors than the CaGP-Opt actions, both of which are more closely aligned than randomly chosen actions._

material.5 The time complexity is \((n^{2}i)\) for dense actions and \((ni(i,k))\) for sparse actions, where \(k\) is the maximum number of non-zero entries per column of \(_{i}\). Both have the same linear memory requirement: \((ni)\). Since the training loss \(^{}_{}\) only involves terms that are also present in the posterior predictive, both model selection and predictions incur the same complexity.

### Related Work

Computational Uncertainty and Probabilistic NumericsAll CaGP variants discussed in this paper fall into the category of probabilistic numerical methods , which aim to quantify approximation error arising from limited computational resources via additional uncertainty about the quantity of interest [e.g. 28, 29, 30, 31]. Specifically, the iterative formulation of CaGP (i.e. Algorithm S1) originally proposed by Wenger et al.  employs a probabilistic linear solver .

Scalable GPs with Lower-Bounded Log Marginal LikelihoodsNumerous scalable GP approximations beyond those in Sections 1 and 2 exist; see Liu et al.  for a comprehensive review. Many GP models [e.g., 37, 38, 39, 4, 5] learn hyperparameters through maximizing variational lower bounds in the same spirit as SGPR, SVGP and our method. Similar to our work, interdomain inducing point methods  learn a variational posterior approximation on a small set of linear functionals applied to the latent GP. However, unlike our method, their resulting approximate posterior is usually prone to underestimating uncertainty in the same manner as SGPR and SVGP. Finally, similar to our proposed training loss for CaGP-CG, Artemev et al.  demonstrate how one can use the method of conjugate gradients to obtain a tighter lower bound on the log marginal likelihood.

GP Approximation Biases and Computational UncertaintyScalable GP methods inevitably introduce approximation error and thus yield biased hyperparameters and predictive distributions, with an exception of Potapczynski et al.  which trade bias for increased variance. Numerous works have studied pathologies associated with optimizing variational lower bounds, especially in the context of SVGP , and various remedies have been proposed. In order to mitigate biases from approximation, several works alternatively propose replacing variational lower bounds with alternative model selection objectives, including leave-one-out cross-validation  and losses that directly target predictive performance .

## 5 Experiments

We benchmark the generalization of computation-aware GPs with two different action choices, CaGP-Opt (ours) and CaGP-CG , using our proposed training objective in Equation (10) on a range of UCI datasets for regression . We compare against SVGP, often considered to be state-of-the-art for large-scale GP regression. Per recommendations by Ober et al. , we also include SGPR as a strong baseline for all datasets where this is computationally feasible. We also train Conjugate Gradient-based GPs (CGGP) [e.g. 7, 9, 10] using the training procedure proposed by Wenger et al. . Note that CaGP-CG recovers CGGP in its posterior mean and produces nearly identical predictive error at half the computational cost for inference [Sec. 2.1 & 4 of 19], which is why the main difference between CaGP-CG and CGGP in our experiments is the training objective. Finally, we also train an exact CholeskyGP on the smallest datasets, where this is still feasible.

Experimental DetailsAll datasets were randomly partitioned into train and test sets using a \((0.9,0.1)\) split for five random seeds. We used a zero-mean GP prior and a Matern(\(3/2\)) kernel with an outputscale \(o^{2}\) and one lengthscale per input dimension \(l_{j}^{2}\), as well as a scalar observation noise for the likelihood \(^{2}\), s.t. \(=(o,l_{1},,l_{d},)^{d+2}\). We used the existing implementations of SGPR, SVGP and CGGP in GPyTorch and also implemented CaGP in this framework (see Section S4.2 for our open-source implementation). For SGPR and SVGP we used \(m=1024\) inducing points and for CGGP, CaGP-CG and CaGP-Opt we chose \(i=512\). We optimized the hyperparameters \(\) either with Adam  for a maximum of 1000 epochs in float32 or with LBFGS  for 100 epochs in float64, depending on the method and problem scale. On the largest dataset "Power", we used 400 epochs for SVGP and 200 for CaGP-Opt due to resource constraints. For SVGP we used a batch size of 1024 throughout. We scheduled the learning rate via PyTorch's  LinearLR(end_factor=0.1) scheduler for all methods and performed a hyperparameter sweep for the (initial) learning rate. All experiments were run on an NVIDIA Tesla V100-PCIE-32GB GPU, except for "Power", where we used an NVIDIA A100 80GB PCIE GPU to have sufficient memory for CaGP-Opt with \(i=512\). Our exact experiment configuration can be found in Table S1.

Evaluation MetricsWe evaluate the generalization performance once per epoch on the test set by computing the (average) negative log-likelihood (NLL) and the root mean squared error (RMSE), as well as recording the wallclock runtime. Runtime is measured at the epoch with the best average performance across random seeds.

CaGP-Opt Matches or Outperforms SVGPTable 1 and Figure 3 show generalization performance of all methods for the best choice of learning rate. In terms of both NLL and RMSE, CaGP-Opt outperforms or matches the variational baselines SGPR and SVGP at comparable runtime (except on "Bike"). SGPR remains competitive for smaller datasets; however, it does not scale to the largest datasets. There are some datasets and metrics in which specific methods dominate, for example on "Bike" SGPR outperforms all other approximations, while on "Protein" methods based on CG, i.e. CGGP and CaGP-CG, perform the best. However, CaGP-Opt consistently performs either best or second best and scales to over a million datapoints. These results are quite remarkable for numerous reasons. First, CaGP is comparable in runtime to SVGP on large datasets despite the fact that it incurs a linear-time computational complexity while SVGP is constant time.6 Second, while all of the methods we compare approximate the GP posterior with low-rank updates, CaGP-Opt (with

    &  &  &  &  &  &  &  &  &  \\  & & & & & & mean & std & & mean & std & \\   &  &  & CholeskyGP & LBFGS & 0.100 & 100 & -3.645 & 0.002 & 0.001 & 0.000 & 1min 3s \\  & & & SGPR & Adam & 0.100 & 268 & -2.837 & 0.087 & 0.031 & 0.022 & 27s \\  & & & LBFGS & 1.000 & 100 & -3.245 & 0.067 & 0.007 & 0.003 & 2min 14s \\  & & & SVGP & Adam & 0.100 & 1000 & -2.858 & 0.016 & 0.006 & 0.002 & 2min 25s \\  & & & COGP & LBFGS & 0.100 & 81 & -2.663 & 0.141 & 0.019 & 0.013 & 1min 12s \\  & & & CGGP-CG & Adam & 1.000 & 250 & -2.936 & 0.007 & 0.009 & 0.006 & 1min 44s \\  & & & CaGP-Opt & Adam & 1.000 & 956 & -3.384 & 0.005 & 0.004 & 0.002 & 1min 27s \\  & & & LBFGS & 0.010 & 37 & -3.449 & 0.009 & **0.002** & 0.000 & 1min 53s \\  &  &  & CholeskyGP & LBFGS & 0.100 & 100 & 100 & -3.472 & 0.012 & 0.006 & 0.007 & 7min 15s \\  & & & SGPR & Adam & 0.100 & 948 & -1.211 & 0.110 & 0.026 & 0.004 & 4min 3s \\  & & & LGFGS & 1.000 & 100 & -3.017 & 0.022 & 0.009 & 0.002 & 4min 10s \\  & & & SVGP & Adam & 0.010 & 1000 & -2.256 & 0.020 & 0.020 & 6min 41s \\  & & & COGP & LBFGS & 1.000 & 15 & -1.952 & 0.078 & 0.024 & 0.004 & 2min 6s \\  & & & CaGP-CG & Adam & 1.000 & 250 & -2.042 & 0.024 & 0.024 & 0.002 & 5min 17s \\  & & & CaGP-Opt & Adam & 1.000 & 1000 & -2.401 & 0.037 & 0.018 & 0.002 & 8min 10s \\  & & & LBFGS & 1.000 & 100 & -2.438 & 0.038 & 0.017 & 0.001 & 14min 48s \\  &  &  & SGPR & Adam & 0.100 & 993 & 0.844 & 0.006 & 0.561 & 0.005 & 10min 25s \\  & & & LBFGS & 0.100 & 96 & 0.846 & 0.006 & 0.562 & 0.005 & 6min 66s \\  & & & SVGP & Adam & 0.010 & 996 & 0.851 & 0.006 & 0.564 & 0.005 & 17min 19s \\  & & & CGGP & LBFGS & 0.100 & 35 & 0.853 & 0.006 & 0.517 & 0.004 & 20min 15s \\  & & & CaGP-CG & Adam & 1.000 & 27 & **0.020** & 0.006 & 0.542 & 0.004 & 1min 26s \\  & & & CaGP-Opt & Adam & 0.100 & 941 & 0.829 & 0.005 & 0.545 & 0.004 & 13min 48s \\  & & & LBFGS & 1.000 & 84 & 0.830 & 0.005 & 0.545 & 0.004 & 14min 29s \\ KEGGu  &  &  & SGPR & Adam & 0.100 & 143 & -0.681 & 0.025 & 0.123 & 0.002 & 2min 4s \\  & & & LBFGS & 1.000 & 100 & -0.712 & 0.028 & 0.118 & 0.003 & 8min 58s \\  & & & SVGP & Adam & 0.010 & 988 & -0.710 & 0.026 & **0.118** & 0.003 & 24min 21s \\  & & & CGGP & LBFGS & 0.100 & 30 & -0.512 & 0.034 & 0.120 & 0.003 & 33min 55s \\  & & & CaGP-CG & Adam & 1.000 & 229 & **-0.699** & 0.026 & **0.120** & 0.003 & 32min 5s \\  & & & CaGP-Opt & Adam & 1.000 & 990 & **-0.693** & 0.026 & **0.120** & 0.003 & 22min 3s \\  & & & LBFGS & 0.010 & 40 & **-0.694** & 0.026 & **0.120** & 0.003 & 22min 0s \\ Road  &  &  & SVGP & Adam & 0.001 & 998 & 0.149 & 0.007 & 0.277 & 0.002 & 2h 7min 37s \\  & & & CaGP-Opt & Adam & 0.100 & 1000 & **-0.291** & 0.011 & **0.159** & 0.003 & 2h 1min 31s \\ Power  &  &  & SVGP & Adam & 0.010 & 399 & -2.104 & 0.007 & **0.029** & 0.000 & 5h 7min 57s \\  & & & CaGP-Opt & Adam & 0.100 & 200 & **-2.103** & 0.006 & 0.030 & 0.000 & 4h 32min 48s \\  

Table 1: _Generalization error (NLL, RMSE, and wall-clock time) on UCI benchmark datasets._ The table shows the best results for all methods across learning rate sweeps, averaged across five random seeds. We report the epoch where each method obtained the lowest average test NLL, and all performance metrics (NLL, RMSE, and wall-clock runtime) are reported for this epoch. Highlighted in bold and color are the best _approximate_ methods per metric (difference \(>1\) standard deviation).

\(i=512\)) here uses half the rank of SGPR/SVGP \(m=1024\). Nevertheless, CaGP-Opt is able to substantially outperform SVGP even on spatial datasets like 3DRoad where low-rank posterior updates are often poor . These results suggest that CaGP-Opt can be a more efficient approximation than inducing point methods, and that low-rank GP approximations may be more applicable than previously thought . Figure 3 shows the NLL and RMSE learning curves for the best choice of learning rate per method. CaGP-Opt often shows a "ramp-up" phase, compared to SVGP, but then improves or matches its generalization performance. This gap is particularly large on "Road", where CaGP-Opt is initially worse than SVGP but dominates in the second half of training.

SVGP Overestimates Observation Noise and (Often) LengthscaleIn Figure S5 we show that SVGP typically learns larger observation noise than other methods as suggested by previous work  and hinted at by observations on synthetic data in Figure 1 and Figure S3(b). Additionally on larger datasets SVGP also often learns large lengthscales, which in combination with a large observation noise can lead to an oversomething effect. In contrast, CaGP-Opt generally learns lower observational noise than SVGP. Of course, learning a small observation noise, in particular, is important for achieving low RMSE and thus also NLL, and points to why we should expect CaGP-Opt to outperform SVGP. These hyperparameter results suggest that CaGP-Opt interprets more of the data as signal, while SVGP interprets more of the data as noise.

CaGP Improves Uncertainty Quantification Over SVGPA key advantage of CaGP-Opt and CaGP-CG is that their posterior uncertainty estimates capture both the uncertainty due to limited data and due to limited computation. To that end, we assess the frequentist coverage of CaGP-Opt's uncertainty estimates. We report the absolute difference between a desired coverage percentage \(\)

Figure 4: _Uncertainty quantification for CaGP-Opt and SVGP._ Difference between the desired coverage (\(95\%\)) and the empirical coverage of the GP \(95\%\) credible interval on the “Power” dataset. After training, CaGP-Opt has better empirical coverage than SVGP.

Figure 3: _Learning curves of GP approximation methods on UCI benchmark datasets._ Rows show train and test loss as a function of wall-clock time for the best choice of learning rate per method. CaGP-Opt generally displays a “ramp-up” phase early in training where performance is worse than that of SVGP. As training progresses, CaGP-Opt matches or surpasses SVGP performance.

and the fraction of data that fall into the \(\) credible interval of the CaGP-Opt posterior; i.e. \(_{}^{}=|-}}_{i= 1}^{n_{}}1(y_{i} I_{q(_{i})}^{})|\). Figure 4 compares the \(95\%\) coverage error for both CaGP-Opt and SVGP on the largest dataset ("Power"). From this plot, we see that the CaGP credible intervals are more aligned with the desired coverage. We hypothesize that these results reflect the different uncertainty properties of the methods: CaGP-Opt overestimates posterior uncertainty while SVGP is prone towards overconfidence.

## 6 Conclusion

In this work, we introduced strategies for model selection and posterior inference for computation-aware Gaussian processes, which scale linearly with the number of training data rather than quadratically. The key technical innovations being a sparse projection of the data, which balances minimizing posterior entropy and computational cost, and a scalable way to optimize kernel hyperparameters, both of which are amenable to GPU acceleration. All together, these advances enable competitive or improved performance over previous approximate inference methods on large-scale datasets, in terms of generalization and uncertainty quantification. Remarkably, our method outperforms SVGP--often considered the de-facto GP approximation standard-- even when compressing the data into a space of half the dimension of the variational parameters. Finally, we also demonstrate that computation-aware GPs avoid many of the pathologies often observed in inducing point methods, such as overconfidence and oversmoothing.

LimitationsWhile CaGP-Opt obtains the same linear time and memory costs as SGPR, it is not amenable to stochastic minibatching and thus cannot achieve the constant time/memory capabilities of SVGP. In practice, this asymptotic difference does not result in substantially different wall clock times, as SVGP requires many more optimizer steps than CaGP-Opt due to batching. (Indeed, on many datasets we find that CaGP-Opt is faster.) CaGP-Opt nevertheless requires larger GPUs than SVGP on datasets with more than a million data points. Moreover, tuning CaGP-Opt requires choosing the appropriate number of actions (i.e. the rank of the approximate posterior update), though we note that most scalable GP approximations have a similar tunable parameter (e.g. number of inducing points). Perhaps the most obvious limitation is that CaGP, unlike SVGP, is limited to GP regression with a conjugate observational noise model. We leave extensions to classification and other non-conjugate likelihoods as future work.

Outlook and Future WorkAn immediate consequence of this work is the ability to apply computation-aware Gaussian processes to "real-world" problems, as our approach solves CaGP's open problems of model selection and scalability. Looking forward, an exciting future vision is a general framework for problems involving a Gaussian process model with a downstream task where the actions are chosen optimally, given resource constraints, to solve said task. Future work will pursue this direction beyond Gaussian likelihoods to non-conjugate models and downstream tasks such as Bayesian optimization.