# Attention as Implicit Structural Inference

Ryan Singh

School of Engineering and Informatics,

University of Sussex.

rs773@sussex.ac.uk

&Christopher L. Buckley

School of Engineering and Informatics,

University of Sussex.

VERSES AI Research Lab,

Los Angeles, CA, USA.

###### Abstract

Attention mechanisms play a crucial role in cognitive systems by allowing them to flexibly allocate cognitive resources. Transformers, in particular, have become a dominant architecture in machine learning, with attention as their central innovation. However, the underlying intuition and formalism of attention in Transformers is based on ideas of keys and queries in database management systems. In this work, we pursue a structural inference perspective, building upon, and bringing together, previous theoretical descriptions of attention such as; Gaussian Mixture Models, alignment mechanisms and Hopfield Networks. Specifically, we demonstrate that attention can be viewed as inference over an implicitly defined set of possible adjacency structures in a graphical model, revealing the generality of such a mechanism. This perspective unifies different attentional architectures in machine learning and suggests potential modifications and generalizations of attention. Here we investigate two and demonstrate their behaviour on explanatory toy problems: (a) extending the value function to incorporate more nodes of a graphical model yielding a mechanism with a bias toward attending multiple tokens; (b) introducing a geometric prior (with conjugate hyper-prior) over the adjacency structures producing a mechanism which dynamically scales the context window depending on input. Moreover, by describing a link between structural inference and precision-regulation in Predictive Coding Networks, we discuss how this framework can bridge the gap between attentional mechanisms in machine learning and Bayesian conceptions of attention in Neuroscience. We hope by providing a new lens on attention architectures our work can guide the development of new and improved attentional mechanisms.

## 1 Introduction

Designing neural network architectures with favourable inductive biases lies behind many recent successes in Deep Learning. The Transformer, and in particular the attention mechanism has allowed language models to achieve human like generation abilities previously thought impossible . The success of the attention mechanism as a domain agnostic architecture has prompted adoption across a diverse range of tasks beyond language modelling, notably reaching state-of-the-art performance in visual reasoning and segmentation tasks .

This depth and breadth of success indicates the attention mechanism expresses a useful computational primitive. Recent work has shown interesting theoretical links to kernel methods , Hopfield networks , and Gaussian mixture models , however a formal understanding that captures the generality of this computation remains outstanding. In this paper, we show the attention mechanism can naturally be described as inference on the structure of a graphical model, agreeing with observations that transformers are able to flexibly choose between models based on context . This Bayesian perspective complements previous theory , adding newmethods for reasoning about inductive biases and the functional role of attention variables. Further, understanding the core computation as inference permits a unified description of multiple attention mechanisms in the literature as well as narrowing the explanatory gap to ideas in neuroscience.

This paper proceeds in three parts: First in Sec.3, we show that'soft' attention mechanisms (e.g. self-attention, cross-attention, graph attention, which we call _transformer attention_ hereafter) can be understood as taking an expectation over possible connectivity structures, providing an interesting link between softmax-based attention and marginal likelihood. Second in Sec.4, we extend the inference over connectivity to a Bayesian setting which, in turn, provides a theoretical grounding for iterative attention mechanisms (slot-attention and block-slot attention) [17; 18; 19], Modern Continuous Hopfield Networks  and Predictive Coding Networks. Finally in Sec.5, we leverage the generality of this description in order to design new mechanisms with predictable properties.

Intuitively, the attention matrix can be seen as the posterior distribution over edges \(E\) in a graph, \(=(K Q,E)\) consisting of a set of query and key nodes \(Q,K\) each of dimension \(d\). Where the full mechanism computes an expectation of a function defined on the graph \(V:^{d||}\) with respect to this posterior.

\[Attention(Q,K,V) =W_{K}^{T}K^{T}}{})}^{p (E Q,K)}W_{V}K\] \[=_{p(E|Q,K)}[V]\]

Crucially, when \(\) is seen as a graphical model, the posterior over edges becomes an inference about dependency structure and the functional form becomes natural. This formalism provides an alternate Bayesian theoretical framing within which to understand attention models, shifting the explanation from one centred around retrieval to one that is fundamentally concerned with in-context inference of probabilistic relationships (including retrieval). Within this framework different attention architectures can be described by considering different implicit probabilistic models, by making these explicit we hope to support more effective analysis and the development of new architectures.

## 2 Related Work

A key benefit of the perspective outlined here is to tie together different approaches taken in the literature. Specifically, structural variables can be seen as the alignment variables discussed in previous Bayesian descriptions [16; 20; 21], on the other hand Gaussian Mixture Models (GMMs) can be seen as a specific instance of the framework developed here. This description maintains the explanatory power of GMMs by constraining the alignment variables to be the edges of an implicit graphical model, while offering the increased flexibility of alignment approaches to describe multiple forms of attention.

**Latent alignment and Bayesian Attention,** several attempts have been made to combine the benefits of soft (differentiability) and stochastic attention, often viewing attention as a probabilistic alignment problem. Most approaches proceed by sampling, e.g., using the REINFORCE estimator  or a \(topK\) approximation . Two notable exceptions are  which embeds an inference algorithm within the forward pass of a neural network, and  which employs the re-parameterisation trick for the alignment variables. In this work, rather than treating attention weights as an independent learning problem, we aim to provide a parsimonious implicit model that would give rise to the attention weights. Additionally showing that'soft' attention weights arise naturally in variational inference from either collapsed variational inference or a mean-field approximation.

**Relationship to Gaussian mixture model,** previous works that have taken a probabilistic perspective on the attention mechanism note the connection to inference in a gaussian mixture model [11; 10; 12; 13]. Indeed  directly show the connection between the Hopfield energy and the variational free energy of a Gaussian mixture model. Although Gaussian mixture models, a special case of the framework we present here, are enough to explain cross attention they do not capture slot or self-attention, obscuring the generality underlying attention mechanisms. In contrast, the description presented here extends to structural inductive biases beyond what can be expressed in a Gaussian mixture model, additionally offering a route to describing the whole transformer block.

**Attention as bi-level optimisation,** mapping feed-forward architecture to a minimisation step on a related energy function has been called unfolded optimisation . Taking this perspective can lead to insights about the inductive biases involved for each architecture. It has been shown that the cross-attention mechanism can be viewed as an optimisation step on the energy function of a form of Hopfield Network , providing a link between attention and associative memory. while  extend this view to account for self-attention. Our framework distinguishes Hopfield attention, which does not allow an arbitrary value matrix, from transformer attention. Although there remains a strong theoretical connection, we interpret the Hopfield Energy as an instance of variational free energy, aligning more closely with iterative attention mechanisms such as slot-attention.

## 3 Transformer Attention

### Attention as Expectation

We begin by demonstrating transformer attention can be seen as calculating an expectation over graph structures. Specifically, let \(x=(x_{1},..,x_{n})\) be observed input variables, \(\) be some set of discrete latent variables representing edges in a graphical model of \(x\) given by \(p(x)\), and \(y\) a variable we need to predict. Our goal is to find \(_{y|x}[y]\), however the graph structure \(\) is unobserved so we calculate the marginal likelihood.

\[_{y|x}[y]=_{}p( x)_{y|x,}[y]\]

Importantly, the softmax function is a natural representation for the posterior,

\[p( x)=p(x,)}=softmax( p(x,))\]

in order to expose the link to transformer attention, let the model of \(y\) given the graph (\(x\), \(\)) be parameterised by a function \(_{y|x,}[y]=v(x,)\).

\[_{y|x}[y]=_{}softmax( p(x,))v(x,)=_{ |x}[v(x,)]\] (1)

In general, transformer attention can be seen as weighting \(v(x,)\) by the posterior distribution \(p( x)\) over different graph structures. We show Eq.1 is exactly the equation underlying self and cross-attention by presenting the specific generative models corresponding to them. In this description the latent variables \(\) are identified as edges between observed variables \(x\) (keys and queries) in a pairwise Markov Random Field, parameterised by matrices \(W_{K}\) and \(W_{Q}\), while the function \(v\) is parameterised by \(W_{V}\).

**Pairwise Markov Random Fields** are a natural tool for modelling the dependencies of random variables, with prominent examples including Ising models (Boltzmann Machine) and multivariate Gaussians. While typically defined given a known structure, the problem of inferring the latent graph is commonly called structural inference.

Formally, given a set of random variables \(X=(X_{v})_{v V}\) with probability distribution \([p]\) and a graph \(G=(V,E)\). The variables form a pairwise Markov Random Field (pMRF)  with respect to \(G\) if the joint density function \(P(X=x)=p(x)\) factorises as follows

\[p(x)=(_{v V}_{v}+_{e E}_{e})\]

where \(Z\) is the partition function \(_{v}(x_{v})\) and \(_{e}=_{u,v}(x_{u},x_{v})\) are known as the node and edge potentials respectively. Bayesian structural inference also requires a structural prior \(p()\) over the space of possible adjacency structures, \(\), of the underlying graph.

**Factorisation,** without constraints this space grows exponentially in the number of nodes (\(2^{|V|}\) possible graphs leading to intractable softmax calculations), all the models we explore here implicitly assume a factorised prior1. We briefly remark that Eq.1 respects factorisation of \([p]\) in the followingsense; if the distribution admits a factorisation (a partition of the space of graphs \(=_{i}_{i}\)) with respect to the latent variables \(p(x,)=_{i}e^{f_{i}(x,_{i})}\) where \(_{i}_{i}\), and the value function distributes over the same partition of edges \(v(x,)=_{i}v_{i}(x,_{i})\) then each of the factors can be marginalised independently:

\[_{|x}[v(x,)]=_{i}_{_{i}|x}[v_{i}]\] (2)

To recover cross-attention and self-attention we need to specify the structural prior, potential functions and a value function. (In order to ease notation, when \(_{i}\) is a set of edges involving a common node \(x_{i}\), such that \(_{i}=(x_{i},x_{j})\) represents a single edge, we use the notation \(_{i}=[j]\), suppressing the shared index.)

### Cross Attention and Self Attention

We first define the model that gives rise to cross-attention:

* Key nodes \(K=(x_{1},..,x_{n})\) and query nodes \(Q=(x^{}_{1},...,x^{}_{m})\)
* Structural prior \(p()=_{i=1}^{m}p(_{i})\), where \(_{i}=\{(x_{1},x^{}_{i}),..,(x_{n},x^{}_{i})\}\) is the set of edges involving \(x^{}_{i}\) and \(_{i} Uniform(_{i})\) such that each query node is uniformly likely to connect to each key node.
* Edge potentials \((x_{j},x^{}_{i})=x^{ T}_{i}W^{T}_{Q}W_{K}x_{j}\), in effect measuring the similarity of \(x_{j}\) and \(x^{}_{i}\) in a projected space.
* Value functions \(v_{i}(x,_{i}=[j])=W_{V}x_{j}\), a linear transformation applied to the node at the start of the edge \(_{i}\).

Taking the expectation with respect to the posterior in each of the factors defined in Eq.2 gives the standard cross-attention mechanism,

\[_{p(_{i}|Q,K)}[v_{i}]=_{j}softmax_{j}(x^{ T}_{i}W^{T}_ {Q}W_{K}x_{j})W_{V}x_{j}\]

If the key nodes are in fact the same as the query nodes and the prior is instead over a directed graph we recover self-attention (A.8.1).

## 4 Iterative Attention

We continue by extending attention to a latent variable setting, where not all the nodes are observed. In essence applying the attention trick, i.e., a marginalisation of structural variables, to a variational free energy (Evidence Lower Bound). This allows us to recover models such as slot attention  and block-slot attention . These mechanisms utilise an EM-like procedure using the current estimation of latent variables to infer the structure and then using the inferred structure to improve estimation of latent variables. Interestingly, Modern Continuous Hopfield Networks fit within this paradigm rather than the one discussed in Sec.3; collapsed variational inference produces an identical energy function to the one proposed by Ramsauer et al. .

### Collapsed Inference

We present a version of collapsed variational inference , where the collapsed variables \(\) are again structural, showing how this results in a Bayesian attention mechanism. In contrast to the previous section, we have a set of (non-structural) latent variables \(z\). The goal is to infer \(z\) given the observed variables, \(x\), and a latent variable model \(p(x,z,)\). Collapsed inference proceeds by marginalising out the extraneous latent variables \(\):

\[p(x,z)=_{}p(x,z,)\] (3)

We define a gaussian recognition density \(q(z) N(z;,)\) and optimise the variational free energy \(()=_{q}[ q_{}(z)- p(x,z)]\) with respect to the parameters, \(=(,)\), of this distribution. Application of Laplace's method yields approximate derivatives of the variational free energy \(_{}-_{} p(x,)\) and \(_{}-_{}^{2} p(x,)\), here we focus on the first order terms 2. Substituting in Eq.3:

\[_{} -_{}_{}p(x,,)\] (4) \[=-p(x,,)}_{}_{}p(x, ,)\] (5)

In order to make the link to attention, we employ the log-derivative trick, substituting \(p()=e^{ p()}\) and re-express Eq.5 in two ways:

\[=-_{}softmax_{}( p(x,,))_{} p(x, ,)\] (6) \[=_{|x,}[-_{} p(x,,)]\] (7)

The first form reveals the softmax which is ubiquitous in all attention models. The second, suggests the variational update should be evaluated as the expectation of the typical variational gradient (the term within the square brackets) with respect to the posterior over the parameters represented by the random variable \(\). In other words, iterative attention is exactly transformer attention applied iteratively where the value function is the variational free energy gradient. We derive updates for a general pMRF before again recovering (iterative) attention models in the literature by specifying particular distributions.

**Free Energy of a marginalised pMRF,** recall the factorised pMRF, \(p(x,)=_{i}e^{f_{i}(x,_{i})}\). Again, independence properties simplify the calculation, the marginalisation can be expressed as a product of local marginals, \(_{}p(x,)=_{i}_{_{i}}e^{f_{i}(x,_{i})}\). Returning to the inference setting, the nodes are partitioned into observed nodes, \(x\), and variational parameters \(\). Hence the (approximate) collapsed variational free energy Eq.5, can be expressed as, \(F(x,)=-_{i}_{_{i}}e^{f_{i}(x,,_{i})}+C\) and it's derivative:

\[}=-_{i}_{_{i}}softmax(f_{i}) }{_{j}}\]

Finally, we follow  in using the Convex-Concave Procedure (CCCP) to derive a simple fixed point equation which necessarily reduces the free energy.

**Quadratic Potentials and the Convex Concave Procedure,** assuming the node potentials are quadratic \((x_{i})=-x_{i}^{2}\) and the edge potentials have the form \((x_{i},x_{j})=x_{i}Wx_{j}\), and define \(_{i}=_{e_{i}}_{e}\). Consider the following fixed point equation,

\[_{j}^{*}=_{i}_{_{i}}softmax(_{i})_{i}}{_{j}}\] (8)

Figure 1: Comparison of models involved in different attention mechanisms. In each case, the highlighted edges indicate \(_{i}\) the support of the uniform prior over \(_{i}\). Attention proceeds by calculating a posterior over these edges, given the current state of the nodes, before using this inference to calculate an expectation of the value function \(v\). For iterative attention mechanisms the value function can be identified as the gradient of a variational free energy, in contrast, transformer attention uses a learnable function.

since (under mild conditions) node potentials are convex and edge potentials are concave (A.7.1.1), we can invoke the CCCP  to show this fixed point equation descends on the energy \(F(x,_{j}^{*}) F(x,_{j})\) with equality if and only if \(_{j}^{*}\) is a stationary point of \(F\). We follow Sec.3 in specifying specific structural priors and potential functions that recover different iterative attention mechanisms.

### Modern Continuous Hopfield Network

Let the observed, or memory, nodes \(x=(x_{1},..,x_{n})\) and latent nodes \(z=(z_{1},..,z_{m})\) have the following structural prior \(p()=_{i=1}^{m}p(_{i})\), where \(_{i} Uniform\{(x_{1},z_{i}),..,(x_{n},z_{i})\}\), meaning each latent node is uniformly likely to connect to a memory node. Define edge potentials \((x_{j},z_{i})=z_{i}^{T}x_{j}\). Application of Eq.8:

\[_{i}^{*}=_{j}softmax_{j}(_{i}^{T}x_{j})x_{j}\]

When \(_{i}\) is initialised to some query \(\) the system the fixed point update is given by \(_{i}^{*}()=_{_{i}|x,}[x_{[j]}]\). If the patterns \(x\) are well separated, \(_{i}^{*}() x_{j^{}}\), where \(x_{j^{}}\) is the closest vector and hence can be used as an associative memory.

### Slot Attention

Slot attention  is an object centric learning module centred around an iterative attention mechanism. Here we show this is a simple adjustment of the prior beliefs on our edge set. With edge potentials of the form \((x_{j},z_{i})=z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\), replace the prior over edges with \(p()=_{j=1}^{n}p(_{j})\), \(_{j} Uniform\{(x_{j},z_{1}),..,(x_{j},z_{m})\}\). Notice, in comparison to MCHN, the prior over edges is swapped, each observed node is uniformly likely to connect to a latent node, in turn altering the index of the softmax.

\[_{i}^{*}=_{j}softmax_{i}(_{i}^{T}W_{Q}^{T}W_{K}x_{j})W_{Q}^{T}W_{K}x _{j}\]

while the original slot attention employed an RNN to aid the basic update shown here, the important feature is that the softmax is taken over the'slots'. This forces competition between slots to account for the observed variables, creating object centric representations.

### Predictive Coding Networks

Predictive Coding Networks (PCN) have emerged as an influential theory in Computational Neuroscience [29; 30; 31]. Building on theories of perception as inference and the Bayesian brain, PCNs perform approximate Bayesian inference by minimising a variational free energy of a graphical model, where incoming sensory data are used as observations. Typical implementations use a hierarchical model with Gaussian conditionals, resulting in a local prediction error minimising scheme. The minimisation happens on two distinct time-scales, which can be seen as E-step and M-steps on the variational free energy: a (fast) inference phase encoded by neural activity corresponding to perception and a (slow) learning phase associated with synaptic plasticity. Gradient descent on the free energy gives the inference dynamics for a particular neuron \(_{i}\), 

\[}{_{i}}=-_{^{-}}k_{}_ {}+_{^{+}}k_{}_{}w_{}\]

Where \(\) are prediction errors, \(w\) represent synaptic strength, \(k\) are node specific precisions representing uncertainty in the generative model and \(^{-},^{+}\) represent pre-synaptic and post-synaptic terminals resectively. Applying a uniform prior over the incoming synapses results in a slightly modified dynamics,

\[}{_{i}}=-_{^{-}}softmax(-{ _{}}^{2})k_{}_{}+_{^{+}}softmax(-{_{} }^{2})k_{}_{}w_{}\]

where the softmax function induces a normalisation across prediction errors received by a neuron. This dovetails with theories of attention as normalisation in Psychology and Neuroscience [33; 34; 35]. In contrast previous predictive coding based theories of attention have focused on the precision terms, \(k\), due to their ability to up and down regulate the impact of prediction errors [36; 37; 38]. Here we see the softmax terms play a functionally equivalent role to precision variables, inheriting their ability to account for bottom-up and top-down attention, while exhibiting the fast winner-takes-all dynamics that are associated with cognitive attention.

## 5 New Designs

By identifying the attention mechanism in terms of an implicit probabilistic model, we can review and modify the underlying modelling assumptions in a principled manner to design new attention mechanisms. Recall transformer attention can be written as the marginal probability \(p(y x)=_{}p( x)_{y|x,}[y]\), the specific mechanism is therefore informed by three pieces of data: (a) the value function \(p(y x,)\), (b) the likelihood \(p(x)\) and (c) the prior \(p()\). Here, we explore modifying (a) and (c) and show they can exhibit favourable biases on toy problems.

### Multi-hop Attention

Our description makes it clear that the value function employed by transformer attention can be extended to any function over the graph. For example, consider the calculation of \(_{y|x,}[y_{i}]\) in transformer attention, a linear transformation is applied to the most likely neighbour, \(x_{j}\), of \(x_{i}\). A natural extension is to include a two-hop neighbourhood, additionally using the most likely neighbour \(x_{k}\) of \(x_{j}\). The attention mechanism then takes a different form \(_{p(_{i}|_{i})p(_{i}|x)}[V(x_{_{i}}+x_{_{j}})]= (P_{}+P_{}^{2})VX\), where \(P_{}\) is the typical attention matrix. While containing the same number of parameters as a single-layer of transformer attention, for some datasets two-hop attention should be able to approximate the behaviour of two-layers of transformer attention.

**Task Setup** We simulate a simple dataset that has this property using the following data generation process: Initialise a projection matrix \(W_{y}^{d 1}\) and a relationship matrix \(W_{r}^{d d}\). \(X\) is then generated causally, using the relationship \(x_{i+1}=W_{r}x_{i}+N(0,)\) to generate \(x_{0}\), \(x_{1}\) and \(x_{2}\), while the remaining nodes are sampled from the noise distribution \(N(0,)\). Finally, the target \(y\) is generated from the history of \(x_{2}\), \(y=W_{y}(x_{1}+x_{0})\) and the nodes of \(X\) are shuffled. Importantly \(W_{r}\) is designed to be low rank, such that performance on the task requires paying attention to both \(x_{1}\) and \(x_{0}\), Figure 2.

### Expanding Attention

One major limitation of transformer attention is the reliance on a fixed context window. From one direction, a small context window does not represent long range relationships, on the other hand a large window does an unnecessary amount of computation when modelling a short range relationship. By replacing the uniform prior with a geometric distribution \(p( q) Geo(q)\)

Figure 2: Multihop Attention: (left) Graphical description of the toy problem, \(x_{2}\) is generated causally from \(x_{1}\) and \(x_{0}\), which are used to generate \(y\). (centre) Comparison of the attention employed by Multihop which takes two steps on the attention graph (top) contrasted with Self Attention (bottom). Multihop Attention has the correct bias to learn the task approaching the performance of two-layer Self Attention, while a single layer of Self Attention is unable (top right). Empirically examining the attention weights, Multihop Attention is able to balance attention across two positions, while self-attention favours a single position.

and a conjugate hyper-prior \(p(q) Beta(,)\) we derive a mechanism that dynamically scales depending on input. We use a (truncated) mean-field variational inference procedure  to iteratively approximate \(p( x)\) using the updates: 1. \(q_{t}}{_{t}+_{t}}\), 2. \(p_{t}=p( x,q_{t})\), 3. \(_{t+1}_{t}+1\), \(_{t+1}_{t}+_{<H(q_{t})}i(p_{t})_{i}\). Where \(\) and \(\) are hyperparameters determining the strength of the prior and \(H\) is the truncation horizon. Since attention dot products can be cached and reused for each calculation of step 2. the iterative procedure is computationally cheap.

The attention mechanism has asymptotic time complexity \(O(n^{2}d)\) where \(n\) is the size of the size of the context window and \(d\) is dimension over which the inner product is computed. In comparison, expanding attention \(O(n(md+k))\) where \(m\) is the size of the window at convergence, and \(k\) is the number of steps to converge. If, as is typical, \(d\) is large such that \(d>>k\) the time complexity of expanding attention should be favourable.

**Task Setup** Input and target sequence are generated similarly to above (without \(x_{0}\)). Here \(x_{1}\) is moved away from \(x_{2}\) according to a draw from a geometric distribution, Figure 3.

## 6 Discussion

### The Full Transformer Block

Transformer attention is typically combined with residual connections and a feedforward network, both of which have been shown important in preventing 'token collapse'. Here we briefly touch upon how these features might relate to the framework presented here.

**Feedforward layer,** it has previously been noticed the feedforward component can also be understood as a key-value memory where the memories are stored as persistent weights [40; 41]. This is due to the observation \(ff(x)=W_{2}(W_{1}x)\) is equivalent to attention when the non-linearity \(\) is a softmax, although a ReLU is typically used. We speculate the framework presented here could be extended explain this discrepancy, intuitively the ReLU relates to an edge prior that fully factorises into binary variables.

**Residual connections** have been shown to encourage iterative inference . This raises the possibility transformer attention, rather than having an arbitrary transformation \(v\) as presented in Sec.3, is in fact approximately implementing the iterative inference of Sec.4 through a form of iterative amortised inference . The view that the transformer is performing iterative refinement is additionally supported by empirical studies of early-decoding .

**Temperature and positional encodings,** both positional encodings and the temperature scaling can be seen as adjustments to the prior edge probability. In the case of relative positional encodings, by breaking the permutation invariance of the prior (A.8.2). While the temperature may be understood

Figure 3: Expanding Attention: (left) Graphical description of the toy problem, \(x_{2}\) and \(y\) are generated from \(x_{1}\) which is shuffled with a (exponentially decaying) recency bias. (centre) Comparison of the geometric prior, with different shades of red representing the iterative refinements during inference, used by Expanding and uniform prior used by Self Attention. (right) The relative number of operations used by Expanding Attention is beneficial when either the recency bias (\(1/p\)) or the number of feature dimensions (\(d\)) is large, training curves (overlaid) across each of these settings remained roughly equivalent.

in terms of tempered (or generalised) Bayesian inference , adjusting the strength of the prior relative to the likelihood.

### Limitations

The connection to structural inference presented here is limited to the attention computation of a single transformer head, an interesting future direction would be to investigate whether multiple layers and multiple heads typically used in a transformer can also be interpreted within this framework. Additionally, the extension to iterative inference employed a crude approximation to the variational free energy, arguably destroying the favourable properties of Bayesian methods. Suggesting the possibility of creating iterative attention mechanisms with alternative inference schemes, possibly producing more robust mechanisms.

### Conclusion

In this paper, we presented a probabilistic description of the attention mechanism, formulating attention as structural inference within a probabilistic model. This approach builds upon previous research that connects cross attention to inference in a Gaussian Mixture Model. By considering the discrete inference step in a Gaussian Mixture Model as inference on marginalised structural variables, we bridge the gap with alignment-focused descriptions. This framework naturally extends to self-attention, graph attention, and iterative mechanisms, such as Hopfield Networks. We hope this work will contribute to a more unified understanding of the functional advantages and disadvantages brought by Transformers.

Furthermore, we argue that viewing Transformers from a structural inference perspective provides different insights into their central mechanism. Typically, optimising structure is considered a learning problem, changing on a relatively slow timescale compared to inference. However, understanding Transformers as fast structural inference suggests that their remarkable success stems from their ability to change effective connectivity on the same timescale as inference. This general idea can potentially be applied to various architectures and systems. For instance, Transformers employ relatively simple switches in connectivity compared to the complex dynamics observed in the brain . Exploring inference over more intricate structural distributions, such as connectivity motifs or modules in network architecture, could offer artificial systems even more flexible control of resources.