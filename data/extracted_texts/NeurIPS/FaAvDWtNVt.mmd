# N Multipliers for N Bits: Learning Bit Multipliers for Non-Uniform Quantization

Raghav Singhal  Anmol Biswas\({}^{}\)  Sivakumar Elangovan  Shreyas Sabnis

&Udayan Ganguly

Indian Institute of Technology Bombay, India

anmolbiswas@gmail.com\({}^{}\)

###### Abstract

Effective resource management is critical for deploying Deep Neural Networks (DNNs) in resource-constrained environments, highlighting the importance of low-bit quantization to optimize memory and speed. In this paper, we introduce N-Multipliers-for-N-Bits, a novel method for non-uniform quantization designed for efficient hardware implementation. Our method uses N parameters, distinct for every layer and corresponding to the N quantization bits, whose linear combinations span the set of allowed weights (and activations). Furthermore, we learn these parameters in conjunction with the weights, ensuring exceptional flexibility in the quantizer model with minimal hardware overhead. We validate our method on CIFAR-10 and ImageNet, achieving competitive results with 3- and 4-bit quantized models. We demonstrate strong performance on 4-bit quantized Spiking Neural Networks (SNNs), evaluated on the CIFAR10-DVS and N-Caltech 101 datasets. Further, we address the issue of stuck-at faults in hardware, and demonstrate robustness to up to 30\(\%\) faulty bits.

## 1 Introduction

Deep learning dominates computer vision and broader AI applications, where cloud-based models perform inference by transferring data to servers. While effective, this approach is inefficient in terms of data transfer and power consumption. A more efficient alternative, especially for simple tasks, is edge inference using low-power accelerators with fixed-point arithmetic and in-memory or near-memory computing architectures . These architectures, such as crossbar arrays, perform matrix-vector multiplication by accumulating parallel operations. They can be implemented using analog components or digital ones, but both approaches encounter a trade-off between energy efficiency and performance . Energy efficiency in edge devices often comes at the cost of circuit non-idealities such as line resistance and device variability  and hardware faults such as stuck-at (SA) faults , where certain weight bits get stuck at either 0 or 1 and become unprogrammable. Addressing both quantization errors and hardware faults is crucial for optimizing edge inference.

Low-bit quantization for weights and activations has been extensively explored through quantization-aware training (QAT). Popular methods in the literature include regularization-based QAT, like sine-regularization  and bin-regularization , and learned quantization scale . Non-uniform methods, such as learning quantization levels or companding functions , offer flexibility by learning key parameters. However, these works are not generally extendable to the specific challenges of low-power neuromorphic accelerators and event-driven Spiking Neural Networks (SNNs) .

In this work, we propose a QAT scheme that optimizes bit-multipliers for each quantization level using a regularization-based approach using Mean Squared Error (MSE) loss. This enables the generalization of our method beyond just QAT for Artificial Neural Networks and into QAT for Spiking Neural Networks and non-ideality and hardware fault mitigation for low-power neuromorphic accelerators. We match the state-of-the-art performance in QAT for 4-bit networks on CIFAR-10  and ImageNet  benchmarks and show excellent performance in 4-bit SNNs on CIFAR10-DVS  and N-Caltech 101  neuromorphic benchmarks. Finally, we also show robust training for low-bit quantized models even with a high rate of hardware faults (up to \(30\%\)).

Our key contributions are summarized as follows:

* We introduce a novel, flexible, and hardware-compatible quantization framework that learns N bit multipliers per layer alongside network weights, enabling adaptable precision with minimal hardware overhead, while spanning a rich set of quantization levels.
* We show our method's effectiveness across multiple networks and datasets, achieving comparable state-of-the-art results for 3- and 4-bit DNNs on CIFAR-10  and ImageNet , and 4-bit SNNs on event-based datasets: CIFAR10-DVS  and N-Caltech 101 .
* We propose a fault-tolerant quantization method that enables low-bit models to maintain performance up to \(30\%\) faulty bits, as demonstrated on CIFAR-10, enhancing robustness.
* We propose a custom implementation of bit-level multipliers for analog/digital crossbars, optimized for our quantization scheme and directly portable to neuromorphic hardware.

## 2 Methodology

**Preliminaries:** Quantization aims to replace floating-point weights and activations in DNNs with low-bit representations to reduce memory usage and speed up computations. A general N-bit quantizer function will have \(2^{N}\) levels, say \(l_{1},l_{2},,l_{2^{N}}\), \(2^{N}-1\) transition thresholds, say \(t_{1},t_{2},,t_{2^{N}-1}\), and is defined as follows:

\[Q(x)=l_{1}&x<t_{1}\\ l_{i}&t_{i-1} x<t_{i}, i=2,3,,2^{N}-1\\ l_{2^{N}}&x t_{2^{N}-1}\] (1)

**Quantizer Model:** We introduce an \(N\)-dimensional learnable vector \(r^{N}\), which defines the N bit multipliers, alongside a scalar offset \(c\) in our quantizer model. The set of allowed quantized weights or activations is given by:

\[W_{r}=\{ r,b+c b\{0,1\}^{N}\}\] (2)

The quantization function maps each full-precision weight to its nearest quantized counterpart:

\[=Q(x,r)=_{w_{q} W_{r}} x-w_{q}\] (3)

This design enables a flexible non-uniform quantizer with multiple step sizes, offering hardware efficiency while preserving the structure of N-bit quantization. Although learning all \(2^{N}\) quantization levels would offer maximum flexibility, it would undermine hardware efficiency and the core benefits of N-bit quantization. Figure 0(a) illustrates a sample quantizer function. Drawing parallels between a general N-bit quantizer and the one introduced above, we can see that the elements of the set \(W_{r}\) serve as the levels, \(l_{1},l_{2},,l_{2^{N}}\), and the transition thresholds are defined as \(t_{i}=(l_{i}+l_{i+1})/2\).

**Loss and Learning:** We jointly optimize the bit multipliers, offsets, and weights by introducing an additional quantization-aware loss alongside the standard cross-entropy loss. This allows the model parameters to be optimized through backpropagation within the usual training pipeline. During training, the weights remain in full precision but gradually align with their quantized counterparts due to the influence of the quantization-aware loss. The actual quantization is applied post-training, where the full-precision weights are mapped to their nearest quantized values.

**Quantization-Aware Loss:** We define a regularization loss that minimizes the squared error between each weight and its nearest quantized value. To balance gradient contributions across layers, we introduce a layer-specific scaling factor. The total loss is formulated as:

\[=_{CE}+_{l=1}^{L}_{l}_{i=1}^{n_{l}} _{w_{q} W_{r}^{l}}|\ w_{i}-w_{q}\ |^{2}\] (4)

where \(_{CE}\) is the cross-entropy loss and \(W_{r}^{l}\) represents the set of quantized weights for layer \(l\), defined by parameters \(r^{l}\) and \(c^{l}\). The term \(_{l}\) is a layer-wise scaling factor, and \(\) controls the regularization strength. Following other works, we set \(_{l}\) as \(}{{}}}\), where \(Q_{P}\) is \(2^{b}-1\) for activations (unsigned data) and \(2^{b-1}-1\) for weights (signed data), respectively; \(b\) denotes the number of bits. Figure 0(b) illustrates the regularization loss for a sample weight using an arbitrary vector \(r\) to define the quantized weight set. Equivalently, the loss can be expressed as a function of the weights and bit multipliers. This formulation jointly optimizes the overall objective and the quantization parameters, including the bit multipliers and offsets that define the quantization function itself.

\[=_{CE}+_{l=1}^{L}_{l}_{i=1}^{n_{l}} |\ w_{i}-Q(w_{i},r^{l})\ |^{2}\] (5)

**Gradient Calculation:** The gradient calculation for the weights and quantizer parameters is straightforward. Since we use full precision weights throughout the training, we can simply define \(=0\), thereby eliminating the need of any gradient approximation techniques. For the quantizer parameters, \(=1\) and \(_{r}Q(w,r)=B_{r}(Q(w,r))\), where \(B_{r}\) is an inverse map defined as \(B_{r}:W_{r}\{0,1\}^{N}\), providing the bit representation vector of the quantized weights. This encoding function satisfies \(w_{q}= r,B_{r}(w_{q})+c w_{q} W_{r}\). The gradients for the weights, bit multipliers, and offsets are calculated as follows:

\[=}{ w}+2 _{l}(w-Q(w,r^{l}))\] (6)

\[}=2_{l}_{i=1}^{n_{l}}(w _{i}-Q(w_{i},r^{l})) B_{r}(Q(w_{i},r^{l}))\] (7)

\[}=2_{l}_{i=1}^{n_{l}}(w _{i}-Q(w_{i},r^{l}))(-,r^{l})}{ c^{l}})=2 _{l}_{i=1}^{n_{l}}(Q(w_{i},r^{l})-w_{i})\] (8)

**SNN Training:** SNNs inherently produce _quantized activations_ in the form of _spike trains_, we thus need to solely quantize the weights of the network. We use a Leaky Integrate-and-Fire (LIF) model  for the spiking neuron in our SNN models. These discrete-time equations describe its dynamics:

\[H[t] =V[t-1]+(X[t]-(V[t-1]-V_{reset}))\] (9) \[S[t] =(H[t]-V_{th})\] (10) \[V[t] =H[t]\ (1-S[t])+V_{reset}\ S[t]\] (11)

Figure 1: **(a)** Non-uniform quantizer model demonstrating the learned bit-multiplier quantization function. **(b)** QAT loss with (MSE) regularization (shown in **red**) to minimize quantization error.

where \(X[t]\) denotes the input current at time step \(t\). \(H[t]\) denotes the membrane potential following neural dynamics and \(V[t]\) denotes the membrane potential after a spike at step \(t\), respectively. The model uses a firing threshold \(V_{th}\) and utilizes the Heaviside step function \((x)\) to determine spike generation. The output spike at step \(t\) is denoted by \(S[t]\), while \(V_{reset}\) represents the reset potential following a spike. The membrane decay constant is denoted by \(\). To facilitate error backpropagation, we use the surrogate gradient method , defining \(^{}(x)^{}(x)\), where \((x)\) is the arctan surrogate function . The remaining part of the training and quantization follows that of the non-spiking networks described earlier.

**Fault-Aware Modification:** We propose a two-pronged approach to address SA faults in quantized neural networks. Firstly, we enhance fault awareness during training by periodically (every 4 epochs) loading faulty weights onto the model. Secondly, we introduce a fault-aware modification to our algorithm, designed to avoid weight configurations rendered impossible by SA faults. We introduce a _validity_ term that constrains weights to only those quantization levels that are achievable, avoiding those rendered unreachable by faulty bits. The _validity_ term is defined for each layer as a binary map that indicates whether a specific weight can attain a given quantization level (1 if achievable, 0 otherwise). This allows us to modify the quantization-aware training loss in Equation 4 as follows:

\[=_{CE}+_{l=1}^{L}_{l}_{i=1}^{n_{l}} _{w_{q} W_{r}^{l}}(val_{i,q}^{l} w_{i}-w_{q}^{2}+(1-val_{i,q}^{ l}))\] (12)

Here, \(val_{i,q}^{l}\) represents the _validity_ term for weight \(w_{i}\) in layer \(l\) with respect to the quantization level \(w_{q} W_{r}^{l}\). If \(w_{i}\) can reach \(w_{q}\), then \(val_{i,q}^{l}=1\); otherwise, \(val_{i,q}^{l}=0\). The term \(\) is a large constant that penalizes unreachable quantization levels, effectively excluding them from the optimization.

## 3 Experiments

We initialize quantized networks with weights from a trained full-precision model of the same architecture, then fine-tune in the quantized space, which has been proven to improve performance [30; 31; 32]. We quantize input activations and weights to 3- or 4-bits for all matrix multiplication layers except the first and last, which use 8-bits. This approach is commonly used for quantizing deep networks, and has been proven to increase effectiveness at the cost of minimal overhead . The weights and the quantization parameters: bit mutlpliers and the offset values, are trained using SGD with a momentum of 0.9 and a cosine learning rate decay schedule . We sweep over different values of the regularization hyperparameter \(\) and chose \(=100\) for our results.

**ANN Training Details.** We use the ResNet-18  architecture for experiments on CIFAR-10  and ImageNet  datasets. Models are trained for 200 epochs on CIFAR-10 and 90 epochs on ImageNet with the weights having a learning rate of 0.01 and 0.1 respectively. The other parameters are trained with a learning rate of 0.001. For ImageNet, we preprocess images by resizing them to \(256 256\) pixels. During training, we apply random \(224 224\) crops and horizontal flips half the time. At inference, we use a center crop of \(224 224\). For CIFAR-10, we augment the training data by padding images with 4 pixels on each side, then taking random 32x32 crops. We also apply random horizontal flips half the time. The results are shown in Table 1 and 2.

**SNN Training Details.** We use the ResNet-19  and VGG-11  models, after adapting them to SNNs. Specifically, we replace all ReLU activation functions with LIF modules and substitute max-pooling layers with average pooling operations. We follow the implementation and data augmentation technique used in NDA  as our baseline training method. The weights and the other parameters are trained with a learning rate of 0.01 and 0.001 respectively. We evaluate on the N-Caltech 101 and CIFAR10-DVS benchmarks. N-Caltech 101 consists of 8,831 DVS images converted from the original Caltech 101 dataset, while CIFAR10-DVS comprises 10,000 DVS images derived from the original CIFAR10 dataset. For both these datasets, we apply a 9:1 train-validation split and resize all images to \(48 48\). Each sample is temporally integrated into 10 frames using spikingjelly . \(V_{reset}\) is set to 0 and the membrane decay \(\) is \(0.25\). Our results are presented in Table 3.

**Fault-Aware Training.** We evaluate our method on the VGG-13 architecture, training with 3-bit and 4-bit precision for both weights and activations on the CIFAR-10 dataset. Our experiments consider varying levels of SA fault density. Figures 1(a) and 1(b) illustrate the efficacy of our approach for 4-bit and 3-bit quantization, respectively.

Results and Analysis

**Comparison with Baselines.** Tables 1 and 2 present our quantized ANN results for CIFAR10 and ImageNet, respectively. Our method outperforms existing approaches, with 4-bit ResNet-18 (**W4/A4** refers to 4-bit weights and 4-bit activations) achieving a \(0.24\%\) accuracy increase over full-precision (FP) on CIFAR-10 and matching FP performance on ImageNet. For 4-bit quantized SNNs (Table 3), we observe performance gains on N-Caltech 101 and marginal losses on CIFAR10-DVS compared to FP. We attribute occasional performance improvements in both 4-bit ANNs and SNNs to the regularization effect of our quantization loss.

**Robustness to Faults.** SA faults represent extreme non-idealities in hardware, with each faulty bit halving the range of possible weight values. High device variability in conductance states can similarly cause significant discrepancies between expected and realized weights. Our approach, combining periodic loading of faulty weights during training with a fault-aware modified QAT algorithm, demonstrates robust performance even under high SA fault densities.

**Hardware Compatibility.** Figures 2(a) and 2(b) illustrate implementations of custom bit multipliers in analog and digital crossbar arrays. For analog arrays, the implementation incurs no additional cost, requiring only the adjustment of bit-multiplier conductance values from power-of-2 proportions to custom values. In digital arrays, the multiply-accumulate operation remains fully fixed-point, and the custom bit-multiplier scaling can be absorbed into the floating-point scaling operation (which is

  
**Method** & **FP** & **W4/A4 (\(\) FP)** & **W3/A3 (\(\) FP)** \\  L1 Reg  & \(93.54\) & \(89.98\) (\(-3.56\)) & - \\ BASQ  & \(91.7\) & \(90.21\) (\(-1.49\)) & - \\ LTS  & \(91.56\) & \(91.7\) (\(+0.1\)) & \(90.58\) (\(-0.98\)) \\ PACT  & \(91.7\) & \(91.3\) (\(-0.4\)) & \(91.1\) (\(-0.6\)) \\ LQ-Nets  & \(92.1\) & - & \(91.6\) (\(-0.5\)) \\  LCQ  & \(93.4\) & \(93.2\) (\(-0.2\)) & \(92.8\) (\(-0.6\)) \\ 
**Ours (N-Multipliers)** & \(93.26\) & \(\) (\(+\)) & \(\) (\(-\)) \\   

Table 1: Accuracy (%) for 3- and 4- bit quantized ResNet-18 models on CIFAR-10. FP denotes full-precision accuracy, \(\) FP denotes difference in performance compared to the corresponding FP network. **Best/**second best relative performances for each bit-width are marked in **bold**/**underlined.

  
**Method** & **Type** & **FP** & **W4/A4 (\(\) FP)** \\  L1 Reg  & No QAT & \(69.7\) & \(57.5\) (\(-12.5\)) \\ SinReQ  & Sine reg. & \(70.5\) & \(64.6\) (\(-5.9\)) \\ LTS  & Lottery & \(69.6\) & \(68.3\) (\(-1.3\)) \\ PACT  & Learned scale & \(69.7\) & \(69.2\) (\(-0.5\)) \\ LQ-Nets  & Linear non-uniform & \(70.3\) & \(69.3\) (\(-1.0\)) \\ QIL  & Non-linear & \(70.2\) & \(70.1\) (\(-0.1\)) \\ QSin  & Sine reg. & \(69.8\) & \(69.7\) (\(-0.1\)) \\ LCQ  & Non-linear & \(70.4\) & \(\) (\(+\)) \\  Ours & Fixed levels & \(69.6\) & \(68.2\) (\(-1.4\)) \\ Ours (N-Mult) & Linear non-uniform & \(69.6\) & \(69.6\) (\(-0.0\)) \\   

Table 2: Accuracy (%) for 4-bit quantized ResNet-18 models on ImageNet. **Best/**second best performances are marked in **bold**/**underlined.

  
**Dataset** & **Model** & **FP** & **W4 (\(\) FP)** \\  CIFAR10-DVS & Spiking VGG-11 & \(71.92\) & \(71.84\) (\(-0.08\)) \\ CIFAR10-DVS & Spiking ResNet-19 & \(72.91\) & \(72.14\) (\(-0.77\)) \\ N-Caltech 101 & Spiking VGG-11 & \(73.19\) & \(74.18\) (\(+0.99\)) \\ N-Caltech 101 & Spiking ResNet-19 & \(75.27\) & \(75.93\) (\(+0.66\)) \\   

Table 3: Accuracy (%) for 4- bit quantized SNNs on CIFAR10-DVS and N-Caltech 101.

common to all quantization schemes), thus eliminating any overhead caused by floating-point bit-multipliers. Learning custom bit multipliers within QAT enables highly effective low-bit quantization models that are compatible with standard in-memory computing architectures.

## 5 Conclusion and Future Work

We introduce a novel algorithm for learning bit multipliers within QAT, enabling efficient low-bit quantization models with learnable, non-uniform levels compatible with in-memory computing architectures. Our approach demonstrates minimal accuracy drops for 3- and 4-bit models compared to FP baselines across various datasets and architectures, including CIFAR-10 and ImageNet using ResNet-18, and CIFAR10-DVS and N-Caltech 101 using spiking VGG-11 and ResNet-19. Notably, our quantized models occasionally outperform their FP counterparts. We further extend our method to address SA faults, maintaining performance with up to \(30\%\) faulty bits. Future directions include extending the method to channel-specific quantizers, conducting fault-aware training experiments on additional benchmarks, expanding ANN and SNN model evaluations, and exploring sub-3-bit quantization. These advancements aim to enhance the efficiency and robustness of quantized neural networks for resource-constrained environments and hardware non-idealities.

Figure 3: **(a)** Analog implementation. **(b)** Digital implementation. While uniform quantization uses bit-multipliers \((r_{0}^{l},r_{1}^{l},r_{2}^{l},r_{3}^{l})\) in powers-of-2 proportions \((1,2,4,8)\), we propose learning custom multiplier factors instead.

Figure 2: Performance preservation with SA faults: periodic faulty weight loading maintains accuracy for low fault densities; our fault-aware modified QAT extends robustness to high fault fractions.