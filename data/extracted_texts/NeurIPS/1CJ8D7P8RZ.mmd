# PoET: A generative model of protein families as sequences-of-sequences

Timothy F. Truong Jr

OpenProtein.AI

NY, USA

ttruong@openprotein.ai&Tristan Bepler

OpenProtein.AI

NY, USA

tbepler@openprotein.ai

###### Abstract

Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose **P**rotein **E**volutionary **T**ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. In extensive experiments on deep mutational scanning datasets, we show that PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction across proteins of all MSA depths. We also demonstrate PoET's ability to controllably generate new protein sequences. 1

## 1 Introduction

Proteins carry out most of the biological functions at the molecular level of life. The function of a protein is encoded by its specific amino acid sequence and the three-dimensional structure that the sequence folds into. Engineering proteins for novel and enhanced function is a key problem in pharmaceuticals and biotechnology and involves designing novel sequences or modifying existing natural proteins for these purposes. Deep mutational scans and directed evolution experiments have been used to successfully design novel proteins , but can be costly and difficult to implement, which makes these experimental methods inapplicable for many proteins and functions of interest. Accurate computational models of sequence-function relationships can narrow down the protein sequence search space, reduce the need for expensive experiments, and enable the design of more novel proteins and functions.

Protein language models have emerged as promising methods for understanding and designing protein sequences . In particular, generative models offer a natural way to produce new protein designs. By training on large corpuses of natural protein sequences, these models learn evolutionary constraints on sequence space. They can then be used either to generate realistic sequences directly by sampling , or to identify promising protein sequence variants by predicting the relative fitness of the variants of interest using the sequence likelihoods as a proxy .

Traditionally, family-specific models learn evolutionary constraints specific to the protein family of interest by training on a multiple sequence alignment (MSA) of homologous sequences [12; 13]. However, this is ineffective for protein families with few sequences due to the lack of sufficient training data and inability to exploit information across families. These models also assume that MSAs are accurate, and cannot model novel insertions or deletions (indels) not present in the training MSA. More recently, unconditional large protein language models [5; 10; 11] have been developed. Trained on all known natural protein sequences, unconditional protein language models generalize across protein families. However, these unconditional single-sequence models cannot be easily directed to generate a protein from a specific family of interest, and underperform family-specific models for relative fitness prediction. Hybrid models such as Tranception  and TranceptEVE  combine unconditional language models with family-specific models to enable specialization to protein families. Nonetheless, it is unclear how to use these models to generate sequences with novel indels, and predictions from the family-specific models do not directly benefit from transfer learning across families.

Here, we propose the **Pro**tein **E**volutionary **T**ransformer (PoET), an autoregressive generative model of whole protein families that addresses these limitations. By learning to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters, PoET is able to generalize about evolutionary processes _across_ protein families, and avoids issues related to conditioning on MSAs. In order to capture conditioning between sequences in an order independent manner (the order of sequences within a family is arbitrary) and to generalize to large context lengths, we propose a novel Transformer layer (SS3.1.2) that models order-dependence between tokens within sequences and order-independence between sequences. PoET has the following properties:

* PoET can be used as a retrieval-augmented protein language model by conditioning the model on sequences from any family of interest. This also allows PoET to be used with any sequence database and to incorporate new sequence information without retraining.
* PoET is a fully autoregressive generative model, able to generate and score novel indels in addition to substitutions, and does not depend on MSAs of the input family, removing problems caused by long insertions, gappy regions, and alignment errors.
* By learning across protein families, PoET is able to extrapolate from short context lengths allowing it to generalize well even for small protein families.
* PoET can be sampled from and can be used to calculate the likelihood of any sequence efficiently.

We demonstrate these properties and show that PoET outperforms existing protein language models and evolutionary sequence models for variant effect prediction in extensive experiments on the \(94\) deep mutational scanning datasets in ProteinGym. PoET improves substitution effect prediction across proteins of all MSA depths, and also improves effect prediction of indels. A simple weighted average ensemble of PoET with existing methods further improves performance both across MSA depths and in sequences with large numbers of mutations. Furthermore, when used for generation, PoET controllably produces diverse, novel sequences that more closely match natural statistical constraints, resulting in better folding pLDDTs, than other generative protein language models. We expect PoET to become integral to future protein mutagenesis efforts.

## 2 Related Work

Evolutionary sequence modelsare well established methods in biological sequences analysis. To model protein families, these models search large protein sequence databases for homologs, align the positions of these homologs in an MSA, and then fit statistical sequence models to the MSA. Common models include site independent models , profile HMMs , and coupling models . Newer variants incorporate higher order correlations between sequence positions by training a VAE  or by building phylogentic trees . These approaches are often referred to as "alignment-based" and must be fit on a family-by-family basis, requiring large numbers of members to generalize. A significant limitation of these models is that they assume the MSA is an accurate model of the evolutionary process generating the sequences, when in fact, MSA algorithms inevitably make alignment errors; regions with long insertions or lots of gaps can be particularly problematic. Furthermore, they cannot model novel insertions or deletions (indels) that are not present in the training MSA.

Unconditional protein language modelsthat do not condition on homologs at inference have emerged as powerful methods for understanding and generating protein sequences. Both bidirectional models  and autoregressive generative models  have demonstrated competitive performance for variant function prediction. The latter type of model has the advantage of being able to score indels, but both cannot integrate evolutionary context not present in the trained model parameters. In contrast to family-specific evolutionary sequence models trained on sequences derived from a specific protein family , these models are trained on large protein databases  that span all known sequences. This enables them to learn evolutionary constraints that generalize across families to improve predictions for small families with few homologs, but they generally underperform family-specific models for larger families.

Conditional protein language modelsfit between the unconditional and family-specific paradigms. Only a few works have explored this direction to date. Masked language models of whole MSAs are able to integrate evolutionary context directly for conditioning site predictions , but are unable to model insertions in the alignment. Ram and Bepler  use an encoder-decoder framework to generate new sequences conditioned on an MSA, which removes the insertion limitation of Rao et al. , but still requires conditioning on aligned input sequences. Notin et al.  combine predictions from an unconditional language model and an alignment-based model and show that integrating retrieval-based methods with protein language models can improve variant function prediction performance. However, the reliance on an alignment-based model means that the combined model is still limited by the constraints of MSAs.

Retrieval-augmented language modelshave shown impressive results in natural language processing, especially on Question Answering tasks. These models incorporate a database search as part of the text generation process in order to generate new text conditioned on prototypes found in the database . In this way, they are conceptually similar to the conditional protein language models above. Retrieval-augmented approaches have the advantage of not requiring the entire training corpus to be encoded within the model parameters and the ability to easily integrate new data without retraining by simply adding it to the retrieval database. We adopt a similar approach with PoET in order to realize similar benefits. However, we build on well-established, fast, and accurate protein search tools for retrieval and frame the protein sequence generation problem as a sequence-of-sequences problem to incorporate retrieved-sequence conditioning, a fundamentally different paradigm than that employed by current retrieval-augmented models in natural language processing.

## 3 The Protein Evolutionary Transformer (PoET)

PoET is an autoregressive generative model of the distribution over protein families, where each family is generated as a sequence-of-sequences. Specifically, it models the distribution \(P(X=x)\), where \(x=s_{1},s_{2},...,s_{n}\) is the concatenation of \(n\) sequences \(s_{i}\) from the same family, and each sequence \(s_{i}=s_{i,1},s_{i,2},...,s_{i,L_{i}}\) is a sequence of \(L_{i}\) amino acids padded by a start and end token. For example, below is a sequence of three protein sequences of lengths \(4\), \(6\), and \(5\) with start token denoted by \(\$\) and stop token denoted by \(\):

\[_{s_{11}}\$&&& \\ s_{11}&s_{12}&s_{13}&s_{14}}_{s_{11}}\$&&&&&\\ s_{21}&s_{22}&s_{23}&s_{24}&s_{25}&s_{26}}_{s_{21}}_{s_{31}}_{s_{32}}_{s_{33}}_{s_{3 4}}_{s_{35}}_{s_{11}}\]

When referring to a sequence-of-sequences, \(s_{i}\), which has one index \(i\), refers to a sequence of tokens - the \(i\)th sequence in the sequence-of-sequences, whereas \(s_{i,j}\), which has two indices \(i,j\), refers to one token, the \(j\)th token of the \(i\)th sequence. We use \(x\) to denote the full sequence-of-sequences.

PoET generates each token in a sequence \(x\) one at a time, decomposing the probability of \(x\) as

\[P(x)=P(s_{1},s_{2},...,s_{n})=_{i=1}^{n}P(s_{i}|s_{<i})=_{i=1}^{n} _{j=1}^{L_{i}}P(s_{i,j}|s_{<i},s_{i,<j})\] (1)

The order of the individual sequences in a sequence-of-sequences is arbitrary, and we propose a novel Transformer-based architecture to exploit this order invariance.

### Model Architecture

We propose a variant of the common Transformer decoder layer (Figure 0(a)) to capture order invariance between sequences while preserving order-dependence between tokens within sequences. We accomplish this using two attention modules: (1) a within-sequence module in which the representation at each position of each sequence is updated based on attending only to the other tokens within this sequence, and (2) a between-sequence module in which the representation at each position of each sequence is updated based on attending to other sequences within the sequence-of-sequences (SS3.1.2). This tiered approach is critical for capturing long-range dependencies between sequences and uniquely allows our model to extrapolate to much longer context lengths than used during training (SS5.2.2), improving sequence generation and performance on downstream tasks (Appendix H). Our full PoET model is a stack of these layers with causal self-attention.

#### 3.1.1 Input Embedding

The input sequence \(x=s_{i,j},i 1..n,j 1..L_{i}\) of amino acids and start/stop tokens is first converted into a sequence of continuous embeddings \(h_{i,j}\) by mapping each token to its learned embedding:

\[h_{i,j}=W_{s_{i,j}},s_{i,j}\{, \}\] (2) \[W^{|\{, \}| d}\]

Figure 1: PoET Architecturewhere AA is the set of 20 standard amino acids, and \(W\) is a matrix of learnable embeddings of dimension \(d\).

#### 3.1.2 Tiered Transformer Decoder Layers

Next, the embeddings \(h_{i,j}\) are transformed by \(N\) layers of TieredTransformerDecoderLayers (Appendix Algorithm 1), a novel layer for processing a sequence-of-sequences that is invariant to the order of the individual sequences, and extrapolates to context lengths substantially longer than the training context length.

The TieredTransformerDecoderLayer is composed of two phases. In the first phase, causal self-attention is applied independently to each sequence \(h_{i}\) of the input sequence-of-sequences, transforming them into new sequences \(f_{i}=(h_{i})\). Relative positional information is encoded by applying Rotary Positional Encodings (RoPE)  to the queries and keys before applying self-attention in the standard manner; the absolute position for \(f_{i,j}\) is \(j\).

The second phase applies causal self-attention to the entire sequence-of-sequences by concatenating the individual \(f_{i}\) from the previous layer into one sequence before applying self-attention: \(g_{i,j}=([f_{<i};f_{i,<j}])\). In order to make self-attention in this phase invariant to sequence order, we adopt a simple but novel inter-sequence relative positional encoding scheme (Figure 0(b)): for \(g_{i,j}\) the absolute position is \(j\). Just as in the first phase, the absolute position for tokens in the \(i\)th sequence \(g_{i}\) is independent of the position \(i\) of the sequence in the sequence-of-sequences. Thus, the positional information encoded by RoPE in this layer alone _does not distinguish_ between the positions of tokens in _different_ sequences. For example, the relative position between the first token of the first sequence \(f_{1,1}\) and the first token of the second sequence \(f_{2,1}\) is \(0\). The fact that these two tokens come from two different sequences is encoded by the first phase, which operates on the two sequences independently. This inter-sequence relative positional encoding scheme has two useful properties:

1. it encodes the fact that amino acids at similar absolute positions in homologous proteins are more likely to be drawn from the same distribution
2. it limits the maximum relative position encoding needed to the number of tokens in an _individual_ protein sequence2, rather than the number of tokens in a sequence-of-sequences, allowing the model to generalize to longer sequences-of-sequences than seen during training

#### 3.1.3 Decoded Probabilities

Lastly, the output from the last TieredTransformerDecoderLayer, \(g_{i,j}\), is decoded into token probabilities by applying a linear transformation \(P(s_{i,j}|s_{<i},s_{<j})=p_{i,j}(s_{i,j})=(g_{i,j})\). Here, \(p_{i,j}\) is a vector of probabilities, one for each distinct token \(\{,\}\), and \(p_{i,j}(s_{i,j})\) is the probability of the token \(s_{i,j}\) according to \(p_{i,j}\).

### Training Data

Models were trained on 29 million sets of homologous sequences. Each set corresponds to a sequence in UniRef50 Version 2103, and contains all its homologs in UniRef50 found using Diamond . We removed any sets with fewer than 10 homologs. To avoid overfitting on promiscuous sequences which may belong to a large number of sets, we sample each set with weight inversely proportional to the size of the set ("inverse count" sequence weighting). The order of sequences within a sequence-of-sequences is randomized to promote order invariance. See Appendix B for more details.

## 4 Protein Variant Fitness Prediction

Protein variant fitness prediction is the task of assigning a score to each sequence in a set of variants \(\{v_{1},v_{2},...,v_{n}\}\) of a target sequence \(t\) that accurately reflects the relative fitness of the variants. A protein variant \(v_{i}\) can be any sequence with a limited number of substitutions, insertions, and/or deletions relative to the target \(t\) that the experimenter believes may have improved fitness. Fitnessrefers to the value of any property of a protein sequence related to function that the experimenter is interested in optimizing e.g. thermostability, enzymatic activity, etc.

Benchmarking using deep mutational scansDeep mutational scanning (DMS) is a method for measuring the fitness properties of thousands to millions of variants of a protein using next-generation sequencing techniques . Data from these experiments have been used to evaluate fitness prediction methods. We evaluate PoET on ProteinGym , the largest collection of such data yet, containing 87 datasets with substitution variants and 7 datasets with indel variants. We use the same validation set as Notin et al.  for tuning hyperparameters.

Fitness Prediction with PoETPoET predicts the fitness of a variant as the conditional log-likelihood of the variant \(v_{i}\) given a set of sequences \(S\) homologous to the target \(t\):

\[_{i}(S=\{s_{1},...,s_{m}\})= P(v_{i}|s_{1},s_{2},...,s_{m})=_{j =1}^{L_{i}} P(v_{i,j}|s_{1},s_{2},...,s_{m},v_{i,<j})\] (3)

The set \(S\) is retrieved by searching a large database of proteins such as UniRef100  for sequences homologous to \(t\). The homologs form a diverse set of sequences that define the protein family of interest. By conditioning on these sequences, PoET can infer evolutionary constraints on the protein family to improve fitness prediction. The sequences are conditioned on in an arbitrary order. Figure 2 illustrates the evaluation of PoET for variant fitness prediction on one DMS dataset.

Based on validation set performance, we selected the best method for (1) retrieving homologous sequences (Appendix C), (2) subsampling and filtering the homologous sequences to a reasonable context length for efficient inference (Appendix D), and (3) ensembling conditional log-likelihoods computed from different subsamples of homologous sequences. We present results with the best settings in the main results (SS5). Our final approach uses the ColabFold protocol  for retrieving homologs, and the final fitness prediction score is obtained by averaging the conditional log-likelihoods across subsamples of the full set of retreived homologous sequences:

\[_{,i}(S)=}}_{j=1}^{N_ {}}_{i}(S_{j} S)\] (4)

The subsets \(S_{j}\) are drawn by sampling with sequence weights , maximum sequence identity to the target \(t\) in \(\{1.0,0.95,0.90,0.70,0.50\}\), and total tokens (i.e. context length) in \(\{6144,12288,24576\}\). All combinations of these parameters are used, for a total of \(N_{}=15\).

Figure 2: Illustration of evaluating PoET for variant fitness prediction on a DMS dataset

## 5 PoET achieves state-of-the-art performance in variant fitness prediction

We evaluate PoET's ability to predict the relative fitness of protein variants by measuring the average Spearman correlation \(\) between the predicted fitness and the measured fitness across the DMS datasets in ProteinGym. We compare PoET to the best existing alignment-based models, unconditional and conditional protein language models, and hybrid models that combine multiple approaches (Results Summary: Table 1, Per Dataset: Figures S1, S2 ). A brief overview of the baseline methods is provided in Appendix F. Statistical significance was assessed using a paired t-test. For a fair comparison, we ran the relevant baselines with all homologous sequence retrieval methods considered for use with PoET (Appendix C), and present the results for each baseline method paired with its best retrieval method. We do not tune parameters for ensembling the same method with different subsamples of the set of retrieved homologs (Equation 4) because these parameters need to be considered and defined on a per method basis, and many baselines already have their own ensembling strategy as defined by their authors [4; 12; 14].

### Comparison to baselines

**Substitutions Benchmark** On the substitutions benchmark, PoET performs comparably to TranceptEVE L, the best performing baseline method (Table 1). PoET improves average Spearman correlation \(=0.013\), but the difference falls just short of statistical significance \(p=0.05029\). Although the methods perform similarly, PoET offers a number of advantages. Inference with PoET is substantially faster when considering on the order of tens to hundreds of thousands of variants (Appendix J), which allows users to more quickly accomplish common tasks such as computing the fitness of all single site substitutions. By avoiding the costly step of training a new model for each protein family, users can more easily experiment with different ways of defining the protein family. For example, users can retrieve the homologous sequences using different sequence search programs and settings, and prior knowledge can be incorporated by selecting the subset of most relevant homologous sequences e.g. sequences from the taxon of interest, or that are known to be functional (Appendix K). Since PoET is able to generalize across protein families, whereas the EVE component of TranceptEVE L cannot, it is less critical that the protein family of interest is defined by a large number of homologous sequences (Appendix C).

    & &  \\  Model & & \# &  & & & \\ Type & Model name & Param & Low & Medium & High & All & Indels \\  Align- & Site independent & N/A & 0.417 & 0.404 & 0.411 & 0.408 & N/A \\ ment- & GEMME & N/A & 0.445 & 0.449 & 0.522 & 0.463 & N/A \\ based & EVE (ensemble) & N/A & 0.414 & 0.441 & 0.498 & 0.448 & N/A \\  Uncond- & ESM-1v (ensemble) & 3.25B & 0.356 & 0.372 & 0.510 & 0.398 & N/A \\  & ProGen2 (ensemble) & 10.8B & 0.357 & 0.416 & 0.448 & 0.411 & 0.407 \\ PLM & Tranception L (no retrieval) & 700M & 0.377 & 0.399 & 0.429 & 0.401 & 0.430 \\  Cond- & MSA Transformer (ens.) & 100M & 0.372 & 0.421 & 0.477 & 0.423 & N/A \\  & PoET (ensemble) & 201M & **0.476** & **0.466** & **0.542** & **0.484** & **0.510** \\   Hybrid & Tranception L & 700M & 0.441 & 0.437 & 0.472 & 0.445 & 0.464 \\  & TranceptEVE M & 300M & - & - & - & - & 0.516 \\  & TranceptEVE L & 700M & 0.454 & 0.463 & 0.508 & 0.471 & 0.466 \\   & PoET (ensemble) & & & & & & \\  & + TranceptEVE L & 901M & **0.479** & **0.480** & **0.537** & **0.492** & **0.521** \\   

Table 1: **Average Spearman correlation between model scores and experimental measurements on ProteinGym by MSA depth, and # of parameters in language models.** MSA depth measures the amount of sequence information the MSA of the homologous sequences defining a protein family contains about the target protein (Appendix E). N/A indicates not applicable, whereas a dash (-) indicates applicable, but not computed.

On the other hand, if one is willing to spend more time on inference and the protein family is well defined _a priori_, better fitness predictions can be obtained by ensembling PoET with TranceptEVE L (Results: Table 1, Methods: Appendix G). An ensemble of PoET with TranceptEVE L performs significantly better than TranceptEVE L alone (\(=0.021,p<7\)e-6). The performance improvement is consistent across subsets of the substitutions benchmark broken down by MSA depth (Table 1), mutation depth (Appendix Table S2), and taxon (Appendix Table S3).

We also considered ensembles of PoET with other baselines methods (Appendix Table 7). The ensemble of PoET with GEMME is also notable as it performs similarly to the ensemble with TranceptEVE L on the substitutions benchmark, and requires very little additional time to compute; GEMME is thousands of times faster than either PoET or TranceptEVE L . The main disadvantage is that GEMME is unable to score indels.

**Indels Benchmark** On the indels benchmark, the best performing baseline model is TranceptEVE M, a variation of TranceptEVE L that uses a protein language model with fewer parameters (\(=0.05\)). PoET and the ensemble of PoET with TranceptEVE L both outperform TranceptEVE L (\([0.044,0.055]\)), and perform comparably to TranceptEVE M (\([-0.006,0.005]\)). However, no differences between the aforementioned models are statistically significant. One advantage of PoET is that it is able to not only score indels, but also _generate_ sequences with indels (SS6). It is also able to score and generate insertions not present in the MSA.

### Characterizing the PoET architecture

#### 5.2.1 Training Distribution and Model Size

We train variations of PoET with different training distributions and model sizes and investigate their effect on fitness prediction (Figure 3).

**Context Length** We trained 57M parameter versions of PoET for up to 3 days on 7 x A100 GPUs with three context lengths: 4K, 8K, and 16K. Notably, these context lengths far exceed the 1K or 2K context lengths typically used to train other protein and natural language models . We reason that longer context lengths are needed for the model to observe a sufficient number of homologs; even with a context length of 8K, for a sequence of length \(500\), which is the length of the typical human protein in ProteinGym, the model would only observe \(\)\(16\) of possibly thousands of homologs. Interestingly, while a context length of 8K performs better than a context length of 4K, increasing the context length further to 16K had a neutral or negative effect on variant effect prediction. It does improve perplexity of heldout sequences.

**Sequence Set Weighting** Next, we trained an 8K context length model using the naive strategy of sampling UniRef50 sequence sets uniformly instead of using our "inverse count" strategy that weights sequence sets based on their size (SS3.2). We find that the "inverse count" sampling strategy substantially outperforms the naive strategy. Other methods that also train on sequence sets like MSA Transformer  may also benefit from this sampling strategy.

Model SizeLastly, using the optimal parameters found thus far, we trained 57M, 201M, and 604M parameter models to explore the effect of model size. Models were trained until validation performance plateaued. Increasing the model size to 201M parameters signficantly improves performance, but increasing the size further to 604M appears to have a neutral effect. We chose the 201M version as the standard PoET model.

#### 5.2.2 Model Architecture

To determine the importance of using TieredTransformerDecoderLayers (SS3.1.2) instead of regular Transformer layers, we trained, on the same task, a regular 201M parameter RoPE-based Transformer that ignores the sequence-of-sequences structure of the input. We compared the generative capabilities of PoET and the Transformer by measuring the perplexity at context lengths both within and beyond the training the context length (Figure 4).

We trained both models with a context length of 8K for 500K steps. As expected based on previous studies, the RoPE-based Transformer does not generalize to context lengths beyond the training context length. In contrast, PoET generalizes to \(\)8x the training context length (61K), which is the longest we tested. PoET also performs better on context lengths within the training context length,but by a smaller margin. Next, we finetuned both models with 16K and 32K context lengths for 50K and 30K steps respectively. While the perplexity of the Transformer improved significantly at the 16K and 32K context lengths, the Transformer still does not generalize to context lengths beyond the training context length, and underperforms all variants of PoET at all context lengths. The Transformer also underperforms PoET for variant fitness prediction (Appendix H).

## 6 PoET generates novel sequences that preserve structure within a protein family

PoET's state-of-the-art performance on variant fitness prediction demonstrates that it assigns higher likelihood to regions of sequence space with high fitness variants. This suggests that PoET can be used to directly generate high fitness variants belonging to a protein family by conditioning PoET on sequences from the protein family and sampling from the resulting conditional distribution. Direct generation of variants makes the exploration of higher order mutants computationally tractable as sequence space grows exponentially large with number of mutations, making it impossible to explore this space by scoring all such variants.

To evaluate PoET's ability to generate sequences from specific protein families, we sampled 1,000 sequences each from PoET conditioned on homologs from two protein families, phage lysoszymes (PF00959) and chorismate mutases with activity in E. coli (Appendix K.2), and examined the novelty of the generated sequences and their predicted structural conservation. Sequence novelty was measured as the max sequence identity to any natural sequence (of the same protein family). Predicted

Figure 4: Comparison of the perplexity of a regular Transformer and PoET when generating a protein sequence conditioned on a fixed number of tokens from other sequences in the same protein family. Protein families consist of UniRef50 sequences, and both models perform much better than a profile HMM baseline (Appendix Figure 12).

Figure 3: Performance of PoET on the ProteinGym validation set when trained with (_left_) various training distributions and (_right_) model sizes.

structural conservation was measured as the max TM-score between the predicted AlphaFold2  structure of the generated sequence and the solved structure of any natural sequence. For both protein families, PoET generates high diversity sequences that are predicted with high confidence to be structurally similar, supporting the quality of sequences generated by PoET (Figure 5).

Furthermore, when tasked to generate phage lysozyme, we find that sequences generated by PoET are more diverse and better preserve structure at similar levels of sequence novelty than sequences generated by ProGen  (Figure 5B), a 1.2B parameter model fine-tuned specifically for generating phage lysozyme. As many sequences generated by ProGen have been experimentally confirmed to be functional phage lysozyme, these results suggest that PoET generates functional sequences with even greater diversity even without fine-tuning. Since fine-tuning ProGen is critical for achieving good variant effect prediction , and a version of ProGen fine-tuned on chorismate mutases is not available, we did not compare PoET and ProGen for generating novel chorismate mutases.

## 7 Conclusion

PoET is a Transformer-based autoregressive generative model of whole protein families. By framing family generation as a sequence-of-sequences generation problem, we are able to train across tens of millions of protein sequence clusters to encode fundamental rules of protein evolution into the PoET model. This enables the model to generalize to protein families unseen during training and extrapolate from small numbers of conditioned-upon sequences. The sequence-of-sequences generative framework allows PoET to be used as a retrieval-augmented language model, generating new sequences conditioned on a set of sequences representing the family or other properties of interest. We demonstrate that PoET improves over other protein language models and evolutionary sequence models for variant fitness prediction across a wide range of deep mutational scanning datasets. PoET also enables efficient sequence generation and the generative distribution can be controlled via conditioning. Phage lysozyme- and chorismate mutase-like sequences sampled from PoET are novel and predicted to fold with high confidence. PoET can be backed by other sequence databases and naturally improves as databases grow without the need for retraining. We anticipate that PoET will become a fundamental component of ML-enabled protein design in the future.

Figure 5: Sequence novelty and predicted structural conservation of (A) functional chorismate mutases generated by PoET and (B) phage lysozyme generated by PoET and ProGen. PoET generates diverse sequences (50-100% seq id to a natural sequence) while preserving 3D structure within the protein family (TM-score \(>0.8\) to a structure of a natural sequence and pLDDT \(>90\)).