# TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning

Hui Chen1, Yanbin Liu2, Yongqiang Ma1, Nanning Zheng1, Xin Yu3

1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center of Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 2Auckland University of Technology, 3The University of Queensland

Work done while visiting The University of Queensland.Corresponding author: nnzheng@mail.xjtu.edu.cn.

###### Abstract

Pre-trained vision-language models (VLMs) such as CLIP have shown excellent performance for zero-shot classification. Based on CLIP, recent methods design various learnable prompts to evaluate the zero-shot generalization capability on a _base-to-novel_ setting. This setting assumes test samples are already divided into either base or novel classes, limiting its application to realistic scenarios. In this paper, we focus on a more challenging and practical setting: _generalized zero-shot learning_ (GZSL), _i.e._, testing with no information about the base/novel division. To address this challenging zero-shot problem, we introduce two unique designs that enable us to classify an image without the need of knowing whether it comes from seen or unseen classes. _Firstly_, most existing methods only adopt a single latent space to align visual and linguistic features, which has a limited ability to represent complex visual-linguistic patterns, especially for fine-grained tasks. Instead, we propose a dual-space feature alignment module that effectively augments the latent space with a novel attribute space induced by a well-devised attribute reservoir. In particular, the attribute reservoir consists of a static vocabulary and learnable tokens complementing each other for flexible control over feature granularity. _Secondly_, finetuning CLIP models (_e.g._, prompt learning) on seen base classes usually sacrifices the model's original generalization capability on unseen novel classes. To mitigate this issue, we present a new topology-preserving objective that can enforce feature topology structures of the combined base and novel classes to resemble the topology of CLIP. In this manner, our model will inherit the generalization ability of CLIP through maintaining the pairwise class angles in the attribute space. Extensive experiments on twelve object recognition datasets demonstrate that our model, termed Topology-Preserving Reservoir (TPR), outperforms strong baselines including both prompt learning and conventional generative-based zero-shot methods.

## 1 Introduction

Young children often exhibit a remarkable capacity to identify novel visual objects only based on verbal descriptions provided by their caregivers. This phenomenon has spurred considerable interest in developing learning models with similar feats, known as zero-shot learning (ZSL). Early ZSL works  trained a model on the seen base classes and evaluated the generalization performance on novel unseen classes. This can be done by aligning the visual features and textual descriptorsinto a shared latent space, where the intrinsic visual-linguistic relationship is extracted for zero-shot classification at test time. However, this conventional ZSL setting assumes test examples only come from novel classes, limiting its application to realistic scenarios where both base and novel classes need to be classified. Therefore, a more realistic setting _generalized zero-shot learning_ (GZSL) [3; 4] has been proposed to recognize both base and novel classes without knowing the base/novel division. Recent GZSL methods such as CE , LSA  and ZLAP  design various generative models, which generate the missing visual features for unseen novel classes and then conduct joint base and novel classification.

The emergence of large vision-language models (VLMs) such as CLIP  have demonstrated good potential for ZSL. For example, CLIP is trained on 400 million image-text pairs, which can effectively capture the visual-linguistic links essential for ZSL. After training, the model can be applied for zero-shot classification using a hand-crafted prompt, _e.g._, 'a photo of <class>'. Starting from CLIP, recent works design diverse learnable prompts (_e.g._, conditional prompt , Multi-modal prompt , and self-regulating prompt ) to improve CLIP's performance for downstream zero-shot tasks. We find that these prompt methods adopt a _base-to-novel_ setting to evaluate their zero-shot generalization capability, similar to the conventional ZSL setting. In particular, they classify the base and novel classes separately, with the strong assumption that test samples have already been divided into either base or novel classes. Driven by the practicality of zero-shot learning, this paper focuses on a more realistic setting of _generalized zero-shot learning_ (GZSL) under the VLM context.

To tackle the challenging GZSL problem, we introduce a Topology-Preserving Reservoir (TPR) model to effectively unleash the generalization potential of VLMs for the simultaneous classification of base and novel categories. To achieve the goal, our proposed TPR embraces two core novel designs: a dual-space feature alignment module and a feature semantic topology preserving objective. Considering that most previous methods establish the visual-linguistic relations by employing a single shared latent space [7; 11; 8; 6], the complex and fine-grained patterns cannot be effectively captured. To mitigate this key issue, we present a dual-space feature alignment module by enhancing the latent space with a representative attribute space, which is constructed from a well-devised attribute reservoir. The reservoir is designed to contain both static and learnable vocabulary tokens. In this fashion, both prior knowledge and task-specific information can be extracted, enriching the feature representations and avoiding overfitting to a single task.

Moreover, recent works  identify the weak generalizability problem of prompt learning on VLMs, _i.e._, the learned prompts on seen classes do not often generalize well to unseen classes (_e.g._, in Fig. 1 (a) (right), ProGrad  underperforms CLIP on unseen classes). To address this problem, we propose a new topology-preserving objective (Fig. 1 (b-d)) to maintain the semantic topology structure of the combined seen and unseen3 classes by referring to the original CLIP embeddings.

Figure 1: (a) In the challenging and realistic _generalized zero-shot learning_ (GZSL) setting, our method significantly outperforms the state-of-the-art methods on both seen and unseen classes. (b-d) Finetuning CLIP will lead to the weak generalization problem  on unseen classes. We tackle this problem by inheriting the topology of CLIP feature space by maintaining the pairwise angles.

Specifically, we adopt the Pearson correlation coefficient to constrain the variation of angles between pairwise categories before and after CLIP finetuning. As a result, after finetuning, our model still inherits the good generalization ability of CLIP, without suffering from the weak generalization problem. This can be shown by the superior performance on the unseen classes in Fig. 1(a)(right).

Extensive experiments conducted on twelve object recognition datasets, including both traditional GZSL benchmarks and prompt learning benchmarks, demonstrate that our proposed method significantly improves the recognition performance on unseen classes over our baselines, and outperforms the state-of-the-art on eleven datasets. Our contributions are summarized as follows:

* Different from the traditional _base-to-novel_ setting, our work emphasizes a challenging yet practical _generalized zero-shot_ learning problem for VLMs, without knowing the division of the base and novel categories. In this scenario, our proposed Topology-Preserving Reservoir model significantly improves the recognition performance compared to prior arts.
* We introduce a dual-space feature alignment module. It enhances the latent space, which is shared by visual and textual features, with a representative attribute space constructed from an attribute reservoir. In this manner, we essentially improve the representativeness of the latent space, leading to better fine-grained alignment between visual and textual features.
* We introduce a new feature semantic topology-preserving objective that is designed to maintain the semantic topology structure of finetuned features. In this fashion, finetuned features will not severely overfit to seen classes. Thus, we can effectively preserve the generalization capability of VLMs on unseen categories.

## 2 Related Works

**Zero-shot Learning.** Zero-shot Learning (ZSL) [2; 13; 14; 15] aims to recognize unseen objects by leveraging auxiliary knowledge such as tags, attributes, or textual descriptions to bridge the gap between seen and unseen classes. Generalized Zero-shot Learning (GZSL) [16; 17; 18] extends the scope of ZSL by considering a more realistic scenario where both seen and unseen classes are present during testing. Several methods [19; 20; 21; 22; 23; 16; 24] seek to learn a latent space where visual and linguistic data are aligned and the inference is performed by searching which class has the highest similarity score. Besides, generative models [25; 26; 27; 7; 6; 5; 28] like GANs have been explored for ZSL/GZSL due to their superior performance. These models generate synthetic samples for unseen classes based on their semantic descriptions. In the Large Language Model (LLM) era, many works [24; 29; 30] focus on generating improved visual-linguistic features for better alignment. While most methods default to utilizing attributes as semantic embeddings, attribute annotation poses scalability challenges for large-scale datasets. Moreover, attribute annotation exhibits subjectivity , leading to perceptible discrepancies among different annotators.

**Vision-Language Models.** Vision-Language Models (VLMs) aim to bridge the semantic gap between visual and textual modalities, enabling tasks such as image captioning, visual question answering, and image-text retrieval. Inspired by the success of self-supervised learning , vision-language pretraining has emerged as a powerful paradigm for learning rich representations of images and text. Recent VLMs, such as CLIP , ALIGN , BLIP , VLMO , CoCa  and FLIP , learn powerful joint representations by using contrastive learning on large amounts of paired vision-language data. After pretraining on 400M pairs of data, CLIP constructs image classifiers using the class names of the target dataset in a zero-shot manner and achieves superior performance. Despite the direct applicability of VLMs to zero-shot recognition, empirical observations show suboptimal performance on fine-grained tasks . In this work, we propose to align multimodal representations in a dual-space to improve the fine-grained representation ability of VLMs for GZSL.

**Prompt Learning in VLMs.** Prompt learning has gained traction in natural language processing as a powerful approach for adapting pre-trained language models to new tasks with minimal supervision [38; 39]. In prompt learning, task-specific prompts or templates are designed to guide the language model to generate outputs tailored to a particular task, enabling effective adaptation to diverse downstream applications. Recent research [40; 8] has extended the concept of prompt learning to VLMs. By providing task-specific prompts that incorporate both visual and textual cues, VLMs can seamlessly integrate knowledge from multiple modalities and generalize to unseen tasks with limited labeled data [10; 41; 11; 12]. For example, CoOp  finetunes the pretrained CLIP model by inserting learnable context vectors into a fixed textual template. However, recent work  identified a weak generalizability problem of prompt learning: the learned prompt is not generalizable to wider unseen classes. In this work, we devise a novel topology-preserving objective to tackle this problem.

## 3 Methodology

We propose a Topology-Preserving Reservoir (TPR) framework (Fig. 2) to unleash the generalization capability of VLMs for GZSL. Specifically, TPR has two unique designs on top of VLMs: (1) _Dual-space feature alignment module_ to strengthen feature discriminability by aligning visual and linguistic features in both latent space and attribute space (constructed by the well-devised attribute reservoir); (2) _Feature semantic topology-preserving objective_ to maintain the generalization capability of VLMs through preserving both seen and unseen class topology before and after fine-tuning.

**Problem Formulation.** Different from conventional ZSL or base-to-novel setting, GZSL needs to address the challenge of recognizing both seen and unseen classes without knowing the seen/unseen division at test time. During training, only samples from the seen classes are available. Formally, given a dataset \(=\{(x_{i}^{s},y_{i}^{s},z_{i}^{s})|x_{i}^{s}^{s},y_{i }^{s}^{s},z_{i}^{s}^{s}\}_{i=1}^{n_{1}}\) consist of \(n_{1}\) samples, where \(x_{i}^{s}\), \(y_{i}^{s}\), \(z_{i}^{s}\) denote the visual feature, class label, and textual description feature of the \(i\)-th seen image, respectively. Meanwhile, another dataset \(=\{(x_{j}^{u},y_{j}^{u},z_{j}^{u})|x_{j}^{u}^{u},y_{j }^{u}^{u},z_{j}^{u}^{u}\}_{i=1}^{n_{2}}\) contains \(n_{2}\) samples, where \(x_{j}^{u}\), \(y_{j}^{u}\), \(z_{j}^{u}\) represent the visual feature, class label, and textual description feature of the \(j\)-th unseen image, respectively. The seen and unseen label sets are disjoint: \(^{s}^{u}=\). Following common practice in GZSL, the seen dataset \(\) is split into a training set \(_{tr}^{s}\) and a test set \(_{te}^{s}\), while the unseen dataset \(\) constitutes the test set \(_{te}^{u}\). Then, the model is trained on \(_{tr}^{s}\) and evaluated on the union set \(_{te}^{s}_{te}^{u}\). In the following, \(s\) and \(u\) will be omitted for simplicity.

### Dual-Space Feature Alignment

**Attribute Reservoir Construction.** We devise the attribute reservoir to construct a new attribute space, which can exploit the meticulous features overlooked by a simple latent space. This further facilitates the effective mining of complex visual-linguistic patterns for better GZSL. The reservoir design takes into account both the generalization capability and task-specific adaptation. Initially, we curate an extensive array of attribute terms sourced from diverse literature repositories, including CUB , MIT-States , MAD , VAW  and LSA , which collectively delineate the shape, color, motion, material, texture, and part of an object. After eliminating redundancies, we

Figure 2: Overview of our TPR framework. The latent space directly aligns visual and linguistic features extracted from frozen VLMs. To augment latent space for fine-grained visual-textual pattern mining, we devise a novel attribute reservoir for constructing a new attribute space. The reservoir consists of both static and learnable vocabulary tokens, enabling flexible exploration and control of feature granularity for the GZSL task. Furthermore, we propose a topology-preserving objective to keep the generalization capability of VLMs, mitigating the weak generalization problem .

obtain a **base attribute vocabulary** of size \(N_{1}\), including attributes such as _washing up_, _on stick_, and _pinstriped_. Subsequently, we employ a pre-trained LLM  to extract features of this base attribute vocabulary for GZSL, obtaining \(A_{1}^{N_{1} d_{a}}\). The attribute vocabulary covers extensive attribute repositories to facilitate the generalization to unseen classes, but it is impossible to exhaustively include all conceivable attributes lying in image and textual data. Therefore, we introduce the flexible **learnable attribute tokens**, denoted as \(A_{2}^{N_{2} d_{a}}\), to augment the base attribute vocabulary. These tokens have two functions: (1) they learn complementary attribute knowledge absent in the base attribute vocabulary in a data-driven manner, and (2) they incorporate the task-specific information into the reservoir for better downstream task adaptation. Eventually, the two components of reservoir are concatenated together to form our attribute reservoir \(A^{N d_{a}}\), where \(N=N_{1}+N_{2}\).

**Multi-Modality Encoding.** Given an input image, we initially utilize the pre-trained CLIP image encoder to extract its visual feature \(x^{1 d}\). Subsequently, we project the visual feature into both a latent space and an attribute space (_i.e._, dual-space). To achieve this, we transform the visual feature and attribute reservoir into the same dimension with two linear layers (\(P_{v},P_{v}^{a}\)):

\[x xP_{v}^{1 d},A_{v}=AP_{v}^{a}^{N  d}.\] (1)

The visual feature \(x\) is then encoded into the dual-space:

\[x^{}=(x,A_{v},A_{v})\,,x^{}=(x^{ },x^{},x^{})\,,f_{v}=x^{}+x^{1  d}\,,g_{v}=f_{v}A_{v}^{T}^{1 N},\] (2)

where \((,,)\) is the attention function : \((,,)=(QK^{T}/ )V\), \(f_{v}\) denotes the visual feature encoded in the latent space, and \(g_{v}\) represents the visual feature encoded in the attribute space. On the text side, given the textual description corresponding to the image, we utilize the CLIP text encoder to extract the linguistic feature \(z^{1 d}\). Subsequently, we transform \(z\) and the attribute reservoir to \(d\)-dimension using two additional linear layers (\(P_{l},P_{l}^{a}\)):

\[z zP_{l}^{1 d},A_{l}=AP_{l}^{a}^{N  d}.\] (3)

The linguistic feature \(z\) is then encoded into the dual-space using cross-attention:

\[z^{}=(z,A_{l},A_{l})\,,f_{l}=z^{}+z^{1  d}\,,g_{l}=f_{l}A_{l}^{T}^{1 N},\] (4)

It is notable that only cross-attention is employed to encode the linguistic feature, as overly intricate operations on the textual side may lead to overfitting.

**Multi-Modality Alignment.** We use the contrastive loss  to align the visual-linguistic features within the dual-space. Specifically, the contrastive loss in the latent space is defined as:

\[L_{cl}(f_{v},f_{l})=- f_{l}^{T}/)}{_{j}( f_{v} f_{l,j}^{T}/)}- f_{v}^{T}/)}{_{j} (f_{l} f_{v,j}^{T}/)},\] (5)

where \(f_{v,j}\) and \(f_{l,j}\) are the \( 2\)-normalized features of the \(j\)-th input image and text, respectively. \(\) is a temperature hyperparameter. The contrastive loss in the attribute space is formulated as follows:

\[L_{ca}(g_{v},g_{l})=- g_{l}^{T}/)}{_{j}( g_{v} g_{l,j}^{T}/)}- g_{v}^{T}/)}{_{j} (g_{l} g_{v,j}^{T}/)},\] (6)

where \(g_{v,j}\) and \(g_{l,j}\) denote the \( 2\)-normalized visual and linguistic attribute representations of the \(j\)-th input image and text, respectively.

### Feature Semantic Topology Preservation

Compared with conventional ZSL, the GZSL problem is more challenging: the model needs to decide between both seen and unseen classes without knowing whether the test example comes from seen or unseen classes. Previous GZSL methods [5; 49] observed the domain bias problem: _trained model is seriously biased towards seen classes in the testing phase_. The bias arises because the model is only trained on seen classes and, therefore, learns features and patterns specific to those classes. This phenomenon often leads to inferior performance on unseen classes in the GZSL evaluation setting. For VLM methods such as prompt learning [8; 10; 11], although the model weights are frozen, another problem called weak generalizability problem is observed : _the learned prompt is not generalizable to unseen classes within the same dataset_. A reasonable explanation is the learned task-specific prompt overfits the seen base classes when finetuing VLMs. CoCoOP  try to alleviate this problem by using instance-conditioned prompt, but the evaluation is only in the base-to-novel setting rather than the more challenging GZSL setting 4. To our knowledge, in the VLM context, no prior work has evaluated the challenging GZSL setting, let alone a principled solution to tackle the generalization problem under this setting.

In this paper, we tackle the weak generalizability problem in the feature space, particularly our newly introduced attribute space. Intuitively, the design of base attribute vocabulary has the effect of combating overfitting to some extent. But here, we are looking for a more effective and principled solution. A straightforward idea is to avoid overfitting to the seen classes by regularizing the features during the model finetuning process. For example, we can constrain the rank of image features to be no-decreasing, which boils down to maximizing the nuclear norm of the feature matrix . However, this method empirically proves less effective. Then, we observe that the pre-trained VLMs such as CLIP perform equally well on both seen and unseen classes (Fig. 1 and also [8; 10; 11]). This motivates us to inherit the generalization capability of CLIP to prevent overfitting to seen classes during the finetuning process, as shown in Fig. 1(b-d). Specifically, we want to maintain the class topology of CLIP embedding space as a way for generalization inheritage. The class topology is composed of all pairwise class angles, which can be calculated by cosine similarities (CLIP embedding is \(\)-2 normalized). Denote the textual description features of all \(c\) (seen+unseen) classes in CLIP's embedding space as \(Z=\{z^{1},z^{2},,z^{c}\}^{c d}\). The corresponding features in the attribute space are denoted as \(G_{l}=\{g_{l}^{l},g_{l}^{r},,g_{l}^{r}\}^{c N}\). Finally, in order to preserve the class topology in attribute space to be similar to that in CLIP embedding space, we adopt the Pearson correlation coefficient to define a topology-preserving loss as follows:

\[L_{tp}(Z,G_{l})=-^{c}(w_{ij}-}_{ij}^{c}w_{ij} )(_{ij}-}_{ij}^{c}_{ij})}{^ {c}(w_{ij}-}_{ij}^{c}w_{ij})^{2}} ^{c}(_{ij}-}_{ij}^{c}_{ij})^{2} }},\] (7)

where \(w_{ij}= z^{j}T}{||z^{i}||||z^{j}||_{2}},_{ij}=^{i} g_{l}^{j}T}{||g_{l}^{i}||||g_{l}^{j}||_{2}},i,j=1,2,..,c\). Note that Eq. 7 maintains the pairwise feature angles between categories before and after finetuning. Hence, both seen and unseen classes can leverage the class topology of CLIP for better GZSL classification. More importantly, with \(L_{tp}(Z,G_{l})\), the finetuned model is less likely to be seriously biased towards seen classes, thus improving the generalization to unseen classes.

### Learning Objective and Inference

The overall learning objective is:

\[L()=L_{cl}+ L_{ca}+ L_{tp},\] (8)

where \(\) and \(\) are loss weights, and \(\) denotes model's trainable parameters. During inference, given a testing sample, we first extract its visual feature \(f_{v}\) and perform the nearest neighbor search from all seen and unseen classes:

\[*{arg\,max}_{y^{u}^{u}}\ f_{v} f _{l,y}^{T},\] (9)

where \(f_{l,y}\) is the linguistic feature of class \(y\) (all vectors are \(\)2-normalized).

## 4 Experiments

**Datasets and Metrics.** We validate the effectiveness of TPR on four widely-used datasets in GZSL: AwA2 , CUB , FLO , and SUN . We follow the commonly-used dataset split  but use the generated textual descriptions instead of attribute annotation. To further evaluate the generalization ability, we conduct experiments on eight other object recognition datasets: FGVC-Aircraft , Country , StanfordCars , EuroSAT , DTD , UCF101 , Food101 , and OxfordPets . These datasets are divided into seen and unseen classes in a similar way. Note that under the GZSL setting, the model can only be trained on seen classes and evaluated on both seen and unseen classes to evaluate its generalization ability. We report the average per-class top-1 accuracy on _seen classes_ (\(S\)) and _unseen classes_ (\(U\)), respectively. To balance the two metrics, we also report the _harmonic mean_ (\(H\)) of the seen and unseen accuracy: \(H=2\).

**Textual Description Generation.** We propose employing ChatGPT  to autonomously generate a descriptive paragraph for each class, leveraging its extensive knowledge base learned from diverse sources in the Internet. To facilitate this process for each dataset, we advocate employing the following prompt structure: "_I have never seen images of <type>. The following is a good description of <class name>_, so I can easily recognize <class name>." Here, the placeholder <type> corresponds to categories such as animals, birds, flowers, _etc._, while <class name> denotes the specific name of each class. As an illustration, consider the antelope class from the AwA2 dataset, where the generated description reads as follows: _"Antelopes are herbivorous mammals known for their slender bodies and long, curved horns. They are often found in grasslands and open savannas, where they graze on vegetation and use their speed and agility to escape predators."_

**Training Details.** We employ pretrained ViT-B/32 CLIP as the feature extractor, which outputs 512-dimensional visual and linguistic features. In total, we collect \(N_{1}=5,996\) attribute words and we use pretrained Bert  to extract attribute features of dimension \(d_{a}=768\). For all datasets, TPR is trained for 200 epochs with a batch-size of 512 via Adam optimizer on a single NVIDIA RTX4090 GPU. Overall, optimal hyperparameters of TPR are chosen from the following ranges: learning rate \(\) {1e-5, 3e-5, 5e-5, 7e-5, 1e-4}, \(\)\(\) {0.2, 0.5, 1.0, 2.0}, \(\)\(\) {1e-4, 5e-4, 1e-3}, \(\)\(\) {0.03, 0.05, 0.07, 0.10}, and \(N_{2}\)\(\) {\(100,200,300,400\)}, which are tuned on the validation set via grid search.

### Comparison with the SoTA

We compare with the state-of-the-art methods including prompt learning and generative-based GZSL. We adapt them to our description-based GZSL setting for a fair comparison. (1) _Prompt learning methods_, including CoOp , CoCoOp , MaPLe , PromptSRC  and ProGrad , fine-tune the pretrained CLIP through integrating trainable prompt tokens within a predefined template. To adapt these methods for GZSL, we input (image, label) pairs while omitting the utilization of textual descriptions. Notably, our setting differs from the base-to-novel setting  typically used in these methods, which originally predicts the seen and unseen classes **separately**. Instead, we conduct the comparison in the GZSL setting  where the prediction space includes **both** the seen and unseen classes, thereby posing a _more challenging task than the base-to-novel setting_. (2) _Generative-based GZSL methods_, including CE , LSA  and ZLAP , are adapted by substituting the input semantic features derived from attribute annotations with those obtained from textual descriptions. For a fair comparison, all methods adopt the same backbone and dataset split.

   &  &  &  &  &  &  \\ Model & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) \\  CLIP  & 81.69 & 77.46 & 26.02 & 29.88 & **26.42** & 53.91 & 51.16 & 52.50 & 46.88 & **49.51** & 47.84 & 18.25 & 11.55 & 13.84 & 13.66 & 12.13 & 12.62 \\ CoOp  & 81.36 & 64.92 & 74.92 & 22.23 & 18.23 & 20.03 & 56.27 & 50.56 & 53.31 & 49.85 & 49.31 & 49.52 & 17.17 & 12.10 & 14.18 & 12.36 & 9.23 & 11.08 \\ CoCoOp  & 78.53 & 78.31 & 76.10 & 22.53 & 19.81 & 48.61 & 40.22 & 50.22 & 54.76 & 49.53 & 49.81 & 49.52 & 18.81 & 13.60 & 15.79 & 5.95 & 6.03 & 10.09 \\ PartMe  & 78.04 & 71.25 & 74.49 & 24.46 & 20.66 & 21.52 & 59.88 & 53.52 & 46.24 & 48.68 & 47.73 & 21.50 & 15.79 & 12.96 & 9.54 & 10.99 \\ PromptSD  & 84.04 & 70.73 & 76.82 & 30.92 & 16.32 & 17.03 & 68.45 & 54.70 & 47.83 & 42.04 & 45.82 & 13.04 & 13.60 & 16.81 & 42.62 & 6.87 & 9.30 \\ PubMed  & 81.73 & 67.46 & 79.51 & 39.72 & 23.18 & 21.12 & 61.51 & 55.56 & 53.87 & 52.94 & **54.03** & 19.00 & 11.00 & 13.93 & 13.99 & 8.77 & 10.78 \\  CE  & 76.69 & 67.50 & 71.97 & 31.80 & 19.04 & 28.30 & 63.02 & 44.09 & 51.88 & 44.11 & 47.15 & 45.58 & 26.83 & 25.25 & 26.63 & 12.90 & 8.07 & 9.90 \\ NAS  & 71.65 & 65.87 & 71.07 & 32.35 & 19.54 & 28.52 & 72.51 & 41.03 & 55.66 & 45.86 & 41.89 & 46.99 & 24.78 & 25.85 & 26.22 & 27.51 & 5.90 \\ ZLA  & 76.35 & 74.74 & 55.44 & 22.54 & 25.52 & 58.22 & 57.82 & 47.76 & 40.68 & 41.29 & 47.73 & 23.92 & 27.10 & 28.19 & 12.64 & 10.42 & 11.25 \\
**TPR** & **87.26** & **81.63** & **44.13** & **42.62** & **28.83** & **73.78** & **64.22** & **74.98** & **70.45** & 45.40 & 47.50 & 47.50 & **43.68** & **29.68** & **32.87** & **19.55** & **16.03** & **17.28** \\  TPA* & 80.52 & 71.70 & 75.56 & 42.42 & 25.97 & 32.22 & 86.22 & 65.99 & 71.74 & 50.87 & 45.49 & 47.67 & 43.15 & 23.55 & 28.38 & 12.58 & 11.58 & 17.85 \\ TPA* & 95.60 & 78.81 & 86.93 & 53.10 & 32.55 & 40.36 & 83.75 & 46.76 & 72.97 & 58.29 & 52.08 & 55.01 & 43.50 & 31.30 & 36.41 & 27.82 & 23.31 & 25.37 \\    
   &  &  &  &  &  &  \\ Model & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) \\  CLIP  & 466.65 & 37.38 & 41.75 & 21.13 & 11.25 & 11.25 & 14.86 & 36.99 & 13.78 & 53.72 & 64.92 & 58.99 & 74.74 & 23.08 & 27.09 & 28.26 & 65.83 & 73.29 \\ CoCoOp  & 49.86 & 38.47 & 34.53 & 29.89 & 12.27The performance analysis presented in Table 1 reveals that TPR outperforms other methods in terms of \(H\) metric in 11 out of 12 datasets. Compared with prompt learning methods, TPR obtains an average absolute performance improvement of 12.46%, 9.46%, and 11.94% on \(S\), \(U\) and \(H\) metrics, respectively. Similarly, in comparison with generative-based GZSL methods, TPR demonstrates an average absolute performance gain of 11.24%, 9.29%, and 10.68% on \(S\), \(U\) and \(H\) metrics, respectively. Remarkably, TPR achieves the best performance on all fine-grained datasets (marked with *), underscoring its exceptional fine-grained data perception capability. Furthermore, while the recognition performance of many baselines, such as CoCoOp, deteriorates for unseen classes compared to zero-shot CLIP, TPR continues to achieve outstanding results. This indicates TPR's proficiency to effectively preserve CLIP's generalization capability to unseen classes. To verify the applicability of TPR to different VLMs, we utilize two VLMs, CoCa  and EVA-02, as backbone networks for feature extraction from input images and text descriptions. The results, presented at the bottom of Table 1, demonstrate TPR's robust generalization across various VLMs.

### Ablation Study

**Loss Functions.** We study the efficacy of two novel module designs in Table 2. By augmenting the latent space with the attribute space (\(L_{cl} L_{cl}+L_{ca}\)), the accuracy of unseen classes is improved by 4.26%, 2.78%, and 3.81% for the three datasets. This underscores the attribute space's adeptness in capturing attribute features overlooked by the latent space, thus promoting the perception of unseen classes. Meanwhile, the inclusion of topology-preserving objective (\(L_{cl}+L_{tp}\)) results in a notable accuracy improvement for unseen classes, manifesting as an increase of 6.36%, 2.91%, and 1.81% across the three datasets. Finally, incorporating both objectives (\(L_{ca}\) and \(L_{tp}\)) into our full model, the accuracy outperforms each single objective on almost all metrics (seen, unseen and harmonic) except for seen on CUB, demonstrating the complementary effect of two devised modules: dual-space feature alignment (\(L_{ca}\)) and topology-preserving objective (\(L_{tp}\)).

**Attribute Reservoir.** We conduct an ablation study across various configurations of the attribute reservoir, as presented in Table 2. Notably, competitive performance is attained when solely relying on the static attribute vocabulary. Conversely, utilizing solely learnable attribute tokens exhibits improved performance on seen classes, albeit with a decrease in accuracy for recognizing unseen classes. Optimal \(H\) performance is achieved when both types of attribute knowledge are combined.

**Static Vocabulary Size \(N_{1}\).** In general, a larger \(N_{1}\) entails the inclusion of a greater number of attribute words, facilitating a finer-grained representation of objects and potentially enhancing recognition performance. The findings depicted in Fig. 3 substantiate this notion. It is evident that as \(N_{1}\) increases, the model performance improves significantly, especially for unseen classes. Overall, the harmonic mean \(H\) tends to saturate beyond 4,000 attribute words on the coarse-grained AwA2

Figure 4: Impact of the number of learnable attribute tokens (_i.e._, \(N_{2}\)) on model performance.

Figure 3: Impact of the size of the attribute vocabulary (_i.e._, \(N_{1}\)) on model performance.

dataset, while \(H\) continues to increase on the fine-grained CUB and FLO datasets. This again verifies the effectiveness of our model in capturing the fine-grained complex patterns.

**Learnable Tokens Quantity \(N_{2}\).** As shown in Fig. 4, the accuracy of unseen classes on the AwA2 dataset exhibits improvement with increasing \(N_{2}\). However, for the CUB and FLO datasets, the accuracy of unseen classes tends to plateau around 200 tokens, diverging from the pattern on AwA2. We conjecture that an excessive number of learnable tokens may lead to overfitting on seen classes for fine-grained datasets, thereby limiting further enhancement.

**Topology-Preserving Constraint.** We ablate on various choices of the topology-preserving loss. These variants encompass: (1) _nuclear norm_: maximizing the nuclear norm to ensure that visual features span the entire space; (2) _orthogonality_: enforcing orthogonality among class text features in the attribute space; (3) _topology-preservation in latent space_ (\(L_{tp}^{lat}\)): maintaining pairwise angles between class features (Eq. 7) in the latent space; (4) _our topology preservation in attribute space_ (\(L_{tp}\)). From Table 3, we observe that both nuclear norm and orthogonality cannot consistently improve the performance, possibly due to the lack of reference on CLIP. As to the topology-preserving objective, enforcing it on the novel attribute space yields consistently superior performance, which again reveals the better expressivity of the proposed attribute space.

**Base-to-Novel Evaluation.** As the prompt learning methods originally evaluate on the base-to-novel setting, we compare with them on standard prompt learning benchmarks for completeness and better understanding of the GZSL setting. Comparing Table 1 and Table 4, we notice that the performance of all methods in base-to-novel setting surpasses the corresponding GZSL results, corroborating that GZSL is a more challenging task. Moreover, our method outperforms or at least on-par-with the prompt learning methods in the base-to-novel setting.

## 5 Conclusion and Future Work

In this paper, we tackle the challenging GZSL problem by designing a Topology-Preserving Reservoir (TPR) model. Specifically, TPR embraces two unique designs for VLMs: a dual-space feature alignment module and a feature semantic topology preserving objective. First, a reservoir containing

    &  &  &  \\ Method & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\  CLIP  & 56.48 & 64.05 & 60.03 & 53.24 & 59.90 & 56.37 & 90.10 & 91.22 & 90.66 \\ CoOp  & 92.19 & 54.74 & 68.69 & 79.44 & 41.18 & 54.24 & 88.33 & 82.26 & 85.19 \\ CoCoOp  & 87.49 & 60.04 & 71.21 & 77.01 & 56.00 & 64.85 & 90.70 & 91.29 & 90.99 \\ MaPLe  & 94.07 & 73.23 & 82.35 & 80.36 & 59.18 & 68.16 & 90.71 & **92.05** & 91.38 \\ PromptSRC  & 92.90 & 73.90 & 82.32 & 83.37 & **62.97** & **71.75** & 90.67 & 91.53 & 91.10 \\ TPR & **95.12** & **76.66** & **84.90** & **84.80** & 59.39 & 69.86 & **94.03** & 91.15 & **92.57** \\   

Table 4: Comparison with the state-of-the-art prompt learning methods under _base-to-novel_ setting .

    &  &  &  \\ Setting & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) \\   & \(L_{cl}\) & 85.70 & 68.14 & 75.92 & **41.55** & 23.24 & 29.81 & 74.01 & 60.45 & 66.55 \\  & \(L_{cl}\)+\(L_{cs}\) & 84.69 & 72.40 & 78.06 & 40.77 & 26.02 & 31.77 & 74.41 & 64.26 & 68.96 \\  & \(L_{cl}\)+\(L_{tp}\) & 84.44 & 74.50 & 79.16 & 40.72 & 26.15 & 31.85 & 76.72 & 62.26 & 68.74 \\   & static vocabulary & 86.71 & 75.23 & 80.56 & 41.11 & 26.12 & 31.94 & 74.62 & 64.11 & 68.97 \\  & learnable tokens & **87.69** & 64.58 & 74.38 & 41.27 & 20.85 & 27.70 & **82.72** & 60.71 & 70.03 \\   

Table 2: Ablation on different regularizers and reservoir components.

    &  &  &  \\ Setting & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) & \(S\) & \(U\) & \(H\) \\  w/o \(L_{tp}\) & 84.69 & 72.40 & 78.06 & 40.77 & 26.02 & 31.77 & 74.41 & 64.26 & 68.96 \\ nuclear norm & **87.88** & 72.48 & 79.44 & 41.09 & 25.36 & 31.36 & 74.72 & 64.47 & 69.22 \\ orthogonality & 87.15 & 74.87 & 80.55 & 40.36 & 26.13 & 31.72 & 73.68 & **64.95** & 69.04 \\ \(L_{tp}^{lat}\) & 86.93 & **77.00** & **81.67** & 40.61 & 26.00 & 31.70 & 74.65 & 64.34 & 69.11 \\ \(L_{tp}^{lat}\) & 87.10 & 76.81 & 81.63 & **41.22** & **26.87** & **32.53** & **77.58** & 64.52 & **70.45** \\   

Table 3: Ablation on the topology preserving loss \(L_{tp}\).

both static and learnable vocabulary tokens is devised to construct a representative attribute space to enhance the latent space, which facilitates the exploitation of complex and fine-grained visual-linguistic patterns. Second, we propose a topology-preserving objective, which inherits the good generalization ability of CLIP to mitigate the weak generalization problem of prompt learning methods. In particular, topology-preserving objective constrains the variations of angles between pairwise categories before and after CLIP finetuning. Comprehensive experiments are conducted on twelve object recognition datasets, validating the superior performance of our method in the challenging and more practical GZSL setting. In the future, we want to investigate TPR on other applications such as few-shot learning and other modalities such as video.

**Limitations**. Text alone may not fully capture the nuances of fine-grained datasets like CUB, while attribute annotations, though more accurate, are costly. Thus, a more desirable solution would be combining the knowledge from expert-provided attribute annotations with LLM-generated text to enhance performance. Additionally, our method may face challenges in aligning visual features of generic scenes with description features in the attribute space, especially when descriptions are not sufficiently specific. This could be alleviated by providing more distinct and human-refined descriptions.