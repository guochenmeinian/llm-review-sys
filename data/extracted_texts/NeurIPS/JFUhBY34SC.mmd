# Explicit Eigenvalue Regularization Improves Sharpness-Aware Minimization

Haocheng Luo\({}^{1}\), Tuan Truong\({}^{2}\), Tung Pham\({}^{2}\), Mehrtash Harandi\({}^{1}\), Dinh Phung\({}^{1}\), Trung Le\({}^{1}\)

\({}^{1}\)Monash University, Australia

\({}^{2}\)VinAI Research, Vietnam

haocheng.luo@monash.edu, v.tuantm27@vinai.io, v.tungph4@vinai.io, mehrtash.harandi@monash.edu, dinh.phung@monash.edu, trunglm@monash.edu

###### Abstract

Sharpness-Aware Minimization (SAM) has attracted significant attention for its effectiveness in improving generalization across various tasks. However, its underlying principles remain poorly understood. In this work, we analyze SAM's training dynamics using the maximum eigenvalue of the Hessian as a measure of sharpness and propose a third-order stochastic differential equation (SDE), which reveals that the dynamics are driven by a complex mixture of second- and third-order terms. We show that alignment between the perturbation vector and the top eigenvector is crucial for SAM's effectiveness in regularizing sharpness, but find that this alignment is often inadequate in practice, which limits SAM's efficiency. Building on these insights, we introduce Eigen-SAM, an algorithm that explicitly aims to regularize the top Hessian eigenvalue by aligning the perturbation vector with the leading eigenvector. We validate the effectiveness of our theory and the practical advantages of our proposed approach through comprehensive experiments. Code is available at https://github.com/RitianLuo/EigenSAM.

## 1 Introduction

Understanding the generalization of deep learning algorithms is one of the core challenges in modern machine learning. Overparameterization makes the loss landscape of neural networks highly non-convex, often featuring numerous global optima, while simple gradient-based algorithms surprisingly tend to find solutions that generalize well. A body of empirical and theoretical work suggests that the "flatness" or "sharpness" of the minima is a promising explanation for generalization (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Dinh et al., 2017; Jiang et al., 2019; Xie et al., 2020; Liu et al., 2023b), and the implicit bias of optimization algorithms drives them toward flatter solutions, thereby ensuring good generalization (Blanc et al., 2020; Wen et al., 2022; Arora et al., 2022; Damian et al., 2022; Ahn et al., 2023; Tahmasebi et al., 2024).

Inspired by research on flatness and generalization, recent work by Foret et al. (2021) proposed Sharpness-Aware Minimization (SAM), a dual optimization method that perturbs parameters before performing gradient descent to enhance generalization performance by minimizing sharpness. Although SAM has demonstrated empirical success across various fields (Foret et al., 2021; Kaddour et al., 2022), theoretical analysis of the principles underlying its success remains limited. The work by Compagnoni et al. (2023) explains SAM's generalization advantage as an implicit minimization of the gradient norm, while Wen et al. (2022) suggests that SAM regularizes the Hessian spectrum near the minima manifold. However, existing theories are either somewhat simplified or rely on overlyidealized assumptions, leading to a noticeable gap between theory and practice. This gap limits their ability to fully explain the advantages of SAM (see Appendix A or contemporaneous work Song et al. (2024) for empirical evidence).

In this paper, we consider a widely used measure of sharpness: the largest eigenvalue of the Hessian matrix (Lyu et al., 2022; Arora et al., 2022; Wen et al., 2022; Damian et al., 2022). We extend the previous PAC-Bayes theory to demonstrate the importance of this measure for generalization. Our main contribution is an in-depth analysis of the training dynamics of SAM, expanding on the second-order Stochastic Differential Equation (SDE) proposed by Compagnoni et al. (2023) and revealing that the complex third-order terms play a crucial role in shaping SAM's implicit bias. Under ideal conditions, where the perturbation vector aligns well with the top eigenvector of the Hessian matrix, these terms effectively reduce the sharpness of the loss function. However, our experiments show that this alignment does not hold in real-world settings, limiting SAM's ability to effectively regularize sharpness. Based on our theoretical findings and experimental observations, we propose a new algorithm, Eigen-SAM, which intermittently estimates the top eigenvector of the Hessian matrix and incorporates its component orthogonal to the gradient into the perturbation, enabling explicit regularization of the top Hessian eigenvalue.

We summarize our contributions as follows:

* We prove a new theorem (Theorem 3.1) to establish the relationship between the top eigenvalue and generalization error, building on the general PAC-Bayes theorem (Alquier et al., 2016).
* We propose a third-order SDE (Theorem 4.1) to model the dynamics of SAM. This approach achieves a lower approximation error compared to the previous second-order SDE by Compagnoni et al. (2023) and additionally infers a close relationship between perturbation-eigenvector alignment and sharpness reduction (Corollary 4.1.1).
* We introduce a novel algorithm, Eigen-SAM, based on our theoretical insights and experimental observations (Section 5). This method aims to enhance alignment between the perturbation vector and the top eigenvector, resulting in a more effective reduction of sharpness.
* We validate our theory and the effectiveness of the proposed algorithm through comprehensive experiments (Section 6).

## 2 Related work

**Theoretical understanding of SAM.** SAM has garnered widespread attention for its significant improvements in generalization performance; however, the theoretical analysis provided in the original paper (Foret et al., 2021) is limited. The authors only presented a PAC-Bayes generalization bound, which is effective only for 0-1 loss. Subsequently, Andriushchenko and Flammarion (2022) attempted to further understand the success of SAM by restricting the network structure to diagonal linear networks and providing an implicit bias for SAM. They also established the first convergence result for SAM. Bartlett et al. (2023) conducted a detailed study of SAM's dynamics for quadratic loss, suggesting that it oscillates between the two sides of the minimum in the direction of greatest curvature and drifts toward flatter minima. However, the assumption of quadratic or locally quadratic loss settings is not realistic for practical deep learning models. More recently, Wen et al. (2022) extended the analysis of SAM's dynamics to general loss functions, assuming that all global minima form a connected manifold. Given sufficient training time and infinitesimally small \(\) and \(\), they rigorously proved that SAM's dynamics would track the trajectory of a Riemannian flow with respect to sharpness, achieving the same sharpness-reducing effect. On a different front, Compagnoni et al. (2023) applied the continuous-time approximation framework from Li et al. (2017) to analyze SAM dynamics, concluding that SAM implicitly minimizes the norm of the gradient scaled by \(\).

**Continuous-time approximations for discrete algorithms.** Substantial research demonstrates that the trajectory of stochastic discrete iterations with decaying step sizes will ultimately follow the solution of certain Ordinary Differential Equations (ODEs) (Harold et al., 1997; Borkar et al., 2009; Duchi and Ruan, 2018). Further developments in understanding deep learning algorithms were made by Li et al. (2017), who proposed a general and rigorous mathematical framework for continuous-time approximations, deriving SDEs for SGD and its various variants. They also provided experimental evidence supporting the reasonableness of continuous-time approximations in real-world models (Li et al., 2021). In this paper, we follow this mathematical framework, reusing some of its notations and definitions.

## 3 Preliminaries

### Notations

We start by introducing the notation used throughout our paper. Let \(\) denote the training set, sampled from the true data distribution \(\). For a given mini-batch \(\), we define the mini-batch loss as \(f_{}(x)\) with parameters \(x^{d}\). The generalization loss is defined as \(f_{}(x)=_{}[f_{ }(x)]\), while the empirical loss is defined as \(f_{}(x)=_{}[f_{ }(x)]\). Since we primarily analyze and discuss the empirical loss in this paper, we drop the dependency on \(\) for simplicity, denoting the empirical loss as \(f(x)\) when no ambiguity arises.

We use \(\|\|\) to denote the Euclidean norm. The \(k\)-th order derivative of the loss \(f\) at \(x\) is denoted by \(^{k}f(x)\), which is a symmetric \(k\)-tensor in \((^{d})^{ k}\) when \(x^{d}\). We denote by \(_{1}(^{2}f(x))\) the largest eigenvalue of the Hessian matrix \(^{2}f(x)\) and \(v_{1}(^{2}f(x))\) its corresponding unit eigenvector, with \(\|v_{1}(^{2}f(x))\|=1\). Additionally, we use \(^{3}f(x)(u,v)^{d}\) to represent the application of the symmetric 3-tensor \(^{3}f(x)\) along directions \(u\) and \(v\).

### Sharpness-Aware Minimization

SAM (Foret et al., 2021) seeks flat minima by minimizing the perturbed loss:

\[_{x}_{\|\| 1}f(x+),\]

where \(\) is a predefined hyperparameter controlling the radius of the perturbation. Solving the inner maximization problem leads to \(^{SAM}(x)=\). Differentiating the perturbed loss with respect to \(x\), we get:

\[ f(x+^{SAM}(x)) =(x))}{dx} f(x)|_{x+ ^{SAM}(x)}\] \[ f(x)|_{x+^{SAM}(x)}.\]

In the last approximation, Foret et al. (2021) ignore the dependency of \(^{SAM}(x)\) on \(x\), leading to faster computational efficiency and higher generalization performance. Applying SAM in the stochastic case, the SAM iteration for mini-batch \(_{k}\) is summarized as:

\[x_{k+1}=x_{k}- f_{_{k}}x_{k}+}}{\| f_{_{k}}\|}.\] (1)

We use \( f_{_{k}}()\) and \( f_{_{k}}\) to distinguish between those needing and not needing backpropagation of gradients, matching the algorithm in practice. Thus, the perturbation vector of mini-batch SAM can be written as:

\[_{}^{SAM}=}{\| f_{}\|}.\] (2)

### SDE approximation for SGD and SAM

Li et al. (2017) developed the following SDE to approximate discrete SGD:

\[dX_{t}=- f(X_{t})dt+(^{1,1}(X_{t}))^{}dW_{t},\]

where \(W_{t}\) is standard Brownian motion. Compagnoni et al. (2023) applied this framework to analyze the dynamics of SAM, deriving the following second-order SDE for SAM:

\[dX_{t}=- f(X_{t})-f_{ }(X_{t}) f_{}}{\| f_{}\|_{2}}dt +^{1,1}(X_{t})+(^{1,2}(X_{t})+^{1,2}( X_{t})^{})^{}dW_{t},\] (3)

where \(^{a,b}\) denotes the covariance matrix of the \(a\)-th and \(b\)-th terms in the Taylor expansion of the perturbed loss (see Appendix B for the full expression). We refer to Eq.3 as the second-order SDE since it includes up to second-order partial derivatives in both the drift and diffusion coefficients.

### Choice of sharpness measure

In this paper, we use the largest eigenvalue \(_{1}(^{2}f(x))\) as a measure of sharpness, similar to prior works (Lyu et al., 2022; Arora et al., 2022; Wen et al., 2022; Damian et al., 2022). Geometrically, the top eigenvalue of the Hessian matrix at a given point represents the maximal curvature of the loss function along any direction. Moreover, it is closely related to the concept of sharpness (defined as the maximum perturbed loss difference) used by Foret et al. (2021) at the minima. Note that at the minima, \( f(x)=0\); thus,

\[_{\|\| 1}f(x+)-f(x) _{\|\| 1}^{} f(x)+ }{2}^{}^{2}f(x)\] \[=_{1}(^{2}f(x))}{2}.\]

Another possible choice is the trace of the Hessian matrix. However, the trace of the Hessian scales with the dimensionality of the parameters, complicating cross-model comparisons and limiting this measure's applicability.

We further establish a PAC-Bayes theorem that bounds the generalization error through the top eigenvalue of the Hessian matrix. This theorem is based on the general PAC-Bayes theorem Alquier et al. (2016) and applies to bounded losses, not limited to 0-1 loss as in the work of Foret et al. (2021); Zhuang et al. (2022).

**Theorem 3.1**.: (Generalization Bound) _Assume that the loss function is bounded by \(L\), and the third-order partial derivative of the loss function is bounded by \(C\). Additionally, we assume \(f_{}(x)_{(0,^{2} _{t})}f_{}(x+)\), as in Foret et al. (2021). For any \((0,1)\) and \(>0\), with a probability over \(1-\) over the choice of \(^{n}\), we have_

\[f_{}(x)  f_{}(x)+}{2}_ {1}(^{2}f_{}(x))+^{ 3}}{6}\] \[+}}{d^{ 2}})+O(1)+2+4(n+d)}.\]

_where \(n\) is the number of samples._

We defer the proof to Appendix C. Theorem 3.1 suggests that minimizing the top eigenvalue of the Hessian matrix is crucial for improving generalization ability.

## 4 Third-order SDE reveals implicit regularization in SAM

In this section, we delve into the discussion and derivation of the third-order SDE continuous-time approximation for SAM. In Section 4.1, we present heuristic derivations that provide intuitive insights into our approach. Following this, Section 4.2 offers a formal third-order SDE approximation for SAM, establishing the mathematical rigor of our framework. Finally, in Section 4.3, we propose a corollary linking perturbation-eigenvector alignment with eigenvalue regularization, furthering our understanding of the implicit regularization effects inherent in SAM.

### Heuristic derivations for the third-order SDE

We begin by examining the drift coefficient in Compagnoni et al. (2023) (Eq. 3):

\[- f(X_{t})-f_{}(X_{t})  f_{}}{\| f_{}\|}=- f(X_{t})- \| f_{}(X_{t})\|,\]

where the second term indicates that SAM penalizes trajectories with large loss gradients. However, this formulation does not reveal an implicit regularization effect concerning sharpness, specifically regarding the top eigenvalue of the Hessian matrix. Thus, understanding the implicit bias on the Hessian matrix requires a third-order Taylor expansion. The missing cubic term in the Taylor expansion is:

\[}{2}f_{}(X_{t})( f_{ }, f_{})}{\| f_{}\|^{2}}= }{2}^{}^{2}f_{ }(X_{t}) f_{}}{\| f_{}\|^{2}}.\]The equality holds because \( f_{}\) is treated as a perturbation vector independent of \(X_{t}\), as SAM's implementation does not involve differentiating with respect to it (See Section 3.2 for a detailed description of this part). This third-order term suggests that SAM employs an additional gradient measurement to compute a specific third derivative: the gradient of the second derivative along the direction of the gradient.

**Hessian-gradient alignment during training.** Recent findings indicate that during training, the gradient implicitly aligns with the top eigenvector of the Hessian matrix under certain conditions: (1) training with normalized full-batch gradient descent (Arora et al., 2022); (2) a locally quadratic loss landscape (Bartlett et al., 2023); (3) training with SAM when very close to the minimizer manifold (Wen et al., 2022). This alignment phenomenon is crucial for interpreting the third-order term in our drift coefficient. Specifically, when the gradient is highly aligned with the top eigenvector, we have:

\[}{2}[f_{}(X_{t} )( f_{}, f_{})}{\| f_{}\|^{2}}] }{2}^{3}f_{}(X_{t})( v_{1}(^{2}f_{}(X_{t})),v_{1}(^{2}f_{}(X_{t})))\] \[=}{2}_{1}(^{2}f_{ }(X_{t})),\]

where the final equality follows from the properties of differentiating eigenvalues (see Magnus (1985) for a detailed discussion).

If this alignment phenomenon holds, we can conclude that the implicit bias of the drift coefficient aligns with the gradient of the top eigenvalue of the Hessian, thereby implicitly minimizing sharpness.

### Formal third-order SDE approximation for SAM

In this subsection, we present the general formulation of the SDE for SAM. We refer to our SDE (Eq. 4) as the third-order SDE, to distinguish it from the second-order SDE (Eq. 3). For the complete statements and proofs, we refer the reader to Appendix B.

**Theorem 4.1**.: (Third-order SDE for SAM, Informal Statement of Theorem B.4) _Let \(0<<1,T>0\), \(N= T/\), and \(\{x_{k}:k 0\}\) denote the sequence of discrete SAM iterations defined by Eq. 1. Define \(\{X_{t}:t[0,T]\}\) as the stochastic process satisfying the SDE_

\[dX_{t}=-^{SAM}(X_{t})dt+(^{SAM}(X_{t}))^ {}dW_{t}, X_{0}=x_{0}\] (4)

_with \(^{SAM}(X_{t}):=f(X_{t})+\| f_{}(X_{t}) \|+}{2}^{}^{2}f_{ }(X_{t}) f_{}}{\| f_{}\|^{2}}\),_

\[^{SAM}(X_{t}):=^{1,1}(X_{t})+(^{1,2}(X_{t})+^{1,2 }(X_{t})^{})+^{2}^{2,2}(X_{t})+(^{1,3}( X_{t})+^{1,3}(X_{t})^{}),\]

_where \(^{a,b}\) denotes the covariance matrix of the \(a\)-th and \(b\)-th terms in the Taylor expansion of the perturbed loss (see Appendix B for the full expression)._

_Under sufficient regularity conditions, let \(=(^{})\). Then, \(\{X_{t}:t[0,T]\}\) is an order-1 weak approximation of \(\{x_{k}:k 0\}\), i.e., for any test function \(g\) of at most polynomial growth, there exists a constant \(C\) independent of \(\) such that_

\[_{k=0,1,,N}|g(x_{k})-g(X_{k})| C.\]

Our proof relies on the third-order Taylor expansion of \(f_{_{k}}(X_{t}+}}{\| f_{_{k}} \|})\), carefully matching the first- and second-order conditional moments and quantifying the errors for higher-order terms. Our third-order SDE reveals that SAM's implicit bias includes a complex combination of second-order and third-order terms, with scales of \(\) and \(}{2}\), respectively. Compared to Compagnoni et al. (2023), our theorem offers two main advantages: first, we allow \(\) to take larger values (\(^{}\) compared to \(^{}\) in Compagnoni et al. (2023)), which is more consistent with real-world settings; equivalently, our SDE achieves a lower approximation error for a fixed \(\). Second, our SDE explicitly captures SAM's implicit bias on the Hessian matrix, manifesting as the gradient of the Hessian in the gradient's direction. Additionally, the diffusion coefficient in Eq. 4 implies that SAM injects additional noise in the form of the covariance of the higher-order terms in the Taylor expansion of the perturbed loss. This curvature-dependent noise aligns with recent studies (Gatmiry et al., 2024, 2024), which demonstrate that label noise in SGD exhibits similar behavior to SAM.

### Perturbation-eigenvector alignment and eigenvalue regularization

The implicit bias introduced by the third term in the drift coefficient of the SDE (Eq. 4), i.e., \(}{2}f_{}(X_{t})( f_{},  f_{})}{\| f_{}\|^{2}}\), remains difficult to understand. As discussed in Section 4.1, if we assume that the perturbation vector is well-aligned with the top eigenvector, we can interpret the cubic term as the gradient of the top eigenvalue, leading to an implicit bias that decreases sharpness. Next, we quantify and formalize this heuristic approach. Define \((,v_{1}):=1-_{s\{ 1\}}\|-s v_{1}\|\) as a measure of alignment between the perturbation vector \(\) and the top eigenvector \(v_{1}\). It is worth noting that \(+v_{1}\) and \(-v_{1}\) are equivalent eigenvectors, which is why we define alignment as the maximum over both \(+v_{1}\) and \(-v_{1}\).

**Corollary 4.1.1**.: _Recall that \(_{}^{SAM}\) is defined in Eq. 2. Let \(s^{*}\) denote the direction scalar, i.e., \(s^{*}=_{s\{ 1\}}\|-s v_{1}\|\). Under the same conditions as in Theorem 4.1, and assuming a positive eigenvalue gap (see Assumption B.2 for a definition), we have the following: 1. If \((_{}^{SAM},v_{1}(^{2}f_{}(X_{t})))  1-()\), then the SDE (Eq. 4) becomes_

\[dX_{t}=-_{}^{SAM}(X_{t})dt+(^{SAM})^{ }dW_{t},\] (5)

_where \(_{}^{SAM}(X_{t}):= f(X_{t})+ \| f_{}(X_{t})\|+}{2}_{1}( ^{2}f_{}(X_{t}))\); 2. If \((_{}^{SAM},v_{1}(^{2}f_{}(X_{t})))  1-(^{2})\), then the SDE (Eq. 4) becomes_

\[dX_{t}=-_{^{2}}^{SAM}(X_{t})dt+(^{SAM })^{}dW_{t},\] (6)

_where \(_{^{2}}^{SAM}(X_{t}):= f(X_{t})+ s^{*}_{1}(^{2}f_{}(X_{t}))v_{1}(^{2}f_{ }(X_{t}))+}{2}_{1}(^ {2}f_{}(X_{t}))\)._

The proof is deferred to Appendix B. In Corollary 4.1.1, we rigorously formalize our intuition from Section 4.1. If the alignment is at least \(1-()\), we conclude that the SAM trajectory comprises three components: the gradient of the loss, the gradient of the gradient norm, and the gradient of the top eigenvalue, with respective scales \(1,\), and \(}{2}\). Under this well-aligned condition, the SAM trajectory minimizes the loss while implicitly regularizing both the gradient norm and sharpness. This demonstrates SAM's complex implicit bias, which is not solely influenced by second- or third-order terms, as summarized in previous work (Wen et al., 2022; Compagnoni et al., 2023). For empirical evidence supporting this observation, we refer readers to Appendix A.

Furthermore, if the alignment is at least \(1-(^{2})\), then the gradient of the gradient norm oscillates in the direction of the top eigenvector. Notably, Bartlett et al. (2023) derived a similar discrete SAM dynamic under specific conditions (Theorem 20), where the parameter trajectory oscillates in the direction of the top eigenvector while regularizing the leading eigenvalue. However, their conditions are stricter than ours, assuming that the parameters are already close to the minimum and requiring a specific initialization. When these conditions are met, they require an alignment of the gradient with the top eigenvector of at least \(1-()=1-(^{4})\). In comparison, our SDE framework is more general.

**Comparison with Wen et al. (2022).** Wen et al. (2022) derived an implicit bias similar to our cubic term in the third-order SDE (Eq. 5) for SAM through the analysis of the Riemannian flow near the minimizer manifold. However, our work fundamentally differs from theirs. First, their theory requires a much longer training time \((^{-1}^{-2})\) compared to our \((^{-1})\). Thus, our SDE corresponds to the initial phase of their analysis regarding time scale, during which they do not conclude any implicit bias. In contrast, our SDE (Eq. 5), which indicates that the implicit bias comprises three components with different scales, provides richer insights in this phase. Second, they require \((1/)\) to be sufficiently small, causing \(\) to be exponentially smaller than \(\), whereas our theory accommodates a more practical range, \(=(^{})\).

## 5 Eigen-SAM: an explicit regularization method for the top eigenvalue of the Hessian

### Failure of perturbation-eigenvector alignment in practice

Within the theoretical framework of Section 4, a natural question arises: _Is the perturbation-eigenvector alignment sufficient in practice for SAM to effectively minimize sharpness?_ Unfortunately,we have empirically verified that such alignment may be poor in practice, even for relatively simple models. As a result, the regularization effect on the largest eigenvalue, as discussed in Corollary 4.1.1, may not be clearly observable in practical scenarios.

To investigate this phenomenon, we trained a 6-layer SimpleCNN model, as used in Jastrzebski et al. (2021); Deng et al. (2024), on CIFAR-10 (Krizhevsky et al., 2009). Figure 1 illustrates two key findings from our experiments. The left panel shows that the alignment between the perturbation vector and the top eigenvector is indeed poor during training. Consequently, the right panel reveals that SAM is unable to efficiently minimize the top eigenvalue when alignment is weak. These results highlight the limitations of SAM in real-world scenarios where ideal alignment cannot be assumed.

### Proposed method: Eigen-SAM

To address the issue of poor alignment, we propose a novel algorithm called Eigen-SAM, which aims to explicitly align the perturbation vector with the top eigenvector. This approach makes SAM's update closer to the SDE approximation in Eq. 5, where the third-order term in the drift coefficient can effectively minimize the largest eigenvalue. To achieve this alignment, we estimate the top eigenvector of the Hessian matrix once every \(p\) mini-batch steps (with \(p=100\) in our implementation) using the power method for \(q\) iterations (with \(q=5\) in our implementation). This strategy of intermittently estimating the Hessian matrix has been shown to be effective in practice (Liu et al., 2023).

After obtaining an estimate \(\) of the top eigenvector, we decompose it into components parallel and perpendicular to the gradient direction, then add the perpendicular component to the perturbation vector to enhance alignment explicitly:

\[ =_{}+_{}\] (7) \[_{}^{} =}{\| f_{}\|}+ ( f_{},)_{},\] (8)

where \(\) is a hyperparameter that controls the strength of the explicit alignment. Since \(+v\) and \(-v\) are equivalent eigenvectors, we include \(( f_{}(x),)\) to determine the direction of \(\). Here, we always choose \(\) to have a smaller angle with the gradient. The full algorithm is presented in Algorithms 1 and 2. In Appendix D, we provide an in-depth discussion of the theoretical properties of Eigen-SAM, including sufficient conditions for improving alignment (Proposition D.1) and its convergence rate (Theorem D.2), which is comparable to that of SAM.

Figure 1: Alignment and top eigenvalue for a 6-layer CNN model trained on CIFAR-10. The left panel shows the trend of alignment during SAM training; the shaded area represents the 95% confidence interval. The right panel displays the trend of the top eigenvalue over the course of training.

### Analysis of additional computational overhead

The additional overhead in Eigen-SAM arises from running the Hessian-vector product \(q\) times every \(p\) steps to estimate the top eigenvector. The Hessian-vector product requires roughly \(1-2\) times the time needed to compute the gradient, so the overhead of our algorithm is approximately \(2+\) to \(2+\) times that of SGD, compared to \(2\) times for standard SAM. For larger models, the computation time for the Hessian-vector product remains nearly constant. For a detailed analysis of the computation cost of Hessian-vector products, we refer readers to Dagreou et al. (2024).

## 6 Experiments

### Numerical simulation of the third-order SDE

In this subsection, we validate the approximation error between our proposed SDE (Eq. 4) and the discrete SAM algorithm (Eq. 1). We trained a fully-connected network with one hidden layer, consisting of 784 hidden units and using GeLU activation, on the MNIST dataset (Deng, 2012). The training was conducted with \(=0.01\) and \(=0.2\) (where \(^{}\)). During training, we carefully tracked several key metrics, including training loss, test loss, test accuracy, parameter norm, gradient norm, and the top eigenvalue of the Hessian, as shown in Figure 2.

Our results demonstrate that the approximation error of our third-order SDE is significantly lower than that of the previous second-order SDE. Specifically, the curves of our SDE closely match those of the discrete SAM across all tracked metrics, underscoring the accuracy and reliability of our approximation. This close alignment suggests that our proposed continuous-time approximation provides a more precise representation of the discrete SAM dynamics, thus enhancing the theoretical understanding of SAM.

### Image classification from scratch

To evaluate the effectiveness of Eigen-SAM, we applied it to several image classification tasks on benchmark datasets, including CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), Fashion-MNIST (Xiao et al., 2017), and SVHN (Netzer et al., 2011). For these tasks, we used ResNet-18 (He et al., 2016), ResNet-50 (He et al., 2016), and WideResNet-28-10 (Zagoruyko and Komodakis, 2016) models.

We selected SGD as the base optimizer and applied basic data augmentation techniques, including horizontal flips, padding by four pixels, and random cropping. The batch size was set to 256, with training conducted for 200 epochs. We used an initial learning rate of 0.1 for CIFAR-10, Fashion-MNIST, and CIFAR-100, and 0.01 for SVHN, adjusting the learning rate over time with a cosine schedule. The weight decay was set to \(5 10^{-5}\), and the momentum was 0.9. Detailed hyperparameter settings are provided in Appendix E.

The test set performance, reported in Table 1 along with the \(95\%\) confidence interval, shows that Eigen-SAM consistently achieves state-of-the-art performance across various datasets and models, validating its effectiveness and robustness.

### Finetuning

We evaluated performance by fine-tuning a ViT-B-16 model Dosovitskiy et al. (2020) pre-trained on ImageNet for CIFAR-10 and CIFAR-100. We used the checkpoint provided by PyTorch's official repository1. For SAM and Eigen-SAM, we used an initial learning rate of 0.01 and trained for 4k steps, while for SGD, we trained for 8k steps. Table 2 shows the test accuracy, where Eigen-SAM consistently outperforms the baselines.

 Architecture & Method & CIFAR-10 & CIFAR-100 & Fashion-MNIST & SVHN \\   ResNet18 & SGD & \(94.8_{ 0.2}\) & \(74.6_{ 0.2}\) & \(94.9_{ 0.2}\) & \(96.1_{ 0.1}\) \\  & SAM & \(95.5_{ 0.1}\) & \(77.4_{ 0.2}\) & \(95.4_{ 0.1}\) & \(96.3_{ 0.1}\) \\  & Eigen-SAM & **95.9\({}_{ 0.2}\)** & **78.3\({}_{ 0.2}\)** & **95.6\({}_{ 0.2}\)** & **96.5\({}_{ 0.1}\)** \\  ResNet50 & SGD & \(95.0_{ 0.1}\) & \(76.6_{ 0.2}\) & \(94.8_{ 0.1}\) & \(96.1_{ 0.1}\) \\  & SAM & \(95.6_{ 0.2}\) & \(79.0_{ 0.2}\) & \(95.4_{ 0.1}\) & \(96.4_{ 0.1}\) \\  & Eigen-SAM & **96.2\({}_{ 0.1}\)** & **79.7\({}_{ 0.1}\)** & **95.7\({}_{ 0.1}\)** & **96.6\({}_{ 0.1}\)** \\  WideResNet-28-10 & SGD & \(95.7_{ 0.1}\) & \(79.8_{ 0.2}\) & \(95.1_{ 0.1}\) & \(96.2_{ 0.1}\) \\  & SAM & \(96.5_{ 0.1}\) & \(82.0_{ 0.2}\) & \(95.6_{ 0.1}\) & \(96.4_{ 0.1}\) \\  & Eigen-SAM & **96.8\({}_{ 0.1}\)** & **82.8\({}_{ 0.1}\)** & **95.9\({}_{ 0.1}\)** & **96.7\({}_{ 0.1}\)** \\   

Table 1: Test accuracy on CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN.

 Architecture & Method & CIFAR-10 & CIFAR-100 \\   ViT-B-16 & SGD & \(98.0_{ 0.1}\) & \(88.6_{ 0.1}\) \\  & SAM & \(98.4_{  0.1}\) & \(89.5_{ 0.1}\) \\  & Eigen-SAM & **98.5\({}_{ 0.1}\)** & **89.8\({}_{ 0.1}\)** \\   

Table 2: Test accuracy for fine-tuning ViT-B-16 pretrained on ImageNet-1K on CIFAR-10 and CIFAR-100.

Figure 2: Training dynamics of discrete SAM, second-order SDE, and third-order SDE during training. Metrics include training loss, test loss, test accuracy, parameter norm, gradient norm, and the top Hessian eigenvalue. Each plot illustrates how each approach affects loss dynamics and key stability metrics.

### Sensitivity analysis

We investigated the impact of varying the hyperparameter \(\) on the test accuracy of Eigen-SAM. We conducted experiments on ResNet-18 with CIFAR-100, testing a range of \(\) values, as shown in Figure 3. We observed that the test accuracy peaks at \(=0.2\), yielding a 0.9% improvement in test accuracy compared to \(=0\), which corresponds to the standard SAM. These results suggest that \(\) is a robust hyperparameter, as its variations do not cause significant performance fluctuations, while consistently enhancing performance.

In Table 3 and Table 4 in Appendix F, we demonstrate how larger values of \(p\) affect generalization performance and observe that setting \(p\) to 1000 (resulting in less than \(1\%\) additional overhead) retains most of the performance gains. In Figure 5 in Appendix F, we demonstrate the convergence speed of Algorithm 1, which typically requires only a few steps to converge.

### Hessian spectrum

Figure 3 shows the Hessian spectrum at the 200th epoch for ResNet-18 trained on CIFAR-100 using SAM and Eigen-SAM. We observe that the model trained with Eigen-SAM has both a smaller top eigenvalue and trace, with more eigenvalues concentrated near zero. This observation aligns with our motivation for proposing Eigen-SAM and explains why Eigen-SAM generalizes better than SAM.

## 7 Conclusion

In this work, we analyzed the training dynamics of SAM using a third-order SDE, identifying the alignment between the perturbation vector and the top eigenvector as a crucial factor for effective sharpness regularization. However, our empirical analysis showed that this alignment is often poor in practice. Building on our theoretical framework and experimental insights, we proposed Eigen-SAM, an algorithm that intermittently estimates the top eigenvector of the Hessian matrix and incorporates its component orthogonal to the gradient into the perturbation, explicitly regularizing the top Hessian eigenvalue. Extensive experiments demonstrated that our third-order SDE yields a smaller approximation error than previous models and that Eigen-SAM achieves state-of-the-art performance across various tasks, validating both its accuracy and effectiveness.