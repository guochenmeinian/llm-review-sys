# InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding

Junda Wu\({}^{1}\)1  Tong Yu\({}^{2}\)1  Rui Wang\({}^{3}\)  Zhao Song\({}^{2}\)  Ruiyi Zhang\({}^{2}\)

Handong Zhao\({}^{2}\)  Chaochao Lu\({}^{4}\)2  Shuai Li\({}^{5}\)3  Ricardo Henao\({}^{3,6}\)

\({}^{1}\)University of California, San Diego \({}^{2}\)Adobe Research \({}^{3}\)Duke University

\({}^{4}\)University of Cambridge \({}^{5}\)Shanghai Jiao Tong University \({}^{6}\)KAUST

juw069@ucsd.edu

{tyu,zsong,ruizhang,hazhao}@adobe.com

{rw161,ricardo.henao}@duke.edu

cl641@cam.ac.uk shuaili8@sjtu.edu.cn

###### Abstract

Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We have also empirically observed that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing the mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method, InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) explore proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant information captured in the learnt prompts. Extensive experiments validate that InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform traditional prompt tuning methods. Finally, we provide a formal theoretical result to show that a gradient descent type algorithm can be used to train our mutual information loss.

## 1 Introduction

Soft prompt tuning has shown great successes in a wide range of natural language processing tasks, especially in low-resource scenarios . With a relatively small size of prompt parameters appended to the input of the context, the language model can be adapted to the downstream tasks with the large scale pretrained parameters frozen. Compared with conventional fine tuning methods, prompt tuning requires less memory and computational resources to update these significantly smaller sized prompt parameters. In addition, in low-shot learning scenarios, prompt tuning can prevent the language model from overfitting on the training data, thus maintaining the generalization ability of pretrained language models.

However, recent works reveal that it is non-trivial to find a proper initialization of the prompt tokens. Several works have investigated the effect of prompt initialization on the prompt tuning performances and showed that the performances of prompt tuning are highly sensitive to the prompt initialization. However, since the proper prompt initialization can vary to different downstream tasks and pretrained language models, it is hard to find very accurate knowledge to guide us to obtain the proper initialization .

In addition to the above limitations,, we also empirically observe that conventional prompt tuning methods cannot effectively learn sufficient task-relevant information from prompt tokens. Specifically, the prompts may fail to learn sufficient information that is relevant to the downstream tasks. To understand the relevance between the prompt tokens and downstream tasks, we calculate the conditional mutual information (CMI) between the prompt tokens and the latent representation from the language model conditioned on the input context. We follow  in determining the positions of prompt tokens inserted between the input sentences. Figure 1 shows the distribution of CMI of the prompts resulting from different methods. The randomly sampled prompts have the lowest CMI. The prompts learned by a soft prompt tuning method, WARP , can have relatively higher CMI than the handcrafted ones. By directly maximizing the CMI, our InfoPrompt (detailed in Section 3) facilitates learning of more informative prompts. Without the guidance of task-relevant information, randomly exploring the optimal prompts within the large continuous embedding space of the prompt tokens can be inefficient, _i.e._ a similar challenge is also discussed in . Some related results  show that prompt tuning takes much larger numbers of epochs to converge than fine tuning. Comparatively, thanks of the guidance of CMI, our propose InfoPrompt allows prompt tuning to converge much faster. We also provide theoretical guarantees of the convergence of the proposed losses when training with gradient descent based algorithms.

Overall, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations), conditioned on the input context. With this framework, we develop InfoPrompt with two novel mutual information based loss functions. (i) To explore proper prompt initialization for the downstream tasks and learn prompts with sufficient task-relevant information, we optimize the _head loss_ which maximizes the mutual information between the prompt and the task-specific classification head. By optimizing this head loss, the prompt can effectively learn task-relevant information from the downstream tasks, since the task-specific classification head usually contains the information from the downstream tasks. Besides, the mutual information loss can help to guide the learning of the prompt tokens in the early steps to tackle the initialization challenge, since the classification head parameters can learn the downstream task information more quickly. (ii) To further encourage the output representation from the pretrained language model (_i.e._, encoded representation) to be more aware of the task-relevant information captured in the learnt prompt, we optimize the _representation loss_ which maximizes the conditional mutual information between the prompt embeddings and the feature representations conditioned on the input context.

Our contributions are summarized as:

* We revisit the challenge of initialization with prompt tuning and show that existing prompt tuning methods fail to learn sufficient task-relevant information.
* We propose InfoPrompt, a framework to solve such challenges from a information-theoretic perspective. Specifically, we develop two novel loss functions that effectively find proper

Figure 1: Distributions of the CMI metrics of the prompts learned or handcrafted from different methods on MRPC and SST-2 . By our InfoPrompt, the relevance between the prompt tokens and downstream tasks is the highest among all methods.

prompt initialization and learn sufficient task-relevant information from down-stream tasks, without requiring any prior knowledge.
* Extensive experiments on multiple tasks and datasets validate that, InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform existing prompt tuning methods with higher accuracy.
* We provide a formal theoretical result to show that our proposed loss functions can be optimized using gradient descent based algorithm with convergence guarantees.

## 2 Preliminary

### Prompt Tuning

Prompt tuning has shown great successes in a wide range tasks of NLP [60; 37; 66]. Let \(\) denote the encoder of a pretrained language model, _e.g._, the Roberta-Large . Assume \(X=\{x_{1},x_{2},,x_{n}\}\) is a length-\(n\) text sequence of and \(Y\) is its classification label. In prompt tuning, we add extra information \(P\) for the encoder to condition on for its prediction of \(Y\). \(P=\{p_{1},,p_{n_{p}}\}\) is a sequence of prompt embeddings and \(n_{p}\) is the number of prompt tokens. \(p_{i}^{D}\), \(i=1,,n_{p}\), is an embedding vector with dimension \(D\) and \(D\) is also the embedding dimension of the pretrained language model. We first embed each token of \(X\) into its corresponding token embedding from the pretrained language model. \(P\) is inserted into the resulting embedding sequence of \(X\), and the resulting sequence is further encoded by the pretrained encoder \(\) into the representation space of the pretrained language model. The template for inserting of prompt tokens is detailed in Section 4. Formally, we denote such a process by \(Z=(P,X)\), with \(Z\) being the output representation from the pretrained encoder. The model prediction for \(X\) is made on top of \(Z\) with a trainable classification head parameterized by \(\), denoted as \(h_{}\), whose output \(h_{}(Z)\) is the probability distribution over all possible classification labels. For classification, the prompts are trained via minimizing the following loss function,

\[_{}=(h_{}(Z),Y)\]

Parameters of the pretrained encoder \(\) is frozen during training. Different from previous works (_e.g._, ) where the prompts are directly learnt, the prompts in our approach are encoded from the input \(X\). In this way, the resulting prompts can better capture task-relevant information from the training text \(X\). We will elaborate on how the prompts are encoded from input \(X\) in Section 4.

### Mutual Information

Mutual information (MI) is a metric in information theory [79; 19], which quantifies the amount of information shared between two random variables. The mutual information between two random variables \(A\) and \(B\) is

\[(A;B)=_{p(a,b)}[D_{KL}[p(a|b)\|p(b)] ].\]

Inspired by , we use MI as the criterion for comparing prompts and other model parameters. MI has also been applied to measure the similarity between the masked token and the corresponding context in the pretraining of Multilingual Masked Language Modeling , the relevance between documents and sentences in document summarization  and the source and target sentences in Neural Machine Translation (NMT) .

## 3 Our Method: InfoPrompt

As mentioned above, we want the learnt prompts to be task-relevant. To achieve this, we notice that the classification head is trained with both the data representation and the classification labels, thus should contain rich information of the learnt tasks. In encouraging the task-relevancy of the learnt prompts, we consider maximizing the mutual information between the prompt and the parameters of the classification head, denoted as \(\). By maximizing such mutual information, the learnt prompt will be more aligned with the training data with which the classification head is trained, thus captures more task-relevant information from training. Further, in order for the pretrained language model to properly leverage the task-relevant information in the prompt, we additionally maximize the mutual information between the prompt and the representation from the pretrained language model, so that the encoded representation can be aware of the task-relevant information captured by the prompt. In addition, we also provide theoretical guarantees of the convergence of those losses when training with gradient descent, demonstrating that our method can converge more easily than existing prompt tuning methods. Below, we denote the negative mutual information between the prompt and parameters of the classification head as the _head loss_. The negative mutual information between the prompt and representations from the pretrained language model is denoted as the _representation loss_.

### The Head Loss

The head loss is the negative mutual information between the prompt \(P\) and parameters \(\), _i.e._, \(-I(P;|X)\). In maximizing \(I(P;|X)\), we follow  that approximates it with the following lower bound,

\[(P;|X) C+_{NCE}(P,,X),\]

where \(C\) is a constant, \(_{NCE}\) is a Noise Contrastive Estimation (NCE) of mutual information,

\[_{NCE}=[^{ K}(l(P_{k},|X))}],\]

and \(\{P_{k}\}_{k=1}^{K}\) are the negative prompt samples for contrastive learning. In practice, we randomly sample \(K-1\) tokens from the context as the negative samples, _i.e._, \(\{P_{k}\}_{k=2}^{K}\), and the positive sample is \(P_{1}=P\).

We model the score function \(l(P,|X)\) as a standard bilinear function with the learnable matrix \(W_{1}\)

\[l(P,|X)=P^{}W_{1}.\]

where \(\) and \(P\) are encoded from \(X\), and \(W_{1}\) is a trainable matrix. Since the classification head is learnt on top of the output from the last layer of the pretrained language model, the learning of its parameters \(\) is easier than the learning of the prompt \(P\) (the input layer of the pretrained language model). Therefore, \(\) may capture more task-relevant information than \(P\) in the early stage of training. By maximizing the mutual information between \(\) and \(P\), the task-relevant information can be transferred to \(P\) in the initial training steps. In this way, \(P\) can be more task-relevant especially in the early training stage. Experiments in Section 6 also show that our head loss, \((P;|X)\), can facilitate the training of the initial training steps.

### The Representation Loss

The representation loss, denoted as \(-(P;Z|X)\), is defined as the negative of mutual information between the prompt \(P\) and the encoded representation from the pretrained language model, _i.e._, \(Z=(P,X)\). Similar to the head loss, we approximate the representation loss with its lower bound,

\[(P;Z|X)(N)+_{NCE}(P,Z|X),\]

and,

\[_{}=[^{K}(l(P,Z_{k}|X))}],\]

\(\{Z_{k}\}_{k=1}^{K}\) are the negative samples. Here, we overload the notations of InfoNCE loss \(_{NCE}\) and score function \(l\) for conciseness. Let \(W_{2}\) be a trainable matrix, the function \(l\) for the representation loss is defined by,

\[l(P,Z|X)=P^{}W_{2}Z.\]

We use variational inference methods  to recover the latent distribution of \(Z\). Specifically, we assume that the latent distribution is \(N(,)\), where \(N(,)\) is the normal distribution with mean \(\) and diagonal covariance matrix \(\). We model \(\) and \(\) via,

\[=f_{}(Z),=f_{}(Z).\]\(f_{}\) and \(f_{}\) are trainable fully connected layers. Since the negative samples of \(Z\), _i.e._, \(\{Z_{k}\}_{k=1}^{K}\), should not be paired with \(P\), we assume the \(\{Z_{k}\}_{k=1}^{K}\) are drawn from \(N(^{},^{})\), _s.t._,

\[^{}=f_{}(Z^{}),^{}=f_{}(Z^{}).\]

In contrast to \(Z=(P,X)\), we have \(Z^{}=(X)\) where \(\{Z_{k}\}_{k=1}^{K}\) are not paired with \(P\). By maximizing the representation loss \(I(P;Z|X)\), we encourage the encoded representation \(Z\) to be more aware of the prompt \(P\), so that the task-relevant information in \(P\) can be properly encoded by the pretrained language model in producing \(Z\).

### Overall Objective

We minimize the following objective in prompt tuning:

\[=_{}-(P;Z|X)- (P;|X).\] (1)

We denote \(_{}\) as the task loss. \(\) and \(\) are balancing parameters for the proposed representation loss and head loss, respectively. We denote our approach as _InfoPrompt_. More details about the implementation and configurations are provided in Section 4.2.

### Theoretical Guarantees

We state our main theoretical result as follows. Due to the space limit, we delay the proof into Appendix.

**Theorem 3.1**.: _Given the Loss function \(\) (Eq. (1)) and conditions specified in Appendix C.1 and \(D\), using gradient descent type of greedy algorithm, we can find the optimal solution of that loss function._

We provide theoretical guarantees of the convergence of those losses trained by conventional gradient descent type algorithms. In Section 4, we empirically observe that our method converges more easily than traditional soft prompt tuning methods and requires fewer training epochs.

## 4 Experiments

### Datasets

We conduct experiments with datasets of sequence classification from the GLUE benchmark , along with those of relation extraction tasks and NER tasks. We choose four sequence classification tasks from the GLUE benchmark: RTE (Recognizing Textual Entailment, ), MRPC (Microsoft Research Paraphrase Corpus, ), CoLA (Corpus of Linguistic Acceptability, ) and SST-2 (Sentence Sentiment Treebank, ). We choose these tasks because their datasets are of smaller sizes and prompt tuning is comparably more effective in low-resource settings [40; 62]. For the task of relation extraction, we evaluate our method on the ACE2005 corpus and the Semeval-2010 datasets . We also use the ACE2005 corpus for the task of NER. Note that the entity spans for NER have been given ACE2005. Unlike the standard NER model that learns to predict the entity span and entity type simultaneously from the raw text sequence, our model only predicts the entity type based on the given entity span. We follow the same data splitting strategy for ACE2005 corpus as the previous work [103; 71]. For the Semeval-2010 tasks, we follow the official data partition .

### Experiment Settings

We follow the resource constrained scenario in  that trains each task with only 64 or 256 samples. We experiment with \(n_{p}=1\) and \(n_{p}=4\) prompt tokens for each task. The prompt tokens are inserted into the template for each task. Similar to , we adopt the RoBERTa-large model as our pretrained encoder. We freeze the pretrained parameters and only train the parameters of the prompt head and prompt tokens. During training, we empirically set \(=0.1\) and \(=0.05\). The number of negative samples is \(K=32\). The learning rate is \(1e-3\) and the batch size is 8. For each task, we report the results after 30 epochs, averaged over 5 random seeds. To encode the prompt \(P=[p_{1},,p_{n_{p}}]\) from \(X\), we first encode \(X\) into \(P^{}^{D}\) via \(P^{}=(X)\). We denote the up-sampling and down-samplingprojections similar in . For each \(p_{i}^{D}\), we have \(p_{i}=W_{i}^{}W_{i}^{}P^{}\), \(W_{i}^{}^{D 64}\), \(W_{i}^{}^{64 D}\).

For the tasks of sequence classification and relation extraction, we follow the template of  that contains a \([]\) token. The representation \(Z\) is obtained from the \([]\) token from the last layer of the RoBERTa-Large encoder. For the task of NER, we have the \([]\) token before the given entity span, with the rest being the same as for sequence classification.

### Baselines and Ablations

As mentioned above, our method with Eq. (1) in denoted as InfoPrompt. In the experiments, we compare our method with the following baselines:

* Finetuning: We fine tune all the parameters from the pretrained encoder on each task. Finetuning is included as the upper bound for the model performance, since it is more computational expensive compared with only training the prompt parameters.
* Adapter : Similar to prompt tuning, this is also a way of parameter-efficient training for pretrained language models. Specifically, instead of adding the prompt tokens in the input, we add adapters after the feed-forward module in each transformer layer.
* WARP : Different from our approach, the prompt tokens of WARP are not generated from the input sequence. the prompt tokens are insert into the input sequence. During training, the pretrained encoder is frozen and only the prompt tokens are trainable.
* IDPG : Similar to our approach, the prompt tokens are generated from the input sequence. The pretrained encoder is frozen and the prompt generator is trainable.

In evaluating the effectiveness of our proposed loss functions, we consider the following two ablations:

* \(=0\): We disable the head loss during training via \(=0\), while keeping \(=0.05\).
* \(=0\): We disable the representation loss during training via \(=0\), while keeping \(=0.1\).
* \(==0\): We disable both the losses. The prompt parameters are trained with \(L_{}\).

   &  &  &  &  & \\  & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & Average \\  Finetuning & 0.6131 & & 0.7798 & 0.8873 & 0.9427 & 0.8057 \\ Adapter  & 0.5552 & & 0.5776 & 0.6814 & 0.9472 & 0.6904 \\  WARP  & 0.5282 & 0.5911 & 0.6282 & 0.6426 & 0.8039 & 0.8186 & 0.9507 & 0.9587 & 0.7403 \\ IDPG  & 0.5556 & 0.5646 & 0.6282 & 0.6534 & 0.7941 & 0.8039 & 0.9587 & 0.9587 & 0.7396 \\ InfoPrompt & 0.5631 & 0.6018 & 0.6751 & 0.6968 & 0.8039 & 0.8137 & 0.9576 & 0.9599 & 0.7590 \\ \(=0\) & 0.5699 & 0.5853 & 0.6751 & 0.6787 & 0.7941 & 0.8137 & 0.9495 & 0.9587 & 0.7531 \\ \(=0\) & 0.5546 & 0.5579 & 0.6065 & 0.6318 & 0.7892 & 0.7966 & 0.9472 & 0.9610 & 0.7306 \\ \(=0,=0\) & 0.5032 & 0.5732 & 0.6173 & 0.6029 & 0.7917 & 0.7672 & 0.9495 & 0.9564 & 0.7202 \\  

Table 1: Results on Sequence Classification.

   &  &  &  & \\  & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & Average \\  Finetuning & 0.8119 & & 0.9054 & & 0.8506 & & 0.8560 \\ Adapter  & 0.5073 & & 0.8329 & & 0.6570 & & 0.6657 \\  WARP  & 0.6384 & 0.6596 & 0.8174 & 0.8607 & 0.6702 & 0.7284 & 0.7291 \\ IDPG  & 0.6079 & 0.6132 & 0.8360 & 0.8931 & 0.6408 & 0.6776 & 0.7114 \\ InfoPrompt & 0.6914 & 0.7616 & 0.8526 & 0.8962 & 0.7563 & 0.7917 & 0.7916 \\ \(=0\) & 0.6914 & 0.7285 & 0.8452 & 0.8635 & 0.7471 & 0.7865 & 0.7770 \\ \(=0\) & 0.6967 & 0.7470 & 0.8351 & 0.8698 & 0.7449 & 0.7538 & 0.7746 \\ \(=0,=0\) & 0.5364 & 0.7285 & 0.8512 & 0.8661 & 0.7490 & 0.7799 & 0.7519 \\  

Table 2: Results on Relation Extraction and NER.

## 5 Experimental Results

### Training with the Full dataset

Table 1 and 2 show the results of training with the full dataset for each task. We can observe that the results with our InfoPrompt are generally higher than those of the other parameters-efficient baselines that freeze the pretrained RoBERTa-Large parameters (_e.g._, WARP and Adapter). Finetuning generally has better performance than the other approaches. This is because it allows training with all the model parameters, which is at the expense of more computation cost during training. As mentioned above Finetuning is intended to be included as the upper bound for performance. Moreover, we can find that the performance with \(=0\) and \(=0\) is lower than that of InfoPrompt, indicating shows that it is beneficial to learn task-relevant prompt tokens with the proposed head loss and representation loss. Further, the performance gap between \(=0=0\) and \(==0\) shows that the proposed functions are effective when added to naive prompt tuning, _i.e._, with only \(_{pred}\).

### Training with the Few-Shot Datasets

The results for training with few-shot datasets are listed in Table 3 and 4. Compared with training with the full dataset (Table 1 and 2), we can find that the performance gap between our proposed InfoPrompt and the baselines is generally larger in the few-shot setting. Unlike the full datasets, the few-shot datasets contain much less information regarding the task to be learnt. As the result, the prompts learnt with solely the task loss (_e.g._, with WARP or \(==0\)) may easily overfit to the task-irrelevant information given the few-shot datasets. In such a scenario, it would be important to explicitly encourage the learnt prompt to be task-relevant, _i.e._, via our proposed loss functions based on mutual information maximization. This explains why InfoPrompt yields larger performance gain when trained with few-shot datasets. Similar to training with the full datasets, the performance gains of InfoPrompt compared with InfoPrompt (\(=0\)) and InfoPrompt (\(=0\)) show the effectiveness of our proposed loss functions in the few-shot scenario.

   &  &  &  & \\  & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & Average \\  Finetuning & 0.1285 & 0.4013 & 0.3033 & 0.4358 & 0.2223 & 0.4829 & 0.3290 \\ Adapter  & 0.1086 & 0.1815 & 0.2345 & 0.2437 & 0.1211 & 0.177 & 0.1777 \\  WARP  & 0.1404 & 0.2556 & 0.3082 & 0.4369 & 0.1708 & 0.3684 & 0.2801 \\ IDPG  & 0.2596 & 0.2503 & 0.3334 & 0.4048 & 0.1984 & 0.3577 & 0.3007 \\ InfoPrompt & 0.2119 & 0.2993 & 0.3331 & 0.4739 & 0.2113 & 0.4034 & 0.3222 \\ \(=0\) & 0.2026 & 0.2834 & 0.3225 & 0.4776 & 0.2153 & 0.3739 & 0.3126 \\ \(=0\) & 0.2013 & 0.2874 & 0.3208 & 0.4615 & 0.2072 & 0.3629 & 0.3069 \\ \(=0,=0\) & 0.1974 & 0.2728 & 0.3142 & 0.4662 & 0.2278 & 0.3276 & 0.3010 \\  

Table 4: Few-shot results on Relation Extraction and NER. We experiment with \(N=64\) and \(N=256\) samples for each task. The number of prompt is fixed to \(n_{p}=4\) for all soft prompt tuning methods.

   &  &  &  &  & \\  & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & Average \\  Finetuning & 0.1746 & 0.4086 & 0.4840 & 0.5687 & 0.7107 & 0.7819 & 0.8027 & 0.8853 & 0.6153 \\ Adapter  & 0.0627 & 0.2486 & 0.5487 & 0.5668 & 0.5931 & 0.6250 & 0.4908 & 0.664 & 0.4750 \\  WARP  & 0.0749 & 0.0785 & 0.5596 & 0.5812 & 0.7083 & 0.7083 & 0.5872 & 0.7638 & 0.5077 \\ IDPG  & 0.0902 & 0.1513 & 0.5018 & 0.5523 & 0.6593 & 0.7010 & 0.5424 & 0.8188 & 0.5021 \\ InfoPrompt & 0.1567 & 0.1750 & 0.6137 & 0.6580 & 0.7059 & 0.7377 & 0.6697 & 0.7305 & 0.5559 \\ \(=0\) & 0.1479 & 0.1447 & 0.5776 & 0.6318 & 0.6936 & 0.7328 & 0.664 & 0.7294 & 0.5402 \\ \(=0\) & 0.1372 & 0.1433 & 0.5812 & 0.5957 & 0.6838 & 0.7132 & 0.5631 & 0.656 & 0.5092 \\ \(=0,=0\) & 0.0919 & 0.1397 & 0.5668 & 0.5523 & 0.6985 & 0.7108 & 0.5505 & 0.6296 & 0.4925 \\  

Table 3: Few-shot results on Sequence Classification. We experiment with \(N=64\) and \(N=256\) samples for each task. The number of prompt is fixed to \(n_{p}=4\) for all soft prompt tuning methods.

## 6 Analysis

### Loss Landscapes

To provide a more comprehensive understanding of the effectiveness of our proposed loss functions, we plot the landscapes of the loss functions in the parameter space of the prompt tokens. The landscapes illustrate how the values of the loss functions vary with the input prompt tokens. Since the prompt tokens are high-dimensional vectors, _i.e.,_ each token has the dimension of 1024 for RoBERTa-Large, we visualize their associated loss values via projecting the prompt tokens into a 2D subspace. Specifically, we follow previous work on token embedding analysis  that projects the prompt tokens into the top-2 principal components computed from the pretrained token embeddings of RoBERTa-Large. We only insert one prompt token into the input sequence during visualization.

Taking the task of MRPC as an example, we plot the 2D landscapes of the task loss and the representation loss in Figure 1(a) and 1(b), respectively. Both figures are plotted with the same scale, _i.e.,_ with the same values of the prompt token. The axis values are the offset from the mean of the pretrained RoBERTa-Large token embeddings. The loss values shown in the figures are the average of 20 random samples from MRPC. In Figure 1(a), we can find that there are a lot of local minimum in the landscapes of the task loss. This is consistent with the observations of the previous works [37; 91] that prompt tuning is difficult to be optimized with and sensitive to initialization, _e.g.,_ the optimization can get easily overfit to a local minimum without proper initialization. From Figure 1(b), we can observe that the loss landscape of our proposed representation loss is much smoother compared to the task loss in Figure 1(a). With smoother landscapes, the optimization with our proposed loss functions can be more stable (also shown in Section 6.2), _i.e.,_ less likely to be trapped in a local minimum and also guaranteed to converge according to our theoretical results (see Theorem 3.1). Additionally, we plot the trajectory of the first 500 steps during training for InfoPrompt (\(=0.05\)) (green) and \(=0\) (purple) in Figure 1(a) and 1(b). The stars in the plot indicate the initial value of the prompt before training. We find that training with \(=0.05\) can render a larger descent for both the task loss and representation loss, compared to \(=0\). As analyzed in Section 3.1, the language head is easier to learn than the prompt. As the result, parameters of the language head may contain more task-relevant information during the earlier stage of training. By maximizing the mutual information between the head parameter and prompt via the proposed head loss (weighted by \(\)), we encourage the learnt prompt to capture more task-relevant information in the initial training steps, thus resulting \(=0.05\) to have a larger descent than \(=0\) in the trajectories shown in Figure 1(a) and 1(b). We also compare our initialization to some common initialization approaches: Random Uniform and Sampled Vocabulary [60; 37]. By Random Uniform, we randomly sample prompt initialization from the continuous latent space. By Sampled Vocabulary, we randomly sample prompt initialization from language model's vocabulary set. The final results by WARP (Random Uniform), WARP (Sampled Vocabulary) and InfoPrompt are \(0.626\), \(0.672\) and \(0.706\) respectively. The results

Figure 2: The landscapes of the loss functions in the parameter space of the prompt tokens. The landscapes illustrates how the values of loss functions varies with the input prompt tokens. The trajectory shows the first 500 steps during training for InfoPrompt with \(=0.05\) or \(=0\).

validate the effectiveness of our initialization approach. Note that our proposed two loss functions are unsupervised and do not require additional labels.

### Learning Curve

We plot the training curve for the task of NER and SST-2 in Figure 2(a) and 2(b), respectively. Unlike WARP  and Finetuning that train with solely the task loss \(L_{}\), our InfoPrompt also trains with the representation loss and head loss. We can observe that the training of our our InfoPrompt is more stabilized and converges faster, compared with WARP. This can be explained from the landscape plots in Section 6.1. Since the landscape of the task loss is not smooth (Figure 1(a)), the training curve of WARP may exhibit significant perturbation when the optimization overfits to a local minimum, _e.g._, the 10000th step in Figure 2(a). Comparably, our proposed InfoPrompt can smooth the optimization landscape, thus stabilizing the training and result in faster convergence, which is guaranteed by our theoretical results. We observe that Finetuning generally converges faster and ends up with a higher accuracy than InfoPrompt. This is because Finetuning, which trains with all the model parameters, has much larger model capacity during training than prompt tuning (InfoPrompt and WARP). Such results for Finetuning is at the expense of larger computation costs, _i.e._, we need to calculate the gradient for all the model parameters (354M) instead of only the prompt parameters \(P\) (1.3M).

We also validate that our approach is less sensitive to initialization in the early learning stage, compared to WARP. Specifically, across 10 different random seeds, we report the standard errors of the performances by InfoPrompt and WARP in the early learning stage. After the first epoch, on NER, the results by WARP and InfoPrompt are \(0.461 0.038\) and \(0.810 0.025\), respectively. On SST2, the results by WARP and InfoPrompt are \(0.735 0.033\) and \(0.764 0.027\), respectively. The results show that our method has lower standard errors and is less sensitive compared to WARP.

## 7 Related Work

### Soft Prompt Tuning

Soft prompt tuning has become a new paradigm in NLP. Based on some large pretrained models (e.g., BERT , RoBERTa ), a relatively small number of trainable parameters can be added to the input, while the parameters of backbones are fixed. Many works have demonstrated the effectiveness of soft prompt tuning in a wide range of NLP downstream tasks [60; 40; 73; 65], especially in low-resource regions [78; 66; 37]. Some recent works also found the transferable power of soft prompts across domains [101; 91; 94], across language models  and for zero-shot generalization . To further enhance the efficiency of soft prompt parameters and enable better generalization abilities, many works consider multi-task learning [6; 27; 94; 43], or multilingual [14; 50]. Some works also try to explore the prompt with prior knowledge encoded [48; 42; 13]. While most of the initial attempts of soft prompt tuning are not context-aware, some recent works suggest that

Figure 3: The learning curves for the task of NER and SST-2. The training of our our InfoPrompt is more stabilized and converges faster, compared with WARP.

the soft prompt tokens should be conditioned on the input context. Hyperprompt  proposes a hyper-network structure to generate prompt tokens based on task indexes.  and  suggest some context-aware soft prompt generation methods.  proposes a structured soft prompt tuning method. BBT  targets the scenarios where the pre-trained model is not available locally (i.e., deployed online) and its back-propagation operation is not available.

### Information-theoretic Methods in NLP

Information-theoretic methods are widely used in many NLP tasks [55; 99; 90; 51; 70].  and  propose information-theoretic methods for text memorization.  suggests an information-theoretic method for dialogue.  views the multimodal NMT problem in an information-theoretic point of view. For model pretraining,  proposes Infobert to improve the robustness of the BERT model. INFOXLM  proposes a cross-lingual language model based on an information-theoretic framework. For fine-tuning,  proposes an information bottleneck model method for low-resource fine-tuning.  introduces an information-theoretic method to engineer discrete prompts.

### Theoretical Attention Computation

Softmax is one of the major unit in the attention scheme of most recent NLP large language models. Computing the attention matrix faster is a practically interesting question [17; 97; 57]. Recently, a number of theoretical work have tried to study the softmax/attention unit from theoretical perspective. The softmax attention computation can be formally defined as follows: suppose we are given three matrices \(Q^{n k}\) (the query), \(K^{n k}\) (the key), and \(V^{n k}\) (the value), the goal is to compute \((Q,K,V)=D^{-1}(QK^{})V\) where the diagonal matrix \(D\) is \(((QK^{})_{n})\). Here \(K^{}\) denote the transpose of matrix \(K\). The work of [106; 3] consider the static setting, and the work of  considers the dynamic setting.  proposed a tight algorithm for computing Att and provided a lower bound result based on the strong exponential time hypothesis.  provide the results for a more general tensor version of attention which capture the three tuples feature, but classical attention cannot . The work  shows a tight positive result and a negative result. In , they provide an upper bound via lazy update techniques . In , they also present a lower bound result which is based on the Hinted MV conjecture . The work of  proposes two sparsification algorithm to compute attention matrix when the feature dimension \(\) the length of sentence.  shows how to provide a differentially private algorithm for computing attention matrix under differential privacy framework [30; 29].  introduces a hyperattention method and presents an nearly linear time algorithm with provable guarantees.  studies the polynomial based attention scheme and shows that sketching techniques can help speeding up the attention computation.

## 8 Conclusion and Future Work

We revisit the limitations of soft prompt tuning in the initialization. We also empirically discover that conventional prompt tuning methods cannot learn sufficient task-relevant information from prompt tokens. We tackle these limitations from an information-theoretic perspective and propose an information-theoretic prompt tuning method InfoPrompt, with two novel loss functions. With extensive experiments, without any prior expert knowledge, InfoPrompt can significantly accelerate the convergence of the prompt tuning and achieve more accurate and robust performances than traditional prompt tuning methods.

Existing instruction-tuned LMs (_e.g._, Llama 2 ) are generally not task-specific and future works may further consider to tune such models with task-specific information. To achieve this, combining soft prompt tuning and prompt engineering [104; 76], from an information-theoretical perspective, can be a promising approach. Another future direction could be to further generalize our method to generation tasks (_e.g._, sequence generation). In addition to prompt learning, it is interesting to explore how to extend our approach to other parameter-efficient fine-tuning methods (_e.g._, LoRA  and HyperFormer ).