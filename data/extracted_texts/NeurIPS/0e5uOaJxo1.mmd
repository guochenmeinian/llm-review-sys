# Learning Complete Protein Representation by

Dynamically Coupling of Sequence and Structure

 Bozhen Hu\({}^{1,2*}\), Cheng Tan\({}^{2*}\), Jun Xia\({}^{2}\), Yue Liu\({}^{3}\), Lirong Wu\({}^{2}\),

**Jiangbin Zheng\({}^{2}\)**, Yongjie Xu\({}^{2}\), Yufei Huang\({}^{2}\), Stan Z. Li\({}^{2}\)

\({}^{1}\)Zhejiang University \({}^{2}\)Westlake University \({}^{3}\)National University of Singapore

{hubozhen, tancheng, stan.zq.li}@westlake.edu.cn

*Equal contribution. \(\)Correspondence: {stan.zq.li}@westlake.edu.cn

###### Abstract

Learning effective representations is imperative for comprehending proteins and deciphering their biological functions. Recent strides in language models and graph neural networks have empowered protein models to harness primary or tertiary structure information for representation learning. Nevertheless, the absence of practical methodologies to appropriately model intricate inter-dependencies between protein sequences and structures has resulted in embeddings that exhibit low performance on tasks such as protein function prediction. In this study, we introduce CoupleNet, a novel framework designed to interlink protein sequences and structures to derive informative protein representations. CoupleNet integrates multiple levels and scales of features in proteins, encompassing residue identities and positions for sequences, as well as geometric representations for tertiary structures from both local and global perspectives. A two-type dynamic graph is constructed to capture adjacent and distant sequential features and structural geometries, achieving completeness at the amino acid and backbone levels. Additionally, convolutions are executed on nodes and edges simultaneously to generate comprehensive protein embeddings. Experimental results on benchmark datasets showcase that CoupleNet outperforms state-of-the-art methods, exhibiting particularly superior performance in low-sequence similarities scenarios, adeptly identifying infrequently encountered functions and effectively capturing remote homology relationships in proteins.

## 1 Introduction

Proteins, the fundamental building blocks of life, serve crucial roles across a diverse array of applications, ranging from therapeutics to materials. Comprising 20 distinct amino acids linked by peptide bonds, proteins form intricate sequences that dictate their one-dimensional (1D) structure, ultimately determining their biochemical functions . Due to recent progress in protein sequencing , massive numbers of protein sequences are now available. For example, the UniProt database, housing over 200 million protein sequences, has become a valuable resource for research . Notably, the development of large-scale language models (LMs) in natural language processing has substantially benefited protein research owing to similarities between human languages and protein sequences [4; 5; 6]. For instance, models like ProtTrans  and ESM-series [8; 9] have proven the successful utility of protein LMs to process protein sequences.

Thanks to the recent significant progress made by AlphaFold2  in three-dimensional (3D) structure prediction, a large number of protein structures from their sequence data are now made available. The latest release of AlphaFold protein structure database  provides broad coverage of UniProt . Recently proposed structure-based protein encoders become to utilize geometric features [12; 13; 14],e.g., ProNet  learns representations of proteins with 3D structures at different levels, like the amino acid, backbone or all-atom levels. Concurrently, methods employing graph neural networks (GNNs) and LMs (LSTMs or attention models) [14; 16; 17], such as GearNet , have been developed to process both sequence and structure information.

The 1D sequence and 3D structure of a protein provide different types of information, the discrete sequential orders, residue types, and coordinates, as shown in Figure 5 and Figure 6 in Appendix A. Models can learn coevolutionary and geometric information from sequences and structures, for example, whether residues contact or not. Although a protein's sequence determines its structure, various works have demonstrated the effectiveness of learning from either sequences or structures [9; 12; 18]. However, directly fusing representations from sequence encoder and structure encoder cannot explore their relationships, and current protein GNN methods have drawbacks in integrating such sequential and structural information. In detail, the information propagation is difficult for long-range dependencies in large protein graphs, and messages attenuate over many rounds of passing in GNNs, although there are several works aiming to tackle such a problem [19; 20]. Besides, message passing typically assumes localized neighborhood relationships, but amino acid interactions can be complex and long-range. We need to consider the structural and chemical properties of a residue that are highly dependent on surrounding neighbors, and capturing different conformers requires modeling the entire protein structure holistically, as the conformation of an amino acid is constrained by steric and hydrogen bonding with nearby residues, and the conformers correspond to the same protein, adopting slightly different 3D structures. Therefore, a proper protein sequence-structure modeling method is necessary and important to recognize these challenges and factors to obtain comprehensive and effective representations.

In this work, we model the relative positions of residues in the sequence and the spatial arrangement of atoms in Euclidean space simultaneously. We propose CoupleNet to construct two categories of graphs dynamically to cover the multiple scales of sequential features and structural geometries, which achieve completeness at the residue and backbone levels. Such global completeness is theoretically guaranteed to incorporate 3D information completely without information loss, while the local view would miss the long-range effects of subtle conformational changes happening distantly. For instance, the open and closed conformers of an enzyme may have similar local binding pockets but differ in global clamshell arrangement . In order to better capture the local and global relationships and relieve the problems that exist in deep GNNs, we dynamically build new protein graphs in different conditions based on the depth of the network. For feature fusing, we take advantage of graph convolutions, performing node and edge convolutions simultaneously rather than passing messages separately on nodes and edges. Thus, the contributions of this paper are threefold: (1) A novel two-graph-based approach for modeling sequential and 3D geometric features, ensuring global completeness in protein representation. (2) The proposal of CoupleNet, which performs concurrent convolutions on nodes and edges, effectively integrating protein sequence and structure. The dynamically changed graphs can better model the node-edge relationships and utilize the intrinsic associations between sequences and structures. (3) Empirical validation showcases the superior performance of the proposed model compared to current mainstream protein representation learning methods across diverse tasks, including protein fold classification, enzyme reaction classification, Gene Ontology (GO) term prediction, and Enzyme Commission (EC) number prediction. Our experiments reveal that this method excels in predicting functions that rarely appear, effectively captures protein remote homology relationships.

## 2 Related Work

Protein Representation Learning.Protein representation learning has emerged as a dynamic and promising field within biology, playing a crucial role in diverse downstream applications in protein science. Given the multifaceted nature of protein structures, current methodologies predominantly fall into three categories: protein LMs tailored for sequences, structure models emphasizing geometry, and hybrid approaches seamlessly integrating both aspects. Considering proteins as sequences of amino acids, akin to the structure of human languages, TAPE  establishes a benchmark for a variety of protein models, including 1D CNN, LSTM, and Transformer architectures. Elnaggar et al. have successfully trained six transformer variants, such as ProtBert and ProtT5, on extensive amino acid sequences. Similarly, the ESM-series [8; 9; 23] adopt a transformer architecture and a masked language modeling strategy, achieving robust representations through training on large-scaledatabases. Besides, several methods aim to encode the spatial information of protein structures using techniques such as convolutional neural networks (CNNs) , or GNNs [17; 25; 26]. For instance, SPROF  employs distance maps to predict protein sequence profiles, while IEConv  introduces a convolution operator to capture relevant structural levels. GVP-GNN  designs the geometric vector perceptrons (GVP) to learn both scalar and vector features in an equivariant and invariant manner. ProNet  learns hierarchical protein representations at multiple tertiary structure levels of granularity. Additionally, CDConv  introduces continuous-discrete convolution, utilizing irregular and regular approaches to model both geometry and sequence structures. A protein clustering method  is proposed by applying an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assigned scores to obtain hierarchical protein representations. Moreover, some protein learning methods concurrently model multiple levels of structures [14; 28; 30], and PromtProtein  adopts a prompt-guided multi-task learning strategy for incorporating various protein structures.

Complete Message Passing.While SphereNet  introduces a spherical message passing scheme for precise 3D molecular learning, ensuring completeness within the edge-based 1-hop local neighborhood, this completeness does not extend to the entire 3D graph. Building upon this limitation, ComENet  innovatively incorporates rotation angles and spherical coordinates to achieve global completeness in 3D information on molecular graphs. By integrating these meticulously designed geometric representations into the established message passing scheme , the complete representation for a whole 3D graph is ultimately achieved .

Unlike these methods, we couple sequence and structure via dynamically changed graphs and different geometric representations to attain complete representations throughout the entire protein 3D graph.

## 3 Methodology

### Preliminaries

Notations.A 3D graph is represented as \(G=(,,)\), where \(=\{v_{i}\}_{i=1,,n}\) and \(=\{_{ij}\}_{i,j=1,,n}\) denote the vertex and edge sets with \(n\) nodes, respectively, and \(=\{P_{i}\}_{i=1,,n}\) is the set of position matrices, where \(P_{i}^{k_{i} 3}\) represents the position matrix for node \(v_{i}\). We treat each amino acid as a graph node for a protein, then \(k_{i}\) depends on the number of atoms in the \(i\)-th amino acid. The node feature matrix is \(X=[_{i}]_{i=1,,n}\), where \(_{i}^{d_{v}}\) is the feature vector of node \(v_{i}\). The edge feature matrix is \(E=[_{ij}]_{i,j=1,,n}\), where \(_{ij}^{d_{e}}\) is the feature vector of edge \(_{ij}\). \(d_{v}\) is the dimension of feature vector \(_{i}\), and \(d_{}\) denotes the dimension of \(_{ij}\).

Invariance and Equivariance.We consider affine transformations that preserve the distance between any two points, i.e., the isometric group SE(3) (refer to Appendix B) in the Euclidean space. This is called the symmetry group, and it turns out that SE(3) is the special Euclidean group that includes 3D translations and the 3D rotation group SO(3) [35; 36].

Given the function \(f:^{m}^{m^{}}\), assuming the given symmetry group \(G\) acts on \(^{m}\) and \(^{m^{}}\), then \(f\) is G-equivariant if,

\[f(T_{g})=S_{g}f(),\;^{m},g G\] (1)

where \(T_{g}\) and \(S_{g}\) are the transformations. For the SE(3) group, when \(m^{{}^{}}=1\), the output of \(f\) is a scalar, we have

\[f(T_{g})=f(),\;^{m},g G\] (2)

thus \(f\) is SE(3)-invariant.

Complete Geometric Representations.A geometric transformation \(()\) is complete if for two 3D graphs \(G^{1}=(,,^{1})\) and \(G^{2}=(,,^{2})\), there exists \(T_{g}(3)\) such that the representations

\[(G^{1})=(G^{2}) P_{i}^{1}=T_{g}(P_{ i}^{2}),\;\;i=1, n\] (3)

The operation \(T_{g}\) would not change the 3D conformation of a 3D graph [15; 32; 33]. And \((G)\), positions can generate geometric representations, which can also be recovered from them.

Message Passing Paradigm.Message passing mechanism is mainly applied in graph convolutional networks (GCNs) , which follows an iterative scheme of updating node representations based on the feature aggregation from nearby nodes.

\[_{i}^{(0)} =((_{i})),\] (4) \[_{i}^{(l)} =f_{}^{(l)}(_{j}^{(l-1)}|v_{j}(v_{ i})),\] \[_{i}^{(l)} =f_{}^{(l)}(_{j}^{(l-1)},_{i}^{(l)})\]

where \(()\) and \(()\) mean the linear transformation and batch normalization respectively. \((v_{i})\) denotes the neighbours of node \(v_{i}\). \(f_{}^{(l)}\) and \(f_{}^{(l)}\) are aggregation and transformation functions at the \(l\)-th layer, which are permutation invariant and equivariant of node representations.

### Sequence-Structure Graph Construction

Specifically, we represent each amino acid as a node, considering the residue types and their positions \(i=1,2,,n\) in the sequence, we define the sequential graph primarily on the sequence, if \(\|i-j\|<l\), the edge \(_{ij}\) exists, where \(l\) is a hyper-parameter, and \(\|\|\) denotes the \(l^{2}\)-norm. Besides, we predefine a radius \(r\), and build the radius graph. There exists a radius edge between node \(v_{i}\) and \(v_{j}\) if \(\|d_{ij,C}\|<r\), where \(d_{ij,C}=P_{i,C}-P_{j,C}\), and \(P_{i,C}\) denotes the 3D position of \(_{}\) in the \(i\)-th residue. We dynamically change the predefined thresholds with the depth of the network to cover nodes from the local to the global.

Firstly, we design a base approach at the amino acid level (aa) called \(_{}\) that only uses the \(_{}\) positions of the structures. Inspired by Ingraham et al., we construct a local coordinate system (LCS) for each residue (Figure 7(a) in the appendix).

\[_{i}=[_{i}_{i}_{i}_{i}]\] (5)

where \(_{i}=-P_{i-1,C}}{\|P_{i,C}-P_{i-1,C }\|},_{i}=_{i}-_{i+1}}{\|_{i}-_{i+1}\|}, _{i}=_{i}_{i+1}}{\|_{i}_{i+1} \|},\) denotes the vector outer product. Then, we can get the geometric representations at the amino acid level of a protein 3D graph,

\[(G)_{ij,aa}=(\|d_{ij,C}\|\,,_{i}^{T}}{\|d_{ij,C}\|},_{i}^{T}_{j})\] (6)

where \(\) is the matrix multiplication. This implementation is SE(3)-equivariant and obtains complete representations at this level; as if we have \(_{i}\), the LCS \(_{j}\) can be easily obtained from \((G)_{ij,aa}\).

For a node \(v_{i}\), the node features \(_{i,aa}\) at the amino acid level is the concatenation of the one-hot embeddings of amino acid types and the physicochemical properties of each residue, namely, a steric parameter, hydrophobicity, volume, polarizability, isoelectric point, helix probability and sheet probability [39; 40], which provide quantitative insights into the biochemical nature of residues.

Secondly, as illustrated in Figure 1, CoupleNet considers all backbone atoms \(,,,\) (as depicted in Figure 2). In detail, the peptide bond displays partial double-bond character due to resonance ,

Figure 1: The polypeptide chain depicting the characteristic backbone bond lengths, angles, and torsion angles (\(_{i},_{i},_{i}\)). The planar peptide groups are denoted as shaded gray regions, indicating that the peptide plane differs from the geometric plane calculated from 3D positions.

indicating that the three non-hydrogen atoms comprising the bond are coplanar, with limited free rotation about the bond due to this coplanar property. The \(_{i}-_{ i}\) and \(_{ i}-_{i}\) bonds constitute the two bonds in the basic repeating unit of the polypeptide backbone. These single bonds allow unrestricted rotation until sterically restricted by side chains [42; 43]. Since the coordinates of \(_{}\) can be obtained as we have the complete representations at the amino acid level, the coordinates of other backbone atoms based on these rigid bond lengths and angles are able to be determined with the remaining degree of the backbone torsion angles \(_{i},_{i},_{i}\). The omega torsion angle around the \(-\) peptide bond is typically restricted to nearly \(180^{}\) (trans) but can approach \(0^{}\) (cis) in rare instances. Other than the bond lengths and angles presented in Figure 1, all the H bond lengths measure approximately 1 A.

For the sequential graph, we compute the sine and cosine values of \(_{i},_{i},_{i}\) for each amino acid \(i\), and also use them as node features for node \(v_{i}\).

\[_{i}=_{i,aa}\|(()(_{i},_{i},_{i}))\] (7)

where \(\|\) denotes concatenation. There is no isolated node for the designed graph, which means the backbone atoms can be determined one by one along the polypeptide chain based on the positions of \(_{}\) and these three backbone dihedral angles. Therefore, the existing presentations \([(G)_{ij,aa}]_{i,j=1,...,n}\) and \([_{i}]_{i=1,...,n}\) are complete at the backbone level for the sequential graph.

For the radius graph, we want to get the positions of backbone atoms in any two amino acids \(i\) and \(j\). Inspired by trRosetta , the relative rotations and distances are computed, including the distance (\(d_{ij,_{}}\)), three dihedral angles (\(_{ij},_{ij},_{ji}\)) and two planar angles (\(_{ij},_{ji}\)), as shown in Figure 7(b) in the appendix. These intersiteable geometries define the relative locations of the backbone atoms of two residues in their details . Therefore, these six geometries are complete for amino acids at the backbone (bb) level for the radius graph. The graph edges contain the relative spatial information between any two neighboring amino acids \(_{ij}=(G)_{ij,aa}\|(G)_{ij,bb}\),

\[(G)_{ij,bb}=(d_{ij,_{}},()(_{ij },_{ij},_{ij}))\] (8)

Protein structures that are SE(3) equivalents have the same 3D conformation, differing in orientation/positioning. Graph representations must encode these structures equivalently since the underlying molecular properties are identical. Constructing the relationships between sequence and structure can help the model learn more comprehensive protein representations, which ensures the model focuses on meaningful aspects of protein structures.

### Sequence-Structure Graph Convolution

We employ graph convolution to embed sequences and structures simultaneously, exploring their relationships to generate effective embeddings. Different from previous works [14; 28], we innovatively construct two categories of graphs for sequence and structure and design comprehensive sequential and structural representations to achieve completeness at the amino acid and backbone levels. We then convolve node and edge features aided by the message passing mechanism.

Figure 2: An illustration of CoupleNet. This framework processes protein sequences and structures to get complete geometries and properties used as graph node and edge features, where the sequential and structural graph is dynamically changed depending on their distance relationships and the network depth. Convolutions happen on the nodes and edges simultaneously to capture the relationships from the local to the global.

In order to implement convolution on nodes and edges simultaneously between sequence and structure, we set \(_{ij}\) to exist if the following conditions are satisfied:

\[\|i-j\|<l\|d_{ij,C_{}}\|<r\] (9)

The existing node and edge feature matrices (\(X,E\)) are complete representations of a protein 3D graph to reconstruct its backbone atom positions. When the thresholds \(r,l\) are small, Eq. 9 defines the local environment  of an amino acid, and the structural and chemical properties of a residue are highly dependent on surrounding residues.

Compared with the equation Eq. 4, the proposed CoupleNet first applies a \(()\) layer and a \(()\) layer to the node features to obtain the initial encoded representation. Then the aggregation function \(f^{(l)}_{}\) is applied to gather neighboring features of nodes and edges by convolution, where \(()\) is the activation function, LeakyReLU. \(W\) is the learnable convolutional kernel matrix whose learnable parameters have no concern with the number of nodes or edges. We use the dropout and \(()\) layer and add a residual connection from the previous layer for update function \(f^{(l)}_{}\):

\[^{(0)}_{i} =((_{i})),\] (10) \[^{(l)}_{i} =((_{v_{j}(v_{i})}W_{ij }^{(l-1)}_{j}),\] \[^{(l)}_{i} =^{(l)}_{i}+((^{(l)}_{i}))\]

By incorporating complete geometric representations to the commonly used message passing framework (Eq. 10), a complete message passing scheme can be achieved [15; 32; 33], which can capture small changes due to such rigid transformations in coordinate positions. Complete representations allow for powerful equivariance and invariance properties to be encoded, which makes the learned models robust. By incorporating complete geometries, the convolution and pooling operations on irregular and non-Euclidean data like graphs are defined and conducted, enabling more expressive modeling for protein data.

### Model Architecture

The overall framework of CoupleNet is shown in Figure 2. The inputs to the graph are the calculated sequential and structural representations (\(X,E\)). We employ complete message passing and sequence pooling layers to obtain the deeply encoded graph-level representations. After one average pooling layer, the number of residues reduces by half. Thus, we expand the radius \(r\) to \(2r\) after once pooling, which makes neighbors of center nodes gradually cover more distant and rare nodes, also reducing the computational complexity.

Differences with Existing Protein Modeling Methods.The proposed approach representing the sequence and the 3D geometric structure of a protein differs from several existing protein models [12; 14]. Specifically, GearNet  has \(2l+1\) types of edges; there are only two different types of graphs in the proposed CoupleNet: radius graph and sequential graph. Importantly, the threshold in the radius graph in GearNet is set to be constant, but we change the threshold of radius dynamically to learn different distance relationships. The message passing mechanism only executes on nodes in CoupleNet instead of on nodes and edges alternately used in GearNet. Moreover, CoupleNet performs convolutions on nodes and edges simultaneously with several pooling layers to reduce the sequence length, which is also largely different from ComENet  and GearNet.

Complexity Analysis.Considering the computational complexity of one message passing layer in this framework, it is \((nd_{n})\), where \(d_{n}\) is the average node degree, and \(d_{n} n\). The time complexity is related to the computational complexity of the message passing layer; as we conduct the graph convolution on nodes and edges simultaneously, the time complexity is also \((nd_{n})\). Assuming there are \(m_{}\) edges in the graph, \(d_{1}\) and \(d_{2}\) mean the feature dimensions of nodes and edges, the space complexity is \((nd_{1}+m_{}d_{2})\) for every message passing layer. Using \(B_{s}\) to denote the size of the batch, the final computational complexity is only \((B_{s}nd_{n})\).

## 4 Experiments

### Datasets, Settings and Baselines

Following the tasks in IEconv  and GearNet , we evaluate CoupleNet on four protein tasks: protein fold classification, enzyme reaction classification, GO term prediction, and EC number prediction. For the task of fold and reaction classification, the performance is measured by mean accuracy. For GO Term and EC Prediction, \(_{}\) is used as the evaluation metric. The performance is measured with mean values of five different initializations. As stated before, we increase the predefined radius \(r\) to \(2r\) after one pooling layer, from 4 to 16, and \(l\) is set to be a constant number 11, and the number of feature channels is also doubled. In this condition, when the number of nodes decreases, \(l\) is constant, \(r\) increases and neighbors of center nodes gradually cover distant nodes. We design the sequential and radius graph instead of the \(k\)-nearest neighbor graph because a constant \(k\) makes some neighboring nodes far away from the center node. Distances of a group of neighbor nodes are larger than 20 A, which cannot be seen as contacts .

We present the dataset statistics Table 4 and conduct experiments to analyze these datasets. Figure 3 shows the distance relationships between sequence and structure on the GO term training dataset with 29898 proteins. We can see that when the sequential distance is large, there still exist nodes spatially adjacent. According to the trend of the medians, when the sequence is long, atoms may need more space to arrange in the 3D space. The violin plot distance relationships on the other three datasets are presented in Figure 8.

We compare our proposed method with existing protein representation learning methods, which are classified into three categories based on their inputs: a sequence (amino acid sequence), 3D structure, or both sequence and structure. 1) Sequence-based encoders, including CNN ,

    &  &  & Enzyme \\   & & Fold & SuperFamily & Family & Reaction \\   & ResNet \({}^{*}\) & 10.1 & 7.21 & 23.5 & 24.1 \\  & Transformer \({}^{*}\) & 9.22 & 8.81 & 40.4 & 26.6 \\   & 3DCNN\_MQA \({}^{*}\) & 31.6 & 45.4 & 92.5 & 72.2 \\  & IEConv (atom level) \({}^{*}\) & 45.0 & 69.7 & 98.9 & 87.2 \\   & GraphQA \({}^{*}\) & 23.7 & 32.5 & 84.4 & 60.8 \\  & GVP \({}^{*}\) & 16.0 & 22.5 & 83.8 & 65.5 \\  & ProNet-Amino Acid  & 51.5 & 69.9 & 99.0 & 86.0 \\  & ProNet-Backbone  & 52.7 & 70.3 & 99.3 & 86.4 \\  & IEConv (residue level) \({}^{*}\) & 47.6 & 70.2 & 99.2 & 87.2 \\  & GearNet  & 28.4 & 42.6 & 95.3 & 79.4 \\  & GearNet-IEConv  & 42.3 & 64.1 & 99.1 & 83.7 \\  & GearNet-Edge  & 44.0 & 66.7 & 99.1 & 86.6 \\  & GearNet-Edge-IEConv  & 48.3 & 70.3 & 99.5 & 85.3 \\  & CDConv  & 56.7 & 77.7 & 99.6 & 88.5 \\   & CoupleNet (Proposed) & **60.6** & **82.1** & **99.7** & **89.0** \\   

Table 1: Accuracy (\(\%\)) on fold classification and enzyme reaction classification. \([^{*}]\) denotes the results are taken from . The best and suboptimal results are shown in bold and underline.

Figure 3: The violin plot of the relationships of distances between sequence and structure on the GO term prediction dataset, the sequential distance \(\|i-j\|\) is from 1 to \(n\)-1, and the x-axis means sequential distance subtract one, the y-axis means \(d_{ij,}\). The dashed red line connects the median values.

ResNet , LSTM  and Transformer . 2) Structure-based methods (GCN , GAT , 3DCNN_MQA , IEConv (atom level) ). 3) Sequence-structure based models, e.g., GVP , ProNet , GearNet , CDConv , etc.

### Results of Fold and Reaction Classification, EC and GO term Prediction.

Table 1 provides the comparisons on the fold and enzyme reaction classification. From this table, we can see that the proposed model CoupleNet achieves the best performance across all four test sets on the fold and enzyme reaction classification compared with recent state-of-the-art methods. Especially on the Fold and SuperFamily test sets, CoupleNet improves the results by about \(4\%\), showing that CoupleNet is proficient at learning the mappings between protein sequences, structures, and functions. Moreover, CDConv  ranks second among these methods. Both CDConv and our method are implemented by sequence-structure convolution. This phenomenon illustrates that coupling sequences and structures of proteins are conducive to learning better protein embeddings. Our proposed model utilizes complete geometric representations and specially designed dynamically changed graphs, achieving state-of-the-art results.

We follow the split method in [14; 17] to guarantee that the test set only comprises PDB chains with sequence identity no higher than \(95\%\) to the training set for GO term and EC number prediction. Proteins are organized into three ontologies: molecular function (MF), biological process (BP), and cellular component (CC) for the task of GO term prediction. Table 2 compares different protein modeling methods on GO term prediction and EC number prediction. The proposed model, CoupleNet yields the highest \(_{}\) across these four test sets of two tasks, outperforming other prevalent models. This indicates that CoupleNet can effectively predict the functions, locations, and enzymatic activities of proteins.

We learn protein representations in terms of protein sequences and structures, which is essential as building hierarchical dependencies gets universal representations when there is a low similarity between the training and test sets. We compare the protein graph methods, GearNet, and the proposed CoupleNet by different cutoff splits. Proteins in the test set are categorized into four groups based on their similarity to the training set (30\(\%\), 40\(\%\), 50\(\%\), 70\(\%\)), not by the default split rate (95\(\%\)). The results are shown in Figure 9 in the appendix, which indicate that even when there is a low similarity between the training and test sets, our model also has the highest scores, which demonstrates the superiority and robustness of the proposed model.

### Analysis of Experiments on Protein Length

   Category & Method & GO-BP & GO-MF & GO-CC & EC \\   & ResNet \({}^{*}\) & 0.280 & 0.405 & 0.304 & 0.605 \\  & Transformer \({}^{*}\) & 0.264 & 0.211 & 0.405 & 0.238 \\   & GCN \({}^{*}\) & 0.252 & 0.195 & 0.329 & 0.320 \\  & GAT \({}^{*}\) & 0.284 & 0.317 & 0.385 & 0.368 \\  & 3DCNN\_MQA \({}^{*}\) & 0.240 & 0.147 & 0.305 & 0.077 \\   & GraphQA \({}^{*}\) & 0.308 & 0.329 & 0.413 & 0.509 \\  & GVP \({}^{*}\) & 0.326 & 0.426 & 0.420 & 0.489 \\  & IEConv (residue level) \({}^{*}\) & 0.421 & 0.624 & 0.431 & - \\   & GearNet  & 0.356 & 0.503 & 0.414 & 0.730 \\   & GearNet-IEConv  & 0.381 & 0.563 & 0.422 & 0.800 \\   & GearNet-Edge  & 0.403 & 0.580 & 0.450 & 0.810 \\   & GearNet-Edge-IEConv  & 0.400 & 0.581 & 0.430 & 0.810 \\   & CDConv  & 0.453 & 0.654 & 0.479 & 0.820 \\    & CoupleNet (Proposed) & **0.467** & **0.669** & **0.494** & **0.866** \\   

Table 2: \(_{}\) on GO term and EC number prediction. \([^{*}]\) means the results are taken from . The best and suboptimal results are shown in bold and underline.

To assess the model's proficiency in handling proteins of varying lengths, we conducted a categorization based on the sequence length of proteins using the mean length of the unseen test set as a threshold. This allowed us to distinguish between small and large proteins. For instance, proteins were grouped into two categories based on the mean length: 149.4 for Fold, 186.7 for SuperFamily, and 162.4 for Family, 299.8 for the GO dataset.

The protein Fold classification task involves identifying remote homology relationships (i.e., determining if proteins belong to the same family or superfamily), CoupleNet demonstrates its capability to capture such relationships. Notably, higher accuracies are observed for relatively large proteins with sequence lengths surpassing the mean length, as indicated in Figure 4. On the other hand, the GO term prediction task exhibits less correlation with protein sequence length, which relies more on the local environment of residues .

### Ablation Study

We examined the impact of removing the sequence information, which means removing the encoding of amino acid types for each node; removing the structure information, which means removing features related to protein geometry (\((G)_{aa},,,,d_{C_{S}},,,\), subscripts are omitted for brevity); removing the backbone torsion angles (w/o \(,,\)) and removing the interresidue geometric structure representations (w/o \(d_{C_{S}},,,\)). As shown in Table 3, removing either sequence or structure causes a performance drop on all tasks, demonstrating that both types of information are critical for the proposed method. When removing the structure, the performance decreases more significantly, suggesting that structural information provides more important and comprehensive clues compared with sequence information alone. From these tables, we can also find that these complete geometries (\(,,\) and \(d_{C_{S}},,,\)) provide similar information, with one of their removals leading to minor performance drops for the reason that they both provide complete geometries, but from different perspectives. Compared with \(_{}\), CoupleNet achieves significant improvements on the four tasks, demonstrating the importance of complete representations at the backbone level in learning protein embeddings.

## 5 Conclusion and Limitation

In this work, we propose CoupleNet, a novel protein representation learning method that dynamically fuses protein sequences and multi-level structures by conducting convolution on graph nodes and edges. We design the sequential and radius graphs, achieving completeness on them at different protein structure levels. Our approach achieves state-of-the-art results on the protein tasks, demonstrating the superiority of our proposed method. A limitation is that this framework needs protein sequences and structures to be available, which may not be suitable for the sequence-only data as the input. A future direction is to develop a large-scale model based on the multi-modal protein data types to enhance performance.

    &  & Enzyme &  &  \\    & Fold & & & & & & & \\  CoupleNet & 60.6 & 82.1 & 99.7 & 89.0 & 0.467 & 0.669 & 0.494 & 0.866 \\  \(_{}\) & 57.8 & 78.7 & 99.6 & 88.6 & 0.458 & 0.660 & 0.484 & 0.851 \\ w/o sequence & 60.0 & 81.6 & 99.6 & 88.4 & 0.441 & 0.650 & 0.456 & 0.700 \\ w/o structure & 26.1 & 36.4 & 92.9 & 81.3 & 0.406 & 0.586 & 0.427 & 0.625 \\ w/o \(,,\) & 60.3 & 81.3 & 99.6 & 88.7 & 0.463 & 0.666 & 0.490 & 0.862 \\ w/o \(d,,,\) & 60.4 & 81.5 & 99.7 & 88.9 & 0.461 & 0.666 & 0.488 & 0.864 \\   

Table 3: Ablation of CoupleNet, we compare it with the base model, \(_{}\), and the models removing the sequence (w/o sequence), structure, or related geometries.

Figure 4: Percentage accumulation chart of results on large and small proteins. The vertical axis shows the percentage, where the red line indicates 50%.