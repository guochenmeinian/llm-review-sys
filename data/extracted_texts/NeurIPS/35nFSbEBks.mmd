# Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics

Liming Wu\({}^{1,2}\), Zhichao Hou\({}^{3}\), Jirui Yuan\({}^{4}\), Yu Rong\({}^{5}\), Wenbing Huang\({}^{1,2}\)

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\)Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{3}\)Department of Computer Science, North Carolina State University

\({}^{4}\)Institute for AI Industry Research (AIR), Tsinghua University

\({}^{5}\)Tencent AI Lab

{maniiowu,hwenbing}@ruc.edu.cn zhou4@ncsu.edu

yu.rong@hotmail.com yuanjirui@air.tsinghua.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Learning to represent and simulate the dynamics of physical systems is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, _e.g._, translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to fulfill our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with the forward attention and equivariant pooling mechanisms to aggregate temporal message. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.

## 1 Introduction

It has been a goal to represent and simulate the dynamics of physical systems by making use of machine learning techniques [37; 8; 12]. The related studies, once pushed forward, have great potential to facilitate a variety of downstream scientific tasks including Molecular Dynamic (MD) simulation , protein structure prediction , virtual screening of drugs and materials , model-based robot planning/control , and many others.

Plenty of solutions have been proposed, amongst which the usage of Graph Neural Networks (GNNs)  becomes one of the most desirable directions. GNNs naturally model particles or unit elements as nodes, physical relations as edges, and the latent interactions as the message passing thereon. More recently, a line of researches [38; 11; 21; 9; 32; 20] have been concerned with generalizing GNNs to fit the symmetry of our physical world. These works, also known as equivariant GNNs , ensure that translating/rotating/reflecting the geometric input of GNNs results in the output transformed in the same way. By handcrafting such Euclidean equivariance, themodels are well predictable to scenarios under arbitrary coordinate systems, giving rise to enhanced generalization ability.

In spite of the fruitful progress, existing methods overlook a vital point: _the observed physical dynamics are almost non-Markovian_. In previous methods, they usually take as input the conformation of a system at a single temporal frame and predict as output the future conformation after a fixed time interval, forming a frame-to-frame forecasting problem. Under the Markovian assumption, this setting is pardonable as future frames are independent of all other past frames given the input one. Nevertheless, the Markovian assumption is rather unrealistic when there are other unobserved objects interacting with the system we are simulating . For instance, when we consider simulating the dynamics of a protein that is interacting with an unobserved solvent (such as water), the Markovian property no longer holds; in other words, even conditional on the current frame, the future dynamics of the protein depends on the current state of the solvent which, however, is influenced by the past states of the protein itself, owing to the interaction between the protein and the solvent. The improper Markovian assumption makes current works immature in dynamics modeling.

To relieve from the Markovian assumption, this paper proposes to employ the states in the past period to reflect the latent and unobserved dynamics. In principle, we can recover the non-Markovian behavior (_e.g._ interacting with a solvent) if the past period is sufficiently long. We collect a period of past system states as spatio-temporal graphs, and utilize them as the input to formulate a spatio-temporal prediction task, other than the frame-to-frame problem as usual (see Figure 1). This motivates us to leverage existing Spatio-Temporal GNNs (STGNNs)  to fulfill our purpose, which, unfortunately, are unable to conform to the aforementioned Euclidean symmetry and the underlying physical laws. Hence, the equivariant version of STGNNs is no doubt in demand. Another point is that periodic motions are frequently observed in typical physical systems . For example, the Aspirin molecular exhibits clear periodic thermal vibration when binding to a target protein. Under the spatio-temporal setting, we are able to model periodicity of dynamics, which, however, is less investigated before.

Our contributions are summarized as follows:

* We reveal the non-Markov behavior in physical dynamics simulation by developing equivariant spatio-temporal graph models. The proposed model dubbed Equivariant Spatio-Temporal Attentive Graph network (ESTAG) conforms to Euclidean symmetry and alleviates the limitation of the Markovian assumption.
* We design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic features from the dynamics, and then construct an Equivariant Spatial Module (ESM), and an Equivariant Temporal Module (ETM) with forward attention and equivariant pooling, to process spatial and temporal message passing, respectively.
* The effectiveness of ESTAG is verified on three real datasets corresponding to the molecular-, protein- and macro-level. We prove that, involving both temporal memory and equivariance are advantageous compared to typical STGNNs and equivariant GNNs with adding trivial spatio-temporal aggregation.

## 2 Related Work

**GNNs for Physical Dynamics Modeling** Graph Neural Networks (GNNs) have shown great potential in physical dynamics modeling. IN , NRI , and HRN  are a series of works to learn physical

Figure 1: Comparison of the problem setting between previous methods and our paper. Here, we choose the dynamics of the Aspirin molecular with time lag as 1 for illustration.

system interaction and evolution. Considering the energy conservation and incorporating physical prior knowledge into GNNs, HNN , HOGN  leverage ODE and Hamiltonian mechanics to capture the interactions in the systems. However, all of the above-mentioned models don't take into account the underlying symmetries of a system. In order to introduce Euclidean equivariance, Tensor-Field networks (TFN)  and SE(3)-Transformer  equip filters with rotation equivariance by irreducible representation of the SO(3) group. LieTransformer and LieConv  leverage the Lie group to enforce equivariance. Besides, EGNN  utilized a simpler E(n)-equivariant framework which can achieve competitive results without computationally expensive information. Based on EGNN, GMN  further proposes an equivariant and constraint-aware architecture by making use of forward kinematics information in physical system. Nevertheless, all of these methods ignore the natural spatiotemporal patterns of physical dynamics and model it as a frame-to-frame forecasting problem.

**Spatio-temporal graph neural networks** STGNNs  aim to capture the spatial and temporal dependency simultaneously and are widely investigated in various applications like traffic forecasting and human recognition. DCRNN  and GaAN  are RNN-based methods which filter inputs and hidden states passed to recurrent unit using graph convolutions. However, RNN-based approaches are time-consuming and may suffer from gradient explosion/vanishing problem. CNN-based approaches, such as STGCN  and ST-GCN , interleave 1D-CNN layers with graph convolutional layers to tackle spatial temporal graphs in a non-recursive manner. Besides, attention mechanism, an important technique STGCNs, is employed by GaAN , AGL-STAN  and ASTGCN  to learn dynamic dependencies in both space and time domain. The aforementioned approaches are targeted to the applications in 2D graph scenarios such as traffic networks and human skeleton graphs, and may not be well applicable to 3D physical systems in which geometric equivariance is a really important property.

**Equivariant spatio-temporal graph neural networks** There are few previous works designing spatio-temporal GNNs while maintaining equivariance. Particularly, by using GRU  to record the memory of past frames, LoCS  additionally incorporates rotation-invariance to improve the model's generalization ability. Different from the recurrent update mechanism used in LoCs, EqMotion  distills the history trajectories of each node into a multi-dimension vector, by which the spatio-temporal graph is compressed as a spatial graph, then it designs an equivariant module and an interaction reasoning module to predict future frames. However, both of LoCS and EqMotion are still defective in exploring the interactions among history trajectories, while in this paper we propose a transformer-alike architecture to fully leverage the spatio-temporal interactions based on equivariant attentions.

## 3 Notations and Task Definition

The dynamics of physical objects (such as molecules) can be formulated with the notion of spatiotemporal graphs, as shown in Figure 1 (left). In particular, a spatiotemporal graph of node number \(N\) and

Figure 2: Schematic overview of ESTAG. After inputting historical graph trajectories \(_{0},...,_{T-1}\), Equivariant Discrete Fourier Transform (EDFT) extracts equivariant frequency features \(}\) from the trajectory. We process them into the invariant node-wise feature \(\) and adjacency matrix \(\) to be adopted for the next stage. Then we stack Equivariant Spatial Module (ESM) and Equivariant Temporal Module (ETM) alternatively for \(L\) times to explore spatial and temporal dependencies. After the equivariant temporal pooling layer, we obtain the estimated position \(}^{*}(T)\).

temporal length \(T\) is denoted as \(\{_{t}=(_{t},)\}_{t=0}^{T-1}\). Here, the nodes \(_{t}\) are shared across time, but each node \(i\) is assigned with different scalar feature \(_{i}(t)^{c}\), position vector \(}_{i}(t)^{3}\) at different temporal frame \(t\); the edges \(\) are associated with an identical adjacency matrix \(^{N N m}\) whose element \(_{ij}^{m}\) defines the edge feature between node \(i\) and \(j\). We henceforth denote by the matrices \((t)^{c N}\) and \(}(t)^{3 N}\) the collection of all nodes in \(_{t}\). Here for simplicity, we specify the time lag as \( t=1\) which could be valued more than 1 in practice. In general, the \(t\)-th frame corresponds to time \(T-t t\).

**Task Definition** This paper is interested in predicting the physical state, particularly the position of each node at frame \(T\) given the historical graph series \(\{_{t}\}_{t=0}^{T-1}\). In form, we learn the function \(\):

\[\{((t),}(t),)\}_{t=0}^{T-1}}(T).\] (1)

Although previous approaches such as EGNN  and GMN  also claim to tackle physical dynamics modeling, they neglect the application of spatiotemporal patterns and only accomplish frame-to-frame prediction, where the input of \(\) is reduced to a single frame, _e.g._, \(_{T-1}\) in Eq. 1.

**Equivariance** A crucial constraint on physical dynamics is that the function \(\) should meet the symmetry of our 3D world. In other words, for any translation/rotation/reflection \(g\) in the group E(3), \(\) satisfies:

\[(\{((t),g}(t),)\}_{t=0}^{T-1})=g }(T),\] (2)

where the group action \(\) is instantiated as \(g}(t):=}(t)+\) for translation \(^{3}\) and \(g}(t):=}(t)\) for rotation/reflection \(^{3 3}\).

## 4 Our approach: ESTAG

Discovering informative spatiotemporal patterns from the input graph series is vital to physical dynamics modeling. In this section, we introduce ESTAG that pursues this goal in a considerate way: We first extract the frequency of each node's trajectory by EDFT (SS 4.1), which captures the node-wise temporal dynamics in a global sense and returns important features for the next stage: We then separately characterize the spatial dependency among the nodes of each input graph \(_{t}\) via ESM (SS 4.2); We finally unveil the temporal dynamics of each node through the attention-based mechanism ETM and output the estimated position of each node in \(_{T}\) after an equivariant temporal pooling layer (SS 4.3). The overall architecture is shown in Figure 2.

### Equivariant Discrete Fourier Transform (EDFT)

The Fourier Transform (FT) gives us insight into the wave frequencies contained in the input signal that is usually periodic. With the extracted frequencies, we are able to view the global behavior of each node \(i\) in different frequency domains. Conventional multidimensional FT employs distinct Fourier bases for different input dimensions of the original signals. Here, to ensure equivariance, we first translate the signals by the mean position and then adopt the same basis over the spatial dimension. To be specific, we compute equivariant DFT as follows:

\[}_{i}(k)=_{t=0}^{T-1}e^{-i^{} kt}\ (}_{i}(t)-}(t)}),\] (3)

where, \(i^{}\) is the imaginary unit, \(k=0,1,,T-1\) is the frequency index, \(}(t)}\) is the average position of all nodes in the \(t\)-th frame \(_{t}\), and the output \(}_{i}(k)^{3}\) is complex. The frequencies calculated by Eq. 3 are then utilized to formulate two crucial quantities: the frequency cross-correlation \(_{ij}^{T}\) between node \(i\) and \(j\), and the frequency amplitude \(_{i}^{T}\) of node \(i\).

In signal processing, cross-correlation measures the similarity of two functions \(f_{1}\) and \(f_{2}\). It satisfies \(\{f_{1} f_{2}\}=\{f_{1}\}} \{f_{2}\}\), where \(\) and \(\) denote the FT and the cross-correlation operator, respectively, and \(}\) indicates the complex conjugate of \(\). Borrowing this idea, we compute the cross-correlation in the frequency domain by Eq. 3 as:

\[_{ij}(k)=w_{k}(_{i})w_{k}(_{j})|(}_{i}(k),}_{j}(k))|,\] (4)where \(,\) defines the complex inner product. Notably, we have added two learnable parameters \(w_{k}(_{i})\) and \(w_{k}(_{j})\) dependent on node features, which act like spectral filters of the \(k\)-th frequency and enable us to select related frequency for the prediction. In the next subsection, we will apply \(_{ij}\) as the edge feature to capture the relationship between different nodes. We use Aspirin as an example and visualize the \(\) in Figure 3.

We further compute for node \(i\) the amplitude of the frequency \(}_{i}(k)\) along with the parameter \(w_{k}(_{i})\):

\[_{i}(k)=w_{k}(_{i})\|}_{i}(k)\|^{2}.\] (5)

This term will be used in the update of the hidden features in the next subsection.

A promising property of Eq. 3 is that it is translation invariant and rotation/reflection equivariant. Therefore, both \(_{ij}\) and \(_{i}\) are E(3)-invariant, which will facilitate the design of following modules.

### Equivariant Spatial Module (ESM)

For each graph \(_{t}\), our ESM is proposed to encode its spatial geometry through equivariant message passing. ESM is built upon EGNN  which is a prevailing kind of equivariant GNNs, but it has subtly involved the FT features from the last subsection for enhanced performance beyond EGNN.

The \(l\)-th layer message passing in ESM is as below:

\[_{ij} =_{m}(_{i}^{(l)}(t),_{j}^{(l)}(t),\|}_{ij}^{(l)}(t)\|^{2},_{ij}),\] (6) \[_{i}^{(l+1)}(t) =_{i}^{(l)}(t)+_{h}(_{i}^{(l)}(t),_{i },_{j i}_{ij}),\] (7) \[}_{i}(t) =(i)|}_{j(i)}}_{ij}^{(l)}(t)_{x}(_{ij}),\] (8) \[}_{i}^{(l+1)}(t) =}_{i}^{(l)}(t)+}_{i}(t),\] (9)

where, \(_{m}\) computes the message \(_{ij}\) from node \(j\) to \(i\), \(_{h}\) updates the hidden representation \(_{i}\), \(_{x}\) returns a one-dimensional scalar for the update of \(}_{i}(t)\), and all the above functions are Multi-Layer Perceptrons (MLPs); \(}_{ij}(t)=}_{i}(t)-}_{j}(t)\) is the relative position and \((i)\) denotes the neighborhoods of node \(i\).

Notably, we leverage the cross-correlation \(_{ij}\) as the edge feature in Eq. 6 to evaluate the connection between node \(i\) and \(j\) over the global temporal window, since it is computed from the entire trajectory. We also make use of \(_{i}\) as the input of the update in Eq. 7. The benefit of considering these two terms will be ablated in our experiments.

Figure 3: Visualization of cross-correlation \(\) on Aspirin. EDFT can not only identify strongly-connected nodes (e.g. Node 8 and Node 11), but also discover latent relationship between two nodes which are disconnected yet may have similar structures or functions (e.g. Node 8 and Node 10).

### Equivariant Temporal Module (ETM)

**Forward Temporal Attention** Inspired by the great success of Transformer  in sequence modeling, we develop ETM that describes the self-correspondence of each node's trajectory based on the forward attention mechanism, and more importantly, in an E(3)-equivariant way.

In detail, each layer of ETM conducts the following process:

\[_{i}^{(l)}(ts) =_{i}^{(l)}(t)^{}_{i}^{(l)}(s))}{_ {s=0}^{t}(_{i}^{(l)}(t)^{}_{i}^{(l)}(s))},\] (10) \[_{i}^{(l+1)}(t) =_{i}^{(l)}(t)+_{s=0}^{t}_{i}^{(l)}(ts)_{i}^ {(l)}(s),\] (11) \[}_{i}^{(l+1)}(t) =}_{i}^{(l)}(t)+_{s=0}^{t}_{i}^{(l)}(ts )}_{i}^{(l)}(ts)_{x}(_{i}^{(l)}(s)),\] (12)

where, \(_{ts}\) is the attention weight between time \(t\) and \(s\), computed by the query \(_{t}\) and key \(_{s}\); the hidden feature \(_{i}(t)\) is updated as a weighted combination of the value \(_{s}\); the position vector \(}_{i}(t)\) is derived from a weighted combination of a one-dimensional scalar \(_{x}(_{s})\) multiplied with the temporal displace vector \(}_{i}^{(l)}(ts)=}_{i}^{(l)}(t)-}_{i}^{(l)}(s)\). Specifically, \(_{i}^{(l)}(t)=_{q}(_{i}^{(l)}(t))\), \(_{i}^{(l)}(t)=_{k}(_{i}^{(l)}(t))\) and \(_{i}^{(l)}(t)=_{v}(_{i}^{(l)}(t))\) are all E(3)-invariant functions. Notably, we derive a particle's next position in a forward-looking way, to keep physical rationality as the derivation of current state should not be dependent on future positions.

**Equivariant Temporal Pooling** We alternate one-layer ESM and one-layer ETM over \(L\) layers, and finally attain the updated coordinates \(}_{i}^{(L)}^{T 3}\) for each node \(i\). Then the predicted coordinates at time \(T\) is given by the following equivariant linear pooling:

\[}_{i}^{*}(T)=}_{i}+}_{i}^{(L)}(T-1),\] (13)

where the parameter \(^{(T-1)}\) consists of learnable weights, and \(}_{i}=[}_{i}^{(L)}(0)-}_{i}^{(L)}(T-1),}_{i}^{(L)}(1)-}_{i}^{(L)}(T-1),,}_{i}^{(L) }(T-2)-}_{i}^{(L)}(T-1)]\) is translated by \(}_{i}^{(L)}(T-1)\) to allow translation invariance.

We train ESTAG end-to-end via the mean squared error (MSE) loss:

\[=_{i=1}^{N}\|}_{i}(T)-}_{i}^{*}(T)\|_{2} ^{2}.\] (14)

By the design of EDFT, ESM and ETM, we have the following property of our model ESTAG.

**Theorem 4.1**.: _We denote ESTAG as \(}(T)=(\{((t),g}(t),)\}_{t=0}^{ T-1})\), then \(\) is E(3)-equivariant._

_Proof. See Appendix A._

Although we mainly exploit EGNN as the backbone (particularly in ESM), our framework is general and can be easily extended to other equivariant GNNs, such as GMN , the multi-channel version of EGNN. In general, the extended models deal with multi-channel coordinate \(}^{3 m}\) instead of \(}^{3}\). The most significant feature of these models is to replace the invariant scalar \(\|}\|^{2}\) in the formulations of the message \(_{ij}\) (Eq. 6) with the term \(}^{}}\). It is easily to prove that this term is equivariant to any orthogonal matrix \(\), \(i.e.\), \((})^{}(})=}^{}}\), \(^{3 3}\), \(^{}=\). Besides, it can be reduced to invariant scalar \(\|}\|^{2}\) when \(m=1\). Empirically, we add the normalization term in order to achieve more stable performance: \(}^{}}}{\|}^{}} \|_{F}}\), where \(\|\|_{F}\) is the Frobenius norm. In the above derivation, we only display the formulation on EGNN, the details of multi-channel ESTAG are shown in Appendix C.1.

## 5 Experiments

**Datasets.** To verify the superiority of the proposed model, we evaluate our model on three real world datasets: **1)** molecular-level: MD17 , **2)** protein-level: AdS equilibrium trajectory dataset  and **3)** macro-level: CMU Motion Capture Database . These datasets involve several continuous long trajectories. Note that all the three datasets contain unobserved dynamics or factors and thus conform to the non-Markovian setting. In particular, the external temperature and pressure are unknown on MD17, the dynamics of water and ions is unobserved on AdS, and the states of the environment are not provided on Motion Capture. The original datasets are composed of long trajectories. We randomize the start point and extract the following \(T+1\) points with the interval \( t\). We take the first \(T\) timestamps as previous observations and last timestamp as the future position label.

**Baselines.** We compare the performance of ESTAG with several baselines: **1)** We regard the previous observation at start/mid/terminal (\(s/m/t\)) timepoint as the estimated position at timestamp \(T\) directly. **2) EGNN** utilizes a simple yet efficient framework which transforms the 3D vectors into invariant scalars. We provide EGNN with only one previous position at \(s/m/t\) timepoint to predict the future position in a frame-to-frame manner. **3) STGCN** is a spatio-temporal GNN that adopts a "sandwich" structure with two gated sequential convolution layers and on spatial graph convolution layer in between. We modify its default settings by predicting the residual coordinate between time \(T-1\) and \(T\), since directly predicting the exact coordinate at time \(T\) yields much worse performance.**4) AGL-STAN** leverages adaptive graph learning and self-attention for a comprehensive representation of intra-temporal dependencies and inter-spatial interactions. We modify AGL-STAN's setting in the same way as we do with STGCN. **5)** Typical GNNs with trivial spatio-temporal aggregation: we implement GNN , and other equivariant models EGNN, TFN , and SE(3)-Transformer  for each temporal frame in the historical trajectory and then estimate the future position as the weighted sum of all past frames, where the weights are learnable. All models are denoted with a prefix "ST" and we initialize their node features along with temporal positional encoding. **6) EqMotion** is one of equivariant spatio-temporal GNNs, which leverages the temporal information by fusing them for the model's initialization.

### Molecular-level: MD17

**Implementation details.** MD17 dataset includes the trajectories of 8 small molecules generated by MD simulation. We use the atomic number as the time-independent input node feature \(h^{(0)}\). The two atoms are 1-hop neighbor if their distance is less than the threshold \(\) and we consider two types of neighbors (i.e. 1-hop neighbor and 2-hop neighbor). Other settings including the hyper-parameters are introduced in Appendix D.1.

    & Aspirin & Benzene & Ethanol & Malonaldehyde & Naphthalene & Salicylic & Toluene & Uracil \\  Pt-\(s\) & 15.579 & 4.457 & 4.332 & 13.206 & 8.958 & 12.256 & 6.818 & 10.269 \\ Pt-\(m\) & 9.058 & 2.536 & 2.688 & 6.749 & 6.918 & 8.122 & 5.622 & 7.257 \\ Pt-\(t\) & 0.715 & 0.114 & 0.456 & 0.596 & 0.737 & 0.688 & 0.688 & 0.674 \\  EGNN-\(s\) & 12.056 & 3.290 & 2.354 & 10.635 & 4.871 & 8.733 & 3.154 & 6.815 \\ EGNN-\(m\) & 6.237 & 1.882 & 1.532 & 4.842 & 3.791 & 4.623 & 2.516 & 3.606 \\ EGNN-\(t\) & 0.625 & 0.112 & 0.416 & 0.513 & 0.614 & 0.598 & 0.577 & 0.568 \\  ST\_TFN & 0.719 & 0.122 & 0.432 & 0.569 & 0.688 & 0.684 & 0.628 & 0.669 \\ ST\_GNN & 1.014 & 0.210 & 0.487 & 0.664 & 0.769 & 0.789 & 0.713 & 0.680 \\ ST\_SE(3)TR & 0.669 & 0.119 & 0.428 & 0.550 & 0.625 & 0.630 & 0.591 & 0.597 \\ ST\_EGNN & 0.735 & 0.163 & 0.245 & 0.427 & 0.745 & 0.687 & 0.553 & 0.445 \\ EGMOTON & 0.721 & 0.156 & 0.476 & 0.600 & 0.747 & 0.697 & 0.691 & 0.681 \\ STGCN & 0.715 & 0.106 & 0.456 & 0.596 & 0.736 & 0.682 & 0.687 & 0.673 \\ AGL-STAN & 0.719 & 0.106 & 0.459 & 0.596 & 0.601 & 0.452 & 0.683 & 0.515 \\  ESTAG & **0.063** & **0.003** & **0.099** & **0.101** & **0.068** & **0.047** & **0.079** & **0.066** \\   

Table 1: Prediction error (\( 10^{-3}\)) on MD17 dataset. Results averaged across 3 runs. We do not display the standard deviation due to its small value.

**Results.** Table 1 shows the average MSE of all models on 8 molecules. We have some interesting observations as follows: **1)** ESTAG exceeds other models in all cases by a significant margin, supporting the general effectiveness of the proposed ideas. **2)** From the Pt-\(s/m/t\), we observe that the point closer to the future point has less prediction error. EGNN-\(s/m/t\) only takes one frame (\(s/m/t\)) as input, and attains slight improvement relative to Pt-\(s/m/t\). **3)** Compared with EGNN-\(t\), ST_EGNN, although equipped with trivial spatio-temporal aggregation, is unable to obtain consistent improvement, which indicates that how to unveil temporal dynamics appropriately is beyond triviality on this dataset. **4.** The non-equivariant methods particularly ST_GNN perform unsatisfactorily in most cases, implying that equivariance is an important property when modeling 3D structures.

**Visualization.** We have provided some visualizations of the predicted molecules by using the PyMol toolkit. Figure 4 shows that the prdicted MSEs of our model are much lower than ST_EGNN and our predicted molecules are closer to the ground-truth molecules. We have also displayed the learned attentions (Eq. 10) and the temporal weight (Eq. 13), where we find meaningful patterns of the temporal correlation. For clearer comparison, we display the molecule URACIL in 3D coordinate system, which can be found in Appendix D.5.

### Protein-level: Protein Dynamics

**Implementation details.** We evaluate our model on the AdK equilibrium trajectory dataset  via MDAnalysis toolkit . In order to reduce the data scale, we utilize MDAnalysis to locate the backbone atoms (\(C_{},C,N,O\)) of the residues and then regard the residues other than atoms in protein as the nodes with \(4\)-channel geometric features. We use the atomic number of four backbone atoms as the time-independent input node feature \(h^{(0)}\). We connect two atoms via an edge if their distance is less than a threshold \(\). Other settings including the hyper-parameters are introduced in Appendix D.2. Slightly different from the model on MD17 dataset, we generalize the single-channel ESTAG into multi-channel version which is presented in detail in Appendix C. We do not conduct EqMotion, TFN and SE(3)-TR used in the last experiment since it is non-trivial to modify them into multi-channel modeling. The state-of-the-art method GMN  is implemented by further adding the weighed temporal pooling similar to ST_EGNN.

   Method & MSE & Time(s) \\  Pt-\(s\) & 3.260 & - \\ Pt-\(m\) & 3.302 & - \\ Pt-\(t\) & 2.022 & - \\  EGNN-\(s\) & 3.254 & 1.062 \\ EGNN-\(m\) & 3.278 & 1.088 \\ EGNN-\(t\) & 1.983 & 1.069 \\  ST\_GNN & 1.871 & 2.769 \\ ST\_GNN & 1.526 & 4.705 \\ ST\_EGNN & 1.543 & 4.705 \\ STGCN & 1.578 & 1.840 \\ AGL-STAN & 1.671 & 1.478 \\  ESTAG & **1.471** & 6.876 \\   

Table 2: Prediction error and training time on Protein dataset. Results averaged across 3 runs.

Figure 4: PyMol visualization of the predicted molecules by our ESTAG and ST_EGNN, where the MSE (\( 10^{-3}\)) with respect to the ground truth is also shown. As expected, the predicted instances by ESTAG exhibit much smaller MSE than ST_EGNN, although the difference is not easy to visualize in some cases. For those obviously mispredicted regions of ST_EGNN, we highlight them with red rectangles. It is observed that ST_EGNN occasionally outputs isolated atoms, which could be caused by violation of the bond length tolerance in PyMol.

**Results.** The predicted MSEs are displayed in Table 2. Generally, the spatio-temporal models are better than Pt-\(s/m/t\) and EGNN-\(s/m/t\), and it suggests that applying spatio-temporal clues on this dataset is crucial, particularly given that the protein trajectories are generated under the interactions with external molecules such as water and ions. The equivariant models always outperform the non-equivariant counterparts (for example, ST_EGNN vs. ST_GNN). Overall, our model ESTAG achieves the best performance owing to its elaboration of equivariant spatio-temporal modeling. Additionally, we report the training time averaged over epochs in Table 2. It shows that the computation overhead of ESTAG over its backbone EGNN is acceptable given its remarkable performance enhancement. It is expected that the superiority of ESTAG on protein dataset is not as obvious as that on MD17, owing to various kinds of physical interactions between different amino acids, let along each amino acid is composed of a certain number of atoms, which makes the dynamics of a protein much more complicated than small molecules.

### Macro-level: Motion Capture

**Implementation details.** We finally adopt CMU Motion Capture Database  to evaluate our model. CMU Motion Capture Database involves the trajectories of human motion under several scenarios and we focus on walking motion (subject #35) and basketball motion (subject #102, only take trajectories whose length is greater than 170). The input feature of all the joints (nodes) \(h_{i}^{(0)}\) are all \(1\)s. The two joints are 1-hop neighbor if they are connected naturally and we consider two types of neighbors (i.e. 1-hop neighbor and 2-hop neighbor). Other settings including the hyper-parameters are introduced in Appendix D.3. Notably, the input of EqMotion only contain node coordinates, as the same as our method and other baselines. We find that EqMotion performs much worse by directly predicting the absolute coordinates. We then modify EqMotion to predict the relative coordinates across two adjacent frames and perform zero-mean normalization of node coordinates, for further improvement.

**Results.** Table 3 summarizes the results of all models on the Motion dataset. The spatio-temporal models are better than Pt-\(s/m/t\) and EGNN-\(s/m/t\), which again implies the necessity of taking the spatio-temporal history into account. Unexpectedly, the non-equivariant models are even superior to the equivariant baselines in walking motion, by, for instance, comparing AGL-STAN with ESTAG. We conjure that the samples of this dataset are usually collected in the same orientation, which potentially subdues the effect of rotation equivariance. It is thus not surprising that GNN even outperforms EGNN since GNN involves more flexible form of message passing. But for basketball motion which is more complicated to simulate, ESTAG yields a much lower MSE. We also notice that the attention-based models including our ESTAG, ST_SE(3)-Tr, and AGL-STAN perform promisingly, which probably due to the advantage of using attention to discovery temporal interactions within the trajectories.

### Ablation Studies

Here we conduct several ablation experiments on MD17 to inspect how our proposed components contribute to the overall performance and the results are shown in Table 4.

**1)** Without EDFT. We replace the FT-based edge features with the predefined edge features based on the connecting atom types and the distance between them. The simplified model encounters an average of increase in MSE, showcasing the effectiveness of the FT-based feature in modeling the spatial relation in graphs.

**2)** Without attention. We remove the ETM of ESTAG and observe slight detriment in the model performance, which demonstrates that attention mechanism can well capture the temporal dynamics.

   Method & Walk & Basketball \\  Pt-\(s\) & 329.474 & 886.023 \\ Pt-\(m\) & 127.152 & 413.306 \\ Pt-\(t\) & 3.831 & 15.878 \\  EGNN-\(s\) & 63.540 & 749.486 \\ EGNN-\(m\) & 32.016 & 335.002 \\ EGNN-\(t\) & 0.786 & 12.492 \\  ST\_GNN & 0.441 & 15.336 \\ ST\_TFN & 0.597 & 13.709 \\ ST\_SE(3)TR & 0.236 & 13.851 \\ ST\_EGNN & 0.538 & 13.199 \\ EqMotion & 1.011 & 4.893 \\ STGCN & 0.062 & 4.919 \\ AGL-STAN & **0.037** & 5.734 \\  ESTAG & 0.040 & **0.746** \\   

Table 3: Prediction error (\( 10^{-1}\)) on Motion dataset. Results averaged across 3 runs.

**3)** Without equivariance. We construct a non-equivariant spatio-temporal attentive framework based on vanilla GNN. ESTAG performs much better than the non-equivariant STAG, indicating that Euclidean equivariance is a crucial property when designing model on geometric graph.

**3)** Without equivariance. We construct a non-equivariant spatio-temporal attentive framework based on vanilla GNN. ESTAG performs much better than the non-equivariant STAG, indicating that Euclidean equivariance is a crucial property when designing model on geometric graph.

**4)** The impact of the number of layers \(L\). We investigate effect of the number of layers \(L\) on Ethanol dataset. We vary \(T\) from 1, 2, 3, 4, 5, 6 and present the results in Table 5. Considering the accuracy and efficiency simultaneously, we choose \(L=2\) for the ESTAG.

More ablation studies will be shown in Appendix E.

## 6 Conclusion

In this paper, we propose ESTAG, an end-to-end equivariant architecture for physical dynamics modeling. ESTAG first extracts frequency features via a novel Equivariant Discrete Fourier Transform (EDFT), and then leverages Equivariant Spatial Module (ESM) and an attentive Equivariant Temporal Module (ETM) to refine the coordinate in space and time domain alternatively. Comprehensive experiments over multiple tasks verify the superiority of ESTAG from molecular-level, protein-level, and to macro-level. Necessary ablations, visualizations, and analyses are also provided to support the validity of our design as well as the generalization of our method. One potential limitation of our model is that we only enforce the E(3) symmetry while other inductive bias like the energy conservation law is also required in physical scenarios.

In the future, we will continue extending our benchmark with more tasks and datasets and evaluate more baselines to validate the effectiveness of our model. It is also promising to extend our model to multi-scale GNN (like SGNN , REMoS-GNN , BSMS-GNN  and MS-MGN ), which is useful particularly for industrial-level applications involving huge graphs. Besides, it is valuable to employ our simulation method as a basic block for other applications such as drug discovery, material design, robotic control, etc.

## 7 Acknowledgement

This work was jointly supported by the following projects: the Scientific Innovation 2030 Major Project for New Generation of AI under Grant NO. 2020AAA0107300, Ministry of Science and Technology of the People's Republic of China; the National Natural Science Foundation of China (62006137); Beijing Nova Program (20230484278); the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (23NNKJ19); Tencent AI Lab Rhino-Bird Focused Research Program (RBFR2023005); Ant Group through CCF-Ant Research Fund (CCF-AFSG RF20220204); Public Computing Cloud, Renmin University of China.

    & Aspirin & Benzene & Ethanol & Malonaldehyde & Naphthalene & Salicylic & Toluene & Uracil \\  ESTAG & **0.063** & **0.003** & **0.099** & **0.101** & **0.068** & **0.047** & **0.079** & 0.066 \\  w/o EDFT & 0.079 & **0.003** & 0.108 & 0.148 & 0.104 & 0.145 & 0.102 & **0.063** \\ w/o Attention & 0.087 & 0.004 & 0.104 & 0.112 & 0.129 & 0.095 & 0.097 & 0.078 \\ w/o Equivariance & 0.762 & 0.114 & 0.458 & 0.604 & 0.738 & 0.698 & 0.690 & 0.680 \\ w/o Temporal & 0.084 & **0.003** & 0.111 & 0.139 & 0.141 & 0.098 & 0.153 & 0.071 \\   

Table 4: Ablation studies (\( 10^{-3}\)) on MD17 dataset. Results averaged across 3 runs.

   \(L\) & 1 & 2 & 3 & 4 & 5 & 6 \\  MSE (\( 10^{-4}\)) & 1.25 & 0.990 & 1.096 & 1.022 & 1.042 & 1.028 \\   

Table 5: MSE on Ethanol \(w.r.t.\) the number of layers \(L\).