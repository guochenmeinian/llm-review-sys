# Banded Square Root Matrix Factorization for Differentially Private Model Training

Nikita Kalinin

Institute of Science and Technology (ISTA)

Klosterneuburg, Austria

nikita.kalinin@ist.ac.at &Christoph Lampert

Institute of Science and Technology (ISTA)

Klosterneuburg, Austria

chl@ist.ac.at

###### Abstract

Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques. However, these methods suffer from high computational overhead because they require numerically solving a demanding optimization problem to determine an approximately optimal factorization prior to the actual model training. In this work, we present a new matrix factorization approach, BSR, which overcomes this computational bottleneck. By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems. For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible. We prove bounds on the approximation quality that hold both in the centralized and in the federated learning setting. Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead.

## 1 Introduction

We study the problem of _differentially private (DP) model training with stochastic gradient descent (SGD)_ in the setting of either federated or centralized learning. This task has recently emerged as one of the most promising ways to train powerful machine learning models but nevertheless guarantee the privacy of the used data, which led to a number of studies, both theoretical as well as application-driven (Abadi et al., 2016; Yu et al., 2020; Zhang et al., 2021; Kairouz et al., 2021; Denisov et al., 2022). The state of the art in the field are approaches based on the _matrix factorization (MF) mechanism_(Li et al., 2015; Henzinger et al., 2024), which combines theoretical guarantees with practical applicability (Choquette-Choo et al., 2023, 2024; c, 2023)1 It is based on the observation that all iterates of SGD are simply linear combinations of model gradients, which are computed at intermediate time steps. Consequently, the iterates can be written formally as the result of multiplying the matrix of coefficients, called _workload matrix_, with the row-stacked gradient vectors. To preserve the privacy of the training data in this process one adds suitably scaled Gaussian noise at intermediate steps of the computation. The MF mechanism provides a way to select the noise covariance structure based on a factorization of the workload matrix into two matrices.

Identifying the minimal amount of noise necessary to achieve a desired privacy level requires solving an optimization problem over all possible factorizations, subject to _data participation constraints_. For some specific settings, the optimal solutions have been characterized: for _streaming learning_, when each data batch contributes at most once to the gradients, Li et al. (2015) presented a formulation of this problem as a semi-definite program. Henzinger et al. (2024) proved that a square root factorizationof the workload matrix is asymptotically optimal for different linear workloads, including continual summation and decaying sums.

In this work, our focus lies on the settings that are most relevant to machine learning tasks: workload matrices that reflect SGD-like optimization, and participation schemes in which each data batch can potentially contribute to more than one gradient vector, as it is the case for standard multi-epoch training. Assuming that this happens at most once every \(b\) steps, for some value \(b 1\), leads to the problem of optimal matrix factorization in the context of \(b\)_-min-separated participation_ sensitivity. Unfortunately, as shown in Choquette-Choo et al. (2023), finding the optimal matrix factorization in this setting is computationally intractable. Instead, the authors proposed an approximate solution by posing additional constraints on the solution set. The result is a semi-definite program that is tractable, but still has high computational cost, making it practical only for small to medium-sized problem settings.

Subsequent work concentrated on improving or better understanding the factorizations for specific algorithms, such as plain SGD without gradient clipping, momentum, or weight decay (Koloskova et al., 2023) or specific, e.g. convex, objective functions (Choquette-Choo et al., 2024). Often, streaming data was asssumed, i.e. each data item can contribute at most to one model update, which is easier to analyze theoretical, but further removed from real-world applications (Dvijotham et al., 2024). A concurrent line of works has also focused on scaling matrix factorizations for large-scale training. McMahan et al. (2024) extended the Buffered Linear Toeplitz (BLT) mechanism to support multi-participation in federated learning, improving privacy-utility tradeoffs, while ensuring memory efficiency. McKenna (2024) improved DP Banded Matrix Factorization's scalability, enabling it to handle millions of iterations and large models efficiently. These advancements enhance the practicality of differentially private training in real-world applications.

Our work aims at a general-purpose solution that covers as many realistic scenarios as possible. Our ultimate goal is to make general-purpose differentially private model training as simple and efficient to use as currently dominating non-private technique. Our main contributions are:

1. We introduce a new factorization, **the _banded squared root (BSR)_, which is efficiently computable even for large workload matrices** and agnostic to the underlying training objective. For matrices stemming from SGD optimization potentially with momentum and/or weight decay, we even provide **closed form expressions.**
2. We provide **general lower bounds** on the approximate error for any factorization, along with specific **upper and lower bounds for the BSR**, Square Root, and baseline factorizations, in the contexts of both single participation (streaming) and **repeated participation** (e.g., multi-epoch) training.
3. We demonstrate experimentally that **BSR's approximation error is comparable to the state-of-the-art** method, and that **both methods also perform comparably in real-world training tasks**.

Overall, the proposed **BSR factorization achieves training high-accuracy models with provable privacy guarantees while staying computationally efficient even for large-scale training tasks.**

## 2 Background

Our work falls into the areas of _differentially private (stochastic) optimization_, of which we remind the reader here, following mostly the description of Denisov et al. (2022) and Choquette-Choo et al. (2023). The goal is to estimate a sequence of parameter vectors, \(=(_{1},,_{n})^{d}\), where each \(_{i}\) is a linear combination of _update vectors_, \(x_{1},,x_{i}^{d}\), that were computed in previous steps, typically as gradients of a model with respect to training data that is meant to stay private. We assume that all \(x_{i}\) have a bounded norm, \(\|x_{i}\|\). Compactly, we write \(=AX\), where the lower triangular _workload matrix_\(A\) contains the coefficients and \(X^{n d}\) is formed by stacking the update vectors as rows. With different choices of \(A\), the setting then reflects many popular first-order optimization algorithms, in particular stochastic gradient descent (SGD), potentially with momentum and/or weight decay. Depending on how exactly \(x_{1},,x_{n}\) are obtained, the setting can express different centralized as well as federated training paradigms. To formalize this aspect, we adopt the concept of _\(b\)-min-separated participation_(Choquette-Choo et al., 2023). For some integer \(b 1\), it states that if a data item (e.g. a single training example in central training, or a client batch in federated learning) contributed to an update \(x_{i}\), the earliest it can contribute again is the update \(x_{i+b}\). Additionally, let \(1 k\) be the maximal number any data point can contribute. In particular, this notion also allows us to treat in a unified way _streaming data_ (\(b=n\) or \(k=1\)), as well as _unrestricted access_ patterns (\(k=n\) with \(b=1\)), but also intermediate settings, such as _multi-epoch training_ on a fixed-size dataset.

The _matrix factorization_ approach (Li et al., 2015) adopts a _factorization_\(A=BC\) of the workload matrix and computes \(^{}=B(CX+Z)\), where \(Z\) is Gaussian noise that is chosen appropriately to make the intermediate result \(CX+Z\) private to the desired level. Algorithm 1 shows the resulting algorithm in pseudocode. It exploits the fact that instead of explicit multiplication by \(C\) and \(B\), standard optimization toolboxes can be employed with suitably modified update vectors, because also \(^{}=A(X+C^{-1}Z)\), and multiplication by \(A\) corresponds to performing the optimization.

```
0: Initial model \(_{0}^{d}\), dataset \(D\), batchsize \(b\), matrix \(C^{n n}\), model loss \((,d)\), clipnorm \(\), noise matrix \(Z^{n d}\) with i.i.d. entries \((0,s^{2})\), where \(s=_{k,b}(C)\). for\(i=1,2,,n\)do \(S_{i}\{d_{1},,d_{m}\} D\) select a data batch, respecting the data participation constraints \(g_{i}_{}(_{i-1},d_{j}))\) for\(j=1,,m\) \(x_{i}_{j=1}^{m}_{}(g_{j})\) where \(_{}(d)=(1,/||d||)d\) \(_{i} x_{i}+[C^{-1}Z]_{[i,]}\) \(_{i}(_{i-1},_{i})\), // SGD model updates Output:\(=(_{1},,_{n})\) ```

**Algorithm 1** Differentially Private SGD with Matrix Factorization

Different factorizations recover different algorithms from the literature. For example, \(B=A\), \(C=\) recovers DP-SGD (Abadi et al., 2016), where noise is added directly to the gradients. Conversely, \(B=\), \(C=A\) simply adds noise to each iterate of the optimization (Dwork et al., 2006). However, better choices than these baselines are possible, in the sense that they can guarantee the same levels of privacy with less added noise, and therefore potentially with higher retained accuracy. The reason lies in the fact that \(B\) and \(C\) play different roles: \(B\) acts as a _post-processing_ operation of already private data. Hence, it has no further effect on privacy, but it influences to what amount the added noise affects the expected error in the approximation of \(\). Specifically, for \(Z(0;s)\),

\[_{Z}\|-^{}\|_{F}^{2}=_{Z}\|BZ\|_{F}^ {2}=s^{2}\|B\|_{F}^{2}.\] (1)

In contrast, \(CX\) is the quantity that is meant to be made private. Doing so requires noise of a strength proportional to \(C\)'s _sensitivity_, \((C):=_{X X^{}}\|CX-CX^{}\|_{F}\), where the _neighborhood relation_, \(X X^{}\), indicates that the two sequences of update vectors differ only in those entries that correspond to a single data item2 As shown in Choquette-Choo et al. (2023), in the setting of \(b\)-min-separated repeated participation, it holds that

\[_{k,b}(C)_{_{k,b}}(C^{}\!C)_{[i,j]}},\] (2)

where \(_{k,b}=\{\ \{1,,n\}\ :\ || k(\{i,j\}\ \ i=j\ \ |i-j| b)\ \}\), is the set of possible \(b\)-min-separated index sets with at most \(k\) participation. Furthermore, (2) holds even with equality if all entries of \(C^{}\!C\) are non-negative.

Combining (1) with \(s=_{k,b}(C)\) yields a quantitative measure for the quality of a factorization.

**Definition 1**.: _For any factorization \(A=BC\), its **expected approximation error** is_

\[(B,C):=_{Z}\|-^{}\|_{F}^{2}/n} =}_{k,b}(C)\|B\|_{F},\] (3)

_where the \(1/\) factor is meant to make the quantity comparable across different problem sizes._The _optimal factorization_ by this reasoning would be the one of smallest expected approximation error. Unfortunately, minimizing (3) across all factorizations it is generally computationally intractable. Instead, Choquette-Choo et al. (2023) propose an _approximately optimal factorization_.

**Definition 2**.: _For a workload matrix \(A\), let \(S\) be the solution to the optimization problem_

\[_{S_{+}^{n}}[A^{}\!A\,S^{-1}] (S)=1\;\;S_{[i,j]}= 0\;\;\;\;|i-j| b,\] (4)

_where \(_{+}^{n}\) is the cone of positive definite \(n n\) matrices. Then, \(A=BC\) is called the **approximately optimal factorization (AOF)**, if \(C\) is lower triangular and fulfills \(C^{}\!C=S\)._

The optimization problem (4) is a semi-definite program (SDP), and can therefore be solved numerically using standard packages. However, this is computationally costly, and for large problems (e.g. \(n>5000\)) computing the AOF solution is impractical. This poses a problem for real-world training tasks, where the number of update steps are commonly thousands or tens of thousands.

Solving (4) itself only approximately can mitigate this problem to some extent, but as we will discuss in Section 4, this can lead to robustness problems, especially because the recovery of \(C\) from \(S\) in Definition 2, e.g. by a Cholesky decomposition, tends to be sensitive to numerical errors.

## 3 Banded Square Root Factorization

In the following section, we introduce our main contribution: a general-purpose factorization for the task of differentially private stochastic optimization that can be computed efficiently even for large problem sizes.

**Definition 3** (Banded Square Root Factorization).: _Let \(A^{n n}\) be a lower triangular workload matrix with strictly positive diagonal entries. Then, we call \(A=C^{2}\) the **square root factorization (SR)**, when \(C\) denotes the unique matrix square root that also has strictly positive diagonal entries. Furthermore, for any bandwidth \(p\{1,,n\}\), we define the **banded square root factorization of bandwidth \(p\) (\(p\)-BSR)** of \(A\), as_

\[A=B^{|p|}C^{|p|}\] (5)

_where \(C^{|p|}\) is created from \(C\) by setting all entries below the \(p\)-th diagonal to \(0\),_

\[C^{|p|}_{[i,j]}=C_{[i,j]}&\\ 0& B^{|p|}=A(C^{|p|})^{-1}.\] (6)

Note that determining the SR, and therefore any \(p\)-BSR, is generally efficient even for large workload matrices, because explicit recursive expressions exist for computing the square root of a lower triangular matrix (Bjorck and Hammarling, 1983; Deadman et al., 2012).

In the rest of this work, we focus on the case where the workload matrix stems from _SGD with momentum and/or weight decay_, and we show that then even closed form expressions for the entries of \(C^{|p|}\) exist that renders the computational cost negligible.

### Banded Square Root Factorization for SGD with Momentum and Weight Decay

We recall the update steps of SGD with momentum and weight decay:

\[_{i}=_{i-1}- m_{i} m_{i}= m _{i-1}+x_{i}\] (7)

where \(x_{1},,x_{n}\) are the update vectors, \(>0\) is the _learning rate_ and \(0<1\) is the _momentum strength_ and \(0< 1\) is the weight decay parameter. Note that our results also hold for \(=0\), i.e. without momentum, and for \(=1\), i.e., without weight decay. In line with real algorithms and to avoid degenerate cases, we assume \(<\) throughout this work. The update vectors are typically gradients of the model with respect to its parameters, but additional operations such as normalization or clipping might have been applied.

Unrolling the recursion, we obtain an expression for \(_{i}\) as a linear combination of update vectors as

\[_{i}=_{j=1}^{i}x_{j}_{k=j}^{i}^{i-k}^{k-j }.\] (8)Consequently, the workload matrix has the explicit form \(A= A_{,}\) for

\[A_{,}=a_{0}&0&0&&0\\ a_{1}&a_{0}&0&&0\\ a_{2}&a_{1}&a_{0}&&0\\ &&&&\\ a_{n-1}&a_{n-2}&&a_{1}&a_{0}a_{j}=_{i=0}^{j}^{i} ^{j-i}=-^{j+1}}{-}.\] (9)

As one can see, \(A_{,}\) is a lower triangular Toeplitz-matrix, so it is completely determined by the entries of its first column. In the following, we use the notation \((m_{1},,m_{n})\) to denote a lower triangular Toeplitz matrix with first column \(m_{1},,m_{n}\), i.e. \(A_{,}=(a_{0},,a_{n-1})\).

Our first result is an explicit expression for the positive square root of \(A_{,}\) (and thereby its \(p\)-BSR).

**Theorem 1** (Square-Root of SGD Workload Matrix).: _Let \(A_{,}\) be the workload matrix (9). Then \(A_{,}=C_{,}^{2}\) for \(C_{,}=(c_{0},,c_{n-1})\), with \(c_{0}=1\) and \(c_{j}=_{i=0}^{j}^{j-i}r_{j-i}r_{i}^{i}\) for \(j=1,,n-1\) with coefficients \(r_{i}=\). For any \(p\{1,,n\}\), the \(p\)-banded BSR matrix \(C_{,}^{|p|}\) is obtained from this by setting all coefficients \(c_{j}=0\) for \(j p\)._

Proof sketch.: The proof be found in Appendix F.1. Its main idea is to factorize \(A_{,}\) into a product of two simpler lower triangular matrices, each of which has a closed-form square root. We show that the two roots commute and that the matrix \(C_{,}\) is their product, which implies the theorem. 

### Efficiency

We first establish that the \(p\)-BSR for SGD can be computed efficiently even for large problem sizes.

**Lemma 1** (Efficiency of BSR).: _The entries of \(C_{,}^{|p|}\) can be determined in runtime \(O(p p)\), i.e., in particular independent of \(n\)._

Proof sketch.: As a lower triangular Toeplitz matrix, \(C_{,}^{|p|}\) is fully determined by the values of its first column. By construction \(c_{p+1},,c_{n}=0\), so only the complexity of computing \(c_{1},,c_{p-1}\) matters. These can be computed efficiently by writing them as the convolution of vectors \((^{i}r_{i})_{i=0,,p-1}\) and \((^{i}r_{i})_{i=0,,p-1}\) and, e.g., employing the fast Fourier transform. 

Note that for running Algorithm 1, the matrix \(B\) of the factorization is not actually required. However, one needs to know the _sensitivity_ of \(C_{,}^{|p|}\), as this determines the necessary amount of noise. The following theorem establishes that for a large class of matrices, including the BSR in the SGD setting, this is possible exactly and in closed form.

**Theorem 2** (Sensitivity for decreasing non-negative Toeplitz matrices).: _Let \(M=(m_{0},,m_{n-1})\) be a lower triangular Toeplitz matrix with decreasing non-negative entries, i.e._

\[m_{0} m_{1} m_{2} m_{n-1} 0.\]

_Then its sensitivity (2) in the setting of \(b\)-min-separation is_

\[_{k,b}(M)=_{j=0}^{k-1}M_{[,1+jb]}= (_{i=0}^{n-1}(_{j=0}^{\{k-1,i/b\}}m_{i-jb})^{2} )^{1/2},\] (10)

_where \(M_{[,1+jb]}\) denotes the \((1+jb)\)-th column of \(M\)._

Proof sketch.: The proof can be found in Appendix F.2. It builds on the identity (2), which holds with equality because of the non-negative entries of \(M\). Using the fact that the entries of \(M\) are non-increasing one establishes that an optimal \(b\)-separated index set is \(\{1,1+b,,1+(k-1)b\}\). From this, the identity (10) follows. 

**Corollary 1**.: _The sensitivity of the \(p\)-BSR for SGD can be computed using formula (10)._

Proof sketch.: It suffices to show that the coefficients \(c_{0},,c_{n-1}\) of Theorem 1 are monotonically decreasing. We do so by an explicit computation, see Appendix F.3.

### Approximation Quality - Single Participation

Having established the efficiency of BSR, we now demonstrate its suitability for high-quality model training. To avoid corner cases, we assume that \(\) is an integer, which does not affect the asymptotic behavior. We also discuss only the case in which the update vectors have bounded norm \(=1\). Results for general \(\) can readily be derived using the linearity of the sensitivity with respect to \(\).

We first discuss the case of model training with single participation (\(k=1\)), where more precise results are possible than the general case. Our main result are bounds on the expected approximation error of the square root factorization that, in particular, prove its asymptotic optimality.

**Theorem 3** (Expected approximation error with single participation).: _Let \(A_{,}^{n n}\) be the workload matrix (9) of SGD with momentum \(0<1\) and weight decay parameter \(0< 1\), where \(>\). Assume that each data item can contribute at most once to an update vector (e.g. single participation, \(k=1\)). Then, the expected approximation error of the square root factorization, \(A_{,}=C_{,}^{2}\), fulfills_

\[1(C_{,},C_{,})}}\] (11)

_for \(<1\), and_

\[\{1,\}(C_{1,},C_{1, })}.\] (12)

Proof sketch.: For the proof, we establish a relations between \(_{1,n}(C)\) and \(\|C_{,}\|_{F}\), and then we bound the resulting expressions by an explicit analysis of the norm. For details, see Appendix F.5. 

The following two results provide context for the interpretation of Theorem 3.

**Theorem 4**.: _Assume the setting of Theorem 3. Then, for any factorization \(A_{,}=BC\) with \(C^{}C 0\), the expected approximation error fulfills_

\[(B,C)=(1)&\\ ( n)&\] (13)

Proof sketch.: The theorem is the special case \(k=1\) of Theorem 8, which we state in the next section and prove in Section F.9. 

**Theorem 5**.: _Assume the setting of Theorem 3. Then, the baseline factorizations \(A_{,}=A_{,}\) and \(A_{,}= A_{,}\) fulfill, for \(<1\),_

\[(A_{,},) =}{) (1-^{2})}}+o(1)(A_{1,}, {Id})}{(1-)}+o()\] (14) \[(,A_{,}) =}{ )(1-^{2})}}\,+o(1)(,A_{ 1,})}{1-}\,+o().\] (15)

Proof sketch.: The result follows from an explicit analysis of the coefficients, see Appendix F.6. 

Discussion.Theorems 3 to 5 provide a full characterization of the approximation quality of the square root factorization as well as its alternatives: 1) the square root factorization has asymptotically optimal approximation quality, because the upper bounds in Equation (12) match the lower bounds in Equation (13); 2) the AOF from Definition 2 also fulfills the conditions of Theorem 4. Therefore, it must also adhere to the lower bound (13) and cannot be asymptotically better than the square root factorization; 3) the approximation qualities of the baseline factorizations in Equation (14) and (15) are asymptotically worse than optimal in the \(=1\) setting, and worse by a constant factor for \(<1\). The BSR factorization can be applied even in more general scenarios, such as with varying learning rates. However, in this case, the workload matrix will no longer be Toeplitz. This makes it difficult to provide analytical guarantees for the matrix, but it can still be applied numerically.

### Approximation Quality - Repeated Participation.

We now provide mostly asymptotic statements about the approximation quality of BSR and baselines in the setting where data items can contribute more than once to the update vectors.

**Theorem 6** (Approximation error of BSR).: _Let \(A_{,}^{n n}\) be the workload matrix (9) of SGD with momentum \(0<1\) and weight decay \(0< 1\), with \(>\). Let \(A_{,}=B_{,}^{|p|}C_{,}^{|p|}\), be its banded square root factorization as in Definition 3. Then, for any \(b\{1,,n\}\), \(p b\), and \(k\{1,,\}\) it holds:_

\[(B_{,}^{|p|},C_{,}^{|p|})=O_{ }(})+O_{,p}()&=1,\\ O_{,p,}()&<1.\] (16)

Proof sketch.: For the proof, we separately bound the sensitivity of \(C_{,}^{|p|}\) and the Frobenius norm of \(B_{,}^{|p|}\). The former is straightforward because of the matrix's band structure. The latter requires an in-depth analysis of the inverse matrix' coefficient. Both steps are detailed in Appendix F.7. 

The following results provide context for the interpretation of Theorem 6.

**Theorem 7** (Approximation error of Square Root Factorization).: _Let \(A_{,}^{n n}\) be the workload matrix (9) of SGD with momentum \(0<1\) and weight decay \(0< 1\), with \(>\). Let \(A_{,}=C_{,}^{2}\) be its square root factorization. Then, for any \(b\{1,,n\}\) and \(k=\) it holds:_

\[(C_{,},C_{,})=_{} (k+ n)&=1,\\ _{,}&<1.\] (17)

Proof sketch.: We bound \(_{k,b}(C_{,})\) and \(\|C_{,}\|_{F}\) using the explicit entries for \(C_{,}\) from Theorem 1. Details are provided in Appendix F.8. 

**Theorem 8**.: _Assume the setting of Theorem 6. Then, for any factorization \(A_{,}=BC\) with \(C^{}C 0\), the approximation error fulfills_

\[(B,C) n&=1,\\ &<1,\] (18)

Proof sketch.: The proof is based on the observation that \(\|X\|_{F}\|Y\|_{F}\|XY\|_{*}\) for any matrices \(X,Y\), where \(\|\|_{*}\) denotes the nuclear norm. To derive (18), we show that \(_{k,b}(C)\) is lower bounded by \(}{n}\|C\|_{F}\), and derive explicit bounds on the singular values of \(A_{,}\). 

**Theorem 9**.: _Assume the setting of Theorem 6. Then, the baseline factorizations \(A_{,}=A_{,}\) and \(A_{,}= A_{,}\) fulfill_

\[(A_{,},)}&=1,\\ &<1.(,A_{ ,})}{}&=1,\\ &<1.\] (19)

Proof sketch.: The proof relies on the fact that the workload matrices can be lower bounded componentwise by simpler matrices: \(A_{,} A_{,0}\) and \(A_{,0}\). For the simpler matrices, the bounds (19) can then be derived analytically, and the general case follows by monotonicity.

Discussion.Analogously to the case of single participation, Theorems 6 to 9 again establish that the proposed BSR is asymptotically superior to the baseline factorizations if \(=1\). A comparison of Theorems 6 and 7 suggests that, at least for maximal participation, \(k=\) and \(p=b\), the bandedness of the \(p\)-BSR improves the approximation quality, specifically in the practically relevant regime where \(b n\). While none of the methods match the lower bound of Theorem 6, we conjecture that this is not because any asymptotically better methods would exist, but rather a sign of Equation (18) is not tight. Both theoretical consideration and experiments suggest that a term linear in \(k\) should appear there. For \(<1\), all studied methods are asymptotically identical and, in fact, optimal.

## 4 Experiments

To demonstrate that BSR can achieve high accuracy not only in theory but also in practice, we compare it to AOF and baselines in numerical experiments. **Our results show that BSR achieves quality comparable to the AOF, but without the computational overhead, and it clearly outperforms the baseline factorizations.** The privacy guarantees are identical for all methods, so we do not discuss them explicitly.

Implementation and computational cost.We implement BSR by the closed-form expressions of Theorem (1). For single data participation, we use the square root decomposition directly. For repeated data participation we use \(p\)-BSR with \(p=b\). Using standard _python/numpy_ code, computing the BSR as dense matrices are memory-bound rather than compute-bound. Even sizes of \(n=10,000\) or more take at most a few seconds. Computing only the Toeplitz coefficients is even faster, of course.

To compute AOF, we solve the optimization problem (4) using the cvxpy package with SCS backend, see Algorithm B for the source code3. With the default numerical tolerance, \(10^{-4}\), each factorization took a few minutes (\(n 100\)) to hours (\(n 500\)) to several days (\(n 700\)) of CPU time. Note that this overhead reappears for any change in the number of update steps, \(n\), weight decay, \(\), or momentum, \(\), as these induce different workload matrices. In our experiments, when the optimization for AOF did not terminate within 10 days, we reran the optimization problem with the tolerance increased by a factor of 10. The runtime depends not only on the matrix size but also on the entries. In particular, we observe matrices with momentum to be harder to factorize than without. For large matrix sizes we frequently encountered numerical problems: the intermediate matrices, \(S\), in (4), often did not fulfill the positive definiteness condition required to solve the subsequent Cholesky decomposition for \(C\). Unfortunately, simply projecting the intermediates back to the cone of positive semi-definite matrices is not enough, because the resulting \(C\) matrices also have to be invertible and not too badly conditioned. Ultimately, we adopted a postprocessing step for \(S\) that ensures that all its eigenvalues were at least of value \(\), which we find to be a reasonable modification to ensure the stability of the convergence. Enforcing this empirically found value leads to generally good results, as our experiments below show, but it does add an undesirable extra level of complexity to the process. In contrast, due to its analytic expressions, BSR does not suffer from numerical problems. It also does not possess additional hyperparameters, such as a numeric tolerance or the number of optimization steps.

Apart from the factorization itself, the computational cost of BSR and AOF are nearly identical. Both methods produce (banded) lower triangular matrices, so computing the inverse matrices or solving linear systems can be done within milliseconds to seconds using forward substitution. Note that, in principle, one could even exploit the Toeplitz structure of \(p\)-BSR, but we found this not to yield any practical benefit in our experiments. Computing the sensitivity is trivial for \(p\)-BSR using Corollary 1, and it is still efficient for AOF by the dynamic program proposed in Choquette-Choo et al. (2023).

Expected Approximation Error.As a first numeric experiment, we evaluate the expected approximation error for workload matrices that reflect different SGD settings. Specifically, we use workload matrices (9) for \(n\{100,200,,1000,1500,2000\}\), with \(=\{0.99,0.999,0.9999,1\}\), and \(\{0,0.9\}\), either with single participation, \(k=1\), or repeated participation, \(b=100\), \(k=n/100\). Figure 1 shows the expected approximate error, \((B,C)\), of the proposed BSR, AOF, as well as the baseline factorizations, \(A=A\) and \(A= A\) in two exemplary cases. Additional results for other privacy levels can be found in Appendix C.

The results confirm our expectations from the theoretical analysis: in particular, BSR's expected approximation error is quite close to AOF's, typically within a few percent (left plot). Both methods are clearly superior to the naive factorizations. For large matrix sizes, BSR sometimes even yields slightly better values than AOF (right plot). However, we believe this to be a numeric artifact of us having to solve AOF with less-than-perfect precision.

**Private Model Training on CIFAR-10.** To demonstrate the usefulness of BSR in practical settings, we follow the setup of Kairouz et al. (2021) and report results for training a simple ConvNet on the CIFAR-10 dataset (see Table 1 in Appendix C for the architecture). Specifically, we adapt Google's reference implementation of DP-SGD in jax Bradbury et al. (2018) to work with the different matrix factorizations: BSR, AOF, and the two baselines. To reflect the setting of single-participation training, we split the 50,000 training examples into batches of size \(m\{1000,500,250,200,100,50,25\}\), resulting in \(n\{100,200,400,500,1000,2000\}\) update steps. For repeated participation, we fix the batch size to \(500\) and run \(k\{1,2,,10,15,20\}\) epoch of training, i.e. \(n=100k\) and \(b=100\). In both cases, 20% of the training examples are used as validation sets to determine the learning rate \(\{0.01,0.05,0.1,0.5,1\}\), weight decay parameters \(\{0.99,0.999,0.9999,1\}\), and momentum \(\{0,0.9\}\). Figure 2 shows the test set accuracy of the model trained with hyperparameters that achieved the highest validation accuracy.4 One can see the expected effect that in DP model training, more update steps/epochs do not necessary lead to higher accuracy due to the need to add more noise. The quality of models trained with BSR is mostly identical to AOF. When training for a large number of epochs it achieves even better slightly results, but this could also be an artifact of us having to solve AOF with reduced precision in this regime. Both methods are clearly superior to the baselines.

Figure 1: Expected approximation error of BSR, AOF and baseline factorizations for two different hyperparameter settings (left: \(=0.999,=0\), right: \(=1,=0.9\)) with repeated participation (\(b=100,k=n/100\)). See Section 4 for details.

Figure 2: Classification accuracy (mean and standard deviation over 5 runs with different random seeds) on CIFAR-10 for BSR, AOF, and baselines for \((,)=(4,10^{-5})\) for independent training runs. Left: one epoch, different batch sizes. Right: different number of epochs, constant batch size.

Conclusion and Discussion

We introduce an efficient and effective approach to the matrix factorization mechanism for SGD-based model training with differential privacy. The proposed banded square root factorization (BSR) factorization achieves results on par with the previous state-of-the-art, and clearly superior to baseline methods. At the same time, it does not suffer from the previous method's computational overhead, thereby making differentially private model training practical even for large scale problems.

Despite the promising results, some open questions remain. On the theoretical side, the asymptotic optimality of BSR without weight decay is still unresolved because the current upper bounds on the expected approximation error do not match the provided lower bounds. Based on the experimental results, we believe this discrepancy lies with the lower bounds, which we suspect should be linear in the number of participations. We observe that BSR achieves results comparable to AOF, although we cannot currently prove this due to the insufficient understanding of AOF's theoretical properties; nonetheless, we consider it a promising research direction. On the practical side, it would be interesting to extend the guarantees to even more learning scenarios, such as variable learning rates.

#### Acknowledgments and Disclosure of Funding

This research was supported by the Scientific Service Units (SSU) of ISTA through resources provided by Scientific Computing (SciComp). We thank Monika Henzinger and Jalaj Upadhyay for their valuable comments on the earlier versions of this manuscript.