# CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics

Wanru Zhao \({}^{1}\)1, Hongxiang Fan2,1, Shell Xu Hu \({}^{3}\), Bofan Chen \({}^{1}\), Nicholas D. Lane\({}^{1,4}\)

\({}^{1}\) University of Cambridge, UK \({}^{2}\) Imperial College London, UK

\({}^{3}\) Samsung AI Center Cambridge \({}^{4}\) Flower Labs

###### Abstract

Recent research has highlighted the importance of data quality in scaling large language models (LLMs). However, automated data quality control faces unique challenges in collaborative settings where sharing is not allowed directly between data silos. To tackle this issue, this paper proposes a novel data quality control technique based on the notion of data influence on the training dynamics of LLMs, that high quality data are more likely to have similar training dynamics to the anchor dataset. We then leverage the influence of the training dynamics to select high-quality data from different private domains, with centralized model updates on the server side in a collaborative training fashion by either model merging or federated learning. As for the data quality indicator, we compute the per-sample gradients with respect to the private data and the anchor dataset, and use the trace of the accumulated inner products as a measurement of data quality. In addition, we develop a quality control evaluation tailored for collaborative settings with heterogeneous medical domain data. Experiments show that training on the high-quality data selected by our method can often outperform other data selection methods for collaborative fine-tuning of LLMs, across diverse private domain datasets, in medical, multilingual and financial settings. Our code is released at CLUES.

## 1 Introduction

Large language models (LLMs) training has predominantly relied on the accumulation of vast datasets. Recent observations suggest that even a modest quantity of high-quality diverse data can significantly enhance the instruction following capacity of LLMs. Previously, data quality control relied heavily on manual selection processes . This approach, while being commonly used, rendered scalability challenges due to the substantial labor costs. Recent advancements have seen automated low-quality data filters , such as perplexity filters  and de-duplication filters . However, their effectiveness in data quality control in more complex environments remains to be explored, where data are spread across silos and locations in different formats and difficult to find.

Collaborative training techniques, such as model merging  and federated learning , are common paradigms for addressing data-sharing constraints and GDPR  compliance. However, data quality control for private data is even more challenging if users are in charge of manually filtering data. We summarize here the two unique challenges: **(1) Quality Heterogeneity** Some clients may possess a higher proportion of low-quality data compared to others, thus we should not select data from all clients with a fixed selection ratio. **(2) Domain Heterogeneity** Different data silos may come from different vertical domains, for example, in the multilingual setting, different languages have different quality standards that are never unified.

In this paper, we propose **CLUES** (collaborative learning **u**nder **s**election), an automated high-quality data selection method for collaborative fine-tuning of Large Language Models (LLMs), showcasing notable performance improvements in mixed-quality data environments from different private domain data. In these domains, private LLM vendors are supposed to build their specialized applications based on open-source LLMs using their own private data, which represent specialized domains with significant private (e.g., patient records) and public data (e.g., scientific papers). By tracing the training dynamics of each training sample, we leverage public dataset to define an anchor dataset and compute the influence of each training sample on the anchor dataset, and set a global threshold to provide effective collaborative quality controls compared with traditional local data quality selection methods in the following aspects: _(1) General_: Our method is a general pipeline to improve the generalization performance for LLM fine-tuning. It has an interpretation in terms of bi-level optimization with inner optimization in the client side and outer optimization in the server side to minimize the loss on the anchor dataset. _(2) Collaborative_: Our method is a collaborative fine-tuning paradigm that can be seamlessly integrated into existing model merging and federated learning frameworks, where the modification occurs on the server side only to incorporate data selection. _(3) Scalable_: We only employ an approximation to solve the bi-level optimization, which makes it scalable to LLMs.

We evaluate our proposed method on medical, multilingual and financial Question Answering (QA) datasets, demonstrating significant improvements of up to 67.3% on challenging medical and financial QA datasets, highlighting the effectiveness of our proposed method. Through extensive analyses, we demonstrate the significant impact of leveraging training dynamics on the collaborative data quality control of LLMs.

## 2 Problem formulation: Collaborative Data Quality

### Related Work

Collaborative LLM Fine-Tuning Paradigms: Model Merging and Federated LearningCollaborative fine-tuning exhibits certain advantageous properties as a distributed machine learning paradigm by shifting the traditional model training process towards sharing model parameters instead of raw data. Participating clients train models using their own private datasets locally, and the updated model parameters are aggregated on the server. This preserves the privacy of the underlying data while collectively benefiting from the knowledge gained during the training process . We focus on merging fine-tuned models that are optimized from the same pre-trained backbone. Different fine-tuned models initialized from the same pre-trained model effectively share a part of the optimization trajectory and can often be merged without accounting for permutation symmetry [40; 11; 17]. Therefore, merging fine-tuned models can improve performance on a single target task [13; 6], improve out-of-domain generalization [2; 1], create multitask models from different tasks , and other settings [23; 4].

One of the most significant challenges plaguing model merging and federated learning methods in previous research is the concern that the model parameters might interfere with each other during weighted averaging or other merging operations. This undesirable interaction could potentially lead to a merged model that performs worse than the individual models before merging. We argue that it can be tackled from the perspective of data attribution.

**Model merging.** let \(f_{}\) denote the language model and \(D_{k}\) denote the training dataset on client \(k\). Given the training datasets \(D_{k}\), we can define a model merging operator \(_{K}(;D_{k},k K=\{1,,n\}): \). The model merging process can be expressed as

\[f_{merging}=_{K}(f)\]

**Federated Averaging.** Based on the notation of model merging, the federated averaging process can be expressed as

\[f_{fed}=(_{t=1}^{T}_{S_{t}(K)})(f)\]

where \(T\) is the round number. \(S_{t}(K)\) is the index set of clients participated in the training in round \(t\).

Data Attribution and Selection for LLMsThe quality of the training data of a machine learning model can have a significant impact on its performance. One measure of data quality is the notion of valuation, i.e., the degree to which a given training example affects the model and its predictive performance. Although data attribution is a well-known concept for researchers, the complexity behind large language models, coupled with their growing size, features, and datasets, has made quantification difficult. Recent methods include Perplexity Score, IFD , and DatalInf , etc. More details are provided in Appendix E. However, those data attribution above have not been used in collaborative settings where each client has statistical heterogeneous and quality heterogeneous private-domain data. And previous data selection methods have not provide a way to determine the golden threshold to decide whether a training data sample should be kept or filter out.

Training DynamicsPrevious works  that analyze training dynamics focus primarily on supervised learning and are largely model- and data-agnostic. Swayamdipta et al.  empirically demonstrated the influence of data by visually mapping individual training samples according to their impact on the correctness, confidence, and variability of a model.

### Assumption and Objective: Collaborative High-quality Data Selection for LLMs

**Definition 1.1** (Data Quality on Specific Domain \(k\)).: Given a model architecture \(\), a training configuration (optimizer, etc.), and a validation set \(D_{val}\) in a specific domain \(k\), the quality of training data \(z\) is defined as follows: for \(z_{1},z_{2}_{train}\), if \(_{val}((z_{1}),D_{val})<_{val}((z_{2}),D_{ val})\), then the quality of \(z_{1}\) is considered higher than that of \(z_{2}\). Here, \(_{val}\) denotes the validation loss. In other words, the lower the validation loss, the higher the data quality.

**Definition 1.2** (Data Quality in Collaborative Private Domains).: Given a model architecture \(\), a training configuration (optimizer, etc.), and a validation set \(D_{val}=_{val}^{(1)},_{val}^{(2)},,_{val }^{(K)}\) for all \(K\) tasks, the quality of training data \(z\) is defined based on the validation loss of the global model \(_{merged}\) on \(D_{val}\). Specifically, for \(z_{1},z_{2}_{train}^{(k)}\), if \(_{val}(_{merged}(z_{1}),D_{val})<_{val}(_{ merged}(z_{2}),D_{val})\), then the quality of \(z_{1}\) is considered higher than that of \(z_{2}\). As in the single-domain case, lower validation loss indicates higher data quality.

**Remarks 1** (Impact of Low Quality Data in Collaborative Private Domains).: We manually construct low-quality data samples on each client. We change the proportion of low-quality data from 0% to 100%. Higher scores indicate better performance. From Fig. 1, a larger portion of low-quality data results in higher validation loss, and more unstable and less effective training loss curve. Fig. 2 shows the performance drop when we change the proportion of low-quality data from 0% to 60%.

Figure 1: Validation loss and training loss.

Figure 2: Performance drop on the performance of collaborative fine-tuning of LLMs when we change the proportion of low-quality data from 0% to 60%. Higher scores indicate better performance.

**Remarks 2** (Enhancing Data Quality on Collaborative Private Domains).: In the collaborative learning framework, the ratio and distribution of low-quality data are unknown a priori. Only the server has access to the global distribution of both high-quality and low-quality data, while individual clients cannot infer the global distribution from their local distributions due to statistical heterogeneity. The server can infer the distribution of high-quality data from public anchor data. Our objective is to select data points that most significantly reduce the validation loss of the global model, rather than optimizing for each local model independently. It is important to note that the scope of this study does not consider new models joining during training or continual learning paradigms.

## 3 Methodology: CLUES

### Overview

In our workflow, each client performs local training using his own high-quality private data. We have a public validation set located on both the clients and the server, which consists of commonly recognized, high-quality public data. As illustrated in Figure 3, the overall workflow consists of two phases designed to achieve data quality control in the collaborative development of LLMs.

**Step One. Local Training for Data Quality Scoring** Local clients compute each sample's quality score via our training dynamics-based methods using the public validation set and their own fine-tuned model. The server determines a global threshold score, serving as a unified standard of data quality with only a very small amount of anchor data, and sends it to the clients.

**Step Two. Collaborative Learning with High-Quality Data** Each client then discards data samples that fall below the global threshold received, ensuring that only high-quality data verified by the unified standard are retained. The clients then utilize the high-quality filtered data sets \(^{}_{k}\) (where \(|^{}_{k}||_{k}|\)) and the initial global model \(^{0}\) for collaborative learning. After local fine-tuning with the selected high-quality curation data, clients send their local LoRA adapter to the server. The server then aggregates the LoRA parameters of the individual models.

### Step One: Training Dynamics-based Data Scoring

The idea behind our method is straightforward -- trace the training process to capture changes in prediction as individual training examples are visited.

For each client, we have designed a data scoring step to calculate the score for each training data sample to measure its contribution to model prediction. Specifically, considering the training set of examples \(_{k}=\{z_{1},,z_{K}\}\) and a model \(\), we represent the validation set as \(^{}_{k}=\{z^{}_{1},,z^{}_{K}\}\). We measure the performance of a model using a loss function \(:^{p} Z\). The loss of the model noted by \(\) on an example \(z\) is given by \((,z)\). We fine-tune the model by finding parameters \(\) that minimize the training loss \(_{i=1}^{K}(,z_{i})\), through an iterative optimization procedure, such as

Figure 3: Overall workflow diagram consists of two phases: 1) Step One: client-side computes each sample’s quality score with scoring functions using the public validation set and global model, then server-side calculates the score of a global threshold by anchor data 2) Step Two: clients filter data according to the global threshold and starts collaborative learning on selected high-quality data with adaptive weights on the model side.

Stochastic Gradient Descent (SGD) or its variant, which utilizes one training example \(z_{t}\) in iteration \(t\), updating the parameter from \(_{t}\) to \(_{t+1}\):

\[^{t+1}-^{t}=-_{t}(;^{ t})\] (1)

We trace the training process to capture changes in prediction as individual training examples are visited. The contribution of a particular training example \(z\) on a given test example \(z^{}\) is defined as the total reduction in loss on the test example \(z^{}\) that is induced by the training process whenever the training example \(z\) is utilized. We define the data quality of a particular training example \(z\) as the sum of the contribution of the whole validation dataset.

The simplified expression for data quality is as follows:

\[S(z)=_{z^{}}_{t=1}^{T}_{i}(},_{t})(,_{t})\] (2)

The per-sample gradients are calculated for each training sample from the checkpoint \(t\) saved during the model training. LLMs are generally tuned using AdamW, which has a more complicated update formula involving the moving averages of the gradient moments.

For Adam,

\[^{t+1}-^{t}=-_{t}(,^{t}),(,^{t}) ^{t+1}}{^{t+1}}+}\] (3)

For AdamW,

\[^{t+1}-^{t}=-_{t}(,^{t}),(,^{t}) ^{t+1}}{^{t+1}}+}+^{t}\] (4)

Therefore, the training data quality score for LLMs is calculated using the following formula:

\[S(z)=_{z^{}}_{t=1}^{T}_{i}(},_{t})(,_{t})\] (5)

The dot product of the loss gradients of the training example (\(z\)) and the test example (\(z^{}\)) is weighted by the learning rate (\(_{i}\)) at different checkpoints and summed up, where we implemented applying point-wise loss gradients to disentangle the relative contributions of each training example. We use the output of the checkpoints from the learning algorithm to capture the training process. The higher the score \(S(z)\), the higher the quality of the training sample \(z\). We demonstrates an optimized training approach for collaborative learning of multiple models. By selecting high-quality training data for each local model, we select gradients that positively impact loss trajectories. These trimmed gradients accumulate, leading to an improved position in the weight space. Considering interference during our data selection (gradient selection) of \(_{1}^{}\) and \(_{2}^{}\), we reduce the interference of weight updates from different models. After parameter aggregation, the merged model \(_{merged}\) can be improved to an enhanced position in the weight space represented by \(_{targeted}\).

It is particularly well-suited for parameter-efficient fine-tuning techniques such as Low-Rank Adaptation (LoRA) , which involves freezing the pre-trained model weights and injecting trainable rank decomposition matrices into linear projects of the Transformer architecture. A neural network contains many dense layers that perform matrix multiplication. In the self-attention module, we denote the query projection matrices as \(W_{q}\), the key projection matrices as \(W_{k}\), the value projection matrices as \(W_{v}\), the output project matrices as \(W_{o}\). In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module \((W_{q},W_{k},W_{v},W_{o})\). In our implementation, we apply LoRA only to \(W_{q}\) and \(W_{v}\) in most experiments for simplicity.

One straightforward solution is to calculate the quality scores on all weight parameters of LoRA, but may be computationally infeasible when larger models with several millions of parameters are used. To address the memory bottleneck of calculating and saving gradients, we take gradients with respect to a given layer. We propose operating on the first layer of the model, which contains the least cancelation effect, since the early layers encode _unique logit_. Therefore, we develop the idea of LoRA-based training-data influence in the context of gradient descent. Our proposed influence score is scalable due to the sparse nature of low-rank gradients and contains both low-level and high-level information since the gradient to the low-rank layer can capture both high-level and low-level information about the input sentence.

Note that the above gradient computation process is based on one single checkpoint and there is no parameter update throughout the process. Hence, for each training data point, we can perform this process in parallel, which can facilitate the computation.

### Step Two: Global Standard with Anchor Data Scoring

On the server, we use a small set of public data (10 samples in our paper) as our anchor data and calculate the average score of these 10 data points as the global threshold. This establishes a unified standard for division between low- and high-quality data for heterogeneous clients, allowing for the further filtering of local data.

Then we merge the parameters of individual models with adaptive weights on different models. For model merging techniques, we implemented _Task Arithmetic_ on task weights, the LoRA matrices are involved in weighted sum. In task arithmetic, one first computes the task weights which is the difference between fine-tuned and base model weights, then calculates a weighted sum of these task weights. Here, the delta weights considered are the individual matrices \(A\) and \(B\) instead of their product \(BA\). Consider two LoRA adapters \((A_{1},B_{1})\) and \((A_{2},B_{2})\) along with weights \(w_{1}\) and \(w_{2}\) for the weighted merging of these two adapters, then the merging happens as follows:

\[A_{}\ =_{1}}A_{1}+_{2}}A_{2}\] (6)

\[B_{}\ =_{1}}B_{1}+_{2}}B_{2}\] (7)

We also implement a more efficient method for merging LoRA adapters by eliminating redundant parameters: _TrIm, Elect, and Merge (TIES) _. First, redundant parameters are trimmed, then conflicting signs are resolved into an aggregated vector, and finally, the parameters whose signs are the same as the aggregate sign are averaged. This method takes into account that some values (redundant and sign disagreement) can degrade performance in the merged model.

## 4 Experiments

Unlike traditional data quality selection methods for pre-trained models or traditional fine-tuning, in our collaborative setting, the training data from vertical domains is very sensitive and subject to strict restrictions regarding sharing and privacy. Therefore, we propose a new experimental setting using medical domain data for downstream tasks and evaluation for open-ended medical QA tasks, considering both quality heterogeneity and domain heterogeneity.

### Experimental Setup

Tasks and DatasetsWe conduct our evaluation on the open-ended question-answering (QA) tasks.

(1) Medical QA: PMC-LLama  and Medalpaca-flashcards  cover medical question-answering, rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens. We use 16k samples in total, with 8k samples randomly sampled from PMC-LLama and Medalpaca-flashcards each. We uniformly partition the total samples into 20 clients in this task to demonstrate the effectiveness of CLUES in terms of the scalability of the clients, where the clients are IID subsets of the original distribution. For low-quality data, 3.2k samples (40% total data) are polluted with cutting, deletion, or substitution. These 40% low-quality data, together with the rest of the high-quality data, composites the mix-quality data set.

(2) Multilingual QA:MMedBench  is a medical muti-choice dataset of 6 different languages. It contains 45k samples for the trainset and 8,518 samples for the testset. Each question is accompanied by a right answer and high-quality rationale. We use 6312 samples randomly sampled from MMedBench and 1052 samples per language for each of the 6 clients. For the low-quality data, a certain ratio is either polluted with random noise.

**(3) Financial QA:** To demonstrate the generalizability of our proposed method across various domains, we also include FiQA , part of the training corpus of FinGPT , which consists 17.1k financial open Question-Answering instructions. We randomly sample 2000 data samples for each of the 4 clients from FiQA dataset, and pollute each of them with a low-quality data ratio of 80%, 20%, 10%, and 50% respectively.

Note that for all tasks, the anchor data and validation dataset used in our proposed method are selected as a held-out high-quality dataset from the same data source.

ModelsWe use LLama2-7b  and Mistral-7b  as our pre-trained models, and fine-tune them with Low-Rank Adaptation (LoRA)  on each of the client side. As for the model merging technique, in our main experiments, we use TIES merging. We also compare it with Task Arithmetic in our ablation studies.

BaselinesThe _Oracle_ shows the results that train only on the remaining high quality data in the mixed-quality dataset, serving as the theoretical upper bound. We implement the three baselines: existing methods mentioned in 2.1: Perplexity score, IFD , and DataInf  independently on each client.

Evaluation metricsThe evaluations focus on two main aspects: **(1) Question-Answering capabilities**, assessed by GPT-4  scoring within the test set splited from the same sources of the training dataset. In the medical QA and multilingual QA tasks, 200 samples are randomly selected from the medical dataset to serve as the test set. We evaluate the models that need to be compared on the test set to generate responses respectively. Then we use the OpenAI GPT-4 model API to assign scores to their responses. Each response of is rated by the judge on a scale from 0 to 1, reflecting how well the answer aligns with the ground truth. In our financial QA task, GPT-4 rate the responses of the fine-tuned model on our data set on a scale of 1 to 10, reflecting criteria including relevance, precision and fluency. To address potential positional bias, we send our response along with the benchmark output to GPT-4 twice, with different orders. We then calculate the average of these scores as the final performance score. **(2) Knowledge acquisition**, measured by average accuracy of responses to multiple-choice questions in the MMLU clinical topics [8; 9], MedMCQA , PubMedQA , and USMLE  datasets. Although the goal of private domain fine-tuning is not to increase knowledge, there shouldn't be too much knowledge forgetting during this process. **(3) Data selection correctness** Precision, Recall, F-1 Score, and Accuracy are widely-used evaluation metrics that provide complementary insights into the model's effectiveness from the data selection perspective. In our case, positive instances represent high-quality data, while negative instances represent low-quality data. Precision quantifies the proportion of correctly identified positive instances among all instances predicted as positive, while Recall measures the proportion of correctly identified positive instances among all actual positive instances in the dataset. The F1 Score offers a balanced measure of Precision and Recall, while Accuracy reflects the overall correctness of our data selection (based on our data scoring and threshold determining method) across all classes.

### Main Results

Based on the low-quality dataset setup, we evaluate our data-quality control pipeline in collaborative LLM fine-tuning in both federated (communication round \(cr=300\)) and model merging (communication round \(cr=1\)) settings. Note that in federated learning, the server and clients need to intensively communicate the model updates during model training. We implement the three baseline methods described in the Section 2.1 to calculate scores for each training data, and set the unified scoring standard using corresponding scoring functions with anchor data.

We demonstrate the performance of data quality control methods in collaborative settings in the medical QA task (Tab. 1) and Multilingual QA task (Tab. 2).

Federated Learning v.s. Model MergingFirstly, for both pre-trained models and tasks, with other settings remaining the same, model merging performs better than federated learning. This indicates that loose communication between the local model and the server, compared to frequent communication, might lead to better generalization. Additionally, the performance boost with selected data in the federated setting is larger than in the model merging setting. This might be because during federated learning, we calculate the data score based on the global model (instead of the local modelin the model merging setting) at each timestamp, which can better trace and regularize the training trajectory to the optimal location.

Data Selection PerformanceIn both federated and model merging settings, our data selection can achieve over 96% and over 91% of the theoretical upper bound performance, respectively. Our method outperforms the other local data selection baselines under the GPT4 Scoring metrics. Compared to the other methods which cause severe forgetting during instruction tuning, the performance of our method on the Knowledge-based benchmark remains within an acceptable range. This shows that our methods are able to improve domain-specific tasks without forgetting knowledge injected during pretraining.

## 5 Analysis

### Qualitative Analysis

We performed a qualitative analysis by manually comparing the outputs generated by models fine-tuned on our selected high-quality data versus the original low-quality data. This comparison (Tab. 7 and Tab. 8) provides insights into the tangible improvements in model performance and output quality.

### Varying Levels of Low-Quality Data

To evaluate the robustness of our data selection method under different data quality conditions, we conducted a series of experiments with varying proportions of low-quality data. We maintained a consistent proportion of low-quality data across all clients for each experiment, ranging from 0% to 100%, including pollution levels 20%, 50%, and 80%.

Fig. 4 presents the performance of models trained with and without our data selection method across these different proportions. The results demonstrate that our method effectively enhances data quality across all scenarios with GPT-4 scoring. And in terms of accuracy of the data selection, our method consistently selected over 99% of the high-quality data across different proportions of low-quality data. Additionally, to understand the adaptability of our global threshold, we analyzed how the global threshold changes with different proportions of low-quality data. Fig. 4 illustrates that our global threshold adjusts across varying levels of data quality.

    &  &  \\  Evaluation Metric & GPT-4 Scoring & Knowledge Avg & GPT-4 Scoring & Knowledge Avg \\  Mix-qual Data & 0.085 & 0.194 & 0.0952 & 0.311 \\ Oracle & 0.160 & 0.233 & 0.099 & **0.440** \\ PPL & 0.079 & **0.346** & 0.045 & 0.362 \\ IFD  & 0.087 & 0.287 & 0.050 & 0.346 \\ DataInf  & 0.093 & 0.106 & 0.103 & 0.335 \\ CLUES **(ours)** & **0.161** (100.6\%) & 0.309 (132\%) & **0.210** (212.1\%) & 0.356 (80.9\%) \\   

Table 1: Data selection performance in federated setting on MedicalQA. We **bold** the highest performance and underline the second highest performance for each row.

    &  &  \\  Setting & Federated & Model Merging & Federated & Model Merging \\  Mix-qual Data & 0.420 & 0.515 & 0.440 & 0.485 \\ Oracle & **0.451** & **0.530** & 0.449 & **0.490** \\ CLUES **(ours)** & 0.435 (96.5\%) & 0.525 (99.1\%) & **0.477**(106.2\%) & 0.487 (99.4\%) \\   

Table 2: Data selection performance on MMedBench. We **bold** the highest performance and underline the second highest performance for each row.

### Quality Heterogeneity

To provide a more comprehensive analysis, in addition to the experiments in the _domain heterogeneity_ setting shown above, we conducted additional experiments in a _quality heterogeneity_ setting using the FiQA dataset, which focuses on the answer of financial questions. Specifically, we randomly polluted 80%, 20%, 10%, and 50% of the training set for each of the four clients, respectively. The findings demonstrate that our method significantly enhances the quality of the data even when clients have different proportions of low-quality data.

Varying Merging TechniquesFig. 5 demonstrates that different weighted merging or aggregation techniques lead to varying performance. Notably, the performance of our data selection method with the _Linear Merging_ technique does not even reach the performance of low-quality data with _TIES Merging_ technique, highlighting the significant impact of weighted merging techniques on overall performance. Furthermore, we experimented with different merging techniques on the FiQA dataset, demonstrating the importance of weighted merging, shown in Fig. 5.

Layer Selection for Low-Rank Tracing GradientIn terms of layer selection, we evaluated both the last layer and the _token embeddings_. We show that layer selection distorts the score (the inner product of two gradients). In our ablation study, we observe that since the activation connected to the last layer of weights contains _shared logic_, the data influenced calculated through the last layer weights are prone to a _cancellation effect_, where the data influence of different examples has a large magnitude that contradicts each other. The cancelation effect lowers the power of the influence score, and deleting influential examples according to this measure often does not change the model's behavior by much. From Fig. 5, we show that the first layer has a less severe cancelation effect than the last layer.

Unified Scoring with Anchor DataWe conducted an ablation study on our global threshold to further validate our approach. Tab. 3 illustrates the advantage of using a global threshold determined by our anchor data for data selection in this heterogeneous setting, compared to selection based on average ratio or pre-determined scores. These results demonstrate that our approach successfully balances the identification of positive cases with the minimization of false positives, offering a robust and superior solution.

Figure 4: Experimental results for different levels of low-quality data

Figure 5: Left: Comparison of different merging techniques. Right: First layer v.s. last layer for low-rank tracing gradient.

## 6 Discussion and Conclusion

Collaborative model development, including model merging and federated averaging, would benefit from different kinds of high-quality data, and for each of them, the definition of quality is slightly different. In this paper, we establish a data quality control pipeline for collaborative fine-tuning of LLMs, avoiding directly sharing any private data. Our experiments show that the selected high-quality data ensures an effective and reliable learning process, leading to improved model performance.

To the best of our knowledge, we are the first to propose a data selection method for large language models in a collaborative setting, while previous work has mainly focused on traditional centralized settings. We bring up the insights to view federated learning and model merging within the same framework, incorporate different experimental setups and unify federated learning and model merging methods, making it universally applicable. Additionally, our method performs well on generation datasets and takes into account scenarios with bad data, while previous work has not considered downstream domain-specific generation tasks for large language models. Our method does not require repeated training.

Societal impactOur work builds large language models that make it possible to create a collaborative instead of a monolithic ecosystem from open-source models while preserving the privacy of users' own data. The constant progress being made in machine learning needs to extend across borders if we are to democratize ML in developing countries. Adapting state-of-the-art (SOTA) methods to resource-constrained environments such as developing countries can be challenging in practice, pushing open source and inclusion.

Limitations and future workOur data quality control methods are based on the assumption that all the local models share the same model architectures. It is easy to achieve when our fine-tuning is based on the LoRA adapter. However, it may be worth extending it to adapt to different local model architectures, for example, different low ranks. Future work may explore the intrinsic relation between data selection and the model parameters and how our data selection methods can help reduce the interference of parameter vectors from different models.