# Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks

Haoyi Duan\({}^{1}\)1  Yan Xia\({}^{1}\)1  Mingze Zhou\({}^{1}\)  Li Tang\({}^{1}\)  Jieming Zhu\({}^{3}\)  Zhou Zhao\({}^{1,2}\)2

\({}^{1}\) Zhejiang University \({}^{2}\)Shanghai Artificial Intelligence Laboratory \({}^{3}\)Huawei Noah's Ark Lab

{haoyiduan075, xiayan.zju}@gmail.com

{3200102572, tanglzju, zhaozhou}@zju.edu.cn jiemingzhu@ieee.org

Equal contribution.Corresponding author.

###### Abstract

In recent years, the deployment of large-scale pre-trained models in audio-visual downstream tasks has yielded remarkable outcomes. However, these models, primarily trained on single-modality unconstrained datasets, still encounter challenges in feature extraction for multi-modal tasks, leading to suboptimal performance. This limitation arises due to the introduction of irrelevant modality-specific information during encoding, which adversely affects the performance of downstream tasks. To address this challenge, this paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention mechanism. This mechanism leverages audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained models based on the current multi-modal input features. Specifically, the DG-SCT module incorporates trainable cross-modal interaction layers into pre-trained audio-visual encoders, allowing adaptive extraction of crucial information from the current modality across spatial, channel, and temporal dimensions, while preserving the frozen parameters of large-scale pre-trained models. Experimental evaluations demonstrate that our proposed model achieves state-of-the-art results across multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our model exhibits promising performance in challenging few-shot and zero-shot scenarios. The source code and pre-trained models are available at https://github.com/haoyi-duan/DG-SCT.

## 1 Introduction

With the increasing availability of hardware resources, large-scale models  pre-trained on extensive data have achieved significant advancements in various multi-modal tasks . Nonetheless, since these models are primarily pre-trained on single modality, they may not be optimally suited for current multi-modal downstream tasks . As depicted in Fig 1 (a), the pre-trained model equally extracts visual features and directly passes them to downstream tasks. However, when perceiving the roaring sound of an engine, the visual region depicting a "car" should receive more attention than the region of "trees". Simultaneously, when observing the car, it is crucial to concentrate on the audio segments of the engine sound. Therefore, the encoder should not only equally extract modal-specific information from the current modality, but also highlight information related to other modalities to enhance feature fusion across diverse modalities in downstream tasks. Retraining these large models based on downstream tasks would impose an unaffordable burden , leading recent works to explore methods for fine-tuning pre-trained models on downstream tasks without full retraining, showing promising progress. However, these CLIP-based methods have primarily focused on text-image tasks , while overlooking another importantmulti-modal scenario: audio-visual tasks. Hence, in this paper, we primarily investigate how to utilize existing large-scale models, such as CLIP  and Swin-Transformer , to adaptively adjust the encoding features with the guidance of counterpart modality when encoding audio or visual information.

The success of prompt learning in large language models (LLMs) [13; 18; 19; 30; 21] has recently sparked growing research interest in multi-modal prompt learning, as seen in works such as CoOp , CoCoOp , CLIP-Adapter , DenseCLIP , and MaPLe . While these approaches enable adaptive adjustment of input features using text-based prompt templates for downstream tasks, an important question arises: _Can audio or video serve as innovative prompt templates to enhance task comprehension for pre-trained models and guide adaptive feature extraction of the counterpart modality?_ Our findings suggest a positive answer to this question.

In this paper, we present the **Dual-Guided Spatial-Channel-Temporal** (DG-SCT) attention mechanism, designed to adaptively adjust feature extraction of pre-trained models based on audio-visual input. Our work is motivated by a recent work, LAVisH , which introduces trainable layers with shared parameters in pre-trained models to enhance fusion of audio-visual features, demonstrating promising performance on various audio-visual downstream tasks with minimal additional parameters. However, LAVisH has a few limitations. First, it relies solely on a visual encoder to encode audio, which we argue is insufficient for capturing key audio features [10; 23]. Second, it only employs cross-attention in trainable layers to introduce information from different modalities, without explicitly highlighting crucial information within the current modality. By contrast, our approach incorporates cross-modal interaction layers into audio (HTS-AT ) and visual (ViT  or Swin-T ) pre-trained models, leveraging different modalities as prompts to focus on special aspects of the input features that are more relevant to the counterpart modal semantics across **spatial**, **channel**, and **temporal** dimensions. We term our proposed approach as "prompts" to denote the guidance provided to the trainable weights in preceding layers. It emphasizes utilizing audio and video cues to guide the representation of the counterpart modalities.

As depicted in Fig. 1, unlike previous audio and visual encoders, which generate audio and visual features separately and uniformly (Fig. 1 (a)), our features contain fine-grained, task-specific information at multiple levels by leveraging the guiding characteristics of multi-modal information [37; 36]. This enables efficient implementation of a wide range of downstream tasks. Notably, unlike previous CLIP works [46; 45] that offer unidirectional prompts, our approach introduces bidirectional prompts. This means that visual and audio modalities can mutually guide each other, facilitating enhanced feature extraction from the respective modalities.

In summary, this paper makes the following contributions:

* We highlight the limitations faced by large-scale pre-trained models in audio-visual downstream tasks, which hinder their optimal performance. To overcome these, we propose to utilize audio-visual features as novel prompts to fully leverage the feature extraction capa

Figure 1: Our **Spatial** and **Temporal** attention can focus on important regions and moments in video and emphasize critical timestamps and frequencies in audio; **Channel** attention enhances the representations of audio and visual features. Take visual modality, our visual features contain fine-grained, task-specific information under the guidance of audio prompts.

bilities of large-scale models, enabling the effective utilization of task-specific information from different modalities.
* We introduce a novel attention mechanism named Dual-Guided Spatial-Channel-Temporal (DG-SCT), which utilizes audio and visual modalities to guide the feature extraction of their respective counterpart modalities across spatial, channel, and temporal dimensions. Notably, our approach adds only a limited number of parameters for the interaction layer, while keeping the original parameters of the large-scale pre-trained models frozen.
* Extensive experimental results on four audio-visual tasks, namely, AVE, AVVP, AVQA, and AVS, demonstrate the superiority of our model compared to state-of-the-art counterparts across various settings. Furthermore, we evaluate the performance of DG-SCT in few-shot and zero-shot scenarios on the AVE and LLP datasets, demonstrating its superiority over CLIP and several competitive CLIP-Adapters.

## 2 Related work

### Audio-visual understanding

Audio-visual understanding tasks involve utilizing both audio and visual modalities to get a better perception of audio-visual scenarios [8; 22; 41; 31]. For instance, **Audio-Visual Event Localization (AVE )** requires models to recognize joint audio-visual events. Previous works [33; 16; 35; 37; 36] use late interaction strategies to better leverage the visual and audio features encoded from modality-specific pre-trained models. **Audio-Visual Video Parsing (AVVP )** task breaks the restriction that audio and visual signals are definitely aligned. To tackle the weakly-supervised AVVP task, previous work  proposes a hybrid attention network and attentive Multimodal Multiple Instance Learning (MMIL) Pooling mechanism to aggregate all features. The task of **Audio-Visual Segmentation (AVS )** focuses on whether each pixel corresponds to the given audio so that a mask of the sounding object(s) is generated. Zhou et al.  use a temporal pixel-wise audio-visual interaction module to inject audio semantics as guidance for the visual segmentation process. Furthermore, the newly introduced **Audio-Visual Question Answering (AVQA )** task requires methods that perceive both audio and visual modalities to answer human-generated questions about the audio-visual content. Li et al. propose a spatiotemporal grounding model  to achieve scene understanding and reasoning over audio and visual modalities.

However, most methods designed for these tasks rely on modality-specific audio and visual pre-trained models, which can not utilize multi-modal cues early in the representation stage. In this paper, we propose a novel early-interaction strategy, adaptively extracting key information from the current modality across spatial-channel-temporal dimensions.

Figure 2: 1) Audio-visual inputs; 2) DG-SCT is injected into every layer of frozen pre-trained audio and visual encoders; 3) After feature extraction, the audio and visual features are sent to various downstream tasks; 4) Details of DG-SCT in spatial-channel-temporal attention levels.

### Vision-language models and prompt learning

**Vision-language models** have made remarkable progress since the introduction of CLIP , with zero-shot and few-shot ideas achieving excellent generalization abilities in many downstream tasks; Meanwhile, **prompt**, a concept in NLP [15; 13; 19], has achieved impressive results in various NLP domains since its introduction, as evidenced by the success of the GPT series [25; 1]. Subsequent works have attempted to combine these two and achieved better results. For example, CoOp  improves the CLIP model by optimizing the continuous prompts in the language branch, and CoCoOp  further improves the model by incorporating prompts in the video branch. However, these works only utilize prompts to guide individual branches. CLIP-adapter  builds on these works by proposing to use embedding of video and language to guide each other at the end of the encoder. MaPLe  is the first to use an adaptor to guide each other inside the encoder, integrating visual and text representations with the semantics of each other to enhance the generalization ability of the model. However, none of these works consider utilizing prompts in the audio-visual domain.

In this paper, we introduce a novel bidirectional prompt that employs audio and video cues independent of text to achieve outstanding information extraction abilities for audio-visual tasks.

## 3 Approach

In this section, more details about our proposed DG-SCT are elaborated. An overview of the proposed framework is illustrated in Fig. 2.

### Representations for audio-visual modalities

**Visual representation** Given an input video sequence, we first sample a fixed number of RGB video frames \(\{V_{t}\}_{t=1}^{T}^{T H W 3}\), where H, W are height and width. Following the Swin-T , we first split each RGB frame \(V_{t}\) into non-overlapping patches by a patch-splitting module with kernel size \((P_{v} P_{v})\). Each patch is treated as a "token" and its feature is set as a concatenation of the raw pixel values. A linear embedding layer is then applied to this raw-valued feature and we can get visual features as \(v_{t}^{}} C_{v}}\), where \(C_{v}\) is the number of visual channels.

**Audio representation** Given an input audio track, we first get an audio mel-spectrogram \(\{A_{t}\}_{t=1}^{T}\), where \(A_{t}^{L F}\) with time \(L\) and frequency bins \(F\). Following the HTS-AT ,3 the audio mel-spectrogram is cut into different patch tokens with a Patch-Embed CNN of kernel size \((P_{a} P_{a})\). A linear embedding layer is then applied to this raw-valued feature and we can obtain audio features as \(a_{t}^{}} C_{a}}\), where \(C_{a}\) is the number of audio channels.

### Adding DG-SCT modules to frozen encoders

Now, we describe how we adjust pre-trained Swin-T and HTS-AT with our proposed DG-SCT. Every layer of the Swin-T and HTS-AT consists of three main operations: 1) multi-head attention (MHA), 2) multi-layer perceptron (MLP), and 3) our DG-SCT modules which use the intermediate layer information of audio and video as prompts to guide each other through spatial-channel-temporal dimensions. We skip the linear normalization layers in both MHA and MLP operations for brevity.

Given audio inputs \(a^{()}^{T(L^{()}.F^{()}) C_{a}^{()}}\) and visual inputs \(v^{()}^{T(H^{()}.W^{()}) C_{v}^{()}}\) from layer \(\), we first use a two-dimensional convolution kernel and a linear projection to make the dimensions of the audio and visual prompts consistent of their counterpart modality. Let \(v^{()}_{f}=^{a2v}(a^{()},v^{()})\) and \(a^{()}_{f}=^{v2a}(v^{()},a^{()})\) denote the operation that implements DG-SCT module, which we will describe in the next subsection. Then, the operations in each layer can be written as:\[v_{y}^{()} =^{()}+(^{()})+^{2 }(a^{()},v^{()}), a_{y}^{()}=a^{()}+(a ^{()})+^{}(^{()},a^{()}),\] (1) \[^{(+1)} =_{y}^{()}+(_{y}^{()})+ ^{2}(a_{y}^{()},_{y}^{()}), a^{(+1 )}=a_{y}^{()}+(a_{y}^{()})+^{}(_{y}^{()},a_{y}^{()}).\] (2)

### Dual-guided spatial-channel-temporal attention

In this subsection, we will describe how DG-SCT works in more detail. Given visual and audio features, the encoder such as Swin-Transformer pre-trained on large-scale single-modal data, will uniformly extract features from audio-visual inputs (See Fig. 1 (a)). However, in practical multi-modal scenarios, not all of this information carries equal importance. For example, as we see in Fig. 1, the region where the car appears in the visual field is evidently more crucial than the background trees. Additionally, the moment when the engine sound emerges in the audio should also be given more attention. Hence, we take advantage of the fact that audio-visual pairs can provide mutual guidance for each other, and utilize different modalities as prompts to help pre-trained models focus more on specific aspects of opposite modal inputs across spatial, channel, and temporal dimensions. Different from previous works [37; 36] which only leverage audio as guidance to extract visual features, our proposed DG-SCT module can achieve triple levels of information highlighting in two directions. We illustrate these cross-modal attention mechanisms in the following parts:

**Channel-wise attention:** Different channels represent different aspects of features. The introduction of channel attention can facilitate the model to ignore irrelevant features and improve the quality of representations . We let the audio and video serve as mutual guidance signals and explicitly model the dependencies between channels on each other's modality. Concretely, We use \(_{a}\) and \(_{v}\) to denote the combinations of convolutional and linear projection in section 3.2, to encode audio and visual inputs as prompts: \(a_{t}^{{}^{}}=_{a}(a_{t})^{C_{v}(H W)}\) and \(v_{t}^{{}^{}}=_{v}(v_{t})^{C_{a}(L F)}\), respectively. For audio-to-visual (A2V), we use the spatial average pooling to process \(a_{t}^{{}^{}}\) and get \(a_{t}^{{}^{}}^{C_{v} 1}\), then fuse it with vision via element-wise multiplication after feeding them to projection layers \(_{a}^{c},_{v}^{c}^{C_{v} C_{v}}\) respectively, generating audio channel-guidance maps \(a_{t}^{cm}=(_{a}^{c}(a_{t}^{{}^{}})_{v}^{c}(v_{t} ))^{C_{v}(H W)}\). After that, we spatially squeeze the fused feature by global average pooling, denoted as \(_{a}\), Finally, a bottleneck layer \(_{a}\) follows with a sigmoid function \(\) is used, yielding channel attention maps \(M_{t}^{vc}\); Similarly, we generate V2A channel attentive maps \(M_{t}^{ac}\):

\[M_{t}^{vc}=(_{a}(_{a}(a_{t}^{cm})))^{C_{v} 1 }, M_{t}^{ac}=(_{v}(_{v}(v_{t}^{cm})))^{C_{a}  1},\] (3)

where \(v_{t}^{cm}^{C_{a}(L F)}\) is the visual channel-guidance maps, \(_{v}\) is spatial-wise global average pooling, \(_{v}\) indicates a bottleneck layer and \(\) denotes the sigmoid function.

**Spatial-wise attention:** Audio can improve visual feature extraction by contributing to visual attention in the spatial dimension . Inspired by this, we leverage the guidance capabilities of audio and visual prompts to guide visual spatial attention and audio frequency attention, respectively. Similar to the aforementioned channel-wise attention, For A2V, we first get channel-attentive visual features \(v_{t}^{c}=(M_{t}^{vc}+1) v_{t}\), then we element-wise multiply audio prompt and \(v_{t}^{c}\) after the projection of \(_{a}^{s}\) and \(_{v}^{s}\) to hidden dimension \(d=256\), generating audio spatial-guidance maps \(a_{t}^{sm}=(_{a}^{s}(a_{t}^{})_{v}^{s}(v_{t}^{c})) ^{d(H W)}\). Then we use a projection layer \(^{s}^{1 d}\) with a sigmoid function \(\) to obtain spatial attention maps \(M_{t}^{vs}\); Similarly, we generate V2A frequency attentive maps \(M_{t}^{af}\):

\[M_{t}^{vs}=(^{s}(a_{t}^{sm}))^{1(H W)},  M_{t}^{af}=(^{f}(v_{t}^{fm}))^{1(L F )},\] (4)

where \(v_{t}^{fm}\) denotes visual frequency-guidance maps, \(^{f}^{1 d}\) is a projection layer.

**Temporal-gated attention:** Given an audio, significant time segments (e.g., "engine sound") should be emphasized, while background information (e.g., "silence") should be attenuated. The same holds for the visual information as well . Inspired by this, we add temporal-gated attention in the final layer. For A2V, we first feed the frequency-channel attentive audio features \(\{a_{t}^{cf}\}_{t=1}^{T}\) through an RNN to capture temporal information better and then pass it through a projection layer with sigmoid function to obtain the temporal attention gates \(G^{v}^{T 1}\); Similarly, for V2A, we feed the spatial-channel attentive visual features to generate \(G^{a}^{T 1}\):

\[G^{v}=(_{a}^{t}((\{a_{t}^{cf}\}_{t=1}^{T}))), G^{a}= (_{v}^{t}((\{v_{t}^{cs}\}_{t=1}^{T}))).\] (5)

**Summary:** We combine the abovementioned three attention mechanisms together to form our DG-SCT module. Given audio features \(a_{t}^{C_{a}(L F)}\) and visual features \(v_{t}^{C_{v}(H W)}\), DG-SCT first generates channel-wise attention maps \(M_{t}^{vc}\) and \(M_{t}^{ac}\) to let audio and video adaptively emphasize informative features of the corresponding modality. It then lets audio pay attention to the important sounding regions to produce spatial-wise attention maps \(M_{t}^{vs}\) and lets video pay attention to the important frequency regions to generate frequency-wise attention maps \(M_{t}^{cf}\), thus the yielding of the spatial-channel attentive visual features \(v_{t}^{es}\) and the frequency-channel attentive audio features \(a_{t}^{cf}\) can be summarized as:

\[v_{t}^{cs}=( M_{t}^{vc}+ M_{t}^{vs}+1) v_{t}, a _{t}^{cf}=( M_{t}^{ac}+ M_{t}^{af}+1) a_{t},\] (6)

Then we generate two temporal attention gates \(G^{v}^{T 1}\) and \(G^{a}^{T 1}\) for \(\{v_{t}^{es}\}_{t=1}^{T}\) and \(\{a_{t}^{cf}\}_{t=1}^{T}\), respectively, thus the final outputs of \(^{2}\) and \(^{22}\) mentioned in section 3.2 are:

\[\{v_{t}^{est}\}_{t=1}^{T}=( G^{v}+1)\{v_{t}^{es}\}_{t=1}^{T}, \ \ \{a_{t}^{cft}\}_{t=1}^{T}=( G^{a}+1)\{a_{t}^{cf}\}_{t=1}^{T},\] (7)

Where \(\), \(\), and \(\) are hyperparameters. Consequently, this approach yields high-quality, fine-grained audio-visual representations, significantly improving the performance of subsequent tasks in the audio-visual domain.

## 4 Experiments

### Tasks and datasets

**Audio-visual event localization (AVE)** focuses on recognizing an audio-visual event that is both visible and audible throughout multiple time segments in a video. We evaluate the AVE  dataset; **Audio-visual video parsing (AVVP)** aims to parse a video into temporal event segments and label them as either audible, visible, or both. We evaluate our method for weakly-supervised AVVP task on the LLP dataset ; The goal of **Audio-visual segmentation (AVS)** is to output a pixel-level map of the object(s) that produce sound at the image frame. We evaluate on AVSBench ; **Audio-visual question answering (AVQA)** aims to answer questions based on the associations between objects and sounds. We conduct our experiments on the MUSIC-AVQA dataset .

Meanwhile, We propose **Audio-visual few-shot/zero-shot** tasks on AVE  and LLP  datasets. We evaluate AVE and classification tasks on AVE dataset and classification task on LLP dataset. More details about tasks and datasets will be illustrated in Appendix.

### Implementation details

**Audio-visual downstream tasks**: To adapt our approach to the four audio-visual downstream tasks, we replace the pre-trained audio and visual encoders with a frozen HTS-AT  and a frozen Swin-Transformer , respectively. The trainable DG-SCT modules in Fig. 2 (4) are injected into the frozen layers to let audio and visual modalities guide each other. We then use this as our audio-visual feature extractor. For **AVE** task, our feature extractor is combined with CMBS . The event category label of each video segments is required to be predicted in supervised manner. We adopt [33; 38; 37; 36] and exploit the overall accuracy of the predicted event category as the evaluation metrics; Combined with MGN , DG-SCT is able to tackle the weakly-supervised **AVVP** task. Following , we evaluate the parsing performance of all events (audio, visual, and audio-visual events) under segment-level and event-level metrics; For **AVS** task, We combine our audio-visual feature extractor with the original AVS model . We use the Jaccard index \(\) and F-scoreas the evaluation metrics; For **AVQA** task, our audio-visual feature extractor is used in the original ST-AVQA . Similarly, we use the answer prediction accuracy as the evaluation metrics.

**Audio-visual few-shot/zero-shot tasks**: We incorporate DG-SCT modules as adapters between the frozen CLIP image encoder ViT  and frozen CLAP audio encoder HTS-AT , generating audio and visual features for text-audio and text-image contrastive learning, as shown in Fig. 2 (e). For the zero-shot setting, the model is pre-trained on the VGG-Sound(40K)  dataset. More details will be discussed in Appendix.

### Compared with state-of-the-arts on audio-visual downstream tasks

First, we challenge our method against current state-of-the-art methods on the four audio-visual tasks. As demonstrated in Table 1, our model outperforms CMBS  and LAVisH  by a significant margin (**2.9%** and **2.5%**); In Table 2, our model attains either a competitive or even better performance. For instance, DG-SCT surpasses MGN by **3.9%** points in the segment-level visual event parsing metric, demonstrating our method can utilize large-scale models to extract more useful features and further promote the fusion of these features in downstream tasks. We also achieve state-of-the-art results on S4 setting of AVS task (Table 3). Lastly, our model performs exceptionally well on AVQA task and outperforms previous leading methods on AQ, VQ, and AVQ question types, respectively. The experimental results reveal that our model has the capability of utilizing pre-trained audio and visual models to extract more comprehensive, higher-quality features tailored for downstream tasks than the cross-attention mechanism of LAVisH. Moreover, our model exhibits excellent generalization abilities, achieving impressive results across various audio-visual tasks.

### Compared with state-of-the-arts on audio-visual few-shot/zero-shot tasks

Furthermore, as presented in Fig. 4, our DG-SCT model surpasses top-performing methods like CoCoOp, CLIP-Adapter, and MaPLe by a non-negligible margin on our newly proposed audio-visual few-shot/zero-shot scenarios. Our few-shot (shot=\(16\)) learning for the AVE task attains **72.4%**,

  
**Method** & **Visual Encoder** & **Audio Encoder** & **Acc** \\  AVEL(Audio-Visual)  & VGG-19 & VGG-like & 71.4 \\ AVEL(Audio-Visual+Att)  & VGG-19 & VGG-like & 72.7 \\ AVSDN  & VGG-19 & VGG-like & 72.6 \\ CMAN  & VGG-19 & VGG-like & 73.3 \\ DAM  & VGG-19 & VGG-like & 74.5 \\ CMRAN  & VGG-19 & VGG-like & 77.4 \\ PSP  & VGG-19 & VGG-like & 77.8 \\ CMBS  & VGG-19 & VGG-like & 79.3 \\ LAVisH  & Swin-V2-L (shared) & 81.1 \\ LAVisH  & Swin-V2-L (shared) & 79.7 \\ LAVisH*4 & Swin-V2-L & HTS-AT & 78.6 \\ 
**Ours** & Swin-V2-L & HTS-AT & **82.2** \\   

Table 1: **Audio-visual event localization. Comparisons on the test set of AVE in supervised manner. The result of our re-implementation of LAVisH  is \(79.7\%\).**

    &  &  \\   & A & V & AV & Type & Event & A & V & AV & Type & Event \\  AVE  & 49.9 & 37.3 & 37.0 & 41.4 & 43.6 & 43.6 & 32.4 & 32.6 & 36.2 & 37.4 \\ AVSDN  & 47.8 & 52.0 & 37.1 & 45.7 & 50.8 & 34.1 & 46.3 & 26.5 & 35.6 & 37.7 \\ HAN  & 60.1 & 52.9 & 48.9 & 54.0 & 55.4 & **51.3** & 48.9 & 43.0 & 47.7 & 48.0 \\ MGN  & **60.7** & 55.5 & 50.6 & 55.6 & **57.2** & 51.0 & 52.4 & 44.4 & 49.3 & **49.2** \\ 
**Ours** & 59.0 & **59.4** & **52.8** & **57.1** & 57.0 & 49.2 & **56.1** & **46.1** & **50.5** & 49.1 \\   

Table 2: **Audio-visual video parsing. Comparisons on the test set of LLP.**

[MISSING_PAGE_FAIL:8]

### Qualitative analysis

Fig. 3 represents examples of the effectiveness of our DG-SCT. On the left, we observe that with the guidance of the audio prompts, the module can accurately focus on the critical visual regions for different AVE events. For instance, when multiple objects are present in video frames, our model is capable of accurately localizing the sounding objects (e.g., bark, cat, the second example of flute, the fifth example of speak). Moreover, our model can precisely pinpoint the sound source location at a fine-grained level, such as the strings of a violin and the hands of the performer, and the speaker's mouth. In addition, DG-SCT achieves excellent results on AVS task. The right part of Fig. 3 indicates that our model can accurately segment the pixels of sounding objects and outline their shapes perfectly. The excellent qualitative results of our model on various downstream tasks illustrate its strong potential for generalization.

As depicted in Fig. 5, we employ t-SNE  to visualize the learned audio and visual features and compare features generated by our model with features generated by baseline without DG-SCT, on various tasks. Each spot denotes the feature of one audio or visual event, and each color corresponds to a particular category, such as "cat" in orange as shown in Fig. 5 (AVE). As we can see, features extracted by the proposed DG-SCT are more intra-class compact and more inter-class separable. These meaningful visualizations further demonstrate that the DG-SCT model successfully learns compact and discriminative features for each modality across diverse downstream tasks.

### Efficiency analysis

Our efficiency analysis on AVE task is presented in Table 6. Our approach utilizes more trainable parameters. However, our proposed DG-SCT attention mechanism requires a comparable number of parameters to the latent tokens utilized in LAVisH . The increase trainable parameters primarily arises from the inclusion of a two-dimensional convolution kernel and a linear projection (section 3.2). These additions ensure consistency of dimensions between the audio and visual prompts. In other words, the increase in parameter count from our approach mainly results from addressing the inconsistency in dimensions between the audio and visual encoders.

We also conducted a comparison of computational cost  on AVE task. Our approach involves fine-tuning the pre-trained model, which inevitably leads to a reduction in speed compared to previous late-interaction baselines (CMBS ). However, we have achieved outstanding results, and our approach is applicable to multiple audio-visual tasks. Overall, the benefits are substantial.

## 5 Conclusion and discussion

This paper introduces DG-SCT, a method that leverages audio and visual modalities as prompts in the early layers of frozen pre-trained encoders. By doing so, our model can extract higher-quality

Figure 4: Results of our model and previous methods on **few-shot/zero-shot tasks.**

Figure 3: The qualitative results of DG-SCT on AVE and AVS tasks.

and finer-grained audio-visual features, enhancing performance in subsequent tasks. We conduct comprehensive experiments on four datasets, including AVE, AVVP, AVS, and AVQA, as well as our newly proposed few-shot/zero-shot audio-visual tasks. Across **25** experimental settings, our approach achieves state-of-the-art results on **19** of them. Additionally, ablation studies conducted on these datasets validate the effectiveness of our proposed spatial, channel, and temporal attention modules. Furthermore, our approach demonstrates robust generalizability and holds potential for application in more audio-visual scenarios in the future.

## 6 Acknowledgements

This work is supported by National Key R\(\&\)D Program of China under Grant No.2022ZD0162000, National Natural Science Foundation of China under Grant No. 62222211 and No.61836002. We also gratefully acknowledge the support of MindSpore (https://www.mindspore.cn), which is a new deep learning computing framework.

  
**Method** & **Trainable Params (\%)** & **Total Params (M)** & **GFLOPs** & **Acc** \\  CMBS  & 14.4 & 216.7 & 40.9 & 79.3 \\ LAVisH  & 10.1 & 238.8 & 406.7 & 79.7 \\ LAVisH* & 17.3+**13.3**=30.6 & 374.9 & 416.1 & 78.6 \\ 
**Ours** & 17.3+**26.3**=43.6 & 461.3 & 460.8 & **82.2** \\   

Table 6: **Efficiency analysis on AVE task. For trainable parameters of LAVisH* and our model, the first number (\(17.3\)) represents the trainable parameters of a two-dimensional convolution kernel and a linear projection.**

Figure 5: Qualitative visualizations of original visual (top row), our visual (second row), original audio (third row), and our audio (bottom row) features on AVE, AVVP, and AVS tasks.