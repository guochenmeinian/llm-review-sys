# Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent reinforcement learning (MARL) can be extremely computationally expensive. Curriculum learning is an effective way to accelerate learning, but an under-explored dimension for generating a curriculum is the difficulty-to-learn of the _subgames_ - games induced by starting from a specific state. In this work, we present a novel subgame curriculum learning framework for zero-sum games. It adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. Building upon this framework, we derive a subgame selection metric that approximates the squared distance to NE values and further adopt a particle-based state sampler for subgame generation. Integrating these techniques leads to our new algorithm, _Subgame Automatic Curriculum Learning_ (SACL), which is a realization of the subgame curriculum learning framework. SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the particle-world environment and Google Research Football environment show SACL produces much stronger policies than baselines. In the challenging hide-and-seek quadrant environment, SACL produces all four emergent stages and uses only half the samples of MAPPO with self-play. The project website is at[https://sites.google.com/view/sacl-neurips](https://sites.google.com/view/sacl-neurips).

## 1 Introduction

Applying reinforcement learning (RL) to zero-sum games has led to enormous success, with trained agents defeating professional humans in Go , StarCraft II , and Dota 2 . To find an approximate Nash equilibrium (NE) in complex games, these works often require a tremendous amount of training resources including hundreds of GPUs and weeks or even months of time. The unaffordable cost prevents RL from more real-world applications beyond these flagship projects supported by big companies and makes it important to develop algorithms that can learn close-to-equilibrium strategies in a substantially more efficient manner.

One way to accelerate training is curriculum learning - training agents in tasks from easy to hard. Many existing works in solving zero-sum games with MARL generate a curriculum by choosing whom to play with. They often use self-play to provide a natural policy curriculum as the agents are trained against increasingly stronger opponents . The self-play framework can be further extended to population-based training (PBT) by maintaining a policy pool and iteratively training new best responses to mixtures of previous policies . Such a policy-level curriculum generation paradigm is very different from the paradigm commonly used in goal-conditioned RL . Most curriculum learning methods for goal-conditioned problems directly reset the goal or initial states for each training episode to ensure the current task is of suitable difficulty for the learning agent. In contrast, the policy-level curriculum in zero-sum games only provides increasingly strongeropponents, and the agents are still trained by playing the full game starting from a fixed initial state distribution, which is often very challenging.

In this paper, we propose a general subgame curriculum learning framework to further accelerate MARL training for zero-sum games. It leverages ideas from goal-conditioned RL. Complementary to policy-level curriculum methods like self-play and PBT, our framework generates subgames (i.e., games induced by starting from a specific state) with growing difficulty for agents to learn and eventually solve the full game. We provide justifications for our proposal by analyzing a simple iterated Rock-Paper-Scissors game. We show that in this game, vanilla MARL requires exponentially many samples to learn the NE. However, by using a buffer to store the visited states and choosing an adaptive order of state-induced subgames to learn, the NE can be learned with linear samples.

A key challenge in our framework is to choose which subgame to train on. This is non-trivial in zero-sum games since there does not exist a clear progression metric like the success rate in goal-conditioned problems. While the squared difference between the current state value and the NE value can measure the progress of learning, it is impossible to calculate this value during training as the NE is generally unknown. We derive an alternative metric that approximates the squared difference with a bias term and a variance term. The bias term measures how fast the state value changes and the variance term measures how uncertain the current value is. We use the combination of the two terms as the sampling weights for states and prioritize subgames with fast change and high uncertainty.

Instantiating our framework with the state selection metric and a non-parametric subgame sampler, we develop an automatic curriculum learning algorithm for zero-sum games, i.e., _Subgame Automatic Curriculum Learning_ (SACL). SACL can adopt any MARL algorithm as its backbone and preserve the overall convergence property. In our implementation, we choose the MAPPO algorithm  for the best empirical performances.

We first evaluate SACL in the Multi-Agent Particle Environment and Google Research Football, where SACL learns stronger policies with lower exploitability than existing MARL algorithms for zero-sum games given the same amount of environment interactions. We then stress-test the efficiency of SACL in the challenging hide-and-seek environment. SACL leads to the emergence of all four phases of different strategies and uses 50% fewer samples than MAPPO with self-play.

## 2 Preliminary

### Markov game

A Markov game  is defined by a tuple \(=(,,},P,,,)\), where \(=\{1,2,,N\}\) is the set of agents, \(\) is the state space, \(}=_{i=1}^{N}_{i}\) is the joint action space with \(_{i}\) being the action space of agent \(i\), \(P:}()\) is the transition probability function, \(=(R_{1},R_{2},,R_{N}):}^{n}\) is the joint reward function with \(R_{i}\) being the reward function for agent \(i\), \(\) is the discount factor, and \(\) is the distribution of initial states. Given the current state \(s\) and the joint action \(=(a_{1},a_{2},,a_{N})\) of all agents, the game moves to the next state \(s^{}\) with probability \(P(s^{}|s,)\) and agent \(i\) receives a reward \(R_{i}(s,)\).

For infinite-horizon Markov games, a subgame \((s)\) is defined as the Markov game induced by starting from state \(s\), i.e., \((s)=1\). Selecting subgames is therefore equivalent to setting the Markov game's initial states. The subgames of finite-horizon Markov games are defined similarly and have an additional variable to denote the current step \(t\).

We focus on two-player zero-sum Markov games, i.e., \(N=2\) and \(R_{1}(s,)+R_{2}(s,)=0\) for all state-action pairs \((s,)}\). We use the subscript \(i\) to denote variables of player \(i\) and the subscript \(-i\) to denote variables of the player other than \(i\). Each player uses a policy \(_{i}:_{i}\) to produce actions and maximize its own accumulated reward. Given the joint policy \(=(_{1},_{2})\), each player's value function of state \(s\) and Q-function of state-action pair \((s,)\) are defined as

\[V_{i}^{}(s) =_{^{t}(|s^{t} ),s^{t+1} P(|s^{t},^{t})}_{t}^{t}R_{i} (s^{t},^{t})s^{0}=s, \] \[Q_{i}^{}(s,) =_{^{t}(|s^{t} ),s^{t+1} P(|s^{t},^{t})}_{t}^{t}R_{i} (s^{t},^{t})s^{0}=s,^{0}= . \]

[MISSING_PAGE_FAIL:3]

is different from playing the RPS game repeatedly for \(n\) times because players can play less than \(n\) rounds and they only receive a non-zero reward if \(P_{1}\) wins in all rounds. We use \(s_{k}\) to denote the state where players have already played \(k\) RPS games and are at the \(k+1\) round. It is easy to verify that the NE policy for both players is to play Rock, Paper, or Scissors with equal probability at each state. Under this joint NE policy, \(P_{1}\) can win one RPS game with \(1/3\) probability, and the probability for \(P_{1}\) to win all \(n\) rounds and get a non-zero reward is \(1/3^{n}\).

Consider using standard minimax-Q learning to solve the \(RPS(n)\) game. With Q-functions initialized to zero, we execute the exploration policy to collect samples and perform the update in Eq. (6). Note all state-actions pairs are required to be visited to guarantee convergence to the NE. Therefore, in this sparse-reward game, random exploration will clearly take \((3^{n})\) steps to get a non-zero reward. Moreover, even if the exploration policy is perfectly set to the NE policy of \(RPS(n)\), the probability for \(P_{1}\) to get the non-zero reward by winning all RPS games is still \((1/3^{n})\), requiring at least \((3^{n})\) samples to learn the NE Q-values of the \(RPS(n)\) game.

### From exponential to linear complexity

An important observation is that the states in later rounds become exponentially rare in the samples generated by starting from the fixed initial state. If we can directly reset the game to these states and design a smart order of minimax-Q updates on the subgames induced by these states, the NE learning can be accelerated significantly. Note that \(RPS(n)\) can be regarded as the composition of \(n\) individual \(RPS(1)\) games, a suitable order of learning would be from the easiest subgame \(RPS(1)\) starting from state \(s_{n-1}\) to the full game \(RPS(n)\) starting from state \(s_{0}\). Assuming we have full access to the state space, we first reset the game to \(s_{n-1}\) and use minimax-Q to solve subgame \(RPS(1)\) with \((1)\) samples. Given that the NE Q-values of \(RPS(k)\) are learned, the next subgame \(RPS(k+1)\) is equivalent to an \(RPS(1)\) game where the winning reward is the value of state \(s_{n-k}\). By sequentially applying minimax-Q to solve all \(n\) subgames from \(RPS(1)\) to \(RPS(n)\), the number of samples required to learn the NE Q-values is reduced substantially from \((3^{n})\) to \((n)\).

In practice, we usually do not have access to the entire state space and cannot directly start from the last subgame \(RPS(1)\). Instead, we can use a buffer to store all visited states and gradually span the state space. By resetting games to the newly visited states, the number of samples required to cover the full state space is still \((n)\), and we can then apply minimax-Q from \(RPS(1)\) to \(RPS(n)\). Therefore, the total number of samples is still \((n)\). The detailed analysis can be found in Appendix A.1 We validate our analysis by running experiments on \(RPS(n)\) games for \(n=1,,10\) and the results averaged over ten seeds are shown in Fig. 1. It can be seen that the sample complexity reduces from exponential to linear by running minimax-Q over a smart order of subgames, and the result of using a state buffer in practice is comparable to the result with full access.

```
Input: state sampler \(()\).  Initialize policy \(\); repeat  Sample \(s^{0}()\);  Rollout \(\) in \((s^{0})\);  Train \(\) via MARL; until\(\) converges; Output: final policy \(\).
```

**Algorithm 1**Subgame curriculum learning

## 4 Method

The motivating example suggests that NE learning can be largely accelerated by running MARL algorithms in a smart order over states. Inspired by this insight, we present a general framework to accelerate NE learning in zero-sum games by training over a curriculum of subgames. We further propose two practical techniques to instantiate the framework and present the overall algorithm.

Figure 2: Number of samples used to learn the NE Q-values of \(RPS(n)\) games.

### Subgame curriculum learning

The key issue of the standard sample-and-update framework is that the rollout trajectories always start from the fixed initial state distribution \(\), so visiting states that are most critical for efficient learning can consume a large number of samples. To accelerate training, we can directly reset the environment to those critical states. Suppose we have an oracle state sampler oracle\(()\) that can initiate suitable states for the current policy to learn, i.e., generate appropriate induced subgames, we can derive a general-purpose framework in Alg. 1, which we call subgame curriculum learning. Note that this framework is compatible with any \(}\) algorithm for zero-sum Markov games.

A desirable feature of subgame curriculum learning is that it does not change the convergence property of the backbone MARL algorithm, as discussed below.

**Proposition 1**.: _If all initial states \(s^{0}\) with \((s^{0})>0\) are sampled infinitely often, and the backbone MARL algorithm is guaranteed to converge to an NE in zero-sum Markov games, then subgame curriculum learning also produces an NE of the original Markov game._

The proof can be found in Appendix A.2. Note that such a requirement is easy to satisfy. For example, given any state sampler oracle\(()\), we can construct a valid mixed sampler by sampling from oracle\(()\) for probability \(0<p<1\) and sampling from \(\) for probability \(1-p\).

**Remark**.: With a given state sampler, the only requirement of our subgame curriculum learning framework is that the environment can be reset to a desired state to generate the induced game. This is a standard assumption in the curriculum learning literature  and is feasible in many RL environments. For environments that do not support this feature, we can simply reimplement the reset function to make them compatible with our framework.

### Subgame sampling metric

A key question is how to instantiate the oracle sampler, i.e., _which subgame should we train on for faster convergence_? Intuitively, for a particular state \(s\), if its value has converged to the NE value, that is, \(V_{i}(s)=V_{i}^{*}(s)\), we should no longer train on the subgame induced by it. By contrast, if the gap between its current value and the NE value is substantial, we should probably train more on the induced subgame. Thus, a simple way is to use the squared difference of the current value and the NE value as the weight for a state and sample states with probabilities proportional to the weights. Concretely, the state weight can be written as

\[w(s) =_{i=1}^{2}(V_{i}^{*}(s)-V_{i}(s))^{2} \] \[=_{i}(V_{1}^{*}(s)-_{i}(s))^{2}\] (8) \[=_{i}V_{1}^{*}(s)-_{i}(s)^{2}+ _{i}V_{1}^{*}(s)-_{i}(s), \]

where \(_{1}(s)=V_{1}(s)\) and \(_{2}(s)=-V_{2}(s)\). The second equality holds because the game is zero-sum and \(V_{2}^{*}(s)=-V_{1}^{*}(s)\). With random initialization and different training samples, \(\{_{i}\}_{i=1}^{2}\) can be regarded as an ensemble of two value functions and the weight \(w(s)\) becomes the expectation over the ensemble. The last equality further expands the expectation to a bias term and a variance term, and we sample state with probability \(P(s)=w(s)/_{s^{}}w(s^{})\). For the motivating example of \(RPS(n)\) game, the NE value decreases exponentially from the last state \(s_{n-1}\) to the initial state \(s_{0}\). With value functions initialized close to zero, the prioritized subgames throughout training will move gradually from the last round to the first round, which is approximately the optimal order.

However, Eq. 8 is very hard to compute in practice because the NE value is generally unknown. Inspired by Eq. 9, we propose the following alternative state weight

\[(s)=_{i}_{i}^{(t)}(s)-_ {i}^{(t-1)}(s)^{2}+_{i}_{i}(s), \]

which takes a hyperparameter \(\) and uses the difference between two consecutive value function checkpoints instead of the difference between the NE value and the current value in Eq. 9. The first term in Eq. (10 measures how fast the value functions change over time. If this term is large, the value functions are changing constantly and still far from the NE value; if this term is marginal, the value functions are probably close to the converged NE value. The second term in Eq. 10 measuresthe uncertainty of the current learned values and is the same as the variance term in Eq. 5 because \(V_{i}^{*}(s)\) is a constant. If \(=1\), Eq. 10 approximates Eq. 5 as \(t\) increases. It is also possible to train an ensemble of value functions for each player to further improve the empirical performance. Additional analysis can be found in Appendix A.3

Since Eq. 10 does not require the unknown NE value to compute, it can be used in practice as the weight for state sampling and can be implemented for most MARL algorithms. By selecting states with fast value change and high uncertainty, our framework prioritizes subgames where agents' performance can quickly improve through learning.

### Particle-based subgame sampler

With the sample weight at hand, we can generate subgames by sampling initial states from the state space. But it is impractical to sample from the entire space which is usually unavailable and can be exponentially large for complex games. Typical solutions include training a generative adversarial network (GAN)  or using a parametric Gaussian mixture model (GMM)  to generate states for automatic curriculum learning. However, parametric models require a large number of samples to fit accurately and cannot adapt instantly to the ever-changing weight in our case. Moreover, the distribution of weights is highly multi-modal, which is hard to capture for many generative models.

We instead adopt a particle-based approach and maintain a large state buffer \(\) using all visited states throughout training to approximate the state space. Since the size of the buffer is limited while the state space can be infinitely large, it is important to keep representative samples that are sufficiently far from each other to ensure good coverage of the state space. When the number of states exceeds the buffer's capacity \(K\), we use farthest point sampling (FPS)  which iteratively selects the farthest point from the current set of points. In our implementation, we first normalize each dimension of the state and then use the deep graph library package to utilize GPUs for fast and stable FPS results.

### Overall algorithm

Combining the subgame sampling metric and the particle-based sampler, we present a realization of the subgame curriculum learning framework, i.e., the _Subgame Automatic Curriculum Learning_ (SACL) algorithm, which is summarized in Alg. 2.

```
Input: state buffers \(\) with capacity \(K\), probability \(p\) to sample initial state from the state buffer.  Randomly initialize policy \(_{i}\) and value function \(V_{i}\) for player \(i=1,2\); repeat \(V_{i}^{} V_{i},\;i=1,2\); // Select subgame and train policy.  Sample \(s^{0}()\) with probability \(p\), else \(s^{0}()\);  Rollout in \((s^{0})\) and train \(\{_{i},V_{i}\}_{i=1}^{2}\) via MARL; // Compute weight by Eq. 10 and update state buffer. \(^{t}[_{i}(s^{t})-_{i} ^{}(s^{t})]^{2}+(\{_{i}(s^{t})\}_{i=1}^{2}),\;t=0, ,T\); \(\{(s^{t},^{t})\}_{t=0}^{T}\); if\(\|\|>K\)then \((,K)\); until\((_{1},_{2})\) converges; Output: final policy \((_{1},_{2})\).
```

**Algorithm 2**Subgame Automatic Curriculum Learning (SACL)

When each episode resets, we use the particle-based sampler to generate suitable initial states \(s_{0}\) for the current policy to learn. To satisfy the requirements in Proposition  we also reset the game according to the initial state distribution \(()\) with \(0.3\) probability. After collecting a number of samples, we train the policies and value functions using MARL. The weights for the newly collected states are computed according to Eq. 10 and used to update the state buffer \(\). If the capacity of the state buffer is exceeded, we use FPS to select representative states-weight pairs and delete the others. An overview of SACL in the hide-and-seek game is illustrated in Fig.

## 5 Experiment

We evaluate SACL in three different zero-sum environments: Multi-Agent Particle Environment (MPE) , Google Research Football (GRF) , and the hide-and-seek (HnS) environment . We use a state-of-the-art MARL algorithm MAPPO  as the backbone in all experiments.

In zero-sum games, because the performance of one player's policy depends on the other player's policy, the return curve throughout training is no longer a good evaluation method. One way to compare the performance of different policies is to use cross-play, which uses a tournament-style match between any two policies and records the results in a payoff matrix. However, due to the non-transitivity of many zero-sum games , winning other policies does not necessarily mean being close to NE policies, so a better way to evaluate the performance of policies is to use exploitability. Given a pair of policies \((_{1},_{2})\), the exploitability is defined as

\[(_{1},_{2})=_{i=1}^{2}_{_{i}^{}} _{s^{0}()}[V_{i}^{(_{i}^{},_{-i})}(s ^{0})]. \]

Exploitability can be roughly interpreted as the "distance" to the joint NE policy. In complex environments like the ones we use, the exact exploitability cannot be calculated because we cannot traverse the policy space to find \(_{i}^{}\) that maximizes the value. Instead, we compute the approximate exploitability by training an approximate best response \(_{i}^{}\) of the fixed policy \(_{i}\) using MARL.

### Main results

We first compare the performance of SACL in three environments against the following baselines for solving zero-sum games: self-play (SP), two popular variants including Fictitious Self-Play (FSP)  and Neural replicator dynamics (NeuRD) , and a population-based training method policy-space response oracles (PSRO) . More implementation details can be found in Appendix B

**Multi-Agent Particle Environment.** We consider the _predator-prey_ scenario in MPE, where three slower cooperating predators chase one faster prey in a square space with two obstacles. In the default setting, all agents are spawned uniformly in the square. We also consider a harder setting where the predators are spawned in the top-right corner and the prey is spawned in the bottom-left corner. All algorithms are trained for 40M environment samples and the curves of approximate exploitability w.r.t. sample over three seeds are shown in Fig. 4(a) and 4(b) SACL converges faster and achieves lower exploitability than all baselines in both settings, and its advantage is more obvious in the hard scenario. This is because the initial state distribution in corners makes the full game challenging to solve, while SACL generates an adaptive state distribution and learns on increasingly harder subgames to accelerate NE learning. More results and discussions can be found in Appendix C

**Google Research Football.** We evaluate SACL in three GRF academy scenarios, namely _pass and shoot_, _run pass and shoot_, and _3 vs 1 with keeper_. In all scenarios, the left team's agents cooperate

Figure 3: Illustration of SACL in the hide-and-seek environment. In the Fort Building stage, the states with hiders near the box have high weights (red) and agents can easily learn to build a fort by practicing on these subgames, while the states with randomly spawned hiders have low weights (green) and contribute less to learning. By sampling initial states with respect to the approximate squared distance to NE values, agents can proceed to new stages more efficiently.

to score a goal and the right team's agents try to defend them. The first two scenarios are trained for 50M environment samples and the last scenario is trained for 100M samples. Table 1 lists the approximate exploitabilities of different methods' policies over three seeds, and SACL achieves the lowest exploitability. Additional cross-play results and discussions can be found in Appendix C

**Hide-and-seek environment.** HnS is a challenging zero-sum game with known NE policies, which makes it possible for us to directly evaluate the number of samples used for NE convergence. We consider the _quadrant_ scenario where there is a room with a door in the lower right corner. Two hiders, one box, and one ramp are spawned uniformly in the environment, and one seeker is spawned uniformly outside the room. Both the box and the ramp can be moved and locked by agents. The hiders aim to avoid the lines of sight from the seeker while the seeker aims to find the hiders.

There is a total of four stages of emergent stages in HnS, i.e., Running and Chasing, Fort Building, Ramp Use, and Ramp Defense. As shown in Fig. 4(c), SACL with MAPPO backbone produces all four stages and converges to the NE policy with only 50% the samples of MAPPO with self-play. We also visualize the initial state distribution to show how SACL selects appropriate subgames for agents to learn. Fig. 5(a) depicts the distribution of hiders' position in the Fort Building stage. The probabilities of states with hiders inside the room are much higher than states with hiders outside, making it easier for hiders to learn to build a fort with the box. Similarly, the distribution of the seeker's position in the Ramp Use stage is shown in Fig. 5(b) and the most sampled subgames start from states where the seeker is close to the walls and is likely to use the ramp.

### Ablation study

We perform ablation studies to examine the effectiveness of the proposed sampling metric and particle-based sampler. All experiments are done in the hard _predator-prey_ scenario of MPE and the results are averaged over three seeds. More ablation studies on state buffer size, subgame sample probability, and other hyperparameters can be found in Appendix C

**Subgame sampling metric.** The sampling metric used in SACL follows Eq. (10) which consists of a bias term and a variance term. We compare it with four other metrics including a uniform metric, a bias-only metric, a variance-only metric, and a temporal difference (TD) error metric. The last metric uses the TD error \(|_{t}|=|r^{t}+ V(s^{t+1})-V(s^{t})|\) as the weight, which can be regarded as an estimation of value uncertainty. The results are shown in Fig. 5(c) and the sampling metric used by SACL achieves the best results and outperforms both the bias-only metric and variance-only metric.

**State generator and buffer update method.** We substitute the particle-based sampler with other state generators including using GAN from the work  and using GMM from the work . We also replace the FPS buffer update method with a uniform one that randomly keeps states and a greedy one that keeps states with the highest weights. Results in Fig. 5(c) show that our particle-based sampler with FPS update leads to the fastest convergence and lowest exploitability.

   Scenario & SACL & SP & FSP & PSRO & NeuRD \\  pass and shoot & **3.79 (0.87)** & 4.17 (1.45) & 4.73 (2.64) & 4.68 (2.46) & 9.18 (1.89) \\ run pass and shoot & **4.05 (1.22)** & 4.45 (1.22) & 4.62 (0.02) & 8.40 (0.48) & 9.27 (0.35) \\
3 vs 1 with keeper & **5.49 (0.93)** & 7.76 (0.67) & 6.23 (1.14) & 7.43 (1.49) & 8.72 (0.15) \\   

Table 1: Approximate exploitability of learned policies in different GRF scenarios.

Figure 4: Main experiment results in (a) MPE, (b) MPE hard, and (c) Hide-and-seek.

## 6 Related work

A large number of works achieve faster convergence in zero-sum games by playing against an increasingly stronger policy. The most popular methods are self-play and its variants . Self-play creates a natural curriculum and leads to emergent complex skills and behaviors . Population-based training like double oracle  and policy-space response oracles (PSRO)  extend self-play by training a pool of policies. Some follow-up works further accelerate training by constructing a smart mixing strategy over the policy pool according to the policy landscape .  extends PSRO to extensive-form games by building policy mixtures at all states rather than only the initial states, but it still directly solves the full game starting from some fixed states.

In addition to policy-level curriculum learning methods, other works to accelerate training in zero-sum games usually adopt heuristics and domain knowledge like the number of agents  or environment specifications . By contrast, our method automatically generates a curriculum over subgames without domain knowledge and only requires the environments can be reset to desired states. Subgame-solving technique  is also used in online strategy refinement to improve the blueprint strategy of a simplified abstract game. Another closely related work to our method is  which combines backward induction with policy learning, but this method requires knowledge of the game topology and can only be applied to finite-horizon Markov games.

Besides zero-sum games, curriculum learning is also studied in cooperative settings. The problem is often formalized as goal-conditioned RL where the agents need to reach a specific goal in each episode. Curriculum learning methods design or train a smart sampler to generate proper task configurations or goals that are most suitable for training advances w.r.t. some progression metric . Such a metric typically relies on an explicit signal, such as the goal-reaching reward, success rates, or the expected value of the testing tasks. However, in the setting of zero-sum games, these explicit progression metrics become no longer valid since the value associated with a Nash equilibrium can be arbitrary. A possible implicit metric is value disagreement  used in goal-reaching tasks, which can be regarded as the variance term in our metric. By adding a bias term, our metric approximates the squared distance to NE values and gives better results in ablation studies.

Our work adopts a non-parametric subgame sampler which is fast to learn and naturally multi-modal, instead of training an expensive deep generative model like GAN . Such an idea has been recently popularized in the literature. Some representative samplers are Gaussian mixture model , Stein variational inference , Gaussian process , or simply evolutionary computation . Technically, our method is also related to prioritized experience replay  with the difference that we maintain a buffer  to approximate the uniform distribution over the state space.

## 7 Conclusion

We present SACL, a general algorithm for accelerating MARL training in zero-sum Markov games based on the subgame curriculum learning framework. We propose to use the approximate squared distance to NE values as the sampling metric and use a particle-based sampler for subgames generation. Instead of starting from the fixed initial states, RL agents trained with SACL can practice more on subgames that are most suitable for the current policy to learn, thus boosting training efficiency. We report appealing experiment results that SACL efficiently discovers all emergent strategies in the challenging hide-and-seek environment and uses only half the samples of MAPPO with self-play. We hope SACL can be helpful to speed up prototype development and help make MARL training on complex zero-sum games more affordable to the community.

Figure 5: Visualization of the state distributions in HnS (a-b) and ablation studies (c-d).