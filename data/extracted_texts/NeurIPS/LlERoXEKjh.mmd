# Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?

Erin George, Michael Murray, William Swartworth, Deanna Needell

Department of Mathematics, UCLA, CA, USA

[egeo,mmurray,wswartworth,deanna]@math.ucla.edu

Equal contribution

###### Abstract

We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; non-benign overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are eventually zeroed by the network. We prove these results using a combinatorial approach that involves bounding the number of clean versus corrupt updates across these phases of training.

## 1 Introduction

Conventional machine learning wisdom suggests that the generalization error of a complex model will typically be worse versus a simpler model when both are trained to interpolate data. Indeed, the bias-variance trade-off implies that although choosing a complex model is advantageous in terms of approximation error, it comes at the price of an increased risk of overfitting. The traditional solution to managing this trade-off is to use some form of regularization, allowing the optimizer to select a predictor from a rich class of functions while at the same time encouraging it to choose one that is in some sense simple. However, in recent years this perspective has been challenged by the observation that deep learning models, trained with minimal if any form of regularization, can almost perfectly interpolate noisy data with nominal cost to their generalization performance (Zhang et al., 2017; Belkin et al., 2018, 2019). This phenomenon is referred to as _benign overfitting_.

Following these empirical observations, a line of research has emerged aiming to theoretically characterize the conditions under which various machine learning models, trained to zero loss on noisy data, obtain, at least asymptotically, optimal generalization error. To date, the majority of analyses in this regard have focused primarily on linear models, including linear regression (Bartlett et al., 2020; Muthukumar et al., 2020; Wu and Xu, 2020; Chatterji and Long, 2021; Zou et al., 2021; Hastie et al., 2022; Koehler et al., 2021; Wang et al., 2021; Chatterji and Long, 2022; Cao et al., 2021; Shamir, 2022), logistic regression (Chatterji and Long, 2021; Muthukumar et al., 2021; Wang et al., 2021) and kernel regression (Belkin et al., 2018; Mei and Montanari, 2019; Liang and Rakhlin, 2020; Liang et al., 2019). With regards to understanding benign overfitting in neural networks, in theNeural Tangent Kernel (NTK) regime (Jacot et al., 2018) the prediction of a neural network is well approximated via kernel regression (Adlam & Pennington, 2020). However, this regime typically requires unrealistically large network width and fails to capture feature learning. Indeed, and despite being the initial source of inspiration, an understanding of when and how neural networks benignly overfit in the rich, feature learning regime is not well understood.

### Contributions and related work

In this work we study benign overfitting in the context of binary classification for two-layer ReLU networks, trained using gradient descent and hinge loss, on label corrupted, linearly separable data. There are a number of recent and or concurrent works which prove benign overfitting results in a similar setting Frei et al. (2022, 2023); Xu & Gu (2023); Cao et al. (2022); Kou et al. (2023); Kornowski et al. (2023), however, we emphasize that these exclusively study exponentially tailed losses, notably the popular logistic loss. Benign overfitting is intimately related to the notion of _implicit bias_, the preference of an algorithm for selecting minimizers with certain properties over others. The implicit bias of homogeneous networks trained with gradient descent on an exponentially tailed loss from a low initial loss is known to converge in direction to a Karush-Kuhn-Tucker (KKT) point of the associated max-margin problem Lyu & Li (2020); Ji & Telgarsky (2020). This implies at least intuitively a certain bias towards margin maximization. In a recent work Frei et al. (2023) it is shown that if the input data is sufficiently orthogonal then a shallow, leaky ReLU network evaluated on such a KKT point is equivalent to a particular linear classifier. Moreover, and under additional data assumptions, the authors show such networks benignly overfit. Another recent paper Kornowski et al. (2023) uses a similar approach to derive benign overfitting results for ReLU networks and also provides a description of the transition between benign and tempered overfitting in the univariate input case. To the best of our knowledge, equivalent results on the implicit bias of homogeneous networks trained with non-exponentially tailed losses are not characterized. Furthermore, training a linear classifier with an exponential versus non-exponential tailed loss is known to result in a different implicit bias, with the non-exponential tailed loss potentially inducing convergence in direction to a classifier with a poor margin Ji et al. (2020). As a result, a priori it is not clear if and how the choice of hinge loss impacts the propensity for a shallow ReLU network to overfit.

There are two main existing lines of work which study benign overfitting in neural networks outside of the kernel regime. Concerning perhaps the most relevant line of prior work to our own, Frei et al. (2022) consider a smooth, leaky ReLU activation function, train the network using the logistic instead of the hinge loss and assume the data is drawn from a mixture of well-separated sub-Gaussian distributions. The key result of this work is that given a sufficient number of iterations of GD, then the network will interpolate the noisy training data while also achieving minimax optimal generalization error up to constants in the exponents. A concurrent work Xu & Gu (2023) extends this result to more general activation functions including ReLU, relaxes the assumptions on the noise distribution to being centered with bounded logarithmic Sobolev constant, and also improves the convergence rate. As highlighted in Xu & Gu (2023), the fact that ReLU is non-smooth and non-leaky significantly complicates the analysis of both the convergence and generalization. A second line of work (Cao et al., 2022; Kou et al., 2023) studies benign overfitting in two-layer convolutional as opposed to feedforward neural networks. Whereas here and in Frei et al. (2022); Xu & Gu (2023) each data point is modeled as the sum of a signal and noise component, in Cao et al. (2022); Kou et al. (2023) the signal and noise components lie in disjoint patches. The weight vector of each neuron is applied to both patches separately and a non-linearity, such as ReLU, is applied to the resulting pre-activation. In this setting, the authors prove interpolation of the noisy training data and derive conditions on the clean margin under which the network benignly vs non-benignly overfits. We emphasize that the data model studied in this work is very different to the setting we study here, and as a result we primarily restrict our comparison to that with Frei et al. (2022) and the concurrent work Xu & Gu (2023). Finally, in regard to optimizing shallow ReLU networks using hinge loss, a line of work (Brutzkus et al., 2018; Wang et al., 2019; Yang et al., 2021) studies the convergence of gradient descent on generic, linearly separable data without label corruptions. These works also require additional assumptions, notably leaky ReLU instead of ReLU, insertion of noise into the optimization algorithm or changes to the loss function.

Before we discuss our contributions we remark that a previous work Mallinar et al. (2022) describes and experimentally explores a taxonomy of overfitting: benign overfitting, where the generalization error is optimal; catastrophic overfitting, where the generalization error is close to random chance;and tempered overfitting, which lies in between. In this work, we do not consider the full breadth of this taxonomy, and use the terms "non-benign overfitting" or equivalently "harmful overfitting" to refer to overfitting that may be either tempered or catastrophic. We now summarize our contributions: in particular, under certain assumptions on the model hyperparameters, we prove conditions on the clean margin resulting in the three distinct training outcomes highlighted below. We remark also that the prior works discussed primarily focus on deriving positive benign overfitting results.

1. **Benign overfitting:** Theorem 3.1 provides conditions under which the training loss converges to zero and bounds the generalization error, showing that it is asymptotically optimal. This result is analogous to those of Frei et al. (2022) and Xu & Gu (2023) but for the hinge instead of logistic loss.
2. **Non-benign overfitting:** Theorem 3.6 provides conditions under which the network achieves zero training loss while generalization error is bounded below by a constant. Unlike Frei et al. (2022) and Xu & Gu (2023), this is not due to the non-separability of the data model but is instead a result of the neural network failing to learn the optimal classifier.
3. **No overfitting:** Theorem 3.8 provides conditions under which the network achieves zero training loss on points with uncorrupted label signs but nonzero loss on points with corrupted signs. Again the generalization error is bounded and shown to be asymptotically optimal.

To conclude this section we further remark that our proof techniques are quite different from those used in Frei et al. (2022); Xu & Gu (2023) and indeed the other works highlighted in this section. Again we emphasize this is due to the fact we study the hinge loss instead of the logistic loss and discuss the differences arising from this in detail in Section 3. In particular, we set up the problem in such a way that the convergence analysis reduces to counting the number of activations of clean versus corrupt points during various stages of training. Our analysis further provides a detailed description of the dynamics of the network's neurons, thereby allowing us to understand how the network fits both the clean and corrupted data.

## 2 Preliminaries

### Data model

We consider a training sample of \(2n\) pairs of points and their labels \((_{i},y_{i})_{i=1}^{2n}\) where \((_{i},y_{i})^{d}\{-1,+1\}\) for all \(i[2n]\). Furthermore, we identify two disjoint subsets \(_{T}[2n]=\{1,,2n\}\) and \(_{F}[2n]\), \(_{T}_{F}=[2n]\), which correspond to the clean and corrupt points in the sample respectively. The categorization of a point as clean or corrupted is determined by its label: for all \(i[2n]\) we assume \(y_{i}=(i)(-1)^{i}\) where \((i)=-1\) iff \(i_{F}\) and \((i)=1\) otherwise. In addition, we assume \(|_{F}[2n]_{e}|=|_{F}[2n]_{o}|=k\) and \(|_{T}[2n]_{e}|=|_{T}[2n]_{o}|=n-k\), where \([2n]_{e}[2n]\) and \([2n]_{o}[2n]\) are the even and odd indices, respectively. We remark that this assumption simplifies the exposition of our results but is not integral to our analysis. Each data point is assumed to have the form

\[_{i}=(-1)^{i}(+ (i)_{i}).\] (1)

Here \(^{d}\) satisfies \(\|\|=1\) and furthermore we refer to \(\) as the signal vector as the alignment of a clean point with \(\) determines its sign. Indeed, \((_{i},)=(-1)^{i}=y_{i}\) for \(i_{T}\) whereas \((_{i},)=-y_{i}\) for \(i_{F}\). Thus we may view the labels of a corrupt point as flipped from their clean state. The vectors \((_{i})_{i=1}^{2n}\) are mutually independent and identically distributed (i.i.d.) random vectors drawn from the uniform distribution over \(^{d-1}\{\}^{}\), which we denote \(U(^{d-1}\{\}^{})\). Clearly this distribution is symmetric, mean zero and for any \( U(^{d-1}\{\}^{})\) it holds that \(\) and \(\|\|=1\). We refer to these vectors as noise components due to the fact that they are independent of the labels of their respective points. The real, scalar quantity \(\) controls the strength of the signal versus the noise and also defines the clean margin. Finally, at test time a clean label \(y U(\{-1,1\})\) is sampled and the corresponding test data point is constructed,

\[=y(+),\] (2)

where again \( U(^{d-1}\{\}^{})\).

The key idea we use to characterize the training dynamics is to reduce the analysis of the trajectory of each neuron to that of counting the number of clean versus corrupt updates to it. This combinatorial approach relies on each point having similar sized signal and noise components. In order to make our analysis as clear as possible, we select a data model which ensures the signal and noise components are consistent in size across all points. We emphasize that these assumptions are not strictly necessary and we believe analogous analyses could be conducted when the signal and noise components are instead appropriately bounded. In addition, and as discussed in more detail in Section 3.2, the orthogonality of the signal and noise components allow us to demonstrate non-benign overfitting even when a perfect classifier exists.

### Network architecture, optimization and initialization

We consider a densely connected, single layer feed-forward neural network \(f:^{2m d}^{d}\) with the following forward pass map,

\[f(,)=_{j=1}^{2m}(-1)^{j}(_{j}, ).\]

Here \(:=\{0,z\}\) denotes the ReLU activation function and \(_{j}\) the \(j\)-th row of the weight matrix \(^{2m d}\). The network weights are optimized using full batch gradient descent (GD) with step size \(>0\) in order to minimize the hinge loss over a training sample \(((_{i},y_{i}))_{i=1}^{2n}(^{d}\{-1,1\})^{2n}\) sampled as described in Section 2.1. After \(t^{}\) iterations this optimization process generates a sequence of weight matrices \((^{(t)})_{t=0}^{t^{}}\). For convenience, we overload our notation for the forward pass map of the network and let \(f(t,):=f(^{(t)},)\). Furthermore, we denote the hinge loss on the \(i\)-th point at iteration \(t\) as \((t,i):=\{0,1-y_{i}f(t,_{i})\}\). The hinge loss over the entire training sample at iteration \(t\) is therefore \(L(t):=_{i=1}^{2n}(t,i)\). Let \(^{(t)}:=\{i[2n]:(t,_{i})>0\}\) and \(_{j}^{(t)}:=\{i[2n]:_{j}^{(t)},_{i} >0\}\) denote the sets of point indices that have nonzero loss and which activate the \(j\)th neuron at iteration \(t\) respectively. With

\[}=0,&_{ j}^{(t)},_{i} 0,\\ -(-1)^{j}y_{i}x_{ir},&_{j}^{(t)},_{i}>0\]

then the GD update rule1 for the neuron weights at iteration \(t 0\) may be written as

\[_{j}^{(t+1)}=_{j}^{(t)}+(-1)^{j}_{l=1}^{2n} (l_{j}^{(t)}^{(t)})\!y_{l} _{l}.\] (3)

In regard to the initialization of the network parameters, for convenience we assume each neuron's weight vector is drawn mutually i.i.d. uniform from the centered sphere with radius \(_{w}>0\). We remark that results analogous to the ones presented hold if the weights are instead initialized mutually i.i.d. as \(w_{jc}^{(0)}(0,_{w}^{2})\) for sufficiently small \(_{w}^{2}\).

### Notation

For indices \(i,j_{ 1}\) we say \(i j\) iff \((-1)^{i}=(-1)^{j}\). We often refer to a data point or neuron by its index alone, e.g. "point \(i\)" refers to the \(i\)-th training point \((_{i},y_{i})\). For two iterations \(t_{0},t_{1}\) with \(t_{1}>t_{0}\) we define the following.

1. \(G_{j}(t_{0},t_{1}):=_{i S_{T}}_{=t_{0}}^{t_{1}-1}(i _{j}^{()}^{()})\) is the number of clean updates applied to the \(j\)-th neuron between iterations \(t_{0}\) and \(t_{1}\).
2. \(B_{j}(t_{0},t_{1}):=_{i S_{T}}_{=t_{0}}^{t_{1}-1}(i _{j}^{()}^{()})\) is the number of corrupt updates applied to the \(j\)-th neuron between iterations \(t_{0}\) and \(t_{1}\).
3. \(G(t_{0},t_{1}):=_{j[2m]}G_{j}(t_{0},t_{1})\) and \(B(t_{0},t_{1}):=_{j[2m]}B_{j}(t_{0},t_{1})\) are the total number of clean and corrupt updates applied to the entire network between iterations \(t_{0}\) and \(t_{1}\).

4. \(T(t_{0},t_{1}):=G(t_{0},t_{1})+B(t_{0},t_{1})\) is the total number of updates from all points applied to the entire network between iterations \(t_{0}\) and \(t_{1}\).

We extend all these definitions to the case \(t_{0}=t_{1}\) by letting the empty sum be 0. Finally, we use \(C 1\) and \(c 1\) to denote generic, positive constants.

## 3 Results

The main contributions of this work are Theorem 3.1, Theorem 3.6 and Theorem 3.8, which characterize how the margin of the clean data drives three different training regimes: namely benign overfitting, non-benign (or harmful) overfitting and no-overfitting respectively. We primarily distinguish between the three aforementioned training outcomes based on conditions on the signal strength \(\) which controls the clean margin. Assuming the corrupt points are the minority in the training sample, then heuristically we might expect the following behavior as \(\) varies: if \(n 1\), then the signal dominates the noise during training, corrupted points are never fitted and the network generalizes well. If \(n 1\), then all points are eventually fitted based on their noise component and the network generalizes poorly. As such, we expect to observe benign overfitting when \(\) is small but not too small: in this regime the network learns the signal, thus ensuring it generalizes well, but corrupted points can still be fitted based on their noise component, thereby allowing training to zero loss.

With each theorem we provide here we give a sketch of its proof: full proofs are contained in the Supplementary Materials, which also contain supporting numerical simulations in Appendix F. Throughout this section, and in order to establish a common setting in which to observe a variety of different behaviors, we make the following assumptions on the network and data hyperparameters.

**Assumption 1**.: _For a sufficiently large constant \(C 1\), failure probability \((0,1/2)\) and noise inner product bound \((0,1)\), let \(d C^{-2}(n/)\), \(k cn\), \(_{w} c\) and \(\), where \(\) depends on \(n\), \(m\), \(k\), \(\), and \(d\)._

We remark that the condition \(d C^{-2}(n/)\) ensures the noise components are nearly-orthogonal: in particular, \(_{i}|_{i},_{}| c\) with high probability for some positive constant \(c\). This near orthogonality condition on the noise terms is restrictive, but is a common assumption in the related works Frei et al. (2022); Xu & Gu (2023). We note that the value of \(\) required for each of our results to hold varies. Likewise, the optimal constants \(c\) and \(C\) required in each case also vary and we will not concern ourselves with finding the tightest possible constants.

While there are differences the proofs of Theorem 3.1, 3.6 and 3.8 generally fit the following outline.

1. Use concentration to show with high probability the training data is nearly orthogonal and a certain initialization pattern is satisfied.
2. Characterize the activation pattern early in training before any point achieves zero loss.
3. Bound the activations at an iteration just before any training point achieves zero loss.
4. Based on bounds on the activations at a given iteration, derive an iteration-independent upper bound on the number of subsequent updates that can occur before convergence. At convergence all points either have zero loss or activate no neurons.

We emphasize that our proof techniques are significantly different from those used in Frei et al. (2022); Xu & Gu (2023) due to the differences between the hinge and logistic loss. In particular, letting \((z)\) denote the logistic loss, a key step in the proof of these prior works is showing at any iteration \(t 0\) that the ratio \(^{}(y_{i}f(t,_{i}))/^{}(y_{l}f(t, _{l}))\) is upper bounded by a constant for all pairs of points \(i,l\) in the training sample. For the hinge loss this approach is not feasible: indeed, if at an iteration \(t\) some points achieve zero loss while others have not then this ratio is unbounded.

### Benign overfitting

The following theorem states conditions in particular on \(\) under which the network simultaneously achieves asymptotically optimal test error and achieves zero loss on both the clean and corrupted data after a finite number of iterations. A detailed proof of this Theorem along with the associated lemmas is provided in Appendix C.

**Theorem 3.1**.: _Let Assumption 1 hold and further assume \(n C(1/)\), \(m C(n/)\), \( c\) and \(C cn^{-1}\). Then there exists a sufficiently small step-size \(\) such that with probability at least \(1-\) over the randomness of the dataset and network initialization the following hold._

1. _The training process terminates at an iteration_ \(_{}\)_._
2. _For all_ \(i[2n]\) _then_ \((_{},_{i})=0\)_._
3. _The generalization error satisfies_ \[((f(_{},)) y) (-cd^{2}).\]

Proof sketch.: Recall the parameter \(\) bounds the inner products of the noise components of the training data. Specifically, the conditions on \(d\) given in Assumption 1 ensure \(_{i l}|_{i},_{l}|\) with high probability. We also identify the following sets of neurons for \(p\{-1,1\}\),

\[_{p} :=\{j[2m]\;:\;(-1)^{j}=p,\;G_{j}(0,1)(-)-B_{j}( 0,1)(+)}{}\},\] \[_{p} :=\{j_{p}\;:\;G_{j}(0,1)(+)-B_{j}(0,1)(- ) 1-+\}.\]

These sets are useful in that neurons in \(_{p}\) have predictable activation patterns during the early phase of training. Furthermore, if \(i\) is the index of a corrupted point which activates a neuron in \(_{y_{i}}\) at initialization, then this point will continue activating this neuron throughout the early phase of training. Concentration argument shows that \(_{p}\) and \(_{p}\) are sufficiently significant subsets of \({[2m]_{p}}^{2}\) with high probability. In summary, for benign overfitting we say we have a _good initialization_ if i) \(_{i l}|_{i},_{l}|\), ii) for some small constant \((0,1)\) then \(|_{p}|(1-)m\) for \(p\{-1,1\}\), and iii) for each \(i_{F}\) there exists a \(j[2m]\) such that \((-1)^{j}=y_{i}\) and \(i_{j}^{(0)}\).

**Lemma 3.2**.: _Under the assumptions of Theorem 3.1 and assuming we have a good initialization, suppose at some iteration \(t_{0}\) the loss of every clean point is bounded above by \(a_{ 0}\), while the loss of every corrupted point is bounded above by \(b_{ 0}\). Then for all \(t t_{0}\) the total number of clean and corrupt updates which occur after \(t_{0}\) are upper bounded as follows,_

\[G(t_{0},t) Cn(), B(t_{0},t) Ck().\]

Because these upper bounds are independent of \(t\) then we may conclude that training reaches a steady state after a finite number of iterations. In particular, this means every point either has zero loss or activates no neurons. To prove the network achieves zero loss we need only show that every training point activates at least one neuron after the last training update. This property is simple to prove for clean points: indeed, if \(i_{T}\) then \(i\) activates every neuron in \(_{y_{i}}\) after the first iteration. An inductive argument then shows \(i\) activates a neuron in every subsequent iteration. Showing that every corrupt point activates a neuron at the end of training is not as simple, and requires a more careful consideration of the training dynamics. To this end we say a neuron is a _carrier_ of a training point between iterations \(t_{0}\) and \(t\) if \(i_{j}^{()}\) for all \([t_{0},t]\). In order to prove the network fits the corrupt data we need to show each corrupt point \((_{i},y_{i})\) has a carrier neuron in \(_{y_{i}}\) throughout training. If too many clean points activate such a neuron, then it is possible it will eventually cease to carry any corrupt points and if a corrupt point loses all of its carrier neurons then it cannot be fitted. We show this event cannot occur by studying the activation patterns of neurons in \(:=_{1}_{-1}\).

**Lemma 3.3**.: _Let the assumptions of Theorem 3.1 hold and suppose we have a good initialization. Let \(j\) and \(t>0\) be an iteration such that no point achieves zero loss at or before this iteration. For a point \(i_{T}\), then \(i_{j}^{(t)}\) iff \(i j\). For a point \(i_{F}\) with \(i j\), \(i_{j}^{(t)}\) iff \(i_{j}^{(1)}\)._

The next lemma bounds the activations just before any points achieve zero loss.

**Lemma 3.4**.: _Under the assumptions of Theorem 3.1 and assuming we have a good initialization, there is an iteration \(_{1}\) before any point achieves zero loss where the following hold for a constant that varies from line to line._

1. _For all_ \(p\{-1,1\}\)_,_ \(j_{p}\)_,_ \(i j\)_, and_ \(i_{T}\)_, then_ \(_{j}^{(_{1})},_{i} cm^{-1}\)_._
2. _For all_ \(p\{-1,1\}\)_,_ \(j_{p}\)_,_ \(i j\)_, and_ \(i_{T}\)_, then_ \(_{j}^{(_{1})},_{i}-cn m ^{-1}\)_._
3. _For all_ \(i_{T}\)_, then_ \((_{1},_{i}) c\)_._

Due to the fact that clean points are the majority and all of them push the network in the same signal direction, then immediately after \(_{1}\) the loss of clean points is small and clean points activate all neurons in the relevant \(_{p}\) strongly. Furthermore, once the loss of a clean point is small it stays small. In subsequent iterations, if the number of corrupt updates since \(_{1}\) is also small, approximately \(C n/((+)\), then each clean point will activate on all but an \(\) proportion of neurons in the relevant \(_{p}\). As the hinge loss switches off the updates from a point once it reaches zero loss, eventually clean points do not participate in every iteration. Furthermore, when they do participate their updates are spread over a large proportion of the neurons. This ensures that most neurons in \(_{p}\) cannot receive too many clean updates in isolation, thereby ensuring carrier neurons continue to carry corrupted points throughout training.

Lastly, the generalization result follows from the near orthogonality of the noise components of both the training and test data. Indeed, using the same concentration bound, a test point satisfies the same inner product noise condition as the training data with high probability.

**Lemma 3.5**.: _Consider a test label \(y\{-1,1\}\) and point \( y+\), where \((^{d-1}\{\}^{})\) is mutually i.i.d. from the training sample. Assume the conditions of Theorem 3.1 hold and that we have a good initialization. In addition, suppose that \(|,_{l}|<\) for all \(l[2n]\), then \(yf(_{},)>0\)._

### Non-benign overfitting

The next theorem states a harmful overfitting result: for sufficiently small \(\) the network achieves again zero loss on both the clean and corrupt data after a finite number of iterations, but the probability of misclassification is bounded from below by a constant. A detailed proof of this Theorem along with the associated lemmas is provided in Appendix D.

**Theorem 3.6**.: _Let Assumption 1 hold and further assume \(m C(n/)\), \( cn^{-1}\), \(<1/(2mn)\) and \(}\). Then with probability at least \(1-\) over the randomness of the dataset and network initialization the following hold._

1. _The training process terminates at an iteration_ \(_{}\)_._
2. _For all_ \(i[2n]\) _then_ \((_{},_{i})=0\)_._
3. _The generalization error satisfies_ \[((f(_{},)) y) .\]

We remark that the above result holds for \(n 1\) and any \(k\). Indeed, in this regime the noise components dominate the training dynamics and we therefore expect the performance of the network on test points to be close to random. We re-emphasize that, unlike in the data model used by Frei et al. (2022) and Xu & Gu (2023), there does exist a classifier with perfect generalization error for arbitrarily small \(\). The significance of Theorem 3.6 is that under the data model considered GD results in a suboptimal classifier.

Proof sketch.: Similar to the proof of Theorem 3.1, in the context of non-benign overfitting we say the initialization is "good" if \(_{i l}|_{i},_{l}|\) and if each point in the training sampleactivates a neuron of the same sign. Under the conditions of Theorem 3.6 it can be shown that a good initialization in this context happens with high probability.

**Lemma 3.7**.: _In addition to the conditions of Theorem 3.6, suppose we have a good initialization and that for some iteration \(t_{0}\) then \((t_{0},_{i}) a\) for all \(i[2n]\). Then \(T(t_{0},t)\)._

As for the benign overfitting case, we need to show that each training point activates a neuron after the last training iteration. Under the assumptions on \(\) it can be shown that the loss of a point decreases during every iteration it participates in, regardless of the status and activations of other points in the training sample. All that remains is to lower bound the generalization error. To this end observe for a test point \((,y)\) that

\[y(f(_{},)-f(_{},- ))=_{j=1}^{2m}y(-1)^{j}_{j}^{(_{ })},.\]

If the right-hand-side of this equality is negative we can conclude that either \(\) or \(-\) is misclassified. That this event is true with probability lower bounded by a constant in turn follows by appropriately upper bounding the norm of the network weights in the signal subspace, as well as lower bounding the norm of the network weights in the noise subspace. 

### No-overfitting

The following theorem illustrates that for \(\) larger than the upper bound required for benign overfitting, then after convergence, which occurs in a finite number of iterations, only the clean points achieve zero loss. By contrast, the corrupt points cease to activate any neurons and are thus zeroed by the network. The network also achieves asymptotically optimal test error. A detailed proof of this theorem along with the associated lemmas is provided in Appendix E.

**Theorem 3.8**.: _Let Assumption 1 hold and further assume \(m 2\), \(n C()\), \( c\) and \(cn^{-1} ck^{-1}\). Then there exists a sufficiently small step-size \(\) such that with probability at least \(1-\) over the randomness of the dataset and network initialization we have the following._

1. _The training process terminates at an iteration_ \(_{}\)_._
2. _For all_ \(i_{T}\) _then_ \((_{},_{i})=0\) _while_ \((_{},_{i})=1\) _for all_ \(i_{F}\)_._
3. _The generalization error satisfies_ \[((f(_{},)) y) (-cd^{2}).\]

We remark that the upper bound on \(\) allows us to re-deploy the same proof technique used to prove convergence in the benign overfitting case, thereby ensuring the training process converges within a finite number of iterations. We conjecture this upper bound can be relaxed but leave such an analysis to future work.

Proof sketch.: In the context of no-overfitting we identify a "good" initialization as one for which \(_{i I}|_{i},_{i}|\) and \(=_{-1}_{+1}=[2m]\). Under the conditions of Theorem 3.8 it can be shown a good initialization in this context occurs with high probability, furthermore the resulting activation pattern early during training is simple to characterize.

**Lemma 3.9**.: _Suppose that the conditions of Theorem 3.8 hold and that we have a good initialization. Consider an arbitrary \(j[2m]\) and iteration \(2 t_{0}\) occurring before a point has achieved zero loss. Then \(i_{j}^{(t)}\) iff \(i j\)._

Next we bound the activations of the training points just before \(_{0}\), the iteration at which any training points first achieve zero loss. In the following we use \(F_{1},F_{2}\) and \(F_{3}\) as placeholders for expressions depending on the data and model parameters. Here, for the sake of conveying the ideas in the proof we do not write them in full and refer the reader to Supplementary Material.

**Lemma 3.10**.: _Suppose that the conditions of Theorem 3.8 hold and that we have a good initialization, then there is an iteration \(_{1}\) before any point achieves zero loss such that_

\[_{j}^{(_{1})},_{i} }{m}i_{F},i j\] \[_{j}^{(_{1})},_{i} }{m}i_{T},i j\] \[_{j}^{(_{1})},_{i} -}{m}i j\]

Next we seek to ensure the activation patterns remain mostly fixed: in particular, we show \(i_{j}^{(t)}\) if \(i_{T}\) and \(i j\), while \(i_{j}^{(t)}\) if \(i j\).

**Lemma 3.11**.: _Suppose that the conditions of Theorem 3.8 hold and that we have a good initialization. In addition, for \(a,b\) assume there is a time \(t_{0}\) such that \((t_{0},_{i}) a\) for all \(i_{T}\) and \((_{j}^{(t_{0})},_{i}) b\) for all \(i_{F}\) and \(i j\). If \(i_{T}\), \(i j\) implies \(i_{j}^{()}\) and \(i j\) implies \(i_{j}^{()}\) for all \(\) satisfying \(t_{0}<t\), then_

\[B_{j}(t_{0},t) (b+), _{j s}G_{j}(t_{0},t) \]

As before, this update bound is finite and iteration-independent, therefore GD converges provided the assumptions on the activation patterns are not violated. Furthermore, if these activation patterns do hold, then every clean point activates a neuron and no corrupt point activates a neuron of the same label sign. Therefore, under the assumption on the activation pattern, at convergence clean points achieve zero loss while corrupt points have non-zero loss, i.e., they activate no neurons. It therefore suffices to prove the condition on the activation pattern, which we show holds as long as

\[\{}{m},}{m}\} Ck(+) (+1-F_{2}}{m})(+)B_{j}(t_{0},t)\]

As \(C\) does not depend on the parameters, we can ensure this condition holds by letting \(Ck(+)\) be sufficiently small. With \( cn^{-1}\), we show it suffices that \(<ck^{-1}\). Finally, the generalization result follows in a fashion almost identical to that used for Lemma 3.5. 

### Comparison of results

We compare the differing regimes of our results side-by-side with those of Frei et al. (2022); Xu and Gu (2023) in Table 1. We note that comparisons are not like-for-like as Frei et al. (2022) consider smooth, leaky ReLU and logistic loss, Xu and Gu (2023) a generalized family of activation functions, which includes ReLU, and logistic loss, and this paper ReLU and hinge loss. Furthermore, in addition to differences in the noise distribution discussed in Section 2.1, Frei et al. (2022); Xu and Gu (2023) assume a data model where the norm of each data point is approximately proportional to \(\). We therefore re-scale their results in order to make comparison with this work in which all data points have unit norm.

Taken together these results suggest, at least under the type of data model considered, that benign overfitting occurs for signal strengths proportional to between roughly \(1/\) and \(1/n\). Furthermore, our results also suggest that above approximately \(1/n\) one might expect to see a transition to no-overfitting, while below approximately \(1/\) a transition to harmful overfitting. We provide preliminary supporting experiments in the Supplementary Material. We again remark that the latter is non-trivial in our setting as for all \(>0\) the classifier \(h()=(,)\) always has perfect accuracy.

## 4 Conclusion

Developing a theoretical description of benign overfitting in neural networks is a highly nascent area, with mathematical results available only for very limited data models. Furthermore, the conditions describing the transitions between overfitting versus non-overfitting and benign versus non-benign even in these simplified settings are yet to be fully characterized. The goal of this work was to address this issue as well as explore the impact of using the hinge loss. In particular, and admittedly for a simple data model, we prove three different training outcomes, corresponding to non-benign overfitting, benign overfitting and no-overfitting, based on conditions on the margin of the clean data. Our analysis also differs significantly from prior works due to the fact the ratio of loss between different training points can be unbounded and the implicit bias of using hinge loss versus exponentially tailed loss is poorly understood.

Limitations and future work:the key limitation of this work is the restrictiveness of the data model. In particular, as in prior and related works we use a near-orthogonal noise model and assume a rank one signal, we also place additional conditions on the noise distribution. In addition to generalizing the signal and noise model as well as improving the bounds required for our results to hold, we believe the following themes are important areas for future research: first relaxing the near orthogonal noise condition, second exploring data models beyond those which are linearly separable, third investigating the role and impact of depth.

#### Acknowledgments

EG, WS and DN were partially supported by NSF DMS 2011140 and NSF DMS 2108479. EG was also partially supported by NSF DGE 2034835.

    & Frei et al. (2022) & Xu \& Gu (2023) & Theorem 3.1 & Theorem 3.6 & Theorem 3.8 \\  \(n C\) & \(()\) & \(()\) & \(()\) & \(1\) & \(()\) \\ \(m C\) & 1 & \(()\) & \(()\) & \(()\) & \(1\) \\ \( c\) & \(\) & \(\) & \(\) & \(}\) & \(\) \\ \( C\) & \(}\) & \()}{nd}}\) & \()}{d}}\) & \(0\) & \(\) \\ Result & Benign\({}^{3}\) & Benign & Benign & Non-benign & No-overfit \\   

Table 1: across all results \(k cn\) while \(d Cn^{2}(n/)\) for (Frei et al., 2022), Xu & Gu (2023) and Theorem 3.1.