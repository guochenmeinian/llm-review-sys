# Streaming Algorithms and Lower Bounds for

Estimating Correlation Clustering Cost

Sepehr Assadi\({}^{1}\)  Vihan Shah\({}^{1}\)  Chen Wang\({}^{2}\)

\({}^{1}\)University of Waterloo \({}^{2}\)Rutgers University

sepehr@assadi.info

vihan.shah@uwaterloo.ca

wc497@cs.rutgers.edu

###### Abstract

Correlation clustering is a fundamental optimization problem at the intersection of machine learning and theoretical computer science. Motivated by applications to big data processing, recent years have witnessed a flurry of results on this problem in the streaming model. In this model, the algorithm needs to process the input \(n\)-vertex graph by making one or few passes over the stream of its edges and using a limited memory, much smaller than the input size.

All previous work on streaming correlation clustering has focused on semi-streaming algorithms with \((n)\) memory, whereas in this work, we study streaming algorithms with much smaller memory requirements of only \((n)\) bits. This stringent memory requirement is in the same spirit of classical streaming algorithms that instead of recovering a full solution to the problem--which can be prohibitively large with such small memory as is the case in our problem--, aimed to learn certain statistical properties of their inputs. In our case, this translates to determining the "(correlation) clusterability" of input graphs, or more precisely, estimating the cost of the optimal correlation clustering solution.

As our main result, we present two novel algorithms that in only \((n)\) space are able to estimate the optimal correlation clustering cost up to some constant multiplicative factor plus some extra additive error. One of the algorithms outputs a \(3\)-multiplicative approximation plus \(o(n^{2})\) additive approximation, and the other one further reduces the additive error at the cost of increasing the multiplicative factor to some large constant. We then present new lower bounds that justify the mix of both multiplicative and additive error approximations in our algorithms.

## 1 Introduction

Correlation clustering is a fundamental optimization problem at the intersection of machine learning and theoretical computer science. This problem was introduced by the work of 1, with motivation to document clustering, as follows: we have a complete graph \(G=(V,E)\) whose edges are labeled by either \((+)\) or \((-)\), and the objective, known as _disagreement minimization_, is to cluster the vertices so that the number of \((+)\) edges across clusters and \((-)\) edges inside the same clusters are minimized. Correlation clustering has since found broad applications in areas such as document categorization , webpage segmentation , microscopy imaging , and community detection , to name a few.

There is an abundant body of literature studying polynomial time algorithms for correlation clustering.  showed that there exists a \(2.06\)-approximation algorithm in polynomial time andthe problem is NP-hard and even APX-hard. Furthermore,  gave a simple (combinatorial) poly-time \(3\)-approximation algorithm that is widely used in practice. Very recently, breakthrough results by  achieved \(1.73\) approximation in polynomial time. In addition, efficient algorithms are also explored for several variants of this problem like the agreement maximization objective , weighted graphs , fixed number of clusters , fair clustering , and others.

In recent years, with the rapid development of the 'big data era', there has been a growing interest in algorithms for correlation clustering under _sublinear_ models. In general, learning algorithms under sublinear models are able to output the answer without processing or storing the _entire input_. For instance,  gave algorithms to (approximately) learn the clustering in sublinear time, and  designed algorithms in the Massively Parallel Computation (MPC) model. Another widely popular model of sublinear algorithms--the focus of our paper--is the _graph streaming model_. In this model, the edges of the input graph are given to the algorithm one by one in a stream and the memory of the algorithm is desired to be substantially smaller than the input. Here,  designed a streaming algorithm with \((n)\) memory2 that achieves \(3\)-approximation with \(O((n))\) passes - here, \(n\) is the number of vertices of the graph and thus the input size is \((n^{2})\) bits. The number of passes of these algorithms were later improved to \(O(1)\) by  and a single pass by , albeit with much larger, yet still a constant, approximation factor. Very recently,  further improved the approximation ratio of single-pass streaming algorithms to a \(5\)-approximation in polynomial time and \((1+)\)-approximation in exponential-time.

The aforementioned line of work in  focused on the \((n)\)-memory regime, otherwise known as the _semi-streaming_ memory. Allowing for \((n)\) memory in these algorithms is necessary given that even outputting the solution, namely, the clustering of the input labeled graph, requires this much memory. Yet, in many application, \((n)\) memory can still be quite large, and the implementations can require significant resource. In such cases, it is highly desirable to determine the "clusterability" of the input graph _before_ running the actual clustering algorithm. If even the optimal cost for the input is high, it implies the clustering cannot provide any meaningful outcome, and we should not waste resources on these instances. This type of "value estimation" problem is extensively studied in the streaming literature, see e.g.  for several examples in this context. This raises the following fundamental question:

_How well can we **estimate** the optimal correlation clustering **cost** with \((n)\)-space streaming algorithms?_

Despite the vast body of work on streaming correlation clustering in general, this question has received almost no attention so far. Indeed, to the best of our knowledge, the only prior work here is that of  who proved that any (finite) _purely multiplicative_ factor approximation of the cost is not possible in \(o(n)\) space. This result however does _not_ rule out any _additive error_ approximation (say, in the spirit of  for local query algorithms).

We remedy this state of affairs in this paper. Our main algorithmic results show that one can obtain a \(3\)-approximation plus \(o(n^{2})\) additive approximation to the cost of optimal correlation clustering in only \((n)\) space - the additive approximation can be further reduced at the cost of increasing the multiplicative approximation to some large constant. We then complement these results with new streaming lower bounds that further justify the necessity of the additive errors in our algorithms. Throughout, we will present algorithms in the manner of insertion-only streams. However, we shall note that our algorithms can be extended to _dynamic_ streams wherein the edges of the graph can be both inserted and deleted during the stream.

#### Our Contributions

To state our results, we need the following notation. Let \(\) be the optimal cost for correlation clustering. We say an algorithm achieves an \((,)\)-approximation of \(\) if it gives an \(\)-multiplicative with a \(\) additive approximation, namely, outputs a number \(\) such that

\[+.\]Our first main result is a single-pass streaming algorithm that achieves an \((O(1), n^{2})\)-approximation with high probability in \(( n,1/)\) space.

**Result 1.** There is a single-pass streaming algorithm that outputs an \((O(1), n^{2})\)-approximation of the optimal correlation clustering cost with high probability4 and uses \(O((n)/^{5})\) space.

Here, and throughout, with high probability means with probability at least \(1-1/n\).

The \(^{-5}\)-dependence of our algorithm ensures that by setting \(=n^{-0.19}\) we can reduce the additive error down to \(n^{1.81}=o(n^{2})\) and still achieve an \(o(n)\)-space algorithm. On the flip side however, the leading constant multiplicative factor is quite large in this algorithm - as we will see shortly, the algorithm is built on the sparse-dense decomposition idea from , which inevitably incurs a (worst-case) constant of at least \(10^{7}\). Our next result addresses this drawback: we present another algorithm that achieves a \((3, n^{2})\)-approximation in expectation with \(n\) space at the cost of an exponential dependence on \(\) in the space.

**Result 2.** There is a single-pass streaming algorithm that outputs a \((3, n^{2})\)-approximation of the optimal correlation clustering cost in expectation and uses \(2^{O(1/)}(n)\) space.

Compared to the algorithm in Result 1, the algorithm in Result 2 is more suitable for instances whose optimal correlation clustering costs are large (e.g. \(=(n^{2})\)). In such a case, we can pick \(\) to be a small constant, and achieve a \((3+)\)-multiplicative approximation with \((n)\) space.

Since both of our upper bounds contain the \( n^{2}\) additive error, we would naturally wonder to what extent these additive terms are necessary. We present two new lower bounds that partially address this question, and show that the additive errors are necessary to a large extent. Our first lower bound shows that if _only_ additive error is allowed, there is no \((n)\)-space streaming algorithm in a single pass achieves additive error substantially better than \(O(n^{2})\).

**Result 3.** No single-pass streaming algorithm with \((n)\) space can output a \((1,n^{2-})\) approximation of the optimal correlation clustering cost with a sufficiently large constant probability of success for some \(=o(1)\).

Result 3 can also be interpreted as the _additive_ counterpart of the lower bounds in , which focus instead on the purely multiplicative approximation. Since our upper bound allows both approximations, we further provide a second lower bound showing that a \((n)\) additive error is necessary even if both multiplicative and additive errors are allowed.

**Result 4.** No single-pass streaming algorithm with even \(o()\) space can output a \((1.19,O(n))\) approximation of the optimal correlation clustering cost with a sufficiently large constant probability of success.

We now discuss the applicable scenarios for our algorithms. To test instances with low vs. high correlation clustering costs (e.g. \(o(n^{2})\) vs. \((n^{2})\)), it suffices to run our algorithm in Result 1 with \(=1/(n)\) which uses \((1)\) space. On the other hand, to separate instances that are 'partially clusterable' (e.g. optimal cost of \(n^{2}/1000\)) vs. instances that are 'not clusterable at all' (e.g. optimal cost of \(n^{2}/5\)), it suffices to run our algorithm in Result 2 with \(=O(1)\) a small constant which uses \((1)\) space. The only case our algorithms are not able to deal with is to separate multiple 'well-clusterable' instances which does not seem that motivated in practice either.

ExperimentsTo further validate our algorithms, we conduct experiments of our algorithms with the stochastic block model (SBM) extensively studied in the literature (e.g., ). For correlation clustering, we can use a variate of the model that plants clusters with sizes \((n)\) and samples \((+)\) edges between the vertices in the same cluster with a large probability and \((-)\) edges with a low probability, and vice versa. Our implementation on the simulations of graph streams from the Stochastic Block Model shows that our algorithms consistently obtain approximations within a factor of \(3\) for the optimal clustering cost, while only storing \(0.04\% 3.6\%\)3 total edges when the graphs is moderately large. Furthermore, our algorithms are able to distinguish instances that are "well-clusterable" vs. "badly-clusterable" using a very small memory.

We now introduce the notation and the problem definition. For standard technical tools, please see the supplementary material.

Notation.We use \(G=(V,E^{+} E^{-})\) to denote a labeled complete graph arriving in a stream. For any vertex \(v\), we denote by \(N^{+}(v)\) all vertices that have an \((+)\) edge from \(v\), and we let \(N^{+}[v]=\{v\} N^{+}(v)\). Similarly, we denote by \(N^{-}(v)\) all vertices that have an \((-)\) edge from \(v\), and we let \(N^{-}[v]=\{v\} N^{-}(v)\). We use \(N^{+}(u) N^{+}(v)\) (resp. \(N^{-}(u) N^{-}(v)\)) to denote the disjoint neighborhoods of \(u\) and \(v\), i.e. \(N^{+}(u) N^{+}(v)=(N^{+}(u) N^{+}(v))(N^{+}(u) N ^{+}(v))\). For a fixed set of vertices \(A V\), we further let \(E^{+}(v,A)\) be the set of \((+)\) edges between \(v\) and vertices in \(A\), and \(E^{-}(v,A)\) be the set of \((-)\) edges between \(v\) and vertices in \(A\).

We use \(G^{+}=(V,E^{+})\) to denote the positive subgraph of \(G\) with all the \((+)\) edges, and we define \(G^{-}=(V,E^{-})\) analogously. Note that since we work with labelled complete graph, the information of \(G^{-}\) edges can be uniquely inferred from the positive subgraph \(G^{+}\). As such, when we work with \(G^{+}\) only, we call a \((+)\) edge \((u,v) E^{+}\) as an _edge in \(G^{+}\)_. Similarly, we call a \((-)\) edge \((u,v) E^{-}\) an _non-edge in \(G^{+}\)_. We may omit \(G^{+}\) when the context is clear.

For a fixed cluster \(\) on a labeled complete graph \(G\), we use \(\,()\) to denote the total cost of correlation clustering on \(G\) by \(\) (we omit \(G\) since it is obvious by the context). Furthermore, we slightly abuse the notation to reload \(()\) as a function of cost on subgraphs of \(G\) in the following occasions: 1) For an induced subgraph \(H G\), we let \((,H)\) be the cost of \(\) induced by the edges whose both endpoints are in \(H\); 2) For two vertex sets \(A B V\), we let \((,(A,B))\) be the cost of \(\) induced by the edges with exactly one endpoint in \(A\) and one endpoint in \(B\).

Finally, for a single edge \((u,v)\), we use the notation \((,(u,v))\) to denote the cost induced by a single edge \((u,v)\) for clustering \(\). We may write a short-hand notation \(((u,v))\) when the context is clear that \(\) is used.

Problem Definition

We now give the formal description of the problem.

Problem 1 (Correlation Clustering Value Estimation).Given a labeled complete graph \(G=(V,E^{-} E^{+})\) and a clustering \(\) that partitions \(V\) into disjoint set of vertices \(C_{1},C_{2},,C_{k}\), the cost of disagreement minimization correlation clustering on \(\) is defined as

\[():=(u,v) E^{+} i j\; s.t.\;u C_{i},v C_{j}}+(u,v) E^{-}  i\;s.t.\;u,v C_{i}}\]

Let \(\) be the minimum \(\) over all possible clusterings. The Correlation Clustering Value Estimation problem asks for a number \(\) such that \( f()\) for some function \(f(x) x\). If \(f()=+ n^{2}\) for \(\) and \(\), we say that \(\) is a \((,)\)-approximation of the value of correlation clustering in this scenario.

We study algorithms for correlation clustering value estimation under the _graph streaming_ model, where edges arrive one after another in a stream with the labels. We give a more formal definition of the streaming model (and the more general dynamic streaming model) in the supplementary material.

## 3 An Algorithm based on Sparse-dense Decomposition

In this section, we present our first streaming algorithm that achieves a \((O(1), n^{2})\)-approximation for testing the value of correlation clustering in \(O((n)/())\) space, as long as \( 1/(n)\). We utilize the idea in  to test a \((O(1), n^{2})\)-approximate value of the sparse-dense decomposition-based correlation clustering cost, which in turn is an \(O(1)\) approximation of the optimal cost. Our algorithm uses a memory of \(O((n)/())\) bits, which is efficient for large-scale inputs. More formally, the guarantee of our algorithm is as follows.

Theorem 3.1: _There is a (dynamic) streaming algorithm that with high probability gives a \((O(1), n^{2})\)-approximation for the correlation clustering value and take space \(O((n)}{^{5}})\) words._

On a high level, our algorithm for Theorem 3.1 uses the idea to approximate the value of optimal correlation clustering with sparse-dense decomposition (see supplementary material). It is shown in  that once we can find such a decomposition on \(G^{+}\), we can achieve an \(O(1)\)-approximation by simply putting every sparse vertex into a singleton cluster and gathering each almost-clique \(K_{i}\) in a separate cluster. Therefore, an approximation of the number of the aforementioned edges in a _fixed_ decomposition will result in a good estimation of the cost.

However, since the sparse-dense decomposition is not unique, it is unclear how to estimate the edges for a fixed decomposition. On the other hand, an algorithm with \((n)\) space is necessarily oblivious of the decomposition since it takes \((n)\) bits to write it down. To overcome the problem, we forgo the strict notion of the sparse-dense decomposition, and utilize the notion of _\(\)-sparse edges_ and _\(\)-dense non-edges_ (of \(G^{+}\)) instead. We now formally define these notions as follows.

**Definition 1**.: Fix an arbitrary graph \(G=(V,E)\) (which is _not_ necessarily labeled and complete) and a vertex pair \((u,v) G\), we say

1. \((u,v)\) is an \(\)-sparse edge (resp. non-edge) if \((u,v) E\) (resp. \((u,v) E\)) and \(|N(v) N(u)|\{(u),(v)\}\).
2. \((u,v)\) is an \(\)-dense edge (resp. non-edge) if \((u,v) E\) (resp. \((u,v) E\)) and \(|N(v) N(u)|\{(u),(v)\}\).

Note that the definitions of the \(\)-sparse and \(\)-dense edges/non-edges are generic and _not_ restricted to the labeled complete graph (or even the correlation clustering application). Similarly, the sketching tools we design in this section are also generic: we will use them in the context of correlation clustering later. We now prove the existence of the following tools.

**Lemma 3.1**.: _There exists a dynamic streaming algorithm_ **Tool-spr** _with parameters \(,\) that given any graph \(G=(V,E)\) in a stream and a pair of vertices \(u,v V\), satisfying the promise \(^{+}(u) n\) and \(^{+}(v) n\), with high probability outputs 'Yes" if \((u,v)\) is at least \(\)-sparse and "No" if \((u,v)\) is not \(\)-sparse using \(O(1/^{2})\) words of space._

**Lemma 3.2**.: _There exists a dynamic streaming algorithm_ **Tool-dns** _with parameters \(,\) that given any graph \(G=(V,E)\) in a stream and a pair of vertices \(u,v V\), satisfying the promise \(^{+}(u) n\) and \(^{+}(v) n\), with high probability outputs 'Yes" if \((u,v)\) is at most \(\)-dense and "No" if \((u,v)\) is not \(8\)-dense using \(O(1/^{2})\) words of space._

We defer the proofs of Lemmas 3.1 and 3.2 to the supplementary material, and use them as blackboxes in the rest of the paper. We now discuss how to use the \(\)-sparse edges and \(\)-dense non-edges (defined in Definition 1) in \(G^{+}\) to estimate the correlation clustering cost. In what follows, we use \(E^{+}_{}\) to denote the set of \(\)-sparse (positive) edges, and \(E^{-}_{}\) to denote the set of \(\)-dense non-edges. Furthermore, for each vertex \(v\), we use \(E^{-}_{,\,v}\) to denote the set of \(\)-dense non-edges incident on \(v\). We let \(m^{+}_{}:=|E^{+}_{}|\) denote the number of \(\)-sparse edges, \(m^{-}_{}:=|E^{-}_{}|\) denote the number of \(\)-dense non-edges and let \(m^{-}_{,v}:=|E^{-}_{,\,v}|\) denote the number of \(\)-dense non-edges incident on \(v\). Finally, we define \(^{-}_{}\) as follows.

**Definition 2**.: For each vertex \(v\) define \(^{-}_{,v}:=\{m^{-}_{,v},^{+}(v)\}\). Furthermore, let \(^{-}_{}:=_{v V}^{-}_{ ,v}\).

The intuition behind \(^{-}_{,v}\) is to count the non-edges (of \(G^{+}\)) in \(E^{-}_{,\,v}\) for _at most_\(^{+}(v)\)_times_. Our estimator for \(\)-dense non-edges will estimate \(^{-}_{}\) instead of \(m^{-}_{}\) for the following reason: it suffices to estimate \(^{-}_{}\) since the number of non-edges inside each almost-clique is at most \(^{+}(v)\); on the other hand, if we estimate _all_\(\)-dense non-edges, there could be a very large overhead since the non-edges _between_ almost-cliques are also counted. We note that if \(^{-}_{,v}=m^{-}_{,v}\) for all \(v V\) then \(^{-}_{}\) is twice \(m^{-}_{}\) (since we are double counting edges). But this is a \(2\)-approximation in the worst case, and we do this to make calculations easier.

We prove the following lemmas that establish the connections between the aforementioned sets of edges and the edges from the sparse-dense decomposition. Due to space limits, we directly present the properties of our estimators, and defer the proof to the supplementary material.

**Lemma 3.3**.: _Suppose \(G=(V,E)\) is any labeled graph and \(V=V_{} K_{1} K_{k}\) is an \(\)-sparse-dense decomposition of \(G^{+}\) for \(0 1/360\) and \(_{0} 1/20\). Let \(C_{}\) be the cost of correlation clustering when the clusters are the almost cliques and sparse vertices are in singleton clusters. Then we have \(} m^{+}_{_{0}}+m^{+}_{}+}} C_{} \)._

We upper bound the cost of \(m^{+}_{}\) and \(^{-}_{}\) by a function of \(\) using a charging argument.

**Lemma 3.4**.: _Suppose \(G=(V,E)\) is any labeled graph and \(\) be the optimal correlation clustering cost, and let \(\) and \(\), then \(m^{+}_{}\) and \(^{-}_{} 8\)._

By Lemmas 3.3 and 3.4 if we are able to exactly recover \(m^{+}_{}\) and \(^{-}_{}\) then we would get an \(O(1)\) approximation of \(\). Such a task seems difficult, however, using Lemmas 3.1 and 3.2 we design our tools for estimating \(m^{+}_{}\) and \(^{-}_{}\) with an additive error.

**Lemma 3.5**.: _There exists a dynamic streaming algorithm with parameters \(,\) that given any graph \(G=(V,E)\) in a stream returns a value \(Z_{sp}\) that is at least \(m^{+}_{}\) and at most \(m^{+}_{/8}+ n^{2}\) with high probability and uses \(O( n/^{2}^{3})\) words of space._

**Lemma 3.6**.: _There exists a dynamic streaming algorithm with parameters \(,\) that given any graph \(G=(V,E)\) in a stream returns a value \(Z_{den}\) that is at least \(^{-}_{}\) and at most \(^{-}_{}+ n^{2}\) with high probability and uses \(O(^{2}n/^{2}^{5})\) words of space._

Finalizing the proof sketch of Theorem 1.We are given a parameter \(\) as input and we want the additive error to be at most \( n^{2}\). We fix \(=1/360\), \(_{0}=1/20\) and \(^{}=(2+})^{-1}\).

Run the algorithm in Lemma 3.5 with parameters \(\) and \(^{}\), and let the output be \(Z_{sp}^{}\). Also, run the algorithm in Lemma 3.5 with parameters \(_{0}\) and \(^{}\), and let the output be \(Z_{sp}^{_{0}}\). Run the algorithm in Lemma 3.6 with parameters \(4\) and \(^{}\), and let the output be \(Z_{den}\).

Lemmas 3.3 to 3.6 imply that the cost \(Z_{CC}\) we output satisfies:

\[ Z_{CC}:=Z_{sp}^{}+(2/_{0}) Z _{sp}^{_{0}}+Z_{den} O(1)+ n^{2}.\]

The space taken is \(O( n/^{3})\) for both copies of Lemma 3.5 and \(O(^{2}n/^{5})\) for Lemma 3.6 giving a total space of \(O(^{2}n/^{5})\) words. This proves Theorem 1.

## 4 An Algorithm based on Pivot

In this section, we give our second streaming algorithm that is a \((3+, n^{2})\)-approximation for the correlation clustering value for any choice of \(\) and \(<1/2\). The algorithm works in \(O((n))\) space as long as \(()\). Consider the following formal statement:

**Theorem 2**.: _There is a (dynamic) streaming algorithm that with high probability gives a \((3+, n^{2})\)-approximation for the correlation clustering value and takes space \(O(^{2}n}{^{5}})\) words._

Our algorithm in Theorem 2 is inspired by the Local Cluster algorithm from . The Local Cluster algorithm samples \(1/\) random vertices in a set \(U\) and computes the greedy maximal independent set (MIS) \(M\) of \(U\) to get the cluster centers \(p_{1},p_{2},,p_{t}\). The clusters generated then are \(N^{+}[p_{i}]-_{j=1}^{i-1}N^{+}[p_{j}]\) and all the remaining vertices (called unclustered vertices) are clustered in their own singleton cluster.  proved that the expected cost of this clustering is at most \(3+n^{2}\). More formally, they showed the following:

**Proposition 4.1** ().: _The expected cost of Local Cluster is at most \(3+n^{2}\)._

To simulate the Local Cluster algorithm in a streaming manner, while forming a cluster we need to know which of its neighbors are already clustered in previous clusters which requires new sketching tools (on \(G^{+}\)) that are different from the ones we used in Section 3. These tools estimate the number of non-edges **within** or the number of edges **going out of** the neighborhood of a vertex (\(u\)) outside of the \((+)\) neighborhood of a known set of vertices (\(S\)). In other words, fix a graph \(G=(V,E)\), a vertex \(u\), and a set \(S\), we want to estimate the number of non-edges within or the number of edges going out of \(N[u]-N[S]\). We again use the generic form to present the sketching tools and do not specify \((+)\) edges. The formal definitions can be given as follows.

**Definition 3**.: A non-edge \((x,y)\) is **within**\(N[u]-N[S]\) iff: \(i\)) \(x N[S]\) and \(ii\)) \(y N[S]\) and \(iii\)) \(x N[u]\) and \(y N[u]\).

**Definition 4**.: An edge \((x,y)\) is **going out of**\(N[u]-N[S]\) iff \(i\)) \(x N[S]\) and \(ii\)) \(y N[S]\) and \(iii\)) \(x N[u]\) or \(y N[u]\) but not both.

**Definition 5**.: An edge \((x,y)\) is **unclustered** w.r.t. \(S\) iff \(i\)) \(x N[S]\) and \(ii\)) \(y N[S]\).

We also define \(m_{ne}(u,S)\) as the number of non-edges **within**\(N[u]-N[S]\), \(m_{e}(u,S)\) as the number of edges **going out of**\(N[u]-N[S]\), and \(m_{u}(S)\) as the number of **unclustered** edges w.r.t. \(S\). We write \(m_{ne},m_{e}\), and \(m_{u}\) when \(u\) and \(S\) are clear from context.

Exactly recovering \(m_{u},m_{ne}\) and \(m_{e}\) is difficult, so we have the following lemma about estimating the number of non-edges **within**\(N[u]-N[S]\), the edges **going out of**\(N[u]-N[S]\), and the **unclustered** edges with respect to \(S\):

**Lemma 4.2**.: _There exist streaming algorithms called NE-Tool, E-Tool, and U-Tool that given \(u\) and \(S\) before the stream, respectively compute with high probability \(i)\). the number of non-edges **within**; \(ii)\). the number of edges **going out of**\(N[u]-N[S]\); and \(iii)\). the number of edges with **unclustered** w.r.t. \(S\) with an overestimation of at most \( n^{2}\) and take space \(O(1/^{2})\) words._

Using the tools we have built so far, we present a \((3, n^{2})\) approximation algorithm in expectation.

**Algorithm 1**.: Simulation of the Local Cluster algorithm

**Input:**\(G=(V,E^{+} E^{-})\) in a (dynamic) stream

**Output:**\((3, n^{2})\)-approximation to the correlation clustering value in expectation

**Pre-Processing:**

1. Sample a set \(U\) of \(1/\) random vertices
2. Let \(\) be a random permutation of the vertices in \(U\)

**During the Stream:**

1. Store \((+)\) all edges between vertices of \(U\)
2. For all \(u U\) and \(S U\) compute \((u,S),(u,S),(S)\) with parameter \(^{2}/6\) using the \(G^{+}\) subgraph.

**Post-Processing:**

1. Compute the greedy MIS \(M:=p_{1},p_{2},,p_{t}\) of \(U\) in the order of \(\) (using edges stored between vertices of \(U\)).
2. \(S_{0}=\). \(=0\).
3. For \(i=1\) to \(t\): * \(=+(p_{i},S_{i-1})+(p_{i},S_{ i-1})\). * \(S_{i}:=S_{i-1}\{p_{i}\}\)
4. Output \(Z:=+(M)\)

Algorithm 1 estimates \(m_{u},m_{ne}\) and \(m_{e}\) with at most \( n^{2}\) additive error. We now show that with the exact values of \(m_{u},m_{ne}\) and \(m_{e}\), we get the exact clustering value of the Local Cluster algorithm.

**Claim 4.3**.: _The clustering value of Local Cluster is equal to \(m_{u}+_{i}m_{ne}(p_{i},S_{i-1})+m_{e}(p_{i},S_{i-1})\)._

Combining Lemma 4.2 and Claim 4.3 gives us the following performance guarantees for Algorithm 1.

**Lemma 4.4**.: _The output \(Z\) of Algorithm 1 is at least \(\) with high probability and is at most \(3+ n^{2}\) in expectation._

Proof.: Using Claim 4.3 we know that if the tools worked with no error, Algorithm 1 would give the exact clustering cost of Local Cluster. Also, we know that the tools do not underestimate and overestimate by \(^{2}n^{2}/6\) with high probability (Lemma 4.2). We first note that \(Z\) is at least \(\) because of the above conditioning and the fact that the clustering cost of Local Cluster is at least \(\). Using Proposition 4.1 we know that the expected cost of the clustering when choosing \(\) random pivots is at most \(3+n^{2}\). Therefore, the clustering cost of Algorithm 1 is between \(\) and \(3+n^{2}\) plus the overestimate.

We now calculate the overestimate. We use parameter \(^{2}/6\) for the tools thus the additive error in each tool is at most \(^{2}n^{2}/6\). There are \(1/\) pivots implying a total additive error of at most \( n^{2}/3\) over all the copies of NE-Tool and E-Tool. U-Tool has an error of at most \(^{2}n^{2}/6\) implying that the overall error is at most \( n^{2}/2+ n^{2}/3+^{2}n^{2}/6 n^{2}\) giving a \((3, n^{2})\) approximation in expectation.

We note that we condition on the high probability events for all copies of the tools and union bound over the failure probabilities. Thus, for the overall failure probability to be small we need \(2^{1/}(n)\). This requirement is trivially satisfied by the fact that \(2^{1/}\) is on the space bound (Lemma 4.5), and the space bound for any streaming algorithm is at most \(O(n^{2})\). 

We now show the space bound of Algorithm 1.

**Lemma 4.5**.: _The space of Algorithm 1 is \(O( n}{^{5}})\) words._

Proof.: Each copy of the tools with parameter \(^{2}/6\) takes \(O( n/^{4})\) words of space, and we compute the tools for all \(v U\) and \(S U\). Thus, the space used is \(O(2^{1/} n/^{5})\) words. Storing edges between vertices in \(U\) takes space at most \(1/^{2}\) words which is a lower order term. 

Lemmas 4.4 and 4.5 together give us a \((3, n^{2})\) approximation in expectation using \(O( n}{^{5}})\) space. We now prove Theorem 2. We run Algorithm 1\(\) times in parallel and let \(Z_{min}\) be the minimum cost over all iterations. \(Z_{min}\) is a \((3+, n^{2})\)-approximation with high probability.

**Claim 4.6**.: \( Z_{min}(3+)+ n^{2}\) _with high probability._

Thus, we get a \((3+, n^{2})\)-approximation with high probability. Each parallel repetition of the algorithm takes \(O( n}{^{5}})\) words of space. Repeating \(O()\) times and re-scaling \(\) by a factor of \(\) gives a total space bound of \(O(^{2}n}{^{5}})\) words. This completes the proof of Theorem 2.

## 5 A Lower Bound for \(O(n^{2-})\) Additive error

In this section, we show that if we _only_ allow additive error, any streaming algorithm with polylogarithm memory cannot cross an error barrier of \((n^{2-})\) for some \(=o(1)\). Here, and throughout, we will refer this lower bound as the _almost-quadratic lower bound_. The lower bound is weaker than the linear lower bound of Section 6 in terms of the multiplicative factor since it only works for \(c=1\). However, it is much stronger in the additive sense: the upper bounds obtained by our algorithms are \(O(n^{2})\), and the almost-quadratic lower bound matches this term up to an \(O(n^{})\) factor - this provides a strong justification of the additive error in our algorithms.

The formal statement of the almost-quadratic lower bound is as follows.

**Theorem 3**.: _There exists a constant \(C\), such that any single-pass streaming algorithm that estimates the optimal value \(\) of correlation clustering by a \(C n^{2-}\) purely additive error (i.e., an estimated value that is at most \((+C n^{2-})\)) with probability at least \(\) has to use a memory of \((n^{})\) bits, even on labeled complete graphs._

Note that Theorem 3 does _not_ require the stream to be dynamic, which is in contrast to our upper bound results that work for dynamic streams. We obtain the almost-quadratic lower bound by a new reduction from the INDEX problem. On the high level, the instance we construct 'hides' and \((n^{2-})\) gap between the yes and no cases inside a case-invariant \((n^{2})\) cost. The reduction can be viewed as a more involved variant of the space lower bound for the _exact_ correlation clustering in a very recent work . In a nutshell, we modify their construction to 'boost' the gap between yes and no cases, and apply a new trick to separate the _values_ of clustering. Due to space limits, we defer the proof of Theorem 3 to the supplementary material.

A Lower Bound for \(O(n)\) Additive error

In this section, we show that any dynamic streaming algorithm that gets a \((c, n)\)-approximation for \(c<1.2\) and \(O(n)\) additive error needs \(()\) bits of space. Here, and throughout, we will call this lower bound the _linear lower bound_. Formally, we have:

**Theorem 4**.: _Let \(c[1,)\) and \((0,-c)\). Any single-pass streaming algorithm that estimates the optimal value \(\) of correlation clustering by a \((c, n)\)-approximation (i.e., an estimated value that is at most \((c+ n)\)) with probability at least \(\) has to use a memory of \(()\) bits, even on labeled (complete) graphs._

Similar to the case in Theorem 3, Theorem 4 does _not_ require the stream to be dynamic. Comparing with the _almost-quadratic lower bound_ we show in Section 5, the lower bound in Theorem 4 is weaker in the additive sense. However, it allows the multiplicative approximation of the algorithm to be \(>1\), while the lower bound in Section 5 only rules out algorithms with purely additive errors.

Our lower bound uses the celebrated machinery from Boolean Hidden Hypermatching (BHH) and Gap Cycle Counting (GCC) pioneered by . The Gap Cycle Counting (GCC) lower bound states that for any algorithm to distinguish whether a graph consists of cycles with length \(2t\) or cycles with length \(4t\) for some \(t 2\), a memory of \((n^{1-})\) bits is necessary. On the high level, our plan is to show that for graphs similar to the ones prescribed in the GCC problem, the values of correlation clustering are different by an additive gap of \(O(n)\). Therefore, by a reduction argument, any algorithm that breaks this barrier of additive gap requires \(((n))\) memory.

Due to space limits, we defer the lower bound construction to the supplementary material.

## 7 Experiments

We describe in this section the experiments of algorithms on the Stochastic Block Model and the Erdos-Renyi random graphs. These experiments show that for a very natural family of graphs, our algorithms can achieve a very competitive performance on the estimated cost values, and the performances are often much better than the worst-case theoretical analysis. Furthermore, our algorithms are capable of separating "well-clusterable" vs. "badly-clusterable" instances.

Experiment SettingsLimited by space, we sketch the key settings of our experiment, and defer the full discussion to the full version in the supplementary material. As we have discussed, we perform our experiments on the data generated from the well-studied Stochastic Block Model (SBM) that plants ground-truth clusters with sizes \((n)\), samples \((+)\) edge between vertex pairs \((x_{i},x_{j})\) in the same planted cluster with probability \(p>0.5\), and samples \((+)\) edges between vertex pairs \((x_{i},x_{j})\) in different clusters with probability \(1-p\). The SBM captures a lot of real-world scenarios, including social networks , community detection , graph clustering , and Bioinformatics , to name a few.

We test SBM instances that are reasonably "clusterable", i.e., we set \(p=0.8\) with vertices \(n=500\), \(n=1000\), and \(n=2000\). Furthermore, we compare the costs estimated by algorithms for the SBM and the Erdos-Renyi random graphs \(G(n,p)\) with \(p=0.5\). In this regime, the Erdos-Renyi graph does not appear to have any clustering property, and the cost is high. Under the \(n=1000\) setting, we tested whether our algorithms can distinguish the costs between "good" (SBM with \(p=0.95\)) and "bad" (Erdos-Renyi with \(p=0.5\)) instances - a property that can be extremely useful in practice.

We implement our algorithms based on our descriptions in Section 3 and Section 4 with some relaxation of parameters. Notably, for the pivot-based algorithm, we slightly relax the requirement to allow 2-pass over the stream; as such, we can use the first pass to perform greedy MIS on the sampled vertex set, so we do not have to pay the \(2^{O(1/)}\) factor in the space.

We evaluate the performances mainly based on two metrics: the multiplicative factor of the cost estimation (which we call the "competitive ratio") and the fraction of the edges used. To overcome the possible effects of random seed, we fix our random seed from \(0\) to \(14\), and run 15 experiments. We include the error bars and the curves of the competitive ratios and fraction of the edges. For the experiment to distinguish SBM and Erdos-Renyi graphs, we simply plot the two types of costs w.r.t. experiment runs, and give the distributions of the costs. All the experiments were conducted on two machbooks and public Colab clusters.

Experimental ResultsWe first show that our algorithms are insensitive to the choice of parameters - this property is evident by the figures we used in the full version in the supplementary material. As such, we focus on the settings with \(=0.01\) for the SDD-based algorithm and \(=0.1\) for the pivot-based algorithm for the rest of this section.

Due to the space limit, we only discuss our algorithm based on sparse dense decomposition (SDD). The left 3 plots in Figure 1 give the plots of the cost estimation for the SDD-based algorithm on \(n=2000\). The figures show that the approximation factor for this algorithm is roughly between \(1\) and \(2\), our algorithm consistently uses less than 4% of the edges in the graph.

The results for our SDD-based algorithm to distinguish between SBM instances with \(p=0.95\) and ER instances with \(p=0.5\) with the SDD-based algorithm is shown in the right two plots of Figure 1. From the figure, it can be observed that the the SDD-based algorithm outputs drastically different clustering costs of the SBM instances vs. the Erdos-Renyi instances. We can observe from the left plot that the supports of the costs are _disjoint_. As such, by a simple linear threshold, our SDD algorithm is able to perfectly distinguish between both types of instances while using less than 10% of the edges (\(n=1000\) case).