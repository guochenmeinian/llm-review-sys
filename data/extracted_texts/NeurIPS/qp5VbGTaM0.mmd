# On Softmax Direct Preference Optimization for Recommendation

Yuxin Chen\({}^{1}\)1 Junfei Tan\({}^{2}\)1 An Zhang\({}^{1}\)2

Zhengyi Yang\({}^{2}\) Leheng Sheng\({}^{1}\) Enzhi Zhang\({}^{3}\)

Xiang Wang\({}^{2}\) Tat-Seng Chua\({}^{1}\)

\({}^{1}\)National University of Singapore

\({}^{2}\)University of Science and Technology of China

\({}^{3}\)Hokkaido University

yuxin.chen@u.nus.edu, sober_clever@mail.ustc.edu.cn,anzhang@u.nus.edu

leheng.sheng@u.nus.edu, enzhi.zhang.n6@elms.hokudai.ac.jp

yangzhy1998@gmail.com, xiangwang1223@gmail.com, dcscts@nus.edu.sg

###### Abstract

Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (**S-DPO**) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, which is extended from the traditional full-ranking Plackett-Luce (PL) model to partial rankings and connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has an inherent benefit of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while providing better rewards for preferred items. Our codes are available at https://github.com/chenyuxin1999/S-DPO.

## 1 Introduction

Recommender systems aim to predict personalized rankings based on user preference data, _i.e.,_ historical interactions such as purchases, clicks, and ratings . Recently, leveraging the extensive world knowledge and powerful reasoning abilities of language models (LMs) , LM-based recommenders have been broadly explored . These recommenders convert historical interaction data into language prompts and either perform in-context learning or fine-tune LMs, demonstrating notableadvantages, including zero-shot and few-shot reasoning , enhanced generalization abilities , and rich semantic understanding . However, current LM-based recommenders typically utilize language modeling loss for personalized ranking objectives--predicting the next token--which significantly differs from the objective of modeling user preferences in recommendation tasks .

We argue that the current objective of LM-based recommenders does not fully utilize preference data and is not optimized for personalized ranking tasks, thereby hindering recommendation performance. Most LM-based recommenders address recommendation tasks by leveraging specialized language prompts , incorporating collaborative signals as a new modality , or extending the vocabulary of LMs with item tokens . Typically, these recommenders pair each language prompt, including the user's historical interaction item lists, with a single positive item and then update LM parameters using language modeling loss . Despite being designed for recommendation tasks, these LM-based recommenders do not consider negative items and are not directly optimized for personalized rankings. Such a training paradigm fails to fully leverage user preference data and overlooks the role of negative items in recommendations, thereby impeding the alignment of LMs with user preferences.

Inspired by the success of using human-labeled data to align LMs with human preferences  and advancements in direct preference optimization (DPO) , we make progress on aligning LMs with recommendations by fine-tuning them to predict the next item in accordance with the user's preference. This preference alignment stage aims to instill ranking information into the LMs and help recommenders distinguish preferred items from negatives, rather than solely focus on positives.

Towards this end, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for recommendation, connected to softmax sampling strategies , which we call **S-DPO**. Specifically, we first devise supervised fine-tuning to inject domain knowledge and improve LM's ability to follow the instructions before preference alignment phase, following . In the preference alignment stage, instead of constructing solely positive pairs, we initially pair each language prompt with both positive and randomly sampled multiple negatives to build text-based preference data. Building upon these preference data, we extend conventional DPO with the Bradley-Terry preference model  on pairwise data to the Plackett-Luce preference model , which handles relative rankings among multiple samples. Furthermore, we generalize the traditional Plackett-Luce preference model, which is designed for full relative rankings, to accommodate partial rankings, a more natural fit for recommendation tasks.

Benefiting from the multiple negatives in preference data, S-DPO offers three appealing properties. On the one hand, S-DPO serves as the first specialized personalized ranking loss for LM-based recommenders, effectively utilizing multiple negatives and acknowledging the importance of preference data. Empirically, we demonstrate that it provides more effective ranking gradients and better rewards for preferred items compared with DPO (_cf._ Section 4.2). On the other hand, we theoretically bridge the DPO loss with the pairwise BPR loss  over pairwise data and connect S-DPO with the softmax loss over negative sampling (also known as contrastive loss in self-supervised recommendations, which achieves state-of-the-art performance ). This connection naturally underscores the ranking performance of S-DPO and highlights the critical role of multiple negatives. Furthermore, gradient analysis demonstrates that S-DPO has an inherent benefit of mining hard negative examples similar to contrastive learning paradigm , which not only boosts the performance but also accelerates the training process (_cf._ Section 3.1), assuring its exceptional capabilities in recommendation tasks.

Overall, our contributions can be concluded as follows:

* We are among the first to point out that the widely used language modeling loss in LM-based recommendation is not designed for ranking tasks and fails to fully utilize user preference data, thereby hindering recommendation performance.
* We propose S-DPO, an alternative version of DPO loss extended from the traditional Plackett-Luce preference model, incorporating multiple negatives to instill ranking information into LM and tailoring for LM-based recommenders.
* We theoretically bridge S-DPO with the softmax loss over negative sampling to highlight the critical role of multiple negatives and find its inherent benefit of mining hard negatives, assuring its capabilities.

## 2 Preliminary

In this section, we first formalize sequential recommendation as the task of aligning language models (LMs) with user preferences. Then, we discuss the general framework of current LM-based recommenders that utilizes language modeling loss to fine-tune LMs. Finally, we outline the training process widely used to align LMs with human preferences, including reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO).

Task Formulation.Given the historical interactions \(_{u}\) of one user \(u\) in chronological order, the goal of LM-based sequential recommender \(_{}\), where \(\) represents trainable parameters, is to select the item \(i_{p}\) preferred by user \(u\) from candidate set \(C=\{i_{j}\}_{j=1}^{N}\), where \(N\) is the number of candidates. This task requires that item \(i_{p}\) be preferred over the other candidate items, denoted by \(_{d}=C\{i_{p}\}\). This requirement explicitly defines a multi-negative preference understanding for LM-based recommenders, which can be formulated as follows:

\[ i_{d}_{d}, i_{p}_{u}i_{d},\] (1)

wherein \(_{u}\) stands for the preference of user \(u\).

Fine-tuning LM-based recommenders.Current LM-based recommenders widely adopt supervised fine-tuning (SFT)  on recommendation-specific data to enhance their performance [7; 9]. Generally, this involves two steps: structuring recommendation data as text-based pairs and then fine-tuning LMs based on these pairs. In the first step, for user \(u\), a recommendation task prompt \(x_{u}\) encompasses the user's historical interactions \(_{u}\), the candidate item set \(C\), and a description of the sequential recommendation task. This prompt \(x_{u}\) is paired with the title of the preferred item \(i_{p}\) in the candidate set \(C\), denoted as \(e_{p}\), to form the pair data \((x_{u},e_{p})\). In the second step, the \((x_{u},e_{p})\) pairs are utilized to fine-tune the LM-based recommender \(_{}\) through language modeling loss. This loss, commonly used in SFT in language modeling tasks, implicitly treats the recommendation task as predicting the next token based on preceding tokens. Formally, the objective of optimizing the LM-based recommender \(_{}\) with pair data \((x_{u},e_{p})\) can be formulated as:

\[_{}_{(x_{u},e_{p})}_{t=1}^{|e_{p}|}(P_{}((e_{p})_{ t}|x_{u},{(e_{p})}_{<t}),\] (2)

where \(|e_{p}|\) is the number of tokens in \(e_{p}\), \((e_{p})_{t}\) is the \(t\)-th token of \(e_{p}\) and \((e_{p})_{<t}\) is the tokens preceding \((e_{p})_{t}\).

However, recommendation tasks are essentially user preference alignment tasks, as formalized in the above task formulation, and differ from language modeling tasks that consider only positive responses. Such a gap necessitates further exploration into aligning LM-based recommenders with user preference, an area that has been underexplored.

RLHF pipeline and DPO.Recent studies in natural language processing (NLP) explore the use of human-labeled pairwise data as a reward signal to align LMs with human preferences, such as RLHF  and DPO . Specifically, the RLHF  pipeline adds two additional phases after the SFT phase: reward model training and reinforcement learning (RL) optimization. After obtaining the SFT model \(^{}\), RLHF further optimizes it with pairwise preference data.

Figure 1: Framework of S-DPO. Different from existing methods which fine-tune LMs with a language modeling loss without tailoring for recommendations, S-DPO proposes to explicitly instill ranking information into LMs. To take one step further, S-DPO incorporates multiple negatives in user preference data and generalizes pairwise DPO loss to softmax ranking loss.

Inspired by the success of RLHF in NLP, we leverage RLHF to inject recommendation-specific user pairwise preference into LM-based recommenders. Let \(=\{e_{j}\}_{j=1}^{N}\) denote the title set of candidate items, where \(e_{j}\) denotes the title of item \(i_{j}\). Given two items \(i_{j},i_{k}\), the user preference \(i_{j}>_{u}i_{k}\) can be seamlessly transformed into a response preference, stipulating that \(e_{j}\) is preferred over \(e_{k}\) when presented with prompt \(x_{u}\), denoted as \(e_{j} e_{k}|x_{u}\). By sampling one misperferred item \(i_{d}\) from misperferred candidate set \(_{d}\), we can curate a preference dataset \(\{(e_{p},e_{d},x_{u})\}\).

After that, RLHF utilizes a preference model for preference distribution modeling, such as Bradley-Terry (BT) model . This preference model assumes there is a latent function \(r(x_{u},e_{j})\) representing the reward of prompt-response pair \((x_{u},e_{j})\). The bigger reward \(r(x_{u},e_{j})\) means the more user \(u\) prefers item \(i\). From this perspective, reward function \(r(x_{u},e_{j})\) serves as a scoring function that quantifies the preference of user \(u\) to item \(i\). Besides, the preference model defines a mapping from the reward function \(r(x_{u},e_{j})\) to a preference distribution \(p_{r}(e_{j} e_{k}|x_{u})\). Based on preference distribution, an optimal reward function is trained by maximizing the likelihood of preference data. The training objective of this phase is as follows:

\[_{}=-_{(x_{u},e_{p},e_{d})}[ p_{r}(e_{p}  e_{d}|x_{u})].\] (3)

Let \(_{}(e|x_{u})\) be the probability that LM-based recommender \(_{}\) output title \(e\) given prompt \(x_{u}\). The final reinforcement learning phase aims to maximize the expected reward of policy while not deviate too far from the reference model, formulating the following objective for optimal policy:

\[_{_{}}_{x_{u},e_{ }(e|x_{u})}[r(x_{u},e)]-_{}[_{}(e|x_{ u})||_{}(e|x_{u})],\] (4)

where \(\) denotes the distribution of \(x_{u}\) and \(_{}=^{}\).

A recent study, DPO , theoretically proves the optimal policy in a closed form to Eq.(4) is

\[^{*}(e|x_{u})=)}_{}(e|x_{u}) (r(x_{u},e)),\] (5)

which is equivalent to

\[r(x_{u},e)=)}{_{}(e|x_{ u})}+ Z(x_{u}),\] (6)

where \(Z(x_{u})=_{e}_{}(e|x_{u})(r (x_{u},e))\) is the partition function.

By defining \(p_{r}(e_{p} e_{d}|x_{u})\) as \((r(x_{u},e_{p})-r(x_{u},e_{d}))\) in Eq.(3) according to the BT model used in RLHF and substituting term \(r(x_{u},e)\) in Eq.(3) with Eq.(6), the last two phases of RLHF pipeline can be equivalently transformed into optimizing DPO loss below:

\[_{}=-_{(x_{u},e_{p},e_{d})}[ ((e_{p}|x_{u})}{_{}(e_{p}|x_{u })}-(e_{d}|x_{u})}{_{}(e_{d}|x_{u})} )],\] (7)

wherein \((x)\) is the sigmoid function.

DPO is able to directly extract the optimal policy from pairwise preference data, making it more practical for preference alignment than RLHF. Nevertheless, DPO and RLHF are usually designed for pairwise preference. The oversight of other negative items impedes the performance of the LM-based recommenders. To bridge the gap, we expand DPO to S-DPO in recommendation tasks, in consideration of multiple negative items.

## 3 Methodology

### Derivation of S-DPO loss

To align LM-based recommender \(_{}\) with multi-negative preference, we first derive the preference distribution and then propose a new loss function called S-DPO (depicted in Figure 1).

Multi-negative Preference Distribution.As mentioned in Section 2, for user \(u\), there is a partial ranking stipulating \(i_{p}_{u}i_{d},\,i_{d}_{d}\) in sequential recommendation tasks. Let \(_{d}\) be the titles of dispreferred items \(_{d}\). The aforementioned partial ranking is equivalent to \(e_{p} e_{d}|x_{u}, e_{d}_{d}\), from which a multi-negative preference dataset \(\{x_{u},e_{p},_{d}\}\) can be curated in an analogous way to RLHF.

For the dataset pairing one preferred item with multiple dispreferred items, we leverage the Plackett-Luce (PL) model  to build preference distribution. Given prompt \(x_{u}\), \(K\) titles \(e_{1},e_{2},,e_{K}\) and a permutation \(:[K][K]\) reflecting the user preference, with \((j)\) denoting the \(j\)-th element of permutation \(\), the PL model estimates that the ranking \(e_{(1)},e_{(2)},,e_{(K)}\) turns out true, as:

\[p(|e_{1},e_{2},,e_{K},x_{u})=_{j=1}^{K},e_{(j)}))}{_{l=j}^{K}(r(x_{u},e _{(l)}))}.\] (8)

By enumerating all the permutations starting with \(p\) and calculating sum of their probability given by the PL model, the final multi-negative preference distribution \(p^{*}\) can be derived as:

\[p^{*}(e_{p} e_{d}, e_{d}_{d}|x_{u})= ,e_{p}))}{_{j=1}^{K}(r(x_{u},e_{j}))}.\] (9)

For brevity, the complete derivation is delegated to Appendix A.1.

Deriving S-DPO.By substituting reward function \(r(x_{u},e)\) in Eq.(9) with Eq.(6), the multi-negative preference distribution can be rewritten as:

\[p^{*}(e_{p} e_{d},\,e_{d}_{d}|x_{u})= _{d}}(|x_ {u})}{_{}(e_{d}|x_{u})}-|x_{u})}{_{ }(e_{p}|x_{u})})}.\] (10)

Through plugging distribution given by Eq.(10) in the reward learning objective in Eq.(3), our S-DPO loss can be formulated for policy \(_{}\) as:

\[_{}(_{};_{})=- _{(x_{u},e_{p},_{d})}[ (-\,_{e_{d}_{d}}((e_{d}|x_{u})}{_{}(e_{d}|x_{u})}-(e_{p}|x_{u})}{_{}(e_{p}|x_{u})})) ].\] (11)

Notably, when the number of candidates \(N\) is 2, which means there is only one dispreferred item, S-DPO reduces to DPO. The proof is provided in Appendix A.2.

Gradient Analysis.We conduct gradient analysis on S-DPO. The gradient of \(_{}\) with respect to parameters \(\) takes the following formulation:

\[_{}_{}(_{};_{ })=\] \[-_{(x_{u},e_{p},_{d})} {(_{e_{d}_{d}}(g(e_{d},e_{p},x_{u})) )}_{}_{ }_{}(e_{p}|x_{u})-_{e_{d}_{d}} _{}(e_{d}|x_{u})}_{},\]

wherein \(g(e_{j},e_{k},x_{u})=r_{}(x_{u},e_{j})-r_{}(x_{u},e_{k})\) and similar to DPO, \(r_{}(x_{u},e)=(e|x_{u})}{_{}(e|x_{u})}\) is the implicit reward function defined by \(_{}\). See Appendix A.3 for a complete derivation.

Recap the DPO gradient below:

\[_{}_{}(_{};_{})=- _{(x_{u},e_{p},e_{d})},e_{p},x_ {u}))}_{}[_{}_{ }(e_{p}|x_{u})-_{}_{}(e_{d}|x_{u})].\]

Similar to DPO, the gradient of S-DPO loss increases the likelihood of the preferred item and decreases the likelihood of all the dispreferred items. Each example is also weighed by how much the implicit reward \(r(x_{u},e)\) deviates from the preference data. However, compared with DPO, S-DPO harnesses information of multiple dispreferred items in this weight.

Moreover, S-DPO treats gradients of different negative (dispreferred) items differently by assigning the gradient of each negative item with an extra weight \(_{d}}(g(e^{}_{d},e_{d},x_{u}) )}=,e_{d}))}{_{s^{}_{d}}(rg(x_{u },e^{}_{d}))}\). This term reflects the relative reward of each negative item compared with other negative items. Similar to , we can categorize negative items into two categories: (1) Hard negative items, whose reward \(r_{}(x_{u},e_{d})=(e_{d}|x_{u})}{_{} (e_{d}|x_{u})}\) is relatively high, making it more probable to be chosen by LM-based recommenders; (2) Easy negative items, whose reward \(r_{}(x_{u},e_{d})\) is relatively low, making it less likely to be output. For hard negative items, the extra weight term \(,e_{d}))}{_{s^{}_{d}}(rg(x_{u },e^{}_{d}))}\) tends to be larger, leading to more decline for likelihood. This mechanism makes LM-based recommenders more discriminative and endows S-DPO with more effectiveness and stability than DPO.

### Properties of S-DPO

In this section, we will discuss the structural correlation between DPO and BPR , together with S-DPO and softmax loss , which demonstrates the advantage of S-DPO over DPO and language modeling loss.

For user \(u\), preferred item \(i_{p}\) and one dispreferred \(i_{d}_{d}\), BPR loss takes the form:

\[_{}=-_{(u,i_{p},i_{d})}[(f( u,i_{p})-f(u,i_{d}))],\] (12)

wherein \(f(u,i)\) represents preference score of user \(u\) for item \(i\).

Similarly, given dispreferred item set \(_{d}\), the softmax loss takes the form:

\[_{}=-_{(u,i_{p},_{d})}[ (-_{i_{d}_{d}}(f(u,i_{d})-f(u,i _{p})))].\] (13)

Review the DPO loss in Eq.(7) and S-DPO loss in Eq.(11). Notably, term \((e|x_{u})}{_{}(e|x_{u})}\) is the implicit reward function, denoted by \(r_{}(x_{u},e)\) in Section 3.1. According to Section 2, \(r_{}(e,x_{u})\) reflects the preference of user \(u\) to item \(i\) corresponding to title \(e\). When the reference model has no knowledge about recommendation, _i.e.,_ when \(_{}(e|x_{u})\) is approximately a uniform distribution, term \(r_{}(x_{u},e)=(e|x_{u})}{_{}(e| x_{u})}\) exactly reveals absolute preference. Hence, \(r_{}(x_{u},e)\) possesses a similar function to \(f(u,i)\).

From this perspective, DPO and S-DPO can be seen as special patterns of BPR and softmax loss, respectively. Given the effectiveness of BPR and InfoNCE loss in recommendation, we argue that sampled-based loss which explicitly compares preferred and dispreferred items such as DPO and S-DPO is more suitable for training LM-based recommenders than only utilizing language modeling loss. Moreover, as softmax loss works better than BPR loss in multi-negative scenarios , it can be inferred that S-DPO will be more tailored for multi-negative user preference alignment than DPO.

## 4 Experiments

In this section, we aim to answer the following research questions:

* **RQ1:** How does S-DPO compare with traditional and LM-based sequential recommendation models on performance?
* **RQ2:** How does the LM-based recommender benefit from the multiple negatives in S-DPO?
* **RQ3:** What are the impacts of the essential parameters (\(\)) on S-DPO?

Baselines.We thoroughly compare S-DPO with three categories of recommenders in sequential recommendations: traditional recommenders (GRU4Rec , Caser , SASRec ), LM-enhanced recommenders (MoRec ) and LM-based recommenders (LLaMA2 , Chat-REC, TALLRec , LLaRA ). See detailed introduction and comparison of baselines in Appendix B.

Datasets.We conduct extensive experiments on three real-world benchmark datasets which differ in size and domain (Movielens , Goodreads3, and LastFM ). Following standard settings of , we employ a commonly used metric Hit Ratio@1 (HR@1) for performance evaluation and an additional metric Valid Ratio to evaluate the LM-based methods' ability to generate appropriate responses. See detailed introductions of datasets and evaluation metrics in Appendix B.

Implementation.We implement all LM-based recommenders on 4 NVIDIA A100 GPUs. For all LM-based recommenders, we conduct a supervised fine-tuning stage for a maximum of 5 epochs. For S-DPO and its variants, we conduct a preference alignment stage for a further 3 epochs. Different from existing methods, we only optimize loss on item titles and find it effective in recommendation tasks. Refer to Appendix B for more implementation details.

### Overall Performance Comparison (RQ1)

Table 1 presents a comparative analysis of the performance of our proposed S-DPO and baselines. Bold and underlined indicate the best and the second-best performance, respectively. We observe that:

* **LM-based recommenders have driven impressive performance breakthroughs compared with traditional recommenders.** Our results reveal that traditional recommenders outperform untuned LM-based recommenders (LLaMA, ChatRec) but fall short compared to LM-based recommenders fine-tuned on historical interactions (TALLRec and LLaRA). It is noted that untuned LM-based recommenders are limited by inadequate instruction-following capabilities (indicated by a low valid ratio) or a lack of domain-specific knowledge (indicated by a suboptimal performance), which highlights the necessity of the supervised fine-tuning stage to further ground the inherent ability of language models down to sequential recommendation tasks. Moreover, MoRec also exhibits suboptimal performance compared to its traditional variant because it leaves the reasoning ability of LM untouched. The superior performance of recent LM-based recommenders indicates the significant roles of knowledge and reasoning ability in language models for recommendation tasks in semantically informative datasets, which highlights the potential of LM-based recommenders.
* **Tailoring language models for recommendation task further boosts the performance of LM-based recommenders.** For LM-based recommenders, the substantial performance gap between fine-tuned and untuned approaches emphasizes the importance of tailoring models for recommendations. TALLRec adapts LM for recommendation by supervised fine-tuning LM on historical interactions, surpassing traditional recommenders. Additionally, LLaRA consistently outperformed TALLRec across all datasets, suggesting that introducing collaborative signals through appropriate item representations is a viable direction for further adapting LM. However, existing LM-based methods adapt LM from either item representation methods or corpus construction, leaving the adaptation of optimization objectives unexplored. Instead, S-DPO aligns the language model with multi-negative

    & &  &  &  \\  & & HR@1 & ValidRatio & Rel.Ipv & HR@1 & ValidRatio & Rel.Ipv & HR@1 & ValidRatio & Rel.Ipv \\  Traditional & GRU4Rec & 0.3867 & 1.0000 & _70.91\%_ & 0.2616 & 1.0000 & _153.36\%_ & 0.3750 & 1.0000 & _40.35\%_ \\  & Caser & 0.4174 & 1.0000 & _58.34\%_ & 0.2233 & 1.0000 & _196.82\%_ & 0.3861 & 1.0000 & _36.31\%_ \\  & SASRec & 0.3581 & 1.0000 & _84.56\%_ & 0.2233 & 1.0000 & _196.82\%_ & 0.3444 & 1.0000 & _52.82\%_ \\  LM-based & LLaMA2 & 0.0233 & 0.3845 & _2736.48\%_ & 0.02046 & 0.3443 & _2594.31\%_ & 0.0421 & 0.4421 & _1150.12\%_ \\  & ChatRec & 0.3306 & 1.0000 & _99.91\%_ & 0.3770 & 1.0000 & _75.81\%_ & 0.2000 & 0.9895 & _163.15\%_ \\  & MoRec & 0.2877 & 1.0000 & _129.72\%_ & 0.1652 & 1.0000 & _301.21\%_ & 0.2822 & 1.0000 & _86.50\%_ \\  & TALLRec & 0.4983 & 0.9573 & _32.63\%_ & 0.4180 & 0.9836 & _58.56\%_ & 0.3895 & 0.9263 & _35.12\%_ \\  & LLaRA & 0.5292 & 0.9950 & 24.89\% & 0.4508 & 0.9918 & _47.03\%_ & 0.4737 & 0.9684 & _11.10\%_ \\  Ours & S-DPO & **0.6609** & 0.9900 & - & **0.6628** & 0.9992 & - & **0.5263** & 0.9895 & - \\   

Table 1: The performance comparison on three real-world datasets. “Rel.Ipv” denotes the relative improvement of S-DPO compared with baselines.

user preference data by extending DPO to include a softmax ranking loss, making it a more appropriate loss function for recommendation tasks.
* **S-DPO consistently outperforms all traditional recommenders and the state-of-the-art LM-based recommenders on all datasets.** S-DPO shows an improvement ranging from 11.10% to 47.03% on Hit Ratio@1 compared to the second-best baseline. Building on a supervised fine-tuning stage, we observe a further improvement to the preference alignment stage, which explicitly instills ranking information into LM and utilizes preference data with multiple negative samples. Such superior performance suggests that explicitly tailoring LM for recommendation using user preference data at the training objective level is more effective than other LM-based recommenders. By leveraging the inherent abilities of the LM and incorporating ranking information from user preference data, S-DPO effectively differentiates between preferred and less preferred items. Notably, the preference alignment stage hardly harms the inherent ability of LM, illustrated by a high valid ratio.

### Study on S-DPO

Ablation Study.To investigate the effect of explicit ranking optimization and multiple negative samples of S-DPO, we compare it with the vanilla supervised fine-tuned model (w/o ranking), and a variant of S-DPO with only a single negative sample (w/o multi-neg), downgrading to pairwise DPO loss. The experimental results are reported in Figure 1(a). We can observe that DPO can achieve an overall better performance compared to SFT, which underscores the effectiveness of instilling ranking relationships into existing LM-based recommenders. With a more effective ranking gradient provided by multiple negative samples, S-DPO can further boost performance and achieve the best among all baseline methods and variants.

Study on the number of negative samples (RQ2).Benefiting from the utilization of multiple negative pairs in preference data, our S-DPO offers two empirically appealing properties compared to DPO: 1) S-DPO has more effective gradients facilitating the optimization; 2) S-DPO provides a better boost for rewards of preferred items compared to DPO. Figure 1(b) provides the comparison of validation loss between S-DPO and DPO, illustrating that the loss of S-DPO decreases faster and more significantly. This observation demonstrates that multiple negative pairs provide larger and more meaningful gradients for model optimization, which is attributed to the inherent benefit of S-DPO to mine negative samples  (_cf._ Section 3.1).

On the other hand, we study the behavior of S-DPO which is illustrated in Figure 1(c). We surprisingly find that S-DPO exhibits continually increasing rewards of preferred items that are more significant and stable than DPO, which shows better effectiveness in distinguishing preferred items and a potential of mitigating data likelihood decline issues.

To further verify the superiority of the multiple negative samples of S-DPO compared with DPO, we select the number of negative samples from {1, 3, 5, 8, 10, 15} to conduct experiments to explore the potential of the number of negative samples, with the results depicted in Figure 2(a). It can be observed that utilizing multiple negative samples allows the model to achieve better performance than with a single one. Furthermore, as the number of negative samples increases, the model's performance exhibits continual improvements. We attribute this success of S-DPO to more effective ranking

Figure 2: Study on S-DPO. (1(a)) Ablation study of S-DPO compared with SFT and DPO on three datasets. (1(b)) Comparison of the trend of validation loss between DPO and S-DPO on LastFM. (1(c)) Comparison of the reward of preferred items between DPO and S-DPO on LastFM.

gradients provided by multiple negatives which can be connected to the superior performance of contrastive loss in self-supervised recommendations .

To validate the superiority of S-DPO over the DPO variant with multi-negatives, we conduct effectiveness and efficiency comparisons. Table 2 demonstrates that introducing more negative samples benefits both DPO and S-DPO, and S-DPO achieves comparable performance with fewer training steps. We further analyze how S-DPO outperforms DPO in terms of computational efficiency. While the complexity of DPO for \(K\) negative samples is \((2KC_{}S_{t})\), where \(C_{}+1\) denotes the base LLM's computational complexity and \(S_{t}\) represents the size of the training data, S-DPO's complexity for the same number of negatives is reduced to \(((K+1)(C_{}+1)S_{t})\). This efficiency gain can be expressed by scaling the complexity of DPO by the factor \(++}}+}}\), highlighting that S-DPO offers significant advantages, especially when working with a larger number of negative samples.

Study on values of \(\) (RQ3).In S-DPO, \(\) is a hyperparameter controlling the deviation of LM from the base reference policy . Typically, a smaller value of \(\) implies that the language model is more heavily influenced by the preference signals and vice versa. In this section, we select the value of \(\) from {0.1, 0.5, 1, 3, 5} to explore the effect of \(\) on S-DPO. As indicated in Figure 2(b) through 2(c), a higher \(\) can achieve overall better performance in our task, while a lower \(\) may overwhelm the model's learned knowledge from the supervised fine-tuning stage, as evidenced by both low valid ratio and hit ratio. On the other hand, an excessively large \(\) prevents the model from effectively learning ranking relationships, leading to suboptimal performance. In all our main experiments and studies, we set \(\) as 1 to achieve a balance between ranking signals and inherent knowledge of language models.

## 5 Related Work

### LM for Recommendation

Recent advancements in recommendation systems have increasingly incorporated Language Models (LMs) due to their extensive knowledge and robust reasoning abilities. This integration occurs primarily in two forms: LM-enhanced recommenders and LM-based recommenders. LM-enhanced recommenders utilize LM embedding as semantic representations to provide contrastive signals  or utilize LM as advanced feature extractors improving the representation of user and

  
**Datasets** &  &  &  &  \\ 
**Measure** & **HitRatio@1** & **ValidRatio** & **HitRatio@1** & **ValidRatio** & **HitRatio@1** & **ValidRatio** & \\ 
**DPO-1negative** & 0.6342 & 0.9972 & 0.4947 & 0.9684 & 0.6381 & 0.9900 & \((2KC_{}S_{t})\) \\
**DPO-\(k\)negative** & 0.6413 & 0.9964 & 0.4947 & 0.9474 & 0.6628 & 0.9900 & \((2KC_{}S_{t})\) \\
**S-DPO-\(k\)negative** & **0.6477** & 0.9980 & **0.5263** & 0.9895 & **0.6661** & 0.9950 & \(((K+1)(C_{}+1)S_{t})\) \\   

Table 2: Effectiveness comparison between DPO with single negative, a variant of DPO with multiple negatives and S-DPO with the same number of negatives (we set \(K\) as 3 to get the performance in this table).

Figure 3: Studies on values of \(\) and negative samples numbers of S-DPO on LastFM. (2(a)) Performance comparisons with varying numbers of negative samples (\(=1\)). (2(b)) Performance comparisons with varying values of \(\) setting negative samples number as 3. (2(c)) Validity comparisons with varying values of \(\) setting negative samples number as 3.

item features [59; 60; 61]. However, these systems still rely on traditional recommenders for the final recommendation task, which leaves the reasoning ability of LM largely untouched.

On the other hand, LM-based recommenders directly employ LMs for making recommendations. Early works leverage LMs' in-context learning capabilities for zero-shot or few-shot recommendations, demonstrating significant potential [10; 11; 12; 15]. However, untuned LM-based recommenders are limited by inadequate instruction-following capabilities and a lack of domain-specific knowledge. To bridge this gap, recent efforts in this category include supervised fine-tuning of LMs on the historical interactions to enhance their performance in recommendation tasks [14; 17; 23; 24; 27]. More recently, researchers have discovered that exploring item representation methods in the finetuning phase may further boost LM's ability for recommendation . This branch of works includes integrating collaborative signals [18; 19; 25; 62; 63; 64], adjusting numeric representations [22; 65; 66] or introducing additional item tokens [16; 26; 28; 29].

However, existing finetuned methods follow the training objective of language generation without any specific adjustments for personalized ranking. Different from them, S-DPO proposes to explicitly optimize item ranking information on preference data.

### Preference Alignment of Language Models

Reinforcement Learning from Human Feedback (RLHF) [31; 32; 33] is a prevalent method of LMs to learn from human preferences. The RLHF pipeline comprises reward model training and reinforcement learning (RL) optimization, the latter of which suffers instability and inefficiency. Direct Preference Optimization (DPO)  bypasses the brittle RL phase via a particular reward model parameterization and is thus simpler to implement while still keeping the performance of RLHF.

DPO proves to be effective in many scopes, like NLP [34; 67] and multimodal LMs [35; 68; 69; 70]. Besides, several variants have been proposed for further improvement of DPO. \(\)PO  is a generalization of DPO loss and its representative IPO can better overcome the problem of overfitting. ODPO  treats preference pairs differently by stipulating that the likelihood gap of two responses should be greater than a corresponding offset value. KTO  utilizes prospect theory for preference alignment tasks. Other variants including GPO , \(f\)-DPO , RSO  also enhance or expand DPO in various aspects. Despite these contributions, the possibilities for leveraging and further adapting DPO for recommendation are still largely unexplored and few studies discuss extending DPO to handle multi-negative scenarios.

## 6 Limitation

Despite effectiveness, there are several limitations not addressed in this paper. On the one hand, the number of negative samples is capped at 15 in our experiments. The potential of multiple negative samples hasn't been fully explored due to the limited time and computation resources. On the other hand, increasing the number of negative examples inevitably results in higher training costs, a phenomenon that becomes more pronounced as the number of negative examples grows in the context of language models.

## 7 Conclusion

In this work, we devised a principled Softmax-DPO (S-DPO) loss specially tailored for LM-based recommenders, utilizing multiple negatives in preference data to explicitly instill ranking information into LM. Empirically, S-DPO surpasses all baseline models including traditional and LM-based methods on three datasets in sequential recommendation tasks while successfully providing better rewards for preferred items compared to DPO. Grounded by theoretical proof, we bridge S-DPO with the softmax loss in self-supervised recommendations, underscoring the ranking performance of S-DPO and highlighting the critical roles of multiple negatives. Also, we theoretically find that S-DPO has an inherent benefit to mine hard negatives which provide larger and more effective gradients to model optimization, assuring its exceptional capabilities in recommendation tasks. We believe that S-DPO, as a generalization of DPO, provides valuable insights for future LM-based recommenders and has the potential to benefit research fields other than recommender systems4.