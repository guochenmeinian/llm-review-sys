# MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer

MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer

 Minghao Zhu Zhengpu Wang Mengxian Hu Ronghao Dang Xiao Lin Xun Zhou Chengiu Qijun Chen Tongji University, Shanghai, China

{zmhh_h, wangzhengpu, humengxian, dangronghao, linc_xx, zhouxun, liuchengju, qjchen}@tongji.edu.cn

Corresponding author.

###### Abstract

Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose _Weight Merging Regularization_, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 & 600, UCF, and HMDB. Code is available at https://github.com/ZHHH-H/MoTE.

## 1 Introduction

With the advent of large-scale Vision-Language Models (VLMs), adapting such foundation models (e.g., CLIP , ALIGN , Florence ) for downstream tasks has become an emerging paradigm of intense scrutiny. Their semantic visual concepts empowered by aligning large-scale image-text pairs can be transferred to a wide range of downstream tasks, such as open-vocabulary classification , detection , and segmentation .

The crux of an adaptation procedure for foundation models lies in injecting specialized knowledge of the domain of interest. For video recognition tasks, this necessity is reflected in the fact that the dynamic nature of video data requires effective comprehension of context and temporal correlation by the model. Therefore, to condition the adapted model on the video-specialized knowledge, one general and effective way is to incorporate additional parameters in the form of well-designed prompts , adapters , and temporal modules . However, we observe that while the increased model capacity brought by more additional parameters enables the better fitting of video-specific inductive bias, it comes at the cost of catastrophically forgetting the generalization knowledge of the original VLM. To better illustrate this phenomenon, we present an overview of existing methods in Figure 1. A clear trade-off problem between zero-shot and close-set performance emerges in existing works, which correlates to the scale of additional parameters. The former relies more on the generalization capability inherent in VLMs while the latter requires intensive video-specialized knowledge, but no method achieves the best of both worlds. On one hand, introducing new knowledge while preserving existing knowledge continually is desirable and significant for the broader adaptation of the foundation model. The rapidly evolving real-world applications also require both specialization and generalization capabilities. However, how to manage the generalization/specialization trade-off of additional parameters in transfer learning remains under-explored.

This paper addresses the aforementioned challenge by delving into two inquiries: **(i)**_How can the generalization capability of additional parameters be enhanced?_ Given the substantially smaller scale of the fine-tuning dataset compared to the pre-training dataset of VLMs, the newly added parameters risk overfitting the fine-tuning data bias, thereby constraining the model generalization. Addressing this question enables steering parameters towards high generalization. Nevertheless, generalization and specialization have proven to be somewhat conflicting  in model training, prompting us to further explore **(ii)**_How can generalization and specialization coexist in one unified model?_ This question is crucial for a balanced model to preserve both new and existing knowledge, yet has received limited exploration. We investigate these two questions with a widely employed temporal module  (i.e. N-layer transformer), which already features a high specialization degree but is less performant on zero-shot tasks.

With this in mind, we present MoTE, a mixture-of-temporal-experts approach with well-balanced generalization and specialization. We offer a novel perspective in addressing the first question: constructing a more generalized model using multiple data bias views. In contrast to conventional temporal modules that encode patterns with a _single_ feedforward network (FFN) in each Transformer layer, MoTE uses _multiple_ FFN experts to capture various data bias views. During fine-tuning, incoming frame token sequences are routed to one of the temporal experts, keeping the computational cost the same as using a single FFN. To enlarge the discrepancy in knowledge learned by each expert, we devise a routing algorithm based on the multinomial distribution. During inference, we apply weights merging to collapse multiple experts into one module, enabling the patched model to aggregate the generalized knowledge of each expert. For the second question, we propose _Weight Merging Regularization_ which regularizes the merging process of experts in the weight space. The proposed regularizer drives a range of the merged parameters optimal with respect to the task-specific objective, allowing a more effective aggregation of generalized and specialized knowledge in the patched model. To further alleviate the overfitting at test time, we devise a plug-and-play module that modulates the contribution of temporal features by measuring the semantic association between the proxy text features from the fine-tuning and the test datasets.

Our main contributions can be summarized as follows:

* We propose MoTE, a knowledge transfer framework from visual-language to video domain. MoTE tackles the fitting trade-off challenge posed by additional parameters, an aspect largely overlooked by previous works.
* We provide new insights for enhancing parameter generalization from the perspective of data bias fitting, all while keeping the computation cost and final structure constant (SS 3.1, SS 3.2).
* We propose Weight Merging Regularization, a novel regularizer for more effective knowledge aggregation, realizing the coexistence of generalization and specialization in weight space (SS 3.3). Together with Temporal Feature Modulation to further improve the generalization of MoTE (SS 3.4).

Figure 1: Overview of existing VLM knowledge transfer methods. (a) Trade-off plots between zero-shot (Harmonic mean of UCF, HMDB, and K600) and close-set (K400) performance of recent CLIP-based methods (ViT-B/16). (b) As the number of temporal layers increases, the generalization of the standard Transformer layer severely degrades while our proposed MoTE consistently improves the zero-shot and close-set performance. (c) Our proposed MoTE seeks to construct a reconciled feature space between the optimal generalized and specialized manifolds.

* Extensive experiments demonstrate MoTE achieves an optimal trade-off between zero-shot and close-set performance with one unified model. Thorough ablation studies show the scalability and effectiveness of our proposed method (SS 4).

## 2 Preliminary: Transferring CLIP for Video Recognition

Recent works have adapted CLIP  to video datasets and obtained superior results [28; 41]. We briefly describe the typical cross-modal video recognition pipeline. Consider a video \(V\) with \(T\) frames and the corresponding text label \(C\) described in a set of textual prompts. Each frame is encoded independently by the CLIP visual encoder \(f(|_{v})\) with parameter \(_{v}\) and produces frame-level embeddings \(\{_{i}^{D}\}_{i=1}^{T}\). The text embedding \(^{D}\) is generated by the CLIP text encoder \(g(|_{c})\) with parameter \(_{c}\), where \(D\) is the embedding dimension,

\[_{1},,_{T}=f(V|_{v}),\ \ =g(C|_{c}).\] (1)

The CLIP visual encoder captures rich spatial information. To bridge the domain gap between image and video, we apply a commonly used temporal module \(h(|_{tem})\) for cross-frame communication [14; 49; 50], which is parameterized by several Transformer layers. The final video embedding \(^{D}\) consists of spatial and temporal embeddings connected in a residual form:

\[=([_{1},,_{T}]+h(_{1},,_{T}|_{tem})).\] (2)

During optimization, the text encoder is typically frozen. We tune the visual encoder and the temporal module to maximize the similarity \((,)=, }{\|\|\|\|}\) between the video embedding \(\) and the text embedding \(\) if \(V\) and \(C\) are matched, otherwise minimize it. The training objective can be formulated as:

\[(_{v},_{tem};)=_{{}_{(V,C) }}[((,), (C))],\] (3)

where \(\) is the fine-tuning dataset, \(\) is the cross-entropy function with softmax operation.

## 3 Methodology

An overview of our proposed method is presented in Figure 2. This section first analyzes the parameter generalization from the perspective of data bias fitting (SS 3.1). Then we detail the structure, routing policy, and inference of MoTE (SS 3.2). Finally, we present Weight Merging Regularization (SS 3.3) and Temporal Feature Modulation (SS 3.4) towards the coexistence of generalization and specialization.

### Intuition and Motivation

Typically, more additional parameters allow for better fitting of the training data distribution, leading to better close-set performance. However, steering the model specialized on the target distribution potentially makes it sensitive to out-of-distribution shifts, resulting in the generalization drop on downstream data distributions with unseen video categories. Although one could enlarge the training data distribution to cover as many potential unseen categories as possible, the costly computation and collection of video data make this infeasible, indicating the need for a more generalized architecture.

Neural network optimization has many solutions in different loss basins due to the non-convexity of the loss landscape. Models optimized with different configurations (e.g. initialization, optimizer, and data) have different optimization trajectories and may converge to separate local minima, thereby capturing various feature patterns. This inspires our method to construct a more generalized model using multiple data bias views. Instead of improving generalization by searching for a flatter minimum in the loss landscape [22; 48], we expect the aggregation of diverse knowledge from multiple minima can provide a more comprehensive representation. Intuitively, diverse temporal patterns can better facilitate recognizing an unseen video category. For example, aggregating information on 'player movement' and 'racket-ball interaction' can help better recognize 'playing tennis'. Our architecture design takes inspiration from Mixture-of-Experts  to learn multiple data bias views with a set of experts. While MoE was originally proposed for building large pre-trained models, we extend it to the context of transfer learning and demonstrate its effectiveness in improving parameter generalization.

### Mixture-of-Temporal-Experts

ArchitectureConsider the temporal module \(h(|_{tem})\) as \(L\) repeated Transformer layers which consist of a self-attention layer and a fully-connected feed-forward network (FFN). For clarity, the temporal module's parameter \(_{tem}\) is factorized into parameter \(_{att}\) for attention layers and parameter \(_{f}\) for FFNs, where \(_{tem}=_{att}_{f}\). Recent studies [5; 9] suggest that factual knowledge is mainly stored in the FFN, which consists of two learnable projection matrices and an activation function. Thus, we replace the FFN in each Transformer layer with \(N\) temporal experts and obtain a set of experts \(\{\{E_{i}^{j}\}_{i=1}^{N}\}_{j=1}^{L}\), where \(E_{i}^{j}\) represents the \(i^{th}\) expert at the \(j^{th}\) layer and has the same structure as the FFN. Each expert starts training from different random initialization to ensure different optimization trajectories, which we experimentally show as critical for learning distinct knowledge (i.e. data bias views). Denote the parameter of the expert \(E_{i}^{j}\) as \(_{i}^{j}\) and the activated expert's index at the \(j^{th}\) layer as \((j)\). The training objective becomes:

\[_{}=(_{v},_{att},(j)}^{j}\}_{j=1}^{L}$},).\] (4)

Routing PolicyThe routing algorithm of MoE determines which experts process inputs. Classic routing mechanisms (i.e. top-\(k\) gating ) make decisions using a learnable gate network and require auxiliary losses to handle load balancing. Instead, we adopt the stochastic routing algorithm  considering the architecture brevity, as it avoids the need for extra network, computation, and loss.

Recall that we expect each expert to learn distinct feature patterns. To further enlarge the discrepancy in knowledge learned by each expert, we present a stochastic routing algorithm based on the multinomial distribution. By assigning different activation probabilities to experts, we can control the training data volume of each expert, empowering their knowledge with different degrees of generalization and specialization. Formally, set \(=[_{i}^{l}]_{i=1}^{N}\) to be a random vector in the \(l^{th}\) layer, where \(_{i}^{l}\{0,1\}\) indicates whether the \(i^{th}\) expert is activated and \(_{i=1}^{N}_{i}^{l}=1\). We specify the variable \(\) to follow the multinomial distribution where the activation probability of each expert is given as

\[P(_{i}^{l}=1)=^{N}(i)},\] (5)

where \(()\) is the natural exponential function. This allows experts with a greater index to be activated more likely and therefore receive a larger volume of data during training. Once the expert is selected, all inputs in a given batch are processed by the same expert.

Figure 2: An overview of the MoTE framework. **(Left):** We independently extract the feature of each frame with the CLIP visual encoder. Then, the frame token sequences from a given batch are routed to an activated expert for temporal pattern encoding. To regularize the merging process, we sample the temperature \(\) from a discrete set and use it to collapse multi-experts into one merged FFN. **(Right):** Temporal feature modulation. We modulate the contribution of the temporal feature with the semantic association, which is measured by the similarity between the proxy text features retrieved from the fine-tuning and the test categories. The modulated embedding is used for inference.

Knowledge Aggregation in InferenceDuring inference, knowledge across experts can be aggregated by either merging weights [44; 47] or ensembling logits . We adopt merging weights with the following benefits: (**i**) More effective knowledge aggregation as evidenced in Table 2. (**ii**) Constant computation cost, while the latter increases linearly with the expert number. (**iii**) Consistent parameter structure with its initial state. Specifically, for the experts \(\{E_{i}^{l}\}_{i=1}^{N}\) in the \(l^{th}\) layer, we average the weights of all the experts to obtain the final parameters \(}\) for inference, where

\[}_{i=1}^{N}_{i}^{l}.\] (6)

### Weight Merging Regularization

While the introduction of MoTE notably improves model generalization, it still incurs a decrease in close-set performance as presented in Table 1, which again proves the conflicting nature of generalization and specialization in weight space. We attribute this phenomenon to the difficulty of simultaneously preserving the generalized and specialized knowledge during expert merging process.

In this regard, we propose Weight Merging Regularization to reconcile and promote the aggregation efficacy of expert knowledge. To maximally aggregate the _specialized knowledge_ of experts, the final merged model necessitates explicit optimization with respect to the task-specific objective. As for the preservation of the _generalization knowledge_, previous works [17; 22; 27] observe that the flatness (i.e. sharpness) of local minima in the loss landscape is strongly correlated with the model generalization. They suggest that models with flatter local minima in the loss landscape generalize better on unseen data, where the flatness refers to the robustness of the loss value to perturbations in the parameters. Within a flat basin of the loss landscape, moderate variations of parameters do not lead to significant increases in loss. Inspired by this observation, we propose to facilitate preserving the generalized knowledge of experts by explicitly constructing a _flat region_ around the loss landscape of the final merged model. We optimize the constructed region with the task-specific loss objective and demonstrate that region construction benefits the coexistence of specialization and generalization in one unified model. Formally, for the experts \(\{E_{i}^{l}\}_{i=1}^{N}\) in the \(l^{th}\) layer, we first merge the weights as

\[^{l}_{i=1}^{N}=1}^{N} (i^{}/)}_{i}^{l},\] (7)

where \(\) is a temperature parameter used to control the softness of the distribution. By varying the value of \(\), we can build a region of the merged parameters in the weight space. Considering that sampling \(\) in a continuous space may lead to difficulties in model convergence, we sample \(\) from a discrete set given as \(\{ 2^{n}\}_{n=0}^{4}\{\}\), where \(\) is a hyper-parameter of the candidate set. With parameters \(\{^{j}\}_{j=1}^{L}\) merged using different \(\) in each layer, the regularizer is defined as:

\[_{}=(_{v},_{att},\}_{j=1}^{L}}\,]).\] (8)

Denote the generated video feature when computing \(_{}\) as \(_{r}\). We further slightly regularize the consistency between \(_{r}\) and the pooled spatial feature e from the CLIP visual encoder:

\[_{}=_{(V,C)}[ \|_{r}-\|_{2}^{2}],\] (9)

where \(\|\|_{2}\) is the L2 norm. This regularizer encourages MoTE to model the temporal dynamics with a light temporal feature, which potentially reduces the model complexity and improves generalization. The overall learning objective can be written as:

\[_{}=_{}+_{ }+_{},\] (10)

where we set \(=0.5\) and \(=0.1\) by default.

### Temporal Feature Modulation

While applying MoTE to downstream recognition tasks, a potential risk is that its semantic space is limited to the category names of the fine-tuning dataset. To demonstrate this, we collect the category names of Kinetics-400, UCF-101, and Kinetics-600 datasets and manually remove duplicate ones. Weconduct tests with the mixed category names and observe 2.9% closed-set performance drops, 29.8% and 31.1% zero-shot performance drops on Kinetics-400, UCF-101, and Kinetics-600, respectively. That is, the model tends to classify all videos into fine-tuning categories.

In light of this, we propose a test-time adaptation strategy for networks where the temporal module is separated from the CLIP encoder, to measure the confidence of temporal features by means of CLIP's text space and modulates the contribution of temporal features to the prediction. Given the pooled feature \(\) from the CLIP visual encoder, we generate fine-tuning and test proxy features \(_{f},_{t}^{K D}\) by retrieving the \(K\) nearest text features in the fine-tuning and the test dataset categories. We estimate the semantic association \(\) between the two proxy features as

\[=(-(1-(_{i}_{f}^{T}))/),\] (11)

where \(\) stands for a scale hyper-parameter and \(()\) is a sequential maximum and average pooling operation. Denote the pooled temporal feature from MoTE as \(\), the final video embedding \(\) used for inference becomes \(=+\).

## 4 Experiments

### Experimental Setup

Architecture.We employ the CLIP  pre-trained ViT-B/16 and ViT-L/14 in our experiments. On top of the visual encoder, we add 6 layers MoTE for ViT-L/14 and 4 layers MoTE for ViT-B/16, with 4 temporal experts per layer by default.

Implementation Details.We fine-tune our model using the Kinetics-400  dataset as in previous works . During fine-tuning, we sparsely sample \(T\) (e.g. 8 or 16) frames as the video input. Each input example is randomly cropped and resized to the size of \(224 224\) and then undergoes random horizontal flip and random grayscale. We adopt AdamW  as the optimizer with a weight decay of 0.2, following a half-period cosine learning rate decay. The initial learning rate is set to \(5 10^{-5}\) with a total batch size of 144. Furthermore, we set the candidate set \(\) for \(_{}\) to 0.6 and the scale parameter \(\) for temporal feature modulation to 0.05, and \(K\) to 5. We apply temporal feature modulation only in evaluation. _Please see supplementary for more details_.

Evaluation Protocols.We thoroughly evaluate our method with close-set, zero-shot, and few-shot video recognition. _Close-set:_ We evaluate the close-set performance on Kinetics-400 , using one single clip with a center crop (i.e. \(1 1\) views) or 4 clips with 3 crops (i.e. \(4 3\) views) per video . Each view contains 8 or 16 sparsely sampled frames. _Zero-shot:_ Following previous works [28; 34], we evaluate zero-shot performance on UCF-101 , HMDB-51 , and Kinetics-600 . For K600, we adopt the three splits provided by . Each split contains 160 categories out of 220 new categories. In zero-shot setting, we test using \(3 1\) views with 8 frames per view. _Few-shot:_ We consider standard K-shot setting and evaluate on UCF-101, HMDB-51, and Something-Something v2 . We adopt a single view for evaluation.

### Ablation Studies

Component-wise analysis of MoTE.In Table 1, we perform in-depth ablations of the proposed components with the ViT-L/14 network. We adopt Text4Vis  as our baseline, which serves as a prevalent CLIP adaptation framework in the video domain. Text4Vis uses a 6-layer Transformer for temporal modeling, featuring a high degree of specialization but low generalization capability. We observe that adopting the temporal experts boosts zero-shot performance significantly, validating our idea of improving generalization with multiple data bias views. We then introduce \(_{}\) for achieving the coexistence of generalization and specialization, which facilitates a more efficient aggregation of generalized knowledge while achieving the same level of specialization as Text4Vis. Adding \(_{}\) further improves the zero-shot performance. Moreover, we find that zero-shot performance benefits from modulating the temporal features for unseen categories during evaluation, especially for HMDB51. In summary, MoTE achieves a sound balance between generalization and specialization.

Expert-wise performance of MoTE.To better understand the reconciliation effect of MoTE, we show the performance of each expert and the final merged model in Figure 3. In particular,

[MISSING_PAGE_FAIL:7]

parameters (i.e. without region construction). Due to the conflicting nature of generalization and specialization in optimization, the regularizer without region construction cannot adequately aggregate expert knowledge. Instead, constructing the region in the loss landscape enables generalization and specialization to coexist in one model. The parameter \(\) is used to construct the candidate set of the weights merging temperature. As shown in the table, the performance gain is robust to this parameter.

**Varying numbers of \(\) for Temporal Feature Modulation.** The parameter \(\) is used to scale the semantic association to an appropriate interval. In general, \(=0.05\) yields better results. We find that different datasets exhibit varying sensitivities to this parameter and a relatively small contribution of temporal features generalizes to the unseen categories. _Please see supplementary for more ablations._

### Main Results

Zero-shot video recognition.In Table 3, we compare our method with the state-of-the-art results under the zero-shot setting. Our method achieves new state-of-the-art results on UCF-101 and HMDB-51, with only 8 frames as input. The remarkable performance can be scaled up as the network size and the input frame number increase, demonstrating the great scalability of our method. For K600, although significant improvements are achieved over the baseline, our method still underperforms some methods that do not apply any additional parameters such as Open-VCLIP  and MAXI . Since the categories of Kinetics-600 are much more complex, we argue that it is still challenging for randomly initialized parameters to generalize well on such complex unseen categories. Note that our method still outperforms any method that employs additional parameters on K600. Overall, our method presents superior generalization performance.

Close-set and zero-shot performance trade-off.Table 4 presents comparisons with the state-of-the-art methods under the close-set setting. We also list the harmonic mean of the zero-shot results on UCF, HMDB, and K600 as an indicator of the generalization capability, denoted as HM\({}_{}\). To evaluate the holistic performance in close-set and zero-shot settings, we define the "Trade-off" score as the harmonic mean of Top-1\({}_{}\) and HM\({}_{}\), as we consider the specialization and generalization capabilities to be equally important for a balanced model.

As shown in the table, our method presents strong close-set results competitive with SOTAs and state-of-the-art zero-shot performance. More importantly, while existing methods can perform well on only one task setting, we achieve superior performance on both settings simultaneously with one unified model. Our method consistently exhibits significant trade-off performance advantages across different networks, evidencing the effectiveness and scalability of MoTE in reconciling generalization and specialization capabilities. Even against ViFi-CLIP  which uses different training hyperparameters for close-set and zero-shot settings and takes more frames as the input, our method still shows better balanced performance (74.7% vs. 72.9%). Noteworthy that FROSTER  is a strong zero-shot action recognition model but performs less well in the close-set setting, which

   Method & Venue & Encoder & Frames & UCF-101 & HMDB-51 & Kinetics-600 \\  ActionCLIP  & arXiv’21 & ViT-B/16 & 32 & 58.3\(\)3.4 & 40.8\(\)5.4 & 67.7\(\)1.1 \\ A5  & ECCV’22 & ViT-B/16 & 32 & 69.3\(\)4.2 & 44.3\(\)2.2 & - \\ X-CLIP  & ECCV’22 & ViT-B/16 & 32 & 72.0\(\)2.3 & 44.6\(\)5.2 & 65.2\(\)0.4 \\ ST-Adapter  & NeurIPS’22 & ViT-B/16 & 8 & 76.9\(\)0.8 & 51.5\(\)0.6 & 60.2\(\)1.8 \\ Vita-CLIP  & CVPR’23 & ViT-B/16 & 832 & 75.0\(\)0.6 & 48.6\(\)0.6 & 67.4\(\)0.5 \\ ViFi-CLIP  & CVPR’23 & ViT-B/16 & 32 & 76.8\(\)0.7 & 51.3\(\)0.6 & 71.2\(\)1.0 \\ OTI  & ACMMM’23 & ViT-B/16 & 8 & 83.3\(\)0.3 & 54.2\(\)1.3 & 66.9\(\)1.0 \\ Open-VCLIP  & ICML’23 & ViT-B/16 & 8 & **83.4\(\)**1.2 & 53.9\(\)1.2 & **73.0\(\)**0.8 \\ MAXI  & ICCV’23 & ViT-B/16 & 1632 & 78.2\(\)0.8 & 52.3\(\)0.7 & 71.5\(\)0.8 \\
**MoTE (ours)** & ViT-B/16 & 8 & **83.4\(\)**0.7 & **55.8\(\)**0.9 & 70.2\(\)0.6 \\  X-Florence  & ECCV’22 & Florence & 32 & 73.2\(\)4.2 & 48.4\(\)4.9 & 68.8\(\)0.9 \\ Text4Visf  & AAAI’23 & ViT-L/14 & 8 & 82.6\(\)0.7 & 52.4\(\)0.4 & 72.1\(\)0.9 \\ OTI  & ACMMM’23 & ViT-L/14 & 8 & 88.1\(\)1.0 & 59.3\(\)1.7 & 70.6\(\)0.5 \\ Open-VCLIP  & ICML’23 & ViT-L/14 & 8 & 87.6\(\)1.2 & 59.0\(\)0.6 & **81.1\(\)**0.8 \\ DiSTI  & ICCV’23 & ViT-L/14 & 32 & 74.9\(\)0.8 & 57.5\(\)1.6 & 75.0\(\)0.7 \\
**MoTE (ours)** & ViT-L/14 & 8 & **88.7\(\)**0.6 & **61.4\(\)**1.3 & 78.4\(\)0.9 \\   

Table 3: Zero-shot video recognition performance compared with the state-of-the-art methods on UCF-101, HMDB-51, and Kinetics-600. \(\) denotes reproduced results with our implementation.

[MISSING_PAGE_EMPTY:9]

## 5 Related Work

Video recognition.In the era of deep learning, early works explored diverse variants of convolutional networks for joint spatiotemporal modeling, such as 3D convolution [2; 39] and factorized spatial and temporal convolution . The advent of Transformer then attracted the attention of researchers and has been widely applied for video recognition [1; 6]. In addition, self-supervised learning methods learn transferable representations from large-scale unlabeled data and achieve promising performances [7; 31; 57].

Transferring VLMs for videos.Transferring VLMs for video recognition task has been proven to be effective. ViFi-CLIP  suggests that a direct fine-tuning process generalizes well on various settings. Open-VCLIP  constructs an open-vocabulary video model by interpolating the model weights and its optimization trajectory. Vita-CLIP  extracts discriminative information using multi-level prompts. X-CLIP  proposes cross-frame attention and multi-frame integration modules for temporal modeling. Text4Vis  and BIKE  explore the way for more effective knowledge transfer and use multi-transformer layers to capture temporal cues. FROSTER  mitigates catastrophic forgetting by ensuring the learned features do not diverge too far from the frozen ones through distillation. This divergence comes from the variations of both CLIP and additional trainable parameters. Differently, we believe the main cause of the forgetting problem is the overfitting of additional parameters regardless of whether the CLIP parameters are tuned. Note that FROSTER can also potentially improve the generalization of additional parameters through knowledge distillation, but our method presents a more explicit way to achieve this goal. Existing methods face a trade-off of introducing more specialized knowledge or preserving more generalized knowledge. Our work addresses this challenge and presents superior results on both close-set and zero-shot settings.

Mixture-of-Experts.The sparsely activated MoE structure  enables the model capacity (i.e. number of parameters) to be vastly scaled up while keeping the computation cost per sample basically unchanged. This technique has been widely investigated in building large-scale pre-trained models in various fields [20; 26; 35]. Several works employ MoE on large-scale language models for parameter-efficient tuning to improve their specialized performance [8; 44]. In this paper, we demonstrate its effectiveness in balancing generalization and specialization in VLM knowledge transfer.

## 6 Conclusion

In this paper, we present MoTE, an effective Visual-Language to video knowledge transfer framework that enjoys both superior generalization and specialization. MoTE leverages a mixture of temporal experts to enhance performance in both close-set and zero-shot video recognition, all while maintaining the conventional temporal module's structure and computational efficiency. With the proposed weight merging regularization and temporal feature modulation, we achieve the coexistence of generalization and specialization in one unified model. Extensive experiments validate MoTE's ability to strike an optimal trade-off between close-set and zero-shot performance.

Figure 4: Visualization of the Top-1 accuracy for each video category sampled from UCF-101 with respect to the merged expert and each individual expert.