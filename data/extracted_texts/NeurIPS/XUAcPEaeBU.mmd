# PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging

Xin Cai\({}^{1,2}\), Zhiyuan You\({}^{1}\), Hailong Zhang\({}^{3}\), Wentao Liu\({}^{2,4}\), Jinwei Gu\({}^{1}\), Tianfan Xue\({}^{1,2}\)

\({}^{1}\)The Chinese University of Hong Kong, \({}^{2}\)Shanghai Artificial Intelligence Laboratory,

\({}^{3}\)Tsinghua University, \({}^{4}\)SenseTime

{cx023, yz023, tfxue}@ie.cuhk.edu.hk, jwgu@cuhk.edu.hk,

{zhanghl21}@mails.tsinghua.edu.cn, liuwentao@sensetime.com

###### Abstract

Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam. Project website: phocolens.github.io.

Figure 1: We introduce PhoCoLens, a lensless reconstruction algorithm that achieves both better visual quality and consistency to the ground truth than existing methods. Our method recovers more details compared to traditional reconstruction algorithms (b) and (c), and also maintains better fidelity to the ground truth compared to the generative approach (d).

## 1 Introduction

Lensless imaging systems  have emerged as a groundbreaking solution for ultra-compact, lightweight, and cost-effective imaging. They replace lenses by amplitude [3; 4] or phase masks [1; 5; 20] placed close to the sensor to modulate incoming light. This design significantly reduces camera size and weight and enables innovative sensor shapes such as spherical  or cylindrical.

Considering the raw measurements from lensless cameras are typically blurry and unrecognizable, it is hard to recover a high-quality image of the original scene. An example of lensless measurement and recovered images by WienerDeconv  is shown in the left column of Fig. 1. Due to the lack of focusing elements, lensless cameras cannot directly record the scene but encode it into a complex diffraction pattern. This encoding process can be approximated as a convolution with a large Point Spread Function (PSF). The PSF acts like a low-pass filter applied to the scene thus introducing ambiguity, which means there could be multiple possible recoveries for a single measurement. Therefore, powerful algorithms are critical for high-quality reconstruction in lensless systems.

The primary challenge of a lensless reconstruction is to achieve both photorealism and consistency. Photorealism requires high-quality reconstruction with rich details, while consistency further requires reconstructed contents to be consistent with the original scene. Traditional techniques, like WienerDeconv, can reconstruct images that align with the ground truth but visual quality is significantly degraded (Fig. 0(b)). Learning-based approaches such as FlatNet-gen  attempt to enhance visual quality by training with paired images and lensless measurements, yet they often fail to recover high-frequency details (Fig. 0(c)). The visual quality can be further improved by the restoration algorithms using generative priors, like DiffBIR  (Fig. 0(d)). While these restoration methods can inject rich details, they may also alter image content or insert non-existent objects, breaking consistency. For example, in Fig. 0(d), the beak in the top row has the wrong shape compared to the ground truth (Fig. 0(f)), and the leaves in the bottom row look fake.

Moreover, an inaccurate imaging process simulation may also hurt data consistency. Most existing reconstruction algorithms [17; 27] simplify the imaging process as a convolution with a shift-invariant PSF. However, in practice, PSFs are spatially varying, particularly when the angle of incidence increases. Consequently, these areas experience a noticeable drop in the reconstructed similarity to the original scene, especially in the peripheral field of view.

To achieve both photorealism and consistency, we propose a two-stage lensless reconstruction based on range-null space decomposition . According to this decomposition, the reconstructed image consists of two components, one from "range space", which can be directly calculated from the lensless measurement (Fig. 0(a)), and the other from "null space", which are the detailed textures lost during the lensless imaging process. Therefore, the first stage prioritizes data consistency by recovering the "range space" component, which is the low-frequency content that matches the ground truth. The second stage focuses on photorealism by adding more high-frequency details from the "null space" while maintaining the consistency established in the first stage.

In the first stage, to improve consistency, we propose a novel spatially varying deconvolution to reconstruct the "range space" content. Unlike traditional methods that assume a shift-invariant PSF, our approach automatically adapts to spatial variations in the PSF across the camera's field of view in a data-driven manner. This innovation more accurately models the forward imaging process, leading to better reconstruction of structural integrity and detail in the low-frequency components. Fig. 2 shows improvement with our method (right) against those using a single kernel for deconvolution (left).

In the second stage, to improve photorealism, we integrate a generative prior using a pre-trained diffusion model, to insert realistic details into the first-stage output. This model specifically targets the recovery of high-frequency details lost in the lensless imaging process. We condition the diffusion model on the low-frequency content reconstructed in the first stage, guiding it to incorporate missing high-frequency elements from the "null space". Our supervised approach ensures the final images are consistent with actual measurements and also achieve photorealistic quality.

Combining these two stages, our lensless reconstruction achieves a good balance between data consistency and visual quality in the reconstructed images, surpassing existing methods, as shown in Fig. 1e. To validate this, we compare our method with others on two types of lensless cameras: PhlatCam  and DiffuserCam , using metrics that assess both fidelity and visual quality. The qualitative comparison on the PhlatCam dataset, demonstrating improvements in both aspects, is shown in Fig. 3, highlighting the superiority of our technique over current methods.

## 2 Related Work

**Lensless imaging** is traditionally addressed by solving regularized least squares problems for convolutional imaging models, often incorporating sparsity constraints like those in the gradient or frequency domain [1; 5; 21; 31]. Deep learning has revolutionized lensless imaging by enabling learnable deconvolution parameters and perception enhancement through image-to-image networks. Recent innovations include deep unrolled techniques [19; 27; 49], alongside feed-forward deconvolution methods in image space  or in feature space . However, these methods generally assume a constant point spread function in the imaging process. Our proposal introduces a spatially varying deconvolution approach to overcome this limitation and achieve more precise image reconstruction.

**Spatially-varying deconvolution** has been a well-studied area [11; 24; 46] due to the prevalence of imaging systems with PSFs that vary across the FoV. However, these methods [2; 20; 25; 29; 47] are often slow, computationally intensive, and result in poor image quality, especially in complex systems. Recently, MultiWienerNet  introduced a deep learning approach for fast, spatially varying deconvolution, but it requires tedious multi-location PSF calibrations. In contrast, our spatially varying deconvolution method, designed for lensless imaging, leverages a single initial PSF and automatically learns variations across the image, eliminating the need for extensive calibration.

**Inverse imaging with diffusion models** can be categorized as supervised or zero-shot. Supervised methods train a conditional diffusion model [8; 13; 12; 32; 50] with paired images to bridge the gap between input and desired outputs, leveraging generative priors for inverse imaging [35; 39] and controlled generation [28; 34]. On the other hand, zero-shot methods  employ guidance to address a wide range of general inverse problems [9; 38]. These techniques typically rely on predefined conditions for guidance. Specific models like DDRM  and DDNM  focus on mathematical decompositions to improve the diffusion process. Our work integrates the strengths of both approaches, combining supervised fine-tuning with a theoretical framework based on range-null space decomposition, aiming for a robust solution to inverse imaging challenges.

## 3 Preliminary

### Range-Null Space Decomposition

The lensless imaging process can be formulated as a linear transformation . Specifically, given a scene plane \(^{M^{2}}\), the sensor measurements \(}^{N^{2}}\) is expressed as \(}=+\). Here, \(^{N^{2} M^{2}}\) is the transfer matrix of the lensless imaging system, and \(\) is the sensor noise. The transfer matrix \(\) essentially encodes how light from each point on the scene plane contributes to each sensor pixel. Given a calibrated camera system, the transfer matrix \(\) is known and the objective of lensless reconstruction is to recover the scene plane image \(\) (like Fig. 1f) from the measurement (like Fig. 1a). For simplicity, below we consider a noise-free case as \(=\).

We then introduce range-null space decomposition. Let \(^{}^{M^{2} N^{2}}\) be the pseudo-inverse of the linear matrix \(\), which satisfies \(^{}\). The operation \(^{}\) can be interpreted as a projection onto the range space of \(\) because for any sample \(\), we have \(^{}=\). In contrast, the operation \((-^{})\) acts as a projection onto the null space of \(\), given that \((-^{})=\). Therefore, any sample \(\) can be decomposed into two orthogonal components: one that lies in the range space of \(\), and the other in the null space of \(\). Mathematically, this decomposition is:

\[^{}+(-^ {}),\] (1)

where \(^{}\) is the range space component, and \((-^{})\) is the null space component.

Through this decomposition, we derive two distinct elements indicative of consistency and photo-realism. The "range space" term \(^{}\) ensures consistency, as its multiplication by \(\) yields the measurement \(\), fulfilling the consistency condition \(=\). Conversely, the "null space" term \((-^{})\) ensures that the reconstructed image \(}\) aligns with natural image statistics.

### Mismatch in Convolutional Lensless Imaging Model

In lensless literature, most researchers  simplify the imaging process as a convolution, because the full transfer matrix \(^{N^{2} M^{2}}\) is too large to compute. Specifically, the measurement \(\) is obtained as \(=*\), where \(\) is the point spread function (PSF) of the lensless system and \(*\) denotes the convolution. Most previous reconstruction algorithms are based on this assumption.

However, the real lensless imaging model is not simply a spatially invariant convolution as shown in Fig. 4a. Consider a lensless camera consisting of a mask placed at the longitudinal position \(z=0\) and a sensor placed at \(z=d\). Let \(U(x,y,z)\) be a complex scalar wave field, a complex function of transverse coordinates \(x,y\), and longitudinal position \(z\). Denote the wave field immediately after propagating through the mask from a point source at infinity with an incident angle \(\) as \(U_{}(,,0^{+})\). Using the Huygens-Fresnel principle , the intensity pattern \(p_{}(x,y)\) captured by the sensor is:

\[p_{}(x,y)=|U_{}(x,y,d)|^{2}=|} U_ {}(,,0^{+})(jkr)d d|^{2},\] (2)

where the distance \(r\) is given by \(r=+(x-)^{2}+(y-)^{2}}\) and \(\) is the wavelength of light.

Therefore, according to Eqn. 2, the spatially invariant convolutional imaging model only satisfies under the Fresnel approximation :

\[r=+(x-)^{2}+(y-)^{2}} d+(x-)^{2}/(2d)+(y-)^{2} /(2d).\] (3)

This approximation is valid only when the distance \(d\) between the lensless mask and the sensor is large enough to satisfy \(d+(y-)^{2}}\). However, most lensless masks are very close to the sensor (\(d=2\) in a typical lensless camera), breaking this assumption.

This mismatch can lead to inaccuracies  in the convolutional imaging model. To demonstrate that, we simulate the PSF of a typical lensless camera (Phatcam ) across various incident angles \(\) using Eqn. (2). As shown in Fig. 4b, PSFs at different angles (\(0^{}\), \(15^{}\), and \(30^{}\)) are visually different. Quantitatively, Fig. 4c shows the similarity between PSFs at the center and -\(30^{}\) drops from 1.0 to 0.7.

Figure 4: Characterization of PSFs in Lensless Camera. (a) Illustration of light propagation in the lensless camera: two point sources A and B at infinity emitting parallel light beams. Source A emits at angle \(\) relative to the optical axis, causing a PSF shift on the sensor plane This PSF shift depends on both the incident angle \(\) and the distance \(d\) between the sensor and the mask. (b) Simulated PSFs for light sources at angles of \(0^{}\), \(15^{}\), and \(30^{}\). (c) Inner product similarity between the on-axis PSF and off-axis PSFs at different field positions. (d) Reconstruction using PSF at \(0^{}\), degradation is more significant at the periphery (red box) than the center (green box).

To further show the consequences of this mismatch, we simulate lensless capture from a clean scene image (Fig. 3(d) top), with the accurate spatially-varying PSF based on Eqn. (2), but solve the inverse imaging problem using a simple convolutional model using the PSF at the center. Fig. 3(d) bottom shows the result, and there are more artifacts at the boundary (red box) than at the center (green box). This is because the mismatch between the actual PSF and the single PSF used for deconvolution is more significant at the periphery field of view.

## 4 Method

In this section, we introduce **PhoCoLens**, a method for achieving both **Pho**torealistic and **C**onsistent reconstruction in **Lensless imaging. As illustrated in Fig. 5, PhoCoLens consists of two main stages: range space reconstruction and null space recovery. We will first introduce the whole framework and then provide a detailed explanation of each stage.

Given an input lensless measurement \(}=+\), where \(=\) is the noise-free part of the lensless measurement, \(\) is the transfer matrix of the lensless camera, \(\) is the original scene we aim to reconstruct, and \(\) is the sensor noise. The objective in lensless image reconstruction is to recover a photo-realistic image \(}\) which satisfies \(}=\).

Inspired by the range null space decomposition in section 3.1, we can decompose any potential solution \(}\) into two orthogonal components: the range space component that maintains consistency and null space component that maximizes photorealism. Formally, this decomposition is given by \(}=^{}+(- ^{})}\), where \(^{}\) is the range space component and \((-^{})}\) is the null space component. Note that any choice of \(}\) satisfies the equation \(}=\), as \((-^{})}=\).

With this decomposition, we reconstruct the range space and null space components in two stages, as illustrated in Fig. 5. In the first stage, we recover the range space content represented by \(^{}\) from the noisy lensless measurement \(}\). Specifically, we propose **SVDeconv**, a physics-inspired **S**patially-**V**arying **Deconvolution Network that learns to reconstruct the range space content effectively. The second stage focuses on adding the null space content represented by \((-^{})}\). For this purpose, we introduce _null space diffusion_, a conditional diffusion model designed for null space recovery. By conditioning on the range space reconstruction obtained from the first stage, the null space diffusion enhances these images by incorporating the null space content, thereby rendering them with a more realistic appearance. Below we introduce each stage.

### Spatially-varying Deconvolution Network

In the first stage, we use SVDeconv, a novel network designed to invert the forward lensless imaging model and reconstruct the range space content \(^{}\) from a lensless measurement \(}=+\) with noise \(\). SVDeconv is composed of two main components: a differentiable multi-kernel deconvolution layer and a refinement U-Net, as depicted in the gray box at the bottom left corner of Fig. 5.

Figure 5: System Overview. The two-stage pipeline begins with a spatially varying deconvolution network mapping lensless measurements to range space. Then a conditional diffusion model for null space recovery refines details using the first stage output, achieving the final reconstruction.

Traditional lensless imaging methods often employ a single PSF to model the imaging system, which is inaccurate for large field of view (FoV) scenarios as analyzed in section 3.2. To address this, SVDeconv utilizes a set of learnable \(K K\) PSF kernels, accounting for spatial variations across the field of view. Specifically, we partition the target image region into a grid of \(K K\) segments, and for each, we apply an individual PSF kernel. This process results in \(K K\) deconvolution operations, producing \(K K\) intermediate deconvolved images, as shown in Fig. 5. The multi-kernel deconvolution is mathematically described as a Hadamard product in the Fourier domain:

\[_{}^{(i)}=^{-1}((^{(i)}) (})),i=1,2,...,K K,\] (4)

where \(_{}^{(i)}\) is the \(i\)-th deconvolution results of the \(i\)-th learnable PSF kernel \(^{(i)}\), \(()\) and \(^{-1}(.)\) are the DFT and the Inverse DFT operations, and \(\) denotes the Hadamard product.

From this deconvolution operation, we obtain \(K K\) intermediate deconvolved images. Assuming that each image accurately reconstructs a specific region of the target image corresponding to its PSF's field point, we propose to integrate these images into a single, unified intermediate image using an innovative interpolation method. The interpolation is expressed as:

\[_{}(u,v)=_{i=1}^{K^{2}}w_{i}(u,v)_{}^{(i)}(u,v),\] (5)

where \(u,v\) are coordinates. The weight \(w_{i}(u,v)\) for each deconvolved image \(_{}^{(i)}\) is inversely proportional to the distance between the point \((u,v)\) and the focal center of each corresponding PSF, formally defined as:

\[w_{i}(u,v)=^{-}(u,v)}{_{j=1}^{K^{2}}d_{j}^{-}(u,v)},\ d_{i}(u,v)=(u-u_{i})^{2}+(v-v_{i})^{2}.\] (6)

In this way, it ensures that \(w_{i}(u,v)\) normalizes the contribution of each deconvolved image based on inverse Euclidean distance to each point \((u_{i},v_{i})\), which represents the center of the FoV corresponding to the \(i\)-th learnable PSF kernel \(^{(i)}\). This weighted sum effectively interpolates the intermediate images to form a unified representation with enhanced clarity and detail across the entire FoV.

One advantage of this weight design is that it significantly simplifies the calibration process. Previous multi-kernel methods, such as , often require multiple calibrated PSFs at different focal centers, leading to a time-consuming calibration process. On the contrary, our approach only requires one calibrated PSF to initialize all kernels. This is because we pre-define the center of the PSF and allow the model to learn its variations automatically.

After the interpolation, the intermediate image \(_{}\) is fed into the refinement U-Net , removing noise and artifacts in the reconstruction to approximate the range space content \(^{}\). We employ a combination of MSE loss and LPIPS loss  to train both the learnable PSFs and U-Net.

### Null Space Content Recovery

With the reconstructed range space content, we aim to recover a final image that maintains consistency and improves photorealism. To ensure consistency, the difference between the final image and the range space content (residual content) should reside in the null space, which ensures the final image aligns with the original lensless measurement. To improve photorealism, the combination of the null space and range space content should appear as a realistic real-world image.

To achieve both objectives, we propose null-space diffusion, which takes the reconstructed range space content as the condition and generates an image that adheres to both constraints. It ensures the residual content lies in the null space while maintaining its ability to produce realistic images.

To ensure the outputs from null-space diffusion meet the consistency requirement, we train the model such that the residual content of these samples falls within the null space. Mathematically, if we denote the output as \(}\), it should satisfy the following equation when conditioned by \(^{}\):

\[(}-^{})= }-=(} -)=.\] (7)

Recall that \(\) is the transfer matrix of the lensless camera and \(^{}\) is its pseudo-inverse. This equation indicates that applying the operator \(\) to the difference between the generated sample \(}\) and the original sample \(\) results in zero. This ensures that the residual content resides in the null space of \(\).

Based on this, we design the null space diffusion as follows. Given an image \(\) and its corresponding range space content \(=^{}\), we add noise progressively to the image resulting in a noisy image \(_{t}\), where \(t\) is the noise addition iteration. Similar to other conditional diffusion models , we train a network \(_{}\) to predict the noise added on the noisy image \(_{t}\) with the optimization objective:

\[_{}_{}=_{}_{,t,,(0,1)}[||(_{}( _{t},t,)-)||^{2}_{2}],\] (8)

which is derived to align with the goal in Eqn. (7). More details are in the Appendix.

Additionally, to ensure the null-space diffusion produces realistic images according to given conditions, we utilize a pre-trained diffusion model such as Stable Diffusion  with its weights frozen. We focus on training supplementary conditioning modules to guide the generative process using range space conditions while preserving its powerful ability to generate realistic images. Specifically, we follow the StableSR  structure, which involves using a conditional encoder to extract multi-scale features from the range space condition and then using them to modulate the intermediate feature maps of the residual blocks in the diffusion model. This modulation can align the generated images with the range space conditions, enhancing the fidelity and realism of output images.

## 5 Experiments

In this section, we assess the performance of our proposed method using two lensless imaging datasets, collected in real-world environments by two different kinds of lensless cameras: PhlatCam  and DiffuserCam . PhlatCam employs a phase mask, while DiffuserCam utilizes a diffuser for its lensless mask. We compare our approach with other methods and conduct a comprehensive ablation study to evaluate the effectiveness of our design.

### Dataset and Metrics

The _PhaltCam_ dataset  contains 10,000 images across 1,000 classes resized to 384\(\)384 pixels. Images are displayed and captured using a lensless PhlatCam , generating images at 1280 x 1480 pixels. We use 990 classes for training and 10 classes for testing, following the original protocol.

The _DiffuserCam_ dataset  contains 25,000 paired images captured simultaneously using a standard lensed camera (ground truth) and a mask-based lensless camera DiffuserCam . These pairs are split into 24,000 images for training and 1,000 for testing. Both cameras utilize sensors with a native resolution of 1080\(\)1920 pixels, which are first downsampled to 270\(\)480 pixels and then further cropped to a final resolution of 210\(\)380 pixels for proper display.

To evaluate both consistency (fidelity) and photorealism (visual quality), we employ two metric sets:

* **Full-reference metrics** to evaluate the consistency. Three full-reference metrics, PSNR, SSIM , and LPIPS , are used to evaluate the distance between the network output and ground truth, which indicates the fidelity of the reconstruction.
* **Non-reference metrics** to evaluate the photorealism. Three non-reference metrics, ManIQA , ClipIQA , and MUSIQ , are used to evaluate the visual quality of reconstructed images.

### Implementation Details

For SVDeconv, we utilize \(3 3\) PSF kernels for deconvolution. We initialize the 9 kernels using a single calibrated PSF from both the DiffuserCam and PhlatCam datasets. The U-Net component in SVDeconv is adapted from the U-Net used in Le-ADMM-U for training on the DiffuserCam, and from the U-Net in FlatNet-gen for the PhlatCam. We train the network for 100 epochs with a batch size of 5, using the Adam optimizer . The learning rate is set to 3e-5 for U-Net training in both datasets. For deconvolution kernel training, the learning rate is set at 4e-9 for PhlatCam and 3e-5 for DiffuserCam. We set the MSE and LPIPS loss weights to be 1 and 0.05, respectively. In the DiffuserCam dataset, where the measurements are cropped by the sensor's limited size, we employ replicate padding  which expands the width and height of the measurements by a factor of two.

For null-space diffusion, we use the range space content reconstructed by SVDeconv as input conditions. We train it for 200 epochs, following the training and inference settings used in StableSR.

In the first stage, SVDeconv is trained using range-space content derived from ground truth images. SVDeconv consists of two main parameter components: a learnable deconvolution kernel initializedwith known PSFs, and a U-Net initialized with standard weights without pretraining. Once trained, SVDeconv processes input lensless measurements to estimate the range-space content of training samples. Subsequently, we use this estimated range-space content as input conditions for fine-tuning via null-space diffusion. During diffusion fine-tuning, we utilize a pre-trained diffusion model with frozen weights. We only train the supplementary conditioning modules like StableSR , to guide the reconstruction process effectively.

### Comparison with Other Approaches

We evaluate the performance of our proposed method by comparing it against both traditional and learning-based approaches, using qualitative and quantitative measures on the two datasets.

We compare traditional methods like Tikhonov regularized reconstruction in the Fourier domain (WienerDeconv ) and total variation regularization via ADMM . Additionally, we evaluate learning-based methods like the unrolled network Le-ADMM-U , MMCN , and UPDN  and the feedforward deconvolution method FlatNet-gen. Additionally, we assess two diffusion-based methods: one uses a pre-trained Stable Diffusion  with a zero-shot inverse imaging sampling method DDNM+ , and another enhances outputs from FlatNet-gen using a pre-trained blind

   Dataset &  &  \\  Metrics & PSNR & SSIM & LPIPS\({}_{}\) & ManIQA & ClipIQA & MUSIQ & PSNR & SSIM & LPIPS\({}_{}\) & ManIQA & ClipIQA & MUSIQ \\  _Ground Truth_ & — & — & — & _0.431_ & _0.583_ & _63.40_ & — & — & — & _0.160_ & _0.333_ & _31.21_ \\ WienerDeconv  & 12.19 & 0.270 & 0.922 & 0.111 & 0.149 & 15.47 & 10.59 & 0.275 & 0.843 & 0.184 & 0.180 & 19.43 \\ ADMM  & 13.45 & 0.301 & 0.877 & 0.132 & 0.159 & 16.45 & 12.87 & 0.305 & 0.705 & 0.141 & 0.136 & 16.51 \\ Le-ADMM-U  & 20.12 & 0.515 & 0.405 & 0.138 & 0.180 & 24.64 & 22.35 & 0.668 & 0.253 & 0.110 & 0.185 & 20.16 \\ MMCN  & 20.39 & 0.524 & 0.346 & 0.145 & 0.206 & 27.65 & 24.09 & 0.744 & 0.238 & 0.121 & 0.183 & 21.47 \\ UPDN  & 20.48 & 0.533 & 0.352 & 0.158 & 0.215 & 29.32 & **24.67** & 0.747 & 0.256 & 0.139 & 0.178 & 22.56 \\ FlatNet-gen  & 20.53 & 0.549 & 0.375 & 0.203 & 0.287 & 44.94 & 21.43 & 0.696 & 0.254 & 0.134 & 0.244 & 23.15 \\ DDNM+ & 15.22 & 0.485 & 0.623 & 0.297 & 0.412 & 45.32 & 18.36 & 0.539 & 0.516 & 0.162 & 0.201 & 27.83 \\ FlatNet+DiffBIR  & 19.96 & 0.544 & 0.391 & 0.335 & 0.523 & 57.13 & 18.97 & 0.517 & 0.505 & **0.312** & **0.548** & **51.20** \\
**PhoCoLens(Ours)** & **22.07** & **0.601** & **0.215** & **0.357** & **0.565** & **62.20** & 24.12 & **0.748** & **0.161** & 0.172 & 0.339 & 32.84 \\   

Table 1: Performance comparison of methods on DiffuserCam and PhlatCam

Figure 6: Qualitative comparison between our method and others on the PhlatCam dataset.

Figure 7: Qualitative comparison between our method and others on the DiffuserCam dataset.

[MISSING_PAGE_FAIL:9]

reconstruction, using the original ground truth as the target. For a fair comparison, all methods utilize the same U-Net for denoising and perceptual enhancement and the same loss for training. Results in Tab. 2 show the efficacy of the proposed spatially-varying deconvolution in the faithful reconstruction of both range space content and original content. Further, Fig. 8 shows our method outperforms others, particularly in reconstructing fine details at the periphery (circled by white dashed boxes).

**Effectiveness of Null-space Diffusion**. Following the reconstruction of range space content, we perform null space content recovery conditioned on the previously reconstructed range space content. We compare our null-space diffusion model with other approaches, including conditional diffusion models like DiffBIR  and StableSR , as well as the zero-shot inverse imaging method DDNM . Quantitative results in Tab. 3 and qualitative comparison in Fig. 9 demonstrate that our null-space diffusion performs the best in both consistency with the ground truth and visual quality.

**Effectiveness of Range Content Conditions**. One important design of our framework is to use the first stage reconstruction as the condition for the second stage diffusion. To evaluate the effectiveness of these conditions, we perform an ablation study where we replace our reconstructed range content with the outputs from WienerDeconv, FlatNet-gen, and the SVD-OC (SVDencov trained with original content), and re-train the conditional diffusion models using these conditions. As evident from Fig. 10, our range content condition significantly improves reconstruction consistency compared to others. This is because using output from models like FlatNet-gen introduces additional artifacts that hinder the second stage from recovering the original image. Furthermore, quantitative analysis in Tab. 4 confirms this observation. Our method outperforms these conditions in all fidelity metrics.

## 6 Conclusion and Limitation

We present PhoCoLens, an innovative two-stage approach for achieving photorealistic and consistent reconstructions in lensless imaging. The method leverages the strengths of two complementary approaches: spatially varying deconvolution for the range space ensures consistency, while null-space diffusion for the null space guarantees photorealism. Our experiments on two lensless cameras demonstrate PhoCoLens' effectiveness in reconstructing realistic scenes while preserving fidelity. One limitation of our approach is that the two-stage nature of PhoCoLens and the diffusion model's sampling time hinder its real-time applicability. Additionally, although PhoCoLens achieves the best fidelity, the diffusion model we used may still introduce high-frequency details that deviate from the original scene, particularly for smooth scenes lacking in detail. Future work will focus on two main aspects: accelerating the diffusion model sampling process to enable real-time photo and video capture with lensless cameras, and exploring 3D spatially varying point spread function (PSF) effects to capture 3D scene information.