# ELDEN: Exploration via Local Dependencies

Zizhao Wang

University of Texas at Austin

zizhao.wang@utexas.edu

&Jiaheng Hu

University of Texas at Austin

jhu@cs.utexas.edu

&Peter Stone

University of Texas at Austin, Sony AI

pstone@cs.utexas.edu

Equal contribution

&Roberto Martin-Martin

University of Texas at Austin

robertomm@cs.utexas.edu

Equal supervision

###### Abstract

Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with _intrinsic reward_, a bonus signal that encourages the agent to visit _interesting states_. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain _whether_ (as opposed to _how_) entities such as the agent or objects have some influence on each other. We present **ELDEN**, **E**xploration via **L**ocal **D**ep**EN**dencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme -- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.

## 1 Introduction

Reinforcement learning (RL) has achieved remarkable success in recent years in tasks where a well-shaped dense reward function is easy to define, such as playing video games  and controlling robots . However, for many real-world tasks, defining a dense reward function is non-trivial, yet a sparse reward function based on success or failure is directly available. For such reward functions, learning good policies is often challenging, as it requires efficient exploration of the state space.

To address this challenge, RL researchers proposed the use of an _intrinsic reward_, an additional task-agnostic signal given to the agent for visiting _interesting states_. Intrinsic reward methods can be roughly classified into two main paradigms: curiosity  and empowerment , where the agent is rewarded either for visiting novel states or for obtaining maximal control over the environment, respectively.

While these methods significantly improve exploration in some domains, there are cases where the aforementioned methods fail. Consider, for example, a kitchen environment with several objects where there are multiple potential agent-object and object-object interactions, and an agent is tasked with putting a meatball in a pot and cooking it on the stove (Fig. 1). On the one hand, curiosity-driven methods will encourage the agent to explore the environment by visiting states where the exact outcome of an action is uncertain. Consequently, for each intractable object, the agent will exhaust any possible interaction until it can accurately predict every change in the object's state. As a result, such an exploration strategy can be inefficient, especially for environments with many objects. In the kitchen example, it is hard to predict how the meatball rolls in the pot, and thus the curiosity-driven agent would keep rolling it. On the other hand, for empowerment methods, the agent is encouraged to remain in states where it can influence as many states (objects) simultaneously as possible (e.g. holding the pot with the meatball inside). By doing so, however, it ignores object-object interactions that the action cannot directly control but indirectly induce, which can be the key to task completion. In the kitchen case, an empowerment-driven agent will therefore not be interested in placing the pot and the meatball on the stove, as it forfeits control of them by doing so, even though this action enables the stove to heat the meatball. Our main insight is that, in this type of environment, an intelligently exploring agent should be able to learn that it can use the pot to move the meatball after a few trials. Then, instead of spending time learning the complex meatball movement or different styles to manipulate the pot, it would move on to explore other modes of interacting with other objects, e.g., putting the pot on the stove and switching on the stove.

Following this motivation, we propose a new definition of interesting states -- focusing on _whether_ the environment entities (consisting of the agents and objects) can interact, rather than _how_ exactly they interact. We present ELDEN, **E**xploration via **L**ocal **D**ep**EN**dencies, a novel intrinsic reward mechanism that models the local dependencies between entities in the scene (agent-object, object-object) and uses the uncertainty about the dependencies to guide exploration. By relaxing the curiosity signal from dynamics prediction to dependencies prediction, ELDEN implicitly biases the exploration toward states where novel interaction modes happen rather than states where the state value is novel but dependencies remain the same. Specifically, ELDEN trains an ensemble of dynamics models. In each model, the local dependencies between objects are modeled by the partial derivatives of state predictions w.r.t. the current state and action. Then, the local dependency uncertainty is measured as the variance across all dynamic models.

We evaluate ELDEN on discrete and continuous domains with multiple objects leading to many interaction modes and tasks with chained dependencies. Our results show that using a partial

Figure 1: **(Left) In a kitchen task with multiple potential agent-object and object-object interactions, (Middle) for a curiosity-based agent interested in hard-to-predict entity motion, it will initially focus on exploring arm movement, then on pot and meatball manipulation, and finally keep rolling the meatball whose outcomes are challenging to predict. On the other hand, for an empowerment-based agent interested in maximizing the actionâ€™s influence, it begins with controlling the arm and then learning to move the pot and meatball simultaneously, but it ignores the potential interaction between the stove and the meatball. (Right) ELDEN avoids those issues by identifying whether dependencies between entities happen and focusing the exploration on novel ones. After the agent learns that it can control the pot and meatball, it will move on to explore other potential interactions, e.g., whether the stove can influence the meatball. Hence it has a larger opportunity to learn this task, compared with a curiosity or empowerment-based agent.**

derivative-based extractor on dynamic models training allows us to accurately identify local connectivities between environment entities. Furthermore, the intrinsic reward derived from the identified local connectivities allows ELDEN to outperform state-of-the-art exploration methods (curiosity and empowerment-driven) on the tested domains.

## 2 Related Work

The idea of using intrinsic reward to facilitate exploration in reinforcement learning is a long-studied topic that can be dated back to Schmidhuber . In this section, we first discuss two main classes of intrinsic reward related to ELDEN. Since ELDEN requires reasoning about the local dependencies between environment entities, we also discuss works that involve utilizing dependencies/causality in reinforcement learning.

### Curiosity-Driven Exploration

Curiosity-driven exploration rewards an agent for visiting "novel" states, where different methods define the "novelness" of a state in different ways. For methods that utilize visit count to define state novelty, Bellemare et al.  utilized a density model to estimate pseudo-count for each state, which is then used to derive intrinsic reward for the agent; Tang et al.  uses locality-sensitive hashing code to convert high-dimensional states to hash codes such that the visit count for each hash code can be explicitly kept track of. For methods that utilize predictiveness to define state novelty, Stadie et al.  learns a forward dynamics model that operates on a learned latent space of the observation, and uses the prediction error as the intrinsic reward for exploration; Burda et al.  uses randomly initialized networks to extract state features, where the agent is encouraged to visit states where predictions about the state features are inaccurate; Pathak et al.  utilizes disagreements within an ensemble of dynamic models as a signal for intrinsic reward, and directly backpropagates from the reward signal to the policy parameters to improve exploration. Pathak et al.  incorporates empowerment into the curiosity-based method by learning an inverse dynamics model that maps from state to action. The inverse dynamics model defines a feature space for the states, where the prediction error of a forward dynamic model is measured in this feature space.

However, learning accurate dynamics can be difficult and require a significant coverage of a (possibly large) state-action space; e.g., when a robot manipulates a block, it would have to experience multiple action-reaction pairs to be able to make accurate predictions . Furthermore, prior curiosity-driven exploration methods can be derailed by the stochasticity in the dynamics, i.e., when the outcome of an action has large entropy (e.g., tossing a coin), so the agent would keep repeating it. ELDEN can be considered a type of curiosity-driven exploration method. However, unlike previous works which only consider improving knowledge about the dynamics, ELDEN explicitly considers knowledge about local dependencies between environment entities, and utilizes it to encourage exploration. Thus, it avoids the need to learn an accurate dynamic model and is less sensitive to environmental noise and stochasticity.

### Empowerment-Driven Exploration

Empowerment-based exploration methods are based on a different understanding of what states should be encouraged for task-agnostic exploration [35; 27; 8; 19]. Their main idea is that the most interesting states to explore for any task are states where the agent has the most controllable diversity about what the next state will be, i.e., states where there are multiple possible next states that can be chosen by the agent. From those states, it is easier to fulfill any downstream task that requires purposefully changing the state. Empowerment-based exploration methods reason about the controllable elements in the environment (states that can be influenced by agent's actions), and encourage the agent to find states where this controllability is large, typically through some form of mutual information maximization between the agent's actions and the next states. In particular, Zhao et al.  uses domain knowledge to divide up the state space into internal state and external state, and maximize the mutual information between them. Seitzer et al.  measures the local dependencies between action and environment entities by estimating their conditional mutual information (CMI) and using it as intrinsic reward signal to encourage the agent to maximize the influence of action over environment entities.

However, due to the difficulty in measuring the mutual information across a multi-step trajectory, existing empowerment-based methods only measure 1-step empowerment, e.g., how much the agent directly influences variables. Thus they cannot detect dependencies between objects (e.g., indirect tool use such as using the stove to cook meals) and the environment's influence on the agent (e.g., the elevator can take the agent to the desired floor). Furthermore, since the objective of empowerment-based methods is to maximize controllability, they tend to only control the easiest-to-manipulate object to maximize its empowerment when there are multiple controllable variables.

ELDEN is closely related to Seitzer et al. , but differs from it in three main aspects: first, unlike Seitzer et al. , which only considers the interaction between action and environment entities,ELDEN also considers the interaction between environment entities that are not locally dependant on the action, which allows curiosity about indirect interaction between the agent and the environment entities to propagate through time during the RL training. Second, unlike Seitzer et al. ,ELDEN tries to visit states with novel interactions, instead of maximizing controllability, thus avoiding the tendency to only interact with easy-to-control objects. Lastly,ELDEN estimates local dependencies through reasoning about the partial derivatives of learned dynamic models, which we empirically show to be more accurate and computationally efficient compared to the CMI-based estimation in Seitzer et al. .

### Causality in Reinforcement Learning

The concept of incorporating causality in the training of a reinforcement learning agent has been utilized in many different forms. Wang et al.  demonstrates that incorporating causal dependencies between environment entities can greatly improve generalization to out-of-distribution states. Hu et al.  exploited causal dependencies between action dimensions and reward terms to reduce variance in the gradients and facilitate policy learning of mobile manipulation tasks. Pitis et al.  shows that knowing the local causal dependencies between objects can facilitate policies to generalize to unseen states and actions. Pitis et al.  uses local dependencies to generate counterfactual samples in order to facilitate sample efficiency. Sontakke et al.  discovers causal factors in the dynamics of a given environment through a causal curiosity reward term.

Like in Pitis et al. ,ELDEN learns to predict the local connectivities between environment entities depending on the state and the action values. However, we do not generate counterfactuals but create an intrinsic reward based on the local dependency that facilitates exploration with RL in sparse-reward setups.

## 3ELDEN: Exploration via Local Dependencies

In the section, we introduceELDEN, which infers the local dependencies between environment entities and uses the uncertainty of dependencies as an intrinsic reward for tackling hard-exploration problems. In Sec. 3.1, we formally define the problem setup ofELDEN. In Sec. 3.2, we discuss howELDEN uncovers local dependencies. In Sec. 3.3, we describe howELDEN improves exploration with the intrinsic reward.

### Problem Statement

We consider decision-making as a discrete-time Markov Decision Process (\(\), \(\), \(\), \(\)), where \(\) is a state space which we assume can factored as \(=^{1}^{N}\), \(\) is an action space, \(\) is a Markovian transition model, and \(\) is a reward function. The goal of the RL agent is to optimize the parameters \(\) of a policy \(_{}\) such that the total expected return under \(_{}\) is maximized. Specifically, we focus on cases where \(\) is sparse, and therefore intelligent exploration is crucial to the discovery of optimal policies.

Local Causal Graph ModelWe can model the transition at time step \(t\) as a Causal Graphical Model (CGM)  consisting of (1) nodes \((_{t},_{t},_{t+1})\), (2) a directed graph \(\) describing _global_ dependencies between nodes, and (3) a conditional distribution \(p\) for each state variable at the next time step, \(^{n}_{t+1}\). We assume the transition can be factorized as \((s_{t+1}|s_{t},a_{t})=_{n=1}^{N}p(s^{n}_{t+1}|( ^{n}_{t+1}))\), where the \((v)\) are parents of a node \(v\) in the causal graph \(\). For many environments, \(\) can be dense or even fully connected, because whenever it is possible for \(^{j}\) to depend on \(^{i}\), no matter how unlikely, it is necessary to include the edge \(^{i}^{j}\). However, in the real world, even if possible to interact, most entities are independent of each other most of the time. Following this observation, we are interested in inferring _local_ dependencies that are specific to \((s_{t},a_{t})\), represented by a local causal graph \(_{t}\) that is minimal by removing inactive edges in \(\).

### Identifying Local Dependencies with Dynamics Partial Derivatives

Based on the definition in Sec. 3.1, the key component of ELDEN is to accurately evaluate which potential dependencies between environment entities are locally active, i.e., identify the local causal graph \(_{t}\) given \((s_{t},a_{t})\). This identification requires answering a challenging question -- whether entity \(i\)'s value, \(^{i}_{t}=s^{i}_{t}\) is the cause of entity \(j\) to have value \(^{j}_{t+1}=s^{j}_{t+1}\). ELDEN approaches it with the inspiration from the _but-for_ test: the local dependency exists if \(^{j}_{t+1}=s^{j}_{t+1}\) would not happen but for \(^{i}_{t}=s^{i}_{t}\). In other words, we assess whether \(^{j}_{t+1}\) would change if \(^{i}_{t}\) has a different value. Notice that, since we focus on _local_ dependencies, we only want to vary \(^{i}_{t}\) near its actual value \(s^{i}_{t}\) rather than trying all its possible values.

To this end, ELDEN utilizes partial derivatives to identify local dependencies, as they naturally capture the extent of change in \(^{j}_{t+1}\) with respect to \(^{i}_{t}\). Specifically, assuming the access of ground truth transition probability \(p\) (which we will relax later), ELDEN considers \(^{j}_{t+1}=s^{j}_{t+1}\) locally depends on \(^{i}_{t}=s^{i}_{t}\) if

\[|_{t+1}|s_{t},a_{t})}{ s^{i}_{ t}}|,\] (1)

where \(\) is a predefined threshold. A large partial derivative indicates that a slight change in \(^{i}_{t}\) will lead to a substantial change in \(^{j}_{t+1}\), thus satisfying the but-for test.

To evaluate partial derivatives without the ground truth transition probability, ELDEN approximates \(p\) with a dynamics model \(f\) parameterized by a neural network \((s^{j}_{t+1})=f(s_{t},a_{t})\) and trains \(f\) by maximizing the log-likelihood of \((s^{j}_{t+1})\) (for notational simplicity, we omit the conditionals \(s_{t},a_{t}\) in \(p\) in this section). Due to limited data and training errors, even when two environment entities are not locally dependent, there occasionally exists a large partial derivative between them. To reduce such false positives, ELDEN further applies regularization to suppress partial derivatives w.r.t. inputs that are not necessary for predicting \(s^{j}_{t+1}\). The overall loss of dynamics training is

\[L_{f}=-(s^{j}_{t+1})+_{i,j}|(s^{j}_{t+1})}{ s^{i}_{t}}|,\] (2)

where \(\) is the regularization coefficient.

### ELDEN Policy Learning

ELDEN utilizes the local dependency identification described in Sec. 3.2 to improve exploration for model-free RL. The key idea behind ELDEN is to encourage an agent to visit states where new local dependencies are likely to emerge. When the ground truth local dependencies are available, the novelty of local dependencies between state variables can be measured by the magnitude of error of the dependencies identified by our method. Unfortunately, in many cases, it is hard to manually specify the ground truth local dependencies. Instead, ELDEN trains an ensemble of dynamics models, and measures dependency novelty by the variance of the local dependency graphs extracted independently from each of the dynamics models in the ensemble. Specifically, ELDEN first computes the variance of each edge in the graph and then uses the mean of the edge variance as the graph variance. Finally, the calculated variance is used as the intrinsic reward and is scaled with a coefficient \(\) that controls the magnitude of the exploration bonus, and added to the task reward \(r_{}\).

ELDEN is applicable to both on-policy and off-policy settings. We show the pseudo-code for on-policy ELDEN in Algorithm 1. The off-policy version of ELDEN can be easily derived by updating the policy with transitions sampled from the replay buffer in line 9 of Algorithm 1. Importantly, any model-free RL algorithm can be used for the policy update step.

## 4 Experiments

In our experiments, we aim to answer two main questions: _Q1:_ Is ELDEN able to accurately detect local dependencies between environment entities in factored state spaces (Sec. 4.1)? _Q2:_ Does ELDEN improve the performance of RL algorithms in sparse-reward environments with chained dependencies (Sec. 4.2)?

EnvironmentsAs shown in Fig. 2, we evaluate ELDEN in four simulated environments with different objects that have complex and chained dependencies: (1) CarWash, (2) Thawing, (4) 2D Minecraft and (3) Kitchen. Both CarWash and Thawing are long-horizon household tasks in discrete gridworld from the Mini-BEHAVIOR Benchmark . Minecraft 2D is an environment modified from the one used by Andreas et al. , where the agent needs to master a complex technology tree to finish the task. Kitchen is a continuous robot table-top manipulation domain implemented in RoboSuite . To complete tasks in these environments and receive the sparse reward, the agent has to conduct a series of actions that change not only the state of the interacted entities but also induce further interaction between interacted entities and others (e.g., interacting with the store switch that enact interaction between the stove and the cooking the meatball). The agents in all environments can select between a set of action primitives, a set of discrete actions that can be applied on each object, e.g., goTo(obj) or pick(obj). Notice that even with the action primitives, these domains are still very hard to solve due to the presence of many interaction modes and the difficulty in finding the correct (potentially long) sequence of interactions among many options that will lead to task success. We provide further descriptions of each environment in Appendix Sec B.

Figure 2: We test ELDEN on three domains and four environments. **(a) (b)** Mini-behavior and **(c)** Minecraft 2D with discrete state spaces, where the agent has to achieve a series of temporally extended tasks with complex object interactions. **(d)** Robosuite, a robot table-top manipulation simulation environment with continuous state spaces, where the robot needs to perform multiple interdependent subtasks to finish the cooking task.

Implementation DetailsFor discrete state or action spaces, the partial derivatives w.r.t. \(s_{t}/a_{t}\) are undefined. To address this issue, we use Mixup  to create synthetic inputs and labels by linearly combining pairs of inputs and labels, thus approximately changing the input space to be continuous. Compared to learning from discrete inputs only, dynamics models trained on such data generate partial derivatives that better reflect local dependencies, as shown in Sec 4.3.1. For the 2D Minecraft and Kitchen environments where some local dependencies have complex preconditions and thus are hard to induce, we apply sample prioritization to dynamics learning, where the priority is measured as prediction error. In this way, the dynamics model gets aware of unknown interactions faster and guides the exploration more efficiently than not using prioritization. Further details are provided in the Appendix.

### Evaluating the Detection of Local Dependencies

We compare the local dependencies extracted by ELDEN with the following baselines (see implementation details in Appendix Sec. C):

* **pCMI** (point-wise conditional mutual information) [27; 31]: defined as \(_{t+1}|s_{t},a_{t})}{p(s^{j}_{t+1}|s_{t} s^{j}_{t}, a_{t})}\). It quantifies how likely it is that \(s^{j}_{t+1}\) depends on \(s^{i}_{t}\).
* **Attn** (attention): Use the score between each entity pair computed by the attention modules inside the dynamics model to quantify local dependencies.
* **Input Mask**: we implement a learnable binary mask to the dynamics model that can zero out some inputs conditioned on \((s_{t},a_{t})\): \(f([s_{t},a_{t}] M(s_{t},a_{t}))\). During training, the mask is regularized to use as few inputs as possible with L1 regularization, leading to a quantification of minimal local dependencies.
* **Attn Mask**: we implement a learnable mask to the dynamics model similar to the one in Input Mask, but in this case, the mask is applied to the attention scores. The mask is regularized following the method by Weiss et al. .

We train the dynamics model of each method with three random seeds on pre-collected transition data and evaluate their performance by predicting the local causal graph \(_{t}\) for 50 unseen episodes based on the state-action pair \((s_{t},a_{t})\). We compare their predictions with the ground truth local dependencies extracted from the simulator. In the three environments, many potential local dependencies are inactive most of the time, and thus only a small portion (\( 3\%\)) of the ground truth labels indicate the existence of local dependencies for a given entity pair. To account for such imbalance, we use the area under the receiver operating characteristic curve (ROC-AUC) and the best achievable F-score (F1) as evaluation metrics.

The results of the evaluation on the detection of local dependencies are summarized in Table 1. ELDEN outperforms all baselines in terms of ROC-AUC consistently across all environments (Q1). For the F1 score, pCMI performs best in most environments (especially in the more complex CarWash and Kitchen), but ELDEN performs comparably or achieves the second-best F1 scores with much less computation: pCMI computation cost is \(N\) times higher than ELDEN, where \(N\) is the number of environment entities, and thus pCMI scales badly to environments with a large number of objects. Further evaluation details can be found in the Appendix.

    &  &  &  \\  & ROC AUC & F1 & ROC AUC & F1 & ROC AUC & F1 \\  ELDEN & **0.71** \(\) 0.01 & 0.57 \(\) 0.00 & **0.78**\(\) 0.02 & 0.66 \(\) 0.02 & **0.66**\(\) 0.01 & 0.25 \(\) 0.01 \\ pCMI & 0.55 \(\) 0.01 & 0.60 \(\) 0.00 & 0.73 \(\) 0.02 & **0.78**\(\) 0.01 & 0.60 \(\) 0.00 & **0.28**\(\) 0.00 \\ Attn & 0.65 \(\) 0.04 & **0.63**\(\) 0.01 & 0.66 \(\) 0.01 & 0.55 \(\) 0.03 & 0.51 \(\) 0.01 & 0.22 \(\) 0.02 \\ Input Mask & 0.50 \(\) 0.00 & 0.40 \(\) 0.00 & 0.50 \(\) 0.00 & 0.32 \(\) 0.01 & 0.50 \(\) 0.00 & 0.08 \(\) 0.00 \\ Attn Mask & 0.45 \(\) 0.03 & 0.47 \(\) 0.02 & 0.47 \(\) 0.07 & 0.43 \(\) 0.03 & 0.52 \(\) 0.01 & 0.13 \(\) 0.01 \\   

Table 1: Mean \(\) std. error of ROC AUC (\(\)) and F1 (\(\)) of local dependency prediction 

### Evaluating Exploration in Sparse-Reward RL Tasks

The ultimate goal of our method is to improve exploration for RL in sparse-reward setups. In the second evaluation, we compare the performance of ELDEN against several state-of-the-art intrinsic-motivation exploration algorithms in reinforcement learning, including:

* **Disagreement**: the intrinsic reward is computed based on the variance of the predictions from an ensemble of forward dynamics models.
* **Dynamics Curiosity**: intrinsic reward is computed based on the prediction error of a trained forward dynamics model.
* **CAI** (Causal Influence Detection) : an empowerment-based method, where the agent is given intrinsic reward for maximizing the number of state entities that depend on its action.
* **Vanilla PPO**: baseline without intrinsic reward that serves as control signal.

While ELDEN can be used with any RL algorithm, in our experiments we use proximal policy optimization (PPO) , as well as with the baselines. To facilitate the introspection of the results, we define manually a set of semantic stages representing internal progress toward task completion. The stage definitions for each of the environments are described in detail in the Appendix. Notice that these stages are not used by the agents during training and do not provide any additional reward; they are only used to facilitate the analysis of the results.

Fig. 3 depicts the count of reached stages per episode during training for each task, normalized by the number of stages to complete the task. In the normalized stage count, a value of \(1\) corresponds to successfully completing the task and it is the only stage where the learning agents receive sparse task reward (not intrinsic). Fig. 3 indicates that ELDEN is able to learn successful policies in all four environments. Importantly, in CarWash, 2D Minecraft and Kitchen, ELDEN is the only method that successfully learns to complete the task, demonstrating the advantage of ELDEN over the baseline algorithms in tackling tasks with complex chained dependencies (Q2).

The normalized stage count of ELDEN in CarWash, 2D Minecraft and Kitchen does not converge to \(1\) (completing the entire task in all episodes) mainly due to two reasons: First, in both tasks, the locations of the objects are randomly initialized at the start of each episode. For some initialization (e.g. a target object is blocked by unmovable obstacles), the task is impossible to solve. Second, in both tasks, two out of the three ELDEN training procedures with different random seeds converge to succeeding most of the time, but the training process with one seed fails to find a good policy, dragging down the mean value of the normalized stage count. This large variance in success is a current limitation of ELDEN.

In the relatively simple Thawing environment, we found ELDEN does not provide a significant advantage over the other baseline methods. The **Dynamics Curiosity** baseline learns faster to achieve the task indicating a better sample efficiency. This was rather expected: as with any exploration heuristic, ELDEN is not universally better than previous intrinsic reward methods -- instead, it is better suited for a specific type of environment, where there are many complex and chained object

Figure 3: Learning curve of ELDEN (ours) compared to baseline approaches. Each method uses three random seeds, and we show the mean \(\) std dev of the number of stages completed toward task success. The stage count is normalized to , where 1 corresponds to task completion. ELDEN learns successful policies in all four test environments, and is the only method that succeeds in the _CarWash_, _2D Minecraft_, and _Kitchen_ environments with complex chained dependencies.

dependencies, not the case for Thawing. We provide additional experimental evaluations of the failure cases of ELDEN in Appendix Sec E.

### Ablation Studies

We ablate different components of ELDEN to examine their importance to the overall methods.

#### 4.3.1 Ablations for Local Dependency Detection

In our ablation study on ELDEN for local dependency detection, we investigate the impact of each component with the following variations:

* No Mixup & No Reg: We disable the use of Mixup for discrete space prediction, and no partial derivative regularization is applied in this case.
* Different partial derivative regularization coefficients: we test with different \(\) values in \(\{0,10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\}\).

As shown in Table. 2, in Thawing and CarWash environments, partial derivative regularization with appropriate coefficients significantly improves ELDEN's detection of local dependencies, compared to no regularization (i.e., \(=0\)) or inappropriate \(\) values. Furthermore, in discrete-state environments, Mixup smooths the landscape of partial derivatives by providing synthesized continuous inputs as exemplified in Fig. 1(b) of Zhang et al. , thus facilitating local dependency prediction -- even when compared to using Mixup without any regularization, not using Mixup leads to a noticeable degradation in the prediction performance.

#### 4.3.2 Ablations for Task Learning

Next, we examine how different components and hyperparameters of ELDEN affect task learning:

**Ablation of Local Dependency Metrics** We compare the exploration performance when using different local dependency detection methods. Specifically, we compare with pCMI as it achieves the best local dependency detection in Sec. 4.1. We present the comparison results between ELDEN and pCMI in the Kitchen environment in Fig. 4(a) where both methods successfully learn to solve the task. However, it is important to notice that the computation cost of pCMI is \(N\) times more than that of ELDEN (where \(N\) is the number of environment entities), and thus may not scale to environments with a large number of entities.

**Ablation of Dynamics Sample Prioritization** We study the effectiveness of applying sample prioritization in dynamics model training. Specifically, we test ELDEN with and without prioritization in the Kitchen environment, and show the result in Fig. 4(b). We can see that ELDEN without prioritization fails to learn a useful policy. The reason is that some key entity interactions occur rather rarely before the agent masters them, e.g., frying meatball with butter. In such cases, the dynamics model needs to quickly learn that unknown dependencies appear so that it can bias the exploration toward reproducing such dependencies. Sample prioritization helps the dynamics model learn such

    &  &  &  \\  & ROC AUC (\(\)) & F1 (\(\)) & ROC AUC (\(\)) & F1 (\(\)) & ROC AUC (\(\)) & F1 (\(\)) \\  no Mixup \&  &  &  &  &  &  \\ no Reg, & & & & & & \\ i.e., \(=0\) &  &  &  &  &  &  \\ \(=10^{-1}\) & & & & & & \\ \(=10^{-2}\) & & & & & & \\ \(=10^{-3}\) & & & & & & \\ \(=10^{-4}\) & & & & & & \\ \(=10^{-5}\) & & & & & & \\   

Table 2: Ablation of ELDEN on local dependency prediction (mean \(\) std. error of ROC AUC and F1)infrequent dependencies quickly, making it critical in environments with novel and hard-to-induce local dependencies.

**Ablation of Partial Derivative Threshold:** The partial derivative threshold \(\) determines the dependency predictions. A threshold that is too large / too small will make all dependency predictions negative / positive respectively, leading to deteriorated performance. In this section, we examine whether our method is sensitive to the choice of threshold in the CarWash environment, where the results are presented in Fig. 4(c). We observe that our method is relatively sensitive to the choice of threshold, and an inappropriate threshold could cause catastrophic failure. A potential next step for ELDEN is to automatically determine the partial derivative threshold.

**Ablation of Intrinsic Reward Coefficient:** The intrinsic reward coefficient controls the scale of the intrinsic reward relative to the task reward. We examine the effect of this coefficient by experimenting with different values in the CarWash environment, where the results are presented in Fig. 4(d). We find that our methods work well in a large range of the intrinsic reward coefficients (1 - 10), since the task only gives sparse rewards and the intrinsic rewards are the only learning signal most of the time. The only exceptions are (1) when the intrinsic reward coefficient is too large (e.g., 100), the intrinsic reward significantly surpasses the task reward, and (2) when the coefficient is too small (e.g., 0.1), the episode intrinsic reward is too small (e.g., 0.03) for PPO to learn any useful policy.

## 5 Limitations and Conclusion

We introduce ELDEN, a method for improving exploration in sparse reward reinforcement learning tasks. ELDEN identifies local dependencies between environment entities and uses the uncertainty about such dependencies as an intrinsic reward to improve exploration. Experiments demonstrate that ELDEN uncovers local dependencies more accurately compared to related methods, and significantly outperforms previous exploration methods in tasks with complex chained dependencies.

However, ELDEN is not without limitations. First, ELDEN intentionally bias exploration towards "covering up the possible interactions between objects" rather than "becoming an expert at manipulating a particular object". While such an inductive bias works well in many practical domains, it may fail when facing tasks that require precise object interaction (e.g. rotating the meatball in the pot to a specific orientation). A future direction to alleviate this problem and expand the scope of solvable tasks is to combine ELDEN with dynamics curiosity and formulate a composite intrinsic reward. Second, as noted in the experiment section, the variance of ELDEN across different random seeds can be large, While the high variance is a general problem to Reinforcement Learning, finding ways to further stabilize ELDEN can be an important direction for future work.

Figure 4: Ablation of ELDEN on task learning. Each curve uses three random seeds and shows the mean \(\) std dev of the normalized stages. We found ELDEN to have moderate tolerance towards hyperparameters. We found sample prioritization in dynamics learning to be crucial to the performance of ELDEN.

AcknowledgementsThis work has taken place in the Robot Interactive Intelligence Lab (RobIn) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin. Both LARG and RobIn are supported by Good Systems, a research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research. We thank Bo Liu and Caleb Chuck for their valuable feedback on the manuscript.