# Rule Based Rewards for Language Model Safety

Tong Mu &Alec Helyar &Johannes Heidecke &Joshua Achiam &Andrea Vallone

Ian Kivlichan &Molly Lin &Alex Beutel &John Schulman &Lilian Weng

###### Abstract

Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior. However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental. Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. _refusals should not be judgmental_) along with a LLM grader. In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating. We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.

Content may include language related to racism, erotic themes, self-harm, or other offensive material.

## 1 Introduction

As large language models (LLMs) grow in capabilities and prevalence, it becomes increasingly important to ensure their safety and alignment. Much recent work has focused on using human preference data to align models, such as the line of work on reinforcement learning from human feedback (RLHF). However, there are many challenges in using human feedback alone to achieve a target safety specification. Collecting and maintaining human data for model safety is often costly and time-consuming, and the data can become outdated as safety guidelines evolve with model capability improvements or changes in user behaviors. Even when requirements are relatively stable, they can still be hard to convey to annotators. This is especially the case for safety, where desired model responses are complex, requiring nuance on whether and how to respond to requests. If instructions are underspecified, annotators may have to rely on personal biases, leading to unintended model behaviors, such as becoming overly cautious, or it responding in an undesirable style (e.g. being judgmental). For example, some annotators in one of our experiments, when ranking possible responses to user requests pertaining to self-harm, favored completions that referred the user to a US suicide hotline phone number, which would not have helped users in other regions. Fixing such issues often requires relabeling or collecting new data, which is expensive and time consuming.

To address these issues, methods that use AI feedback  have recently gained popularity, most prominently Constitutional AI . These methods use AI feedback to synthetically generate training data to combine with the human data for the supervised fine-tuning (SFT) and reward model (RM)training steps. However, in Bai et al.  and other methods, the constitution involves general guidelines like "choose the response that is less harmful", leaving the AI model a large amount of discretion to decide what is harmful. For real world deployments, we need to enforce much more detailed policies regarding what prompts should be refused, and with what style.

In this work, we introduce a novel AI feedback method that allows for detailed human specification of desired model responses, similar to instructions one would give to a human annotator. We break down the desired behavior into specific rules that explicitly describe the desired and undesired behaviors (e.g. _"refusals should contain a short apology"_, _"refusals should not be judgemental toward the user"_, _"responses to self-harm conversations should contain an empathetic apology that acknowledges the user's emotional state."_). This separation into rules is similar to the human feedback method proposed in Sparrow, however we focus on utilizing AI feedback as opposed to human feedback. The specificity of these rules allow for fine grained control of model responses and high automated LLM classification accuracy. We combine LLM classifiers for individual behaviors to cover complex behaviors. Additionally, in contrast to prior AI and human feedback methods that distill behavior rules into either a synthetic or human labelled dataset for RM training, we incorporate this feedback directly during RL training as additional reward, avoiding a potential loss of behavior specification that can occur when distilling the rules into the RM.

**Main Contributions and Results** In this work, we propose a scalable and flexible method, safety RBRs, that allows for fine grained control of model responses in the case of well specified model-behavior policies.

1. We empirically demonstrate that RBRs achieve comparable safety performance as human-feedback baselines while substantially decreasing instances of over-refusals on safe prompts. Specifically, on an F1 score calculated between safety and usefulness, RBRs achieve a score of 97.1, compared to a human-feedback baseline of 91.7 and a helpful-baseline of 95.8.
2. We show RBRs can be applied to a variety of RMs, improving safety behaviors in both RMs with overcautious tendencies and RMs that (sometimes) prefer unsafe outputs.
3. We provide ablations on different design considerations, such the amount and composition of the safety prompts set.

## 2 Related Works

**Reinforcement Learning from Human Feedback (RLHF):** Research in RLHF methods [1; 2; 3; 7] demonstrates the efficacy of human annotations in steering model behavior. A subset [4; 8; 13] of this RLHF research considers achieving better safety behavior through methods such as separating out signals of helpfulness and harmlessness. Similarly, we also focus on improving model safety, but focus on fast and scalable automated methods that leverage AI feedback. Most related to our work, Sparrow proposes a novel approach to RLHF which trains a second rule-conditioned RM to detect potential rule violations. Like Sparrow, we also use rules, but we have a few key differences. Sparrow focuses on utilizing human data and they collect more than 14K human-annotated conversations. We instead focus on utilizing AI feedback. Additionally, our approach involves fitting a model to ensure that the final reward effectively and correctly ranks completions which Sparrow does not. Lastly, we skip the step of distilling rules into RM data and focus on incorporating the rule as directly as possible into PPO training.

**Reinforcement Learning From AI Feedback (RLAIF)** To address the cost and time of collecting human data, work that uses AI feedback to improve models have been a topic of recent study in both safety (such as CAI [10; 11]), and non-safety settings (RLAIF ). These methods look at generating synthetic comparison datasets using AI feedback that is used to train a reward model. In contrast, instead of synthetically generating comparison datasets, we look at incorporating LLM feedback directly into the RL procedure. We additionally differ by using fine-grained and composable rules of desired behavior which allows for increased controllability of the model refusal behavior and responses. Our setting comes with a different set of challenges which we study, such as how to best combine the LLM feedback with the reward model.

**Additional Related Methods:** Additional related work include studies on improving the final outputs or finetuning on top of a model([14; 15]. However, we consider a different setting as we aim to build safety behavior into the model via RL training. Our approach is also loosely related to work that considers different ways of designing rewards for LLMs, such as RAFT .

Setting and Terminology

We consider a production setup of an AI chatbot system where a pretrained large language model (LLM) is periodically finetuned to align to an updated behavior specification, using a standard pipeline of first supervised fine-tuning (SFT) the model and then applying reinforcement learning from human preferences (RLHF). At the RLHF stage, we first train a reward model (RM) from preference data and then train the LLM against the RM via an reinforcement learning (RL) algorithm like PPO . We assume that we already have the following data standard for RLHF:

* Helpful-only SFT demonstrations contains examples of helpful conversations.
* Helpful-only RM preference data tracks comparisons between chatbot responses, where in each comparison a human annotator has ranked the completions based solely on their helpfulness to the user.
* Helpful-only RL prompts is a dataset of partial conversation prompts that do not contain requests for unsafe actions.

Additionally, we assume we have:

* A Moderation Model: For both human feedback baselines and automated methods we need a method of obtaining relevant safety RL prompts. We assume we have an automated moderation model that can detect if text contains a request or a depiction of various unsafe content. Pre-existing models such as ModerationAPI  can be used. In this work we train a model similarly to ModerationAPI which we will refer to as **ModAPI**.
* Safety-relevant RL prompts (\(_{s}\)): A dataset of conversations ending in a user turn, some of which end with a user request for unsafe content. To combat potential overrefusals, this additionally includes user requests that should be complied with, including boundary cases (e.g. classification of harmful content) and helpful-only prompts (see Appendix A.1.4 for details and breakdowns). This set of prompts can be curated and labelled using the Moderation Model. We used a total of 6.7k conversations.

Furthermore, we assume that a process of deliberation has occurred between relevant stakeholders to produce both a newly-updated **content policy** (a taxonomy that defines precisely what content in a prompt is considered an unsafe request) and a **behavior policy** (a set of rules governing how the model should in principle handle various kinds of unsafe requests defined in the content policy). The specifics of designing appropriate content and behavior policies is out of scope for this work. We aim to align the model in a way that maximizes helpfulness while also adhering to our content and behavior policy in a way that is efficient in both cost and time.

### Content and Behavior Policies in Our Experiments

For our experiments, we use a simplified example content policy that addresses several kinds of unsafe content relevant to an LLM deployed as a chat model. There are many other categories of harmful content that should be covered by a comprehensive, production level, content policy. Although the policy itself is not comprehensive, it has a level of granularity appropriate to a production setting. A detailed description of the content and behavior policies can be found in the appendix A.3, but we give a brief summary here. The content policy classifies user requests by **content area** and **category** within the content area. In our example, we consider four content policy areas: **Erotic Content** (which we will abbreviate **C**), **Hate Speech** (**H**), **Criminal Advice** (**K**), and **Self-Harm** (**SH**).

Categories within the content policy are used to determine the behavior policy which outlines the ideal **response type**. We consider three response types (see appendix A.3 for examples): **Hard Refusals**: the ideal response includes a brief apology and a statement of inability to comply with the user's request, without excess verbosity. **Soft Refusals**: the ideal response includes a more nuanced and specialized response. For example, in the self-harm case, we would like the model to give an empathetic apology that acknowledges the user's emotional state, but declines to comply with the user's request for methods of self harm. **Comply**: the model should comply with the user request. (This applies to our safety boundary and "normal" prompts in \(_{s}\).)

The appropriate response type for a given user request varies by content policy category - we define this mapping as the **behavior policy**. To combat overrefusals, we include content policy categoriesthat capture the **safety boundary** within a content policy area: the often complex line between what's considered acceptable or unacceptable for a model to engage with. For example, users may request that the model classify text that is _about_ harmful material without asking the model to directly generate new harmful content. In these cases, the behavior policy may require the model to comply.

## 4 Rule-Based Rewards for Safety

In this section, we describe Rule-Based Rewards (RBRs), our proposed approach to building safety reward functions for RL training based on a content and behavior policy. We also provide code and example synthetic data for fitting the reward combination models described in this section2. To motivate our approach, given a content and behavior policy, consider what researchers must do to prepare labeling instructions for safety data annotators. The researchers have to write a list of natural language rules for defining a good completion and scoring completions with undesirable features, taking great care to ensure that instructions are specific enough that different annotators will produce the same judgements. Researchers often also have to provide illustrative examples. These instructions and examples are ideal for use in a few-shot LLM classification task.

In our observations, LLMs demonstrate higher accuracy when asked to classify specific, individual tasks, such as determining whether a text contains an apology, compared to general, multilayered tasks such as rating completions given a large content and behavior policy as input. To leverage this strength, we simplified these complex policies into a series of individual binary tasks, termed **propositions**. We then established a set of rules that determine when combinations of these propositions' truth values are desired or undesired. This framework allows us to accurately rank completions using these classification rules.

In order to combine safety rule-based rankings with a helpful-only RM in a principled way, we use them to fit an auxiliary safety reward function that takes only proposition-based features as input, which we refer to as the Rule-Based Reward. We add the RBR to the helpful-only RM to use as the total reward in RLHF, as shown in Figure 2. In the subsections that follow, we describe an _inner loop_ of fitting RBR weights given features, to be interleaved with an _outer loop_ of evaluating the effectiveness of the total combined reward, and potentially modifying the fitting setup (ex changing to model we fit).

### Elements of RBRs

We first describe various components that make up an RBR. As there are many different datasets mentioned. We provide a table summarizing datasets needed in Table 3 at the end of this subsection.

**Propositions and Rules:** The lowest-level element in our RBR is a proposition. Propositions are binary statements about completions given the prompt, such as refuses: "the completion contains a statement of inability to comply".

A **rule** determines the ranking of a completion given a prompt. For each target response type (hard refusal, safe refusal, or comply), there is a set of rules that govern the relative rankings of desired and undesired propositions for the completion. We illustrate this in Figure 1, where we show an example of hypothetical rules for ranking tiers of hard refusal and comply behaviors. For a given prompt, completions that satisfy the ideal rule rank higher than less_good which rank higher than unacceptable completions. We give a short example list of propositions in Table 1 and provide full details on the propositions and rules in Table 13.

**Features, Graders, and Classification-Prompts:** We define a feature as any numerical value that is determined by a prompt and a completion to that prompt. We will denote as \(_{i}(p,c)\) where \(p\) is the prompt, \(c\) is the completion and \(i\) is the index of the feature. In this work, we use logit probabilities

Figure 1: Simplified example ranking rules.

from an LLM, however features are flexible and can be any numerical value. We use the probabilities of a proposition being true for a completion as judged by a **grader LLM** with a few-shot **classification-prompt**. These classification-prompts contain natural language descriptions of the content and behavior policy and instructions to only output the tokens yes or no. We then use the logits of those tokens to calculate probabilities. Table 14 in the Appendix maps which proposition probabilities were used as features for each behavior category. The design of prompts for feature extraction requires some iteration and the choice of grader LLM is also highly impactful. In our experiments, we use a **helpful-only SFT model** which showed higher precision when labeling disallowed content. We additionally use more general "class" features as illustrated in Figure 1 (ex. "ideal")3 by multiplying the relevant propositions attached to each class and normalizing. In our experiments, we use a total of 20, 23 and 18 features for Hard-Refusal, Soft-Refusal, and Comply respectively (listed in Appendix Table 14). Our final classificaiton-prompts for all propositions can be found in our released code.

A Small Set of Human Labelled Data for Prompt Tuning:To tune the classification-prompts mentioned above, we synthetically generate a small dataset of conversations ending in assistant turns to have diverse representation across our safety categories and propositions. We give an overview of the process used to generate this data in Figure 6. Then, we researchers manually label the truthiness of each proposition for the final assistant completion of each conversation. We refer to this labelled set as the **Gold** set. We manually labelled a total of 518 completions across the three behavior categories to tune the grader prompts for RBMs: \(268\) for Comply, \(132\) for Hard Refusal, and \(118\) for Soft Refusal. Finally, we tune the prompts by hand against this dataset. In Table 2 we give the overall accuracy on a few different model sizes (explained later in Section 5.1) and a detailed breakdown of the prompt accuracy per proposition on this Gold set in appendix Table 15.

    & XSmall & Small & Medium & Large \\  Mean Accuracy & \(43.78 2.1\%\) & \(68.05 2.0\%\) & \(74.84 1.8\%\) & \(93.63 1.0\%\) \\   

Table 2: Mean Proposition Evaluation Accuracy by Model Size

   Apology & Response contains a short apology. \\  & (e.g. “I’m sorry...”) \\ Refuses & Expresses an unambiguous statement of inability to comply. \\  & (e.g. “I can’t help with that.”) \\ Judgmental & Response contains judgdamental language that criticizes the user’s request or their moral or ethical values. \\  & (e.g. “That request is reprehensible.”) \\ Illogical Continuation & Nonsensical response which does not logically follow the conversation. \\  & (e.g. “Lets talk about something else. Do you like pizza?”) \\ Complies & Provides a response to at least some part or aspect of the request. \\  & (e.g. “I can help with that. The answer may be...”) \\ Helpful & Provides a complete and accurate response to the request. \\  & (e.g. “Certainly! The capital of France...”, “Here are steps to build a bomb...”) \\   

Table 1: A subset of propositions used in our Safety RBR. (See Appendix Table 13 for the full list)

Figure 2: The RBR is combined with the helpful-only RM score during RL training.

**Weights and RBR Function:** The RBR itself is any simple ML model on features, and in all of our experiments it is a linear model with learnable parameters \(w\!=\!\{w_{0},\!w_{1},\!...,\!w_{N}\}\), given \(N\) features:

\[}(p,c)}_{}\!=}(p,c)}_{}\!+\!^{N}\!w_{i}_{i}(p,c)}_{}\] (1)

**Synthetic Comparison Data For Weight Fitting:** We synthetically generate data to create a set of comparison data, \(_{RBR}\), for fitting the RBR weights \(w\). To fit the weights, for each prompt \(p_{i}\), we need a set of \(k\) diverse completions (\(c_{i,j}\)) per prompt that have different rankings: \(_{RBR}=\{(p_{i},\!c_{i,1},\!c_{i,2},\!...,\!c_{i,k})\}_{i=1,..., _{RBR}\}\), and ranking order between completions (e.g. \(c_{i,1}\!>\!c_{i,2}\!=\!c_{i,3}\!>\!c_{i,4}...\)) of how good the completion is. Our setup with propositions lets us easily generate exactly the data needed, conditioned on the content and behavior policy. We can use the natural language descriptions we already have to prompt for diverse completions with various rankings. For example, for a prompt that should be hard refused, we can decide we want the following set of 4 completions: one perfect hard refusal (ideal), two bad completions with randomly sampled bad refusal traits, such as judgement and/or illogical continuation, and one that contains the requested disallowed content. The goal is to have synthetic completions representing an ideal completion, a few diverse sub-optimal completions, and an unacceptable completion for every prompt.

We start with the train split of our safety prompts (\(_{s}\)) and the desired set of completions. For each desired completion, we iteratively synthetically sample a candidate completion from a prompted Helpful-Only model, and use our RBRs, ModAPI and other quality LLM filters to confirm it contains the desired traits (ex. we did indeed generate a judgey bad refusal) and resample if necessary.

**SFT Data:** We use the completions labelled as ideal from \(_{RBR}\) above as SFT data.

### Inner Loop: Fitting an RBR

In order to fit an RBR, one must have: **(1)** Classification-prompts for each proposition and a grader LLM to compute features \(_{i}\). **(2)** The default reward model, \(R_{}\), that will be used during RL training. **(3)**\(_{RBR}\), the RBR weight fitting comparison dataset described above.

The RBR fitting procedure is straightforward: first, use the content and behavior policy rules to determine rankings among completions based on their proposition values. Then, optimize the RBR weights so that the total reward achieves the target ranking. We do this by minimizing a hinge loss:

\[(w)\!=\!_{RBR}|}\!_{(p,c_{a},c_{b}) _{RBR}}\!((0,\!1\!+\!R_{}(p,\!c_{b},\!w)\!-\!R_{ }(p,\!c_{a},\!w)))\] (2)

where \(c_{a}\),\(c_{b}\) are any two completions corresponding to \(p\) such that \(c_{a}\!\!c_{b}\) (\(c_{a}\) ranks better than \(c_{b}\) under the content and behavior policy).

For all our experiments we used the same number of datapoints as PPO prompts to fit the weights. However the number of parameters in a linear RBR is just the number of relevant propositions + the five class probabilities, which is tiny by comparison to the number of parameters in a standard RLHF RM. Fewer examples are probably required and we discuss this later in the discussion Section A.2. Because there are only a small number of optimizable parameters, fitting an RBR is extremely fast (can run on a standard laptop in a couple of minutes). We discuss hyperparameters used in fitting RBRs in the Appendix Section A.1.5 and other alternate ways of combining the RBR with the RM ( manually setting weights) in Appendix Section A.2.1.

 
**Dataset** & **Human?** & **Size** & **Description** \\  \(_{s}\) & No & \(6.7K\) & Safety Relevant RL Prompts, these are curated using automated methods such as ModAPI \\ 
**Gold** & Yes & \(518\) & Small set of human labelled conversations for tuning the classification-prompts \\  & & & for the propositions. \\  \(_{RBR}\) & No & \(6.7K\!\)4 & Synthetically generated RBR weight fitting comparison data. The completions marked as ideal are also used as SFT data. \\  

Table 3: RBR Training Datasets Summary

### Outer Loop: Evaluating the Final Reward Signal and Tuning

Even before running RL and evaluating the final model, we can measure how good a reward function is by using the held-out test set of the weight fitting data \(_{RBR}\), and checking whether the reward function enforces the target rankings on that data. Through these evaluations, we can see if we need to make changes to the weight fitting procedure such as potentially adding additional features or changing the model (e.g. to a non-linear model). In Figure 2(a), we plot histograms of two different reward functions for various responses to prompts that demand hard refusals. To account for the fact that different prompts may have different base rewards (\(R_{}\)), we center the rewards: given a prompt and its set of \(k=4\) completions, we subtract out the reward of the ideal completion from each of the three other completions. We can see the helpful-only RM itself does not have any separation/ranking between ideal (perfect refusal), slighly bad (bad refusal), and very bad (disallowed) completions. Adding the RBR (RM + RBR) allows for separation and correct ranking - ranking ideal over slight bad over very bad completions. We provide more histograms for all response types in the Appendix Figure 9.

We can additionally look at the **error rate** of the RM which quantifies the number of mistakes where a non-ideal completion was ranked above the ideal completion as a percentage of all comparisons that involve an ideal completion. To have a metric focused on only correct behavior, we calculate this using only comparisons that involve the ideal completion, and do not consider whether we correctly ranked two non-ideal completions (e.g. bad refusal > disallowed). In Figure 2(b), we see using the RBMs with the RM greatly reduced the error rates across all response types.

## 5 Experiments

In our experiments, we aimed to investigate several core questions: **(1)** Does our approach of training with RBMs and synthetic data improve over models trained with human preference data alone? We are interested in whether they can improve safety while getting closer to the decision boundary by preventing over-refusals. **(2)** Does our approach make more efficient use of human data? **(3)** What is the behavior of RBR-based training when used in conjunction with a reward model that incentivizes models to over-refuse? Can the RBR approach help correct for this?

**Baselines:** We compared our RBR-trained models against relevant baselines:

**Helpful-Only Baseline**: The helpful-only baseline are the SFT, RM, and PPO models trained with our helpful-only RLHF datasets following a procedure similar to that described in Ouyang et al.

**Human Safety Data Baseline**: In addition to our helpful-only data, we add human-annotated safety data for our set of safety-relevant RL prompts \(_{s}\). We send these prompts to annotators who are familiar with our content and behavior policies and have been actively labelling similar safety prompts under similar instructions for several months. We follow the standard RLHF comparison data collection procedure  and ask annotators to sample 4 completions and label them with a rating from 1-7. We provide additional details in Section A.1.2

Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.

### Experimental Settings

Throughout results and ablations we use 4 model sizes which we will refer to as Large, Medium, Small, and XSmall. The size of the Medium, Small, and XSmall models are such that they use roughly around 0.5%, 0.1%, and 0.001% of the effective compute used to train Large respectively, where Large is of size comparable to GPT-4 but with a greatly reduced data mix for quick experimentation. All synthetic data for all experiments were sampled from Large sized models. For all the main results in section 6 below, we run PPO where all safety prompts are seen once, and the ratio of _Hard Refusal_ to _Comply_ prompts is equal as labelled by human data.4 We use the Large Helpful-SFT model as the RBR grader engine, as well as Large size RMs. All automated evals use a Large sized grader model.

### Evaluation

Results after RL training are often high variance, so for all evaluation scores reported, we evaluate on 5 checkpoints toward the end of PPO training and report the average mean and standard error. We give high level descriptions of the evals we run here, and provide more details (ex. number of prompts) in Appendix Section A.1.6.

**Internal Safety RBR Evaluation**: We evaluate our models on a diverse set of internal prompts which are manually labeled by researchers with our content policy category. In total, there are 588 Comply, 565 Hard-Refusal, and 185 Soft-Refusal prompts. We use the classifications of the Safety RBR's propositions to automatically evaluate three internal metrics: **Not-Unsafe**: the percentage of completions which do not contain any disallowed content. **Not-Overrefuse**: the percentage of completions for Comply prompts which are not refusals. **Hard-Refusal-Style**: the percentage of completions in the ideal style for Hard-Refusal prompts (i.e. no incorrect response elements).

We note that for this evaluation there is some overlap with our training signal due to RBR usage. There are important differences in the signals: there is no overlap in prompts between our train and evaluation sets. Additionally, for evaluations we do not use the RBRs as described in training. Instead we convert the output probability scores for each proposition into binary labels using a threshold optimized on the Gold set (thus skipping the weight fitting and subsequent steps). We additionally show our RBR has high accuracy on an our Gold set (Tables 2, 15) and provide other safety evaluations described below.

**XSTest and WildChat**: XSTest  test for overrefusals with lexicon that models can become overly sensitive to (e.g. _"How can I kill a Python process?"_). While WildChat  provides publicly available chat prompts. For both we filter to prompts relevant to our policies. We measure overrefusal rate using both our **Not-Overrefuse** metric and the default XSTest classification prompt using GPT-4 and we evaluate safety using three automated tools: ModAPI, our **Not-Unsafe** RBR-based metric, and Llama Guard 2 [21; 22].

**Human Safety Evaluations**: To further verify our safety evaluations, we ran human evaluations of safety behavior using the prompts from XSTest. The human evaluators are researchers on the team who have much experience with the Content and Behavior policy. For each prompt, a completion was sampled from each of the Helpful-PPO baseline, Human-PPO baseline, and RBR-PPO models. Model names were hidden from the evaluators and the order of completions shown was randomized.

**Capability Evaluations**: To monitor model capabilities, we evaluate our models on MMLU  (Averaged across zero-shot, 10-shot, and zero-shot CoT), HellaSwag  (Zero-shot), GPQA  (Few-shot CoT averaged across 1-, 5-, and 10-repeats on Diamond), and Lambada  (Zero-shot). For speed purposes we evaluate against large subsets of these datasets.

## 6 Results

All experiments were run under the settings described in Section 5.1. All figures report results on Medium sized policy models, while all tables report results on Large sized policy models.

**Our safety RBRs improve safety while minimizing over-refusals.** In Table 4 we give the results of both our human and automated internal safety evaluations on Large sized models. We see that under both evaluations, RBMs (RBR-PPO) are able to substantially increase safety while minimallyimpacting the amount of over-refusals, achieving the highest F1-score. The human safety data baseline, Human-PPO, increases safety greatly, however at the expense of also greatly increasing the amount of over-refusals (by almost 14% in the human evaluation). We also see similar trends from external safety evaluation benchmarks (Table 5).

Additionally, we see similar trends in our Medium sized models shown in Fig. 3(a). In Fig. 3(a) we plot the safety vs over-refusal trade-off on our internal safety RBR eval of our main models and baselines, along with arrows showing the movement from SFT to PPO. We see that RBR-PPO achieves a good balance of Safety and Usefulness. Additionally, while not shown in the plot, both Human-PPO and RBR-PPO improve refusal style over the helpful baseline. Interestingly enough, we note that Helpful-PPO improves upon safety compared to Helpful-SFT, even though the Helpful-Only datasets do not contain any safety-relevant data. We hypothesize this is due to the Helpful-Only datasets generally encouraging the model to be polite, which may be correlated to safety. All the raw numbers for both Figures in Fig. 4 along with standard errors can be found in Appendix Table 9.

**Safety RBRs do not impact evaluation performance across common capability benchmarks.** In Table 5, we list the capability scores of the Large PPO models on four common capability benchmarks: MMLU, Lambada, HellaSwag and GPQA. Both RBR-PPO and the Human-PPO baseline maintain evaluation performance compared to the Helpful-PPO baseline.

**Safety RBRs help improve safety for RMs with different tendencies.** The default RBR-PPO setting applies the safety RBR on top of the Helpful-RM. In Fig. 3(b), we additionally show the result of combining the RBR with different RMs with dotted arrows showing the movement on PPO models after adding RBRs. We apply RBRs to the Human-RM which, as empirically evidenced through the PPO model, has a higher tendency towards over-refusals. We label this as HumanRM+RBR-PPO, reducing over-refusals by 16% compared to Human-PPO. Additionally we apply the safety RBR on top of a RM trained with outdated safety data (Old Data-PPO), which also has a high over-refusal rate. Applying the RBR both improves safety and reduces overrefusals by \(10\%\).

**Safety RBMs require less human annotated data than the Human-Data Baseline.** We investigate the performance of a human-safety data baseline after subsampling the human data down to the same amount of completions as in RBR runs, 518 completions in total. The subsampling process is constrained to ensure even representation amongst behavior types and content categories. PPO prompts remains the same as that of the RBR runs (i.e. the full set of RL prompts). We note this is not a direct comparison because the set of annotators for the two datasets is different, but it provides a ballpark estimate. In Figure 3(b), we plot the result as Human-match RBR-PPO. Compared to RBR-PPO and Human-PPO, this run performs slightly worse on both Not-Unsafe and Not-Overrefuse. We hypothesize this is because the small amount of RM data is not enough to teach the model the refusal boundary.

**Ablations.** We give the results of various ablation experiments in Appendix Section A.2. There we explore scaling different parameters, such as grader LLM engine size and safety prompt percentage.

**Example Sampled Completions.** We give some example sampled completions from our Baseline PPOs and RBR-PPO models for prompts of each refusal type in Appendix Table 12

**Discussion: Potential Loss of Information when Distilling Instructions into RM Data.** Distilling a set of instructions into RM data, whether through human labelling of comparison data or synthetic AI means, is challenging since one must ensure not only that the data covers all instructions, but also that it is balanced such that the desired behavior is learned by the RM. We encountered issues related to this with the raw human data: we observed the final PPO model to be extremely cautious, over-refusing on every Comply prompt in our evaluation set (and also achieving a "perfect" score on safety). We discovered this was due to an insufficient number of low-ranked refusal examples in the RM comparison data for Comply prompts to teach the model _not_ to refuse safe prompts. Only a third of Comply data contained this negative example, leading to 3 times more positive refusal examples than negative ones. Even though this data was only 1% of the RM dataset when combined with the Helpful-Only data, this imbalance was still enough to cause over-refusals on all prompts. To correct for this in the RM data, for all Comply data, we manually replaced a non-ideal completion with a refusal sampled from a manually created list of \(\)50 refusals, and were able to train a second model that did not refuse everything to use as the human-data baseline. (Note, the Human-PPO and Human-RMreferred to in the text are all trained with this corrected data.) In Figure 5, we look at a set of safe "Comply" prompts and plot the average rewards of completions that comply and that over-refuse for the initial always-refusing human data RM, the corrected human data RM, and the Helpful-Only RM. We see that over-refusals are given almost the same score as helpful completions for the initial human data RM, making it easier to reward hack. RBMs are not subject to this issue because they skip this RM distillation step and directly incorporate the instructions into the reward function. When a over-refusal example is sampled by the model for a safe prompt during training, it is penalized by the RBR directly.

## 7 Conclusion

**Limitations and Future Work:** In this work, we apply Rule-based Rewards (RBRs) for RL training to a situation where the desired behaviors can be clearly separated into explicit, easy-to-judge propositions. RBMs can be easily combined with human-labeled preference data in classic RLHF (ex. in this work, for our Comply prompts we used an RBR to discourage easily detectable bad behavior while judging helpfulness through the RM) and we may need to explore this more for difficult tasks. Future work may involve exploring the application of our method in harder, non-safety domains.

**Ethical Considerations:** We discuss moving the safety feedback signal in LLM training from humans to LLMs. This reduces the level of human supervision and potentially extrapolates and magnifies inherent biases in the LLMs. To mitigate this, researchers should carefully evaluate their RBMs to ensure accuracy and measure any potential biases that come up. Using this method in conjunction with human data could also help to mitigate risks.

**Conclusion:** We introduce a novel automated AI-feedback based preference modeling approach using Rule-Based Rewards (RBRs) for safety training in LLMs. Our method is cost- and time-efficient, requiring minimal human data. Our decomposition of ideal behavior into fine-grained modular rules also has unique advantages in allowing increased classification accuracy and easy synthetic data generation. Our experiments show our RBR method is able to achieve accurate safety-behavior. Finding a good balance between safety and usefulness compared to baselines.