# Diffusion Probabilistic Models for

Structured Node Classification

 Hyosoon Jang\({}^{1}\), Seonghyun Park\({}^{1}\), Sangwoo Mo\({}^{2}\), Sungsoo Ahn\({}^{1}\)

\({}^{1}\)POSTECH \({}^{2}\)University of Michigan

{hsjang1205,shpark26,sungsoo.ahn}@postech.ac.kr, swmo@umich.edu

###### Abstract

This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs based on proposing AGG-WL, which is strictly more powerful than the classic 1-WL test. We extensively verify the superiority of our DPM-SNC in diverse scenarios, which include not only the transductive setting on partially labeled graphs but also the inductive setting and unlabeled graphs.

## 1 Introduction

In this paper, we address the node classification problem, which is a fundamental problem in machine learning on graphs with various applications, such as social networks  and citation networks . One representative example is a transductive problem to classify documents from a partially labeled citation graph. Recently, graph neural networks (GNNs) [3; 4] have shown great success in this problem over their predecessors [5; 6]. Their success stems from the high capacity of non-linear neural architectures combined with message passing.

However, the conventional GNNs are incapable of _structured node classification_: predicting node labels while considering the node-wise label dependencies [7; 8; 9]. That is, given a graph \(G\) with vertices \(\) and node labels \(\{y_{i}:i\}\), a GNN with parameter \(\) outputs an unstructured prediction, i.e., \(p_{}(y_{i},y_{j}|G)=p_{}(y_{i}|G)p_{}(y_{j}|G)\) for \(i,j\). Especially, this limitation becomes problematic in a transductive setting where the prediction can be improved by incorporating the known labels, e.g., output \(p_{}(y_{i}|G,y_{j})\) for known \(y_{j}\). In Figures 1(a) and 1(b), we elaborate on this issue with an example where the conventional GNN fails to make the correct prediction.

To resolve this issue, recent studies have investigated combining GNNs with classical structured prediction algorithms, i.e., schemes that consider dependencies between the node labels [8; 9; 10; 11; 12; 13]. They combine GNNs with conditional random fields , label propagation , or iterative classification algorithm . Despite the promising outcomes demonstrated by these studies, their approach relies on the classical algorithms to express joint dependencies between node labels and may lack sufficient expressive power or consistency in incorporating known labels.

**Contribution.** We propose a novel framework for structured node classification called DPM-SNC. Our key idea is to leverage the diffusion probabilistic model (DPM) , motivated by two significant advantages of DPM for structured node classification: (a) it can effectively learn a joint distribution over both the unknown and the known labels, and (b) it can easily incorporate conditions during inference via posterior sampling. Figure 1 highlights that DPM significantly outperforms previous methods when the prediction problem requires a complete understanding of label dependencies.

However, DPM cannot be directly applied to the transductive scenario, where the model needs to maximize its log-likelihood for partially labeled graphs. We propose a novel training algorithm to address this challenge. Our method maximizes a new variational lower bound of the marginal likelihood of the graph over the unlabeled nodes, involving the alternative optimization of DPM and a variational distribution. In this process, we estimate the variational distribution from the current DPM using a similar approach to fixed-point iteration . After training, DPM can predict the remaining node labels by constraining its predictions with the given labeled nodes, utilizing the manifold-constrained sampling technique .

In addition, we provide a theoretical analysis that supports the benefits of DPM-SNC for node classification by enhancing the expressive power of GNNs. To this end, we derive an analog of our DPM-SNC based on the Weisfeiler-Lehman (WL)  test, evaluating the ability to distinguish non-isomorphic graphs. Specifically, we prove that DPM-SNC is as powerful as our newly derived aggregated Weisfeiler-Lehman (AGG-WL) test, which is strictly more powerful than the 1-WL test, a known analog to standard GNN architectures  such as GCN  and GAT .

We demonstrate the effectiveness of DPM-SNC on various datasets, covering both transductive and inductive settings. In the transductive setting, we conduct experiments on seven benchmarks: Pubmed, Cora, and Citeseer ; Photo and Computer ; and Empire and Ratings . Additionally, we introduce a synthetic benchmark to evaluate the ability to capture both short and long-range dependencies. DPM-SNC outperforms the baselines, including both homophilic and heterophilic graphs. In the inductive setting, we conduct experiments on four benchmarks: Pubmed, Cora, Citeseer, and PPI . Furthermore, we evaluate DPM-SNC on the algorithmic reasoning benchmark , which requires a higher-level understanding of node relations. DPM-SNC also outperforms previous algorithms for these tasks.

To summarize, our contribution can be listed as follows:

* We explore DPM as an effective solution for structured node classification due to its inherent capability in (a) learning the joint node-wise label dependency in data and (b) making predictions conditioned on partially known data (Section 3).
* We propose a novel method for training a DPM on partially labeled graphs that leverages the probabilistic formulation of DPM to maximize the variational lower bound (Section 4).
* We provide a theoretical analysis of how DPM enhances the expressive power of graph neural networks, strictly improving its capability over the 1-WL test (Section 5).
* Our experiments demonstrate the superiority over the baselines in both transductive and inductive settings. We additionally consider heterophilic graphs and algorithmic reasoning benchmarks to extensively evaluate the capability of DPMs in understanding the label dependencies (Section 6).

Figure 1: Results of various methods to solve node classification on a non-attributed and partially labeled cyclic grid. (a) The task is to estimate \(p(_{U}|_{L},G)\) where the known labels \(_{L}\) are highlighted in green and the unknown labels \(_{U}\) are colored in gray. (b)-(e) Red highlights the incorrect predictions made by the corresponding algorithm. Conventional GNN (GCN) fails to make an informative prediction while our DPM-SNC is the only method to perfectly predict all the labels.

Related Work

**Graph neural networks (GNNs).** Graph neural networks (GNNs) are neural networks specifically designed to handle graph-structured data. Over the past years, GNNs have shown promising outcomes in various tasks related to graph representation learning [3; 4; 27; 28]. In the case of node classification, GNNs aggregate neighboring node information and generate node representations for label predictions. Their expressive power has been analyzed through the lens of the Weisfeiler-Lehman (WL) test , a classical algorithm to distinguish non-isomorphic graphs. For example, Xu et al.  proved how the expressive power of conventional GNNs is bounded by the 1-dimensional WL (1-WL) test.

**Structured node classification with GNNs.** Apart from investigating the power of GNNs to discriminate isomorphic graphs, several studies focused on improving the ability of GNNs to learn the joint dependencies [7; 9; 13; 26; 29; 30; 31]. In particular, researchers have considered GNN-based structured node classification [7; 9; 13]. First, Qu et al.,  parameterize the potential functions of conditional random fields using a GNN and train it using pseudo-likelihood maximization . Next, Qu et al.,  proposed a training scheme with a GNN-based proxy for the conditional random field. Hang et al.,  proposed an iterative classification algorithm-based learning framework of GNNs that updates each node-wise prediction by aggregating previous predictions on neighboring nodes.

**Diffusion probabilistic models (DPMs).** Inspired by non-equilibrium thermodynamics, DPMs are latent variable models (LVMs) that learn to reverse a diffusion process and construct data from a noise distribution. Recent studies demonstrated great success of DPMs in various domains, e.g., generating images , text , and molecules . An important advantage of diffusion models is their ability to incorporate additional constraints. This allows DPM to solve the posterior inference problems, e.g., conditioning on the partially observed data to recover the original data [18; 36; 37].

## 3 Diffusion Probabilistic Models for Structured Node Classification

In this section, we introduce the problem of structured node classification and discuss the limitations of graph neural networks (GNNs) in addressing this problem (Section 3.1). We then explain how the diffusion probabilistic models (DPMs) offer a promising solution for this problem (Section 3.2).

### Structured node classification

We address structured node classification, which involves predicting node labels while considering their joint dependencies. Specifically, we focus on predicting labels in partially labeled graphs, where some true labels are known in advance.1 Our goal is to devise an algorithm that considers the dependencies with known labels to enhance predictions for unknown labels.

To be specific, our problem involves a graph \(G=(,,)\) consisting of nodes \(\), edges \(\), and node attributes \(=\{x_{i}:i\}\). We also denote the node labels by \(=\{y_{i}:i\}\). We let \(_{L}\) and \(_{U}\) denote the set of labeled and unlabeled nodes, while \(_{L}\) and \(_{U}\) denote the corresponding labels for each set, e.g., \(_{L}=\{y_{i}:i_{L}\}\). Our objective is to predict the unknown labels \(_{U}\) by training on the partially labeled graph, aiming to infer the true conditional distribution \(p(_{U}|G,_{L})\).

We point out that the conventional GNNs [3; 4; 27; 28] are suboptimal for estimating \(p(_{U}|G,_{L})\) since their predictions on the node labels are independently conditioned on the input, i.e., their predictions are factorized by \(p_{}(_{U}|G,_{L})=_{i_{U}}p_{}(y_{i }|G)\). Namely, conventional GNNs cannot incorporate the information of the known labels \(_{L}\) into a prediction of the unknown labels \(_{U}\).

Figure 2: The reverse process makes predictions with label dependencies. The node color indicates the label value, and opacity reflects the likelihood.

Structured prediction algorithms [2; 14] overcome this issue by solving two tasks: (a) learning a joint distribution \(p_{}(_{U},_{L}|G)\) to maximize the likelihood of labeled data \(p_{}(_{L}|G)\) and (b) inferring from the distribution \(p_{}(_{U}|G,_{L})\) conditioned on known labels.

### Diffusion probabilistic models for structured node classification

In this work, we consider diffusion probabilistic models for structured node classification (DPM-SNC) motivated by how their strengths align with the essential tasks for solving structured node classification, outlined in Section 3.1. In particular, DPMs are equipped with (a) high expressivity in learning a joint distribution over data and (b) the ability to easily infer from a posterior distribution conditioned on partially observed data.

To this end, we formally describe DPMs that generate node-wise labels \(\) associated with a graph \(G\). At a high level, our DPM consists of two parts: the forward process, which injects noise into the labels, and the reverse process, which denoises to predict the true labels \(\). Given the number of diffusion steps \(T\), the forward process constructs a sequence of noisy labels \(^{(1:T)}=[^{(1)},,^{(T)}]\) using a fixed distribution \(q(^{(1:T)}|^{(0)})\) starting from the true label \(^{(0)}=\).2 Next, given an initial noises \(^{(T)}\) sampled from \(p(^{(T)})\), the reverse process \(p_{}(^{(0:T-1)}|^{(T)},G)\) is trained to recover the forward process conditioned on the graph \(G\). We defer the detailed parameterization to Section 4. To be specific, the forward and the reverse process are factorized as follows:

\[q(^{(1:T)}|^{(0)})=_{t=1}^{T}q(^{(t)}|^{(t-1)}),  p_{}(^{(0:T-1)}|^{(T)},G)=_{t=1}^{T}p_{}(^{(t-1)}|^{(t)},G),\]

where the training of the reverse process is trivial on a fully-labeled graph \((,G)\) by maximizing the variational lower bound of the marginal log-likelihood \( p_{}(|G)=_{^{(1:T)}}p_{}( ^{(0:T)}|G)\) with respect to the noisy labels \(^{(1:T)}\) sampled from the forward process .

By leveraging shared latent variables \(^{(t)}\) across multiple steps in the reverse process, the reverse process effectively considers the dependencies in the output. Next, DPMs can easily infer from a posterior distribution conditioned on partially observed data. Specifically, the incremental updating of \(^{(t)}\) for \(t=1,,T\) allows the DPM to incorporate the known label \(_{L}\), e.g., applying iterative projection with manifold-based correction . This enables inference from the distribution \(p_{}(_{U}|G,_{L})\) conditioned on the known labels \(_{L}\). See Figure 2 for an illustration of DPM for structured node classification with known labels.

Figure 3: Illustration of DPM-SNC training on partially labeled graphs, repeating steps (a) and (b).

Training Diffusion Probabilistic Models on Partially Labeled Graphs

Despite the potential of DPMs for structured node classification, they lack the algorithms to learn from partially labeled graphs, i.e., maximize the likelihood of known labels. To resolve this issue, we introduce a novel training algorithm for DPM-SNC, based on maximizing a variational lower bound for the log-likelihood of known labels.

**Variational lower bound.** At a high-level, our algorithm trains the DPM to maximize the log-likelihood of training data \( p_{}(_{L}|G)\), which is defined as follows:

\[= p_{}(_{L}|G)=_{_{U}}_{^{ (1:T)}}p_{}(_{L},_{U},^{(1:T)}|G).\]

However, this likelihood is intractable due to the exponentially large number of possible combinations for the unknown labels \(_{U}\) and the noisy sequence of labels \(^{(1:T)}\). To address this issue, we train the DPM based on a new variational lower bound \(_{}\), expressed as follows:

\[_{}=_{q(_{U}|_{L})}_{q(^{(1:T)}|)} p_{}(,^{(1:T)}| )- q(^{(1:T)}|)- q(_{U}|_{L}) .\] (1)

Here, \(q()\) is a variational distribution factorized by \(q(_{U},^{(1:T)}|_{L})=q(^{(1:T)}|)q(_{U}| _{L})\), where \(=_{U}_{L}\). We provide detailed derivation in Appendix A.1.

**Parameterization.** We define the reverse process \(p_{}(^{(t-1)}|^{(t)},G)\) with a Gaussian distribution, where the mean value parameterized by a graph neural network (GNN) and the variance set to be a hyper-parameter. Following the prior work , we also employ a Gaussian diffusion to parameterize the forward process \(q(^{(1:T)}|)\) as follows:

\[q(^{(1)},,^{(T)}|^{(0)})=_{t=1}^{T}( ^{(t)};}^{(t-1)},_{t}),\]

where \(\) is an identity matrix, \(_{1},,_{T}\) are fixed variance schedules. We set the variance schedule to promote \(q(^{(T)}|^{(0)})(^{(T)};,)\) by setting \(_{t}<_{t+1}\) for \(t=0,,T-1\) and \(_{T}=1\). Finally, we describe the variational distribution \(q(_{U}|_{L})\) as an empirical distribution over a fixed-size buffer \(\) containing multiple estimates of the unknown labels \(_{U}\). This buffer is updated throughout the training. We provide more details on our parameterization in Appendix A.2.

**Training algorithm.** To maximize the variational lower bound, we alternatively update the GNN-based reverse process \(p_{}(,^{(1:T)}|G)\) and the buffer-based variational distribution \(q(_{U}|_{L})\). In particular, we update the parameters \(\) of the reverse process \(p_{}(,^{(1:T)}|G)\) to maximize the Monte Carlo approximation of \(_{}\) by applying ancestral sampling to the variational distribution \(q(^{(1:T)}|)q(_{U}|_{L})\), i.e., sampling \(_{U}\) from the buffer \(\) and applying the diffusion process to \(\). This is the same as the original denoising-based training of DPM  with estimated unknown labels, where the detailed training objective is described in Appendix A.3.

Next, we update the variational distribution \(q(_{U}|_{L})\) by inserting samples from the distribution \(p_{}(_{U}|G,_{L})\) into the buffer \(\).3 This update is derived from the condition \(q(_{U}|_{L})=p_{}(_{U}|G,_{L})\) being necessary to maximize \(_{}\), similar to the derivation of fixed-point iteration for optimization . We describe the overall optimization procedure in Figure 3 and Algorithm 1.4

Finally, we emphasize that our training algorithm is specialized for DPMs. Previous studies introducing variational lower bounds for structured node classifications [7; 38] face intractability in maximizing \( p_{}(|G)\) or sampling from \(p_{}(_{U}|G,_{L})\). They require defining the pseudo-likelihood for the former , or parameterizing variational distribution for the latter. However, in our case, the formal simply requires maximizing the lower bound \(_{}\), and the latter is easily solved by manifold-constrained sampling .

Theoretical Analysis

In this section, we present a secondary motivation for using DPMs in node classification, distinct from the one given in Section 3 and Section 4. Specifically, we demonstrate that DPMs provably enhance the expressive power of conventional GNNs for solving the graph isomorphism test, implying improved expressive power for node classification problems as well.

To this end, we assess the expressive power of our DPM-SNC by its analog for discriminating isomorphic graphs, which we call the aggregated Weisfeiler-Lehman (AGG-WL) test. Then, we show that AGG-WL is strictly more powerful than the 1-dimensional WL (1-WL) test , which is an analog of GNNs . We formalize our results in the following theorem.

**Theorem 1**.: _Let 1-WL-GNN be a GNN as powerful as the 1-WL test. Then, DPM-SNC using a 1-WL-GNN is strictly more powerful than the 1-WL-GNN in distinguishing non-isomorphic graphs._

We provide the formal proof in Appendix B. At a high level, both the 1-WL test and the AGG-WL test assign colors to nodes and iteratively refine them based on the colors of neighboring nodes, enabling the comparison of graph structures. The key difference is that AGG-WL applies perturbations into the initial graph to create multiple augmented graphs, and aggregates the results of the 1-WL algorithm on each augmented graph. The perturbations and aggregation can be done by node-wise concatenations with binary random features and hashing a multiset of the refined graph colors, respectively. Hence, any graph pair distinguishable by the 1-WL test is also distinguishable by our AGG-WL test, and there exist pairs of graphs indistinguishable by the WL test but distinguishable by our variant.

We remark that our analysis can be easily extended to applying other latent variable models, e.g., variational auto-encoders  or normalizing flows , for node classification. In this analysis, sampling a latent variable corresponds to fixing a perturbation for color assignments at initialization, and aggregating over the perturbed graphs corresponds to computing the prediction over the true labels with marginalization over the latent variables. Our theory is based on that of Bevilaqua et al.  which was originally developed for analyzing a particular GNN architecture; our key contribution lies in extending this analysis to the latent variable models.

## 6 Experiments

### Transductive setting

We first evaluate the performance of our algorithm in the transductive setting. Specifically, we conduct an evaluation of DPM-SNC on synthetic data, as well as real-world node classification problems. We consider different types of label dependencies by considering both homophilic and heterophilic graphs. We provide the details of our implementation in Appendix C.1.

**Synthetic data.** In this experiment, we create a \(2 n\) non-attributed cyclic grid. Each node is assigned either a binary label, with neighboring nodes having different labels. We consider two scenarios: one where the known labels are randomly scattered throughout the graph, and another where they are clustered in a local region. Examples of both scenarios are illustrated in Figures 4(a) and 4(b). These two scenarios verify the capability for capturing both short and long-range dependencies between node labels. The data statistics are described in Appendix D.

We compare DPM-SNC with conventional GNN that ignores the label dependencies and other structured node classification methods: GMNN , G\({}^{3}\)NN , and CLGNN . We describe the detailed experimental setup in Appendix E.1. We report the performance using five different random seeds.

   Method & Scattered & Localized \\  GCN  & 50.1\(\)0.6 & 51.3\(\)1.4 \\ \(+\) GMNN  & 64.2\(\)3.2 & 48.2\(\)4.8 \\ \(+\) G\({}^{3}\)NN  & 50.1\(\)0.6 & 51.3\(\)1.4 \\ \(+\) CLGNN  & 87.0\(\)2.4 & 53.4\(\)1.1 \\ \(+\) DPM-SNC & **98.5\(\)**0.7 & **90.9\(\)**3.4 \\   

Table 1: The transductive node classification accuracy on synthetic data. **Bold** numbers indicate the best performance.

Figure 4: Illustration of two scenarios. **green** highlights known labels.

The results, presented in Table 1, demonstrate that our method significantly improves accuracy compared to the baselines when labeled nodes are scattered. This highlights the superiority of DPM-SNC in considering label dependencies. Furthermore, our method also excels in the localized labeled nodes scenario, while the other baselines fail. These results can be attributed to the capabilities of DPMs for capturing long-range dependencies through iterative reverse diffusion steps.

**Homophilic graph.** In this experiment, we consider five datasets: Pubmed, Cora, and Citeseer ; Photo and Computer . For all datasets, 20 nodes per class are used for training, and the remaining nodes are used for validation and testing. Detailed data statistics are in Appendix D.

We compare DPM-SNC with conventional GNN, and structured prediction baselines. We compare with label propagation-based methods: LP , C&S , BPN , and PTA . We also consider G\({}^{3}\)NN, GMNN, and CLGNN. We describe the detailed experimental setup in Appendix E.2. As the backbone network, we consider GCN and GAT . We further evaluate our algorithm with the recently developed GCNII . For all the datasets, we evaluate the node-level accuracy. We also report the subgraph-level accuracy, which measures the ratio of nodes with all neighboring nodes being correctly classified. The performance is measured using ten different random seeds.

The results are presented in Table 2. Our method outperforms the structured prediction-specialized baselines in both node-label and subgraph-level accuracy. These results highlight the superiority of DPM-SNC in solving real-world node classification problems. Furthermore, even when we combine with GCNII , DPM-SNC consistently improves performance regardless of the backbone network.

**Heterophilic graph.** To validate whether our framework can also consider heterophily dependencies, we consider recently proposed heterophilic graph datasets: Empire and Ratings , where most heterophily-specific GNNs fail to solve. In Table 3, we compare our method with six GNNs:

    &  &  &  &  &  \\  Method & N-Acc. & Sub-Acc. & N-Acc. & Sub-Acc. & N-Acc. & Sub-Acc. & N-Acc. & Sub-Acc. & N-Acc. & Sub-Acc. \\  LP  & 69.1\(\)0.0 & 45.7\(\)0.0 & 68.1\(\)0.0 & 46.9\(\)0.0 & 46.1\(\)0.0 & 29.8\(\)0.0 & 81.0\(\)2.0 & 37.2\(\)1.3 & 69.9\(\)29 & 15.1\(\)1.1 \\ C\&S  & 77.3\(\)0.0 & 57.1\(\)0.0 & 80.2\(\)0.0 & 60.1\(\)0.0 & 69.5\(\)0.0 & 47.9\(\)0.0 & 90.1\(\)1.4 & 48.5\(\)2.8 & 81.6\(\)0.09 & 24.5\(\)1.3 \\ BPN  & 78.2\(\)1.5 & 48.4\(\)2.5 & 82.5\(\)0.9 & 63.8\(\)0.9 & 73.3\(\)0.0 & 51.9\(\)1.1 & 89.0\(\)1.0 & 41.4\(\)1.4 & 80.4\(\)1.4 & 28.0\(\)1.4 & 23.2\(\)1.5 \\ PTA  & 80.1\(\)1.5 & 55.2\(\)2.0 & 82.9\(\)0.4 & 62.6\(\)0.3 & 71.3\(\)0.4 & 51.4\(\)0.9 & 91.1\(\)1.5 & 51.0\(\)1.3 & 51.0\(\)1.3 & 61.5\(\)2.7 & 26.3\(\)1.0 \\  GCN  & 79.7\(\)0.3 & 55.8\(\)0.6 & 81.4\(\)0.8 & 59.3\(\)1.1 & 70.9\(\)0.8 & 49.8\(\)0.6 & 91.0\(\)1.2 & 52.0\(\)1.0 & 82.4\(\)1.5 & 27.0\(\)1.5 \\ +LPA  & 79.6\(\)0.6 & 53.5\(\)0.9 & 81.7\(\)0.7 & 60.3\(\)1.3 & 71.0\(\)0.6 & 50.2\(\)1.0 & 91.3\(\)1.2 & 52.9\(\)1.0 & 83.7\(\)1.4 & 28.5\(\)2.4 \\ +GMNN  & 82.6\(\)1.0 & 58.1\(\)1.4 & 82.6\(\)0.9 & 61.8\(\)1.3 & 72.8\(\)0.7 & 52.0\(\)0.8 & 91.2\(\)1.2 & 54.3\(\)1.4 & 82.0\(\)1.0 & 28.0\(\)1.6 \\ +G3NN  & 80.9\(\)0.7 & 56.9\(\)1.1 & 82.5\(\)4.2 & 62.3\(\)0.7 & 73.9\(\)0.7 & 53.1\(\)1.0 & 90.7\(\)1.3 & 53.0\(\)0.2 & 82.1\(\)1.2 & 28.1\(\)1.2 \\ +CLGNN  & 81.7\(\)0.5 & 57.8\(\)0.9 & 81.9\(\)0.5 & 61.8\(\)0.8 & 72.0\(\)0.7 & 51.6\(\)0.9 & 91.1\(\)1.9 & 53.4\(\)1.8 & 83.3\(\)1.2 & 28.5\(\)1.4 \\ +DPM-SNC & **83.0\(\)0.9** & **59.2\(\)1.2** & **83.2\(\)0.5** & **63.1\(\)0.9** & **74.4\(\)0.5** & **53.6\(\)0.6** & **92.2\(\)0.8** & **55.3\(\)1.2** & **84.1\(\)1.2** & **29.7\(\)1.3** \\  GAT  & 79.1\(\)0.5 & 55.8\(\)0.5 & 81.5\(\)0.6 & 61.3\(\)0.9 & 71.0\(\)0.8 & 50.8\(\)1.0 & 90.8\(\)1.0 & 50.8\(\)1.9 & 83.1\(\)1.6 & 27.8\(\)2.2 \\ +LPA  & 78.7\(\)1.1 & 56.0\(\)1.2 & 81.5\(\)0.9 & 60.7\(\)0.8 & 71.3\(\)0.9 & 50.1\(\)0.9 & 91.3\(\)0.8 & 52.7\(\)2.1 & **84.4\(\)**1.0 & 29.4\(\)2.6 \\ +GMNN  & 79.6\(\)0.8 & 57.0\(\)0.7 & 82.3\(\)1.7 & 62.2\(\)0.8 & 71.7\(\)0.9 & 51.4\(\)0.9 & 91.4\(\)1.0 & 53.1\(\)1.6 & 83.3\(\)2.0 & 29.1\(\)1.3 \\ +G3NN  & 77.7\(\)0.4 & 55.9\(\)0.9 & 82.7\(\)1.3 & 62.7\(\)1.3 & 74.0\(\)0.8 & 53.7\(\)0.9 & 91.5\(\)0.9 & 52.6\(\)1.2 & 83.1\(\)1.7 & 28.8\(\)2.4 \\ +CLGNN  & 80.0\(\)0.4GCN, SAGE , GAT, GAT-sep , GT , and GT-sep . We also compare with eight heterophily-specific GNNs: H\({}_{2}\)GCN , CPGNN , GPR-GNN , FSRNN , GloGNN , FAGCN , GBK-GNN , and JacobiConv . We employ GAT-sep as a backbone network of DPM-SNC. For all the baselines, we use the numbers reported by Platonov et al. .

Table 3 shows that our method again achieves competitive performance on heterophilic graphs. While existing heterophily-specific GNNs do not perform well on these datasets , our method shows improved performance stems from the extraordinary ability for considering label dependencies involving heterophily label dependencies.

### Inductive setting

We further show that our DPM-SNC works well not only in transductive settings but also in inductive settings which involve inductive node classification and graph algorithmic reasoning. We provide the details of our implementation in Appendix C.2.

**Inductive node classification.** Following Qu et al. , we conduct experiments on both small-scale and large-scale graphs. We construct small-scale graphs from Pubmed, Cora, and Citeseer, and construct large-scale graphs from PPI . The detailed data statistics are described in Appendix D.

We compare our DPM-SNC with the conventional GNN and three structured node classification methods: G3NN, CLGNN, and SPN . As the backbone network of each method, we consider GCN and GAT. We evaluate node-level accuracy across all datasets and supplement it with additional metrics: graph-level accuracy for small-scale graphs and micro-F1 score for large-scale graphs. The graph-level accuracy measures the ratio of graphs with where all the predictions are correct. We report the performance measured using ten and five different random seeds for small-scale and large-scale graphs, respectively. The detailed experimental setup is described in Appendix E.3.

We report the results in Table 4. Here, DPM-SNC shows competitive results compared to all the baselines except for Pubmed. These results suggest that the DPM-SNC also solves inductive node classification effectively, thanks to their capability for learning joint dependencies.

**Algorithmic reasoning.** We also evaluate our DPM-SNC to predict the outcomes of graph algorithms, e.g., computing the shortest path between two nodes. Solving such tasks using GNNs has gained much attention since it builds connections between deep learning and classical computer science algorithms. Here, we show that the capability of DPM-SNC to make a structured prediction even brings benefits to solving the reasoning tasks by a deep understanding between algorithmic elements.

We evaluate the performance of our DPM-SNC on three graph algorithmic reasoning benchmarks proposed by Du et al. : edge copy, connected component, and shortest path. The detailed data statistics are described in Appendix D. Here, we evaluate performance on graphs with ten nodes. Furthermore, we also use graphs with 15 nodes to evaluate generalization capabilities. We report the performance using element-wise mean square error.

    &  &  &  & PPI \\  Method & N-Acc. & G-Acc. & N-Acc. & G-Acc. & N-Acc. & G-Acc. & F1 \\  GCN  & 80.25\({}_{ 0.42}\) & 54.58\({}_{ 0.51}\) & 83.36\({}_{ 0.43}\) & 59.67\({}_{ 0.51}\) & 76.37\({}_{ 0.35}\) & 49.84\({}_{ 0.47}\) & 99.15\({}_{ 0.03}\) \\ \(+\)G\({}^{3}\)NN  & 80.32\({}_{ 0.30}\) & 53.93\({}_{ 0.71}\) & 83.60\({}_{ 0.25}\) & 59.78\({}_{ 0.47}\) & 76.34\({}_{ 0.7}\) & 50.76\({}_{ 0.47}\) & 99.33\({}_{ 0.02}\) \\ \(+\)CLGNN  & 80.22\({}_{ 0.48}\) & 53.98\({}_{ 0.54}\) & 83.45\({}_{ 0.34}\) & 60.24\({}_{ 0.38}\) & 75.71\({}_{ 0.40}\) & 50.51\({}_{ 0.38}\) & 99.22\({}_{ 0.04}\) \\ \(+\)SPN  & **80.78\({}_{ 0.34}\)** & 54.91\({}_{ 0.40}\) & 83.85\({}_{ 0.40}\) & 60.35\({}_{ 0.47}\) & 76.25\({}_{ 0.48}\) & 51.02\({}_{ 1.06}\) & 99.35\({}_{ 0.02}\) \\ \(+\)DPM-SNC & 80.58\({}_{ 0.41}\) & **55.16\({}_{ 0.43}\)** & **84.09\({}_{ 0.27}\)** & **60.88\({}_{ 0.36}\)** & **77.01\({}_{ 0.49}\)** & **51.44\({}_{ 0.56}\)** & **99.46\({}_{ 0.02}\)** \\  GAT  & 80.10\({}_{ 0.49}\) & 54.38\({}_{ 0.44}\) & 79.71\({}_{ 1.41}\) & 56.66\({}_{ 0.44}\) & 74.91\({}_{ 0.22}\) & 49.87\({}_{ 0.44}\) & 99.54\({}_{ 0.01}\) \\ \(+\)G\({}^{3}\)NN  & 79.85\({}_{ 0.52}\) & 54.66\({}_{ 0.29}\) & 81.19\({}_{ 0.48}\) & 55.68\({}_{ 0.48}\) & 75.45\({}_{ 0.26}\) & 50.86\({}_{ 0.46}\) & 99.56\({}_{ 0.04}\) \\ \(+\)CLGNN  & 80.23\({}_{ 0.40}\) & 54.51\({}_{ 0.36}\) & 81.38\({}_{ 0.35}\) & 55.81\({}_{ 0.41}\) & 75.45\({}_{ 0.36}\) & 50.60\({}_{ 0.46}\) & 99.55\({}_{ 0.01}\) \\ \(+\)SPN  & 79.95\({}_{ 0.34}\) & **54.82\({}_{ 0.33}\)** & 81.61\({}_{ 0.31}\) & 59.17\({}_{ 0.31}\) & 75.41\({}_{ 0.35}\) & 51.04\({}_{ 0.53}\) & 99.46\({}_{ 0.02}\) \\ \(+\)DPM-SNC & **80.26\({}_{ 0.37}\)** & 54.26\({}_{ 0.47}\) & **81.79\({}_{ 0.46}\)** & **59.55\({}_{ 0.49}\)** & **76.46\({}_{ 0.60}\)** & **52.05\({}_{ 0.71}\)** & **99.63\({}_{ 0.01}\)** \\   

Table 4: The inductive node classification performance. N-Acc., G-Acc., and F1 denote the node-level accuracy, graph-level accuracy, and micro-F1 score, respectively. **Bold** numbers indicate the best performance among the structured prediction methods using the same GNN.

We compare our method with five methods reported by Du et al. , including a feedforward neural network, recurrent neural network , Pondernet , iterative feedforward , and IREM . For all the baselines, we use the numbers reported by Du et al. . As these tasks are defined on edge-wise targets, we modify DPM-SNC to make edge-wise predictions. We describe the detailed experimental setup in Appendix E.4.

We report the results in Table 5. Our DPM-SNC outperforms the considered baselines for five out of the six tasks. These results suggest that the diffusion model can easily solve algorithmic reasoning tasks thanks to its superior ability to make structured predictions.

### Ablation studies

Here, we conduct three ablation studies to empirically analyze our framework. All the results are measured over ten different random seeds.

**Diffusion steps vs. number of GNN layers.** We first verify that the performance gains in DPM-SNC mainly stem from the reverse diffusion process which learns the joint dependency between labels. To this end, we vary the number of diffusion steps along with the number of GNN layers. We report the corresponding results in Figure 5. One can observe that increasing the number of diffusion steps provides a non-trivial improvement in performance, which cannot be achieved by just increasing the number of GNN layers.

**Accuracy over diffusion steps.** We investigate whether the iteration in the reverse process progressively improves the quality of predictions. In Figure 6, we plot the changes in node-level accuracy in the reverse process as the number of iterations increases. The results confirm that the iterative inference process gradually increases accuracy, eventually reaching convergence.

**Running time vs. performance.** Here, we investigate whether the DPM-SNC can make a good trade-off between running time and performance. In Figure 7, we compare the change in accuracy of DPM-SNC with GCNII over the inference time on Citeseer by changing the number of layers and diffusion steps for DPM-SNC and GCNII, respectively. The backbone network of DPM-SNC is a two-layer GCN. One can observe that our DPM-SNC shows competitive performances compared to the GCNII at a similar time. Also, while increasing the inference times of the GCNII does not enhance performance, DPM-SNC shows further performance improvement.

    &  &  &  \\  Method & Same-MSE & Large-MSE & Same-MSE & Large-MSE & Same-MSE & Large-MSE \\  Feedforward & 0.3016 & 0.3124 & 0.1796 & 0.3460 & 0.1233 & 1.4089 \\ Recurrent  & 0.3015 & 0.3113 & 0.1794 & 0.2766 & 0.1259 & 0.1083 \\ Programmatic  & 0.3053 & 0.4409 & 0.2338 & 3.1381 & 0.1375 & 0.1290 \\ Iterative feedforward  & 0.6163 & 0.6498 & 0.4908 & 1.2064 & 0.4588 & 0.7688 \\ IREM  & 0.0019 & **0.0019** & 0.1424 & 0.2171 & 0.0274 & 0.0464 \\ DPM-SNC & **0.0011** & 0.0038 & **0.0724** & **0.1884** & **0.0138** & **0.0286** \\   

Table 5: Performance on graph algorithmic reasoning tasks. **Bold** numbers indicate the best performance. Same-MSE and Large-MSE indicate the performance on ten, and 15 nodes, respectively.

Conclusion and Discussion

In this paper, we propose diffusion probabilistic models for solving structured node classification (DPM-SNC). Extensive experiments on both transductive and inductive settings show that DPM-SNC outperforms existing structured node classification methods. An interesting avenue for future work is the study of characterizing the expressive power of GNNs to make structured predictions, i.e., the ability to learn complex dependencies between node-wise labels.

**Limitations.** Our DPM-SCN makes a trade-off between accuracy and inference time through the number of diffusion steps. Therefore, we believe accelerating our framework with faster diffusion-based models, e.g., denoising diffusion implicit models (DDIM)  is an interesting future work.