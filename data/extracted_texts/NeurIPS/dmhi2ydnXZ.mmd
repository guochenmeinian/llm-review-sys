# Scalable DBSCAN with Random Projections

HaoChuan Xu

School of Computer Science

University of Auckland

hxu612@aucklanduni.ac.nz&Ninh Pham

School of Computer Science

University of Auckland

ninh.pham@auckland.ac.nz

###### Abstract

We present _sDBSCAN_, a scalable density-based clustering algorithm in high dimensions with cosine distance. sDBSCAN leverages recent advancements in random projections given a significantly large number of random vectors to quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN preserves the DBSCAN's clustering structure under mild conditions with high probability. To facilitate sDBSCAN, we present _sOPTICS_, a scalable visual tool to guide the parameter setting of sDBSCAN. We also extend sDBSCAN and sOPTICS to L2, L1, \(^{2}\), and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than competitive DBSCAN variants on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn counterparts and other clustering competitors demand several hours or cannot run on our hardware due to memory constraints. Our code is available at https://github.com/NinhPham/sDBscan.

## 1 Introduction

DBSCAN  is one of the most fundamental clustering algorithms with many applications in data mining and machine learning . It has been featured in several data analysis tool kits, including scikit-learn in Python, ELKI in Java, and CRAN in R. In principle, DBSCAN connects neighboring points from nearby high-density areas to form a cluster where the high density is decided by a sufficiently large number of points in the neighborhood. DBSCAN is parameterized by \((,minPts)\) where \(\) is the distance threshold to govern the point's neighborhood and to connect nearby areas; and \(minPts\) is the density threshold to identify high-density areas.

Apart from other popular clustering algorithms, including \(k\)-means variants [3; 4] and spectral clustering , DBSCAN is non-parametric. It can find the number of clusters, detect arbitrary clustering shapes and sizes, and work on any arbitrary distance measure.

Given a distance measure, DBSCAN has two primary steps, including (1) finding the \(\)-neighborhood (i.e. points within a radius \(\)) for every point to discover the density areas surrounding the point and (2) forming the cluster by connecting neighboring points. The first step is the main bottleneck as finding \(\)-neighborhoods for all points requires a worst-case \(O(n^{2})\) time for a data set of \(n\) points in high-dimensions [6; 7]. This limits the applications of DBSCAN on modern million-point data sets.

Another hurdle of \((,minPts)\)-DBSCAN is the choice of \(\), which highly depends on the data distribution and distance measure. While \(minPts\) is easier to set for smoothing the density estimate, DBSCAN's outputs are susceptible to \(\), especially in high dimensions where the range of \(\) is very sensitive. For instance, when applying DBSCAN with cosine distance on the Pamp2 data set, changing \(\) by just 0.005 can diminish the clustering accuracy by 10%. In practice, ones often need to compute an \(\)-neighborhood for each point given a large value of \(\), and use them to explore the quality of clustering structures over smaller values of \(\). Using large \(\) causes \(O(n^{2})\) memory bottleneckas the \(\)-neighborhood of one point might need \(O(n)\) space. We observe that on our hardware, such memory constraint is the primary hurdle limiting the current scikit-learn implementation on million-point data sets. Therefore, it is essential to not only develop scalable versions of DBSCAN, but also to provide feasible tools to guide its parameter setting on large-scale data sets.

**Prior arts on scaling up DBSCAN.** Due to the quadratic time bottleneck of DBSCAN in high-dimensional space, researchers study efficient solutions to scale up the process of identifying the neighborhood for each point in exact and approximate manners.

Exact DBSCAN approaches  partition the data set into several subsets and iteratively extract and refine clusters from these subsets by performing additional \(\)-neighborhood queries on a small set of important points. Other grid-based methods on L2  efficiently identify neighborhood areas by confining the neighbor search exclusively to neighboring grids. However, these approaches still have worst-case quadratic time or their complexity grows exponentially to the dimension. Random projections  have been used to build grid-based or tree-based indexes for faster approximate \(\)-neighborhoods on L2, though they do not offer theoretical guarantees on the clustering accuracy.

Instead of finding the \(\)-neighborhood for every point, DBSCAN++  finds \(\)-neighborhoods for a subset of random points chosen through uniform sampling or \(k\)-centering methods. sngDBSCAN  approximates the \(\)-neighborhood for every point by computing the distance between the point to a subset of random points. Though sampling-based DBSCAN variants are simple and efficient, they offer statistical guarantees on the clustering accuracy via level set estimation  that requires many strong assumptions on the data distribution. Moreover, selecting suitable parameter values (especially for \(\)) for sampling-based approaches is challenging due to the nature of sampling.

**DBSCAN's parameter setting guideline with OPTICS.** OPTICS  attempts to mitigate the problem of selecting relevant \(\) by linearly ordering the data points such that close points become neighbors in the ordering. Besides the cluster ordering, OPTICS also computes a reachability distance for each point. The dendrogram provided by OPTICS visualizes the density-based clustering results corresponding to a broad range of \(\), which indicates a relevant range of \(\) for DBSCAN.

Like DBSCAN, OPTICS requires the \(\)-neighborhood for every point, leading to an \(O(n^{2})\) time complexity. In practice, OPTICS often needs a large \(\) to discover clustering structures on a wide range of \(\). Such large \(\) demands \(O(n^{2})\) memory as the \(\)-neighborhood of one point needs \(O(n)\) space. Such memory constraint is infeasible for million-point data sets.

**Contribution.** Inspired by sampling approaches, we observe that the exact \(\)-neighborhood for every point is unnecessary to form and visualize the density-based clustering results. Our approach, named _sDBSCAN_, first builds a _lightweight_ random projection-based index with a sufficiently large number of projection vectors. Utilizing the asymptotic property of the extreme order statistics associated with some specific random vectors , sDBSCAN can select high-quality candidates to identify \(\)-neighborhoods with _theoretical_ guarantees. sDBSCAN provably outputs DBSCAN's clustering structure similar to DBSCAN on cosine distance under mild conditions from data distribution. These conditions are much weaker than the ones used on recent sampling-based DBSCAN . Empirically, sDBSCAN runs significantly faster than several competitive DBSCAN variants while achieving similar DBSCAN clustering accuracy on million-point data sets.

To further facilitate sDBSCAN, we propose _sOPTICS_, a scalable OPTICS derived from the random projection-based indexing, to guide the parameter setting. We also extend sDBSCAN and sOPTICS to other popular distance measures, including L2, L1, \(^{2}\), and Jensen-Shannon (JS), that allow random kernel features .

**Scalability.** Both sDBSCAN and sOPTICS are scalable and multi-thread friendly. Multi-threading sDBSCAN and sOPTICS take _a few minutes_ to cluster and visualize clustering structures for million-point data sets while the scikit-learn counterparts cannot run on our hardware. On Mnist8m with 8.1 million points, sDBSCAN gives 38% accuracy (measuring by the normalized mutual information NMI), running in 15 minutes on a _single_ machine of 2.2GHz 32-core (64 threads) AMD processor with 128GB of DRAM. In contrast, kernel \(k\)-means  achieves 41% accuracy with Spark, running in 15 minutes on a _supercomputer_ with 32 nodes, each of which has two 2.3GHz 16-core (32 threads) Haswell processors and 128GB of DRAM.

## 2 Preliminary

We briefly describe DBSCAN , OPTICS , and the connection with approximate near neighbor search (ANNS). We present a recent advanced random projection method [22; 23] for ANNS on the extreme setting where the number of random projection vectors is sufficiently large. The data structures inspired by these approaches can scale up DBSCAN and OPTICS on large-scale data sets.

### Dbscan

DBSCAN is a density-based approach that links nearby dense areas to form the cluster. For a distance measure \(dist(,)\), DBSCAN has two parameters \(\) and \(minPts\). Given the data set \(\), for each point \(\), DBSCAN executes a _range reporting query_\(B_{}()\) that finds _all_ points \(\) within the \(\)-neighborhood of \(\), i.e. \(B_{}()=\{\,|\,dist(, )\}\). Based on the size of the range query result, DBSCAN determines \(\) as _core_ if \(|B_{}()| minPts\); otherwise, _non-core_ points. We will call the \(\)-neighborhood \(B_{}()\) as the _neighborhood_ of \(\) for short.

DBSCAN forms clusters by connecting core points and their neighborhoods where two core points \(_{1}\) and \(_{2}\) are connected if \(_{1} B_{}(_{2})\). A non-core point belonging to a core point's neighborhood will be considered a _border_ point and share the core point's label. Non-core points not belonging to any core point's neighborhood will be classified as _noise_. Indeed, DBSCAN forms a graph \(G\) that connects \(n\) points together [7; 14]. \(G\) will have several disconnected components corresponding to the cluster structure. Each connected component contains connected core points and their neighborhoods, as shown in Alg. 1.

### Optics

OPTICS  attempts to mitigate the problem of selecting relevant \(\) by linearly ordering the data points such that close points become neighbors in the ordering. For each point \(\), OPTICS computes a reachability distance from its closest core point. The cluster ordering and reachability distance of each point are used to construct a reachability-plot dendrogram that visualizes the density-based clustering results corresponding to a broad range of \(\). Valleys in the reachability-plot are considered as clustering indicators. The OPTICS's algorithm is detailed in the appendix.

Given a pair \((,minPts)\), OPTICS first identifies the core points, the neighborhoods of core points, and then computes the core distance of every point, defined as below.

\[coreDist()=&$ is non-core,}\\ minPts-&\]

Then, OPTICS iterates \(\), and for each \(\), computes the _smallest_ reachability distance, defined by \(.reach\), between \(\) and the _processed_ core points so far. The point with the minimum reachability distance will be processed first and inserted into the cluster ordering \(O\). The reachability distance \(reachDist(,)\) is defined as follows.

\[reachDist(,)=&$ is non-core,}\\ max(coreDist(),dist(,))&\]

For a core point \(\), \(reachDist(,)\) is \(dist(,)\) if \(\) is not belonging to \(minPts-$}\). Among several core points whose neighborhood contains \(\), OPTICS tends to seek the _smallest_ reachability distance, i.e. \(.reach\), for \(\) from these core points, which reflects the distance between \(\) and its nearby cluster. Therefore, points tend to be grouped with its neighborhood to form a cluster. A sharp decrease of \(.reach\) of the point \(\) in the group indicates that we are processing points in denser regions, and a slight increase indicates that we are processing points in sparser regions. This creates valleys on the dendrograms reflecting the number of clusters where points downwards the valley floor are on denser regions while points upwards the valley head are on sparser regions. By selecting \(\) to separate valleys provided by OPTICS, DBSCAN can achieve the peak of accuracy.

**Time and space complexity.** Similar to DBSCAN, the running time of OPTICS is \(O(n^{2})\) makes it impractical on large-scale data sets. Fast implementations of OPTICS with large values of \(\) might require \(O(n^{2})\) memory to store the matrix distance between the core points and their neighborhood points. Such implementations are infeasible for large \(n\).

### Random projection-based neighborhood preservation

Since the primary bottleneck of DBSCAN is to find core points and their neighborhoods by executing \(n\) range queries, reducing the computational cost of this step will significantly improve the performance. Scaling up DBSCAN poses the need for _lightweight_ indexing data structures, ideally with linear construction time, to approximately answer \(n\) range queries. Heavyweight graph-based indexes [24; 25] with \(O(n^{2})\) construction time and locality-sensitive hashing (LSH)-based indexes [23; 26] with subquadratic construction time will dominate the clustering time.

We elaborate on the recent work, called CEOs , that studies extreme order statistics properties of random projection methods to approximate inner product. Given \(D\) random vectors \(_{i}^{d}\), \(i[D]\), whose coordinates are randomly selected from the standard normal distribution \(N(0,1)\), and the sign function \(()\). CEOs randomly projects \(\) and \(\) onto these \(D\) Gaussian random vectors. It studies the behavior of the projection values on specific random vectors that are closest or furthest to \(\), e.g., \(_{_{i}}|^{}_{i}|\). Given a sufficiently large \(D\), the projection values on the closest or furthest random vector to \(\) approximately preserve \(^{}\). The proof is described in the appendix.

**Lemma 1**.: _[_22_]_ _For two points \(,^{d-1}\) and significantly large \(D\) random vectors \(_{i}\), w.l.o.g. we let \(_{*}=_{_{i}}|^{}_{i}|\). Then, we have_

\[^{}_{*}N((^{ }_{*})^{}\,\,,1-(^{})^{2})\.\] (1)

As a geometric intuition, for significantly large \(D\) random vectors, if \(_{*}\) is closest or furthest to \(\), the projection values of all points in \(\) onto \(_{*}\) tend to preserve their inner product order with \(\). For a constant \(k>0\), Lemma 1 also holds for the top-\(k\) closest/furthest vectors to \(\) due to the asymptotic property of extreme normal order statistics [18; 22]. Therefore, by maintaining a few points that are closest/furthest to random vectors, we can approximate the neighborhood of each point accurately and efficiently. We will utilize this observation to significantly reduce the cost of identifying core points and, hence, the running time of DBSCAN and OPTICS in high dimensions.

## 3 Scalable density-based clustering with random projections

We first present simple and scalable DBSCAN, called _sDBSCAN_, with cosine distance. We then leverage well-known random feature embeddings [19; 20] to extend our proposed _sDBSCAN_ to other popular distance measures, including L1 and L2 metrics, and widely used similarity measures for image data, including \(^{2}\) and Jensen-Shannon (JS). We also present _sOPTICS_ to select relevant values of \(\) for sDBSCAN. The detailed discussion and complexity of sOPTICS are in the appendix.

### sDBSCAN: A simple and scalable density-based clustering

For each point \(^{d-1}\), we compute \(D\) random projection values \(^{}_{i}\) where \(i[D]\). W.l.o.g., we let \(_{*}=_{_{i}}|^{}_{i}|= _{_{i}}^{}_{i}\). Lemma 1 indicates that, given a sufficiently large \(D\), points closest to \(_{*}\) tend to have smaller distances to \(\), and hence are important to discover \(\)'s neighborhood. Hence, computing the distance between \(\) to the top-\(m\) points closest to \(_{*}\) where \(m=O(minPts)\) suffices to ensure whether or not \(\) is a core point.

Algorithm 2 shows how we preprocess the data set. For each point \(\), we keep top-\(k\) closest and furthest random vectors. For each random vector \(_{i}\), we keep top-\(m\) closest and furthest points. Algorithm 3 identifies a core point \(\) and its approximate neighborhood by computing the distance between \(\) and \(2km\) points associated to the \(k\) closest and \(k\) furthest random vectors to \(\). sDBSCAN and sOPTICS are essential DBSCAN and OPTICS using the outputs of Algorithm 3 as inputs.

**Scalability.** Given \(m=O(minPts)\), sDBSCAN and sOPTICS need \(O(k minPts)\) distance computations, compared to \(O(n)\) of the exact solution, to identify a core point \(\) and its neighborhoodsubset \(_{}() B_{}()\). The memory usage to store \(_{}()\) is also bounded by \(O(k minPts)\). This makes sDBSCAN and sOPTICS scalable regarding both time and space complexity.

**Multi-threading.** Like DBSCAN, the main computational bottlenecks of sDBSCAN and sOPTICS are identifying core points and approximating their neighborhood. Fortunately, Algorithm 2 and 3 are fast and parallel-friendly. We only need to add #pragma omp parallel directive on the for loops to run in multi-threads. This enables sDBSCAN and sOPTICS to deal with million-point data sets in _minutes_ while the scikit-learn counterparts take hours or cannot finish due to the memory constraints.

```
1:Inputs: \(^{d-1}\), \(D\) random vectors \(_{i}\), \(k,m=O(minPts)\)
2:Initialize an empty set \(_{}()\) for each \(\)
3:for each \(\)do
4:for each \(_{i}\) from top-\(k\) closest (or furthest) random vectors of \(\)do
5:for each \(\) from top-\(m\) closest (or furthest) points of \(_{i}\)do
6:if\(dist(,)\)then
7: Insert \(\) into \(_{}()\) and insert \(\) into \(_{}()\)
8:for each \(\)do
9:if\(|_{}()| minPts\)then
10: Output \(\) as a core point and \(_{}()\) as an approximate \(B_{}()\) for DBSCAN (Alg. 1)
11: Output \(dist(,)\) for each \(_{}()\) for OPTICS (Alg. 6) ```

**Algorithm 3** Finding core points and their approximate neighborhoods

### Theoretical analysis of sDBSCAN

In practice, setting \(m=O(minPts)\) suffices to identify core points and approximate their neighborhoods to ensure the quality of sDBSCAN. However, to _theoretically_ guarantee the quality of sDBSCAN, we need to adjust \(m\) based on the data distribution since ensuring the density-based clustering quality without any information about the data distribution is hard [14; 15].

As in the unit sphere, if \(L_{2}(,)\) then \(^{} 1-^{2}/2\), we make a change in our algorithm. For each random vector \(_{i}\), we maintain two sets \(S_{i}=\{\,|\,^{}_{i}(1- ^{2}/2)}\}\) and \(R_{i}=\{\,|\,^{}_{i}-(1- ^{2}/2)}\}\), instead of keeping only top-\(m\) closest/furthest points. In other words, for each random vector \(_{i}\), the value of \(m\) is set _adaptively_ to the data distributed around \(_{i}\). For a point \(\) and its closest random vector \(_{*}\), if we compute \(L_{2}(,)\) for all \( S_{*}\) where \(S_{*}=\{\,|\,^{}_{*}(1- ^{2}/2)}\}\), then any \( B_{}()\) is found with a probability at least 1/2 due to the Gaussian distribution in Eq. (1). By computing the distance between \(\) and all points in \(S_{i}\) or \(R_{i}\) corresponding to the top-\(k\) closest/furthest random vectors \(_{i}\) of \(\), we can boost this probability to \(s=1-{(1/2)}^{2k}\) due to the asymptotic independence among these \(2k\) vectors [18; 22].

By connecting core points and their associated border points, DBSCAN outputs an \(\)-neighborhood graph with \(l\) disconnected components \(G_{1},G_{2},,G_{l}\) corresponding to \(l\) density-based clusters \(C_{1},C_{2},,C_{l}\). Hence, by maintaining \(S_{i}\) and \(R_{i}\) for each vector \(_{i}\), sDBSCAN forms a subgraph \(G^{}_{i} G_{i}\), \(i=1,,l\), by sampling each edge of \(G_{i}\) with probability at least \(s\). We now use the following lemma  to guarantee the connectivity of subgraphs \(G^{}_{i}\) provided by sDBSCAN.

**Lemma 2**.: _Let \(G\) be a graph of \(n\) vertices with min-cut \(t\) and \(0<<1\). There exists a universal constant \(c\) such that if \(s+)}{t}\), then with probability at least \(1-\), the graph \(G^{}\) derived by sampling each edge of \(G\) with probability \(s\) is connected._

Since DBSCAN connects core points, there exists a constant \(t>0\) such that, for any pair of nearby core points \(_{1},_{2}\), if \(L_{2}(_{1},_{2})\), then \(B_{}(_{1})\) and \(B_{}(_{2})\) share _at least_\(t\) common core points. We assume that \(t\) will not be small to ensure that the density-based clusters will not become arbitrarily thin anywhere. Given that DBSCAN produces \(l\) disconnected graphs \(G_{1},G_{2},,G_{l}\), it is clear that \(t\) will be the lower bound of the min-cut of any \(G_{i}\). By applying Lemma 2 on each \(G_{i}\), we state our main result.

**Theorem 1**.: _Let \(G_{1},G_{2},,G_{l}\) be connected subgraphs produced by DBSCAN where each \(G_{i}\) corresponds to a cluster \(C_{i}\) with \(n_{i}\) core points. Assume that any pair of nearby core points \(_{1},_{2}\), if \(L_{2}(_{1},_{2})\), then \(B_{}(_{1})\) and \(B_{}(_{2})\) share at least \(t\) common core points. There exists a constant \(c\) such that if \(t(1-{(1/2)}^{2k}) c(+)})\) for \(i[l]\), sDBSCAN will recover \(G_{1},G_{2},,G_{l}\) as clusters with probability at least \(1-\)._

Theorem 1 indicates that when the cluster \(C_{i}\) of \(n_{i}\) core points is not thin anywhere, i.e. the common neighborhood of any two nearby core points has at least \(t=O()})\) core points, sDBSCAN can recover \(C_{i}\) with probability \(1-{1/n_{i}}\). While our statistical guarantee is inspired by , we do not need any strong assumption about the data distribution as used in . However, it comes with the cost of maintaining larger neighborhoods around the random vector \(_{i}\) (i.e. \(S_{i}\), \(R_{i}\)) that causes significant computational resources.

In practice, a core point \(\) is often surrounded by many other core points in a dense cluster. Therefore, instead of maintaining the sets \(S_{i},R_{i}\) for each random vector \(_{i}\), we can keep the top-\(m\) points of these sets where \(m=O(minPts)\). This practical setting substantially reduces the memory usage and running time of sDBSCAN without degrading clustering results. The next subsection justifies it.

### From theory to practice

We observe that for any core point \(\) regarding \((,minPts)\), \(|B_{}()|\) is often substantially larger than \(minPts\). We will present heuristics to improve the performance of sDBSCAN.

**Identify core points with \(m=O(minPts)\).** Given a core point \(\), we denote by \( B_{}()\) and \( B_{}()\) any close and far away points to \(\) regarding the distance threshold \(\). W.l.o.g., we assume that \(_{*}=_{_{i}}^{}_{i}\), and let \(X=^{}_{*},Y=^{}_{*}\) be the projection values of \(,\) on \(_{*}\), respectively. From Eq. (1), \(X-Y\) follows a Gaussian distribution whose variance is bounded by \((^{})^{2}}+^{})^{2}})^{2}\). Let \(_{}=(^{}-^{ })/(^{} )^{2}}+^{})^{2}})\) and \(_{*}=_{ B_{}(),  B_{}()}_{}\). As Lemma 1 holds for \(k\) closest/furthest random vectors, assume that the event any point \( B_{}()\) is ranked higher than all \( B_{}()\) is independent, Lemma 3 justifies that \(m=minPts\) suffices to identify \(\) as a core point. The proof based on the tail bound of the Gaussian variable \(X-Y\) and the union bound is left in the appendix.

**Lemma 3**.: _Given \(D=n^{1/k_{*}^{2}}\) for a given core point \(^{d-1}\) where \(|B_{}()| minPts\), maintaining top-\(minPts\) points associated to \(k\) closest/furthest vectors to \(\) ensures \(|_{}()| minPts,_{ }() B_{}()\) with probability at least \((1-1/n)^{minPts} e^{-minPts/n}\)._

Lemma 3 holds given the dependence of \(D\) on the data distribution around \(\) (i.e. \(_{*}\)). We empirically observe that the distance gap between \(\) and points inside and outside \(B_{}()\) is significant, making \(_{*}^{2}\) large. Hence, setting \(D=1,024,k=\{5,10\}\) suffices to identify core points and enrich their neighborhoods, achieving high clustering quality on many real-world data sets.

**sDBSCAN-1NN to cluster misidentified border points.** As sDBSCAN focuses on identifying core points and approximating their neighborhoods, it misclassifies border points as noise. As the neighborhood size of detected core points is upper bound by \(2km\), sDBSCAN might suffer accuracy loss on data sets with a significantly large number of border points. Denote by \(C\) and \(\) the set of clustered core points and noisy points found by sDBSCAN, we propose a simple heuristic to classify any \(\). We will build a nearest neighbor classifier (1NN) where training points are _sampled_ from \(C\), and scale up this classifier with the CEOs-based estimation approach [22, Alg. 1]. We call sDBSCAN with the approximate 1NN heuristic as _sDBSCAN-1NN_.

In particular, for each sampled core point \( C\), we recompute their random projection values with the stored \(D\) random vector \(_{i}\) as we do not keep these values after preprocessing. For each noisy point \(\), we retrieve the _precomputed_ top-\(k\) closest/furthest vectors and use the projection values of \(\) at these top-\(k\) vectors to estimate \(^{}\). To ensure this heuristic does not affect sDBSCAN time where \(|C||| n/2\), we sample \(0.01n\) core points from \(C\) to build the training set. Empirically, sDBSCAN time dominates this heuristic time, and the extra space usage to store \(0.01nD\) projection values is negligible.

_Remark_. sDBSCAN quickly finds core points and assigns cluster labels for non-core points using a lightweight index. In contrast, LSH-based approaches  need several LSH tables or multi-probes  that cause significant space or time overheads. Also, it seems non-trivial for multi-probe LSH to achieve DBSCAN's accuracy with guarantees as the collision probability of multi-probes is hard to control. Assigning label for identified non-core points with LSH seems non-trivial as these points might collide with many core points from different clusters.

**Extend to other distance measures.** We extend sDBSCAN to other popular distance measures, including L2, L1, \(^{2}\), and Jensen-Shannon (JS) via popular randomized kernel features . In particular, we study fast randomized feature mapping \(f:^{d}^{d^{}}\) such that \([f()^{}f()]=K(,)\) where \(K\) are Gaussian, Laplacian, \(^{2}\), and JS kernels. As we execute random projections on the constructed random features for each point and compute \(dist(,)\) using the original data, we only need a small extra space to store the randomness associated with \(f\). These embeddings' extra costs are negligible compared to sDBSCAN. Detailed discussion is left in the appendix.

**Reduce the random projection costs.** We will use the Structured Spinners  that exploits Fast Hadamard Transform (FHT) to reduce the cost of Gaussian random projections. In particular, we generate 3 random diagonal matrices \(_{1},_{2},_{3}\) whose values are randomly chosen in \(\{+1,-1\}\). The random projection of \(\) is simulated by the mapping \(_{3}_{2}_{1}\) where \(\) is the Hadamard matrix. With FHT, the random projection can be simulated in \((D(D))\) time and use additional \(O(D)\) extra space to store random matrices.

### The time and space complexity of sDBSCAN

We analyze the time and space complexity of sDBSCAN with \(m=O(minPts)\).

**Time complexity.** With FHT, retrieving top-\(k\) closest/furthest vectors for each \(\), and top-\(m\) closest/furthest points for each random vector \(_{i}\) in the preprocessing (Alg. 2) runs in \(O(nD(D)+nD(k)+nD(minPts) )\) time. Finding approximate neighborhoods for \(n\) points (Alg. 3) runs in \(O(dnk minPts)\) time since each point needs \(O(k minPts)\) distance computations. For a constant \(k\), sDBSCAN runs in \(O(dn minPts+nD(D))\) time. When \(D=o(n)\), sDBSCAN runs in _subquadratic_ time.

Empirically, the cost of identifying core points and their neighborhood dominates the preprocessing cost due to the expensive random memory access of distance computations. Nevertheless, preprocessing and finding neighborhood steps are elementary to run in parallel. Our straightforward multi-threading implementation of sDBSCAN and sOPTICS shows a 10\(\) speedup with 64 threads.

**Space complexity.** sDBSCAN needs \(O(nk+D minPts)\) extra space to store \(O(k)\) closest/furthest vectors for each point, and \(O(minPts)\) points closest/furthest to each random vector. When \(D=o(n)\), sDBSCAN's additional memory is negligible compared to the data size \(O(nd)\). This key feature makes sDBSCAN highly scalable on million-point data sets compared to standard DBSCAN and several kernel-based clustering [4; 5].

## 4 Experiment

We implement sDBSCAN and sOPTICS in C++ and compile with g++ -O3 -std=c++17 -fopenmp -march=native. We conducted experiments on Ubuntu 20.04.4 with an AMD Ryzen Threadripper 3970X 2.2GHz 32-core processor (64 threads) with 128GB of DRAM. We present empirical evaluations on the clustering quality compared to the ground truth (i.e. data labels) to verify our claims, including:

* sDBSCAN with the suggested parameter values provided by sOPTICS runs significantly faster and achieves competitive accuracy compared to other clustering algorithms.
* Multi-threading sDBSCAN and sOPTICS run in _minutes_, while the scikit-learn counterparts cannot run on million-point data sets due to memory constraints on our hardware.

Our competitors include pDBSCAN  as a representative grid-based approach, DBSCAN++  and sngDBSCAN . DBSCAN++ has two variants, including DBSCAN++ with uniform initialization (uDBSCAN++) that uses KD-Trees to speed up the search of core points and \(k\)-center initialization (kDBSCAN++). We also compare with multi-threading scikit-learn implementations of DBSCAN and OPTICS. To demonstrate the scalability and utility of sDBSCAN on other distance measures, we compare it with the result of kernel \(k\)-means in . We found other clustering competitors could not work on million-point data sets given 128 GB of RAM, detailed in the appendix.

We use the adjusted mutual information (AMI)  to measure the clustering quality. Results on other measures , including normalized mutual information (NMI) and correlated coefficients (CC) are in the appendix. We conduct experiments on three popular data sets: Mnist (\(n=70,000,d=784\), # clusters = 10), Pamp2 (\(n=1,770,131,d=51\), # clusters = 18), and Mnist8m (\(n=8,100,000,d=784\), # clusters = 10). All results are the average of 5 runs of the algorithms.

**Parameter settings.** We consider \(minPts=50\) for all experiments. sDBSCAN and sOPTICS use \(D=1024\), \(m=minPts\). Randomized kernel embeddings use \(=2,d^{}=1024\). We use \(k=5\) for Mnist and \(k=10\) for Pamp2 and Mnist8m. We first run sOPTICS to select a relevant range of

Figure 1: Top: sOPTICSâ€™s graphs on L1, L2, cosine, JS on Mnist. sOPTICS runs within **3 seconds** while scikit-learn OPTICS requires **1.5 hours** on L2. Bottom: AMI of DBSCAN variants on L1, L2, cosine, JS over the range of \(\) suggested by sOPTICS. Cosine and JS give the highest AMI.

values of \(\) for DBSCAN variants. For DBSCAN++ and sngDBSCAN variants, we set the sampling probability \(p=0.01\) for Mnist and Pamap2, and \(p=0.001\) for Mnist8m to have a similar number of distance computations with sDBSCAN. pDBSCAN uses \(=0.001\). Experiments on the sensitivity of parameters \(m,k,,d^{},minPts\) of sDBSCAN and sOPTICS are left in the appendix.

### An ablation study of sOPTICS and sDBSCAN on Mnist

While DBSCAN on L2 is popular, the capacity to use arbitrary distances is an advantage of DBSCAN compared to other clustering algorithms. We use sOPTICS to visualize the cluster structure on several distance measures, including L1, L2, cosine, and JS on Mnist with \(D=1,024,m=minPts=50,k=5\). By using the average top-100 nearest neighbor distances of 100 random points to find the setting of \(\) for sOPTICS, we set \(=1,800\) for L2, \(=18,000\) for L1, and \(=0.25\) for the others.

The top subfigures of Figure 1 show reachability-plot dendrograms of sOPTICS on 4 studied distance measures. Since points belonging to a cluster have a small reachability distance to their nearest neighbors, the number of valleys in the dendrograms reflects the cluster structure. Therefore, we can predict that cosine and JS provide higher clustering quality than L2 while L1 suffers low-quality clustering. Importantly, any \(\) in the range \([0.1,0.15]\) can differentiate the 4 valleys by cosine and JS while selecting a specific value to separate the 3 valleys by L2 is impossible. Note that 64-thread sOPTICS runs in less than _3 seconds_ while 64-thread scikit-learn OPTICS demands _1.5 hours_ on L2. Indeed, sOPTICS can output similar OPTICS graphs within 30 seconds, as shown in the appendix.

The bottom subfigures of Figure 1 show AMI scores of several DBSCAN variants over the recommended ranges of \(\) by sOPTICS. Note that sDBSCAN and DBSCAN reach the peak on such ranges while sampling-based variants do not. sDbscan is superior on all 4 supported distances, except L2 by uDBSCAN++. While sDBSCAN reaches DBSCAN's accuracy of AMI 43% on cosine and JS, sngDBSCAN gives significantly lower accuracy on all 4 distances. uDBSCAN++ gives at most 32% AMI on L2 and cosine while kDBSCAN++ does not provide a meaningful result on the studied range values of \(\). L2 and L1 distances show inferior performance on clustering compared to cosine and JS, as can be predicted from their corresponding sOPTICS graphs.

Table 1 summarizes the performance of studied DBSCAN variants on the _best_\([0.1,0.2]\) with cosine distance. On 1 thread, sDBSCAN runs nearly \(2,8\) and \(10\) faster than sngDBSCAN, uDBSCAN++ and kDBSCAN++, respectively. 64-thread sDBSCAN runs nearly \(10\) faster than 1-thread sDBSCAN and \(100\) faster than 64-thread scikit-learn. Though pDBSCAN shares the same AMI with scikit-learn, its running time makes it infeasible for high-dimensional data sets.

### Comparison on million-point data sets: Pamap2 and Mnist8m

This subsection compares the performance of sDBSCAN, DBSCAN++, and sngDBSCAN on million-point data sets. scikit-learn DBSCAN and pDBSCAN cannot finish after 4 hours. Our implemented sngDBSCAN runs faster than  and supports multi-threading. We use sOPTICS graphs in the appendix to select relevant ranges of \(\). sOPTICS runs in **2 mins** and **11 mins** on Pamap2 and Mnist8m, respectively, significantly faster than DBSCAN++ and sngDBSCAN. The released DBSCAN++ does not support L1, \(^{2}\), JS and multi-threading while the rest are in multi-threading.

**Pamap2.** Figure 2 shows the performance of sDBSCAN compared to DBSCAN++ and sngDBSCAN. Given suggested ranges of \(\) by sOPTICS, sDBSCAN's AMI peaks are consistently higher than that of sngDBSCAN but 5% lower than DBSCAN on L1 and L2. While sDBSCAN achieves the AMI peak on the studied ranges of \(\) on 3 distances, the performance of the others is very different on different range of \(\) with cosine. We found that DBSCAN with \(=0.005\) returns AMI 47% but

   Alg. & scikit-learn & sDBSCAN & uDBSCAN++ & kDBSCAN++ & sngDBSCAN & pDBSCAN \\  AMI & **43\%** & **43\%** & **43\%** & 7\% & 33\% & **43\%** \\ Time & 86s & **8.8s** & 67s & 87s & 18s & \(1.85\) hours \\ \(\) & 0.11 & 0.14 & 0.18 & 0.2 & 0.15 & 0.11 \\   

Table 1: AMI on the best \([0.1,0.2]\) and running time of **64-thread** scikit-learn vs. **1-thread** DBSCAN variants using cosine distance on Mnist. **64-thread** sDBSCAN runs in **0.9 seconds**.

offers only 37% AMI on \(=0.01\). This explains the reliability of sOPTICS in guiding the parameter setting for sDBSCAN and the difficulty in selecting relevant \(\) for other DBSCAN variants without any scalable visual tools.

**Mnist8m.** As DBSCAN++ with \(p=0.001\) could not finish after 4 hours, we only report sngDBSCAN. As we cannot run scikit-learn \(k\)-means++ or any kernel-based clustering [4; 5] with 128 GB of RAM, we use the result of 41% NMI given by a fast kernel \(k\)-means  running on a supercomputer with _32 nodes_, each of which has _two_ 2.3GHz 16-core (32 threads) Haskell processors and 128GB of DRAM. This configuration of a _single_ node is similar to our conducted machine. Figure 3 shows the NMI scores of sDBSCAN-1NN with 1-NN heuristic described in Subsection 3.3, sDBSCAN, sgnDBSCAN, and kernel \(k\)-means on Mnist8m. sDBSCAN-1NN shows superiority among DBSCAN variants. Its peak is at least 10% and 5% higher than sngDBSCAN and sDBSCAN, respectively, on studied measures. sDBSCAN consistently gives higher accuracy than sngDBSCAN with the most significant gap of 5% on \(^{2}\) and cosine. Note that sDBSCAN-1NN samples \(0.01n\) core points, and uses CEOs to build the approximate 1NN classifier, the time overhead of this step is smaller than sDBSCAN's time. Indeed, sDBSCAN-1NN with \(minPts=100\) reaches 40% NMI on \(^{2}\), running within 15 minutes. Details of the running time comparison are in the appendix.

## 5 Conclusion

The paper presents a simple and scalable sDBSCAN for density-based clustering, and sOPTICS for interactive clustering exploration for high-dimensional data. By leveraging the neighborhood preservation of random projections, sDBSCAN preserves the DBSCAN's output with theoretical guarantees under mild conditions. We extend our proposed algorithms to other distance measures to facilitate density-based clustering on many applications with image data. Empirically, both sDBSCAN and sOPTICS are highly scalable, run in minutes on million-point data sets, and provide very competitive accuracy compared to other clustering algorithms. We hope sDBSCAN and sOPTICS will be featured on popular clustering libraries shortly.

Figure 3: NMI comparison of DBSCAN variants on L2, cosine, \(^{2}\), and JS and kernel \(k\)-means on Mnist8m over a wide ranges of \(\) suggested by sOPTICS. sDBSCAN and sDBSCAN-1NN runs within **10 mins** and **15 mins** while sngDBSCAN demands nearly **1 hour**. Kernel \(k\)-means (\(k=10\))  runs in **15 mins** on a supercomputer of 32 nodes, each has 64 threads and 128 GB of DRAM.

Figure 2: AMI comparison of DBSCAN variants on L1, L2 and cosine on Pamp2 over a wide range of \(\) suggested by sOPTICS. sDBSCAN runs within **0.3 mins**, nearly **10 \(\), **10 \(\), **45 \(\), **100 \(\)** faster than sngDBSCAN, uDBSCAN++, kDBSCAN++, and DBSCAN. L1 gives the highest AMI.