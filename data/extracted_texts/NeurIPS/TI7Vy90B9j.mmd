# Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a payoff perturbation technique, introducing a strong convexity to players' payoff functions in games. This technique is specifically designed for first-order methods to achieve last-iterate convergence in games where the gradient of the payoff functions is monotone in the strategy profile space, potentially containing additive noise. Although perturbation is known to facilitate the convergence of learning algorithms, the magnitude of perturbation requires careful adjustment to ensure last-iterate convergence. Previous studies have proposed a scheme in which the magnitude is determined by the distance from an anchoring or reference strategy, which is periodically re-initialized. In response, this paper proposes Gradient Ascent with Boosting Payoff Perturbation, which incorporates a novel perturbation into the underlying payoff function, maintaining the periodically re-initializing anchoring strategy scheme. This innovation empowers us to provide faster last-iterate convergence rates against the existing payoff perturbed algorithms, even in the presence of additive noise.

## 1 Introduction

This study considers online learning in monotone games, where the gradient of the payoff function is monotone in the strategy profile space. Monotone games encompassed diverse well-studied games as special instances, such as concave-convex games, zero-sum polymatrix games (Cai and Daskalakis, 2011; Cai et al., 2016), \(\)-cocoercive games (Lin et al., 2020), and Cournot competition (Bravo et al., 2018). Due to their wide-ranging applications, there has been growing interest in developing learning algorithms to compute Nash equilibria in monotone games.

Typical learning algorithms such as Gradient Ascent (Zinkevich, 2003) and Multiplicative Weights Update (Bailey and Piliouras, 2018) have been extensively studied and shown to converge to equilibria in an average-iterate sense, which is termed _average-iterate convergence_. However, averaging the strategies can be undesirable because it can lead to additional memory or computational costs in the context of training Generative Adversarial Networks (Goodfellow et al., 2014) and preference-based fine-tuning of large language models (Munos et al., 2023; Swamy et al., 2024). In contrast, _last-iterate convergence_, in which the updated strategy profile itself converges to a Nash equilibrium, has emerged as a stronger notion than average-iterate convergence.

Payoff-perturbed algorithms have recently been regaining attention in this context (Sokota et al., 2023; Liu et al., 2023). Payoff perturbation is a classical technique, e.g., (Facchinei and Pang, 2003) and introduces a strongly convex penalty to the players' payoff functions to stabilize learning, which leads to convergence to approximate equilibria, not only in the _full feedback_ setting where the perfect gradient vector of the payoff function can be used to update strategies, but also in the _noisy feedback_ setting where the gradient vector is contaminated by noise.

However, to ensure convergence toward a Nash equilibrium of the underlying game, the magnitude of perturbation requires careful adjustment. As a remedy, it is adjusted by the distance from an anchoring or reference strategy. Koshal et al. (2010) and Tatarenko and Kamgarpour (2019) simply decay the magnitude in each iteration, and their methods asymptotically converge, since the perturbed function gradually loses strong convexity. In response to this, recent studies (Perolat et al., 2021; Abe et al., 2023, 2024) re-initialize the anchoring strategies periodically, or in a predefined interval, so that they keep the perturbed function strongly convex and achieve non-asymptotic convergence.

We should also mention the _optimistic_ family of learning algorithms, which incorporates recency bias and exhibits last-iterate convergence (Daskalakis et al., 2018; Daskalakis and Panageas, 2019; Mertikopoulos et al., 2019; Wei et al., 2021). Unfortunately, the property has mainly been proven in the full feedback setting. Although it might empirically work with noisy feedback, the convergence is slower, as demonstrated in Section 6. The fast convergence in the noisy feedback setting is another reason why payoff-perturbed algorithms have been gaining renewed interest.

This paper, in particular, focuses on _Adaptively Perturbed Mirror Descent_ (APMD) (Abe et al., 2024), which achieves \(}(1/)\)1 and \(}(1/T^{})\) last-iterate convergence rates in the full/noisy feedback setting, respectively. The motivation of this study lies in improving the convergence rates of APMD. We propose an elegant one-line modification of APMD, which effectively accelerates convergence. In fact, we just add the difference between the current anchoring strategy and the initial anchoring strategy to the payoff perturbation function in APMD.

Our contributions are manifold. Firstly, we propose a novel payoff-perturbed learning algorithm named _Gradient Ascent with Boosting Payoff Perturbation_ (GABP). This method incorporates a unique perturbation payoff function, enabling it to achieve faster convergence rates than APMD. Subsequently, we prove that GABP exhibits accelerated \(}(1/T)\) and \(}(1/T^{})\) last-iterate convergence rates to a Nash equilibrium with full and noisy feedback, respectively. We further show that each player's individual regret is at most \((( T)^{2})\) in the full feedback setting, provided all players play according to GABP. Finally, through our experiments, we demonstrate the competitive or superior performance of GABP over Optimistic Gradient Ascent (Daskalakis et al., 2018; Wei et al., 2021) and APMD in concave-convex games, irrespective of the presence of noise.

## 2 Preliminaries

Monotone games.In this study, we focus on a continuous multi-player game, which is denoted as \(([N],(_{i})_{i[N]},(v_{i})_{i[N]})\). \([N]=\{1,2,,N\}\) denotes the set of \(N\) players. Each player \(i[N]\) chooses a _strategy_\(_{i}\) from a \(d_{i}\)-dimensional compact convex strategy space \(_{i}\), and we write \(=_{i[N]}_{i}\). Each player \(i\) aims to maximize her payoff function \(v_{i}:\), which is differentiable on \(\). We denote \(_{-i}_{j i}_{j}\) as the strategies of all players except player \(i\), and \(=(_{i})_{i[N]}\) as the _strategy profile_. This paper particularly studies learning in _smooth monotone games_, where the gradient operator \(V()=(_{_{i}}v_{i}())_{i[N]}\) of the payoff functions is monotone: \(,^{}\),

\[ V()-V(^{}),-^{} 0,\] (1)

and \(L\)-Lipschitz for \(L>0\)

\[\|V()-V(^{})\| L\|-^{}\|,\] (2)

where \(\|\|\) denotes the \(_{2}\)-norm.

Many common and well-studied games, such as concave-convex games, zero-sum polymatrix games (Cai et al., 2016), \(\)-cocoercive games (Lin et al., 2020), and Cournot competition (Bravo et al., 2018), are included in the class of monotone games.

**Example 2.1** (Concave-Convex Games).: Consider a game defined by \((\{1,2\},(_{1},_{2}),(v,-v))\), where \(v:_{1}_{2}\). In this game, player \(1\) wishes to maximize \(v\), while player \(2\) aims to minimize \(v\). If \(v\) is concave in \(x_{1}_{1}\) and convex in \(x_{2}_{2}\), the game is called a concave-convex game or minimax optimization problem, and it is not hard to see that this game is a special case of monotone games.

Nash equilibrium and gap function.A _Nash equilibrium_(Nash, 1951) is a widely used solution concept for a game, which is a strategy profile where no player can gain by changing her own strategy. Formally, a strategy profile \(^{*}\) is called a Nash equilibrium, if and only if \(^{*}\) satisfies the following condition:

\[ i[N],_{i}_{i},\;v_{i}(_{i}^{*},_{-i}^ {*}) v_{i}(_{i},_{-i}^{*}).\]

We define the set of all Nash equilibria to be \(^{*}\). It has been shown that there exists at least one Nash equilibrium (Debreu, 1952) for any smooth monotone games.

To quantify the proximity to Nash equilibrium for a given strategy profile \(\), we use the _gap function_, which is defined as:

\[():=_{} V(), {}-.\]

Additionally, we use another measure of proximity to Nash equilibrium, referred to as the _tangent residual_. This measure is defined as:

\[r^{}():=_{a N_{}()}\|-V()+a\|,\]

where \(N_{}()=\{(a_{i})_{i[N]}_{i=1}^{N}^{d_{i}} _{i=1}^{N} a_{i},_{i}^{}-_{i} 0,\; ^{}\}\) is the normal cone of \(\). It is easy to see that \(() 0\) (resp. \(r^{}() 0\)) for any \(\), and the equality holds if and only if \(\) is a Nash equilibrium. Defining \(D:=_{,^{}}\|-^{}\|\) as the diameter of \(\), the gap function for any given strategy profile \(\) is upper bounded by its tangent residual.

**Lemma 2.2** (Lemma 2 of Cai et al. (2022)).: _For any \(\), we have:_

\[() D r^{}().\]

The gap function and the tangent residual are standard measures of proximity to Nash equilibrium; e.g., it has been used in Cai and Zheng (2023); Abe et al. (2024).

Problem setting.This study focuses on the online learning setting in which the following process repeats from iterations \(t=1\) to \(T\): (i) Each player \(i[N]\) chooses her strategy \(_{i}^{t}_{i}\), based on previously observed feedback; (ii) Each player \(i\) receives the (noisy) gradient vector \(_{_{i}}v_{i}(^{t})\) as feedback. This study examines two feedback models: _full feedback_ and _noisy feedback_. In the full feedback setting, each player observes the perfect gradient vector \(_{_{i}}v_{i}(^{t})=_{_{i}}v_{i}(^{t})\). In the noisy feedback setting, each player's gradient feedback \(_{_{i}}v_{i}(^{t})\) is contaminated by an additive noise vector \(_{i}^{t}\), i.e., \(_{_{i}}v_{i}(^{t})=_{_{i}}v_{i}(^{t})+_{i}^ {t}\), where \(_{i}^{t}^{d_{i}}\). Throughout the paper, we assume that \(_{i}^{t}\) is the zero-mean and bounded-variance noise vector at each iteration \(t\).

Adaptively perturbed Mirror Descent.To facilitate the convergence in the online learning setting, recent studies have utilized a _payoff perturbation_ technique, where payoff functions are perturbed by strongly convex functions (Sokota et al., 2023; Liu et al., 2023; Abe et al., 2022). However, while the addition of these strongly convex functions leads learning algorithms to converge to a stationary point, this stationary point may be significantly distant from a Nash equilibrium. Therefore, the magnitude of perturbation requires careful adjustment. Perolat et al. (2021); Abe et al. (2023, 2024) have introduced a scheme in which the magnitude is determined by the distance (or divergence function) from an anchoring strategy \(_{i}\), which is periodically re-initialized. Specifically, Adaptively Perturbed Mirror Descent (APMD) (Abe et al., 2024) perturbs each player's payoff function by a strongly convex divergence function \(G(_{i},_{i}):_{i}_{i}[0,)\), where the anchoring strategy \(_{i}\) is periodically replaced by the current strategy \(_{i}^{t}\) every predefined iterations \(T_{}\).

Let us define \(_{i}^{k(t)}\) as the anchoring strategy after \(k(t)\) updates. Since \(_{i}\) is overwritten every \(T_{}\) iterations, we can write \(k(t)=(t-1)/T_{}+1\) and \(_{i}^{k(t)}=_{i}^{T_{}(k(t)-1)+1}\). Except for the payoff perturbation and the update of the anchor strategy, APMD updates each player \(i\)'s strategy in the same way as standard Mirror Descent algorithms:

\[_{i}^{t+1}=*{arg\,max}_{x_{i}}\{_{t} _{_{i}}v_{i}(^{t})-_{_{i}}G(_ {i}^{t},_{i}^{k(t)}),x-D_{}(x,_{i}^{t})\},\]where \(_{t}\) is the learning rate at iteration \(t\), \((0,)\) is the _perturbation strength_, and \(D_{}(_{i},_{i}^{})=(_{i})-(_{i}^{})- (_{i}^{}),_{i}-_{i}^{}\) as the Bregman divergence associated with a strictly convex function \(:_{i}\). When both \(G\) and \(D_{}\) is set to the squared \(^{2}\)-distance, this algorithm can be equivalently written as:

\[_{i}^{t+1}=*{arg\,max}_{x_{i}}\{_{t} _{_{i}}v_{i}(^{t})-(_{i}^{t}- _{i}^{k(t)}),x-\|x-_{i}^{t}\| ^{2}\}.\] (3)

We refer to this version of APMD as Adaptively Perturbed Gradient Ascent (APGA). Abe et al.  have shown that APGA exhibits the convergence rates of \(}(1/)\) and \(}(1/T^{})\) with full and noisy feedback, respectively.

## 3 Gradient ascent with boosting payoff perturbation

This section proposes an accelerated version of APGA, Gradient Ascent with Boosting Payoff Perturbation (GABP). The pseudo-code of GABP is outlined in Algorithm 1. In order to obtain faster last-iterate convergence rates compared to APGA, GABP introduces a novel payoff perturbation term in addition to APGA's original payoff perturbation term, \((_{i}^{t}-_{i}^{k(t)})\). Formally, GABP updates each player's strategy as follows:

\[_{i}^{t+1}=*{arg\,max}_{x_{i}}_{t} _{_{i}}v_{i}(^{t})-^{k(t)}-_{i}^{1}}{k(t)+1}}_{(*)}-(_{i}^{t}-_{ i}^{k(t)}),x-\|x-_{i}^{t}\|^{2} }.\] (4)

The term \((*)\) is our proposed additional perturbation term. It shrinks as \(k(t)\), the number of updates of \(_{i}^{k(t)}\), increases.

For a more intuitive explanation of the proposed perturbation term, we present the following update rule, which is equivalent to (4):

\[_{i}^{t+1}=*{arg\,max}_{x_{i}}\{_{t} _{}v_{i}(^{t})-(_{i}^{t}-^{k(t)}+_{i}^{1}}{k(t)+1}),x- \|x-_{i}^{t}\|^{2}\}.\]

From this formula, it appears that GABP replaces the reference strategy \(_{i}^{k(t)}\) for the perturbation term in (3) of APGA with \(^{k(t)}+_{i}^{1}}{k(t)+1}\). As a result, the anchoring strategy in GABP evolves more gradually than in APGA, leading to further stabilization of the learning dynamics. There is a tradeoffbetween the shrinking speed of the term (*) and the stabilizing impact on the last-iterate convergence rate of GABP. The shrinking speed of \(1/(k(t)+1)\) achieves a faster convergence rate, and we believe that this represents the optimal balance for this trade-off. Although one might think that the term (*) is closely related to Accelerated Optimistic Gradient (AOG) (Cai and Zheng, 2023), we discuss the detail in Appendix F to be concise and avoid a complicated explanation.

## 4 Last-iterate convergence rates

This section provides the last-iterate convergence rates of GABP in the full/noisy feedback setting, respectively.

### Full feedback setting

First, we demonstrate the last-iterate convergence rate of GABP with _full feedback_ where each player receives the perfect gradient vector as feedback at each iteration \(t\), i.e., \(_{_{i}}v_{i}(^{t})=_{_{i}}v_{i}(^{t})\). Theorem 4.1 shows that the last-iterate strategy profile \(^{T}\) updated by GABP converges to a Nash equilibrium with an \(}(1/T)\) rate in the full feedback setting.

**Theorem 4.1**.: _If we use the constant learning rate \(_{t}=(0,})\) and the constant perturbation strength \(>0\), and set \(T_{}=c(1,)\) for some constant \(c 1\), then the strategy \(^{t}\) updated by GABP satisfies for any \(t\{2,3,,T+1\}\):_

\[(^{t}) (+1 )}{t-1}(+),\] \[r^{}(^{t}) +1)}{t-1 }(+).\]

This rate is significantly faster than APGA's rate of \(}(1/)\). Moreover, it is a competitive rate compared to the previous state-of-the-art rate of \((1/T)\)(Yoon and Ryu, 2021; Cai and Zheng, 2023). Note that the rate in Theorem 4.1 holds for any constant perturbation strength \(>0\).

#### 4.1.1 Proof sketch of Theorem 4.1

To derive the bound of the gap function \((^{t})\), it is sufficient to derive that of \(r^{}(^{t})\) due to Lemma 2.2. This section provides the proof sketch of Theorem 4.1. The complete proof is placed in Appendix B.

**(1) Decomposition of the tangent residual of the last-iterate strategy profile.** From the first-order optimality condition for \(^{t}\), we can see that \(V(^{t-1})-(^{t-1}-+^{t}}{k(t-1 )+1})-(^{t}-^{t-1}) N_{}( ^{t})\). Therefore, from the triangle inequality and \(L\)-smoothness (2) of the gradient operator, the tangent residual \(r^{}(^{t})\) can be bounded as:

\[r^{}(^{t}) =_{a N_{}(^{t})}\|-V(^{t})+a\|\] \[(\|^{t}-^{t-1}\|)+ (\|^{t-1}-^{k(t-1)}\|)+ ().\]

Let us define the stationary point \(^{,^{k(t)}}\), which satisfies the following condition: \( i[N]\),

\[^{,^{k(t)}}_{i}=*{arg\,max}_{x_{i}} \{v_{i}(x,^{,^{k(t)}}_{-i})-\|x-^{k(t)}\|^{2}\},\]

where \(^{k(t)}_{i}=_{i}+^{1}_{i}}{k(t)+1}\). We will show that \(^{t}\) converges to the stationary point \(^{,^{k(t)}}\) at an exponential rate later. By using \(^{,^{k(t)}}\) and applying the triangle inequality to \(\|^{t}-^{t-1}\|\), we decompose the term of \((\|^{t}-^{t-1}\|)\) into \((\|^{t}-^{,^{k(t-1)}}\|)\) and \((\|^{,^{k(t-1)}}-^{t-1}\|)\).

Similarly, the term of \((\|^{t-1}-^{k(t-1)}\|)\) is decomposed into \((\|^{t-1}-^{,^{k(t)-1}}\|)\) and \((\|^{,^{k(t)-1}}-^{k(t-1)}\|)\). Then, the tangent residual is bounded as follows:

\[r^{}(^{t}) (\|^{,^{k(t-1)}}-^{t} \|)+(\|^{,^{k(t-1)}}-^{t-1} \|)\] \[+(\|^{,^{k(t-1)}}-^{ k(t-1)}\|)+().\] (5)

Therefore, it is enough to derive the convergence rate on \(\|^{,^{k(t)-1}}-^{t}\|\) and \(\|^{,^{k(t-1)}}-^{k(t-1)}\|\).

**(2) Convergence rate of \(^{t}\) to the stationary point \(^{,^{k(t)}}\).** Using the strong convexity of the perturbation payoff function, \(\|x-_{i}^{k(t)}\|^{2}\), we show that \(^{t}\) converges to \(^{,^{k(t)}}\) exponentially fast (in Lemma B.1). That is, we have for any \(t 1\):

\[\|^{,^{k(t)}}-^{t}\|^{2}()^{t-(k(t)-1)T_{}-1}\|^{,^{k(t)}}-^{k(t)} \|^{2}.\] (6)

Since the first and second terms of the right-hand side of (5) are bounded by the distance between the stationary point and the anchoring strategy by using (6), we have:

\[r^{}(^{t})(\|^{,^{k(t-1)}}-^ {k(t-1)}\|)+().\] (7)

**(3) Potential function for bounding the distance between \(^{,^{k(t)-1}}\) and \(^{k(t)-1}\).** To derive the upper bound on \(\|^{,^{k(t-1)}}-^{k(t-1)}\|\), we define the following potential function \(P^{k(t)}\):

\[P^{k(t)} :=\|^{,^{k(t)-1}}-^{k(t)-1}\|^{2}\] \[+k(t)(k(t)+1)^{k(t)}-^{, ^{k(t)-1}},^{,^{k(t)-1}}-^{k(t)-1}.\]

By some algebra, we can see that \(P^{k(t)}\) is approximately non-increasing (in Lemma B.3). That is, we have for any \(t 1\) such that \(k(t) 2\):

\[P^{k(t)+1} P^{k(t)}+(k(t)+1)^{2}(\|^{, ^{k(t)}}-^{k(t)+1}\|+\|^{,^{k(t)-1}}- ^{k(t)}\|).\] (8)

Using (6) again, it is easy to show that \(\|^{,^{k(t)}}-^{k(t)+1}\|+\|^{, ^{k(t)-1}}-^{k(t)}\|(})\) for a sufficiently large \(T_{}\). Therefore, under the assumption that \(T_{}( T)\), by telescoping of (8) and some algebra, we can derive the following upper bound on \(\|^{,^{k(t)}}-^{k(t)}\|\) (in Lemma B.2):

\[\|^{,^{k(t)}}-^{k(t)}\|().\] (9)

**(4) Putting it all together: last-iterate convergence rate of \(^{t}\).** By combining (7) and (9), we get \(r^{}(^{t})()\). Therefore, since \(k(t)=}+1\), it holds that \(r^{}(^{t})(}{t+T_{}-2})\). Finally, taking \(T_{}=( T)\), we have:

\[r^{}(^{t})().\]

The upper bound on the gap function is immediately obtained since we have Lemma 2.2. 

### Noisy feedback setting

Next, we establish the last-iterate convergence rate in the _noisy feedback_ setting, where each player \(i\) observes a noisy gradient vector contaminated by an additive noise vector \(_{i}^{t}^{d_{i}}\): \(_{_{i}}v_{i}(^{t})+_{i}^{t}\). We assume that the noisy vector \(_{i}^{t}\) is zero-mean and its variance is bounded. Formally, defining the sigma-algebra generated by the history of the observations as \(_{t}:=((_{_{i}}v_{i}(^{1}))_{i[ N]},,(_{_{i}}v_{i}(^{t-1}))_{i[N]})\), \( t 1\), the noisy vector \(_{i}^{t}\) is assumed to satisfy the following conditions:

**Assumption 4.2**.: _For all \(t 1\) and \(i[N]\), the noise vector \(_{i}^{t}\) satisfies the following properties: (a) Zero-mean: \([_{i}^{t}|_{t}]=(0,,0)^{}\); (b) Bounded variance: \([\|_{i}^{t}\|^{2}|_{t}] C^{2}\) with some constant \(C>0\)._

Assumption 4.2 is standard in online learning in games with noisy feedback (Mertikopoulos and Zhou, 2019; Hsieh et al., 2019; Abe et al., 2024) and stochastic optimization (Nemirovski et al., 2009; Nedic and Lee, 2014). Under Assumption 4.2 and a decreasing learning rate sequence \(_{t}\), we can obtain a faster last convergence rate \(}(1/T^{}})\) than the convergence rate \(}(1/T^{}})\) of APGA.

**Theorem 4.3**.: _Let \(=,=+8L^{2}}{2}\). Suppose that Assumption 4.2 holds and \(V()\) for any \(\). We also assume that \(T_{}\) is set to satisfy \(T_{}=c(T^{},1)\) for some constant \(c 1\). If we use the constant perturbation strength \(>0\) and the decreasing learning rate sequence \(_{t}=(k(t-1))+2}\), then the strategy \(^{T+1}\) satisfies:_

\[[(^{T+1})]\] \[}{T^{}}((D^{2}+}{}(+1))}+1).\]

Note that the non-increasing property, as described in (8), of the potential function holds regardless of the presence of noise. This implies that a proof technique similar to the one used with the potential function in the full feedback setting can also be applied in the noisy feedback setting. The detailed proof can be found in Appendix C.

## 5 Individual regret bound

In this section, we present an upper bound on an individual regret for each player. Specifically, we examine two performance measures in our study: the _external regret_ and the _dynamic regret_(Zinkevich, 2003). The external regret is a conventional measure in online learning. In online learning in games, the external regret for player \(i\) is defined as the gap between the player's realized cumulative payoff and the cumulative payoff of the best fixed strategy in hindsight:

\[_{i}(T):=_{x_{i}}_{t=1}^{T}(v_{i}(x, _{-i}^{t})-v_{i}(^{t})).\]

The dynamics regret is a much stronger performance metric, which is given by:

\[_{i}(T):=_{t=1}^{T}(_{x_{i}}v_ {i}(x,_{-i}^{t})-v_{i}(^{t})).\]

We show in Theorem 5.1 that the individual regret is at most \((( T)^{2})\) if each player \(i[N]\) plays according to GABP in the full feedback setting:

**Theorem 5.1**.: _In the same setup of Theorem 4.1, we have for any player \(i[N]\) and \(T 3\):_

\[_{i}(T)_{i}(T)(( T) ^{2}).\]

This regret bound is significantly superior to the \(()\) regret bound of Optimistic Gradient Ascent, and it is slightly inferior to the \(( T)\) regret bound of AOG (Cai and Zheng, 2023). The proof is given in Appendix D.

## 6 Experiments

In this section, we present the empirical results of our GABP, comparing its performance with Adaptively Perturbed Gradient Ascent (APGA) (Abe et al., 2024) and Optimistic Gradient Ascent (OGA) (Daskalakis et al., 2018; Wei et al., 2021). We conduct experiments on two classes of concave-convex games. The first experiment is carried out on random payoff games, which are two-player zero-sum normal-form games with payoff matrices of size \(d\). In this game, each player's strategyspace is represented by the \(d\)-dimensional probability simplex, i.e., \(_{1}=_{2}=^{d}\). All entries of the payoff matrix are drawn independently from a uniform distribution over the interval \([-1,1]\). We set \(d=50\) and the initial strategies are set to \(_{1}^{1}=_{2}^{1}=\). The second instance is a _hard concave-convex game_, formulated as the following max-min optimization problem: \(_{x_{1}}_{y_{2}}f(x,y)\), where \(f(x,y)=-x^{}Hx+h^{}x+ Ax-b,y\). Following the setup in Cai and Zheng , we choose \(_{1}=_{2}=[-200,200]^{d}\) with \(d=100\). The precise terms of \(H^{d d},A^{d d},b^{d}\), and \(h^{d}\) are provided in Appendix E.2. All algorithms are executed with initial strategies \(_{1}^{1}=_{2}^{1}=\). The detailed hyperparameters of the algorithms, tuned for best performance, are shown in Table 1 in Appendix E.3.

The numerical results of the random payoff game and the hard concave-convex game are shown in Figure 1. Both the full feedback and noisy feedback experiments in the random payoff game were conducted with \(50\) different random seeds, which corresponds to using \(50\) different payoff matrices. For experiments on the hard concave-convex game with noisy feedback, we use \(10\) different random seeds. We assume that the noise vector \(_{i}^{t}\) is generated from the multivariate Gaussian distribution \((0,\ 0.1^{2})\) in an i.i.d. manner for both games. We observe that GABP exhibits competitive or faster performance over APGA and OGA in all experiments.

Figure 2 illustrates the dynamic regret in the hard concave-convex game. GABP exhibits lower regret than APGA and OGA in both settings, demonstrating its efficiency and robustness. Note that APGA and OGA exhibit almost identical trajectories with full feedback, with their plots overlapping completely.

## 7 Related literature

No-regret learning algorithms have been extensively studied with the intent of achieving key objectives such as average-iterate convergence or last-iterate convergence. Recently, learning algorithms introducing optimism , such as optimistic Follow the Regularized Leader  and optimistic Mirror Descent , have been introduced to admit last-iterate convergence in a broad spectrum of game

Figure 1: Performance of \(^{t}\) for GABP, APGA, and OGA with full and noisy feedback in the random payoff and hard concave-convex games, respectively. The shaded area represents the standard errors. Note that we report the gap function for the random payoff game, while the tangent residual is reported for the hard concave-convex game.

Figure 2: Dynamic regret for GABP, APGA, and OGA with full and noisy feedback.

settings. These optimistic algorithms with full feedback have been shown to achieve last-iterate convergence in various classes of games, including bilinear games (Daskalakis et al., 2018; Daskalakis and Panageas, 2019; Liang and Stokes, 2019; de Montbrun and Renault, 2022), cocoercive games (Lin et al., 2020), and saddle point problems (Daskalakis and Panageas, 2018; Mertikopoulos et al., 2019; Golowich et al., 2020; Wei et al., 2021; Lei et al., 2021; Yoon and Ryu, 2021; Lee and Kim, 2021; Cevher et al., 2023). Recent studies have provided finite convergence rates for monotone games (Golowich et al., 2020; Cai et al., 2022; Nguyen et al., 2022; Gorbunov et al., 2022; Cai and Zheng, 2023).

Compared to the full feedback setting, there are significant challenges in learning with noisy feedback. For example, a learning algorithm must estimate the gradient from feedback that is contaminated by noise. Despite the challenge, a vast literature has successfully achieved last-iterate convergence with noisy feedback in specific classes of games, including potential games (Cohen et al., 2017), strongly monotone games (Giannou et al., 2021; Li et al., 2021), and two-player zero-sum games (Abe et al., 2023). These results have often leveraged unique structures of their payoff functions, such as strict (or strong) monotonicity (Bravo et al., 2018; Kannan and Shanbhag, 2019; Hsieh et al., 2019; Anagnostides and Panageas, 2022) and strict variational stability (Mertikopoulos et al., 2019; Azizian et al., 2021; Mertikopoulos and Zhou, 2019; Mertikopoulos et al., 2022). Without these restrictions, convergence is mainly demonstrated in an asymptotic manner, with no quantification of the rate (Koshal et al., 2010, 2013; Yousefian et al., 2017; Tatarenko and Kamgarpour, 2019; Hsieh et al., 2020, 2022; Abe et al., 2023). Consequently, an exceedingly large number of iterations might be necessary to reach an equilibrium.

There have been several studies focusing on payoff-regularized learning, where each player's payoff or utility function is perturbed or regularized via strongly convex functions (Cen et al., 2021, 2023; Pattathil et al., 2023). Previous studies have successfully achieved convergence to stationary points, which are approximate equilibria. For instance, Sokota et al. (2023) have demonstrated that their perturbed mirror descent algorithm converges to a quantal response equilibrium (McKelvey and Palfrey, 1995, 1998). Similar results have been obtained with the Boltzmann Q-learning dynamics (Tuyls et al., 2006) and penalty-regularized dynamics (Coucheney et al., 2015) in continuous-time settings (Leslie and Collins, 2005; Abe et al., 2022; Hussain et al., 2023). To ensure convergence toward a Nash equilibrium of the underlying game, the magnitude of perturbation requires careful adjustment. Several learning algorithms have been proposed to gradually reduce the perturbation strength \(\) in response to this (Bernasconi et al., 2022; Liu et al., 2023; Cai et al., 2023). These include well-studied methods such as iterative Tikhonov regularization (Facchinei and Pang, 2003; Koshal et al., 2010; Tatarenko and Kamgarpour, 2019). Alternatively, Perolat et al. (2021) and Abe et al. (2023) have employed a payoff perturbation scheme, where the magnitude of perturbation is determined by the distance from an anchoring strategy, which is periodically re-initialized by the current strategy. Recently, Abe et al. (2024) have established \(}(1/)\) and \(}(1/T^{})\) last-iterate convergence rates for the payoff perturbation scheme in the full/noisy feedback setting, respectively. Our algorithm achieves faster \(}(1/T)\) and \(}(1/T^{})\) last-iterate convergence rates by modifying the periodically re-initializing anchoring strategy scheme so that the anchoring strategy evolves more gradually.

## 8 Conclusion

This study proposes a novel payoff-perturbed algorithm, Gradient Ascent with Boosting Payoff Perturbation, which achieves \(}(1/T)\) and \(}(1/T^{})\) last-iterate convergence rates in monotone games with full/noisy feedback, respectively. Extending our results in settings where each player only observes bandit feedback is an intriguing and challenging future direction.