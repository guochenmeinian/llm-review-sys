# Distributionally Robust Performance Prediction

Songkai Xue

Department of Statistics

University of Michigan

sxue@umich.edu &Yuekai Sun

Department of Statistics

University of Michigan

yuekai@umich.edu

###### Abstract

Performance prediction aims to model scenarios where predictive outcomes subsequently influence the very systems they target. The pursuit of a performative optimum (PO)--minimizing performative risk--is generally reliant on modeling of the _distribution map_, which characterizes how a deployed ML model alters the data distribution. Unfortunately, inevitable misspecification of the distribution map can lead to a poor approximation of the true PO. To address this issue, we introduce a novel framework of distributionally robust performative prediction and study a new solution concept termed as distributionally robust performative optimum (DRPO). We show provable guarantees for DRPO as a robust approximation to the true PO when the nominal distribution map is different from the actual one. Moreover, distributionally robust performative prediction can be reformulated as an augmented performative prediction problem, enabling efficient optimization. The experimental results demonstrate that DRPO offers potential advantages over traditional PO approach when the distribution map is misspecified at either micro- or macro-level.

## 1 Introduction

In numerous fields where predictive analytics play an important role, decisions made on the basis of machine learning models do not just passively predict outcomes but actively influence future input data. Consider the domain of financial services, such as credit scoring and loan issuance, where a model's decision to grant or deny an application can affect the applicant's future financial behaviors and, consequently, the profile of future applicants. Similarly, in educational settings, the decision process for school admissions can shape the applicant pool, as those who are accepted often share their success strategies, indirectly influencing the preparation of future candidates. These examples highlight the study of performative prediction , a recent framework that facilitates a formal examination of learning in the presence of performative distribution shift resulting from deployed ML models.

Delving deeper into the formulations of performative prediction, the concept of a distribution map emerges as pivotal. This map characterizes the impact that a deployed ML model has on the underlying data distribution, which is a crucial element in navigating performative effects. The literature primarily revolves around the pursuit of performative stability (PS), a model which is optimal for the distribution it induces . However, a more ambitious target is the performative optimum (PO), which seeks the minimization of performative risk, the risk of the deployed model on the distribution it induces. Efficiently achieving PO typically necessitates a modeling of the distribution map . Practically, the precise influence of a model on the data ecosystem is intricate and dynamic, making perfect specification an unattainable ideal.

In this work, we propose a distributionally robust performative prediction framework that aims to enhance robustness against a spectrum of distribution maps, thereby mitigating the issue of misspecification. Our contributions are summarized as follows: 1) in Section 2, we formalize theDRPO concept, anchoring it within the performative prediction literature as a robust alternative; 2) in Section 3, we provide theoretical insights into the efficacy of DRPO, demonstrating its resilience in the face of distribution map misspecification; 3) in Section 4, we recast distributionally robust performative risk minimization as an augmented performative risk minimization problem, facilitating efficient optimization; 4) in Section 5, we showcase DRPO's advantages over conventional PO by empirical experiments. The paper concludes with a summary and discussion.

### Related Work

**Performance prediction.** Performative prediction is an emerging framework for learning models that influence the data they intend to predict. The majority of research focuses on performative stability [29; 25; 5; 30; 21; 8; 7], albeit with a few exceptions aiming at performative optimality.  propose a two-stage plug-in method for finding the PO.  propose to find the PO by a parametric model of the distribution map. This method is extended by  to stateful performative prediction.  solves the PO when the problem is outcome performative only.  argue that the PO with a misspecified nominal distribution map can still reasonably approximate the true PO, as long as the misspecification level is not significant. This claim is also supported by our theory and experimental findings. Unlike them, we demonstrate that the DRPO is comparable to the PO if the misspecification is small, whereas the DRPO can offer substantial advantages over the PO if the misspecification is moderate to large. Moreover, the DRPO ensures reasonable performance for all distribution maps in an uncertainty collection surrounding the nominal distribution map, rather than just a single distribution map.

**Distributionally robust optimization.** DRO solves a stochastic optimization problem by optimizing under the worst-case scenario over an uncertainty set of probability distributions. Most popular DRO frameworks are based on \(\)-divergence [13; 2; 11; 12; 20; 9] and Wasserstein distance [11; 27; 6; 31; 4; 3]. The existing DRO literature pays no attention to performative prediction except for .  study the repeated distributionally robust optimization algorithm, which repeatedly minimizes the distributionally robust risk at the induced distribution. They show such repeated training algorithm yields a distributionally robust performative stable (DRPS) solution, assuming conditions analogous to the validity of repeated risk minimization [29; 25] in finding the performative stable (PS) solution. Nevertheless, the DRPO solution, which lies at the heart of the distributionally robust performative prediction problem, is not the subject of their study. Furthermore, they lacks theoretical guarantees regarding the proximity of the DRPS to either the PS or PO solution.

## 2 Methodology

### Performative Prediction Essentials

Let \(\) denote the (finite-dimensional) model parameter space, \(\) denote the data sample space, and \(()\) denote the set of probability measures supported on \(\). In performative prediction, we aim to find a \(\) that achieves low _performance risk_

\[_{}()=_{Z_{}()}[(;)], \]

where \(:\) is the (known) loss function, and \(_{}:()\) is the _true distribution map_. The _true performative optimum_ (true PO) \(_{}\) is known to minimize the (true) _performative risk_:

\[_{}*{arg\,min}_{}\{ _{}()=_{Z_{}()}[(;)]\}. \]

It is easy to see that if we _know_ the true distribution map, then we can evaluate the true performative risk and find the true performative optimum well up to some finite sample error which is negligible as the sample size goes to infinity. However, the true map \(_{}()\) is _unknown_ in general, thus posing a significant obstacle in the pursuit of evaluating and optimizing the true performative risk.

To enable the optimization of performative risk, it is necessary to have a known _nominal distribution map_\(()\) that is believed to closely approximate the unknown true distribution map \(_{}()\). Then one can find the _performative optimum_ (PO) by minimizing the _nominal performative risk_:

\[_{}*{arg\,min}_{}\{ ()=_{Z()}[(; )]\}. \]

Because the distribution map is inevitably _misspecified_, \(()_{}()\), the true performative risk is generally not minimized by \(_{}\). Therefore, we treat (2.3) as a solution concept to approximately solve the true performative risk minimization problem (2.2), and refer (2.3) to _standard performative prediction_. Now we provide several illustrative instances of potential sources that may lead to the misspecification of distribution map, _i.e._, \(()_{}()\).

**Modeling error.** The modeling of \(()\) can be either a _deterministic model_ of explicit form or a _statistical model_ with model parameters to be estimated. The misspecification of \(()\) may stem from modeling error.

**Example 2.1** (Strategic classification).: _Let \(=\) where \(\) is the feature space and \(\) is the label space, so that we are in the supervised learning regime. Strategic classification relies on a working model of individual's data manipulation strategy:_

\[_{}(x)=_{x^{}}\{u_{}(x^{})-c(x,x^{ })\}, \]

_where \(_{}()\), \(u_{}()\), and \(c(,)\) are known as individual's best response function, utility function, and cost function, respectively. The best response function can be lifted to the measurable space of \(\) so that we have the response map_

\[T_{}(x\\ y)=_{}(x)\\ y=_{x^{}}\{u_{}(x^{})-c( x,x^{})\}\\ y.\]

_The nominal distribution map \(()\) is fully characterized by \(T_{}()\) and the sampling distribution of \(_{}()\) (see details in Appendix E). In this example, the distribution map \(()\) could be misspecified because the individual's utility function and cost function could be misspecified._

**Example 2.2** (Location family).: _Location family postulates a translation model:_

\[Z() ZZ_{0}+AZ_{0}_{}(),\]

_where \(A^{()()}\) is unknown and therefore must be estimated. If we observe the sampling distributions of \((_{0}),(_{1}),(_{K})\), then \(A\) is partially identified up to a linear subspace of \(^{()()}\):_

\[A\{M M(_{k}-_{0})=_{k}-_{0}k[K]\},\]

_where \(_{k}\) is the mean of \((_{k})\). In this example, the distribution map \(()\) can be misspecified because the model parameter \(A\) is only partially identifiable._

**Distribution shift.** Consider the training and test environments have different distribution maps, \(_{}()\) and \(_{}()\), respectively. We specify the nominal distribution map as the training distribution map \(()=_{}()\). Then \(()\) can be misspecified for the true distribution map \(_{}()=_{}()\) due to the difference between the training and test environments \(_{}()_{}()\).

**Example 2.3** (Disparate impacts and fairness).: _A population is comprised of majority and minority subpopulations (e.g., by race or gender). The population distribution map is a mixture of the subpopulation distribution maps: \(_{}()_{}()+(1-)_{}()\). In fair machine learning, a theme is to check whether an ML model has disparate impacts on different subpopulations or biased against the minority. Suppose that we work with \(()=_{}()\) and target at the minority \(_{}()=_{}()\). However, the population distribution map and the minority distribution map may differ. In this example, the distribution map \(()\) is misspecified because of subpopulation shift._

### Distributionally Robust Performative Prediction

From an intuitive perspective, the PO solution (2.3) has the potential to achieve low performative risk \(_{}(_{})\) when the nominal distribution map \(()\) closely aligns with the true distribution map \(_{}()\). However, \(()\) and \(_{}()\) may be quite different. In such cases, the PO solution may incur high performative risk. To address this issue, we propose a distributionally robust formulation for performative prediction, where we explicitly incorporate into the learning phase the consideration that the true distribution map \(_{}()\) is different from the nominal one \(()\).

Let \(D(\|)\) be the KL divergence, _i.e._,

\[D(Q\|P)=()dP=( )dQ,\]

where \((t)=t t\) for any \(t>0\). Here \(dQ/dP\) is the Radon-Nikodym derivative, and we implicitly require the probability measure \(Q\) to be absolutely continuous with respect to \(P\). With the KL divergence, we can define a family of distribution maps around the nominal distribution map. Specifically, the _uncertainty collection_ around \(\) with _critical radius_\(\) is defined as

\[()=\{}:() D(}()\|()), \}.\]

The radius \(\) reflects the the magnitude of shifts in distribution map we seek to be robust to. We remark that the value of \(\) can be prescribed or be selected in data-driven ways. We postpone the discussion on critical radius calibration to Section 4.3.

**Definition 2.4** (Distributionally robust performative risk).: _The distributionally robust performative risk with the uncertainty collection \(()\) is defined as_

\[()=_{}:} ()}_{Z}()} [(Z;)]. \]

In other words, the distributionally robust performative risk \(()\) measures the worst possible performative risk incurred by the model parameterized by \(\) among the collection of all alternative distribution maps that are \(\)-close to the nominal distribution map \(\). With this intuition, it is natural to define an alternative solution concept which minimizes (2.5).

**Definition 2.5** (Distributionally robust performative optimum).: _The distributionally robust performative optimum (DRPO) is defined as_

\[_{}*{arg\,min}_{}(). \]

We refer the method (2.6) to _distributionally robust performative prediction_. When comparing (2.6) and (2.3), we view the DRPO (2.6) as a competing solution concept to the PO (2.3), because both of them aim for achieving low performance risk (2.1).

### Generalization Principle of DRPO

Distributionally robust performative prediction asks to not only perform well on a fixed performative prediction problem (parameterized by the distribution map \(\)), but simultaneously for a range of performative prediction problems, each determined by a distribution map in an uncertainty collection \(\). This results in more robust solutions, that is, those DRPOs which are robust to misspecification of distribution map. The uncertainty collection plays a key role: it implicitly defines the induced notion of robustness. Moreover, distributionally robust performative prediction yields a natural approach for certifying out-of-sample performance, which is summarized by the following principle.

**Proposition 2.6** (Generalization principle of distributionally robust performative prediction).: _Suppose that the uncertainty collection \(\) contains the true distribution map \(_{}\), then the true performative risk is bounded by the distribution robust performative risk: \(_{}()()\) for any \(\). In consequence, we have \(_{}(_{})(_{ })\)._

Essentially, if \(\) is chosen appropriately, the corresponding DR performance risk upper bounds the true performative risk, and thus DRPO enjoys provable guarantees on its incurred performative risk.

### Benefits of DRPO: A Toy Example

A toy example is employed to facilitate a conceptual understanding of the advantages of DRPO over PO in relation to achieving improved control over the worst-case performative risk.

Let \(=\), \(=[-1,1]\), and \((z;)= z\). Let the nominal distribution map be \(()=(f(),^{2})\) for some \(f:[-1,1]\) and \(^{2}>0\). Then the nominal performative risk is

\[()=_{Z(f(),^{2})}[(Z ;)]= f().\]

**Regularization effect.** With the dual formula given in Section 3.1, one can derive the distributionally robust performative risk directly:

\[()=()+( ),\]

where the penalty function \(()=}||\) penalizes the deviation of \(\) from the origin \(0\), and the critical radius \(\) tunes the level of regularization. That is to say, in this toy example, the distributionally robust performative risk minimization problem is essentially a \(L_{1}\)_-regularized_ performative risk minimization problem.

**Better worst-case control.** To be more concrete, we let \(f()=a_{1}+a_{0}\) for some \(a_{1},a_{0}>0\). For any \(}()\), let the performative risk of \(}\) be \(_{}}()=_{Z}()}[(Z;)]\). If \(}\) is the true distribution map, then \(_{}}()\) is the true performative risk that we incur. Through direct calculation (see details in Appendix A), one can show that \(_{}\) is more robust than \(_{}\) in the sense of _worst-case performative risk_ control, that is, \(_{}()}_{ }}(_{})_{} ()}_{}}(_ {})+}{2a_{1}}\) for any fixed \(^{2}}{2^{2}}\). This example shows that the worst-case performative risk of the PO can be _arbitrarily larger than_ that of the DRPO, as \(a_{1} 0\). The message that DRPO offers certain advantages over PO in terms of mitigating worst-case performative risk is also supported by the empirical results shown in Section 5.

## 3 Theory

### Strong Duality of DRPR

The evaluation of distributionally robust performative risk \(()\) given in (2.5) involves an infinite dimensional maximization problem which is generally intractable. Fortunately, it is in fact equivalent to a minimization problem over a single dual variable.

**Proposition 3.1** (Strong duality of DRPR).: _For any \(\), we have_

\[()=_{ 0}\{_{Z ()}[e^{(Z;)/}]+\}. \]

The dual reformulation (3.1) will be served as the cornerstone of developing algorithms for finding the DRPO in Section 4. As a byproduct of Proposition 3.1, a characterization of the worst-case distribution map which attains the supremum in (2.5) is given in Appendix B.

### Excess Risk Bound

For now, we are interested in bounding the excess risk: \(()=_{}()- _{}_{}()= {PR}_{}()-_{}(_ {})\), where \(\) is an approximate solution to the true PO \(_{}\), which could particularly be \(_{}\) and \(_{}\). The excess risk captures the true performance of \(\) relative to the oracle performative optimum \(_{}\), providing a direct measurement of the suboptimality of \(\) in terms of performative risk. As follows, we show the excess risk bounds of the PO solution and the DRPO solution, \((_{})\) and \((_{})\), and compare them.

**Proposition 3.2** (Excess risk bound of the PO).: _Suppose that we have bounded loss function \(|(z;)| B\) for any \(z,\) and some \(B>0\). Then we have_

\[(_{})B_{}_{}()\|())}. \]

**Proposition 3.3** (Excess risk bound of the DRPO).: _Suppose that \(D(_{}(_{})\|(_{ }))\). Then we have_

\[(_{})_{Z (_{})}[(Z;_{})]}+o (). \]

Comparing Proposition 3.2 and 3.3, we see that the excess risk bound of the DRPO can be localized to the true PO while the excess risk bound of the PO is entangled with the full parametric space \(\). Although we are comparing two upper bounds which can be not tight enough, the comparison sheds lights to the potential benefits of using DRPO over PO solution. Even if in the case of no significant improvement of using DRPO over PO solution, the excess risks of them are comparable, thus doing no harm. Our insight has been verified through a toy example in Section 2.4 and as well experimental results in Section 5. In Appendix C, we generalize Proposition 3.3 to the scenario when the uncertainty collection doesn't cover the true distribution map, _i.e._, \(D(_{}(_{})\|(_{ }))>\).

## 4 Algorithms

We recall a standard algorithm for performative risk minimization in Appendix E. In the following subsections, we will see that any off-the-shelf algorithms for finding the PO can be utilized as an _intermediate algorithm_ for finding the DRPO by using our proposed algorithms. Moreover, we provide practical considerations for the selection of a critical radius in the last subsection.

### Distributionally Robust Performance Risk Minimization

By the strong duality in Proposition 3.1, DR performative risk minimization is equivalent to the following optimization problem jointly over \((,)\):

\[_{}()_{}_{  0}\{(,)=_{Z()} [e^{(Z;)/}]+\}. \]

This suggests an alternating minimization, summarized in Algorithm 1, where we learn \(_{}\) by fixing \(\) and minimizing on \(\) and then fixing \(\) and minimizing on \(\) alternatively until convergence.

```
1:Input: radius \(\), nominal distribution map \(()\)
2:Initialize \(\)
3:while\(\) has not converged do
4: Update \(_{}\{_{Z( )}[e^{(Z;)/}]\}\)
5: Update \(_{ 0}\{(,)\}\) (\(\) in (4.1))
6:endwhile
7:Return:\(\)
```

**Algorithm 1** DR Performative Risk Minimization

The step of minimizing on \(\) with fixed \(\) (Line 4 in Algorithm 1) is itself a performative risk minimization problem, which can be solved by any suitable performative risk minimization algorithm. The step of minimizing on \(\) with fixed \(\) (Line 5 in Algorithm 1) can be solved by the line search or the Newton-Raphson method since \((,)\) is convex in \(\). The total cost of Algorithm 1 is therefore comparable to that of the performative risk minimization algorithm used in the intermediate step. Lastly, the alternating minimization algorithm in common practice guarantees global convergence (to stationary point) regardless of how the optimization parameters are initialized. With the strong convexity assumption, the alternating minimization algorithm guarantees convergence to the global minimum.

### Tilted Performative Risk Minimization

Treating \(\) as a hyperparameter which can be tuned by a practitioner, the solution of the dual problem (4.1) can be denoted by \((^{}(),^{}())\). One can show that \(^{}()\) is a decreasing function of \(\). An intuition is that as \(\), we have \(_{}\{(,)\}_{ }\{_{Z()}[(Z;)]\}\), which reduces to the original performative risk minimization problem, or the distributionally robust performative risk minimization problem with \(=0\) (see an formal explanation in Appendix F).

With this motivation, instead of tuning \(\), we can tune \(\) (or the inverse of it, denoted by \(=^{-1}\)) and solve the \(\)_-tilted performative risk minimization_ problem:

\[_{}*{arg\,min}_{} \{()=_{Z()}[e^{ (Z;)}]\}, \]

where \(()\) stands for the tilted performative risk and \(_{}\) is the _tilted performative optimum_, that is, the performative optimum of the tilted problem. In order to have stronger distributional robustness property, we tune \(\) to be larger. Given the correspondence \(^{}()\), we should have \(_{}\) with \(\) equals \(_{}\) with \(=(^{})^{-1}(1/)\). Therefore, the tilted performative risk minimization _implicitly_ solves a corresponding DR performative risk minimization problem. Finally, we remark that exponential tilting is a statistical method that has been around at least since the exponential family  was first invented. More recently, it has been applied to operation research  and machine learning [22; 23].

### Calibration of Critical Radius

The performance of the DRPO is contingent on the uncertainty collection radius \(\), which is typically difficult to choose a priori without additional information. The greater the value of \(\), the higher the level of distributional robustness, and thus the greater the tolerance for distribution map misspecification. Therefore, the selection of \(\) reflects a practitioner's risk-aversion preference. In this subsection, we present two simple, yet effective calibration techniques for selecting \(\).

**Post-fitting calibration.** Without additional information, we can only hope to be robust to a prescribed set of distribution maps, say \(\). The assumption of Proposition 3.3 requires only the uncertainty collection at the DRPO, which reduces to an uncertainty ball centered at \((_{})\), encompassing the true distribution \(_{}(_{})\). Therefore, in order to provide a provable guarantee for all distribution maps in \(\), the radius \(\) can be chosen based on the following criterion:\[_{ cal}=*{arg\,min}_{ 0}\{_{ _{ true}}(_{ true}\|) \},\] \[(_{ true}\|):=( _{ true}(_{ DRPO}())\|(_{ DRPO} ())).\]

Here \(\) is the estimated KL divergence, and \(_{ DRPO}()\) is indexed by the radius \(\) to highlight its dependence on \(\) as a tuning parameter to be calibrated. We implement the post-fitting calibration approach (with bisection search for \(\)) in Section 5.1.

**Calibration set.** With additional information, such as a small set of calibration data, we can pick \(\) (or \(\) if we use Algorithm 2) by evaluating the performance of \(_{ DRPO}()\) on the calibration set. Consider Example 2.3 where the training and test distribution maps may differ, we can conduct the following grid searching procedure to choose \(\): 1) for a candidate set \(\) of \(\)'s, we compute \(\{_{ DRPO}():\}\) under \(_{ train}\); 2) we obtain a few calibrating samples from \(_{ test}\), on which we evaluate the performance of \(_{ DRPO}()\); 3) we select \(_{ cal}\) with the best calibration set performance. The calibration set approach is implemented in Section 5.3.

To conclude this subsection, we discuss the computational costs of the proposed calibration methods. For general problems, these calibration methods necessitate a grid search, which may be computationally expensive. Fortunately, for specific problems (for example, experiments in Section 5.1), we can exploit the decreasing nature of the estimated KL divergence as a function in \(\). As a result, we can use bisection search rather than grid search to significantly reduce computational costs.

## 5 Experiments

We revisit Examples 2.1, 2.2, and 2.3 and compare the PO and the DRPO empirically. For a particular (true) distribution map, either the PO or the DRPO may have the potential to outperform the other in terms of the performative risk evaluated at this particular distribution map. In contrast to the PO, however, the DRPO aims to guarantee reasonable performance for _all_ distribution maps in an uncertainty collection around the nominal distribution map. To ensure the performance of the DRPO on a (set of) specific distribution map(s), the radius \(\) (or the tilt \(\)) must be calibrated. Lastly, each shaded region in figures below shows the curve's standard error of the mean from 30 trials.

### Strategic Classification with Misspecified Cost Function

In reference to Example 2.1, we examine strategic classification involving a cost function that is misspecified. The experimental setup resembles that in . The task is credit scoring, specifically predicting debt default. Individuals strategically manipulate their features to obtain a favorable classification.

Consider an instantiation of the response map (2.4) such that \(u_{}(x)=-^{}x\) and \(c(x,x^{})=\|x_{ strat}-x^{}_{ strat }\|_{2}^{2}+\|x_{ non-strat}-x^{}_{ non-strat}\|_{2} ^{2}\). Without loss of generality, let the first \(m\) features be strategic features and the last \(d-m\) features be non-strategic features. Let \(B=(1,,1,0,,0)^{d d}\) where the first \(m\) diagonal elements are \(1\)'s and the others are \(0\)'s. Then the best response function is \(_{}(x)=x- B\).

For the base distribution \(()\), we use a class-balanced subset of a Kaggle credit scoring dataset (, CC-BY 4.0). Features encompass an individual's historical data, including their monthly income and credit line count. Labels are binary where the value of \(1\) represents a default on a debt and \(0\) otherwise. There are a total of \(3\) strategic features and \(6\) non-strategic features. We use logistic model for the classifier and the cross-entropy loss with \(L_{2}\)-regularization for the loss function \(\).

Consider the cost function is misspecified by its performativity level \(\). We specify the cost function with the nominal performativity level \(=0.5\). However, the true performativity level \(_{ true}\) might not be \(0.5\), but in the range of \([0.5-0.5,0.5+0.5]\) for some \( 0\).

The left plot of Figure 1 shows performative risk incurred by the PO and the DRPO's with various radius \(\)'s. Note that the PO can be understood as the DRPO with \(=0\). As \(\) increases, the DRPO aims to achieve more uniform performance across a wider range of \(_{ true}\). The middle plot of Figure 1 shows relative improvement in worst-case performative risk1 of the DRPO to the PO as the radius \(\)increases, for different range of misspecification \(\)'s. Curves positioned above the horizontal dotted line indicate that the DRPO outperform the PO in terms of worst-case performative risk. When there is a larger range of misspecification \(\), the DRPO has greater potential to beat the PO. The right plot of Figure 1 demonstrates the post-fitting calibration of radius \(_{}\) described in Section 4.3. The vertical bands indicate the calibrated radius \(_{}\)'s, which lead to the DRPO's with sound relative worst-case improvement and avoid overconservative solutions, as depicted by the corresponding bands in the middle plot of Figure 1.

### Partially Identifiable Distribution Map

Recall Example 2.2, we examine a location model for distribution map where the misspecification arises from the estimation error of the model parameter. Let \(V=[_{1}-_{0}_{2}-_{0}_{K}- _{0}]^{d K}\) and \(U=[_{1}-_{0}_{2}-_{0}_{K}-_{0}]^{d K}\) where \(d=()\). The unknown parameter \(A\) can only be partially indentified through the equation \(AV=U\) when \(K<d\). A particular estimate of \(A\) is \(=UV^{}=U(V^{}V)^{-1}V^{}\), where \(V^{}\) is the Moore-Penrose inverse of \(V\). In fact, the parameter \(A\) is only identifiable up to the subspace \(=\{UV^{}+E\{E^{}\}(V ^{})\}\), where \((V^{})\) is the left null space of \(V\). Precicely, we have \(AV=U\) if and only if \(A\).

In this experiment, we still use the credit data. We observe sampling distribution of \(()\), \((_{1})\), and \((_{2})\), where \(_{i}\) is the \(i\)-th canonical basis. For the true distribution map, the performativity of the first two features is 0.5, while the performativity of the other \(7\) features is \(_{}\). In short, \(A_{}=(0.5,0.5,_{},, _{})\). We set the range of \(_{}\) to be \([-0.5,0.5]\) for \( 0\). By using the estiamte \(\), we model the performativity of the first two features correctly, but wrongly model the other features as non-strategic. This time we fit TPO by Algorithm 2 instead of DRPO.

The left plot of Figure 2 shows performative risk incurred by the PO and the TPO's with various tilt \(\)'s. As \(\) increases, the TPO performs more uniformly across a wider range of \(_{}\). The middle plot of Figure 2 shows relative improvement in worst-case performative risk of the TPO to the PO as the tilt \(\) increases, for different range of misspecification \(\)'s. Without misspeicification, \(=0\), the PO is for sure better than the TPO. With moderate to large misspecification, \(\{2/3,4/3,2\}\), the TPO demonstrates certain advantages over the PO. The right of Figure 2 shows the relationship

Figure 1: Results of Experiment 5.1. Left: performative risk incurred by the PO and the DRPO’s with various radius \(\)’s. Middle: relative improvement in worst-case performative risk of the DRPO to the PO as the radius \(\) increases, for different range of misspecification \(\)’s. Right: radius \(\) versus estimated KL divergence between \(_{}(_{}())\) and \((_{}())\), where vertical bands indicate the calibrated radius \(_{}\)’s.

Figure 2: Results of Experiment 5.2. Left: performative risk incurred by the PO and the TPO’s with various tilt \(\)’s. Middle: relative improvement in worst-case performative risk of the TPO to the PO as the tilt \(\) increases, for different range of misspecification \(\)’s. Right: the correspondence relationship between the radius \(\) and the (inverse of) optimal dual variable \(^{}\).

between the distributionally robust performative risk minimization and the tilted performative risk minimization: fitting DRPO with \(\) (which returns the optimal dual variable \(^{}\)) is equivalent to fitting TPO with \(=1/^{}\).

### Fairness without Demographics

Referencing to Example 2.3, we examine the scenario where the population distribution map is a mixture of two subpopulation distribution maps. We train a classifier using the population distribution map \(_{}\), but target at its performance on both the majority and minority, \(_{}\) and \(_{}\). The distribution map is therefore misspecified due to subpopulation shift.

The experimental setup resembles that in . Note that the credit dataset used in the previous experiments lacks demographic features. To enable oracle access to demographic information, synthetic data is generated for a performative classification task. The synthetic dataset exemplifies a scenario in which a linear decision boundary is not able to effectively classify both the majority and minority groups, necessitating a trade-off between them.

Figure 3 shows performative risk of the population, the majority, and the minority incurred by the TPO, as the tilt \(\) increases. The PO (which is TPO with \(=0\)) exhibits the lowest performative risk at the population, but the greatest disparity between its performance for the majority and minority groups. As \(\) increases, the TPO reduces the performance gap between the two groups at the expense of an increased population performative risk. This suggests that the distributionally robust performative prediction framework has the potential to mitigate unfairness towards the minority group, even in the absence of demographic information. Using a small calibration set with demographics, we can calibrate the tilt \(_{}\)'s via the calibration set approach described in Section 4.3. The goal is to calibrate the tilt to satisfy a four-fifth rule2 with minimal population performative risk. The vertical bands in Figure 3 shows the calibrated tilt \(_{}\)'s reasonably meet the goal.

## 6 Summary and Discussion

In this work, we present a distributionally robust performative prediction framework that aims to improve robustness against a variety of distribution maps, thereby mitigating the problem of distribution map misspecification. We show provable guarantees for DRPO as a robust approximation to the true PO when the nominal distribution map differs from the actual one. We developed efficient algorithms for minimizing the distributionally robust performative risk. Empirical experiments are conducted to support our theoretical findings and validate the proposed algorithms.

The components of our approach are not new, but we are combining them in a novel way to solve a relevant problem. To be precise, the proposed approach is novel in the context to use the idea of distributional robustness to solve the practical problem of distribution map misspecification in performative prediction. In addition, it is novel to study the solution concept of distributionally robust performative optimum (DRPO), both theoretically and algorithmically.

In Appendix H, we extend the KL divergence distributionally robust performative prediction framework to a general \(\)-divergence distributionally robust performative prediction framework. Furthermore, it is possible to go beyond general \(\)-divergence. An extension to a Wasserstein DRO version is a natural direction for future research, calling for the development of new algorithms.3

Figure 3: Results of Experiment 5.3. Performance risk of the population, the majority, and the minority, as the tilt \(\) increases. The vertical band indicates the calibrated tilt \(_{}\)’s.