# Efficient Bayesian Learning Curve Extrapolation

using Prior-Data Fitted Networks

 Steven Adriaensen

Machine Learning Lab

University of Freiburg

adriaens@cs.uni-freiburg.de &Herilalaina Rakotoarison

Machine Learning Lab

University of Freiburg

rakotoah@cs.uni-freiburg.de &Samuel Muller

Machine Learning Lab

University of Freiburg

muellesa@cs.uni-freiburg.de &Frank Hutter

Machine Learning Lab

University of Freiburg

fh@cs.uni-freiburg.de

 Equal Contribution.

###### Abstract

Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs. In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution over learning curves more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6\(\) speed-ups on 45 of these datasets, at virtually no overhead.

## 1 Introduction

Learning curve extrapolation (Mohr and van Rijn, 2022) aims to predict how much a machine learning model will improve with more training, e.g., to determine how much more training data to collect (Cortes et al., 1993; Frey and Fisher, 1999; Leite and Brazdil, 2004; Kolachina et al., 2012), or to define an early stopping criterion in online learning (Yao et al., 2007). Learning curve extrapolation has recently been widely studied to speed up automated machine learning (AutoML) and hyperparameter optimization (HPO) of deep neural networks, by discarding non-promising configurations early (Swersky et al., 2014; Domhan et al., 2015; Klein et al., 2017; Baker et al., 2017; Chandrashekaran and Lane, 2017; Gargiani et al., 2019; Wistuba et al., 2022).

Despite these efforts, learning curve extrapolation is not yet widely adopted in practice, e.g., state-of-the-art multi-fidelity hyperparameter optimization techniques, such as BOHB (Falkner et al., 2018), still rely on successive halving (Li et al., 2017), i.e., the crude heuristic that learning curves mostly do not cross each other.

One reason for this is that, while many learning curves are well-behaved, some exhibit chaotic behavior and are intrinsically difficult to predict accurately (Choi et al., 2018). In this setting, Bayesian approaches (Swersky et al., 2014; Domhan et al., 2015; Klein et al., 2017; Wistuba et al., 2022), which also quantify the reliability of their extrapolation, show great potential. However, existing methods for Bayesian inference either (i) put strong restrictions on the prior, and are incapable of modeling the variable nature of learning curves, or (ii) are too computationally expensive, limiting their practical applicability. Furthermore, most of this related work focuses on demonstrating the potential that learning curve extrapolation has to accelerate downstream AutoML/HPO tasks, yet fails to fully investigate the quality of the extrapolations themselves, and to quantify the approach's ability to handle the heterogeneity of real-world learning curves, e.g., varying performance metrics, curve shapes, divergence, heteroscedastic noise, etc.

In this work, we investigate the potential of learning curve extrapolation using prior-data fitted networks (PFNs), a meta-learned approximate Bayesian inference method recently proposed by Muller et al. (2022). PFNs combine great flexibility with efficient and accurate approximation of the posterior predictive distribution (PPD) in a single forward pass of a transformer (Vaswani et al., 2017) trained on artificial data from the prior only. As PFNs are a promising alternative to Markov Chain Monte Carlo (MCMC) for approximating Bayesian inference, we compare our approach (LC-PFN) to the MCMC approach for learning curve extrapolation of Domhan et al. (2015), taking into account both the quality and the cost of PPD approximation.

In summary, our contributions are as follow:

* We are the first to apply PFNs to an extrapolation task, introducing LC-PFN, the first PFN for learning curve extrapolation.
* We demonstrate that LC-PFN can be more than 10 000\(\) faster than MCMC while still yielding better probabilistic extrapolations.
* We show that LC-PFN does not only yield better probabilistic extrapolations on prior samples, but also on real learning curves of a wide range of architectures (MLPs, CNNs, RNNs, Transformers) on varying input modalities (tabular, image, text and protein data).
* 6\(\) speedups over baselines.
* To facilitate reproducibility and allow others to build on our work, we open-source all code, data, and models used in our experiments at https://github.com/automl/lcpfn.

## 2 Related work

Learning curves and how to use them for decision-making has been an active research area, as recently surveyed by Mohr and van Rijn (2022). Most related work considers point estimates of the curve or a specific property thereof (Cortes et al., 1993; Frey and Fisher, 1999; Kolachina et al., 2012; Baker et al., 2017; Kaplan et al., 2020), or follows a non-Bayesian approach to quantify uncertainty (Chandrashekaran and Lane, 2017; Gargiani et al., 2019).

Only a few works have explored Bayesian learning curve extrapolation. For example, the Freeze-Thaw Bayesian optimization method Swersky et al. (2014) used a Gaussian process (GP) as a joint model of learning curves and hyperparameters to decide what learning run to continue for a few epochs (or whether to start a new one). The model is then dynamically updated to fit the partial learning curve data. Training data grows quickly since each performance observation of the curve is treated as a datapoint, making exact GPs intractable, and thus the work relies on approximate GPs. Furthermore, their approach makes strong (prior) assumptions. On top of the standard GP assumptions, they used a specialized kernel assuming exponential growth to improve extrapolation. Domhan et al. (2015) proposed a less restrictive parametric prior (see Section 3.2 for more details) and used the gradient-free MCMC method from Foreman-Mackey et al. (2013) as approximate inference method. While MCMC is a very general approach, it can be sensitive to its hyperparameters (e.g.,burn-in period, chain length, etc.) and, as we will show in Section 4, generating sufficient samples to reliably approximate the PPD may impose significant overhead. Klein et al. (2017) extended this parametric prior to also capture the effect of hyperparameter settings. In particular, they used a Bayesian neural network with a specialized learning curve layer, and trained this network using gradient-based MCMC on learning curve data from previously tested hyperparameter settings. While this approach is able to predict learning curves of previously unseen configurations, conditioning on the current partial learning curve requires retraining the Bayesian neural network online, which is costly. Recently, DyHPO (Wistuba et al., 2022) followed a similar dynamic HPO setup as Swersky et al. (2014), but used _deep_ GPs (Damianou and Lawrence, 2013). While deep GPs relax some of the standard GP assumptions, extrapolation abilities were not thoroughly analyzed, and DyHPO only predicts one epoch into the future. Finally, it is worth noting that, except for Domhan et al. (2015), all the aforementioned probabilistic approaches (Swersky et al., 2014; Klein et al., 2017; Chandrashekaran and Lane, 2017; Gargiani et al., 2019; Wistuba et al., 2022) utilize meta-learning across the learner's hyperparameter settings. While this is an interesting line of work, it limits applicability, and introduces confounding factors. We will therefore consider a simpler and more general setting in this work (see Section 3.1). Indeed, akin to the approach of Domhan et al. (2015), we operate without the assumption of access to data from previous runs employing different hyperparameter settings, nor do we assume the ability to generalize across these settings. Our results highlight that prior-data fitted networks (PFNs) offer a significantly more efficient and practical alternative to Markov Chain Monte Carlo (MCMC) methods. As categorized by Mohr and van Rijn (2022), Domhan et al. (2015) is the only comparable prior work within this category. This underlines the novelty and importance of our approach in the context of advancing current methodologies.

While we are the first to apply PFNs (Muller et al., 2022) to learning curve extrapolation, PFNs have previously been applied in different settings: Hollmann et al. (2023) used them to meta-learn a classifier for tabular data; Muller et al. (2023) as a surrogate model for Bayesian optimization; and most recently concurrent work by Dooley et al. (2023) as a zero-shot time series forecaster.

## 3 Methods

### Bayesian learning curve extrapolation

Let \(y_{t}\) represent the model performance (e.g., validation accuracy) at training step \(t\{1,,m\}\). The problem we consider in this paper can be formulated as follows: Given a partial learning curve \(y_{1},,y_{T}\) up to some cutoff \(T\), and a prior distribution \(p()\) over learning curves, approximate the posterior predictive distribution (PPD) \(q(y_{t^{}}\,|\,y_{1},,y_{T})\) for \(T<t^{} m\). We will further assume that we can calculate the relative probability density of \(p()\), a requirement for MCMC, and that we can generate samples from \(p()\), a requirement for PFNs. Figure 1 provides an illustration of Bayesian learning curve extrapolation, showcasing the posterior predictive distributions (PPDs) of the extrapolated curves generated by LC-FFN and MCMC, along with a few representative curves sampled from the prior distribution \(p()\).

Figure 1: **(Left)** Visualization of Bayesian learning curve extrapolation. The plot shows the median and the 90% confidence interval of the PPDs inferred using MCMC and LC-PFN, given two partial empirical learning curves of 10 and 20 epochs, respectively, and the prior described in Section 3.2. **(Right)** Example of learning curves sampled from the prior \(p()\).

While the fixed ranges for \(y_{t}\) and \(t\) are well-suited for modeling particular learning curves (e.g., accuracy over epochs), they also are restrictive. In Appendix A, we discuss the invertible normalization procedure we apply to support extrapolating, possibly diverging, iteration-based learning curves across a broad range of performance metrics (e.g., log loss).

### Learning curve prior

Following Domhan et al. (2015), we model \(\) as a linear combination of \(K\) basis growth curves \(f_{k}\), each parameterized by \(_{k}\), and i.i.d. additive Gaussian noise with variance \(^{2}\), i.e.,

\[y_{t}(f_{}(t|),^{2}) f_{}(t|)=_{k=1}^{K}w_{k} f_{k}(t|_{k}),\]

where we assume our model parameters

\[=(w_{1},,w_{K},_{1},,_{K},^{2})\]

to be random variables with prior \(p()\). Here, Domhan et al. (2015) assumed an uninformative prior (i.e., \(p() 1\)), with the exception of some hard constraints. We adopt a strictly more informative prior, because (i) the original prior puts almost all probability mass on parameterizations yielding invalid learning curves, e.g., \(y_{t}\); and (ii) we cannot practically sample from this prior having unbounded support, a requirement for PFNs.1 Specifically, to mimic realistic learning curves we use bounded uniform weight priors \(w_{k}(0,1)\), a low-noise prior \((^{2})(-8,2)\), only allow curves with values in \(\) and, like Domhan et al. (2015), only accept curves whose last value is higher than its first. Putting all of these together, our prior distribution thus takes the form:

\[p()(_{k=1}^{K}p(w_{k}) p(_{k}))  p(^{2})(f_{}(1|)<f_{ }(m|))(_{t=1}^{m}(f_{}(t|))).\]

Finally, we limit ourselves to three parametric families of learning curves (\(K=3\), see Table 1).2 These basis curves were chosen to capture a variety of growth trends and convergence behavior. We show examples of curves sampled from this prior in Figure 1 (right).

### Prior-data fitted networks (PFNs)

In this paper, we propose to use prior-data fitted networks (PFNs, Muller et al., 2022) instead of MCMC for learning curve extrapolation. PFNs are neural networks trained to perform approximate Bayesian prediction for supervised learning settings. That is, PFNs are trained to predict some output \(y\), conditioned on an input \(t\) and a training set \(D_{train}\) of given input-output examples. The PFN is trained for this task with samples obtained from a prior over datasets \(p()\). The loss function for training a PFN \(q_{}\) with parameters \(\) is the cross entropy \(_{}=_{(t,y) D_{train} p()}[q_{}(y|t,D_{train})]\) for predicting the hold-out example's label \(y\), given \(t\) and \(D_{train}\). Muller et al. (2022) proved that minimizing this loss over many sampled tasks \((t,y) D_{train}\) directly coincides with minimizing the KL divergence between the PFN's predictions and the true PPD. In essence, the PFN meta-learns to perform approximate posterior inference on (meta-train) synthetic tasks sampled from the prior, and at inference time also does so for a (meta-test) real task.

   Reference name & Formula \(f_{k}(t)\) & Prior \(p(_{k})\) \\  pow\({}_{3}\) & \(c-at^{-}\) & \(c(0,1.25)\) & \(a(-0.6,0.6)\) & \(()(0,4)\) \\ Janoschek & \(-(-)e^{-at^{k}}\) & \((0,1)\) & \((0,2)\) & \(()(-2,1)\) & \(()(0,0.25)\) \\ ilog\({}_{2}\) & \(c-\) & \(c(0,1)\) & \(a(-0.5,0.5)\) & \\   

Table 1: Formulas of the three parametric basis curves and priors over their parameters.

### PFNs for Learning Curve Extrapolation (LC-PFNs)

To apply PFNs to learning curve extrapolation, we train them on learning curves sampled from a given prior over learning curves (in our case, the prior defined in Section 3.2). Specifically, the training set we condition on is the available partial learning curve up to some varying cutoff point \(T^{}\), i.e., \(D_{train}=\{(t^{},y_{t^{}})\}_{t^{}=1}^{T^{}}\), the input we condition on is the epoch \(t\{T^{}+1,,m\}\) to predict for, and the desired output \(y\) is the value of the learning curve at epoch \(t\). During training, we randomly sample the cutoff points \(T^{}(0,m-1)\) for every batch in order to learn to predict for initial learning curves of varying sizes. Figure 2 illustrates the information flow in a LC-PFN during learning curve extrapolation.

LC-PFN architecture and hyperparametersWe use the PFN architecture proposed by Muller et al. (2022) and visualized in Figure 2 (a). That is, we use a sequence Transformer (Vaswani et al., 2017) and treat each pair \((t,y)\) (for train) and \(t\) (for test) as a separate position/token. We encode these using a simple linear layer. We do not use positional encoding such that we are permutation invariant. Furthermore, the attention matrix is masked such that every position only attends to the training positions. This way training examples can attend to each other, but the test examples do not influence each other's predictions. Note that the output of the PFN with parameters \(\) is a distribution \(q_{}(y|t,D_{train})\). Following Muller et al. (2022), we discretize \(q_{}\) in a finite number of bins whose probability mass is predicted by the PFN, as is shown in Figure 2 (b). The size of each bin is set such that, under the prior, \(y_{t}\) is equally likely to fall in each bin. The number of bins is a hyperparameter that we set to 1 000. The LC-PFN model further inherits hyperparameters from the Transformer, including the number of layers (nlayers), number of heads (nheads), embedding size (emsize), and hidden size (nhidden). We use four heads, a hidden size of 1 024, and conduct a thorough ablation study to investigate the effects of the number of layers and embedding size on the final performance, exploring a grid of values (see Table 2). We use a standard training procedure for all experiments, employing the Adam optimizer (Kingma and Ba, 2015) (learning rate \(0.0001\), batch size \(100\)) with cosine annealing (Loshchilov and Hutter, 2017) with a linear warmup over the first 25% epochs of the training. Finally, we set \(m=100\), implying LC-PFN is trained for extrapolating sequences of up to \(100\) training steps (e.g., epochs). We found that most curves are shorter in practice, and when longer sequences are encountered, we subsample them as described in Appendix B.

## 4 Experiments

Our experiments aim to test the hypothesis that PFNs present a practical Bayesian approach to learning curve extrapolation. To this end, we first compare our LC-PFN approach against the MCMC approach of Domhan et al. (2015), using the same prior on samples generated from it (Section 4.1). Then, we extend the comparison to four real-world learning curve benchmarks (Section 4.2). Finally, we look beyond the quality of individual extrapolations and evaluate the potential of LC-PFN in the context of predictive early stopping to accelerate model selection (Section 4.3).

Figure 2: A visualization of our LC-PFN model on the task of predicting an accuracy over epochs curve. \(D\) represents the epoch accuracies up to epoch 3 (\(=T^{}\)). Attention between test and training positions is shown using red and blue arrows, respectively. Plots based on Müller et al. (2022).

### Extrapolating samples of the prior

The goal of this first experiment is to assess the ability of LC-PFNs and MCMC to approximate the true posterior predictive distribution (PPD). To avoid artifacts due to out-of-distribution data, in this experiment, we use curves sampled from the prior (defined in Section 3.2). Furthermore, we modified the original implementation of MCMC (Domban et al., 2015), to use the curve prior we proposed in Section 3.2. In the following, we refer to this MCMC variant as MCMC-PP and denote the one using the original prior (Domban et al., 2015) as MCMC-OP (used in Section 4.2). Since the LC-PFN and MCMC-PP methods are both (approximate) Bayesian inference methods, using the same prior, they aim to approximate the same true target PPD, given a partial learning curve.

As a performance metric, we evaluate the log-likelihood (LL) of the unseen data (right-censored curve) under the inferred PPD. We use this metric, also known as _logarithmic score_, to assess a model's ability to infer the remaining part of the curve based on the initial observed values. A benefit of this metric is that it measures the quality of the PPD as a whole (rather than merely focusing on the error associated to a specific PPD statistic) and, assuming data is generated by the prior, the exact PPD maximizes this metric.

Importantly, we vary the cutoff, i.e., the percentage of the observed curve used as input, to better assess the model's performance across different amounts of available information. Furthermore, to allow a more comprehensive comparison, we vary the hyperparameters of both LC-PFN and MCMC-PP. For LC-PFN, we vary the embedding size (emsize), the number of layers (nlayers), and the total number of learning curves used during training (nb_data). For MCMC-PP, we vary the number of chains generated by the emce (Foreman-Mackey et al., 2013) ensemble sampler (nwalkers), the length of each chain (burn-in + nsamples), the part of the chain omitted to account for mixing (burn-in), and the sub-sample frequency (thin). The considered values for each hyperparameter are summarized in Table 2.

We conducted the comparison on 10 000 sampled curves. Figure 1 shows a few curve examples, as well as inferences using LC-PFN and MCMC-PP given the data of the first 10 - 20 epochs (cutoff). We observe that both predicted median and uncertainties are indeed similar. More inference examples, with different cutoffs can be found in Appendix C.4.

ResultsFigure 3 displays the average log-likelihood across MCMC-PP/LC-PFN inferences for varying hyperparameters and a 10% cutoff. The log-likelihood is shown w.r.t. runtime, which is measured as the average wall-clock time for a single inference on a single Intel(R) Xeon(R) Gold 6242 CPU. Note that this inference time includes both the fit and prediction times for MCMC variants. Table 3 provides results on higher cutoffs (\(20\%,40\%,80\%\)) and corresponding runtimes for three variants of each method (M1-3, P1-3), labeled in Figure 3. Generally, the LC-PFN variants (left side of figure 3) are significantly faster than MCMC-PP variants (right side). LC-PFN always ran in less than \(0.1\) seconds while the best MCMC-PP (M3) took over 100 seconds. Figure 3 also offers insights into the importance of LC-PFN and MCMC-PP hyperparameters. For MCMC-PP, both the cost and quality of inference increase with longer chain lengths and higher cutoffs. For LC-PFN, the inference cost increases with the model complexity which is closely related to the number of trainable parameters. Inference quality positively correlates with model size ("larger is better"), and the number of data LC-PFN was trained on. Among the hyperparameter grid we examined (Table 2), except for the smallest model (P1), all LC-PFN variants that were trained on 10M samples produce higher log-likelihood than the best MCMC-PP variant (M3). In particular, an LC-PFN (P2) with 3 layers, embedding size 256, trained on 10M samples achieved better performance (log-likelihood of PPD) than the best MCMC-PP, but more than 15 000 times faster. We also find that while the runtime of the best MCMC-PP can be reduced (with minor loss of quality) by using thinning (M2), the better LC-PFN is still approximately 7 000 times faster. Finally, it is important to note that training the largest LC-PFN (P3, 10M samples with 26M parameters) on the prior took approximately eight hours (single CPU, single RTX2080 GPU), but this cost is incurred only _once_ for all of our experiments.

   & Hyperparameters \\  \(_{i}\) & \(\) \\ \(\) & \(\) \\ \(\) & \(\) \\ \(\) & \(\) \\  \(_{i}\) & \([100k,,10]\) \\ \(\) & \(\) \\ \(\) & \(\) \\  

Table 2: Grid of hyperparameter values evaluated for MCMC-PP and LC-PFN.

### Extrapolating real-world learning curves

While evaluation on data from the prior gives us a controlled setting to analyse quality and cost of the PPD approximation, performance on real-world learning curves is essential for practical usefulness. This second experiment aims to extend the previous comparison of MCMC and LC-PFN to real-world learning curve benchmarks.

We consider the best-performing variants of LC-PFN and MCMC-PP according to the average log-likelihood they obtained in the first experiment. For LC-PFN, the optimal variant (P3) features an embedding size of 512 and 12 layers, resulting in a total of 26M trainable parameters, and is trained on 10 million prior curves. For MCMC-PP, the optimal configuration (M3) involves a chain length of 4 500, 100 walkers, 500 burn-in samples, without thinning. As an additional baseline, we include MCMC-OP, the original MCMC variant proposed by Domhan et al. (2015), which uses the original hyperparameters and curve prior (11 basis curves and uninformative prior over the curve parameters).

BenchmarksTo evaluate the generalization capabilities of our model, we consider a diverse set of real-world curves. Our dataset comprises 20 000 learning curves, sourced from four distinct benchmarks: LCBench (Zimmer et al., 2021), NAS-Bench-201 (Dong and Yang, 2020), Taskset (Metz et al., 2020) and PD1 (Wang et al., 2022), each contributing 5 000 curves, randomly selected from specific subtasks. These benchmarks and subtasks were chosen to span a broad spectrum of supervised deep learning problems, training MLP (LCBench), CNN (NAS-Bench-201), RNN (Taskset), and Transformer (PD1) architectures on input modalities ranging from tabular data (LCBench), text (Taskset and PD1), protein sequence (PD1), to vision problems (NAS-Bench-201). From LCBench and NAS-Bench-201 we use validation accuracy curves whereas from Taskset and PD1 the log loss validation curves. In terms of curve length, LCBench and Taskset cover 50 epochs, NAS-Bench-201 contains up to 200 epochs, and PD1 curves have varying lengths (22 - 1414). Further details, including sample curves, on these benchmarks are provided in Appendix B.

Figure 3: Runtime (lower is better) _vs_ log-likelihood of the true curve under the PPD (higher is better), with 10% of the curve observed. See Figure 9 in Appendix C.1 for higher cutoffs. Blue and red markers correspond respectively to LC-PFN and MCMC-PP with varying hyperparameters values. The M1-3/P1-3 labels refer to the PFN / MCMC variants listed in Table 3. The horizontal dashed line indicates the performance of the best MCMC variant.

  
**Label** & **Method** & **Parameters** & **10\%** & **20\%** & **40\%** & **80\%** & **Avg. Runtime (s)** \\  M1 & MCMC & nsamples=2000, walkers=100, burn-in=500, thin=1 & 1.628 & 1.939 & 2.265 & 2.469 & 54.401 \\ M2 & MCMC & nsamples=4000, walkers=100, burn-in=100, thin=100 & 1.641 & 1.958 & 2.277 & 2.477 & 45.160 \\ M3 & MCMC & nsamples=4000, walkers=100, burn-in=500, thin=1 & 1.642 & 1.956 & 2.285 & 2.486 & 103.151 \\  P1 & PFN & nb\_data=10M, nlayers=3, emsize=128 & 1.58 & 1.99 & 2.28 & 2.43 & 0.004 \\ P2 & PFN & nb\_data=10M, nlayers=3, emsize=256 & 1.65 & 2.04 & 2.35 & 2.49 & 0.006 \\ P3 & PFN & nb\_data=10M, nlayers=12, emsize=512 & **1.76** & **2.13** & **2.40** & **2.52** & 0.050 \\   

Table 3: Comparison of three LC-PFN and MCMC-PP variants on prior curves in terms of log-likelihood (higher is better) at 10%, 20%, 40%, and 80% cutoffs. Here, M1 corresponds to the configuration used in Domhan et al. (2015). Please refer to Table 5 for more comprehensive results.

MetricsOur focus lies on the relative performance of MCMC and LC-PFN, as absolute performance is significantly influenced by the choice of prior. We consider the log-likelihood and the mean squared error (MSE) of the predictions as metrics. Here, the MSE is calculated w.r.t. the median of the PPD.

For each benchmark, we report the average rank of these metrics to aggregate results from different curves, as supposed to the average values. While the latter better captures performance differences, it is very sensitive to outliers and scale-dependent. When computed in normalized space, it would strongly depend on our choice of normalization parameters (see Appendix A).

ResultsFor each benchmark, Figures 3(a) and 3(b) display the average rank obtained by each method in terms of log-likelihood and MSE, respectively, where ranks are averaged across all 5 000 curves. We do not include error bars as the standard errors are neglectable (less than \(0.02\)). In summary, we observe similar trends for both metrics on all benchmarks. LC-PFN is never truly outperformed by MCMC-PP. On LCBench both methods rank similarly, with LC-PFN being slightly worse at high cutoffs. LC-PFN ranks better on PD1, NAS-Bench-201, and Taskset. MCMC-OP performs clearly worse on LCBench, Taskset, and PD1. On NAS-Bench-201, MCMC-OP performs best for lower cutoffs, outperforming MCMC-PP, suggesting that NAS-Bench-201 curves are better captured by the original prior. Figure 11 and Figure 12 in Appendix C.2 show the log-likelihoods and MSEs, respectively, for all three methods, for each curve and cutoff, per benchmark, providing a more detailed perspective.

### Application: Extrapolation-based early stopping in model selection

Thus far, we have shown that LC-PFN produces extrapolations of similar or better quality to MCMC, at a small fraction of the cost. However, these extrapolations are not perfect. In many cases, the practical relevance of any errors can only be assessed in the context of a specific application.

In this final experiment, we thus consider a model selection setting where after every epoch of training we have the choice between (i) continuing the current training, or (ii) stopping early (\(T<m\)) and starting a new training run using a different training pipeline (e.g., model architecture, hyperparameters, etc.). Here, we assume that runs cannot be resumed once stopped and that the order in which training pipelines are to be considered is given. Our objective is to obtain high-quality models as quickly as possible, by stopping suboptimal training runs as early as possible. This setting is also known as vertical model selection (Mohr and van Rijn, 2022) and was also considered by Domhan et al. (2015).

Figure 4: Comparison of LC-PFN with two MCMC variants on three real-data benchmarks.

We consider the extrapolation-based termination criterion proposed by Domhan et al. (2015), but use LC-PFN instead of MCMC to predict the likelihood \((y_{t^{}}>y_{}\,|\,y_{1},,y_{T})\) that the current run will at epoch \(t^{}\) obtain a model better than the best obtained by any run thus far (\(y_{}\)), for \(T<t^{} m\) and decide to stop the current run if that probability does not exceed some fixed threshold \(\) (at any point). In our experiments, we use confidence level \(1-=0.95\) as Domhan et al. (2015). Note that this criterion can be applied at every step or only at specific cutoffs. To simulate the effect of varying granularity, we consider a coarse-grained variant with 4 cutoffs \(T\{[0.1m],[0.2m],[0.4m],[0.8m]\}\), and a fine-grained variant with \(T\{T\,|\,1<T<m\}\). We investigate alternative choices for cutoffs and confidence levels in Appendix C.3.2.

Following Domhan et al. (2015), we compare against a black box approach no-stop that does not implement early stopping. We further compare against a criterion Patience(k) that implements the popular heuristic to terminate a training run when model performance did not improve for \(k\) epochs. For evaluation, we consider the same benchmarks as in Section 4.2 (for details, see Appendix B). We chose the total budget for model selection to correspond to 20 full runs (\(20m\)). For each task, we consider the training runs in 40 different random orderings. This totals 2 120 model selection experiments per method, spread across the 53 different tasks.

ResultsFigure 5 shows the anytime performance of all methods in our comparison, on each of the benchmarks, in terms of regret. Here, regret is the absolute difference in performance between the best model obtained thus far and the best model attained by any run on the task. Results are averaged over the different run orderings. For LCBench, NAS-Bench-201, and Taskset results are further averaged across all tasks (results for individual tasks can be found in Appendix C.3.1). The shaded area corresponds to \(\) 1 standard error. On all benchmarks, except for NAS-Bench-201, we observe that LC-PFN based termination criteria clearly perform best. Also, the fine-grained variant performs better on average, suggesting that errors in inference are compensated for by the time saved by stopping runs earlier. In terms of expected speed-up, this LC-PFN variant obtains an expected regret lower than that obtained by no-stop, approximately 3.3\(\) faster on LCBench and Taskset. Looking at individual tasks, we note 2 - 6\(\) speed-ups on all 3 PD1 tasks, all 12 Taskset tasks, and 30 of the 35 LCBench tasks (we obtain 1 - 2 \(\) speed-ups on the remaining 5). On NAS-Bench-201, we find that all termination criteria considered, including standard Patience heuristics, fail on all 3 tasks; this is likely related to the particular shape of learning curves on this benchmark, having an inflection point (see Figure 8 and Figure 19), not resembling any in the prior.

Figure 5: Comparison of our LC-PFN based early stopping mechanism to naive baselines no-stop (no stopping, train for the full budget \(m\)) and Patience(k) (stop training after k epochs without improvement) for vertical model selection where runs are considered in a fixed order and cannot be resumed. Shown is the anytime regret (lower is better) different approaches achieve after a total number of training epochs, averaged per benchmark and across 40 orderings per task.

Finally, in terms of overhead, the computational costs of the LC-PFN inferences per model selection experiment range from 3 seconds (coarse-grained) up to 1 minute (fine-grained), and are negligible compared to the cost of the 20 full training runs of deep neural networks.

## 5 Summary, limitations, and future research

We presented the first work using prior-data fitted networks (PFNs) for Bayesian learning curve extrapolation. We show that our LC-PFN obtains qualitatively similar extrapolations, for a wide variety of learning curves, more than \(10\,000\) faster than the MCMC method proposed by Domhan et al. (2015). These inferences are now _fast enough_ (under 100 milliseconds on CPU, and even less on GPU), to be used in the context of online learning, at virtually no overhead. This opens up a wide variety of possible applications, e.g., to speed up automated model selection in AutoML and HPO by discarding poor configurations early. It would be interesting to integrate LC-PFN as a new termination criterion in existing deep learning libraries.

Often, we also have more data available than a single partial learning curve, e.g., other curves on the same task, their hyperparameters, and/or curves of the same method on a different task, and meta-features. Previous work (Swersky et al., 2014; Klein et al., 2017; Wistuba et al., 2022; Ruhkopf et al., 2022) has already exploited this, and we could explore the potential of using PFNs for few-shot _in-context_ meta-learning, by feeding the model multiple curves and hyperparameters as input.

While for a fair comparison to Domhan et al. (2015), reusing the original code, our prior (Section 3.2) closely resembled the one of Domhan et al. (2015), future work could improve upon this prior and overcome its limitations (e.g., on the NAS-Bench-201 tasks) by modelling divergence, slow start, double-descent, correlated heteroscedastic noise, etc.

Finally, PFNs, unlike other Bayesian methods, must learn the prior from data, which implies that the prior must be generative. Also, it suggests that high entropy priors may present challenges. Future research should investigate these limitations and how to overcome them.

## 6 Acknowledgments and disclosure of funding

We acknowledge funding by the European Union (via ERC Consolidator Grant Deep Learning 2.0, grant no. 101045765), TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215, the state of Baden-Wurttemberg through bwHPC and the German Research Foundation (DFG) through grant numbers INST 39/963-1 FUGG and 417962828. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.