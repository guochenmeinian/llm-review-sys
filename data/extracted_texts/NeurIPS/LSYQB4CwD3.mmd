# Three Towers: Flexible Contrastive Learning

with Pretrained Image Models

 Jannik Kossen\({}^{1\,}\) Mark Collier\({}^{2\,}\) Basil Mustafa\({}^{3}\) Xiao Wang\({}^{3}\) Xiaohua Zhai\({}^{3}\)

Lucas Beyer\({}^{3}\) Andreas Steiner\({}^{3}\) Jesse Berent\({}^{2}\) Rodolphe Jenatton\({}^{3\,}\) Efi Kokiopoulou\({}^{2\,}\)

\({}^{1}\) OATML, Department of Computer Science, University of Oxford

\({}^{2}\) Google Research \({}^{3}\) Google DeepMind

###### Abstract

We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT  has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits from training the image tower contrastively. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperforms relative to LiT for JFT-pretrained models, it outperforms LiT for ImageNet-21k and Places365 pretraining.

+
Footnote †: Box}\) Equal advising.

+
Footnote †: Box}\) Equal advising.

+
Footnote †: Box}\) Equal advising.

## 1 Introduction

Approaches such as CLIP  and ALIGN  have popularized the contrastive learning of aligned image and text representations from large scale web-scraped datasets of image-caption pairs. Compared to image-only contrastive learning, e.g. , the bi-modal image-text objective allows these approaches to perform tasks that require language understanding, such as retrieval or zero-shot classification . Compared to traditional transfer learning from supervised image representations , contrastive approaches can forego expensive labelling and instead collect much larger datasets via inexpensive web-scraping . A growing body of work seeks to improve upon various aspects of contrastive vision-language modelling, cf. related work in SS5.

CLIP and ALIGN train the image and text towers from randomly initialized weights, i.e. 'from scratch'. However, strong pretrained models for either image or text inputs are often readily available, and one may benefit from their use in contrastive learning. Recently, Zhai et al.  have shown that pretrained classifiers can be used to improve downstream task performance. They propose LiT, short for 'locked-image text tuning', which is a variation of the standard CLIP/ALIGN setup that uses frozen embeddings from a pretrained classifier as the image tower. In other words, the text tower in LiT is contrastively trained from scratch to match locked and pretrained embeddings in the image tower. Incorporating knowledge from pretrained models into contrastive learning is an important research direction, and LiT provides a simple and effective recipe for doing so.

However, a concern with LiT is that it may be overly reliant on the pretrained model, completely missing out on any potential benefits the image tower might get from contrastive training. Zhai et al.

 themselves give one example where LiT performs worse than standard contrastive training: when using models pretrained on Places365 --a dataset relating images to the place they were taken--the fixed embeddings do not generalize to downstream tasks such as ImageNet-1k (IN-1k)  or CIFAR-100 . For their main results, Zhai et al.  therefore use models pretrained on datasets such as ImageNet-21k (IN-21k)  and JFT  which cover a variety of classes and inputs. However, even then, we believe that constraining the image tower to fixed classifier embeddings is not ideal: later, we will show examples where LiT performs worse than standard contrastive learning due to labels or input examples not covered by IN-21k, cf. SS4.2. Given the scale and variety of contrastive learning datasets, we believe it should be possible to improve the image tower by making use of both pretrained models _and_ contrastive training.

In this work, we propose Three Towers (3T): a flexible approach that improves the contrastive learning of vision-language models by effectively transferring knowledge from pretrained classifiers. Instead of locking the main image tower, we introduce a third tower that contains the embeddings of a frozen pretrained model. The main image and text towers are trained from scratch and aligned to the third tower with additional contrastive loss terms (cf. Fig. 1). Only the main two towers are used for downstream task applications such that no additional inference costs are incurred compared to LiT or a CLIP/ALIGN baseline. This simple approach allows us to explicitly trade off the main contrastive learning objective against the transfer of prior knowledge from the pretrained model. Compared to LiT, the image tower in 3T can benefit from both contrastive training _and_ the pretrained model. We highlight the following methodological and empirical contributions:

* We propose and formalize the 3T method for flexible and effective transfer of pretrained classifiers into contrastive vision-language models (SS3).
* 3T consistently improves over LiT and a from-scratch baseline for retrieval tasks (e.g. Table 1, SS4.1).
* For classification tasks, 3T outperforms LiT and the baseline with IN-21k pretrained models; for JFT pretraining, 3T outperforms the baseline but not LiT (SS4.2).
* We extend the evaluation of Zhai et al.  to additional tasks and pretraining datasets, showing that 3T is significantly more robust than LiT to deficits in the pretrained model (SS4.2 and SS4.4).
* We show that 3T benefits more from model size or training budget increases than LiT (SS4.3).
* We introduce a simple post-hoc method that allows us to further improve performance by combining 3T- and LiT-like prediction (SS4.5).

## 2 Background: Contrastive Learning of Vision-Language Models

Before introducing 3T, we recap contrastive learning of vision-language models as popularized by  and give a more formal introduction to LiT .

**CLIP/ALIGN.** We assume two parameterized models: an image tower \(f=f_{}\) with parameters \(\) and a text tower \(g=g_{}\) with parameters \(\). Each input sample \((I_{i},T_{i})\) consists of a pair of matching image \(I_{i}\) and text \(T_{i}\), and the contrastive loss is computed over a batch \(i\{1,,N\}\)

   Method & Basel. & LiT & 3T \\  Flickr img2txt & 85.0 & 83.9 & 87.3 \\ Flickr txt2img & 67.0 & 66.5 & 72.1 \\ COCO img2txt & 60.0 & 59.5 & 64.1 \\ COCO txt2img & 44.7 & 43.6 & 48.5 \\   

Table 1: For retrieval, 3T improves on LiT and the CLIP-style baseline (top-1 recall \(\)). Models are g scale, using Text-Filtered WebLI, and JFT pretraining for LiT/3T, cf. §4.1.

Figure 1: CLIP and ALIGN do not make use of pretrained models, and LiT directly uses a frozen pretrained model as the image tower. With Three Towers (3T), we propose a flexible strategy to improve contrastive learning with pretrained models: in addition to a pair of CLIP-style from-scratch image and text towers, we introduce a third tower which contains fixed pretrained image embeddings; extra loss terms align the main towers to the third tower. Unlike for CLIP/ALIGN and LiT, the image tower can benefit from both contrastive learning and pretrained classifier embeddings.

of examples. The towers map the input modalities to a common \(D\)-dimensional embedding space, \(f:^{D}\) and \(g:^{D}\). We further assume that \(f\) and \(g\) produce embeddings that are normalized with respect to their L2 norm, \(\|f(I)\|_{2}=\|g(T)\|_{2}=1\) for any \(I\) and \(T\). For a batch of input samples, the bi-directional contrastive loss [66; 77; 51; 9; 86] is computed as

\[_{f g} =(_{f g}+_{g  f}),\] (1) \[_{f g} =-_{i=1}^{N})^{}g(T_{i}) \ /)}{_{j=1}^{N}(f(I_{i})^{}g(T_{j})/)},\] (2) \[_{g f} =-_{i=1}^{N})^{}g(T_{i}) /)}{_{j=1}^{N}(f(I_{j})^{}g(T_{i})/)}.\] (3)

Here, \(\) is a learned temperature parameter and \(f(I)^{}g(T)\) are dot products. The two directional loss terms, \(_{f g}\) and \(_{g f}\), have a natural interpretation as standard cross-entropy objectives for classifying the correct matches in each batch. The parameters \(\) and \(\) of the two towers, \(f_{}\) and \(g_{}\), are jointly updated with standard stochastic optimization based on Eq. (1).

**Downstream Tasks.** After training, \(f\) and \(g\) are treated as fixed representation extractors. For retrieval, the dot product \(f(I)^{}g(T)\) ranks similarity between inputs. For few-shot image classification, a linear classifier is trained atop the feature representations of \(f\) from few examples; \(g\) is not used. For zero-shot image classification, \(f\) embeds images and \(g\) all possible class labels (see ). For each image, one predicts the label with the largest dot product similarity in embedding space.

**LiT.** Zhai et al.  initialize the parameters \(\) of the image tower from a pretrained classifier and then keep them frozen them during training. That is, only the parameters \(\) of the text tower are optimized during contrastive training. As the image tower \(f_{}\), LiT uses the pre-softmax embeddings of large scale classifiers, such as vision transformers  trained on JFT-3B  or IN-21k. During contrastive training, the text tower is trained from scratch using the same objective Eq. (1).

Experimentally, Zhai et al.  investigate all combinations for 'training from scratch', locking and finetuning a pretrained model for both towers on a custom union of the CLIP-subset of YFCC-100M  and CC12M  (cf. Fig. 3 in ). For the image tower, locking gives a significant lead on IN-1k over finetuning and training from scratch, and performs similarly to finetuning and better than training from scratch for retrieval. For the text tower, a locked configuration performs badly, and finetuning gives small to negligible gains over training from scratch. Given these results, Zhai et al.  choose the 'locked image tower and from-scratch text tower' setup that they call LiT. At large scale, they show that a locked image tower outperforms from-scratch training and finetuning on zero-shot IN-1k, ImageNet-v2 (IN-v2) , CIFAR-100, and Oxford-IIIT Pet  classification tasks. They further show LiT outperforms CLIP/ALIGN on IN-R , IN-A , and ObjectNet .

While Zhai et al.  show strong classification performance with LiT on a wide range of datasets, locking the image tower is a drastic measure that introduces a severe dependency on the pretrained model, prohibiting the image tower from improving during contrastive training. We will later show that, if the embeddings in the frozen image tower are not suited to a particular downstream tasks, LiT underperforms compared to approaches that train the image tower on the varied contrastive learning dataset, see, for example, SS4.2 and SS4.4. The 3T approach seeks to address these concerns.

## 3 Three Towers: Flexible Contrastive Learning with Pretrained Models

With Three Towers (3T), we propose a simple and flexible approach to incorporate knowledge from pretrained models into contrastive learning. Instead of directly using the pretrained model locked as the main image tower, we instead add a _third_ tower, \(h\), which contains the fixed pretrained embeddings. The main image and text towers are trained from scratch, and we transfer representations from the third tower to the main towers with additional contrastive losses. In this setup, the main image tower benefits from _both_ pretraining knowledge and contrastive learning.

More formally, in addition to the standard image and text towers, \(f_{}\) and \(g_{}\), cf. SS2, we now have access to fixed pretrained image embeddings \(p:^{P}\). Because \(P\) can be different from the target dimension \(D\), we define the third tower as \(h(I)=(p(I))\), where \(:^{P}^{D}\) projectsembeddings to the desired dimensionality. In principle, the 3T architecture is also compatible with pre-trained text models. However, like Zhai et al. , we do not observe benefits from using pretrained text models, cf. SSA.3, and so our exposition and evaluation focuses on pretrained image classifiers.

When computing loss terms involving the third tower, we make use of learned linear projection heads. These heads afford the model a degree of flexibility when aligning representations between towers. First, we define \(()=(())\), where \((x)=x/\|x\|_{2}\) normalizes with respect to L2 norm and, overloading notation, \(:^{D}^{D}\) now preserves dimensionality. We adapt the main image and text towers as \(f_{h}(I)=(f(I))\) and \(g_{h}(T)=(g(T))\). We project the third tower embeddings \(h\) to \(h_{f}(I)=(h(I))\) and \(h_{g}(I)=(h(I))\) for computation of the loss with the image and text towers respectively. The linear layers introduced for \(f_{h}\), \(g_{h}\), \(h_{f}\), and \(h_{g}\) are independently learned from scratch. Per input batch, the 3T approach then optimizes the following loss objective:

\[_{}=(_{f g}+ _{f_{h} h_{f}}+_{g_{h} h _{g}}).\] (4)

Here, \(_{f g}\) is the original contrastive loss, cf. Eq. (1), and \(_{f_{h} h_{f}}\) and \(_{g_{h} h_{g}}\) are additional contrastive losses between the image/text tower and the third tower projections. All loss terms share a global temperature \(\). We train both towers, \(f\) and \(g\), and all linear layers from scratch by optimizing Eq. (4) over input batches. Figure 2 (a) visualizes the adaptor heads and loss computation for 3T.

After training, the third tower is discarded and we use only the main two towers, cf. Fig. 2 (b). Therefore, the inference cost of 3T is equal to the baseline methods. For training, the additional cost over the from-scratch CLIP/ALIGN baseline is negligible, as frozen embeddings from the third tower can be pre-computed and then stored with the dataset, as also done in Zhai et al. .

**Intuitions for the 3T Architecture.** Intuitively, the additional losses align the representations of the main towers to the pretrained embeddings in the third tower. In fact, Tian et al.  show that contrastive losses can be seen as _distillation_ objectives that align representations between a teacher and a student model. They demonstrate that contrastive losses are highly effective for representation transfer, outperforming alternative methods of distillation. Thus, the additional terms, \(_{f_{h} h_{f}}\) and \(_{g_{h} h_{g}}\), transfer representations from the pretrained model to the unlocked main towers, albeit without the usual capacity bottleneck between the student and teacher models. Of course, for 3T, we also need to consider the original objective \(_{f g}\) between the unlocked text and image towers. In sum, the unlocked towers benefit both from the pretrained model and contrastive training.

**Design Choices.** In SS4.6, we ablate various design choices, such as our use of equal weights among the terms of Eq. (4), the contrastive loss for representation transfer, the shared global temperature \(\), the linear layers for \(h\) in the third tower, as well as our design of the adaptor heads.

**Discussion.** The 3T approach includes pretrained knowledge without suffering from the inflexibility of directly using the pretrained model as the main image tower. For example, unlike LiT, 3T allows for architectural differences between the unlocked image tower and pretrained model. Further, it seems plausible that 3T should generally be more robust than LiT: as the image tower learns from the highly-diverse contrastive learning datasets, the chances of encountering 'blindspots' in downstream applications, e.g. due to labels or examples not included in the pretraining dataset, should be lower. In a similar vein, LiT is most appropriate for pretrained models _so capable_ that they need not adapt during contrastive training. For example, few-shot classification performance, which uses only the image tower, by design cannot improve at all during contrastive training with LiT. On the other hand, with 3T

Figure 2: Details of the 3T approach. (a) Linear adaptor heads (gray) align the representations between the main towers and the third tower. (b) For downstream tasks, 3T is used in the same way as CLIP/ALIGN and LiT. We discard the third tower, using only the main towers, \(f(I)\) and \(g(T)\).

we may be able to successfully incorporate knowledge from weaker models, too. Lastly, finetuning--instead of locking--the main image tower from a pretrained classifier often has at most marginal positive effects over training from scratch after a high number of training steps, cf. SS2. In contrast, the additional terms in Eq. (4) consistently ensure the main towers align with the pretrained model.

## 4 Experiments

In this section, we compare 3T to LiT and to a standard CLIP/ALIGN baseline trained from scratch, which we will refer to as the 'baseline' for simplicity. Our experimental setup largely follows Zhai et al. : for all methods, we use Vision Transformers (ViT)  for the image and text towers, replacing visual patching with SentencePiece encoding  for text inputs, further sharing optimization and implementation details with . We rely on the recently proposed WebLI dataset , a large-scale dataset of 10B image-caption pairs (Unfiltered WebLI). We also explore two higher-quality subsets derived from WebLI: Text-Filtered WebLI, which uses text-based filters following Jia et al. , and Pair-Filtered WebLI (see SSA.7), which retains about half of the examples with the highest image-text pair similarity. For image tower pretraining, we consider both proprietary JFT-3B  and the publicly available IN-21k checkpoints of Dosovitskiy et al. . For IN-21k experiments, our largest model scale is L, with a 16\(\)16 patch size for ViT, and for JFT pretraining we go up to g scale at a patch size of 14\(\)14. Unless otherwise stated, we train for 5B examples seen at a batch size of \(14\,336\).

### Retrieval

We study zero-shot retrieval performance on COCO  and Flickr . Table 1 shows results for g scale models trained on Text-Filtered WebLI, with JFT pretraining for 3T and LiT. In Table 2, we report performance of L scale models trained on Unfiltered WebLI for JFT and IN-21k pretraining. We give results for additional WebLI splits for IN-21k and JFT pretraining in Tables A.4 and A.6.

_3T improves on LiT and the baseline for retrieval tasks_ across scales, datasets, and for both JFT and IN-21k pretraining. A rare exception to this are the Unfiltered WebLI results in Table A.6, where 3T beats LiT for retrieval on average and for txt2img, but not for img2txt. In general however, LiT underperforms for retrieval and regularly does not outperform the baseline: at L scale, LiT shows a strong dependence on the pretrained model and can only outperform the baseline with JFT pretraining. In contrast, 3T obtains similar improvements over the baseline for both pretraining datasets. We will continue to see this pattern in our experiments: 3T consistently improves over the baseline, while LiT results can vary wildly and depend strongly on the pretraining dataset. We discuss our retrieval results in the context of SOTA performance in SSA.7: the SOTA method CoCA  achieves better results but uses about 4 times more compute; increasing the compute budget for 3T would likely reduce the gap.

Intuitively, while the fixed classifier embeddings in LiT can categorize inputs into tens of thousands of labels, they may not be fine-grained enough for retrieval applications. On the other hand, the contrastive training of the baseline is closely related to the retrieval task but misses out on knowledge from pretrained models. Only 3T is able to combine benefits from both for improved performance.

### Few-Shot and Zero-Shot Image Classification

Next, we compare the approaches on few- and zero-shot image classification. See SSD for citations of all the datasets that we use. For zero-shot classification, we follow the procedure described in SS2. For few-shot tasks, we report 10-shot accuracy, more specifically, the accuracy of a linear classifier trained on top of fixed image representations, averaging over 3 seeds for the 10 random examples per class. As few-shot performance depends only on the image embeddings, LiT's few-shot accuracy is precisely the same as that of the pretrained model. Despite this, Zhai et al.  show that LiT outperforms the unlocked baseline on few-shot IN-1k and CIFAR-100 evaluations.

   Pretraining & – & IN-21k &  \\ Method & Basel. & LiT & 3T & LiT & 3T \\  Flickr\({}^{*}\) img2txt & 75.6 & 71.7 & 80.0 & 78.7 & 80.0 \\ Flickr\({}^{*}\) txt2img & 57.1 & 49.3 & 60.9 & 58.8 & 61.4 \\ COCO img2txt & 51.0 & 46.1 & 54.4 & 52.7 & 54.4 \\ COCO txt2img & 34.2 & 27.8 & 37.7 & 36.7 & 37.9 \\   

Table 2: 3T outperforms LiT and the baseline for retrieval (top-1 recall \(\), L scale models, Unfiltered WebLI, see §4.1).

For IN-21k pretraining, Table 3 reports the performance of L scale models trained on Unfiltered WebLI, and we give results on additional datasets in Table A.4. Here, 3T outperforms both LiT and the baseline. For JFT pretraining, Table A.5 gives results at L scale on Unfiltered WebLI and Table A.6 presents results at g scale across WebLI splits. In all JFT settings, LiT performs best on average for image classification tasks, despite few-shot performance being fixed for LiT. However, for both JFT and IN-21k pretraining, 3T improves over the baseline for almost all tasks. This is different for LiT, where performance heavily depends on the pretraining data.

**Risk of Locking.** There is a risk associated with LiT, both in a positive and negative sense. Using fixed classifier representations can result in excellent performance if the downstream task distribution and pretraining dataset are well-aligned: for example, IN-21k contains hundreds of labels of bird species, and IN-21k-LiT performs well on the Birds task, outperforming 3T by \(18\,\). However, the IN-21k label set does not contain a single car brand, and thus, IN-21k-LiT does not perform well on Stanford Cars, underperforming relative to the _baseline_ and 3T by almost \(40\,\). ObjectNet, IN-A, and IN-R were created to be challenging for ImageNet models, and so IN-21k-LiT performs worse than the baseline and 3T here, too. For example, IN-R contains artistic renditions of objects that are challenging for IN-21k-based models as they have mostly been trained on realistic images. IN-21k-LiT also struggles with more specialized tasks, performing \(29\,\) worse than the baseline on the remote sensing RESISC dataset.

The above results support our hypothesis that image embeddings trained on the highly diverse contrastive learning dataset will be more broadly applicable. Strikingly, even when using the same IN-21k model, 3T almost always improves over the baseline and never suffers the same failures as LiT. However, it seems that JFT-pretrained models can fix many of LiT's shortcomings for image classification. JFT-LiT performs remarkably well and almost never underperforms significantly compared to the baseline. A deviation from this pattern are the results for Eurosat and RESISC at g scale for JFT pretraining in Table A.6, where, e.g. on the Text-Filtered WebLI split, LiT lacks behind 3T by \(12\,\) and \(8\,\).

### Scaling Model Sizes and Training Duration

Since the publication of LiT, the scale of contrastive learning datasets, both public and proprietary, has increased, for example with the release of LAION-5B  or WebLI-10B ; we use the latter here. Given their cheap collection costs, it seems likely that this growth will continue to outpace that of more expensive classification datasets. However, locking the image tower ignores any potential benefits from the increased contrastive learning data for the image tower. Additionally, larger datasets often lead to increases in model scales to fully make use of the additional information ; unlike LiT, 3T can increase the scale of the main image tower independently of the pretrained model, cf. SS4.4.

Here, we separately study the effects that model scale and dataset size have on 3T, LiT, and the baseline. First, we vary the scale of all involved towers, including the pretrained model. We study both IN-21k and JFT pretraining, always train contrastively for 5B examples seen, and the S, B, L, and g scales correspond to S/32, B/32, L/16, and g/14 for the ViT models. We compute averages separately across retrieval, few-shot, and zero-shot classification tasks, where the tasks are those from Tables 2 and 3.

In Fig. 3, we observe that 3T's lead over LiT and the baseline in retrieval performance holds across scales and pretraining datasets. Further, across all tasks and scales, 3T maintains a consistent performance gain over the baseline. Also across tasks and pretraining datasets, we observe that

    &  &  &  \\   & IN-1k & 62.8 & 79.0 & 68.0 \\  & CIFAR-100 & 70.4 & 83.6 & 72.5 \\  & Caltech & 91.0 & 88.4 & 92.3 \\  & Pets & 85.9 & 89.2 & 86.5 \\  & DTD & 70.3 & 69.2 & 73.3 \\  & UC Merced & 91.8 & 92.8 & 94.0 \\  & Cars & 81.5 & 41.9 & 84.9 \\  & Col-Hist & 71.7 & 86.4 & 76.6 \\  & Birds & 53.4 & 83.4 & 65.0 \\   & IN-1k & 69.5 & 76.0 & 71.7 \\  & CIFAR-100 & 73.5 & 82.9 & 73.4 \\  & Caltech & 81.9 & 82.4 & 84.1 \\  & Pets & 84.2 & 87.1 & 87.0 \\  & DTD & 58.6 & 51.8 & 60.3 \\  & IN-C & 49.6 & 62.0 & 51.8 \\  & IN-A & 53.0 & 45.6 & 54.3 \\  & IN-R & 85.8 & 66.1 & 88.1 \\  & IN-v2 & 62.2 & 67.2 & 64.9 \\  & ObjectNet & 56.2 & 41.9 & 58.3 \\  & EuroSat & 32.7 & 27.6 & 42.8 \\  & Flowers & 62.0 & 72.6 & 65.7 \\  & RESISC & 58.0 & 29.0 & 57.9 \\  & Sun397 & 67.6 & 65.4 & 68.7 \\ 
**Average** & 68.4 & 68.3 & 71.4 \\   

Table 3: For IN-21k pretraining, 3T has the best average classification accuracy (\(\)) (L scale models, Unfiltered WebLI).

scaling is more beneficial for 3T than for LiT: as we increase the scale, 3T's performance increases more than that of LiT. This means that 3T either extends its lead over, overtakes outright, or reduces its gap to LiT as we increase scale. At L scale with IN-21k pretraining, 3T gives the best average performance of all methods across tasks. For JFT pretraining, LiT maintains an edge for classification performance, although the scaling behavior suggests this gap may fully collapse at larger scales. We observe similar trends when scaling the number of examples seen during training, cf. Fig. A.4.

### Pretraining Robustness

Next, we study what happens when 3T and LiT are used with pretrained models that do not conform to expectations. We consider two setups: one that we call'mismatched' and one that considers pretraining on the Places365  dataset. In Table 4, we display zero-shot accuracies on Pets, IN-1k, and CIFAR-100--tasks for which LiT usually performs best--as well as the average performance over the full set of tasks, see Table A.7 for individual results.

**Mismatched Setup.** So far, we have always matched the scale of the pretrained model to the scale of the models trained contrastively. For the mismatched setup, we now break this symmetry: we use an IN-21k-pretrained B/32 scale image model (3T and LiT) with an L scale text tower (all approaches) and an L/16 unlocked image tower (3T and baseline). This setup is relevant when pretrained models are not available at the desired scale: for example, given ever larger contrastive learning datasets one may want to train larger image models than are available from supervised training, cf. SS4.3. Of course, increasing the image tower scale also comes at increased compute costs for 3T and the baseline. We observe that LiT suffers from this mismatched setup much more than 3T, which is not restricted by the smaller pretrained model and now achieves higher accuracy than LiT on Pets and IN-1k.

**Places365.** Zhai et al.  demonstrate that LiT performs badly when used with models pretrained on Places365. In Table 4, we reproduce this result and observe LiT performing much worse than the baseline. (We here train B scale models for 900M examples seen, and based on our discussion in SS4.3, would expect LiT to perform even worse, in comparison to the baseline and 3T, when training longer or with larger scale models.) The embeddings obtained from Places365 pretraining do not allow LiT to perform well on our set of downstream tasks. 3T behaves much more robustly and does not suffer from any performance collapse because it incorporates both the pretrained model and contrastive data when training the image tower. Notably, 3T manages to improve average performance over the baseline even for Places365 pretraining. We further suspect the linear projection heads afford 3T some flexibility in aligning to the pretrained model without restricting the generality of the embeddings learned in the main two towers.

   Setup &  &  \\ Method & Basel. & LiT & 3T & Basel. & LiT & 3T \\  IN-1k & 69.5 & 69.5 & 71.5 & 45.6 & 24.5 & 47.4 \\ CIFAR-100 & 73.5 & 78.6 & 75.6 & 48.3 & 27.4 & 52.4 \\ Pets & 84.2 & 84.7 & 87.4 & 61.5 & 30.3 & 60.2 \\... &... &... &... &... &... \\ 
**Full Average** & 66.4 & 61.7 & 69.8 & 47.5 & 29.4 & 49.3 \\   

Table 4: 3T is more robust to the pretraining setup than LiT. Zero-shot classification accuracies (\(\)), full details in main text.

Figure 3: We increase the model scale for 3T, LiT, and the baseline and report average retrieval, few- and zero-shot classification performance for both IN-21k and JFT pretraining. 3T and the baseline benefit more from increases in scale than LiT (their curves are steeper), with 3T performing better than the baseline. The baseline does not use a pretrained model and is displayed twice (at scale B and L).

### Benefits From Using 3T With All Three Towers at Test Time

We usually discard the pretrained model when applying 3T to downstream tasks, cf. Fig. 2 (b). In this section, we instead explore if we can find benefits from using the locked third tower at test time, similar to LiT. More specifically, we study the convex combination of the main image tower and locked pretrained model in the third tower, \( h(I)+(1-) f(I)\), to see if we can interpolate between 3T- and LiT-like prediction at \(=0\) and \(=1\) respectively. We train 3T without linear projection heads to make embeddings from all towers compatible. Additional details of this setup can be found in SSA.10.

Fig. 4 shows we can generally interpolate between LiT- and 3T-like performance as we vary \(\) from \(0\) to \(1\). Note that we do not always recover LiT or 3T performance at \(\{0,1\}\) as explained in SSA.10. Interestingly, for retrieval and few-shot classification tasks, the convex combination yields better performance than either of the underlying methods for a relatively broad region around \( 0.5\). We believe that further study of the convex combination could be exciting future work: the method is entirely post-hoc and no additional training costs are incurred, although inference costs do increase.

### Ablations

Next, we provide ablations for some of the design decisions of 3T as well as insights into LiT training. We perform the ablation study at B scale with patch size \(32\), training for 900M examples seen, and use JFT-pretrained models. In Table 5, we report the average difference in performance to our default runs across all tasks, together with two standard errors computed over the downstream tasks as an indication of statistical significance. We refer to SSA.11 for full details and results.

**3T Ablations.** 'Rerun': To study per-run variance, we perform a rerun of the base 3T model, obtaining an average performance difference of \(-0.22\,\) across tasks. 'No \(_{}\)': When leaving out either of the three loss terms, average performance suffers significantly. 'Head Variants': We try a selection of different projection head variants, see SSA.11. None give significantly better performance than our default setup. 'MLP Embedding': Replacing the linear projection \(h\) in the third tower with an MLP does not improve performance. 'More Temperatures': Using three learned temperatures, one per 3T loss term, instead of a global temperature as in Eq. (4), does not improve results. 'Loss weights': Replacing the loss with a weighted objective, \((_{f g}+w( _{f_{h} h_{f}}+_{g_{h} h_{g}}))\), does not improve performance significantly across a variety of choices for \(w\). 'L2 Transfer': Using squared losses for the representation transfer objectives \(_{f_{h} h_{f}}\) and \(_{g_{h} h_{g}}\), cf. , results in significantly worse performance, even when optimizing the weight of the transfer terms. '3T Finetuning': Initializing the main tower in 3T with the same JFT-pretrained model as the third tower increases performance significantly; however, we find this effect becomes negligible for larger scale experiments, cf. SSA.11.

Figure 4: Convex combination of the image models in 3T: \( h(I)+(1-) f(I)\). By varying \(\), we can generally interpolate between 3T and LiT performance. Interestingly, for a broad range of weights, the retrieval and few-shot classification performance of the combination outperforms 3T and LiT.

    & Difference to 3T \\  Rerun & \(-0.22 0.25\) \\  No \(_{f g}\) Loss & \(-26.63 10.61\) \\ No \(_{f_{h} h_{f}}\) Loss & \(-1.19 0.75\) \\ No \(_{g_{h} h_{g}}\) Loss & \(-2.77 0.91\) \\ Head Variants & \(0.09 0.35\) \\ MLP Embedding & \(-0.08 0.35\) \\ More Temperatures & \(-0.26 0.48\) \\ Loss Weights & \(0.17 0.53\) \\ L2 Transfer & \(-3.80 1.13\) \\
3T Finetuning & \(1.85 1.27\) \\    \\  Rerun & \(-0.10 0.22\) \\  LiT Finetune & \(-14.99 6.09\) \\ FlexLiLiT1 & \(-4.63 1.36\) \\ FlexLiLiT2 & \(-5.04 1.54\) \\   

Table 5: Ablation study, see text for details.

**LiT Ablations.** 'Rerun': We observe similar between-run variance for LiT. 'LiT Finetune': We confirm the result of Zhai et al.  that finetuning from a pretrained model results in worse performance than locking. 'FlexiLiT 1/2': We investigate simple ways of modifying LiT such that it can adjust the image tower during contrastive learning, see SSA.11, but find these are not successful.

**Additional Experiments.** In SSA.1, we study the optimization behavior of 3T, finding evidence for beneficial knowledge transfer from the pretrained model. In SSA.2, we study the calibration of all methods for zero-shot classification, as well as their performance for out-of-distribution (OOD) detection: 3T is generally better calibrated than LiT, and for OOD tasks, we find trends similar to SS4.3, with all methods generally performing well. In SSA.3, we confirm the results of Zhai et al.  that there are no benefits from using pretrained _language_ models. In SSA.4, a detailed investigation of predictions suggests that 3T performs well because it combines knowledge from contrastive learning and the pretrained model. Lastly, SSA.5 shows 3T continues to perform well with other pretrained image models.

## 5 Related Work

CLIP  and ALIGN  are examples of vision-language models that have received significant attention, e.g. for their impressive ImageNet zero-shot results. Concurrently with LiT , BASIC  investigates locking and finetuning from JFT-pretrained models. Previously, [46; 57; 74; 67] have explored representation learning from images with natural language descriptions before the deep learning era. Subsequently, [21; 36; 35; 19; 63; 8] explore image-text understanding with CNNs or Transformers. In this context, Li et al.  introduced the idea of zero-shot transfer to novel classification tasks. The loss objective, Eq. (1), was proposed by Sohn  for image representation learning, and also appears in [77; 51; 9]. Zhang et al.  then used the objective to align images and captions, although their setting used medical data. Lots of work has built on CLIP and ALIGN. For example, [82; 79] have augmented the objective to optionally allow for labels, [89; 88] proposed methods for improving zero-shot prompts, [2; 43] applied CLIP to video, [54; 49; 62; 33] used CLIP embeddings to improve generative modelling, and  study different ways of incorporating image-only self-supervision objectives into CLIP-style contrastive learning. Relatedly, vast amounts of work have explored self-supervised or contrastive representation learning of images only, e.g. [16; 25; 24; 6]. Transfer learning  applies embeddings from large-scale (weakly) labelled datasets to downstream task [68; 44; 38].

## 6 Limitations, Impact, and Conclusions

**Limitations.** While 3T consistently improves over LiT for retrieval tasks, for classification, 3T outperforms LiT with ImageNet-21k-pretrained models only at large scales, and may require even larger scales for JFT pretraining. Further, while inference costs are equal for all methods, 3T incurs additional training costs compared to LiT. We have compared methods at matching inference cost for simplicity because there are many ways to account for the cost of pretraining and embedding computation.

**Impact.** We believe that locking is a suboptimal way to incorporate pretrained image models, and we have demonstrated clear benefits from exposing the image tower to both the contrastive learning dataset and the pretrained model, particularly as scale increases. 3T is a simple and effective method to incorporate pretrained models into contrastive learning and should be considered by future research and applications whenever strong pretrained models are available. For future work that seeks to improve 3T, we consider it important to understand the differences between embeddings from 3T, LiT, and the baseline. If we can obtain insights into why they excel at different tasks, we can perhaps (learn to) combine them for further performance improvements. Our convex combination experiments are a starting point; it would be interesting to continue this direction, possibly looking at combinations in parameter space [75; 76]. Lastly, future work could explore 3T for distillation of large pretrained models into smaller models, extend 3T to multiple pretrained models, potentially from diverse modalities, or explore the benefits of 3T-like ideas for other approaches such as CoCa .

**Conclusions.** We have introduced the Three Tower (3T) method, a straight-forward and effective approach for incorporating pretrained image models into the contrastive learning of vision-language models. Unlike the previously proposed LiT, which directly uses a frozen pretrained model, 3T allows the image tower to benefit from both contrastive training and embeddings from the pretrained model. Empirically, 3T outperforms both LiT and the CLIP/ALIGN baseline for retrieval tasks. In contrast to LiT, 3T consistently improves over the baseline across all tasks. Further, for ImageNet-21k-pretrained models, 3T also outperforms LiT for few- and zero-shot classification. We believe that the robustness and simplicity of 3T makes it attractive to practitioners and an exciting object of further research.