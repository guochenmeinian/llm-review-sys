# Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers

Alberto Alfarano

FAIR, Meta

albealfa@meta.com

&Francois Charton

FAIR, Meta - CERMICS, Ecole des Ponts

fcharton@meta.com

Equal contribution

&Amaury Hayat

CERMICS, Ecole des Ponts -- Institut Polytechnique de Paris

amaury.hayat@enpc.fr

Equal contribution

###### Abstract

Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics. We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems. We propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.

## 1 Introduction

As large language models achieve human-level performance over a broad set of tasks [4; 35; 45], their capability to _reason_ becomes a focus of discussion and research. There is no single definition of reasoning, and work in this area encompasses factuality, real world alignment, compositionality, the discovery and following of rules, &c. Still, mathematics are considered as one of the purest, and most demanding, forms of reasoning . As such, solving research-level mathematical problems is a major milestone in demonstrating the reasoning capabilities of language models. Such an advance in AI would also transform mathematical practice.

There is little research on applying language models to open problems of mathematics. Except a few papers on combinatorial optimization and graph theory [34; 39], most prior works focus on problems with known solutions [37; 23; 30; 8]. We believe this lack of results is due to two main reasons. First, research problems may require specialized work by mathematicians  before they can be handed to language models. Second, most math transformers are trained on sets of problems and solutions which are hard to generate in the case of open problems, when no generic method for finding a solution is known.

In this paper, we focus on a long-standing, yet easy to formalize, open problem in mathematics: discovering the Lyapunov functions that control the global stability of dynamical systems - the boundedness of their solutions when time goes to infinity with respect to an equilibrium or an orbit. A famous instance of this problem is the _three-body problem_: the long-term stability of a system of three celestial bodies subjected to gravitation. The stability problem was studied by Newton, Lagrange and Poincare. Lyapunov discovered that stability is guaranteed if an entropy-like function for the system-the Lyapunov function- can be found. Unfortunately, no method is known for deriving Lyapunov functions in the general case, and Lyapunov functions are only known for a small number of systems.

We propose a new technique for generating training data from randomly sampled Lyapunov functions. Sequence-to-sequence transformers trained on these datasets achieve near perfect accuracy (\(99\%\)) on held-out test sets, and very high performance (\(73\%\)) on out-of-distribution test sets. We show that higher accuracies (\(84\%\)) can be achieved by enriching the training set with a small number (\(300\)) of easier examples that can be solved with existing algorithmic methods. These enriched models greatly outperform state-of-the-art techniques and human performance on a variety of benchmarks.

Finally, we test the capability of our models to discover yet unknown Lyapunov functions on randomly generated systems. On polynomial systems, the only ones current methods can solve, our models find Lyapunov function for \(10.1\%\) or systems, vs \(2.1\%\) for state-of-the-art techniques. On non-polynomial systems, where no algorithm is known, our best models discover new Lyapunov functions for \(12.7\%\) of systems. Our research demonstrates that generative models can be used to solve research-level problems in mathematics, by providing mathematicians with guesses of possible solutions. The solutions proposed by the black-box model are explicit and their mathematical correctness can be verified. We believe this research is an AI-driven blueprint for solving open problems in mathematics.

#### Related works

Most classical methods for finding Lyapunov rely on parameterized families of candidate solutions, and attempt to derive conditions on the parameters [11; 14]. Additional techniques such as backstepping or forwarding [11; Chap. 12], were introduced to leverage the specifics of particular systems (see also physics-based methods ). These techniques are limited to specific, or simple, systems. The global Lyapunov functions of polynomial systems that are sums of squares of polynomials of given degree can be found by computational-intensive algorithmic tools, such as SOSTOOLS [32; 33], which leverage the fact that the Lyapunov function belongs to a finite-dimensional space.

Methods involving neural networks have been proposed in recent years [7; 15; 12; 24; 25]. They train feed-forward networks to approximate Lyapunov functions of a given system, and use a Satisfiability Modulo Theories (SMT) solver as a verifier which proposes potential counter-examples. This approach, very different from ours, was shown to be successful for several well-studied high dimensional systems. However, it only finds local or semi-global Lyapunov functions (see Definition A.3). Since the Lyapunov functions that are found are implicit, it would be hard for mathematicians to check whether they are global Lyapunov functions or not. Semi-global Lyapunov functions are useful in many engineering fields such as robotics, where one wants a system to be robust to small perturbations. In other fields, like epidemics, being resilient to large perturbations is central, and global Lyapunov functions are required.

Transformers trained on synthetic datasets have been proposed for many problems of mathematics, including arithmetic , linear algebra , symbolic integration , symbolic regression , Shortest Vector Problem , Grobner basis computation  and theorem proving .  investigate a problem related to ours: the local stability of dynamical systems. Different architectures were used to solve hard problems in combinatorial optimisation , and graph theory .

## 2 System stability and Lyapunov functions

The stability of dynamical systems is a hard mathematical question, which intrigued many generations of mathematicians, from Newton and Lagrange in the 18th century, to Poincare in the 20th in the context of the three-body problem. The main mathematical tool for assessing stability was proposed by Lyapunov, who showed in 1892 that a system is stable if a decreasing entropy-like function -the Lyapunov function- can be found [20; 11; 26]. Later, the existence of a Lyapunov function was shown to be a necessary condition for the stability of large classes of systems [29; 27; 18]. Unfortunately, these very strong results provide no clue on how to find Lyapunov functions, or just proving their existence for a particular system. In fact, 130 years later, systematic derivations of global Lyapunov functions are only known in a few special cases, and their derivation in the general case remains a well-known open problem.

In mathematical terms, we consider the dynamical system

\[=f(x),\] (1)where \(x^{n}\), \(f C^{1}(^{n})\) and \(=\). We want to know if the system has a stable equilibrium around a point \(x^{*}\) such that \(f(x^{*})=0\). We assume, without loss of generality, that \(x^{*}=0\).

**Definition 2.1**.: The system (1) is _stable_ when, for any \(>0\), there exists \(>0\) such that, if \(\|x(0)\|<\), the system (1) with initial condition \(x(0)\) has a unique solution \(x C^{1}([0,+))\) and

\[\|x(t)\|,\ \ \ t[0,+).\] (2)

In other words, a system is stable if a solution that begins close to the origin (\(\|x(0)\|<\)) stays close to the origin at all time (\(\|x(t)\|\)). Lyapunov proved that the stability is related to the existence of what is now called a Lyapunov function.

**Definition 2.2**.: The function \(V C^{1}(^{n},_{+})\) is said to be a _(global) Lyapunov function_ for the system (1) if the following condition are satisfied

\[ V(0)=0,&_{\|x\|+}V(x)=+ ,\\ V(x)>0,& V(x) f(x) 0x 0. \] (3)

**Theorem 2.3** (Lyapunov 1892).: _If the system (1) has a Lyapunov function, then it is stable._

In fact, the existence of a Lyapunov function is more powerful and provides additional information.

**Theorem 2.4** (LaSalle, 1961).: _If the system (1) has a Lyapunov function \(V\), then all the solutions of (1) converge to the largest invariant set of \(\{f(x) V(x)=0\}\)._

In many cases this largest invariant set is reduced to \(\{x^{*}=0\}\) and the system is said _globally asymptotically stable_ (all solutions converge to the equilibrium, see Appendix A).

Most dynamical systems are unstable. For instance, the solutions of the simple system \((t)=x(t)\) grow exponentially with time, and the solutions of \((t)=1+x(t)^{2}\) (\(x\)) always blow up before \(t=\). No Lyapunov functions can be found for these systems.

On the other hand, stable systems can have an infinite number of Lyapunov functions. The system

\[_{0}(t)=-x_{0}(t)\\ _{1}(t)=-x_{1}(t)\]

has \(V(x)=a_{0}x_{0}^{2}+a_{1}x_{1}^{2}\) as a Lyapunov function for any choice of \(a_{0}>0\) and \(a_{1}>0\).

In the general case, there is no systematic way of discovering a Lyapunov function, or even showing that one exist. Tools exist for small polynomial systems with special "sum of squares" (SOS) Lyapunov functions, but they need a lot of resources, do not always find a solution, and fail once the systems involve more than a few variables.

We also consider a related, but easier, problem: finding nontrivial \(V\) which are semi-definite positive, i.e. \(V\) verifying \(V(x) 0\) instead of \(V(x)>0\) in Equation (3). These functions, called _barrier

Figure 1: Dynamic of a stable system: trajectories may be complicated but as long as they start in the red ball they remain in the blue ball.

Figure 2: Two stable systems and associated Lyapunov functions discovered by our model. The second, a polynomial system with a non-polynomial Lyapunov function, was studied in .

[MISSING_PAGE_FAIL:4]

system is sampled, there is no general technique for finding a Lyapunov function, except in particular cases. In this paper, we rely on **Backward generation**, sampling solutions and generating associated problems, for the general case, and **forward generation**, sampling systems and calculating their solutions with a solver, for the tractable polynomial systems of small degree.

### Backward generation

Backward generation methods, sampling problems from their solutions, are only useful if the model can be prevented from learning to reverse the generation procedure, or from "reading" the solutions in the generated problems. For instance, when training a model to solve the hard problem of finding the roots of an integer polynomial , one can easily generate a polynomial from its roots, i.e. from the roots \(3,5\) and \(7\), generate the polynomial:

\[P(X)=2(X^{2}+1)(X-3)(X-5)(X-7).\]

However, if the model is trained from factorized form of \(P(X)\), it will learn to read the roots in the problem, instead of computing them. On the other hand, the developed and simplified form

\[P(X)=2X^{5}-30X^{4}+144X^{3}-240X^{2}+142X-210\]

offers no clues. A second difficulty of backward generation is that sampling solutions instead of problems biases the training distribution. A model trained on backward-generated data may not perform well on a forward-generated test set. Finally, prior work  observed that, for hard problems, backward generation methods sometimes focus on easier sub-problems (see, for instance, our comment below about choosing \(f=- V\) in step 2).

We propose a procedure for generating a stable system \(S\) from a random Lyapunov function \(V\). The rationale is the following. Since \(V\) must be positive with a strict minimum in \(0\), and tend to infinity at infinity ((3)), we first generate \(V=V_{}+V_{}\) where \(V_{}\) belongs to a class of functions with a guaranteed strict minimum in zero and \(V_{}\) to a larger class of non-negative functions, valued 0 at the origin, but with no guarantee of a strict minimum (step 1 and Appendix B). From \(V\), we need to generate \(f\) so that the third condition of (3) is met. A naive solution would be \(f=- V\) since \(f V 0\) would hold. But this would severely limit the systems we create, and turn the Lyapunov function discovery problem (find \(V\) from \(f\)) into an easier integration problem (find \(V\) from \(- V\)). Instead, starting from \(f_{0}=- V\), we apply the following transformations:

* multiply each coordinate of \(f_{0}\) by random non-negative functions \(h_{i}^{2}\) (step 4) and call it \(_{0}\).
* generate a random function \(=_{i=1}^{p}g_{i}(x)e^{i}(x)\) (steps 2 and 3), where \(e^{i}\) are orthogonal to \( V(x)\), and set \(f=+_{0}\). We have \( V=0\) and \((+_{0}) V 0\).

These transformations guarantee that all conditions in (3) are met. On the other hand, they allow \(f\) to span a very large set of systems, since any \(f\) satisfying \( V(x) f(x) 0\) can be written as the sum of a function collinear to \( V(x)\) and a function orthogonal to \( V(x)\).

Specifically, the procedure can be summarized as follows (see Appendix B for more details).

**Step 1** Generate a random function \(V\), satisfying \(V(x)>V(0),\;\;\; x^{n}\{0\}\), and \(V(x)+\) when \(\|x\|+\).

**Step 2** Compute the gradient \( V(x)\) and denote \(_{x}=\{z^{n} z V(x)=0\}\) the hyperplane2 orthogonal to \( V(x)\), for any \(x^{n}\).

**Step 3** Select \(1 p n\) at random and sample \(p\) vectors \(\{e^{i}(x)\}_{i\{1,,p\}}\) from hyperplane \(_{x}\). Generate \(p\) real-valued functions \((g_{i})_{i\{1,,p\}}\).

**Step 4** Select \(1<k_{1} n\) at random, generate \(k_{1}\) random real-valued functions \((h_{i})_{i\{1,,k_{1}\}}\), set \(h_{i}=0\) for \(k_{1}+1 i n\).

**Step 5** Build the \(n\) functions

\[f(x)=-(h_{(i)}^{2}(x)( V)_{i}(x))_{i\{1,,n\}}+_{i=1}^{p} g_{i}(x)e^{i}(x),\]

with \(\) a random permutation of \(\{1,...,n\}\).

**Step 6** Simplify the functions \(f_{i}\), obscuring patterns from the generative process.

This method produces a stable system \(S:=f(x)\), with \(V\) as its Lyapunov function. The difficulty of inferring \(V\) from \(S\) hinges on a careful choice of the vectors \(e^{i}\). For instance, if we naively select \(e^{i}\) as an orthonormal basis of \(_{x}\), computed from \( V(x)\) by Gram-Schmidt orthogonalization, prefactors like \(1/\| V(x)\|\) appear at step 3, and are unlikely to simplify away at step 6. This provides the model with a shortcut: reading \(\| V(x)\|\) in \(S\), and using it to recover \( V\) and then \(V\), not a trivial task, but an easier one than discovering Lyapunov functions. To counter this, we relax the orthonormality condition on \(e^{i}(x)\), so that \(1/\| V(x)\|\) never appears, yet keep the \(e^{i}(x)\) simple enough for \( V\)-specific patterns in \(_{i}g_{i}(x)e^{i}(x)\) to simplify away at step 6. We also want to ensure that the \(e^{i}\) span all of \(_{x}\), or the systems generated will not be diverse enough.

In our experiments, we slightly modify this procedure, by running steps \(2\) to \(6\) five times for each Lyapunov function \(V\) created at step \(1\). As a result, \(5\) systems are generated that share the same Lyapunov function (a discussion of this choice can be found in Appendix C.1). From a mathematical point of view, a Lyapunov function describes a hidden quantity in a system, and we believe that providing the model with several systems that share this hidden quantity should help it learn the parts of the system that contribute to this hidden quantity, and therefore learn a Lyapunov function.

This procedure can be tuned to generate specific classes of systems. By choosing \(V\), \(g_{i}\) and \(h_{i}\) in particular classes, we can constrain the system functions \(f_{i}\) to be polynomials, polynomials of functions (e.g. trigonometric polynomials), or more general functions (see Appendix B.4 for more).

The Lyapunov functions obtained here are correct by design. Nevertheless, we still performed an evaluation of the solutions both as a safeguard and to benchmark the failure and timeout rates of the SMT and SOS solvers on correct solutions, which we report in Table 1.

### Forward generation

Whereas the stability problem is unsolved in the general case, methods exist to calculate Lyapunov functions of polynomial systems, when they exist and can be written as a sum of squares of polynomials (see Section 1). These algorithms, of polynomial complexity, are very efficient for small systems, but their CPU and memory requirements explode as the size of the systems grows. We leverage them to generate forward datasets, as follows.

**Step 1** Generate a polynomial system at random

**Step 2** Use a routine to find a polynomial sum-of-squares (SOS) Lyapunov function.

**Step 3** Keep the system if such function exists, restart from step 1 otherwise.

This approach has several limitations. First, since most polynomial systems are not stable, and the computation of SOS Lyapunov function involves a complicated search , it is slow and limited to small systems of polynomials with small degree. Second, because not all stable polynomial systems have polynomial SOS Lyapunov functions , it can only generate a subset of stable polynomial systems.

Finally, SOS routines process the constraints in Equation (3) by solving semi-definite programming (SDP) problems. This guarantees that \(V\) is a sum-of-squares, hence we have \(V(x) 0\), but not necessarily \(V(x)>0\), for \(x 0\). As a result, these methods can only discover _barrier functions_. State-of-the-art methods circumvent this by introducing the stronger constraint \(V(x)_{i=1}^{n}_{i}x_{i}^{2}\), with \(_{i}\) small . \(V\) then has a unique minimum in \(x=0\), which makes it a Lyapunov function, but this further restricts the class of polynomial systems that the method can solve.

### Datasets

We generate \(2\) backward and \(2\) forward datasets for training and evaluation purpose, and one smaller forward dataset for evaluation purposes (see Table 8 in Appendix B.6 for a list).

    &  &  \\ Timeout & 10 minutes & 60 minutes & 10 minutes & 60 minutes \\  Correct Lyap function & 82.6 & 94.1 & 89.6 & 95.3 \\ Solver Timeouts & 17.4 & 5.9 & 10.4 & 4.7 \\ Incorrect Lyap function & 0 & 0 & 0 & 0 \\   

Table 1: **SMT and SOS timeout and error rates,** benchmarked on correct Lyapunov functions.

**Backward datasets** Our main backward set, **BPoly**, features \(1\) million non-degenerate polynomial systems \(S\) with integer coefficients, and \(2\) to \(5\) equations (in equal proportions). We also create **BNonPoly**, a dataset of \(1\) million non-degenerate non-polynomial systems with \(2\) to \(5\) equations. In this dataset, the coordinates of \(f\) are polynomials of general functions, e.g. trigonometric polynomials, or functions such as \(3(x_{1})+2x_{1}e^{x_{2}}\). For such general systems, no method for discovering a Lyapunov function is known.

**Forward datasets** All \(2\) forward datasets are generated using a solver derived from the SumOfSquares package in Python, and implementing techniques similar to those used in SOSTOOLS (see Appendix B.5). All systems in these datasets are non-zero integer polynomials with \(2\) to \(3\) equations, and integer polynomial Lyapunov functions - the only systems these methods can solve. We create **FLayap**, a dataset of 100,000 systems having a non-homogeneous polynomial as a Lyapunov function. We also have a dataset focusing on barrier functions (see the end of section 4.2): **FBarr** features 300,000 systems having a non-homogeneous polynomial as a barrier function. The small size of these datasets is due to the computational cost of SOS methods, and the difficulty of discovering Lyapunov or barrier functions.

To allow for comparison with SOSTOOL, the state-of-the-art method for discovering Lyapunov functions of polynomial systems, we also generated a test set of 1,500 polynomial systems with integer coefficients that SOSTOOLS can solve (**FSOSTOOLS**).

## 5 Results

Our models trained on different datasets achieve near perfect accuracy on held-out test sets, and very high performances on out-of-distribution test sets, especially when enriching the training set with a small number of forward examples. They greatly outperform state-of-the-art techniques and also allow to discover Lyapunov functions for new systems. These results are detailed below.

### In and out-of-distribution accuracy

In this section, we present the performance of models trained on the \(4\) datasets. All models achieve high in-domain accuracy - when tested on held-out test sets from the datasets they were trained on (Table 2). On the forward datasets, barrier functions are predicted with more than \(90\%\) accuracy, and Lyapunov functions with more than \(80\%\). On backward datasets, models trained on BPoly achieve close to \(100\%\) accuracy. We note that beam search, i.e. allowing several guesses at the solution, brings a significant increase in performance (\(7\) to \(10\%\) with beam size \(50\), for the low-performing models). We use beam size \(50\) in all further experiments.

The litmus test for models trained on generated data is their ability to generalize out-of-distribution (OOD). Table 3 presents evaluations of backward models on forward-generated sets (and the other way around). All backward models achieve high accuracy (\(73\) to \(75\%\)) when tested on forward-generated random polynomial systems with a sum-of-squares Lyapunov functions (FLyap). The best performances are achieved by non-polynomial systems (BNonPoly), the most diverse training set. The lower accuracy of backward models on forward-generated sets of systems with barrier functions (FBarr) may be due to the fact that many barrier functions are not necessarily Lyapunov functions. On those test sets, backward models must cope with a different distribution and a (slightly) different task. Forward models, on the other hand, achieve low performance on backward test sets. This is possibly due to the small size of these training set.

Overall, these results seem to confirm that backward-trained models are not learning to invert their generative procedure. If it were the case, their performance on the forward test sets would be close to zero. They also display good OOD accuracy.

    &  &  \\ Backward datasets & bs=1 & bs=50 & Forward datasets & bs=1 & bs=50 \\  BPoly (polynomial) & 99 & 100 & FBarr (barrier) & 93 & 98 \\ BNonPoly (non-poly) & 77 & 87 & FLyap (Lyapunov) & 81 & 88 \\   

Table 2: **In-domain accuracy of models**. Beam size (bs) 1 and 50.

### Enriching training distributions for improved performance

To improve the OOD performance of backward models, we add to their training set a tiny number of forward-generated examples, as in . Interestingly, this brings a significant increase in performance (Table 4). Adding \(300\) examples from FBarr to BPoly brings accuracy on FBarr from \(35\) to \(89\%\) (even though the proportion of forward examples in the training set is only \(0.03\%\)) and increases OOD accuracy on FLyap by more than \(10\) points. Adding examples from FLyap brings less improvement.

These results indicate that the OOD performance of models trained on backward-generated data can be greatly improved by adding to the training set a small number of examples (tens or hundreds) that we know how to solve. Here, the additional examples solve a weaker but related problem: discovering barrier functions. The small number of examples needed to boost performance makes this technique especially cost-effective.

### Comparing with state-of-the-art baselines

To provide a baseline for our models, we developed findlyap, a Python counterpart to the MATLAB Lyapunov function finder from SOSTOOLS (see Appendix B.5). We also introduce FSOSTOOLS, a test set of 1,500 polynomial systems with integer coefficients that SOSTOOLS can solve. We also tested AI-based tools (see Appendix E for the full list of parameters sweeps we used for each of these methods), such as Fossil 2 , ANLC v2  and LyzNet . These methods achieve low accuracies on our test sets. This might be due to the fact that these tools are designed to solve a different problem: discovering local or semi-global Lyapunov function (and potentially finding a control function), while we target global Lyapunov functions.

Table 5 compares findlyap and AI-based tools to our models on all available test sets. A model trained on BPoly complemented with \(500\) systems from FBarr (PolyMixture) achieves \(84\%\) on FSOSTOOLS, confirming the high OOD accuracy of mixture models. On all generated test sets, PolyMixture achieves accuracies over \(84\%\) whereas findlyap achieves \(15\%\) on the backward generated test

   Forward &  &  & \\ datasets & (1M in training set) & FLyap & FBarr \\  No mixture & 0 & 73 & 35 \\  FBarr & 30 & 75 & 61 \\  & 300 & 83 & 89 \\  & 3,000 & 85 & 93 \\  & 30,000 & 89 & 95 \\  FLyap & 10 & 75 & 25 \\  & 100 & 82 & 29 \\  & 1,000 & 83 & 37 \\  & 10,000 & 86 & 38 \\   

Table 4: **Mixing backward data (BPoly) with a small number of forward examples**. Beam size 50.

    & SOSTOOL &  &  \\ Test sets & findlyap & Fossil 2 & ANLC & LyzNet & PolyMixture & FBarr & FLyap & BPoly \\  FSOSTOOLS & - & 32 & 30 & 46 & **84** & 80 & 53 & 54 \\ FBarr & - & 12 & 18 & 28 & **89** & - & 28 & 35 \\ FLyap & - & 42 & 32 & 66 & 83 & **93** & - & 73 \\ BPoly & 15 & 10 & 6 & 24 & **99** & 15 & 10 & - \\   

Table 5: **Performance comparison on different test sets**. Beam size 50. PolyMixture is BPoly + 300 FBarr.

set. This demonstrates that, on polynomial systems, transformers trained from backward-generated data achieve very strong results compared to the previous state of the art.

On average Transformer-based models are also much faster than SOS methods. When trying to solve a random polynomial system with 2 to 5 equations (as used in Section 5.4), findlyap takes an average of 935.2s (with a timeout of 2400s). For our models, inference and verification of one system takes 2.6s on average with greedy decoding, and 13.9s with beam size \(50\).

### Into the wild - discovering new mathematics

Our ultimate goal is to discover new Lyapunov functions. To test our models' ability to do so, we generate three datasets of random systems: polynomials systems with \(2\) or \(3\) equations (**Poly3**), polynomial systems with \(2\) to \(5\) equations (**Poly5**), and non-polynomial systems with \(2\) or \(3\) equations (**NonPoly**). For each dataset, we generate 100,000 random systems and eliminate those that are trivially locally exponentially unstable in \(x^{*}=0\), because the Jacobian of the system has an eigenvalue with strictly positive real part . We compare findlyap and AI based methods with two models trained on polynomial systems, FBarr, and PolyM(ixture) - a mixture of BPoly and 300 examples from FBarr- and one model trained on a mixture of BPoly, BNonPoly and 300 examples from FBarr (NonPolyM).

Table 6 presents the percentage of correct solutions found by our models. On the polynomial datasets, our best model (PolyM) discover Lyapunov functions for \(11.8\) and \(10.1\%\) of the (degree 3 and degree 5) systems, ten times more than findlyap. For non-polynomial systems, Lyapunov functions are found for \(12.7\%\) of examples. These results demonstrate that language model trained from generated datasets of systems and Lyapunov function can indeed discover yet unknown Lyapunov functions and perform at a much higher level that state-of-the-art SOS solvers.

### Expert iteration

Given the performance on our model in Table 6, we can use the newly solved problems to further fine-tune the model. Specifically, we create a sample of verified model predictions for polynomial systems, **FIntoTheWild**, we add it to the original training sample and we continue training the model.

We test different strategy to finetune the model and we report performance on forward benchmarks and "into the wild" in Table 7.

* Add 20,600 samples from BPoly (20,000), FBarr (50), FLyap (50) and FIntoTheWild (500) to keep similar proportion used during pretraining
* Add 2,000 samples from FLyap (1,000) and FIntoTheWild (1,000) to improve on both forward benchmark and in the wild
* Add 50 samples from FIntoTheWild to show that this indeed helps
* Add 1,000 samples from FIntoTheWild
* Add 2,000 samples from FIntoTheWild
* Add 5,000 samples from FIntoTheWild to see if there are benefits to add more samples

We also retrain a model (_n7_) from scratch using a mixture of BPoly (1M), FBarr (500), FLyap (500) and FIntoTheWild (2,000).

We notice that the addition of 1,000 verified predictions to our training set of 1 million improves performance on the "into to wild" test sets by about 15%, while not affecting the other test sets (_n4_). Adding more examples seems to be detrimental, as it decreases the performance on other benchmarks (_n5_ and _n6_). We also notice that finetuning with mixed data from other distributions is not efficient (_n1_ and _n2_) and a small contribution already help to get some improvements (result _n3_). Finally, it's not efficient to pretrain the model from scratch using data from FIntoTheWild (_n7_).

    & Sample & SOSTOOL &  & Forward &  \\ Test set & size & findlyap & Fossil 2 & ANLC & LyzNet & FBarr & PolyM & NonPolyM \\  Poly3 & 65,215 & 1.1 & 0.9 & 0.6 & 4.3 & 11.7 & **11.8** & 11.2 \\ Poly5 & 60,412 & 0.7 & 0.3 & 0.2 & 2.1 & 8.0 & **10.1** & 9.9 \\ NonPoly & 19,746 & - & 1.0 & 0.6 & 3.5 & - & - & **12.7** \\   

Table 6: **Discovering Lyapunov comparison for random systems**. Beam size 50. PolyM is BPoly + 300 FBarr. NonPolyM is BNonPoly + BPoly + 300 FBarr.

## 6 Discussion

We have shown that models can be trained from generated datasets to solve a long-standing open problem in mathematics: discovering the Lyapunov functions of stable dynamical systems. For random polynomial systems, our best models can discover Lyapunov functions in five times more cases than state-of-the-art methods. They can also discover Lyapunov functions of non-polynomial systems, for which no algorithm is yet known, and were able to re-discover a non-polynomial Lyapunov function of a polynomial systems discovered by  (Appendix F).

The backward generation method introduced in section 4.1 is the key innovation in this paper. The main problem with such approaches is their tendency to generate training sets with very specific distributions, which prevent models from generalizing to general instances of the problem. Our models can generalize out of their training distributions (Table 3), and we can improve their performance by adding to their training set a tiny number of systems that we know how to solve (Table 5).

While our models exceed the algorithmic state of the art, one might wonder **how they compare to human mathematicians**. To this effect, we proposed \(75\) problems from the FSOSTOOLS dataset (polynomial systems with \(2\) or \(3\) equations) as an examination for 25 first year Masters students in mathematics, following a course on the subject. Each student was given 3 systems chosen at random and had a total of 30 min. Their performance was 9.33%, significantly lower than our models (\(84\%\)).

Our work has a number of limitations. Because there is no known way to tell whether a random system is stable, we lack a good benchmark on non-polynomial systems. Also, all the systems studied in this paper are relatively small, at most \(5\) equations for polynomial systems and \(3\) for non-polynomial. We believe that scaling to larger models should help tackle larger, and more complex, systems. Finally, this work could be extended to take into account the domain of definition of non-polynomial systems.

The broader implications of our work extend into two directions: the capability of transformers to reason, and the potential role of AI in scientific discovery. While large language models perform at human level on a broad set of tasks, they are still embarrassingly clumsy on many simple problems of logic and reasoning, to the point that it was suggested that planning and high level reasoning may be an inherent limitation of auto-regressive transformer architectures. Our results suggest that transformers can indeed be trained to discover solutions to a hard problem of symbolic mathematics that humans solve through reasoning, and that this is enabled by a careful selection of training examples, instead of a change of architecture. We do not claim that the Transformer is reasoning but it may instead solve the problem by a kind of "super-intuition" that stems from a deep understanding of a mathematical problem.

From a mathematical point of view, we propose a new, AI-based, procedure for finding Lyapunov functions, for a broader class of systems than were previously solvable using current mathematical theories. While this systematic procedure remains a black box, and the "thought process" of the transformer cannot be elucidated, the solutions are explicit and their mathematical correctness can be verified. This suggests that generative models can already be used to solve research-level problems in mathematics, by providing mathematicians with guesses of possible solutions. While a small minority of mathematicians is currently using deep-learning tools, we believe generative models have the potential to foster tremendous progress on a number of research subjects, and may eventually become a central component in the future landscape of mathematical practice.

    &  &  \\ Strategy & FBarr & FLyap & Poly3 & Poly5 \\  Baseline & 93 & 84 & 11.7 & 9.6 \\  _n1_ & 94 & 84 & 10.3 & 9.6 \\ _n2_ & 90 & 85 & 12.2 & 11.3 \\ _n3_ & 92 & 84 & 12.4 & 10.1 \\ _n4_ & 92 & 84 & **13.5** & **11.9** \\ _n5_ & 89 & 79 & 13.5 & 11.9 \\ _n6_ & 85 & 72 & 13.5 & 11.9 \\ _n7_ & 93 & 81 & 12.1 & 10.0 \\   

Table 7: **Expert iteration using IntoTheWild correct guesses**. The Poly3 and Poly5 test sets are regenerated, to prevent data contamination.

#### Acknowledgements

This work was performed in part using HPC resources from GENCI-IDRIS (Grant 2023-AD011014527). The authors also acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey. The authors would also like to thank the Master students of the Mathematics and Computer Science Department of the Ecole des Ponts - IP Paris from the year 2023-2024 who attended the course Control of Dynamical Systems. The authors also wish to thank Guillaume Lample for fruitful discussion.