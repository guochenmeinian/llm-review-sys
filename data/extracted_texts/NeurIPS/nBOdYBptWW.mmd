# UniTS: A Unified Multi-Task Time Series Model

Shanghua Gao

Harvard University

shanghua_gao@hms.harvard.edu &Teddy Koker

MIT Lincoln Laboratory

tekoker@mit.edu &Owen Queen

Harvard University

owen_queen@hms.harvard.edu &Thomas Hartvigsen

University of Virginia

hartvigsen@virginia.edu &Theodoros Tsiligkaridis

MIT Lincoln Laboratory

ttsili@ll.mit.edu &Marinka Zitnik

Harvard University

marinka@hms.harvard.edu

###### Abstract

Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset--characterized by diverse dynamic patterns, sampling rates, and temporal scales--to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.

## 1 Introduction

Foundation models, particularly large language models (LLMs), have transformed deep learning by enabling a single pre-trained model to support multiple tasks, eliminating the need for task-specific models. Language and vision models  can be adapted to new tasks with minimal additional training through approaches such as multi-task learning , few-shot learning , and prompting . Beyond language and vision, there is a growing need for similarly versatile models in time series that can accommodate data from diverse domains--including medicine , engineering , and science --and support a wide range of tasks, such as forecasting, classification, imputation, and anomaly detection.

Developing multi-task time series models that unify predictive and generative tasks under a single framework remains an open challenge. Time series datasets span multiple domains and exhibit varied temporal scales, sampling rates, and dynamic patterns, making them complex to manage .

Existing models often fall short in adaptability, as they either struggle to handle samples with varying numbers of variables [112; 67; 14] or treat each variable as independent, overlooking important interdependencies . Time series tasks are also highly diverse, encompassing distinct objectives and specifications across generative and predictive tasks. For example, generative forecasting tasks aim to produce future values within a time series, while predictive tasks may involve making discrete predictions for entire samples. Additionally, task requirements can vary significantly even within the same task type; for instance, generative tasks may involve different forecast lengths, and predictive tasks may feature multiple classification categories. As a result, time series models have mainly remained task-specific, with unique architectures typically designed and trained from scratch for forecasting [67; 82; 119], classification [30; 113], or other specialized tasks [116; 112]. Recent efforts to pre-train unified models [36; 22] or adapt LLMs for time series [118; 12; 129; 47; 97; 100] still heavily depend on extensive fine-tuning or the addition of task- and dataset-specific modules. Some models have explored generative pre-training transformers specifically for time series forecasting [10; 118; 47; 28], reporting strong results but focusing exclusively on forecasting without addressing other types of time series tasks. Consequently, these approaches require users to design and train new modules for each task or limit their application to a single type of tasks. To achieve a versatile, unified time series model--akin to foundational models in vision and language that operate across unified task spaces--a model must accommodate both _generative_ and _predictive_ tasks. Such a unified model would leverage a single set of weights for multiple tasks, removing the need to develop task-specific models from scratch. This approach would support a broad range of tasks and facilitate rapid adaptation to new datasets.

**Present work.** To address these challenges, we introduce UniTS, a unified multi-task time series model capable of handling a broad spectrum of time series tasks. We rigorously compare UniTS against 12 forecasting methods, 20 classification methods, 18 anomaly detection methods, and 16 imputation methods, including transformer-based, LLM-based, RNN-based, and traditional approaches, to highlight UniTS's generalizability to new tasks. This capability is achieved through the following model design: 1) _Task tokenization:_ UniTS encodes task specifications into a unified token representation, enabling universal task specification without post-hoc architectural modifications. 2) _Unified time series architecture:_ UniTS processes heterogeneous time series data with varying numbers of variables and sequence lengths without altering its network structure. To accomplish this, UniTS employs self-attention across time and variable dimensions to adapt to diverse temporal dynamics. We introduce a dynamic linear operator to model complex relationships between data points along the time dimension and a module to reduce interference in the feature space of heterogeneous data. 3) _Support for generative and predictive tasks:_ The combination of universal task specification and a unified time series architecture allows UniTS to share weights across tasks by co-training on multiple datasets. We use a masked reconstruction pre-training approach, enabling UniTS to be jointly optimized for generative and predictive tasks.

In the single-task setting, where models are trained individually for each dataset, UniTS outperforms task-specialized time series models and repurposed LLMs across forecasting, classification, anomaly detection, and imputation. In a challenging multi-domain, multi-task setting, we find that a single shared-weight UniTS model successfully handles 38 tasks, demonstrating its versatility as a multi-task time series model. UniTS surpasses top baselines that rely on data- and task-specific modules, achieving the highest average performance across tasks and excelling in 27 out of 38 tasks. Additionally, UniTS supports prompt-based learning and direct multi-step forecasting with flexible sequence lengths, capabilities not offered by models using task- and data-specific heads. In direct multi-step forecasting, UniTS outperforms the strongest baseline (which uses a sliding-window approach) by 10.5%. UniTS can also adapt to new tasks through parameter-efficient prompting, achieving results comparable to its fully fine-tuned counterpart. For example, across 20 forecasting datasets, prompted UniTS slightly outperforms the fully fine-tuned model, reducing MAE from 0.381

Figure 1: UniTS is a unified multi-task time series model for predictive and generative tasks.

to 0.376. Furthermore, UniTS demonstrates effective few-shot transfer, successfully addressing tasks like imputation, anomaly detection, and out-of-domain forecasting and classification without requiring specialized modules. For instance, UniTS improves on the strongest baseline by 12.4% in MSE on imputation and 2.3% in F1-score on anomaly detection. UniTS paves the way toward unified time series models, offering strong performance and adaptability across tasks and domains.

## 2 Related Work

**Traditional time series modeling.** Time series analysis has been extensively explored in both the statistics and machine learning communities for many years [45; 103; 123; 18; 80]. Numerous neural architectures have been developed for specific time series tasks such as forecasting [114; 65; 68; 67; 107], classification [115; 71; 70], anomaly detection [25; 56; 16], and imputation [17; 49; 3]. Task-specific models are typically trained via supervised learning on individual datasets, necessitating specialized modules. For example, a classification model requires a classification head with a specific number of classes, while data processing modules must handle a predetermined number of variables. In contrast, UniTS aims to unify various tasks into a universal task specification, enabling the handling of diverse data with a single, unified network architecture. This approach facilitates training a multi-task model capable of addressing multiple time series tasks.

**General time series modeling.** Foundation models, including language models [9; 101] and vision models [62; 50], are trained on broad data at scale to address diverse tasks with no or minimal additional training . Recent studies in time series analysis have sought to develop models with similar capabilities. This includes developing novel architectures to capture diverse time series signals. For instance, TimesNet  uses multiple frequency-based features obtained through Fourier transform to capture complex time series signals. There have been several efforts to reprogram LLMs for time series tasks [81; 12; 129; 47; 10]. Models such as GPT4TS  and Time-LLM  adapt LLMs by fine-tuning their embedding layers or aligning time series samples with LLM-based text prototypes (e.g., GPT-2 ). Unlike these models, UniTS is trained exclusively on time series data rather than relying on LLM architectures. Another approach, Lag-Llama , pre-trains a model on time series data from multiple domains specifically for forecasting tasks. Similarly, the Moment model  is pre-trained on a diverse range of time series data. However, these approaches still require task-specific modules and tuning for each task. In contrast, our UniTS model supports generative and predictive tasks without requiring extensive task-specific model adjustments.

**Prompt learning.** Prompt learning has emerged as an efficient method for task adaptation in large models [55; 88; 121; 13; 42]. Some approaches construct prompts directly in the model's input domain, such as text prompts for LLMs . Other methods involve tuning soft token inputs to frozen language models . In time series, PromptCast  and LLMTime  convert time series data into prompts for LLMs to facilitate forecasting. TEMPO  is another prompt-based approach that uses a learned set of prompts for LLM-based forecasting applications, while GPT4MTS  integrates both textual and numerical data to fine-tune LLMs for forecasting. In contrast, UniTS is trained exclusively on time series data, eliminating the need for computationally expensive pre-trained LLMs. Moreover, the universal task tokenization enables a frozen UniTS to adapt to new tasks beyond forecasting, such as classification and imputation. Further discussion of related work can be found in Appendix A.

## 3 Problem Formulation

**Notation.** We are given a set of multi-domain datasets \(=\{_{i}|i=1,,n\}\), where each dataset \(_{i}\) can have a varying number of time series samples; samples can be of varying time lengths and have varying numbers of sensors/variables. Each dataset is described as \(_{i}=(_{i},_{i})\), where \(_{i}\) denotes time series samples and \(_{i}\) specifies a task defined on \(_{i}\). Let \(\) and \(\) be collections, defined as \(=\{_{i}|i=1,,n\}\) and \(=\{_{i}|i=1,,n\}\), respectively. A time series sample in datasets is denoted as \(^{t v}\), where \(t\) and \(v\) are the length of the time series sample and the number of variables, respectively. We use _time dimension_ and _variable dimension_ to indicate the row and column dimensions in \(\). \(_{i}\) contains four common time series tasks: forecasting, classification, anomaly detection, and imputation. Further, each task type can be instantiated in numerous ways, e.g., forecasting over different time lengths and classification with varying numbers of classes. We use \(F(,)\) to denote a multi-task model trained on \(\). See Table 12 for notation details.

**Desiderata for a unified multi-task time series model.** Unlike specialized time series models designed and separately trained for each specific dataset \(_{i}\), a unified time series model \(F(,)\) is a single model with weights \(\) that are shared across all types of tasks and satisfies the following three desiderata: 1) _Heterogeneous time series:_ To process time series from all sources, the model \(F\) must be agnostic with any input samples in \(\), given the heterogeneity in time series lengths \(t\) and variable counts \(v\) in time series samples \(\) from various sources. 2) _Universal task specification_: For easy multi-task support and swift adaptation to new tasks, the model \(F\) should adopt a universal task specification \(F(,)\) applicable across all type of tasks \(\). 3) _One shared model:_ Sharing weights \(\) across tasks enables the unified model \(F\) to handle multiple tasks simultaneously. It contrasts with existing methods that typically train separate models on task-specific datasets, often involving elaborately tuned training parameters.

To realize the above desiderata, UniTS supports multi-task, prompt-based, and few-shot learning. **Multi-task learning**: UniTS specifies a single model \(F(,)\) for tasks \(\) defined on datasets \(\). Multi-task learning showcases the flexibility of the model to learn across time series domains and tasks. **Prompt learning**: By leveraging prompt tokens, UniTS supports prompt learning, \(\{F(,),\}\), across tasks while keeping the model frozen. Additionally, UniTS can be trained in a single-task manner, following the same setup as used by many existing models. Other settings are described in Appendix C.1.

## 4 UniTS Model

UniTS is a multi-task model with a unified network architecture. It uses a token-based format to describe tasks and time series from different domains. We introduce a novel approach with three distinct token types: sample, prompt, and task tokens, each serving a unique purpose in time series analysis. The input time series sample is tokenized into sample tokens. Prompt tokens provide essential context for the task, guiding the model to accomplish the user-specified task. Task tokens (GEN and CLS) are combined with other tokens and used for generative and predictive tasks. UniTS then converts task tokens into task predictions to produce the final model output. Unlike transformers such as PatchTST , UniTS introduces new token types: sample tokens allow for modeling of multivariate time series, prompt tokens enable efficient multi-task and prompt learning , and task tokens unify predictive and generative tasks into one format.

### Prompting UniTS with Unified Time Series Data Tokens

We introduce how to use unified tokens to unify different task types and data for inference. Tokens on different network layers have the same shape, so we omit the layer index for simplicity.

**Sample tokens.** We divide time series input sample \(^{t v}\) into patches along the time dimension using a non-overlapping patch size of \(k\). A linear layer projects each patch into an embedding vector of length \(d\), obtaining sample tokens \(}^{s v d}\), where \(s=t/k\). Since \(v\) and \(s\) vary across time

Figure 2: **a) UniTS for forecasting; input is tokenized, and GEN tokens are un-patchified to infer the forecast horizon. b) UniTS for classification; a CLS token is used to represent class information and then compared to class tokens to get prediction class. c) Architecture of UniTS model.**series data domains, we keep the variable and time dimension in tokens. \(}\) are then added with learnable positional embeddings.

**Prompt tokens.** Prompt tokens \(_{p}^{p v d}\) are defined as learnable embeddings, where \(p\) is the number of tokens. In a multi-task setting, each dataset has its own set of prompt tokens. These tokens incorporate the specific context related to the data and the task the model needs to complete. For each sample in the dataset, these prompt tokens are appended to the sample tokens and sent to the network to provide context information about the current sample. For prompt learning, with the pre-trained model weights being frozen, UniTS adapts to new tasks by utilizing prompt tokens learned with the prompt tuning. Prompt learning is more efficient than tuning new data/task-specific heads and achieves comparable performance to full model fine-tuning, as shown by few-shot learning experiments on new tasks (Tables 4 and 5) and new datasets (Table 3).

**Task tokens.** In Figure 1(b), we categorize task tokens into two types: 1) GEN (Generation) tokens used in forecasting, imputation, and anomaly detection, and 2) CLS (Classification) tokens, which are used for classification tasks (in a given task, the number of CLS tokens corresponds to the number of classes in the task). Task tokens define a general format for representing tasks and support flexible adaptation to new tasks. For tasks involving forecasting, in Figure 1(a), the GEN token \(_{m}^{1 v d}\), is replicated \(f\)-times based on desired forecasting length to get \(}_{m}^{f v d}\). These tokens \(}_{m}\) are then concatenated with the sample and prompt tokens and fed into the UniTS network:

\[}}=(_{p},},}_{m})^{(p+s+f) v d},\] (1)

where CA is the concatenation operation along the time dimension. At the output of the model, embedding vectors with length \(d\) in \(}_{m}\) are unpatchified to patches with size \(e\) to obtain the forecasting sample \(}\), i.e. \(}=(}_{m})^{(f e)  v}\). This approach allows the UniTS model to perform direct multi-step forecasting  over arbitrary time lengths, as illustrated in Figure 3. For classification, in Figure 1(b), CLS token \(_{c}^{1 v d}\) is concatenated along the time dimension with the prompt and sample tokens, resulting in:

\[}}=(_{p},},_{ c})^{(p+s+1) v d},\] (2)

which is then fed into the model. We define class embeddings \(_{e}^{e v d}\) for each of \(e\) classes in the task. These class embeddings are either trained or generated by averaging CLS tokens of training samples in each class. Finally, the class for sample \(\) is predicted by finding the class embedding vector in \(_{e}\) that is the closest to the CLS token \(_{c}\) from the model output:

\[=*{argmin}_{i}||_{c}-_{ e_{i}}||^{2},i[0,e).\] (3)

For imputation, missing values are imputed using the GEN tokens. For anomaly detection, the model takes a time series sample containing any number of potentially anomalous values, generates the output sample by reading out the sample tokens, and then determines anomalous values based on the reconstruction error between the input sample and the generated sample. Details on using tokens for imputation and anomaly detection are in Appendix C.2. All tokens and embeddings are trained to achieve their functions.

### Unified Network Architecture in UniTS

Time series samples can have varying numbers of variables, temporal dynamics, and time lengths across different domains and types of tasks. UniTS uses a modified transformer architecture  to handle heterogeneous multi-domain data with varying dynamics and the number of variables (Figure 1(c)). In the following, we describe key modules of UniTS architecture. Note that UniTS can also be used with other backbones, such as Mamba .

**Time and variable self-attention.** We use a two-way self-attention to both variable and time dimensions. This approach contrasts with previous methods that apply self-attention to either time  or variable dimension , but not to both dimensions. Time and variable self-attention effectively handle time series samples with various numbers of variables \(v\) and different time lengths \(t\).

**DyLinear.** We modify the transformer block by adding a dynamic operator (DyLinear) into the feed-forward network layer (FFN). This modification enables the FFN to capture dependencies between tokens. In contrast to the standard FFN, which processes embedding vectors on a point-wise basis, DyLinear uses weight interpolation to accommodate varying time lengths. Given a sequence of sample tokens \(_{t}^{l_{} d}\), DyLinear interpolates weights \(^{w_{} w_{}}\) to accommodate varying time lengths as follows:

\[(_{t},)=_{}_{t}\ ^{l_{} d};_{}=( )^{l_{} l_{}},\] (4)

where Interp is a bi-linear interpolation to resize \(\) from shape \(w_{} w_{}\) to \(l_{} l_{}\) to match the input and output length. DyLinear captures dependency patterns across time series samples, which leads to improved performance on generative tasks (Table 23).

**Gating module.** We add a gating module after each layer to mitigate interference in the latent representation space caused by multi-domain and multi-task datasets (Figure 2). This module dynamically re-scales features in layer-wise latent spaces and promotes the stability of latent representations.

**Generative and predictive towers.** We design a shared GEN tower (\(H_{}\)) and CLS tower (\(H_{}\)) for transferring GEN/CLS tokens to generate time series samples and classification classes, as introduced in Section 4.1. Unlike existing works that use standalone, task-specific heads for individual datasets, our approach leverages GEN tower and CLS tower for all generative and predictive tasks, respectively, ensuring a more unified and efficient model architecture.

The UniTS architecture includes the backbone network composed of \(N\) modified transformer blocks described above, a CLS tower, and a GEN tower. Implementation details are in Appendix C.3. Ablations in Appendix F verify the effectiveness of this architecture.

### UniTS Model Training

**Unified masked reconstruction pre-training.** To enhance UniTS's abilities to 1) learn general features applicable to both generative and predictive tasks and 2) efficiently adapt to downstream tasks via prompt learning, we introduce a unified mask reconstruction pre-training scheme. It leverages the semantics of both prompt and CLS tokens (Section 4.1) for masked reconstruction pre-training, therefore learning representations for both generative and predictive capabilities. This is distinct from pre-training strategies that use either generative [82; 120; 26; 54] or predictive [72; 109; 117; 29; 87] approach. Unlike these approaches that pre-train only the model backbone, our strategy pre-trains all components of UniTS, including the backbone and GEN/CLS towers (Section 4.2), enabling prompt and zero-shot learning over a frozen pre-trained model. For each time-series sample \(\), a handful of sample tokens get masked and replaced with GEN tokens. These masked sample tokens is then concatenated with prompt tokens and CLS tokens, sent to the UniTS backbone network. In the unified pre-training loss, tokens from the backbone network output are sent to the CLS/GEN towers to reconstruct the input sample \(\), formulating as follows:

\[L_{}=L_{}(H_{}(_{p},_{ }),)+L_{}(_{}(H_{}( _{}),\ _{}),\ ).\] (5)

\(L_{}\) is the MSE loss to predict the full sample \(\). For the left side of the loss, prompt token \(_{p}\) is sent along with sample token \(_{}\) to GEN tower \(H_{}\) to help with the reconstruction. For the right side of the loss, to leverage the semantics of the CLS token and train the CLS tower \(H_{}\) for predictive tasks, \(_{}\) (Eq. 2) from the model output is processed by the CLS tower \(H_{}\) to get classification-related embedding vectors \(}_{}=H_{}(_{})\), and another GEN tower \(_{}\) takes in \(}_{}\) and \(_{}\) to predict the full sample. \(_{}\) is only used for pre-training and will be removed for downstream tasks. This unified pre-training strategy involves pre-training both tokens, the backbone network, and the GEN/CLS towers for both generative and predictive abilities.

**Training UniTS models.** We implement and evaluate two UniTS models, each trained in a different regime. We start with a pre-trained UniTS that is optimized using self-supervised \(L_{}\) in Eq. 5 and trained across a collection of multi-domain datasets. Given a self-supervised pre-trained UniTS whose weights are frozen, we consider a fine-tuned model where only tokens for predictive or generative tasks are fine-tuned (denoted as UniTS-_PMT_ in Experiments). We also consider a standard multi-task supervised learning regime, where a single UniTS model is trained from scratch to simultaneously perform many tasks (denoted as UniTS-_SUP_ in Experiments). These two regimes use a multi-task setup, where a single model is trained and tested on multiple tasks and datasets. During multi-task training, we sample batches of time series samples and aggregate dataset-centric loss values: \(L_{}=_{i=1}^{I}_{i}L_{i}(D_{i})\), where \(L_{i}\) is the loss of batch \(i\), \(_{i}\) is the weight for each loss, and \(I\) denotes the number of batches. We follow  and use the MSE loss for forecasting and cross-entropy loss for classification. For fair comparison with models trained in a single-task manner, we follow the experimental setup of [112; 67] and benchmark UniTS in a single-task setting (denoted as UniTS-_ST_ in Experiments), where the model is trained separately on each dataset/task.

[MISSING_PAGE_FAIL:7]

"\(p\)" is forecasting length. "Class./Num." denotes the "number of classes in each task"/"number of datasets".

**Results.** Table 1 shows the single-task performance for four types of tasks. On forecasting tasks with forecasting lengths of 92, 196, 336, and 720, compared with 11 forecasting methods, UniTS-_ST_ achieves the best results on 28 out of 32 datasets for MSE and 27 out of 32 for MAE, surpassing the previous best method, iTransformer, by a clear margin. In Table 34, we demonstrate that UniTS-_ST_ outperforms the concurrent MOMENT  model, which was trained on a large and diverse collection of time series data. Additionally, UniTS-_ST_ achieves stronger performance than LLM-reprogrammed methods that are pre-trained with extensive natural language data, e.g. GPT4TS , TEST , LLM4TS , and TEMPO . On 10 classification datasets, UniTS-_ST_ outperforms 19 classification methods on the average accuracy, such as the transformer/MLP/frequency-based methods. It has a gain of 1.4% compared to the previous best TimesNet model. On 5 anomaly detection datasets, UniTS-_ST_ has a clear gain of 3.95% in F1 score compared to the TimesNet and also beat other 15 anomaly detection methods, such as Anomaly Transformer . On 16 imputation datasets with a mask ratio of 12.5%, 25%, 37.5%, UniTS-_ST_ has the best results on all datasets in terms of MSE and MAE, outperforming 14 baseline methods. UniTS-_ST_ has the SoTA performance on these single-task benchmarks, showing its effectiveness.

### Benchmarking UniTS for Multi-Task Learning

**Setup.** In a multi-task setting, we benchmark a single UniTS model co-trained and evaluated on 38 datasets, comprising 20 forecasting tasks and 18 classification tasks, with variations in the number of variables/sensors, classification classes, and forecasting lengths. We consider two variants of UniTS; the fully supervised UniTS-_SUP_ and the more challenging UniTS-_PMT_ with prompting, as introduced in Section 4.3. Baselines use the same fully supervised multi-task training as our approach but cannot handle differences across data types and task specifications with a single model. To benchmark them, a shared backbone is used for all tasks, augmented by data-specific input modules and task-specific output modules.

**Results: Model benchmarking.** Table 2 shows multi-task learning performance. UniTS consistently outperforms baseline methods, achieving the best results in 17 out of 20 forecasting tasks (MSE) and 10 out of 18 classification tasks (accuracy). Performance gains are especially remarkable because UniTS has one fully shared model, whereas all existing methods require task or dataset-specific modules. We find that baseline methods encounter difficulties performing well across different types of tasks. For example, TimesNet, which excels in classification tasks, underperforms in forecasting tasks. Conversely, iTransformer, the top-performing forecaster, struggles with classification tasks. In contrast, the UniTS model exhibits robust performance across classification and forecasting. On forecasting, UniTS-_SUP_ surpasses the leading baseline, iTransformer, by 5.8% (0.439 vs. 0.466) in MSE and 3.3% (0.381 vs. 0.394) in MAE. On classification, UniTS-_SUP_ has an average gain of 0.7% accuracy (81.6% vs. 80.9%) over the strongest baseline (TimesNet). UniTS shows promising potential to unify data and task diversity across time series domains.

   &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  \\  NN5 & **All** & 589 & 62.6 & 67.1 & 35.2 & 48.6 & 52.4 & 65.4 & 65.4 & 65.8 & 10.7 & 11.1 & 53.0 & 54.5 & **All** & **UnITS** & **UnITS** & **UnITS** & **UnITS** & **UnITS** & **UnITS** \\  PC\({}_{11}\) & 21 & 21 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 39 & 40 & 46 & 35 & 24 & 20 & 23 &Recent research has adapted pre-trained LLMs to time series [47; 12; 129; 37]. Most approaches [47; 12; 129], such as GPT4TS, incorporate additional task-specific modules to align the modalities of time series and natural language. We compare UniTS with GPT4TS that reprograms pre-trained GPT-2 model . Despite the substantial data amount and model scale gap, e.g., GPT4TS is 48\(\) larger than UniTS-_SUP_ (164.5dN vs. 3.4M), UniTS-_SUP_ still compares favorably to GPT4TS. On forecasting tasks, UniTS-_SUP_ even outperforms GPT4TS by 2.2% (0.439 vs. 0.449; MSE).

**Results: Prompting is competitive with supervised training.** Using tokens to prompt a frozen UniTS, the SSL-pre-trained UniTS achieves performance comparable to its fully supervised counterpart (Table 2). UniTS-_PMT_ even outperforms the supervised model in forecasting, with a lower MAE score (0.379 vs. 0.381), highlighting the effectiveness of prompt learning in UniTS. Furthermore, prompt learning with UniTS surpasses the performance of supervised baseline methods with separate modules. This indicates that the SSL-pre-trained model captures valuable time series representations and that prompt learning allows the model to efficiently adapt to target tasks.

### UniTS for Direct Multi-Step Forecasting

**Setup.** Direct multi-step forecasting predicts across varying time horizons by adjusting from the original trained length, with offsets ranging from 0 to 384. We use 14 out of 20 forecasting datasets with varying lengths. UniTS achieves this flexibility by repeating the GEN token, as described in Section 4.1, a capability not supported by existing methods. For comparison with baseline models, we implement a sliding-window approach for forecasting. In this method, predictions are made over a fixed window size, which then shifts forward incrementally to cover progressively extended time horizons. This sliding mechanism allows us to adapt the model to forecast over new, unseen time periods while maintaining consistency with the evaluation setup used by baseline methods.

**Results: Direct multi-step inference outperforms sliding window approach.** In Figure 3, UniTS demonstrates improved performance over baseline models across various forecasting lengths when using the sliding-window approach. For example, in the longest forecasting extension of +384, UniTS outperforms the iTransformer by 8.7% in MSE, achieving a score of 0.451 compared to 0.494. When using direct multi-step inference, UniTS gains an even larger advantage over the iTransformer, reducing MSE by 10.5% (0.442 vs. 0.494). This approach also reduces the average number of inference steps from 3.66 to 1, resulting in a 3x speedup.

### UniTS for Few-Shot Learning on New Datasets and Tasks

For transfer learning on new tasks and datasets, we load the model weights pre-trained on 38 datasets and apply them in a multi-task setting. We evaluate two approaches: the fully fine-tuned UniTS-_FT_ model and the prompted UniTS-_PMT_ model, in which task-specific tokens are trained.

**Setup: Few-shot classification and forecasting.** Pre-trained models, undergo fine-tuning using 5%, 15%, and 20% of the 11 training set shown in Table 8. Average performance is reported.

**Results.** UniTS achieves superior performance compared to iTransformer across all training data ratios (Table 3). At the 20% data ratio, UniTS-_FT_ achieves a gain of 8.8% in classification accuracy and a reduction of 5.7% in forecasting MSE. UniTS-_PMT_ surpasses the fully supervised iTrans

Figure 3: Direct multi-step forecasting on new lengths. UniTS achieves any new forecasting length with unified direct multi-step inference. Baseline methods use the sliding windows inference as they do not support direct multi-step inference.

   Model & Ratio & Acc\(\) & MSE\(\) & MAE\(\) & Best Count & Shared \\  iTransformer-_FT_ & 5\% & 56.4 & 0.598 & 0.487 & 1/24 & \(\) \\ UniTS-_PMT_ & 5\% & 55.7 & **0.508** & **0.440** & 16/24 & \(\) \\ UniTS-_FT_ & 5\% & **57.4** & 0.530 & 0.448 & 7/24 & \(\) \\  iTransformer-_FT_ & 15\% & 56.5 & 0.524 & 0.447 & 4/24 & \(\) \\ UniTS-_PMT_ & 15\% & 59.5 & 0.496 & 0.435 & 4/24 & \(\) \\ UniTS-_FT_ & 15\% & **61.8** & **0.487** & **0.428** & 16/24 & \(\) \\  iTransformer-_FT_ & 20\% & 59.9 & 0.510 & 0.438 & 4/24 & \(\) \\ UniTS-_PMT_ & 20\% & 63.6 & 0.494 & 0.435 & 3/24 & \(\) \\ UniTS-_FT_ & 20\% & **65.2** & **0.481** & **0.425** & 17/24 & \(\) \\   

Table 3: Few-shot multi-task learning on 9 forecasting and 6 classification tasks on out-of-domain datasets. Ratio is the data ratio of the dataset used for training. Full results in Table 29.

former, leading to 6.2% increase in classification accuracy and 3.1% decrease in forecasting MSE. When trained under a 5% data ratio,UniTS-_PMT_ exceeds UniTS-_FT_ performance for forecasting, suggesting that prompt learning is effective for transfer learning when training data is scarce.

**Setup: Few-shot imputation.** Models are fine-tuned with 10% of 6 imputation training data listed in Table 10, asked to impute 25% and 50% of missing data points.

**Results.** A unified UniTS-_FT_ outperforms models that use separate task-specific modules (Table 4), indicating that UniTS has robust few-shot imputation performance. Specifically, on a 25% masking ratio, UniTS-_FT_ exceeds the top-performing baseline iTransformer by 12.4% in MSE and 7.9% in MAE. The margin remains notable at a 50% masking ratio, where UniTS-_FT_ surpasses iTransformer by 8.8% in MSE and 6.8% in MAE. UniTS-_PMT_, the fixed model with appropriate prompt tokens, outperforms all baseline methods and achieves results comparable to its fully fine-tuned counterpart, suggesting that prompting can adapt UniTS for imputation.

**Setup: Few-shot anomaly detection.** The pre-trained models have been fine-tuned using 5% of five training datasets as listed in Table 10. The average F1-score is used as the metric.

**Results.** UniTS outperforms the top-performing baseline (PathTST) across all metrics (Table 5). UniTS-_FT_ achieves an F1-score of 86.3 compared to PathTST's F1-score of 84.3. UniTS-_PMT_ also outperforms specialized models (Anomaly Transformer) trained from scratch.

**Additional results and ablations.** Zero-shot learning is significantly more challenging than few-shot learning. Our work primarily focuses on few-shot learning, with some initial exploration of zero-shot learning for forecasting tasks of UniTS on new datasets in Appendix G. Additional analysis and ablation results are in Appendix F and Appendix E.

## 6 Conclusion

We have developed UniTS, a unified model for time series that uses a universal specification of time series tasks. UniTS handles multi-domain time series data with heterogeneous representations, outperforming task-specific models and reprogrammed LLMs on 38 multi-domain and multi-task datasets. UniTS also shows strong few-shot and prompt-based performance and can generalize to new domains and tasks. The unified token scheme in UniTS allows it to represent data and tasks in a general manner. UniTS uses a transformer architecture, and we plan to explore other types of backbones, such MLP-based blocks [107; 14] and Mamba , to further enhance UniTS. Limitations and future directions are discussed in Appendix M.