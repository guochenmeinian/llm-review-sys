# _GO4Align_: Group Optimization for

Multi-Task Alignment

 Jiayi Shen\({}^{1}\), Qi (Cheems) Wang\({}^{2}\), Zehao Xiao\({}^{1}\)1, Nanne Van Noord\({}^{1}\), Marcel Worring\({}^{1}\)

\({}^{1}\)University of Amsterdam, Amsterdam, the Netherlands

\({}^{2}\)Department of Automation, Tsinghua University, Beijing, China

###### Abstract

This paper proposes _GO4Align_, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, comprising two techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse benchmarks demonstrate our method's performance superiority with even lower computational costs.

## 1 Introduction

Multi-task learning is a promising paradigm for handling several tasks simultaneously using a unified architecture. It can achieve data efficiency, improve generalization, and reduce computation costs compared with addressing each task individually . Due to these benefits, there is a growing surge of applications with multi-task learning in several domains, _e.g._, natural language processing , computer vision  and reinforcement learning . The crux of multi-task learning is to enable positive transfer among tasks while avoiding negative transfer, which usually exists among irrelevant tasks .

**Existing Challenges:** In avoiding the negative transfer, numerous multi-task optimization (MTO) methods  have emerged and attracted rising attention in recent years. A lasting concern in MTO is the _task imbalance issue_.

It describes a phenomenon where some tasks are severely under-optimized , which can lead to worse overall performance with larger convergence differences across tasks.

To deal with the task imbalance issue, scaling methods are proposed for MTO. According to differences in scaling manipulations, we roughly divide MTO methods into _gradient-oriented_ and _loss-oriented_. The former tends to exhibit impressive results at the expense of higher computational or memory requirements during training time due to the assessment of per-task

Figure 1: **Performance and computational efficiency evaluation for MTO methods evaluated on NYUv2. Each method’s training time is relative to a baseline method, which minimizes the sum of task-specific empirical risks. Left-bottom marks comprehensive optimal results.**gradients .2 In contrast, the latter preserves training-time efficiency but usually suffers from unsatisfactory overall performance. As shown in Fig. 1, most existing methods cannot simultaneously achieve superior performance and computational efficiency.

**Proposed Solution:** To improve the overall performance and maintain computational and memory efficiency, we propose **G**roup **O**ptimization **for** multi-task **Alignment** (_GO4Align_), a novel and effective loss-oriented method in MTO. As shown in Fig. 2, this work identifies _multi-task alignment_ as a crucial factor in solving task imbalance, which means learning progress across tasks should synchronously achieve superior performance over all tasks. The proposed model dynamically aligns learning progress across tasks by exploiting group-based task interactions for multi-task empirical risk minimization. The rationale behind this is that groupings can implicitly capture task correlations for more effective multi-task alignment and thus help multi-task optimizers benefit from positive interactions among relevant tasks. The primary contribution is two-fold:

* As a new member of the loss-oriented MTO branch, _GO4Align_ recasts the task imbalance issue to a bi-level optimization problem, yielding an adaptive group risk minimization principle for MTO. Such a principle allocates weights over task losses at a group level to achieve learning progress alignment among relevant tasks.
* We develop a heuristic optimization pipeline in _GO4Align_ to tractably achieve the principle, involving _dynamical group assignment_ and _risk-guided group indicators_. The pipeline incorporates beneficial task interactions into the group assignments and exploits task correlations for multi-task alignment, improving overall multi-task performance.

Experimental results show that our approach can outperform existing state-of-the-art baselines in extensive benchmarks. Moreover, it does not sacrifice computational efficiency.

## 2 Preliminary

**Notations.** This work considers a multi-task problem over an input space \(\) and a collection of task target spaces \(\{^{m}\}_{m=1}^{M}\), where \(M 2\) denotes the number of tasks. A composite dataset for multi-task learning is \(\{(_{n},y_{n}^{1},...,y_{n}^{M})\}_{n=1}^{N}\), where \(N\) is the number of training samples. Let \(^{s}\) and \(^{m}\) respectively be the shared and task-specific parameters in a given multi-task model, thus we have a parametric hypothesis class for the \(m\)-th task as \(f(_{n};^{s},^{m}):^{m}\). Then the empirical risk for the \(m\)-th task can be written as \(}^{m}(^{s},^{m})=_{n=1}^{N }^{m}(f(_{n};^{s},^{m}),y_{n}^{m})\), where \(^{m}(,):^{m}^{m}_{+}\) denotes the task-specific loss function. The ultimate goal of MTO is to achieve superior performance over all tasks.

**Scale Empirical Risk Minimization (Scale-ERM).** As preliminary, we recap a representative and related strategy in MTO, Scale-ERM, through the lens of risk minimization. Scale-ERM introduces a task-specific weight \(^{m} 0\) to scale the corresponding empirical risk. For conciseness, this principle is formulated using vector notations. Here, \(=[^{1},^{2},,^{M}]^{} ^{M}\) represents a \(M\)-dimensional vector comprising all task-specific weights. And \(}()=[}^{1}(^{s},^{1}),}^{2}(^{s},^{2}),, }^{M}(^{s},^{M})]^{} ^{M}\) denotes the corresponding vector of empirical risks, where \(=\{^{s},^{1},^{2},,^{M}\}\) represents all learnable parameters in a multi-task backbone network. Thus, we obtain the objective of Scale-ERM as follows:

\[_{,}^{}}()+ (),\] (1)

where \(()\) is a regularization term over task weights, designed to prevent the rapid collapse of these weights to zero, as discussed in Kendall et al. . When all task weights are the same in scale, Scale-ERM will degenerate to the most simple strategy in MTO, where each task is treated equally during the joint training.

In Scale-ERM, each task weight controls task-specific learning progress, either by adapting the task-specific weights with loss information (loss-oriented) or by operating on task-specific gradients (gradient-oriented). As previously indicated, there still remains a research gap in MTO to improve multi-task performance without affecting computational efficiency.

Methodology

In resolving the task imbalance issue effectively and efficiently, we develop _GO4Align_ in this section. Our approach relies on grouping-based task interactions to align learning progress across tasks. As a new member of the loss-oriented branch, _GO4Align_ holds the advantage of computational efficiency without the requirements of per-task gradients.

**Motivation of Multi-Task Alignment.** Empirically, we can observe that the overall performance is worse when there is a larger _convergence difference_ between tasks. The convergence difference is measured by the standard deviation of the task-specific epoch numbers to reach convergence. As shown in Fig. 2, UW  underperforms FAMO  in terms of a overall MTL metric \(\%\) (lower is better); while UW has a larger convergence difference than FAMO. Intuitively, a larger convergence difference means that per-task training dynamics are more asynchronous, usually leading to worse overall performance.

To perform MTO, we consider aligning tasks with group information in the multi-task risk minimization. We first present an adaptive group risk minimization principle for MTO, which targets the alignment of tasks' learning progress from grouping-based task interactions. Then, in tractable problem-solving, we decompose the whole optimization process into two entangled phases: _(i) dynamical group assignment_ and _(ii) risk-guided group indicators_. The pseudo-code of _GO4Align_ is provided in Appendix A.

### Adaptive Group Risk Minimization Principle

Recent advances [21; 22] have explored incorporating multi-task grouping in feature sharing and revealed its benefit of aligning the learning progress through task interactions. Nevertheless, their grouping mechanisms ignore monitoring the learning progress, _e.g._, failure to capture variations in loss scales among tasks, weakening the effectiveness of multi-task alignment.

As a result, we design a new grouping mechanism for MTO with _task-specific learning dynamics_, which directly impacts the _convergence behaviors in optimization_. This induces the adaptive group risk minimization principle suitable for multi-task alignment. The hypothesis is that the dynamical grouping tends to implicitly exploit task correlations  and encourages beneficial task interactions from empirical risk information along the learning progress. Meanwhile, such a principle retains computational efficiency as it avoids the computations of per-task gradients.

**Adaptive Group Risk Minimization (AGRM).** We first achieve beneficial task interactions by producing task weights with a grouping mechanism, which is adaptive to various loss scales and their learning dynamics over time. Then, we recast the task imbalance issue with the grouping mechanism into a bi-level optimization problem: (i) In the _lower-level_ optimization, the model aims to cluster \(M\) tasks of interest into \(K\) groups. This implicitly exploits task correlations, where similar tasks should be clustered into one group, yielding more beneficial task interactions. With group assignments, group weights are designed to conduct learning progress alignment at the group level. The group weights in the multi-task objective are motivated as an extension of the observation that similar tasks benefit greatly from training together through parameter sharing . (ii) In the _upper-level_ optimization, the proposed principle updates model parameters from the grouped empirical risks, which implicitly relies on the lower-level optimized results. We illustrate _GO4Align_'s bi-level optimization with grouping-based task interactions in Fig. 3.

Given \(K\) as the number of groups, we denote the assignment matrix as \(}_{t}^{K M}\), where \(_{t}(k,m)\) equals \(1\) if the \(k\)-th group contains the \(m\)-th task and \(0\) otherwise. Note that \(}_{t}\) is updated in the optimization, with \(t\)-th indexing iteration step. The group number generally is smaller than the task number, _e.g._, \(1<K M\). To balance different groups, we place weights over groups \(_{t}=[_{t}^{1},_{t}^{2},,_{t}^{K }]^{}^{K}\), where \(_{t}^{k} 0\) is specific to the \(k\)-th group at the \(t\)-th iteration.

Figure 2: **Multi-task alignment and effects on performance.** We visualize relative task performance curves (lower is better) over training epochs. Better overall performance usually occurs with lower convergence differences. Our method effectively reduces the convergence difference and achieves a better overall performance.

Formally, we formulate the bi-level optimization problem as:

\[_{}_{t}{}^{}}_{t}}()}\ \{_{t},}_{t}\}=_{,}} (,};_{t}),\] (2)

where \(_{t}\) and \(}_{t}\) reflect adaptive group information in the lower-level optimization. \((,};_{t})\) is the corresponding optimization objective for aligning the learning progress across tasks at the group levels, which is further explained in Sec. 3.2.

To be specific, we perform the bi-level optimization in Eq. (2) according to the following steps. For the \(t\)-th iteration, we first compute the group information \(_{t}\) and \(}_{t}\) in the lower-level optimization; and then we update the model's parameter in the upper-level optimization. As a result, we obtain the updated parameter \(_{t+1}\), which is used to compute task-specific risk information at the next iteration. In the proposed principle, intra-group tasks share the same scaling weight \(^{k} 0\) to prevent similar tasks from inconsistent learning progress, improving knowledge sharing among similar tasks.

Important in AGRM is to accommodate the grouping during training dynamically. In practice, the proposed principle is compatible with any gradient-based optimizer, such as SGD and Adam , yielding dynamical training for each task. As a new member of the loss-oriented branch, the grouping assignment matrix and group weights in _GO4Align_ can sufficiently utilize the loss information over time to adaptively assign tasks and weight groups.

Unlike prior works on multi-task grouping [21; 22], which require group-specific architectures, our proposed principle focuses on group-specific scaling and adaptively executing grouping operations. Considering the differences in architecture and optimization within grouping mechanisms, we provide two insights: (i) the criteria for grouping tasks should take both _learning dynamics_ and _loss scales_ into consideration so that similar tasks can benefit from each other's intermediate feature information and boost performance; (ii) adaptive grouping _aligns learning progress across tasks_ and provides a more effective way for across-task information transfer.

### Dynamical Group Assignment

In solving the optimization problem in Eq. (2), the main challenge lies in the involvement of discrete and continuous variables, which are implicitly entangled in the objective. In detail, the lower-level optimization requires adaptively adjusting the discrete variable \(}_{t}\) and the continuous variable \(_{t}\) for the learning progress alignment such that the model's parameter \(\) updates from the lastest grouping information.

Before executing the _lower-level_ optimization, we need to introduce task-specific group indicators \(_{t}(_{t})=[_{t}^{1}_{t}^ {s},_{t}^{1},_{t}^{2}_{t}^{s},_{t}^{2},,_{t}^{M}(_{t}^{s},_ {t}^{M})]^{}^{M}\). In general, this involves the entanglement of the model's parameters, which is obtained from high-level optimization. These group indicators work for exploiting cross-task correlations along the learning progress and provide group information to enable task interactions in the _lower-level_ optimization. We will further discuss the design of the group indicator in Sec. 3.3.

Intuitively, we conduct the group assignment as a clustering process based on these group indicators \(_{t}(_{t})\). In this case, each cluster represents a group, and the cluster center is set to the group weight. Many clustering algorithms are available to achieve this. In this work, we take the K-means clustering algorithm [24; 25; 26] as a practical clustering implementation. Thus, we specify the optimization

Figure 3: _GO4Align using adaptive group risk minimization in the bi-level optimization framework_. In the lower-level optimization, the model assigns tasks to groups with different group weights, encouraging task interactions and aligning learning progress. Such group information is nested into the upper-level optimization for updating the multi-task model’s parameters.

objective of the dynamical group assignment as:

\[_{,}}J(,};_{t}) :=\|_{t}^{}(_{t})-^{}}\|^{2},\] (3)

where \(_{t}^{}=_{t}^{}(_{t})} _{t}^{-1}\) indicates that the cluster center closely relates to the group assignment matrix. It is worth noting that \(}_{t}^{-1}\) is a generalized inverse, especially one-sided right inverse, \(}^{-1}=}^{}(}} ^{})^{-1}\). The designed dynamical group assignment plays an important role in the lower-level optimization of the proposed AGRM, and it tends to cluster similar tasks into the same group while scattering dissimilar ones in clusters.

By integrating the dynamical group assignment in Eq. (3) and the group indicators into Eq. (2), we provide an instantiation for the AGRM's optimization objective:

\[_{}_{t}{}^{}}_{t}}( ) }\{_{t},}_{t}\}=*{arg\,min}_{,}}\|_{t}^{}(_{t})- ^{}}\|^{2}.\] (4)

Moreover, the dynamic group assignment heuristically clusters tasks from the group indicators, which avoids the exhausted search of appropriate task combinations for performance gains like previous works .

### Risk-guided Group Indicators

This subsection discusses the appropriate design of the group indicators for dynamic group assignment introduced in Sec. 3.2.

The misalignment of learning across tasks can usually be attributed to various risk scales and asynchronous learning dynamics among tasks over time. To address this, we take two operations with risk information, _scale-balance_ and _smooth-alignment_, into consideration and then obtain the risk-guided group indicators by combining them. The role of the group indicators is to use the risk information to explore the relationships among tasks without incurring the expensive computational cost associated with gradients. Compared with other loss-oriented methods, our group indicator can capture the differences in the per-task risk scale and fully utilize the learning dynamics over time, yielding better representations of risk information.

**Scale-balance.** To alleviate the misalignment caused by differences in per-task risk scales, we introduce _scale-balance_, which enlarges the importance of tasks with smaller risks in optimization. Given task-specific risks at iteration \(t\), we normalize them to their average risk for efficient scale balancing. In each iteration, the scale vector for all tasks is denoted as \(_{t}(_{t})=p_{t}^{1}(_{t}),p_{t}^{2} (_{t}),,p_{t}^{M}(_{t})^{}^{M}\), which can be calculated as:

\[_{t}(_{t})=(}(_{t}))^{-1}}(_{t})_{M},\] (5)

where \(()\) constructs a diagonal matrix with the elements of the vector placed on the diagonal. \(}(_{t})\) is a scalar to represent the average risk, and \([]_{M}\) represents the construction of an \(M\)-dimentional vector whose elements are all equal to the average risk. To avoid the upper-level optimization degenerating into a fixed scalar, the gradients of empirical risks in the lower-level optimization are not being computed through them. However, in practice, the learning dynamics over time tend to make the scale vector inconsistent over iterations [12; 16], which is not conducive to aligning learning progress. We therefore introduce _smooth-alignment_ to update the scale vector with historical information from previous iterations.

**Smooth-alignment**. To avoid sudden fluctuations of scale vectors over iterations, we introduce the smoothness vector \(_{t}(_{1:t})=q_{t}^{1}(_{1:t}),q_{t}^ {2}(_{1:t}),,q_{t}^{M}(_{1:t})^{} ^{M}\), which smooths the updating of the scale vector with previous risk information. Thus, the smoothness vector can deal with asynchronous learning dynamics over time, which helps the model reduce the imbalance of training across tasks. To be specific, we compute the smoothness vector by a normalized exponential moving average as follows:

\[_{t}(_{1:t})=_{t-1}(_ {1:t-1})-}(_{t}),\] (6)

where \(\) denotes the element-wise multiplication and \([]\) normalizes the sum of all smoothness elements to be \(1\). \(\) is a temperature hyperparameter to control the influence of current risk information.

Note that when \(\) is close to zero, each element in the smoothness vector will degrade to a fixed value \(\), which does not capture any historical information to group indicators.

**Risk-guided Group Indicators**. By element-wise multiplying the scale vector in Eq. (5) and the smoothness vector in Eq. (6), we obtain the group indicators with sufficient risk information as:

\[_{t}(_{t})=_{t}(_{t})_{t}(_{1:t}).\] (7)

Then, with the group indicators \(_{t}(_{t})\), we optimize the dynamical group assignment in Eq. (3) to assign tasks into groups in the lower-level optimization.

For each group indicator, the role of \(_{t}(_{t})\) in Eq. (5) and \(_{t}(_{1:t})\) in Eq. (6) differs in optimization: the smoothness vector requires the accumulated loss information from previous iterations, while the scale vector are independent of iterations. Thus, the smoothness vector can iteratively exploit more consistent task correlations to better align learning progress across tasks. The experimental section will show that the risk-guided group indicators empirically boost the proposed adaptive group risk minimization in aligning learning tasks.

## 4 Related Work

**Multi-Task Optimization.** Multi-task optimization addresses the task imbalance issue in multi-task learning, where each task usually influences a shared network differently. Task-imbalance in MTO [1; 27] refers to imbalanced optimization rather than uneven data distributions in the task space. According to different manipulations in optimization, we roughly divide MTO methods into two branches: (i) _gradient-oriented_ methods, which solve the task balancing problem by fully utilizing the gradient information of the shared network from different tasks. Some studies report impressive performance based on Pareto optimal solutions , gradient normalization , gradient conflicting , gradient sign Dropout , conflict-averse gradient , Nash bargaining solution . However, most gradient manipulation methods usually suffer from high computational cost . (ii) _loss-oriented_ methods, which reweight task-specific losses with the help of inductive biases from the loss space, e.g., using homoscedastic uncertainty , task prioritization , self-paced learning , similar learning paces [6; 14], random loss weight . Although loss-oriented methods are more computationally efficient, they often underperform gradient-oriented ones in most multi-task benchmarks. This paper tries to trade off the overall performance and computational efficiency.

Recent work  weights tasks under the meta-learning setup but has lower training-time efficiency for large-scale systems with high dimensional parameter space, such as deep neural networks, limiting their applications for dense prediction tasks in MTL. The closest method to ours is the recent work FAMO , which balances task-specific losses by decreasing task loss approximately at an equal rate. However, _GO4Align_ proposes a new MTL optimizer that dynamically aligns learning progress across tasks by introducing group-based task interactions.

**Multi-Task Grouping.** Multi-task grouping [11; 21; 31] assigns tasks into different groups and trains intra-group tasks together in a shared multi-task network. Previous work  first evaluates the transferring gains for \(2^{M}-1\) candidate multi-task networks (\(M\) is the number of tasks) and then conducts the brute-force search for the best grouping. Some works follow high-order approximation (HOA)  to reduce the prohibitive computational cost. However, they also suffer from inaccurate estimations due to non-linear relationships between high-order gains and corresponding pairwise gains . Meanwhile, Yao et al.  represents a clustered multi-task learning method, which clusters tasks into several groups by learning the representative tasks. The benefit of multi-task grouping is performance gains by training similar tasks together, and this inspires us to capture helpful group information in multi-task optimization. Also, rather than employing different multi-task networks, _GO4Align_ introduce group-based task interactions in scaling for multi-task alignment. Moreover, we share a high-level idea of task clustering with . However, task clustering in  is limited to pairwise relationships among tasks; meanwhile, our work allows grouping-based task interactions, thus capturing more complex relationships among tasks.

## 5 Experiments

### Comparisons on MTL Benchmarks

**Datasets and Settings.** We conduct experiments on four benchmarks commonly used in multi-task optimization literature [6; 13; 14; 16]: NYUv2 , CityScapes , QM9 , and CelebA . For all benchmarks, we follow the training and evaluation protocol in [13; 14].

**Baselines.** We compare _GO4Align_ with a single-task learning baseline, six gradient-oriented methods, and six loss-oriented methods. Note that single-task learning (STL) trains an independent deep network for each task. The gradient-oriented methods include MGDA , PCGrad, CAGrad, IMTL-G , GradDrop, and NashMTL . As for the loss-oriented methods, they are Linear scalarization (LS), Scale-invariant (SI), Dynamic Weight Average (DWA) , Uncertainty Weighting (UW) , Random Loss Weighting (RLW) , and FAMO . Detailed information about datasets and baselines is in Appendix B.

**Evaluations.** Following previous work [13; 16; 40], we report two MTL metrics that demonstrate the overall performance over various task-specific metrics: (1) \(\%\) is the average per-task performance drop relative to STL. We assume there are \(S\) metrics for all tasks. \(^{s}\) denote the \(s\)-th metric value of a multi-task method, while \(^{s}\) is the corresponding metric value of the STL baseline. Thus, we formulate the average relative performance drop as:\(\%=_{s=1}^{S}(-1)^{^{s}}^{s} -^{s})}{^{s}}\), where \(^{s}=1\) if higher values for the \(s\)-th metric are better and \(0\) otherwise. (2) \(=_{s=1}^{S}(^{s})\) is the average rank of all task-specific metrics, where \((^{s})\) denotes the ranking of the \(s\)-th metric value of the model among all comparison methods. Note that in practice _the lower \(\%\) and_ **MR**_, the better overall performance_.

    &  &  &  \\   & & & & & & & Angle Dist \(\) & & & Within \(t^{s}\) & &  &  \\   & mIoU \(\) & Pix Acc \(\) & Abs Err \(\) & Rel Err \(\) & & & Mean & Median & 11.25 & 22.5 & 30 & \\  STL & 38.30 & 63.76 & 0.6754 & 0.2780 & 25.01 & 19.21 & 30.14 & 57.20 & 69.15 & - & - \\  MGDA & 30.47 & 59.90 & 0.6070 & 0.2555 & 24.88 & 19.45 & 29.18 & 56.88 & 69.36 & 7.00 & 1.38 \\ PCGrad & 38.06 & 64.64 & 0.5550 & 0.2325 & 27.41 & 22.80 & 23.86 & 49.83 & 63.14 & 9.00 & 3.97 \\ GradDrop & 39.39 & 65.12 & 0.5455 & 0.2279 & 27.48 & 22.96 & 23.38 & 49.44 & 62.87 & 7.89 & 3.58 \\ CAGrad & 39.79 & 65.49 & 0.5486 & 0.2250 & 26.31 & 21.58 & 25.61 & 52.36 & 65.58 & 5.33 & 0.20 \\ IMTL-G & 39.35 & 65.60 & 0.5426 & 0.2256 & 26.02 & 21.19 & 26.20 & 53.13 & 66.24 & 4.56 & -0.76 \\ NashMTL & 40.13 & **65.93** & **0.5261** & 0.2171 & 25.26 & 20.08 & 28.40 & 55.47 & 68.15 & 2.89 & -4.04 \\  LS & 39.29 & 65.33 & 0.5493 & 0.2263 & 28.15 & 23.96 & 22.09 & 47.50 & 61.08 & 9.89 & 5.59 \\ SI & 38.45 & 64.27 & 0.5354 & 0.2201 & 27.60 & 23.37 & 22.53 & 48.57 & 62.32 & 8.78 & 4.39 \\ RLW & 37.17 & 63.71 & 0.5759 & 0.2410 & 28.27 & 24.18 & 22.26 & 47.05 & 60.62 & 12.22 & 7.78 \\ DWA & 39.11 & 65.31 & 0.5510 & 0.2285 & 27.61 & 23.18 & 24.17 & 50.18 & 62.39 & 8.67 & 3.57 \\ UW & 36.87 & 63.17 & 0.5446 & 0.2260 & 27.04 & 22.61 & 23.54 & 49.05 & 63.65 & 8.33 & 4.05 \\ FAMO & 38.88 & 64.90 & 0.5474 & 0.2194 & 25.06 & 19.57 & 29.21 & 56.61 & 68.98 & 4.33 & -4.10 \\ _GO4Align_ & **40.42** & 65.37 & 0.5492 & **0.2167** & 24.76 & 18.94 & **30.54** & 57.87 & **69.84** & **2.11** & **-6.08** \\   

Table 1: **Results on NYUv2 (3 tasks). The upper and lower tables categorize baseline methods into gradient-oriented and loss-oriented types, respectively. Each experiment is repeated over 3 random seeds, and the mean is reported. The best average result is marked in bold. MR and \(\%\) are the main metrics for overall MTL performance. Metrics with \(\) denote that the lower the better.**

Figure 4: **Efficiency comparisons on training time. Each method’s training time is relative to a simple baseline method with Eq. (1), which minimizes the sum of task-specific empirical risks.**

**Effectiveness Comparison.** We provide performance comparisons on NYUv2 in Table 1. In this benchmark, our method achieves the best overall MTL performance among both gradient-oriented and loss-oriented methods. We observe that our work is the only one that improves each task's performance relative to the corresponding STL performance. This suggests that grouping-based task interactions can adequately alleviate the imbalance of learning progress across tasks.

The experimental results on QM9, CityScapes and CelebA are reported in Table 2. _GO4Align_ obtains the lowest \(\%\) on QM9. It also shows comparable performance with FAMO on CityScapes, one possible reason could be that this dataset only contains \(2\) tasks, which limits the potential of the grouping mechanism in our method. In CelebA, even though our work does not achieve the lowest average performance drop, it outperforms all loss-oriented methods, which further verifies the effectiveness of the proposed method.

**Efficiency Comparison.** To show the computational efficiency, in Fig. 4, we report the average training time per epoch over 5 epochs for each method. We choose LS as the relative baseline for training time (cf. RLW  and FAMO ) as it is a commonly used MTL baseline with equal weights for each task, and it does not require additional loss-oriented or gradient-oriented techniques.

We note that we run all experiments on an NVIDIA A100 and the code of baseline methods comes from Liu et al.  and Navon et al. .

As shown in this figure, the proposed method _GO4Align_, as a new member of the loss-oriented branch, can perform more efficiently than most gradient-oriented methods. Moreover, when the number of tasks scales up from \(2\) to \(40\), the reduction in computational cost between our method and other gradient-oriented methods becomes increasingly significant, e.g., NashMTL (\(2.07\)) versus Ours (\(1.01\)) with \(2\) tasks, NashMTL (\(12.49\)) versus Ours (\(1.01\)) with \(40\) tasks. The main reason is that the training time of gradient-oriented methods is proportional to the number of tasks, but our work can avoid this. More experimental results are provided in Appendix C.

### Ablation Study

The effectiveness and efficiency of our proposed method are shown in Sec. 5.1. Next, we answer the following questions with our ablation study: (1) Can we quantify the contributions of each phrase? (2) Can we disentangle the roles of the group assignment and group weights? (3) Can the proposed AGRM principle seamlessly integrate with existing MTO methods? (4) Are there practical ways to appropriately configure hyperparameters, _e.g._, group number \(K\)? (5) Why do we choose the K-means clustering in the proposed method?

**Contributions of Each Phase.** To quantify the contributions of each phase in achieving the proposed AGRM principle on NYUv2, we report the detailed performance of our method in each phase. As the grouping is performed by Eq.(4), the first two rows in Table 3 are the variants of our method without grouping, and the last row is our method with grouping. Table 3 empirically examines the performance gains of the variant with task grouping over without grouping.

In detail, compared with the scale vector in Eq. (5), the smoothness vector in Eq. (6) can compromise the performance of the "Normal" and "Depth" tasks, however, scarifying that of "Seg.". Based on the scale and smoothness vectors, the proposed method employs dynamical group assignment in Eq. (4) to exploit the grouping-based task interactions, thus well aligning the learning progress of similar tasks "Depth" and "Seg.". We also observe that our method with both phases can improve the task-specific performance relative to STL. This demonstrates that each phase in the method complements each other, resulting in more balanced performance across tasks.

    &  &  &  \\   & **\#S** & **\(\%\)** & **\(\%\)** & **\(\%\)** & **\(\%\)** & **\(\%\)** \\  MMDA & 7.73 & 128.5 & 18.00 & 44.41 & 10.05 & 14.85 \\ PCGrad & 6.09 & 125.7 & 6.25 & 18.99 & 6.05 & 3.17 \\ CAGAJB & 7.09 & 112.8 & 5.00 & 11.64 & 5.65 & 2.48 \\ INTL-C & 5.91 & 77.2 & 4.00 & 11.10 & 4.08 & **6.54** \\  LS & 8.00 & 717.6 & 8.00 & 14.11 & 5.55 & 4.15 \\ SL & 5.00 & 77.8 & 8.50 & 14.11 & 7.10 & 7.29 \\ BLW & 9.36 & 203.8 & 7.75 & 24.38 & 4.60 & 1.46 \\ DWA & 7.64 & 175.3 & 6.00 & 21.48 & 6.25 & 1.29 \\ UW & 6.64 & 105.0 & 5.75 & 8.99 & 5.18 & 5.23 \\ FAMO & 4.73 & 58.5 & 5.50 & 8.13 & 4.10 & 1.21 \\ _GO4Align_ & 4.35 & 52.7 & 7.00 & 8.11 & **3.10** & 0.58 \\   

Table 2: **Comparisons on QM9 (11 tasks), CityScapes (2 tasks) and CelebA (40 tasks). Detailed results are in Appendix C.**

    &  &  & \(\%\)** } & \(\%\)** } & \(\%\)** } & \(\%\)** } & \(\%\) \\  ✓ & & & -0.02 & -21.76 & 13.14 & 2.46 \\ ✓ & ✓ & & 14.22 & -15.27 & 2.52 & 1.16 \\ ✓ & ✓ & ✓ & -4.03 & -20.37 & -4.18 & **4.08** \\   

Table 3: **Effectiveness of each phase in _GO4Align_ on NYUv2. ✓ denote whether the component joins the pipeline.**

**Influence of Group Assignment Matrix.** To explore the influence of the group assignment matrix \(\), we assume the group number is \(2\) on NYUv2 and make comparisons with several variants, which have various group assignments with fixed group weights \(=[0.8,0.1]\). As shown in Fig. 5 (a-c), grouping "Seg." and "Depth" outperforms other options. The main reason could be that these two tasks are very similar and far away from the "Normal" task . We observe that variant (d) with a random grouping strategy shows lower performance than the fixed grouping options (a) and (c), which further implies the importance of appropriate group assignment. It is worth mentioning that the proposed method in (e) without the prior information of the appropriate group assignment also captures such task correlations and each task can get performance gains compared with STL. This demonstrates that group assignment plays an important role in exploring task correlations over time in the proposed method.

**Influence of Group Weights.** To study the influence of group weights, we conduct another visualization in Fig. 5 (f-i), where we focus on various group weights \(\) with the "optimal" group assignment matrix \(}=\{;,\}\). We observe that with the fixed group assignment matrix, group weights have effects on the extent of compromising among different groups. On the NYUv2 dataset, lower weights for the first groups obtain better overall performance. The variant method (i) with random group weights achieves surprising performance, \(1.75\%\), in terms of the average relative performance drop. (e) shows that our method also tends to dynamically weight the first group with a high value. This demonstrates that group weights are necessary to align the learning progress of different groups over time.

**Effect of Adaptive Group Risk Minimization Principle.** Empirically, the proposed adaptive group risk minimization principle (AGRM) in Sec. 3.1 can be seamlessly integrated with existing MTO methods, taking their updated task weights as group indicators. As detailed in Table 4, MTO methods combined with AGRM consistently show improved performance. MGDA with adaptive group risk minimization achieves the biggest improvement gap. Moreover, our method still outperforms others. The reason could be that the designed risk-guided group indicators are more suitable for AGRM by balancing risk scales and exploiting historical information from previous iterations.

  
**Methods** & \(}\%}\%}\%}\%}\%\) \\  MGDA & 13.25 & -9.11 & 0.83 & 1.38 \\ MGDA + AGRM & 6.06 & -11.69 & -1.14 & -1.89 \\  NastimMTL & -4.09 & -22.01 & 3.15 & -4.04 \\ NastimMTL + AGRM & -7.75 & -20.14 & 3.60 & -4.20 \\  FAMO & -1.65 & -20.02 & 1.29 & -4.10 \\ FAMO + AGRM & 1.76 & -21.17 & -0.03 & -4.32 \\  _GOALign_ & -4.03 & -20.37 & -**1.18** & -**6.08** \\   

Table 4: **Comparisons of existing MTO methods with the proposed AGRM on NYUv2.**

Figure 5: **Comparative analysis of the influence of the group assignment matrix and group weights on NYUv2. The x-axis in the subplots denotes the epoch, and the intensity of the color indicates the weight value. (a-d) have fixed group weights \(=[^{1},^{2}]\) but various group assignment matrices \(\). (f-i) have various group weights \(\) but a fixed group assignment matrix \(\). (e) is our method that dynamically exploits a group assignment matrix and group weights for each iteration. The right side of each method shows relative performance drops on each task and their average one.**

**Influence of Group Number.** In the proposed method, group number \(K\) is an important hyperparameter, especially when we instantiate the clustering process in dynamical group assignment with \(K\)-means. In this case, there are many different techniques for choosing the right \(K\). To be visualizable, here we apply the conventional elbow method. As shown in Fig. 6, the overall performance (lower is better) of our method in (a) and (b) drops at \(2\) and \(5\), respectively, after that both reach a plateau when the group numbers increase. Thus, in this paper we set \(K=2\) and \(K=5\) for NYUv2 and QM9.

**Effect of different clustering methods.** In our main experiments, we employed standard K-means for instantiation. K-means is a widely used clustering approach. To investigate the effect of different clustering methods, we evaluate the impact of using alternative clustering algorithms.

Specifically, we tested our proposed method on NYUv2 by substituting K-means with SDP-based clustering  and spectral clustering . As demonstrated in Table 5, these alternative clustering methods also outperform state-of-the-art approaches (FAMO, \(-4.10\%\)), particularly by enhancing the performance of each task over STL. Interestingly, our experiments show that the K-means clustering algorithm outperforms spectral and SDP-based clustering methods.

## 6 Conclusion

**Technical Discussion.** This paper focuses on the task imbalance issue in MTO. Previous MTO methods suffer from either intensive computations or non-competitive performance. Our proposed _GO4Align_ addresses the issue by aligning learning progress across tasks with the help of the AGRM principle. In problem-solving, we present a tractable optimization pipeline, which incorporates grouping-based task interactions into the loss scaling of MTO.

**Limitation.** The main limitation of this work is the heuristic configuration of the group numbers. Although the search space is significantly smaller than some grouping multi-task methods , it still needs maximum \(M\) runs to find the best number. Some related techniques  automatically set the group number can be added to avoid this limitation in future work.

**Broader Impact.** This paper is the first to consider task grouping in multi-task optimization with deep multi-task models. We propose a simple and principled way to fasten multi-task optimization with better training-time efficiency, which has many potential societal impacts, especially in dense prediction tasks. We provide the code for our method to encourage follow-up work.3