# Deep Gaussian Markov Random Fields for

Graph-Structured Dynamical Systems

 Fiona Lippert

University of Amsterdam

f.lippert@uva.nl

&Bart Kranstauber

University of Amsterdam

b.kranstauber@uva.nl

E. Emiel van Loon

University of Amsterdam

e.e.vanloon@uva.nl

&Patrick Forre

University of Amsterdam

p.d.forre@uva.nl

###### Abstract

Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman filter based approaches.

## 1 Introduction

Consider the problem of monitoring air pollution, the leading environmental risk factor for mortality worldwide . In the past years, sensor networks have been installed across major metropolitan areas, measuring the concentration of pollutants across time and space . These measurements are, however, prone to noise or might be missing completely due to hardware failures or limited sensor coverage. To identify sources of pollution or travel routes with limited exposure, it is essential to recover pollution levels and provide principled uncertainty estimates for unobserved times and locations. Similar problems occur also in the geosciences, ecology, neuroscience, epidemiology, or transportation systems, where animal movements, the spread of diseases, or traffic load need to be estimated from imperfect data to facilitate scientific discovery and decision-making. From a probabilistic inference perspective, all these problems amount to estimating the posterior distribution over the latent states of a spatiotemporal system given partial and noisy multivariate time series data.

To make inference in these systems feasible, it is common to assume a state-space model, where the latent states evolve according to a Markov chain. Then, the classical Kalman filter (KF)  or its variants  can be used for efficient inference, scaling linearly with the time series length. However, the complexity with respect to the state dimension remains cubic due to matrix-matrix multiplications and inversions. Thus, as the spatial coverage of available measurements and thereby the dimensionality of the state variables increase, KF-based approaches quickly become computationally prohibitive. This is especially problematic in cases where the underlying dynamics are (partially) unknown and need to be learned from data, requiring repeated inference during theoptimization loop [21; 40; 9]. If, additionally, the access to historical training data is limited, it is essential to incorporate prior knowledge for learning and inference to remain both data-efficient and computationally feasible .

For spatiotemporal systems, prior knowledge is often available in the form of graphs, representing the dependency structure of state variables. For air pollution, this could be the city road network and knowledge about wind directions which influence the spread of particulate matter. Incorporating this knowledge into the initial state distribution and the transition model of a state-space model results in a Dynamic Bayesian network (DBN) [12; 39; 56], an interpretable and data-efficient graphical model for multivariate time series data. If the joint space-time graph is sparse and acyclic, belief propagation methods allow for fast and principled inference of marginal posteriors in DBNs, scaling linear with the number of edges [45; 39]. However, when the underlying dynamics are complex, requiring denser graph structures and flexible error terms with a loopy spatial structure, the scalability and convergence of these methods is no longer guaranteed.

Combining the favorable statistical properties of classical graphical models with the flexibility and scalability of deep learning offers promising solutions in this regard . Deep Gaussian Markov random fields (DGMRF), for example, integrate principled inference in Gaussian Markov random fields (GMRF) with convolutional  or graph layers  to define a new flexible family of GMRFs that can be learned efficiently by maximizing a variational lower bound, while facilitating principled Bayesian inference that remains computationally feasible even for high-dimensional complex models with dense dependency structures. While in principle DGMRFs can be applied to spatiotemporal systems by treating each time step independently, they are by design limited to spatial structures.

In this paper we extend the DGMRF framework to spatiotemporal systems with graph-structured states, transitions and noise terms. To this end, we formulate a dynamical prior based on a DBN that relates state variables over adjacent time steps, and DGMRFs that capture the spatial structure of unmodeled exogenous influences and errors in the transition model. The key to our approach is the insight that, using locally linear Gaussian transition models, the precision matrix of the joint space-time process preserves the structure of transition and noise precision matrices. This is in contrast to the marginal distributions used in Kalman filter based approaches for which both precision and covariance matrices become dense over time. A convenient factorization of the joint precision matrix then leads to a spatiotemporal DGMRF formulation that is equivalent to the generative state-space formulation, but inherits the favorable properties of DMGRFs for learning and inference.

### Related work

The simplest approach to scaling Kalman filtering and smoothing to high-dimensional state variables is to assume a decomposition into subprocesses, that can be reconstructed independently . For spatiotemporal processes, however, this assumption can be detrimental. Sample-based approaches, like the Ensemble KF [17; 18; 26; 31], instead approximate state distributions with Monte Carlo estimates, balancing computational complexity and accuracy. This is widely used in the geosciences [53; 55] where accurate models of the underlying process are available. If the dynamics are (partially) unknown, the parameters are typically estimated jointly with the system states via state augmentation . However, this becomes problematic if the transition model is only a poor approximation and complex error covariances need to be estimated [15; 52]. Alternatively, expectation maximization

Figure 1: ST-DGMRF overview. We reconstruct the latent states of a graph-structured dynamical system from partial and noisy observations (orange arrow with question mark). Temporal and spatial layers transform the state \(\) to a standard Gaussian, which implicitly defines a space-time GMRF prior.

(EM) is used to estimate parameters iteratively , which requires repeated ensemble simulations and thus quickly becomes computationally demanding.

In machine learning, neural network-based approaches map high-dimensional data to a latent space that is either low-dimensional  or factorizes conveniently , such that standard KF inference remains feasible. To learn suitable maps, however, these methods require sufficiently large training data sets. Moreover, the projection to an abstract latent space hinders the incorporation of structural or functional prior knowledge. Alternatively, variational inference allows for fast inference in (deep) state space models [20; 3; 28; 34; 54] by approximating the true posterior with a computationally convenient variational distribution. While this approach is highly flexible and scalable, it is known to severely underestimate uncertainties , which can be detrimental for science and decision-making.

Gaussian processes (GP) are another widely used model class for spatiotemporal inference . In contrast to KF-based approaches, GPs encode prior knowledge into their covariance kernel. To increase the expressivity beyond standard kernels and overcome computational limitations, GPs have been combined with deep learning [14; 13]. However, scaling GPs to large-scale spatiotemporal processes with graph structure remains challenging. Scalable approaches for temporal data involve reformulating GPs as state-space models and applying KF-based recursions [24; 49; 10; 1; 58; 23] which, as discussed before, remains computationally prohibitive for large state spaces. Extensions to graph-structured domains [8; 41] rely on the relation between GPs and stochastic differential equations (SDE) [36; 51], which makes it cumbersome to design new kernels or to incorporate prior knowledge that cannot be formulated as an SDE.

### Our contributions

In contrast to most previous approaches, we focus on settings where the dynamics are (partially) unknown and need to be learned from data, while historical training data is limited or even unavailable. Our contributions can be summarized as follows:

1. We propose ST-DGMRF, which extends the DGMRF framework to spatiotemporal systems, by reformulating graph-structured state-space models as joint space-time GMRFs that are implicitly defined by simple spatial and temporal graph layers.
2. We show that the multi-layer space-time formulation facilitates efficient learning and inference, scaling linearly w.r.t. the number of layers, time series length, and state dimensionality.
3. In experiments on synthetic and real world data, we demonstrate that our approach provides accurate state and uncertainty estimates, comparing favorably to other scalable approaches relying on ensembles or simplifications of the dependency structure.

## 2 Preliminaries

### Problem formulation

Consider a single sequence of high-dimensional measurements \(_{0:K}=(_{0},,_{K})\) taken at consecutive time points \(t_{0},,t_{K}\). Each \(_{k}^{M_{k}}\) represents partial and noisy observations (e.g. from spatially distributed sensors) generated according to a linear Gaussian noise model

\[_{k}=_{k}_{k}+_{k}, _{k}(,_{k})\] (1)

from the latent state \(_{k}^{N}\) of an underlying dynamical process of dimensionality \(N M_{k}\) (e.g. air pollution along \(N\) roads within an urban area). It is common to assume either \(_{k}=(_{k,1}^{2},,_{k,M_{k}}^{2})\), representing independent sensors with varying noise levels, or \(_{k}=^{2}_{M_{k}}\), representing a fixed noise level across all sensors and time points . Further, the observation matrices \(_{k}^{M_{k} N}\) are typically very sparse with \((M_{k})\) number of non-zero entries.

Assuming that all \(_{k}\) are known, we aim at reconstructing the latent system states \(_{0:K}=(_{0},,_{K})^{(K+1)  N}\) from data \(_{0:K}\). Since observations are partial and noisy, \(_{0:K}\) cannot be recovered without ambiguity. It is therefore essential to treat the problem probabilistically and incorporate available domain knowledge into the prior distribution \(p(_{0:K})\). The task then amounts to (i) specifying a suitable prior that captures the spatiotemporal structure of \(_{0:T}\), (ii) estimating unknown parameters, and (iii) computing the posterior distribution \(p(_{0:K}_{0:K},})\).

NotationWe use \(=(_{0:K})^{(K+1)N}\) and \(=(_{0:K})^{M}\) with \(M=_{k=0}^{K}M_{k}\) to denote vectorized states and measurements. Similarly, we use \(=(_{0},,_{K})^{M (K+1)N}\) and \(=(_{0},,_{K})^{M  M}\) to denote the joint spatiotemporal observation and covariance matrix.

In the following sections, we briefly introduce two graphical models, linear dynamical systems and Gaussian Markov random fields, which form the temporal and spatial backbone of our method. We then introduce the deep GMRF framework on which we build in Section 3 to formulate a flexible multi-layer spatiotemporal prior.

### Linear dynamical system

A discrete-time linear dynamical system (LDS) is a state-space model that defines the prior over states \(_{0:K}\) in terms of linear Gaussian transition models

\[_{k}=_{k}_{k-1}+_{k}+_{k },_{k}(,_{k}^{-1})  k\{1,,K\}\] (2)

and an initial distribution \(_{0}(_{0},_{0}^{-1})\). The classical Kalman filter  and its variants [47; 37] use recursive algorithms that exploit the temporal independence structure of the prior and its conjugacy to the observation model (see Eq. (1)) to compute marginal posterior distributions \(p(_{k}_{0:k})\) in \((KN^{3})\). The same formulas can be used within the EM-algorithm to learn unknown parameters \(_{0},_{0},_{k},_{k},_{k}^{-1}\) in \((JKN^{3})\) for \(J\) iterations. While this scales favorably for long time series, it remains computationally prohibitive for high-dimensional systems with \(N K\).

### Gaussian Markov random fields

A multivariate Gaussian \((,^{-1})\) forms a GMRF  with respect to an undirected graph \(=(,)\) if the edges \(\) correspond to the non-zero entries of the precision matrix \(\), i.e. \(_{ij} 0(i,j)\). Given a linear Gaussian observation model \((,)\), the posterior can be written in closed form \(p(,})=(^{+},(^{+})^{-1})\) with precision matrix \(^{+}=+^{T}^{-1}\) and mean \(^{+}=(^{+})^{-1}(+^{T}^{-1})\).

However, naively computing \(^{+}\) by matrix inversion requires \((K^{3}N^{3})\) computations and \((K^{2}N^{2})\) memory, which is infeasible for high-dimensional systems. Instead, the conjugate gradient method  can be used to iteratively solve the sparse linear system \(^{+}^{+}=+^{T}^{-1} \) for \(^{+}\). The same approach allows to generate samples \(}(^{+},(^{+})^{-1})\) and obtain Monte Carlo estimates of marginal variances . In practice, however, it remains difficult to design suitable precision matrices that are expressive but sparse enough to remain computationally feasible.

### Deep Gaussian Markov random fields

A DGMRF  is defined by an affine transformation

\[=_{}()=_{}+ _{}(, ),\] (3)

where \(_{}=_{}^{(L)}_{ }^{(1)}\) is a composition of \(L\) simple linear layers with convenient computational properties. This implicitly defines a GMRF \((,^{-1})\) with \(=-_{}^{-1}_{}\) and \(=_{}^{T}_{}\). The multi-layer construction facilitates fast and statistically sound posterior inference using the conjugate gradient method, as well as fast parameter learning using a variational approximation, even when the resulting precision matrix \(\) becomes dense.

While originally,  defined their layers only for lattice-structured data,  generalized this to arbitrary graphs. Central to their approach is that each layer \(^{(l)}=_{}^{(l)}^{(l-1)}+_{ }^{(l)}\) is defined on a sparse base graph \(}\) with adjacency matrix \(\) and degree matrix \(\) such that

\[^{(l)}=_{l}^{_{l}}+_{l}^{_{l }-1}_{}^{(l)}=b_{l}\] (4)

with parameters \(_{l}=(_{l},_{l},_{l},b_{l})\). This construction allows for fast log-determinant computations during learning, and GPU-accelerated computation of \(_{}()\) using existing software for graph neural networks. Note that for \(L\) layers defined on \(}\), the sparsity pattern of \(\) corresponds to the sparsity pattern of \((+)^{2L}\). And thus, the resulting model defines a GMRF w.r.t the \(2L\)-hop graph of \(}\).

Spatiotemporal DGMRFs

To extend the DGMRF framework to spatiotemporal systems, we first formulate a dynamical prior in terms of a GMRF that encodes prior knowledge in the form of spatial and temporal independence assumptions. We then parameterize this prior using simple spatial and temporal layers, which results in a flexible model architecture that facilitates efficient learning and principled Bayesian inference in high-dimensional dynamical systems, scaling favorably compared to Kalman filter based methods.

### Graph-structured dynamical prior

Consider a dynamical system for which the state evolution is well described by Eq. (2). We say that this process is graph-structured if each dimension \(d_{i}\) of the system state \(_{k}\) can be associated with a node \(i\) in a multigraph \(=(,_{},_{})\), where the set of undirected spatial edges \(_{}\) defines the sparsity pattern of noise precision (inverse covariance) matrices

\[(_{k})_{ij} 0( _{k})_{ji} 0(j,i) _{} k\{0,,K\},\] (5)

and the set of directed temporal edges \(_{}\) defines the sparsity pattern of the state transition matrices

\[(_{k})_{ij} 0(j,i )_{} k\{1, ,K\}.\] (6)

This defines a DBN over \(_{0:K}\) encoding conditional independencies of the form

\[x_{k,i}\!\!\!_{k-1, n_{}(i)}_{k-1,n_{}(i)} x_{k,i} \!\!\!_{l,}_{k-1,n_{}(i)}\; l<k-1,\] (7)

where \(n_{}(i)\) denotes the set of neighbors \(j\) of \(i\) for which \((j,i)_{}\). Intuitively, \(_{}\) represent causal effects over time, while \(_{}\) represent the structure of random effects that are not captured by the transition model. To define the graph structure, prior knowledge about, for example, the physical system structure or underlying causal mechanisms can be exploited.

#### 3.1.1 Joint distribution

The graph-structured dynamical prior induces a multivariate Gaussian prior \((,^{-1})\) on \(\) with sparse precision matrix \(^{(K+1)N(K+1)N}\). Importantly, \(\) can be shown to factorize as \(^{T}\) with block diagonal matrix \(\) and unit lower block bidiagonal matrix \(\) defined as

\[=(_{0},_{1},,_{K}), :=&&&\\ -_{1}&&&\\ &&&&\\ &&-_{K}&,\] (8)

where empty positions represent zero-blocks. Further, while the mean \(^{(K+1)N}\) needs to be computed iteratively as \(_{k}=_{k}_{k-1}+_{k}\), the information vector \(=\) can be expressed compactly as

\[=^{T}=^{T}\] (9)

with \(=[_{0},_{1},,_{K}] ^{(K+1)N}\). See Appendix A.1 for detailed derivations.

#### 3.1.2 DGMRF formulation

We now reformulate \((,^{-1})\) as a DGMRF. Importantly, in contrast to [50; 43], the graph-structured dynamical prior allows us to impose additional structure, reflecting the spatiotemporal nature of the underlying system. In particular, since \(_{0},_{1},,_{K}\) are symmetric positive definite, we can factorize \(=^{T}\) with symmetric block-diagonal matrix \(\), and thus \(=^{T}^{T}\). Together with Eq. (9), this results in a GMRF defined by

\[_{}_{}-_{}=-\] (10)

Finally, we separate \(_{}\) into a temporal map \(_{}:^{(K+1)N}^{(K+1)N}\) and a spatial map \(_{}:^{(K+1)N}^{(K+1)N}\) defined as

\[_{}()+_{f}= _{}() +_{s}=.\] (11)

Note that this results in an overall bias \(_{}=_{f}+_{s}\) and thus \(=-(_{f}+^{-1}_{s})\), which allows for modelling long-range spatial dependencies in \(_{0}\) and \(_{1},,_{K}\). The combined transformation \(=(_{}_{})()\) essentially defines a standard two-layer DGMRF, which describes a graph-structured dynamical system if the parameters \(=(_{s},_{f},_{1},, _{K},_{0},,_{K})\) are subject to sparsity constraints (5) and (6).

### Parameterization

Learning the unknown parameters \(\) directly from a single sequence \(_{0:K}\) will result in a highly over-parameterized model that is unsuitable for the problem of interest. In addition, careful parameterization of \(_{}\) can greatly reduce the computational complexity of the transformation and the associated log-determinant computations required during learning [50; 43]. To achieve a good trade-off between data-efficiency, expressivity and scalability, we define both \(_{}\) and \(_{}\) in terms of simple layers with few parameters and convenient computational properties.

#### 3.2.1 Spatial layer(s)

To enable fast log-determinant computations for arbitrary graph structures, we follow  and parameterize \(_{}\) in terms of \(K+1\) independent DGMRFs \(_{k}=_{k}_{k}+(_{s})_{k}\), each defining a GMRF w.r.t \(_{}=(,_{})\). Note that due to the multi-layer construction (see Section 2.4), \(_{}\) is implicitly defined by the base graph \(}_{}\) and the number of layers \(L\), with special case \(_{}=}_{}\) if \(L_{}=1\).

#### 3.2.2 Temporal layer(s)

Compared to \(_{}\), the definition of \(_{}\) is much less constrained as it does not affect the log-determinant computation (see Section 3.3.1), and should hence incorporate as much domain knowledge into transition matrices \(_{k}\) as possible. This could be, for example, based on conservation laws or knowledge about relevant covariates. To increase the flexibility in cases where the dynamics are unknown or involve long-range dependencies, each \(_{k}\) can again be decomposed into simpler layers \(_{k}^{(L_{})}_{k}^{(1)}\), each defined according to a temporal base graph \(}_{}\). Table 1 provides several example layers to give an idea of what is possible.

Note that any function can be used to define the entries of \(_{k}^{(l)}\), including neural networks taking available covariates or node/edge features as inputs . The associated parameters can simply be included in \(\). Similarly, non-linear dynamics can be approximated either through linearization akin to the Extended Kalman Filter , or through neural linearization [19; 7]. We, however, recommend sharing parameters where appropriate to avoid overparameterization.

Higher order Markov processesIt is straight forward to extend \(_{}\) to describe a \(p\)-th order Markov process

\[_{k}=_{=1}^{p}_{k,}_{k-}+ _{k}+_{k},(,_{k}^{-1}),_{0}(_{0}, _{0}^{-1}),\] (12)

where \(_{-}=\) for \(=1,,p\). This can be done by introducing edges \(_{(2)},,_{(p)}\) and adding the corresponding higher-order transition matrices \((_{,},,_{K,})\) to the \(\)-th lower block diagonal of \(\) (see Appendix A.2 for derivations).

### Learning and inference

Although the marginal likelihood \(p()\) is available in closed form (see Section 2.3), maximum likelihood parameter estimation becomes computationally prohibitive in high dimensional settings

    & layer definition & properties of \(_{}\) \\  AR process & \(_{k}^{(l)}=_{k,l}\) & self-edges only \\ Diffusion & \(_{k}^{(l)}=_{k,l}+_{k,l}(-)\) & bidirected (symmetric \(\)) \\ Directed flow & \(_{k}^{(l)}=_{k,l}+_{k,l}(-_{})+_{k,l}(^{T}-_{})\) & directed \\  Advection & \((_{k}^{(l)})_{ij}=w_{ij}_{j}^{T} _{l}\) & discretized \(^{d}\) (e.g. triangulation), \\  & \((_{k}^{(l)})_{ii}=1-_{j(i)}(_{k}^{( l)})_{ij}\) & edge weights \(w_{ij}\) and normals \(_{ij}\) \\  Neural network & \((_{k}^{(l)})_{ij}=f_{}(_{i}, _{j},_{ij})\) & node and edge features \(_{i},_{ij}\) \\   

Table 1: Examples of linear transition layers \(_{k}^{(l)}\). The adjacency matrix \(\) can be symmetric or asymmetric, weighted or unweighted.

. Instead, a variational approximation is used to learn parameters \(}\) for large-scale DGMRFs. Then, the conjugate gradient method allows to efficiently compute the exact posterior mean and to draw samples from \(p(,})\).

#### 3.3.1 Scalable parameter estimation

Given a variational distribution \(q_{}()=(_{},_{})\), the parameters \(\{,\}\) are optimized jointly by maximizing the Evidence Lower Bound (ELBO)

\[(_{0:K},,) =_{q_{}()}[ p_{}()+_{k=0}^{K} p(_{k}_{k})]+H[q_{ }()]\] (13) \[=-_{q_{}()}[_{ }()^{T}_{}()+_{k=0}^{K}(_{k}-_{k}_{k})^{T}_{k}^{-1}(_{k}- _{k}_{k})]\] (14) \[+|(_{})|+| {det}(_{})|-_{k=0}^{K}|(_{k})|+,\] (15)

using stochastic gradient descent, where the expectation is replaced by a Monte-Carlo estimate based on samples \(} q_{}\).

Variational distributionTo facilitate efficient log-determinant computations and sampling via the reparameterization trick , we follow  and define \(q_{}\) as an affine transformation \(=_{}+_{}\) with \((,)\), resulting in \(_{}=_{}(_{})^{T}\). Then, \(_{}\) can be defined as a block-diagonal matrix \((_{0},,_{K})\) with \(_{k}=(_{k})}_{k}( _{k})\), where \(_{k},_{k}_{+}^{N}\) are variational parameters and \(}_{k}\) is based on spatial DGMRF layers as discussed in Section 3.2.1. To relax the temporal independence assumptions in \(_{}\), we propose an extension \(_{}=(_{0},,_{K})}\), where \(}\) has a similar structure to \(\) (see Eq. (8)). Again, the design of \(}_{k}\) is highly flexible as it does not enter the log-determinant computation.

Log-determinant computations proposed specific lattice and graph layers for which the associated log-determinants \(|(_{})|\) are computationally scalable. Conveniently, the spatiotemporal case does not require a new type of layer. Instead, we can build directly on top of existing spatial layers. Specifically, using Eq. (8)&(10), the log-determinant \(|(_{})|\) simplifies to

\[|()||()| _{k=0}^{K}|(_{k})|,\] (16)

where \((i)\) follows from \(\) being block-diagonal and \((ii)\) follows from \(\) being unit-lower triangular with \(()=1\). Note that each \(_{k}\) is defined by a spatial DGMRF of dimension \(N\) (see Section 3.2.1), for which efficient log-determinant methods have been developed. The same arguments hold for the variational distribution proposed above. Finally, due to the diagonality assumptions on \(_{t}\) (see Section 2.1), \(|(_{k})|\) simplifies to \(_{i=1}^{M_{k}}(_{i})\).

Computational complexityDuring training, \(|(_{})|\) and \(|(_{})|\) can be computed in \((KNL_{})\) using Eq. (16) together with the methods proposed in . Note that no complexity is added when introducing conditional dependencies between time steps. The necessary preprocessing steps, i.e. computing eigenvalues or traces, only need to be performed for the spatial base graph and are thus independent of the number of transitions \(K\). Finally, assuming an average of \(d_{}\) and \(d_{}\) edges per node in \(}_{}\) and \(}_{}\), the transformations \(_{}\) and \(_{}\) scale linearly with the number of nodes \(N\). In particular, \(_{}()=(_{}_{})( )\) can be computed in \((KNd_{}L_{}+KNd_{}L_{ })\), which dominates the computational complexity of the training loop. In addition, we can leverage massively parallel GPU computations to speed up this process even further.

#### 3.3.2 Exact inference with conjugate gradients

As discussed in Section 2.3, the conjugate gradient method can be used to iteratively compute the posterior mean and obtain Monte Carlo estimates of marginal variances. Importantly, each iteration is dominated by a single matrix-vector multiplication of the form

\[^{+}=^{T}^{T}+^{T}^{-1}.\] (17)

This amounts to a series of sparse matrix-vector multiplications with total computational complexity \((KNd_{}L_{}+KNd_{}L_{ })\), which is again linear in the number of nodes, time steps, and layers, and can be implemented in parallel on a GPU.

## 4 Experiments

We implemented ST-DGMRF in Pytorch and Pytorch Geometric, and conducted experiments on a consumer-grade GPU, leveraging parallel computations in both spatial and temporal layers. In all experiments, we optimize parameters for \(10\,000\) iterations using Adam  with learning rate \(0.01\), and draw 100 posterior samples to estimate marginal variances. Unless specified otherwise, we use \(L_{}=2\), \(L_{}=4\) and \(p=1\), and define the variational distribution based on one spatial layer and one temporal _diffusion_ layer. Additional details on our experiments are provided in Appendix B.

### Advection-diffusion process

We start with a synthetic dataset for which we have access to both the ground truth posterior distribution and transition matrix. The dataset consists of \(K=20\) system states that are sampled from an ST-DGMRF with time-invariant transition matrix \(_{}\). This matrix is defined according to the third-order Taylor approximation of an advection-diffusion process with constant velocity and diffusion parameters, discretized on a \(30 30\) lattice with periodic boundary conditions. From the sampled state trajectory, we generate observations with varying amount of missing data by removing pixels within a square mask of width \(w\) for 10 consecutive time steps and adding noise with \(=0.01\). For all experiments, we use the masked pixels as test set, and 10% of the observed pixels as validation set for hyperparameter tuning.

Two ST-DGMRF variants are considered: (1) using advection-diffusion matrices \(_{k}^{(l)}\) defined based on the first-order Taylor approximation of the ground-truth dynamics, with trainable diffusion and velocity parameters, and (2) replacing parts of these advection-diffusion matrices with small neural networks, taking the edge unit vector \(_{ij}\) pointing from pixel \(i\) to \(j\) as input. In both cases, \(}_{}\) and \(}_{}\) are defined as the 4-nearest neighbor graph. Further, as the underlying dynamics are time-invariant, we enforce \(_{k}^{(l)}=_{k+1}^{(l)}\) and \(_{k}^{(l)}=_{k+1}^{(l)}\)\( k 1\).

BaselinesWe compare our approach to a range of baselines accounting for varying degrees of spatial and/or temporal dependencies. In particular, we consider the original DGMRF applied to all time steps independently, an ARMA state-space model assuming spatial independence among the state variables, and a spatiotemporal AR state-space model (ST-AR) with spatially correlated error terms \(_{k}\) for which an unconstrained covariance matrix \(^{-1}\) is estimated using the EM algorithm. For both ARMA and ST-AR, we use the standard Kalman smoother  to obtain posterior estimates. Additionally, we consider two Ensemble Kalman Smoother (EnKS) variants, one with an advection-diffusion transition model matching the true data-generating process, and one using state augmentation to estimate the velocity and diffusion parameters jointly with the system states. Note that, in contrast to the ST-DGMRF approach, we consider initial and transition noise parameters to be fixed in order to avoid divergence of the EnKS.

Performance evaluationWe evaluate the estimated posterior mean and marginal standard deviations in terms of the root-mean-square-error (RMSE\({}_{}\) and RMSE\({}_{}\)) with respect to the ground truth posterior. In addition, we use the mean negative continuous ranked probability score (CRPS)  to evaluate the predictive distribution with respect to the masked out data. Table 2 shows that, by exploiting the spatiotemporal structure of the process, our ST-DGMRF variants provide much more

    & RMSE\({}_{}\) & RMSE\({}_{}\) & CRPS \\  ARMA & 2.3054 & 0.6812 & 1.7064 \\ ST-AR & 1.4595 & 1.9216 & 0.9707 \\ DGMRF & 0.5901 & 0.3808 & 0.3495 \\  EnKS & & & \\ _true dynamics_ & 0.0661 & 0.0046 & 0.1027 \\ _estimated dynamics_ & 0.1654 & **0.0039** & 0.1434 \\  ST-DGMRF (ours) & & & \\ _advection-diffusion_ & **0.0526** & 0.1146 & **0.0726** \\ _neural network_ & 0.0854 & 0.1402 & 0.0839 \\   

Table 2: Performance on the advection-diffusion data with \(w=9\). We report the mean over 5 runs with different random seeds.

accurate estimates than the purely spatial DGMRF, the purely temporal ARMA model, and the ST-AR model with highly simplified transitions and unstructured noise terms. As expected, the two EnKS variants with fixed noise parameters provide the most accurate uncertainty estimates. However, the CRPS scores indicate that overall our ST-DGMRF approach results in better calibrated posterior distributions. More detailed results are reported in Appendix C.

Increasing mask sizeTo evaluate the robustness to missing data, we analyze how posterior estimates change with increasing mask size. Figure 2 (right) shows the effect on RMSE\({}_{}\) when varying \(w\) from 6 to 12. Clearly, the purely spatial DGMRF suffers the most from expanding the unobserved region. In contrast, the ST-DGMRF variants continue to provide accurate state estimates that are on par with the EnKS using ground truth dynamics. Note that the decreasing errors for ST-AR can be attributed to the changing set of pixels used for evaluation.

Evaluation of learned transition modelsFinally, we assess how well the temporal ST-DGMRF layers can approximate the true data-generating dynamics, given varying levels of complexity in the transition model. For this purpose, we extract the stencil, i.e. the weights assigned to nearby pixels, from the ground truth and learned transition matrices and compare them in terms of absolute error and Pearson correlation. We find that the learned transition weights converge rapidly towards the true weights as the number of temporal layers increases (see Figure 3). For small \(L_{}\), the neural network based layers show a stronger agreement with the true dynamics, suggesting that some flexibility in the temporal layers helps to compensate for simplifications in the transition structure.

### Air quality data

To test our method on a real world system exhibiting more complex dynamics and graph structures, we conduct experiments on an air quality dataset obtained from . The dataset contains hourly PM2.5 measurements from 246 sensors distributed around Beijing, China, covering a time period of \(K=400\) hours. To ensure that predicted PM2.5 concentrations are non-negative, we model both system states and observations in log-space. The spatial and temporal base graphs are defined

Figure 3: Stencil comparison. Center: absolute error between true and learned transition weights for the neural network based ST-DGMRF. Right: Pearson correlation for increasing temporal depth.

Figure 2: Left: snapshot of the advection-diffusion data at time \(k\)=8, and reconstructions by the time-independent DGMRF and our ST-DGMRF with advection-diffusion \(_{k}\). Center: corresponding time series for a single pixel (marked on the left). Shaded areas represent posterior mean \(\) std of a single run. Right: RMSE\({}_{}\) as a function of the mask width (mean \(\) std over 5 runs).

based on the Delaunay triangulation, with edge weights proportional to the inverse distance between sensors. To mimic a realistic scenario of local network failures, we randomly choose 10 time points \(t_{k}\) and mask out all data points within a spatial block containing 50% of all sensors, for time steps \(t_{k},,t_{k}+20\) (see Figure 4). As before, we use the masked data for model evaluation, and use 10% of the remaining data as validation set. The observation noise is assumed to be uniform with \(=0.01\).

The spatiotemporal distribution of particulate matter is strongly influenced by atmospheric processes that transport and diffuse emitted particles. To incorporate this knowledge into the transition model, we extract temperature and wind conditions for all sensors and time points from the ERA5 reanalysis dataset  and feed them together with static graph features into a set of neural networks which transform them into spatially and temporally varying bias and velocity parameters. The matrices \(_{k}^{(l)}\) are then formed in the same way as in Section 4.1. In addition, we consider a simplified diffusion transition model (see Table 1) which cannot capture any directional transport processes.

ResultsNext to the baselines introduced in Section 4.1, we consider a multi-layer perceptron (MLP) mapping local weather features to the corresponding log-transformed PM2.5 concentration. Following , we also include weather features into the spatial DGMRF model by adding linear effects to the measurement model. Table 3 reports the resulting CRPS and the RMSE with respect to the masked out data. Clearly, the neural network based ST-DGMRF, accounting for time-varying directional transport processes, provides the most accurate state estimates. However, even with highly simplified _diffusion_ transitions our approach provides better estimates than the considered baselines. We attribute this to the expressive DGMRF noise terms which can capture complex error structures resulting from inaccuracies in the transition model. Increasing the Markov order from \(p=1\) to \(2\) clearly improves the resulting posterior estimates for both ST-DGMRF variants, reflecting the complexity of the modeled process. A similar trend is observed as we increase \(L_{}\) (see Appendix C).

## 5 Conclusion

We have presented ST-DGMRF, an extension to Deep Gaussian Markov Random Fields for inference in spatiotemporal dynamical systems with partial and noisy observations, (partially) unknown dynamics, and limited historical data. Our reformulation of graph-structured state-space models as multi-layer space-time GMRFs enables computationally efficient learning and inference even in high-dimensional settings, scaling linear w.r.t. the number of both time steps and state variables. Empirically, ST-DGMRF provides more accurate posterior estimates than other scalable approaches relying on simplifications of the dependency structure. While our approach relies on linear Gaussian assumptions, which can be restrictive for real systems, we find that expressive DGMRF noise terms can compensate (to a certain extend) for approximations in the transition model. In the future, non-linearities between the layers could be explored as discussed in . Further, we see potential in defining more flexible time-varying transition matrices based on a hierarchy of latent variables that again follow a ST-DGMRF.

Figure 4: ST-DGMRF posterior estimates (_neural network_\(_{k}\) and \(p=2\)). Left: time series of log-transformed and normalized PM2.5 levels for a masked sensor. Light red areas represent posterior mean \(\) std of a single run. Unobserved time points are marked with gray bars. Right: Marginal std for all sensors at \(k=100\). Masked out sensors (gray box) feature higher uncertainties.

    & p & RMSE & CRPS \\  ARMA & & 0.6820 & 0.3625 \\ ST-AR & & 0.7350 & 0.4261 \\ DGMRF & & 0.7456 & 0.4037 \\ MLP & & 0.8038 & â€“ \\  ST-DGMRF (ours) & & \\ _diffusion_ & 1 & 0.6190 & 0.3258 \\ _diffusion_ & 2 & 0.5928 & 0.3161 \\ _neural network_ & 1 & 0.5853 & 0.3092 \\ _neural network_ & 2 & **0.5565** & **0.2925** \\   

Table 3: Performance on the air quality data. We report the mean over 5 runs with different random seeds.