# Active Observing in Continuous-time Control

Samuel Holt

University of Cambridge

sih31@cam.ac.uk &Alihan Huyuk

University of Cambridge

ah2075@cam.ac.uk &Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

###### Abstract

The control of continuous-time environments while actively deciding when to take costly observations in time is a crucial yet unexplored problem, particularly relevant to real-world scenarios such as medicine, low-power systems, and resource management. Existing approaches either rely on continuous-time control methods that take regular, expensive observations in time or discrete-time control with costly observation methods, which are inapplicable to continuous-time settings due to the compounding discretization errors introduced by time discretization. In this work, we are the first to formalize the continuous-time control problem with costly observations. Our key theoretical contribution shows that observing at regular time intervals is not optimal in certain environments, while irregular observation policies yield higher expected utility. This perspective paves the way for the development of novel methods that can take irregular observations in continuous-time control with costly observations. We empirically validate our theoretical findings in various continuous-time environments, including a cancer simulation, by constructing a simple initial method to solve this new problem, with a heuristic threshold on the variance of reward rollouts in an offline continuous-time model-based model predictive control (MPC) planner. Although determining the optimal method remains an open problem, our work offers valuable insights and understanding of this unique problem, laying the foundation for future research in this area.

## 1 Introduction

The problem of continuous control with costly observations is ubiquitous with applications in medicine, biological systems, low power systems, robotics, resource management and surveillance (Yoshioka and Tsujimura, 2020; Brunereau et al., 2012; Mastronarde and van der Schaar, 2012). A setting that is shared across all these domains is that a decision-maker needs to continually control (e.g., chemotherapy dosing, quantities of food, data to transmit, etc.) whilst deciding _when_ to take a costly observation (e.g., a medical computed tomography (CT) scan, measuring the population of a species, measuring bandwidth, etc.). The decision-maker's observing policy must be timely to determine whether the action plan is effective (e.g., errors in treating stage 4 cancer can be fatal (Reinhardt et al., 2019), with further application examples in Appendix B).

In many of these real-world systems (e.g., medicine, low-power systems, resource management), an offline setup is beneficial as it enables decision-makers to learn control policies without incurring excessive costs or risking adverse effects. Where an offline setup refers to learning a policy from a previously collected dataset of state-action trajectories, without interacting with the environment (Argenson and Dulac-Arnold, 2020). The development of novel methods that can take irregular observations in continuous-time control with costly observations, within an offline setup, provides a safe and cost-effective way to improve real-world decision-making processes.

Recent work falls into three main categories. First, _sensing approaches_ determine when to informatively observe to identify an underlying state, but are unable to continually control. Second, _planning approaches_ only continually control and have the restrictive assumption that the observing schedule is_given_--most often at a regularly fixed observation time interval. While these methods can optimally solve many challenging tasks, with regular, frequent observations of the state, frequent observations are overly costly with many being unnecessary, and where the optimal regular observation interval is unknown.

Third, _discrete monitoring approaches_ control and determine when to observe only for discrete time. These are fundamentally incompatible with continuous-time systems which can exhibit irregular observations in time and states that evolve on different time scales. Moreover, these rely on a discretization interval being carefully selected to use but still suffer from time discretization errors that can compound, leading to unstable and poor-performing controllers (Yildiz et al., 2021). For example, in medicine, a crucial test may be administered too late, and therefore delay updating the treatment dosing--which can be fatal when treating patients for cancer (Geng et al., 2017).

**Contributions:** **1** We are the first to formalize the problem of continuous-time control whilst deciding when to take costly observations. Theoretically, we show that regular observing in continuous time with costly observations is not optimal for some systems and that irregularly observing can achieve a higher expected utility (Section 2). **2** Empirically we verify this key theoretical result in a cancer simulation and standard continuous-time control environments with costly observations. We construct a simple initial method to solve this new problem, called Active Observing Control. This uses a heuristic threshold on the variance of reward rollouts in an offline continuous-time model-based model predictive control (MPC) planner (Sections 4 and 5.1). However, the optimal method is still an open problem, and we leave this for future work. We gain insight into this unique problem, uncovering how our initial method is capable of irregularly observing, and identifies observation frequencies that correlate to different stages of cancer fixed observing frequencies of clinicians. Further, we demonstrate how it can avoid discretization errors in time, thus achieving a better utility as a result (Section 5.2), and confirm its robustness to its threshold hyperparameter (Section 5.3).

We have released a PyTorch (Paszke et al., 2019) implementation of the code at https://github.com/samholt/ActiveObservingInContinuous-timeControl and have a broader research group codebase at https://github.com/vanderschaarlab/ActiveObservingInContinuous-timeControl.

## 2 Problem

**States & actions.** For a system with _state_ space \(=^{d_{}}\) and _action_ space \(=^{d_{}}\), the state at time \(t_{+}\) is denoted as \(s(t)=[s_{1}(t),,s_{d_{}}(t)]\) and the action at time \(t\) is denoted as \(a(t)=[a_{1}(t),,a_{d_{}}(t)]\). We elaborate that a _state trajectory_\(s^{}\) and an _action trajectory_\(a^{}\) are both functions of time, where an individual state \(s(t)\) or an individual action \(a(t)\) are points on these trajectories. In practical applications, action values are usually bounded by an actuator's limits; hence we also restrict the action space to a Euclidean box \(=[a_{},a_{}]^{d_{}}\).

**Environment dynamics.** The dynamics of the system are given by \(s(t)/t=f(s(t),a(t))\), and the system is stationary. We consider the setting where the true state trajectory \(s\) is latent and, at a given time \(t\), only a noisy observation \(z(t)=s(t)+(t)\) can be observed, where \((t)(0,_{}^{2})\) is Gaussian noise with zero mean and standard deviation \(_{}\) and \((t)\!\!\!(t^{}), t,t^{} _{+}\). We denote as a tuple \((t,z(t),a(t))\) a discrete sample taken at time \(t\), and with \(h=\{(t_{j},z(t_{j}),a(t_{j}))\}\) a history of samples taken at times \(\{t_{j}\}\) where \(=_{n=0}^{}(_{+})^{n}\) is the space of all possible histories.

**Policies.** Policies consist of an _observing policy_\(:\) and a _decision policy_\(:\). These policies interact with the environment as described in Algorithm 1. We denote with \(_{j+1}=t_{j+1}-t_{j}\), where \(j\) is a dummy variable.

**Objective.** Suppose we are given a reward function \(r:\) to maximize, and each observation has a fixed cost \(c_{+}\). Then, the utility achieved by a policy up to a time horizon \(T\) is given by

\[=^{T}r(s(t),a(t),t)t}_{}-:t_{i}[0,T]\}|}_{}\] (1)

Our objective is to find the optimal observing and decision policies \(^{*},^{*}=*{arg\,max}_{,}[]\) given an offline dataset of histories \(=\{h^{(i)}\}\) but without knowing \(f\) and \(_{}\) or having online access to the system described by \(f\) and \(_{}\).

### Taking regular observations in time is not optimal for some systems

This is a highly non-trivial task since, in many cases, the obvious solution of taking regular observations in time is not optimal, and rather taking irregular observations achieves a higher expected utility--a point we make formally in Proposition 2.1.

**Proposition 2.1**.: _For some systems, it is not optimal to observe regularly--that is \( f,_{},r,c,h,h^{}:^{*}(h)^{*}(h^{ })\)._

Proof.: Full proof is in Appendix C. However, we present the following sketch. Consider the task of maximizing the utility \(\) given a reward function \(r(s,a,t)=(t-T)\{s(t) a(t)>0\}\), observation cost \(c>0\) for the specific system \(ds/dt=s(t)\) with observation noise (\(_{}>0\)). Intuitively this reward function only provides a positive reward of \(+1\) at the end of the episode (\(t=T\)) if the chosen action \(a(T)\) has the same sign as the unobserved state \(s(T)\), otherwise, the reward is \(0\). Therefore, this \(r\) is maximized when the sign of the correct state \(s(t)\) is identified. Since this system is defined by \(ds/dt=s(t)\), we know that state solutions are of the form \(s(t)=s(0)e^{t}\). Thus, to determine the sign of \(s(t)\), since \(e^{t}\) is always strictly positive, the sign of the state is determined by the initial condition \(s(0)\). Our observing policy can observe anywhere inside the interval of \([0,T]\). We prove that the optimal observing policy, \(^{*}\) cannot be a regular observing policy by showing that, for each \(^{(,)}\), there exists at least one \(^{(,)}\) with \( 2\) that achieves higher expected utility, where \(\) is the number of observations taken. A higher expected utility can be achieved by an irregular observing policy that takes all observations at the end of the episode, i.e. \(^{*}(h_{i})=T-t_{i}\) for \(i\{2,,\}\)--that is when the signal-to-noise ratio (\(s(t)^{2}/_{}^{2}\)) is the highest, i.e. \(*{arg\,max}_{t}e^{2t}}{_{}^{2}}:t[ 0,T]\), which occurs when \(t=T\). 

This motivates us to develop new continuous-time methods for solving this problem, where a solution should be able to flexibly adapt to take irregular observations when it achieves a higher expected utility. Intuitively, based on the above system, it is more informative to take an observation when the predicted state change magnitude is larger, which can occur faster for a large state velocity \(\)--indicating observations should be taken more frequently. We later show experimentally illustrative cases (Section 5.2) of how irregularly observing can lead to better performance.

## 3 Related work

Table 1 summarizes the key differences between Active Observing Control from related approaches to planning and monitoring in reinforcement learning (RL). Moreover, we provide an extended related work in Appendix D, which includes discussions of the benefits of offline RL, model-based RL, why model predictive control is our preferred action policy, event-based sampling, Semi-MDP methods, and Linear Quadratic Regression (LQR) & Linear Quadratic Gaussian (LQG) methods.

**Sensing** approaches have been proposed of when to optimally take an observation in both discrete time (Jarrett and Van Der Schaar, 2020) and continuous time (Alaa and Van Der Schaar, 2016; Barfoot et al., 2014)--where their goal is to identify an underlying state. However, these approaches cannot also continuously control the system. In contrast, Active Observing Control seeks to both actively observe and control the system.

**Planning** approaches only focus on optimal decision policies \(\), and therefore observing has to be provided by a schedule, which is often assumed to be at a regular constant interval in time\(_{i}\)--that is, observations are not actively planned (Yildiz et al., 2021). Within planning approaches, there exist many discrete-time approaches (Chua et al., 2018; Mnih et al., 2013; Williams et al., 2017) and recently more continuous-time approaches (Du et al., 2020; Yildiz et al., 2021; Du et al., 2020). Specifically, Yildiz et al. (2021) presented a seminal online continuous-time model-based RL algorithm, leveraging a continuous-time dynamics model that can predict the next state at an arbitrary time \(s(t)\). However, all these methods are _unable_ to plan when to take the next observation, in contrast to Active Observing Control which _is_ able to determine when to take the next observation whilst planning an action trajectory.

**Monitoring** approaches consist of both a decision policy \(\) and an observing policy \(\); however, existing approaches only consider the discrete-time and discrete-state setting with observation costs (Sharma et al., 2017; Nam et al., 2021; Aguiar et al., 2020; Bellinger et al., 2020; Krueger et al., 2020; Bellinger et al., 2020). In particular, Krueger et al. (2020) proposed that even in a simple setting of a discrete-state multi-armed bandit, computing the optimal time to take the next observation is intractable--therefore, they must rely on heuristics for their observing policy. Broadly, discrete-time monitoring methods use a discrete-time model and propagate the last observed state, often appending either a flag or a counter to the state-action tuple to indicate the time since an observation was taken (Aguiar et al., 2020; Bellinger et al., 2020). However, these methods cannot be applied to continuous-time environments or propagate the predicted current state and its associated prediction interval of the uncertainty associated with the state estimate. Moreover, training a policy (Aguiar et al., 2020) that decides at each state whether to observe it or not, requires a discretization of time. Whereas Active Observing Control, which determines the continuous-time interval of when to observe the next state, does not require any discretization of time, and hence it is a continuous-time method and avoids compounding time discretization errors.

One approach exists that we term _semi-continuous monitoring_, where Huang et al. (2019) proposes a discrete-state, constant action policy, that determines when to take an observation in continuous time of a controlled Markov Jump Process. However, this approach is limiting, as it assumes actions are constant until the next sample of the state is taken (Ni and Jang, 2022)--which is clearly suboptimal, uses discrete states, and assumes a Markov Jump Process (Gillespie, 1991). Instead, Active Observing Control is fully continuous in both states and observing times--controlled by an action trajectory \(a\), giving rise to _smoothly-varying_ states.

## 4 Active Observing Control

We now propose Active Observing Control (AOC), an initial method for the defined problem of continuous-time control whilst deciding when to take costly observations. AOC, can plan action trajectories in continuous time and plan when to observe next in continuous time. The key idea is to use a _continuous-time uncertainty-aware_ dynamics model \(_{}\) to plan 1) action trajectories and 2) the next time to take an observation, such that it is informative to do so--Figure 2 provides a block diagram. We adhere to the standard offline model-based setup (Lutter et al., 2021) of first learning our dynamics model (Section 4.1), and then using it at run-time with our planning method (Section 4.2).

  
**Approach** & **Ref.** & **Dennis** & **Environment Dynamics** & **Short Implications** & **Polios** & **Polios** \\  Discuss Example & (Burn and D.S. Davis, 2003) & Discrete-time & \(s_{1}-s_{2}\)-s_{3}\) & A & Observation-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Discuss Example & (Chua et al., 2018) & Discrete-time & \(s_{1}-s_{2}\)-s_{3}\) & B & Decursurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Discuss Example & (Burn et al., 2021) & Discrete time & \(s_{1}-s_{2}\)-s_{3}\) & C & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Continuous-Time Example & (Julia and D.S. Davis, 2004) & Continuous-time & \(s_{1}-s_{2}\)-s_{3}\) & D & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Continuous-Time Example & (Yildiz et al., 2021) & Continuous-time & \(s_{1}-s_{2}\)-s_{3}\) & E & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Continuous-Time Example & (Zhang et al., 2021) & Continuous-time & \(s_{1}+s_{2}\)-s_{3}\) & \(s_{1}+s_{2}\)-s_{3}\) & F & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Active Observing Control & (Zhang et al., 2021) & Continuous-time & \(s_{1}+s_{2}\)-s_{3}\) & \(s_{1}+s_{2}\)-s_{3}\) & F & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\ Active Observing Control & (Zhang et al., 2021) & Continuous-time & \(s_{1}+s_{2}\)-s_{3}\) & \(s_{1}+s_{2}\)-s_{3}\) & F & Decurs-step & \(s_{1}+s_{2}\)-s_{3}\) \\   

Table 1: **Comparison with related approaches to planning and monitoring in RL.** Plots for the corresponding state trajectories for each approach are illustrated in Figure 1. Our initial method, Active Observing Control, is the only method for continuous-time control whilst deciding _when to observe_ with _smoothly-varying_ states.

Figure 1: Comparison of related approaches of an environment’s latent state \(s(t)\)—where green lines represent observing times and red dots observations that have observation noise.

### Learning a continuous-time uncertainty-aware dynamics model

Fundamentally the goal of offline model-based RL is to learn an _accurate_ dynamics model that can be used for planning from an offline dataset \(\)(Lutter et al., 2021). In particular, we require a dynamics model that is both uncertainty aware and continuous in time; that is, it can provide a prediction uncertainty for the next state at a future time \(t+\), i.e. \(p(z(t+)|(z(t),a(t),))\). Here we use the time difference \(\) input to create a continuous-time dynamics model (Yildiz et al., 2021).

Model-based RL has shown the crucial importance of the performance of learning an uncertainty-aware dynamics model that captures both 1) _aleatoric_ uncertainty--due to the inherent stochasticity of the environment (e.g., observation noise and environment process noise) that is irreducible and 2) _epistemic_ uncertainty--due to the lack of data, for a given state-action space (which should vanish when trained on the limit of infinite data) (Chua et al., 2018).

We learn an uncertainty-aware dynamics model by training an ensemble of high-capacity multi-layer perceptron (MLP) neural network models that each parameterize a multivariate Gaussian distribution--where the ensembling captures the _epistemic_ uncertainty and each model individually captures the _aleatoric_ uncertainty. We note that as shown by other works, ensembles of high-capacity neural networks outperform Gaussian process dynamics models, as they have constant-time inference--scaling better to larger offline datasets, while still providing well-calibrated uncertainty estimates (Lakshminarayanan et al., 2017) and can model more complex functions--inclusively non-smooth dynamics (Chua et al., 2018).

Precisely, the ensemble consists of \(M=5\) neural network models that output the parameters for a multivariate Gaussian distribution with diagonal covariances, each with \(_{m}\) parameters, i.e., \(_{_{m}}=p_{_{m}}(z(t+)|(z(t),a(t),))=(_{_{m}}(z(t),a(t),),_{_{m}}(z(t),a(t), ))\) where the elements of the diagonal covariances are given by \(^{2}_{_{m}}(z(t),a(t),)\), with a total of \(=\{_{m}\}_{m=1}^{M}\) parameters of the ensemble. Moreover, we denote all the models in the ensemble as \(_{}\).

To create our ensemble of parameterized Gaussian distribution models, we train each model independently with unique initialized random weights on a unique permutation of all the samples in the offline dataset--whereby each base model converges to its own local optima and is more effective than training the models on subsets of the dataset, i.e., bootstrapping (Lakshminarayanan et al., 2017). Therefore, we minimize the negative log-likelihood for every single model separately, that is

\[(_{m})=_{k}_{j=1}^{|h|-1}[_{_{ m}}(z(t_{j}),a(t_{j}),_{j+1})-z(t_{j+1})]^{}_{_{m}}^{-1}(z(t_ {j}),a(t_{j}),_{j+1})\]

\[[_{_{m}}(z(t_{j}),a(t_{j}),_{j+1})-z(t_{j+1})]+_ {_{m}}(z(t_{j}),a(t_{j}),_{j+1}),\]

where \(_{j+1}=_{j+1}=t_{j+1}-t_{j}\) arises from the offline dataset \(\). Here each parameterized Gaussian distribution model captures heteroskedastic (i.e., the output noise distribution is a function of the

Figure 2: **Block diagram of Active Observing Control.** An uncertainty-aware dynamics model \(_{}\) is learned from an offline dataset \(\) of state-action trajectories. At run-time, planning consists of two steps: **1)** The actions are determined by a Model Predictive Control (MPC) Planner, and **2)** the determined action trajectory \(a\) is forward simulated to provide uncertainty \((r(t))\) on the planned path reward. We determine the continuous time \(t_{i+1}\) to execute the action plan \(a\) up to, such that \((r(t))<\), as in Algorithm 3. We then execute \(a(t)\)\( t[t_{i},t_{i+1})\) up until the time to take the next observation \(z(t)\) at.

input) _aleatoric_ uncertainty. To capture the heteroskedastic _epistemic_ uncertainty, we combine the individual models as a uniformly weighted mixture model and combine the predictions as \(p_{}(z(t+)|(z(t),a(t),),)=_{m=1}^{M}p_{ _{m}}(z(t+)|(z(t),a(t),),_{m})\). Therefore, we can compute the effective mean and covariance (diagonal elements) of the mixture as

\[_{*}(z(t),a(t),)= _{m}[_{_{m}}(z(t),a(t),)]\] \[_{*}^{2}(z(t),a(t),)= _{m}[(_{_{m}}^{2}(z(t),a(t),)+_{ _{m}}^{2}(z(t),a(t),)]-_{*}^{2}(z(t),a(t),)\]

### Active Observing Control Planning

We desire to use the trained uncertainty-aware dynamics model \(_{}\) to **1)** plan the optimal action trajectory \(a\) to execute and **2)** plan when best to take the next observation. Intuitively, we use the probabilistic dynamics model \(_{}\) to plan an optimal action trajectory \(a\) and execute this until the next observing time--as determined by the heuristic of when the predicted reward distribution over the planned trajectory crosses a set threshold, Figure 2. In the following, we detail these two steps.

**1) Planning optimal actions.** To plan an optimal action trajectory \(a\), we specify that any model predictive controller (MPC) planner can be used--that can optimize the action trajectory up to a specified time horizon \(H_{+}\). We opt to use an MPC planner, as it optimizes an entire action trajectory \(a\) up to \(H\), does not require the probabilistic dynamics model \(_{}\) to be differentiable (it uses no gradients), and the reward function \(r\) can be changed on the fly--allowing changing goals/tasks at run-time. We use our probabilistic dynamics model \(_{}\), with the MPC planner of Model Predictive Path Integral Control (MPPI), a zeroth-order particle-based trajectory optimizer (Williams et al., 2017), due to its competitive performance (Wang et al., 2022).

To optimize the action trajectory \(a\) up to a time horizon \(H\), it discretizes \(H\) into smaller constant action time intervals \(_{a}_{+}\) which are then optimized--where there are \(K=}_{+}\) time steps in \(H\)--i.e., \(t^{(k)}=t_{i}+k_{a},k\{0,,K-1\}\). It forward simulates several parallel rollouts \(G_{+}\), where the next state estimate at each time step is simulated as \(z(t^{(k+1)})=_{*}(z(t^{(k)}),a(t^{(k)}),_{a})\) recursively. This requires a state estimate \(z(t_{i})\) to plan from. Therefore, we recompute the planned trajectory when we observe a new sample of the state. Furthermore, we provide MPC MPPI planner pseudocode and details in Appendix E.

**2) When to observe.** We desire to observe when it is most informative to do so. Therefore, we seek to determine the time interval \(_{i+1}\) that the action trajectory \(a\) can be followed until an observation is taken. Intuitively, we create a reward distribution over continuous time following the MPC planned action trajectory \(a\), which we can follow until the uncertainty crosses a set threshold \(_{+}\)--a hyperparameter to be tuned. Intuitively we prefer the reward uncertainty rather than that of the state because we can achieve the task with a very certain reward despite having uncertain states. For instance, when there might be multiple ways to achieve a task--and we know that our multiple action trajectories guarantee this where we can take any; however, are uncertain about which one. We empirically verify this in Appendix L.1.

To create the continuous-time reward distribution and state distribution, we use our learned probabilistic dynamics model \(_{}\) to forward propagate the last observed state, according to the planned action trajectory \(a\). As noted by others (Chua et al., 2018), computing a closed-form expression for the distribution of the expected state trajectory is generally intractable; therefore, it is common to approximate the uncertainty propagation (Chua et al., 2018; Girard et al., 2002; Candela et al., 2003). We generate this through Monte Carlo sampling by forward simulating \(P_{+}\) particle rollouts of state trajectories. That is, for a given state particle, for the next step of \(_{a}\) we sample a new state particle \(z_{p}(t^{(k+1)})(_{*}(z_{p}(t^{(k)}),a(t^{(k)}),_{a}),_{*}^{2}(z_{p}(t^{(k)}),a(t^{(k)}),_{a}))\) recursively along the planned action trajectory. This allows us to apply the reward function to the state particles, generating reward particles--whereby we can compute the mean and standard deviation of the reward at each time step \(t^{(k)}\), i.e., \(r(t^{(k)})(_{r}(t^{(k)}),_{r}^{2}(t^{(k)}))\). Therefore, using our estimate of \((r(t^{(k)}))\) over the state trajectory \(a\) allows us to determine the maximum time interval \(_{i+1}\) until the reward uncertainty becomes greater than a set threshold \(\). Thus, the next time to observe at is

\[(z(t_{i}))=\{^{}_{+}:_{z_{p}}[r (t_{i}+^{})]}<\}\]

This provides an estimate of the next observation interval \(_{i+1}\) that is discretized to an integer number of \(_{a}\) intervals. However, we seek the continuous-time observation interval \(_{i+1}\); therefore,we further refine this estimate through a continuous binary search (root finding algorithm) up to a tolerance of \(_{t}_{+}\). We outline the AOC planning algorithm pseudocode in Appendix F.

**Run-time complexity.** Intuitively, we find if the time taken to plan using an MPC controller is feasible in a problem setting, then AOC is feasible. As the run-time complexity of AOC is \(O(GK+P(K+W))\) where \(W=((_{a})-(_{t}))/(2)_{+}\) (Appendix G). Empirically we chose \(P=10G,W=4,G=10,000\) for all experiments. Although AOC takes approximately \(2.4\) longer to plan, which includes both planning the actions and when to observe compared to Continuous Planning methods that only plan actions (an MPC step)--it can often take fewer observations overall, leading to less time spent planning overall. AOC is still a practical method that can plan in less time than the action interval \(_{a}\), even in robotics environments, where it takes \(24.9\) ms to plan the actions and the next observation time (Appendix G).

An important parameter is the uncertainty threshold \(\) hyperparameter. We find the following procedure to tune this parameter for a benchmark sufficient. For a particular environment after training its dynamics model, one episode is run, where the action trajectory \(a\) is re-planned at every \(_{a}\) time step and compute the median of the reward uncertainty over time for each rollout action plan, and then takes the mean of these over an episode to produce \(\). This step is akin to a calibration step, where calibration is common in medicine (Preston et al., 1988) and robotics (Nubiola and Bonev, 2013). We also include practical guidelines to select \(c\) in Appendix H.

## 5 Experiments and Evaluation

**Benchmark environments.** We use four continuous-time control environments and adapt them to add a fixed cost \(c\) for taking an observation--these environments were selected as they exhibit a range of dynamical system state region regimes. First, the Cancer environment uses a simulation of a Pharmacokine-Pharmacodynamic (PK-PD) model of lung cancer tumor growth (Geng et al., 2017) under continuous dosing treatment of chemotherapy and radiotherapy. We note that the same underlying model has also been used by others (Lim, 2018; Bica et al., 2020; Seedat et al., 2022). Moreover, we use the standard continuous-time control environments from the ODE-RL suite (Yildiz et al., 2021), which consists of three environments: Pendulum, CartPole, and Acrobot. We note that all the environments are described by a differential equation (DE) and use a DE solver to simulate the fully continuous in-time states and actions, unlike discrete environments (Yildiz et al., 2021; Brockman et al., 2016). Furthermore, to increase realism, we add Gaussian noise to observations taken, that of \((0,_{e}^{2})\) with standard deviation \(=0.01\). Specifically, the ODE-RL starting states have the poles hanging down at the stable equilibrium of the DE system, and the goal is to swing up and stabilize the pole(s) upright to the unstable equilibrium (Yildiz et al., 2021). Additionally, the reward function from is the same across the environments--i.e., the exponential of the negative distance from the current state to the desired goal state \(s^{*}\), while penalizing the action magnitudes--and we assume we are given this simple reward function when planning (however, we also show AOC can work with a learned reward model in Appendix L.9). Furthermore, we generate an offline dataset \(\) from these environments that has irregular times between state-action samples \(_{i+1}()\), with a mean of \(=_{a}\) seconds. We detail all environments, including offline dataset generation in Appendix I.

**Benchmark methods**. We seek to provide more competitive benchmarks than those outlined in Table 1; therefore, we compare against the following ablations of our method, **Active Observing Control**. Specifically, we focus on benchmarking observing policies \(\) and use the same action policy \(\) across all the benchmark methods, that of the same MPC MPPI planner with the hyperparameters fixed. We implement two discrete-time methods by first learning a discrete-time uncertainty-aware dynamics model (an ablation of the same model and hyperparameters, without the time interval input \(\) to predict the next state for) on the same offline dataset \(\). Second, we use this discrete-time uncertainty-aware dynamics model to create two benchmark methods, that of **Discrete Planning** that observes at each action time interval \(_{a}\) and **Discrete Monitoring** a discrete ablation of our observing policy (Algorithm 3)--that uses the same reward distribution estimation and determines the discrete action sequence to execute up until the reward uncertainty crosses the threshold \(\)--at a discrete-time resolution of \(_{a}\). Moreover,

  
**Benchmark** & **Observing Policy** \\  Discrete Planning & \((z(t_{i}))=_{a}\) \\ Discrete Monitoring & \((z(t_{i}))=_{a}(z(t_{i}+k_{s})<)\) \\ Continuous Planning & \((z(t_{i}))=_{a}\) \\ 
**Active Observing Control** & \((z(t_{i}))=(z(t_{i}+)<)\) \\   

Table 2: Benchmark observing policieswe also benchmark against **Continuous Planning** that uses our trained continuous-time uncertainty-aware dynamics model and takes observations at regular time intervals of a multiple of \(_{a}\). Finally, we also compare with a random action policy, **Random** that observes at each action interval \(_{a}\). For ease of comparison, we list these observing policies in Table 2. We further provide the dynamics model implementation details, hyperparameters, and training details in Appendix J.

**Evaluation.** An uncertainty-aware dynamics model is trained for each environment on an offline dataset \(\) of collected trajectories consisting of \(10^{6}\) samples. The trained dynamics model weights are then frozen when used at run-time. We record the average undiscounted utility \(\) (Equation (1)), average undiscounted reward \(\) and the number of observations taken \(\), after running one episode of the benchmark method--and repeat this for 100 random seed runs for each result with their 95\(\%\) confidence intervals throughout. Moreover, we normalize \(\) and \(\) following the standard normalization of offline-RL (Yu et al., 2020)--normalized to the interval of 0 to 100, where a score of 0 corresponds to a random policy performance, and 100 to an expert--which we assume here is the Continuous Planning benchmark. Furthermore, we detail these metrics and the experimental setup further in Appendix K.

### Main results

We evaluated all the benchmark methods across all our environments with results tabulated in Table 3. Active Observing Control achieves high average utility \(\) on all environments. Specifically, AOC can achieve near-optimal state control performance \(\) while using fewer observations \(\) compared to taking regular, frequent observations of Continuous Planning. Furthermore, it outperforms Discrete

Figure 3: **Comparison of Active Observing Control against Continuous Planning on the cancer environment for one episode.** Rows: A) Cancer volume \(v\) state trajectory—with green vertical lines representing observation times. B) Reward uncertainty of the planned state trajectory \(a\) after an observation is taken—where the red horizontal line indicates AOC’s threshold used \(\). C) Bar chart of the frequency of observations per state region. AOC automatically determines to observe larger cancer volumes more frequently as they are more informative, as the future state change magnitude is larger (Section 2.1)—which correlates to clinician’s findings (Geng et al., 2017). Whereas with Continuous Planning the observing frequency is regular, and therefore observations can be taken when the reward is still certain, suggesting an unnecessary observation.

    &  &  &  &  &  \\   & & \(\) & \(R\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Random & 90.0 & 90.0 & 124.9 & 0.0 & 0.0 & 0.0 & 90.0 & 90.0 & 90.0 & 90.0 & 90.0 & 90.0 & 90.0 & 90.0 & 90.0 \\ Discent Planning & 91.91.98 & 91.19.98 & 91.19.98 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 & 91.19.19 \\ Discent Training & 91.01.02 & 91.02.13 & 91.02.13 & 91.02.13 & 90.02.02 & 90.04.02 & 90.04.02 & 90.02 & 90.02.07 & 90.00.04 & 90.00.04 & 90.00.04 & 90.00.04 & 90.00.04 \\ 
**Active Observing Control** & **1001.02.18** & **0.03.100** & 3.70.010 & **2.001.011** & **0.03.010** & **0.03.010** & **0.03.010** & **0.03.010** & **0.03.010** & **0.03.010** & **0.03.010** & **0.03.010** \\   

Table 3: Normalized utilities \(\), rewards \(\) and observations \(\) for the benchmark methods, across each environment. AOC performs the best across all environments. Results are averaged over 1,000 random seeds, with \(\) indicating 95\(\%\) confidence intervals. Utilities and rewards are undiscounted and normalized to be between 0 and 100, where 0 corresponds to a Random agent, and 100 corresponds to the expert, that of Continuous Planning, taking state observations at every \(_{a}\).

Monitoring as it can determine when, in continuous time to observe, whereas those methods suffer from discretization errors--that compound, leading to worse performance.

### Insight experiments

In the following, we gain insight into why AOC performs better than Continuous Planning, i.e., better than taking regular observations in time and the importance of continuous-time monitoring as opposed to discrete-time monitoring.

**How does irregularly observing achieve a higher expected utility than regularly observing?** To better understand why monitoring (active observing) approaches outperform planning that regularly observes, we provide a qualitative and quantitative study.

On the one hand, qualitatively, as detailed in Figure 3, we observe that for a given threshold, AOC can still solve the Cancer environment while taking less costly observations and that AOC automatically determines to increase the observing frequency when the tumor volume \(v\) is large and less when it is smaller. Crucially, this matches what clinicians have already discovered, where they categorize the cancer volume into five discrete bins where the observing frequency is correlated to tumor volume (Geng et al., 2017; Seedat et al., 2022). Moreover, this provides insight into how irregular observing can achieve a higher expected utility (Section 2.1), as observations could be more informative (hence taken more frequently) when the future state change magnitude is larger--occurring for large \(v\).

Furthermore, as in Figure 4 for the Pendulum environment, we observe that AOC observes infrequently the state of the pendulum when it has been swung up to the upright goal state and is balancing it there. This matches our intuition from Section 2.1, as future state changes of \(\) the angle of the pendulum are large when the pendulum is not at the equilibrium goal state, thus having \(>0\), which is maximally large in the swing up state trajectory phase and necessitates frequent observing, whereas balancing near the top equilibrium point, \( 0\) necessitates infrequent observing.

On the other hand, quantitatively, as tabulated in Table 4, we make Continuous Planning take the same number of observations as AOC, however, taking them regularly. Since AOC can take better-located observations in time, it outperforms Continuous Planning, achieving a higher reward \(\). In summary, these experiments demonstrate that actively observing can be superior to planning in continuous time.

**Why is it crucial to actively observe with continuous-time methods, rather than discrete-time methods?** We compare Active Observing Control to Discrete Monitoring, which is the closest discrete counterpart (a discrete ablation of AOC) and look at when the observing times are reached, relative to their reward uncertainty estimates, to determine whether time discretization affects observing times. As shown in Figure 5, we see that Discrete Monitoring takes observations that are delayed, and therefore achieves a lower state reward \(\). This allows AOC to capture fast-moving states, where missing these can be catastrophic or deadly, as in the case of cancer.

    &  \\ Policy & \(\) & \(\) & \(\) \\ 
**Active Observing Control** & 105\(\)0.138 & 98\(\)0.173 & 3.39\(\)0.0306 \\ Continuous Planning with \(=\) & 102\(\)0.243 & 95\(\)0.234 & 3\(\)0 \\ Continuous Planning with \(=\) & 103\(\)0.226 & 97.3\(\)0.226 & 4\(\)0 \\   

Table 4: Normalized utilities \(\) and rewards \(\) for the cancer environment—using the same normalization as in Table 3. Even when Continuous Planning takes the same number of observations as determined by AOC, it still performs worse, because those observations are not _well located_. As Continuous Planning observations are taken blindly at regular times, rather than at more informative points.

Figure 4: **Frequency of observations per state region for Pendulum. The intuition from Section 2.1 indicates that it is more informative to sample when \(\) is larger, hence near the top goal equilibrium point \( 0\) necessitates infrequent observations.**

### Sensitivity analysis of \(\)

Active Observing Control still outperforms Continuous Planning even at variable \(\) on the cancer environment, Figure 6. We note that as \(\) is increased, this increases the observing times \(_{i+1}\), and hence fewer observations of the state are taken. Although we set \(=6.67\) for the cancer environment, following the procedure in Section 4, we observe robustness to other choices of \(\) are also suitable with decreasing reward \(\); however, AOC still outperforms and with a high utility \(\). Moreover, we tuned \(_{}\) so that frequent observations were taken to be at least equal to Continuous Planning, and then \(_{}\) to take the fewest observations, which is often minimally 1, as we start by taking an observation, or until the entire action trajectory plan is executed, and requires re-planning by taking another observation.

## 6 Conclusion and Future work

This work lays the foundation for the significant yet unexplored real-world problem of continuous-time control whilst deciding when to take costly observations, which has profound implications across diverse fields such as medicine, resource management, and low-power systems. Our key theoretical contribution, verified empirically is that regular observing is not optimal and that irregularly observing can achieve a higher expected utility. To demonstrate the power of active irregular observing, we provided an initial solution using a heuristic threshold based on the variance of reward rollouts in an offline continuous-time model-based model predictive control planner. However, the quest for the optimal method remains an open problem, providing fertile ground for future research.

However, this work is not without limitations. The heuristic threshold, while a robust initial solution in our experiments, may not be optimal or suitable for all scenarios. Moreover, our approach relies on the assumption that the offline dataset provides sufficient coverage of the state-action space to learn a useful enough model of the system dynamics and that the observation observes all the states plus Gaussian noise. Also, in some practical applications, the cost of observations could vary or be unknown.

Navigating these limitations illuminates exciting pathways for future research. Potential directions include: optimizing multiple observations concurrently over a planned action trajectory, jointly optimizing action trajectories and observing times, and theoretically deriving the optimal solution to reveal characteristics of optimal observing policies.

Figure 5: **Reward uncertainty \((r)\) normalized by the method specific threshold \(\) on the cancer environment—thus the threshold to observe is at 1.0, as indicated by the red horizontal line. We observe that AOC can detect and catch when the uncertainty crosses the threshold, independent of the time discretization. In contrast, Discrete Monitoring suffers from time discretization error and takes a delayed observation, leading to compounding errors and worse performance. Here Discrete Monitoring misses observing a critical test by over a day.**

Figure 6: **Normalized utility \(\) on the cancer environment of Active Observing Control and Continuous Planning, plotted against changing uncertainty threshold \(\). We plot AOC’s threshold as used in Table 3 as the red line. AOC maintains a high utility and reward over a wide range of thresholds—whilst using fewer observations as \(\) increases, compared to Continuous Planning which takes expensive frequent observations. The threshold \(\) is between the minimum feasible \(_{}\) and maximum feasible \(_{}\) values.**