# Persuading Farsighted Receivers in MDPs:

the Power of Honesty

 Martino Bernasconi

Bocconi University

martino.bernasconi@unibocconi.it &Matteo Castiglioni

Politecnico di Milano

matteo.castiglioni@polimi.it &Alberto Marchesi

Politecnico di Milano

alberto.marchesi@polimi.it &Mirco Mutti

Technion

mirco.m@technion.ac.il

Work done while the author was at Politecnico di Milano.

Technion

mirco.m@technion.ac.il

###### Abstract

_Bayesian persuasion_ studies the problem faced by an informed sender who strategically discloses information to influence the behavior of an uninformed receiver. Recently, a growing attention has been devoted to settings where the sender and the receiver interact _sequentially_, in which the receiver's decision-making problem is usually modeled as a _Markov decision process_ (MDP). However, previous works focused on computing optimal information-revelation policies (a.k.a. _signaling schemes_) under the restrictive assumption that the receiver acts _myopically_, selecting actions to maximize the one-step reward and disregarding future rewards. This is justified by the fact that, when the receiver is _farsighted_ and thus considers future rewards, finding an optimal Markovian signaling scheme is \(\)-hard. In this paper, we show that Markovian signaling schemes do _not_ constitute the "right" class of policies. Indeed, differently from most of the MDPs settings, we prove that Markovian signaling schemes are _not_ optimal, and general _history-dependent_ signaling schemes should be considered. Moreover, we also show that history-dependent signaling schemes circumvent the negative complexity results affecting Markovian signaling schemes. Formally, we design an algorithm that computes an optimal and \(\)-persuasive history-dependent signaling scheme in time polynomial in \(1/\) and in the instance size. The crucial challenge is that general history-dependent signaling schemes cannot be represented in polynomial space. Nevertheless, we introduce a convenient subclass of history-dependent signaling schemes, called _promise-form_, which are as powerful as general history-dependent ones and efficiently representable. Intuitively, promise-form signaling schemes compactly encode histories in the form of _honest_ promises on future receiver's rewards.

## 1 Introduction

_Bayesian persuasion_(Kamenica and Gentzkow, 2011) is the problem faced by an informed _sender_ who wants to influence the behavior of an uninformed, self-interested _receiver_ through the provision of payoff relevant information. Bayesian persuasion captures many fundamental problems arising in real-world applications, _e.g._, online advertising (Bro Miltersen and Sheffet, 2012), voting (Alonso and Camara, 2016; Castiglioni et al., 2020; Castiglioni and Gatti, 2021), traffic routing (Bhaskar et al., 2016; Castiglioni et al., 2021), recommendation systems (Mansour et al., 2016), security (Rabinovich et al., 2015; Xu et al., 2016), marketing (Babichenko and Barman, 2017; Candogan, 2019), medical research (Koltoilin, 2015), and financial regulation (Goldstein and Leitner, 2018).

Most of the previous works study the classical, one-shot version of the Bayesian persuasion problem. However, in many application scenarios it is natural to assume that the sender and the receiver interact multiple times in a sequential manner. In spite of this, only a few very recent works addressed _sequential_ versions of the Bayesian persuasion problem (Wu et al., 2022; Bernasconi et al., 2022; Gan et al., 2022a,b). In particular, Wu et al. (2022) and Gan et al. (2022a,b) study settings where the sender and the receiver interact sequentially in a _Markov decision process_ (MDP).

In Bayesian persuasion problems in MDPs, at each step of the interaction both the sender and the receiver know the current state of the MDP, and the former has also access to some _private observation_ drawn according to a commonly-known, state-dependent distribution. The sender commits beforehand to an information-revelation policy, which is implemented by means of a _signaling scheme_ that sends randomized _action recommendations_ to the receiver, conditioned on the current (public) state and the sender's private observation. Specifically, the sender commits to a _persuasive_ signaling scheme, meaning that the receiver is always incentivized to follow recommendations. At the end of each step, the next state of the MDP and the agents' rewards are determined as a function of the current state, the action actually played by the receiver, and the sender's private observation in the current step.

Wu et al. (2022) and Gan et al. (2022a,b) provide algorithms that compute an optimal (_i.e._, reward-maximizing) persuasive signaling scheme under the restrictive assumption that the receiver acts _myopically_, selecting actions to maximize the one-step reward and disregarding future ones. This is justified by the fact that, when the receiver is _farsighted_ and thus considers future rewards, finding an optimal Markovian signaling scheme is \(\)-hard to approximate, as shown by Gan et al. (2022a) for infinite-horizon MDPs. An analogous result also holds in finite-horizon MDPs for non-stationary Markovian signaling schemes, as we prove in this work as a preliminary result.

In this paper, we show that Wu et al. (2022) and Gan et al. (2022a,b) failed to provide positive results with farsighted receivers since Markovian signaling schemes do not constitute the "right" class of policies to consider. This is in stark contrast with most of the MDPs settings in which Markovian policies are optimal. Indeed, we prove that Markovian signaling schemes are _not_ optimal, and general _history-dependent_ signaling schemes should be considered. As a result, we focus on the problem of computing an optimal persuasive history-dependent signaling scheme. Surprisingly, we show that taking history into account allows to circumvent the negative result affecting Markovian signaling schemes. We do that by providing an approximation scheme that finds an optimal \(\)-persuasive (_i.e._, one approximately incentivizing the receiver to follow action recommendations) history-dependent signaling scheme in time polynomial in \(1/\) and the size of the problem instance.

The crucial challenge in designing our approximation scheme is that general, history-dependent signaling schemes cannot be represented in polynomial space. Our algorithm overcomes such an issue by using a convenient subclass of history-dependent signaling schemes, which we call _promise-form_ signaling schemes. The core idea of such signaling schemes is to compactly encode all the relevant information contained in an history into a _promise_ on future receiver's rewards. At each step of the process, a promise-form signaling scheme does _not_ only determines an action recommendation for the receiver, but it also makes a promise to them. First, we prove that promise-form signaling schemes are as powerful as general history-dependent ones. Then, we show how an optimal \(\)-persuasive promise-form signaling scheme can be computed in polynomial time by means of a recursive procedure. To do that, we rely on a crucial result showing that, for signaling schemes that _honestly_ keep promises made to the receiver, persuasiveness constraints can be expressed as conditions defined locally at each step of the MDP, since the receiver only cares about sender's promises on their future rewards.2

## 2 Preliminaries

In this work, we study Bayesian persuasion problems where a _farsighted_ receiver takes actions in a _time-inhomogeneous finite-horizon_ MDP (Puterman, 2014). Formally, a problem instance is a tuple

\[(,,,,\{r_{h}^{}\}_{h },\{r_{h}^{}\}_{h},\{p_{h}\}_{h},\{_{h}\}_{h},),\]

\(\) is a finite set of states, \(\) is a finite set of receiver's actions available in each state, \([1,,H]\) is a set of time steps with \(H\) being the time horizon, \(\) is a finite set of sender's private observations, \(r_{h}^{},r_{h}^{}: \) are reward functions for the sender and the receiver, respectively,\(p_{h}:()\) is a transition function, \(_{h}:()\) is a function defining probabilities of sender's private observations at each state, while \(()\) is the initial state distribution.3

We consider the most general setting in which the sender commits to a _non-stationary_ and _non-Markovian signaling scheme_ (henceforth called _history-dependent_ signaling scheme for short). For every step \(h\) and state \(s_{h}\) reached at that step, a history-dependent signaling scheme defines a randomized mapping from sender's private observations to action recommendations for the receiver, based on the whole history of states and receiver's actions observed up to step \(h\).4 Formally, in the following we let \(_{h}\) be the set of all the possible _histories_ up to step \(h\), which is defined as

\[_{h}\{=(s_{1},a_{1},,s_{h-1},a_{h-1},s_ {h}):s_{i},a_{i}\},\]

while we let \(:=_{1}_{H}\) be the set of all the possible histories (of any length).5 Then, a history-dependent signaling scheme is defined as a set \(:=\{_{}\}_{}\) of functions \(_{}:()\), which define a mapping from sender's private observations to probability distributions over action recommendations for every possible history.

The interaction between the sender and the receiver goes as follows (Algorithm 1). **(i)** The sender publicly commits to a history-dependent signaling scheme \(:=\{_{}\}_{}\). **(ii)** An initial state \(s_{1}\) is drawn. **(iii)** At each step \(h\), both agents observe the current state \(s_{h}\) and the sender also gets a private observation \(_{h}\) drawn according to \(_{h}(s_{h})\), with the function \(_{h}\) being known to both the sender and the receiver. **(iv)** The sender communicates to the receiver an action recommendation \(a_{h}\) sampled according to \(_{_{h}}(_{h})\), where \(_{h}_{h}\) is the history up to step \(h\). **(v)** The receiver plays an action \(_{h}\), possibly different from \(a_{h}\). **(vi)** The sender and the receiver get rewards \(r^{}_{h}(s_{h},a_{h},_{h})\) and \(r^{}_{h}(s_{h},a_{h},_{h})\), respectively. **(vii)** If \(h=H\), the interaction ends, otherwise the next state \(s_{h+1} p_{h}(s_{h},_{h},_{h})\) is drawn and the interaction continues to step \(h+1\) starting from the third point. As customary in the literature (see, _e.g._, ), we assume that, if the receiver does _not_ follow recommendations at some step \(h\) by playing an action \(_{h} a_{h}\), then the sender stops issuing future recommendations to the receiver.

For ease of presentation, we introduce the sender's value function \(V^{,}_{h}:_{h}\) to encode sender's expected rewards by using a history-dependent signaling scheme \(:=\{_{}\}_{}\) from step \(h\) onwards, assuming the receiver always follows recommendations. Given a history \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\) up to step \(h\), such a value function is recursively defined as:

\[V^{,}_{h}()=_{}_{a}_{ h}(|s_{h})_{}(a|)(r^{}_{h}(a,s_{h},)+ _{s^{}}p_{h}(s^{}|s_{h},a,)V^{, }_{h+1}((a,s^{}))).\]

Similarly, we introduce the receiver's action-value function \(V^{,}_{h}:_{h}\) to encode the receiver's expected rewards by following sender's action recommendations from step \(h\) onwards. Formally, given a history \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\) up to step \(h\), the receiver's expected reward by following the recommendation to play \(a\) is recursively defined as follows:

\[V^{,}_{h}(a,)=_{}_{h}(|s_{h})_{ }(a|)(r^{}_{h}(a,s_{h},)+_{s^{} }p_{h}(s^{}|s_{h},a,)V^{,}_{h+1}( (a,s^{}))),\]where \(V_{h}^{,}:_{h}\) is such that \(V_{h}^{,}()=_{a}V_{h}^{,}(a,)\) for every \(h\) and \(_{h}\).

Finally, we introduce an additional receiver's value function, denoted by \(_{h}^{}:\), to encode receiver's expected rewards from step \(h\) onwards _after having deviated_ from recommendations. Formally, for every state \(s\), such a value function is recursively defined as follows:

\[_{h}^{}(s)=_{a}_{} _{h}(|s)(r_{h}^{}(a,s,)+_{s^{} }p_{h}(s^{}|s,a,)_{h+1}^{}(s^{ })),\]

where the maximum operator encodes the fact that the receiver plays so as to maximize future rewards without knowledge of realized sender's private observations after having deviated.6

By the revelation principle [Kamenica and Gentzkow, 2011], it is well known that in order to find an optimal signaling scheme it is possible to focus w.l.o.g. on (direct) history-dependent signaling schemes under which the receiver is always incentivized to follow sender's action recommendations. These are called _persuasive_ signaling scheme, and they are formally defined as follows:

**Definition 1** (\(\)-persuasiveness).: _Let \( 0\). A history-dependent signaling scheme \(:=\{_{}\}_{}\) is said to be \(\)-persuasive if, for every step \(h\), history \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\) up to step \(h\), and pair of actions \(a,a^{}\), the following holds:_

\[V_{h}^{,}(a,)_{}_{h}(|s_{h} )_{}(a|)(r_{h}^{}(s_{h},a^{},)+_ {s^{}}p_{h}(s^{}|s_{h},a^{},)_{h+1}^{}(s^{})-).\]

_Moreover, we say that the signaling scheme is persuasive if the conditions above hold for \(=0\). We denote the set of all the persuasive signaling schemes by \(\)._

Whenever the sender commits to an \(\)-persuasive history-dependent signaling scheme, we assume that the receiver always follows the recommendation issued by the sender. Notably, the recommended action is guaranteed to provide the receiver expected rewards that are at worst \(\) less than those attained by the best possible action.

In conclusion, the goal is to find an _optimal_ signaling scheme for the sender, which is one achieving a sender's expected reward (from step one) \(V^{,}\) greater than or equal to \(\), defined as follows:

\[:=_{}\,V^{,},V^{,}:=_{s}(s)V_{1}^{,}((s)).\]

## 3 History-dependent signaling schemes are necessary

Previous works studying Bayesian persuasion in MDPs [Wu et al., 2022, Gan et al., 2022a] focus on Markovian signaling schemes, in which the action recommendation at step \(h\) only depends on the current state \(s_{h}\) and private observation \(_{h}\). Indeed, considering this class of signaling schemes is sufficient to optimize the utility of a sender facing a myopic receiver. Here, we show that this is _not_ the case when the receiver is farsighted. In particular, we show that non-stationary Markovian signaling scheme are suboptimal. To do so, we show that there exists an MDP (see Appendix A) in which the optimal persuasive signaling scheme is history-dependent.7 Intuitively, an history-dependent signaling scheme can adjust action recommendations depending on the choices available to the receiver in previous steps. Thus, if the receiver had profitable opportunities in the past, the sender must provide a larger expected reward to the receiver in order to be persuasive. Otherwise, the sender can aggressively maximize their expected rewards irrespective of receiver's ones. Formally:

**Theorem 1**.: _There exist instances in which a persuasive history-dependent signaling scheme guarantees sender's expected rewards strictly greater than that obtained by an optimal persuasive non-stationary Markovian signaling scheme._Moreover, we show that optimal non-stationary Markovian signaling schemes are \(\)-hard to approximate in polynomial time, even when the persuasiveness requirement is relaxed. This further motivates the use of history-dependent signaling schemes when addressing Bayesian persuasion problems in finite-horizon MDPs with a farsighted receiver. Formally, we prove the following:

**Theorem 2**.: _There exist two constants \(<1\) and \(>0\) such that computing an \(\)-persuasive non-stationary Markovian signaling that provides the sender with at least a fraction \(\) of the optimal sender's expected reward \(\) is \(\)-hard._

## 4 A sufficient subclass of efficiently-representable signaling schemes

Working with history-dependent signaling schemes begets unavoidable computational issues. These are due to the fact that explicitly representing such signaling schemes requires a number of bits growing exponentially in the size of the problem instance, since the number of possible histories is exponential in the time horizon \(H\). In this section, we show how to circumvent such an issue by introducing a convenient subclass of signaling schemes--called _promise-form_ signaling schemes--which are efficiently representable while being as good as history-dependent ones. In particular, our main result in this section (Theorem 3) shows that there always exists a promise-form signaling scheme which results in sender's expected rewards equal to its optimal value \(\).

### Promise-form signaling schemes

A promise-form signaling scheme is defined by a set \(:=\{(I_{h},_{h},g_{h})\}_{h}\) of triplets, where:

* \(I_{h}: 2^{[0,H]}\) is a function defining, for every state \(s\), a finite set \(I_{h}(s)[0,H]\) of _promises_ for step \(h\). We add the additional requirement that \(0 I_{1}(s)\) for all \(s\), and, for ease of notation, we set \(I_{H+1}(s):=\{0\}\) for every \(s\) and \(:=_{h}_{s}I_{h}(s)\).
* \(_{h}:()\) is an _action-recommendation strategy_ to be employed at step \(h\), where \(_{h}(a|s,,)\) is the probability of recommending action \(a\) in state \(s\) when the promise is \( I_{h}(s)\) and the sender's private observation is \(\).
* \(g_{h}: \) is a _promise function_ for step \(h\) such that, whenever \(h H\) and \( I_{h}(s)\), \(g_{h}(s,a,,s^{}) I_{h+1}(s^{})\) represents the promise for step \(h+1\) if the next state is \(s^{}\) and, at the current step \(h\), action \(a\) is recommended in state \(s\).8

Intuitively, the rationale behind promise-form signaling schemes is that, when reaching a state \(s_{h}\) at step \(h\), the sender "promises" a value \( I_{h}(s_{h})\) to the receiver, representing a lower bound on future rewards obtained by following recommendations. Moreover, sender's action recommendations only depend on the current state \(s_{h}\), the sender's private observation \(_{h}\), and the current promise \( I_{h}(s_{h})\), through the distribution \(_{h}(s_{h},_{h},_{h})\). Notice that it is always possible to infer the current promise by looking at the history of past states and action recommendations, by "composing" the functions \(g_{h^{}}\) for the steps \(h^{}<h\). This crucially avoids having to specify an explicit dependency on the full history of past states and action recommendations.

Notice that a promise-form signaling scheme as defined above does _not_ automatically guarantee that the sender _honestly_ keeps their promises. Indeed, in order to ensure that this is the case, we need to enforce additional constraints on the components of the signaling scheme, as we show in Section 4.3.

Let us remark that representing promise-form signaling schemes requires a number of bits polynomial in the size of the problem instance and in \(||\), which is the cardinality of the set of promises. While \(||\) could be arbitrarily large in general, the algorithm that we will present in the following Section 5 guarantees that \(||\) has "small" size, by means of a clever choice of the functions \(I_{h}\).

### From promise-form to history-dependent signaling schemes

In the following, we show how the sender can implement promise-form signaling schemes, proving that they represent a subclass of history-dependent ones.

The sender can implement a promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\) as follows. After committing to \(\) (Line 1 of Algorithm 1), at each step \(h\), in Line 6 of Algorithm 1 they select which action-recommendation strategy to use by reconstructing the current promise on the basis of the history \(_{h}\). Such a reconstruction is done by recursively "composing" functions \(g_{h^{}}\) for the preceding steps \(h^{}<h\), by means of the procedure in Algorithm 2 with \(\) and \(_{h}\) as inputs. By letting \( I_{h}(s_{h})\) be the continuation value obtained by running Algorithm 2, the action recommendation \(a_{h}\) in Line 6 is then sampled from \(_{h}(s_{h},,_{h})\). Algorithm 2 clearly runs in time polynomial in the instance size, and, thus, the sender can implement a promise-form signaling scheme efficiently.

```
0:\(:=\{(I_{h},_{h},g_{h})\}_{h}\), \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\)
1: Initialize \( 0 I_{1}(s_{1})\)
2:for each step \(h^{}=1,,h-1\)do
3:\( g_{h^{}}(s_{h^{}},a_{h^{}},,s_{h^{ }+1})\)
4:return\(\) ```

**Algorithm 2** From histories to promises

In the rest of this section, given a promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\), we denote by \(^{}:=\{^{}_{}\}_{}\) the history-dependent signaling scheme _induced_ by \(\) thorough the implementation procedure described above. Formally, for every history \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\) up to step \(h\), the function \(^{}_{}:()\) is defined so that \(^{}_{}()=(s_{h},^{}_{},)\) for every \(\), where \(^{}_{} I_{h}(s_{h})\) denotes the promise value corresponding to history \(\), as computed by Algorithm 2 with \(\) and \(\) as inputs. As it is easy to see, implementing the promise-form signaling scheme \(\) as described above is equivalent to using \(^{}\) in Algorithm 1.

Next, we show that the value functions of the sender and the receiver associated with the induced history-dependent signaling scheme \(^{}\) can be efficiently computed by only accessing the components of the promise-form signaling scheme \(\). In the following, given any \(:=\{(I_{h},_{h},g_{h})\}_{h}\), for every step \(h\) we introduce the functions \(^{,}_{h}:\) and \(^{,}_{h}: \), which are jointly recursively defined so that, for every \(a\), \(s\), and \( I_{h}(s)\), it holds:

\[^{,}_{h}(a,s,)= _{}_{h}(|s)_{h}(a|s,, )r^{}_{h}(s,a,)+_{s^{}}p _{h}(s^{}|s,a,)^{,}_{h+1}(s^{},g_ {h}(s,a,,s^{}))\]

and \(^{,}_{h}(s,)=_{a}^{,}_{h}(a,s,)\). Similarly, \(^{,}_{h}: \) is such that, for \(s\), \( I_{h}(s)\):

\[^{,}_{h}(s,)= _{a}_{}_{h}(|s) _{h}(a|s,,)r^{}_{h}(s,a,)+_{s^{ }}p_{h}(s^{}|s,a,)^{, }_{h+1}(s^{},g_{h}(s,a,,s^{})).\]

Then, we can prove the following lemma:

**Lemma 1**.: _Given a promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\), for every \(h\) and history \(=(s_{1},a_{1},,s_{h-1},a_{h-1},s_{h})_{h}\) up to step \(h\), the following holds:_

\[V^{,^{}}_{h}(a,)=^{,}_{h}(a,s _{h},^{}_{}),\ V^{,^{}}_{h}()=^{,}_{h}(s_{h},^{}_{}),\ \ V^{,^{}}_{h}()=^{, }_{h}(s_{h},^{}_{}).\]

Intuitively, Lemma 1 establishes that the functions \(^{,}_{h}\), \(^{,}_{h}\), and \(^{,}_{h}\) "correctly" encode the value functions of the sender and the receiver for a promise-form signaling scheme \(\), when it is implemented according to the procedure described at the beginning of the section.

Finally, given how a promise-form signaling scheme \(\) is implemented, in the following we say that \(\) is \(\)-persuasive (for some \( 0\)) if the induced history-dependent signaling scheme \(^{}\) is \(\)-persuasive according to Definition 1. However, using such a definition to check whether \(\) is \(\)-persuasive is clearly computationally inefficient, since it would require working with exponentially-many histories. In Section 4.3, we introduce an easy way to ensure that a promise-form signaling scheme "keeps its promises", and we show that this allows to encode \(\)-persuasiveness constraints in an efficient way.

### The power of honesty

Next, we introduce a particular class of promise-form signaling schemes which always guarantee that the sender _honestly_ (approximately) assures promised rewards to the receiver. We call \(\)_-honest_ the promise-form signaling schemes with such a property, which are formally defined as follows:

**Definition 2** (\(\)-honesty).: _Let \( 0\). A promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\) is \(\)-honest if, for every step \(h\), state \(s\), and promise \( I_{h}(s)\), the following holds:_

\[_{a}_{}_{h}(|s)_{h}(a|s, ,)(r^{}_{h}(s,a,)+_{s^{}}p _{h}(s^{}|s,a,)g_{h}(s,a,,s^{}))-.\] (1)Then, we can prove the following result on \(\)-honest promise-form signaling schemes:

**Lemma 2**.: _Let \(:=\{(I_{h},_{h},g_{h})\}_{h}\) be a promise-form signaling scheme. If \(\) is \(\)-honest, then, for every step \(h\) and state \(s\), it holds that \(_{h}^{,}(s,)-(H-h+1)\) for all \( I_{h}(s)\)._

Since by Lemma 1 the function \(_{h}^{,}\) encodes the receiver's value function when \(\) is implemented by the sender, Lemma 2 intuitively establishes that, at each step \(h\) and state \(s\), the signaling scheme actually "keeps the promise" of giving at least \( I_{h}(s)\) future rewards to the receiver, up to an error depending on \(\). Checking whether a promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\) is \(\)-honest or not can be done in time polynomial in the instance size and \(||\).

Now, we are ready to prove the following crucial lemma:

**Lemma 3**.: _Let \(:=\{(I_{h},_{h},g_{h})\}_{h}\) be an \(\)-honest promise-form signaling scheme such that, for every \(h\), \(s\), \( I_{h}(s)\), and \(a,a^{}\), the following constraint is satisfied:_

\[_{}_{h}(|s)_{h}(a|s,, )(r_{h}^{}(s,a,)+_{s^{}}p_{ h}(s^{}|s,a,)g_{h}(s,a,,s^{}))\] \[_{}_{h}(|s)_{h}(a|s,, )(r_{h}^{}(s,a^{},)+_{s^{} }p_{h}(s^{}|s,a^{},)_{h+1}^{}(s^{})).\] (2)

_Then, we can conclude that \(\) is \(( H)\)-persuasive._

Intuitively, Lemma 3 states that for an \(\)-honest promise-form signaling scheme, the constraint in Equation (2) is equivalent to the one in Definition 1. The crucial advantage of Equation (2) is that it allows to express persuasiveness conditions as "local" constraints which do _not_ require recursion.

### Promise-form signaling schemes are sufficient

Finally, by exploiting Lemma 3, we can prove the main result of this section: promise-form signaling schemes represent a sufficient subclass of history-dependent ones. Formally:

**Theorem 3**.: _There is always a persuasive promise-form signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\) with sender's expected reward equal to \(\). More formally, it holds that \(V^{,^{}}=\) for the history-dependent signaling scheme \(^{}\) induced by \(\)._

## 5 Approximation scheme

Theorem 3 shows that, in order to find an optimal signaling scheme, one can focus on promise-form signaling schemes that have the nice property of being polynomially representable in the instance size and the cardinality \(||\) of the set of promises. In this section, we show how to compute an \(\)-persuasive promise-form signaling scheme with sender's expected rewards at least \(\) in polynomial time.

We design an algorithm working with sets \(I_{h}(s)\) defined on a suitable grid, whose size can be properly controlled by a discretization step \(\). The algorithm solves a recursively-defined optimization problem for each \(h\), \(s\), and \( I_{h}(s)\), by starting from step \(H\) and proceeding in bottom up fashion.

For every step \(h\) and state \(s\), the set \(I_{h}(s)\) of promises is defined as a suitable subset of a grid \(_{}\{k\, k k }{{}}\},\) where \(>0\) is a discretization step to be set depending on the desired relaxation \(\) of the persuasiveness constraints. This also allows us to control the representation size of promise-form signaling schemes, as well as the running time of the algorithm. In particular, it holds \(|_{}|=O(1/)\) and, thus, \(||=O(1/)\) since \(_{}\) by definition. In the following, for ease of notation, we let \( x_{}:=_{k:k x}k\) be the smallest multiple of \(\) greater than \(x\), while \( x_{}:=_{k:k x}k\) is the greatest multiple of \(\) smaller than \(x\).

The algorithm keeps track of recursively-computed values in a set of tables, one for each step. The table at step \(h\) is encoded by means of a function \(M_{h}^{}:_{}\{-\}\). Intuitively, for every \(s\) and \(_{}\), the entry \(M_{h}^{}(s,)\) is related to the expected rewards achieved by the sender when "promising" the receiver expected rewards "approximately equal" to \(\) in state \(s\) at step \(h\). We also admit the functions \(M_{h}^{}\) to take value \(-\), which semantically corresponds to the case in which it is impossible to guarantee the promise \(\) to the receiver in state \(s\) at step \(h\). The entry \(M_{h}^{}(s,)\) of the table \(M_{h}^{}\) at step \(h\) is computed recursively by solving a problem \(_{h,s,}(M_{h+1}^{})\) that we define in the following, where \(M_{h+1}^{}\) is the (previously-computed) table at step \(h+1\).

By letting \(M:_{}\{-\}\) be a function encoding a generic table over \(_{}\), for every step \(h\), state \(s\), and promise \( I_{h}(s)\), we define the value \(_{h,s,}(M)\) of the optimization problem \(_{h,s,}(M)\) as follows:

\[_{h,s,}(M):=_{:()\\ :_{}}F _{h,s,M}(,q)(,q)_{}^{h,s},\]

where problem variables are encoded by the functions \(:()\) and \(q:_{}\), which represent an action-recommendation strategy and a promise function, respectively.9 The objective function \(F_{h,s,M}(,q)\) of the optimization problem is defined as:

\[F_{h,s,M}(,q):=_{}_{a}_{h}( |s)(a|)(r_{h}^{}(s,a,)+_{s^{} }p_{h}(s^{}|s,a,)M(s^{},q(a,s^{}))),\]

which encodes the sender's expected reward when their values for the next step \(h+1\) are those specified by the table \(M\). We assume that \(0(-)=-\). Moreover, the set \(_{}^{h,s}\) is comprised of the functions \(:()\) and \(q:_{}\) that satisfy the following constraints:

\[_{a}_{}_{h}(|s) (a|)(r_{h}^{}(s,a,)+_{s^{} }p_{h}(s^{}|s,a,)q(a,s^{}))\] (4a) \[_{}_{h}(|s)(a|)(r_{ h}^{}(s,a,)+_{s^{}}p_{h}(s^{}|s,a, )q(a,s^{}))\] \[_{}_{h}(|s)(a|) (r_{h}^{}(s,a^{},)+_{s^{}}p _{h}(s^{}|s,a^{},)}_{h+1}^{}(s ^{})) a,a^{},\] (4b)

where Equation (4a) and Equation (4b) play the role of the honesty and the persuasiveness constraints, respectively. Notice that relaxing the honesty constraint yields larger feasible sets. Formally, for any \(^{} 0\) we have that the following holds: \(_{}^{h,s}_{^{}}^{h,s}\).

If \(F_{h,s,M}(,q)=-\) for all \((,q)_{}^{h,s}\), we have that \(_{h,s,}(M)=-\). This intuitively comes from the fact that, if \(_{h,s,}(M)=-\), then the value \(\) promised to the receiver is not realizable.

The optimization problem \(_{h,s,}(M)\) could be easily cast as a mixed-integer quadratic program, which are too general to be solved efficiently. Thus, we need specifically-tailored procedures to find an approximate solution to it. This discussion is deferred to Section 6. In the following, we assume to have access to an oracle \(_{h,s,}(M)\) that provides a suitable approximate solution to \(_{h,s,}(M)\). In particular, \(_{h,s,}(M)\) must satisfy the requirements introduced by the following definition:

**Definition 3** (Approximate Oracle).: _An algorithm \(_{h,s,}(M)\) is an approximate oracle for \(_{h,s,}(M)\) if it returns a tuple \((,q,v)\) such that \(_{h,s,}(M) v F_{h,s,M}(,q)\) and \((,q)_{-}^{h,s}\)._

Intuitively, we ask that an approximate oracle finds a solution \((,q)\) in a slightly larger set \(_{-}^{h,s}\). The oracle also returns a value \(v\) to be inserted into \(M_{h}^{}(s,)\), where \(v\) is possibly different from the value \(F_{h,s,M}(,q)\) of the objective function. This is needed for technical reasons in order to recover some concavity properties of the functions defining the tables used by the algorithm, which may be lost due to approximations. A complete discussion on this last aspect can be found in Section 6.

Equipped with an approximate oracle as in Definition 3, we are ready to design our approximation scheme that computes an \(\)-persuasive promise-form signaling scheme attaining sender's expected reward at least \(\) (Algorithm 3). The algorithm iteratively builds each table \(M_{h}^{}\) by filling it with the values \(v\) returned by the approximate oracle. Moreover, it sets action-recommendation strategies \(_{h}\) and promise functions \(g_{h}\) of the signaling scheme \(:=\{(I_{h},_{h},g_{h})\}_{h}\) to be equal to the functions \(\), \(q\) returned by the approximate oracle.

[MISSING_PAGE_FAIL:9]

\(s^{}\). Notice that, for ease of notation, we drop the dependence of the operator \(\) from \(M\), as such a dependence is always clear from the context.

By exploiting the notation introduced above, the relaxed optimization problem \(_{h,s,}(M)\) reads as:

\[_{h,s,}(M):=_{:( )\\ :(D_{})} _{h,s,M}(,)(, _{})_{}^{h,s},\]

where we relax the functions \(\) to be distributions over \(_{}\), rather than deterministic. Then, we can prove the following:

**Lemma 6**.: _The problem \(_{h,s,}(M)\) can be solved in time polynomial in \(1/\) and the instance size._

By relying on a solution \((,)\) to \(_{h,s,}(M)\), Algorithm 4 is an approximate oracle for \(_{h,s,}(M)\). In particular, when the relaxed problem is feasible, the algorithm returns \(k\) and the function \(q\) obtained by de-randomizing the randomized function \(\) with its discrete average. Formally:

**Theorem 5**.: _For every \(h\), \(s\), and \(_{}\), if Algorithm 4 is used for all \(h^{}>h\) as approximate oracle \(_{h^{},s,}\) in Algorithm 3, then it implements an approximate oracle as in Definition 3._

Notice that the value \(v\) returned by Algorithm 4 is the optimal value of the relaxed problem \(_{h,s,}(M)\). It is easy to prove (see the proof of Theorem 5) that the tables built by Algorithm 3 through Algorithm 4 are encoded by concave functions. This is the key feature that allows us to go from a randomized solution to its discrete average without loss in sender's expected rewards, and it is the reason why we need to assume that Algorithm 4 is employed at every step \(h\) in Theorem 5.

## 7 Conclusion

In this paper, we provided a crucial advancement in Bayesian persuasion for sequential decision making, mastering the setting in which a farsighted receiver interacts with an MDP, a problem that was previously thought to be intractable (Wu et al., 2022, Gan et al., 2022, 2022).

First, we have shown that the class of Markovian signaling schemes, the standard choice in previous works and most MDP settings, do not constitute the "right" class of policies, since finding an optimal Markovian signaling scheme is \(\)-hard. Instead, we demonstrated that history-dependent signaling schemes allow to both circumvent the negative complexity result affecting Markovian signaling schemes and guarantee higher expected rewards to the sender in general. Specifically, we designed an algorithm to find an optimal \(\)-persuasive history-dependent signaling scheme in time polynomial in \(1/\) and in the instance size. The crucial component of the algorithm is to restrict the optimization to a convenient subclass of history-dependent signaling schemes, which we call _promise-form_ as they encode the history into a promise of future rewards to the receiver. We showed that promise-form signaling schemes are both efficient to represent and as powerful as the general class of history-dependent signaling schemes. An interesting insight of our analysis reveals that being _honest_, _i.e._, keeping promises of future rewards, is the key to persuade farsighted receivers in MDPs.

As a future work, we will investigate the online learning variant of the problem studied in this paper, as done in (Zu et al., 2021, Wu et al., 2022, Bernasconi et al., 2022), in which the transition function is _not_ known. Moreover, it would be interesting to study the problem faced by a sender that needs to persuade a stream of receivers having adversarially-selected types, as done in (Balcan et al., 2015, Castiglioni et al., 2020, 2021).