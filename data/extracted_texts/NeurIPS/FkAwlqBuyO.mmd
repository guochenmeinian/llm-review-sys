# Direct Preference-based Policy Optimization without Reward Modeling

Gaon An

Seoul National University

white0234@mllab.snu.ac.kr

&Junhyeok Lee

Seoul National University

riman314@mllab.snu.ac.kr

&Xingdong Zuo

NAVER

xingdong.zuo@navercorp.com

&Norio Kosaka

NAVER

Line Corporation

kosaka.norio@linecorp.com

&Kyung-Min Kim

NAVER

kyungmin.kim.ml@navercorp.com

&Hyun Oh Song

Seoul National University

hyunoh@mllab.snu.ac.kr

Corresponding author

###### Abstract

Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the existing PbRL methods. Notably, on high-dimensional control tasks, our algorithm surpasses offline RL methods that learn with ground-truth reward information. Finally, we show that our algorithm can be successfully applied to fine-tune large language models.

## 1 Introduction

Deep reinforcement learning has been successful in solving various decision-making tasks where a well-defined reward function is available . However, in many real-world tasks, it can be challenging to design a quantitative reward function that accurately reflects the desired behavior, particularly when the task involves human evaluation. Preference-based RL (PbRL) seeks to provide an alternative solution by enabling agents to learn from preference information between pairs of trajectory segments . PbRL has gained considerable interest in recent years as making a relative judgment is much easier than providing a real-valued score, which makes human feedback much more viable .

Recent PbRL methods take a two-step approach: they first learn a reward model from the given preference dataset and then run off-the-shelf reinforcement learning algorithms on top of the learned reward model [10; 31; 40]. However, acquiring an accurate reward model only from preference labels, typically provided by human teachers, poses a significant challenge as it is unclear how to extract the underlying reward structure from preference. Current methods rely on modeling the reward with certain specific assumptions, though there are some concerns regarding whether those assumptions hold in practice [13; 27].

Alternatively, predicting the preference itself is comparatively more straightforward since we have direct access to training labels, allowing us to leverage powerful techniques from supervised learning. Building upon this observation, we introduce a PbRL algorithm that bypasses the need for reward function modeling by directly learning from preference labels. Our approach begins by devising a policy scoring metric that assigns high scores to policies aligning with the provided preference dataset. Concretely, the PbRL objective is formulated as a contrastive learning problem, guiding the learned policy to be closer to more preferred trajectory segments while distancing itself from the less preferred ones [9; 20]. Furthermore, we enhance the performance of the preference predictors from previous works by introducing a novel prediction smoothness regularizer. Experiment results on offline RL settings with actual human preference labels show that the proposed algorithm outperforms or is on par with the baselines on all of the tasks considered . Notably, in high-dimensional control tasks, our algorithm outperforms offline RL methods that utilize ground-truth reward information. Moreover, our preliminary experiments show that our algorithm can be successfully applied for fine-tuning large language models. Our official code is available at https://github.com/snu-mllab/DPPO.

## 2 Preliminaries

### Preference-based reinforcement learning

Reinforcement learning considers an environment formulated as a Markov Decision Process (MDP) defined by a tuple \((,,T,,p_{0},H)\), where \(\) is a state space, \(\) is an action space, \(T(^{}|,)\) is the state transition dynamics, \((,)\) is the reward function, \(p_{0}()\) is the initial state distribution, and \(H\) is the time horizon. The goal of reinforcement learning is to learn a policy \(\) that optimizes the expected return:

\[J()=_{_{0} p_{0},_{t}(| _{t}),_{t+1} T(|_{t},_{t})} [_{t=0}^{H}r_{t}].\]

Conventional RL assumes the reward information \((r_{t})\) is given and uses this to optimize their policy. However, finding a suitable reward metric can be costly in many real-world scenarios.

For example, if the goal task is to train a robot to scramble an egg, it would be unclear how to design a reward function that captures all the desired properties. Instead, PbRL assumes the supervision is given in the form of preference. Concretely, for a pair of trajectory segments \((^{0},^{1})\), where \(^{i}=(_{0}^{i},_{0}^{i},_{1}^{i}, _{1}^{i},,_{k}^{i},_{k}^{i})\), a (human) teacher indicates which segment it prefers. The preference label \(y\) is given as \(0\) if \(^{0}\) is preferred, _i.e._, \(^{0}^{1}\). In a similar manner, \(y\) is set to \(1\) if \(^{1}\) is preferred and \(0.5\) if the two are equally preferred.

To learn a reward model \(\), prior works assume the preference depends on the value of the underlying rewards summed over each timestep [10; 6; 31; 40]:

\[[^{0}^{1};]=^{k} (_{t}^{0},_{t}^{0};))}{ (_{t=0}^{k}(_{t}^{0},_{t}^{0 };))+(_{t=0}^{k}(_{t}^ {1},_{t}^{1};))},\]

where \(\) denotes the learnable parameters of the reward model. Given a dataset \(_{}\) of preference triples \((^{0},^{1},y)\), the reward model is trained by minimizing the cross-entropy loss between the preference predictions and the ground-truth labels:

\[_{}(;_{})=-}_{( ^{0},^{1},y)_{}}[(1-y)[^{0}^{1};]+y[^{1} ^{0};]].\] (1)

After training, any standard RL algorithm can be used to maximize the expected return under the learned reward model. Especially in the offline PbRL setting, we assume there exists a small dataset \(_{pref}\) with preference labels along with a much larger unlabeled dataset \(\) without any reward or preference labels [27; 49]. A typical approach involves utilizing \(_{pref}\) to learn the reward model and applying the model to label \(\).

### Contrastive learning

The goal of contrastive learning is to learn representations where similar sample pairs are close to each other while dissimilar pairs are far apart. For an anchor sample \(\) (_e.g._, an image), suppose we have a positive sample \(^{+}\) (_e.g._, the same image with data augmentation applied) and a set of negative samples \(\{_{i}^{-}\}_{i=1}^{m}\) (_e.g._, samples from other images). To learn an encoder \(f\), contrastive learning minimizes the following loss:

\[_{f}(,^{+},\{_{i}^{-}\}_{i= 1}^{m})=-)^{}f (^{+}))}{(f()^{ }f(^{+}))+_{i=1}^{m}(f ()^{}f(_{i}^{-}))},\]

where a dot product of two representations (_e.g._, \(f()^{}f(^{+})\)) is considered as the similarity score between the two samples.

## 3 Learning directly from preference

While prior PbRL approaches adopt a two-step procedure involving the construction of a reward model using the preference data, the fidelity of the learned reward model in reflecting the original reward function remains uncertain. The main challenge lies in extracting the underlying rewards from the given preference. Previous works assume that the preference for a trajectory segment can be represented as an average of the underlying rewards [10; 21]. However, considering PbRL typically assumes human teachers provide the preference labels, it is unclear whether human preferences can also be modeled in this manner. For instance, humans may focus on a specific subset of the segment while ignoring other parts [24; 27]. Empirically, we find that the reward models learned from preferences often fail to accurately capture the underlying reward structure, as illustrated in Figure 2. This issue is problematic since within the current PbRL framework, the quality of the learned policy relies heavily on the quality of the learned rewards.

Meanwhile, predicting the preference itself is a more straightforward task as we have explicit labels to train with and can therefore apply powerful tools from supervised learning [18; 11; 36]. Based on this observation, our goal is to introduce a PbRL algorithm that directly learns with preference information without the need for reward modeling, as illustrated in Figure 1. To achieve this, we design a policy scoring metric that yields a high score when a policy aligns with the given preference dataset. In other words, we assume that a desirable policy should be closer to \(^{0}\) than \(^{1}\) if \(^{0}^{1}\). To formulate this property into an optimizable objective, we first define the _closeness_ between a policy and a trajectory segment.

### Policy-segment distance

We define the distance between a policy and a trajectory segment as an aggregation of the distance between a policy and each transition tuple in the trajectory segment. Concretely, the policy-segment distance can be expressed as

\[d(,^{i})=(d_{}(, _{0}^{i},_{0}^{i}),,d_{}(,_{ k}^{i},_{k}^{i})),\]

where \(d_{}\) denotes the policy-transition tuple distance and AGG denotes an aggregation function. There can be multiple ways for instantiating \(d_{}\). For simplicity, we employ the expected \(_{2}\) distance between the policy action and the trajectory action: \(d_{}(,,)=_{} (|)}[\|}-\|_{2}]\). Similarly, we opt to use the mean operator as the aggregation function. To sum up, our policy-segment distance function becomes

\[d(,^{i})=_{t=0}^{k}(}_{ }(|_{t}^{i})}[\| }-_{t}^{i}\|_{2}]).\]

### Preference score metric

Using the policy-segment distance defined above, we can now build a score metric that satisfies our desired property. Given a preference triple \((^{0},^{1},y)\), assume that \(^{0}\) is preferred over \(^{1}\), _i.e._, \(y\!=\!0\). We want to assign a high score if a policy is closer to \(^{0}\) than \(^{1}\). This condition can be expressed in terms of the policy-segment distance as \(^{0}^{1} d(,^{0})<d(,^{1})\). To capture this condition across multiple segment pairs into a single metric, we adopt a contrastive learning formulation:

\[S(;_{})=}_{( ^{0},^{1},y)_{}}[(1-y) s( _{},^{0},^{1})+y s(_{},^{ 1},^{0})]\] (2) \[\ \ \ s(,^{i},^{j})= ))}{(-d(, ^{i}))+(-d(,^{j}))},\]

where \(\) denotes the learnable parameters of the policy \(\). In terms of contrastive learning, we can interpret \((-d(_{},^{i}))\) as representing the similarity between the policy \(_{}\) and the segment \(^{i}\). Then, \(_{}\), \(^{0}\), and \(^{1}\) are each considered the anchor, the positive sample, and the negative sample, assuming \(^{0}^{1}\). Note that a high score can only be achieved if the policy is closer to more preferred trajectory segments.

While Equation (2) is sufficient to express our desired property, a minor drawback is that the score function is indifferent to the increase or decrease of the distances in the same magnitude. To understand this in detail, let us first denote \(d^{i}=d(_{},^{i})\) for brevity. Then, for a preference triple \((^{0},^{1},y)\) with \(y=0\),

\[s(_{},^{0},^{1})= -d^{0}-((-d^{0})+(-d^{1}) )\{0,d^{0}-d^{1}\}.\] (3)

Figure 3: An overview of the score calculation process. To score a given policy, (1) the first step is to calculate the distance between each transition tuple and the policy. (2) Second, these distances are aggregated to a policy-segment distance through a predefined aggregation function. (3) Finally, we obtain the score value by contrasting the policy-segment distances according to their preference.

Therefore, the score value remains the same when the distances increase or decrease in the same magnitude. In other words, the score for the distances \((d^{0},d^{1})\) and \((d^{0}+,d^{1}+)\) are identical, indicating there is no penalty when a policy deviates from even the preferred trajectory segment. To solve this, we add a regularizing factor \((0,1)\) that decreases the score when the overall scale of the policy-segment distances increases:

\[S(;_{},)=, ^{1},y)_{}}{}[(1-y) s( _{},^{0},^{1};)+y s(_{}, ^{1},^{0};)]\] (4) \[ s(,^{i},^{j};)= ))}{(-d( ,^{i}))+(- d(,^{j}) )}.\]

If we set \(\) smaller than 1 and plug in this new formulation to Equation (3), it is easy to find out that the score will decrease when the overall distances increase. The resulting score calculation process for Equation (4) is illustrated in Figure 3.

### Policy optimization with preference predictor

We can directly optimize a policy with preference labels by maximizing the score function in Equation (4). However, to leverage the unlabeled dataset \(\), we train a preference predictor using the labeled dataset \(_{}\). We formulate this process as a simple binary classification problem and use the cross-entropy loss to optimize the predictor \(\):

\[_{}(;_{},)= -,^{1},y)_{}} {}[(1-y)[^{0}^{1}; ]+y[^{1}^{0};]] }\] (5) \[+)}{ }[([^{};]-0.5)^{2} ],\]

where \(\) denotes the learnable parameters of the preference predictor \(\). The first term resembles Equation (1), but the difference is that here we directly model the preference predictor instead of modeling the reward function. The second term is a smoothness regularizer which guides the predictor to have a similar preference against two largely overlapping segments. Concretely, given \(=(_{i},_{i},,_{i+k},_{i+k})\), we sample \(^{}\) from the same trajectory as \((_{i+},_{i+},,_{i++k}, _{i++k})\), where \((0,m^{2})\) with \(m k\). We observe that if no smoothness regularization is applied (\(=0\)), the preference can vary significantly between two almost identical segments, as illustrated in Figure 4. This behavior is undesirable as a human teacher will unlikely be able to even identify the difference between the two segments.

After training the preference predictor with Equation (5), we can train our policy with the unlabeled dataset \(\) by sampling pairs of trajectory segments and labeling their preferences:

\[S(;,,)=}_{(^{0}, ^{1})}[(1-) s(_{}, ^{0},^{1};)+ s(_{}, ^{1},^{0};)],\] \[=\{[ ^{0}^{1};]>0.5\}.\]

Algorithm 1 summarizes the full process of our PbRL algorithm. Figure 5 shows an example of two policies each learned using our algorithm and behavior cloning (BC), which shows that vanilla BC suffers from imitating the behavior of unpreferred trajectory segments while our algorithm successfully distances from it. We name our preference-based policy optimization algorithm as DPPO, an abbreviation for _Direct Preference-based Policy Optimization_. The sections below evaluate the performance of DPPO in the offline setting.

## 4 Experiments

### Offline RL experiment details

Following recent PbRL works, we evaluate our algorithm on the offline setting which assumes a large unlabeled dataset \(\) is given along with a much smaller preference-labeled dataset \(_{}\)[49; 27]. We evaluate our algorithm on D4RL, a standard benchmark for offline RL, with preference datasets generated by actual human teachers . We did not experiment on Antmaze, a widely used task in D4RL, due to a crucial bug in its environment implementation (please refer to Appendix F for more details). Below is a brief description of the tasks considered in our experiments:

GymD4RL Gym provides datasets for the Gym locomotion environments where the goal task is to move forward while maintaining body balance. D4RL Gym contains various types of datasets each from a different data collection process. Among those datasets, we focus on the _medium-replay_ and _medium-expert_ datasets, which contain trajectories with the most diverse quality.

Advit penAdvit tasks involve controlling a 24-DoF robotic hand to perform tasks such as grabbing a pen or hammering a nail . Adroit tests if offline RL methods can learn from human demonstrations on these high-dimensional tasks. We focus on the _pen_ task since standard offline RL algorithms fail to achieve reasonable performance on other tasks, even with the reward information.

KitchenKitchen requires learning a 9-DoF robot manipulation task . Similar to Adroit, the reward function is sparse and the dataset contains human demonstrations. However, Kitchen requires solving multiple sub-tasks sequentially (_e.g._, opening the microwave, then turning off the switch) in a single episode. While Kitchen has three types of datasets (_complete_, _partial_, and _mixed_), we consider the latter two as _complete_ only contains successful trajectories with almost identical quality.

Figure 5: DPPO vs. BC for two example trajectory segments on the walker2d-medium-replay dataset of D4RL Gym. DPPO successfully avoids the behavior from the unpreferred trajectory segment while BC also clones the unpreferred behavior.

For the Gym hopper, Gym walker2d, and Adroit pen tasks, we utilize publicly available human preference datasets released by . For the other tasks, we generate a new preference dataset as there are no preference datasets available. To collect the human preference datasets for each task, we strictly adhere to the protocol outlined by . This involves defining a set of desirable behaviors and instructing the human teacher to label preferences accordingly. For example, a desired behavior in the Hopper environment would be to maintain body balance and prevent falling down. The size of the resulting preference datasets ranges from 100 to 500 samples, depending on the specific task. For more details regarding the dataset generation process, please refer to Appendix A.

We consider PreferenceTransformer (PT) as our baseline method, which is a state-of-the-art approach in offline PbRL . PT employs a transformer network to train the reward model and leverages IQL, an offline RL algorithm, for policy optimization using the learned rewards . This original version is denoted as PT+IQL. Considering that the choice of policy optimization algorithm significantly impacts the performance of reward modeling methods, we also explore using CQL for policy optimization . This modified version of PT is denoted as PT+CQL. Additionally, we present the performance of CQL and IQL when utilizing the ground-truth reward information for reference. Note that these reward-based RL baselines do not provide a fair comparison with our method as they have access to a much denser supervision signal. For more implementation details regarding the baselines and our algorithm, please refer to Appendix B. For more experiments including behavior-cloning baselines, please refer to Appendix C.

### Evaluation results

Table 1 shows the evaluation results for the D4RL Gym tasks. DPPO demonstrates superior or comparable performance to the preference-based learning methods across all considered tasks. In terms of average performance, our method outperforms the baselines by a large margin with a minimum of %11p and reaches a performance level similar to the methods that learn with ground-truth rewards. Also, DPPO exhibits significantly lower variance in performance compared to the baseline methods like PT+IQL, which suffer from pronounced fluctuations in performance.

    &  &  \\  Task Name & CQL & IQL & PT+CQL & PT+IQL & DPPO (Ours) \\  halfcheetah-medium-replay & 45.7 \(\) 0.6 & 44.3 \(\) 0.7 & 27.1 \(\) 17.7 & **42.3 \(\) 0.5** & 40.8 \(\) 0.4 \\ hopper-medium-replay & 84.1 \(\) 14.2 & 100.5 \(\) 1.4 & 49.1 \(\) 22.0 & 59.7 \(\) 25.8 & **73.2 \(\) 4.7** \\ walker-medium-replay & 80.0 \(\) 3.4 & 74.8 \(\) 3.4 & **52.8 \(\) 7.2** & 43.3 \(\) 39.8 & **50.9 \(\) 5.1** \\  halfcheetah-medium-expert & 88.5 \(\) 9.7 & 85.2 \(\) 7.4 & 77.1 \(\) 0.9 & 83.6 \(\) 3.8 & **92.6 \(\) 0.7** \\ hopper-medium-expert & 103.7 \(\) 7.5 & 84.1 \(\) 24.1 & 89.2 \(\) 14.4 & 67.8 \(\) 32.3 & **107.2 \(\) 5.2** \\ walker2d-medium-expert & 108.4 \(\) 0.3 & 107.5 \(\) 4.4 & 77.7 \(\) 1.2 & **109.8 \(\) 0.4** & **108.6 \(\) 0.1** \\  Average & 85.1 & 82.7 & 62.2 & 67.8 & **78.8** \\   

Table 1: Normalized average return on D4RL Gym tasks, averaged over 5 seeds. \(\) denotes the standard deviation.

    &  &  \\  Task Name & CQL & IQL & PT+CQL & PT+IQL & DPPO (Ours) \\  pen-human & 44.2 \(\) 7.8 & 53.8 \(\) 36.9 & 31.6 \(\) 3.3 & 53.0 \(\) 31.7 & **76.3 \(\) 14.4** \\ pen-cloned & 42.4 \(\) 5.1 & 51.3 \(\) 37.1 & 18.3 \(\) 10.6 & 42.9 \(\) 24.4 & **75.1 \(\) 7.7** \\  Average & 43.3 & 52.6 & 25.0 & 48.0 & **75.7** \\  kitchen-mixed & 10.7 \(\) 10.8 & 50.6 \(\) 6.2 & 12.3 \(\) 7.7 & 48.0 \(\) 11.9 & **52.5 \(\) 3.1** \\ kitchen-partial & 12.9 \(\) 13.0 & 58.8 \(\) 6.5 & 14.1 \(\) 13.0 & 40.2 \(\) 12.3 & **49.4 \(\) 5.7** \\  Average & 11.8 & 54.7 & 13.2 & 44.1 & **51.0** \\   

Table 2: Normalized average return on D4RL Adroit pen and Kitchen tasks, averaged over 5 seeds. \(\) denotes the standard deviation.

Table 2 shows the results on the more challenging D4RL Adroit pen and Kitchen tasks. Once again, DPPO outperforms all the baselines by a large margin. Especially, the performance gap is the largest in the Adroit pen tasks, where DPPO even surpasses the methods that learn with ground-truth rewards. A major distinguishing factor between the Adroit pen and the other environments is the dimensionality of the action space, which amounts to 24, significantly larger than the action spaces ranging from 3 to 9 in other environments. We conjecture that the value learning approach of the baseline methods struggles to scale up to the high-dimensional action spaces of Adroit. Furthermore, DPPO outperforms the baselines on the Kitchen tasks as well, underscoring the scalability of our method to tackle more complex tasks.

### Ablation studies

We assess the importance of the two crucial components of DPPO, which are the conservativeness regularizer \(\) and the smoothness regularizer \(\). The ablation results are presented in Figure 6. The results show that both components are crucial for achieving high performance. Moreover, the experiments demonstrate that DPPO exhibits a considerable degree of robustness to variations in the strength of the regularizations. Since our smoothness regularizer can also be easily applied to reward-modeling baselines, we evaluate how applying this regularizer to the baselines affects the overall performance in Appendix D. While the smoothness regularizer does improve the performance of the baselines, the improved performance falls behind DPPO.

### Effect of dataset size

Figure 6: Ablation study results on the hopper-medium-replay dataset. (a) and (b) each shows the average performance results for DPPO while varying \(\) and \(\).

Figure 7: Average return results of each method while varying the size of the preference dataset.

Previous works primarily focus on evaluating their algorithms using a fixed preference dataset for each task [27; 13]. In this section, we evaluate how the size of the preference dataset impacts the overall performance of PbRL algorithms. Concretely, we vary the size of the preference dataset in the hopper-medium-replay and hopper-medium-expert tasks and measure the performance of PbRL methods on each dataset size. The results are shown in Figure 7. We observe that DPPO consistently outperforms the baseline method on all dataset sizes, displaying higher average performance. Interestingly, on hopper-medium-replay, we find that the baseline method PT+IQL falls into a unique failure mode when the dataset size is 200, which is to stand still and not move forward.

### Experiments with scripted teachers

Following prior works [10; 31; 32], we additionally evaluate our algorithm using preference labels generated by scripted teachers. A scripted teacher is a synthetic teacher who adheres strictly to the task rewards when making decisions: \(^{0}^{1}_{t=0}^{k}r_{t}^{0}>_{t=0}^{k} r_{t}^{1}\). It is important to note that the scripted teacher setting is not as realistic as the human teacher setting, and we include this section for reference purposes. The results in Figure 8 show that our algorithm continues to outperform the baselines in terms of average performance, even in this synthetic setting. In detail, the results for DPPO and PT+IQL resemble the results from the human teacher setting. However, the performance of PT+CQL has dropped significantly compared to the human teacher setting. This disparity of performance reassures the observations from  that preferences from human teachers and scripted teachers have different characteristics and should not be treated interchangeably.

### Fine-tuning LLMs with DPPO

Recent works show that PbRL can be used to fine-tune large language models with human preference, which is commonly termed RLHF (reinforcement learning from human feedback) [52; 39; 4]. In RLHF, a reward model is trained to predict human preference between two output texts, and then the learned reward model is employed to fine-tune the language model through off-the-shelf RL algorithms such as PPO. As from offline RL, we can fine-tune a language model directly with the preference predictions by replacing PPO with DPPO. This replacement allows removing unnecessary assumptions required to run reward-based policy optimization techniques like PPO, for example assuming the reward is given only at the end of the output sequence. To empirically examine if DPPO can be used to fine-tune large language models, we conducted some preliminary experiments using human preference datasets to train the preference models and human evaluation to evaluate the tuned language model. We refer to Appendix E for more details regarding the implementation details and the experimental setup.

The results in Table 3 show that DPPO can be successfully used to fine-tune large language models with real human preference, achieving performance comparable to the reward-based RLHF method. We believe that this result is promising as it shows that DPPO can be used to fine-tune large language models without the need for reward modeling.

   Fine-tuning method & Avg. reward (\(\)) & KL divergence (\(\)) & Human eval. \\  & & win rate (\(\)) \\  PPO & 4.335 (+1.192) & 0.0091 & 0.667 \\ DPPO (Ours) & **4.515 (+1.372)** & **0.0083** & **0.697** \\   

Table 3: RLHF results using DPPO (ours) compared to PPO. The values in the parentheses denote the gain on average reward compared to the original model.

Figure 8: Average performance on the scripted teacher setting.

Related works

Preference-based reinforcement learningA long line of works has studied learning agents from human preference [1; 41; 12; 3; 47]. Notably,  showed that we can scale PbRL by learning a reward model from preference and applying it to off-the-shelf RL algorithms. Several follow-up works have been proposed building upon this work. For example, a work merges the PbRL framework with imitation learning by introducing other types of supervision such as demonstrations . Other works utilize techniques from semi-supervised learning or data augmentation to improve sample efficiency [31; 40]. Another line of work aims to improve the reward model by removing Markovian assumptions [13; 27]. Recent works have shown that PbRL can greatly boost the performance of large-scale language models by finetuning them with human feedback [52; 56; 37; 39; 45; 38]. Similar to our work,  leverages the preference information to directly optimize the policy using the concept of trajectory distance. However, their method requires the two trajectories to start from the same initial state and to roll out from the current policy being trained, which makes it challenging to utilize extensive pre-collected data. Concurrent to our work, [25; 43] also explore the idea of directly optimizing the policy with preference information.

Offline reinforcement learningOffline reinforcement learning methods adopt various techniques to bias the learned policy towards the given offline dataset. For example, some works directly regularize the policy to prefer the actions from the dataset [29; 57; 17]. Another line of works implicitly biases the policy by regularizing the value function [30; 2; 58]. Some works instead start from a SARSA-style TD-update algorithm to avoid querying values for out-of-distribution actions [7; 28]. A more recent line of work casts the offline RL problem as a sequence modeling problem, where a model is learned to generate actions conditional to the given target return [8; 22; 33; 46; 14]. This approach soon gained popularity in the literature due to its high stability and scalability compared to the more traditional value-based approaches [33; 46].

Contrastive learningA core design choice for contrastive learning is to define the positive and negative sample pairs. An earlier work utilizes the structure of the data and considers two different patches from a single sample as a positive pair . The most popular approach is to apply a heavy data augmentation to a single sample repeatedly to produce positive pairs [9; 20]. The success of these methods in unsupervised vision representation learning has inspired many follow-up works, such as application to supervised learning or extension to other domains such as NLP or graph [26; 42; 59].

In the RL domain, there also have been works that leverage contrastive learning to learn unsupervised representations tailored for RL . Also, a recent work directly casts the contrastive learning framework as a goal-conditioned RL problem .

## 6 Discussion

Our proposed PbRL algorithm enables agents to learn directly from preference signals, removing the need for reward modeling. We achieve this by formulating a new policy optimization problem under the contrastive learning framework. Thorough empirical evaluations on various offline PbRL tasks with actual human rewards show our method outperforms the baselines in most of the tasks considered. Interestingly, our algorithm shows better scalability to high-dimensional control tasks when compared to other RL baselines, including those that learn with ground-truth reward information. Additionally, our algorithm demonstrates better data efficiency. These results show that directly utilizing preference signals without reward modeling is a promising direction for PbRL.

A limitation of our work is that label noise, which is likely to exist for human evaluations, is not modeled in our algorithm. Investigating the effect of label noise stemming from human teachers would be an interesting direction for future research . Also, our current algorithm does not incorporate the prediction confidence information, which was excluded as the empirical performance is already strong compared to the baselines. How to appropriately incorporate the prediction confidence would also be a meaningful research direction.