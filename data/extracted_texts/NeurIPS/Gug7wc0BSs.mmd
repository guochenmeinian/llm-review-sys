# Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training

Pihe Hu

Tsinghua University

Beijing, China

hupihe@gmail.com

&Shaolong Li

Central South University

Changsha, China

shaolongli16@gmail.com

&Zhuoran Li

Tsinghua University

Beijing, China

lizr20@mails.tsinghua.edu.cn

&Ling Pan

Hong Kong University of

Science and Technology

Hong Kong, China

lingpan@ust.hk

Equal contribution.Corresponding author.

Longbo Huang

Tsinghua University

Beijing, China

longbouhang@tsinghua.edu.cn

###### Abstract

Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-(\(\)) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to \(20\) in Floating Point Operations (FLOPs) for both training and inference, with less than \(3\%\) performance degradation.

## 1 Introduction

Multi-agent reinforcement learning (MARL) (Shoham and Leyton-Brown, 2008), coupled with deep neural networks, has not only revolutionized artificial intelligence but also showcased remarkable success across a wide range of critical applications. From mastering multi-agent video games such as Quake III Arena (Jaderberg et al., 2019), StarCraft II (Mathieu et al., 2021), Dota 2 (Berner et al., 2019), and Hide and Seek (Baker et al., 2020) to guiding autonomous robots through intricate real-world environments (Shalev-Shwartz et al., 2016; Da Silva et al., 2017; Chen et al., 2020), deep MARL has emerged as an indispensable and versatile tool for addressing complex, multifaceted challenges. Its unique capability to capture intricate interactions and dependencies among multipleagents has spurred novel solutions, solidifying its position as a transformative paradigm across various domains (Zhang et al., 2021; Albrecht et al., 2023).

However, the exceptional success of deep MARL comes at a considerable computational cost. Training these agents involves the intricate task of adapting neural networks to accommodate an expanded parameter space, especially in scenarios with a substantial number of agents. For instance, the training regimen for AlphaStar (Mathieu et al., 2021), tailored for StarCraft II, extended over a grueling 14-day period, employing 16 TPUs per agent. Similarly, the OpenAI Five (Berner et al., 2019) model for Dota 2 underwent an extensive training cycle spanning 180 days and harnessing thousands of GPUs. This exponential increase in computational demands as the number of agents grows (and the corresponding joint action and state spaces) poses a significant challenge during MARL deployment.

To tackle these computational challenges, researchers have delved into dynamic sparse training (DST), a method that trains neural network models with dynamically sparse topology. For example, RigL (Evci et al., 2020) can train a 90%-sparse network from scratch in deep supervised learning without performance degradation. However, in deep reinforcement learning (DRL), the learning target evolves in a bootstrapping manner (Tesauro et al., 1995), and the distribution of training data is path-dependent (Desai et al., 2019), posing additional challenges to sparse training. Improper sparsification can result in irreversible damage to the learning path (Igl et al., 2020). Initial attempts at DST in sparse single-agent DRL training have faced difficulties in achieving consistent model compression across diverse environments, as documented in (Sokar et al., 2022; Graesser et al., 2022). This is mainly because sparse models may introduce significant bias, leading to unreliable learning targets and exacerbating training instability as agents learn through bootstrapping. Moreover, the partially observable nature of each agent makes training non-stationarity inherently more severe in multi-agent settings. Collectively, these factors pose significant challenges for value learning in each agent under sparse models.

We present a motivating experiment in Figure 1, where we evaluated various sparse training methods on the 3s5z task from SMAC (Samvelyan et al., 2019) using a neural network with only \(10\%\) of its original parameters. Classical DST methods, including SET (Mocanu et al., 2018) and RigL (Evci et al., 2020), demonstrate poor performance in MARL scenarios, along with using static sparse networks (SS). Additionally, RLx2 as proposed in (Tan et al., 2022) proves ineffective for multi-agent settings, despite enabling DST for single-agent settings with \(90\%\) sparsity. In contrast, our MAST framework in this work achieves a win rate of over \(90\%\). In addition, the only prior attempt to train sparse MARL agents, as described in (Yang et al., 2022), prunes agent networks during training with weight grouping (Wang et al., 2019). However, this approach fails to maintain sparsity throughout training, and only achieves a final model sparsity of only \(80\%\). Moreover, their experiment is confined to a two-user environment, PredatorPrey-v2, in MuJoCo (Todorov et al., 2012). These observations highlight the fact that, despite its potential, the application of sparse networks in the context of MARL remains largely unexplored (A comprehensive literature review is deferred in Appendix A.1). Consequently, a critical and intriguing question arises:

_Can we train MARL agents effectively using ultra-sparse networks throughout?_

We affirmatively address the question by introducing a novel sparse training framework, Multi-Agent Sparse Training (MAST). Since improper sparsification results in network fitting errors in the learning targets and incurs large policy inconsistency errors in the training samples, MAST ingeniously integrates the Soft Mellowmax Operator with a hybrid TD-(\(\)) schema to establish reliable learning targets. Additionally, it incorporates a novel dual replay buffer mechanism to enhance the distribution of training samples. Leveraging these components, MAST employs gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Consequently, MAST facilitates the training of highly efficient MARL agents with minimal performance compromise, employing ultra-sparse networks throughout the training process.

Our extensive experimental investigation across various value-based MARL algorithms on multiple SMAC benchmarks reveals MAST's ability to achieve model compression ranging from \(5\) to \(20\), while incurring minimal performance trade-offs (under \(3\%\)). Moreover, MAST demonstrates an

Figure 1: Comparison of different sparse training methods.

impressive capability to reduce the Floating Point Operations (FLOPs) required for both training and inference by up to \(20\), showcasing a significant margin over other baselines (detailed in Section 4).

## 2 Preliminaries

Deep MARLWe model the MARL problem as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek et al., 2016), represented by a tuple \(,,,P,r,,O,\). Deep Multi-Agent \(Q\)-learning extends the deep \(Q\) learning method (Mnih et al., 2013) to multi-agent scenarios (Sunehag et al., 2018; Rashid et al., 2020; Son et al., 2019). The agent-wise Q function is defined over its history \(_{i}\) as \(Q_{i}\) for agent \(i\). Subsequently, the joint action-value function \(Q_{}(,)\) operates over the joint action-observation history \(\) and joint action \(\). The objective, given transitions \((,,r,^{})\) sampled from the experience replay buffer \(\), is to minimize the mean squared error loss \(()\) on the temporal-difference (TD) error \(=y-Q_{}(,)\). Here, the TD target \(y=r+_{^{}}_{}(^{},^{})\), where \(_{}\) is the target network for the joint action \(Q\)-function, periodically copied from \(Q_{}\). Parameters of \(Q_{}\) are updated using \(^{}=-_{}()\), with \(\) representing the learning rate.

We focus on algorithms that adhere to the Centralized Training with Decentralized Execution (CTDE) paradigm (Kraemer and Banerjee, 2016), within which, agents undergo centralized training, where the complete action-observation history and global state are available. However, during execution, they are constrained to individual local action-observation histories. To efficiently implement CTDE, the Individual-Global-Maximum (IGM) property (Son et al., 2019) in Eq. (1), serves as a key mechanism:

\[_{}Q_{}(s,)=_{u_ {1}}Q_{1}(s,u_{1}),,_{u_{N}}Q_{N}(s,u_{N}) .\] (1)

Many deep MARL algorithms (Yu et al., 2022; Pan et al., 2022) adhere to the IGM criterion, such as the QMIX series algorithms (R Rashid et al., 2020; Ba et al., 2021; Pan et al., 2021). These algorithms employ a mixing network \(f_{s}\) with non-negative weights, enabling the joint Q-function to be expressed as \(Q_{}(s,)=f_{s}(Q_{1}(s,u_{1}),,Q _{N}(s,u_{N}))\).

Dynamic Sparse TrainingDynamic sparse training (DST), initially proposed in deep supervised learning, can train a 90% sparse network without performance degradation from scratch, such as in ResNet-50 (He et al., 2016) and MobileNet (Howard et al., 2017). In DST, the dense network is randomly sparsified at initialization, as shown in Figure 2, and its topology is dynamically changed during training by link dropping and growing. Specifically, the topology evolution mechanism in MAST follows the RigL method (Evci et al., 2020), which improves the optimization of sparse neural networks by leveraging weight magnitude and gradient information to jointly optimize model parameters and connectivity.

RigL periodically and dynamically drops a subset of existing connections with the smallest absolute weight values and concurrently grows an equivalent number of empty connections with the largest gradients. The pseudo-code of RigL is given in Algorithm 1, where the symbol \(\) denotes the element-wise multiplication operator, \(M_{}\) symbolizes the binary mask that delineates the sparse topology for the network \(\), and \(_{t}\) is the update fraction in training step \(t\). This process maintains the network sparsity throughout the training with a strong evolutionary ability that saves training However, in DRL, the learning target evolves in a bootstrapping manner (Tesauro et al., 1995), such that the distribution of training data is path-dependent (Desai et al., 2019), posing additional challenges to sparse training. Moreover, the partially observable nature of each agent exacerbates training non-stationarity, particularly in multi-agent settings. These factors collectively present significant hurdles for value learning in each agent under sparse models. As illustrated in Figure 1,

Figure 2: Illustration of dynamic sparse training.

attempts to train ultra-sparse MARL models using simplistic topology evolution or the sparse training framework for single-agent RL have failed to achieve satisfactory performance. Therefore, MAST introduces innovative solutions to enhance value learning in ultra-sparse models by simultaneously improving the reliability of learning targets and the rationality of sample distribution.

## 3 Enhancing Value Learning in Sparse Models

This section outlines the pivotal components of the MAST framework for training sparse MARL agents. MAST introduces innovative solutions to enhance the accuracy of value learning in ultra-sparse models by concurrently refining training data targets and distributions. Consequently, the topology evolution in MAST effectively identifies appropriate ultra-sparse network topologies. This approach aligns with single-agent DRL, where sparse training necessitates co-design with the value learning method as described in (Tan et al., 2022). However, the partially observable nature of each agent exacerbates training non-stationarity in multi-agent settings. As illustrated in Figure 3, MAST implements two key innovations to achieve accurate value learning in ultra-sparse models: i) hybrid TD(\(\)) targets combined with the Soft Mellowmax operator to mitigate estimation errors arising from network sparsity, and ii) dual replay buffers to reduce policy inconsistency errors due to sparsification.

### Improving the Reliability of Training Targets

Initially, we observe that the expected error is amplified under sparse models, motivating the introduction of multi-step TD targets. However, different environments may require varying values of step lengths to achieve optimal performance. Consequently, we focus on **hybrid TD(\(\)) targets**, which can achieve performance comparable to the best step length across different settings. Additionally, we find that the overestimation problem remains significant in sparse models. To address this, we propose the use of the **Soft Mellowmax operator** in constructing learning targets. This operator is effective in reducing overestimation bias without incurring additional computational costs.

**Hybrid TD(\(\)) Targets.** In deep multi-agent Q-learning, temporal difference (TD) learning is a fundamental method for finding an optimal policy, where the joint action-value network is iteratively updated by minimizing a squared loss driven by the TD target. Let \(M_{}\) be a binary mask representing the network's sparse topology, and denote the sparse network as \(= M_{}\), where \(\) signifies element-wise multiplication. Since sparse networks operate within a reduced hypothesis space with fewer parameters, the sparse network \(\) may induce a large bias, making the learning targets unreliable, as evidenced in (Sokar et al., 2022).

Moreover, we establish Theorem 3.1 to characterize the upper bound of the expected multi-step TD error under sparse models, where the multi-step return at \((s_{t},_{t})\) is \(_{n}(s_{t},_{t})=_{k=0}^{n-1}^{k}r_{t+k}+^{n} _{}_{}(s_{t+n},;)\) under sparse models. As Eq (2) shows, the expected multi-step TD error comes from two parts, intrinsical policy inconsistency error and network fitting error. Thus, Eq (2) implies that the expected TD error will be enlarged if the network is sparsified improperly with a larger network fitting error. Indeed, the upper bound of the expected TD error will be enlarged if the network fitting error is increased. Subsequently, it is infeasible for the model to learn a good policy. Eq. (2) also shows that introducing a multi-step return target discounts the network fitting error by a factor of \(^{n}\) in the upper bound of the expected TD error. Thus, employing a multi-step return \(_{t}^{(n)}\) with a sufficiently large \(n\), or even Monte Carlo methods (Sutton and Barto, 2018), can effectively diminish the TD error caused by network sparsification for \(<1\).

**Theorem 3.1**.: _Denote \(\) as the target policy at timestep \(t\), and \(\) as the behavior policy generating the transitions \((s_{t},_{t},,s_{t+n},_{t+n})\). Denote the network fitting error as \((s,)=|Q_{}(s,;)-(s,)|\)._

Figure 3: An example of the MAST framework based on QMIX.

\(Q_{}^{}(s,)|\). Then, the expected error between the multi-step TD target \(_{n}\) conditioned on transitions from the behavior policy \(\) and the true joint action-value function \(Q_{}^{}\) is_

\[&|_{}[_{n}(s_{t},_{t})] -Q_{}^{}(s_{t},_{t})|^{n}_{}[ ,(s_{t+n}))+(s_{t+n},(s_{t+n}))}_{ Network}\\ &+}^{}(s_{t},_{t})-Q_{ {tot}}^{}(s_{t},_{t})|}_{}+ _{}[|Q_{}^{}(s_{t+n},(s_{ t+n}))-Q_{}^{}(s_{t+n},(s_{t+n}))|]}_{}.\] (2)

Proof.: Please refer to Appendix A.3. 

However, the Monte Carlo method is prone to high variance, suggesting that an optimal TD target in sparse models should be a multi-step return with a judiciously chosen step length, balancing network fitting error due to sparsification and training variance. Figure 4 illustrates model performance across different step lengths and model sizes, revealing that an optimal step length exists for various model sizes. Moreover, the optimal step length increases as model size decreases, which aligns with Theorem 3.1 due to the increased network fitting error in models with higher sparsity.

The above facts suggest the need to increase the step length in the learning targets to maintain the performance of sparse models. However, the optimal step length varies across different settings. To address this, we introduce the TD(\(\)) target [Sutton and Barto, 2018] to achieve a good trade-off: \(_{}=(1-)_{n=1}^{}^{n-1} _{n}\) for \(\). This target averages all possible multi-step returns \(\{_{n}\}_{n=1}^{}\) into a single return using exponentially decaying weights, providing a computationally efficient approach with episode-form data. In Figure 4, we also plot two representative performances of TD(\(\)) under \(10\%\) and \(5\%\) model sizes, which are both close to the optimal step length under different model sizes.

Previous studies [Fedus et al., 2020] have highlighted that an immediate shift to multi-step targets can exacerbate policy inconsistency error as shown in Eq. (2). Since the TD(\(\)) target \(_{}\) averages all potential multi-step returns \(\{_{n}\}_{n=1}^{}\), an immediate transition to this target may encounter similar issues. To address this challenge, we adopt a hybrid strategy inspired by the delayed mechanism proposed in [Tan et al., 2022]. Initially, when the training step is less than a threshold \(T_{0}\), we employ one-step TD targets (\(_{1}\)) to minimize policy inconsistency errors. As training progresses and the policy stabilizes, we transit to TD(\(\)) targets to mitigate sparse network fitting errors. This mechanism ensures consistent and reliable learning targets throughout the sparse training process.

Soft Mellowmax Operator.The max operator in the Bellman operator poses a well-known theoretical challenge, namely overestimation, which hinders the convergence of various linear and non-linear approximation schemes [Tsitsiklis and Van Roy, 1996]. Deep MARL algorithms, including QMIX [Rashid et al., 2020b], also grapple with the overestimation problem. Several works have addressed this issue in dense MARL algorithms, such as double critics [Ackermann et al., 2019], weighted critic updates [Sarkar and Kalita, 2021], the Softmax operator [Pan et al., 2021], and the Sub-Avg operator [Wu et al., 2022]. However, these methods introduce additional computational costs, sometimes even doubling the computational budget, which is infeasible for our sparse training framework.

We turn our attention to the Soft Mellowmax operator, which has been proven effective in reducing overestimation for dense MARL algorithms in [Gan et al., 2021]. For MARL algorithms satisfying the IGM property in Eq. (1), we replace the max operator in \(Q_{i}\) with the Soft Mellowmax operator in Eq. (3) to mitigate overestimation bias in the joint-action Q function within sparse models:

\[_{}(Q_{i}(,))= {1}{}[_{u}( ,u))}{_{u^{}}( Q_{i} (,u^{}))}( Q_{i}(,u) )],\] (3)

where \(>0\), and \(\). Let \(\) be the value estimation operator that estimates the value of the next state \(s^{}\). Theorem 3.2 shows that the Soft Mellowmax operator can reduce the severe overestimation bias in sparse models.

**Theorem 3.2**.: _Let \(B()=[(s^{})]- _{^{}}Q_{}^{*}(s^{},^{})\) be the bias of value estimates of \(\). For an arbitrary joint-action \(Q\)-function \(Q_{}\), if there exists some \(V_{}^{*}(s^{})\) such that

Figure 4: Performance of different step lengths.

\(V_{tot}^{*}(s^{})=Q_{tot}^{*}(s^{},^{})\) for different joint actions, \(_{^{}}(Q_{tot}(s^{},^{})-V_{ tot}^{*}(s^{}))=0\), and \(|}_{^{}}(_{tot} (s^{},^{})-V_{tot}^{*}(s^{}) )^{2}=C(C>0)\), then \(B(_{}) B(_{})\)._

Proof.: Please refer to Appendix A.4.

Our empirical investigations reveal that overestimation remains a significant performance barrier in sparse models, resulting in substantial performance degradation. Figure 12 illustrates the win rates and estimated values of QMIX with and without our Soft Mellowmax operator on 3s5z in the SMAC. Figure 5(a) shows that the performance of RigL-QMIX-SM outperforms RigL-QMIX, while Figure 5(b) demonstrates that the Soft Mellowmax operator effectively mitigates overestimation bias. These findings highlight that QMIX still faces overestimation issues in sparse models and underscore the efficacy of the Soft Mellowmax operator in addressing this problem.

Additionally, the Soft Mellowmax operator introduces negligible extra computational costs, as it averages the Q function over each agent's individual action spaces rather than the joint action spaces used in the Softmax operator (Pan et al., 2021), which grow exponentially with the number of agents.

### Improving the Rationality of Sample Distribution

Although we have improved the reliability of the learning target's confidence as discussed above, the learning process can still suffer from instability due to improper sparsification. This suggests the need to enhance the distribution of training samples to stabilize the training process. Improper sparsification can lead to irreversible damage to the learning path (Igl et al., 2020) and exacerbate training instability as agents learn through bootstrapping.

Generally, sparse models are more challenging to train compared to dense models due to the reduced hypothesis space (Evci et al., 2019). Therefore, it is important to prioritize more recent training samples to estimate the true value function within the same training budget. Failing to do so can result in excessively large policy inconsistency errors, thereby damaging the learning process irreversibly.

MAST introduces a dual buffer mechanism utilizing two First-in-First-Out (FIFO) replay buffers: \(_{1}\) (with large capacity) and \(_{2}\) (with small capacity), with some data overlap between them. While \(_{1}\) adopts an off-policy style, \(_{2}\) follows an on-policy approach. During each training step, MAST samples \(b_{1}\) episodes from \(_{1}\) and \(b_{2}\) episodes from \(_{2}\), conducting a gradient update using a combined batch size of \((b_{1}+b_{2})\). For instance, in our experiments, we set \(|_{1}|:|_{2}|=50:1\) and \(b_{1}:b_{2}=3:1\). Generally, training with online data enhances learning stability, as the behavior policy closely matches the target policy. Conversely, training with offline data improves sample efficiency but can lead to instability.

Figure 6 illustrates the training dynamics for RigL-QMIX in the SMAC's 3s5z task. The green curve with large variances highlights the training instability of QMIX in sparse models. However, with the integration of dual buffers, QMIX's training stability and efficiency are significantly improved under sparse conditions, leading to consistent policy enhancements and higher rewards. Notably, the dual buffer mechanism does not enhance dense training, suggesting that this approach is particularly effective in sparse scenarios where network parameters are crucial for ensuring stable policy improvements. Although prior works (Schaul et al., 2015; Hou et al., 2017; Banerjee et al., 2022) have explored prioritized or dynamic-capacity buffers, their applicability in this context may be limited due to the data being in episode form in value-based deep MARL algorithms, making it difficult to determine the priority of each training episode. Similarly, the dynamic buffer approach in (Tan et al., 2022) is also inapplicable, as the policy distance measure cannot be established for episode-form data. This further emphasizes the unique effectiveness of the dual buffer approach in enhancing training stability for sparse MARL models.

Figure 5: Effects of Soft Mellowmax operator.

Figure 6: Different buffers.

We highlight that the performance improvement observed with MAST is not due to providing more training data samples to agents. Instead, it results from a more balanced training data distribution enabled by the utilization of dual buffers. The primary objective of incorporating dual buffers is to shift the overall distribution of training data. Figure 7 illustrates the distribution of samples induced by different policies, where behavior policy and target policy are defined in Theorem 3.1. Specifically, samples in the original single buffer are subject to the distribution of the behavior policy (blue), which introduces a policy inconsistency error \(d_{1}\). However, by utilizing dual buffers, the distribution of training samples shifts towards the target policy, as there are more recent samples from the on-policy buffer. This reduces the policy inconsistency in our dual buffers, thereby bolstering the stability and effectiveness of the learning process under sparse models. Additionally, the extra on-policy buffer does not significantly reduce sample efficiency, as its capacity is small.

## 4 Experiments

In this section, we conduct a comprehensive performance evaluation of MAST across various tasks in the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) benchmark. Additional experiments on the multi-agent MuJoCo (MAMuJoCo) (Peng et al., 2021) benchmark are provided in Appendix B.9. MAST serves as a versatile sparse training framework specifically tailored for value decomposition-based MARL algorithms. In Section 4.1, we integrate MAST with state-of-the-art value-based deep MARL algorithms, including QMIX (Rashid et al., 2020), WQMIX (Rashid et al., 2020), and RES (Pan et al., 2021). We also apply MAST to a hybrid value-based and policy-based algorithm, FACMAC (Peng et al., 2021). Subsequently, we assess the performance of sparse models generated by MAST in Section 4.2. Furthermore, a comprehensive ablation study of MAST components is detailed in Appendix B.7. Each reported result represents the average performance over eight independent runs, each utilizing distinct random seeds.

### Comparative Evaluation

Table 1 presents a comprehensive summary of our comparative evaluation in the SMAC benchmark, where MAST is benchmarked against the following baseline methods: (i) Tiny: Utilizing tiny dense networks with a parameter count matching that of the sparse model during training. (ii) SS: Employing static sparse networks with random initialization. (iii) SET (Mocanu et al., 2018): Pruning connections based on their magnitude and randomly expanding connections. (iv) RigL (Evci et al., 2020): This approach leverages dynamic sparse training, akin to MAST, by removing and adding

    &  &  &  & FLOPs & FLOPs & Tiny & SS & SET & RigL & RLx2 & MAST \\  & & & Size & (Train) & (Test) & (\%) & (\%) & (\%) & (\%) & (\%) & (\%) \\   & 3m & 95\% & 0.066x & 0.051x & 0.050x & 98.3 & 91.6 & 96.0 & 95.3 & 12.1 & **100.9** \\  & 2s3z & 95\% & 0.062x & 0.051x & 0.050x & 83.7 & 73.0 & 77.6 & 69.4 & 45.8 & **98.0** \\  & 3s5z & 90\% & 0.109x & 0.101x & 0.100x & 68.2 & 34.0 & 52.3 & 45.2 & 50.1 & **99.0** \\  & 64* & 90\% & 0.106x & 0.100x & 0.100x & 58.2 & 40.2 & 67.1 & 48.7 & 9.9 & **97.6** \\   & Avg. & 92\% & 0.086x & 0.076x & 0.075x & 77.1 & 59.7 & 73.2 & 64.6 & 29.8 & **98.9** \\   & 3m & 90\% & 0.108x & 0.100x & 0.100x & 98.3 & 96.9 & 97.8 & 97.8 & 98.0 & **98.6** \\  & 2s3z & 90\% & 0.106x & 0.100x & 0.100x & 89.6 & 75.4 & 85.9 & 86.8 & 87.3 & **100.2** \\  & 3s5z & 90\% & 0.105x & 0.100x & 0.100x & 70.7 & 62.5 & 56.0 & 50.4 & 60.7 & **96.1** \\  & 64* & 90\% & 0.104x & 0.100x & 0.100x & 51.0 & 29.6 & 44.1 & 41.0 & 52.8 & **98.4** \\   & Avg. & 90\% & 0.106x & 0.100x & 0.100x & 77.4 & 66.1 & 70.9 & 69.0 & 74.7 & **98.1** \\   & 3m & 95\% & 0.066x & 0.055x & 0.050x & 97.8 & 95.6 & 97.3 & 91.1 & 97.9 & **99.8** \\  & 2s3z & 90\% & 0.111x & 0.104x & 0.100x & 96.5 & 92.8 & 92.8 & 94.7 & 94.0 & **98.4** \\   & 3s5z & 85\% & 0.158x & 0.154x & 0.150x & 95.1 & 89.0 & 90.3 & 92.8 & 86.2 & **99.4** \\   & 64* & 85\% & 0.155x & 0.151x & 0.150x & 83.3 & 39.1 & 44.1 & 35.3 & 72.7 & **104.9** \\    & Avg. & 89\% & 0.122x & 0.116x & 0.112x & 93.2 & 79.1 & 81.1 & 78.5 & 87.7 & **100.6** \\   

Table 1: Comparisons of MAST with different sparse training baselines: “Sp.” stands for “sparsity”, “Total Size” means total model parameters, and the data is all normalized w.r.t. the dense model.

Figure 7: Distribution Shift: \(d_{1}\) and \(d_{2}\) are distribution distances.

connections based on magnitude and gradient criteria, respectively. (v) RLx2 (Tan et al., 2022): A specialized dynamic sparse training framework tailored for single-agent reinforcement learning.

We set the same sparsity levels for both the joint Q function \(Q_{}\), and each individual agent's Q function \(Q_{i}\). For every algorithm and task, the sparsity level indicated in Table 1 corresponds to the highest admissible sparsity threshold of MAST. Within this range, MAST's performance consistently remains within a \(3\%\) margin compared to the dense counterpart, effectively representing the minimal sparse model size capable of achieving performance parity with the original dense model. All other baselines are evaluated under the same sparsity level as MAST. We assess the performance of each algorithm by computing the average win rate per episode over the final \(20\) policy evaluations conducted during training, with policy evaluations taking place at \(10000\)-step intervals. Identical hyperparameters are employed across all scenarios.

PerformanceTable 1 unequivocally illustrates MAST's substantial performance superiority over all baseline methods in all four environments across the three algorithms. Notably, static sparse (SS) consistently exhibits the lowest performance on average, highlighting the difficulty of finding optimal sparse network topologies in the context of sparse MARL models. Dynamic sparse training methods, namely SET and RigL, slightly outperform SS, although their performance remains unsatisfactory. Sparse networks also, on average, underperform tiny dense networks. However, MAST significantly outpaces all other baselines, indicating the successful realization of accurate value estimation through our MAST method, which effectively guides gradient-based topology evolution. Notably, the single-agent method RLx2 consistently delivers subpar results in all experiments, potentially due to the sensitivity of the step length in the multi-step targets, and the failure of dynamic buffer for episode-form training samples.

To further substantiate the efficacy of MAST, we conduct performance comparisons across various sparsity levels in 3s5z, as depicted in Figure 8. This reveals an intriguing observation: the performance of sparse models experiences a sharp decline beyond a critical sparsity threshold. Compared to conventional DST techniques, MAST significantly extends this critical sparsity threshold, enabling higher levels of sparsity while maintaining performance. Moreover, MAST achieves a higher critical sparsity threshold than the other two algorithms with existing baselines, e.g., SET and RigL, achieving a sparsity level of over \(80\%\) on average. However, it is essential to note that the Softmax operator in RES averages the Q function over joint action spaces, which grow exponentially with the number of agents, resulting in significantly higher computational FLOPs and making it computationally incomparable to MAST. The detailed FLOPs calculation is deferred to Appendix B.4.2.

FLOP Reduction and Model CompressionIn contrast to knowledge distillation or behavior cloning methodologies, exemplified by works such as (Livne and Cohen, 2020; Vischer et al., 2022), MAST maintains a sparse network consistently throughout the entire training regimen. Consequently, MAST endows itself with a unique advantage, manifesting in a remarkable acceleration of training FLOPs. We observed an acceleration of up to \(20\) in training and inference FLOPs for MAST-QMIX in the 2s3z task, with an average acceleration of \(10\), \(9\), and \(8\) for QMIX, WQMIX, and RES-QMIX, respectively. Moreover, MAST showcases significant model compression ratios, achieving reductions in model size ranging from \(5\) to \(20\) for QMIX, WQMIX, and RES-QMIX, while incurring only minor performance degradation, all below \(3\%\).

Figure 8: Different sparsity.

Figure 9: Mean episode win rates on different SMAC tasks with MAST-FACMAC.

Results on FACMACIn addition to pure value-based deep MARL algorithms, we also evaluate MAST with a hybrid value-based and policy-based algorithm, FACMAC (Peng et al., 2021), in SMAC. The results are presented in Figure 18. From the figure, we observe that MAST consistently achieves a performance comparable with that of the dense models, and outperforms other methods in three environments, demonstrating its applicability across different algorithms.

### Sparse Models Obtained by MAST

We conduct a comparative analysis of diverse sparse network architectures. With identical sparsity levels, distinct sparse architectures lead to different hypothesis spaces. As emphasized in (Frankle and Carbin, 2019), specific architectures, such as the "winning ticket," outperform randomly generated counterparts. We compare three architectures: the "random ticket" (randomly sampled topology held constant during training), the "winning ticket" (topology from a MAST or RigL run and kept unchanged during training), and the "cheating ticket" (trained by MAST).

Figure 10 illustrates that both the "cheating ticket" and "winning ticket" by MAST achieve the highest performance, closely approaching the original dense model's performance. Importantly, using a fixed random topology during training fails to fully exploit the benefits of high sparsity, resulting in significant performance degradation. Furthermore, RigL's "winning ticket" fares poorly, akin to the "random ticket." These results underscore the advantages of our MAST approach, which automatically discovers effective sparse architectures through gradient-based topology evolution, without the need for pretraining methods, e.g., knowledge distillation (Schmitt et al., 2018). Crucially, our MAST method incorporates key elements: the hybrid TD(\(\)) mechanism, Soft Mellowmax operator, and dual buffers. Compared to RigL, these components significantly improve value estimation and training stability in sparse models facilitating efficient topology evolution.

Figure 11 showcases the evolving sparse mask of a hidden layer during MAST-QMIX training in 3s5z, capturing snapshots at \(0\), \(5\), \(10\), and \(20\) million steps. In Figure 11, the light pixel in row \(i\) and column \(j\) indicates the existence of the connection for input dimension \(j\) and output dimension \(i\), while the dark pixel represents the empty connection. Notably, a pronounced shift in the mask is evident at the start of training, followed by a gradual convergence of connections within the layer onto a subset of input neurons. This convergence is discernible from the clustering of light pixels forming continuous rows in the lower segment of the final mask visualization, where several output dimensions exhibit minimal or no connections. This observation underscores the distinct roles played by various neurons in the representation process, showing the prevalent redundancy in dense models.

When sparsifying agent networks, MAST only requires setting the total sparsity. The sparsity of different agents is determined automatically by MAST, by concatenating all the agent networks together in our implementation based on PyMARL (Samvelyan et al., 2019) (detailed in Appendix B.3), and treating these networks as a single network during topology evolution with only a total sparsity requirement. We visualize the trained masks of different agents in 3s5z in Figure 12(a), including the masks of two stalkers and two zealots, respectively.

Figure 11: Visualization of weight masks in the first hidden layer of agent \(1\) by MAST-QMIX.

Figure 12: Agent roles.

Figure 10: Comparison of different sparse masks.

Interestingly, we find that the network topology in the same type of agents looks very similar. However, stalkers have more connections than zealots, which aligns with the fact that stalkers play more critical roles due to their higher attack power and wider attack ranges. This observation highlights an advantage of the MAST framework, i.e., it can automatically discover the proper sparsity levels for different agents to meet the total sparsity budget. To further validate this point, we compare the adaptive allocation scheme with fixed manually-set patterns in Figure 12(b). The manual patterns include (stalker-10%, zealot-10%), (stalker-8%, zealot-12%), and (stalker-14%, zealot-6%). The results also show that the adaptive sparsity allocation in MAST outperforms other manual sparsity patterns, demonstrating the superiority of MAST.

## 5 Conclusion

This paper introduces MAST, a novel sparse training framework for valued-based deep MARL. We identify and address value estimation errors and policy inconsistency caused by sparsification, two significant challenges in training sparse agents. MAST offers innovative solutions: a hybrid TD(\(\)) target mechanism combined with the Soft Mellowmax operator for precise value estimation under extreme sparsity, and a dual buffer mechanism to reduce policy inconsistency and enhance training stability. Extensive experiments validate MAST's effectiveness in sparse training, achieving model compression ratios of \(5\) to \(20\) with minimal performance degradation and up to a remarkable \(20\) reduction in FLOPs for both training and inference.