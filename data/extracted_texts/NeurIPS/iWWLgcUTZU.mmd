# PCF-GAN: generating sequential data via the characteristic function of measures on the path space

Hang Lou

Department of Mathematics

University College London

hang.lou.19@ucl.ac.uk

&Siran Li

Department of Mathematics

Shanghai Jiao Tong University

sl4025@nyu.edu

&Hao Ni

Department of Mathematics

University College London

h.ni@ucl.ac.uk

###### Abstract

Generating high-fidelity time series data using generative adversarial networks (GANs) remains a challenging task, as it is difficult to capture the temporal dependence of joint probability distributions induced by time-series data. Towards this goal, a key step is the development of an effective discriminator to distinguish between time series distributions. We propose the so-called PCF-GAN, a novel GAN that incorporates the path characteristic function (PCF) as the principled representation of time series distribution into the discriminator to enhance its generative performance. On the one hand, we establish theoretical foundations of the PCF distance by proving its characteristicity, boundedness, differentiability with respect to generator parameters, and weak continuity, which ensure the stability and feasibility of training the PCF-GAN. On the other hand, we design efficient initialisation and optimisation schemes for PCFs to strengthen the discriminative power and accelerate training efficiency. To further boost the capabilities of complex time series generation, we integrate the auto-encoder structure via sequential embedding into the PCF-GAN, which provides additional reconstruction functionality. Extensive numerical experiments on various datasets demonstrate the consistently superior performance of PCF-GAN over state-of-the-art baselines, in both generation and reconstruction quality.

## 1 Introduction

Generative Adversarial Networks (GANs) have been a powerful tool for generating complex data distributions, _e.g._, image data. The original GAN suffers from optimisation instability and mode collapse, partially remedied later by an alternative training scheme using _integral probability metric_ (IPM) in lieu of Jensen-Shannon divergence. The IPMs, _e.g._, metrics based on Wasserstein distances or Maximum Mean Discrepancy (MMD), consistently yield good measures between generated and real data distributions, thus resulting in more powerful GANs on empirical data ([14; 2; 24]).

More recently,  proposed an IPM based on the characteristic function (CF) of measures on \(^{d}\), which has the characteristic property, boundedness, and differentiability. Such properties enable the GAN constructed using this IPM as discriminator ("CF-GAN") to stabilise training and improve generative performance. However, ineffective in capturing the temporal dependency of sequential data, such CF-metric fails to address high-frequency cases due to the curse of dimensionality. To tackle this issue, we take the continuous time perspective of time series and lift discrete time series to the path space ([28; 29; 23]). This allows us to treat time series of variable length, unequal sampling, and high frequency in a unified approach. We propose a _path characteristic function (PCF)_ distance to characterise distributions on the path space, and propose the corresponding PCF distance as a novel IPM to quantify the distance between measures on the path space.

Built on top of the unitary feature of paths (), our proposed PCF has theoretical foundations deeply rooted in the rough path theory (), which exploits the non-commutativity and the group structure of the unitary feature to encode information on order of paths. The CF may be regarded as the special case of PCF with linear random path and \(1 1\) unitary matrix. We show that the _PCF distance_ (PCFD) possesses favourable analytic properties, including boundedness and differentiability in model parameters, and we establish the linkages between PCFD and MMD. These results vastly generalise classical theorems on measures on \(^{d}\) (), with much more technically involved proofs due to the infinite-dimensionality of path space.

On the numerical side, we design an efficient algorithm which, by optimising the trainable parameters of PCFD, maximises the discriminative power and improves the stability and efficiency of GAN training. Inspired by [25; 41], we integrate the proposed PCF into the IPM-GAN framework, utilising an auto-encoder architecture specifically tailored to sequential data. This model design enables our algorithm to generate and reconstruct realistic time series simultaneously, which has advantages in diverse applications, including privacy preservation () and semantic representation extraction for downstream tasks (). To assess the efficacy of our PCF-GAN, we conduct extensive numerical experiments on several standard time series benchmarking datasets for both generation and reconstruction tasks.

We summarize key contributions of this work below:

* proposing a new metric for the distributions on the path space via PCF;
* providing theoretical proofs for analytic properties of the proposed loss metric which benefit GAN training;
* introducing a novel PCF-GAN to generate \(\&\) reconstruct time series simultaneously; and
* reporting substantial empirical results validating the out-performance of our approach, compared with several state-of-the-art GANs with different loss functions on various time series generation and reconstruction tasks.

**Related work**. Given the wide practical use of, and challenges for, realistic time series synthesis ([3; 4]), various approaches are proposed to improve the quality of GANs for synthetic time series generation. Several works, _e.g._, [43; 45; 36], are devoted to improving the discriminator of GANs to be better suited to distributions induced by time series. Among them, COT-GAN in  shares a similar philosophy with PCF-GAN by introducing a novel discriminator based on causal optimal transport (which can be seen as an improved variant of the Sinkhorn divergence tailored to sequential data), while TimeGAN () shares a similar auto-encoder structure, which improves the generator's quality and enables time series reconstruction. Unlike PCF-GAN, the reconstruction and generation modules of TimeGAN are separated, whereas it has additional stepwise supervised loss and discriminative loss. In a different vein, CEGEN, GT-GAN , COSCI-GAN , and EWGAN focus primarily on the design of network framework and generator architecture, which achieve state-of-the-art results on several benchmarking datasets.

## 2 Preliminaries

The characteristic function of a measure on \(^{d}\), namely that the Fourier transform, plays a central role in probability theory and analysis. The path characteristic function (PCF) is a natural extension of the characteristic function to the path space.

### Characteristic function distance (CFD) between random variables in \(^{d}\)

Let \(X\) be an \(^{d}\)-valued random variable with the law \(= X^{-1}\). The characteristic function of \(X\), denoted as \(_{X}:^{d}\), maps each \(^{d}\) to the expectation of its complex unitary transform: \(_{X}:_{X}[e^{i,X }]\). Here \(U_{}:^{d},x e^{i, x}\) is the solution to the linear controlled differential equation:

\[U_{}(x)=iU_{}(x),x,  U_{}()=1,\] (1)

where \(\) is the zero vector in \(^{d}\) and \(,\) is the Euclidean inner product on \(^{d}\).

References [11; 16] studied the squared _characteristic function distance_ (CFD) between two \(^{d}\)-valued random variables \(X\) and \(Y\) with respect to another probability distribution \(\) on \(^{d}\):

\[_{}^{2}(X,Y)=_{Z}[ |_{X}(Z)-_{Y}(Z)|^{2}].\] (2)

It is proved in [25; 1] that if the support of \(\) is \(^{d}\), then \(_{}\) is a distance metric, so that \(_{}^{2}(X,Y)=0\) if and only if \(X\) and \(Y\) have the same distribution. This justifies the usage of \(_{}^{2}\) as a discriminator for GAN training to learn finite-dimensional random variables from data.

### Unitary feature of a path

Let \(([0,T];^{d})\) be the space of \(^{d}\)-valued paths of bounded variation over \([0,T]\). Consider

\[:=\{}:[0,T]^{d+1}:}(t)=(t,(t))t[0,T];\, ([0,T];^{d});\,(0)=0\}.\] (3)

For a discrete time series \(x=(t_{i},x_{i})_{i=0}^{N}\), where \(0=t_{0}<t_{1}<<t_{N}=T\) and \(x_{i}^{d}\) (\(i\{0,,N\}\)), we can embed it into some \(\) whose evaluation at \((t_{i})_{i=1}^{N}\) coincides with \(x\). This is well suited for sequence-valued data in the high-frequency limit with finer time-discretisation and is often robust in practice ([27; 26]). Such embeddings are not unique. In this work, we adopt the linear interpolation for embedding, following [23; 18; 32].

Let \(^{m m}:=\{m m\}\), \(I_{m}\) be the identity matrix, and \(*\) be conjugate transpose. Write \(U(m)\) and \((m)\) for the Lie group of \(m m\) unitary matrices and its Lie algebra, resp.:

\[U(m)=\{A^{m m}:A^{*}A=I_{m}\},(m):=\{A ^{m m}:A^{*}+A=0\}.\]

**Definition 2.1**.: Let \(([0,T];^{d})\) be a continuous path and \(M:^{d}(m)\) be a linear map. The unitary feature of \(\) under \(M\) is the solution \(:[0,T] U(m)\) to the following equation:

\[_{t}=_{t} M(_{t}), _{0}=I_{m}.\] (4)

We write \(_{M}():=_{T}\), _i.e._, the endpoint of the solution path.

By a slight abuse of notations, \(_{M}()\) is also called the unitary feature of \(\) under \(M\). Unitary feature is a special case of the _Cartan/path development_, for which one may consider paths taking values in any Lie group \(G\). We take only \(G=U(m)\) here; \(m d\) in general ([6; 30]).

**Example 2.2**.: _For \(M(^{d},(m))\) and \(([0,T];^{d})\) linear, \(_{M}(X)=e^{M(_{T}-_{0})}\). In particular, when \(m=1\), \((1)\) is reduced to \(i\) and \(M(y)=i<_{M},y>\) for some \(_{M}^{d}\)._

Motivated by the universality and characteristic property of unitary features (, see Appendix A.3), we constructed a unitary layer which transforms any \(d\)-dimensional time series \(x=(x_{0},,x_{N})\) to the unitary feature of its piecewise linear interpolation \(\). It is a special case of the path development layer , when Lie algebra is chosen as \((m)\). In fact, the explicit formula holds: \(_{M}()=_{i=1}^{N+1}(M( x_{i}))\), where \( x_{i}:=x_{i}-x_{i-1}\) and \(\) is the matrix exponential.

**Convention 2.3**.: _The space \((^{d},(m))\) in which \(M\) of Eq. (4) resides is isomorphic to \((m)^{d}\), where \((m)\) is Lie algebra isomorphic to \(^{}\). For each \((m)^{d}\) given by anti-Hermitian matrices \(\{^{(i)}\}_{i=1}^{d}\), a linear map \(M\) is uniquely induced: \(M(x)=_{i=1}^{d}^{(i)}<x,e_{i}>, x^{d}\)._

## 3 Path characteristic function loss

### Path characteristic function (PCF)

The unitary feature of a path \(\) plays a role similar to that played by \(e^{i<x,>}\) to an \(^{d}\)-valued random variable. Thus, for a _random_ path \(\), the _expected_ unitary feature can be viewed as the characteristic function for measures on the path space ().

**Definition 3.1**.: Let \(\) be an \(\)-valued random variable and \(_{}\) be its measure. The path characteristic function (PCF) of \(\) of order \(m\) is the map \(_{}^{(m)}:(^{d},( m))^{m m}\) given by

\[_{}(M):=[_{M}()]=_{ }_{M}()\,_{}( ).\]The path characteristic function (PCF) \(_{}:_{m=0}^{}(^{d}, (m))_{m=0}^{}^{m m}\) is defined by the natural grading: \(_{}_{(^{d},(m))}= _{}^{(m)}\) for each \(m\).

In the above, \(_{M}() U(m)\) is the unitary feature of the path \(\) under \(M\). See Definition 2.1.

Similarly to the characteristic function of \(^{d}\)-valued random variables, the PCF always exists. Moreover, we have the following important result, whose proof is presented in Appendix A.

**Theorem 3.2** (Characteristicity).: _Let \(\) and \(\) be \(\)-valued random variables. They have the same distribution (denoted as \(}{{=}}\)) if and only if \(_{}=_{}\)._

### A new distance measure via PCF

We now introduce a novel and natural distance metric, which measures the discrepancy between distributions on the path space via comparing their PCFs. Throughout, \(d_{}\) denotes the metric associated with the Hilbert-Schmidt norm \(\|\|_{}\) on \(^{m m}\):

\[d_{}(A,B):=}^{2}}=\, [(A-B)(A-B)^{*}]}.\]

**Definition 3.3**.: Let \(,:[0,T]^{d}\) be stochastic processes and \(_{}\) be a probability distribution on \((m)^{d}:=(^{d},(m))\) (recall Convention 2.3). Define the squared PCF-based distance (PCFD) between \(\) and \(\) with respect to \(_{}\) as

\[_{}^{2}(,)=_{M _{}}[d_{}^{2}_{} (M),_{}(M)].\] (5)

We shall not distinguish between \(\) and \(_{}\) for simplicity.

PCFD exhibits several mathematical properties, which provide the theoretical justification for its efficacy as the discriminator on the space of measures on the path space, leading to empirical performance boost. First, PCFD has the characteristic property.

**Lemma 3.4** (Separation of points).: _Let \(,()\) and \(\). Then there exists \(m\), such that if \(\) is a \((m)^{d}\)-valued random variable with full support, then \(_{}(,) 0\)._

Furthermore, \(_{}\) has a simple uniform upper bound for any fixed \(m\):

**Lemma 3.5**.: _Let \(\) be a \((m)^{d}\)-valued random variable. Then, for any \(([0,T];^{d})\)-valued random variables \(\) and \(\), it holds that \(_{}^{2}(,) 2m^{2}\)._

Under mild conditions, \(\) is _a.e._ differentiable with respect to a continuous parameter, thus ensuring the feasibility of gradient descent in training.

**Theorem 3.6** (Lipschitz dependence on continuous parameter).: _Let \(\) and \(\) be subsets of \(([0,T];^{d})\), \((,)\) be a metric space, \(\) be a Borel probability measure on \(\), and \(\) be a Borel probability measure on \((m)^{d}\). Assume that \(g:\), \((,) g_{}()\) is Lipschitz in \(\) such that \([g_{}()-g_{^{}}() ]()(,^{})\). In addition, suppose that \(_{M_{}}[||M||^{2} ]<\) and \(_{}[()]<\). Then \(_{}(g_{}(),)\) is Lipschitz in \(\). Moreover, it holds that_

\[|_{}(g_{}(), )-_{}(g_{^{}}(), )|_{M_{}} [||M||^{2}]}\,_{ }[()]\,(,^{})\]

_for any \(,^{}\), \(\), \(\), and \(((m)^{d})\)._

_Remark 3.7_.: The parameter space \((,)\) is usually taken to be \(^{}\) for some \(\). In this case, by Rademacher's theorem \(_{}(g_{}(),)\) is _a.e._ differentiable in \(\).

Similarly to metrics on measures over \(^{d}\) (_cf._), we construct a metric based on PCFD, denoted as \(}\), on the space \(()\) of Borel probability measures over the path space, and we prove that it metrises the weak-star topology on \(()\). Throughout, \(}}{{}}\) denotes the convergence in law.

**Theorem 3.8** (Informal, convergence in law).: _Let \(\{_{n}\}_{n}\) and \(\) be \(\)-valued random variables with measures supported in a compact subset of \(\). Then \(}(_{n},) 0_{n} }{{}}\)._

The formal statement and proof can be found in Lemma B.2 and Theorem B.8 in the Appendix.

Similar to  for \(^{d}\), we prove that PCFD can be interpreted as an MMD with a specific kernel \(\) (see Appendix B.3). Example B.12 illustrates that the PCFD has the superior test power for hypothesis testing on stochastic processes compared with CF distance on the flattened time series.

### Computing PCFD under empirical measures

Now, we shall illustrate how to compute the PCFD on the path space.

Let \(}:=\{^{i}\}_{i=1}^{n}\) and \(}:=\{^{i}\}_{i=1}^{n^{}}\) be i.i.d. drawn respectively from \(\)-valued random variables \(\) and \(\). First, for any linear map \(M(m)^{d}\), the empirical estimator of \(}(M)\) is the average of unitary features of all observations \(}=\{_{i}\}_{i=1}^{n}\), _i.e._, \(}}(M)=_{i=1}^{n}_{M}(_{i})\). We then parameterise the \((m)^{d}\)-valued random variable \(\) via the empirical measure \(_{_{M}}\), i.e., \(_{_{M}}=_{i=1}^{k}_{M_{i}}\), where \(_{M}:=\{M_{i}\}_{i=1}^{k}(m)^{d k}\) are the trainable model parameters. Finally, define the corresponding _empirical path characteristic function distance_ (EPCFD) as

\[_{_{M}}(},})= _{i=1}^{k}\|}}(M_{i})- }}(M_{i})\|_{}^{2}}\,.\] (6)

Our approach to approximating \(\) via the empirical distribution differs from that in , where \(\) is parameterised by mixture of Gaussian distributions. In SS4.1 and SS5, it is shown that, by optimising the empirical distribution, a moderately sized \(k\) is sufficient for achieving superior performance, in contrast to a larger sample size required by .

## 4 PCF-GAN for time series generation

### Training of the EPCFD

In this subsection, we apply the EPCFD to GAN training for time series generation as the discriminator. We train the generator to minimise the EPCFD between true and synthetic data distribution, whereas the empirical distribution of \(\) characterised by \(_{M}(m)^{d k}\) is optimised by maximising EPCFD.

By an abuse of notation, let \(:=^{d n_{T}}\) (\(:=^{e n_{T}}\), resp.) denote the data (noise, resp.) space, composed of \(^{d}\) (\(^{e}\), resp.) time series of length \(n_{T}\). As discussed in SS2.2, \(\) and \(\) can be viewed as path spaces via linear interpolation. Like the standard GANs, our model is comprised of a generator \(G_{_{g}}:^{d n_{T}}\) and the discriminator \(_{_{M}}:()()^{+}\), where \(_{M}(m)^{k d}\) is the model parameter of the discriminator, which fully characterises the empirical measure of \(\). The pre-specified noise random variable \(=(Z_{t_{i}})_{i=0}^{n_{T}-1}\) is the discretised Brownian motion on \(\) with time mesh \(}\). The induced distribution of the fake data is given by \(G_{_{g}}()\). Hence, the min-max objective of our basic version PCF-GAN is

\[_{_{g}}_{_{M}}\ _{_{M}}(G_{_{g}}( ),).\]

We apply mini-batch gradient descent to optimise the model parameters of the generator and discriminator in an alternative manner. In particular, to compute gradients of the discriminator parameter \(_{M}\), we use the efficient backpropagation algorithm through time introduced in , which effectively leverages the Lie group-valued outputs and the recurrence structure of the unitary feature. The initialisation of \(_{M}\) for the optimisation is outlined in the Appendix B.4.1.

Learning time-dependent Ornstein-Uhlenbeck processFollowing , we apply the proposed PCF-GAN to the toy example of learning the distribution of synthetic time series data simulated via the time-dependent Ornstein-Uhlenbeck (OU) process. Let \((_{t})_{t[0,T]}\) be an \(\)-valued stochastic process described by the SDE, i.e., \(d_{t}=( t-_{t})\,dt+ d}\)with \(}(,)\), where \((})_{[,]}\) is 1D Brownian motion and \((0,1)\) is the standard normal distribution. We set \(=0.01\), \(=0.02\), \(=0.4\) and time discretisation \( t=0.1\). We generate 10000 samples from \(t=0\) to \(t=63\), down-sampled at each integer time point. Figure 2 shows that the synthetic data generated by our GAN model, which uses the EPCFD discriminator, is visually indistinguishable from true data. Also, our model accurately captures the marginal distribution at various time points.

### PCF-GAN: learning with PCFD and sequential embedding

In order to effectively learn the distribution of high-dimensional or complex time series, using solely the EPCF loss as the GAN discriminator fails to be the best approach, due to the computational limitations imposed by the sample size \(k\) and the order \(m\) of EPCFD. To overcome this issue, we adopt the approach [41; 25], and train a generator that matches the distribution of the _embedding_ of time series via the auto-encoder structure. Figure 3 illustrates the mechanics of our model.

To proceed, let us first recall the generator \(G_{_{g}}:\) and introduce the embedding layer \(F_{_{f}}\), which maps \(\) to \(\) (the noise space). Here \(_{f}\) is the model parameters of the embedding layer and will be learned from data. To this end, it is natural to optimize the model parameters \(_{g}\) of the generator by minimising the generative loss \(L_{}\), which is the EPCFD distance of the embedding between true distribution \(\) and synthetic distribution \(G_{_{g}}()\); in formula,

\[L_{}(_{g},_{M},_{f})=_{_{M}}(F_{_{f}}(G_{_{g}}()),F_{_{f}}( ))).\] (7)

**Encoder\((F_{_{f}})\)-decoder\((G_{_{g}})\) structure**: The motivation to consider the auto-encoder structure is based on the observation that the embedding might be degenerated when optimizing \(L_{}\). For example, no matter whether true and synthetic distribution agrees or not, \(F_{_{f}}\) could be simply a constant function to achieve the perfect generator loss 0. Such a degeneracy can be prohibited if \(F_{_{f}}\) is injective. In heuristic terms, the "good" embedding should capture essential information about real time series of \(\) and allows the reconstruction of time series \(\) from its embedding \(F_{_{f}}()\). This motivates us to train the embedding \(F_{_{f}}\) such that \(F_{_{f}} G_{_{g}}\) is close to the identity map. If this condition is satisfied, it implies that \(F_{_{f}}\) and \(G_{_{g}}\) are pseudo-inverses of each other, thereby ensuring the desired injectivity. In this way, \(F_{_{f}}\) and \(G_{_{g}}\) serve as the encoder and decoder of raw data, respectively.

To impose the injectivity of \(F_{_{f}}\), we consider two additional loss functions for training \(_{f}\) as follows:

**Reconstruction loss \(L_{}\)**: It is defined as the \(l^{2}\) samplewise distance between the original and reconstructed noise by \(F_{_{f}} G_{_{g}}\), i.e., \(L_{}=[|Z-F_{_{f}}(G_{_{g}}())|^ {2}]\). Note that \(L_{}=0\) implies that \(F_{_{f}}(G_{_{g}}())=\), for any sample \(\) in the support of \(\) almost surely.

**Regularization loss \(L_{}\)**: It is proposed to match the distribution of the original noise variable \(\) and embedding of true distribution \(\). It is motivated by the observation that if the perfect generator \(G_{}()=\) and \(F_{_{f}} G_{_{g}}\) is the identity map, then \(=F_{_{f}}()\). Specifically,

\[L_{}=_{_{M}^{}}(,F_{ _{f}}()),\] (8)

where we distinguish \(_{M}^{}\) from \(_{M}\) in \(L_{}\). The regularization loss effectively stabilises the training and resolves the mode collapse  due to the lack of infectivity of the embedding.

**Training the embedding parameters \(_{f}\)**: The embedding layer \(F_{_{f}}\) aims to not only discriminate the real and fake data distributions as a critic, but also preserve injectivity. Hence we optimise the

Figure 3: Visualization of the PCF-GAN architecture

Figure 2: **Left**: Sample paths generated from the time-dependent OU process and synthetic paths from PCF-GAN. **Right**: The marginal distribution comparison at \(t\{10,20,30,40,50,60\}\).

embedding parameter \(_{f}\) by the following hybrid loss function:

\[_{_{f}}(L_{}-_{1}L_{}- _{2}L_{}),\] (9)

where \(_{1}\) and \(_{2}\) are hyper-parameters that balance the three losses.

**Training the EPCFD parameters \((_{M},_{M}^{})\)**: Note that \(L_{}\) and \(L_{}\) have trainable parameters of EPCFD, i.e., \(_{M}\) and \(_{M}^{}\). Similar to the basic PCF-GAN, we optimize \(_{M}\) and \(_{M}^{}\) by maximising the EPCFD to improve the discriminative power.

\[_{_{M}}L_{},_{_{M}^{}}L_{ }\] (10)

By doing so, we enhance the discriminative power of EPCFD\({}_{_{M}}\) and EPCFD\({}_{_{M}^{}}\). Consequently, this facilitates the training of the generator such that the embedding of the true data aligns with both the noise distribution and the reconstructed noise distribution.

Differentiability of EPCFD with respect to parameters of the embedding layer and generators are guaranteed by Theorem 3.6, as long as \(F_{_{f}} G_{_{g}}\) satisfies the Lipschitz condition thereof. Let us also stress on two key advantages of our proposed PCF-GAN. First, it possesses the ability to generate synthetic time series with reconstruction functionality, thanks to the auto-encoder structure in PCF-GAN. Second, by virtue of the uniform boundedness of PCFD shown in Lemma 3.5, our PCF-GAN does not require any additional gradient constraints of the embedding layer and EPCFD parameters, in contrast to other MMD-based GANs and Wasserstein-GAN. It helps with the training efficiency and alleviates the vanishing gradient problem in training sequential networks like RNNs.

We provide the pseudo-code for the proposed PCF-GAN in Algorithm 1.

```
1:Input:\(_{d}\) (real time series distribution), \(_{z}\) (noise distribution), \(_{M}_{M}^{}_{f},_{g}\)(model parameters for EPCFD, critic \(F\) and generator \(G\)), \(_{1},_{2}^{+}\) (penalty weights), \(b\) (batch size), \(\) (learning rate), \(n_{c}\) the iteration number of discriminator per generator update,.
2:while\(_{M},_{M}^{},_{M},_{M},_{c},_{g}\) not converge do
3:for\(i\{1,,n_{c}\}\)do
4:#train the unitary linear maps in EPCFD
5: Sample from distributions: \(X_{d},Z_{z}\).
6: Generator Loss: \(L_{generator}=_{_{M}}(F_{_{f}}(X),F_{_{f}}(G_{ _{g}}(Z)))\)
7: Update: \(_{M}_{M}+_{_{M}}L_{}\)
8: Regularization Loss: \(L_{regularization}=_{_{M}^{}}(Z,F_{_{f}}(X))\)
9: Update: \(_{M}^{}_{M}^{}+_{ _{M}^{}}(L_{})\)
10:#train the embedding
11: Reconstruction Loss: \(L_{}=[|Z-F_{_{f}}(G_{_{g}}(Z))|^{2}]\)
12: Loss on critic: \(L_{c}=L_{}-_{1} L_{}-_{2}  L_{}\)
13: Update: \(_{f}_{f}+_{_{c}}L_{c}\)
14:endfor
15:#train the generator
16: Sample from distributions: \(X_{d},Z_{z}\).
17: Generator Loss: \(L_{}=_{}(F_{_{f}}(X),F_{_{ f}}(G_{_{g}}(Z)))\)
18: Update: \(_{g}_{g}-_{_{g}}L_{g}\)
19:endwhile ```

**Algorithm 1** PCF-GAN.

## 5 Numerical Experiments

To validate its efficacy, we apply our proposed PCF-GAN to a broad range of time series data and benchmark with state-of-the-art GANs for time series generation using various test metrics. Full details on numerics (dataset, evaluation metrics, and hyperparameter choices) are in Appendix C. Additional ablation studies and visualisations of generated samples are reported in Appendix D.

**Baselines**: We take Recurrent GAN (RGAN), TimeGAN , and COT-GAN  as benchmarking models. These are representatives of GANs exhibiting strong empirical performance for time series generation. For fairness, we compare our model to the baselines while fixing the generators and embedding/discriminator to be the common sequential neural network (2 layers of LSTMs).

**Dataset**: We benchmark our model on four different time series datasets with various characteristics: dimensions, sample frequency, periodicity, noise level, and correlation. **(1) Rough Volatility**: High-frequency synthetic time series data with low noise-to-signal. **(2) Stock**: The daily historical data on ten publicly traded stocks from 2013 to 2021, including as features the volume and high, low, opening, closing, and adjusted closing prices. **(3) Beijing Air Quality**: An UCI multivariate time series on hourly air pollutants data from different monitoring sites. **(4) EEG Eye State**: An UCI dataset of a high frequency and continuous EEG eye measurement. We summarise the key statistics of the datasets in Table 1.

**Evaluation metrics**: The following three metrics are used to assess the quality of generative models. For time series generation/reconstruction, we compare the true and fake/reconstructed distribution by \(G_{_{{}_{}}} F_{_{{}_{f}}}\) via the below test metrics. **(1) Discriminative score**: We train a post-hoc classifier to distinguish true and fake data. We report the classification error on the test data. The better generative model yields a lower classification error, as it means that the classifier struggles to differentiate between true and fake data. **(2) Predictive score**: We train a post-hoc sequence-to-sequence regression model to predict the latter part of a time series given the first part from the generated data. We then evaluate and report the mean square error (MSE) on the true time series data. The lower MSE indicates better the generated data can be used to train a predictive model. **(3) Sig-MMD**: We use MMD with the signature feature as a generic metric on time series distribution. Smaller the values, indicating closer the distributions, are better. To compute three evaluation metrics, we randomly generated 10,000 samples of true and synthetic (reconstructed) distribution resp. The mean and standard deviation of each metric based on 10 repeated random sampling are reported.

### Time series generation

Table 2 indicates that PCF-GAN consistently outperforms the other baselines across all datasets, as demonstrated by all three test metrics. Specifically, in terms of the discriminative score, PCF-GAN achieves a remarkable performance with values of \(0.0108\) and \(0.0784\) on the Rough volatility and Stock datasets, respectively. These values are \(61\%\) and \(39\%\) lower than those achieved by the second-best model. Regarding the predictive score, PCF-GAN achieves the best result across all four datasets. While COT-GAN surpasses PCF-GAN in terms of the Sig-MMD metric on the EEG dataset, PCF-GAN consistently outperforms the other models in the remaining three datasets. Additionally, to assess the fitting on auto-correlation, cross-correlation and marginal distribution, we include the corresponding numerical results in Table 4 in Appendix D.4. For a qualitative analysis of generative quality, we provide the visualizations of generated samples for all models and datasets in Appendix D without selective bias. Furthermore, to showcase the effectiveness of our auto-encoder architecture for the generation task, we present an ablation study in Appendix D.

   _Dataset_ & _Dimension_ & _Length_ & _Sample rate_ & _Auto-cor (lag 1)_ & _Auto-cor (lag 5)_ & _Cross-cor_ \\  _RV_ & 2 & 200 & - & 0.967 & 0.916 & -0.014 \\ _Stock_ & 5 & 20 & 1day & 0.958 & 0.922 & 0.604 \\ _Air_ & 10 & 24 & 1hour & 0.947 & 0.752 & 0.0487 \\ _EEG_ & 14 & 20 & 8ms & 0.517 & 0.457 & 0.418 \\   

Table 1: Summuary statistics for four datasets

    & Task &  &  \\  _Dataset_ & _Test Metrics_ & _RGAN_ & _COT-GAN_ & _TimeGAN_ & _PCF-GAN_ & _TimeGAN (R)_ & _PCF-GAN(R)_ \\   & _Discriminative_ &.0271\(\).048 &.0499\(\).068 &.0327\(\).019 & **.0108\(\).006** &.5000\(\).000 & **.2820\(\).082** \\  & _Predictive_ &.0393\(\).000 &.0395\(\).000 &.0395\(\).001 & **.0390\(\).000** &.0590\(\).003 & **.0398\(\).001** \\  & _Sig-MMD_ &.0163\(\).004 &.0116\(\).003 &.0027\(\).004 & **.0024\(\).001** &.3308\(\).134 & **.0960\(\).050** \\   & _Discriminative_ &.1283\(\).015 &.4966\(\).002 &.3286\(\).063 & **.0784\(\).028** &.4943\(\).002 & **.3181\(\).038** \\  & _Predictive_ &.0132\(\).000 &.0144\(\).000 &.0139\(\).000 & **.0125\(\).000** &.1180\(\).012 & **.0127\(\).000** \\  & _Sig-MMD_ &.0248\(\).008 &.0029\(\).000 &.0027\(\).006 & **.0017\(\).000** &.7587\(\).186 & **.0078\(\).004** \\   & _Discriminative_ &.4549\(\).012 &.4992\(\).002 &.3064\(\).025 & **.2236\(\).058** &.4999\(\).000 & **.4140\(\).013** \\  & _Predictive_ &.0261\(\).001 &.0260\(\).001 &.0256\(\).000 & **.0237\(\).000** &.0619\(\).004 & **.0289\(\).000** \\  & _Sig-MMD_ &.0456\(\).015 &.0128\(\).002 &.0146\(\).026 & **.0126\(\).005** &.4141\(\).078 & **.0359\(\).012** \\   & _Discriminative_ &.4908\(\).003 &.4931\(\).007 &.4771\(\).008 & **.3660\(\).025** &.5000\(\).000 & **.4959\(\).003** \\  & _Predictive_ &.0315\(\).000 &.0304\(\).000 &.0342\(\).001 & **.0246\(\).000** &.0499\(\).001 & **.0328\(\).001** \\   & _Sig-MMD_ &.0602\(\).010 & **.0102\(\).002** &.0640\(\).025 &.0180\(\).004 &.0700\(\).021 & **.0641\(\).019** \\   

Table 2: Performance comparison of PCF-GAN and baselines. Best for each task shown in bold.

### Time series reconstruction

As TimeGAN is the only baseline model incorporating reconstruction capability, for reconstruction tasks we only compare with TimeGAN. The reconstructed examples of time series using both PCF-GAN and TimeGAN are shown in Figure 4; see Appendix D for more samples.

### Training stability and efficiency

Figure 5 demonstrates the training progress of the PCF-GAN on RV dataset. Compared to the fluctuating generator loss typically observed in traditional GANs, the PCF-GAN yields better convergence by leveraging the autoencoder structure. This is achieved by minimising reconstruction and regularisation losses, which ensures the injectivity of \(F_{_{f}}\) and enables production of a semantic embedding throughout the training process. The decay of generator loss in the embedding space directly reflects the improvement in the quality of the generated time series. This is particularly useful for debugging and conducting hyperparameter searches. Furthermore, decay in both recovery and regularisation loss signifies the enhanced performance of the autoencoder.

By leveraging the effective critic \(F_{_{f}}\), we achieve enhanced performance with a moderate increase in parameters (ranging from 1200 to 6400) within \(_{M}\) of EPCFD. The training of these additional parameters is highly efficient in PCF-GAN, while still outperforming all baseline models. Specifically, our algorithm is approximately twice as fast as TimeGAN (using three extra critic modules) and three times as fast as COT-GAN (with one additional critic module and the Sinkhorn algorithm). However, it takes 1.5 times as long as RGAN due to the extra training required on \(_{M}\).

## 6 Conclusion & Broader impact

**Conclusion** We introduce a novel, principled and efficient PCF-GAN model based on PCF for generating high-fidelity sequential data. With theoretical support, it achieves state-of-the-art generative performance with additional reconstruction functionality in various tasks of time series generation.

**Limitation and future work** In this work, we use LSTM-based networks for the autoencoder and do not explore other sequential models (e.g., transformers). The suitable choice of network architecture for the autoencoder may further improve the efficacy of the proposed PCF-GAN on more complicated data, _e.g._, video and skeletal human action sequence, which merits further investigation. As a distance metric on time series, PCFD can be flexibly incorporated with other advanced generators of time

Figure 4: Examples of time series reconstruction using PCF-GAN and TimeGAN.

Figure 5: Training curves for PCF-GAN of real (left) and generated (right) time series distributions on the Rough Volatility dataset at different training iterations. Plotted by a moving average over a window with 500 iterations.

series GAN models, hence may further improve the performance. For example, one can replace the average cross-entropy loss used in [17; 39] and the Wasserstein distance in  by PCFD, with some simple modifications on the discriminators. Furthermore, although we establish the link between PCFD and MMD, it is interesting to design efficient algorithms to compute the kernel specified in Appendix B.3.

**Broader impact** Like other GAN models, this model has the potential to aid data-hungry algorithms by augmenting small datasets. Additionally, it can enable data sharing in domains such as finance and healthcare, where sensitive time series data is plentiful. However, it is important to acknowledge that the generation of synthetic data also carries the risk of potential misuse (_e.g._ generating fake news).