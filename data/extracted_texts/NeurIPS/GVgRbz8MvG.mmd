# Nonparametric Evaluation of Noisy ICA Solutions

Syamantak Kumar\({}^{1}\) &Purnamrita Sarkar\({}^{2}\) &Peter Bickel\({}^{3}\) &Derek Bean\({}^{4}\)

\({}^{1}\)Department of Computer Science, UT Austin

\({}^{2}\)Department of Statistics and Data Sciences, UT Austin

\({}^{3}\)Department of Statistics, University of California, Berkeley

\({}^{4}\) Department of Statistics, University of Wisconsin, Madison

syamantak@utexas.edu, purna.sarkar@austin.utexas.edu,

bickel@stat.berkeley.edu, derekb@stat.wisc.edu

###### Abstract

Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses, our proposed diagnostic, as shown by our simulations, can remedy them. Finally, we propose a theoretical framework to analyze the local and global convergence properties of our algorithms.

## 1 Introduction

Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS) , which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. It has become a powerful alternative to the Gaussian model, leading to PCA in factor analysis. A good account of the history, many results, and the role of dimensionality in noiseless ICA can be found in . In one of its first forms, ICA is based on observing \(n\) independent samples from a \(k\)-dimensional population:

\[=B+\] (1)

where \(B^{d k}\) (\(d k\)) is an unknown mixing matrix, \(^{k}\) is a vector of statistically independent mean zero "sources" or "factors" and \( N(,)\) is a \(d\) dimensional mean zero Gaussian noise vector with covariance matrix \(\), independent of \(\). We denote \(():=()\) and \(S:=()=B()B^{T}+\). The goal of the problem is to estimate \(B\).

ICA was initially developed in the context of \(=0\), now called noiseless ICA. In that form, it spawned an enormous literature  and at least two well-established algorithms FASTICA  and JADE  which are widely used. The general model with nonzero \(\) and unknown noise covariance matrix \(\), known as _noisy_ ICA, has developed more slowly, with its own literature .

In the following sections, we will show that various ICA and noisy ICA methods have distinct shortcomings. Some struggle with heavy-tailed distributions and outliers, while others require approximations of entropy-based objectives, which have their own challenges (see eg. ).

Although methods for noiseless cases may sometimes work in noisy settings , they are not always reliable (e.g., see Figure 1 in  and Theorem 2 in ). Despite the plethora of algorithms for noisy and noiseless ICA, the literature has largely been missing a diagnostic to decide which algorithm to pick for a given dataset. This is our primary goal.

The paper is organized as follows. Section 2 contains background and notation. Section 3.1 contains the independence score, its properties, and a Meta algorithm 1 that utilizes this score. Section 3.3 introduces new contrast functions based on the natural logarithm of the characteristic function and moment generating function of \(^{T}\). Section 4 presents the global and local convergence results for noisy ICA. Section 5 demonstrates the empirical performance of our new contrast functions as well as the Meta algorithm applied to a variety of methods for inferring \(B\).

## 2 Background and overview

This section contains a general overview of the main concepts of ICA.

**Notation:** We denote vectors using bold font as \(^{d}\). The dataset is represented as \(\{^{(i)}\}_{i[n]}\) for \(^{(i)}^{d}\). Scalar random variables and matrices are represented using upper case letters, respectively. \(_{k}\) denotes the \(k\)- dimensional identity matrix. Entries of vector \(\) (matrix \(M\)) are represented as \(v_{i}\) (\(M_{ij}\)). \(M(:,i)\) (\(M(i,:)\)) represents the \(i^{}\) column (row) of matrix \(M\). \(M^{}\) represents the Moore-Penrose inverse of \(M\). \(\|.\|\) represents the Euclidean \(l_{2}\) norm for vectors and operator norm for matrices. \(\|.\|_{F}\) similarly represents the Frobenius norm for matrices. \([.]\) (\(}[.]\)) denotes the expectation (empirical average); \(\!\!\!\) denotes statistical independence. \(\|.\|_{_{2}}\) represents the subgaussian Orlicz norm (see Def. 2.5.6 in ). \(X_{n}=O_{P}(a_{n})\) implies that \(}{a_{n}}\) is stochastically bounded i.e, \(>0\), there exists a finite \(M\) such that \(_{n}(|}{a_{n}}|>M)\) (See ).

**Identifiability:** One key issue in Eq 1 is the identifiability of \(A:=B^{-11}\), which holds up to permutation and scaling of the coordinates of \(\) if, at most, one component of \(\) is Gaussian (see [14; 43]). If \(d=k\) and \(\) are unknown, it is possible, as we shall see later, to identify the canonical versions of \(B\) and \(\). For \(d>k\) and unknown \(\), it is possible (see ) to reduce to the \(d=k\) case. In this work, we assume \(d=k\). For classical factor models, when \(\) is Gaussian, identifiability can only be identified up to rotation and scale, leading to non-interpretability. This suggests fitting strategies focusing on recovering \(B^{-1}\) through nongaussianity as well as independence. In the noisy model (Eq 1), an additional difficulty is that recovery of the source signals becomes impossible even if \(B\) is known exactly. This is because, the ICA models \(=B(+)+\) and \(=B+(B+)\) are indistinguishable (see  pages 2-3). This is resolved in  via maximizing the Signal-to-Interference-plus-Noise ratio (SINR). In classical work on ICA, a large class of algorithms (see ) choose to optimize a measure of non-gaussianity, referred to as contrast functions.

**Contrast functions:** Methods for estimating \(B\) typically optimize an empirical contrast function. Formally, we define a contrast function \(f(|P)\) by \(f:_{k}\) where \(_{k}\) is the unit ball in \(^{k}\) and \(\) is essentially the class of mean zero distributions on \(^{d}\). More concretely, for suitable choices of \(g\), \(f(|P) f(|)=_{ P}[g(^{T})]\) for any random variable \( P\). The most popular example of a contrast function is the kurtosis, which is defined as \(f(|P)=_{ P}(^{T})^{4}/[(^{T})^{2}]^{2}-3\). The aim is to maximize an empirical estimate, \((|P)\). Notable contrast functions are the negentropy, mutual information (used by INFOMAX ), the \(\) function, etc. (See  for further details).

**Fitting strategies:** There are two broad categories of the existing fitting strategies. (I) Find \(A\) such that the components of \(A\) are both as nongaussian and as independent as possible. See, for example, the multivariate cumulant-based method JADE  and characteristic function-based methods [21; 11]. The latter we shall call the PFICA method. (II) Find successively the \(j^{th}\) row of \(A\), denoted by \(A(j,:)^{d}\), \(j=1,,k\) such that \(A(j,:)^{T}\) are independent and each as nongaussian as possible, that is, estimate \(A(1,:)\), project, and then apply the method again on the residuals successively. The chief representative of these methods is FastICA  based on univariate kurtosis.

**From noiseless to noisy ICA:** In the noiseless case one can first prewhiten the dataset using the empirical variance-covariance matrix, \(\), of \(^{(i)}\), using \(^{-1/2}\). Therefore, WLOG, \(B\) can be assumed to be orthogonal. Searching over orthogonal matrices not only simplifies algorithm design for fitting strategy (I) but also makes strategy (II) meaningful. It also greatly simplifies the analysis of kurtosis-based contrast functions (see ). For noisy ICA, prewhitening requires knowledge of the noise covariance, and therefore many elegant methods [48; 4; 49] avoid this step.

**Individual shortcomings of different contrast functions and fitting strategies:** To our knowledge, no single ICA algorithm or contrast function works universally across all source distributions. Adaptively determining which algorithm is useful in a given setting would be an important tool in a practitioner's toolbox. Key challenges include:

**Cumulants:** Even though contrast functions based on _even cumulants_ have no spurious local optima (Theorem 3, ), they can vanish for some non-Gaussian distributions. Our experiments (Section 5) show that kurtosis-based contrast functions can perform poorly even when there are a few independent components with zero kurtosis. They also suffer under heavy-tailed source distributions [3; 11; 26].

**PFICA:** PFICA can outperform cumulant-based methods in some situations (Section 5). However, it is computationally much slower, and poorly performs for Bernoulli(\(p\)) sources with small \(p\).

**FastICA:** show that FastICA's use of Negentropy approximations for computational efficiency may result in a contrast function where optimal directions do not correspond to directions of low entropy. \(\) is another popular smooth contrast function used by FastICA. However, in , the authors show that this tends to return spurious solutions for some distributions .

Contrast functions are usually nonconvex and may have spurious local optima. However, they may still, under suitable conditions, converge to maximal directions. We introduce such a contrast function which vanishes iff \(\) is Gaussian in Section 3.3 and does not require higher moments.

**Our contributions:**

1) Using Theorem 1 we obtain a computationally efficient scoring method for evaluating the \(B\) estimated by different algorithms. Our score can also be extended to evaluate each demixing direction in sequential settings, which we do not pursue here.

2) We propose new contrast functions that work under scenarios where cumulant-based methods fail. We propose a fast sequential algorithm as in  to optimize these and further provide theoretical guarantees for convergence.

## 3 Main contributions

In this section, we present the "Independence score" and state the key result that justifies it. We conclude with two new contrast functions.

### Independence score

While there are many tests for independence [22; 34; 42; 6], we choose the Characteristic function-based one because it is easy to compute and has been widely used in normality testing [19; 20; 25] and ICA [11; 21; 44]. Let us start with noiseless ICA to build intuition. Say we have estimated the inverse of the mixing matrix, i.e. \(B^{-1}\) using a matrix \(F\). Ideally, if \(F=B^{-1}\), then \(F=\), where \(\) is a vector of independent random variables. Kac's theorem  tells us that \([(it^{T})]=_{j=1}^{k}[ (it_{j}z_{j})]\) if and only if \(z_{i}\) are independent. In , a novel characteristic function-based objective (CHFICA), is further analyzed and studied by  (PFICA). Here one minimizes \(|[(it^{T}F)]-_{j=1}^{k} [(it_{j}(F)_{j})]|\) over \(F\). We refer to this objective as the _uncorrected_ independence score. We propose to adapt the CHFICA objective using _estimable parameters_, \(S\), to the noisy ICA setting. We will minimize:

\[(,F|P):=|[(i^{T}F)](^{T}(FSF^{T})}{2} )}_{}-^{k}[(it_{j} (F)_{j})](^{T}FSF^{T}}{2})}_{ }.\] (2)

where \(S=[^{T}]\) and can be estimated using the sample covariance matrix. Hence, we do not require knowledge of any model parameters. We refer to this score as the (_corrected_) independence score. The second terms in JOINT and PRODUCT (Eq 2) _corrects_ the original score by canceling out the additional terms resulting from the Gaussian noise using the covariance matrix S of the data (estimated via the sample covariance). See  for a related score requiring knowledge of the unknown Gaussian covariance matrix \(\). We are now ready to present our first theoretical result for consistency of \((,F|P)\).

**Theorem 1**.: _If \(F^{k k}\) is invertible and the joint and marginal characteristic functions of all independent components, \(\{z_{i}\}_{i[k]}\), are twice-differentiable, then \(^{k}\), \((,F|P)=0\) iff \(F=DB^{-1}\) where \(D\) is a permutation of an arbitrary diagonal matrix._

Unfortunately, \(F\) is not uniquely defined if \(\) is unknown as we have noted in Section 1. The proof of Theorem 1 is provided in the Appendix Section A.1. When using this score in practice, \(S\) is replaced with the sample covariance matrix of the data, and the expectations are replaced by the empirical estimates of the characteristic function. The convergence of this empirical score, \((,F|)\), to the population score, \((,F|P)\), is given by the following theorem.

**Theorem 2**.: _Let \(:=\{F^{k k}:\|F\| 1\}\), \(()\) and \(C_{k}:=(1,k(S))\). Then, we have_

\[_{F}_{ N(0,I_{k})}(,F |P)-_{ N(0,I_{k})}(,F|)=O_{P} ( S(k,^{4} S)^{2 }(nC_{k})}{n}})\]

This bound shows that uniformly over \(F\), the empirical average \(_{ N(0,I_{k})}(,F|)\), is close to the population score. This guarantees that as long as the difference between the population scores of the two candidate algorithms is not too small, the meta-algorithm can pick up the better score.

**Remark 1** (Subgaussianity assumption).: _The subgaussianity assumption in Theorem 2 simply ensures the concentration of the sample covariance matrix since it is used in the score (see Theorem A.3, line 825). It can be relaxed if the concentration in the operator norm is ensured. Appendix Section A.2.1 contains experiments with more super-Gaussian source signals._

The proof of this result is deferred to the Appendix section A.1.1. It is important to note that \((,F|)\) is not easy to optimize. As we show in Section 3.2, objective functions that are computationally more amenable to optimize for ICA, e.g., cumulants, satisfy some properties (see Assumption 1). The independence score does not satisfy the first property (Assumption 1 (a)). In the _noiseless_ case, whitening reduces the problem to \(B\) being orthogonal. This facilitates efficient gradient descent in the Stiefel manifold of orthogonal matrices . However, in the noisy setting, the prewhitening matrix contains the unknown noise covariance \(\), making optimization hard. Furthermore, as noted in Remark 2, in practice, it is better to use \(_{ N(0,I_{k})}(,F|)\) instead of using a fixed vector, \(\), to evaluate \((,F|)\). Any iterative optimization method requires repeatedly estimating this expectation at each gradient step. Therefore, instead of using this score directly as the optimization objective, we use it to choose between different contrast functions after extracting the demixing matrix with each. This process is referred to as the Meta algorithm (Algorithm 1).

**Remark 2** (Averaging over **t)**.: _Algorithm 1 averages the independence score over \((0,_{k})\). While Eq 2 defines the independence score for one direction \(\), in practice, however, there may be some directions such that a non-gaussian signal has a small score in that direction. Hence, it is desirable to average over \(\), following the convention in ._

Figure 1: Correlation of independence score (with std. dev.) with Amari error between \(B^{}= B+(1-)\)\(I\) and \(B\), averaged over 10 random runs.

To gain an intuition of the independence score, we conduct an experiment where we use the dataset mentioned in Section 5 (Figure 2b) and compute the independence score for \(B^{}= B+(1-)\,I,(0.5,1)\). As we increase \(\), \(B^{}\) approaches \(B\), and hence the Amari error \(d_{B^{},B}\) (see Eq 8) decreases. Figure 1 shows the independence score versus Amari error, indicating that the independence score accurately predicts solution quality even without knowing the true \(B\).

### Desired properties of contrast functions

The following properties are often useful for designing provable optimization algorithms for ICA.

**Assumption 1** (Properties of contrast functions).: _Let \(f\) be a contrast function defined for a mean zero random variable \(X\). Then,_

_(a) \(f(u|X+Y)=f(u|X)+f(u|Y)\) for \(X\!\!\! Y\)_

_(b) \(f(0|X)=0,f^{}(0|X)=0\), \(f^{}(0|X)=0\)_

_(c) \(f(u|G)=0, u\) for \(G(0,1)\)_

_(d) WLOG, \(f(u|X)=f(-u|X)\) (symmetry)_

Properties (a) and (c) ensure that \(f(u|G+X)=f(u|X)\) for non-gaussian \(X\) and independent non-gaussian \(G\), which means that the additive independent Gaussian noise does not change \(f\). Property (c) ensures that \(|f(u|G)|\) is minimal for a Gaussian. Property (d) holds without loss of generality because one can always symmetrize the distribution. 2

### New contrast functions

In Section 2, we provided a discussion of the individual shortcomings of different contrast functions for existing contrast functions. Before we introduce new contrast functions in this section, we revisit the algorithmic issues posed by the added Gaussian noise with unknown \(\) in Eq 1.

Prewhitening the data is challenging for noisy ICA because \([^{T}]\) includes the unknown Gaussian covariance matrix \(\).  show that it is enough to _quasi_ orthogonalize the data, i.e., multiply it by a matrix, which makes the data have a diagonal covariance matrix (not necessarily the identity). Subsequent work [50; 49] uses cumulants and the Hessian of \(f()\) to construct a matrix \(C:=BDB^{T}\) where \(D\) is a diagonal matrix, and then use this matrix to achieve quasi-orthogonalization. In general, \(D\) may not be positive semidefinite (e.g. when components of \(\) have kurtosis of different signs). To remedy this, in , the authors propose computing the \(C\) matrix using the Hessian of \(f()\) at some fixed unit vector and then perform an elegant power method in the pseudo-Euclidean space:

\[_{t}\, f(C^{}_{t-1}|P)\| f (C^{}_{t-1}|P)\|\] (3)

A pseudo-Euclidean space is a generalization of the Euclidean space used by . Here the produce between vectors \(,\) is given as \(u^{}Av\),

``` Input:\(C^{k k}\), \( f\) \( 0\), \( 0\) for\(j\) in range[1, \(k\)] do  Draw \(\) uniformly at random from \(^{k-1}\) while Convergence (up to sign) do \(\) \( f(C^{})/\| f (C^{})\|_{2}\) endwhile \(_{j}\), \(_{j}[C^{}_{j}/((C^{} _{j})^{}_{j})]^{}\) endfor return\(,\) ```

**Algorithm 2** Pseudo-Euclidean Power Iteration for ICA (Algorithm 2 in ) \(\) is the recovered matrix for the ICA model in Eq 1. \(\) is a running estimate of \(^{}\).

In this section, we present two new contrast functions based on the logarithm of the characteristic function (CHF) and the cumulant generating function (CGF). These do not depend on a particular cumulant, and the first only requires a finite second moment, which makes it suitable for heavy-tailed source signals. We will use \(f(|P)\) or \(f(|)\) where \( P\) interchangeably to represent the population contrast function. In constructing both of the following contrast functions, we use the fact that, like the cumulants, the CGF and CHF-based contrast functions satisfy Assumption 1 (a). To satisfy Assumption 1 (c), we subtract out the part resulting from the Gaussian noise, which leads to the additional terms involving \(^{}S\).

**CHF-based contrast function:** Recall that \(S\) denotes the covariance matrix of \(\). We maximize the _absolute value_ of following:

\[f(|P) =(i^{T})+(-i^{T})+^{T}S\] \[=((^{T}))^{2}+(( ^{T}))^{2}+^{T}S\] (4)

The intuition is that this is exactly zero for zero mean Gaussian data and maximizing this leads to extracting non-gaussian signal from the data. It also satisfies Assumption 1 a), b) and c).

**CGF-based contrast function:** The cumulant generating function also has similar properties (see  for the noiseless case). In this case, we maximize the _absolute value_ of following:

\[f(|P)=(^{T})-^{T}S\] (5)

Like CHF, this vanishes iff \(\) is mean zero Gaussian, and satisfies Assumption 1 a), b) and c). However, it cannot be expected to behave well in the heavy-tailed case. In the Appendix (Section A.1, Theorem A.1), we show that the Hessian of both functions obtained from Eq 4 and Eq 5 evaluated at some \(\) is, in fact, of the form \(BDB^{T}\) where \(D\) is some diagonal matrix.

## 4 Global and local Convergence: loss landscape of noisy ICA

Here we present sufficient conditions for global and local convergence of a broad class of contrast functions. This covers the cumulant-based contrast functions and our CHF and CGF-based proposals.

### Global convergence

The classical argument for global optima of kurtosis for noiseless ICA in  assumes WLOG that \(B\) is orthogonal. This reduces the optimization over \(\) into one for \(=B\). Since \(B\) is orthogonal, \(\|\|\!=\!1\) translates into \(\|\|\!=\!1\). It may seem that this idea should extend to the noisy case due to property 1 a) and c) by optimizing over \(=B\) over potentially non-orthogonal mixing matrices \(B\).

However, for non-orthogonal \(B\), the norm constraint on \(\) is no longer \(^{T}=1\), _rendering the original argument invalid_. In what follows, we extend the original argument to the noisy ICA setting by introducing pseudo-euclidean constraints. Our framework includes a large family of contrast functions, including cumulants.

To our knowledge, this is the first work to characterize the loss functions in the pseudo-euclidean space. In contrast,  provides global convergence of the cumulant-based methods by a convergence argument of the power method itself.

Consider the contrast function \(f(|P):=_{ P}[g(^{T})]\) and recall the definition of the quasi orthogonalization matrix \(C=BDB^{T}\), where \(D\) is a diagonal matrix. For simplicity, WLOG let us assume that \(D\) is invertible. We now aim to find the optimization objective that leads to the pseudo-Euclidean update in Eq 3. For \(f(C^{-1}|P)=_{ P}[g(^{T}C ^{-1})]\), consider the following:

\[f(C^{-1}|P)=_{i[k]}[g( (B^{-1})_{i}z_{i}/D_{ii})]=_{i[k]}[g(_{i}_{i})]=_{i[k]}f( _{i}/D_{ii}|z_{i})\] (6)

where we define \(:=B^{-1}\) and \(_{i}=z_{i}/D_{ii}\). We now examine \(f(C^{-1})\) subject to the "pseudo" norm constraint \(^{T}C^{-1}=1\). The key point is that for suitably defined \(f\), one can construct a matrix \(C=BDB^{T}\) from the data even when \(B\) is unknown. So our new objective is to optimize

\[f(C^{-1}|P)^{T}C^{-1}=1\]Using the Lagrange multiplier method, optimizing the above simply gives the power method update (Eq 3) \( f(C^{-1})\). Furthermore, optimizing Eq 6 leads to the following transformed objective:

\[_{}|_{i}f(._{i}/D_{ii}|z_{i} )|\ \ _{i}_{i}^{2}/D_{ii}=1\] (7)

Now we provide our theorem about maximizing \(f(C^{-1}|P)\). Analogous results hold for minimization. We will denote the corresponding derivatives evaluated at \(t_{0}\) as \(f^{}(t_{0}|z_{i})\) and \(f^{}(t_{0}|z_{i})\).

**Theorem 3**.: _Let \(C\) be a matrix of the form \(BDB^{T}\), where \(d_{i}:=D_{ii}=f^{}(u_{i}|z_{i})\) for some random \(u_{i}\). Let \(S_{+}=\{i:d_{i}>0\}\). Consider a contrast function \(f:\). Assume hard for every non-Gaussian independent component, Assumption 1 holds and the third derivative, \(f^{}(u|X)\), does not change the sign in the half-lines \([0,),\,(,0]\). Then \(f(.C^{-1}|)\) with the constraint of \(,_{C^{-1}}=1\) has local maxima at \(B^{-1}=_{i},i S_{+}\). All solutions satisfying \(_{i}^{T}B^{-1} 0,\  i[1,k]\) are minima, and all solutions with \(|\{i:_{i}^{T}B^{-1} 0\}|<k\) are saddle points. These are the only fixed points._

Note that the above result immediately implies that for \(=-C\), the same optimization in Theorem 3 will have maxima at all \(\) such that \(B^{-1}=_{i},i S_{-}\).

**Corollary 3.1**.: \(2k^{th}\) _cumulant-based contrast functions for \(k>1\), have maxima and minima at the directions of the columns of \(B\), provided \(\{_{i}\}_{i[k]}\) have a corresponding non-zero cumulant._

Proof.: This proof (see Appendix Section A.1) is immediate since cumulants satisfy (a), (c). For (c) and (d), note that \(f(u|Z)\) is simply \(u^{2j}_{2j}(Z)\), where \(_{2j}(Z)\) is the \(2j^{th}\) cumulant of \(Z\). 

Theorem 3 can be easily extended to the case where \(C\) is not invertible. The condition on the third derivative, \(f^{}(u|X)\), may seem strong, and it is not hard to construct examples of random variables \(Z\) where this fails for suitable contrast functions (see ). However, for such settings, it is hard to separate \(Z\) from a Gaussian. See the Appendix A.4.1 for more details.

### Local convergence

In this section, we establish a local convergence result for the power iteration-based method described in Eq 3. Define \( t, i[d]\), the functions \(q_{i}(t):=f^{}(.}{D_{ii}}|z_{ i})\). Then, without loss of generality, we have the following result for the first corner:

**Theorem 4**.: _Let \(_{i}=B^{-1}_{i}\) and \(^{}:=_{1}\). Let the contrast function \(f(.|X)\), satisfy assumptions 1 (a), (b), (c), and (d). Let \(:=[-\|B^{-1}\|_{2},\|B^{-1}\|_{2}]\) and define \(c_{1},c_{2},c_{3}>0\) such that \( i[d]\), \(_{x}\{(x)|}{c_{1}},^{}(x )|}{c_{2}},^{}(x)|}{c_{3}}\} 1\). Define \(:=}{\|B_{1}\|^{2}}\{+c_{2}\| B_{1}\|}{|q_{1}(_{1}^{})|},^{2}}{|q_{1}(_{1}^{ })|},^{2}}{|q_{1}^{}(_{1}^{})|}\}\). Let \(\|_{0}-^{}\|_{2} R\), \(R\{}{c_{3}},\}\). Then, we have \( t 1,\ _{t+1}-^{}\|}{\|_{t}-^{}\|}\)._

Therefore, we can establish linear convergence without the third derivative constraint required for global convergence (see Theorem 3), by assuming some mild regularity conditions on the contrast functional and a sufficiently close initialization. The proof is included in Appendix Section A.1.2.

**Remark 3**.: _Let \(=B^{-1}\) denote the derived direction, \(\). Theorem 4 shows that if the initial \(_{0}\) is such that \(_{0}\) is close to the ground truth \(^{}\) (which is WLOG taken to be \(_{1}\)), then there is geometric convergence to \(^{}\). \(|q_{1}(_{1}^{})|\) and \(|q_{1}^{}(_{1}^{})|\) essentially quantify how non-gaussian the corresponding independent component is since they are zero for a mean zero Gaussian. When these are large quantities, \(\) is small, and the initialization radius \(\|_{0}-^{}\|_{2}\) can be relatively larger._

## 5 Experiments

In this section, we provide experiments to compare the fixed-point algorithms based on the characteristic function (CHF), the cumulant generating function (CGF) (Eqs 4 and 5) with the kurtosis-based algorithm (PEGI-\(_{4}\)). We also compare against noiseless ICA3 algorithms - FastICA, JADE, and PFICA. These six algorithms are included as candidates for the Meta algorithm (see Algorithm1). We also present the _Uncorrected Meta_ algorithm (_Unc. Meta_) to denote the Meta algorithm with the uncorrected independence score proposed in CHFICA . Table 1 and Figure 2 provide experiments on simulated data, and Figure 3 provides an application for image demixing . We provide additional experiments on denoising MNIST images in the Appendix Section A.2.

**Experimental setup:** Similar to , the mixing matrix \(B\) is constructed as \(B=U V^{T}\), where \(U,V\) are \(k\) dimensional random orthonormal matrices, and \(\) is a diagonal matrix with \(_{ii}\). The covariance matrix \(\) of the noise \(\) follows the Wishart distribution and is chosen to be \(RR^{T}\), where \(k\) is the number of sources, and \(R\) is a random Gaussian matrix. Higher values of the _noise power_\(\) can make the noisy ICA problem harder. Keeping \(B\) fixed, we report the median of 100 random runs of data generated from a given distribution (different for different experiments). The quasi-orthogonalization matrix for CHF and CGF is initialized as \(^{T}\) using the mixing matrix, \(\), estimated via PFICA. The performance of CHF and CGF is based on a single random initialization of the vector in the power method (see Algorithm 2). Our experiments were performed on a Macbook Pro M2 2022 CPU with 8 GB RAM.

**Error Metrics:** Due to the inherent ambiguity in signal recovery discussed in Section 2, we measure error using the accuracy of estimating \(B\). We report the widely used Amari error  for our results. For estimated demixing matrix \(\), define \(W=^{-1}B\), after normalizing the rows of \(^{-1}\) and \(B^{-1}\). Then we measure \(d_{,B}\) as -

\[d_{,B}:=(_{i=1}^{k}^{k} W _{ij}}{_{j} W_{ij}}+_{j=1}^{k}^{k}  W_{ij}}{_{i} W_{ij}})-2\] (8)

**Variance reduction using Meta:** We first show that the independence score can also be used to pick the best solution from many random initializations. In Figure 2 (a), the top panel has a histogram of 40 runs of CHF, each with one random initialization. The bottom panel shows the histogram of 40 experiments, where, for each experiment, the _best out of 30 random initializations_ are picked using the independence score. The top and bottom panels have (mean, standard deviation) (0.51, 0.51) and (0.39,0.34) respectively. This shows a reduction in variance and overall better performance.

**Effect of varying kurtosis:** For our second experiment (see Table 1), we use \(k=5\) independent components, sample size \(n=10^{5}\) and noise power \(=0.2\) from a Bernoulli(\(p\)) distribution, where we vary \(p\) from 0.001 to \(0.5-1/\). The last parameter makes kurtosis zero. Different algorithms perform differently (best candidate algorithm is highlighted in bold font) for different \(p\). In particular, characteristic function-based methods like PFICA and CHF perform poorly for small values of \(p\). We attribute this to the fact that the characteristic function is close to one for small \(p\). Kurtosis-based algorithms like PEGI, JADE, and FASTICA perform poorly for kurtosis close to zero. Furthermore, the Uncorrected Meta algorithm performs worse than the Meta algorithm since it shadows PFICA.

**Effect of varying noise power:** For our next two experiments, we use \(k=9\), out of which 3 are from Uniform \((-,)\), 3 are from Exponential\((5)\) and 3 are from \(((-}))\) and hence have zero kurtosis. In these experiments, we vary the sample size (Figure 2b), \(n\) fixing \(=0.2\), and

 
**Algorithm\(_{4}\)** & **994** & **194** & **95** & **15** & **5** & **2** & **0.8** & **0.13** & **0** \\  
**Meta** & **0.007** & **0.010** & **0.011** & **0.010** & **0.011** & **0.0128** & **0.01981** & **0.023** \\ 
**CHF** & 1.524 & 0.336 & **0.011** & **0.010** & **0.011** & **0.011** & **0.0129** & **0.0213** & 0.029 \\ 
**CGF** & **0.007** & 0.011 & **0.011** & 0.016 & 0.029 & 0.044 & 0.05779 & 0.06521 & 0.071 \\ 
**PEGI** & **0.007** & **0.010** & **0.011** & **0.010** & 0.012 & 0.017 & 0.02795 & 0.13097 & 1.802 \\ 
**PFICA** & 1.525 & 0.885 & 0.540 & 0.024 & 0.023 & 0.023 & 0.0212 & 0.0224 & **0.024** \\ 
**JADE** & 0.021 & 0.022 & 0.021 & 0.022 & 0.022 & 0.023 & 0.029 & 0.089 & 1.909 \\ 
**FastICA** & 0.024 & 0.027 & 0.026 & 0.026 & 0.026 & 0.027 & 0.02874 & 0.0703 & - \\ 
**Unc. Meta** & 1.52 & 0.049 & 0.041 & 0.0408 & 0.0413 & 0.0414 & 0.0413 & 0.0416 & 0.0419 \\  

Table 1: Median Amari error with varying \(p\) in Bernoulli\((p)\) data. The scaled kurtosis is given as \(_{4}:=(1-6p(1-p))/(p(1-p))\). Observe that the Meta algorithm (shaded in red) performs at par or better than the best candidate algorithms. FastICA did not converge for zero-kurtosis data.

the noise power (Figure 1(a)), \(\) fixing \(n=10^{5}\), respectively. Note that with most mixture distributions, it is easily possible to have low or zero kurtosis. We include such signals in our data to highlight some limitations of PEGI-\(_{4}\) and show that Algorithm 1 can choose adaptively to obtain better results.

Figure 1(a) shows that large noise power leads to worse performance for all methods. PEGI, JADE, and FastICA perform poorly consistently, because of some zero kurtosis components. CGF suffers because of heavy-tailed components. However, CHF and PFICA perform well consistently. The Meta algorithm mostly picks the best algorithm except for a few points, where the difference between the two leading algorithms is small. The Uncorrected Meta algorithm (which uses the independence score without the additional correction term introduced for noise) performs identically to PFICA.

**Effect of varying sample size:** Figure 1(b) shows that the noiseless ICA methods have good performance at smaller sample sizes. However, they suffer a bias in performance compared to their noiseless counterparts as the sample size increases. The Meta algorithm can consistently pick the best option amongst the candidates, irrespective of the distribution, leading to significant improvements in performance. What is interesting is that up to a sample size of \(10^{4}\), PFICA dominates other algorithms and Meta performs like PFICA. However, after that CHF dominates, and one can see that Meta starts to have a similar trend as CHF. We also see that the Uncorrected Meta algorithm (which uses the independence score without the additional correction term introduced for noise) has a near identical error as PFICA and has a bias for large \(n\).

## 6 Conclusion

ICA is a classical problem that aims to extract independent non-Gaussian components. The vast literature on both noiseless and noisy ICA introduces many different inference methods based on a

Figure 2: Amari error in the \(\)-scale on \(y\) axis and varying noise powers (for \(n=10^{5}\)) and varying sample sizes (for \(=0.2\)) on \(x\) axis for figures 1(a) and 1(b) respectively. For figure 1(c), the top panel contains a histogram of 40 runs with one random initialization. The bottom panel contains a histogram of 40 runs, each of which is the best independence score out of 30 random initializations.

variety of contrast functions for separating the non-Gaussian signal from the Gaussian noise. Each has its own set of shortcomings. We aim to identify the best method for a given dataset in a data-driven fashion. In this paper, we propose a nonparametric score, which is used to evaluate the quality of the solution of any inference method for the noisy ICA model. Using this score, we design a Meta algorithm, which chooses among a set of candidate solutions of the demixing matrix. We also provide new contrast functions and a computationally efficient optimization framework. While they also have shortcomings, we show that our diagnostic can remedy them. We provide uniform convergence properties of our score and theoretical results for local and global convergence of our methods. Simulated and real-world experiments show that our Meta-algorithm matches the accuracy of the best candidate across various distributional settings.