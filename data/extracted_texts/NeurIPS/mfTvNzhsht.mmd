# Simina Branzei

## Dueling Over Dessert, Mastering the Art of Repeated Cake CuttingPurdue University

simina.branzei@gmail.com

**MohammadTaghi Hajiaghayi**

University of Maryland

hajiaghayi@gmail.com

**Reed Phillips**

Purdue University

phill289@purdue.edu

**Suho Shin**

University of Maryland

suhoshin@umd.edu

**Kun Wang**

Purdue University

wang5675@purdue.edu

## Abstract

We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: _sequential_, where Bob observes Alice's cut point before choosing left/right, and _simultaneous_, where he only observes her cut point after making his choice. The simultaneous version was first considered in Aumann and Maschler (1995).

We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.

We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce the equitable utility profile of \((1/2,1/2)\) in the limit on every trajectory of play, by keeping the other player's utility to approximately \(1/2\) on average while guaranteeing they themselves get at least approximately \(1/2\) on average. We show this theorem using a connection with Blackwell approachability.

Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that fictitious play converges to the equitable utility profile of \((1/2,1/2)\) at a rate of \(O(1/)\).

Introduction

Cake cutting is a model of fair division Steinhaus (1948), where the cake is a metaphor for a heterogeneous divisible resource such as land, time, memory in shared computing systems, clean water, greenhouse gas emissions, fossil fuels, or other natural deposits (Procaccia (2013)). The problem is to divide the resource among multiple participants so that everyone believes the allocation is fair. There is an extensive literature on cake cutting in mathematics, political science, economics (Robertson and Webb (1998); Brams and Taylor (1996); Moulin (2003)) and computer science (Brandt et al. (2016)), with a number of protocols implemented (Goldman and Procaccia (2014)).

Traditional approaches to cake cutting often consider single instances of division. However, many real-world scenarios require a repeated division of resources. For instance, consider the recurring task of allocating classroom space in educational institutions each quarter or that of repeatedly dividing computational resources (such as CPU and memory) among the members of an organization. These settings reflect the reality of many social and economic interactions, necessitating a model that not only addresses the fairness of a single division, but also the dynamics and strategies that emerge among participants over repeated interactions.

Repeated fair division is a classic problem first considered by Aumann and Maschler (1995), where two players--denoted Alice and Bob--have private valuations over the cake and interact in the following environment. Every day a new cake arrives, which is the same as the ones in previous days. Alice cuts the cake at a point of her choosing, while Bob chooses either the left piece or the right piece, leaving the remainder to Alice. Aumann and Maschler (1995) considered the simultaneous setting, where both players take their actions at the same time each day, and analyzed the payoffs achievable by Bob when he can have one of two types of valuations.

In this paper, we provide the first substantial progress in this classic setting. We further analyze the simultaneous version from Aumann and Maschler (1995) and also go beyond it, by considering the sequential version where Bob has the advantage of observing Alice's chosen cut point before making his selection. Tactical considerations remain pivotal in the sequential version, which is none other than the repeated _Cut-and-choose_ protocol with strategic players.

A key observation in our study is the strategic vulnerability inherent in repeated Cut-and-choose. At a high level, if Bob consistently chooses his preferred piece, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time. To fight back Alice's attempt to exploit him, Bob could deceive her by being unpredictable, thus hiding his preferences. While this behavior has the potential to reduce Alice's share of the cake, it could also come at the price of affecting Bob's own payoff guarantees in the long term.

Our analysis of the repeated cake cutting game formalizes the intuition that Alice can exploit a (nearly) myopic Bob that often chooses his favorite piece. This outcome, where Alice gains more value, is not entirely fair, as it leaves her happier than Bob. The fairness notion of _equitability_ addresses this imbalance, embodying the idea that players should be equally happy. Formally, it requires that Alice's value for her allocation should equal Bob's value for his allocation. Achieving equatiability is particularly important in scenarios with potential for conflict, such as splitting an inheritance.

We show that achieving equitable outcomes in the repeated interaction is in fact possible. Specifically, each player has a strategy that guarantees the other player receives no more than approximately \(1/2\) on average, while securing at least approximately \(1/2\) for themselves. This approaches the equitable utility profile of \((1/2,1/2)\) in the limit. We obtain this result by using a connection with Blackwell approachability (1956). Moreover, we consider a natural dynamic known as fictitious play (Brown (1951)), where players best respond to the empirical frequency of the other player's past actions. We show that fictitious play converges to the equitable utility profile of \((1/2,1/2)\) at a rate of \(O(1/)\).

### Model

Cake cutting model for two players.The cake is modelled as the interval \(\). There are players \(N=\{A,B\}\), where \(A\) stands for Alice and \(B\) for Bob. Each player \(i\) has a private value density function \(v_{i}:_{+}\). A _piece_ of cake is a measurable set \(S\). The value of player \(i\) for \(S\) is \(V_{i}(S)=_{x S}v_{i}(x)\ dx\). Atoms are worth zero and the valuations are normalized so that \(V_{i}()=1\  i[n]\). We require bounded densities, i.e. there exist fixed arbitrary \(,>0\) such that \( v_{i}(x)\) for all \(x\).

An allocation \(Z=(Z_{A},Z_{B})\) is a partition of the cake among the players such that each player \(i\) receives piece \(Z_{i}\), the pieces are disjoint, and \(_{i N}Z_{i}=\). The valuation (aka utility or payoff) of player \(i\) at an allocation \(Z\) is \(V_{i}(Z_{i})\). An allocation \(Z\) is _equitable_ if the players are equally happy with their pieces, meaning \(V_{A}(Z_{A})=V_{B}(Z_{B})\).

Let \(m_{A}\) be Alice's midpoint of the cake and \(m_{B}\) Bob's midpoint. Alice's _Stackelberg value_, denoted \(u^{*}_{A}\), is the utility she receives when she cuts the cake at \(m_{B}\) and Bob chooses his favorite piece, breaking ties in Alice's favor. The midpoints and Alice's Stackelberg value are depicted in Figure 1.

Repeated cake cutting.Each round \(t=1,2,,T\), the next steps take place:

* A new cake arrives, which is identical to the ones in previous rounds.
* Alice cuts the cake at a point \(a_{t}\) of her choice. Bob chooses either the left piece or the right piece, then Alice takes the remainder.

We consider two versions: _sequential_, where Bob observes Alice's cut point \(a_{t}\) before choosing left/right, and _simultaneous_, where he only observes her cut point after making his choice.

A pure strategy is a map from the history observed by a player to the next action to play. A mixed strategy is a probability distribution over pure strategies.

### Our Results

Our results will examine how players fare in the repeated game over \(T\) rounds. Given a history \(H\), Alice's Stackelberg regret is \(_{A}(H)=_{t=1}^{T}[u^{*}_{A}-u^{t}_{A}(H)]\), where \(u^{*}_{A}\) is Alice's Stackelberg value and \(u^{t}_{A}(H)\) is Alice's utility in round \(t\) under history \(H\).

Suppose Alice uses a mixed strategy \(S_{A}\) and Bob uses a mixed strategy \(S_{B}\). Then \(S_{A}\) ensures Alice's Stackelberg regret is at most \(\) against \(S_{B}\) if \(_{A}(H)\) for all \(T\)-round histories \(H\) that could have arisen under the strategies \((S_{A},S_{B})\). Precise definitions for strategies/regret and the remaining notation needed for the full proofs can be found in Section 3.

#### Alice exploiting Bob

We start with an observation about the sequential setting. If Bob chooses his favorite piece in each round, then Alice can exploit him by running binary search until identifying his midpoint within a small error and then cutting near it for the rest of time. This will lead to Alice getting essentially her Stackelberg value in all but \(O( T)\) rounds, while Bob will get \(1/2\) in all but \(O( T)\) rounds.

**Proposition 1**.: _If Bob plays myopically in the sequential setting, then Alice has a strategy that ensures her Stackelberg regret is \(O( T)\)._

This exploitation phenomenon holds more generally: if Bob's strategy has bounded regret with respect to the standard of selecting his preferred piece in every round in hindsight, then Alice can almost get her Stackelberg value in each round. Her Stackelberg regret is a function of Bob's regret guarantee, as quantified in the next theorem.

Figure 1: Densities for Alice (blue) and Bob (red). Figure (a) shows Alice’s midpoint \(m_{A}\) and Bob’s midpoint \(m_{B}\). The shaded area in Figure (b) is Alice’s Stackelberg value.

**Theorem 1** (Exploiting a nearly myopic Bob).: _Let \([0,1)\). Suppose Bob plays a strategy that ensures his regret is \(O(T^{})\) in the sequential setting. Let \(^{}\) denote the set of all such Bob strategies._

\(\) _If Alice knows \(\), she has a strategy \(S_{A}=S_{A}()\) that ensures her Stackelberg regret is \(O(T^{} T)\). The exponent is sharp: Alice's Stackelberg regret is \((T^{})\) for some Bob strategy in \(^{}\)._

\(\) _If Alice does not know_ \(\)_, she has a strategy_ \(S_{A}\) _that ensures her Stackelberg regret is_ \(O\)_. The exponent is sharp: if_ \(S_{A}\) _guarantees Alice Stackelberg regret_ \(O(T^{})\) _against all Bob strategies in_ \(^{}\) _for some_ \([0,1)\)_, then_ \(S_{A}\) _has Stackelberg regret_ \((T)\) _for some Bob strategy in_ \(^{}\)_._

In contrast, in the simultaneous setting, Alice may not approach her Stackelberg value on _every_ trajectory of play. In order to get her Stackelberg value in any given round, Alice needs to cut near Bob's midpoint and Bob needs to pick the piece he prefers, say \(R\). However, if Bob deterministically commits to picking \(R\), he will be completely exploited by an Alice who cuts at \(1\), breaking any reasonable regret guarantee he might have. Indeed, any Bob with a deterministic strategy (possibly using different actions over the rounds) has a corresponding Alice who can completely exploit him. Therefore, any Bob strategy with a good regret guarantee would behave randomly, making it impossible for Alice to reliably get her Stackelberg value on every trajectory. For this reason, we focus on the sequential setting when studying how Alice can exploit Bob.1

#### Equitable payoffs.

Motivated by Theorem 1, we examine the general limits of how much each player can exploit the other and whether fair outcomes are achievable, in both the sequential and simultaneous settings.

Given a history \(H\), player \(i\) is said to get an average payoff of \(\) if \(_{t=1}^{T}u_{i}^{t}(H)=\), where the left hand side is not expected utility, but rather the observed total utility averaged over \(T\) rounds.

We say a utility profile \((u_{A},u_{B})\) is _equitable_ if \(u_{A}=u_{B}\). In the single round setting, \(u_{A}\) and \(u_{B}\) will naturally represent the utilities of the players at an allocation. In the repeated setting, \(u_{A}\) and \(u_{B}\) will represent the time-average utilities of the players.

The next theorems show that each player can keep the other player at \(1/2\) while guaranteeing \(1/2\) for themselves. This type of behavior is reminiscent of spiteful bidding in auctions (Tang and Sandholm (2012)), where a buyer's utility diminishes if other bidders are too satisfied.

**Theorem 2** (Alice enforcing equitable payoffs; informal).: _In both the sequential and simultaneous settings, Alice has a pure strategy \(S_{A}\), such that for every Bob strategy \(S_{B}\):_

\(\) _on every trajectory of play, Alice's average payoff is at least \(1/2-o(1)\), while Bob's average payoff is at most \(1/2+o(1)\). More precisely, \(}{T}-(})\) and \(}{T}+()\), where \(u_{i}\) is the cumulative payoff of player \(i\) over the time horizon \(T\)._

A key ingredient in the proof of Theorem 2 is a connection with Blackwell's approachability theorem Blackwell (1956). Generally speaking, Blackwell approachability can be used by a player to limit the payoff of the other player in a certain region of the utility profile. However, the main challenge is that there are uncountably many types of Bob and so Alice cannot apply the strategy from Blackwell directly. Instead, Alice's strategy constructs a countably infinite set of representatives, which allows us to adapt Blackwell's argument to this setting.

We show a symmetric theorem for Bob in the sequential setting, while in the simultaneous setting Bob's guarantee only holds in expectation.

**Theorem 3** (Bob enforcing equitable payoffs; informal).:

\(\) In the sequential setting: _Bob has a pure strategy \(S_{B}\), such that for every Alice strategy \(S_{A}\), on every trajectory of play, Bob's average payoff is at least \(1/2-o(1)\), while Alice's average payoff is at most \(1/2+o(1)\). More precisely, \(}{T}-}\) and \(}{T}+(})\)._* In the simultaneous setting: _Bob has a mixed strategy \(S_{B}\), such that for every Alice strategy \(S_{A}\), both players have average payoff \(1/2\) in expectation._

When Alice and Bob play such strategies against each other, they approach an equitable utility profile of \((1/2,1/2)\). If just one player follows such a strategy, then the best the other player can do is to ensure the safety value of \(1/2\) for themselves, thus achieving the utility profile \((1/2,1/2)\).

#### Fictitious play

Fictitious play is a classic learning rule where at each round, each player best responds to the empirical frequency of play of the other player. Fictitious play was introduced in Brown (1951). Convergence to Nash equilibria has been shown for zero-sum games (Robinson (1951)) and special cases of general-sum games (Nachbar (1990); Monderer and Shapley (1996b, a)).

In the cake cutting model, learning rules such as fictitious play are more meaningful in the simultaneous setting, where there is uncertainty for both players due to the simultaneous actions. The precise definition of the fictitious play dynamic is in Section 6, while an example of trajectories for an instance with random valuations and uniform random tie-breaking can be found in Figure 2.

The convergence properties of fictitious play can be characterized as follows.

**Theorem 4** (Fictitious Play; informal).: _When both Alice and Bob run fictitious play, the average payoff of each player converges to \(1/2\) at a rate of \(O(1/)\)._

Roadmap to the paper.Related work is in Section 2. Formal notation and preliminaries can be found in Section 3. An overview of how Alice can exploit a nearly myopic Bob can be found in Section 4, with formal proofs in Appendix A. An overview of how players can enforce equitable payoffs can be found in Section 5, with formal proofs in Appendix B. Fictitious play can be found in Section 6, with formal proofs in Appendix C. Concluding remarks can be found in Section 7.

## 2 Related Work

**Cake cutting and fairness notions.** The cake cutting model is due to Steinhaus (1948). Standard fairness notions include proportionality, equitability, envy-freeness (Even and Paz (1984); Dubins and Spanier (1961); Edward Su (1999); Stromquist (1980); Alon (1987)). For surveys, see Robertson and Webb (1998); Brams and Taylor (1996); Moulin (2003); Brandt et al. (2016); Procaccia (2013).

In the Robertson-Webb (RW) query model for cake cutting (Woeginger and Sgall (2007)), a mediator asks the players enough queries about their preferences until it can output a fair allocation. For studies on the query complexity of cake cutting, see Even and Paz (1984); Woeginger and Sgall (2007); Edmonds and Pruhs (2006); Procaccia (2009); Aziz and Mackenzie (2016); Amanatidis et al. (2018); Cheze (2020); Stromquist (2008); Deng et al. (2012); Goldberg et al. (2020a); Filos-Ratsikas et al. (2022); Segal-Halevi (2018); Deligkas et al. (2021); Goldberg et al. (2020b); Branzei and Nisan (2022, 2019); Filos-Ratsikas et al. (2020); Alon and Graur (2020); Filos-Ratsikas et al. (2021).

Figure 2: Illustration of Alice’s and Bob’s average payoff in a randomly generated instance of valuations. The X axis shows the time and the Y axis shows the average payoff up to that round.

Mossel and Tamuz (2010); Branzei and Miltersen (2015) studied truthful cake cutting in the RW query model, and Chen et al. (2013); Bu et al. (2023); Bei et al. (2022); Tao (2022) in the direct revelation model. The equilibria of cake cutting protocols were considered in Nicolo and Yu (2008); Branzei and Miltersen (2013); Branzei et al. (2016); Goldberg and Iaru (2021).

Multiple divisible/indivisible goods and chores.The algorithms and complexity of finding fair allocations in settings with multiple divisible/indivisible goods/bads were considered in Oh et al. (2021); Plaut and Roughgarden (2020, 2019); Manurangsi and Suksompong (2021); Chaudhury et al. (2021c); Bilo et al. (2019); Amanatidis et al. (2022); Procaccia (2020); Chaudhury et al. (2020, 2021b); Procaccia and Wang (2014); Kulkarni et al. (2021); Chaudhury et al. (2021a). Tucker-Foltz and Zeckhauser (2023) analyze how the cutter should act in a single-round cut-and-choose on multiple goods where the players' valuations of the goods are drawn from a publicly known distribution.

Ghodsi et al. (2011) studied fairness in cloud computing settings, where there are multiple divisible goods (e.g. CPU and memory) and the users have to run jobs with different resource requirements. Kandasamy et al. (2020) studied players who do not know their own resource requirements.

Dynamic fair division.Closest to our setting is the analysis in the book of Aumann and Maschler (1995) (page 243), where two players are dividing a cake with a cherry. Alice (the cutter) has a uniform density and so she does not care for the cherry, while Bob (the chooser) may or may not like the cherry. Alice and Bob declare their actions simultaneously and Alice is only allowed to cut in one of two locations. Additionally, Alice has a prior over the type of Bob she is facing. Aumann and Maschler (1995) analyzes the set of payoffs approachable for Bob using Blackwell approachability. In contrast, we allow arbitrary value densities for the players and do not assume priors and also consider the sequential version of the game.

Tamuz et al. (2018) introduced exploitability in repeated cut-and-choose protocols, with some cuts made by a mediator, designing non-exploitable protocols. Online cake cutting was studied by Walsh (2011), where agents can arrive/depart over time. For dynamic fair division where goods are allocated irrevocably upon arrival, see Kash et al. (2014); Friedman et al. (2015); Benade et al. (2022).

Learning in repeated Stackelberg games.The Stackelberg game was introduced by Stackelberg (1934) to understand the first mover advantage of firms when entering a market. The Stackelberg equilibrium concept has important applications such as security games Tambe (2011); Balcan et al. (2015), online strategic classification Dong et al. (2018), and online principal agent problems Hajiaghayi et al. (2023). Our model can be seen as each player facing an online learning version of a repeated Stackelberg game.

Kleinberg and Leighton (2003) considered a seller's problem of designing an efficient repeated posted price mechanism to buy identical goods when it interacts with a sequence of myopic buyers. Gan et al. (2019); Birmpas et al. (2020); Zhao et al. (2023) considered a repeated Stackelberg game to study how the follower or leader can exploit the opponent in a general game with arbitrary payoffs. Their techniques, however, do not apply to our model as they typically consider the setting of one player knowing the entire payoff matrix trying to deceive the other player given behavioral assumptions.

Exploiting no-regret agents.Several works considered the extent to which one player can exploit the knowledge that the other player has a strategy with sublinear regret. The goal is often to approach the Stackelberg value, the maximum payoff that the exploiter could get by selecting an action first and allowing the opponent to best-respond. In simultaneous games, Deng et al. (2019) showed the exploiter can approach their Stackelberg value, assuming knowledge of the other player's payoff function. Haghtalab et al. (2022) showed that, for certain types of sequential games, an exploiting leader can approach their Stackelberg value in the limit. Theorem 1 is a similar statement in our setting, but we bound the exploited agent's behavior with an explicit regret guarantee rather than using discounted future payoffs; also, our setting is not captured by the types of games they consider.

Fictitious play.Fictitious play was introduced in Brown (1951). Convergence to Nash equilibria has been shown for zero-sum games (Robinson (1951)) and special cases of general-sum games (Nachbar (1990); Monderer and Shapley (1996b, a)). None of these results directly apply to our setting, but the most relevant is Berger (2005), which covered non-degenerate \(2 n\) games (i.e. where every action has a unique best response). Our "\(2\)" game is degenerate, as Bob does not have a unique best response to Alice cutting at \(m_{B}\). Few existing works apply fictitious play to games with continuous action spaces. An example is Perkins and Leslie (2014), which showed that stochastic fictitious play does converge in two-player zero-sum games with continuous action spaces.

Karlin (1959) conjectured that fictitious play converges at a rate of \(O(T^{-1/2})\). Brandt et al. (2013) found small games where the convergence rate is \(O(T^{-1/2})\), but with very large constants in the \(O()\). Daskalakis and Pan (2014) disproved Karlin's conjecture, showing there exist games with \(n n\) payoff matrices in which convergence takes place at a rate of \((T^{-1/n})\) using adversarial tie-breaking rules. Panageas et al. (2023) found examples of even slower convergence.

Harris (1998) showed that fictitious play converges at a rate of \(O(T^{-1})\) in \(2 2\) zero-sum games. Abernethy et al. (2021) considered diagonal payoff matrices with non-adversarial tie-breaking rules, showing convergence rates of \(O(T^{-1/2})\). Abernethy et al. (2021) does not give a rate of convergence in our setting because requiring the payoff matrix to be diagonal would correspond to Alice only being allowed to cut at \(0\) or \(1\). This assumption is not as natural in our setting. In fact, if Alice can only cut at \(0\) or \(1\) the game becomes zero-sum. Furthermore, we allow arbitrary tie-breaking rules.

## 3 Preliminaries

In this section we formally define the notation used in our proofs. All our notation applies to both the sequential and simultaneous settings, unless otherwise stated.

**History.** Recall \(T\) is the number of rounds. For each round \(t[T]\),

* let \(a_{t}\) be Alice's cut at time \(t\) and \(b_{t}\{L,R\}\) be Bob's choice at time \(t\), where \(L\) stands for the left piece \([0,a_{t}]\) and \(R\) for the right piece \([a_{t},1]\).
* let \(A_{t}=(a_{1},,a_{t})\) be the history of cuts until the end of round \(t\) and \(B_{t}=(b_{1},,b_{t})\) the history of choices made by Bob until the end of round \(t\).

A history \(H=(A_{T},B_{T})\) denotes an entire trajectory of play.

**Strategies.** Let \(\) be the space of integrable value densities over \(\). A pure strategy for Alice at time \(t\) is a function \(S_{A}^{t}:^{t-1}\{L,R\}^{t-1} \), such that \(S_{A}^{t}(A_{t-1},B_{t-1},v_{A},T)\) is the next cut point made by Alice as a function of the history \(A_{t-1}\) of Alice's cuts, the history \(B_{t-1}\) of Bob's choices, Alice's valuation \(v_{A}\), and the horizon \(T\).

For Bob, we define pure strategies separately for the sequential and simultaneous settings due to the different feedback that he gets:

_Sequential:_ A pure Bob strategy at time \(t\) is a map \(S_{B}^{t}:^{t}\{L,R\}^{t-1} \{L,R\}\). That is, Bob observes Alice's cut point and then responds.

_Simultaneous:_ A pure Bob strategy at time \(t\) is a map \(S_{B}^{t}:^{t-1}\{L,R\}^{t-1} \{L,R\}\). Thus here Bob chooses \(L\!/R\) before observing Alice's cut point at time \(t\).

A pure strategy for Alice over the entire time horizon \(T\) is denoted \(S_{A}=(S_{A}^{1},,S_{A}^{T})\) and tells Alice what cut to make at each time \(t\). A pure strategy for Bob over the entire time horizon \(T\) is denoted \(S_{B}=(S_{B}^{1},,S_{B}^{T})\) and tells Bob whether to play \(L\!/R\) at each time \(t\).

A mixed strategy is a probability distribution over the set of pure strategies. 2

**Rewards and utilities.** Suppose Alice has mixed strategy \(S_{A}\) and Bob has mixed strategy \(S_{B}\). Let \(u_{A}^{t}\) and \(u_{B}^{t}\) be the random variables for the utility (payoff) experienced by Alice and Bob, respectively, at round \(t\). The utility of player \(i\{A,B\}\) is denoted \(u_{i}=u_{i}(S_{A},S_{B})=_{t=1}^{T}u_{i}^{t}\). The utility of player \(i\) from round \(t_{1}\) to \(t_{2}\) is \(u_{i}(t_{1},t_{2})=_{t=t_{1}}^{t_{2}}u_{i}^{t}\).

The expected utility of player \(i\) is \([u_{i}]=_{t=1}^{T}[u_{i}^{t}],\) where the expectation is taken over the randomness of the strategies \(S_{A}\) and \(S_{B}\).

Given a history \(H\), let \(u_{i}^{t}(H)\) be player \(i\)'s utility in round \(t\) under \(H\) and let \(u_{i}(H)=_{t=1}^{T}u_{i}^{t}(H)\) be player \(i\)'s cumulative utility under \(H\).

**Midpoints and Stackelberg value.** Let \(m_{A}\) be Alice's midpoint of the cake, with \(V_{A}([0,m_{A}])=1/2\), and \(m_{B}\) be Bob's midpoint, with \(V_{B}([0,m_{B}])=1/2\). Since the densities are bounded from below, the midpoint of each player is uniquely defined. Alice's _Stackelberg value_, denoted \(u^{*}_{A}\), is the utility Alice gets when she cuts at \(m_{B}\) and Bob chooses his favorite piece breaking ties in favor of Alice (i.e. taking the piece she prefers less).

## 4 Alice exploiting Bob

In this section we give an overview of Theorem 1, which considers the sequential setting and quantifies the extent to which Alice can exploit a Bob that has sub-linear regret with respect to the benchmark of choosing the best piece in each round. Formal proofs for this section are in Appendix A.

We start by defining the notion of Stackelberg regret (Dong et al. (2018); Haghtalab et al. (2022)).

**Definition 1** (Stackelberg regret).: _Given a history \(H\) of the places Alice cut and the pieces Bob chose in each round, Alice's Stackelberg regret is \(_{A}(H)=_{t=1}^{T}u^{*}_{A}-u^{t}_{A}(H)\), where \(u^{*}_{A}\) is Alice's Stackelberg value and \(u^{t}_{A}(H)\) is Alice's utility in round \(t\) under history \(H\)._

For Bob, we consider the basic notion of static regret, where Bob compares his payoff to what would have happened if Alice's actions remained the same but he chose the best piece in each round.

**Definition 2** (Regret).: _Given a history \(H\), Bob's regret is_

\[_{B}(H)=_{t=1}^{T}V_{B}([0,a_{t}]),V_{B}([a _{t},1])}-u^{t}_{B}(H),\]

_recalling that \(u^{t}_{B}(H)\) is Bob's utility in round \(t\) under history \(H\)._

Next we provide a proof sketch for Theorem 1, which is divided in the next two propositions, corresponding to the cases where Alice knows \(\) and does not know \(\).

**Proposition 2**.: _Let \([0,1)\). Suppose Bob plays a strategy that ensures his regret is \(O(T^{})\) and let \(^{}\) denote the set of all such Bob strategies. Assume Alice knows \(\). Then she has a strategy \(S_{A}=S_{A}()\) that ensures her Stackelberg regret is \(OT^{} T\). The exponent is sharp: Alice's Stackelberg regret is \(T^{}\) for some Bob strategy in \(^{}\)._

Proof sketch.: We sketch both the upper and lower bounds.

**Sketch for the upper bound.** Let \(S_{B}\) denote Bob's strategy, which guarantees his regret is \(O(T^{})\). Suppose Alice knows \(\). Then Alice initializes an interval \(I=\) and uses the following strategy.

Iteratively, for \(i=0,1,\);

**(1)**: Alice discretizes the interval \(I=[u,w]\) in a constant number of sub-intervals (set to \(6\)) of equal value to her, by cutting at points \(a_{i,j}\) for \(j\) such that \(u<a_{i,1}<a_{i,2}<<a_{i,5}<w\). Denote \(a_{i,0}=u\) and \(a_{i,6}=w\). An illustration is in Figure 3.

**(2)**: Alice selects a number \(\), which will be set "large enough" as a function of \(T\) and \(\). In the next \(5\) rounds, Alice cuts an equal number of times at each point \(a_{i,j}\) for \(j\). That is:

* In each of the next \(\) rounds, Alice cuts at \(a_{i,1}\) and observes Bob's choices there, computing the majority answer as \(c_{i,1}=L\) if Bob picked the left piece more times than the right piece, and \(c_{i,1}=R\) otherwise. The next \(\) rounds after that Alice switches to cutting at \(a_{i,2}\), and so on.

In this fashion, Alice computes \(c_{i,j}\) as Bob's majority answer corresponding to cut point \(a_{i,j}\) for all \(j\). Also, by default \(c_{i,0}=R\) and \(c_{i,6}=L\). An illustration is in Figure 4.

Figure 3: Illustration of step \((1)\) for \(i=0\). Alice divides the interval \(\) in \(6\) disjoint intervals of equal value to her, demarcated by points \(a_{0,0}=0<a_{0,1}<a_{0,2}<a_{0,3}<a_{0,4}<a_{0,5}<1=a_{0,6}\).

**(3)**: The points \(a_{i,j}\) for \(j\{0,,6\}\) are arranged on a line and each is labelled \(L\) or \(R\), with the leftmost point \(a_{i,0}=0\) labelled \(R\) and the rightmost point \(a_{i,6}=1\) labelled \(L\). Then there is an index \(j\{0,,5\}\) such that \(c_{i,j}=R\) and \(c_{i,j+1}=L\).

Alice computes a smaller interval \(I_{i+1}\), essentially consisting of \([a_{i,j},a_{i,j+1}]\) and some extra space around it to make sure that \(I_{i+1}\) contains Bob's midpoint as follows. If \(j\{1,,4\}\), set \(I_{i+1}=[a_{i,j-1},a_{i,j+2}]\). If \(j=0\), set \(I_{i+1}=[a_{i,0},a_{i,3}]\). If \(j=5\), set \(I_{i+1}=[a_{i,3},a_{i,6}]\). Then Alice iterates steps \((1-3)\) on the interval \(I_{1}\). An illustration is in Figure 5.

The full proof explains why the index \(j\) from step \(3\) is unique and why it is in fact necessary to include a slightly larger interval than \([a_{i,j},a_{i,j+1}]\) in the recursion step, due to Bob potentially having lied if his midpoint was very close to a boundary of \([a_{i,j},a_{i,j+1}]\) but on the other side.

Sketch for the lower bound.The lower bound of \(T^{}\) relies on the observation that rounds where Alice cuts near \(m_{B}\) and Bob picks his less-preferred piece cost Bob very little but cost Alice a lot. More precisely, suppose \(m_{A}<m_{B}\) and Alice cuts at \(m_{B}-\). Then compared to his regret bound, Bob loses \(()\) if he picks the wrong piece. On the other hand, Alice loses \((m_{B}-m_{A})=(1)\) compared to her Stackelberg value.

Bob can use this asymmetry by acting as if his midpoint were \(T^{}\) closer to \(m_{A}\) than it really is. Living \(T^{}\) times costs Bob only \((T^{})\) regret, but costs Alice \(T^{}\) regret. To avoid accumulating more regret than this, Bob can afterwards revert to picking his truly preferred piece; the damage to Alice's payoff has already been done. 

**Proposition 3**.: _Let \([0,1)\). Suppose Bob plays a strategy that ensures his regret is \(O(T^{})\). Let \(^{}\) denote the set of all such Bob strategies. If Alice does not know \(\), she has a strategy \(S_{A}\) that ensures her Stackelberg regret is \(O\)._

_The exponent is sharp: if \(S_{A}\) guarantees Alice Stackelberg regret \(O(T^{})\) against all Bob strategies in \(^{}\) for some \([0,1)\), then \(S_{A}\) has Stackelberg regret \((T)\) for some Bob strategy in \(^{3}\)._

Proof sketch.: Alice's strategy that achieves \(O(T/ T)\) regret follows the same template as her strategy from Proposition 2. The only difference is that she sets \(\) differently (and much larger) to cover any possible regret bound Bob could have.

The idea of the lower bound is that, if Alice does not know the value of \(\) in Bob's regret bound, she cannot know when she has true information about Bob's preferences. We use this by having a Bob with \(O(T^{})\) regret behave exactly like one with \(O(T^{})\) regret but a different midpoint. Then Bob can hide his deception from an Alice with \(O(T^{})\) regret since he can tolerate more regret than her. 

Theorem 1 is implied by Propositions 2 and 3. The players' value densities must be bounded for Theorem 1 to hold; see Remark 1 in Appendix A.2 for a counterexample with unbounded densities.

Figure 4: Illustration of step \((2)\) for \(i=0\). Suppose \(=3\). Alice cuts \(3\) times at each of the points \(a_{0,j}\) and observes Bob’s choices, which are marked near each such cut point. By default, Alice knows what the answer would be if she cut at \(0\) or \(1\), so those are set to \(R\) and \(L\), respectively. The truthful answers (reflecting Bob’s favorite piece according to his actual valuation) are marked with green, while the lying answers are marked with orange.

Figure 5: Illustration of step \((3)\) for \(i=0\). Alice labels each point \(a_{0,j}\) with the majority answer there. Then she identifies the index \(j\) such that the point \(a_{0,j}\) is labelled \(R\) and the point \(a_{0,j+1}\) is labelled \(L\). At this stage she is assured that either the interval \([a_{0,j},a_{0,j+1}]\) or one of the adjacent ones contains Bob’s midpoint. Alice sets \(I_{1}=[a_{0,3},a_{0,6}]\) and recurses on it.

Equitable payoffs

Here we sketch the proofs of Theorems 2 and 3. The formal proofs can be found in Appendix B.

Theorem 2 shows how Alice can get at least \(1/2\) per round while keeping Bob at \(1/2\) per round.

Proof sketch of Theorem 2.: Alice's strategy uses Blackwell approachability (1956). A challenge is that Blackwell's original version required the number of player types to be finite, but Alice has to be prepared for an uncountably infinite variety of Bob's valuation functions. Another difference is that Alice's action space is also infinite, which turns out to be necessary.

We get around the infinite-Bob issue in two steps. First, Alice defines a countably infinite set \(}\) as a stand-in for the full variety of Bobs; \(}\) includes arbitrarily good approximations to any valuation.

Second, we replace Blackwell's original finite-dimensional space with a countably-infinite-dimensional one, where the elements of \(}\) are the axes. We define an inner product on this space and adapt Blackwell's argument for it. Briefly, Alice's strategy tracks the average payoff to each type of Bob in \(}\) and defines \(\) to be the region of the space where all of them have payoffs at most \(1/2\). In each round, she constructs a cut point which moves the Bobs' average payoff closer to \(\), and in the limit traps them in \(\).

Under this strategy, Alice's payoff guarantee is mostly a byproduct of Bob's. If Bob and Alice have the same value density, then their payoffs sum to \(1\), so bounding Bob's payoff to \(1/2\) also bounds Alice's to \(1/2\). We achieve the substantially better bound on Alice's payoff by explicitly including her value density \(v_{A}\) in the set \(}\) of Bobs, thus eliminating any approximation error. 

Theorem 3 shows how Bob can do the same, albeit only in expectation in the simultaneous setting.

Proof sketch of Theorem 3.: We cover the simultaneous setting first because it informs the sequential setting. In the simultaneous setting, Bob's algorithm is extremely simple: in each round, randomly select \(L\) or \(R\) with equal probability. The expected payoffs to each player follow immediately.

Bob's strategy for the sequential setting can be seen as a derandomized version of the simultaneous strategy. The simplest way to derandomize it would be to strictly alternate between \(L\) and \(R\), but if Bob runs that strategy Alice can easily exploit it. Instead, Bob mentally partitions the cake into \(\) intervals \(I_{1},,I_{}\) of equal value to him. He then treats each interval \(I_{i}\) as a separate cake, alternating between \(L\) and \(R\) for the rounds Alice cuts in \(I_{i}\). Alice can still exploit this strategy on a single interval \(I_{i}\), but doing so can only give her an average payoff of \(1/2+O(V_{A}(I_{i})) 1/2+O(1/)\). The full proof shows this bound applies for any Alice strategy. 

## 6 Fictitious play

In this section we include a proof sketch of Theorem 4, which analyzes the fictitious play dynamic. The formal proof can be found in Appendix C.

Proof sketch of Theorem 4.: To analyze the fictitious play dynamic, we define for each \(t=0,,T\) two quantities called \(_{t}\) and \(_{t}\). Let \(_{t}=r_{t}-_{t}\), where \(r_{t}\) is the number of times Bob picked \(R\) up to round \(t\) and \(_{t}\) is the number of times he picked \(L\). Let \(_{t}=_{=1}^{t}(2V_{B}([0,a_{}])-1)\).

The quantities \(_{t}\) and \(_{t}\) control what happens under fictitious play: Alice's decision in round \(t+1\) is based on \(_{t}\) and Bob's decision in round \(t+1\) is based on \(_{t}\). These decisions in turn affect \(_{t+1}\) and \(_{t+1}\), forming a dynamical system that results in a counterclockwise spiral through \(\)-\(\) space. Figure 6 illustrates the sequences \(_{t}\) and \(_{t}\) for the instance in Figure 2.

We define \(_{t}=|_{t}|+|_{t}|\) and formalize this spiral, by showing that the sequence \(\{\}_{t=0}^{T}\) is non-decreasing and analyzing the change in \((_{t},_{t})\) from round to round. Figure 6 illustrates the parameter \(_{t}\) over time, while Figure 7 illustrates the spiral (associated with the same trajectory as in Figure 2 and 6), where the spiral is visualized as a scatter plot of the sequence \((_{t},_{t})_{t 1}\).

We first use these dynamics to bound Bob's payoff. Bob's payoff can be almost directly read off due to changes in \(_{t}\) closely matching changes in Bob's payoff. Bob's total payoff to round \(t\) turns out to be of the order \(t/2_{t}\), so bounding the rate at which the spiral expands also bounds Bob's payoff.

We then use the dynamics to bound the total payoff to Alice and Bob. Alice can only cut in the interior of the cake when \(_{t}=0\), which happens less and less often as the spiral expands. The players' total payoff when Alice cuts at one end of the cake is \(1\), so across \(T\) rounds we show the sum of cumulative payoffs of the players is of the order \(T()\). Combining the bound on the total payoff with the bound on Bob's payoff gives a bound for Alice's payoff. 

## 7 Concluding remarks

There are several directions for future work. One direction is to consider a wider class of regret benchmarks and understand how the choice of benchmark influences the outcomes reached. Moreover, what payoff profiles are attained when the players use randomized algorithms such as exponential weights to update their strategies? It would also make sense to consider settings where the cake has both good and bad parts. Finally, studying richer feedback models, _e.g.,_ when Alice and Bob takes turns cutting and choosing, or allowing Alice to divide the cake into any multiple measurable sets would be intriguing directions.

Figure 6: Illustration of the sequences \(\{_{t}\}_{t=1}^{}\), \(\{_{t}\}_{t=1}^{}\), and \(\{_{t}\}_{t=1}^{}\) for the instance with trajectories shown in Figure 2. The X axis shows the round number \(t\) and the Y axis the variable plotted.

Figure 7: Scatter plot of the sequence \((_{t},_{t})_{t 1}\), illustrating the spiral for the instance with trajectories shown in Figure 2, where the sequences \(_{t}\) and \(_{t}\) are illustrated separately in Figure 6.