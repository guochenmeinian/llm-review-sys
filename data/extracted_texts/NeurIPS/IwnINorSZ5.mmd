# Conformal Meta-learners for Predictive Inference of Individual Treatment Effects

Ahmed M. Alaa

UC Berkeley and UCSF

amalaa@berkeley.edu

&Zaid Ahmad

UC Berkeley

zaidahmad@berkeley.edu

&Mark van der Laan

UC Berkeley

laan@stat.berkeley.edu

###### Abstract

We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based "meta-learners" that can provide point estimates of the conditional average treatment effect (CATE)--these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop _conformal meta-learners_, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a _stochastic ordering_ framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo-outcome) conformity scores stochastically dominate "oracle" conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta-learners, such as the _doubly-robust_ learner, satisfy a model- and distribution-free stochastic (or convex) dominance condition, making their conformal inferences valid for practically-relevant levels of target coverage. Whereas existing procedures conduct inference on nuisance parameters (i.e., potential outcomes) via weighted CP , conformal meta-learners enable direct inference on the target parameter (ITE). Numerical experiments show that conformal meta-learners provide valid intervals with competitive efficiency while retaining the favorable point estimation properties of CATE meta-learners.

**Code:**https://github.com/AlaaLab/conformal-metalearners

## 1 Introduction

Identifying heterogeneity in the effects of interventions across individuals is a central problem in various fields, including medical, political, and social sciences . In recent years, there has been growing interest in developing machine learning (ML) models to estimate heterogeneous treatment effects using observational or experimental data . However, most of these models only provide _point_ estimates of the conditional average treatment effect (CATE), which is a deterministic function that describes the expected treatment effect based on a given individual's covariates. In this paper, we focus on quantifying uncertainty in these estimates, which arises from both errors in the model and the variation of individual treatment effects (ITEs) for individuals with the same covariates. We adopt a _predictive inference_ approach to this problem, with the goal of devising valid procedures to issue predictive intervals that cover ITEs on unseen data with a predetermined probability.

Traditionally, predictive inference on ITEs has been conducted through Bayesian methods such as BART  and Gaussian processes . These methods can provide interval-valued predictions of ITEs through their induced posterior distributions (e.g., posterior credible intervals). However, Bayesian methods tend to be model-specific and cannot be straightforwardly generalized to modern ML models, e.g., transformer-based architectures used to model visual and textual covariate spaces . More importantly, Bayesian methods generally do not provide guarantees on the frequentist coverage oftheir credible intervals--achieved (finite-sample) coverage depends on the prior . This paper is motivated by the advent of _conformal prediction_ (CP), a frequentist alternative that can be used to conduct model-agnostic, distribution-free valid predictive inference on top of any ML model [12; 13; 14]. Throughout this paper, we will study the validity of CP-based procedures for inference of ITEs.

_What makes CP-based inference of ITEs different from its application to the standard regression (supervised) setup?_ The "fundamental problem of causal inference" is that we never observe counterfactual outcomes . That is, our "label" is the ITE which is a difference between two potential outcomes (treated and untreated) for an individual subject--this label is never observed for any given subject because we only ever observe factual outcomes. This poses two challenges :

**(1) How to handle _covariate shift_?** When treatments are assigned to individuals with probabilities that depend on their covariates, then the distributions of covariates in treated and untreated groups differ. Consequently, the distribution of training data differs from that of the target population.

**(2) How to incorporate _inductive biases_?** Unlike supervised learning wherein we fit a single function using examples of covariates and _observed_ targets, models of treatment effects cannot be directly fit to the _unobserved_ effects. Thus, estimates of treatment effects comprise intermediate estimates of nuisance parameters. Different approaches for combining nuisance estimates entail different inductive priors on the potential outcomes that affect the accuracy of the resulting ITE estimates.

The literature on ML-based CATE estimation focuses on addressing the two questions above. **Covariate shift** affects the generalization performance of ML models--existing CATE estimation models address this problem using importance weighting  or balanced representation learning methods for unsupervised domain adaptation [6; 18; 19]. In , the notion of "meta-learners" was coined to describe various model-agnostic approaches to incorporating **inductive biases** and combining nuisance estimates. In [5; 20], it was shown that the choice of the meta-learner influences the CATE estimation rates. While the impact of **(1)** and **(2)** on the generalization performance of CATE estimators has been extensively investigated, their impact on the validity and efficiency of predictive inference methods for ITE is less well-understood. This forms the central focus of our paper.

**Contributions.** We propose a CP procedure for predictive inference of ITEs that jointly addresses **(1)** and **(2)** in an end-to-end fashion. Our proposed inference strategy applies the standard CP procedure on top of a broad class of CATE meta-learners based on two-stage _pseudo-outcome_ regression. These meta-learners operate by first estimating pseudo-outcomes, i.e., transformed targets that depend on observable variables only, and then regressing the pseudo-outcomes on covariates to obtain point estimates of CATE. We then construct intervals for ITEs by computing the empirical quantile of conformity scores evaluated on pseudo-outcomes in a held-out calibration set. Conformal meta-learners address **(1)** because the distribution of covariates associated with pseudo-outcomes is the same for training and testing data, and they address **(2)** since the calibration step is decoupled from model architecture, enabling flexible choice of inductive priors and the possibility of re-purposing existing meta-learners and architectures that have been shown to provide accurate estimates of CATE.

Conformal meta-learners inherit the guarantees of CP, i.e., their resulting intervals cover pseudo-outcomes on test data with high probability. However, the original CP guarantees do not immediately translate to guarantees on coverage of ITEs. To this end, we develop a unified _stochastic ordering_ framework to study the validity of conformal meta-learners for inference on ITEs. We show that inference with conformal meta-learners is valid if their conformity scores satisfy certain stochastic ordering conditions with respect to "oracle" conformity scores evaluated on unobserved ITEs. We prove that some of the commonly used meta-learners, such as the _doubly-robust_ learner , satisfy a weaker stochastic (or convex) dominance condition which makes them valid for relevant levels of target coverage. Our numerical experiments show that, with careful choice of the pseudo-outcome transformation, conformal meta-learners inherit both the coverage properties of CP as well as the efficiency and point estimation accuracy of their underlying CATE meta-learners.

## 2 Predictive Inference of Individual Treatment Effects (ITEs)

### Problem setup

We consider the standard potential outcomes (PO) framework with a binary treatment ([21; 22]). Let \(W\{0,1\}\) be the treatment indicator, \(X\) be the covariates, and \(Y\) be the outcome of interest. For each subject \(i\), let \((Y_{i}(0),Y_{i}(1))\) be the pair of potential outcomes under \(W=0\) and \(W=1\)

[MISSING_PAGE_FAIL:3]

### Oracle conformal prediction of ITEs

How can we adapt the CP framework for predictive inference of ITEs? In a hypothetical world where we have access to counterfactual outcomes, we can apply the standard CP in Section 2.2 to a dataset of covariates and ITE tuples, \(^{*}=\{(X_{i},Y_{i}(1)-Y_{i}(0))\}_{i}\), and compute conformity scores as:

\[V_{k}^{*}() V(X_{k},\,Y_{k}(1)-\,Y_{k}(0)\,;),\, k_{c}^{*},\] (7)

where \(\) is an ML model fit to estimate the CATE function \((x)\) using \(_{t}^{*}\), and \(^{*}=_{t}^{*}_{c}^{*}\). We will refer to this procedure as "oracle" conformal prediction and to \(V_{k}^{*}()\) as the oracle conformity scores. Since the oracle problem is a standard regression, the oracle procedure is marginally valid--i.e., it satisfies the guarantee in (3), \((Y(1)-Y(0)^{*}(X)) 1-\). However, oracle CP is infeasible since we can only observe one of the potential outcomes (colored in red and blue in (7)), hence we need an alternative procedure that operates only on the observed variable \(Z=(X,W,Y)\).

### The two challenges of predictive inference on ITEs

A naive approach to inference of ITEs is to split the observed sample \(\{Z_{i}=(X_{i},W_{i},Y_{i})\}_{i}\) by the treatment group and create two datasets: \(_{0}=\{(X_{i},Y_{i}):\,W_{i}=0\}_{i}\), \(_{1}=\{(X_{i},Y_{i}):\,W_{i}=1\}_{i}\), then generate two sets of conformity scores for the nuisance estimates \(_{0}\) and \(_{1}\) as follows:

\[V_{k}^{(0)}(_{0}) V(X_{k},Y_{k}(0);_{0}), \, k_{c,0}, V_{k}^{(1)}(_{1})  V(X_{k},Y_{k}(1);_{1}),\, k_{c, 1},\] (8)

where \(_{c,0}\) and \(_{c,1}\) are calibration subsets of \(_{0}\) and \(_{1}\). In order to construct valid predictive intervals for ITE using the conformity scores in (8), we need to reconsider how the two distinct characteristics of CATE estimation, previously discussed in Section 1, interact with the CP procedure:

_(1) Covariate shift_

_(2) Inductive biases_

The distributions of covariates for treated and untreated subjects differ from that of the target population: \(P_{X|W=0} P_{X|W=1} P_{X}\), i.e., the following holds for the conformity scores in (8):

\[P_{X,Y^{(0)}|W=0} P_{X,Y^{(0)}}|W=1} P_{X,Y^{ (1)}}}\]

Covariate shift breaks the exchangeability assumption necessary for the validity of CP. Current methods have primarily focused on **(1)** with \(Y(0)\) and \(Y(1)\) as inference targets, and developed approaches for handling covariate shift by reweighting conformity scores [1; 26]. The resulting intervals for POs are then combined to produce intervals for ITEs. However, these method tie the CP procedure to model architecture, requiring inference on nuisance parameters, and hence lose the desirable post-hoc nature of CP. Furthermore, inference on POs is likely to provide conservative ITE intervals, and limits the inductive priors that can be assumed since not all CATE models provide explicit PO estimates.

## 3 Conformal Meta-learners

In , a taxonomy of "meta-learners" was introduced to categorize different inductive priors that can be incorporated into CATE estimators by structuring the regression models for \(_{0}\) and \(_{1}\). For example, the _T-learner_ estimates \(_{0}\) and \(_{1}\) independently using \(_{0}\) and \(_{1}\), while the _S-learner_ models the treatment variable \(W\) as an additional covariate in a joint regression model \((X,W)\) and estimates CATE as \((x)=(x,1)-(x,0)\). In this Section, we propose an end-to-end solution to **(1)** and **(2)** by applying CP on top of CATE meta-learners in a post-hoc fashion, thereby decoupling the CP procedure from the CATE model and allowing direct inference on ITEs. In the next Section, we develop a unified framework for analyzing the validity of this broad class of procedures.

### Pseudo-outcome regression for CATE estimation

We focus on a broad subclass of CATE meta-learners based on two-stage _pseudo-outcome_ regression. These models replace the (unobserved) oracle ITEs with "proximal" targets that are estimated from observed variables only, and then train an ML model to predict the estimated targets from covariates. The two stages of this general pseudo-outcome regression framework can be described as follows:

**Stage 1.** We obtain a plug-in estimate \(\) of the nuisance parameters \(=(,_{0},_{1})\). Note that since we assume that the propensity score is known, we only need to estimate \(_{0}\) and \(_{1}\) using \(_{0}\) and \(_{1}\).

**Stage 2.** We use the nuisance estimates obtained in Stage 1 to create pseudo-outcomes \(_{}\) that depend only on \(\) and the observable variables \(Z=(X,W,Y)\), i.e., \(_{}=f(Z,)\) for some function \(f\). The CATE estimate is then obtained by regressing the pseudo-outcome \(_{}\) on the covariate \(X\). This is typically conducted using a different dataset than the one used to obtain the nuisance estimate \(\).

The general framework described above captures various models in previous literature. We study 3 examples of such meta-learners: X-learner, Inverse propensity weighted (IPW) learner and doubly-robust (DR) learner.

Table 1 lists the pseudo-outcomes \(_{}\) for the three meta-learners: IPW-learner reweights factual outcomes using propensity scores to match CATE, i.e., \([_{}\,|\,X=x]=(x)\); X-learner uses regression adjustment to impute counterfactuals; DR-learner combines both approaches. DR- and X-learners1, coupled with specific architectures for joint modeling of \(_{0}\) and \(_{1}\), have shown competitive performance for CATE estimation in previous studies [5; 16; 20]. The conformal meta-learner framework decouples the CP procedure (Section 3.2) from the inductive priors encoded by these meta-learners, hence it inherits their favorable CATE estimation properties and enables a potentially more efficient direct inference on ITEs as opposed to inference on POs. This addresses **challenge (2)** in Section 2.4.

### Conformal pseudo-intervals for ITEs

Pseudo-outcome regression is based on the notion that accurate proxies for treatment effects can produce reliable CATE point estimates. This concept can be extended to predictive inference: using CP to calibrate meta-learners via held-out pseudo-outcomes can yield accurate "pseudo-intervals" for ITEs.

Given a dataset \(=\{Z_{i}=(X_{i},W_{i},Y_{i})\}_{i}\), we create three mutually-exclusive subsets: \(_{}\), \(_{t}\) and \(_{c}\). \(_{}\) is used to estimate the nuisance parameters \(\). Next, the estimates \(=(,_{0},_{1})\) are used to transform \(\{Z_{i}=(X_{i},W_{i},Y_{i}):i_{t}\}\) into covariate/pseudo-outcome pairs \(\{(X_{i},_{,i}):i_{t}\}\) which are used to train a CATE model \(\). Finally, we compute conformity scores for \(\) on pseudo-outcomes, i.e.,

\[V_{,k}() V(X_{k},_{,k}; ),\; k_{c}.\] (9)

For a target coverage of \(1-\), we construct a predictive interval at a new point \(X_{n+1}=x\) as follows:

\[_{}(x)=[\,(x)-Q_{_{}}(1- ),\,(x)+Q_{_{}}(1-)\,],\] (10)

where \(_{}=\{V_{,k}():k_{c}\}\). We call \(_{}(x)\) a _pseudo-interval_. The conformal meta-learner approach is depicted in Figure 1 and a summary of the procedure is given in Algorithm 1.

   \\  _IPW-learner_ & \(_{}=Y\) \\ _X-learner_ & \(_{}=W(Y-_{0}(X))+(1-W)(_{1}(X)-Y)\) \\ _DR-learner_ & \(_{}=(Y-_{W}(X ))+_{1}(X)-_{0}(X)\) \\  

Table 1: Existing meta-learners as instantiations of pseudo-outcome regression.

Note that conditional on \(\), the pseudo-outcomes \((X,_{})\) in calibration data are drawn from the target distribution, which maintains the exchangeability of conformity scores and addresses covariate shift (**challenge (1)** in Section 2.4). However, the conformity scores \(V\) are evaluated on transformed outcomes, which means that \(V_{}\) and \(V^{*}\) are not exchangeable, even though they are drawn from the same covariate distribution. Consequently, the usual CP guarantees, i.e., \((_{}_{}(X)) 1-\), do not immediately translate to coverage guarantees for the true ITE \(Y(1)-Y(0)\). In the next section, we show that for certain choices of the pseudo-outcomes, the corresponding pseudo-intervals can provide valid inferences for ITE without requiring the exchangeability of \(V_{}\) and \(V^{*}\).

## 4 Validity of Conformal Meta-learners: A Stochastic Ordering Framework

Under what conditions are pseudo-intervals valid for inference of ITEs? Recall that these intervals are constructed by evaluating the empirical quantile of pseudo-outcome conformity scores. Intuitively, the pseudo-intervals will cover the true ITE if the conformity scores are "stochastically larger" than the oracle scores in Section 2.3, i.e., \(Q_{_{}}() Q_{^{*}}()\) in some stochastic sense (Figure 1(b)). Hence, to study the validity of conformal meta-learners, we analyze the _stochastic orders_ of \(V_{}\) and \(V^{*}\), and identify conditions under which pseudo-intervals cover oracle intervals.

Stochastic orders are partial orders of random variables used to compare their location, magnitude, or variability [28; 29]. In our analysis, we utilize two key notions of stochastic order among cumulative distribution functions (CDFs) \(F\) and \(G\), which we formally define below.

**Definition 1** (Stochastic dominance): \(F\) _has first-order stochastic dominance (FOSD) on \(G\), \(F_{(1)}G\), iff \(F(x) G(x), x\), with strict inequality for some \(x\). \(F\) has second-order stochastic dominance (SOSD) over \(G\), \(F_{}G\), iff \(_{-}^{x}[G(t)-F(t)]dt 0\), \( x\), with strict inequality for some \(x\)._

**Definition 2** (Convex dominance): \(F\) _has monotone convex dominance (MCX) over \(G\), \(F_{mcx}G\), iff \(_{X F}[u(X)]_{X G}[u(X)]\) for all non-decreasing convex functions \(u:\)._

Stochastic ordering is useful tool in decision theory and quantitative finance used to analyze the decisions of utility maximizers with varying risk attitudes . A distribution \(F\) has FOSD over \(G\) if it is favored by any decision-maker with a non-decreasing utility function, i.e., \(F\) is more likely to give higher outcomes than \(G\) because its CDF is strictly lower (Figure 2(a)). If \(F\) has SOSD over \(G\), then it is favored by risk-averse decision-makers, i.e., \(f\) has smaller spread than \(g\) and is favored by all decision-makers with a non-decreasing _concave_ utility function . In this case, the CDFs can cross but \(G\) is always lower after the last crossing point (Figure 2(b)). \(F\) has MCX over \(G\) if it is favored by decision-makers with a non-decreasing _convex_ utility--in this case, the CDFs can cross but \(F\) is always lower after the last crossing point (See Appendix A for a detailed analysis).

In the following Theorem, we provide sufficient conditions for the validity of conformal meta-learners in terms of the stochastic orders of their conformity scores.

**Theorem 1**.: _If \((X_{i},W_{i},Y_{i}(0),Y_{i}(1)),\,i=1,,n+1\) are exchangeable, then \(\,^{*}(0,1)\) such that the pseudo-interval \(_{}(X_{n+1})\) constructed using the dataset \(=\{(X_{i},W_{i},Y_{i})\}_{i=1}^{n}\) satisfies_

\[(Y_{n+1}(1)-Y_{n+1}(0)_{}(X_{n+1})) 1- ,\,(0,^{*}),\]

_if at least one of the following stochastic ordering conditions hold: (i) \(V_{}_{(i)}V^{*}\), (ii) \(V_{}_{(2)}V^{*}\), and (iii) \(V_{}_{mcx}V^{*}\). Under condition (i), we have \(^{*}=1\)._

All proofs are provided in Appendix A. Theorem 1 states that if the conformity score \(V_{}\) of the meta-learner is stochastically larger (FOSD) or has a larger spread (SOSD and MCX) than the oracle conformity score, then the conformal meta-learner is valid for high-probability coverage (Figure 3). (This is the range of target coverage that is of practical relevance, i.e., \(\) is typically set to 0.05 or 0.1.)

Figure 2: Graphical illustration of stochastic dominance among two exemplary distributions \(F\) and \(G\).

Because stochastic (or convex) dominance pertain to more variable conformity scores, the predictive intervals of conformal meta-learners will naturally be more conservative than the oracle intervals.

Whether a meta-learner meets conditions _(i)-(iii)_ of Theorem 1 depends on how the pseudo-outcome, \(_{}=f(Z;)\), is constructed. The following Theorem provides an answer to the question of which of the meta-learners listed in Table 1 satisfy the stochastic ordering conditions in Theorem 1.

**Theorem 2**.: _Let \(V_{}()=|(X)-_{}|\) and assume that the propensity score function \(:\) is known. Then, the following holds: (i) For the \(X\)-learner, \(V_{}\) and \(V^{*}\) do not admit to a model- and distribution-free stochastic order, (ii) For any distribution \(P(X,W,Y(0),Y(1))\), CATE estimate \(\), and nuisance estimate \(\), the IPW- and the DR-learners satisfy \(V_{}_{mcx}V^{*}\)._

Theorem 2 states that the stochastic ordering of \(V_{}\) and \(V^{*}\) depends on the specific choice of the conformity score function \(V(X,_{};)\) as well as the choice of the meta-learner, i.e., the pseudo-outcome generation function \(_{}=f(Z;)\). The IPW- and DR-learners ensure that, by construction, the pseudo-outcome is equal to CATE in expectation: \([_{} X=x]=(x)\). This construction enables the IPW- and DR-learners to provide unbiased estimates of average treatment effects (ATE) independent of the data distribution and the ML model used for the nuisance estimates \(_{0}\) and \(_{1}\). By the same logic, IPW- and DR-learners also guarantee stochastic (convex) dominance of their conformity scores irrespective of the data distribution and the ML model choice, hence preserving the model- and distribution-free nature of the CP coverage guarantees. Contrarily, the X-learner does not use the knowledge of \(\) to construct its pseudo-outcomes, hence it does not guarantee a (distribution-free) stochastic order and the achieved coverage depends on the nuisance estimates \(_{0}\) and \(_{1}\). In Table 2, we list the stochastic orders achieved for different choices of meta-learners and conformity scores. (The analysis of stochastic orders for the signed distance score used in  and  is provided in Appendix A.)

**Key limitations of conformal meta-learners.** While conformalized meta-learners can enable valid end-to-end predictive inference of ITEs, they have two key limitations. First, the propensity score \(\) must be known to guarantee model- and distribution-free stochastic ordering of conformity scores. However, we note that this limitation in not unique to our method and is also encountered in methods based on weighted CP [1; 26]. The second limitation is peculiar to our method: exact characterization of \(^{*}\) is difficult and depends on the data distribution. Devising procedures for inferring \(^{*}\) based on observable variables or deriving theoretical upper bounds on \(^{*}\) are interesting directions for future work. Here, we focus on empirical evaluation of \(^{*}\) in semi-synthetic experiments. A detailed comparison between our method and previous work is provided in Appendix B.

## 5 Experiments

### Experimental setup

Since the true ITEs are never observed in real-world datasets, we follow the common practice of conducting numerical experiments using synthetic and semi-synthetic datasets [1; 8; 19]. We present a number of representative experiments in this Section and defer further results to Appendix C.

**Synthetic datasets.** We consider a variant of the data-generation process in Section 3.6 in  which was originally proposed in . We create synthetic datasets by sampling covariates \(X U(^{d})\) and treatments \(W|X=x((x))\) with \((x)=(1+I_{x}(2,4))/4\), where \(I_{x}(2,4)\) is the regularized incomplete beta function (i.e., CDF of a Beta distribution with shape parameters 2 and 4). Outcomes are modeled as \(_{1}(x)=(x_{1})(x_{2})\) and \(_{0}(x)=\,(x_{1})(x_{2})\), where \(\) is a parameter that controls the treatment effect, and \(\) is a function given by \((x)=1/(1+(-12(x-0.5)))\)

 
**Meta-learner** &  \\  & _Absolute residual_ & _Signed distance_ \\  _X-learner_ & No stochastic order & No stochastic order \\ _IPW-learner_ & \(V_{}_{mcx}V^{*}\) & \(V_{}_{(2)}V^{*}\) \\ _DR-learner_ & \(V_{}_{mcx}V^{*}\) & \(V_{}_{(2)}V^{*}\) \\  

Table 2: Stochastic orders of conformity scores for the three meta-learners considered in Table 1.

Figure 3: Validity condition in Theorem 1.

We assume that POs are sampled from \(Y(w)|X=x(_{w}(x),^{2}(x)),\,w\{0,1\}\) and consider a heteroscedastic noise model \(^{2}(x)=-(x_{1})\). We define two setups within this model: **Setup A** where the treatment has not effect (\(=1\)), and **Setup B** where the effects are heterogeneous (\(=0\)).

**Semi-synthetic datasets.** We also consider two well-known semi-synthetic datasets that involve real covariates and simulated outcomes. The first is the National Study of Learning Mindsets (NLSM) , and the second is the IHDP benchmark originally developed in . Details on the data generation process for NLSM can be founded in Section 2 in . Details on the IHDP benchmark can be found in [6; 8; 16; 19]. Appendix C provides detailed description of both datasets for completeness.

**Baselines.** We consider baseline models that provide valid predictive intervals for ITEs. Specifically, we consider state-of-the-art methods based on weighted conformal prediction (WCP) proposed in . These methods apply weighted CP to construct intervals for the two POs or plug-in estimates of ITEs. We consider the three variants of WCP in : (1) _Naive WCP_ which combines the PO intervals using Bonferroni correction, (2) _Exact Nested WCP_ which applies WCP to plug-in estimates of ITEs in treatment and control groups followed by a secondary CP procedure, and (3) _Inexact Nested WCP_ which follows the same steps of the exact version but replaces the secondary CP with conditional quantile regression. (Note that Inexact Nested WCP does not provide coverage guarantees.) For all baselines, we use the same model (Gradient Boosting) for nuisance and pseudo-outcome modeling, and we use the conformal quantile regression method in  to construct predictive intervals.

### Results and discussion

Our experimental findings yield the following key takeaways: Firstly, the _IPW- and DR-learners demonstrate a robust FOSD (i.e., \(^{*}=1\)) in the majority of experiments, surpassing the MCX conditions outlined in Theorem 2_. Secondly, _the DR-learner exhibits superior point estimation accuracy and interval efficiency in most experiment_ compared to all other baselines that ensure valid inference. Thirdly, the _effectiveness of conformal meta-learners depends on the discrepancy between the CDFs of conformity scores and oracle scores_--pseudo-outcome transformations that induce thicker tails in the resulting conformtiy scores can cause conformal meta-learners to under-perform.

**Empirical assessment of stochastic orders.** Figure 4(a) depicts the empirical CDF of the conformity scores \(V_{}\) and oracle scores \(V^{*}\) for the three meta-learners under study (DR-, IPW- and X-learners). These CDFs are averaged over 100 runs of **Setups A** and **B** of the synthetic generation process outlined in Section 5.1. (The shaded regions represent the lowest and highest bounds on the empirical

Figure 4: Performance of all baseline in the synthetic setup described in Section 5.1. In (b), red vertical lines correspond to target coverage (\(1-=0.9\)), and blue vertical lines correspond to optimal interval width. In (c), baseline methods are color-coded as follows: \(\) CM-DR, \(\) CM-IPW, \(\) CM-X, \(\) WCP-Naïve, \(\) WCP-Exact, and \(\) WCP-Inexact. Here, WCP stands for weighted CP and CM stands for conformal meta-learners.

CDFs evaluated across all runs.) In both setups, the conformity scores for the DR- and IPW-learners demonstrate FOSD over the oracle scores with respect to the average CDFs and in almost all realizations. This aligns with the result of Theorem 2, and shows that the stochastic dominance condition achieved in practice is even stronger than our theoretical guarantee since FOSD (\(V_{}_{(i)}V^{*}\)) implies the weaker conditions of \(V_{}_{(2)}V^{*}\) and \(V_{}_{mcx}V^{*}\). On the contrary, the conformity scores of the X-learner are dominated by oracle scores in the FOSD sense. This is not surprising in light of Theorem 2, which indicates that X-learners do not guarantee a distribution-free stochastic order. These observations are also replicated in the semi-synthetic datasets as shown in Figure 5.

Based on Theorem 1, the empirical stochastic orders observed in Figures 4(a) and 5 predict that the IPW- and DR-learners will cover ITEs, whereas the X-learner will not achieve coverage. This is confirmed by the results in Figures 4(b), 4(c) and Table 3. The fact that the IPW- and DR-learners satisfy a stronger FOSD condition is promising because it indicates that the range of validity for these models spans all levels of coverage (\(^{*}=1\) in Figure 3). It also means that a stronger version of Theorem 2 outlining the conditions under which IPW- and DR-learners achieve FOSD could be possible.

**Coverage, efficiency and point estimation accuracy.** The performance of a predictive inference procedure can be characterized in terms of three metrics: achieved coverage for true ITEs, expected length of predictive intervals, and root-mean-square error (RMSE) in CATE estimates. In most experiments, we find that the DR-learner strikes a balance between these metrics (See Appendix C for further experiments). In Figure 4(b), we can see that the DR-learner outperforms the valid (naive and exact) WCP procedures in terms of RMSE and interval length, while achieving the target coverage of 90%. The X-learner outperforms all baselines in terms of RMSE, but as expected, it under-covers ITEs in all experiments. The inexact WCP baseline offers competitive efficiency and calibration, however, in addition to not offering coverage guarantees it also lacks consistency in RMSE performance under different inductive biases (i.e., no treatment effects in **Setup A** and heterogeneous effects in **Setup B**). These performance trends hold true across all levels of target coverage as shown in Figure 4(c).

The semi-synthetic experiments on IHDP and NLSM datasets shed light on when meta-learners may perform poorly. The DR-learner outperforms all baselines on the IHDP dataset in terms of RMSE, interval efficiency, while achieving the desired coverage of 90%. However, we observe that empirical performance depends on how closely the CDF of conformity scores matches the oracle CDF. The DR-learner performance deteriorates when conformity scores have "very strong" dominance over oracle scores, as observed in the NLSM dataset (Figure 5, bottom). Conversely, when the CDF of conformity scores is a closer lower bound on the oracle CDF; the DR-learner performance is competitive (Figure 4 and Figure 5, top). This is intuitive because if the pseudo-outcome transformation induces significant variability in regression targets, it will result in a lower CDF, poorer accuracy of pseudo-outcome regression, and longer predictive intervals. This is why the DR-learner consistently outperforms the IPW-learner, as it provides a closer approximation of the oracle CDF. Future work could focus on analyzing the gap between pseudo-outcome and oracle score CDFs and designing pseudo-outcome transformations that optimize efficiency while preserving stochastic orders.

## 6 Conclusions

Estimation and inference of treatment effects is challenging because causal effects are not directly observable. In this paper, we developed a general framework for inference of treatment effects, dubbed conformal meta-learners, that is compatible with any machine learning model. Our framework inherits the model- and distribution-free validity of conformal prediction as well as the estimation accuracy of model-agnostic meta-learners of treatment effects. Additionally, we introduce a new theoretical framework based on stochastic ordering to assess the validity of our method, which can guide the development of new models optimized for both accurate estimation and valid inference.