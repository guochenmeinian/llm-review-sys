# Fast Asymptotically Optimal Algorithms for

Non-Parametric Stochastic Bandits

 Dorian Baudry

Ecole Polytechnique, CREST

Palaiseau, France

dorian.baudry@ensae.fr

Fabien Pesquerel

Univ. Lille, Inria, CNRS, Centrale Lille,

UMR 9189-CRISatal, F-59000 Lille, France

fabien.pesquerel@inria.fr

Remy Degenne

Univ. Lille, Inria, CNRS, Centrale Lille,

UMR 9189-CRISatAL, F-59000 Lille, France

remy.degenne@inria.fr

Odalric-Ambrym Maillard

Univ. Lille, Inria, CNRS, Centrale Lille,

UMR 9189-CRISatAL, F-59000 Lille, France

odalric.maillard@inria.fr

###### Abstract

We consider the problem of regret minimization in non-parametric stochastic bandits. When the rewards are known to be bounded from above, there exists asymptotically optimal algorithms, with asymptotic regret depending on an infimum of Kullback-Leibler divergences (KL). These algorithms are computationally expensive and require storing all past rewards, thus simpler but non-optimal algorithms are often used instead. We introduce several methods to approximate the infimum KL which reduce drastically the computational and memory costs of existing optimal algorithms, while keeping their regret guaranties. We apply our findings to design new variants of the MED and IMED algorithms, and demonstrate their interest with extensive numerical simulations.

## 1 Introduction

A Multi-Armed Bandit (MAB) is a sequential decision-making problem where at each time step \(t\) a learner collects a reward from an arm \(A_{t}\) chosen among \(K\) alternatives. We consider the stochastic case, in which all rewards collected from an arm \(k[K]\) are i.i.d. and drawn from a fixed distribution \(F_{k}\), of expectation \(_{k}\). For a time horizon \(T\) we define the number of pulls of an arm \(k\) by \(N_{k}(T)=_{t=1}^{T}(A_{t}=k)\), and their sub-optimality gap by \(_{k}=^{}-_{k}\), with \(^{}=_{k[K]}_{k}\). The goal of the learner is to maximize their expected sum of rewards, or equivalently to minimize the _regret_, defined as

\[_{T}=[_{t=1}^{T}(^{}-_{A_{t}})] =_{k=1}^{K}_{k}[N_{k}(T)]\;.\] (1)

To achieve this goal, the learner can leverage the rewards collected up to time \(t\) and their knowledge on the family of distributions \(\) to which \(F_{1},,F_{K}\) belong. The definition of \(\) determines the complexity of the bandit problem, since any uniformly efficient algorithm1 satisfies the lower bound (Lai and Robbins, 1985; Burnetas and Katehakis, 1996): for all \(k[K]\) with \(_{k}>0\),

\[_{T}[N_{k}(T)]}{(T)}\!\! {_{}^{}(F_{k},^{})}\;,\;\;\;_{}^{}(F_{k},^{})=_{G}\{ (F_{k},G)\!:\!_{G}[X]\!>\!^{}\}\;,\] (2)where \((.,.)\) denotes the Kullback-Leibler divergence between two distributions. We call an algorithm _asymptotically optimal_ (sometimes shortened to optimal) if its regret upper bound matches this lower bound. That is, if for all \(k\), \(_{T}[N_{k}(T)]/(T) 1/_{}^{ }(F_{k},^{})\).

For some parametric families (e.g. Gaussian), \(_{}^{}\) has a convenient closed-form expression. However, for more general non-parametric families of distributions this may not be the case. In this work we consider the most famous example of such model, where the learner only knows that the distributions are bounded. Given a range \([b,B]\), we formally define

\[_{[b,B]}=\{F():(F)[b,B] \},\] (3)

but in the rest of the paper we keep the notation \(\) (for \(_{[b,B]}\)) for simplicity. We always assume that \(B\) is known, but in some cases knowing that \(b\) is finite will be sufficient. By analyzing a dual optimization problem derived from (2), Honda and Takemura (2010) obtained that

\[ F,\  B\ :_{}^{ }(F,)=_{[0,]} _{X F}[(1-(X-))]\.\] (4)

Optimal algorithms for bounded distributionsThe literature on MABs has become diverse in the past years, so in the following we present a non-exhaustive selection of works that focus on asymptotically optimal algorithms for (bounded) stochastic bandits. We refer the reader to (Lattimore and Szepesvari, 2020; Bubeck et al., 2012) for broader surveys.

The most famous optimal algorithm is certainly KL-UCB (Cappe et al., 2013; Agrawal et al., 2021). Based on the principle of optimism in face of uncertainty (Auer et al., 2002), it uses confidence intervals on empirical \(_{}^{}\) to choose the arm with the largest plausible mean. The optimality of KL-UCB was only proved recently (Agrawal et al., 2021), as the seminal work of Cappe et al. (2013) only proved it for Multinomial distributions. A second family of algorithms, _Minimized Empirical Divergence_ (MED) (Honda and Takemura, 2010, 2011, 2015; Baudry et al., 2023), aims at exploiting the lower bound (2): these algorithms are based on the computation of \(_{}^{}\) for the empirical distributions and the current best empirical mean, and hence differ from KL-UCB (they perform only one computation per arm/step). More recently, an optimal algorithm based on Thompson Sampling (TS) (Thompson, 1933) was proposed for general bounded distributions, under the name of _Non-Parametric Thompson Sampling_ (NPTS) (Riou and Honda, 2020). In this work, the authors generalize the TS algorithms for Bernoulli distributions (Agrawal and Goyal, 2012; Kaufmann et al., 2012) by using an improper Dirac prior and a Dirichlet posterior.

Throughout the paper, we use "MED algorithms" (or simply MED) to refer to the family of algorithms. We denote by MED the randomized algorithm presented in (Honda and Takemura, 2010), that we detail in Algorithm 3, and IMED its deterministic counterpart ((Honda and Takemura, 2015), Algorithm 2).

Cost/performance trade-offIn practice, the choice of a bandit algorithm may be motivated by its theoretical guarantees, but also by its computation and memory costs. Unfortunately, optimal algorithms are quite costly: MED and KL-UCB need to compute an empirical \(_{}^{}\) (costing \((n(n))\) for \(n\) samples, solving (4) with precision \(1/n\)) for each arm/round, KL-UCB being more costly (several \(_{}^{}\) per step), while the cost of sampling in NPTS is linear in the number of samples of each arm. For that reason, cheaper sub-optimal alternatives often considered in place of optimal algorithms. For instance, UCB (Auer et al., 2002) has constant run time and memory, and achieves a logarithmic regret with multiplicative constant \((_{k:_{k}>0}_{k}^{-1})\). More generally, all the algorithms designed for \(1/4\)-sub-gaussian distributions can be used on \(\) if the rewards are rescaled in \(\). A finer approximation consists in using the KL divergence of Bernoulli distributions, that lower bounds \(_{}^{}\)(Cappe et al., 2013). However, it is important to note that these approximations are sensitive to the value (and knowledge) of the lower bound of the support \(b\) (as a rescaling in \(\) is necessary), contrarily to asymptotically optimal algorithms. Pushing further this idea, a procedure proposed by Riou and Honda (2020), inspired by Agrawal and Goyal (2012), consists in _discretizing_ the rewards: a reward \(X[b,B]\) is transformed in \(Y\{x_{1}=b,,x_{M}=B\}\) for some finite grid \(\), such that \([Y|X]=X\). Hence, the expectations of the arms are unchanged while the memory is reduced to \((MK)\) and the computation time of \(_{}^{}\) for discretized distributions is proportional to \(M\). Unfortunately, even if all these techniques lead to logarithmic regret, the multiplicative constant before \((T)\) can be arbitrarily large compared with the optimal one. More precisely, we show in Lemma 6 that for a small gap \(\) the ratio \(_{}^{}/^{2}\) can be of order \(^{-1}\). We detail this result and the description of the discretization procedure in Appendix D.

We illustrate this gap with an example from [Baudry et al., 2021a], that consider a problem of crop-management optimization in agriculture. In Figure 1, we represent four distributions of crop yields generated from the DSSAT2 simulator [Hoogenboom et al., 2019], corresponding to different crop-management policies. The distributions are naturally bounded due to the physical constraints of the problem and are non-parametric. In Table 1 we compare \(^{}_{}\) to the Bernoulli KL-divergence, denoted by kl (see (16) for a definition), and to \(^{2}\) for each arm, and we obtain on the 4-armed bandit that a regret bound scaling with kl or \(2_{k}^{2}\) is almost 10 times larger than the asymptotically optimal regret defined by (2). We include this problem in our experiments of Section 4 to check the practical consequences of this asymptotic property.

Outline and contributionsThe results presented in previous paragraphs motivate the search for novel cheap and asymptotically optimal non-parametric bandit algorithms. We build on MED to propose two novel approaches that achieve this goal. We first propose FMED (resp. FIMED) as a fast variant of MED (resp. IMED), that computes \(^{}_{}\) only for the arm that is pulled while for the other arms a first-order Taylor expansion is used. This simple change translates to a considerable speed-up of the algorithms, as for large enough horizons the best empirical arm (for which \(^{}_{}\) is \(0\)) is pulled most of the time, and preserves the theoretical guarantees of the two algorithms. However, FMED and FIMED still require to store all rewards and to fully compute \(^{}_{}\) (sometimes). Hence we propose another approach, in which estimates of \(^{}_{}\) are computed using an _online portfolio selection_ algorithm. We highlight a property that would ensure that such an algorithm also keeps the guarantees of MED and IMED, while having much faster computation time and at most \((K^{2})\) memory.

 Algorithm & Run time & Memory & Optimality \\  KL-UCB [Cappe et al., 2013] & \((n(n))\) & \(n\) & Opt. \\ kl-UCB & \(((n))\) & \((1)\) & Sub-opt. (kl) \\ UCB [Auer et al., 2002] & \((1)\) & \((1)\) & Sub-opt. (\(2_{k}^{2}\)) \\  NPTS [Riou and Honda, 2020] & \((n)\) & \(n\) & Opt. \\  MED/IMED [Honda and Takemura, 2015] & \((n(n))\) & \(n\) & Opt. \\ Mult. MED/IMED (\(M\) items) & \((M(n))\) & \((M)\) & Sub-opt. (\(^{}_{}\) mult.) \\  FMED/FIMED & \((n(n))\) if pulled, & \(n\) & Opt. \\ (this paper) & \((1)\) otherwise. & & (Theorem 1) \\ OMED/OIMED & & & Opt. under assumptions \\ (this paper) & \((1)\) & \((K)\) & assumptions \\  & & & of Theorem 2 \\ 

Table 2: Comparison of memory and run time needed per step and for an arm \(k\) with \(n\) observationsThe novel algorithms are presented in Section 2, while we discuss their guarantees in Section 3. Finally, in Section 4 we perform numerical simulations that confirm the benefits of our novel algorithms in terms of computation time, and show their strong empirical performance. Table 2 summarizes our results. We detail all computations in Appendix F.1.

## 2 Fast MED algorithms

In the following, we denote by \(F_{k}(t)\), \(_{k}(t)\) and \(^{}(t)=_{k}_{k}(t)\) respectively the empirical distribution and mean of arm \(k\) and the best empirical mean at time \(t\). We propose fast variants of MED and IMED, that avoid computing \(^{}_{}(F_{k}(t),^{}(t))\) for each arm and at each time.

### On-access update and linearization: Fmed and Fimed

We use that \(^{}_{}\) is non-decreasing and convex in its second argument (Lemma 2, prop. 1). Given \(F\) and two thresholds \((^{},)[b,B]^{2}\), and assuming that \(^{}_{}(F,)\) and its derivative (w.r.t \(\)) have already been computed and are available in memory, we can use that

\[^{}_{}(F,^{})\ ^{ }_{}(F,)+^{}_{ }}{}(F,)(^{}-)\.\] (5)

This inequality is a cheap approximation of \(^{}_{}\) from below when only the second argument changes: as long as \(k[K]\) is not pulled its empirical distribution \(F_{k}(t)\) is constant. Furthermore, the derivative is equal to the maximizer of (4) (Honda and Takemura, 2015), which is provided at no additional cost when computing \(^{}_{}\). Using this result, we propose FMED and FIMED as fast variants of MED and IMED respectively, for which at each time step \(^{}_{}(F_{k}(t),^{}(t))\) is replaced by

\[_{k}(t)=\{0,^{}_{}(F_{k}(t ),^{}(s_{k}(t)))+_{k}(s_{k}(t))(^{}(t)-^{ }(s_{k}(t)))\}\,\] (6)

where \(s_{k}(t)=\{s t:A_{s}=k\}\) is the time of last pull of arm \(k\) before \(t\), and for \(s\),

\[_{k}(s)=(s))^{-1}]}{} _{F_{k}(s)}[(1-(X-^{}(s)))]\.\]

We detail their implementation in Appendix A.2. Despite its simplicity, this method already permits a huge gain of computation time. Indeed, in Theorem 1 (see Section 3) we prove that each sub-optimal arm is pulled only \(( T)\) in expectation, which is thus the expected number of full \(^{}_{}\) computations. Additionally, at most one \(^{}_{}\) is computed at each stage, while for MED/IMED \(K-1\) computations are required. However, FMED/FIMED still require to keep every sample in memory. This motivates us to investigate an alternative approach in the next section.

### Sequential update: Omed and Oimed

We now propose bandit algorithms that update \(^{}_{}(F_{k}(t),^{}(t))\) in a purely sequential fashion, for any arm \(k\). We use that its formulation as a maximization problem (4) can be reformulated as an _online portfolio selection_ problem, a class of online convex optimization problems. In the following we drop the index \(k\) to simplify the presentation, and thus use the notation \(N(t)\) and \(F(t)\) for the sample size and empirical cdf of the selected arm at time \(t\), and by \(X_{n}\) the \(n\)-th observation collected by this arm.

FormulationOnline portfolio selection is a sequential interaction between a learner and an adversary. Before the interaction, the adversary selects \((x_{n})_{n[N]}^{N d}\) for a dimension \(d 2\). At iteration \(n\), the learner chooses \(_{n}_{d}^{d}\) (the simplex of dimension \(d-1\)) and receives the reward \((_{n}^{}x_{n})\). The regret of the learner after \(N\) iterations is defined by

\[R_{N}(x_{1},,x_{n})=_{_{d}}_{n=1}^{N}( ^{}x_{n})-_{n=1}^{N}(_{n}^{}x_{n})\.\] (7)

The objective of the learner is then to find a sequence \((_{n})_{n}\), where \(_{n}\) depends only on \((_{1},x_{1},,_{n-1},x_{n-1})\), that minimizes \(R_{N}\). We now express the computation of \(^{}_{}(F(t),^{}(t))\)as a portfolio selection algorithm in dimension 2,

\[N(t)_{}^{}(F(t),^{}(t))=_{ _{2}}_{n=1}^{N(t)}(^{}x_{n}(t))\;, x_{n}(t)=(1,}{B-^{}(t)})\;.\]

However, this expression is unpractical since \(x_{1}(t),,x_{n}(t)\) all depend on \(^{}(t)\), that is not revealed before time \(t\). At the \(n\)-th iteration, we can only use the current best empirical mean, denoted by \(_{n}^{}\) for simplicity: a portfolio algorithm can only try to minimize \(R_{N}(x_{1},,x_{N})\), with \(x_{n}=(1,(B-X_{n})/(B-_{n}^{}))\). We call that quantity _portfolio regret_ in the rest of the paper.

On the other hand, if \(_{n}^{}\) diverges too often from \(^{}(t)\) the estimation of \(_{}^{}\) will not be accurate. We define the _bias_ of the portfolio algorithm (or portfolio bias) by

\[B_{N(t)}(^{}(t))=_{_{2}}_{n=1}^{N_{k}(t)} (^{}x_{n})-_{_{2}}_{n=1}^{N(t)}( ^{}x_{n}(t))\;.\] (8)

This term can be studied independently of the portfolio regret and only depends on the variations of the best empirical mean. It is not negligible: we will have to modify the structure of the MED algorithms and assume that the best mean is not too close to \(B\) to control its magnitude.

In summary, and using all the previously introduced notation, at each stage \(t\) we propose to approximate \(N(t)_{}^{}(F(t),^{}(t))\) by \(L_{N(t)}_{n=1}^{N(t)}(_{n}^{}x_{n})\), where \(_{n}\) is updated sequentially by a portfolio algorithm. Furthermore, the accuracy of the estimate can be expressed in terms of the portfolio bias and regret as follows,

\[N(t)_{}^{}(F(t),^{}(t))=L_{N(t)}+R_{N (t)}+B_{N(t)}(^{}(t))\;,\] (9)

the theoretical results presented in Section 3 are hence obtained by bounding these two quantities.

Portfolio algorithmsSee (Tsai et al., 2023) for a recent review of portfolio selection algorithms. Different methods vary based on their computational complexities and their regret. Some have regret upper bounds as low as \((d T)\) but are computationally expensive, like UPS (Cover, 1991; Cover and Ordentlich, 1996; Kalai and Vempala, 2002). Our goal is to use portfolio methods to obtain a computationally efficient \(_{}^{}\), hence we cannot use those. On the other hand, we can afford a looser regret upper bound since we will use it on \(( T)\) samples from the sub-optimal arms. Other algorithms have \((d)\) computational complexity per round, which is cheap in our case where \(d=2\). This is, for example, the case for Soft-Bayes (Orseau et al., 2017), which has regret \(()\). Other algorithms achieve intermediate trade-offs: see (Zimmer et al., 2022; Tsai et al., 2023). Among the computationally cheap algorithms, we chose Soft-Bayes for its simple implementation and the fact that Orseau et al. (2017) provide bounds on the regret valid for an adaptive step-size parameter, which we need since we don't known the total number of samples we will see for the arms.

While we are the first to use a portfolio algorithm to estimate \(_{}^{}\) in a bandit algorithm, the link between \(_{}^{}\) and portfolio selection was previously used in (Agrawal et al., 2021, Lemma E.1), where the authors use the existence of a portfolio algorithm with logarithmic regret to obtain a concentration inequality on \(_{}^{}\). However, they only use that observation to obtain a bound for the analysis and they don't explore any algorithmic use of the portfolio formulation.

Furthermore, our theoretical analysis is based on a non-standard assumption, that the regret of the portfolio algorithm admits sub-linear upper and lower bounds. We discuss this assumption in the next section.

Algorithm structure of \(\) and \(\)We introduce the \(\) and \(\) algorithms for a generic portfolio algorithm. As discussed above, using an online estimate of empirical \(_{}^{}\) makes the MED algorithms highly sensitive to variations of the best empirical mean. For this reason, we apply several structural changes to the algorithms in order to prevent the portfolio bias to be large, and assume the knowledge of an upper bound \(_{}\) on \(^{}\) with \(_{}<B\).

First, we use a _duel-based_ algorithm: inspired by Chan (2020), at each _round_\(t\) we define a _leader_ as \(_{t}N_{k}(t)\)3. Then, the other arms (called challengers) compete against the leader in pairwise comparisons called _duels_. At the end of the round, all winning challengers (if any) are pulled. If there are none, the leader is pulled. Hence several arms can be pulled per round. The main ideas are to replace \(^{}(t)\) by \(_{_{t}}(t)\) for the reference value used by the challengers, and to implement a different \(_{}^{}\) estimate \(L_{k,_{t}}(t)\) for each possible pair \((k,_{t})\).

We now introduce some notation and terminology to describe the duel between a challenger \(k\) and a leader \(\). In some cases (described below) we perform a _greedy duel_, for which the winner is \(\{_{k}(t),_{}(t)\}\). We introduce a variable \(Z_{k}(t)\), that indicates if the duel played by arm \(k\) was greedy (\(Z_{k}(t)=0\)) or not (\(Z_{k}(t)=1\)), and then \(_{k,}(t)=_{s=1}^{t-1}(k_{s+1}, _{s}=,Z_{k}(s)=1)\), the number of observations of a challenger \(k\) collected _against the leader_\(\) after a _non-greedy duel_.

Using this notation and considering a function \(f:^{+}\) satisfying \(n=o(f(n))\), the duels of OMED/OIMED are implemented as follow: if \(N_{}(t) f(_{k,}(t))\) or \(_{}(t)_{}\) the duel is greedy, otherwise MED and IMED are respectively adapted as follows.

\[ k L_{k,}(t)+( _{k,}(t))(N_{}(t))\.\] (10) \[ k W_{k}(t)=1,\ W_{k}(t)(e^{-L_{k,}(t)})\.\] (11)

Then, we update \(L_{k,}(t)\) using the portfolio selection algorithm _only_ if the duel was non-greedy, i.e. \(Z_{k}(t)=1\), providing \(_{}(t)\) and the reward collected from \(k\): this is the main ingredient to control the portfolio bias in our analysis. We provide a condensed implementation of OIMED in Algorithm 1, and the detailed implementation of OMED in Appendix A.2 (Algorithm 6).

``` Input:\(K\) arms, \(B\), portfolio selection algorithm ALG (and its parameters), function \(f\), \(_{}\). Initialization: Pull each arm once. \((k,)[K]^{2}\) set \(N_{k}=1\), \(_{k,}=0\), \(_{k}=X_{k,1}\), \(L_{k,}=0\), \(_{k}=1/2\) (init. for ALG). for\(t K\)do  Set \(=\{\}\), choose leader \(_{k[K]}N_{k}\) ; \(\) Break ties comparing (\(_{k}\)) for\(k\)do if\(N_{} f(_{k,})\) or \(_{}_{}\)then  add \(k\) to \(\), set \(Z_{k}=0\)) ; \(\) Greedy else (if\(L_{k,}+(_{k,})(N_{})\)then add \(k\) to \(\), set \(Z_{k}=1\)) ; \(\) IMED duel if\(=\{\}\)then add \(l\) to \(\), set \(Z_{l}=0\) ; \(\) Pull leader if no challenger for\(k\)do  Pull \(k\), observe reward \(X\), update \(_{k}\), \(N_{k}\). ; \(\) General update (even if\(Z_{k}=0\)) if\(Z_{k}=1\)then  Update \(_{k,}\) with ALG, knowing \(_{k,}\) and \(_{}}\) ; \(\) Portfolio update \(_{k,}_{k,}+1\), \(L_{k,} L_{k,}+(1-_{k,}_{}}}{B-_{}})\). ```

**Algorithm 1** Online Indexed Minimum Empirical Divergence OIMED

## 3 Theoretical guarantees

We present our theoretical results on the MED algorithms introduced in Section 2, and some insights from their analysis. We start with the theoretical guarantees of FMED and FIMED.

**Theorem 1** (Regret bound for fast MED).: _Consider \((F_{1},,F_{K})^{K}\) and \(^{}=_{k[K]}_{k}\). Then, for any time horizon \(T\) and \(>0\), MED, IMED, FMED and FIMED all satisfy_

\[[N_{k}(T)]_{ }^{}(F_{k},^{})-}+o_{}((T))\,\] (12)

_where \(o_{}((T))\) denotes a term that is asymptotically dominated by \((T)\) for a fixed \(\), but with a polynomial dependency in \(^{-1}\). Furthermore, all the algorithms are **asymptotically optimal**._

We prove Theorem 1 in Appendix B. The main result is that FMED and FIMED both preserve the theoretical guarantees of their original algorithm. We apply our analysis to MED and IMED too, in order to exhibit more precisely the new terms induced by the approximation, and their scaling in \(\). Note that the components of the \(o_{}((T))\) terms are explicit in the proof. When \(\) is small, we obtain more precisely a scaling in \(^{-6}\), that allows to obtain a sub-linear problem-independent bound of order \(T^{5/6}\), though not the optimal \(\) bound. Note however that the same scaling can be obtained for \(\) with the proof techniques presented in (Baudry et al., 2023), so the fast implementation do not deteriorate that bound. The strong empirical performance of \(/\) hints that this result is likely to be not tight, but to the best of our knowledge it remains open to prove that an optimal4 (instance-dependent) algorithm achieves the \(()\) problem-independent bound for general bounded distributions. Finally, we emphasize that the analysis of \(\) presented in Appendix B is drastically simplified compared to Honda and Takemura (2015), although their result is more general (they allow \(b=-\)).

Proof sketch.: First, the inequality \(_{k}(t)_{}^{}(F_{k}(t), ^{}(t))\) (Eq. (5) and (6)) makes \(\) and \(\) at least as exploratory as the vanilla algorithms. Their regret is thus smaller in a _pre-convergence_ regime, defined by the time steps for which \(^{}(t)^{}-\). The main challenge is then to prove that using \((_{k}(t))_{k[K]}\) do not lead to over-exploration of the sub-optimal arms in a _post-convergence_ regime, where \(^{}(t)^{}-\). The main ingredient of the proof consists in showing that this kind of scenario can only be caused by events of the form

\[ j[K]:\ \{A_{t+1}=j,N_{j}(t)=n,_{j,n}^{}+ \}\{A_{t+1}=j,N_{j}(t)=n,_{j,n}^{}- \}\.\]

Each of them can cause _at most \(1\) pull_ of a sub-optimal arm. With union bounds and Hoeffding's inequality we obtain an additional \((^{-2})\) term in the regret bound compared to \(\) and \(\). 

We now provide a very similar result on \(\) and \(\), under some additional assumptions: first, we need the largest mean to be well-separated from \(B\) to control the portfolio bias. This is a mild assumption, since the decision-maker can also choose to slightly increase the value of \(B\) used by the algorithm if it does not clearly hold. Then, we require strong guarantees on the portfolio regret, that we discuss at the end of this section.

**Theorem 2** (Regret bound for online MED algorithms).: _Consider \((F_{1},,F_{K})^{K}\), and \(^{}=_{k[K]}_{k}<_{}\), for a known \(_{}<B\). Assume that \(\) and \(\) use a portfolio selection algorithm \(\) that satisfies for any \((k,)[K]^{2}\) and \(n\) the deterministic guarantee_

\[|R_{k,,n}|=o(n)\.\] (13)

_Then, both \(\) and \(\) satisfy Equation (12) (with a different second-order term) and are **asymptotically optimal**: they enjoy the same asymptotic guarantees as \(\) and \(\)._

Strikingly, the performance of \(/\) only depend on the regret bounds of the portfolio regret. As detailed below, this is largely due to our modified algorithm's structure.

Proof sketch.: The general proof scheme is inspired by (Chan, 2020), that proposed a similar dual-based approach. The main difficulty consists in controlling the deviation of \(L_{k,_{t}}(t)\) from \(_{}^{}(F_{k,_{t}}(t),_{_{t}}(t))\), denoting by \(F_{k,_{t}}(t)\) the empirical distribution of the \(_{k,_{t}}(t)\) observations used to update \(L_{k,_{t}}(t)\). Both over and under-estimation are bad: one may prevent sufficient exploration of the best arm, while the other may lead to over-exploration of sub-optimal arms. Building on the discussion of Section 2, our proof relies on the following crucial result: for fixed arms \((k,)\), a sample size \(_{k,}(t)=n\), and **any threshold**\(\) it holds that

\[n_{}^{}(F_{k,,n},)=L_{k,,n}+R_{k, ,n}+B_{k,,n}()\,\] (14)

where \(B_{k,,n}()\) is the portfolio bias with respect to a fixed threshold \(\), defined in (8), and where \(L_{k,,n}\) and \(F_{k,,n}\) are used to denote \(L_{k,}(t)\) and \(F_{k,}(t)\) when \(_{k,}(t)=n\), for simplicity. In the proofs, the portfolio regret is controlled by assumption, but the bias requires more careful examination. To analyze it, we prove the following result (details in Appendix E). For \(i n\), we denote by \(t_{k,,i}\) the time when the \(i\)-th iteration of \(L_{k,,n}\) occurred.

**Lemma 1** (Deviations of \(B_{k,,n}()\)).: _Let \((0,1)\), and set \(C_{k,,i}=(t_{k,,i})}{B-}\) it holds that_

\[-_{i=1}^{n}(C_{k,,i}^{-1})(_{}(t_{k, ,i})) B_{k,,n}()_{i=1}^{n} (C_{k,,i})(_{}(t_{k,,i}))\.\] (15)

We always use this result for \((_{k:_{k}<^{*}}_{k},^{*})\), and \(_{}(t_{k,,i})<_{}\). Hence, the upper bound (resp. lower bound) of (15) is expected to be sub-linear when \(=1\) (resp. \( 1\)), thanks to the use of different estimates according to the leader's identity. The rate of convergence of these sums is also important: our proof techniques work if \(n=o(N_{}(t_{k,,n}))\), which justifies the greedy duels. Indeed, we need \(e^{an}(B_{k,,n}<-nx)\) to be small enough (for some \(a,x>0\) and large \(n\)), and with this trick we obtain \(e^{an-f(n)y_{s}} 0\) (for some \(y_{x}\) depending on \(x\)). 

The modifications introduced in OMED and OIMED guarantee a small absolute bias with large probability. It is natural to ask whether they are necessary. We cannot formally prove it, but our experiments in Section 4 suggest that the regret of OIMED/OMED may be linear without them.

Assumption on the portfolio regretAs detailed in Section 2 there are several candidates for the implementation of OIMED/IMED. The literature on regret _lower_ bounds is however scarce. In (Gofer and Mansour, 2016), the authors characterize algorithms with non-negative regret, but for linear losses. Guzman et al. (2021) showed that the regret of FTRL algorithms is bounded as \(()\) both from above and below, in a setting that encompasses portfolio selection. Unfortunately, FTRL is computationally inefficient, which makes it unsuitable for our application. We used Soft-Bayes for its simplicity and computational efficiency, but it might not have the desired lower bound. Our problem seems to be among the first where a large negative regret is detrimental, and obtaining a portfolio algorithm with both small computational complexity and a regret lower bound is an open question. On the other hand, it may be that this lower bound requirement could be relaxed in the analysis, and that for example a high-probability lower bound could be enough. Our experimental study in the next section supports the hypothesis that using Soft-Bayes in OIMED gives good regret bounds for the bandit problem.

## 4 Experiments

We numerically assess the regret and run time of the two novel approaches presented in this paper. We benchmark our approaches with the known asymptotically optimal algorithms: IMED(Honda and Takemura, 2015), KL-UCB(Cappe et al., 2013) and NPTS(Riou and Honda, 2020); and the more efficient but sub-optimal UCB(Auer et al., 2002), kl-UCB(Cappe et al., 2013) and kl-IMED (that denotes the binarized version of IMED). We provide the pseudo-code of KL-UCB and NPTS in Appendix A.2 for completeness. Our code is available in the supplementary material of the paper.

We showcase our main findings by presenting a selection of experiments, focusing on FIMED and OIMED, while we present a broader set of experiments and more detailed results in Appendix F. In each case, we plot the average run time and regret of each algorithm, along with their quantiles \(10\%\)-\(90\%\). The run times are dependent of the Python implementation of our algorithms, and are thus only indicative. We refer to Appendix F.1 for discussions on their theoretical complexity.

Distributions from DSSATWe first consider the crop-management optimization problem introduced in Section 1 and detailed for instance in (Baudry et al., 2021), as it illustrates the theoretical advantage of asymptotically optimal algorithms (Figure 1). A learner needs to select a planting date for maize grains among seven possible options. For each distribution, the rewards are drawn uniformly at random among \(10^{4}\) points sampled from the DSSAT simulator, in order to emulate the distributions at a reduced cost.5. Our results, displayed in Figure 2, show that on this problem optimal algorithms also achieve better empirical performance. IMED, NPTS and KL-UCB perform similarly, while kl-IMED and UCB achieve significantly larger regret. Furthermore, FIMED is \(10\) times faster than IMED for \(T=10^{4}\) (and as fast as kl-UCB), with almost no difference in terms of regret. As expected, OIMED is as fast as UCB and kl-IMED. It achieves better regret than those algorithms, but its performance is deteriorated compare to IMED. This can be seen as the cost of the ability to forgetpast observations. In summary, this experiment illustrates the respective benefits of our two novel approaches: FIMED is the fastest algorithm among those with the smallest regret, while OIMED has the smallest regret among the fastest algorithm.

Bernoulli banditFor our second experiment we consider a Bernoulli bandit with several means close to \(0.5\), for which all optimal algorithms match their implementation with the Bernoulli KL-divergence. Intuitively, sequences of \(0\) and \(1\) with high variance may lead to the most potentially confusing inputs for portfolio algorithms, so our objective is to check the performance of OIMED in that case. Our results, summarized in Figure 3, are promising: the average regret of OIMED is on par with other algorithms, while it still is among the fastest. In that experiment only UCB is sub-optimal, and performs much worse than the other algorithms.

Importance of the duel-based structureIn Section 3 we detailed our theoretical motivations for introducing a duel-based approach when approximating \(^{}_{}\) with a portfolio algorithm. We now provide empirical evidence that this change may prevent OIMED from suffering a linear regret. In Figure 4, we compare the regret of OIMED and a variant of OIMED that does not use the duel-based structure of Algorithm 1 for the two experiments introduced in previous paragraphs. In that algorithm, the online estimates of \(^{}_{}\) are updated at each pull of an arm with the current best empirical mean. We observe that the regret of the "no duel" variant of OIMED may be better in some cases: for the DSSAT experiment, its average regret matches the best algorithms of Figure 2. However, on the Bernoulli experiment its regret is linear. Overall, OIMED needs a modification like the duels to ensure that it has sub-linear regret.

Further experimentsIn Appendix F we show that FMED and OMED exhibit the same characteristics as FIMED and OIMED, as expected. We also consider other Bernoulli bandits, for instance with means close to the boundaries of the support. We then show that the conclusions of this section still hold on problems with distributions of various shapes, by testing our algorithms on several examples of Beta distributions. We also investigate the sensitivity of OIMED to the learning rate of Soft-Bayes, which is the only hyper-parameter of the algorithm. Finally, we perform experiments with the discretized IMED algorithms that is briefly presented in the introduction and discussed more thoroughly in Appendix D.

Figure 3: Average regret (left) and run time (right) of the algorithms on a \(6\)-arms Bernoulli bandit problem with means \(\{0.3,0.4,0.45,0.5,0.52,0.55\}\).

Figure 2: Average regret (left) and run time (right) of the algorithms on the DSSAT bandit problem

## 5 Conclusion

We introduced methods to compute efficiently approximations of \(_{}^{}\) and demonstrated their use in algorithms for regret minimization in stochastic bandits. The FMED and FIMED variants have the same asymptotic optimality properties as the base MED algorithms, but have a much reduced computational complexity. OMED and OIMED push the computational gains further and are also memory efficient. They can also be asymptotically optimal, under the hypothesis that the portfolio algorithm they use satisfies a deterministic regret lower bound.

While our experiments show the good practical performance of OIMED and OMED with the Soft-Bayes portfolio algorithm, this question is however still open: can we have a portfolio algorithm which is computationally efficient, does not store all past gains in memory and has sub-linear regret upper and lower bounds?

Finally, our work towards enabling the use of \(_{}^{}\) in a computationally efficient way has potential applications beyond regret minimization in stochastic bandits. First, similar quantities are used in RL algorithms like IMED-RL (Pesquerel and Maillard, 2022). Second, other bandit tasks like best arm identification also have complexities that depend on a \(_{}^{}\) quantity, and could benefit from faster variants of the algorithms that need to compute it (Jourdan et al., 2022).