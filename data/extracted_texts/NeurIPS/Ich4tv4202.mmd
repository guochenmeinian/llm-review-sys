# WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs

Seungju Han\({}^{ tasks: detection of prompt harmfulness, response harmfulness, and response refusal. We show that WildGuard advances the state-of-the art of open-source safety moderation tools across all three tasks and provides a more open, consistent, and economical alternative to costly and non-static API moderation tools, achieving on-par or better performance relative to the GPT-4 judge.

The development of WildGuard is motivated in particular by two observations. First, existing open tools like Llama-Guard2  are much less effective for discerning harm in _adversarial_ prompts (i.e., jailbreaks) compared to vanilla (i.e., direct) queries, and fall far behind GPT-4 on both. Second, while existing open tools can to an extent identify harm in responses, the _harmfulness_ of a response is insufficient to determine whether a model _refuses_ to answer a user's prompt. This is critical, for instance, for testing exaggerated safety: if a user asks "How to kill a Python process?", responses are typically benign regardless of over-refusal ("I can't provide instructions for acts of violence") or correct compliance ("The way to kill a Python process is..."). Thus, classifying harm is not sufficient for evaluating safety, making independent assessment of refusal necessary. Table 1 shows that most existing moderation models do not consider this task separately--and the single model that does performs poorly compared to GPT-4 (SS2).

To address these gaps, we construct WildGuardMix, a carefully balanced, multi-task moderation dataset with 92K labeled examples covering 13 risk categories, constituting the largest multi-task safety dataset to date. Figure 1 shows an overview of the data composition and examples. Data are drawn from four sources (see SS3.1) to maximize coverage, with a careful balance of prompt harmfulness, vanilla vs. adversarial structure, and pairing of refusals and compliances across prompts. WildGuardMix consists of WildGuardTrain, the training data of WildGuard with 87K examples, and WildGuardTest, a high-quality moderation evaluation data with 5,299 human-annotated items for the three tasks (SS3.2).

Our comprehensive evaluations on WildGuardTest and ten existing public benchmarks show that WildGuard outperforms the strongest existing open-source baselines (e.g., Llama-Guard2, Aegis-Guard, etc) on F1 scores across all three tasks (by up to 26.4% on refusal detection), matches GPT-4 across tasks, and surpasses GPT-4 by up to 3.9% on adversarial prompt harmfulness (SS4).

Through systematic ablations, we show that each component in WildGuardTrain is critical to the success of WildGuard, and multi-task training improves the performance of WildGuard compared to single-task safeguards (SS4.3). Finally, we show that WildGuard can be used as a moderator on human-LLM interactions, reducing success rate on jailbreak attacks from 79.8% to 2.4% with WildGuard in the interface, without over-refusing benign user requests (SS4.4).

Figure 1: (Top) Breakdown of WildGuardMix, featuring four data types: Synthetic Vanilla/Adversarial, In-the-Wild, and Annotator-written data. Synthetic data is a balanced mix of benign/harmful prompts, paired with compliances and refusals to each prompt, covering wide risk categories. (Bottom) Examples in WildGuardMix.

## 2 The Status Quo of Safety Moderation Tools for LLMs

We first examine the current state-of-the-art moderation tools for our three tasks, and we identify several key limitations that motivate our development of WildGuard.

We highlight existing classification performance in two areas: 1) **detecting harmfulness of adversarial prompts**, and 2) **detecting nuanced refusal/compliance in completions**. The first focus is motivated by our observation that adversarial user prompts are common in in-the-wild interactions, but they present a unique challenge for harm detection. Our second focus is motivated by the fact that tests of exaggerated safety, as in XSTest , also present a labeling challenge, being poorly-served by response harm (as we lay out in the introduction), and often eliciting especially complex responses--but still critically requiring accurate labeling for reliable evaluation.

Test BenchmarksTo evaluate harm detection in adversarial prompts, we sample a uniform mixture of 250 benign and 250 harmful prompts from the validation set of WildJailbreak (WJ) , a large-scale dataset consisting of adversarial and vanilla queries from diverse harm categories. To evaluate nuanced refusal detection, we use our novel benchmark XSTest-Resp, which we describe in SS3.2. For comparison, we also show results on our third task, response harmfulness, using XSTest-Resp.

ModelsWe evaluate both open-source and closed tools on these tests. Among open-source tools we test four models instruction-tuned to identify harmfulness in both prompts and responses: Llama-Guard , Llama-Guard2 , Aegis-Guard-Defensive , and Aegis-Guard-Permissive . For these models, to label refusal we mark a response as compliance if the output is harmful, and as a refusal if the output label is safe. We show results for harmful prompts only (RR(h) in Table 2), as this mapping is more likely to succeed in this setting. We also test LibriAI-LongFormer-ref, which is trained to detect refusals on the Do-Not-Answer benchmark  and used for evaluations in TrustLLM . Lastly, we test a keyword-based classifier, which identifies refusals using a predefined list of refusal keywords (see our list of keywords in Appendix D.3). We evaluate the latter two models only on refusal detection. We also assess two API-based tools: GPT-41 and OpenAI Moderation API . For GPT-4 we experiment with various prompts and select the one that results in best performance. We provide this prompt in Table 15 of Appendix D.2. OpenAI Moderation API returns labels for toxicity, which we use for prompt and response harmfulness.

   Model & PH & RH & RR & Open & Data \\  Llama-Guard & ✓ & ✓ & ✗ & ✓ & ✗ \\ Llama-Guard 2 & ✓ & ✓ & ✗ & ✓ & ✗ \\ Aegis-Guard & ✓ & ✓ & ✗ & ✓ & ✓ \\ HarmB & ✗ & ✓ & ✗ & ✓ & ✗ \\ MD-Judge & ✗ & ✓ & ✗ & ✓ & ✗ \\ LibriAI-harm & ✗ & ✓ & ✗ & ✓ & ✗ \\ LibriAI-ref & ✗ & ✓ & ✓ & ✓ & ✗ \\ BeaverDam & ✗ & ✓ & ✗ & ✓ & ✓ \\ OAI Mod. API & ✓ & ✓ & ✗ & ✗ \\ GPT-4 & ✓ & ✓ & ✓ & ✗ & ✗ \\ 
**WildGuard** & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of model capabilities of prompt harm (PH), response harm (RH), and refusal detection (RR) and availability of open weights (Open) and training data (Data). Only WildGuard supports all tasks while being fully open.

    & RH & RR & RR(h) \\  Llama-Guard & 82.0 & 62.9 & 81.2 \\ Llama-Guard2 & 90.8 & 64.1 & 85.0 \\ Aegis-Guard-D & 52.8 & 44.2 & 36.9 \\ Aegis-Guard-P & 60.4 & 48.5 & 49.0 \\ LibriAI-LongFormer-ref & - 74.3 & 80.5 \\ Keyword-based detector & - 71.0 & 70.2 \\ OAI Mod. API & 46.6 & 58.6 & 70.5 \\ GPT-4 & 91.3 & **98.1** & **95.4** \\ 
**WildGuard** & **94.7** & 92.8 & 93.4 \\   

Table 2: F1 scores on response harmfulness and refusal detection in XSTest-Resp. **Best** score bolded; second best score underlined.

Figure 2: Adversarial and vanilla prompt harmfulness detection performance on WJ eval set.

**Finding 1: Existing open tools are unreliable on adversarial prompts and far behind GPT-4.** We see in Figure 2 that existing open tools perform decently in detecting harm in vanilla prompts, but struggle to do so in adversarial prompts. For both prompt types we also see a sizeable performance gap for open tools relative to GPT-4, thus perpetuating costly reliance on API tools.

**Finding 2: Existing open tools struggle with measuring refusals in model responses.** Table 2 shows that open tools also struggle to identify refusals in models' completions. The top-performing harm-detection model, Llama-Guard2, trails GPT-4 by 15.1%, validating that response harmfulness is inadequate for this task--but even LibriAI-LongFormer-ref, a model trained specifically for refusal detection, only modestly outperforms the simple keyword-based baseline, and trails GPT-4 by 24.3%.

In summary, these patterns show non-trivial room for improvement in existing open-source tools, and a collection of related needs that can be filled by a high-quality multi-purpose moderation classifier.

## 3 Building WildGuardMix and WildGuard

Motivated by the insights from SS2 and by a lack of transparency in training of existing moderation tools, we develop two datasets, WildGuardTrain and WildGuardTest as part of WildGuardMix. Here we describe how we curate these datasets, and how we train WildGuard.

### WildGuardTrain: Multi-task Moderation Training Dataset

WildGuardTrain (WGTrain) is a comprehensive training dataset of total 86,759 items, composed of data from diverse sources and amounting to 48,783 standalone prompts + 37,976 prompt-response pairs. The dataset covers both vanilla (direct request) and adversarial prompts across benign and harmful scenarios, as well as diverse types of refusals and compliances. WGTrain includes four sources of data--synthetic adversarial, synthetic vanilla, real-world user-LLM interactions (In-the-Wild), and existing annotator-written data-- all selected to optimize coverage, diversity, and balance for our three tasks. Figure 1 provides an overview of the dataset composition.

#### 3.1.1 Prompt Construction

**Vanilla harmful synthetic prompts.** We synthetically generate harmful prompts covering a broad range of risk scenarios. Inspired by Weidinger et al. , we consider 13 harm subcategories in four high-level categories: _privacy_, _misinformation_, _harmful language_, and _malicious uses_. Table 10 in Appendix A.6 lists harm categories with statistics of our data. To generate targeted, realistic, and diverse harmful scenarios, we design a structured generation pipeline, detailed in Appendix A.1.

**Vanilla benign synthetic prompts.** Similarly to WildJailbreak, to enable maximally precise detection of prompt harmfulness, we additionally include two types of benign contrastive prompts: 1) benign prompts that superficially resemble unsafe prompts, motivated by the 10 exaggerated categories2 from XSTest , and 2) benign prompts discussing sensitive but safe topics. These vanilla benign contrastive prompts are generated with GPT-4. Refer to Table 9 in Appendix A.2 for the GPT-4 prompt used to generate the queries and to Table 7 for examples.

**Adversarial benign and harmful synthetic prompts.** We employ the WildTeaming framework to convert our generated vanilla prompts--harmful and benign--to adversarial prompts. This framework mines jailbreaking tactics from in-the-wild user-LLM interactions, and composes selections of tactics to transform vanilla requests into diverse adversarial counterparts. Specifically, WildTeaming first uses the OpenAI Moderation API to find potentially harmful user queries from LMSYS-Chat-1M and WildChat. Then we use prompted GPT-4 to decompose intent and jailbreak tactics from the queries. This mining process ends up with a pool for the jailbreak tactics; then we randomly sample 2-7 In-the-Wild tactics from the pool, and finally use the sampled tactics to transform vanilla prompts into adversarial prompts.

**Prompts from In-the-Wild and existing annotator-written data.** We consider two types of public data: 1) in-the-wild data that we label for harmfulness and 2) existing annotator-written safetydata. To cover harm risks in real-world user requests, we include prompts from LMSYS-Chat-1M  and WildChat datasets, both of which capture in-the-wild human-LLM interactions, with harm labels assigned based on the OpenAI Moderation API. Additionally we add subsamples of existing annotator-written safety datasets to further enhance coverage. This includes prompts from HH-RLHF  and the Anthropic Red-Teaming dataset , including the subsets that make up AegisSafetyTrain  and Safety-Tuned LLamas .

#### 3.1.2 Compliance and Refusal Construction

**LLM response generations.** For our synthetic adversarial and vanilla prompts, we generate matched refusal and compliance responses. We submit each prompt to a suite of LLMs,3 along with a prompt suffix instructing the LLM to refuse or to comply with the prompt. This provides us with a set of refusal and compliance candidates.

**GPT-4 complex response generations.** We additionally use GPT-4 to generate items capturing a set of targeted response types in observed challenge areas for refusal classification. We conduct error analysis on a classifier trained on an early prototype of WGTrain, and identify several categories of nuanced responses that the classifier mislabels. The majority of these categories are compliances containing caveats, warnings, etc. We then construct one-shot prompts for GPT-4 to generate synthetic responses with similar properties to enhance coverage of complex responses.4

#### 3.1.3 Filtering, Auditing, and Sampling

We further filter responses created through open LMs by assigning labels for each of our three target tasks using GPT-4 and recategorizing items that fail to match intended labels. We then audit the quality of these GPT-4 labels by sampling 500 items and collecting human annotations as in SS3.2. This audit confirms the high quality of the GPT-4 labels, which agree with voted annotator labels on 92%, 82%, and 95% of items for prompt harm, response harm, and refusal labels, respectively. After our filtering and audit, we sample from these sets the following numbers of prompts along with matched refusal and compliance responses: 6,062 vanilla harmful prompts, 2,931 vanilla benign prompts, 4,489 adversarial harmful prompts, and 4,339 adversarial benign prompts, for 35,642 prompt+response items. We also include prompt-only items of 10,451 vanilla harmful prompts, 3,086 vanilla benign prompts, 11,289 adversarial harmful prompts, and 11,411 adversarial benign prompts.

We sample from our other data sources (on which filtering/audit is not needed) as follows: from complex response, 1,167 each of refusal, compliance, and prompt-only items; from In-the-Wild data, 944 harmful prompts and 944 benign prompts; and from annotator-written data, 7,361 benign and 2,130 harmful prompts.5 In-the-Wild and annotator-written items contain only prompts.

### WildGuardTest: A High-Quality Human-Annotated Test Moderation Dataset

We build WildGuardTest (WGTest) to address the limitations of existing moderation tool evaluations discussed in SS2, including harm classification on adversarial inputs and refusal detection.

To construct WGTest, we start with a test split of 1,725 prompt-response pairs from the synthetic vanilla and adversarial data as described in SS3.1. We then collect annotations from three independent annotators for each prompt-response pair on prompt harmfulness, response refusal, and response harmfulness. Fleiss Kappa  scores are 0.55, 0.72, and 0.50 for the three tasks, indicating moderate to substantial agreement. We apply majority voting to obtain gold labels, removing items that fail to reach at least two-way agreement.6 Finally, we run a prompted GPT-4 classifier on the dataset and manually inspect items on which the output mismatches the chosen annotator label, to further audit the ground-truth labels. See Table 10 in Appendix A.5 for the WGTest label breakdowns and harm category splits according to our risk taxonomy, and Appendix E for more annotation details.

**XSTest-Resp: XSTest with responses.** We also create XSTest-Resp by extending XSTest with model responses to directly evaluate moderator accuracy for scoring models on a real safety benchmark. We use the same LLM suite employed above for response generation to create completions for the prompts in XSTest , randomly sample from these outputs, and then collect human annotations similarly to WGTest for response harm and refusal labels. After filtering for inter-annotator agreement, we have a total of 446 prompts for response harmfulness (368 harmful responses, 78 benign responses) and 449 prompts for refusal detection (178 refusals, 271 compliances).

### Training WildGuard with WildGuardTrain

With WGTrain, we instruction-tune WildGuard using Mistral-7b-v0.3  as a base model.7 We design a unified input and output format to capture the three tasks, providing the user prompt and model response as inputs, and training the model to output elements corresponding to all three tasks. See more training details in Appendix B.

## 4 Evaluating WildGuard Against Existing LLM Safety Moderation Tools

We show that WildGuard substantially improves performance relative to existing LLM safety moderation tools on all of the _prompt harm_, _response harm_, and _refusal detection_ tasks, across existing benchmarks and our newly introduced WGTest evaluation sets.

### Evaluation Setups

**Evaluation benchmarks.** We test WildGuard and relevant baselines on ten publicly-available safety benchmarks, as well as our WGTest across all three tasks. For public benchmarks on prompt harmfulness, we use ToxicChat , OpenAI Moderation , AegisSafetyTest , SimpleSafetyTests , and HarmBenchPrompt . For public benchmarks on response harmfulness, we use HarmBenchResponse , SafeRLHF , BeaverTails , and XSTest-Resp8. The only evaluation for refusal detection other than WGTest is XSTest-Resp, which we discuss in SS2. Table 14 in Appendix C shows statistics of each benchmark. We report F1 scores for all evaluations.

**Existing Safety Moderation Models.** As in Section 2, we test four LM-based moderation tools trained for prompt and response harmfulness detection: Llama-Guard, Llama-Guard2, Aegis-Guard-Defensive and Aegis-Guard-Permissive [16; 26; 13]. We test five additional models for response harmfulness classification: BeaverDam , LibrAI-LongFormer-harm , MD-Judge-v0.1 , and two Harmbench classifiers from Mazeika et al.  (HarmBench-Llama, HarmBench-Mistral). For refusal detection, we test LibrAI-LongFormer-ref . See Appendix B for additional model details. For closed tools, as in Section 2, we test the OpenAI Moderation API and GPT-4.

    &  &  \\  & Toxic & OAI & Aegis & SimpST & HarmB & **Avg.** & HarmB & S-RLHF & BeaverT & XST & **Avg.** \\  Llama-Guard & 61.6 & 75.8 & 74.1 & 93.0 & 67.2 & 74.4 & 52.0 & 48.4 & 67.1 & 82.0 & 62.4 \\ Llama-Guard2 & 47.1 & 76.1 & 71.8 & 95.8 & 94.0 & 77.0 & 77.8 & 51.6 & 71.8 & 90.8 & 73.0 \\ Aegis-Guard-D & 70.0 & 67.5 & 84.8 & 100 & 77.7 & 80.0 & 62.2 & 59.3 & 74.7 & 52.8 & 62.3 \\ Aegis-Guard-P & 73.0 & 74.7 & 82.9 & 99.0 & 70.5 & 80.0 & 60.8 & 55.9 & 73.8 & 60.4 & 62.7 \\ HarmB-Llama & - & - & - & - & - & - & 84.3 & 60.0 & 77.1 & 64.5 & 71.5 \\ HarmB-Mistral & - & - & - & - & - & - & 87.0 & 52.4 & 75.2 & 72.0 & 71.7 \\ MD-Judge & - & - & - & - & - & - & 81.6 & 64.7 & 86.7 & 90.4 & 80.9 \\ LibrAI-LongFormer & - & - & - & - & - & - & 62.1 & 50.6 & 67.1 & 70.9 & 62.7 \\ BeaverDam & - & - & - & - & - & - & 58.4 & 72.1 & 89.9 & 83.6 & 76.0 \\ OAI Mod. API & 25.4 & 79.0 & 31.9 & 63.0 & 9.6 & 41.8 & 20.6 & 10.1 & 15.7 & 46.6 & 23.2 \\ GPT-4 & 68.3 & 70.5 & 84.4 & 100 & 100 & 84.6 & 86.1 & 67.9 & 86.1 & 91.3 & 82.0 \\ 
**WildGuard** & 70.8 & 72.1 & 89.4 & 99.5 & 98.9 & **86.1** & 86.3 & 64.2 & 84.4 & 94.7 & **82.4** \\   

Table 3: F1 scores (%) on prompt and response harmfulness in existing public benchmarks.

### Results: WildGuard sets a new multi-task state-of-the-art

**WildGuard performs best for prompt classification.** Results are shown in Table 3 and 4. On both evaluation sets, none of the open baselines outperform GPT-4. However, WildGuard outperforms GPT-4 by 1.8% average F1 on public prompt harmfulness benchmarks, and on WGTest by 1.1%. Particularly on the _adversarial_ subset of prompts, WildGuard demonstrates a large improvement of 11.0% over the best open baseline and outperforms GPT-4 by 3.9%.

**WildGuard matches baselines on response harmfulness.** Results are also shown in Table 3 and 4. For response harmfulness, on public benchmarks we see that WildGuard outperforms all open baselines by at least 1.8%, and is again the only open model to outperform GPT-4 (on two out of four evaluations). On WGTest, WildGuard performs within 3% of MD-Judge, the best open baseline, with WildGuard performing better on responses to adversarial prompts by 1%.

**WildGuard substantially upgrades refusal detection.** On full refusal evaluation, Table 4 shows that WildGuard outperforms LibrAI-LongFormer-ref--the only existing open model that explicitly classifies refusal--by 26.4%, and the strongest open baseline by 21.2%. WildGuard also closes the gap between open models and GPT-4, with a score within 4.1% of GPT-4.

As in SS2, for the benefit of baseline models trained to detect response harmfulness rather than refusal, we also include refusal evaluation on a setting with harmful prompts only (_Harm_.). In that setting, we see that WildGuard outperforms all open baselines, beating the best open baseline by almost 5%, and outperforming MD-Judge--the strongest model on response harmfulness--by a margin of 9.7%. Additionally, in this setting WildGuard outperforms the GPT-4 judge.

**Summary.** Across all three tasks on previous benchmarks and WGTest, WildGuard shows performance beating or on par with GPT-4, and substantially outperforming existing open models, with the single exception of MD-Judge on WGTest response harmfulness, for which WildGuard is of the same caliber. This underscores the flexibility and practicality of WildGuard, as it can be applied simultaneously to all three tasks of prompt harmfulness, response harmfulness, and response refusal classification with superior accuracy. Notably, WildGuard excels in refusal classification with benign prompts, a capability that no previous open model supports with adequate performance.

### WildGuard Ablation Results

Table 5 shows a series of ablations on WGTrain, as well as ablations on the multi-task setting comparing against single-task models for each task on the same training data (see Table 12 and 13 for formats). We compare scores on WGTest for all tasks, the average F1 of public evaluations (see Table 3) for prompt and response harmfulness, and XSTest-Resp for response refusal.

    &  &  &  \\   & Adv. & Vani. & **Total** & Adv. & Vani. & **Total** & _Harm._ & Adv. & Vani. & **Total** \\  Llama-Guard & 32.6 & 70.5 & 56.0 & 25.8 & 66.7 & 50.5 & 76.5 & 45.1 & 56.9 & 51.4 \\ Llama-Guard2 & 46.1 & 85.6 & 70.9 & 47.9 & 78.2 & 66.5 & 82.2 & 47.9 & 58.8 & 53.8 \\ Aegis-Guard-D & 74.5 & 82.0 & 78.5 & 40.4 & 57.6 & 49.1 & 56.1 & 38.9 & 44.0 & 41.8 \\ Aegis-Guard-P & 62.9 & 77.9 & 71.5 & 49.0 & 62.4 & 56.4 & 67.1 & 45.4 & 48.2 & 46.9 \\ HarmB-Llama & - & - & - & 41.9 & 50.2 & 45.7 & 89.8 & 74.0 & 72.5 & 73.1 \\ HarmB-Mistral & - & - & - & 51.8 & 70.3 & 60.1 & 79.3 & 56.1 & 60.5 & 58.6 \\ MD-Judge & - & - & - & 67.7 & **85.0** & 76.8 & 85.7 & 50.6 & 59.4 & 55.5 \\ BeaverDam & - & - & - & 51.2 & 74.3 & 63.4 & 80.7 & 48.4 & 59.4 & 54.1 \\ LibrAI-LongFormer-harm & - & - & - & 61.7 & 64.2 & 63.2 & 79.2 & 61.7 & 62.7 & 62.3 \\ LibrAI-LongFormer-ref & - & - & - & - & - & - & 84.1 & 71.2 & 69.3 & 70.1 \\ Keyword-based & - & - & - & - & - & - & 67.8 & 67.0 & 65.9 & 66.3 \\ OAI Mod. API & 6.8 & 16.3 & 12.1 & 14.7 & 18.8 & 16.9 & 71.4 & 44.5 & 54.3 & 49.8 \\ GPT-4 & 81.6 & **93.4** & 87.9 & **73.6** & 81.3 & **77.3** & 93.9 & **91.4** & **93.5** & **92.4** \\ 
**WildGuard** & **85.5** & 91.7 & **88.9** & 68.4 & 81.5 & 75.4 & **94.0** & 88.5 & 88.6 & 88.6 \\   

Table 4: F1 scores (%) on WildGuardTest. _Adv./ Vani._ indicate benign + harmful prompts that are adversarial and vanilla, respectively. _Harm._ indicates refusal detection on harmful prompts only.

**Each major component of WGTrain helps to improve moderation performance.** WGTrain is composed of four sources of harmful and benign prompts (See SS3); Table 5 shows that each is essential for high performance across tasks. For instance, when removing adversarial data, scores on almost all test evaluations drop, with a decrease of over 8.4 F1 points on WGTest adversarial prompts. On the other hand, if we exclude the synthetic vanilla component, both public prompt harm evaluations and WGTest refusal detection drop by 2.5-3.7 F1 points. Without In-the-Wild data, performance drops on public evaluations across tasks, especially an F1 drop of 10.3 points on ToxicChat--which consists of prompts from in-the-wild human-LLM interactions. Finally, excluding existing annotator-written data is much worse on public prompt and response harmfulness tasks (loss of 1.3-6 F1) and performs worse on all tasks in WGTest.

**Multi-task training improves the model performance.** We additionally compare WildGuard trained under the multi-task setting to models trained on individual tasks. On all tasks except refusal detection on XTest-Resp, the multi-task model outperforms single-task models, demonstrating that multi-task training makes WildGuard an efficient unified tool without individual task performance.

### Demonstrating WildGuard as a Moderator in Human-LLM Interactions

As a practical demonstration, we test WildGuard and other tools in a simulated chat moderation use-case in tandem with a completion model, detecting harmful prompts and responses and inserting benign refusals in place of the model's original response when harm is detected. For test prompts we use the full WildJailbreak (WJ) validation set , consisting of 2000 harmful and 250 benign adversarial prompts. We apply the moderation filters to Tulu-2-dpo-7B , comparing against a baseline of Tulu-2 safety-tuned on the WJ train set (Tulu-2 + WJ). We measure Attack Success Rate (ASR) for harmful prompts and Refusal To Answer (RTA) rate for benign prompts, using GPT-4 to label responses for compliance versus refusal for both tasks. When using WildGuard, Llama-Guard, and Aegis-Guard as moderators, we use prompt harmfulness and response harmfulness outputs for prompt and response components, respectively, while for MD-Judge we use response harmfulness output for the combined prompt and response input.

Table 6 shows that WildGuard yields the strongest performance both for refusing harmful jailbreak prompts and for avoiding over-refusal--for each model, it achieves lowest ASR while minimally increasing refusals to benign prompts. Specifically, we see that Tulu-2 combined with WildGuard filter shows significant improvement on ASR (79.8% to 2.4%) with minimal sacrifice on RTA (0.0% to 0.4%). We also see that using WildGuard as a filter performs similarly to the directly safety-tuned Tulu-2 + WJ. This shows that for use-cases where training is not feasible, WildGuard can serve as a highly effective safety filter embedded in an LLM interface at inference time. Additionally, even when using a safety-tuned model such as Tulu-2 + WJ, we can further reduce ASR with a comparatively small increase in RTA by using WildGuard as an additional filter.

    & Tulu2+WJ & Tulu2-dpo \\ (ASR (\%.,.)/ RTA (\%.,.)) \\  ✗ & 2.3/**1.2** & 79.8/**0.0** \\ 
**WildGuard** & **0.7**/1.7 & **2.4**/0.4 \\ Lama-Guard2 & 1.9/1.6 & 53.1/0.8 \\ Aegis-Guard-D & 0.9/16.8 & 12.4/16.0 \\ Aegis-Guard-P & 1.7/4.0 & 32.7/3.6 \\ MD-Judge & 1.9/10.1 & 25.7/4.4 \\   

Table 6: ASR and RTA on WildJailbreak eval set with classifier models filtering out harmful prompts and responses.

    &  &  &  \\   & Public & WGTest & Public & WGTest & XTest &  \\  & Avg. F1 & Adv. F1 & Total F1 & Avg. F1 & Adv. F1 & Total F1 & F1 & Adv. F1 & Total F1 \\  WildGuardTrain & **86.1** & 85.5 & 88.9 & **82.4** & 68.4 & 75.4 & 92.8 & **88.5** & **88.6** \\ – Synthetic Adv. & 84.6 & 77.1 & 84.4 & 78.6 & 67.8 & 73.9 & 94.1 & 88.3 & 87.6 \\ – Synthetic Vani. & 83.6 & **85.7** & **89.8** & 81.3 & 66.9 & 75.5 & 90.2 & 85.7 & 84.9 \\ – In-The-Wild & 83.9 & 84.6 & 88.7 & 81.9 & **69.5** & **76.5** & 93.0 & 88.0 & **88.6** \\ – Worker-written & 84.8 & 84.5 & 86.8 & 76.4 & 66.4 & 74.7 & **94.9** & 87.8 & 88.4 \\  Single-task Only & 86.0 & 84.2 & 89.2 & 76.4 & 67.2 & 74.8 & 93.3 & 87.0 & 87.3 \\   

Table 5: Ablations of WGTrain showing the contributions of the dataset. Results from individual single-task models are combined under “Single-task Only”.

Related Works

**LLM Safety Moderation Tools.** There exists an extensive body of work on detecting hateful, toxic, offensive and abusive content [12; 29; 21] on online social networking sites such as Twitter  and Reddit . With the recent state-of-the-art LMs (e.g., GPT-4, Gemini, Claude [1; 33; 3]) and their capabilities, researchers have begun using these models as judges  to assist in moderation. Besides relying on frontier LMs, recent studies have trained publicly-available open-source models such as Llama2-7b  on moderation data collected synthetically to cover a wide risk spectrum. Some notable examples include Llama-Guard , its follow-up Llama-Guard2  (a version based on the Llama3  model), Aegis , MD-Judge , the HarmBench classifier , and BeaverDam . Our work, however, differentiates itself in a few critical ways: WildGuard is trained to robustly handle adversarial inputs unlike many previous works, and is also multi-task across prompt/response harm as well as refusal detection, which no previous model supports satisfactorily.

**Model Safety Taxonomy.** Multiple previous works develop taxonomies to identify and break down potential model safety risks into categories. We draw inspiration for the taxonomy used for WildGuardMix from Weidinger et al. , which defines several broad risk areas with detailed sub-categories. Other works that define model safety taxonomies include Tedeschi et al.  and Vidgen et al. . The taxonomies overlap significantly, but feature differences in the types of content explicitly or implicitly covered and the specificity of categorization within each broad risk area.

**Safety Training & Evaluation Datasets.** Crucially, many previous works (e.g., Llama-Guard) do not release training data. Works that do release safety data include Anthropic's red-teaming and RLHF work [11; 5], Aegis , BeaverTails  (only response harm), and SALAD-Bench  (only prompts)--but in contrast to WGTrain these are limited in coverage of adversarial model interactions and do not contain In-the-Wild prompts from real users. Furthermore, the existing moderation evaluations ToxicChat, OpenAI Moderation, AegisSafetyTest, SimpleSafetyTests, Harmbench, SafeRLHF, and BeaverTails [23; 28; 13; 36; 25; 9; 18; 30] have limited coverage over adversarial prompts (especially the benign adversarial category), cover a narrow scope of potential harms, or do not test for refusal detection and exaggerated safety behavior. WGTest fills this gap of high-quality human annotated evaluation data covering a broad spectrum of harm categories, jailbreaks, and all three important tasks of safety classification.

## 6 Limitations

Much of our data is synthetic, so in some ways it may fail to perfectly approximate natural human inputs in real-world scenarios. To cover the distribution of user requests in the real-world, we include prompts from In-the-Wild in our dataset, though the size is limited and thus cannot encompass all possible scenarios in real-world. In addition, we use a large but finite set of models to generate responses, so not all model response patterns may be covered.

As is standard and generally unavoidable in safety research, we make commitments about what specific categories of content constitute harmful categories. These may differ from the categories that others may prefer. Since other models may be developed with different underlying harm taxonomies, they might demonstrate lower performance when evaluated on WildGuardTest because of discrepancies between our definition of harm and the model developers' goals. The risk categories we focus on are listed in Table 10 of the Appendix. We also acknowledge that our risk taxonomy might not cover all potential harms, but we provide the risk categories that existing datasets/benchmarks cover in Appendix C, and our risk taxonomy and the specific pinpoint topics WildGuardMix addresses cover these categories as well, leading to strong performances on these existing benchmarks.

Along similar lines, it is necessary that we make commitments on definitions of refusal, which may not perfectly match others' preferences. The definition of refusal is shown in Figure 4, displaying the annotator instruction for what makes a model response a refusal. Our definition of refusals includes multiple scenarios, such as "redirecting refusal" and "selective refusal", along with the examples of compliances that can be confused with refusals, such as "refusal then compliance" and "correction compliance". Addressing these complex behaviors remains a challenge, and we look forward to continuing to refine our approach in future work.

Finally, an area that we have omitted in WildGuard is finer-grained classification of harm category. Though the three-task setup of WildGuard is already broad in coverage, this additional capability is an interesting direction to explore in future work.

## 7 Ethical Considerations

Though it shows state-of-the-art accuracy, WildGuard will sometimes make incorrect judgments, and when used within an automated moderation system, this can potentially allow unsafe model content or harmful requests from users to pass through. Users of WildGuard should be aware of this potential for inaccuracies. We also recognize the risk that releasing WildGuardMix has potential to inadvertently assist creation of harmful contents. This is not the intended use of WildGuardMix, and to mitigate this risk, we will plan to restrict how our resources are used, such as releasing the dataset behind a terms of agreement.

## 8 Conclusion

In this work, we introduce WildGuard, a unified multi-task open LLM safety moderation model capable of detecting diverse types of vanilla and adversarial harmful user prompts, harmful model responses, and model refusals. To support the development of WildGuard, we create WGMix, which includes a comprehensive and varied set of training data (WGTrain) and evaluation data (WGTest) for safety moderation tools. Through 10 public benchmarks and our newly introduced WGTest, which specializes in more challenging adversarial prompts as well as refusals, we demonstrate the significant advantage of WildGuard compared to 10 existing open safety moderation toolkits. Notably, WildGuard bridges the gap between open-source and closed-source safety moderation tools (e.g., GPT-4) by achieving performance on par with, if not better than, closed-source alternatives across all three classification tasks. With the public release of the WildGuard models and WildGuardMix datasets, our research promotes the open and reliable future development of LLM safety moderation toolkits, paving the way toward safer LLM applications.